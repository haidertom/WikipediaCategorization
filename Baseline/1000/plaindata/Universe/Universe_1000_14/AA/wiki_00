{"id": "39136", "url": "https://en.wikipedia.org/wiki?curid=39136", "title": "Accelerating expansion of the universe", "text": "Accelerating expansion of the universe\n\nThe accelerating expansion of the universe is the observation that the expansion of the universe is such that the velocity at which a distant galaxy is receding from the observer is continuously increasing with time.\n\nThe accelerated expansion was discovered in 1998, by two independent projects, the Supernova Cosmology Project and the High-Z Supernova Search Team, which both used distant type Ia supernovae to measure the acceleration. The idea was that as type 1a supernovae have almost the same intrinsic brightness (a standard candle), and since objects that are further away appear dimmer, we can use the observed brightness of these supernovae to measure the distance to them. The distance can then be compared to the supernovae's cosmological redshift, which measures how much the universe has expanded since the supernova occurred. The unexpected result was that objects in the universe are moving away from another at an accelerated rate. Cosmologists at the time expected that recession velocity would always be decelerating to the gravitational attraction of the matter in the universe. Three members of these two groups have subsequently been awarded Nobel Prizes for their discovery. Confirmatory evidence has been found in baryon acoustic oscillations, and in analyses of the clustering of galaxies.\n\nThe accelerated expansion of the universe is thought to have begun since the universe entered its dark-energy-dominated era roughly 5 billion years ago.\nWithin the framework of general relativity, an accelerated expansion can be accounted for by a positive value of the cosmological constant , equivalent to the presence of a positive vacuum energy, dubbed \"dark energy\". While there are alternative possible explanations, the description assuming dark energy (positive ) is used in the current standard model of cosmology, which also includes cold dark matter (CDM) and is known as the Lambda-CDM model.\n\nIn the decades since the detection of cosmic microwave background (CMB) in 1965, the Big Bang model has become the most accepted model explaining the evolution of our universe. The Friedmann equation defines how the energy in the universe drives its expansion.\n\nwhere the pressure is defined by the cosmological model chosen. (see explanatory models below)\n\nPhysicists at one time were so assured of the deceleration of the universe's expansion that they introduced a so-called deceleration parameter . Current observations point towards this deceleration parameter being negative.\n\nAccording to the theory of cosmic inflation, the very early universe underwent a period of very rapid, quasi-exponential expansion. While the time-scale for this period of expansion was far shorter than that of the current expansion, this was a period of accelerated expansion with some similarities to the current epoch.\n\nThe definition of \"accelerating expansion\" is that the second time derivative of the cosmic scale factor, formula_2, is positive, which implies that the deceleration parameter is negative. However, note this does not imply that the Hubble parameter is increasing with time. Since the Hubble parameter is defined as formula_3, it follows from the definitions that the derivative of the Hubble parameter is given by \n\nso the Hubble parameter is decreasing with time unless formula_5. Observations prefer formula_6, which implies that formula_2 is positive but formula_8 is negative. Essentially, this implies that the cosmic recession velocity of any one particular galaxy is increasing with time, but its velocity/distance ratio is still decreasing; thus different galaxies expanding across a sphere of fixed radius cross the sphere more slowly at later times.\n\nIt is seen from above that the case of \"zero acceleration/deceleration\" corresponds to formula_9 is a linear function of formula_10, formula_11, formula_12, and formula_13.\n\nTo learn about the rate of expansion of the universe we look at the magnitude-redshift relationship of astronomical objects using standard candles, or their distance-redshift relationship using standard rulers. We can also look at the growth of large-scale structure, and find that the observed values of the cosmological parameters are best described by models which include an accelerating expansion.\n\nThe first evidence for acceleration came from the observation of Type Ia supernovae, which are exploding white dwarfs that have exceeded their stability limit. Because they all have similar masses, their intrinsic luminosity is standardizable. Repeated imaging of selected areas of the sky is used to discover the supernovae, then follow-up observations give their peak brightness, which is converted into a quantity known as luminosity distance (see distance measures in cosmology for details). Spectral lines of their light can be used to determine their redshift.\n\nFor supernovae at redshift less than around 0.1, or light travel time less than 10 percent of the age of the universe, this gives a nearly linear distance–redshift relation due to Hubble's law. At larger distances, since the expansion rate of the universe has changed over time, the distance-redshift relation deviates from linearity, and this deviation depends on how the expansion rate has changed over time. The full calculation requires computer integration of the Friedmann equation, but a simple derivation can be given as follows: the redshift directly gives the cosmic scale factor at the time the supernova exploded.\n\nSo a supernova with a measured redshift implies the universe was  =  of its present size when the supernova exploded. In the standard accelerated expansion scenario the rate of expansion still decreases, but does so more slowly than the non-accelerated case. This means that in the accelerated case, the past rate of expansion is slower than it would be in the non-accelerated case. Thus in a universe with accelerated expansion, it takes a longer time to expand from two thirds its present size, compared to a non-accelerating universe with the same present-day value of the Hubble constant. This results in a larger light-travel time, larger distance and fainter supernovae, which corresponds to the actual observations. Adam Riess \"et al.\" found that \"the distances of the high-redshift SNe Ia were, on average, 10% to 15% farther than expected in a low mass density universe without a cosmological constant\". This means that the measured high-redshift distances were too large, compared to nearby ones, for a decelerating universe.\n\nIn the early universe before recombination and decoupling took place, photons and matter existed in a primordial plasma. Points of higher density in the photon-baryon plasma would contract, being compressed by gravity until the pressure became too large and they expanded again. This contraction and expansion created vibrations in the plasma analogous to sound waves. Since dark matter only interacts gravitationally it stayed at the centre of the sound wave, the origin of the original overdensity. When decoupling occurred, approximately 380,000 years after the Big Bang, photons separated from matter and were able to stream freely through the universe, creating the cosmic microwave background as we know it. This left shells of baryonic matter at a fixed radius from the overdensities of dark matter, a distance known as the sound horizon. As time passed and the universe expanded, it was at these anisotropies of matter density where galaxies started to form. So by looking at the distances at which galaxies at different redshifts tend to cluster, it is possible to determine a standard angular diameter distance and use that to compare to the distances predicted by different cosmological models.\n\nPeaks have been found in the correlation function (the probability that two galaxies will be a certain distance apart) at , indicating that this is the size of the sound horizon today, and by comparing this to the sound horizon at the time of decoupling (using the CMB), we can confirm the accelerated expansion of the universe.\n\nMeasuring the mass functions of galaxy clusters, which describe the number density of the clusters above a threshold mass, also provides evidence for dark energy . By comparing these mass functions at high and low redshifts to those predicted by different cosmological models, values for and are obtained which confirm a low matter density and a non zero amount of dark energy.\n\nGiven a cosmological model with certain values of the cosmological density parameters, it is possible to integrate the Friedmann equations and derive the age of the universe.\n\nBy comparing this to actual measured values of the cosmological parameters, we can confirm the validity of a model which is accelerating now, and had a slower expansion in the past.\n\nRecent discoveries of gravitational waves through LIGO and VIRGO not only confirmed Einstein's predictions but also opened a new window into the universe. These gravitational waves can work as sort of standard sirens to measure the expansion rate of the universe. Abbot et al. 2017 measured the Hubble constant value to be approximately 70 kilometres per second per megaparsec. The amplitudes of the strain 'h' is dependent on the masses of the objects causing waves, distances from observation point and gravitational waves detection frequencies. The associated distance measures are dependent on the cosmological parameters like the Hubble Constant for nearby objects and will be dependent on other cosmological parameters like the dark energy density, matter density, etc. for distant sources.\n\nThe most important property of dark energy is that it has negative pressure (repulsive action) which is distributed relatively homogeneously in space.\n\nwhere is the speed of light and is the energy density. Different theories of dark energy suggest different values of , with for cosmic acceleration (this leads to a positive value of in the acceleration equation above).\n\nThe simplest explanation for dark energy is that it is a cosmological constant or vacuum energy; in this case . This leads to the Lambda-CDM model, which has generally been known as the Standard Model of Cosmology from 2003 through the present, since it is the simplest model in good agreement with a variety of recent observations. Riess \"et al.\" found that their results from supernovae observations favoured expanding models with positive cosmological constant () and a current accelerated expansion ().\n\nCurrent observations allow the possibility of a cosmological model containing a dark energy component with equation of state . This phantom energy density would become infinite in finite time, causing such a huge gravitational repulsion that the universe would lose all structure and end in a Big Rip. For example, for and  =70 km·s·Mpc, the time remaining before the universe ends in this Big Rip is 22 billion years.\n\nThere are many alternative explanations for the accelerating universe. Some examples are quintessence, a proposed form of dark energy with a non-constant state equation, whose density decreases with time. Dark fluid is an alternative explanation for accelerating expansion which attempts to unite dark matter and dark energy into a single framework. Alternatively, some authors have argued that the accelerated expansion of the universe could be due to a repulsive gravitational interaction of antimatter or a deviation of the gravitational laws from general relativity. The measurement of the speed of gravity with the gravitational wave event GW170817 ruled out many modified gravity theories as alternative explanation to dark energy.\n\nAnother type of model, the backreaction conjecture, was proposed by cosmologist Syksy Räsänen: the rate of expansion is not homogenous, but we are in a region where expansion is faster than the background. Inhomogeneities in the early universe cause the formation of walls and bubbles, where the inside of a bubble has less matter than on average. According to general relativity, space is less curved than on the walls, and thus appears to have more volume and a higher expansion rate. In the denser regions, the expansion is slowed by a higher gravitational attraction. Therefore, the inward collapse of the denser regions looks the same as an accelerating expansion of the bubbles, leading us to conclude that the universe is undergoing an accelerated expansion. The benefit is that it does not require any new physics such as dark energy. Räsänen does not consider the model likely, but without any falsification, it must remain a possibility. It would require rather large density fluctuations (20%) to work.\n\nA final possibility is that dark energy is an illusion caused by some bias in measurements. For example, if we are located in an emptier-than-average region of space, the observed cosmic expansion rate could be mistaken for a variation in time, or acceleration. A different approach uses a cosmological extension of the equivalence principle to show how space might appear to be expanding more rapidly in the voids surrounding our local cluster. While weak, such effects considered cumulatively over billions of years could become significant, creating the illusion of cosmic acceleration, and making it appear as if we live in a Hubble bubble. Yet other possibilities are that the accelerated expansion of the universe is an illusion caused by the relative motion of us to the rest of the universe, or that the supernovae sample size used wasn't large enough.\n\nAs the universe expands, the density of radiation and ordinary dark matter declines more quickly than the density of dark energy (see equation of state) and, eventually, dark energy dominates. Specifically, when the scale of the universe doubles, the density of matter is reduced by a factor of 8, but the density of dark energy is nearly unchanged (it is exactly constant if the dark energy is a cosmological constant).\n\nIn models where dark energy is a cosmological constant, the universe will expand exponentially with time in the far future, coming closer and closer to a de Sitter spacetime. This will eventually lead to all evidence for the Big Bang disappearing, as the cosmic microwave background is redshifted to lower intensities and longer wavelengths. Eventually its frequency will be low enough that it will be absorbed by the interstellar medium, and so be screened from any observer within the galaxy. This will occur when the universe is less than 50 times its current age, leading to the end of cosmology as we know it as the distant universe turns dark.\n\nA constantly expanding universe with non-zero cosmological constant has mass density decreasing over time, to an undetermined point when zero matter density is reached. All matter (electrons, protons and neutrons) would ionize and disintegrate, with objects dissipating away.\n\nAlternatives for the ultimate fate of the universe include the Big Rip mentioned above, a Big Bounce, Big Freeze or Big Crunch.\n"}
{"id": "13662027", "url": "https://en.wikipedia.org/wiki?curid=13662027", "title": "Colloid vibration current", "text": "Colloid vibration current\n\nColloid vibration current is an electroacoustic phenomenon that arises when ultrasound propagates through a fluid that contains ions and either solid particles or emulsion droplets. \n\nThe pressure gradient in an ultrasonic wave moves particles relative to the fluid. This motion disturbs the double layer that exists at the particle-fluid interface. The picture illustrates the mechanism of this distortion. Practically all particles in fluids carry a surface charge. This surface charge is screened with an equally charged diffuse layer; this structure is called the double layer. Ions of the diffuse layer are located in the fluid and can move with the fluid. Fluid motion relative to the particle drags these diffuse ions in the direction of one or the other of the particle's poles. The picture shows ions dragged towards the left hand pole. As a result of this drag, there is an excess of negative ions in the vicinity of the left hand pole and an excess of positive surface charge at the right hand pole. As a result of this charge excess, particles gain a dipole moment. These dipole moments generate an electric field that in turn generates measurable electric current. This phenomenon is widely used for measuring zeta potential in concentrated colloids.\n\n"}
{"id": "40836275", "url": "https://en.wikipedia.org/wiki?curid=40836275", "title": "Cosmic age problem", "text": "Cosmic age problem\n\nThe cosmic age problem is a historical problem in astronomy concerning the age of the universe. The problem was that at various times in the 20th century, some objects in the universe were estimated to be older than the time elapsed since the Big Bang, as estimated from measurements of the expansion rate of the universe known as the Hubble constant, denoted H. (This is more correctly called the Hubble parameter, since it generally varies with time).\nIf so, this would represent a contradiction, since objects such as galaxies, stars and planets could not have existed in the extreme temperatures and densities shortly after the Big Bang.\n\nSince around 1997–2003, the problem is believed to be solved by most cosmologists: modern cosmological measurements lead to a precise estimate of the age of the universe (i.e. time since the Big Bang) of 13.8 billion years, and recent age estimates for the oldest objects are either younger than this, or consistent allowing for measurement uncertainties.\n\nFollowing theoretical developments of the Friedmann equations by Alexander Friedmann and Georges Lemaître in the 1920s, and the discovery of the expanding universe by Edwin Hubble in 1929, it was immediately clear that tracing this expansion backwards in time predicts that the universe had almost zero size at a finite time in the past. This concept, initially known as the \"Primeval Atom\" by Lemaitre, was later elaborated into the modern Big Bang theory. If the universe had expanded at a constant rate in the past, the age of the universe now (i.e. the time since the Big Bang) is simply the inverse of the Hubble constant, often known as the \"Hubble time\". For Big Bang models with zero cosmological constant and positive matter density, the actual age must be somewhat younger than this Hubble time; typically the age would be between 66% and 90% of the Hubble time, depending on the density of matter.\n\nHubble's early estimate of his constant was 550 (km/s)/Mpc, and the inverse of that is 1.8 billion years. It was believed by many geologists such as Arthur Holmes in the 1920s that the Earth was probably over 2 billion years old, but with large uncertainty. The possible discrepancy between the ages of the Earth and the universe was probably one motivation for the development of the Steady State theory in 1948 as an alternative to the Big Bang; in the (now obsolete) steady state theory, the universe is infinitely old and on average unchanging with time. The steady state theory postulated spontaneous creation of matter to keep the average density constant as the universe expands, and therefore most galaxies still have an age less than 1/H. However, if H had been 550 (km/s)/Mpc, our Milky Way galaxy would be exceptionally large compared to most other galaxies, so it could well be much older than an average galaxy, therefore eliminating the age problem.\n\nIn the 1950s, two substantial errors were discovered in Hubble's extragalactic distance scale: first in 1952, Walter Baade discovered there were two classes of Cepheid variable star. Hubble's sample comprised different classes nearby and in other galaxies, and correcting this error made all other galaxies twice as distant as Hubble's values, thus doubling the Hubble time. A second error was discovered by Allan Sandage and coworkers: for galaxies beyond the Local Group, Cepheids were too faint to observe with Hubble's instruments, so Hubble used the brightest stars as distance indicators. Many of Hubble's \"brightest stars\" were actually HII regions or clusters containing many stars, which caused another underestimation of distances for these more distant galaxies. Thus, in 1958 Sandage published the first reasonably accurate measurement of the Hubble constant, at 75 (km/s)/Mpc, which is close to modern estimates of 68–74 (km/s)/Mpc.\n\nThe age of the Earth (actually the Solar System) was first accurately measured around 1955 by Clair Patterson at 4.55 billion years, essentially identical to the modern value. For H ~ 75 (km/s)/Mpc, the inverse of H is 13.0 billion years; so after 1958 the Big Bang model age was comfortably older than the Earth.\n\nHowever, in the 1960s and onwards, new developments in the theory of stellar evolution enabled age estimates for large star clusters called globular clusters: these generally gave age estimates of around 15 billion years, with substantial scatter. Further revisions of the Hubble constant by Sandage and Gustav Tammann in the 1970s gave values around 50–60 (km/s)/Mpc, and an inverse of 16-20 billion years, consistent with globular cluster ages.\n\nHowever, in the late 1970s to early 1990s, the age problem re-appeared: new estimates of the Hubble constant gave higher values, with Gerard de Vaucouleurs estimating values 90–100 (km/s)/Mpc, while Marc Aaronson and co-workers gave values around 80-90  (km/s)/Mpc. Sandage and Tammann continued to argue for values 50-60, leading to a period of controversy sometimes called the \"Hubble wars\". The higher values for H appeared to predict a universe younger than the globular cluster ages, and gave rise to some speculations during the 1980s that the Big Bang model was seriously incorrect.\n\nThe age problem was eventually thought to be resolved by several developments between 1995-2003: firstly, a large program with the Hubble space telescope measured the Hubble constant at 72 (km/s)/Mpc with 10 percent uncertainty. Secondly, measurements of parallax by the Hipparcos spacecraft in 1995 revised globular cluster distances upwards by 5-10 percent; this made their stars brighter than previously estimated and therefore younger, shifting their age estimates down to around 12-13 billion years. Finally, from 1998-2003 a number of new cosmological observations including supernovae, cosmic microwave background observations and large galaxy redshift surveys led to the acceptance of dark energy and the establishment of the Lambda-CDM model as the standard model of cosmology. The presence of dark energy implies that the universe was expanding more slowly at around half its present age than today, which makes the universe older for a given value of the Hubble constant. The combination of the three results above essentially removed the discrepancy between estimated globular cluster ages and the age of the universe.\n\nMore recent measurements from WMAP and the Planck spacecraft lead to an estimate of the age of the universe of 13.80 billion years with only 0.3 percent uncertainty (based on the standard Lambda-CDM model), and modern age measurements for globular clusters and other objects are currently smaller than this value (within the measurement uncertainties). A substantial majority of cosmologists therefore believe the age problem is now resolved.\n\n"}
{"id": "1643492", "url": "https://en.wikipedia.org/wiki?curid=1643492", "title": "Cosmic latte", "text": "Cosmic latte\n\nCosmic latte is a name assigned to the average color of the universe, found by a team of astronomers from Johns Hopkins University. In 2001, Karl Glazebrook and Ivan Baldry determined that the average color of the universe was a greenish white, but they soon corrected their analysis in a 2002 paper in which they reported that their survey of the light from over 200,000 galaxies averaged to a slightly beigeish white. The hex triplet value for cosmic latte is #FFF8E7.\n\nFinding the average color of the universe was not the focus of the study. Rather, the study examined spectral analysis of different galaxies to study star formation. Like Fraunhofer lines, the dark lines displayed in the study's spectral ranges display older and younger stars and allow Glazebrook and Baldry to determine the age of different galaxies and star systems. What the study revealed is that the overwhelming majority of stars formed about 5 billion years ago. Because these stars would have been \"brighter\" in the past, the color of the universe changes over time shifting from blue to red as more blue stars change to yellow and eventually red giants.\n\nAs light from distant galaxies reaches the Earth, the average \"color of the universe\" (as seen from Earth) tends towards pure white, due to the light coming from the stars when they were much younger and bluer.\n\nThe corrected color was initially published on the Johns Hopkins News website and updated on the team's initial announcement. Multiple news outlets, including NPR and BBC, displayed the color in stories and some relayed the request by Glazebrook on the announcement asking for suggestions for names, jokingly adding all were welcome as long as they were not \"beige\".\n\nThese were the results of a vote of the scientists involved based on the new color:\nThough Drum's suggestion of \"cappuccino cosmico\" received the most votes, the researchers favored Drum's other suggestion, \"cosmic latte\". This is because the similar \"Latteo\" means \"Milky\" in Italian, Galileo's native language. It also leads to the similarity to the Italian term for the Milky Way, \"Via Lattea\", and they enjoyed the fact that the color would be similar to the Milky Way's average color as well, as it is part of the sum of the universe. They also claimed to be \"caffeine biased\".\n\nDrum came up with the name while sitting in a Starbucks drinking a latte and reading the \"Washington Post\". Drum noticed that the color of the universe as displayed in the newspaper was the same color as his latte.\n\n"}
{"id": "42882", "url": "https://en.wikipedia.org/wiki?curid=42882", "title": "Cosmogony", "text": "Cosmogony\n\nCosmogony is any model concerning the origin of either the cosmos or universe. Developing a complete theoretical model has implications in both the philosophy of science and epistemology.\n\nThe word comes from the Koine Greek κοσμογονία (from κόσμος \"cosmos, the world\") and the root of γί(γ)νομαι / γέγονα (\"come into a new state of being\"). In astronomy, cosmogony refers to the study of the origin of particular astrophysical objects or systems, and is most commonly used in reference to the origin of the Universe, the Solar System, or the Earth–Moon system.\n\nThe Big Bang theory is the prevailing cosmological model of the early development of the universe. \nThe most commonly held view is that the universe originates in a gravitational singularity, which expanded extremely rapidly from its hot and dense state. \nCosmologist and science communicator Sean M. Carroll explains two competing types of explanations for the origins of the singularity which is the main disagreement between the scientists who study cosmogony and centers on the question of whether time existed \"before\" the emergence of our universe or not. One cosmogonical view sees time as fundamental and even eternal: The universe could have contained the singularity because the universe evolved or changed from a prior state (the prior state was \"empty space\", or maybe a state that could not be called \"space\" at all). The other view, held by proponents like Stephen Hawking, says that there was no change through time because \"time\" itself emerged along with this universe (in other words, there can be no \"prior\" to the universe). Thus, it remains unclear what combination of \"stuff\", space, or time emerged with the singularity and this universe.\n\nOne problem in cosmogony is that there is currently no theoretical model that explains the earliest moments of the universe's existence (during the Planck time) because of a lack of a testable theory of quantum gravity. Researchers in string theory and its extensions (for example, M theory), and of loop quantum cosmology, have nevertheless proposed solutions of the type just discussed.\n\nCosmogony can be distinguished from cosmology, which studies the universe at large and throughout its existence, and which technically does not inquire directly into the source of its origins. There is some ambiguity between the two terms. For example, the cosmological argument from theology regarding the existence of God is technically an appeal to cosmogonical rather than cosmological ideas. In practice, there is a scientific distinction between cosmological and cosmogonical ideas. Physical cosmology is the science that attempts to explain all observations relevant to the development and characteristics of the universe as a whole. Questions regarding why the universe behaves in such a way have been described by physicists and cosmologists as being extra-scientific (i.e., metaphysical), though speculations are made from a variety of perspectives that include extrapolation of scientific theories to untested regimes (i.e., at Planck scales), and philosophical or religious ideas.\n\nCosmogonists have only tentative theories for the early stages of the universe and its beginning. , no accelerator experiments probe energies of sufficient magnitude to provide any experimental insight into the behavior of matter at the energy levels that prevailed shortly after the Big Bang. \n\nProposed theoretical scenarios differ radically, and include string theory and M-theory, the Hartle–Hawking initial state, string landscape, brane inflation, the Big Bang, and the ekpyrotic universe. Some of these models are mutually compatible, whereas others are not.\n"}
{"id": "585226", "url": "https://en.wikipedia.org/wiki?curid=585226", "title": "Cosmography", "text": "Cosmography\n\nCosmography is the science that maps the general features of the cosmos or universe, describing both heaven and Earth (but without encroaching on geography or astronomy). The 14th-century work \"'Aja'ib al-makhluqat wa-ghara'ib al-mawjudat\" by Persian physician Zakariya al-Qazwini is considered to be an early work of cosmography.\n\nTraditional Hindu, Buddhist and Jain cosmography schematize a universe centered on Mount Meru surrounded by rivers, continents and seas. These cosmographies posit a universe being repeatedly created and destroyed over time cycles of immense lengths.\n\nIn 1551, Martín Cortés de Albacar, from Zaragoza, Spain, published \"Breve compendio de la esfera y del arte de navegar\". Translated into English and reprinted several times, the work was of great influence in Britain for many years. He proposed spherical charts and mentioned magnetic deviation and the existence of magnetic poles.\n\nPeter Heylin's 1652 book \"Cosmographie\" (enlarged from his \"Microcosmos\" of 1621) was one of the earliest attempts to describe the entire world in English, and being the first known description of Australia and among the first of California. The book has 4 sections, examining the geography, politics, and cultures of Europe, Asia, Africa, and America, with an addendum on \"Terra Incognita\", including Australia, and extending to Utopia, Fairyland, and the \"Land of Chivalrie\".\n\nIn 1659, Thomas Porter published a smaller, but extensive \"Compendious Description of the Whole World\", which also included a chronology of world events from Creation forward. These were all part of a major trend in the European Renaissance to explore (and perhaps comprehend) the known world.\n\nThe word was also commonly used by Buckminster Fuller in his lectures.\n\nIn astrophysics, the term \"cosmography\" is beginning to be used to describe attempts to determine the large-scale matter distribution and kinematics of the observable universe, dependent on the Friedmann–Lemaître–Robertson–Walker metric but independent of the temporal dependence of the scale factor on the matter/energy composition of the Universe.\n"}
{"id": "2848730", "url": "https://en.wikipedia.org/wiki?curid=2848730", "title": "Cosmological horizon", "text": "Cosmological horizon\n\nA cosmological horizon is a measure of the distance from which one could possibly retrieve information. This observable constraint is due to various properties of general relativity, the expanding universe, and the physics of Big Bang cosmology. Cosmological horizons set the size and scale of the observable universe. This article explains a number of these horizons.\n\nThe particle horizon (also called the cosmological horizon, the comoving horizon, or the cosmic light horizon) is the maximum distance from which particles could have traveled to the observer in the age of the universe. It represents the boundary between the observable and the unobservable regions of the universe, so its distance at the present epoch defines the size of the observable universe. Due to the expansion of the universe it is not simply the age of the universe times the speed of light, as in the Hubble horizon, but rather the speed of light multiplied by the conformal time. The existence, properties, and significance of a cosmological horizon depend on the particular cosmological model.\n\nIn terms of comoving distance, the particle horizon is equal to the conformal time that has passed since the Big Bang, times the speed of light. In general, the conformal time at a certain time is given in terms of the scale factor formula_1 by,\n\nor\n\nThe particle horizon is the boundary between two regions at a point at a given time: one region defined by events that have already been observed by an observer, and the other by events which cannot be observed \"at that time\". It represents the furthest distance from which we can retrieve information from the past, and so defines the observable universe.\n\nHubble radius, Hubble sphere, Hubble volume, or Hubble horizon is a conceptual horizon defining the boundary between particles that are moving slower and faster than the speed of light relative to an observer at one given time. Note that this does not mean the particle is unobservable, the light from the past is reaching and will continue to reach the observer for a while. Also, more importantly, in the current expansion model e.g., light emitted from the Hubble radius will reach us in a finite amount of time. It is a common misconception that light from the Hubble radius can never reach us. It is true that particles on the Hubble radius recede from us with the speed of light, but the Hubble radius gets larger over time (because the Hubble parameter H gets smaller over time), so light emitted towards us from a particle on the Hubble radius will be inside the Hubble radius some time later. Only light emitted from the cosmic event horizon or further will never reach us in a finite amount of time.\n\nThe Hubble velocity of an object is given by Hubble's law,\n\nReplacing formula_5 with speed of light formula_6 and solving for proper distance formula_7 we obtain the radius of Hubble sphere as\n\nIn an ever-accelerating universe, if two particles are separated by a distance greater than the Hubble radius, they cannot talk to each other from now on (as they are now, not as they have been in the past), However, if they are outside of each other's particle horizon, they could have never communicated. Depending on the form of expansion of the universe, they may be able to exchange information in the future. Today,\n\nyielding a Hubble horizon of some 4.1 Gpc. This horizon is not really a physical size, but it is often used as useful length scale as most physical sizes in cosmology can be written in terms of those factors.\n\nOne can also define comoving Hubble horizon by simply dividing Hubble radius by the scale factor\n\nThe particle horizon differs from the cosmic event horizon, in that the particle horizon represents the largest comoving distance from which light could have reached the observer by a specific time, while the event horizon is the largest comoving distance from which light emitted now can \"ever\" reach the observer in the future. The current distance to our cosmic event horizon is about 5 Gpc (16 billion light years), well within our observable range given by the particle horizon.\n\nIn general, the proper distance to the event horizon at time formula_11 is given by\n\nwhere formula_13 is the time-coordinate of the end of the universe, which would be infinite in the case of a universe that expands forever.\n\nFor our case, assuming that dark energy is due to a cosmological constant, formula_14.\n\nIn an accelerating universe, there are events which will be unobservable as formula_15 as signals from future events become redshifted to arbitrarily long wavelengths in the exponentially expanding de Sitter space. This sets a limit on the farthest distance that we can possibly see as measured in units of proper distance today. Or, more precisely, there are events that are spatially separated for a certain frame of reference happening simultaneously with the event occurring right now for which no signal will ever reach us, even though we can observe events that occurred at the same location in space that happened in the distant past. While we will continue to receive signals from this location in space, even if we wait an infinite amount of time, a signal that left from that location today will never reach us. Additionally, the signals coming from that location will have less and less energy and be less and less frequent until the location, for all practical purposes, becomes unobservable. In a universe that is dominated by dark energy which is undergoing an exponential expansion of the scale factor, all objects that are gravitationally unbound with respect to the Milky Way will become unobservable, in a futuristic version of Kapteyn's universe.\n\nWhile not technically \"horizons\" in the sense of an impossibility for observations due to relativity or cosmological solutions, there are practical horizons which include the optical horizon, set at the surface of last scattering. This is the farthest distance that any photon can freely stream. Similarly, there is a \"neutrino horizon\" set for the farthest distance a neutrino can freely stream and a gravitational wave horizon at the farthest distance that gravitational waves can freely stream. The latter is predicted to be a direct probe of the end of cosmic inflation.\n\nFor a simplified summary and overview of different horizons in cosmology, see Different Horizons in Cosmology\n"}
{"id": "1864889", "url": "https://en.wikipedia.org/wiki?curid=1864889", "title": "Cosmology", "text": "Cosmology\n\nCosmology (from the Greek κόσμος, \"kosmos\" \"world\" and -λογία, \"-logia\" \"study of\") is the study of the origin, evolution, and eventual fate of the universe. Physical cosmology is the scientific study of the universe's origin, its large-scale structures and dynamics, and its ultimate fate, as well as the scientific laws that govern these areas.\n\nThe term \"cosmology\" was first used in English in 1656 in Thomas Blount's \"Glossographia\", and in 1731 taken up in Latin by German philosopher Christian Wolff, in \"Cosmologia Generalis\".\n\nReligious or mythological cosmology is a body of beliefs based on mythological, religious, and esoteric literature and traditions of creation myths and eschatology.\n\nPhysical cosmology is studied by scientists, such as astronomers and physicists, as well as philosophers, such as metaphysicians, philosophers of physics, and philosophers of space and time. Because of this shared scope with philosophy, theories in physical cosmology may include both scientific and non-scientific propositions, and may depend upon assumptions that cannot be tested. Cosmology differs from astronomy in that the former is concerned with the Universe as a whole while the latter deals with individual celestial objects. Modern physical cosmology is dominated by the Big Bang theory, which attempts to bring together observational astronomy and particle physics; more specifically, a standard parameterization of the Big Bang with dark matter and dark energy, known as the Lambda-CDM model.\n\nTheoretical astrophysicist David N. Spergel has described cosmology as a \"historical science\" because \"when we look out in space, we look back in time\" due to the finite nature of the speed of light.\n\nPhysics and astrophysics have played a central role in shaping the understanding of the universe through scientific observation and experiment. Physical cosmology was shaped through both mathematics and observation in an analysis of the whole universe. The universe is generally understood to have begun with the Big Bang, followed almost instantaneously by cosmic inflation; an expansion of space from which the universe is thought to have emerged 13.799 ± 0.021 billion years ago. Cosmogony studies the origin of the Universe, and cosmography maps the features of the Universe.\n\nIn Diderot's Encyclopédie, cosmology is broken down into uranology (the science of the heavens), aerology (the science of the air), geology (the science of the continents), and hydrology (the science of waters).\n\nMetaphysical cosmology has also been described as the placing of humans in the universe in relationship to all other entities. This is exemplified by Marcus Aurelius's observation that a man's place in that relationship: \"He who does not know what the world is does not know where he is, and he who does not know for what purpose the world exists, does not know who he is, nor what the world is.\"\n\nPhysical cosmology is the branch of physics and astrophysics that deals with the study of the physical origins and evolution of the Universe. It also includes the study of the nature of the Universe on a large scale. In its earliest form, it was what is now known as \"celestial mechanics\", the study of the heavens. Greek philosophers Aristarchus of Samos, Aristotle, and Ptolemy proposed different cosmological theories. The geocentric Ptolemaic system was the prevailing theory until the 16th century when Nicolaus Copernicus, and subsequently Johannes Kepler and Galileo Galilei, proposed a heliocentric system. This is one of the most famous examples of epistemological rupture in physical cosmology.\nIsaac Newton's \"Principia Mathematica\", published in 1687, was the first description of the law of universal gravitation. It provided a physical mechanism for Kepler's laws and also allowed the anomalies in previous systems, caused by gravitational interaction between the planets, to be resolved. A fundamental difference between Newton's cosmology and those preceding it was the Copernican principle—that the bodies on earth obey the same physical laws as all the celestial bodies. This was a crucial philosophical advance in physical cosmology.\n\nModern scientific cosmology is usually considered to have begun in 1917 with Albert Einstein's publication of his final modification of general relativity in the paper \"Cosmological Considerations of the General Theory of Relativity\" (although this paper was not widely available outside of Germany until the end of World War I). General relativity prompted cosmogonists such as Willem de Sitter, Karl Schwarzschild, and Arthur Eddington to explore its astronomical ramifications, which enhanced the ability of astronomers to study very distant objects. Physicists began changing the assumption that the Universe was static and unchanging. In 1922 Alexander Friedmann introduced the idea of an expanding universe that contained moving matter. Around the same time (1917 to 1922) the Great Debate took place, with early cosmologists such as Heber Curtis and Ernst Öpik determining that some nebulae seen in telescopes were separate galaxies far distant from our own.\nIn parallel to this dynamic approach to cosmology, one long-standing debate about the structure of the cosmos was coming to a climax. Mount Wilson astronomer Harlow Shapley championed the model of a cosmos made up of the Milky Way star system only; while Heber D. Curtis argued for the idea that spiral nebulae were star systems in their own right as island universes. This difference of ideas came to a climax with the organization of the Great Debate on 26 April 1920 at the meeting of the U.S. National Academy of Sciences in Washington, D.C. The debate was resolved when Edwin Hubble detected Cepheid Variables in the Andromeda galaxy in 1923 and 1924. Their distance established spiral nebulae well beyond the edge of the Milky Way.\n\nSubsequent modelling of the universe explored the possibility that the cosmological constant, introduced by Einstein in his 1917 paper, may result in an expanding universe, depending on its value. Thus the Big Bang model was proposed by the Belgian priest Georges Lemaître in 1927 which was subsequently corroborated by Edwin Hubble's discovery of the red shift in 1929 and later by the discovery of the cosmic microwave background radiation by Arno Penzias and Robert Woodrow Wilson in 1964. These findings were a first step to rule out some of many alternative cosmologies.\n\nSince around 1990, several dramatic advances in observational cosmology have transformed cosmology from a largely speculative science into a predictive science with precise agreement between theory and observation. These advances include observations of the microwave background from the COBE, WMAP and Planck satellites, large new galaxy redshift surveys including 2dfGRS and SDSS, and observations of distant supernovae and gravitational lensing. These observations matched the predictions of the cosmic inflation theory, a modified Big Bang theory, and the specific version known as the Lambda-CDM model. This has led many to refer to modern times as the \"golden age of cosmology\".\n\nOn 17 March 2014, astronomers at the Harvard-Smithsonian Center for Astrophysics announced the detection of gravitational waves, providing strong evidence for inflation and the Big Bang. However, on 19 June 2014, lowered confidence in confirming the cosmic inflation findings was reported.\n\nOn 1 December 2014, at the \"Planck 2014\" meeting in Ferrara, Italy, astronomers reported that the universe is 13.8 billion years old and is composed of 4.9% atomic matter, 26.6% dark matter and 68.5% dark energy.\n\nReligious or mythological cosmology is a body of beliefs based on mythological, religious, and esoteric literature and traditions of creation and eschatology.\n\nCosmology deals with the world as the totality of space, time and all phenomena. Historically, it has had quite a broad scope, and in many cases was founded in religion. In modern use metaphysical cosmology addresses questions about the Universe which are beyond the scope of science. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods like dialectics. Modern metaphysical cosmology tries to address questions such as:\n\n\nTable notes: the term \"static\" simply means not expanding and not contracting. Symbol \"G\" represents Newton's gravitational constant; Λ (Lambda) is the cosmological constant.\n\n"}
{"id": "1045142", "url": "https://en.wikipedia.org/wiki?curid=1045142", "title": "Debris", "text": "Debris\n\nDebris or débris (, ) is rubble, wreckage, ruins, litter and discarded garbage/refuse/trash, scattered remains of something destroyed, discarded, or as in geology, large rock fragments left by a melting glacier etc. Depending on context, \"debris\" can refer to a number of different things. The first apparent use of the French word in English is in a 1701 description of the army of Prince Rupert upon its retreat from a battle with the army of Oliver Cromwell, in England.\n\nIn disaster scenarios, tornados leave behind large pieces of houses and mass destruction overall. This debris also flies around the tornado itself when it is in progress. The tornado's winds capture debris it kicks up in its wind orbit, and spins it inside its vortex. The tornado's wind radius is larger than the funnel itself. tsunamis and hurricanes also bring large amounts of debris, such as Hurricane Katrina in 2005 and Hurricane Sandy in 2012. Earthquakes rock cities to rubble debris.\n\nIn geology, debris usually applies to the remains of geological activity including landslides, volcanic explosions, avalanches, mudflows or Glacial lake outburst floods (Jökulhlaups) and moraine, lahars, and lava eruptions. Geological debris sometimes moves in a stream called a debris flow. When it accumulates at the base of hillsides, it can be called \"talus\" or \"scree\".\n\nIn mining, debris called \"attle\" usually consists of rock fragments which contain little or no ore.\n\n\"Marine debris\" applies to floating garbage such as bottles, cans, styrofoam, cruise ship waste, offshore oil and gas exploration and production facilities pollution, and fishing paraphernalia from professional and recreational boaters. Marine debris is also called litter or flotsam and jetsam. Objects that can constitute marine debris include used automobile tires, detergent bottles, medical wastes, discarded fishing line and nets, soda cans, and bilge waste solids.\n\nIn addition to being unsightly, it can pose a serious threat to marine life, boats, swimmers, divers, and others. For example, each year millions of seabirds, sea turtles, fish, and marine mammals become entangled in marine debris, or ingest plastics which they have mistaken for food. As many as 30,000 northern fur seals per year get caught in abandoned fishing nets and either drown or suffocate. Whales mistake plastic bags for squid, and birds may mistake plastic pellets for fish eggs. At other times, animals accidentally eat the plastic while feeding on natural food.\n\nThe largest concentration of marine debris is the Great Pacific Garbage Patch.\n\nMarine debris most commonly originates from land-based sources. Various international agencies are currently working to reduce marine debris levels around the world.\n\nIn meteorology, debris usually applies to the remains of human habitation and natural flora after storm related destruction. This debris is also commonly referred to as storm debris. Storm debris commonly consists of roofing material, downed tree limbs, downed signs, downed power lines and poles, and wind-blown garbage. Storm debris can become a serious problem immediately after a storm, in that it often blocks access to individuals and communities that may require emergency services. This material frequently exists in such large quantities that disposing of it becomes a serious issue for a community. In addition, storm debris is often hazardous by its very nature, since, for example, downed power lines annually account for storm-related deaths.\n\n\"Space debris\" usually refers to the remains of spacecraft that have either fallen to Earth or are still orbiting Earth. Space debris may also consist of natural components such as chunks of rock and ice. The problem of space debris has grown as various space programs have left legacies of launches, explosions, repairs, and discards in both low Earth orbit and more remote orbits. These orbiting fragments have reached a great enough proportion to constitute a hazard to future space launches of both satellite and manned vehicles. Various government agencies and international organizations are beginning to track space debris and also research possible solutions to the problem. While many of these items, ranging in size from nuts and bolts to entire satellites and spacecraft, may fall to Earth, other items located in more remote orbits may stay aloft for centuries. The velocity of some of these pieces of space junk have been clocked in excess of 17,000 miles per hour (27,000 km/h). A piece of space debris falling to Earth leaves a fiery trail, just like a meteor.\n\nA debris disk is a circumstellar disk of dust and debris in orbit around a star.\n\nIn medicine, debris usually refers to biological matter that has accumulated or lodged in surgical instruments and is referred to as surgical debris. The presence of surgical debris can result in cross-infections or nosocomial infections if not removed and the affected surgical instruments or equipment properly disinfected.\n\nIn the aftermath of a war, large areas of the region of conflict are often strewn with \"war debris\" in the form of abandoned or destroyed hardware and vehicles, mines, unexploded ordnance, bullet casings and other fragments of metal.\n\nMuch war debris has the potential to be lethal and continues to kill and maim civilian populations for years after the end of a conflict. The risks from war debris may be sufficiently high to prevent or delay the return of refugees. In addition war debris may contain hazardous chemicals or radioactive components that can contaminate the land or poison civilians who come into contact with it. Many Mine clearance agencies are also involved in the clearance of war debris.\n\nLand mines in particular are very dangerous as they can remain active for decades after a conflict, which is why they have been banned by international war regulations.\n\nIn November 2006 the Protocol on Explosive Remnants of War\ncame into effect with 92 countries subscribing to the treaty that requires the parties involved in a conflict to assist with the removal of unexploded ordnance following the end of hostilities.\n\nSome of the countries most affected by war debris are Afghanistan, Angola, Cambodia, Iraq and Laos.\n\nSimilarly \"military debris\" may be found in and around firing ranges and military training areas.\n\nDebris can also be used as cover for military purposes, depending on the situation.\n\nIn South Louisiana's Creole and Cajun cultures, debris (pronounced \"DAY-bree\") refers to chopped organs such as liver, heart, kidneys, tripe, spleen, brain, lungs and pancreas.\n\n\n"}
{"id": "9228", "url": "https://en.wikipedia.org/wiki?curid=9228", "title": "Earth", "text": "Earth\n\nEarth is the third planet from the Sun and the only astronomical object known to harbor life. According to radiometric dating and other sources of evidence, Earth formed over 4.5 billion years ago. Earth's gravity interacts with other objects in space, especially the Sun and the Moon, Earth's only natural satellite. Earth revolves around the Sun in 365.26 days, a period known as an Earth year. During this time, Earth rotates about its axis about 366.26 times.\n\nEarth's axis of rotation is tilted with respect to its orbital plane, producing seasons on Earth. The gravitational interaction between Earth and the Moon causes ocean tides, stabilizes Earth's orientation on its axis, and gradually slows its rotation. Earth is the densest planet in the Solar System and the largest of the four terrestrial planets.\n\nEarth's lithosphere is divided into several rigid tectonic plates that migrate across the surface over periods of many millions of years. About 71% of Earth's surface is covered with water, mostly by oceans. The remaining 29% is land consisting of continents and islands that together have many lakes, rivers and other sources of water that contribute to the hydrosphere. The majority of Earth's polar regions are covered in ice, including the Antarctic ice sheet and the sea ice of the Arctic ice pack. Earth's interior remains active with a solid iron inner core, a liquid outer core that generates the Earth's magnetic field, and a convecting mantle that drives plate tectonics.\n\nWithin the first billion years of Earth's history, life appeared in the oceans and began to affect the Earth's atmosphere and surface, leading to the proliferation of aerobic and anaerobic organisms. Some geological evidence indicates that life may have arisen as much as 4.1 billion years ago. Since then, the combination of Earth's distance from the Sun, physical properties, and geological history have allowed life to evolve and thrive. In the history of the Earth, biodiversity has gone through long periods of expansion, occasionally punctuated by mass extinction events. Over 99% of all species that ever lived on Earth are extinct. Estimates of the number of species on Earth today vary widely; most species have not been described. Over 7.6 billion humans live on Earth and depend on its biosphere and natural resources for their survival. Humans have developed diverse societies and cultures; politically, the world has about 200 sovereign states.\n\nThe modern English word \"Earth\" developed from a wide variety of Middle English forms, which derived from an Old English noun most often spelled '. It has cognates in every Germanic language, and their proto-Germanic root has been reconstructed as *\"erþō\". In its earliest appearances, \"eorðe\" was already being used to translate the many senses of Latin ' and Greek (\"gē\"): the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Terra and Gaia, Earth was a personified goddess in Germanic paganism: the Angles were listed by Tacitus as among the devotees of Nerthus, and later Norse mythology included Jörð, a giantess often given as the mother of Thor.\n\nOriginally, \"earth\" was written in lowercase, and from early Middle English, its definite sense as \"the globe\" was expressed as \"the earth\". By Early Modern English, many nouns were capitalized, and \"the earth\" became (and often remained) \"the Earth\", particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as \"Earth\", by analogy with the names of the other planets. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes \"Earth\" when appearing as a name (e.g. \"Earth's atmosphere\") but writes it in lowercase when preceded by \"the\" (e.g. \"the atmosphere of the earth\"). It almost always appears in lowercase in colloquial expressions such as \"what on earth are you doing?\"\n\nThe oldest material found in the Solar System is dated to (Bya). By the primordial Earth had formed. The bodies in the Solar System formed and evolved with the Sun. In theory, a solar nebula partitions a volume out of a molecular cloud by gravitational collapse, which begins to spin and flatten into a circumstellar disk, and then the planets grow out of that disk with the Sun. A nebula contains gas, ice grains, and dust (including primordial nuclides). According to nebular theory, planetesimals formed by accretion, with the primordial Earth taking 10– (Mys) to form.\n\nA subject of research is the formation of the Moon, some 4.53 Bya. A leading hypothesis is that it was formed by accretion from material loosed from Earth after a Mars-sized object, named Theia, hit Earth. In this view, the mass of Theia was approximately 10 percent of Earth, it hit Earth with a glancing blow and some of its mass merged with Earth. Between approximately 4.1 and , numerous asteroid impacts during the Late Heavy Bombardment caused significant changes to the greater surface environment of the Moon and, by inference, to that of Earth.\n\nEarth's atmosphere and oceans were formed by volcanic activity and outgassing. Water vapor from these sources condensed into the oceans, augmented by water and ice from asteroids, protoplanets, and comets. In this model, atmospheric \"greenhouse gases\" kept the oceans from freezing when the newly forming Sun had only 70% of its current luminosity. By , Earth's magnetic field was established, which helped prevent the atmosphere from being stripped away by the solar wind.\n\nA crust formed when the molten outer layer of Earth cooled to form a solid. The two models that explain land mass propose either a steady growth to the present-day forms or, more likely, a rapid growth early in Earth history followed by a long-term steady continental area. Continents formed by plate tectonics, a process ultimately driven by the continuous loss of heat from Earth's interior. Over the period of hundreds of millions of years, the supercontinents have assembled and broken apart. Roughly (Mya), one of the earliest known supercontinents, Rodinia, began to break apart. The continents later recombined to form Pannotia , then finally Pangaea, which also broke apart .\n\nThe present pattern of ice ages began about and then intensified during the Pleistocene about . High-latitude regions have since undergone repeated cycles of glaciation and thaw, repeating about every . The last continental glaciation ended ago.\n\nChemical reactions led to the first self-replicating molecules about four billion years ago. A half billion years later, the last common ancestor of all current life arose. The evolution of photosynthesis allowed the Sun's energy to be harvested directly by life forms. The resultant molecular oxygen () accumulated in the atmosphere and due to interaction with ultraviolet solar radiation, formed a protective ozone layer () in the upper atmosphere. The incorporation of smaller cells within larger ones resulted in the development of complex cells called eukaryotes. True multicellular organisms formed as cells within colonies became increasingly specialized. Aided by the absorption of harmful ultraviolet radiation by the ozone layer, life colonized Earth's surface. Among the earliest fossil evidence for life is microbial mat fossils found in 3.48 billion-year-old sandstone in Western Australia, biogenic graphite found in 3.7 billion-year-old metasedimentary rocks in Western Greenland, and remains of biotic material found in 4.1 billion-year-old rocks in Western Australia. The earliest direct evidence of life on Earth is contained in 3.45 billion-year-old Australian rocks showing fossils of microorganisms.\n\nDuring the Neoproterozoic, , much of Earth might have been covered in ice. This hypothesis has been termed \"Snowball Earth\", and it is of particular interest because it preceded the Cambrian explosion, when multicellular life forms significantly increased in complexity. Following the Cambrian explosion, , there have been five mass extinctions. The most recent such event was , when an asteroid impact triggered the extinction of the non-avian dinosaurs and other large reptiles, but spared some small animals such as mammals, which at the time resembled shrews. Mammalian life has diversified over the past , and several million years ago an African ape-like animal such as \"Orrorin tugenensis\" gained the ability to stand upright. This facilitated tool use and encouraged communication that provided the nutrition and stimulation needed for a larger brain, which led to the evolution of humans. The development of agriculture, and then civilization, led to humans having an influence on Earth and the nature and quantity of other life forms that continues to this day.\n\nEarth's expected long-term future is tied to that of the Sun. Over the next , solar luminosity will increase by 10%, and over the next by 40%. The Earth's increasing surface temperature will accelerate the inorganic carbon cycle, reducing concentration to levels lethally low for plants ( for C4 photosynthesis) in approximately . The lack of vegetation will result in the loss of oxygen in the atmosphere, making animal life impossible. After another billion years all surface water will have disappeared and the mean global temperature will reach . From that point, the Earth is expected to be habitable for another , possibly up to if nitrogen is removed from the atmosphere. Even if the Sun were eternal and stable, 27% of the water in the modern oceans will descend to the mantle in one billion years, due to reduced steam venting from mid-ocean ridges.\n\nThe Sun will evolve to become a red giant in about . Models predict that the Sun will expand to roughly , about 250 times its present radius. Earth's fate is less clear. As a red giant, the Sun will lose roughly 30% of its mass, so, without tidal effects, Earth will move to an orbit from the Sun when the star reaches its maximum radius. Most, if not all, remaining life will be destroyed by the Sun's increased luminosity (peaking at about 5,000 times its present level). A 2008 simulation indicates that Earth's orbit will eventually decay due to tidal effects and drag, causing it to enter the Sun's atmosphere and be vaporized.\n\nThe shape of Earth is approximately oblate spheroidal. Due to rotation, the Earth is flattened at the poles and bulging around the equator. The diameter of the Earth at the equator is larger than the pole-to-pole diameter. Thus the point on the surface farthest from Earth's center of mass is the summit of the equatorial Chimborazo volcano in Ecuador. The average diameter of the reference spheroid is . Local topography deviates from this idealized spheroid, although on a global scale these deviations are small compared to Earth's radius: The maximum deviation of only 0.17% is at the Mariana Trench ( below local sea level), whereas Mount Everest ( above local sea level) represents a deviation of 0.14%.\n\nIn geodesy, the exact shape that Earth's oceans would adopt in the absence of land and perturbations such as tides and winds is called the geoid. More precisely, the geoid is the surface of gravitational equipotential at mean sea level.\n\nEarth's mass is approximately (5,970 Yg). It is composed mostly of iron (32.1%), oxygen (30.1%), silicon (15.1%), magnesium (13.9%), sulfur (2.9%), nickel (1.8%), calcium (1.5%), and aluminium (1.4%), with the remaining 1.2% consisting of trace amounts of other elements. Due to mass segregation, the core region is estimated to be primarily composed of iron (88.8%), with smaller amounts of nickel (5.8%), sulfur (4.5%), and less than 1% trace elements.\n\nThe most common rock constituents of the crust are nearly all oxides: chlorine, sulfur, and fluorine are the important exceptions to this and their total amount in any rock is usually much less than 1%. Over 99% of the crust is composed of 11 oxides, principally silica, alumina, iron oxides, lime, magnesia, potash, and soda.\n\nEarth's interior, like that of the other terrestrial planets, is divided into layers by their chemical or physical (rheological) properties. The outer layer is a chemically distinct silicate solid crust, which is underlain by a highly viscous solid mantle. The crust is separated from the mantle by the Mohorovičić discontinuity. The thickness of the crust varies from about under the oceans to for the continents. The crust and the cold, rigid, top of the upper mantle are collectively known as the lithosphere, and it is of the lithosphere that the tectonic plates are composed. Beneath the lithosphere is the asthenosphere, a relatively low-viscosity layer on which the lithosphere rides. Important changes in crystal structure within the mantle occur at below the surface, spanning a transition zone that separates the upper and lower mantle. Beneath the mantle, an extremely low viscosity liquid outer core lies above a solid inner core. The Earth's inner core might rotate at a slightly higher angular velocity than the remainder of the planet, advancing by 0.1–0.5° per year. The radius of the inner core is about one fifth of that of Earth.\n\nEarth's internal heat comes from a combination of residual heat from planetary accretion (about 20%) and heat produced through radioactive decay (80%). The major heat-producing isotopes within Earth are potassium-40, uranium-238, and thorium-232. At the center, the temperature may be up to , and the pressure could reach . Because much of the heat is provided by radioactive decay, scientists postulate that early in Earth's history, before isotopes with short half-lives were depleted, Earth's heat production was much higher. At approximately , twice the present-day heat would have been produced, increasing the rates of mantle convection and plate tectonics, and allowing the production of uncommon igneous rocks such as komatiites that are rarely formed today.\n\nThe mean heat loss from Earth is , for a global heat loss of . A portion of the core's thermal energy is transported toward the crust by mantle plumes, a form of convection consisting of upwellings of higher-temperature rock. These plumes can produce hotspots and flood basalts. More of the heat in Earth is lost through plate tectonics, by mantle upwelling associated with mid-ocean ridges. The final major mode of heat loss is through conduction through the lithosphere, the majority of which occurs under the oceans because the crust there is much thinner than that of the continents.\n\nEarth's mechanically rigid outer layer, the lithosphere, is divided into tectonic plates. These plates are rigid segments that move relative to each other at one of three boundaries types: At convergent boundaries, two plates come together; at divergent boundaries, two plates are pulled apart; and at transform boundaries, two plates slide past one another laterally. Along these plate boundaries, earthquakes, volcanic activity, mountain-building, and oceanic trench formation can occur. The tectonic plates ride on top of the asthenosphere, the solid but less-viscous part of the upper mantle that can flow and move along with the plates.\n\nAs the tectonic plates migrate, oceanic crust is subducted under the leading edges of the plates at convergent boundaries. At the same time, the upwelling of mantle material at divergent boundaries creates mid-ocean ridges. The combination of these processes recycles the oceanic crust back into the mantle. Due to this recycling, most of the ocean floor is less than old. The oldest oceanic crust is located in the Western Pacific and is estimated to be old. By comparison, the oldest dated continental crust is .\n\nThe seven major plates are the Pacific, North American, Eurasian, African, Antarctic, Indo-Australian, and South American. Other notable plates include the Arabian Plate, the Caribbean Plate, the Nazca Plate off the west coast of South America and the Scotia Plate in the southern Atlantic Ocean. The Australian Plate fused with the Indian Plate between . The fastest-moving plates are the oceanic plates, with the Cocos Plate advancing at a rate of and the Pacific Plate moving . At the other extreme, the slowest-moving plate is the Eurasian Plate, progressing at a typical rate of .\n\nThe total surface area of Earth is about . Of this, 70.8%, or , is below sea level and covered by ocean water. Below the ocean's surface are much of the continental shelf, mountains, volcanoes, oceanic trenches, submarine canyons, oceanic plateaus, abyssal plains, and a globe-spanning mid-ocean ridge system. The remaining 29.2%, or , not covered by water has terrain that varies greatly from place to place and consists of mountains, deserts, plains, plateaus, and other landforms. Tectonics and erosion, volcanic eruptions, flooding, weathering, glaciation, the growth of coral reefs, and meteorite impacts are among the processes that constantly reshape the Earth's surface over geological time.\n\nThe continental crust consists of lower density material such as the igneous rocks granite and andesite. Less common is basalt, a denser volcanic rock that is the primary constituent of the ocean floors. Sedimentary rock is formed from the accumulation of sediment that becomes buried and compacted together. Nearly 75% of the continental surfaces are covered by sedimentary rocks, although they form about 5% of the crust. The third form of rock material found on Earth is metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both. The most abundant silicate minerals on Earth's surface include quartz, feldspars, amphibole, mica, pyroxene and olivine. Common carbonate minerals include calcite (found in limestone) and dolomite.\n\nThe elevation of the land surface varies from the low point of at the Dead Sea, to a maximum altitude of at the top of Mount Everest. The mean height of land above sea level is about .\n\nThe pedosphere is the outermost layer of Earth's continental surface and is composed of soil and subject to soil formation processes. The total arable land is 10.9% of the land surface, with 1.3% being permanent cropland. Close to 40% of Earth's land surface is used for agriculture, or an estimated of cropland and of pastureland.\n\nThe abundance of water on Earth's surface is a unique feature that distinguishes the \"Blue Planet\" from other planets in the Solar System. Earth's hydrosphere consists chiefly of the oceans, but technically includes all water surfaces in the world, including inland seas, lakes, rivers, and underground waters down to a depth of . The deepest underwater location is Challenger Deep of the Mariana Trench in the Pacific Ocean with a depth of .\n\nThe mass of the oceans is approximately 1.35 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of with a mean depth of , resulting in an estimated volume of . If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be .\n\nAbout 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers.\n\nThe average salinity of Earth's oceans is about 35 grams of salt per kilogram of sea water (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño–Southern Oscillation.\n\nThe atmospheric pressure at Earth's sea level averages , with a scale height of about . A dry atmosphere is composed of 78.084% nitrogen, 20.946% oxygen, 0.934% argon, and trace amounts of carbon dioxide and other gaseous molecules. Water vapor content varies between 0.01% and 4% but averages about 1%. The height of the troposphere varies with latitude, ranging between at the poles to at the equator, with some variation resulting from weather and seasonal factors.\n\nEarth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved , forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric into. The ozone layer blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is known as the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the ground, thereby raising the average temperature. Water vapor, carbon dioxide, methane, nitrous oxide, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be , in contrast to the current , and life on Earth probably would not exist in its current form. In May 2017, glints of light, seen as twinkling from an orbiting satellite a million miles away, were found to be reflected light from ice crystals in the atmosphere.\n\nEarth's atmosphere has no definite boundary, slowly becoming thinner and fading into outer space. Three-quarters of the atmosphere's mass is contained within the first of the surface. This lowest layer is called the troposphere. Energy from the Sun heats this layer, and the surface below, causing expansion of the air. This lower-density air then rises and is replaced by cooler, higher-density air. The result is atmospheric circulation that drives the weather and climate through redistribution of thermal energy.\n\nThe primary atmospheric circulation bands consist of the trade winds in the equatorial region below 30° latitude and the westerlies in the mid-latitudes between 30° and 60°. Ocean currents are also important factors in determining climate, particularly the thermohaline circulation that distributes thermal energy from the equatorial oceans to the polar regions.\n\nWater vapor generated through surface evaporation is transported by circulatory patterns in the atmosphere. When atmospheric conditions permit an uplift of warm, humid air, this water condenses and falls to the surface as precipitation. Most of the water is then transported to lower elevations by river systems and usually returned to the oceans or deposited into lakes. This water cycle is a vital mechanism for supporting life on land and is a primary factor in the erosion of surface features over geological periods. Precipitation patterns vary widely, ranging from several meters of water per year to less than a millimeter. Atmospheric circulation, topographic features, and temperature differences determine the average precipitation that falls in each region.\n\nThe amount of solar energy reaching Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about per degree of latitude from the equator. Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.\n\nThis latitudinal rule has several anomalies:\n\nThe commonly used Köppen climate classification system has five broad groups (humid tropics, arid, humid middle latitudes, continental and cold polar), which are further divided into more specific subtypes. The Köppen system rates regions of terrain based on observed temperature and precipitation.\n\nThe highest air temperature ever measured on Earth was in Furnace Creek, California, in Death Valley, in 1913. The lowest air temperature ever directly measured on Earth was at Vostok Station in 1983, but satellites have used remote sensing to measure temperatures as low as in East Antarctica. These temperature records are only measurements made with modern instruments from the 20th century onwards and likely do not reflect the full range of temperature on Earth.\n\nAbove the troposphere, the atmosphere is usually divided into the stratosphere, mesosphere, and thermosphere. Each layer has a different lapse rate, defining the rate of change in temperature with height. Beyond these, the exosphere thins out into the magnetosphere, where the geomagnetic fields interact with the solar wind. Within the stratosphere is the ozone layer, a component that partially shields the surface from ultraviolet light and thus is important for life on Earth. The Kármán line, defined as 100 km above Earth's surface, is a working definition for the boundary between the atmosphere and outer space.\n\nThermal energy causes some of the molecules at the outer edge of the atmosphere to increase their velocity to the point where they can escape from Earth's gravity. This causes a slow but steady loss of the atmosphere into space. Because unfixed hydrogen has a low molecular mass, it can achieve escape velocity more readily, and it leaks into outer space at a greater rate than other gases. The leakage of hydrogen into space contributes to the shifting of Earth's atmosphere and surface from an initially reducing state to its current oxidizing one. Photosynthesis provided a source of free oxygen, but the loss of reducing agents such as hydrogen is thought to have been a necessary precondition for the widespread accumulation of oxygen in the atmosphere. Hence the ability of hydrogen to escape from the atmosphere may have influenced the nature of life that developed on Earth. In the current, oxygen-rich atmosphere most hydrogen is converted into water before it has an opportunity to escape. Instead, most of the hydrogen loss comes from the destruction of methane in the upper atmosphere.\n\nThe gravity of Earth is the acceleration that is imparted to objects due to the distribution of mass within the Earth. Near the Earth's surface, gravitational acceleration is approximately . Local differences in topography, geology, and deeper tectonic structure cause local and broad, regional differences in the Earth's gravitational field, known as gravity anomalies.\n\nThe main part of Earth's magnetic field is generated in the core, the site of a dynamo process that converts the kinetic energy of thermally and compositionally driven convection into electrical and magnetic field energy. The field extends outwards from the core, through the mantle, and up to Earth's surface, where it is, approximately, a dipole. The poles of the dipole are located close to Earth's geographic poles. At the equator of the magnetic field, the magnetic-field strength at the surface is , with global magnetic dipole moment of . The convection movements in the core are chaotic; the magnetic poles drift and periodically change alignment. This causes secular variation of the main field and field reversals at irregular intervals averaging a few times every million years. The most recent reversal occurred approximately 700,000 years ago.\n\nThe extent of Earth's magnetic field in space defines the magnetosphere. Ions and electrons of the solar wind are deflected by the magnetosphere; solar wind pressure compresses the dayside of the magnetosphere, to about 10 Earth radii, and extends the nightside magnetosphere into a long tail. Because the velocity of the solar wind is greater than the speed at which waves propagate through the solar wind, a supersonic bowshock precedes the dayside magnetosphere within the solar wind. Charged particles are contained within the magnetosphere; the plasmasphere is defined by low-energy particles that essentially follow magnetic field lines as Earth rotates; the ring current is defined by medium-energy particles that drift relative to the geomagnetic field, but with paths that are still dominated by the magnetic field, and the Van Allen radiation belt are formed by high-energy particles whose motion is essentially random, but otherwise contained by the magnetosphere.\n\nDuring magnetic storms and substorms, charged particles can be deflected from the outer magnetosphere and especially the magnetotail, directed along field lines into Earth's ionosphere, where atmospheric atoms can be excited and ionized, causing the aurora.\n\nEarth's rotation period relative to the Sun—its mean solar day—is of mean solar time (). Because Earth's solar day is now slightly longer than it was during the 19th century due to tidal deceleration, each day varies between longer.\n\nEarth's rotation period relative to the fixed stars, called its \"stellar day\" by the International Earth Rotation and Reference Systems Service (IERS), is of mean solar time (UT1), or Earth's rotation period relative to the precessing or moving mean vernal equinox, misnamed its \"sidereal day\", is of mean solar time (UT1) . Thus the sidereal day is shorter than the stellar day by about 8.4 ms. The length of the mean solar day in SI seconds is available from the IERS for the periods 1623–2005 and 1962–2005.\n\nApart from meteors within the atmosphere and low-orbiting satellites, the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15°/h = 15'/min. For bodies near the celestial equator, this is equivalent to an apparent diameter of the Sun or the Moon every two minutes; from Earth's surface, the apparent sizes of the Sun and the Moon are approximately the same.\n\nEarth orbits the Sun at an average distance of about every 365.2564 mean solar days, or one sidereal year. This gives an apparent movement of the Sun eastward with respect to the stars at a rate of about 1°/day, which is one apparent Sun or Moon diameter every 12 hours. Due to this motion, on average it takes 24 hours—a solar day—for Earth to complete a full rotation about its axis so that the Sun returns to the meridian. The orbital speed of Earth averages about , which is fast enough to travel a distance equal to Earth's diameter, about , in seven minutes, and the distance to the Moon, , in about 3.5 hours.\n\nThe Moon and Earth orbit a common barycenter every 27.32 days relative to the background stars. When combined with the Earth–Moon system's common orbit around the Sun, the period of the synodic month, from new moon to new moon, is 29.53 days. Viewed from the celestial north pole, the motion of Earth, the Moon, and their axial rotations are all counterclockwise. Viewed from a vantage point above the north poles of both the Sun and Earth, Earth orbits in a counterclockwise direction about the Sun. The orbital and axial planes are not precisely aligned: Earth's axis is tilted some 23.44 degrees from the perpendicular to the Earth–Sun plane (the ecliptic), and the Earth–Moon plane is tilted up to ±5.1 degrees against the Earth–Sun plane. Without this tilt, there would be an eclipse every two weeks, alternating between lunar eclipses and solar eclipses.\n\nThe Hill sphere, or the sphere of gravitational influence, of the Earth is about in radius. This is the maximum distance at which the Earth's gravitational influence is stronger than the more distant Sun and planets. Objects must orbit the Earth within this radius, or they can become unbound by the gravitational perturbation of the Sun.\n\nEarth, along with the Solar System, is situated in the Milky Way and orbits about 28,000 light-years from its center. It is about 20 light-years above the galactic plane in the Orion Arm.\n\nThe axial tilt of the Earth is approximately 23.439281° with the axis of its orbit plane, always pointing towards the Celestial Poles. Due to Earth's axial tilt, the amount of sunlight reaching any given point on the surface varies over the course of the year. This causes the seasonal change in climate, with summer in the Northern Hemisphere occurring when the Tropic of Cancer is facing the Sun, and winter taking place when the Tropic of Capricorn in the Southern Hemisphere faces the Sun. During the summer, the day lasts longer, and the Sun climbs higher in the sky. In winter, the climate becomes cooler and the days shorter. In northern temperate latitudes, the Sun rises north of true east during the summer solstice, and sets north of true west, reversing in the winter. The Sun rises south of true east in the summer for the southern temperate zone and sets south of true west.\n\nAbove the Arctic Circle, an extreme case is reached where there is no daylight at all for part of the year, up to six months at the North Pole itself, a polar night. In the Southern Hemisphere, the situation is exactly reversed, with the South Pole oriented opposite the direction of the North Pole. Six months later, this pole will experience a midnight sun, a day of 24 hours, again reversing with the South Pole.\n\nBy astronomical convention, the four seasons can be determined by the solstices—the points in the orbit of maximum axial tilt toward or away from the Sun—and the equinoxes, when the direction of the tilt and the direction to the Sun are perpendicular. In the Northern Hemisphere, winter solstice currently occurs around 21 December; summer solstice is near 21 June, spring equinox is around 20 March and autumnal equinox is about 22 or 23 September. In the Southern Hemisphere, the situation is reversed, with the summer and winter solstices exchanged and the spring and autumnal equinox dates swapped.\n\nThe angle of Earth's axial tilt is relatively stable over long periods of time. Its axial tilt does undergo nutation; a slight, irregular motion with a main period of 18.6 years. The orientation (rather than the angle) of Earth's axis also changes over time, precessing around in a complete circle over each 25,800 year cycle; this precession is the reason for the difference between a sidereal year and a tropical year. Both of these motions are caused by the varying attraction of the Sun and the Moon on Earth's equatorial bulge. The poles also migrate a few meters across Earth's surface. This polar motion has multiple, cyclical components, which collectively are termed quasiperiodic motion. In addition to an annual component to this motion, there is a 14-month cycle called the Chandler wobble. Earth's rotational velocity also varies in a phenomenon known as length-of-day variation.\n\nIn modern times, Earth's perihelion occurs around 3 January, and its aphelion around 4 July. These dates change over time due to precession and other orbital factors, which follow cyclical patterns known as Milankovitch cycles. The changing Earth–Sun distance causes an increase of about 6.9% in solar energy reaching Earth at perihelion relative to aphelion. Because the Southern Hemisphere is tilted toward the Sun at about the same time that Earth reaches the closest approach to the Sun, the Southern Hemisphere receives slightly more energy from the Sun than does the northern over the course of a year. This effect is much less significant than the total energy change due to the axial tilt, and most of the excess energy is absorbed by the higher proportion of water in the Southern Hemisphere.\n\nA study from 2016 suggested that Planet Nine tilted all Solar System planets, including Earth's, by about six degrees.\n\nA planet that can sustain life is termed habitable, even if life did not originate there. Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain metabolism. The distance of Earth from the Sun, as well as its orbital eccentricity, rate of rotation, axial tilt, geological history, sustaining atmosphere, and magnetic field all contribute to the current climatic conditions at the surface.\n\nA planet's life forms inhabit ecosystems, whose total is sometimes said to form a \"biosphere\". Earth's biosphere is thought to have begun evolving about . The biosphere is divided into a number of biomes, inhabited by broadly similar plants and animals. On land, biomes are separated primarily by differences in latitude, height above sea level and humidity. Terrestrial biomes lying within the Arctic or Antarctic Circles, at high altitudes or in extremely arid areas are relatively barren of plant and animal life; species diversity reaches a peak in humid lowlands at equatorial latitudes.\n\nIn July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.\n\nEarth has resources that have been exploited by humans. Those termed non-renewable resources, such as fossil fuels, only renew over geological timescales.\n\nLarge deposits of fossil fuels are obtained from Earth's crust, consisting of coal, petroleum, and natural gas. These deposits are used by humans both for energy production and as feedstock for chemical production. Mineral ore bodies have also been formed within the crust through a process of ore genesis, resulting from actions of magmatism, erosion, and plate tectonics. These bodies form concentrated sources for many metals and other useful elements.\n\nEarth's biosphere produces many useful biological products for humans, including food, wood, pharmaceuticals, oxygen, and the recycling of many organic wastes. The land-based ecosystem depends upon topsoil and fresh water, and the oceanic ecosystem depends upon dissolved nutrients washed down from the land. In 1980, of Earth's land surface consisted of forest and woodlands, was grasslands and pasture, and was cultivated as croplands. The estimated amount of irrigated land in 1993 was . Humans also live on the land by using building materials to construct shelters.\n\nLarge areas of Earth's surface are subject to extreme weather such as tropical cyclones, hurricanes, or typhoons that dominate life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, sinkholes, blizzards, floods, droughts, wildfires, and other calamities and disasters.\n\nMany localized areas are subject to human-made pollution of the air and water, acid rain and toxic substances, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion.\n\nThere is a scientific consensus linking human activities to global warming due to industrial carbon dioxide emissions. This is predicted to produce changes such as the melting of glaciers and ice sheets, more extreme temperature ranges, significant changes in weather and a global rise in average sea levels.\n\nCartography, the study and practice of map-making, and geography, the study of the lands, features, inhabitants and phenomena on Earth, have historically been the disciplines devoted to depicting Earth. Surveying, the determination of locations and distances, and to a lesser extent navigation, the determination of position and direction, have developed alongside cartography and geography, providing and suitably quantifying the requisite information.\n\nEarth's human population reached approximately seven billion on 31 October 2011. Projections indicate that the world's human population will reach 9.2 billion in 2050. Most of the growth is expected to take place in developing nations. Human population density varies widely around the world, but a majority live in Asia. By 2020, 60% of the world's population is expected to be living in urban, rather than rural, areas.\n\nIt is estimated that one-eighth of Earth's surface is suitable for humans to live on – three-quarters of Earth's surface is covered by oceans, leaving one-quarter as land. Half of that land area is desert (14%), high mountains (27%), or other unsuitable terrains. The northernmost permanent settlement in the world is Alert, on Ellesmere Island in Nunavut, Canada. (82°28′N) The southernmost is the Amundsen–Scott South Pole Station, in Antarctica, almost exactly at the South Pole. (90°S)\nIndependent sovereign nations claim the planet's entire land surface, except for some parts of Antarctica, a few land parcels along the Danube river's western bank, and the unclaimed area of Bir Tawil between Egypt and Sudan. , there are 193 sovereign states that are member states of the United Nations, plus two observer states and 72 dependent territories and states with limited recognition. Earth has never had a sovereign government with authority over the entire globe, although some nation-states have striven for world domination and failed.\n\nThe United Nations is a worldwide intergovernmental organization that was created with the goal of intervening in the disputes between nations, thereby avoiding armed conflict. The U.N. serves primarily as a forum for international diplomacy and international law. When the consensus of the membership permits, it provides a mechanism for armed intervention.\n\nThe first human to orbit Earth was Yuri Gagarin on 12 April 1961. In total, about 487 people have visited outer space and reached orbit , and, of these, twelve have walked on the Moon. Normally, the only humans in space are those on the International Space Station. The station's crew, made up of six people, is usually replaced every six months. The farthest that humans have traveled from Earth is , achieved during the Apollo 13 mission in 1970.\n\nThe Moon is a relatively large, terrestrial, planet-like natural satellite, with a diameter about one-quarter of Earth's. It is the largest moon in the Solar System relative to the size of its planet, although Charon is larger relative to the dwarf planet Pluto. The natural satellites of other planets are also referred to as \"moons\", after Earth's.\n\nThe gravitational attraction between Earth and the Moon causes tides on Earth. The same effect on the Moon has led to its tidal locking: its rotation period is the same as the time it takes to orbit Earth. As a result, it always presents the same face to the planet. As the Moon orbits Earth, different parts of its face are illuminated by the Sun, leading to the lunar phases; the dark part of the face is separated from the light part by the solar terminator.\nDue to their tidal interaction, the Moon recedes from Earth at the rate of approximately . Over millions of years, these tiny modifications—and the lengthening of Earth's day by about 23 µs/yr—add up to significant changes. During the Devonian period, for example, (approximately ) there were 400 days in a year, with each day lasting 21.8 hours.\n\nThe Moon may have dramatically affected the development of life by moderating the planet's climate. Paleontological evidence and computer simulations show that Earth's axial tilt is stabilized by tidal interactions with the Moon. Some theorists think that without this stabilization against the torques applied by the Sun and planets to Earth's equatorial bulge, the rotational axis might be chaotically unstable, exhibiting chaotic changes over millions of years, as appears to be the case for Mars.\n\nViewed from Earth, the Moon is just far enough away to have almost the same apparent-sized disk as the Sun. The angular size (or solid angle) of these two bodies match because, although the Sun's diameter is about 400 times as large as the Moon's, it is also 400 times more distant. This allows total and annular solar eclipses to occur on Earth.\n\nThe most widely accepted theory of the Moon's origin, the giant-impact hypothesis, states that it formed from the collision of a Mars-size protoplanet called Theia with the early Earth. This hypothesis explains (among other things) the Moon's relative lack of iron and volatile elements and the fact that its composition is nearly identical to that of Earth's crust.\n\nEarth has at least five co-orbital asteroids, including 3753 Cruithne and . A trojan asteroid companion, , is librating around the leading Lagrange triangular point, L4, in the Earth's orbit around the Sun.\n\nThe tiny near-Earth asteroid makes close approaches to the Earth–Moon system roughly every twenty years. During these approaches, it can orbit Earth for brief periods of time.\n\n, there are 1,886 operational, human-made satellites orbiting Earth. There are also inoperative satellites, including Vanguard 1, the oldest satellite currently in orbit, and over 16,000 pieces of tracked space debris. Earth's largest artificial satellite is the International Space Station.\n\nThe standard astronomical symbol of Earth consists of a cross circumscribed by a circle, , representing the four corners of the world.\n\nHuman cultures have developed many views of the planet. Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity, and by the mid-20th century, the Gaia Principle compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability. Creation myths in many religions involve the creation of Earth by a supernatural deity or deities.\n\nScientific investigation has resulted in several culturally transformative shifts in people's view of the planet. Initial belief in a flat Earth was gradually displaced in the Greek colonies of southern Italy during the late 6th century BC by the idea of spherical Earth, which was attributed to both the philosophers Pythagoras and Parmenides. By the end of the 5th century BC, the sphericity of Earth was universally accepted among Greek intellectuals. Earth was generally believed to be the center of the universe until the 16th century, when scientists first conclusively demonstrated that it was a moving object, comparable to the other planets in the Solar System. Due to the efforts of influential Christian scholars and clerics such as James Ussher, who sought to determine the age of Earth through analysis of genealogies in Scripture, Westerners before the 19th century generally believed Earth to be a few thousand years old at most. It was only during the 19th century that geologists realized Earth's age was at least many millions of years.\n\nLord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old. The perception of Earth shifted again in the 20th century when humans first viewed it from orbit, and especially with photographs of Earth returned by the Apollo program.\n\n</math>, where \"m\" is the mass of Earth, \"a\" is an astronomical unit, and \"M\" is the mass of the Sun. So the radius in AU is about formula_1.</ref>\n\n"}
{"id": "944638", "url": "https://en.wikipedia.org/wiki?curid=944638", "title": "Earth's energy budget", "text": "Earth's energy budget\n\nEarth's energy budget accounts for the balance between the energy Earth receives from the Sun, the energy Earth radiates back into outer space after having been distributed throughout the five components of Earth's climate system and having thus powered the so-called Earth’s heat engine. This system is made up of earth's water, ice, atmosphere, rocky crust, and all living things.\n\nQuantifying changes in these amounts is required to accurately model the Earth's climate. \n\nReceived radiation is unevenly distributed over the planet, because the Sun heats equatorial regions more than polar regions. The atmosphere and ocean work non-stop to even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation. Earth is very close to being in radiative equilibrium, the situation where the incoming solar energy is balanced by an equal flow of heat to space; under that condition, global temperatures will be \"relatively\" stable. Globally, over the course of the year, the Earth system—land surfaces, oceans, and atmosphere—absorbs and then radiates back to space an average of about 240 watts of solar power per square meter. Anything that increases or decreases the amount of incoming or outgoing energy will change global temperatures in response.\n\nHowever, Earth's energy balance and heat fluxes depend on many factors, such as atmospheric composition (mainly aerosols and greenhouse gases), the albedo (reflectivity) of surface properties, cloud cover and vegetation and land use patterns.\n\nChanges in surface temperature due to Earth's energy budget do not occur instantaneously, due to the inertia of the oceans and the cryosphere. The net heat flux is buffered primarily by becoming part of the ocean's heat content, until a new equilibrium state is established between radiative forcings and the climate response.\n\nIn spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via insolation (all forms of electromagnetic radiation).\n\nTo quantify Earth's \"heat budget\" or \"heat balance\", let the insolation received at the top of the atmosphere be 100 units (100 units = about 1,360 watts per square meter facing the sun), as shown in the accompanying illustration. Called the albedo of Earth, around 35 units are reflected back to space: 27 from the top of clouds, 2 from snow and ice-covered areas, and 6 by other parts of the atmosphere. The 65 remaining units are absorbed: 14 within the atmosphere and 51 by the Earth’s surface. These 51 units are radiated to space in the form of terrestrial radiation: 17 directly radiated to space and 34 absorbed by the atmosphere (19 through latent heat of condensation, 9 via convection and turbulence, and 6 directly absorbed). The 48 units absorbed by the atmosphere (34 units from terrestrial radiation and 14 from insolation) are finally radiated back to space. These 65 units (17 from the ground and 48 from the atmosphere) balance the 65 units absorbed from the sun in order to maintain zero net gain of energy by the Earth.\n\nThe total amount of energy received per second at the top of Earth's atmosphere (TOA) is measured in watts and is given by the solar constant times the cross-sectional area of the Earth. Because the surface area of a sphere is four times the cross-sectional surface area of a sphere (i.e. the area of a circle), the average TOA flux is one quarter of the solar constant and so is approximately 340 W/m². Since the absorption varies with location as well as with diurnal, seasonal and annual variations, the numbers quoted are long-term averages, typically averaged from multiple satellite measurements.\n\nOf the ~340 W/m² of solar radiation received by the Earth, an average of ~77 W/m² is reflected back to space by clouds and the atmosphere and ~23 W/m² is reflected by the surface albedo, leaving ~240 W/m² of solar energy input to the Earth's energy budget. This gives the earth a mean net albedo of 0.29.\n\nThe geothermal heat flux from the Earth's interior is estimated to be 47 terawatts. This comes to 0.087 watt/square metre, which represents only 0.027% of Earth's total energy budget at the surface, which is dominated by 173,000 terawatts of incoming solar radiation.\n\nHuman production of energy is even lower, at an estimated 18 TW.\n\nPhotosynthesis has a larger effect: photosynthetic efficiency turns up to 2% of incoming sunlight into biomass, for a total photosynthetic productivity of earth between ~1500–2250 TW (~1%+/-0.26% solar energy hitting the Earth's surface).\n\nOther minor sources of energy are usually ignored in these calculations, including accretion of interplanetary dust and solar wind, light from stars other than the Sun and the thermal radiation from space. Earlier, Joseph Fourier had claimed that deep space radiation was significant in a paper often cited as the first on the greenhouse effect.\n\nLongwave radiation is usually defined as outgoing infrared energy leaving the planet. However, the atmosphere absorbs parts initially, or cloud cover can reflect radiation. Generally, heat energy is transported between the planet's surface layers (land and ocean) to the atmosphere, transported via evapotranspiration and latent heat fluxes or conduction/convection processes. Ultimately, energy is radiated in the form of longwave infrared radiation back into space.\n\nRecent satellite observations indicate additional precipitation, which is sustained by increased energy leaving the surface through evaporation (the latent heat flux), offsetting increases in longwave flux to the surface.\n\nIf the incoming energy flux is not equal to the outgoing energy flux, net heat is added to or lost by the planet (if the incoming flux is larger or smaller than the outgoing respectively).\n\nAn imbalance must show in something on Earth warming or cooling (depending on the direction of the imbalance), and the ocean being the larger thermal reservoir on Earth, is a prime candidate for measurements.\n\nEarth's energy imbalance measurements provided by Argo floats have detected an accumulation of ocean heat content (OHC). The estimated imbalance was measured during a deep solar minimum of 2005–2010 to be 0.58 ± 0.15 W/m². This level of detail cannot be inferred directly from measurements of surface energy fluxes, which have combined uncertainties of the order of ± 17 W/m².\n\nSeveral satellites indirectly measure the energy absorbed and radiated by Earth and by inference the energy imbalance. The NASA Earth Radiation Budget Experiment (ERBE) project involves three such satellites: the Earth Radiation Budget Satellite (ERBS), launched October 1984; NOAA-9, launched December 1984; and NOAA-10, launched September 1986.\n\nToday NASA's satellite instruments, provided by CERES, part of the NASA's Earth Observing System (EOS), are designed to measure both solar-reflected and Earth-emitted radiation.\n\nThe major atmospheric gases (oxygen and nitrogen) are transparent to incoming sunlight but are also transparent to outgoing thermal (infrared) radiation. However, water vapor, carbon dioxide, methane and other trace gases are opaque to many wavelengths of thermal radiation. The Earth's surface radiates the net equivalent of 17 percent of the incoming solar energy in the form of thermal infrared. However, the amount that directly escapes to space is only about 12 percent of incoming solar energy. The remaining fraction, 5 to 6 percent, is absorbed by the atmosphere by greenhouse gas molecules.\nWhen greenhouse gas molecules absorb thermal infrared energy, their temperature rises. Those gases then radiate an increased amount of thermal infrared energy in all directions. Heat radiated upward continues to encounter greenhouse gas molecules; those molecules also absorb the heat, and their temperature rises and the amount of heat they radiate increases. The atmosphere thins with altitude, and at roughly 5–6 kilometres, the concentration of greenhouse gases in the overlying atmosphere is so thin that heat can escape to space.\n\nBecause greenhouse gas molecules radiate infrared energy in all directions, some of it spreads downward and ultimately returns to the Earth's surface, where it is absorbed. The Earth's surface temperature is thus higher than it would be if it were heated only by direct solar heating. This supplemental heating is the natural greenhouse effect. It is as if the Earth is covered by a blanket that allows high frequency radiation (sunlight) to enter, but slows the rate at which the low frequency infrared radiant energy emitted by the Earth leaves.\n\nA change in the incident radiated portion of the energy budget is referred to as a radiative forcing.\n\nClimate sensitivity is the steady state change in the equilibrium temperature as a result of changes in the energy budget.\n\nClimate forcings are changes that cause temperatures to rise or fall, disrupting the energy balance. Natural climate forcings include changes in the Sun's brightness, Milankovitch cycles (small variations in the shape of Earth's orbit and its axis of rotation that occur over thousands of years) and volcanic eruptions that inject light-reflecting particles as high as the stratosphere. Man-made forcings include particle pollution (aerosols) that absorb and reflect incoming sunlight; deforestation, which changes how the surface reflects and absorbs sunlight; and the rising concentration of atmospheric carbon dioxide and other greenhouse gases, which decreases the rate at which heat is radiated to space.\n\nA forcing can trigger feedbacks that intensify (positive feedback) or weaken (negative feedback) the original forcing. For example, loss of ice at the poles, which makes them less reflective, causes greater absorption of energy and so increases the rate at which the ice melts, is an example of a positive feedback.\n\nThe observed planetary energy imbalance during the recent solar minimum shows that solar forcing of climate, although natural and significant, is overwhelmed by anthropogenic climate forcing.\n\nIn 2012, NASA scientists reported that to stop global warming atmospheric CO content would have to be reduced to 350 ppm or less, assuming all other climate forcings were fixed. The impact of anthropogenic aerosols has not been quantified, but individual aerosol types are thought to have substantial heating and cooling effects.\n\n\n"}
{"id": "40159918", "url": "https://en.wikipedia.org/wiki?curid=40159918", "title": "Ecosystem health", "text": "Ecosystem health\n\nEcosystem health is a metaphor used to describe the condition of an ecosystem. Ecosystem condition can vary as a result of fire, flooding, drought, extinctions, invasive species, climate change, mining, overexploitation in fishing, farming or logging, chemical spills, and a host of other reasons. There is no universally accepted benchmark for a healthy ecosystem, rather the apparent health status of an ecosystem can vary depending upon which health metrics are employed in judging it and which societal aspirations are driving the assessment. Advocates of the health metaphor argue for its simplicity as a communication tool. \"Policy-makers and the public need simple, understandable concepts like health.\" Critics worry that ecosystem health, a \"value-laden construct\", is often \"passed off as science to unsuspecting policy makers and the public.\"\n\nThe health metaphor applied to the environment has been in use at least since the early 1800s and the great American conservationist Aldo Leopold (1887–1948) spoke metaphorically of land health, land sickness, mutilation, and violence when describing land use practices. The term \"ecosystem management\" has been in use at least since the 1950s. The term \"ecosystem health\" has become widespread in the ecological literature, as a general metaphor meaning something good, and as an environmental quality goal in field assessments of rivers, lakes, seas, and forests.\n\nRecently however this metaphor has been subject of quantitative formulation using complex systems concepts such as criticality, meaning that a healthy ecosystem is in some sort of balance between adaptability (randomness) and robustness (order) . Nevertheless the universality of criticality is still under examination and is known as the Criticality Hypothesis, which states that systems in a dynamic regime shifting between order and disorder, attain the highest level of computational capabilities and achieve an optimal trade-off between robustness and flexibility. Recent results in cell and evolutionary biology, neuroscience and computer science have great interest in the criticality hypothesis, emphasizing its role as a viable candidate general law in the realm of adaptive complex systems (see and references therein).\n\nThe term ecosystem health has been employed to embrace some suite of environmental goals deemed desirable. Edward Grumbine's highly cited paper \"What is ecosystem management?\" surveyed ecosystem management and ecosystem health literature and summarized frequently encountered goal statements:\n\nGrumbine describes each of these goals as a \"value statement\" and stresses the role of human values in setting ecosystem management goals.\n\nIt is the last goal mentioned in the survey, accommodating humans, that is most contentious. \"We have observed that when groups of stakeholders work to define … visions, this leads to debate over whether to emphasize ecosystem health or human well-being … Whether the priority is ecosystems or people greatly influences stakeholders' assessment of desirable ecological and social states.\" and, for example, \"For some, wolves are critical to ecosystem health and an essential part of nature, for others they are a symbol of government overreach threatening their livelihoods and cultural values.\"\n\nMeasuring ecosystem health requires extensive goal-driven environmental sampling. For example, a vision for ecosystem health of Lake Superior was developed by a public forum and a series of objectives were prepared for protection of habitat and maintenance of populations of some 70 indigenous fish species. A suite of 80 lake health indicators was developed for the Great Lakes Basin including monitoring native fish species, exotic species, water levels, phosphorus levels, toxic chemicals, phytoplankton, zooplankton, fish tissue contaminants, etc.\n\nSome authors have attempted broad definitions of ecosystem health, such as benchmarking as healthy the historical ecosystem state \"prior to the onset of anthropogenic stress.\" A difficulty is that the historical composition of many human-altered ecosystems is unknown or unknowable. Also, fossil and pollen records indicate that the species that occupy an ecosystem reshuffle through time, so it is difficult to identify one snapshot in time as optimum or \"healthy.\".\n\nA commonly cited broad definition states that a healthy ecosystem has three attributes:\n\nWhile this captures significant ecosystem properties, a generalization is elusive as those properties do not necessarily co-vary in nature. For example, there is not necessarily a clear or consistent relationship between productivity and species richness. Similarly, the relationship between resilience and diversity is complex, and ecosystem stability may depend upon one or a few species rather than overall diversity. And some undesirable ecosystems are highly productive.\n\n\"Resilience is not desirable per se. There can be highly resilient states of ecosystems which are very undesirable from some human perspectives , such as algal-dominated coral reefs.\" Ecological resilience is a \"capacity\" that varies depending upon which properties of the ecosystem are to be studied and depending upon what kinds of disturbances are considered and how they are to be quantified. Approaches to assessing it \"face high uncertainties and still require a considerable amount of empirical and theoretical research.\"\n\nOther authors have sought a numerical index of ecosystem health that would permit quantitative comparisons among ecosystems and within ecosystems over time. One such system employs ratings of the three properties mentioned above: Health = system vigor x system organization x system resilience. Ecologist Glenn Suter argues that such indices employ \"nonsense units,\" the indices have \"no meaning; they cannot be predicted, so they are not applicable to most regulatory problems; they have no diagnostic power; effects of one component are eclipsed by responses of other components, and the reason for a high or low index value is unknown.\"\n\nHealth metrics are determined by stakeholder goals, which drive ecosystem definition. An ecosystem is an abstraction. \"Ecosystems cannot be identified or found in nature. Instead, they must be delimited by an observer. This can be done in many different ways for the same chunk of nature, depending on the specific perspectives of interest.\"\n\nEcosystem definition determines the acceptable range of variability (reference conditions) and determines measurement variables. The latter are used as indicators of ecosystem structure and function, and can be used as indicators of \"health\".\n\nAn indicator is a variable, such as a chemical or biological property, that when measured, is used to infer trends in another (unmeasured) environmental variable or cluster of unmeasured variables (the indicandum). For example, rising mortality rate of canaries in a coal mine is an indicator of rising carbon monoxide levels. Rising chlorophyll-a levels in a lake may signal eutrophication.\n\nEcosystem assessments employ two kinds of indicators, descriptive indicators and normative indicators. \"Indicators can be used descriptively for a scientific purpose or normatively for a political purpose.\"\n\nUsed descriptively, high chlorophyll-a is an indicator of eutrophication, but it may also be used as an ecosystem health indicator. When used as a normative (health) indicator, it indicates a rank on a health scale, a rank that can vary widely depending on societal preferences as to what is desirable. A high chlorophyll-a level in a natural successional wetland might be viewed as healthy whereas a human-impacted wetland with the \"same\" indicator value may be judged unhealthy.\n\nEstimation of ecosystem health has been criticized for intermingling the two types of environmental indicators. A health indicator is a normative indicator, and if conflated with descriptive indicators \"implies that normative values can be measured objectively, which is certainly not true. Thus, implicit values are insinuated to the reader, a situation which has to be avoided.\"\n\nIt can be argued that the very act of selecting indicators of any kind is biased by the observer's perspective but separation of goals from descriptions has been advocated as a step toward transparency: \"A separation of descriptive and normative indicators is essential from the perspective of the philosophy of science … Goals and values cannot be deduced directly from descriptions … a fact that is emphasized repeatedly in the literature of environmental ethics … Hence, we advise always specifying the definition of indicators and propose clearly distinguishing ecological indicators in science from policy indicators used for decision-making processes.\"\n\nAnd integration of multiple, possibly conflicting, normative indicators into a single measure of \"ecosystem health\" is problematic. Using 56 indicators, \"determining environmental status and assessing marine ecosystems health in an integrative way is still one of the grand challenges in marine ecosystems ecology, research and management\"\n\nAnother issue with indicators is validity. Good indicators must have an independently validated high predictive value, that is high sensitivity (high probability of indicating a significant change in the indicandum) and high specificity (low probability of wrongly indicating a change). The reliability of various health metrics has been questioned and \"what combination of measurements should be used to evaluate ecosystems is a matter of current scientific debate.\" Most attempts to identify ecological indicators have been correlative rather than derived from prospective testing of their predictive value and the selection process for many indicators has been based upon weak evidence or has been lacking in evidence.\n\nIn some cases no reliable indicators are known: \"We found no examples of invertebrates successfully used in [forest] monitoring programs. Their richness and abundance ensure that they play significant roles in ecosystem function but thwart focus on a few key species.\" And, \"Reviews of species-based monitoring approaches reveal that no single species, nor even a group of species, accurately reflects entire communities. Understanding the response of a single species may not provide reliable predictions about a group of species even when the group is a few very similar species.\"\n\nA trade-off between human health and the \"health\" of nature has been termed the \"health paradox\" and it illuminates how human values drive perceptions of ecosystem health.\n\nHuman health has benefited by sacrificing the \"health\" of wild ecosystems, such as dismantling and damming of wild valleys, destruction of mosquito-bearing wetlands, diversion of water for irrigation, conversion of wilderness to farmland, timber removal, and extirpation of tigers, whales, ferrets, and wolves.\n\nThere has been an acrimonious schism among conservationists and resource managers over the question of whether to \"ratchet back human domination of the biosphere\" or whether to embrace it. These two perspectives have been characterized as utilitarian vs protectionist.\n\nThe utilitarian view treats human health and well-being as criteria of ecosystem health. For example, destruction of wetlands to control malaria mosquitoes \"resulted in an improvement in ecosystem health.\"\nThe protectionist view treats humans as an invasive species: \"If there was ever a species that qualified as an invasive pest, it is \"Homo sapiens\",\"\n\nProponents of the utilitarian view argue that \"healthy ecosystems are characterized by their capability to sustain healthy human populations,\" and \"healthy ecosystems must be economically viable,\" as it is \"unhealthy\" ecosystems that are likely to result in increases in contamination, infectious diseases, fires, floods, crop failures and fishery collapse.\n\nProtectionists argue that privileging of human health is a conflict of interest as humans have demolished massive numbers of ecosystems to maintain their welfare, also disease and parasitism are historically normal in pre-industrial nature. Diseases and parasites promote ecosystem functioning, driving biodiversity and productivity, and parasites may constitute a significant fraction of ecosystem biomass.\n\nThe very choice of the word \"health\" applied to ecology has been questioned as lacking in neutrality in a BioScience article on responsible use of scientific language: \"Some conservationists fear that these terms could endorse human domination of the planet … and could exacerbate the shifting cognitive baseline whereby humans tend to become accustomed to new and often degraded ecosystems and thus forget the nature of the past.\"\n\nCriticism of ecosystem health largely targets the failure of proponents to explicitly distinguish the normative dimension from the descriptive dimension, and has included the following:\n\nAlternatives have been proposed for the term ecosystem health, including more neutral language such as ecosystem status, ecosystem prognosis, and ecosystem sustainability. Another alternative to the use of a health metaphor is to \"express exactly and clearly the public policy and the management objective\", to employ habitat descriptors and real properties of ecosystems. An example of a policy statement is \"The maintenance of viable natural populations of wildlife and ecological functions always takes precedence over any human use of wildlife.\" An example of a goal is \"Maintain viable populations of all native species in situ.\" An example of a management objective is \"Maintain self-sustaining populations of lake whitefish within the range of abundance observed during 1990-99.\"\n\nKurt Jax presented an ecosystem assessment format that avoids imposing a preconceived notion of normality, that avoids the muddling of normative and descriptive, and that gives serious attention to ecosystem definition. (1) Societal purposes for the ecosystem are negotiated by stakeholders, (2) a functioning ecosystem is defined with emphasis on phenomena relevant to stakeholder goals, (3) benchmark reference conditions and permissible variation of the system are established, (4) measurement variables are chosen for use as indicators, and (5) the time scale and spatial scale of assessment are decided.\n\nEcological health has been used as a medical term in reference to human allergy and multiple chemical sensitivity and as a public health term for programs to modify health risks (diabetes, obesity, smoking, etc.). Human health itself, when viewed in its broadest sense, is viewed as having ecological foundations. It is also an urban planning term in reference to \"green\" cities (composting, recycling), and has been used loosely with regard to various environmental issues, and as the condition of human-disturbed environmental sites. Ecosystem integrity implies a condition of an ecosystem exposed to a minimum of human influence. Ecohealth is the relationship of human health to the environment, including the effect of climate change, wars, food production, urbanization, and ecosystem structure and function. Ecosystem management and ecosystem-based management refer to the sustainable management of ecosystems and in some cases may employ the terms ecosystem health or ecosystem integrity as a goal.\n"}
{"id": "31450053", "url": "https://en.wikipedia.org/wiki?curid=31450053", "title": "Energy accidents", "text": "Energy accidents\n\nEnergy resources bring with them great social and economic promise, providing financial growth for communities and energy services for local economies. However, the infrastructure which delivers energy services can break down in an energy accident, sometimes causing much damage, and energy fatalities can occur, and with many systems often deaths will happen even when the systems are working as intended.\n\nHistorically, coal mining has been the most dangerous energy activity and the list of historical coal mining disasters is a long one. Underground mining hazards include suffocation, gas poisoning, roof collapse and gas explosions. Open cut mining hazards are principally mine wall failures and vehicle collisions. In the US alone, more than 100,000 coal miners have been killed in accidents over the past century, with more than 3,200 dying in 1907 alone.\n\nAccording to Benjamin K. Sovacool, 279 \"major\" energy accidents occurred from 1907 to 2007 and they caused 182,156 deaths with $41 billion in property damages, with these figures not including deaths from smaller accidents.\n\nHowever, by far the greatest energy fatalities that result from energy generation by humanity, is the creation of air pollution. The most lethal of which, particulate matter, which is primarily generated from the burning of fossil fuels and biomass is (counting outdoor air pollution effects only) estimated to cause 2.1 million deaths annually.\n\nAccording to Benjamin K. Sovacool, while responsible for less than 1 percent of the total number of energy accidents, hydroelectric facilities claimed 94 percent of reported immediate fatalities. Results on immediate fatalities are dominated by one disaster in which Typhoon Nina in 1975 washed out the Shimantan Dam (Henan Province, China) and 171,000 people perished. While the other major accident that involved greater than 1000 immediate deaths followed the rupture of the NNPC petroleum pipeline in 1998 and the resulting explosion. The other singular accident described by Sovacool is the \"predicted\" latent death toll of greater than 1000, as a result of the 1986 steam explosion at the Chernobyl nuclear reactor in the Ukraine. With approximately 4000 deaths in total, to eventually result in the decades ahead due to the radio-isotope pollution released.\n\nIn the oil and gas industry, the need for improved safety culture and training within companies is evidenced by the finding that workers new to a company are more likely to be involved in fatalities.\n\nCoal mining accidents resulted in 5,938 immediate deaths in 2005, and 4746 immediate deaths in 2006 in China alone according to the World Wildlife Fund. Coal mining is the most dangerous occupation in China, the death rate for every 100 tons of coal mined is 100 times that of the death rate in the US and 30 times that achieved in South Africa. Moreover, 600,000 Chinese coal miners, as of 2004, were suffering from Coalworker's pneumoconiosis (known as \"black lung\") a disease of the lungs caused by long-continued inhalation of coal dust. And the figure increases by 70,000 miners every year in China.\n\nHistorically, coal mining has been a very dangerous activity and the list of historical coal mining disasters is a long one. In the US alone, more than 100,000 coal miners were killed in accidents over the past century, with more than 3,200 dying in 1907 alone. In the decades following this peak, an annual death toll of 1,500 miner fatalities occurred every year in the US until approximately the 1970s. Coal mining fatalities in the US between 1990 and 2012 have continued to decline, with fewer than 100 each year. (See more Coal mining disasters in the United States)\n\nIn the United States, in the 2000s, after three decades of regulation on the Environmental impact of the coal industry, including regulations in the 1970s and 1990s from the Clean Air Act, an act created to cut down on pollution related deaths from fossil fuel usage, US coal fired power plants were estimated, in the 2000s, to continue to cause between 10,000 and 30,000 latent, or air pollution related deaths per year, due to the emissions of sulfur dioxide, nitrogen oxides and directly emitted particulate matter that result when coal is burnt.\n\nAccording to the World Health Organization in 2012, urban outdoor air pollution, from the burning of fossil fuels and biomass is estimated to cause 3 million deaths worldwide per year and indoor air pollution from biomass and fossil fuel burning is estimated to cause approximately 4.3 million premature deaths. In 2013 a team of researchers estimated the number of premature deaths caused by particulate matter in outdoor air pollution as 2.1 million, occurring annually.\n\nBenjamin Sovacool says that while hydroelectric plants were responsible for the most fatalities, nuclear power plants rank first in terms of their economic cost, accounting for 41 percent of all property damage. Oil and hydroelectric follow at around 25 percent each, followed by natural gas at 9 percent and coal at 2 percent. Excluding Chernobyl and the Shimantan Dam, the three other most expensive accidents involved the Exxon Valdez oil spill (Alaska), The Prestige oil spill (Spain), and the Three Mile Island nuclear accident (Pennsylvania). However analysis presented in the international Journal, \"Human and Ecological Risk Assessment\" found that coal, oil, Liquid petroleum gas and hydro accidents have cost more than nuclear power accidents.\n\nModern-day U.S. regulatory agencies frequently implement regulations on conventional pollution if one life or more is predicted saved per $6 million to $8 million of economic costs incurred.\n\n\n\n"}
{"id": "1610231", "url": "https://en.wikipedia.org/wiki?curid=1610231", "title": "Energy density", "text": "Energy density\n\nEnergy density is the amount of energy stored in a given system or region of space per unit volume. Colloquially it may also be used for energy per unit mass, though the accurate term for this is specific energy. Often only the \"useful\" or extractable energy is measured, which is to say that inaccessible energy (such as rest mass energy) is ignored. In cosmological and other general relativistic contexts, however, the energy densities considered are those that correspond to the elements of the stress–energy tensor and therefore do include mass energy as well as energy densities associated with the pressures described in the next paragraph.\n\nEnergy per unit volume has the same physical units as pressure, and in many circumstances is a synonym: for example, the energy density of a magnetic field may be expressed as (and behaves as) a physical pressure, and the energy required to compress a compressed gas a little more may be determined by multiplying the difference between the gas pressure and the external pressure by the change in volume. In short, pressure is a measure of the enthalpy per unit volume of a system. A pressure gradient has the potential to perform work on the surroundings by converting enthalpy to work until equilibrium is reached.\n\nThere are many different types of energy stored in materials, and it takes a particular type of reaction to release each type of energy. In order of the typical magnitude of the energy released, these types of reactions are: nuclear, chemical, electrochemical, and electrical.\n\nNuclear reactions are used by stars and nuclear power plants, both of which derive energy from the binding energy of nuclei. Chemical reactions are used by animals to derive energy from food, and by automobiles to derive energy from gasoline. Liquid hydrocarbons (fuels such as gasoline, diesel and kerozene) are today the most dense way known to economically store and transport chemical energy at a very large scale (1 kg of diesel fuel burns with the oxygen contained in ~15 kg of air). Electrochemical reactions are used by most mobile devices such as laptop computers and mobile phones to release the energy from batteries.\n\nThe following is a list of the thermal energy densities (that is to say: the amount of heat energy that can be extracted) of commonly used or well-known energy storage materials; it doesn't include uncommon or experimental materials. Note that this list does not consider the mass of reactants commonly available such as the oxygen required for combustion or the energy efficiency in use. An extended version of this table is found at Energy density#Extended Reference Table. Major reference = .\n\nThe following unit conversions may be helpful when considering the data in the table: 3.6 MJ = 1 kWh ≈ 1.34 HPh.\n\nIn energy storage applications the energy density relates the mass of an energy store to the volume of the storage facility, e.g. the fuel tank. The higher the energy density of the fuel, the more energy may be stored or transported for the same amount of volume. The energy density of a fuel per unit mass is called the specific energy of that fuel. In general an engine using that fuel will generate less kinetic energy due to inefficiencies and thermodynamic considerations—hence the specific fuel consumption of an engine will always be greater than its rate of production of the kinetic energy of motion.\n\nThe greatest energy source by far is mass itself. This energy, \"E = mc\", where \"m = ρV\", \"ρ\" is the mass per unit volume, \"V\" is the volume of the mass itself and \"c\" is the speed of light. This energy, however, can be released only by the processes of nuclear fission (0.1%), nuclear fusion (1%), or the annihilation of some or all of the matter in the volume \"V\" by matter-antimatter collisions (100%). Nuclear reactions cannot be realized by chemical reactions such as combustion. Although greater matter densities can be achieved, the density of a neutron star would approximate the most dense system capable of matter-antimatter annihilation possible. A black hole, although denser than a neutron star, does not have an equivalent anti-particle form, but would offer the same 100% conversion rate of mass to energy in the form of Hawking radiation. In the case of relatively small black holes (smaller than astronomical objects) the power output would be tremendous.\n\nThe highest density sources of energy aside from antimatter are fusion and fission. Fusion includes energy from the sun which will be available for billions of years (in the form of sunlight) but so far (2018), sustained fusion power production continues to be elusive. \n\nPower from fission of uranium and thorium in nuclear power plants will be available for many decades or even centuries because of the plentiful supply of the elements on earth, though the full potential of this source can only be realised through breeder reactors, which are, apart from the BN-600 reactor, not yet used commercially. Coal, gas, and petroleum are the current primary energy sources in the U.S. but have a much lower energy density. Burning local biomass fuels supplies household energy needs (cooking fires, oil lamps, etc.) worldwide. \n\nThe density of thermal energy contained in the core of a light water reactor (PWR or BWR) of typically 1 GWe (1 000 MW electrical corresponding to ~3 000 MW thermal) is in the range of 10 to 100 MW of thermal energy per cubic meter of cooling water depending on the location considered in the system (the core itself (~30 m), the reactor pressure vessel (~50 m), or the whole primary circuit (~300 m)). This represents a considerable density of energy which requires under all circumstances a continuous water flow at high velocity in order to be able to remove the heat from the core, even after an emergency shutdown of the reactor. The incapacity to cool the cores of three boiling water reactors (BWR) at Fukushima in 2011 after the tsunami and the resulting loss of the external electrical power and of the cold source was the cause of the meltdown of the three cores in only a few hours. Meanwhile, the three reactors were correctly shut down just after the Tōhoku earthquake. This extremely high power density distinguishes nuclear power plants (NPP's) from any thermal power plants (burning coal, fuel or gas) or any chemical plants and explains the large redundancy required to permanently control the neutron reactivity and to remove the residual heat from the core of NPP's.\n\nEnergy density differs from energy conversion efficiency (net output per input) or embodied energy (the energy output costs to provide, as harvesting, refining, distributing, and dealing with pollution all use energy). Large scale, intensive energy use impacts and is impacted by climate, waste storage, and environmental consequences.\n\nNo single energy storage method boasts the best in specific power, specific energy, and energy density. Peukert's Law describes how the amount of useful energy that can be obtained (for a lead-acid cell) depends on how quickly we pull it out. To maximize both specific energy and energy density, one can compute the specific energy density of a substance by multiplying the two values together, where the higher the number, the better the substance is at storing energy efficiently.\n\nAlternative options are discussed for energy storage to increase energy density and decrease charging time.\n\nGravimetric and volumetric energy density of some fuels and storage technologies (modified from the Gasoline article):\n\nThis table lists energy densities of systems that require external components, such as oxidisers or a heat sink or source. These figures do not take into account the mass and volume of the required components as they are assumed to be freely available and present in the atmosphere. Such systems cannot be compared with self-contained systems. These values may not be computed at the same reference conditions.\n\nDivide joule/m by 10 to get MJ/L. Divide MJ/L by 3.6 to get kWh/L.\n\nElectric and magnetic fields store energy. In a vacuum, the (volumetric) energy density is given by\n\nwhere E is the electric field and B is the magnetic field. The solution will be (in SI units) in Joules per cubic metre. In the context of magnetohydrodynamics, the physics of conductive fluids, the magnetic energy density behaves like an additional pressure that adds to the gas pressure of a plasma.\n\nIn normal (linear and nondispersive) substances, the energy density (in SI units) is\n\nwhere D is the electric displacement field and H is the magnetizing field.\n\nIn the case of absence of magnetic fields, by exploting Fröhlich's relationships it is also possible to extend these equations to anisotropy and nonlinearity dielectrics, as well as to calculate the correlated Helmholtz free energy and entropy densities.\n\n\n\n"}
{"id": "321382", "url": "https://en.wikipedia.org/wiki?curid=321382", "title": "Energy flow (ecology)", "text": "Energy flow (ecology)\n\nIn ecology, energy flow, also called the calorific flow, refers to the flow of energy through a food chain, and is the focus of study in ecological energetics. In an ecosystem, ecologists seek to quantify the relative importance of different component species and feeding relationships.\n\nA general energy flow scenario follows:\n\nThe energy is passed on from trophic level to trophic level and each time about 90% of the energy is lost, with some being lost as heat into the environment (an effect of respiration) and some being lost as incompletely digested food (egesta). Therefore, primary consumers get about 10% of the energy produced by autotrophs, while secondary consumers get 1% and tertiary consumers get 0.1%. This means the top consumer of a food chain receives the least energy, as a lot of the food chain's energy has been lost between trophic levels. This loss of energy at each level limits typical food chains to only four to six links.\n\nEcological energetics appears to have grown out of the Age of Enlightenment and the concerns of the Physiocrats. It began in the works of Sergei Podolinsky in the late 1800s, and subsequently was developed by the Soviet ecologist Vladmir Stanchinsky, the Austro-American Alfred J. Lotka, and American limnologists, Raymond Lindeman and G. Evelyn Hutchinson. It underwent substantial development by Howard T. Odum and was applied by systems ecologists, and radiation ecologists.\n\n\n"}
{"id": "44689684", "url": "https://en.wikipedia.org/wiki?curid=44689684", "title": "Energy informatics", "text": "Energy informatics\n\nEnergy Informatics is founded on flow networks that are the major suppliers and consumers of energy. Their efficiency can be improved by collecting and analyzing information.\nEnergy informatics is a research field covering the use of information and communication technology to address energy challenges. Methods used for \"smart\" implementations often combine sensors with artificial intelligence and machine learning.\n\nThe field among other consider application areas within:\n\n\n"}
{"id": "22807593", "url": "https://en.wikipedia.org/wiki?curid=22807593", "title": "Energy management software", "text": "Energy management software\n\nEnergy Management Software (EMS) is a general term and category referring to a variety of energy-related software applications which may provide utility bill tracking, real-time metering, building HVAC and lighting control systems, building simulation and modeling, carbon and sustainability reporting, IT equipment management, demand response, and/or energy audits. Managing energy can require a system of systems approach.\n\nEnergy management software often provides tools for reducing energy costs and consumption for buildings or communities. EMS collects energy data and uses it for three main purposes: Reporting, Monitoring and Engagement. Reporting may include verification of energy data, benchmarking, and setting high-level energy use reduction targets. Monitoring may include trend analysis and tracking energy consumption to identify cost-saving opportunities. Engagement can mean real-time responses (automated or manual), or the initiation of a dialogue between occupants and building managers to promote energy conservation. One engagement method that has recently gained popularity is the real-time energy consumption display available in web applications or an onsite energy dashboard/display.\n\nEnergy Management Software collects historic and/or real-time interval data, with intervals varying from quarterly billing statements to minute-by-minute smart meter readings. The data are collected from interval meters, Building Automation Systems (BAS), directly from utilities, directly from sensors on electrical circuits, or other sources. Past bills can be used to provide a comparison between pre- and post-EMS energy consumption.\n\nElectricity and Natural Gas are the most common utilities measured, though systems may monitor steam, petroleum or other energy uses, water use, and even locally generated energy. Renewable energy sources have contributed to the spurred growth in EMS data collection markets.\n\nReporting tools are targeted at owners and executives who want to automate energy and emissions auditing. Cost and consumption data from a number of buildings can be aggregated or compared with the software, saving time relative to manual reporting. EMS offers more detailed energy information than utility billing can provide; another advantage is that outside factors affecting energy use, such as weather or building occupancy, can be accounted for as part of the reporting process. This information can be used to prioritize energy savings initiatives and balance energy savings against energy-related capital expenditures.\n\nBill verification can be used to compare metered consumption against billed consumption. Bill analysis can also demonstrate the impact of different energy costs, for example by comparing electrical demand charges to consumption costs.\n\nGreenhouse gas (GHG) accounting can calculate direct or indirect GHG emissions, which may be used for internal reporting or enterprise carbon accounting.\n\nMonitoring tools track and display real-time and historical data. Often, EMS includes various benchmarking tools, such as energy consumption per square foot, weather normalization or more advanced analysis using energy modeling algorithms to identify anomalous consumption. Seeing exactly when energy is used, combined with anomaly recognition, can allow Facility or Energy Managers to identify savings opportunities.\n\nInitiatives such as demand shaving, replacement of malfunctioning equipment, retrofits of inefficient equipment, and removal of unnecessary loads can be discovered and coordinated using the EMS. For example, an unexpected energy spike at a specific time each day may indicate an improperly set or malfunctioning timer. These tools can also be used for Energy Monitoring and Targeting. EMS uses models to correct for variable factors such as weather when performing historical comparisons to verify the effect of conservation and efficiency initiatives.\n\nEMS may offer alerts, via text or email messages, when consumption values exceed pre-defined thresholds based on consumption or cost. These thresholds may be set at absolute levels, or use an energy model to determine when consumption is abnormally high or low. More recently, smartphones and tablets are becoming mainstream platforms for EMS.\n\nEngagement can refer to automated or manual responses to collected and analyzed energy data. Building control systems can respond as readily to energy fluctuation as a heating system can respond to temperature variation. Demand spikes can trigger equipment power-down processes, with or without human intervention.\n\nAnother objective of Engagement is to connect occupants’ daily choices with building energy consumption. By displaying real-time consumption information, occupants see the immediate impact of their actions. The software can be used to promote energy conservation initiatives, offer advice to the occupants, or provide a forum for feedback on sustainability initiatives.\n\nPeople-driven energy conservation programs, such as those sponsored by Energy Education, can be highly effective in reducing energy use and cost. \n\nLetting occupants know their real-time consumption alone can be responsible for a 7% reduction in energy consumption.\n\n\n"}
{"id": "26356935", "url": "https://en.wikipedia.org/wiki?curid=26356935", "title": "Energy operator", "text": "Energy operator\n\nIn quantum mechanics, energy is defined in terms of the energy operator, acting on the wave function of the system as a consequence of time translation symmetry.\n\nIt is given by:\n\nIt acts on the wave function (the probability amplitude for different configurations of the system)\n\nThe energy operator corresponds to the full energy of a system. The Schrödinger equation describes the space- and time-dependence of the slow changing (non-relativistic) wave function of a quantum system. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta.\n\nUsing the energy operator to the Schrödinger equation:\n\ncan be obtained:\n\nwhere \"i\" is the imaginary unit, \"ħ\" is the reduced Planck constant, and formula_5 is the Hamiltonian operator.\n\nIn a stationary state additionally occurs the time-independent Schrödinger equation:\nwhere \"E\" is an eigenvalue of energy.\n\nThe relativistic mass-energy relation:\n\nwhere again \"E\" = total energy, \"p\" = total 3-momentum of the particle, \"m\" = invariant mass, and \"c\" = speed of light, can similarly yield the Klein–Gordon equation:\n\nthat is:\n\nThe energy operator is easily derived from using the free particle wave function (plane wave solution to Schrödinger's equation). Starting in one dimension the wave function is\n\nThe time derivative of \"Ψ\" is\n\nBy the De Broglie relation:\n\nwe have\n\nRe-arranging the equation leads to\n\nwhere the energy factor \"E\" is a scalar value, the energy the particle has and the value that is measured. The partial derivative is a linear operator so this expression \"is\" the operator for energy:\n\nIt can be concluded that the scalar \"E\" is the eigenvalue of the operator, while formula_16 is the operator. Summarizing these results:\n\nFor a 3-d plane wave\n\nthe derivation is exactly identical, as no change is made to the term including time and therefore the time derivative. Since the operator is linear, they are valid for any linear combination of plane waves, and so they can act on any wave function without affecting the properties of the wave function or operators. Hence this must be true for any wave function. It turns out to work even in relativistic quantum mechanics, such as the Klein–Gordon equation above.\n\n"}
{"id": "52058583", "url": "https://en.wikipedia.org/wiki?curid=52058583", "title": "Energy system", "text": "Energy system\n\nAn energy system is a system primarily designed to supply energy-services to end-users. Taking a structural viewpoint, the IPCC Fifth Assessment Report defines an energy system as \"all components related to the production, conversion, delivery, and use of energy\". The field of energy economics includes energy markets and treats an energy system as the technical and economic systems that satisfy consumer demand for energy in the forms of heat, fuels, and electricity.\n\nThe first two definitions allow for demand-side measures, including daylighting, retrofitted building insulation, and passive solar building design, as well as socio-economic factors, such as aspects of energy demand management and even telecommuting, while the third does not. Neither does the third account for the informal economy in traditional biomass that is significant in many developing countries.\n\nThe analysis of energy systems thus spans the disciplines of engineering and economics. Merging ideas from both areas to form a coherent description, particularly where macroeconomic dynamics are involved, is challenging.\n\nThe concept of an energy system is evolving as new regulations, technologies, and practices enter into service – for example, emissions trading, the development of smart grids, and the greater use of energy demand management, respectively.\n\nFrom a structural perspective, an energy system is like any general system and is made up of a set of interacting component parts, located within an environment. These components derive from ideas found in engineering and economics. Taking a process view, an energy system \"consists of an integrated set of technical and economic activities operating within a complex societal framework\". The identification of the components and behaviors of an energy system depends on the circumstances, the purpose of the analysis, and the questions under investigation. The concept of an energy system is therefore an abstraction which usually precedes some form of computer-based investigation, such as the construction and use of a suitable energy model.\n\nViewed in engineering terms, an energy system lends itself to representation as a flow network: the vertices map to engineering components like power stations and pipelines and the edges map to the interfaces between these components. This approach allows collections of similar or adjacent components to be aggregated and treated as one to simplify the model. Once described thus, flow network algorithms, such as minimum cost flow, may be applied. The components themselves can be treated as simple dynamical systems in their own right.\n\nConversely, relatively pure economic modeling may adopt a sectorial approach with only limited engineering detail present. The sector and sub-sector categories published by the International Energy Agency are often used as a basis for this analysis. A 2009 study of the UK residential energy sector contrasts the use of the technology-rich Markal model with several UK sectoral housing stock models.\n\nInternational energy statistics are typically broken down by carrier, sector and sub-sector, and country. Energy carriers ( energy products) are further classified as primary energy and secondary (or intermediate) energy and sometimes final (or end-use) energy. Published energy datasets are normally adjusted so that they are internally consistent, meaning that all energy stocks and flows must balance. The IEA regularly publishes energy statistics and energy balances with varying levels of detail and cost and also offers mid-term projections based on this data. The notion of an energy carrier, as used in energy economics, is distinct and different from the definition of energy used in physics.\n\nEnergy systems can range in scope, from local, municipal, national, and regional, to global, depending on issues under investigation. Researchers may or may not include demand side measures within their definition of an energy system. The IPCC does so, for instance, but covers these measures in separate chapters on transport, buildings, industry, and agriculture.\n\nHousehold consumption and investment decisions may also be included within the ambit of an energy system. Such considerations are not common because consumer behavior is difficult to characterize, but the trend is to include human factors in models. Household decision-taking may be represented using techniques from bounded rationality and agent-based behavior. The American Association for the Advancement of Science (AAAS) specifically advocates that \"more attention should be paid to incorporating behavioral considerations other than price- and income-driven behavior into economic models [of the energy system]\".\n\nThe concept of an energy-service is central, particularly when defining the purpose of an energy system:\n\nEnergy-services can be defined as amenities that are either furnished through energy consumption or could have been thus supplied. More explicitly:\n\nA consideration of energy-services per capita and how such services contribute to human welfare and individual quality of life is paramount to the debate on sustainable energy. People living in poor regions with low levels of energy-services consumption would clearly benefit from greater consumption, but the same is not generally true for those with high levels of consumption.\n\nThe notion of energy-services has given rise to energy-service companies (ESCo) who contract to provide energy-services to a client for an extended period. The ESCo is then free to choose the best means to do so, including investments in the thermal performance and HVAC equipment of the buildings in question.\n\nISO13600, ISO13601, and ISO13602 form a set of international standards covering technical energy systems (TES). Although withdrawn prior to 2016, these documents provide useful definitions and a framework for formalizing such systems. The standards depict an energy system broken down into supply and demand sectors, linked by the flow of tradable energy commodities (or energywares). Each sector has a set of inputs and outputs, some intentional and some harmful byproducts. Sectors may be further divided into subsectors, each fulfilling a dedicated purpose. The demand sector is ultimately present to supply energyware-based services to consumers (see energy-services).\n\n"}
{"id": "32974036", "url": "https://en.wikipedia.org/wiki?curid=32974036", "title": "Fowkes hypothesis", "text": "Fowkes hypothesis\n\nThe Fowkes hypothesis (after F. M. Fowkes) is a first order approximation for surface energy. It states the surface energy is the sum of each component's forces:\nγ=γ+γ+γ+...\nwhere γ is the dispersion component, γ is the polar, γ is the dipole and so on.\n\nThe Fowkes hypothesis goes further making the approximation that the interface between an apolar liquid and apolar solid where there are only dispersive interactions acting across the interface can be estimated using the geometric mean of the contributions from each surface i.e.\n\nγ=γ+γ-2(γ x γ)\n\n\n"}
{"id": "1686779", "url": "https://en.wikipedia.org/wiki?curid=1686779", "title": "Fusion energy gain factor", "text": "Fusion energy gain factor\n\nThe fusion energy gain factor, usually expressed with the symbol Q, is the ratio of fusion power produced in a nuclear fusion reactor to the power required to maintain the plasma in steady state. The condition of \"Q\" = 1, when the power being released by the fusion reactions is equal to the required heating power, is referred to as breakeven.\n\nThe power given off by the fusion reactions may be captured within the fuel, leading to \"self-heating\". Most fusion reactions release at least some of their energy in a form that cannot be captured within the plasma, so a system at \"Q\" = 1 will cool without external heating. With typical fuels, self-heating in fusion reactors is not expected to match the external sources until at least \"Q\" = 5. If \"Q\" increases past this point, increasing self-heating eventually removes the need for external heating. At this point the reaction becomes self-sustaining, a condition called ignition. Ignition corresponds to infinite \"Q\", and is generally regarded as highly desirable for a practical reactor design.\n\nOver time, several related terms have entered the fusion lexicon. As a reactor does not cover its own heating losses until about \"Q\" = 5, the term engineering breakeven is sometimes used to describe a reactor that produces enough electricity to provide that heating. Above engineering breakeven a machine would produce more electricity than it uses, and could sell that excess. A machine that can sell enough electricity to cover its operating costs, estimated to require at least \"Q\" = 20, is sometimes known as economic breakeven.\n\n, the record for \"Q\" is held by the JET tokamak in the UK, at \"Q\" = (16 MW)/(24 MW) ≈ 0.67, first attained in 1997. ITER was originally designed to reach ignition, but is currently designed to reach \"Q\" = 10, producing 500 MW of fusion power from 50 MW of injected thermal power.\n\n\"Q\" is simply the comparison of the power being released by the fusion reactions in a reactor, \"P\", to the constant heating power being supplied, \"P\". However, there are several definitions of breakeven that consider additional power losses.\n\nIn 1955, John Lawson was the first to explore the energy balance mechanisms in detail, initially in classified works but published openly in a now-famous 1957 paper. In this paper he considered and refined work by earlier researchers, notably Hans Thirring, Peter Thonemann, and a review article by Richard Post. Expanding on all of these, Lawson's paper made detailed predictions for the amount of power that would be lost through various mechanisms, and compared that to the energy needed to sustain the reaction. This balance is today known as the Lawson criterion.\n\nIn a successful fusion reactor design, the fusion reactions generate an amount of power designated \"P\". Some amount of this energy, \"P\", is lost through a variety of mechanisms, mostly convection of the fuel to the walls of the reactor chamber and various forms of radiation that cannot be captured to generate power. In order to keep the reaction going, the system has to provide heating to make up for these losses, where \"P\" = \"P\" to maintain thermal equilibrium.\n\nThe most basic definition of breakeven is when \"Q\" = 1, that is, \"P\" = \"P\".\n\nSome works refer to this definition as scientific breakeven, to contrast it with similar terms. However, this usage is rare outside certain areas, specifically the inertial confinement fusion field, where the term is much more widely used.\n\nSince the 1950s, most commercial fusion reactor designs have been based on a mix of deuterium and tritium as their primary fuel; others fuels have been studied for a variety of reasons but are much harder to ignite. As tritium is radioactive, highly bioactive and highly mobile, it represents a significant safety concern and adds to the cost of designing and operating such a reactor.\n\nIn order to lower costs, many experimental machines are designed to run on test fuels of hydrogen or deuterium alone, leaving out the tritium. In this case, the term extrapolated breakeven is used to define the expected performance of the machine running on D-T fuel based on the performance when running on hydrogen or deuterium alone.\n\nThe records for extrapolated breakeven are slightly higher than the records for scientific breakeven. Both JET and JT-60 have reached values around 1.25 (see below for details) while running on D-D fuel. When running on D-T, only possible in JET, the maximum performance is about half the extrapolated value.\n\nAnother related term, engineering breakeven, considers the need to extract the energy from the reactor, turn that into electrical energy, and feed that back into the heating system. This closed loop is known as \"recirculation\". In this case, the basic definition changes by adding additional terms to the \"P\" side to consider the efficiencies of these processes.\n\nMost fusion reactions release energy in a variety of forms, mostly neutrons and a variety of charged particles like alpha particles. Neutrons are electrically neutral and will travel out of any magnetic confinement fusion (MFE) design, and in spite of the very high densities found in inertial confinement fusion (ICF) designs, they tend to easily escape the fuel mass in these designs as well. This means that only the charged particles from the reactions can be captured within the fuel mass and give rise to self-heating. If the fraction of the energy being released in the charged particles is \"f\", then the power in these particles is \"P\" = \"f\"\"P\". If this self-heating process is perfect, that is, all of \"P\" is captured in the fuel, that means the power available for generating electricity is the power that is not released in that form, or (1 − \"f\")\"P\".\n\nIn the case of neutrons carrying most of the practical energy, as is the case in the D-T fuel studied in most designs, this neutron energy is normally captured in a \"blanket\" of lithium that produces more tritium that is used to fuel the reactor. Due to various exothermic and endothermic reactions, the blanket may have a power gain factor a few percent higher or lower than 100%, but that will be neglected here. The blanket is then cooled and the cooling fluid used in a heat exchanger driving conventional steam turbines. These have an efficiency η which is around 35 to 40%.\n\nConsider a system that uses external heaters to heat the fusion fuel, then extracts the power from those reactions to generate electrical power. Some fraction of that power, \"f\", is needed to recirculate back into the heaters to close the loop. This is not the same as the \"P\" because the self-heating processes are providing some of the required energy. While the system as a whole requires additional power for building climate control, lighting, and the confinement system, these are generally much smaller than the plasma heating system requirements.\nConsidering all of these factors, the heating power can thus be related to the fusion power by the following equation:\n\nformula_1\n\nwhere formula_2 is the efficiency that power supplied to the heating systems is turned into heat in the fuel, as opposed to lost in the equipment itself, and formula_3 is the efficiency achieved when turning the heat into electrical power, for instance, through the Rankine cycle.\n\nThe fusion energy gain factor is then defined as:\n\nformula_4\n\nAs the temperature of the plasma increases, the rate of fusion reactions grows rapidly, and with it, the rate of self heating. In contrast, the non-capturable energy losses like x-rays do not grow at the same rate. Thus, in overall terms, the self-heating process becomes more efficient as the temperature increases, and less energy is needed from external sources to keep it hot.\n\nEventually \"P\" reaches zero, that is, all of the energy needed to keep the plasma at the operational temperature is being supplied by self-heating, and the amount of external energy that needs to be added drops to zero. This point is known as ignition.\n\nIgnition, by definition, corresponds to an infinite \"Q\", but it does not mean that \"f\" drops to zero as the other power sinks in the system, like the magnets and cooling systems, still need to be powered. Generally, however, these are much smaller than the energy in the heaters, and require a much smaller \"f\". More importantly, this number is more likely to be near constant, meaning that further improvements in plasma performance will result in more energy that can be directly used for commercial generation, as opposed to recirculation.\n\nThe final definition of breakeven is commercial breakeven, which occurs when the economic value of any net energy left over after recirculation is enough to finance the construction of the reactor. This value depends both on the reactor and the spot price of electrical power.\n\nCommercial breakeven relies on factors outside the technology of the reactor itself, and it is possible that even a reactor with a fully ignited plasma will not generate enough energy to pay for itself. Whether any of the mainline concepts like ITER can reach this goal is being debated in the field.\n\nMost fusion reactor designs being studied are based on the D-T reaction, as this is by far the easiest to ignite, and is energy dense. However, this reaction also gives off most of its energy in the form of a single highly energetic neutron, and only 20% of the energy in the form of an alpha. Thus, for the D-T reaction, \"f\" = 0.2. This means that self-heating does not become equal to the external heating until at least \"Q\" = 5. \n\nEfficiency values depend on design details but may be in the range of η = 0.7 (70%) and η = 0.4 (40%). The purpose of a fusion reactor is to produce power, not to recirculate it, so a practical reactor must have \"f\" = 0.2 approximately. Lower would be better but will be hard to achieve. Using these values we find for a practical reactor \"Q\" = 22.\n\nMany early fusion devices operated for microseconds, using some sort of pulsed power source to feed their magnetic confinement system and used the confinement as the heating source. Lawson defined breakeven in this context as the total energy released by the entire reaction cycle compared to the total energy supplied to the machine during the same cycle.\n\nOver time, as performance increased by orders of magnitude, the reaction times have extended from microseconds to seconds, and in ITER, on the order of minutes. In this case definition of \"the entire reaction cycle\" becomes blurred. In the case of an ignited plasma, for instance, P may be quite high while the system is being set up, and then drop to zero when it is fully developed, so one may be tempted to pick an instant in time when it is operating at its best to determine \"Q\". A better solution in these cases is to use the original Lawson definition averaged over the reaction to produce a similar value as the original definition.\n\nHowever, there is a complication. During the heating phase when the system is being brought up to operational conditions, some of the energy released by the fusion reactions will be used to heat the surrounding fuel, and thus not be released. This is no longer true when the plasma reaches its operational temperature and enters thermal equilibrium. Thus, if one averages over the entire cycle, this energy will be included as part of the heating term, that is, some of the energy that was captured for heating would otherwise have been released in P and is therefore not indicative of an operational \"Q\".\n\nOperators of the JET reactor argued that this input should be removed from the total:\n\nformula_5\n\nwhere:\n\nformula_6\n\nThat is, P is the amount of energy needed to raise the internal energy of the plasma. It is this definition that was used when reporting JET's record 0.67 value.\n\nSome debate over this definition continues. In 1998, the operators of the JT-60 claimed to have reached \"Q\" = 1.25 running on D-D fuel, thus reaching extrapolated breakeven. However, this measurement was based on the JET definition of Q*. Using this definition, JET had also reached extrapolated breakeven some time earlier. If one considers the energy balance in these conditions, and the analysis of previous machines, it is argued the original definition should be used, and thus both machines remain well below break-even of any sort.\n\nAlthough most fusion experiments use some form of magnetic confinement, another major branch is inertial confinement fusion (ICF) that mechanically presses together the fuel mass (the \"target\") to increase its density. This greatly increases the rate of fusion events and lowers the need to confine the fuel for long periods. This compression is accomplished by heating a lightweight capsule holding the fuel so rapidly that it explodes outwards, driving the fuel mass on the inside inward in accordance with Newton's third law. There are a variety of proposed \"drivers\" to cause the implosion process, but to date most experiments have used lasers. \n\nUsing the traditional definition of \"Q\", \"P\" / \"P\", ICF devices have extremely low \"Q\". This is because the laser is extremely inefficient; whereas formula_2 for the heaters used in magnetic systems might be on the order of 70%, lasers are on the order of 1.5%. For this reason, Lawrence Livermore National Laboratory (LLNL), the leader in ICF research, has proposed another modification of \"Q\" that defines \"P\" as the energy delivered by the driver, as opposed to the energy put into the driver. This definition produces much higher \"Q\" values, and changes the definition of breakeven to be \"P\" / \"P\" = 1. On occasion, they referred to this definition as \"scientific breakeven\". This term was not universally used, other groups adopted the redefinition of \"Q\" but continued to refer to \"P\" = \"P\" simply as breakeven. \n\nOn 7 October 2013, the BBC announced that LLNL had achieved scientific breakeven in the National Ignition Facility (NIF) on 29 September. In this experiment, \"P\" was approximately 14 kJ, while the laser output was 1.8 MJ. By their previous definition, this would be a \"Q\" of 0.0077. However, for this press release, they re-defined \"Q\" once again, this time equating \"P\" to be only the amount energy delivered to \"the hottest portion of the fuel\", calculating that only 10 kJ of the original laser energy reached the part of the fuel that was undergoing fusion reactions. This release has been heavily criticized in the field.\n\n"}
{"id": "23149866", "url": "https://en.wikipedia.org/wiki?curid=23149866", "title": "Gold universe", "text": "Gold universe\n\nA Gold universe is a cosmological model of the universe. In these models, the universe starts with a Big Bang and expands for some time, with increasing entropy and a thermodynamic arrow of time pointing in the direction of the expansion. After the universe reaches a low-density state, it recontracts, but entropy now decreases, pointing the thermodynamic arrow of time in the opposite direction, until the universe ends in a low-entropy, high-density Big Crunch. \nThere are two models of the universe which support the possibility of a reversed direction of time. The first begins with a state of low entropy at the Big Bang which continually increases until the Big Crunch. The second, a Gold Universe, posits that entropy will increase only until a moment of contraction, then gradually decrease. This latter model suggests the universe will become more orderly after the moment of contraction. The Gold model has been linked to the possibility of retrocausal change, questions concerning the preservation of information in a time-reversed universe (states of decreasing entropy), and causation in general. The Gold Universe is named after the cosmologist Thomas Gold, who proposed the model in the 1960s.\n"}
{"id": "180236", "url": "https://en.wikipedia.org/wiki?curid=180236", "title": "Greisen–Zatsepin–Kuzmin limit", "text": "Greisen–Zatsepin–Kuzmin limit\n\nThe Greisen–Zatsepin–Kuzmin limit (GZK limit) is a theoretical upper limit on the energy of cosmic ray protons traveling from other galaxies through the intergalactic medium to our galaxy. The limit is , or about 8 joules. The limit is set by slowing-interactions of the protons with the microwave background radiation over long distances (~160 million light-years). The limit is at the same order of magnitude as the upper limit for energy at which cosmic rays have experimentally been detected. For example, one extreme-energy cosmic ray has been detected which appeared to possess a record (50 joules) of energy (about the same as the kinetic energy of a 35 mph baseball).\n\nThe GZK limit is derived under the assumption that ultra-high energy cosmic rays are protons. Measurements by the largest cosmic-ray observatory, the Pierre Auger Observatory, suggest that most ultra-high energy cosmic rays are heavier elements. In this case, the argument behind the GZK limit does not apply in the originally simple form and there is no fundamental contradiction in observing cosmic rays with energies that violate the limit.\n\nIn the past, the apparent violation of the GZK limit has inspired cosmologists and theoretical physicists to suggest other ways that circumvent the limit. These theories propose that ultra-high energy cosmic rays are produced nearby our galaxy or that Lorentz covariance is violated in such a way that protons do not lose energy on their way to our galaxy.\n\nThe limit was independently computed in 1966 by Kenneth Greisen, Vadim Kuzmin, and Georgiy Zatsepin, based on interactions between cosmic rays and the photons of the cosmic microwave background radiation (CMB). They predicted that cosmic rays with energies over the threshold energy of would interact with cosmic microwave background photons formula_1, relatively blueshifted by the speed of the cosmic rays, to produce pions via the formula_2 resonance,\n\nor\n\nPions produced in this manner proceed to decay in the standard pion channels—ultimately to photons for neutral pions, and photons, positrons, and various neutrinos for positive pions. Neutrons decay also to similar products, so that ultimately the energy of any cosmic ray proton is drained off by production of high energy photons plus (in some cases) high energy electron/positron pairs and neutrino pairs.\n\nThe pion production process begins at a higher energy than ordinary electron-positron pair production (lepton production) from protons impacting the CMB, which starts at cosmic ray proton energies of only about . However, pion production events drain 20% of the energy of a cosmic ray proton as compared with only 0.1% of its energy for electron positron pair production. This factor of 200 is from two sources: the pion has only about ~130 times the mass of the leptons, but the extra energy appears as different kinetic energies of the pion or leptons, and results in relatively more kinetic energy transferred to a heavier product pion, in order to conserve momentum. The much larger total energy losses from pion production result in the pion production process becoming the limiting one to high energy cosmic ray travel, rather than the lower-energy light-lepton production process.\n\nThe pion production process continues until the cosmic ray energy falls below the pion production threshold. Due to the mean path associated with this interaction, extragalactic cosmic rays traveling over distances larger than () and with energies greater than this threshold should never be observed on Earth. This distance is also known as GZK horizon.\n\nA number of observations have been made by the largest cosmic ray experiments Akeno Giant Air Shower Array, High Resolution Fly's Eye Cosmic Ray Detector, the Pierre Auger Observatory and Telescope Array Project that appeared to show cosmic rays with energies above this limit (called extreme-energy cosmic rays, or EECRs). The observation of these particles was the so-called GZK paradox or cosmic ray paradox.\n\nThese observations appear to contradict the predictions of special relativity and particle physics as they are presently understood. However, there are a number of possible explanations for these observations that may resolve this inconsistency.\n\nAnother suggestion involves ultra-high energy weakly interacting particles (for instance, neutrinos) which might be created at great distances and later react locally to give rise to the particles observed. In the proposed Z-burst model, an ultra-high energy cosmic neutrino collides with a relic anti-neutrino in our galaxy and annihilates to hadrons. This process proceeds via a (virtual) Z-boson:\n\nformula_8\n\nThe cross section for this process becomes large if the center of mass energy of the neutrino antineutrino pair is equal to the Z-boson mass (such a peak in the cross section is called \"resonance\"). Assuming that the relic anti-neutrino is at rest, the energy of the incident cosmic neutrino has to be:\n\nformula_9\n\nwhere formula_10 is the mass of the Z-boson and formula_11 the mass of the neutrino.\n\nA number of exotic theories have been advanced to explain the AGASA observations, including doubly special relativity. However, it is now established that standard doubly special relativity does not predict any GZK suppression (or GZK cutoff), contrary to models of Lorentz symmetry violation involving an absolute rest frame. Other possible theories involve a relation with dark matter, decays of exotic super-heavy particles beyond those known in the Standard Model.\n\nA suppression of the cosmic ray flux which can be explained with the GZK limit has been confirmed by the latest generation of cosmic ray observatories. A former claim by the AGASA experiment that there is no suppression was overruled. It remains controversial, whether the suppression is due to the GZK effect. The GZK limit only applies if ultra-high energy cosmic rays are mostly protons.\n\nIn July 2007, during the 30th International Cosmic Ray Conference in Mérida, Yucatán, México, the High Resolution Fly's Eye Experiment (HiRes) and the Pierre Auger Observatory (Auger) presented their results on ultra-high-energy cosmic rays. HiRes observed a suppression in the UHECR spectrum at just the right energy, observing only 13 events with an energy above the threshold, while expecting 43 with no suppression. This was interpreted as the first observation of the GZK limit. Auger confirmed the flux suppression, but did not claim it to be the GZK limit: instead of the 30 events necessary to confirm the AGASA results, Auger saw only two, which are believed to be heavy nuclei events. The flux suppression was previously brought into question when the AGASA experiment found no suppression in their spectrum. According to Alan Watson, spokesperson for the Auger Collaboration, AGASA results have been shown to be incorrect, possibly due to the systematic shift in energy assignment.\n\nIn 2010 and the following years, both the Pierre Auger Observatory and HiRes confirmed again a flux suppression, in case of the Pierre Auger Observatory the effect is statistically significant at the level of 20 standard deviations.\n\nAfter the flux suppression was established, a heated debate ensued whether cosmic rays that violate the GZK limit are protons. The Pierre Auger Observatory, the world's largest observatory, found with high statistical significance that ultra-high energy cosmic rays are not purely protons, but a mixture of elements which is getting heavier with increasing energy.\nThe Telescope Array Project, a joint effort from members of the HiRes and AGASA collaborations, agrees with the former HiRes result that these cosmic rays look like protons. The claim is based on data with lower statistical significance, however. The area covered by Telescope Array is about one third of the area covered by the Pierre Auger Observatory, and the latter has been running for a longer time.\n\nThe controversy was partially resolved in 2017, when a joint working group formed by members of both experiments presented a report at the 35th International Cosmic Ray Conference. According to the report, the raw experimental results are not in contradiction with each other. The different interpretations are mainly based on the use of different theoretical models (Telescope Array uses an outdated model for its interpretation), and the fact that Telescope Array has not collected enough events yet to distinguish the pure proton hypothesis from the mixed-nuclei hypothesis.\n\nEUSO, which was scheduled to fly on the International Space Station (ISS) in 2009, was designed to use the atmospheric-fluorescence technique to monitor a huge area and boost the statistics of UHECRs considerably. EUSO is to make a deep survey of UHECR-induced extensive air showers (EASs) from space, extending the measured energy spectrum well beyond the GZK-cutoff. It is to search for the origin of UHECRs, determine the nature of the origin of UHECRs, make an all-sky survey of the arrival direction of UHECRs, and seek to open the astronomical window on the extreme-energy universe with neutrinos. The fate of the EUSO Observatory is still unclear since NASA is considering early retirement of the ISS.\n\nLaunched in June 2008, the Fermi Gamma-ray Space Telescope (formerly GLAST) will also provide data that will help resolve these inconsistencies.\n\nIn November 2007, researchers at the Pierre Auger Observatory announced that they had evidence that UHECRs appear to come from the active galactic nuclei (AGNs) of energetic galaxies powered by matter swirling onto a supermassive black hole. The cosmic rays were detected and traced back to the AGNs using the Véron-Cetty-Véron catalog. These results are reported in the journal \"Science\". Nevertheless, the strength of the correlation with AGNs from this particular catalog for the Auger data recorded after 2007 has been slowly diminishing.\n\n"}
{"id": "23721650", "url": "https://en.wikipedia.org/wiki?curid=23721650", "title": "Index of energy articles", "text": "Index of energy articles\n\nThis is an index of energy articles.\n\nActivation energy\n- Alternative energy\n- Alternative energy indexes\n- American Museum of Science and Energy (AMSE)\n- Anisotropy energy\n- Atomic energy\n\nBinding energy\n- Black hole\n- Breeder reactor\n- Brown energy\n\nCharacteristic energy\n- Conservation of energy\n- Consol Energy\n\nDark energy\n- Decay energy\n- Direct Energy\n- Dirichlet's energy\n- Dyson's sphere\n\nEcological energetics\n- Electric potential energy\n- Electrochemical energy conversion\n- Embodied energy\n- Encircled energy\n- Energy\n- Energy accidents\n- Energy accounting\n- Energy amplifier\n- Energy analyser\n- Energy applications of nanotechnology\n- Energy balance (biology)\n- Energy bar\n- Energy barrier\n- Energy being\n- Energy carrier\n- Energy Catalyzer\n- Energy cell\n- Energy charge\n- Energy conservation\n- Energy conversion efficiency\n- Energy crop\n- Energy current\n- Energy density\n- Energy-depth relationship in a rectangular channel\n- Energy development\n- Energy-dispersive X-ray spectroscopy\n- Energy distance\n- Energy drift\n- Energy drink\n- Energy efficiency gap\n- Energy-Efficient Ethernet\n- Energy-efficient landscaping\n- Energy elasticity\n- Energy engineering\n- Energy (esotericism)\n- Energy expenditure\n- Energy factor\n- Energy field disturbance\n- Energy filtered transmission electron microscopy\n- Energy transfer\n- Energy flow (ecology)\n- Energy flux\n- Energy forestry\n- Energy functional\n- Energy gel\n- Energy harvesting\n- Energy input labeling\n- Energy landscape\n- Energy level\n- Energy level splitting\n- Energy management software\n- Energy management system\n- Energy–maneuverability theory\n- Energy Manufacturing Co. Inc\n- Energy medicine\n- Energy–momentum relation\n- Energy monitoring and targeting\n- Energy Probe\n- Energy profile (chemistry)\n- Energy quality\n- Energy recovery ventilation\n- Energy security\n- Energy (signal processing)\n- Energy Slave\n- Energy Star\n- Energy statistics\n- Energy Storage Challenge\n- Energy storage\n- Energy system\n- Energy technology\n- Energy tower (downdraft)\n- Energy transfer\n- Energy transfer upconversion\n- Energy transformation\n- Energy value of coal\n- Energy vortex (stargate)\n- Enthalpy\n- Entropy\n- Equipartition theorem\n- E-statistic\n- Exertion\n\nFermi energy\n- Forms of energy\n- Fuel\n- Fusion power\n\nGeothermal energy\n- Gravitational energy\n- Gravitational potential\n\nHistory of energy\n- Hydroelectricity\n\nInteraction energy\n- Intermittent energy source\n- Internal energy\n- Invariant mass\n\nJosephson energy\n\nKinetic energy\n\nLatent heat\n\nMagnetic confinement fusion \n- Marine energy\n- Mass–energy equivalence\n- Mechanical energy\n- Möbius energy\n\nNegative energy\n- Nuclear fusion\n- Nuclear power\n- Nuclear reactor\n\nOrders of magnitude (energy)\n- Osmotic power\n\nPhotosynthesis\n- Potential energy\n- Power (physics)\n- Primary energy\n\nQi\n- Quasar\n\nRelativistic jet\n- Renewable energy - Rotational energy\n\nSeismic scale\n- Solar energy\n- Solar thermal energy\n- Sound energy\n- Specific energy\n- Specific kinetic energy\n- Specific orbital energy\n- Surface energy\n\nThermodynamic free energy\n- Threshold energy\n- Tidal power\n- Turbulence kinetic energy\n\nUnits of energy\n- Universe of Energy\n\nVacuum energy\n\nWork (physics)\n- World energy resources and consumption\n- World Forum on Energy Regulation\n\nZero-energy building\n- Zero-energy universe\n- Zero-point energy\n\n"}
{"id": "40111102", "url": "https://en.wikipedia.org/wiki?curid=40111102", "title": "Intelligent Energy", "text": "Intelligent Energy\n\nIntelligent Energy is a fuel cell engineering company focused on the development and commercialisation of its PEM fuel cell technologies for a range of markets including automotive, stationary power and UAVs. We are headquartered in the UK, with offices and representation in the US, Japan, India, and China.\n\nThe origins of Intelligent Energy began at Loughborough University in the UK during the late 1980s, when the University became one of Europe’s first research and development centres for proton exchange membrane (PEM) fuel cell technology. In 1995, the UK’s first kW-level PEM fuel cell stack was produced by the R&D team. In June of that year, Advanced Power Sources (APS) Ltd was founded as a spin-out from Loughborough University by Paul Adcock, Phil Mitchell, Jon Moore and Anthony Newbold, and was the first company in the UK formed specifically to address the development and commercialisation of PEM fuel cells.\n\nFounded by Harry Bradbury, Intelligent Energy was established in 2001, acquiring Advanced Power Sources Ltd, together with its personnel and fuel cell related intellectual property that originated from research conducted by both APS and Loughborough University into PEM fuel cell technology. This triggered investment and enabled the company to grow its business activities.\nIn March 2005, it launched the ENV, the world’s first purpose-built fuel cell motorbike which gained the company recognition as a Technology Pioneer by the World Economic Forum in 2006. The ENV incorporated the company’s air-cooled fuel cell technology hybridised with a battery pack to provide 6 kW peak load to the motor to improve performance during spikes in power demand i.e. acceleration.\n\nIn 2007, a partnership was announced with Suzuki Motor Corporation to develop hydrogen fuel cells for a range of vehicles. In 2008, Intelligent Energy established the company, IE-CHP in a joint venture with SSE plc, to develop fuel cells and other technologies for CHP (Combined Heat and Power) applications. In the same year, Intelligent Energy also produced the power system for the first fuel cell powered manned flight in conjunction with Boeing.\nIn 2010, its fuel-cell taxi received The Engineer Technology and Innovation Award.\n\nIn March 2011, the Suzuki Burgman fuel cell scooter, equipped with Intelligent Energy’s fuel cell system, became the first fuel cell vehicle to achieve European Whole Vehicle Type Approval.\n\nIn 2012, SMILE FC System Corporation, a joint venture between Intelligent Energy and Suzuki Motor Corporation, was established to develop and manufacture air-cooled fuel cell systems for the automotive and a range of industry sectors.\nDuring the same year, a fleet of fuel cell taxis incorporating Intelligent Energy’s technology was used during the 2012 London Olympics. Part of the European Union-funded HyTEC (Hydrogen Technologies in European Cities) project launched in 2011, the taxis were used to transport VIP guests of the Mayor of London around the city.\nIn 2013, SMILE FC Corporation announced that it had established a ready-to-scale production line for its fuel cell systems, utilising Intelligent Energy’s semi-automated production technology. IE-CHP also received CE certification for its first-generation product, a 10 kWe/12 kWth combined heat and power (CHP) fuel cell. The certification allows the product to be sold in the European Economic Area, confirming that the product satisfies all the EU regulatory and conformity assessment procedures covering the design, manufacture, and testing of the system.\n\nIntelligent Energy was acquired by Meditor Energy, part of the Meditor Group, in October 2017.\n\nIntelligent Energy's fuel-cell technology is divided into two platforms: air-cooled (AC) and evaporatively-cooled (EC). The air-cooled fuel cell systems use low-power fans to provide cooling and the oxidant supply for operation. Heat from the fuel cell stack is conducted to cooling plates and removed through airflow channels, a simplified and cost-effective system for the power range from a few watts to several kilowatts. They are used in a wide range of UAV, stationary power and automotive applications for two-wheel and small car range extender applications.\n\nEvaporatively-cooled (EC) fuel cell systems provide power generation from a few kilowatts up to 200 kW. Efficient thermal management of the EC fuel cell stack reduces system complexity, mass and cost. These systems are designed for high-volume, low-cost manufacturing, and use modular architecture that can be quickly modified to suit the application.\n\nThe firm's fuel cell stacks have been developed for small and large cars, scooters and motorbikes. \nIn 2010, the company was involved in the development of the report entitled “A portfolio of power-trains for Europe: a fact-based analysis. The role of Battery Electric Vehicles, Plug-In Hybrids and Fuel Cell Electric Vehicles”, produced by McKinsey & Company with input from car manufacturers, oil and gas suppliers, utilities and industrial gas companies, wind turbine and electrolyser companies as well as governmental and non-governmental organisations. The report concluded, amongst other findings, that fuel cell vehicles are technology ready, and cost competitive, and that decarbonisation targets for Europe are unlikely to be met without the introduction of fuel cell powertrains.\n\nThe firm provides fuel cells to power UAVs and aerial drones. Its UAV Fuel Cell Modules run on hydrogen and ambient air to produce DC power in a lightweight package providing extended flight times when compared to battery systems.\n\nThe company’s fuel cell systems are used to provide diesel replacement and backup power initially for telecom towers but also for other sectors. The company has field proven its fuel cell products in the Indian telecommunications market with a tower uptime of close to 100%.\n\nThe company is a founding member of UKH Mobility, a government and industry group aiming to accelerate the commercial roll out of hydrogen vehicles in 2014/15;\nIt is also a member of the Fuel Cell and Hydrogen Energy Association (FCHEA), the US-based trade association for the fuel cell and hydrogen energy industry, dedicated to the commercialisation of fuel cells and hydrogen energy technologies.\n\n"}
{"id": "3591456", "url": "https://en.wikipedia.org/wiki?curid=3591456", "title": "Interface (matter)", "text": "Interface (matter)\n\nIn the physical sciences, an interface is the boundary between two spatial regions occupied by different matter, or by matter in different physical states. The interface between matter and air, or matter and vacuum, is called a surface, and studied in surface science. In thermal equilibrium, the regions in contact are called phases, and the interface is called a phase boundary. An example for an interface out of equilibrium is the grain boundary in polycrystalline matter.\n\nThe importance of the interface depends on the type of system: the bigger the quotient area/volume, the greater the effect the interface will have. Consequently, interfaces are very important in systems with large interface area-to-volume ratios, such as colloids.\n\nInterfaces can be flat or curved. For example, oil droplets in a salad dressing are spherical but the interface between water and air in a glass of water is mostly flat.\n\nSurface tension is the physical property which rules interface processes involving liquids. For a liquid film on flat surfaces, the liquid-vapor interface keeps flat to minimize interfacial area and system free energy. For a liquid film on rough surfaces, the surface tension tends to keep the meniscus flat, while the disjoining pressure makes the film conformal to the substrate. The equilibrium meniscus shape is a result of the competition between the capillary pressure and disjoining pressure.\n\nInterfaces may cause various optical phenomena, such as refraction. Optical lenses serve as an example of a practical application of the interface between glass and air.\n\nOne topical interface system is the gas-liquid interface between aerosols and other atmospheric molecules.\n\n"}
{"id": "13680406", "url": "https://en.wikipedia.org/wiki?curid=13680406", "title": "Ion vibration current", "text": "Ion vibration current\n\nThe ion vibration current (IVI) and the associated ion vibration potential is an electric signal that arises when an acoustic wave propagates through a homogeneous fluid.\n\nHistorically, the IVI was the first known electroacoustic phenomenon. It was predicted by Peter Debye in 1933.\n\nWhen a longitudinal sound wave travels through a solvent, the associated pressure gradients push the fluid particles back and forth, and it is easy in practice to create such accelerations that measure thousands or millions of g's. If a solute molecule is more dense or less dense than the surrounding liquid, then in this accelerating environment, the molecule will move relative to the surrounding liquid. This relative motion is essentially the same phenomenon that occurs in a centrifuge, or more simply, it is essentially the same phenomenon that occurs when low-density objects float to the top of a glass of water, and high-density particles sink to the bottom (see the equivalence principle, which states that gravity is just like any other acceleration). The amount of relative motion depends on the balance between the molecule's effective mass (which includes both the mass of the molecule itself and any solvent molecules that are so tightly bound to the molecule that they follow along with the molecule's motion), its effective volume (related to buoyant force), and the viscous drag (friction) between the molecule and the surrounding fluid.\n\nIVI concerns the case where the particles in question are anions and cations. In general, they will have different amounts of motion relative to the fluid during the sound wave oscillations, and that discrepancy creates an alternating electric potential between various points in a sound wave.\n\nThis effect was extensively used in the 1950s and 1960s for characterizing ion solvation. These works are mostly associated with the names of Zana and Yaeger, who published a review of their studies in 1982.\n"}
{"id": "985963", "url": "https://en.wikipedia.org/wiki?curid=985963", "title": "Lambda-CDM model", "text": "Lambda-CDM model\n\nThe ΛCDM (Lambda cold dark matter) or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant, denoted by Lambda (Greek Λ), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology because it is the simplest model that provides a reasonably good account of the following properties of the cosmos:\n\nThe model assumes that general relativity is the correct theory of gravity on cosmological scales.\nIt emerged in the late 1990s as a concordance cosmology, after a period of time when disparate observed properties of the universe appeared mutually inconsistent, and there was no consensus on the makeup of the energy density of the universe.\n\nThe ΛCDM model can be extended by adding cosmological inflation, quintessence and other elements that are current areas of speculation and research in cosmology.\n\nSome alternative models challenge the assumptions of the ΛCDM model. Examples of these are modified Newtonian dynamics, modified gravity, theories of large-scale variations in the matter density of the universe, and scale invariance of empty space.\n\nMost modern cosmological models are based on the cosmological principle, which states that our observational location in the universe is not unusual or special; on a large-enough scale, the universe looks the same in all directions (isotropy) and from every location (homogeneity).\n\nThe model includes an expansion of metric space that is well documented both as the red shift of prominent spectral absorption or emission lines in the light from distant galaxies and as the time dilation in the light decay of supernova luminosity curves. Both effects are attributed to a Doppler shift in electromagnetic radiation as it travels across expanding space. Although this expansion increases the distance between objects that are not under shared gravitational influence, it does not increase the size of the objects (e.g. galaxies) in space. It also allows for distant galaxies to recede from each other at speeds greater than the speed of light; local expansion is less than the speed of light, but expansion summed across great distances can collectively exceed the speed of light.\n\nThe letter formula_1 (lambda) represents the cosmological constant, which is currently associated with a vacuum energy or dark energy in empty space that is used to explain the contemporary accelerating expansion of space against the attractive effects of gravity. A cosmological constant has negative pressure, formula_2, which contributes to the stress-energy tensor that, according to the general theory of relativity, causes accelerating expansion. The fraction of the total energy density of our (flat or almost flat) universe that is dark energy, formula_3, is currently (2015) estimated to be 0.692 ± 0.012, or even 0.6911 ± 0.0062 based on Planck satellite data.\n\nDark matter is postulated in order to account for gravitational effects observed in very large-scale structures (the \"flat\" rotation curves of galaxies; the gravitational lensing of light by galaxy clusters; and enhanced clustering of galaxies) that cannot be accounted for by the quantity of observed matter. Cold dark matter is \"non-baryonic\", i.e. it consists of matter other than protons and neutrons (and electrons, by convention, although electrons are not baryons); \"cold\", i.e. its velocity is far less than the speed of light at the epoch of radiation-matter equality (thus neutrinos are excluded, being non-baryonic but not cold); \"dissipationless\", i.e. it cannot cool by radiating photons; and \"collisionless\", i.e. the dark matter particles interact with each other and other particles only through gravity and possibly the weak force. The dark matter component is currently (2013) estimated to constitute about 26.8% of the mass-energy density of the universe.\n\nThe remaining 4.9% (2013) comprises all ordinary matter observed as atoms, chemical elements, gas and plasma, the stuff of which visible planets, stars and galaxies are made. The great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10% of the ordinary matter contribution to the mass-energy density of the universe.\n\nAlso, the energy density includes a very small fraction (~ 0.01%) in cosmic microwave background radiation, and not more than 0.5% in relic neutrinos. Although very small today, these were much more important in the distant past, dominating the matter at redshift > 3200.\n\nThe model includes a single originating event, the \"Big Bang\", which was not an explosion but the abrupt appearance of expanding space-time containing radiation at temperatures of around 10 K. This was immediately (within 10 seconds) followed by an exponential expansion of space by a scale multiplier of 10 or more, known as cosmic inflation. The early universe remained hot (above 10,000 K) for several hundred thousand years, a state that is detectable as a residual cosmic microwave background, or CMB, a very low energy radiation emanating from all parts of the sky. The \"Big Bang\" scenario, with cosmic inflation and standard particle physics, is the only current cosmological model consistent with the observed continuing expansion of space, the observed distribution of lighter elements in the universe (hydrogen, helium, and lithium), and the spatial texture of minute irregularities (anisotropies) in the CMB radiation. Cosmic inflation also addresses the \"horizon problem\" in the CMB; indeed, it seems likely that the universe is larger than the observable particle horizon.\n\nThe model uses the Friedmann–Lemaître–Robertson–Walker metric, the Friedmann equations and the cosmological equations of state to describe the observable universe from right after the inflationary epoch to present and future.\n\nThe expansion of the universe is parametrized by a dimensionless scale factor formula_4 (with time formula_5 counted from the birth of the universe), defined relative to the present day, so formula_6; the usual convention in cosmology is that subscript 0 denotes present-day values, so formula_7 is the current age of the universe. The scale factor is related to the observed redshift formula_8 of the light emitted at time formula_9 by\n\nThe expansion rate is described by the time-dependent Hubble parameter, formula_11, defined as\nwhere formula_13 is the time-derivative of the scale factor. The first Friedmann equation gives the expansion rate in terms of the matter+radiation density the curvature and the cosmological constant \n\nwhere as usual is the speed of light and is the gravitational constant. \nA critical density formula_15 is the present-day density, which gives zero curvature formula_16, assuming the cosmological constant formula_1 is zero, regardless of its actual value. Substituting these conditions to the Friedmann equation gives\n\nwhere formula_19 is the reduced Hubble constant.\nIf the cosmological constant were actually zero, the critical density would also mark the dividing line between eventual recollapse of the universe to a Big Crunch, or unlimited expansion. For the Lambda-CDM model with a positive cosmological constant (as observed), the universe is predicted to expand forever regardless of whether the total density is slightly above or below the critical density; though other outcomes are possible in extended models where the dark energy is not constant but actually time-dependent.\n\nIt is standard to define the present-day density parameter formula_20 for various species as the dimensionless ratio\nwhere the subscript formula_22 is one of formula_23 for baryons, formula_24 for cold dark matter, formula_25 for radiation (photons plus relativistic neutrinos), and formula_26 or formula_1 for dark energy.\n\nSince the densities of various species scale as different powers of formula_28, e.g. formula_29 for matter etc.,\nthe Friedmann equation can be conveniently rewritten in terms of the various density parameters as\nwhere w is the equation of state of dark energy, and assuming negligible neutrino mass (significant neutrino mass requires a more complex equation). The various formula_31 parameters add up to formula_32 by construction.\nIn the general case this is integrated by computer to give\nthe expansion history formula_33 and also observable distance-redshift relations for any chosen values of the cosmological parameters, which can then be compared with observations such as supernovae and baryon acoustic oscillations.\n\nIn the minimal 6-parameter Lambda-CDM model, it is assumed that curvature formula_34 is zero and formula_35, so this simplifies to\n\nObservations show that the radiation density is very small today, formula_37; if this term is neglected\nthe above has an analytic solution\nwhere formula_39\nthis is fairly accurate for formula_40 or formula_41million years.\nSolving for formula_42 gives the present age of the universe formula_43 in terms of the other parameters.\n\nIt follows that the transition from decelerating to accelerating expansion (the second derivative formula_44 crossing zero) occurred when\n\nwhich evaluates to formula_46 or formula_47 for the best-fit parameters estimated from the Planck spacecraft.\n\nThe discovery of the Cosmic Microwave Background (CMB) in 1964 confirmed a key prediction of the Big Bang cosmology. From that point on, it was generally accepted that the universe started in a hot, dense state and has been expanding over time. The rate of expansion depends on the types of matter and energy present in the universe, and in particular, whether the total density is above or below the so-called critical density. During the 1970s, most attention focused on pure-baryonic models, but there were serious challenges explaining the formation of galaxies, given the small anisotropies in the CMB (upper limits at that time). In the early 1980s, it was realized that this could be resolved if cold dark matter dominated over the baryons, and the theory of cosmic inflation motivated models with critical density. During the 1980s, most research focused on cold dark matter with critical density in matter, around 95% CDM and 5% baryons: these showed success at forming galaxies and clusters of galaxies, but problems remained; notably, the model required a Hubble constant lower than preferred by observations, and observations around 1988-1990 showed more large-scale galaxy clustering than predicted. These difficulties sharpened with the discovery of CMB anisotropy by COBE in 1992, and several modified CDM models, including ΛCDM and mixed cold and hot dark matter, came under active consideration through the mid-1990s. The ΛCDM model then became the leading model following the observations of accelerating expansion in 1998, and was quickly supported by other observations: in 2000, the BOOMERanG microwave background experiment measured the total (matter–energy) density to be close to 100% of critical, whereas in 2001 the 2dFGRS galaxy redshift survey measured the matter density to be near 25%; the large difference between these values supports a positive Λ or dark energy. Much more precise spacecraft measurements of the microwave background from WMAP in 2003 – 2010 and Planck in 2013 - 2015 have continued to support the model and pin down the parameter values, most of which are now constrained below 1 percent uncertainty.\n\nThere is currently active research into many aspects of the ΛCDM model, both to refine the parameters and possibly detect deviations. In addition, ΛCDM has no explicit physical theory for the origin or physical nature of dark matter or dark energy; the nearly scale-invariant spectrum of the CMB perturbations, and their image across the celestial sphere, are believed to result from very small thermal and acoustic irregularities at the point of recombination. A large majority of astronomers and astrophysicists support the ΛCDM model or close relatives of it, but Milgrom, McGaugh, and Kroupa are leading critics, attacking the dark matter portions of the theory from the perspective of galaxy formation models and supporting the alternative MOND theory, which requires a modification of the Einstein field equations and the Friedmann equations as seen in proposals such as MOG theory or TeVeS theory. Other proposals by theoretical astrophysicists of cosmological alternatives to Einstein's general relativity that attempt to account for dark energy or dark matter include f(R) gravity, scalar–tensor theories such as galileon theories, brane cosmologies, the DGP model, and massive gravity and its extensions such as bimetric gravity.\n\nIn addition to explaining pre-2000 observations,\nthe model has made a number of successful predictions: notably the existence of the\nbaryon acoustic oscillation feature, discovered in 2005 in the predicted location; and the statistics of weak gravitational lensing, first observed in 2000 by several teams. The polarization of the CMB, discovered in 2002 by DASI is now a dramatic success: in the 2015 Planck data release, there are seven observed peaks in the temperature (TT) power spectrum, six peaks in the temperature-polarization (TE) cross spectrum, and five peaks in the polarization (EE) spectrum. The six free parameters can be well constrained by the TT spectrum alone, and then the TE and EE spectra can be predicted theoretically to few-percent precision with no further adjustments allowed: comparison of theory and observations shows an excellent match.\n\nExtensive searches for dark matter particles have so far shown no well-agreed detection;\nthe dark energy may be almost impossible to detect in a laboratory, and its value is unnaturally small compared to naive theoretical predictions.\n\nComparison of the model with observations is very successful on large scales (larger than galaxies, up to the observable horizon), but may have some problems on sub-galaxy scales, possibly predicting too many dwarf galaxies and too much dark matter in the innermost regions of galaxies. This problem is called the \"small scale crisis\". These small scales are harder to resolve in computer simulations, so it is not yet clear whether the problem is the simulations, non-standard properties of dark matter, or a more radical error in the model.\n\nIt has been argued that the ΛCDM model is built upon a foundation of conventionalist stratagems, rendering it unfalsifiable in the sense defined by Karl Popper.\n\nThe simple ΛCDM model is based on six parameters: physical baryon density parameter; physical dark matter density parameter; the age of the universe; scalar spectral index; curvature fluctuation amplitude; and reionization optical depth. In accordance with Occam's razor, six is the smallest number of parameters needed to give an acceptable fit to current observations; other possible parameters are fixed at \"natural\" values, e.g. total density parameter = 1.00, dark energy equation of state = −1. (See below for extended models that allow these to vary.)\n\nThe values of these six parameters are mostly not predicted by current theory (though, ideally, they may be related by a future \"Theory of Everything\"), except that most versions of cosmic inflation predict the scalar spectral index should be slightly smaller than 1, consistent with the estimated value 0.96. The parameter values, and uncertainties, are estimated using large computer searches to locate the region of parameter space providing an acceptable match to cosmological observations. From these six parameters, the other model values, such as the Hubble constant and the dark energy density, can be readily calculated.\n\nCommonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature. Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less precisely measured at present.\n\nParameter values listed below are from the Planck Collaboration Cosmological parameters 68% confidence limits for the base ΛCDM model from Planck CMB power spectra, in combination with lensing reconstruction and external data (BAO + JLA + H). See also Planck (spacecraft).\n\nMassimo Persic and Paolo Salucci firstly estimated the baryonic density today present in ellipticals, spirals, groups and clusters of galaxies.\nThey performed an integration of the baryonic mass-to-light ratio over luminosity (in the following formula_48), weighted with the luminosity function formula_49 over the previously mentioned classes of astrophysical objects: \n\nThe result was:\n\nwhere formula_52.\n\nNote that this value is much lower than the prediction of standard cosmic nucleosynthesis formula_53, so that stars and gas in galaxies and in galaxy groups and clusters account for less than 10% of the primordially synthesized baryons. This issue is known as the problem of the \"missing baryons\".\n\nExtended models allow one or more of the \"fixed\" parameters above to vary, in addition to the basic six; so these models join smoothly to the basic six-parameter model in the limit that the additional parameter(s) approach the default values. For example, possible extensions of the simplest ΛCDM model allow for spatial curvature (formula_54 may be different from 1); or quintessence rather than a cosmological constant where the equation of state of dark energy is allowed to differ from −1. Cosmic inflation predicts tensor fluctuations (gravitational waves). Their amplitude is parameterized by the tensor-to-scalar ratio (denoted formula_55), which is determined by the unknown energy scale of inflation. Other modifications allow hot dark matter in the form of neutrinos more massive than the minimal value, or a running spectral index; the latter is generally not favoured by simple cosmic inflation models.\n\nAllowing additional variable parameter(s) will generally \"increase\" the uncertainties in the standard six parameters quoted above, and may also shift the central values slightly. The Table below shows results for each of the possible \"6+1\" scenarios with one additional variable parameter; this indicates that, as of 2015, there is no convincing evidence that any additional parameter is different from its default value.\n\nSome researchers have suggested that there is a running spectral index, but no statistically significant study has revealed one. Theoretical expectations suggest that the tensor-to-scalar ratio formula_55 should be between 0 and 0.3, and the latest results are now within those limits.\n\n\n"}
{"id": "9332507", "url": "https://en.wikipedia.org/wiki?curid=9332507", "title": "Leibniz–Clarke correspondence", "text": "Leibniz–Clarke correspondence\n\nThe Leibniz–Clarke correspondence was a scientific, theological and philosophical debate conducted in an exchange of letters between the German thinker Gottfried Wilhelm Leibniz and Samuel Clarke, an English supporter of Isaac Newton during the years 1715 and 1716. The exchange began because of a letter Leibniz wrote to Caroline of Ansbach, in which he remarked that Newtonian physics was detrimental to natural theology. Eager to defend the Newtonian view, Clarke responded, and the correspondence continued until the death of Leibniz in 1716.\n\nAlthough a variety of subjects is touched on in the letters, the main interest for modern readers is in the dispute between the absolute theory of space favoured by Newton and Clarke, and Leibniz's relational approach. Also important is the conflict between Clarke's and Leibniz's opinions on free will and whether God must create the best of all possible worlds.\n\nLeibniz had published only a book on moral matters, the \"Theodicée\" (1710), and his more metaphysical views had never been exposed to a sufficient extent, so the collected letters were met with interest by their contemporaries. The priority dispute between Leibniz and Newton about the calculus was still fresh in the public's mind and it was taken as a matter of course that it was Newton himself who stood behind Clarke's replies.\n\nThe Leibniz-Clarke letters were first published under Clarke's name in the year following Leibniz' death. He wrote a preface, took care of the translation from French, added notes and some of his own writing. In 1720 Pierre Desmaizeaux published a similar volume in a French translation, including quotes from Newton's work. It is quite certain that for both editions the opinion of Newton himself has been sought and Leibniz left at a disadvantage. However the German translation of the correspondence published by Kohler, also in 1720, contained a reply to Clarke's last letter which Leibniz had not been able to answer. The letters have been reprinted in most collections of Leibniz' works and regularly published in stand alone editions.\n\n\n\n"}
{"id": "14997569", "url": "https://en.wikipedia.org/wiki?curid=14997569", "title": "Location of Earth", "text": "Location of Earth\n\nKnowledge of the location of Earth has been shaped by 400 years of telescopic observations, and has expanded radically in the last century. Initially, Earth was believed to be the center of the Universe, \nwhich consisted only of those planets visible with the naked eye and an outlying sphere of fixed stars. After the acceptance of the heliocentric model in the 17th century, observations by William Herschel and others showed that the Sun lay within a vast, disc-shaped galaxy of stars. By the 20th century, observations of spiral nebulae revealed that our galaxy was one of billions in an expanding universe, grouped into clusters and superclusters. By the end of the 20th century, the overall structure of the visible universe was becoming clearer, with superclusters forming into a vast web of filaments and voids. Superclusters, filaments and voids are the largest coherent structures in the Universe that we can observe. At still larger scales (over 1000 megaparsecs) the Universe becomes homogeneous meaning that all its parts have on average the same density, composition and structure.\n\nSince there is believed to be no \"center\" or \"edge\" of the Universe, there is no particular reference point with which to plot the overall location of the Earth in the universe. Because the observable universe is defined as that region of the Universe visible to terrestrial observers, Earth is, by definition, the center of Earth's observable universe. Reference can be made to the Earth's position with respect to specific structures, which exist at various scales. It is still undetermined whether the Universe is infinite. There have been numerous hypotheses that our universe may be only one such example within a higher multiverse; however, no direct evidence of any sort of multiverse has ever been observed, and some have argued that the hypothesis is not falsifiable.\n\n"}
{"id": "58439642", "url": "https://en.wikipedia.org/wiki?curid=58439642", "title": "Margham", "text": "Margham\n\nMargham is an oil and gas field in Dubai, United Arab Emirates (UAE) and the largest onshore gas field in the emirate. The field is managed by Dusup - the Dubai Supply Authority. Condensate production ran at some 25,000 barrels per day in 2010. Margham also has an oil production capability.\n\nProduction at Margham commenced in 1984, with three major gas-bearing formations located up to 10,000 feet below sea level. The field is connected by pipeline to Jebel Ali, where the gas condensate is loaded onto tankers for export. Dry gas is now also sent by pipeline to supply the Dubai grid, with consumption increasing since 2015.\n\nMargham was initially developed as a liquids stripping/gas recycling project (dry gas was pumped back into the reservoir), but now operates as a gas storage facility for Dubai since 2008, allowing Dubai to depend on gas produced from Margham for its elecricity generation and desalination needs. This usage, together with sustainables such as DEWA's Mohammed bin Rashid Al Maktoum Solar Park, means that Dubai has eliminated the use of oil as a domestic energy fuel.\n\nAlthough it is a major producer with ambitions to develop its trading activities to become a major global LNG hub, the UAE is actually a net importer of LNG.\n"}
{"id": "558685", "url": "https://en.wikipedia.org/wiki?curid=558685", "title": "Natural environment", "text": "Natural environment\n"}
{"id": "208999", "url": "https://en.wikipedia.org/wiki?curid=208999", "title": "Non-renewable resource", "text": "Non-renewable resource\n\nA non-renewable resource (also called a finite resource) is a resource that does not renew itself at a sufficient rate for sustainable economic extraction in meaningful human time-frames. An example is carbon-based, organically-derived fuel. The original organic material, with the aid of heat and pressure, becomes a fuel such as oil or gas. Earth minerals and metal ores, fossil fuels (coal, petroleum, natural gas) and groundwater in certain aquifers are all considered non-renewable resources, though individual elements are almost always conserved.\n\nOn the other hand, resources such as timber (when harvested sustainably) and wind (used to power energy conversion systems) are considered renewable resources, largely because their localized replenishment can occur within time frames meaningful to humans.\n\nEarth minerals and metal ores are examples of non-renewable resources. The metals themselves are present in vast amounts in Earth's crust, and their extraction by humans only occurs where they are concentrated by natural geological processes (such as heat, pressure, organic activity, weathering and other processes) enough to become economically viable to extract. These processes generally take from tens of thousands to millions of years, through plate tectonics, tectonic subsidence and crustal recycling.\n\nThe localized deposits of metal ores near the surface which can be extracted economically by humans are non-renewable in human time-frames. There are certain rare earth minerals and elements that are more scarce and exhaustible than others. These are in high demand in manufacturing, particularly for the electronics industry.\n\nMost metal ores are considered vastly greater in supply to fossil fuels, because metal ores are formed by crustal-scale processes which make up a much larger portion of the Earth's near-surface environment, than those that form fossil fuels which are limited to areas where carbon-based life forms flourish, die, and are quickly buried.\n\nNatural resources such as coal, petroleum (crude oil) and natural gas take thousands of years to form naturally and cannot be replaced as fast as they are being consumed. Eventually it is considered that fossil-based resources will become too costly to harvest and humanity will need to shift its reliance to other sources of energy such as solar or wind power, see renewable energy.\n\nAn alternative hypothesis is that carbon based fuel is virtually inexhaustible in human terms, if one includes all sources of carbon-based energy such as methane hydrates on the sea floor, which are vastly greater than all other carbon based fossil fuel resources combined. These sources of carbon are also considered non-renewable, although their rate of formation/replenishment on the sea floor is not known. However their extraction at economically viable costs and rates has yet to be determined.\n\nAt present, the main energy source used by humans is non-renewable fossil fuels. Since the dawn of internal combustion engine technologies in the 19th century, petroleum and other fossil fuels have remained in continual demand. As a result, conventional infrastructure and transport systems, which are fitted to combustion engines, remain prominent throughout the globe. The continual use of fossil fuels at the current rate is believed to increase global warming and cause more severe climate change.\n\nIn 1987, the World Commission on Environment and Development (WCED) an organization set up by but independent from the United Nations classified fission reactors that produce more fissile nuclear fuel than they consume -i.e. breeder reactors, and when it is developed, fusion power, among conventional renewable energy sources, such as solar and falling water. The American Petroleum Institute likewise does not consider conventional nuclear fission as renewable, but that breeder reactor nuclear power fuel is considered renewable and sustainable, before explaining that radioactive waste from used spent fuel rods remains radioactive, and so has to be very carefully stored for up to a thousand years. With the careful monitoring of radioactive waste products also being required upon the use of other renewable energy sources, such as geothermal energy.\n\nThe use of nuclear technology relying on fission requires Naturally occurring radioactive material as fuel. Uranium, the most common fission fuel, and is present in the ground at relatively low concentrations and mined in 19 countries. This mined uranium is used to fuel energy-generating nuclear reactors with fissionable uranium-235 which generates heat that is ultimately used to power turbines to generate electricity.\n\nAs of 2013 only a few kilograms (picture available) of uranium have been extracted from the ocean in pilot programs and it is also believed that the uranium extracted on an industrial scale from the seawater would constantly be replenished from uranium leached from the ocean floor, maintaining the seawater concentration at a stable level. In 2014, with the advances made in the efficiency of seawater uranium extraction, a paper in the journal of \"Marine Science & Engineering\" suggests that with, light water reactors as its target, the process would be economically competitive if implemented on a large scale.\n\nNuclear power provides about 6% of the world's energy and 13–14% of the world's electricity. Nuclear energy production is associated with potentially dangerous radioactive contamination as it relies upon unstable elements. In particular, nuclear power facilities produce about 200,000 metric tons of low and intermediate level waste (LILW) and 10,000 metric tons of high level waste (HLW) (including spent fuel designated as waste) each year worldwide.\n\nIssues entirely separate from the question of the sustainability of nuclear fuel, relate to the use of nuclear fuel and the high-level radioactive waste the nuclear industry generates that if not properly contained, is highly hazardous to people and wildlife. The United Nations (UNSCEAR) estimated in 2008 that average annual human radiation exposure includes 0.01 millisievert (mSv) from the legacy of past atmospheric nuclear testing plus the Chernobyl disaster and the nuclear fuel cycle, along with 2.0 mSv from natural radioisotopes and 0.4 mSv from cosmic rays; all exposures vary by location. natural uranium in some inefficient reactor nuclear fuel cycles, becomes part of the nuclear waste \"once through\" stream, and in a similar manner to the scenario were this uranium remained naturally in the ground, this uranium emits various forms of radiation in a decay chain that has a half-life of about 4.5 billion years, the storage of this unused uranium and the accompanying fission reaction products have raised public concerns about risks of leaks and containment, however the knowledge gained from studying the Natural nuclear fission reactor in Oklo Gabon, has informed geologists on the proven processes that kept the waste from this 2 billion year old natural nuclear reactor that operated for hundreds of thousands of years, from negatively impacting the surrounding plant and animal life.\n\nNatural resources, known as renewable resources, are replaced by natural processes and forces persistent in the natural environment. There are intermittent and reoccurring renewables, and recyclable materials, which are utilized during a cycle across a certain amount of time, and can be harnessed for any number of cycles.\n\nThe production of goods and services by manufacturing products in economic systems creates many types of waste during production and after the consumer has made use of it. The material is then either incinerated, buried in a landfill or recycled for reuse. Recycling turns materials of value that would otherwise become waste into valuable resources again.\nThe natural environment, with soil, water, forests, plants and animals are all renewable resources, as long as they are adequately monitored, protected and conserved. Sustainable agriculture is the cultivation of plant and animal materials in a manner that preserves plant and animal ecosystems over the long term. The overfishing of the oceans is one example of where an industry practice or method can threaten an ecosystem, endanger species and possibly even determine whether or not a fishery is sustainable for use by humans. An unregulated industry practice or method can lead to a complete resource depletion.\nThe renewable energy from the sun, wind, wave, biomass and geothermal energies are based on renewable resources. Renewable resources such as the movement of water (hydropower, tidal power and wave power), wind and radiant energy from geothermal heat (used for geothermal power) and solar energy (used for solar power) are practically infinite and cannot be depleted, unlike their non-renewable counterparts, which are likely to run out if not used sparingly.\n\nThe potential wave energy on coastlines can provide 1/5 of world demand. Hydroelectric power can supply 1/3 of our total energy global needs. Geothermal energy can provide 1.5 more times the energy we need. There is enough wind to power the planet 30 times over, wind power could power all of humanity's needs alone. Solar currently supplies only 0.1% of our world energy needs, but there is enough out there to power humanity's needs 4,000 times over, the entire global projected energy demand by 2050.\n\nRenewable energy and energy efficiency are no longer niche sectors that are promoted only by governments and environmentalists. The increasing levels of investment and that more of the capital is from conventional financial actors, both suggest that sustainable energy has become mainstream and the future of energy production, as non-renewable resources decline. This is reinforced by climate change concerns, nuclear dangers and accumulating radioactive waste, high oil prices, peak oil and increasing government support for renewable energy. These factors are commercializing renewable energy, enlarging the market and growing demand, the adoption of new products to replace obsolete technology and the conversion of existing infrastructure to a renewable standard.\n\nIn economics, a non-renewable resource is defined as goods, where greater consumption today implies less consumption tomorrow. David Ricardo in his early works analysed the pricing of exhaustible resources, where he argued that the price of a mineral resource should increase over time. He argued that the spot price is always determined by the mine with the highest cost of extraction, and mine owners with lower extraction costs benefit from a differential rent. The first model is defined by Hotelling's rule, which is a 1931 economic model of non-renewable resource management by Harold Hotelling. It shows that efficient exploitation of a nonrenewable and nonaugmentable resource would, under otherwise stable conditions, lead to a depletion of the resource. The rule states that this would lead to a net price or \"Hotelling rent\" for it that rose annually at a rate equal to the rate of interest, reflecting the increasing scarcity of the resources.\nThe Hartwick's rule provides an important result about the sustainability of welfare in an economy that uses non-renewable source.\n\n\n"}
{"id": "58590304", "url": "https://en.wikipedia.org/wiki?curid=58590304", "title": "Orapa Power Station", "text": "Orapa Power Station\n\nThe Orapa Power Station is a Peak load power generation plant located in the mining town of Orapa in northeastern Botswana in the Central District. It is built within the Debswana Diamond Company Ltd Orapa diamond mine fenced leased area and is operated by the Botswana Power Corporation.\nThe plant construction was initiated when Botswana started experiencing electricity supply challenges from year 2007 when demand for electricity in the country started exceeding the county's electricity generation capacity leading to forced periodic nation wide load shedding excises by the Botswana Power Corporation. The plant was a short term response to the mining industry electricity requirements prior to the development of Mmamabula and Morupule B power stations.\n\nThe plant site is located next to an electrical substation through which it connects to the national grid and was designed to use either natural gas or diesel as fuel. It was commissioned using diesel with a planned conversion to natural gas once construction of a gas pipeline from nearby gas fields was complete and commissioned.\n\nThe plant consists of two 45MW GE LM6000 Sprint Simple Cycle gas turbines fueled using diesel. The energy produced is transferred to the national grid via the interconnected Orapa Substation by use of two short 132kV overhead power lines.\n\n"}
{"id": "1841288", "url": "https://en.wikipedia.org/wiki?curid=1841288", "title": "Outline of energy", "text": "Outline of energy\n\nThe following outline is provided as an overview of and topical guide to energy:\n\nEnergy – in physics, this is an indirectly observed quantity often understood as the ability of a physical system to do work on other physical systems. Since work is defined as a force acting through a distance (a length of space), energy is always equivalent to the ability to exert force (a pull or a push) against an object that is moving along a definite path of certain length.\n\n\nUnits of energy\n\n\nEnergy industry\n\nSee especially and for a large number of conventional energy related topics.\n\n\nHistory of energy\n\n\n\n\n\n\nEnergy economics\n\n\n\n\n\n\n"}
{"id": "58664232", "url": "https://en.wikipedia.org/wiki?curid=58664232", "title": "Phakalane power station", "text": "Phakalane power station\n\nPhakalane Power Station is a photovoltaic pilot power plant located in Phakalane, Botswana. The power station was funded through a Japanese grant which was part of Prime Minister Hatoyama's initiative strategy called Cool Earth Partnership aimed at supporting developing countries in their efforts to combat global warming. The Cool Earth Partnership is part of the initiatives which saw Hatoyama win the Sustainable Development Leadership Award in 2010.\n\n"}
{"id": "2399976", "url": "https://en.wikipedia.org/wiki?curid=2399976", "title": "Physical universe", "text": "Physical universe\n\nIn religion and esotericism, the term \"physical universe\" or \"material universe\" is used to distinguish the physical matter of the universe from a proposed spiritual or supernatural essence. \n\nIn the Book of Veles, and perhaps in traditional Slavic mythology, the physical universe is referred to as Yav. Gnosticism holds that the physical universe was created by a Demiurge. In Dharmic religions Maya is believed to be the illusion of a physical universe.\n\nPhysicalism, a type of monism, holds that only physical things exist. This is also known as metaphysical naturalism.\n\n"}
{"id": "3759820", "url": "https://en.wikipedia.org/wiki?curid=3759820", "title": "Physis", "text": "Physis\n\nPhysis (Greek: \"phusis\") is a Greek theological, philosophical, and scientific term usually translated into English as \"nature\".\n\nThe term is central to Greek philosophy, and as a consequence to Western philosophy as a whole.\nIn pre-Socratic usage, \"phusis\" contrasted with \"nomos\" \"law, human convention\"\nSince Aristotle, however, the \"physical\" (the subject matter of \"physics\", properly \"natural things\") has more typically been juxtaposed to the \"metaphysical\".\n\nThe word φύσις is a verbal noun based on φύω \"to grow, to appear\" (cognate with English \"to be\"). In Homeric Greek it is used quite literally, of the manner of growth of a particular species of plant. \n\nIn pre-Socratic philosophy, beginning with Heraclitus, \"phusis\" in keeping with its etymology of \"growing, becoming\" is always used in the sense of the \"natural\" \"development\", although the focus might lie either with the origin, or the process, or the end result of the process. There is some evidence that by the 6th century BC, beginning with the Ionian School, the word could also be used \nin the comprehensive sense, as referring to \"\"all\" things\", as it were \"Nature\" in the sense of \"Universe\".\n\nIn the Sophist tradition, the term stood in opposition to \"nomos\" (), \"law\" or \"custom\", in the debate on which parts of human existence are natural, and which are due to convention. \nThe contrast of \"phisis\" vs. \"nomos\" could be applied to any subject, much like the modern contrast of \"nature vs. nurture\".\n\nIn book 10 of \"Laws\", Plato criticizes those who write works \"peri phuseōs\". The criticism is that such authors tend to focus on a purely \"naturalistic\" explanation of the world, ignoring the role of \"intention\" or \"technē\", and thus becoming prone to the error of naive atheism. Plato accuses even Hesiod of this, for the reason that the gods in Hesiod \"grow\" out of primordial entities after the physical universe had been established.\n\n\"Because those who use the term mean to say that nature is the first creative power; but if the soul turns out to be the primeval element, and not fire or air, then in the truest sense and beyond other things the soul may be said to exist \"by\" nature; and this would be true if you proved that the soul is older than the body, but not otherwise.\"\n\nAristotle sought out the definition of \"physis\" to prove that there was more than one definition of \"physis\", and more than one way to interpret nature. \"Though Aristotle retains the ancient sense of \"physis\" as growth, he insists that an adequate definition of \"physis\" requires the different perspectives of the four causes (aitia): material, efficient, formal, and final.\" Aristotle believed that nature itself contained its own source of matter (material), power/motion (efficiency), form, and end (final). A unique feature about Aristotle's definition of \"physis\" was his relationship between art and nature. Aristotle said that \"physis\" (nature) is dependent on techne (art). \"The critical distinction between art and nature concerns their different efficient causes: nature is its own source of motion, whereas techne always requires a source of motion outside itself.\" What Aristotle was trying to bring to light, was that art does not contain within itself its form or source of motion. Consider the process of an acorn becoming an oak tree. This is a natural process that has its own driving force behind it. There is no external force pushing this acorn to its final state, rather it is progressively developing towards one specific end (telos).\nThough φύσις was often used in Hellenistic philosophy, it is used only 14 times in the New Testament (10 of those in the writings of Paul). Its meaning varies throughout Paul's writings. One usage refers to the established or natural order of things, as in \"Romans 2:14\" where Paul writes \"For when Gentiles, who do not have the law, by \"nature\" do what the law requires, they are a law to themselves, even though they do not have the law.\" Another use of φύσις in the sense of \"natural order\" is \"Romans 1:26\" where he writes \"the men likewise gave up \"natural\" relations with women and were consumed with passion for one another\". In \"1 Corinthians 11:14\", Paul asks \"Does not nature itself teach you that if a man wears long hair it is a disgrace for him?\"\n\nThis use of φύσις as referring to a \"natural order\" in \"Romans 1:26\" and \"1 Corinthians 11:14\" may have been influenced by Stoicism. The Greek philosophers, including Aristotle and the Stoics are credited with distinguishing between man-made laws and a natural law of universal validity, but Gerhard Kittel states that the Stoic philosophers were not able to combine the concepts of νόμος (law) and φύσις (nature) to produce the concept of \"natural law\" in the sense that was made possible by Judeo-Christian theology.\n\nAs part of the Pauline theology of salvation by grace, Paul writes in \"Ephesians 2:3\" that \"we all once lived in the passions of our flesh, carrying out the desires of the body and the mind, and were by \"nature\" children of wrath, like the rest of mankind. In the next verse he writes, \"by grace you have been saved.\" \n\nTheologians of the early Christian period differed in the usage of this term. In Antiochene circles, it connoted the humanity or divinity of Christ conceived as a concrete set of characteristics or attributes. In Alexandrine thinking, it meant a concrete individual or independent existent and approximated to hypostasis without being a synonym. While it refers to much the same thing as ousia it is more empirical and descriptive focussing on function while ousia is metaphysical and focuses more on reality. Although found in the context of the Trinitarian debate, it is chiefly important in the Christology of Cyril of Alexandria.\n\nThe Greek adjective \"phusikos\" is represented in various forms in modern English:\nAs \"physics\" \"the study of nature\", as \"physical\" (via Middle Latin \"physicalis\") referring both to physics (the study of nature, the material universe) and to the human body. The term physiology (\"physiologia\") is of 16th-century coinage (Jean Fernel). The term \"physique\", for \"the bodily constitution of a person\", is a 19th-century loan from French. \n\nIn medicine the suffix \"-physis\" occurs in such compounds as \"symphysis\", \"epiphysis\", and a few others, in the sense of \"a growth\". The physis also refers to the \"growth plate\", or site of growth at the end of long bones.\n\n"}
{"id": "169115", "url": "https://en.wikipedia.org/wiki?curid=169115", "title": "Preternatural", "text": "Preternatural\n\nThe preternatural or praeternatural is that which appears outside or beside (Latin \"\") the natural. It is \"suspended between the mundane and the miraculous\".\n\nIn theology, the term is often used to distinguish marvels or deceptive trickery, often attributed to witchcraft or demons, from the purely divine power of the genuinely supernatural to violate the laws of nature. In the early modern period the term was used by scientists to refer to abnormalities and strange phenomena of various kinds that seemed to depart from the norms of nature.\n\nMedieval theologians made a clear distinction between the natural, the preternatural and the supernatural. Thomas Aquinas argued that the supernatural consists in \"God’s unmediated actions\"; the natural is \"what happens always or most of the time\"; and the preternatural is \"what happens rarely, but nonetheless by the agency of created beings...Marvels belong, properly speaking, to the realm of the preternatural.\" Theologians, following Aquinas, argued that only God had the power to disregard the laws of nature that he has created, but that demons could manipulate the laws of nature by a form of trickery, to deceive the unwary into believing they had experienced real miracles. According to historian Lorraine Daston,\n\nAlthough demons, astral intelligences, and other spirits might manipulate natural causes with superhuman dexterity and thereby work marvels, as mere creatures they could never transcend from the preternatural to the supernatural and work genuine miracles.\n\nBy the 16th century, the term \"preternatural\" was increasingly used to refer to demonic activity comparable to the use of magic by human adepts: The Devil, \"being a natural Magician … may perform many acts in ways above our knowledge, though not transcending our natural power.\" According to the philosophy of the time, preternatural phenomena were not contrary to divine law, but used hidden, or occult powers that violated the \"normal\" pattern of natural phenomena.\n\nWith the emergence of early modern science, the concept of the preternatural increasingly came to be used to refer to strange or abnormal phenomena that seemed to violate the normal working of nature, but which were not associated with magic and witchcraft. This was a development of the idea that preternatural phenomena were fake miracles. As Daston puts it, \"To simplify the historical sequence somewhat: first, preternatural phenomena were demonized and thereby incidentally naturalized; then the demons were deleted, leaving only the natural causes.\" The use of the term was especially common in medicine, for example in John Brown's \"A Compleat Treatise of Preternatural Tumours\" (1678), or William Smellie's \"A Collection of Preternatural Cases and Observations in Midwifery\" (1754).\n\nIn the 19th century the term was appropriated in anthropology to refer to folk beliefs about fairies, trolls and other such creatures which were not thought of as demonic, but which were perceived to affect the natural world in unpredictable ways. According to Thorstein Veblen, such preternatural agents were often thought of as forces somewhere between supernatural beings and material processes. \"The preternatural agency is not necessarily conceived to be a personal agent in the full sense, but it is an agency which partakes of the attributes of personality to the extent of somewhat arbitrarily influencing the outcome of any enterprise, and especially of any contest.\"\n\nThe linguistic association between individual agents and unexplained or unfortunate circumstances remains. Many people attribute occurrences that are known to be material processes, such as \"gremlins in the engine\", a \"ghost in the machine\", or attributing motives to objects: \"the clouds are threatening\". The anthropomorphism in our daily life is a combination of the above cultural stems, as well as the manifestation of our pattern-projecting minds.\n\nIn 2011, Penn State Press began publishing a learned journal entitled \"Preternature: Critical and Historical Studies on the Preternatural\". Edited by Kirsten Uszkalo and Richard Raiswell, the journal is dedicated to publishing articles, reviews and short editions of original texts that deal with conceptions and perceptions of the preternatural in any culture and in any historical period. The journal covers \"magics, witchcraft, spiritualism, occultism, prophecy, monstrophy, demonology, and folklore.\"\n\n\n"}
{"id": "10854000", "url": "https://en.wikipedia.org/wiki?curid=10854000", "title": "Statistical study of energy data", "text": "Statistical study of energy data\n\nEnergy statistics refers to collecting, compiling, analyzing and disseminating data on commodities such as coal, crude oil, natural gas, electricity, or renewable energy sources (biomass, geothermal, wind or solar energy), when they are used for the energy they contain. Energy is the capability of some substances, resulting from their physico-chemical properties, to do work or produce heat. Some energy commodities, called fuels, release their energy content as heat when they burn. This heat could be used to run an internal or external combustion engine.\n\nThe need to have statistics on energy commodities became obvious during the 1973 oil crisis that brought tenfold increase in petroleum prices. Before the crisis, to have accurate data on global energy supply and demand was not deemed critical. Another concern of energy statistics today is a huge gap in energy use between developed and developing countries. As the gap narrows (\"see picture\"), the pressure on energy supply increases tremendously. \n\nThe data on energy and electricity come from three principal sources:\nThe flows of and trade in energy commodities are measured both in physical units (e.g., metric tons), and, when energy balances are calculated, in energy units (e.g., terajoules or tons of oil equivalent). What makes energy statistics specific and different from other fields of economic statistics is the fact that energy commodities undergo greater number of transformations (flows) than other commodities. In these transformations energy is conserved, as defined by and within the limitations of the first and second laws of thermodynamics. \n\n\n\n"}
{"id": "54716184", "url": "https://en.wikipedia.org/wiki?curid=54716184", "title": "United States Energy Association", "text": "United States Energy Association\n\nThe United States Energy Association (USEA) is the U.S. Member Committee of the World Energy Council. Headquartered in Washington, D.C., USEA is an association of public and private energy-related organizations, corporations, and government agencies. \nThe association hosts annual events such as the Carbon Sequestration Leadership Forum, Energy Supply Forum, and State of the Energy Industry Forum.\n\nBarry Worthington has served as USEA’s executive director since 1988. Worthington chairs the Clean Electricity Production working group within the UNECE Committee on Sustainable Energy. He also sits on numerous energy boards, including the National Energy Foundation (chairman) and Energy Law Foundation.\n\nWorthington meets with domestic and international energy leaders to discuss energy infrastructure partnerships. He often advocates for energy cultivation in developing countries, claiming there are \"few priorities greater for the world than getting people linked to the grid.\" Worthington is a firm supporter of energy \"sovereignty.\"\n\nVicky Bailey currently chairs USEA’s board of directors. She succeeded Jack Futcher, the President and COO of Bechtel.\n\nFor 25 years, USEA has been a partner with USAID, expanding energy infrastructure, improving energy access, and reducing energy poverty in developing economies through international energy partnerships. A major function of USEA’s is to help USAID expand energy infrastructure and programs in developing countries. In 2012, the association launched the U.S.-East Africa Geothermal Partnership (EAGP), a public-private partnership “offering assistance at early stages of project development in East Africa.” Through the Djibouti Geothermal Partnership, Ethiopia Geothermal Partnership, and Kenya Electricity Generating Company (KenGen), USEA partners with the Department of Energy and local governments to promote U.S. companies’ involvement in developing additional geothermal generation capacity. According to USEA, the number of U.S. companies conducting geothermal work in East Africa has more than tripled since EAGP’s inception.\n\nUSEA represents the interests of the U.S. energy sector through public education and advocacy. The association supports an “all-of-the-above energy strategy,” from renewable energy to fossil fuels. USEA advocates for the exploration and production of oil and natural gas.\n\n"}
{"id": "13873874", "url": "https://en.wikipedia.org/wiki?curid=13873874", "title": "Zeta potential titration", "text": "Zeta potential titration\n\nZeta potential titration is a titration of heterogeneous systems, for example colloids and emulsions. Solids in such systems have very high surface area. This type of titration is used to study the zeta potential of these surfaces under different conditions.\n\nThe iso-electric point is one such property. The iso-electric point is the pH value at which the zeta potential is approximately zero. At a pH near the iso-electric point (± 2 pH units), colloids are usually unstable; the particles tend to coagulate or flocculate. Such titrations use acids or bases as titration reagents. Tables of iso-electric points for different materials are available. The attached figure illustrates results of such titrations for concentrated dispersions of alumina (4% v/v) and rutile (7% v/v). It is seen that iso-electric point of alumina is around pH 9.3, whereas for rutile it is around pH 4. Alumina is unstable in the pH range from 7 to 11. Rutile is unstable in the pH range from 2 to 6.\n\nAnother purpose of this titration is determination of the optimum dose of surfactant for achieving stabilization or flocculation of a heterogeneous system. Examples can be found in the book by Dukhin and Goetz.\n\nIn a zeta-potential titration, the Zeta potential is the indicator. Measurement of the zeta potential can be performed using microelectrophoresis, or electrophoretic light scattering, or electroacoustic phenomena. The last method makes possible to perform titrations in concentrated systems, with no dilution. The book by Dukhin and Goetz provides a detailed description of such titrations.\n\n"}
