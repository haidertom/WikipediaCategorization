{"id": "3972731", "url": "https://en.wikipedia.org/wiki?curid=3972731", "title": "Akinetic mutism", "text": "Akinetic mutism\n\nAkinetic mutism is a medical term describing patients tending neither to move (akinesia) nor speak (mutism). Akinetic mutism was first described in 1941 as a mental state where patients lack the ability to move or speak. However, their eyes may follow their observer or be diverted by sound. Patients lack most motor functions such as speech, facial expressions, and gestures, but demonstrate apparent alertness. They exhibit reduced activity and slowness, and can speak in whispered monosyllables. Patients often show visual fixation on their examiner, move their eyes in response to an auditory stimulus, or move after often repeated commands. Patients with akinetic mutism are not paralyzed, but lack the will to move. Many patients describe that as soon as they 'will' or attempt a movement, a 'counter-will' or 'resistance' rises up to meet them.\n\nAkinetic mutism varies across all patients. Its form, intensity, and clinical features correspond more closely to its functional anatomy rather than to its pathology. However, akinetic mutism most often appears in two different forms: frontal and mesencephalic.\n\nAkinetic mutism can occur in the frontal region of the brain and occurs because of bilateral frontal lobe damage. Akinetic mutism as a result of frontal lobe damage is clinically characterized as hyperpathic. It occurs in patients with bilateral circulatory disturbances in the supply area of the anterior cerebral artery.\n\nAkinetic mutism can also occur as a result of damage to the mesencephalic region of the brain. Mesencephalic akinetic mutism is clinically categorized as somnolent or apathetic akinetic mutism. It is characterized by vertical gaze palsy and ophthalmoplegia. This state of akinetic mutism varies in intensity, but it is distinguished by drowsiness, lack of motivation, hyper-somnolence, and reduction in spontaneous verbal and motor actions.\n\nSymptoms of akinetic mutism progress over time. The occurrence of akinetic mutism takes place approximately four months after the symptoms first appear.\n\nAkinetic mutism can be caused by a variety of things. It often occurs after brain injury or as a symptom of other diseases.\n\nAkinetic mutism is often the result of severe frontal lobe injury in which the pattern of inhibitory control is one of increasing passivity and gradually decreasing speech and motion.\n\nMany cases of akinetic mutism occur after a thalamic stroke. The thalamus helps regulate consciousness and alertness.\n\nAnother cause of both akinesia and mutism is ablation of the cingulate gyrus. Destruction of the cingulate gyrus has been used in the treatment of psychosis. Such lesions result in akinesia, mutism, apathy, and indifference to painful stimuli. The anterior cingulate cortex is thought to supply a \"global energizing factor\" that stimulates decision making. When the anterior cingulate cortex is damaged, it can result in akinetic mutism.\n\nAkinetic mutism is a symptom during the final stages of Creutzfeldt–Jakob disease (a rare degenerative brain disease) and can help diagnose patients with this disease. It can also occur in a stroke that affects both anterior cerebral artery territories. Another cause is neurotoxicity due to exposure to certain drugs such as tacrolimus and cyclosporine.\n\nOther causes of akinetic mutism are as follows:\n\nAkinetic mutism can be misdiagnosed as depression, delirium, or locked-in syndrome, all of which are common following a stroke. Patients with depression can experience apathy, slurring of speech, and body movements similar to akinetic mutism. Similarly to akinetic mutism, patients with locked-in syndrome experience paralysis and can only communicate with their eyes. Correct diagnosis is important to ensure proper treatment. A variety of treatments for akinetic mutism have been documented, but treatments vary between patients and cases.\n\nTreatments using intravenous magnesium sulfate have shown to reduce the symptoms of akinetic mutism. In one case, a 59-year-old woman was administered intravenous magnesium sulfate in an attempt to resolve her akinetic mutism. The patient was given 500 mg of magnesium every eight hours, and improvement was seen after 24 hours. She became more verbal and attentive, and treatment was increased to 1000 mg every eight hours as conditions continued to improve.\n\nAs seen in the case of Elsie Nicks, the puncture or removal of a cyst causing akinetic mutism can relieve symptoms almost immediately. However, if the cyst fills up again, the symptoms can reappear.\n\nSymptoms of akinetic mutism suggest a possible presynaptic deficit in the nigrostriatal pathway, which transmits dopamine. Some patients with akinetic mutism have shown to improve with levodopa or dopamine agonist therapy, or by repleting dopamine in the motivational circuit with stimulants, antidepressants, or agonists such as bromocriptine or amantadine.\n\nOther treatments include amantadine, carbidopa-levodopa, donepezil, memantine, and oral magnesium oxide.\n\nFourteen-year-old Elsie Nicks was the first patient to be diagnosed with akinetic mutism by Cairns in 1941. She suffered from severe headaches her entire life and was eventually given morphine to help with treatment. She began to enter a state of akinetic mutism, experiencing apathy and loss of speech and motor control. A cyst on her right lateral ventricle was tapped, and as soon as the needle advanced toward the cyst, she let out a loud noise and was able to state her name, age, and address. After her cyst was emptied, she regained her alertness and intelligence, and she had no recollection of her time spent in the hospital. The cyst was drained two more times over the next seven months and was eventually removed. After eight months of rehabilitation, Elsie no longer experienced headaches or akinetic mutism symptoms.\n\n"}
{"id": "406551", "url": "https://en.wikipedia.org/wiki?curid=406551", "title": "Applied linguistics", "text": "Applied linguistics\n\nApplied linguistics is an interdisciplinary field of linguistics which identifies, investigates, and offers solutions to language-related real-life problems. Some of the academic fields related to applied linguistics are education, psychology, communication research, anthropology, and sociology.\n\nApplied linguistics is an interdisciplinary field of linguistics. Major branches of applied linguistics include bilingualism and multilingualism, conversation analysis, contrastive linguistics, sign linguistics, language assessment, literacies, discourse analysis, language pedagogy, second language acquisition, language planning and policy, interlinguistics, stylistics, pragmatics, forensic linguistics and translation.\n\nMajor journals of the field include \"Annual Review of Applied Linguistics\", \"Applied Linguistics\", \"Journal of Applied Linguistics\", \"International Review of Applied Linguistics\", \"International Journal of Applied Linguistics\", \"European Journal of Applied Linguistics\", \"Issues in Applied Linguistics\", \"Language Learning\", \"Language and Education, TESOL Quarterly\", and \"Linguistics and Education\".\n\nThe tradition of applied linguistics established itself in part as a response to the narrowing of focus in linguistics with the advent in the late 1950s of generative linguistics, and has always maintained a socially-accountable role, demonstrated by its central interest in language problems.\n\nAlthough the field of applied linguistics started from Europe and the United States, the field rapidly flourished in the international context.\n\nApplied linguistics first concerned itself with principles and practices on the basis of linguistics. In the early days, applied linguistics was thought as “linguistics-applied” at least from the outside of the field. In the 1960s, however, applied linguistics was expanded to include language assessment, language policy, and second language acquisition. As early as the 1970s, applied linguistics became a problem-driven field rather than theoretical linguistics, including the solution of language-related problems in the real world. By the 1990s, applied linguistics had broadened including critical studies and multilingualism. Research in applied linguistics was shifted to \"the theoretical and empirical investigation of real world problems in which language is a central issue.\"\n\nIn the United States, applied linguistics also began narrowly as the application of insights from structural linguistics—first to the teaching of English in schools and subsequently to second and foreign language teaching. The \"linguistics applied\" approach to language teaching was promulgated most strenuously by Leonard Bloomfield, who developed the foundation for the Army Specialized Training Program, and by Charles C. Fries, who established the English Language Institute (ELI) at the University of Michigan in 1941. In 1946, Applied linguistics became a recognized field of studies in the aforementioned university. In 1948, the Research Club at Michigan established \"Language Learning: A Journal of Applied Linguistics\", the first journal to bear the term \"applied linguistics.\" In the late 1960s, applied linguistics began to establish its own identity as an interdisciplinary field of linguistics concerned with real-world language issues. The new identity was solidified by the creation of the American Association for Applied Linguistics in 1977.\n\nThe International Association of Applied Linguistics was founded in France in 1964, where it is better known as Association Internationale de Linguistique Appliquée, or AILA. AILA has affiliates in more than thirty countries, some of which are listed below.\n\nAustralia\nAustralian applied linguistics took as its target the applied linguistics of mother tongue teaching and teaching English to immigrants. The Australia tradition shows a strong influence of continental Europe and of the USA, rather than of Britain. Applied Linguistics Association of Australia (ALAA) was established at a national congress of applied linguists held in August 1976. ALAA holds a joint annual conference in collaboration with the Association for Applied Linguistics in New Zealand (ALANZ).\n\nCanada\nThe Canadian Association of Applied Linguistics / L’Association canadienne de linguistique appliquée (CAAL/ACLA), is an officially bilingual (English and French) scholarly association with approximately 200 members. They produce the \"Canadian Journal of Applied Linguistics\" and hold an annual conference.\n\nIreland\nThe Irish Association for Applied Linguistics/Cumann na Teangeolaíochta Feidhmí (IRAAL) was founded in 1975. They produce the journal \"Teanga\", the Irish word for 'language'.\n\nJapan\nIn 1982, the Japan Association of Applied Linguistics (JAAL) was established in the Japan Association of College English Teachers (JACET) in order to engage in activities on a more international scale. In 1984, JAAL became an affiliate of the International Association of Applied Linguistics (AILA).\n\nNew Zealand\nThe Applied Linguistics Association of New Zealand (ALANZ) produces the journal \"New Zealand Studies in Applied Linguistics\" and has been collaborating with the Applied Linguistics Association of Australia in a combined annual conference since 2010, with the Association for Language Testing and Assessment of Australia and New Zealand (ALTAANZ) later joining the now three-way conference collaboration. \n\nSouth Africa\nThe Southern African Applied Linguistics Association (SAALA) was founded in 1980. There are currently four publications associated with SAALA including the \"Southern African Linguistics and Applied Language Studies Journal\" (SAJALS).\n\nUnited Kingdom\nThe British Association for Applied Linguistics (BAAL) was established in 1967. Its mission is \"the advancement of education by fostering and promoting, by any lawful charitable means, the study of language use, language acquisition and language teaching and the fostering of interdisciplinary collaboration in this study [...]\". BAAL hosts an annual conference, as well as many additional smaller conferences and events organised by its Special Interest Groups (SIGs). \n\nUnited States\nThe American Association for Applied Linguistics (AAAL) was founded in 1977. AAAL holds an annual conference, usually in March or April, in the United States or Canada.\n\n\n"}
{"id": "12328438", "url": "https://en.wikipedia.org/wiki?curid=12328438", "title": "Auditory processing disorder", "text": "Auditory processing disorder\n\nAuditory processing disorder (APD), also known as central auditory processing disorder (CAPD), is an umbrella term for a variety of disorders that affect the way the brain processes auditory information. Individuals with APD usually have normal structure and function of the outer, middle, and inner ear (peripheral hearing). However, they cannot process the information they hear in the same way as others do, which leads to difficulties in recognizing and interpreting sounds, especially the sounds composing speech. It is thought that these difficulties arise from dysfunction in the central nervous system.\n\nThe American Academy of Audiology notes that APD is diagnosed by difficulties in one or more auditory processes known to reflect the function of the central auditory nervous system.\n\nAPD can affect both children and adults, although the actual prevalence is currently unknown. It has been suggested that males are twice as likely to be affected by the disorder as females, but there are no good epidemiological studies.\n\nCAPD can continue into adulthood. Cooper and Gates (1991) estimated the prevalence of adult APD to be 10 to 20%. Many people experience problems with learning and day-to-day tasks with difficulties over time. Adults with this disorder\n\nIt has been discovered that APD and ADHD present overlapping symptoms. Below is a ranked order of behavioral symptoms that are most frequently observed in each disorder. Professionals evaluated the overlap of symptoms between the two disorders. The order below is of symptoms that are almost always observed. This chart proves that although the symptoms listed are different, it is easy to get confused between many of them.\n\nThere is a high rate of co-occurrence between AD/HD and CAPD. An article published in 1994 showed that 84% of children with APD have confirmed or suspected ADHD. Co-occurrence between ADHD and APD is 41% for children with confirmed diagnosis of ADHD, and 43% for children suspected of having ADHD. \"more recently published data is needed to support or refute this statement.\" \n\nThere has been considerable debate over the relationship between APD and Specific language impairment (SLI).\n\nSLI is diagnosed when a child has difficulties with understanding or producing spoken language for no obvious cause. The problems cannot be explained in terms of peripheral hearing loss. The child is typically late in starting to talk, and may have problems in producing speech sounds clearly, and in producing or understanding complex sentences. Some theoretical accounts of SLI regard it as the result of auditory processing problems. However, this view of SLI is not universally accepted, and others regard the main difficulties in SLI as stemming from problems with higher-level aspects of language processing. Where a child has both auditory and language problems, it can be hard to sort out cause-and-effect.\n\nSimilarly with developmental dyslexia, there has been considerable interest in the idea that for some children reading problems are downstream consequences of difficulties in rapid auditory processing. Again, cause and effect can be hard to unravel. This is one reason why experts such as Moore have recommended using non-verbal auditory tests to diagnose APD. Specifically regarding the neurological factors of dyslexia, the disorder has been linked to polymicrogyria which causes cell migrational problems. This relates to APD because children that have polymicrogyri almost always present deficits on APD testing.\n\nIt has also been suggested that APD may be related to cluttering, a fluency disorder marked by word and phrase repetitions.\n\nIt has been found that a higher than expected proportion of individuals diagnosed with SLI and dyslexia on the basis of language and reading tests also perform poorly on tests in which APD is tested. APD can be assessed using tests that involve identifying, repeating or discriminating speech, and a child may do poorly because of primary language problems. In a study comparing children with a diagnosis of dyslexia and those with a diagnosis of APD, they found the two groups could not be distinguished. obtained similar findings in studies comparing children diagnosed with SLI or APD. The two groups had very similar profiles. This raises the worrying possibility that the diagnosis that a child receives may be largely a function of the specialist they see: the same child who would be diagnosed with APD by an audiologist may be diagnosed with SLI by a speech-language therapist or with dyslexia by a psychologist.\n\nAcquired APD can be caused by any damage to or dysfunction of the central auditory nervous system and can cause auditory processing problems. For an overview of neurological aspects of APD, see Griffiths.\n\nThe ability to listen to and comprehend multiple messages at the same time is a trait that is heavily influenced by our genes say federal researchers. These \"short circuits in the wiring\" sometimes run in families or result from a difficult birth, just like any learning disability. Auditory processing disorder can be associated with conditions affected by genetic traits, such as various developmental disorders. Inheritance of Auditory Processing Disorder refers to whether the condition is inherited from your parents or \"runs\" in families. Central auditory processing disorder may be hereditary neurological traits from the mother or the father.\n\nIn the majority of cases of developmental APD, the cause is unknown. An exception is acquired epileptic aphasia or Landau-Kleffner syndrome, where a child's development regresses, with language comprehension severely affected. The child is often thought to be deaf, but normal peripheral hearing is found. In other cases, suspected or known causes of APD in children include delay in myelin maturation, ectopic (misplaced) cells in the auditory cortical areas, or genetic predisposition. In a family with autosomal dominant epilepsy, seizures which affected the left temporal lobe seemed to cause problems with auditory processing. In another extended family with a high rate of APD, genetic analysis showed a haplotype in chromosome 12 that fully co-segregated with language impairment.\n\nHearing begins in utero, but the central auditory system continues to develop for at least the first decade. There is considerable interest in the idea that disruption to hearing during a sensitive period may have long-term consequences for auditory development. One study showed thalamocortical connectivity in vitro was associated with a time sensitive developmental window and required a specific cell adhesion molecule (lcam5) for proper brain plasticity to occur. This points to connectivity between the thalamus and cortex shortly after being able to hear (in vitro) as at least one critical period for auditory processing. Another study showed that rats reared in a single tone environment during critical periods of development had permanently impaired auditory processing. ‘Bad’ auditory experiences, such as temporary deafness by cochlear removal in rats leads to neuron shrinkage. In a study looking at attention in APD patients, children with one ear blocked developed a strong right-ear advantage but were not able to modulate that advantage during directed-attention tasks.\n\nIn the 1980s and 1990s, there was considerable interest in the role of chronic Otitis media (middle ear disease or 'glue ear') in causing APD and related language and literacy problems. Otitis media with effusion is a very common childhood disease that causes a fluctuating conductive hearing loss, and there was concern this may disrupt auditory development if it occurred during a sensitive period. Consistent with this, in a sample of young children with chronic ear infections recruited from a hospital otolargyngology department, increased rates of auditory difficulties were found later in childhood. However, this kind of study will suffer from sampling bias because children with otitis media will be more likely to be referred to hospital departments if they are experiencing developmental difficulties. Compared with hospital studies, epidemiological studies, which assesses a whole population for otitis media and then evaluate outcomes, have found much weaker evidence for long-term impacts of otitis media on language outcomes.\n\nQuestionnaires can be used for the identification persons with possible auditory processing disorders, as these address common problems of listening. They can help in the decision for pursuing clinical evaluation. One of the most common listening problems is speech recognition in the presence of background noise. According to the respondents who participated in a study by Neijenhuis, de Wit, and Luinge (2017), the following symptoms are characteristic in children with listening difficulties, and they are typically problematic with adolescents and adults. They include:\n\n\nAccording to the New Zealand Guidelines on Auditory Processing Disorders (2017) a checklist of key symptoms of APD or comorbidities that can be used to identify individuals who should be referred for audiological and APD assessment includes, among others:\n\n\nFinally, the New Zealand guidelines state that behavioral checklists and questionnaires should only be used to provide guidance for referrals, for information gathering (for example, prior to assessment or as outcome measures for interventions), and as measures to describe the functional impact of auditory processing disorder.  They are not designed for the purpose of diagnosing auditory processing disorders. The New Zealand guidelines indicate that a number of questionnaires have been developed to identify children who might benefit from evaluation of their problems in listening.  Examples of available questionnaires include the Fisher’s Auditory Problems Checklist (1976), the Children’s Auditory Performance Scale (1998), the Screening Instrument for Targeting Educational Risk (1989), and the Auditory Processing Domains Questionnaire (O’Haraa and Mealings, 2017) among others. All of the previous questionnaires were designed for children and none are useful for adolescents and adults.   \n\nThe University of Cincinnati Auditory Processing Inventory (UCAPI) (Keith, Tektas and Ramsey, 2018) was designed for use with adolescents and adults seeking testing for evaluation of problems with listening and/or to be used following diagnosis of an auditory processing disorder to determine the subject’s status.  Following a model described by Zoppo et al. (2015) a 34-item questionnaire was developed that investigates auditory processing abilities in each of the six common areas of complaint in APD (listening and concentration, understanding speech, following spoken instructions, attention, and other.)  The final questionnaire was standardized on normally achieving young adults ranging from 18 to 27 years of age.  Validation data was acquired from subjects with language-learning or auditory processing disorders who were either self-reported or confirmed by diagnostic testing.  A UCAPI  total score is calculated by combining the totals from the six listening conditions and provides an overall value to categorize listening abilities. Additionally, analysis of the scores from the six listening conditions provides an auditory profile for the subject. Each listening condition can then be utilized by the professional in making recommendation for diagnosing problem of learning through listening and treatment decisions.  The UCAPI provides information on listening problems in various populations that can aid examiners in making recommendations for assessment and management.\n\nAPD is a difficult disorder to detect and diagnose. The subjective symptoms that lead to an evaluation for APD include an intermittent inability to process verbal information, leading the person to guess to fill in the processing gaps. There may also be disproportionate problems with decoding speech in noisy environments.\n\nAPD has been defined anatomically in terms of the integrity of the auditory areas of the nervous system. However, children with symptoms of APD typically have no evidence of neurological disease and the diagnosis is made on the basis of performance on behavioral auditory tests. Auditory processing is \"what we do with what we hear\", and in APD there is a mismatch between peripheral hearing ability (which is typically normal) and ability to interpret or discriminate sounds. Thus in those with no signs of neurological impairment, APD is diagnosed on the basis of auditory tests. There is, however, no consensus as to which tests should be used for diagnosis, as evidenced by the succession of task force reports that have appeared in recent years. The first of these occurred in 1996. This was followed by a conference organized by the American Academy of Audiology. Experts attempting to define diagnostic criteria have to grapple with the problem that a child may do poorly on an auditory test for reasons other than poor auditory perception: for instance, failure could be due to inattention, difficulty in coping with task demands, or limited language ability. In an attempt to rule out at least some of these factors, the American Academy of Audiology conference explicitly advocated that for APD to be diagnosed, the child must have a modality-specific problem, i.e. affecting auditory but not visual processing. However, an ASHA committee subsequently rejected modality-specificity as a defining characteristic of auditory processing disorders.\nThe American Speech-Language-Hearing Association (ASHA) published \"(Central) Auditory Processing Disorders\" in January 2005 as an update to the \"Central Auditory Processing: Current Status of Research and Implications for Clinical Practice (ASHA, 1996)\". The American Academy of Audiology has released more current practice guidelines related to the disorder. ASHA formally defines APA as \"a difficulty in the efficiency and effectiveness by which the central nervous system (CNS) utilizes auditory information.\"\n\nIn 2011, the British Society of Audiology published 'best practice guidelines'.\n\nAuditory processing disorder can be developmental or acquired. It may result from ear infections, head injuries or neurodevelopmental delays that affect processing of auditory information. This can include problems with: \"...sound localization and lateralization (see also binaural fusion); auditory discrimination; auditory pattern recognition; temporal aspects of audition, including temporal integration, temporal discrimination (e.g., temporal gap detection), temporal ordering, and temporal masking; auditory performance in competing acoustic signals (including dichotic listening); and auditory performance with degraded acoustic signals\".\n\nThe Committee of UK Medical Professionals Steering the UK Auditory Processing Disorder Research Program have developed the following working definition of Auditory Processing Disorder: \"APD results from impaired neural function and is characterized by poor recognition, discrimination, separation, grouping, localization, or ordering of speech sounds. It does not solely result from a deficit in general attention, language or other cognitive processes.\"\n\n1. The SCAN-C for children and SCAN-A for adolescents and adults are the most common tool for screening and diagnosing APD in the USA. Both tests are standardized on a large number of subjects and include validation data on subjects with auditory processing disorders. The test batteries include screening tests: norm-based criterion-referenced scores; diagnostic tests: scaled scores, percentile ranks and ear advantage scores for all tests except the Gap Detection test. The four tests include four subsets on which the subject scores are derived include: discrimination of monaurally presented single words against background noise (speech in noise), acoustically degraded single words (filtered words), dichotically presented single words and sentences. \n\n2. Random Gap Detection Test (RGDT) is also a standardized test. It assesses an individual’s gap detection threshold of tones and white noise. The exam includes stimuli at four different frequencies (500, 1000, 2000, and 4000 Hz) and white noise clicks of 50 ms duration. It is a useful test because it provides an index of auditory temporal resolution. In children, an overall gap detection threshold greater than 20 ms means they have failed and may have an auditory processing disorder based on abnormal perception of sound in the time domain.\n\n3. Gaps in Noise Test (GIN) also measures temporal resolution by testing the patient's gap detection threshold in white noise.\n\n4. Pitch Patterns Sequence Test (PPT) and Duration Patterns Sequence Test (DPT) measure auditory pattern identification. The PPS has s series of three tones presented at either of two pitches (high or low). Meanwhile, the DPS has a series of three tones that vary in duration rather than pitch (long or short). Patients are then asked to describe the pattern of pitches presented.\n\nThe issue of modality-specificity has led to considerable debate among experts in this field. Cacace and McFarland have argued that APD should be defined as a \"modality-specific\" perceptual dysfunction that is not due to peripheral hearing loss. They criticise more inclusive conceptualizations of APD as lacking diagnostic specificity. A requirement for modality-specificity could potentially avoid including children whose poor auditory performance is due to general factors such as poor attention or memory. Others, however, have argued that a modality-specific approach is too narrow, and that it would miss children who had genuine perceptual problems affecting both visual and auditory processing. It is also impractical, as audiologists do not have access to standardized tests that are visual analogs of auditory tests. The debate over this issue remains unresolved. It is clear, however, that a modality-specific approach will diagnose fewer children with APD than a modality-general one, and that the latter approach runs a risk of including children who fail auditory tests for reasons other than poor auditory processing. Although modality-specific testing has been advocated for well over a decade, to date no tests have been published which would allow audiologists to perform a modality-specific evaluation (i.e., no clinical versions of visual analogs to auditory processing tests exist).\n\nAnother controversy concerns the fact that most traditional tests of APD use verbal materials. The British Society of Audiology has embraced Moore's (2006) recommendation that tests for APD should assess processing of \"non-speech sounds\". The concern is that if verbal materials are used to test for APD, then children may fail because of limited language ability. An analogy may be drawn with trying to listen to sounds in a foreign language. It is much harder to distinguish between sounds or to remember a sequence of words in a language you do not know well: the problem is not an auditory one, but rather due to lack of expertise in the language.\n\nIn recent years there have been additional criticisms of some popular tests for diagnosis of APD. Tests that use tape-recorded American English have been shown to over-identify APD in speakers of other forms of English. Performance on a battery of non-verbal auditory tests devised by the Medical Research Council's Institute of Hearing Research was found to be heavily influenced by non-sensory task demands, and indices of APD had low reliability when this was controlled for. This research undermines the validity of APD as a distinct entity in its own right and suggests that the use of the term \"disorder\" itself is unwarranted. In a recent review of such diagnostic issues, it was recommended that children with suspected auditory processing impairments receive a holistic psychometric assessment including general intellectual ability, auditory memory, and attention, phonological processing, language, and literacy. The authors state that \"a clearer understanding of the relative contributions of perceptual and non-sensory, unimodal and supramodal factors to performance on psychoacoustic tests may well be the key to unravelling the clinical presentation of these individuals.\"\n\nDepending on how it is defined, APD may share common symptoms with ADD/ADHD, specific language impairment, and autism spectrum disorders. A review showed substantial evidence for atypical processing of auditory information in children with autism. Dawes and Bishop noted how specialists in audiology and speech-language pathology often adopted different approaches to child assessment, and they concluded their review as follows: \"We regard it as crucial that these different professional groups work together in carrying out assessment, treatment and management of children and undertaking cross-disciplinary research.\" In practice, this seems rare.\n\nTo ensure that APD is correctly diagnosed, the examiners must differentiate APD from other disorders with similar symptoms. Factors that should be taken into account during the diagnosis are: attention, auditory neuropathy, fatigue, hearing and sensitivity, intellectual and developmental age, medications, motivation, motor skills, native language and language experience, response strategies and decision-making style, and visual acuity.\n\nIt should also be noted that children under the age of seven cannot be evaluated correctly because their language and auditory processes are still developing. In addition, the presence of APD cannot be evaluated when a child's primary language is not English.\n\nThe National Institute on Deafness and Other Communication Disorders state that children with Auditory Processing Disorder often:\n\nAPD can manifest as problems determining the direction of sounds, difficulty perceiving differences between speech sounds and the sequencing of these sounds into meaningful words, confusing similar sounds such as \"hat\" with \"bat\", \"there\" with \"where\", etc. Fewer words may be perceived than were actually said, as there can be problems detecting the gaps between words, creating the sense that someone is speaking unfamiliar or nonsense words. In addition, it is common for APD to cause speech errors involving the distortion and substitution of consonant sounds. Those suffering from APD may have problems relating what has been said with its meaning, despite obvious recognition that a word has been said, as well as repetition of the word. Background noise, such as the sound of a radio, television or a noisy bar can make it difficult to impossible to understand speech, since spoken words may sound distorted either into irrelevant words or words that don't exist, depending on the severity of the auditory processing disorder. Using a telephone can be problematic for someone with auditory processing disorder, in comparison with someone with normal auditory processing, due to low quality audio, poor signal, intermittent sounds and the chopping of words. Many who have auditory processing disorder subconsciously develop visual coping strategies, such as lip reading, reading body language, and eye contact, to compensate for their auditory deficit, and these coping strategies are not available when using a telephone.\n\nAs noted above, the status of APD as a distinct disorder has been queried, especially by speech-language pathologists and psychologists, who note the overlap between clinical profiles of children diagnosed with APD and those with other forms of specific learning disability. Many audiologists, however, would dispute that APD is just an alternative label for dyslexia, SLI, or ADHD, noting that although it often co-occurs with these conditions, it can be found in isolation.\n\nTreatment of APD typically focuses on three primary areas: changing learning environment, developing higher-order skills to compensate for the disorder, and remediation of the auditory deficit itself. However, there is a lack of well-conducted evaluations of intervention using randomized controlled trial methodology. Most evidence for effectiveness adopts weaker standards of evidence, such as showing that performance improves after training. This does not control for possible influences of practice, maturation, or placebo effects. Recent research has shown that practice with basic auditory processing tasks (i.e. auditory training) may improve performance on auditory processing measures and phonemic awareness measures. Changes after auditory training have also been recorded at the physiological level. Many of these tasks are incorporated into computer-based auditory training programs such as Earobics and Fast ForWord, an adaptive software available at home and in clinics worldwide, but overall, evidence for effectiveness of these computerised interventions in improving language and literacy is not impressive. One small-scale uncontrolled study reported successful outcomes for children with APD using auditory training software.\n\nTreating additional issues related to APD can result in success. For example, treatment for phonological disorders (difficulty in speech) can result in success in terms of both the phonological disorder as well as APD. In one study, speech therapy improved auditory evoked potentials (a measure of brain activity in the auditory portions of the brain).\n\nWhile there is evidence that language training is effective for improving APD, there is no current research supporting the following APD treatments:\n\nHowever, use of a FM transmitter has been shown to produce significant improvements over time with children.\n\nThe first research into APD began in 1954 with Helmer Myklebust's study, \"Auditory Disorders in Children\". Myklebust's work suggested auditory processing disorder was separate from language learning difficulties. His work sparked interest in auditory deficits after acquired brain lesions affecting the temporal lobes and led to additional work looking at the physiological basis of auditory processing, but it was not until the late seventies and early eighties that research began on APD in depth.\nIn 1977, the first conference on the topic of APD was organized by Robert W. Keith, Ph.D. at the University of Cincinnati. The proceedings of that conference was published by Grune and Stratton under the title \"Central Auditory Dysfunction\" (Keith RW Ed.) That conference started a new series of studies focusing on APD in children. Virtually all tests currently used to diagnose APD originate from this work. These early researchers also invented many of the auditory training approaches, including interhemispheric transfer training and interaural intensity difference training. This period gave us a rough understanding of the causes and possible treatment options for APD.\nMuch of the work in the late nineties and 2000s has been looking to refining testing, developing more sophisticated treatment options, and looking for genetic risk factors for APD. Scientists have worked on improving behavioral tests of auditory function, neuroimaging, electroacoustic, and electrophysiologic testing. Working with new technology has led to a number of software programs for auditory training. With global awareness of mental disorders and increasing understanding of neuroscience, auditory processing is more in the public and academic consciousness than ever before.\n\n\n"}
{"id": "25991071", "url": "https://en.wikipedia.org/wiki?curid=25991071", "title": "Begging letter", "text": "Begging letter\n\nA begging letter is a letter to a rich person or organisation, usually written by a poor person, or a person claiming to be poor, begging for money or help.\n\nExamples of begging letters include a variant of the Nigerian 419 scam, where a letter is sent to a wealthy individual asking for financial assistance for orphaned children, emergency surgery, etc.\n\nThe May 1850 edition of \"Household Words\" contained an article entitled \"The Begging-Letter Writer\" written by the novelist Charles Dickens. In the article Dickens describes examples of the many begging letters he had received over the years, and the ruses employed by their writers to gain funds from the recipients.\n\nCharities such as Children International tell child sponsors not to release their mailing addresses because of the potential for being a target of begging letters. Instead, sponsors use Children International as a go between.\n\n"}
{"id": "38357394", "url": "https://en.wikipedia.org/wiki?curid=38357394", "title": "Budya language", "text": "Budya language\n\nBudya is a minor Bantu language. It is listed among Luban languages in Maho (2009).\n\nShona (Korekore) has a dialect of the same name (Budya/Budjga) in Zimbabwe.\n"}
{"id": "1074568", "url": "https://en.wikipedia.org/wiki?curid=1074568", "title": "Carlos Albizu University", "text": "Carlos Albizu University\n\nAlbizu University is a private, non-profit university offering undergraduate and graduate degrees in the fields of psychology, education, speech and language, criminal justice, ESOL, and human services. With the main campus in San Juan, Puerto Rico, a branch campus in Miami, Florida, and an additional instructional location in Mayagüez, Puerto Rico, the university provides professional training that is relevant and responsive to the mental health needs of multicultural communities and supports culturally sensitive research that contributes to and helps grow the professions of psychology, health, education, and human services.\n\nThe university’s history began in 1966, when a renowned Puerto Rican psychologist and educator, Dr. Carlos Albizu-Miranda, founded the Instituto Psicológico de Puerto Rico (Puerto Rico Institute of Psychology) in response to the need for culturally sensitive professional training in the area of clinical psychology. At the time, there were no graduate programs in clinical psychology in Puerto Rico. The University of Puerto Rico and the Normal School (later known as the College of Education) included psychology as part of the core curricula as early as 1903—but only for undergraduate studies that were heavily based on the American higher education system and its standards. Graduate-level degrees in psychology could only be obtained through schools in the United States.\n\nBy the early 1960s, little had changed, with mental health professionals being trained abroad and then returning to their home country with the challenge of adapting what they had learned in the United States to fit the sociocultural realities of a Hispanic community. Dr. Albizu-Miranda himself received his training at Purdue University in West Lafayette, Indiana, and incurred the same difficulties of incorporating what he had learned into the culture of the island upon returning home to Puerto Rico.\n\nWhat had changed, though, was the social climate of the island. Between 1947 and the late 1960s, Puerto Rico saw major industrialization that transformed the island from a rural agrarian to an urban industrial society. With this, came the establishment of social classes, modern capitalism, and economic consumption—along with confusion and insecurity that led to issues such as violence and drug use. By the 1960s, the population had grown to nearly three million people, with only five clinical psychologists on the island to support the growing need for social services. In an effort to meet the public’s demands, the government established what were known as “assistant psychologists.” These assistant psychologists for the most part had had little training and were not equipped to serve what was becoming an increasingly complex population. By 1964, Dr. Albizu-Miranda had begun envisioning a Puerto Rican-based graduate program that would address the need for multicultural professional training in psychology on the island, along with the need for greater numbers of clinically trained psychologists to serve the growing population.\n\nBetween 1964 and 1966, Dr. Albizu-Miranda held multiple meetings with the chancellor of the University of Puerto Rico to discuss the development of a graduate program in psychology. After two years of fruitless efforts, Dr. Albizu-Miranda decided to turn away from the state university and establish a self-governing and independent institution. On August 1, 1966, he incorporated the Puerto Rico Institute of Psychology and welcomed the Institute’s inaugural academic year in 1967-1968.\n\nRecognizing a parallel need for multicultural training in clinical psychology in Southern Florida, which has a large Hispanic population, Dr. Albizu-Miranda opened the Miami Institute of Psychology in Miami, Florida, in 1980. In January 2000, the two main campuses were merged under the shared name Carlos Albizu University in honor of their founder, becoming the first institution in North America to be named after a Hispanic. Today, they are commonly known as Albizu University and continue the tradition of offering programs, both in theory and practice, that stay true to addressing and honoring the multicultural heritages found in both Puerto Rico and South Florida.\n\nAlbizu University has two campuses; the main campus in Old San Juan, Puerto Rico and a branch campus in Miami, Florida. There is a third, instructional, location in Mayagüez, Puerto Rico.\n\nLocated in the heart of Puerto Rico’s capital city, the San Juan campus set a precedent as the first independent professional school of psychology in North America. Originally called the Puerto Rico Institute of Psychology, Dr. Albizu-Miranda modeled the university after institutes of psychology in Europe where practice and internship coexisted. Now, over 50 years since its founding, the San Juan campus has over 1,000 students and has expanded to include two on-campus training clinics: the Clínica de Salud Mental de la Comunidad (Community Mental Health Clinic) and the Clínica de Patología del Habla y Lenguaje (Speech and Language Pathology Clinic).\n\nThe Community Mental Health Clinic (CMHC) is a training clinic on the campus of Albizu University in San Juan, Puerto Rico. An active clinic that provides culturally sensitive mental health services to low income and minority clients in San Juan and surrounding communities, CMHC also serves as a practicum and internship site for Albizu University graduate students.\n\nBased at Albizu University’s San Juan campus, the Speech and Language Pathology Clinic provides culturally sensitive care in the areas of autism, dysphagia, motor-speech disorders, neurological impairments, hearing impairments, cognitive disabilities, and developmental disorders. Graduate students from the university’s M.S. in Speech and Language Pathology program work closely with licensed and certified pathologists to provide care to a diverse population while gaining clinical experience.\n\nLocated in the Miami, Florida, metropolitan area, the Miami campus offers over 30 undergraduate and graduate degree and certificate programs in the fields of psychology, education, speech and language, English as a second language (ESOL), criminal justice, and human services. Known as the “Gateway to Latin America and the Caribbean,” Miami has one of the largest multi-ethnic populations in the United States, thus providing the perfect environment to support Albizu University in its mission to offer culturally sensitive training to a diverse population in an area with a wide range of practical training sites.\n\nFounded in 1980, The Goodman Psychological Services Center (GPSC), was established with the purpose of providing quality, culturally sensitive, mental health services to low-income, minority, clients in Southern Florida. Sponsored by Albizu University, the Goodman Center is also a practicum site housing four training programs; the Psy.D. in Clinical Psychology program, the M.S. in Psychology program with a major in Marriage and Family Therapy, the M.S. in Speech and Language Pathology program, and the GPSC Doctoral Internship Consortium Program.\n\nSince its inception, the Goodman Center has provided care to nearly 10,000 people of all ages in communities that are typically underserved due to factors such as financial hardship, limited or no insurance coverage, and lack of proficiency in the English language. In addition to the mental health services provided at the center, the Goodman Center is contracted by Miami-Dade County Public Schools to conduct psychoeducational evaluations for both private referrals and a host of community agencies, including the Florida Department of Families and Children and other health centers. \n\nIn addition to partnering with the Miami-Dade County Public Schools, the Goodman Center has also established a close relationship with Camillus House, a long-standing community center in Miami-Dade County that provides humanitarian services to the poor and homeless. Through its Doctoral Internship Consortium Program, Albizu provides eight full-time internship spots per year—six at the Goodman Center and two at Camillus House—to clinical psychology doctoral students. In the past, these internships were available only to Albizu University students. This year, for the first time, the internships will be available on a national level to all graduate students who come from an APA-accredited clinical psychology doctoral program.\n\nIn January 2015, Albizu opened an extension in Mayagüez, Puerto Rico. Under the jurisdiction of the San Juan Campus, the location was created to provide academic offerings and clinical services to the western region of Puerto Rico. Known as the Mayagüez University Center, the facility is much smaller than the other two campuses, with degrees offered exclusively in the fields of psychology and speech and language pathology. With an on-site clinic, the Center is also able to offer students clinical practice while providing health services to the community.\n\nThe Mayagüez University Center is located in the heart of the city of Mayagüez. Known as an intellectual mecca, the city houses a number of museums, historic buildings, an array of colleges and universities, and a large student population. In recognition of the cultural and linguistic diversity found not only in Mayagüez but in Puerto Rico as a whole, the Center adopted both Spanish and English as official languages. Though students primarily speak Spanish, they are expected to have adequate reading, writing, and conversational skills in English as well. On occasion, if enough students express interest, teachers will instruct entire classes in English.\n\nRanked as one of the nation’s top Hispanic-serving institutions specializing in human behavior science, Albizu attracts a student body committed to preparing themselves to work in community-oriented fields. With curriculums carefully designed to promote cultural sensitivity in today’s diverse society, students are able to choose from over 30 degree and certificate programs, with the largest number of offerings being in the field of psychology.\n\nOver half of the programs offered at Albizu are for degrees in psychology, and students are able to select from certificate, undergraduate, graduate, and doctoral programs with a wide range of specializations that include marriage and family therapy, mental health counseling, school counseling, industrial/organizational psychology, child psychology, and clinical psychology.\n\nWhen Albizu was founded in the late 1960s, Dr. Albizu-Miranda’s blueprint was to create a school offering a master’s degree in clinical psychology that would foster the development of therapeutic processes sensitive to cultural and class differences. At first, there were only 20 students and a handful of teachers committed to the pioneer institution. Over time, the clinical psychology program gained momentum and became the inaugural program at all three locations. To this day, it continues to be the most extensive program that Albizu offers.\n\nToday, all of the psychology programs emphasize cultural and ethnic sensitivity, in recognition of Albizu’s origins as an institution built on the belief that a multicultural society is best served by professionals who are culturally competent and inclusive. In the classroom, where a majority of the professors are practicing mental health professionals, students are taught the intricacies of a multicultural society from the real-world experience and examples provided by their professors. Outside of the classroom, students gain hands-on exposure at over 100 practicum sites that between them offer experience with a wide range of psychological disorders and clinical environments.\n\nAlbizu University has numerous clubs and organizations to encourage student involvement in larger communities. Opportunities range from organizations exclusive to Albizu, such as the Student Council, to associations that operate on a national or international level and welcome Albizu students to participate as campus representatives.\n\nOf the clubs and organizations offered to Albizu students, there is a fairly even balance between those hosted by the university and those that represent outside associations. Of the outside associations, some of the most notable with chapters at Albizu include The American Psychological Association of Graduate Students (APAGS), The Society for Military Psychology, and The International Honor Society in Psychology: Psi Chi. All offer students the opportunity to have a voice on a national to international level and to advocate on behalf of the science and practice of psychology through events including panel discussions, fundraisers, and walks to raise awareness for different causes such as suicide amongst veterans.\n\n\n"}
{"id": "40842674", "url": "https://en.wikipedia.org/wiki?curid=40842674", "title": "Chinese Characters Dictation Competition", "text": "Chinese Characters Dictation Competition\n\nChinese Characters Dictation Competition () is a weekly television program where contestants write Chinese characters after hearing the words. The show now broadcasts on CCTV 1.\n\nThe show was inspired by spelling bees in the United States.\n"}
{"id": "11517229", "url": "https://en.wikipedia.org/wiki?curid=11517229", "title": "Closed-ended question", "text": "Closed-ended question\n\nA closed-ended question refers to any question for which a researcher provides research participants with options from which to choose a response. Open-ended questions are sometimes phrased as a statement which requires a response.\n\nA closed-ended question contrasts with an open-ended question, which cannot easily be answered with specific information.\n\nExamples of close-ended questions which may elicit a \"yes\" or \"no\" response include:\n\nSimilarly, variants of the above close-ended questions which possess specific responses are:\n\nAt the same time, there are closed-ended questions which are sometimes impossible to answer correctly with a yes or no without confusion, for example: \"Have you stopped taking heroin?\" (if you never took it) or \"Who told you to take heroin?\"; see \"loaded question\".\n\nA study by the University of Cincinnati found 20 to 40 percent of Americans will provide an opinion when they do not have one because of social pressure, using context clues to select an answer they believe will please the questioner. A classic example of this phenomenon was the 1947 study of the fictional Metallic Metals Act.\n\nSome in the field of education argue that closed-ended questions are broadly speaking \"bad\" questions. They are questions that are often asked to obtain a specific answer and are therefore good for testing knowledge. It is often argued that open-ended questions (i.e. questions that elicit more than a yes/no answers) are preferable because they open up discussion and enquiry.\n\nPeter Worley argues that this is a false assumption. This is based on Worley’s central arguments that there are two different kinds of open and closed questions: grammatical and conceptual. He argues that educational practitioners should be aiming for questions that are \"grammatically closed, but conceptually open\". For example, in standard parlance, \"is it ever right to lie?\" would be regarded as a closed question: it elicits a yes–no response. Significantly, however, it is conceptually open. Any initial yes–no answer to it can be \"opened up\" by the questioner (\"why do you think that?\", \"Could there be an instance where that's not the case?), inviting elaboration and enquiry.\n\nThis grammatically closed but cognitively open style of questioning, Worley argues, \"gives [educators] the best of both worlds: the focus and specificity of a closed question (this, after all, is why teachers use them) and the inviting, elaborating character of an open question\". Closed questions, simply require \"opening up\" strategies to ensure that conceptually open questions can fulfil their educational potential.\n\nWorley's structural and semantic distinction between open and closed questions is integral to his pedagogical invention \"Open Questioning Mindset\" (OQM). OQM refers to the development, in educators, of an open attitude towards the process of learning and the questioning at the heart of that process. It is a mind-set that is applicable to all subject areas and all pedagogical environments. Teachers who develop an Open Questioning Mindset listen openly for the cognitive content of student's contributions and looks for ways to use what is given for learning opportunities, whether right, wrong, relevant or apparently irrelevant. OQM encourages a style of pedagogy that values genuine enquiry in the classroom. It provides teachers with the tools to move beyond what Worley calls \"guess what's in my head\" teaching, that relies on closed and leading questions.\n\n"}
{"id": "232905", "url": "https://en.wikipedia.org/wiki?curid=232905", "title": "Code-switching", "text": "Code-switching\n\nIn linguistics, code-switching occurs when a speaker alternates between two or more languages, or language varieties, in the context of a single conversation. Multilinguals, speakers of more than one language, sometimes use elements of multiple languages when conversing with each other. Thus, code-switching is the use of more than one linguistic variety in a manner consistent with the syntax and phonology of each variety.\n\nCode-switching is distinct from other language contact phenomena, such as borrowing, pidgins and creoles, loan translation (calques), and language transfer (language interference). Borrowing affects the lexicon, the words that make up a language, while code-switching takes place in individual utterances. Speakers form and establish a pidgin language when two or more speakers who do not speak a common language form an intermediate, third language. On the other hand, speakers practice code-switching when they are each fluent in both languages. Code mixing is a thematically related term, but the usage of the terms \"code-switching\" and \"code-mixing\" varies. Some scholars use either term to denote the same practice, while others apply \"code-mixing\" to denote the formal linguistic properties of language-contact phenomena and \"code-switching\" to denote the actual, spoken usages by multilingual persons.\n\nIn the 1940s and the 1950s, many scholars considered code-switching to be a substandard use of language. Since the 1980s, however, most scholars have come to regard it as a normal, natural product of bilingual and multilingual language use.\n\nThe term \"code-switching\" is also used outside the field of linguistics. Some scholars of literature use the term to describe literary styles that include elements from more than one language, as in novels by Chinese-American, Anglo-Indian, or Latino writers. In popular usage, \"code-switching\" is sometimes used to refer to relatively stable informal mixtures of two languages, such as Spanglish, Taglish, or Hinglish. Both in popular usage and in sociolinguistic study, the name code-switching is sometimes used to refer to switching among dialects, styles or registers. This form of switching is practiced, for example, by speakers of African American Vernacular English as they move from less formal to more formal settings. Such shifts, when performed by public figures such as politicians, are sometimes criticized as signalling inauthenticity or insincerity.\n\nCode-switching relates to, and sometimes indexes social-group membership in bilingual and multilingual communities. Some sociolinguists describe the relationships between code-switching behaviours and class, ethnicity, and other social positions.\nIn addition, scholars in interactional linguistics and conversation analysis have studied code-switching as a means of structuring speech in interaction. Some discourse analysts, including conversation analyst Peter Auer, suggest that code-switching does not simply reflect social situations, but that it is a means to create social situations.\n\nThe Markedness Model, developed by Carol Myers-Scotton, is one of the more complete theories of code-switching motivations. It posits that language users are rational and choose to speak a language that clearly marks their rights and obligations, relative to other speakers, in the conversation and its setting. When there is no clear, unmarked language choice, speakers practice code-switching to explore possible language choices. Many sociolinguists, however, object to the Markedness Model’s postulation that language-choice is entirely rational.\n\nScholars of conversation analysis such as Peter Auer and Li Wei argue that the social motivation behind code-switching lies in the way code-switching is structured and managed in conversational interaction; in other words, the question of why code-switching occurs cannot be answered without first addressing the question of how it occurs. Using conversation analysis (CA), these scholars focus their attention on the sequential implications of code-switching. That is, whatever language a speaker chooses to use for a conversational turn, or part of a turn, impacts the subsequent choices of language by the speaker as well as the hearer. Rather than focusing on the social values inherent in the languages the speaker chooses (\"brought-along meaning\"), the analysis concentrates on the meaning that the act of code-switching itself creates (\"brought-about meaning\").\n\nThe communication accommodation theory (CAT), developed by Howard Giles, professor of communication at the University of California, Santa Barbara, seeks to explain the cognitive reasons for code-switching, and other changes in speech, as a person either emphasizes or minimizes the social differences between himself and the other person(s) in conversation. Giles posits that when speakers seek approval in a social situation they are likely to converge their speech with that of the other speaker. This can include, but is not limited to, the language of choice, accent, dialect, and para-linguistic features used in the conversation. In contrast to convergence, speakers might also engage in divergent speech, in which an individual person emphasizes the social distance between himself and other speakers by using speech with linguistic features characteristic of his own group.\n\nIn a diglossic situation, some topics and situations are better suited to the use of one language over another. Joshua Fishman proposes a domain-specific code-switching model (later refined by Blom and Gumperz) wherein bilingual speakers choose which code to speak depending on where they are and what they are discussing. For example, a child who is a bilingual Spanish-English speaker might speak Spanish at home and English in class, but Spanish at recess.\n\nScholars use different names for various types of code-switching.\n\n\nMost code-switching studies primarily focus on intra-sentential switching, as it creates many hybrid grammar structures that require explanation. The other types involve utterances that simply follow the grammar of one language or the other. Intra-sentential switching can be alternational or insertional. In alternational code-switching, a new grammar emerges that is a combination of the grammars of the two languages involved. Insertional code-switching involves \"the insertion of elements from one language into the morphosyntactic frame of the other.\"\n\nThere are several reasons to switch codes in a single conversation.\n\nIn studying the syntactic and morphological patterns of language alternation, linguists have postulated specific grammatical rules and specific syntactic boundaries for where code-switching might occur.\n\nShana Poplack's model of code-switching is the best known theory of the underlying grammar of code-switching. In this model, code-switching is subject to two constraints. The \"free-morpheme constraint\" stipulates that code-switching cannot occur between a lexical stem and bound morphemes. Essentially, this constraint distinguishes code-switching from borrowing. Generally, borrowing occurs in the lexicon, while code-switching occurs at either the syntax level or the utterance-construction level. The \"equivalence constraint\" predicts that switches occur only at points where the surface structures of the languages coincide, or between sentence elements that are normally ordered in the same way by each individual grammar. For example, the sentence: \"I like you \"porque eres simpático\"\" (\"I like you \"because you are nice\"\") is allowed because it obeys the syntactic rules of both Spanish and English. Cases like the noun phrases \"the casa white\" and \"the blanca house\" are ruled out because the combinations are ungrammatical in at least one of the languages involved. Spanish noun phrases are made up of determiners, then nouns, then adjectives, while the adjectives come before the nouns in English noun phrases. \"The casa white\" is ruled out by the equivalence constraint because it does not obey the syntactic rules of English, and \"the blanca house\" is ruled out because it does not follow the syntactic rules of Spanish.\n\nCritics cite weaknesses of Sankoff and Poplack's model. The free-morpheme and equivalence constraints are insufficiently restrictive, meaning there are numerous exceptions that occur. For example, the free morpheme constraint does not account for why switching is impossible between certain free morphemes. The sentence: \"The students had \"visto la película italiana\"\" (\"The students had \"seen the Italian movie\"\") does not occur in Spanish-English code-switching, yet the free-morpheme constraint would seem to posit that it can. The equivalence constraint would also rule out switches that occur commonly in languages, as when Hindi postpositional phrases are switched with English prepositional phrases like in the sentence: \"John gave a book \"ek larakii ko\"\" (\"John gave a book \"to a girl\"\"). The phrase \"ek larakii ko\" is literally translated as \"a girl to,\" making it ungrammatical in English, and yet this is a sentence that occurs in English-Hindi code-switching despite the requirements of the equivalence constraint. The Sankoff and Poplack model only identifies points at which switching is blocked, as opposed to explaining which constituents can be switched and why.\n\nCarol Myers-Scotton's Matrix Language-Frame (MLF) model is the dominant model of insertional code-switching. The MLF model posits that there is a Matrix Language (ML) and an Embedded Language (EL). In this case, elements of the Embedded Language are inserted into the morphosyntactic frame of the Matrix Language. The hypotheses are as follows (Myers-Scotton 1993b: 7):\n\nThe Matrix Language Hypothesis states that those grammatical procedures in the central structure in the language production system which account for the surface structure of the Matrix Language + Embedded Language constituent (linguistics) are only Matrix Language–based procedures. Further, the hypothesis is intended to imply that frame-building precedes content morpheme insertion. A Matrix Language can be the first language of the speaker or the language in which the morphemes or words are more frequently used in speech, so the dominant language is the Matrix Language and the other is the Embedded Language. A Matrix Language island is a constituent composed entirely of Matrix Language morphemes.\n\nAccording to the Blocking Hypothesis, in Matrix Language + Embedded Language constituents, a blocking filter blocks any Embedded Language content morpheme which is not congruent with the Matrix Language with respect to three levels of abstraction regarding subcategorization. \"Congruence\" is used in the sense that two entities, linguistic categories in this case, are congruent if they correspond in respect of relevant qualities.\n\nThe three levels of abstraction are:\n\n\nWe see that example 1 is consistent with the Blocking Hypothesis and the system content morpheme criteria, so the prediction is that the Hindi equivalents are also content morphemes. Sometimes non-congruence between counterparts in the Matrix Language and Embedded Language can be circumvented by accessing bare forms. \"Cell\" is a bare form and so the thematic role of \"cell\" is assigned by the verb \"-wek-\" 'put in/on'; this means that the verb is a content morpheme.\n\nThe Embedded Language Island Trigger Hypothesis states that when an Embedded Language morpheme appears which is not permitted under either the Matrix Language Hypothesis or Blocking Hypothesis, it triggers the inhibition of all Matrix Language accessing procedures and completes the current constituent as an Embedded Language island. Embedded Language islands consist only of Embedded Language morphemes and are well-formed by Embedded Language grammar, but they are inserted in the Matrix Language frame. Therefore, Embedded Language islands are under the constraint of Matrix Language grammar.\n\n\nExample 1 is ungrammatical (indicated by the leading asterisk) because \"your\" is accessed, so the Embedded Language Island Trigger Hypothesis predicts that it must be followed by an English head (e.g., \"your letter\") as an Embedded Language island. The reason is that possessive adjectives are system morphemes. We see the same thing happen in example 2, which is therefore ungrammatical. However, the correct way to finish the sentence is not \"for wewe\", switching back to Swahili; rather, it should end with \"for you\", which would be an Embedded Language island.\n\nThe Embedded Language Implicational Hierarchy Hypothesis can be stated as two sub-hypotheses: \n\nThe Implication Hierarchy of Embedded Language Islands:\n\n\nWe see example 1 work because the French Embedded Language island \"Le matin de bonne heure\", \"early in the morning\", is a time expression. (Also, it is repeated in Wolof in the second sentence.) In example 2, we see the quantifier \"a lot of\" is a predicted Embedded Language island. Here we see an objective complement of a finite verb begin with the quantifier.\n\nJeff MacSwan has posited a \"constraint-free approach\" to analyzing code-switching. This approach views explicit reference to code-switching in grammatical analysis as tautological, and seeks to explain specific instances of grammaticality in terms of the unique contributions of the grammatical properties of the languages involved. MacSwan characterizes the approach with the refrain, \"Nothing constrains code-switching apart from the requirements of the mixed grammars.\" The approach focuses on the repudiation of any rule or principle which explicitly refers to code-switching itself. This approach does not recognize or accept terms such as \"matrix language\", \"embedded language\", or \"language frame\", which are typical in constraint-based approaches such as the MLF Model.\n\nRather than posit constraints specific to language alternation, as in traditional work in the field, MacSwan advocates that mixed utterances be analyzed with a focus on the specific and unique linguistic contributions of each language found in a mixed utterance. Because these analyses draw on the full range of linguistic theory, and each data set presents its own unique challenges, a much broader understanding of linguistics is generally needed to understand and participate in this style of codeswitching research.\n\nFor example, Cantone and MacSwan (2009) analyzed word order differences for nouns and adjectives in Italian-German codeswitching using a typological theory of Cinque that had been independently proposed in the syntax literature; their account derives the word order facts of Italian-German codeswitching from underlying differences between the two languages, according to Cinque's theory.\n\nMuch remains to be done before a more complete understanding of code-switching phenomena is achieved. Linguists continue to debate apparent counter-examples to proposed code-switching theories and constraints.\n\nThe \"Closed-class Constraint\", developed by Aravind Joshi, posits that closed class items (pronouns, prepositions, conjunctions, etc.) cannot be switched. The \"Functional Head Constraint\" developed by Belazi et al. holds that code-switching cannot occur between a functional head (a complementizer, a determiner, an inflection, etc.) and its complement (sentence, noun-phrase, verb-phrase). These constraints, among others like the Matrix Language-Frame model, are controversial among linguists positing alternative theories, as they are seen to claim universality and make general predictions based upon specific presumptions about the nature of syntax.\n\nMyers-Scotton and MacSwan debated the relative merits of their approaches in a series of exchanges published in 2005 in \"\", issues 8(1) and 8(2).\n\nIn this section, segments that are switched from the primary language of the conversation are shown in red.\n\nResearcher Ana Celia Zentella offers this example from her work with Puerto Rican Spanish-English bilingual speakers in New York City. In this example, and her younger sister, , speak Spanish and English with outside of their apartment building.\n\nZentella explains that the children of the predominantly Puerto Rican neighbourhood speak both English and Spanish: \"Within the children’s network, English predominated, but code-switching from English to Spanish occurred once every three minutes, on average.\"\n\nThis example of switching from French to Tamil comes from ethnographer Sonia Das's work with immigrants from Jaffna, Sri Lanka, to Quebec.\n\n, who moved from Sri Lanka to Quebec as a child and now identifies as Québécois, speaks to Das in French. When Selvamani's sister, Mala, laughs, switches to Tamil to ask Mala why she is laughing. After this aside, continues to speak in French. also uses the word \"tsé\" (\"you know\", contraction of \"tu sais\") and the expression \"je me pas poigné\" (\"I will not be handled\"), which are not standard French but are typical of the working-class Montreal dialect Joual.\n\nResearcher Paul Kroskrity offers the following example of code-switching by , who are trilingual in Tewa, Hopi, and English. They are discussing the selection of a site for a new high school in the eastern Hopi Reservation:\n\nIn their two-hour conversation, the primarily speak Tewa; however, when addresses the Hopi Reservation as a whole, he code-switches to Hopi. His speaking Hopi when talking of Hopi-related matters is a conversational norm in the Arizona Tewa speech community. Kroskrity reports that these Arizona Tewa men, who culturally identify themselves as Hopi \"and\" Tewa, use the different languages to linguistically construct and maintain their discrete ethnic identities.\n"}
{"id": "29819979", "url": "https://en.wikipedia.org/wiki?curid=29819979", "title": "Cognitive hearing science", "text": "Cognitive hearing science\n\nCognitive hearing science is an interdisciplinary science field concerned with the physiological and cognitive basis of hearing and its interplay with signal processing in hearing aids. The field includes genetics, physiology, medical and technical audiology, cognitive neuroscience, cognitive psychology, linguistics and social psychology.\n\nTheoretically the research in cognitive hearing science combines a physiological model for the information transfer from the outer auditory organ to the auditory cerebral cortex, and a cognitive model for how language comprehension is influenced by the interplay between the incoming language signal and the individual's cognitive skills, especially the long-term memory and the working memory.\n\nResearchers examine the interplay between type of hearing impairment or deafness, type of signal processing in different hearing aids, type of listening environment and the individual's cognitive skills.\n\nResearch in cognitive hearing science has importance for the knowledge about different types of hearing impairment and its effects, as for the possibilities to determine which individuals can make use of certain type of signal processing in hearing aid or cochlear implant and thereby adapt hearing aid to the individual.\n\nCognitive hearing science has been introduced by researchers at the Linköping University research centre Linnaeus Centre HEAD (HEaring And Deafness) in Sweden, created in 2008 with a major 10-year grant from the Swedish Research Council.\n\n"}
{"id": "514932", "url": "https://en.wikipedia.org/wiki?curid=514932", "title": "Cursive", "text": "Cursive\n\nCursive (also known as script or longhand, among other names) is any style of penmanship in which some characters are written joined together in a flowing manner, generally for the purpose of making writing faster. Formal cursive is generally joined, but casual cursive is a combination of joins and pen lifts. The writing style can be further divided as \"looped\", \"italic\" or \"connected\".\n\nThe cursive method is used with a number of alphabets due to its improved writing speed and infrequent pen lifting. In some alphabets, many or all letters in a word are connected, sometimes making a word one single complex stroke.\n\nCursive is a style of penmanship in which the symbols of the language are written in a conjoined and/or \"flowing\" manner, generally for the purpose of making writing faster. This writing style is distinct from \"printscript\" using block letters, in which the letters of a word are unconnected and in Roman/Gothic letterform rather than joined-up script. Not all cursive copybooks join all letters: formal cursive is generally joined, but casual cursive is a combination of joins and pen lifts. In the Arabic, Syriac, Latin, and Cyrillic alphabets, many or all letters in a word are connected, sometimes making a word one single complex stroke. In Hebrew cursive and Roman cursive, the letters are not connected. In Maharashtra there is a version of Cursive called 'Modi'\n\nLigature is writing the letters of words with lines connecting the letters so that one does not have to pick up the pen or pencil between letters. Commonly some of the letters are written in a looped manner to facilitate the connections. In common printed Greek texts, the modern small letter fonts are called \"cursive\" (as opposed to uncial) though the letters do not connect.\n\nIn \"looped cursive\" penmanship, some ascenders and descenders have loops which provide for joins. This is generally what people refer to when they say \"cursive\".\n\n\"Cursive italic\" penmanship—derived from chancery cursive—uses non-looped joins or no joins. In italic cursive, there are no joins from g, j, q or y, and a few other joins are discouraged. Italic penmanship became popular in the 15th-century Italian Renaissance. The term \"italic\" as it relates to handwriting is not to be confused with italic typed letters that slant forward. Many, but not all, letters in the handwriting of the Renaissance were joined, as most are today in cursive italic.\n\nThe origins of the cursive method are associated with practical advantages of writing speed and infrequent pen-lifting to accommodate the limitations of the quill. Quills are fragile, easily broken, and will spatter unless used properly. Steel dip pens followed quills; they were sturdier, but still had some limitations. The individuality of the provenance of a document (see Signature) was a factor also, as opposed to machine font.\nCursive was also favored because the writing tool was rarely taken off the paper.\nThe term \"cursive\" derives from the 18th century Italian \"corsivo\" from Medieval Latin \"cursivus\", which literally means \"running\". This term in turn derives from Latin \"currere\" (\"to run, hasten\").\n\nIn Bengali cursive script\n\n(also known in Bengali as \"professional writing\") the letters are more likely to be more curvy in appearance than in standard Bengali handwriting. Also, the horizontal supporting bar on each letter (\"matra\") runs continuously through the entire word, unlike in standard handwriting. This cursive handwriting often used by literature experts differs in appearance from the standard Bengali alphabet as it is free hand writing, where sometimes the alphabets are complex and appear different from the standard handwriting.\n\"Roman cursive\" is a form of handwriting (or a script) used in ancient Rome and to some extent into the Middle Ages. It is customarily divided into old (or ancient) cursive, and new cursive. Old Roman cursive, also called majuscule cursive and capitalis cursive, was the everyday form of handwriting used for writing letters, by merchants writing business accounts, by schoolchildren learning the Latin alphabet, and even by emperors issuing commands. New Roman, also called minuscule cursive or later cursive, developed from old cursive. It was used from approximately the 3rd century to the 7th century, and uses letter forms that are more recognizable to modern eyes; \"a\", \"b\", \"d\", and \"e\" have taken a more familiar shape, and the other letters are proportionate to each other rather than varying wildly in size and placement on a line.\n\nThe Greek alphabet has had several cursive forms in the course of its development. In antiquity, a cursive form of handwriting was used in writing on papyrus. It employed slanted and partly connected letter forms as well as many ligatures. Some features of this handwriting were later adopted into Greek minuscule, the dominant form of handwriting in the medieval and early modern era. In the 19th and 20th centuries, an entirely new form of cursive Greek, more similar to contemporary Western European cursive scripts, was developed.\n\nDuring the Middle Ages, the flowing, connected cursive script of the Arabic language inspired Western Christian scholars to develop similar cursive scripts for Latin. These scripts then became the basis for all of the Latin-based cursive scripts used in Europe.\n\nCursive writing was used in English before the Norman conquest. Anglo-Saxon Charters typically include a boundary clause written in Old English in a cursive script. A cursive handwriting style—secretary hand—was widely used for both personal correspondence and official documents in England from early in the 16th century.\n\nCursive handwriting developed into something approximating its current form from the 17th century, but its use was neither uniform, nor standardized either in England itself or elsewhere in the British Empire. In the English colonies of the early 17th century, most of the letters are clearly separated in the handwriting of William Bradford, though a few were joined as in a cursive hand. In England itself, Edward Cocker had begun to introduce a version of the French \"ronde\" style, which was then further developed and popularized throughout the British Empire in the 17th and 18th centuries as round hand by John Ayers and William Banson.\n\nIn the American colonies, on the eve of their independence from the Kingdom of Great Britain, it is notable that Thomas Jefferson joined most, but not all the letters when drafting the United States Declaration of Independence. However, a few days later, Timothy Matlack professionally re-wrote the presentation copy of the Declaration in a fully joined, cursive hand. Eighty-seven years later, in the middle of the 19th century, Abraham Lincoln drafted the Gettysburg Address in a cursive hand that would not look out of place today.\n\nNote that not all such cursive, then or now, joined all of the letters within a word.\nIn both the British Empire and the United States in the 18th and 19th centuries, before the typewriter, professionals used cursive for their correspondence. This was called a \"fair hand\", meaning it looked good, and firms trained their clerks to write in exactly the same script.\n\nIn the early days of the post office, letters were written in cursive – and to fit more text on a single sheet, the text was continued in lines crossing at 90 degrees from the original text. Block letters were not suitable for this.\n\nAlthough women's handwriting had noticeably different particulars from men's, the general forms were not prone to rapid change. In the mid-19th century, most children were taught the contemporary cursive; in the United States, this usually occurred in second or third grade (around ages seven to nine). Few simplifications appeared as the middle of the 20th century approached.\n\nAfter the 1960s, a movement originally begun by Paul Standard in the 1930s to replace looped cursive with cursive italic penmanship resurfaced. It was motivated by the claim that cursive instruction was more difficult than it needed to be: that conventional (looped) cursive was unnecessary, and it was easier to write in cursive italic. Because of this, a number of various new forms of cursive italic appeared, including Getty-Dubay, and Barchowsky Fluent Handwriting. In the 21st century, some of the surviving cursive writing styles are Spencerian, Palmer Method, D'Nealian, and Zaner-Bloser script.\n\nOne of the earliest forms of new technology that caused the decline of handwriting was the invention of the ballpoint pen, patented in 1888 by John Loud. Two brothers, László and György Bíró, further developed the pen by changing the design and using different ink that dried quickly. With their design, it was guaranteed that the ink would not smudge, as it would with the earlier design of pen, and it no longer required the careful penmanship one would use with the older design of pen. After World War II, the ballpoint pen was mass-produced and sold for a cheap price, changing the way people wrote. Over time the emphasis of using the style of cursive to write slowly declined, only to be later impacted by other technologies such as the phone, computer, and keyboard.\nCursive has been in decline throughout the 21st century due to its perceived lack of necessity. The Fairfax Education Association, the largest teachers' union in Fairfax County, Virginia, has called cursive a \"dying art\". Many consider cursive too tedious to learn and believe that it is not a useful skill.\n\nOn the 2006 SAT, a United States post-secondary education entrance exam, only 15 percent of the students wrote their essay answers in cursive. However, students might be discouraged from using cursive on standardized tests due to exams written in hard to read handwriting receive less marks, and some graders may have difficulties reading cursive.\n\nIn a 2007 survey of 200 teachers of first through third grades in all 50 American states, 90 percent of respondents said their schools required the teaching of cursive.\n\nA 2008 nationwide survey found elementary school teachers lacking formal training in teaching handwriting to students. Only 12 percent of teachers reported having taken a course in how to teach it.\n\nIn 2012, the American states of Indiana and Hawaii announced that their schools will no longer be required to teach cursive (but will still be permitted to), and instead will be required to teach \"keyboard proficiency\". Since the nationwide proposal of the Common Core State Standards in 2009, which do not include instruction in cursive, the standards have been adopted by 44 states as of July 2011, all of which have debated whether to augment them with cursive.\n\nMany historical documents, such as the United States Constitution, are written in cursive—the inability to read cursive therefore precludes one from being able to fully appreciate such documents in their original format. Despite the decline in the day-to-day use of cursive, it is being reintroduced to the curriculum of schools in the United States. States such as California, Idaho, Kansas, Massachusetts, North Carolina, South Carolina, New Jersey, and Tennessee have already mandated cursive in schools as a part of the Back to Basics program designed to maintain the integrity of cursive handwriting. Cursive instruction is required by grade 5 in Illinois, starting with the 2018-2019 school year. Some argue that cursive is not worth teaching in schools and \"in the 1960s cursive was implemented because of preference and not an educational basis; Hawaii and Indiana have replaced cursive instruction with 'keyboard proficiency' and 44 other states are currently weighing similar measures.\"\n\nWith the widespread use of computers, researchers set out to test the effectiveness of both mediums. In a study done by Pam Mueller which compared scores of students who took notes by hand and via laptop computer showed that students who took notes by hand showed advantages in both factual and conceptual learning. Another study done by Anne Mangen showed that children showed an acceleration in learning new words when they wrote them by hand rather than on a computer screen. Learning to write in cursive is alleged (by its practitioners) to be a stepping stone to developing neat handwriting, and, in a third study conducted by Florida International University, professor Laura Dinehart concluded that students with neater handwriting tend to develop better reading and writing skills, though it is difficult to conclude causation from such an association. Aside from these cognitive benefits, students with dyslexia, who have difficulty learning to read because their brains have difficulty associating sounds and letter combinations efficiently, have found that cursive can help them with the decoding process because it integrates hand-eye coordination, fine motor skills and other brain and memory functions.. However, students with dysgraphia may be badly served, even substantially hindered, by demands for cursive.\n\nUp to the 19th century, Kurrent (also known as \"German cursive\") was used in German language longhand. Kurrent was not used exclusively, but in parallel to modern cursive (which is the same as English cursive). Writers used both cursive styles: location, contents and context of the text determined which style to use. A successor of Kurrent, Sütterlin, was widely used in the period 1911-1941 until the Nazi Party banned it, and German speakers brought up with Sütterlin continued to use it well into the post-war period.\n\nToday, three different styles of cursive writing are taught in German schools, the (introduced in 1953), the (1968), and the (1969). The German National Primary Schoolteachers' Union has proposed replacing all three with Grundschrift, a simplified form of non-cursive handwriting adopted by Hamburg schools.\n\nThe \"Russian Cursive Cyrillic\" alphabet is used (instead of the block letters) when handwriting the modern Russian language. While several letters resemble Latin counterparts, many of them represent different sounds. Most handwritten Russian, especially personal letters and schoolwork, uses the cursive Russian (Cyrillic) alphabet. Most children in Russian schools are taught in the 1st grade how to write using this Russian script.\n\nCursive forms of Chinese characters are used in calligraphy; \"running script\" is the semi-cursive form and \"rough script\" (mistakenly called \"grass script\" due to misinterpretation) is the cursive. The running aspect of this script has more to do with the formation and connectedness of strokes \"within\" an individual character than with connections between characters as in Western connected cursive. The latter are rare in hanzi and in the derived Japanese kanji characters which are usually well separated by the writer.\n\n"}
{"id": "491865", "url": "https://en.wikipedia.org/wiki?curid=491865", "title": "Dichotomy", "text": "Dichotomy\n\nA dichotomy is a partition of a whole (or a set) into two parts (subsets). In other words, this couple of parts must be\n\nSuch a partition is also frequently called a bipartition.\nThe two parts thus formed are complements. In logic, the partitions are opposites if there exists a proposition such that it holds over one and not the other.\n\nTreating continuous variables or multicategorical variables as binary variables is called dichotomization. The discretization error inherent in dichotomization is temporarily ignored for modeling purposes.\n\nThe term \"dichotomy\" is from the Greek language \"dichotomía\" \"dividing in two\" from δίχα \"dícha\" \"in two, asunder\" and τομή \"tomḗ\" \"a cutting, incision\".\n\n\n\n"}
{"id": "26408628", "url": "https://en.wikipedia.org/wiki?curid=26408628", "title": "Dictionary writing system", "text": "Dictionary writing system\n\nA dictionary writing system (DWS), or dictionary production/publishing system (DPS) is a piece of software for writing and producing a dictionary, glossary, vocabulary, thesaurus etc. It might include an editor, a database, a web interface for collaborative work and various management tools.\n\n\n"}
{"id": "303486", "url": "https://en.wikipedia.org/wiki?curid=303486", "title": "Ethnolinguistics", "text": "Ethnolinguistics\n\nEthnolinguistics (sometimes called cultural linguistics) is a field of linguistics that studies the relationship between language and culture and how different ethnic groups perceive the world. It is the combination between ethnology and linguistics. The former refers to the way of life of an entire community: all the characteristics that distinguish one community from the other. Such characteristics make the cultural aspects of a community or a society.\n\nEthnolinguists study the way perception and conceptualization influences language and show how that is linked to different cultures and societies. An example is how spatial orientation is expressed in various cultures. In many societies, words for the cardinal directions \"east\" and \"west\" are derived from terms for sunrise/sunset. The nomenclature for cardinal directions of Inuit speakers of Greenland, however, is based on geographical landmarks such as the river system and one's position on the coast. Similarly, the Yurok lack the idea of cardinal directions; they orient themselves with respect to their principal geographic feature, the Klamath River.\n\nCultural Linguistics is a related branch of linguistics that explores the relationship between language and cultural conceptualisations. Cultural Linguistics draws on and expands the theoretical and analytical advancements in cognitive science (including complexity science and distributed cognition) and anthropology. Cultural linguistics examines how various features of human languages encode cultural conceptualisations, including cultural schemas, cultural categories, and cultural metaphors. In , language is viewed as deeply entrenched in the group-level, cultural cognition of communities of speakers. Thus far, the approach of Cultural Linguistics has been adopted in several areas of applied linguistic research, including intercultural communication, second language learning, Teaching English as an International Language, and World Englishes.\n\n\n"}
{"id": "34008425", "url": "https://en.wikipedia.org/wiki?curid=34008425", "title": "European Association for the Teaching of Academic Writing", "text": "European Association for the Teaching of Academic Writing\n\nThe European Association for the Teaching of Academic Writing (EATAW) is an academic association supporting scholarly activity in academic writing. The association was first established in 1999 with the first conference being held in 2001. The Europe-wide association has three main activities: a bi-annual conference, an Internet forum and \"the Journal of Academic Writing\".\n\nThe EATAW conference is held every two years in a European University. It was first held in 2001 in Groningen. The occasion of the bi-annual conference is when the EATAW board is elected for a term of two years.\n\n\"The Journal of Academic Writing\" is a peer reviewed journal established by EATAW.\n\n\n"}
{"id": "10526158", "url": "https://en.wikipedia.org/wiki?curid=10526158", "title": "Fossil word", "text": "Fossil word\n\nA fossil word is a word that is broadly obsolete but remains in current use due to its presence within an idiom.\n\nFossil status can also occur for word senses and for phrases. An example for a word sense is 'navy' in 'merchant navy', which means 'commercial fleet' (although that sense of navy is obsolete elsewhere). An example for a phrase is 'in point' (relevant), which is retained in the larger phrases 'case in point' (also 'case on point' in the legal context) and 'in point of fact', but is rarely used outside of a legal context.\n\n\nThese words were formed from other languages, by elision, or by mincing of other fixed phrases.\n\n"}
{"id": "47548689", "url": "https://en.wikipedia.org/wiki?curid=47548689", "title": "Gangwon dialect", "text": "Gangwon dialect\n\nThe Gangwon dialect is spoken in South Korea's Gangwon Province and in North Korea's Kangwŏn Province. Although they are large provinces by area, relatively few people lived in the Gangwon Province. As a result, people living in the western side of Gangwon (Yeongseo) did not develop a highly distinctive dialect. However, the part of Gangwon that stretches on the eastern coast of Korea (Yeongdong), did develop a distinctive dialect. This is because the Taebaek Mountains bisect the Gangwon Province, and the people on eastern Gangwon are isolated from the high mountains. The Gangwon dialect uses tones distinguish homophonic words, much like Chinese or Vietnamese do.\n"}
{"id": "2836229", "url": "https://en.wikipedia.org/wiki?curid=2836229", "title": "Gibberish (language game)", "text": "Gibberish (language game)\n\nGibberish (sometimes Jibberish) is a language game that is played in the United States, Canada and Ireland. Similar games are played in many other countries. The name Gibberish refers to the nonsensical sound of words spoken according to the rules of this game.\n\nThere are several variations of Gibberish in the English-speaking world. They use \"-itherg-\", \"-uthug-\", \"-elag-\", \"-itug-\", \"-uthaga-\", \"-uvug-\", \"-idig-\", \"-atheg-\" (\"th\" in \"then\" and the two vowels are pronounced with a schwa), and \"-adeg-\". The dialects are given different names. Another form of gibberish known as allibi is spoken using the insertion \"-allib-\".\n\nThese four dialects of Gibberish are spoken by adding the infix to each syllable after the onset. Example:\nWhen a syllable starts with more than one consonant, the infix is added after the onset consonants. Example:\nWhen the syllable begins with a vowel, that vowel is used in place of the first \"i\" in the \"-ithieg-\" infix. Example:\nWords of more than one syllable repeat the rules for each syllable.\n\nThis dialect works in much the same way as the previous dialects, with an additional rule. When a single syllable word begins with a vowel, the infix acts as a prefix, with the initial \"a\" becoming like that vowel.\nThe sentence \"I hit the alarm clock when I woke up\" could be \"Ittiguy Hittigit thittagee addagalitigarm clidigock wittigen Ittiguy wittigoke uttigup\".\n\nAnother paradigm involves infixing \"(V)rV+g\" following the onset of a monosyllabic word, or less usually after each onset or nucleus of polysyllabic words. In words consisting of a single diphthong, the Gibberish morpheme breaks up the syllable into a sequence of vowels plus a glide. The vowels of the Gibberish morpheme typically harmonize for quality with the vowel of the syllable nucleus, but can be reduced if unstressed according to English stress rules. The syllabifies into a new onset. Examples:\n\n\nL can also be commonly used instead of r. Examples:\n\n\nThe term \"gibberish\" is used more generally to refer to all language games created by inserting a certain infix before the vowel in each syllable. For example, if the code infix were \"ob\", then \"Hello, Thomas\" would be translated as \"Hobellobo, Thobomobas\". While a relatively simple code, this can be difficult to understand when spoken swiftly and sounds merely like meaningless babble, which is how it received its name. The terms \"Double Talk\" and \"Double Dutch\" are alternate names for such codes. While any syllables could be used as code syllables, some syllables are more commonly used. These include:\n\n\nAnother variation consists in the code syllables not having a specific vowel, but repeating the vowel of the syllable in which it is being inserted. This variation is common in Switzerland, where the inserted syllable thus could be \"@n@f\", where \"@\" denotes the original vowel; e.g. \"Hallo, Chrige\" would be translated into \"Hanafallonofo, Chrinifigenefe\". Similarly, \"Lalafa\" replaces each occurrence of a vowel with \"@ləf@.\" In Gibberish as spoken in the United Kingdom, the infix code syllable is often \"@rag\".\n\nIn some variants only the first syllable of each word is modified. On the other hand, combining (or double-encoding) forms of Gibberish, or by further encoding with other languages games such as Pig Latin and Tutnese can result in increasingly hard to decipher (and pronounce) words. For instance, combining Pig Latin, Hard Gibberish and Openglopish might result in a phrase ('that sounds like this').\n\nLanguage games in the Gibberish family are not unique to English-speaking countries. Gibberish games in other languages include:\n\n"}
{"id": "1596101", "url": "https://en.wikipedia.org/wiki?curid=1596101", "title": "Grammelot", "text": "Grammelot\n\nGrammelot (or gromalot) is a style of language used in satirical theatre, a gibberish with macaronic and onomatopoeic elements, used in association with mime and mimicry. The satirical use of such a format may date back to the 16th century commedia dell'arte; the group of cognate terms appears to belong to the 20th century.\n\nIn an essay entitled “L’art du grommelot” (Le Figaro, April 20, 2006), French scholar Claude Duneton suggests the word (not the technique) – in its French form, \"grommelot\" – has its origins in the \"commedia dell’arte\"-derived Italian theatre of the early part of the sixteenth century. Duneton studied briefly with Léon Chancerel (1886–1965), who was a major figure in this branch of theatre. Chancerel in fact uses the word in his book, \"Le théâtre et la jeunesse\" (Paris: Bourrellier 1946:47). Others, such as theatre scholar John Rudlin in \"Commedia dell'arte: An Actor's Handbook\" (London: Routledge 1994:60), suggest this origin as well.\n\nWhile the historical origin of the term is unclear, it has been particularly popularized by the Nobel-winning Italian playwright Dario Fo. His 1969 show \"Mistero Buffo\" (\"Comic Mystery Play\") was a satirical touring performance involving sketches based on mediaeval sources, told in Fo's own grammelots constructed from archaic Po Valley dialects and phonemes from modern languages (he has coined separate Italian, French and American grammelots). In his Nobel lecture, Fo referred to the 16th-century Italian playwright Ruzzante's invention of a similar language based on Italian dialects, Latin, Spanish, German and onomatopoeic sounds.\n\nAnother notable modern Italian exponent is the Milan actor/writer Gianni Ferrario. Voice actor Carlo Bonomi, also from Milan, used grammelot to voice Osvaldo Cavandoli's cartoon La Linea and many years later, outside of Italy, Otmar Gutmann's Pingu. Mainstream comics have also used Grammelot-like language: for instance, Stanley Unwin. The Canadian circus and entertainment troupe Cirque du Soleil uses in its routines similar forms of language; journalists often term them \"Cirquish\", but Cirque du Soleil's own staff use the word \"Grommelot\".\n\n\n"}
{"id": "9178719", "url": "https://en.wikipedia.org/wiki?curid=9178719", "title": "Graphism", "text": "Graphism\n\nGraphism refers to the \"expression of thought in material symbols\". Graphism began some 30,000 years BC, not as a photographic representation of reality but as an abstraction that was geared toward magical-religious matters. Early graphism then was a form of writing that constitutes a 'symbolic transposition, not copying of reality'.\n\nThe earliest traces of graphism date back to 30,000 years BC at the end of the Mousterian period and became more prevalent in the Chatelperronian period toward 35,000 BC. While it can be claimed that language merely represents a logical development of the vocal signals of the animal world, nothing comparable to the writing and reading of symbols existed before the dawn of homo sapiens. While motor function determines expression in the techniques and language of all anthropoids, reflection determines graphism in the figurative language of the most recent anthropoids.\n\nIt has been hypothesized that graphism first appeared in the form of tight curves or series of lines engraved in bone or stone. However, there has been no substantial proof to support this hypothesis, with the only comparison being the Australian tjurunga, stone or wood tablets engraved with abstract designs (spirals, straight lines, and clusters of dots) that represented objects of religious significance. The first forms of graphism that allow one to hazardly identify an animal, did not appear until around 30,000 B.C. Prehistoric art records are very numerous, and statistical processing has allowed us to unravel the general meaning of what they represented. The earliest known paintings do not represent a hunt or a family scene, but are graphic building blocks without any associated description. All these early forms therefore suggests that figurative art was directly linked with language and was, in the broadest sense, much closer to writing than to what we understand by a work of art. It was symbolic transposition, not copying of reality, that is to say that graphism did not begin start by reproducing reality in a slavishly photographic manner, but with abstraction.\n\nThe discovery of prehistoric art in the late 19th century raised the issue of a \"naive\" state, an art by which humans supposedly represented what they saw as a result of an aesthetic triggering effect. It was soon realized near the beginning of the 20th century that this view was mistaken, and that magical-religious concerns were responsible for the figurative art of the Cenozoic Era, as indeed for almost all art except in a few rare \"hunting tallies\" etched on bone during the Paleolithic period.\n\n"}
{"id": "13398581", "url": "https://en.wikipedia.org/wiki?curid=13398581", "title": "Indic computing", "text": "Indic computing\n\nIndic Computing means \"computing in Indic\" i.e. Indian Scripts and Languages. It involves developing software in Indic Scripts/languages, Input methods, Localization of computer applications, web development, Database Management, Spell checkers, Speech to Text and Text to Speech applications and OCR in Indian languages.\n\nMost of the widely used Indic scripts are encoded in Unicode for working on Computers and Internet. As of version 10.0, Bengali, Devanagari, Gujarati, Gurmukhi, Kannada, Limbu, Malayalam, Masaram Gondi, Newari, Ol Chiki, Oriya, Sinhala, Tamil and Telugu scripts are encoded and supported. Historically used writing systems like Arwi, Ahom alphabet, Grantha, Khudabadi, Mahajani, Modi alphabet, Siddham script, Syloti Nagri, Tirhuta are also included. Some more Indic scripts are in development and will be included in unicode, for instance Tulu Script.\n\nA lot of Indic Computing projects are going on. They involve some government sector companies, some volunteer groups and individual people.\n\nIndian Union Government made it mandatory for Mobile phone companies whose handsets manufactured, stored, sold and distributed in India to have support for reading of text in all 22 languages. This move has seen rise in use of Indian languages by millions of users.\n\nThe Department of Electronics and Information Technology, India initiated the TDIL (Technology Development for Indian Languages) with the objective of developing Information Processing Tools and Techniques to facilitate human-machine interaction without language barrier; creating and accessing multilingual knowledge resources; and integrating them to develop innovative user products and services.\n\nIn 2005, it started distributing language software tools developed by Government/Academic/Private companies in the form of CD for non commercial use.\n\nSome of the outcome of TDIL program deployed on Indian Language Technology Proliferation & Deployment Centre. This Centre disseminate all the linguistic resources, tools & applications which have been developed under TDIL funding.\n\nC-DAC is an India based government software company which is involved in developing language related software. It is best known for developing InScript Keyboard, the standard keyboard for Indian languages. It has also developed lot of Indic language solutions including Word Processors, typing tools, text to speech software, OCR in Indian languages etc.\n\nThe work developed out of CDAC, Bangalore (earlier known as NCST, Bangalore) became BharateeyaOO. OpenOffice 2.1 had support for over 10 Indian languages.\n\nBOSS is developed by National Resource Centre for free/open source software, an initiative of DIT. Its activities are coordinated by C-DAC Chennai and Anna University KBC Research Center. Support Centres are established at several cities in India to provide support to Users.\n\nIndlinux organisation helped organise the individual volunteers working on different indic language versions of Linux and its applications.\n\nSarovar.org is India's first portal to host projects under Free/Open source licenses. It is located in Trivandrum, India and hosted at Asianet data center. Sarovar.org is customised, installed and maintained by Linuxense as part of their community services and sponsored by River Valley Technologies. Sarovar.org is built on Debian Etch and GForge and runs off METTLE.\n\nPinaak is a non-government charitable society devoted to Indic language computing. It works for software localization, developing language software, localizing open source software, enriching online encyclopedias etc. In addition to this Pinaak works for educating people about computing, ethical use of Internet and use of Indian languages on Internet.\n\nAnkur Group is working toward supporting Bengali language (Bengali) on Linux operating system including localized Bengali GUI, Live CD, English-to-Bengali translator, Bengali OCR and Bengali Dictionary etc. \n\nSMC is a free software group, working to bridge the language divide in Kerala in the technology front and is today the biggest language computing community in India.\n\nWith the advent of Unicode inputting Indic text on computer has become very easy. A number of methods exist for this purpose, but the main ones are:-\n\nInscript is the standard keyboard for Indian languages. Developed by C-DAC and standardized by Government of India. Nowadays it comes inbuilt in all major operating systems including Microsoft Windows (2000, XP, Vista, 7), Linux and Macintosh.\n\nThis is a typing method in which, for instance, the user types text in an Indian language using Roman characters and it is phonetically converted to equivalent text in Indian script in real time. This type of conversion is done by phonetic text editors, word processors and software plugins. Building up on the idea, one can use phonetic IME tools that allow Indic text to be input in any application.\n\nSome examples of phonetic transliterators are Xlit, Google Indic Transliteration, BarahaIME, Indic IME, Rupantar, SMC's Indic Keyboard and Microsoft Indic Language Input Tool. SMC's Indic Keyboard has support for as many as 23 languages whereas Google Indic Keyboard only supports 11 Indian languages.\n\nThey can be broadly classified as:\n\nThis layout was developed when computers had not been invented or deployed with Indic languages, and typewriters were the only means to type text in Indic scripts. Since typewriters were mechanical and could not include a script processor engine, each character had to be placed on the keyboard separately, which resulted in a very complex and difficult to learn keyboard layout.\n\nWith the advent of Unicode, the Remington layout was added to various typing tools for sake of backward compatibility, so that old typists did not have to learn a new keyboard layout. Nowadays this layout is only used by old typists who are used to this layout due to several years of usage. One tool to include Remington layout is Indic IME. A font that is based on the Remington keyboard layout is Kruti Dev. Another online tool that very closely supports the old Remington keyboard layout using Kruti Dev is the Remington Typing tool.\n\nIBus Sharada Braille, which supports seven Indian languages was developed by SMC.\n\nMobile/Hand/cell phone basic models have 12 keys like the plain old telephone keypad. Each key is mapped to 3 or 4 English letters to facilitate data entry in English. For inputting Indian languages with this kind of keypad, there are two ways to do so. First is the Multi-tap Method and second uses visual help from the screen like Panini Keypad. The primary usage is SMS. 140 characters size used for English/Roman languages can be used to accommodate only about 70 language characters when Unicode Proprietary compression is used some times to increase the size of single message for Complex script languages like Hindi. A research study of the available methods and recommendations of proposed standard was released by Broadband Wireless Consortium of India (BWCI).\n\nEnglish is used to type in Indian languages.\nQuillPad\nIndiSMS\n\nIn native methods, the letters of the language are displayed on the screen corresponding to the numeral keys based on the probabilities of those letters for that language. Additional letters can be accessed by using a special key. When a word is partially typed, options are presented from which the user can make a selection.\n\nMost smart phones have about 35 keys catering primarily to English language. Numerals and some symbols are accessed with a special key called Alt. Indic input methods are yet to evolve for these types of phones, as support of Unicode for rendering is not widely available.\n\nInscript is being adopted for smart phone usage. For Android phones which can render Indic languages, Swalekh Multilingual Keypad Multiling Keyboard app are available. Gboard offers support for several Indian languages.\n\nLocalization means translating software, operating systems, websites etc. various applications in Indian language. Various volunteers groups are working in this direction.\nA notable example is the Tamil version of Mandrake linux. Tamil speakers in Toronto (Canada) released Mandrake, a GNU/Linux software, in coming out with a Tamil version. It can be noted that all the features can be accessed in Tamil. By this, the prerequisite of English knowledge for using computers has been eliminated, for those who know Tamil.\n\nIndLinux is a volunteer group aiming to translate the Linux operating system into Indian languages. By the efforts of this group, Linux has been localized almost completely in Hindi and other Indian languages.\n\nNipun is an online translation system aimed to translate various application in Hindi. It is part of \"Akshargram Network\".\n\nGoDaddy has localised its website in Hindi, Marathi and Tamil and also noted that 40% of the call volume for IVR is in Indian Languages.\n\nIndic blogging refers to blogging in Indic languages. Various efforts have been done to promote blogging in Indian languages.\n\nSome Social networks are started in Indian languages.\n\n\nGoogle offers improved translation feature for Hindi, Bengali, Marathi, Tamil, Telugu, Gujarati, Punjabi, Malayalam and Kannada, with offline support as well. Microsoft also offers translation for some of these languages.\n\nTransliteration tools allows users to read a text in different script. As of now, Aksharamukha is the tool which allows most Indian scripts. Google also offers Indic Transliteration. Text from any of these scripts can be converted to any other scripts and vice versa. Whereas Google and Microsoft allow transliteration from Latin letters to Indic scripts.\n\nCarnegie Mellon University, in collaboration with the Hear2Read project, has developed a text-to-speech (TTS) software that helps the visually impaired listen to text in native Indian languages. Currently, Tamil is being offered and , releases in Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi and Telugu are expected over the remainder of 2016.\n\n\nMicrosoft Inc. supports Hindi, Bengali and Tamil email addresses. It is expected to include other Indian languages in future.\n\nAI based Virtual Assistants Google Assistant provides support to various Indian languages.\n\nAccording to GoDaddy, Hindi, Marathi and Tamil languages accounted for 61% of India's internet traffic. It is to be noted that less than 1% of online content is in Indian languages. The newly created top apps have support for multiple Indian languages and/or promote Indian language content. 61% of the Indian users of WhatsApp primarily use their native languages to communicate with it. A recent study revealed that adoption of Internet is highest among local languages such as Tamil, Hindi, Kannada, Bengali, Marathi, Telugu, Gujarati and Malayalam. It estimates that Marathi, Bengali, Tamil, and Telugu will form 30% of the total local-language user base in the country. Currently, Tamil at 42% has the highest Internet adoption levels, followed by Hindi at 39% and Kannada at 37%. Intex also reported that 87 % of its regional language usage came from Hindi, Bengali,Tamil, Gujarati and Marathi speakers. Lava mobiles reported that Tamil and Malayalam are the most popular on their phones, more than even Hindi.\n\n"}
{"id": "17185239", "url": "https://en.wikipedia.org/wiki?curid=17185239", "title": "Joint attention", "text": "Joint attention\n\nJoint attention or shared attention is the shared focus of two individuals on an object. It is achieved when one individual alerts another to an object by means of eye-gazing, pointing or other verbal or non-verbal indications. An individual gazes at another individual, points to an object and then returns their gaze to the individual. Scaife and Bruner were the first researchers to present a cross-sectional description of children's ability to follow eye gaze in 1975. They found that most eight- to ten-month-old children followed a line of regard, and that all 11- to 14-month-old children did so. This early research showed it was possible for an adult to bring certain objects in the environment to an infant's attention using eye gaze.\n\nSubsequent research demonstrates that two important skills in joint attention are following eye gaze and identifying intention. The ability to share gaze with another individual is an important skill in establishing reference. The ability to identify intention is important in a child's ability to learn language and direct the attention of others. Joint attention is important for many aspects of language development including comprehension, production and word learning. Episodes of joint attention provide children with information about their environment, allowing individuals to establish reference from spoken language and learn words. Socio-emotional development and the ability to take part in normal relationships are also influenced by joint attention abilities. The ability to establish joint attention may be negatively affected by deafness, blindness, and developmental disorders such as autism.\n\nOther animals such as great apes, orangutans, chimpanzees, dogs, and horses also show some elements of joint attention.\n\nDefining levels of joint attention is important in determining if children are engaging in age-appropriate joint attention. There are three levels of joint attention: triadic, dyadic, and shared gaze.\n\nTriadic joint attention is the highest level of joint attention and involves two individuals looking at an object. Each individual must understand that the other individual is looking at the same object and realize that there is an element of shared attention. For an instance of social engagement to count as triadic joint attention it requires at least two individuals attending to an object or focusing their attention on each other. Additionally, the individual must display awareness that focus is shared between himself or herself and another individual. Triadic attention is marked by the individual looking back to the other individual after looking at the object.\n\nDyadic joint attention is a conversation-like behavior that individuals engage in. This is especially true for human adults and infants, who engage in this behavior starting at two months of age. Adults and infants take turns exchanging facial expressions, noises, and in the case of the adult, speech.\n\nShared gaze occurs when two individuals are simply looking at an object. Shared gaze is the lowest level of joint attention.\n\nIndividuals who engage in triadic joint attention must understand both gaze and intention to establish common reference. Gaze refers to a child's understanding of the link between mental activity and the physical act of seeing. Intention refers to the child's ability to understand the goal of another person's mental processes.\n\nFor an individual to engage in joint attention they must establish reference. Following the gaze or directive actions (such as pointing) of others is a common way of establishing reference. For an individual to understand that following gaze establishes reference the individual must display:\n\nGaze becomes more complex with age and practice. As gaze increases in complexity, individuals are better able to discriminate what others are referring to. Joint attention is also important for social learning. Gaze following reflects an expectation-based type of orienting in which an individual's attention is cued by another's head turn or eye turn. Individuals are motivated to follow another's gaze and engage in joint attention because gaze is a cue for which rewarding events occur.\n\nThe ability to identify intention is critical to joint attention. When individuals understand that others have goals, intentions, and attentional states, they are able to enter into and direct another's attention. Joint attention promotes and maintains dyadic exchanges and learning about the nature of social partners. The ability to engage in joint attention is crucial for language development.\n\nIndividuals who are intentional in their actions display regularity in their behavior. Individuals locate objects with their eyes, move towards the object, and then use hands to make contact with and manipulate the object. Change in gaze direction is one of several behavioral cues that individuals use in combination with changes in facial and vocal displays and body posture to mark the intention to act on an object. Individuals who seek or follow a joint focus of attention display knowledge that what is in their awareness is also in another's awareness. They believe that they are experiencing the same world as others.\n\nJoint attention plays an important role in the development of theory of mind. Theory of mind and joint attention are important precursors to a fully developed grasp of another individual's mental activity.\n\nThe ability of children to extract information from their environment rests on understandings of attentional behaviors such as pointing. Episodes of joint attention provide children with a great deal of information about objects by establishing reference and intention. Joint attention occurs within particular environments. The items and events in that environment provide a context that enables the child to associate meaning with a particular utterance. Joint attention makes relevant aspects of the context salient, helping children comprehend what is taking place.\n\nAn infant's social environment relates to his or her later language development. Children's first words are closely linked to their early language experience. For children with typically developing language skills, there is a close match between maternal speech and their environment: up to 78% of maternal speech is matched to the object the child is focusing on. In children with delayed language development, only 50% of maternal speech is matched to the object the infant is focusing on. Infants are more likely to engage in joint attention when the parent talks about an object that the child is attending to as opposed to an object outside of the infant's attention. This increased level of joint attention aids in encouraging normal language development, including word comprehension and production. When joint attention is present, it plays an important role in word learning, a crucial aspect of language development.\n\nJoint attention and the ability to attend to an aspect of one's environment are fundamental to normal relationships that rely on the sharing of experience and knowledge. Infants are highly motivated to share experience. An infant's motivation to engage in joint attention is strong enough that infants voluntarily turn away from interesting sights to engage in joint attention with others.\n\nAs described in attachment theory, infants need to develop a relationship with a primary caregiver to achieve normal social and emotional development. A key part of the ability to develop this relationship may be joint attention. In addition to language development, joint attention serves the function of preparing infants for more complex social structures involved in adult conversation. Children's skills in initiating and responding to joint attention predict their social competence at 30 months of age. Anticipatory smiling (a low level form of joint attention involving smiling at an object then turning the smile to one's communicative partner) at 9 months positively predicts parent-rated social competence scores at 30 months in infants. Early joint attention abilities account for differences in social and emotional abilities in later life.\n\nAt the age of 2 months, children engage in dyadic joint attention and conversation-like exchanges with adults during which each is the focus of the other's attention and they take turns exchanging looks, noises and mouth movements. At age 3 months, children display joint attention skills by calling to a caregiver when they are not perceivable. When caregiver does not respond in a similar manner, child exhibits a series of responses that were first studied in early 1970s by Edward Tronick in collaboration with pediatrician T. Berry Brazelton at the time when the latter was creating the Neonatal Behavioral Assessment Scale. At age 6 months, infants display joint attentional skills by:\n\nAt age 8 months, infants demonstrate joint attention through proto-declarative pointing, particularly in girls. At 9 months of age, infants begin to display triadic joint attention. Infants also will display joint attention activities, such as communicative gestures, social referencing, and using the behavior of others to guide response to novel things.\n\nAt one year of age, joint attention is displayed through a child's understanding of pointing as an intentional act. One-year-olds also establish joint attention for objects within their visual field before objects beyond their current visual field. At this age, infants are not yet able to represent their entire environment, only what they can see. At age 15 months, children recognize the minds of others. At this age, children also recognize the importance of eyes for seeing and that physical objects can block sight. At age 18 months, infants are capable of following an individual's gaze to outside their visual field and establishing (representative) joint attention. 18-month-olds also grasp the intentional, referential nature of looking, the mentalistic experience of seeing and the role of eyes and are skilled at following both gaze and pointing with precision. At two years of age, children display joint attention by extending attention beyond the present and understanding that the targets of other's attention extends to the past as well. Two-year-olds are also capable of representational thought or increased memory.\n\nSeveral studies have shown that problems with joint attention are associated with developmental processes. Difficulties in establishing joint attention may partially account for differences in social abilities of children with developmental disorders (i.e. Autism spectrum disorders). A core deficit noted in autism is eye gaze. Autistic children have difficulty alternating their attention towards a partner and third object. This difficulty is attributed to their deficiencies in following gaze, resulting in difficulty initiating and maintaining joint attention. Deaf infants are able to engage in joint attention similar to hearing infants; however, the time spent engaged in joint attention is often reduced in deaf infants born to hearing parents. Hearing parents of deaf infants often are less likely to respond and expand on their deaf infants' initiative and communicative acts. Deaf infants of deaf parents do not show reduced time spent in joint attention. Auditory input is not critical to joint attention but similar modes of communication and understanding are vital. Furthermore, mothers who are unable to successfully establish regular joint attention with their child rate that infant lower on scales of social competence. Judgement of low social competence can be made as early as 18 months of age. In blind infants, joint attention is established by means of auditory input or feeling another person's hand on an object and may be delayed compared to sighted infants.\n\nTriadic joint attention is the highest level of joint attention and involves two individuals looking at an object. Each individual must understand that the other individual is looking at the same object and realize that there is an element of shared attention. Triadic attention is marked by the individual looking back to the other individual after looking at the object. Dyadic joint attention involves mutual gaze between the parent and infant. Mutual gaze is marked by both the parent and infant looking at each other's face. If two individuals are simply looking at an object, it is referred to as shared gaze.\n\nInfant and parent chimpanzees show dyadic joint attention in an affectionate manner by looking at each other's eyes Non-human animals such as Japanese monkeys, baboons, and other Old World monkeys seldom engage in dyadic joint attention. For these animals, the eye contact involved in dyadic joint attention is deemed threatening.\n\nGaze following, or shared gaze, can be found in a number of primates. Domesticated animals such as dogs and horses also demonstrate shared gaze. This type of joint attention is important for animals because gaze shifts serve as indicators alerting the animal to the location of predators, mates, or food.\n\nChimpanzees are capable of actively locating objects that are the focus of another individual's attention by tracking the gaze of others. They are not limited to following eye gaze to the first interesting object in their view. They use a number of different cues to engage in shared focus, including head movement and eye gaze. Infant chimpanzees start to follow tap, point, and head turn cues of an experimenter by nine months of age. By 13 months of age, they show following responses to glance cues without a head turn. There is no evidence to support that infant chimpanzees are able to use eye gaze alone as a cue for following responses. By 20 months of age, infant chimpanzees are able to follow an experimenter's cues to a target behind the chimpanzee but infant chimpanzees do not look back to the experimenter after looking at the target. Moving targets are more salient than stationary targets for infant chimpanzees. Chimpanzee infants are sensitive to faces which are gazing at them, but chimpanzees less than three to four years old only look within their visual field when using the experimenter's head turn as their cue.\n\n"}
{"id": "1136479", "url": "https://en.wikipedia.org/wiki?curid=1136479", "title": "Kinesics", "text": "Kinesics\n\nKinesics is the interpretation of body motion communication such as facial expressions and gestures, nonverbal behavior related to movement of any part of the body or the body as a whole. The equivalent popular culture term is body language, a term Ray Birdwhistell, considered the founder of this area of study, neither used nor liked (on the grounds that what can be conveyed with the body does not meet the linguist's definition of language).\n\nKinesics was first used in 1952 by an anthropologist named Ray Birdwhistell. Birdwhistell wished to study how people communicate through posture, gesture, stance and movement. His ideas over several decades were synthesized and resulted in the book \"Kinesics and Context.\" Interest in kinesics specifically and nonverbal behavior generally was popularized in the late 1960s and early 1970s by such popular mass market (nonacademic) publications as \"How to Read a Person Like a Book\". Part of Birdwhistell's work involved filming people in social situations and analyzing them to show elements of communication that were not clearly seen otherwise. One of his most important projects was \"The Natural History of an Interview,\" a long-term interdisciplinary collaboration including Gregory Bateson, Frieda Fromm-Reichmann, Norman A. McQuown, Henry W. Brosin and others.\n\nDrawing heavily on descriptive linguistics, Birdwhistell argued that all movements of the body have meaning and that nonverbal behavior has a grammar that can be analyzed in similar terms to spoken language. Thus, a \"kineme\" is \"similar to a phoneme because it consists of a group of movements which are not identical, but which may be used interchangeably without affecting social meaning.\"\n\nBirdwhistell estimated that no more than 30 to 35 percent of the social meaning of a conversation or an interaction is carried by the words. He also concluded that there were no universals in these kinesic displays, a claim that was disputed by Paul Ekman, who was interested in analysis of universals, especially in facial expression.\n\nIn a current application, kinesic behaviors are sometimes used as signs of deception by interviewers looking for clusters of movements to determine the veracity of the statement being uttered, although kinesics can be equally applied in any context and type of setting to construe innocuous messages whose carriers are indolent or unable to express verbally.\n\nRelevant concepts include:\n\nKinesic behaviors are an important part of nonverbal communication. Body movements convey information, but interpretations vary by culture. As many movements are carried out at a subconscious or at least a low-awareness level, kinesic movements carry a significant risk of being misinterpreted in an intercultural communication situation.\n\n\n"}
{"id": "719333", "url": "https://en.wikipedia.org/wiki?curid=719333", "title": "Kwadi language", "text": "Kwadi language\n\nKwadi was a \"click language\" of uncertain classification once spoken in the southwest corner of Angola. It is believed to be extinct. There were only fifty Kwadi in the 1950s, of whom only 4–5 were competent speakers of the language. Three partial speakers were known in 1965, but in 1981 no speakers could be found.\n\nBecause Kwadi is poorly recorded, there is not much evidence with which to classify it. It is sometimes classified as the most known divergent member of the Khoe family, linking it to the Khoe languages in a \"Kwadi–Khoe\" family, though this conclusion is disputed. Proponents say it appears to have preserved elements of proto-Khoe that were lost in the western Khoe languages under the influence of Juu languages in Botswana.\n\nThe Kwadi people, called \"Kwepe\" (\"Cuepe\") by the Bantu, appear to have been a remnant population of southwestern African hunter-gatherers, otherwise only represented by the Cimba, Kwisi, and the Damara, who adopted the Khoekhoe language. Like the Kwisi they were fishermen, on the lower reaches of the Coroca River.\n\nKwadi was alternatively known by the varieties of \"Koroka\" (\"Ba-koroka, Curoca, Ma-koroko, Mu-coroca\") and \"Cuanhoca\".\n\n\n"}
{"id": "1224162", "url": "https://en.wikipedia.org/wiki?curid=1224162", "title": "Language delay", "text": "Language delay\n\nLanguage delay is a failure in children to develop language abilities on the usual age-appropriate for their developmental timetable. It is most common during the ages of 2 to 7 years and can sometimes continue into late childhood. Language delay is distinct from speech delay, in which the development of the mechanical and motor aspects of speech production is delayed.\n\nCommunication is a two-stage process. The first stage is to encode the message into a set of words (or signs in the case of Sign Languages) and sentence structures that convey the required meaning, i.e. into language. In the second stage, language is translated into motor commands that control the articulators (hands, face, body, lungs, vocal cords, mouth, tongue, teeth, etc.), thereby creating language.\n\nBecause language and speech are independent, they may be individually delayed. For example, a child may be delayed in speech\n(i.e., unable to produce intelligible speech sounds), but not delayed in language because they use a Sign Language or other means of communication.\n\nLanguage delay is commonly divided into \"receptive\" and \"expressive\" categories. Receptive language refers to the process of understanding language. Expressive language refers to the use of sentences (made of words or signs) to communicate messages to others. Both categories are essential to effective communication.\n\nLanguage delay is a risk factor for other types of developmental delay, including social, emotional, and cognitive delay. Even though speech and language delays may affect a smaller portion of the population in children, it still can have an incredible impact on their life and their accomplishments in the future. Some of these include problems with behavior, difficulty with reading, and other issues related to spelling and low IQ scores. Some children may grow out of these deficits, even coming to excel where they once lagged, while others do not. One particularly common result of language delay is delayed or inadequate acquisition of reading skills. Reading depends upon an ability to code and decode script (i.e., match speech sounds with symbols, and vice versa). If a child is still struggling to master language and speech, it is very difficult to learn another level of complexity (writing). Thus, it is crucial that children have facility with language to be successful readers.\n\nNeuroscientist Steven Pinker postulates that a certain form of language delay may be associated with exceptional and innate analytical prowess in some individuals, such as Albert Einstein, Richard Feynman and Edward Teller.\n\nLanguage delays are the most frequent developmental delays, and can occur for many reasons. A delay can be due to being a “late bloomer”, “late talker”, or a more serious problem. The most common causes of speech delay include\n\nSuch delays can occur in conjunction with a lack of mirroring of facial responses, unresponsiveness or unawareness of certain noises, a lack of interest in playing with other children or toys, or no pain response to stimuli.\n\nOther causes include:\n\nStudies have failed to find clear evidence that a language delay can be prevented by training or educating health care professionals in the subject. Overall, some of the reviews show positive results regarding interventions in language delay, but are not curative.\nTo treat an already existing language delay a child would need Speech and Language Therapy to correct any deficits. These therapists can be found in schools, clinics, through home care agencies, and also colleges where Communication Sciences and Disorders are studied. Aside from these, it is still encouraged for the child's parent to get involved. A few ways that a parent could get involved with helping to improve a child’s language and speech skills includes speaking to their child with enthusiasm, engaging in conversations revolving what the child is focusing on, and reading to their child frequently. \n\nSocial and play skills appear to be more difficult for children with language delays due to their decreased experience in conversation. Speech Pathologists utilize methods such as prompting to improve a child's social skills through play intervention. While recent studies have consistently found play intervention to be helpful, further research is required in order to determine the effectiveness of this form of therapy. \n\n"}
{"id": "8334597", "url": "https://en.wikipedia.org/wiki?curid=8334597", "title": "Languages of the Soviet Union", "text": "Languages of the Soviet Union\n\nThe languages of the Soviet Union are hundreds of different languages and dialects from several different language groups.\n\nIn 1918, it was decreed that all nationalities in the Soviet Union had the right to education in their own language. The new orthography used the Cyrillic, Latin, or Arabic alphabet, depending on geography and culture. After 1937, all languages that had received new alphabets after 1917 began using the Cyrillic alphabet. This way, it would be easier for linguistic minorities to learn to write both Russian and their native language. In 1960, the school educational laws were changed and teaching became more dominated by Russian. \n\nIn 1975, Brezhnev said \"under developed socialism, when the economies in our country have melted together in a coherent economic complex; when there is a new historical concept—the Soviet people—it is an objective growth in the Russian language's role as the language of international communications when one builds Communism, in the education of the new man! Together with one's own mother tongue one will speak fluent Russian, which the Soviet people have voluntarily accepted as a common historical heritage and contributes to a further stabilization of the political, economic and spiritual unity of the Soviet people.\"\n\nEast Slavic languages (Russian, Belarusian and Ukrainian) dominated in the European part of the Soviet Union, the Baltic languages Lithuanian and Latvian, and the Finnic language Estonian were used next to Russian in the Baltic region, while the Moldovan (the only Romance language in the union) was used in the southeast region. In the Caucasus alongside Russian there were Armenian, Azeri and Georgian. In the Russian far north, there were several minority groups who spoke different Uralic languages; most of the languages in Central Asia were Turkic with the exception of Tajik, which is an Iranian language.\n\nThe USSR was a multilingual state, with over 120 languages spoken natively. Although discrimination on the basis of language was illegal under the Soviet Constitution, the \"de facto\" status of these languages differed.\n\nAlthough the USSR did not have \"de jure\" an official language over most of its history, until 1990, and Russian was merely defined as the \"language of interethnic communication\" (), it assumed \"de facto\" the role of official language. For its role and influence in the USSR, see Russification.\n\nOn a second level were the languages of the other 14 Union Republics. In line with their \"de jure\" status in a federal state, they had a small formal role at the Union level (being e.g. present in the Coat of arms of the USSR and its banknotes) and as the main language of its republic. Their effective weight, however, varied with the republic (from strong in places like in Armenia to weak in places like in Byelorussia), or even inside it.\n\nOf these fourteen languages, two are often considered varieties of other languages: Tajik of Persian, and Moldovan of Romanian. Strongly promoted use of Cyrillic in many republics however, combined with lack of contact, led to the separate development of the literary languages. Some of the former Soviet republics, now independent states, continue to use the Cyrillic alphabet at present (such as Kyrgyzstan), while others have opted to use the Latin alphabet instead (such as Turkmenistan and Moldova – although the unrecognized Transnistria officially uses the Cyrillic alphabet).\n\nThe Autonomous republics of the Soviet Union and other subdivision of the USSR lacked even this \"de jure\" autonomy, and their languages had virtually no presence at the national level (and often, not even in the urban areas of the republic itself). They were, however, present in education (although often only at lower grades).\n\nSome smaller languages with very dwindling small communities, like Livonian, were neglected, and weren't present either in education or in publishing.\n\nSeveral languages of non-titular nations, like German, Korean or Polish, although having sizable communities in the USSR, and in some cases being present in education and in publishing, were not considered to be Soviet languages. On the other hand, Finnish, although not generally considered a language of the USSR, was an official language of the Karelia and its predecessor as a Soviet republic. Also Yiddish and Romany were considered Soviet languages.\n\n\n\n"}
{"id": "18077", "url": "https://en.wikipedia.org/wiki?curid=18077", "title": "Lexicon", "text": "Lexicon\n\nA lexicon, word-hoard, wordbook, or word-stock is the vocabulary of a person, language, or branch of knowledge (such as nautical or medical). In linguistics, a lexicon is a language's inventory of lexemes. The word \"lexicon\" derives from the Greek (\"lexicon\"), neuter of (\"lexikos\") meaning \"of or for words.\"\n\nLinguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalogue of a language's words (its wordstock); and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. The lexicon is also thought to include bound morphemes, which cannot stand alone as words (such as most affixes). In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included.\n\nItems in the lexicon are called lexemes, or lexical items, or word forms. Lexemes are not atomic elements but contain both phonological and morphological components. When describing the lexicon, a reductionist approach is used, trying to remain general while using a minimal description. To describe the size of a lexicon, lexemes are grouped into lemmas. A lemma is a group of lexemes generated by inflectional morphology. Lemmas are represented in dictionaries by headwords which list the citation forms and any irregular forms, since these must be learned to use the words correctly. Lexemes derived from a word by derivational morphology are considered new lemmas. The lexicon is also organized according to open and closed categories. Closed categories, such as determiners or pronouns, are rarely given new lexemes; their function is primarily syntactic. Open categories, such as nouns and verbs, have highly active generation mechanisms and their lexemes are more semantic in nature.\n\nA central role of the lexicon is the documenting of established \"lexical norms and conventions\". Lexicalization is the process in which new words, having gained widespread usage, enter the lexicon. Since lexicalization may modify lexemes phonologically and morphologically, it is possible that a single etymological source may be inserted into a single lexicon in two or more forms. These pairs, called a doublet, are often close semantically. Two examples are \"aptitude\" versus \"attitude\" and \"employ\" versus \"imply\".\n\nThe mechanisms, not mutually exclusive, are:\n\nNeologisms are new lexeme candidates which, if they gain wide usage over time, become part of a language's lexicon. Neologisms are often introduced by children who produce erroneous forms by mistake. Another common source is slang and activities such as advertising and branding.\n\nThere are two types of borrowings (neologisms based on external sources) that retain the sound of the source language material:\n\nThe following are examples of external lexical expansion using the source language lexical item as the basic material for the neologization, listed in decreasing order of phonetic resemblance to the original lexical item (in the source language):\n\nThe following are examples of simultaneous external and internal lexical expansion using target language lexical items as the basic material for the neologization but still resembling the sound of the lexical item in the source language:\n\nAnother mechanism involves generative devices that combine morphemes according to a language's rules. For example, the suffix \"-able\" is usually only added to transitive verbs, as in \"readable\" but not \"cryable\".\n\nA compound word is a lexeme composed of several established lexemes, whose semantics is not the sum of that of their constituents. They can be interpreted through analogy, common sense and, most commonly, context. Compound words can have simple or complex morphological structures. Usually only the head requires inflection for agreement. Compounding may result in lexemes of unwieldy proportion. This is compensated by mechanisms that reduce the length of words. A similar phenomenon has been recently shown to feature in social media also where hashtags compound to form longer-sized hashtags that are at times more popular than the individual constituent hashtags forming the compound. Compounding is the most common of word formation strategies cross-linguistically. \n\nComparative historical linguistics studies the evolutions languages and takes a diachronic view of the lexicon. The evolution of lexicons in different languages occurs through parallel mechanism. Over time historical forces work to shape the lexicon, making it simpler to acquire and often creating an illusion of great regularity in language.\n\nThe term \"lexicon\" is generally used in the context of single language. Therefore, multi-lingual speakers are generally thought to have multiple lexicons. Speakers of language variants (Brazilian Portuguese and European Portuguese, for example) may be considered to possess a single lexicon. Thus a \"cash dispenser\" (British English) as well as an automatic teller machine or ATM in American English would be understood by both American and British speakers, despite each group using different dialects.\n\nWhen linguists study a lexicon, they consider such things as what constitutes a word; the word/concept relationship; lexical access and lexical access failure; how a word's phonology, syntax, and meaning intersect; the morphology-word relationship; vocabulary structure within a given language; language use (pragmatics); language acquisition; the history and evolution of words (etymology); and the relationships between words, often studied within philosophy of language.\n\nVarious models of how lexicons are organized and how words are retrieved have been proposed in psycholinguistics, neurolinguistics and computational linguistics.\n\n\n"}
{"id": "3605739", "url": "https://en.wikipedia.org/wiki?curid=3605739", "title": "Locutionary act", "text": "Locutionary act\n\nIn linguistics and the philosophy of mind, a locutionary act is the performance of an utterance, and hence of a speech act. The term equally refers to the surface meaning of an utterance because, according to J. L. Austin's posthumous \"How To Do Things With Words\", a speech act should be analysed as a locutionary act (\"i.e.\" the actual utterance and its ostensible meaning, comprising phonetic, phatic and rhetic acts corresponding to the verbal, syntactic and semantic aspects of any meaningful utterance), as well as an illocutionary act (the semantic 'illocutionary force' of the utterance, thus its real, intended meaning), and in certain cases a further perlocutionary act (\"i.e.\" its actual effect, whether intended or not).\n\nFor example, the phrase \"Don't go into the water\" (a locutionary act with distinct phonetic, syntactic and semantic features) counts as warning to the listener not to go into the water (an illocutionary act). If the listener heeds the warning the speech-act has been successful in persuading the listener not to go into the water (a perlocutionary act). This taxonomy of speech acts was inherited by John R. Searle, Austin's pupil at Oxford and subsequently an influential exponent of speech act theory.\n\n"}
{"id": "651326", "url": "https://en.wikipedia.org/wiki?curid=651326", "title": "Macaronic language", "text": "Macaronic language\n\nMacaronic language refers to text using a mixture of languages, particularly bilingual puns or situations in which the languages are otherwise used in the same context (rather than simply discrete segments of a text being in different languages). The term can also denote hybrid words, which are effectively \"internally macaronic\". A rough equivalent in spoken language is code-switching, a term in linguistics referring to using more than one language or dialect within the same conversation.\n\nMacaronic Latin in particular is a jumbled jargon made up of vernacular words given Latin endings, or for Latin words mixed with the vernacular in a pastiche (compare dog Latin).\n\nThe word \"macaronic\" comes from the New Latin \"macaronicus\" which is from the Italian \"maccarone\" (\"dumpling\", regarded as coarse peasant fare). The term can have derogatory overtones, and is usually reserved for works where the mixing of languages has a humorous or satirical intent or effect. It is a matter of debate whether the term can be applied to mixed-language literature of a more serious nature and purpose.\n\nTexts that mixed Latin and vernacular language apparently arose throughout Europe at the end of the Middle Ages—a time when Latin was still the working language of scholars, clerics and university students, but was losing ground to vernacular among poets, minstrels and storytellers.\n\nAn early example is from 1130, in the Gospel book of Munsterbilzen Abbey. The following sentence mixes late Old Dutch and Latin:\n<poem>\nTesi samanunga was edele unde scona\net omnium virtutum pleniter plena\n</poem>\nTranslated: \"This community was noble and pure, and completely full of all virtues.\"\n\nThe \"Carmina Burana\" (collected c.1230) contains several poems mixing Latin with Medieval German or French. Another well-known example is the first stanza of the famous carol \"In Dulci Jubilo\", whose original version (written around 1328) had Latin mixed with German, with a hint of Greek. While some of those early works had a clear humorous intent, many use the language mix for lyrical effect.\n\nAnother early example is in the Middle English recitals \"The Towneley Plays\" (c.1460). In \"The Talents\" (play 24), Pontius Pilate delivers a rhyming speech in mixed English and Latin.\n\nA number of English political poems in the 14th century alternated (Middle) English and Latin lines, such as in MS Digby 196:\n<poem>The taxe hath tened [ruined] vs alle,\n\nSeveral Anthems also contain both Latin and English. In the case of 'Nolo mortem pecatoris' by Thomas Morley, the Latin is used as a refrain:\n\n<poem>\nNolo mortem peccatoris; Haec sunt verba Salvatoris.\nFather I am thine only Son, sent down from heav’n mankind to save.\nFather, all things fulfilled and done according to thy will, I have.\nFather, my will now all is this: Nolo mortem peccatoris.\nFather, behold my painful smart, taken for man on ev’ry side;\nEv'n from my birth to death most tart, no kind of pain I have denied,\nbut suffered all, and all for this: Nolo mortem peccatoris.\n</poem>\nTranslated: \"'I do not wish the death of the wicked'; These are the words of the Saviour.\" An allusion to John 3:17 and 2 Peter 3:9.\n\nThe term \"macaronic\" is believed to have originated in Padua in the late 15th century, apparently from \"maccarona\", a kind of pasta or dumpling eaten by peasants at that time. (That is also the presumed origin of \"maccheroni\".) Its association with the genre comes from the \"Macaronea\", a comical poem by Tifi Odasi in mixed Latin and Italian, published in 1488 or 1489. Another example of the genre is \"Tosontea\" by Corrado of Padua, which was published at about the same time as Tifi's \"Macaronea\".\n\nTifi and his contemporaries clearly intended to satirize the broken Latin used by many doctors, scholars and bureaucrats of their time. While this \"macaronic Latin\" (\"macaronica verba\") could be due to ignorance or carelessness, it could also be the result of its speakers trying to make themselves understood by the vulgar folk without resorting to their speech.\n\nAn important and unusual example of mixed-language text is the \"Hypnerotomachia Poliphili\" of Francesco Colonna (1499), which was basically written using Italian syntax and morphology, but using a made-up vocabulary based on roots from Latin, Greek, and occasionally others. However, while the \"Hypnerotomachia\" is contemporary with Tifi's \"Macaronea\", its mixed language is not used for plain humor, but rather as an aesthetic device to underscore the fantastic but refined nature of the book.\n\nTifi's \"Macaronea\" was a popular success, and the writing of humorous texts in macaronic Latin became a fad in the 16th and 17th centuries, particularly in Italian, but also in many other European languages. An important Italian example was \"Baldo\" by Teofilo Folengo, who described his own verses as \"a gross, rude, and rustic mixture of flour, cheese, and butter\".\n\nMacaronic verse is especially common in cultures with widespread bilingualism or language contact, such as Ireland before the middle of the nineteenth century. Macaronic traditional songs such as \"Siúil A Rúin\" are quite common in Ireland. In Scotland, macaronic songs have been popular among Highland immigrants to Glasgow, using English and Scottish Gaelic as a device to express the alien nature of the anglophone environment. An example:\n\nWhen I came down to Glasgow first,\na-mach air Tìr nan Gall.\nI was like a man adrift,\nair iomrall 's doll air chall.\n\nFolk and popular music of the Andes frequently alternates between Spanish and the given South American language of its region of origin.\n\nSome Classical Persian poems were written with alternating Persian and Arabic verses or hemistichs, most famously by Saadi and Hafez. Such poems were called \"molamma‘\" (, literally \"speckled\", plural \"molamma‘āt\" ), Residing in Anatolia, in some of his poems Rumi mixed Persian with Arabic as well as the local languages of Turkish and Greek.\n\nMacaronic verse was also common in medieval India, where the influence of the Muslim rulers led to poems being written in alternating indigenous Hindi and the Persian language. This style was used by the famous poet Amir Khusro and played a major role in the rise of the Urdu or Hindustani language.\n\nThe Sublime song \"Caress Me Down\" alternates between English and Spanish lyrics, at some points between verses, at others between lines within a verse, and occasionally between phrases within a line.\n\nOccasionally language is unintentionally macaronic. One particularly famed piece of schoolyard Greek in France is Xenophon's line \"they did not take the city; but in fact they had no hope of taking it\" (, \"ouk élabon pólin; álla gàr elpìs éphē kaká\"). Read in the French manner, this becomes \"Où qu'est la bonne Pauline? A la gare. Elle pisse et fait caca.\" (\"Where is Pauline the maid? At the [railway] station. She's pissing and taking a shit.\") In English literature, the untranslated line makes an appearance in James Joyce's \"Finnegans Wake\".\n\nMacaronic text is still used by modern Italian authors, e.g. by Carlo Emilio Gadda and Beppe Fenoglio. Other examples are provided by the character Salvatore in Umberto Eco's \"The Name of the Rose\", and the peasant hero of his \"Baudolino\". Dario Fo's \"Mistero Buffo\" (\"Comic Mystery Play\") features grammelot sketches using language with macaronic elements.\n\nThe 2001 novel \"The Last Samurai\" by Helen DeWitt includes portions of Japanese, Classical Greek, and Inuktitut, although the reader is not expected to understand the passages that are not in English.\n\nMacaronic games are used by the literary group Oulipo in the form of interlinguistic homophonic transformation: replacing\na known phrase with homophones from another language. The archetypal example is by François Le Lionnais, who transformed John Keats' \"A thing of beauty is a joy forever\" into \"Un singe de beauté est un jouet pour l'hiver\": 'A monkey of beauty is a toy for the winter'. Another example is the book \"Mots d'Heures\".\n\nMacaronisms figure prominently in \"The Trilogy\" by the Polish novelist Henryk Sienkiewicz, and are one of the major compositional principles for James Joyce's novel \"Finnegans Wake\".\n\nTwo well-known examples of non-humorous macaronic verse are Byron's \"Maid of Athens, ere we part\" (1810, in English with a Greek refrain); and Pearsall's translation of the carol \"In Dulci Jubilo\" (1837, in mixed English and Latin verse).\n\nAn example of modern humorous macaronic verse is the anonymous English/Latin poem \"Carmen Possum\" (\"The Opossum's Song\"), which is sometimes used as a teaching and motivational aid in elementary Latin language classes. Other similar examples are \"The Motor Bus\" by A. D. Godley, and the anonymous \"Up I arose in verno tempore\".\n\nRecent examples are the \"mużajki\" or 'mosaics' (2007) of Maltese poet Antoine Cassar mixing English, Spanish, Maltese, Italian, and French; works of Italian writer Guido Monte; and the late poetry of Ivan Blatný combining Czech with English. \n\nBrian P. Cleary's \"What Can I C'est?\" makes use of macaronic verse, as do other poems in his book \"Rainbow Soup: Adventures in Poetry\":\n\nMy auntie Michelle is big in the BON\n(As well as the hip and the thigh).\nAnd when she exhales, OUI haul out our sails\nAnd ride on the wind of VERSAILLES.\n\nA whole body of comic verse exists created by John O'Mill, pseudonym of Johan van der Meulen, a teacher of English at the Rijks HBS (State Grammar School), Breda, the Netherlands. These are in a mixture of English and Dutch, often playing on common mistakes made when translating from the latter to the former.\n\nThe finale of act 1 of Gilbert and Sullivan's Savoy Opera Iolanthe has several instances of humorous macaronic verse. \n\nFirst, the three Lords mix Italian and Latin phrases into their discussion of Iolanthe's age:\nLord Mountararat: This gentleman is seen, / With a maid of seventeen, / A-taking of his \"dolce far niente\"...\nLord Chancellor: Recollect yourself, I pray, / And be careful what you say- / As the ancient Romans said, \"festina lente\"...\nLord Tolloller: I have often had a use / For a thorough-bred excuse / Of a sudden (which is English for \"repente\")...\nLord Mountararat: Now, listen, pray to me, / For this paradox will be / Carried, nobody at all \"contradicente\"...\n\nThen, the chorus of peers sing macaronic verse as they attempt to resist the fairies' powers:\nOur lordly style you shall not quench with base \"canaille\"! (That word is French.) \nDistinction ebbs before a herd of vulgar \"plebs\"! (A Latin word.)\nTwould fill with joy and madness stark the \"oι πoλλoί\"! (A Greek remark.) \nOne Latin word, one Greek remark, and one that's French.\n\n'Macaronisms' are frequently used in films, especially comedies. In Charlie Chaplin's anti-war comedy \"The Great Dictator\", the title character speaks English mixed with a parody of German (e.g. \"Cheese-und-cracken\"). This was also used by Benzino Napaloni, the parody character of Benito Mussolini, using Italian foods (such as salami and ravioli) as insults.\n\nOther movies featuring macaronic language are the Italian historical comedies \"L'armata Brancaleone\" and \"Brancaleone alle crociate\" (d. Mario Monicelli), which mix modern and medieval Italian as well as Latin (sometimes in rhyme, and sometimes with regional connotations, such as the Italo-Normans using words from modern Sicilian).\n\nMacaronic language appearing in popular songs include Dean Martin's \"That's Amore\" (Italian and English), the Beatles’ \"Michelle\" (French and English), The Clash’s \"Spanish Bombs\" and José Feliciano’s \"Feliz Navidad\" (Spanish and English), the Talking Heads \"Psycho Killer\" (French and English), and The Weeknd's Montreal (French and English).\n\n\n\n"}
{"id": "876206", "url": "https://en.wikipedia.org/wiki?curid=876206", "title": "Mathesis universalis", "text": "Mathesis universalis\n\nMathesis universalis (Greek μάθησις, \"mathesis\" \"science or learning\", Latin \"universalis\" \"universal\") is a hypothetical universal science modeled on mathematics envisaged by Descartes and Leibniz, among a number of more minor 16th and 17th century philosophers and mathematicians. John Wallis invokes the name as the title to a textbook on Cartesian geometry. For Leibniz, it would be supported by a calculus ratiocinator.\n\nDescartes' clearest description of the \"mathesis universalis\" occurs in Rule IV of the Rules for the Direction of the Mind, written before 1628. The desire for a language more perfect than any natural language had been expressed before Leibniz by John Wilkins in his \"An Essay towards a Real Character and a Philosophical Language\" in 1668. Leibniz attempts to work out the possible connections between algebra, infinitesimal calculus, and universal character in an incomplete treatise titled \"Mathesis Universalis\" in 1695.\n\nPredicate logic could be seen as a modern system with some of these \"universal\" characteristics, at least as far as mathematics and computer science are concerned. More generally, \"mathesis universalis\", along with perhaps François Viète's algebra, represents one of the earliest attempts to construct a formal system.\n\nOne of the perhaps most prominent critics of the idea of \"mathesis universalis\" was Ludwig Wittgenstein and his philosophy of mathematics. As Anthropologist Prof. Emily Martin notes: 'Tackling mathematics, the realm of symbolic life perhaps most difficult to regard as contingent on social norms, Wittgenstein commented that people found the idea that numbers rested on conventional social understandings \"unbearable\"'\n\n\n"}
{"id": "33056513", "url": "https://en.wikipedia.org/wiki?curid=33056513", "title": "Mental fact", "text": "Mental fact\n\nMental facts include such things as perceptions, feelings, and judgments. Mental facts are ultimately caused by physical facts, in that mental facts depend on physical and biological functions which are required for consciousness. The physical and biological processes which are necessary for consciousness enable conscious individuals to recognize physical and mental facts. Thus, mental facts are based on physical facts, and both physical and mental facts are required for the construction of social reality.\n\nAccording to John Searle, mental facts may be intentional or nonintentional, depending on whether or not they are directed at something.\n\n"}
{"id": "57717488", "url": "https://en.wikipedia.org/wiki?curid=57717488", "title": "Natangian", "text": "Natangian\n\nNatangian is a part of Low Prussian, belonging to Low German. It is from East Prussia. The name is from the Natangians, a tribe of the Old Prussians. It is or used to be spoken around Kornevo, Bartoszyce, Pravdinsk, Srokowo and Kętrzyn. Natangian has or used to have a border with Standard German, Mundart des Kürzungsgebiets, Westkäslausch, Ostsamländisch, Mundart des Ostgebiets, Ostkäslausch and Breslausch. \n"}
{"id": "10417456", "url": "https://en.wikipedia.org/wiki?curid=10417456", "title": "Orality", "text": "Orality\n\nOrality is thought and verbal expression in societies where the technologies of literacy (especially writing and print) are unfamiliar to most of the population. The study of orality is closely allied to the study of oral tradition. \n\nThe term “orality” has been used in a variety of ways, often to describe, in a generalised fashion, the structures of consciousness found in cultures that do not employ, or employ minimally, the technologies of writing. \n\nWalter J. Ong’s work was foundational for the study of orality, and reminds us that despite the striking success and subsequent power of written language, the vast majority of languages are never written, and the basic orality of language is permanent. \n\nIn his later publications Ong distinguishes between two forms of orality: 'primary orality' and 'secondary orality'. Primary orality is thought and expression un-touched by the culture of writing of print; \"secondary orality\" is explained by Ong as oral culture defined by (implicitly influenced) by the written and printed word, and includes oral culture made possible by technology such as a newscaster reading a news report on television.\n\nIn addition, 'residual orality' is also defined - it is the remnants, legacy, or influence of a predominately oral culture carried over into the written realm - an example might include the use of dialogue as a philosophical or didactic tool in written literature, such as used by the Greek thinker Plato.\n\nBefore writing became a way for many cultures before we had orality. Unfortunately many of the retained orality has been lost or drastically changed. Those that were able to be preserved gives us insight to past cultures and just how much we have evolved since then. In \"Orality and Literacy\" (2nd ed. ), Ong sums up his own work over the previous three decades as well as the work of numerous other scholars. With regard to oral tradition and primary orality he draws on pioneering work by Milman Parry, Albert B. Lord, and Eric A. Havelock. Marshall McLuhan was among the first to fully appreciate the significance of the Ong's earlier work about print culture and the written and printed word as a technology. In his work \"The Gutenberg Galaxy\" McLuhan quotes and discusses works by Ong in the 1950s regarding print culture.; Orality gave us the stepping stones that allowed us to get where we are today, it was a necessity for the growth of civilization. But using his own examples to amplify Ong's thought, McLuhan shows how each stage in the development of this technology throughout the history of communication – from the invention of speech (primary orality), to pictograms, to the phonetic alphabet, to typography, to the electronic communications of today – restructures human consciousness, profoundly changing not only the frontiers of human possibility, but even the frontiers it is possible for humans to imagine.\n\n'Primary orality' refers to thought and its verbal expression within cultures \"totally untouched by any knowledge of writing or print.\"\n\nAll sound is inherently powerful. If a hunter kills a lion he can see it, touch it, feel it and smell it. But if he hears a lion he must act, fast, because the sound of the lion signals its presence and its power. Speech is a form of sound that shares this common power. Like other sounds, it comes from within a living organism. A text can be ignored; it is just writing on paper. But to ignore speech can be unwise; our basic instincts compel us to pay attention.\n\nWriting is powerful in a different way: it permits people to generate ideas, store them, and retrieve them as needed across time in a highly efficient and accurate way. The absence of this technology in oral societies limits the development of complex ideas and the institutions that depend on them. Instead, sustained thought in oral settings depends on interpersonal communication, and storing complex ideas over a long period of time requires packaging them in highly memorable ways, generally by using mnemonic tools.\n\nIn his studies of the Homeric Question, Milman Parry was able to show that the poetic metre found in the \"Iliad\" and the \"Odyssey\" had been 'packaged' by oral Greek society to meet its information management needs. These insights first opened the door to a wider appreciation of the sophistication of oral traditions, and their various methods of managing information. Later, ancient and medieval mnemonic tools were extensively documented by Frances Yates in her book \"The Art of Memory\".\n\n‘Residual orality’ refers to thought and its verbal expression in cultures that have been exposed to writing and print, but have not fully ‘interiorized’ (in McLuhan’s term) the use of these technologies in their daily lives. As a culture interiorizes the technologies of literacy, the ‘oral residue’ diminishes.\n\nBut the availability of a technology of literacy to a society is not enough to ensure its widespread diffusion and use. For example, Eric Havelock observed in \"A Preface to Plato\" that after the ancient Greeks invented writing they adopted a scribal culture that lasted for generations. Few people, other than the scribes, considered it necessary to learn to read or write. In other societies, such as ancient Egypt or medieval Europe, literacy has been a domain confined to political and religious elites.\n\nMany cultures have experienced an equilibrium state in which writing and mass illiteracy have co-existed for hundreds or even thousands of years.\n\nOral residue rarely disappears quickly and never vanishes completely. Speech is inherently an oral event, based on human relationships, unlike texts. Oral societies can mount strong resistance to literate technologies, as vividly shown in the arguments of Socrates against writing in Plato's \"Phaedrus\". Writing, Socrates argues, is inhuman. It attempts to turn living thoughts dwelling in the human mind into mere objects in the physical world. By causing people to rely on what is written rather than what they are able to think, it weakens the powers of the mind and of memory. True knowledge can only emerge from a relationship between active human minds. And unlike a person, a text can’t respond to a question; it will just keep saying the same thing over and over again, no matter how often it is refuted.\n\nThe Canadian communications scholar, Harold Innis argued that a balance between the spoken word and writing contributed to the cultural and intellectual vitality of ancient Greece in Plato's time. Plato conveyed his ideas by writing down the conversations of Socrates thus \"preserving the power of the spoken word on the written page.\" Aristotle, Innis wrote, regarded Plato's style as \"halfway between poetry and prose.\" Plato was able to arrive at new philosophical positions \"through the use of dialogues, allegories and illustrations.\"\n\nFurthermore, as McLuhan emphasizes, modernization attenuates some oral capabilities. For example, in medieval Europe silent reading was largely unknown. This tilted the readers' attention towards the poetic and other auditory aspects of the text. Educated modern adults may also occasionally long for something like \"the capacious medieval memory, which, untrammeled by the associations of print, could learn a strange language with ease and by the methods of a child, and could retain in memory and reproduce lengthy epic and elaborate lyric poems.\"\nMcLuhan and Ong also document the apparent re-emergence, in the electronic age, of a kind of 'secondary orality' that displaces written words with audio/visual technologies like radio, telephones, and television. Unlike primary oral modes of communication, these technologies depend on print for their existence. Mass Internet collaborations, such as Wikipedia, rely primarily on writing, but re-introduce relationships and responsiveness into the text.\n\nIt has been a habit of literate cultures to view oral cultures simply in terms of their lack of the technologies of writing. This habit, argues Ong, is dangerously misled. Oral cultures are living cultures in their own right. A 1971 study found that of 3,000 extant languages, only 78 had a written literature. While literacy extends human possibilities in both thought and action, all literate technologies ultimately depend on the ability of humans to learn oral languages and then translate sound into symbolic imagery.\n\nUnderstanding between nations may depend to some degree on understanding oral culture. Ong argues that \"many of the contrasts often made between 'western' and other views seem reducible to contrasts between deeply interiorized literacy and more or less residually oral states of consciousness.\"\n\nDrawing on hundreds of studies from anthropology, linguistics and the study of oral tradition, Ong summarizes ten key aspects of the 'psychodynamics of orality'. While these are subject to continuing debate, his list remains an important milestone. Ong draws his examples from both primary oral societies, and societies with a very high 'oral residue'.\n\nTo retain complex ideas requires that they be packaged memorably for easy recall.\n\nAnthropologist Marcel Jousse identifies a close linkage between rhythm and breathing patterns, gestures and the bilateral symmetry of the human body in several ancient verse traditions. This synergy between the body and the construction of oral thought further fuels memory.\n\nOral cultures avoid complex 'subordinative' clauses. Ong cites an example from the Douay-Rheims version of Genesis (1609–10), noting that this basic additive pattern (in \"italics\") has been identified in many oral contexts around the world:\n\nDemonstrating how oral modes of communication tend to evolve into literate ones, Ong additionally cites the New American Bible (1970), which offers a translation that is grammatically far more complex:\n\nOral expression brings words together in pithy phrases that are the product of generations of evolution: the 'sturdy oak tree', the 'beautiful princess' or 'clever Odysseus'. This does not apply specifically to poetry or song; rather the words are brought together out of habit during general communication. 'Analyzing' or breaking apart such expressions is risky: they represent the work of generations and \"there is nowhere outside the mind to store them.\"\n\nOng cites an American example, noting that in some parts of the United States with heavy oral residue, until the early twentieth-century it was still considered normal or even obligatory to use the adjective ‘glorious’ when referring to the 'Fourth of July'.\n\nSpeech that repeats earlier thoughts or thought-pictures, or shines a different light on them somehow, helps to keep both the speaker and the listener focused on the topic, and makes it easier for all to recall the key points later. \"Oral cultures encourage fluency, fulsomeness, volubility. Rhetoricians were to call this \"\n\nBecause oral societies have no effective access to writing and print technologies, they must invest considerable energy in basic information management. Storage of information, being primarily dependent on individual or collective recall, must be handled with particular thrift. It is possible to approximately measure oral residue \"from the amount of memorization the culture's educational procedures require.\"\n\nThis creates incentives to avoid exploring new ideas and particularly to avoid the burden of having to store them. It does not prevent oral societies from demonstrating dynamism and change, but there is a premium on ensuring that changes cleave to traditional formulas, and \"are presented as fitting the traditions of the ancestors.\"\n\nOral cultures take a practical approach to information storage. To qualify for storage, information must usually concern matters of immediate practical concern or familiarity to most members of the society.\n\nLong after the invention of writing, and often long after the invention of print, basic information on how to perform a society’s most important trades was left unwritten, passed from one generation to the next as it always had been: through apprenticeship, observation and practice.\n\nBy contrast, only literary cultures have launched phenomenological analyses, abstract classifications, ordered lists and tables, etc. Nothing analogous exists in oral societies.\n\n'Agonistic' means 'combative', but Ong actually advances a deeper thesis with this point. Writing and to an even greater extent print, he argues, disengage humans from direct, interpersonal struggle.\n\nProducts of \"the highly polarized, agonistic, oral world of good and evil, virtue and vice, villains and heroes,\" the great works of oral literature from Homer to Beowulf, from the Mwindo epic to the Old Testament, are extremely violent by modern standards. They are also punctuated by frequent and intense intellectual combat and tongue-lashings on the one hand, and effusive praise (perhaps reaching its height among African praise singers) on the other.\n\nIn an oral culture the most reliable and trusted technique for learning is to share a \"close, empathetic, communal association\" with others who know.\n\nOng cites a study of community decision-making from 12th Century England. Writing already had a long history in England, and it would have been possible to use texts to establish for example, the age of majority of the heir to an estate. But people were skeptical about texts, noting not only the cost of generating and managing them, but the problems involved in preventing tampering or frauds.\n\nAs a result, they retained the traditional solution: gathering together \"mature wise seniors of many years, having good testimony\", and publicly discussing the age of the heir with them, until agreement was reached. This hallmark principle of orality, that truth emerges best from communal process, resonates today in the jury system.\n\nOral societies conserve their limited capacity to store information, and retain the relevance of their information to the interest of their present members, by shedding memories that have lost their past significance.\n\nWhile many examples exist, the classic example was reported by . Written records prepared by the British in Ghana in the early 1900s show that Ndewura Jakpa, the seventeenth century founder of the state of the Gonja people, had seven sons, each of whom ruled a territorial division within the state. Six decades later two of the divisions had disappeared for various reasons. The myths of the Gonja had been revised to recount that Jakpa had five sons, and that five divisions were created. Since they had no practical, present purpose, the other two sons and divisions had evaporated.\n\nIn oral cultures, concepts are used in a way that minimizes abstraction, focusing to the greatest extent possible on objects and situations directly known by the speaker. A study by Alexander Luria, a psychologist who did extensive fieldwork comparing oral and literate subjects in remote areas of Uzbekistan and Kirghizia in 1931–2 documented the highly situational nature of oral thinking.\n\n\n\n"}
{"id": "24566", "url": "https://en.wikipedia.org/wiki?curid=24566", "title": "Palaeography", "text": "Palaeography\n\nPalaeography (UK) or paleography (US; ultimately from , \"palaiós\", \"old\", and , \"graphein\", \"to write\") is the study of ancient and historical handwriting (that is to say, of the forms and processes of writing; not the textual content of documents). Included in the discipline is the practice of deciphering, reading, and dating historical manuscripts, and the cultural context of writing, including the methods with which writing and books were produced, and the history of scriptoria.\n\nThe discipline is important for understanding, authenticating, and dating ancient texts. However, it cannot in general be used to pinpoint dates with high precision.\n\nPalaeography can be an essential skill for historians and philologists, as it tackles two main difficulties. First, since the style of a single alphabet in each given language has evolved constantly, it is necessary to know how to decipher its individual characters as they existed in various eras. Second, scribes often used many abbreviations, usually so as to write more quickly and sometimes to save space, so the specialist-palaeographer must know how to interpret them. Knowledge of individual letter-forms, ligatures, punctuation, and abbreviations enables the palaeographer to read and understand the text. The palaeographer must know, first, the language of the text (that is, one must become expert in the relevant earlier forms of these languages); and second, the historical usages of various styles of handwriting, common writing customs, and scribal or notarial abbreviations. Philological knowledge of the language, vocabulary, and grammar generally used at a given time or place can help palaeographers identify ancient or more recent forgeries versus authentic documents.\n\nKnowledge of writing materials is also essential to the study of handwriting and to the identification of the periods in which a document or manuscript may have been produced. An important goal may be to assign the text a date and a place of origin: this is why the palaeographer must take into account the style and formation of the manuscript and the handwriting used in it.\n\nPalaeography can be used to provide information about the date at which a document was written. However, \"paleography is a last resort for dating\" and, \"for book hands, a period of 50 years is the least acceptable spread of time\" with it being suggested that \"the 'rule of thumb' should probably be to avoid dating a hand more precisely than a range of at least seventy or eighty years\". In a 2005 e-mail addendum to his 1996 \"The Paleographical Dating of P-46\" paper Bruce W. Griffin stated \"Until more rigorous methodologies are developed, it is difficult to construct a 95% confidence interval for NT manuscripts without allowing a century for an assigned date.\" William M Schniedewind went even further in the abstract to his 2005 paper \"Problems of Paleographic Dating of Inscriptions\" and stated that \"The so-called science of paleography often relies on circular reasoning because there is insufficient data to draw precise conclusion about dating. Scholars also tend to oversimplify diachronic development, assuming models of simplicity rather than complexity\".\n\n\nThe Aramaic language was the international trade language of the Ancient Middle East, originating in what is modern-day Syria, between 1000 and 600 BC. It spread from the Mediterranean coast to the borders of India, becoming extremely popular and being adopted by many people, both with or without any previous writing system. The Aramaic script was written in a consonantal form with a direction from right to left. The Aramaic alphabet, a modified form of Phoenician, was the ancestor of the modern Arabic and Hebrew scripts, as well as the Brāhmī script, the parent writing system of most modern abugidas in India, Southeast Asia, Tibet, and Mongolia. Initially, the Aramaic script did not differ from the Phoenician, but then the Aramaeans simplified some of the letters, thickened and rounded their lines: a specific feature of its letters is the distinction between d and r. One innovation in Aramaic is the \"matres lectionis\" system to indicate certain vowels. Early Phoenician-derived scripts did not have letters for vowels, and so most texts recorded just consonants. Most likely as a consequence of phonetic changes in North Semitic languages, the Aramaeans reused certain letters in the alphabet to represent long vowels. The letter \"aleph\" was employed to write /ā/, \"he\" for /ō/, \"yod\" for /ī/, and \"vav\" for /ū/.\n\nAramaic writing and language supplanted Babylonian cuneiform and Akkadian language, even in their homeland in Mesopotamia. The wide diffusion of Aramaic letters led to its writing being used not only in monumental inscriptions, but also on papyrus and potsherds. Aramaic papyri have been found in large numbers in Egypt, especially at Elephantine—among them are official and private documents of the Jewish military settlement in 5 BC. In the Aramaic papyri and potsherds, words are separated usually by a small gap, as in modern writing. At the turn of the 3rd to 2nd centuries BC, the heretofore uniform Aramaic letters developed new forms, as a result of dialectal and political fragmentation in several subgroups. The most important of these is the so-called square Hebrew block script, followed by Palmyrene, Nabataean, and the much later Syriac script.\n\nAramaic is usually divided into three main parts:\n\nThe term Middle Aramaic refers to the form of Aramaic which appears in pointed texts and is reached in the 3rd century AD with the loss of short unstressed vowels in open syllables, and continues until the triumph of Arabic.\n\nOld Aramaic appeared in the 11th century BC as the official language of the first Aramaean states. The oldest witnesses to it are inscriptions from northern Syria of the 10th to 8th centuries BC, especially extensive state treaties (c. 750 BC) and royal inscriptions. The early Old Ancient should be classified as \"Ancient Aramaic\" and consists of two clearly distinguished and standardised written languages, the Early Ancient Aramaic and the Late Ancient Aramaic. Aramaic was influenced at first principally by Akkadian, then from the 5th century BC by Persian and from the 3rd century BC onwards by Greek, as well as by Hebrew, especially in Palestine. As Aramaic evolved into the imperial language of the Neo-Assyrian Empire, the script used to write it underwent a change into something more cursive. The best examples of this script come from documents written on papyrus from Egypt. About 500 BC, Darius I (522–486) made the Aramaic used by the Achaemenid imperial administration into the official language of the western half of the Persian Empire. This so-called \"Imperial Aramaic\" (the oldest dated example, from Egypt, belonging to 495 BC) is based on an otherwise unknown written form of Ancient Aramaic from Babylonia. In orthography, Imperial Aramaic preserves historical forms—alphabet, orthography, morphology, pronunciation, vocabulary, syntax and style are highly standardised. Only the formularies of the private documents and the Proverbs of Ahiqar have maintained an older tradition of sentence structure and style. Imperial Aramaic immediately replaced Ancient Aramaic as a written language and, with slight modifications, it remained the official, commercial and literary language of the Near East until gradually, beginning with the fall of the Persian Empire (331 BC) and ending in the 4th century AD, it was replaced by Greek, Persian, the eastern and western dialects of Aramaic and Arabic, though not without leaving its traces in the written form of most of these. In its original Achaemenid form, Imperial Aramaic is found in texts of the 5th to 3rd centuries BC. These come mostly from Egypt and especially from the Jewish military colony of Elephantine, which existed at least from 530 to 399 BC.\n\n\n\nA history of Greek handwriting must be incomplete owing to the fragmentary nature of evidence. If one rules out the inscriptions on stone or metal, which belong to the science of , we are practically dependent for the period preceding the 4th or 5th century AD on the papyri from Egypt (cf. ), the earliest of which take back our knowledge only to the end of the 4th century BC. This limitation is less serious than might appear, since the few manuscripts not of Egyptian origin which have survived from this period, like the parchments from Avroman or Dura, the Herculaneum papyri, and a few documents found in Egypt but written elsewhere, reveal a uniformity of style in the various portions of the Greek world; but some differences can be discerned, and it is probable that, were there more material, distinct local styles could be traced.\n\nFurther, any given period several types of hand may exist together. There was a marked difference between the hand used for literary works (generally called \"uncials\" but, in the papyrus period, better styled \"book-hand\") and that of documents (\"cursive\") and within each of these classes several distinct styles were employed side by side; and the various types are not equally well represented in the surviving papyri.\n\nThe development of any hand is largely influenced by the materials used. To this general rule the Greek script is no exception. Whatever may have been the period at which the use of papyrus or leather as a writing material began in Greece (and papyrus was employed in the 5th century BC), it is highly probable that for some time after the introduction of the alphabet the characters were incised with a sharp tool on stones or metal far oftener than they were written with a pen. In cutting a hard surface, it is easier to form angles than curves; in writing the reverse is the case; hence the development of writing was from angular letters (\"capitals\") inherited from epigraphic style to rounded ones (\"uncials\"). But only certain letters were affected by this development, in particular E (uncial ε), Σ (c), Ω (ω), and to a lesser extent A (α).\n\nThe earliest Greek papyrus yet discovered is probably that containing the \"Persae\" of Timotheus, which dates from the second half of the 4th century BC and its script has a curiously archaic appearance. E, Σ, and Ω have the capital form, and apart from these test letters the general effect is one of stiffness and angularity. More striking is the hand of the earliest dated papyrus, a contract of 311 BC. Written with more ease and elegance, it shows little trace of any development towards a truly cursive style; the letters are not linked, and though the uncial c is used throughout, E and Ω have the capital forms. A similar impression is made by the few other papyri, chiefly literary, dating from about 300 BC; E may be slightly rounded, Ω approach the uncial form, and the angular Σ occurs as a letter only in the Timotheus papyrus, though it survived longer as a numeral (= 200), but the hands hardly suggest that for at least a century and a half the art of writing on papyrus had been well established. Yet before the middle of the 3rd century BC, one finds both a practised book-hand and a developed and often remarkably handsome cursive.\n\nThese facts may be due to accident, the few early papyri happening to represent an archaic style which had survived along with a more advanced one; but it is likely that there was a rapid development at this period, due partly to the opening of Egypt, with its supplies of papyri, and still more to the establishment of the great Alexandrian Library, which systematically copied literary and scientific works, and to the multifarious activities of Hellenistic bureaucracy. From here onward, the two types of script were sufficiently distinct (though each influenced the other) to require separate treatment. Some literary papyri, like the roll containing Aristotle's \"Constitution of Athens\", were written in cursive hands, and, conversely, the book-hand was occasionally used for documents. Since the scribe did not date literary rolls, such papyri are useful in tracing the development of the book-hand.\n\nThe documents of the mid-3rd century BC show a great variety of cursive hands. There are none from chancelleries of the Hellenistic monarchs, but some letters, notably those of Apollonius, the finance minister of Ptolemy II, to this agent, Zeno, and those of the Palestianian sheikh, Toubias, are in a type of script which cannot be very unlike the Chancery hand of the time, and show the Ptolemaic cursive at its best. These hands have a noble spaciousness and strength, and though the individual letters are by no means uniform in size there is a real unity of style, the general impression being one of breadth and uprightness. H, with the cross-stroke high, Π, Μ, with the middle stroke reduced to a very shallow curve, sometimes approaching a horizontal line, Υ, and Τ, with its cross-bar extending much further to the left than to the right of the up-stroke, Γ and Ν, whose last stroke is prolonged upwards above the line, often curving backwards, are all broad; ε, c, θ and β, which sometimes takes the form of two almost perpendicular strokes joined only at the top, are usually small; ω is rather flat, its second loop reduced to a practically straight line. Partly by the broad flat tops of the larger letters, partly by the insertion of a stroke connecting those (like H, Υ) which are not naturally adapted to linking, the scribes produced the effect of a horizontal line along the top of the writing, from which the letters seem to hang. This feature is indeed a general characteristic of the more formal Ptolemaic script, but it is specially marked in the 3rd century BC.\n\nBesides these hand of Chancery type, there are numerous less elaborate examples of cursive, varying according to the writer's skill and degree of education, and many of them strikingly easy and handsome. In some cursiveness is carried very far, the linking of letters reaching the point of illegibility, and the characters sloping to the right. A is reduced to a mere acute angle (∠), T has the cross-stroke only on the left, ω becomes an almost straight line, H acquires a shape somewhat like h, and the last stroke of N is extended far upwards and at times flattened out until it is little more than a diagonal stroke to the right. The attempt to secure a horizontal line along the top is here abandoned. This style was not due to inexpertness, but to the desire for speed, being used especially in accounts and drafts, and was generally the work of practised writers. How well established the cursive hand had now become is shown in some wax tablets of this period, the writing on which, despite the difference of material, closely resemble the hands of papyri.\n\nDocuments of the late 3rd and early 2nd centuries BC show, perhaps partly by the accident of survival (there is nothing analogous to the Apollonius letters, a loss of breadth and spaciousness. In the more formal types the letters stand rather stiffly upright, often without the linking strokes, and are more uniform in size; in the more cursive they are apt to be packed closely together. These features are more marked in the hands of the 2nd century. The less cursive often show am approximation to the book-hand, the letters growing rounder and less angular than in the 3rd century; in the more cursive linking was carried further, both by the insertion of coupling strokes and by the writing of several letters continuously without raising the pen, so that before the end of the century an almost current hand was evolved. A characteristic letter, which survived into the early Roman period, is T, with its cross-stroke made in two portions (variants:). In the 1st century, the hand tended, so far as can be inferred from surviving examples, to disintegrate; one can recognise the signs which portend a change of style, irregularity, want of direction, and the loss of the feeling for style. A fortunate accident has preserved two Greek parchments written in Parthia, one dated 88 BC, in a practically unligatured hand, the other, 22/21 BC, in a very cursive script of Ptolemaic type; and though each has non-Egyptian features the general character indicates a uniformity of style in the Hellenistic world.\n\nThe development of the Ptolemaic book-hand is difficult to trace, as there are few examples, mostly not datable on external grounds. Only for the 3rd century BC have we a secure basis. The hands of that period have an angular appearance; there is little uniformity in the size of individual letters, and though sometimes, notably in the Petrie papyrus containing the \"Phaedo\" of Plato, a style of considerable delicacy is attained, the book-hand in general shows less mastery than the contemporary cursive. In the 2nd century the letters grew rounder and more uniform in size, but in the 1st century there is perceptible, here as in the cursive hand, a certain disintegration. Probably at no time did the Ptolemaic book-hand acquire such unity of stylistic effect as the cursive.\n\nPapyri of the Roman period are far more numerous and show greater variety. The cursive of the 1st century has a rather broken appearance, part of one character being often made separately from the rest and linked to the next letter. A form characteristic of the 1st and 2nd century and surviving after that only as a fraction sign (=⅛) is η in the shape . By the end of the 1st century, there had been developed several excellent types of cursive, which, though differing considerably both in the forms of individual letters and in general appearance, bear a family likeness to one another. Qualities which are specially noticeable are roundness in the shape of letters, continuity of formation, the pen being carried on from character to character, and regularity, the letters not differing strikingly in size and projecting strokes above or below the line being avoided. Sometimes, especially in tax-receipts and in stereotyped formulae, cursiveness is carried to an extreme. In a letter of the prefect, dated in 209, we have a fine example of the Chancery hand, with tall and laterally compressed letters, ο very narrow and α and ω often written high in the line. This style, from at least the latter part of the 2nd century, exercised considerable influence on the local hands, many of which show the same characteristics less pronounced; and its effects may be traced into the early part of the 4th century. Hands of the 3rd century uninfluenced by it show a falling off from the perfection of the 2nd century; stylistic uncertainty and a growing coarseness of execution mark a period of decline and transition.\n\nSeveral different types of book-hand were used in the Roman period. Particularly handsome is a round, upright hand seen, for example, in a British Museum papyrus containing \"Odyssey\" III. The cross-stroke of ε is high, Μ deeply curved and Α has the form α. Uniformity of size is well attained, and a few strokes project, and these but slightly, above or below the line. Another type, well called by palaeographer Schubart the \"severe\" style, has a more angular appearance and not infrequently slopes to the right; though handsome, it has not the sumptuous appearance of the former. There are various classes of a less pretentious style, in which convenience rather than beauty was the first consideration and no pains were taken to avoid irregularities in the shape and alignment of the letters. Lastly may be mentioned a hand which is of great interest as being the ancestor of the type called (from its later occurrence in vellum codices of the Bible) the biblical hand. This, which can be traced back at least the late 2nd century, has a square, rather heavy appearance; the letters, of uniform size, stand upright, and thick and thin strokes are well distinguished. In the 3rd century the book-hand, like the cursive, appears to have deteriorated in regularity and stylistic accomplishment.\n\nIn the charred rolls found at Herculaneum and dating from about the beginning of our era, are specimens of Greek literary hands from outside Egypt; and a comparison with the Egyptian papyri reveals great similarity in style and shows that conclusions drawn from the henads of Egypt may, with caution, be applied to the development of writing in the Greek world generally.\n\nThe cursive hand of the 4th century shows some uncertainty of character. Side by side with the style founded on the Chancery hand, regular in formation and with tall and narrow letters, which characterised the period of Diocletian, and lasted well into the century, we find many other types mostly marked by a certain looseness and irregularity. A general progress towards a florid and sprawling hand is easily recognisable, but a consistent and deliberate style was hardly evolved before the 5th century, from which unfortunately few dated documents have survived. Byzantine cursive tends to an exuberant hand, in which the long strokes are excessively extended and individual letters often much enlarged. But not a few hands of the 5th and 6th centuries are truly handsome and show considerable technical accomplishment. Both an upright and a sloping type occur and there are many less ornamental hands, but there gradually emerged towards the 7th century two general types, one (especially used in letters and contracts) a current hand, sloping to the right, with long strokes in such characters at τ, ρ, ξ, η (which has the h shape), ι, and κ, and with much linking of letters, and another (frequent in accounts), which shows, at least in essence, most of the forms of the later minuscule. (cf. below.) This is often upright, though a slope to the right is quite common, and sometimes, especially in one or two documents of the early Arab period, it has an almost calligraphic effect.\n\nIn the Byzantine period, the book-hand, which in earlier times had more than once approximated to the contemporary cursive, diverged widely from it.\n\nThe change from papyrus to vellum involved no such modification in the forms of letters as followed that from metal to papyrus. The justification for considering the two materials separately is that after the general adoption of vellum, the Egyptian evidence is first supplemented and later superseded by that of manuscripts from elsewhere, and that during this period the hand most used was one not previously employed for literary purposes.\n\nThe prevailing type of book-hand during what in papyrology is called the Byzantine period, that is, roughly from AD 300 to 650, is known as the biblical hand. It went back to at least the end of the 2nd century and had had originally no special connection with Christian literature. In manuscripts, whether vellum or paper, of the 4th century found in Egypt are met other forms of script, particularly a sloping, rather inelegant hand derived from the literary hand of the 3rd century, which persisted to at least the 5th century; but the three great early codices of the Bible are all written in uncials of the biblical type. In the Vaticanus, placed in the 4th century, the characteristics of the hand are least strongly marked; the letters have the forms characteristic of the type but without the heavy appearance of later manuscripts, and the general impression is one of greater roundness. In the Sinaiticus, which is not much later, the letters are larger and more heavily made; and in the Alexandrinus (5th century) a later development is seen, with emphatic distinction of thick and thin strokes. By the 6th century, alike in vellum and in papyrus manuscripts, the heaviness had become very marked, though the hand still retained, in its best examples, a handsome appearance; but after this it steadily deteriorated, becoming ever more mechanical and artificial. The thick strokes grew heavier; the cross strokes of T and Θ and the base of Δ were furnished with drooping spurs. The hand, which is often singularly ugly, passed through various modifications, now sloping, now upright, though it is not certain that these variations were really successive rather than concurrent. A different type of uncials, derived from the Chancery hand and seen in two papyrus examples of the Festal letters despatched annually by the Patriarch of Alexandria, was occasionally used, the best known example being the Codex Marchalianus (6th or 7th century). A combination of this hand with the other type is also known.\n\nThe uncial hand lingered on, mainly for liturgical manuscripts, where a large and easily legible script was serviceable, as late as the 12th century, but in ordinary use it had long been superseded by a new type of hand, the minuscule, which originated in the 8th century, as an adaptation to literary purposes of the second of the types of Byzantine cursive mentioned above. A first attempt at a calligraphic use of this hand, seen in one or two manuscripts of the 8th or early 9th century, in which it slopes to the right and has a narrow, angular appearance, did not find favour, but by the end of the 9th century a more ornamental type, from which modern Greek script descended, was already established. It has been suggested that it was evolved in the Monastery of Stoudios at Constantinople. In its earliest examples it is upright and exact but lacks flexibility; accents are small, breathings square in formation, and in general only such ligatures are used as involve no change in the shape of letters. The single forms have a general resemblance (with considerable differences in detail) both to the minuscule cursive of late papyri, and to those used in modern Greek type; uncial forms were avoided.\n\nIn the course of the 10th century the hand, without losing its beauty and exactness, gained in freedom. Its finest period was from the 9th to the 12th century, after which it rapidly declined. The development was marked by a tendency\nBut from the first there were several styles, varying from the formal, regular hands characteristic of service books to the informal style, marked by numerous abbreviations, used in manuscripts intended only for a scholar's private use. The more formal hands were exceedingly conservative, and there are few classes of script more difficult to date than the Greek minuscule of this class. In the 10th, 11th and 12th centuries a sloping hand, less dignified than the upright, formal type, but often very handsome, was especially used for manuscripts of the classics.\n\nHands of the 11th century are marked in general (though there are exceptions) by a certain grace and delicacy, exact but easy; those of the 12th by a broad, bold sweep and an increasing freedom, which readily admits uncial forms, ligatures and enlarged letters but has not lost the sense of style and decorative effect. In the 13th and still more in the 14th centuries there was a steady decline; the less formal hands lost their beauty and exactness, becoming ever more disorderly and chaotic in their effect, while formal style imitated the precision of an earlier period without attaining its freedom and naturalness, and often appears singularly lifeless. In the 15th century, especially in the West, where Greek scribes were in request to produce manuscripts of the classical authors, there was a revival, and several manuscripts of this period, though markedly inferior to those of the 11th and 12th centuries, are by no means without beauty.\n\nIn the book-hand of early papyri, neither accents nor breathings were employed. Their use was established by the beginning of the Roman period, but was sporadic in papyri, where they were used as an aid to understanding, and therefore more frequently in poetry than prose, and in lyrical oftener than in other verse. In the cursive of papyri they are practically unknown, as are marks of punctuation. Punctuation was effected in early papyri, literary and documentary, by spaces, reinforced in the book-hand by the paragraphos, a horizontal stroke under the beginning of the line. The coronis, a more elaborate form of this, marked the beginning of lyrics or the principal sections of a longer work. Punctuation marks, the comma, the high, low and middle points, were established in the book-hand by the Roman period; in early Ptolemaic papyri, a double point (:) is found.\n\nIn vellum and paper manuscripts, punctuation marks and accents were regularly used from at least the 8th century, though with some differences from modern practice. At no period down to the invention of printing did Greek scribes consistently separate words. The book-hand of papyri aimed at an unbroken succession of letters, except for distinction of sections; in cursive hands, especially where abbreviations were numerous, some tendency to separate words may be recognised, but in reality it was phrases or groups of letters rather than words which were divided. In the later minuscule word-division is much commoner but never became systematic, accents and breathings serving of themselves to indicate the proper division.\n\nThe view that the art of writing in India developed gradually, as in other areas of the world, by going through the stages of pictographic, ideographic and transitional phases of the phonetic script, which in turn developed into syllabic and alphabetic scripts was challenged by Falk and others in the early 1990s. In the new paradigm, Indian alphabetic writing, called Brāhmī, was discontinuous with earlier, undeciphered, glyphs, and was invented specifically by King Ashoka for application in his royal edicts. In the subcontinent, three scripts like Indus, Kharoṣṭhī and Brāhmī became prevalent. In addition, Greek and Arabic scripts were also added to the Indian context after their penetration in the early centuries of the common era (CE). The decipherment and subsequent development of Indus glyphs is also a matter for continuing research and discussion. After a lapse of a few centuries the Kharoṣṭhī script became obsolete; the Greek script in India went through a similar fate and disappeared. But the Brāhmī and Arabic scripts endured for a much longer period. Moreover, there was a change and development in the Brāhmī script which may be traced in time and space through the Maurya, Kuṣāṇa, Gupta and early medieval periods. The present day Nāgarī script is derived from Brāhmī. The Brāhmī is also the ancestral script of many other Indian scripts, in northern and southern South Asia. Legends and inscriptions in Brāhmī are engraved upon leather, wood, terracotta, ivory, stone, copper, bronze, silver and gold. Arabic got an important place, particularly in the royalty, during the medieval period and it provides rich material for history writing.\n\nMost of the available inscriptions and manuscripts written in the above scripts—in languages like Prākrita, Pāḷi, Saṃskṛta, Apabhraṃśa, Tamil and Persian—have been read and exploited for history writing, but numerous inscriptions preserved in different museums still remain undeciphered for lack of competent palaeographic Indologists, as there is a gradual decline in the subcontinent of such disciplines as palaeography, epigraphy and numismatics. The discipline of ancient Indian scripts and the languages they are written needs new scholars who, by adopting traditional palaeographic methods and modern technology, may decipher, study and transcribe the various types of epigraphs and legends still extant today.\n\nThe language of the earliest written records, that is, the Edicts of Ashoka, is Prakrit. Besides Prakrit, the Ashokan edicts are also written in Greek and Aramaic. Moreover, all the edicts of Ashoka engraved in the Kharoshthi and Brahmi scripts are in the Prakrit language: thus, originally the language employed in the inscriptions was Prakrit, with Sanskrit adopted at a later stage. Past the period of the Maurya Empire, the use of Prakrit continued in inscriptions for a few more centuries. In north India, Prakrit was replaced by Sanskrit by the end of the 3rd century, while this change took place about a century later in south India. Some of the inscriptions though written in Prakrit, were influenced by Sanskrit and vice versa. The epigraphs of the Kushana kings are found in a mixture of Prakrit and Sanskrit, while the Mathura inscriptions of the time of Sodasa, belonging to the first quarter of the 1st century, contain verses in classical Sanskrit. From the 4th century onwards, the Guptas came to power and made Sanskrit flourish by supporting it in language and literature.\n\nIn western India and also in some regions of Andhra Pradesh and Karnataka, Prakrit was used till the 4th century, mostly in the Buddhist writings though in a few contemporary records of the Ikshvakus of Nagarjunakonda, Sanskrit was applied. The inscription of Yajna Sri Satakarni (2nd century) from Amaravati is considered to be the earliest so far. The earlier writings (4th century) of Salankayanas of the Telugu region are in Prakrit, while their later records (belonging to the 5th century) are written in Sanskrit. In the Kannada speaking area, inscriptions belonging to later Satavahanas and Chutus were written in Prakrit. From the 4th century onwards, with the rise of the Guptas, Sanskrit became the predominant language of India and continued to be employed in texts and inscriptions of all parts of India along with the regional languages in the subsequent centuries. The copper-plate charters of the Pallavas, the Cholas and the Pandyas documents are written in both Sanskrit and Tamil. Kannada is used in texts dating from about the 5th century and the Halmidi inscription is considered to be the earliest epigraph written in the Kannada language. Inscriptions in Telugu began to appear from the 6th or 7th century. Malayalam made its beginning in writings from the 15th century onwards.\n\nIn north India, the Brahmi script was used over a vast area; however, Ashokan inscriptions are also found using Kharoshthi, Aramaic and Greek scripts. With the advent of the Saka-Kshatrapas and the Kushanas as political powers in north India, the writing system underwent a definite change due to the use of new writing tools and techniques. Further development of the Brahmi script and perceivable changes in its evolutionary trend can be discerned during the Gupta period: in fact, the Gupta script is considered to be the successor of the Kushana script in north India.\n\nFrom the 6th to about the 10th century of the common era, the inscriptions in north India were written in a script variously named, e.g., Siddhamatrika and Kutila (\"Rañjanā script\"). From the 8th century, Siddhamatrika developed into the Śāradā script in Kashmir and Punjab, into Proto-Bengali or Gaudi in Bengal and Orissa, and into Nagari in other parts of north India. Nāgarī script was used widely in northern India from the 10th century onwards. The use of Nandinagari, a variant of Nagari script, is mostly confined to the Karnataka region.\n\nIn central India, mostly in Madhya Pradesh, the inscriptions of the Vakatakas, and the kings of Sarabhapura and Kosala were written in what are known as \"box-headed\" and \"nail-headed\" characters. It may be noted that the early Kadambas of Karnataka also employed \"nail-headed\" characters in some of their inscriptions. During the 3rd–4th century, the script used in the inscriptions of Ikshvakus of Nagarjunakonda developed a unique style of letter-forms with elongated verticals and artistic flourishes, which did not continue after their rule.\n\nThe earliest attested form of writing in South India is represented by inscriptions found in caves, associated with the Chalukya and Chera dynasties. These are written in variants of what is known as the Cave character, and their script differs from the Northern version in being more angular. Most of the modern scripts of South India have evolved from this script, with the exception of Vatteluttu, the exact origins of which are unknown, and Nandinagari, which is a variant of Devanagari that developed due to later Northern influence. In south India from the 7th century of the common era onwards, a number of inscriptions belonging to the dynasties of Pallava, Chola and Pandya are found. These records are written in three different scripts known as Tamil, Vattezhuttu and Grantha scripts, the last variety being used to write Sanskrit inscriptions. In the Kerala region, the Vattezhuttu script developed into a still more cursive script called Kolezhuthu during the 14th and 15th centuries. At the same time, the modern Malayalam script developed out of the Grantha script. The early form of the Telugu-Kannada script is found in the inscriptions of the early Kadambas of Banavasi and the early Chalukyas of Badami in the west, and Salankayana and the early Eastern Chalukyas in the east who ruled the Kannada and Telugu speaking areas respectively, during the 4th to 7th centuries.\n\nAttention should be drawn at the outset to certain fundamental definitions and principles of the science. The original characters of an alphabet are modified by the material and the implements used. When stone and chisel are discarded for papyrus and reed-pen, the hand encounters less resistance and moves more rapidly. This leads to changes in the size and position of the letters, and then to the joining of letters, and, consequently, to altered shapes. We are thus confronted at an early date with quite distinct types. The majuscule style of writing, based on two parallel lines, ADPL, is opposed to the minuscule, based on a system of four lines, with letters of unequal height, adpl. Another classification, according to the care taken in forming the letters, distinguishes between the set book-hand and the cursive script. The difference in this case is determined by the subject matter of the text; the writing used for books (\"scriptura libraria\") is in all periods quite distinct from that used for letters and documents (\"epistolaris, diplomatica\"). While the set book-hand, in majuscule or minuscule, shows a tendency to stabilise the forms of the letters, the cursive, often carelessly written, is continually changing in the course of years and according to the preferences of the writers.\n\nThis being granted, a summary survey of the morphological history of the Latin alphabet shows the zenith of its modifications at once, for its history is divided into two very unequal periods, the first dominated by majuscule and the second by minuscule writing.\n\nJean Mabillon, a French Benedictine monk, scholar and antiquary, whose work \"De re diplomatica\" was published in 1681, is widely regarded as the founder of the twin disciplines of palaeography and diplomatics. However, the actual term \"palaeography\" was coined (in Latin) by Bernard de Montfaucon, a Benedictine monk, in the title of his \"Palaeographia Graeca\" (1708), which remained a standard work in the specific field of Greek palaeography for more than a century. With their establishment of palaeography, Mabillon and his fellow Benedictines were responding to the Jesuit Daniel Papebroch, who doubted the authenticity of some of the documents which the Benedictines offered as credentials for the authorisation of their monasteries. In the 19th century such scholars as Wilhelm Wattenbach, Leopold Delisle and Ludwig Traube contributed greatly to making palaeography independent from diplomatic. In the 20th century, the 'New French School' of palaeographers, especially Jean Mallon, gave a new direction to the study of scripts by stressing the importance of ductus (the shape and order of the strokes used to compose letters) in studying the historical development of scripts.\n\nThe Latin alphabet first appears in the epigraphic type of majuscule writing, known as capitals. These characters form the main stem from which developed all the branches of Latin writing. On the oldest monuments (the \"inscriptiones bello Hannibalico antiquiores\" of the \"Corpus Inscriptionum Latinarum = CIL\"), it is far from showing the orderly regularity of the later period. Side by side with upright and square characters are angular and sloping forms, sometimes very distorted, which seem to indicate the existence of an early cursive writing from which they would have been borrowed. Certain literary texts clearly allude to such a hand. Later, the characters of the cursive type were progressively eliminated from formal inscriptions, and capital writing reached its perfection in the Augustan Age.\n\nEpigraphists divide the numerous inscriptions of this period into two quite distinct classes: \"tituli\", or formal inscriptions engraved on stone in elegant and regular capitals, and \"acta\", or legal texts, documents, etc., generally engraved on bronze in cramped and careless capitals. Palaeography inherits both these types. Reproduced by scribes on papyrus or parchment, the elegant characters of the inscriptions become the square capitals of the manuscripts, and the \"actuaria\", as the writing of the \"acta\" is called, becomes the rustic capital.\n\nOf the many books written in square capitals, the \"éditions de luxe\" of ancient times, only a few fragments have survived, the most famous being pages from manuscripts of Virgil. The finest examples of rustic capitals, the use of which is attested by papyri of the 1st century, are to be found in manuscripts of Virgil and Terence. Neither of these forms of capital writing offers any difficulty in reading, except that no space is left between the words. Their dates are still uncertain, in spite of attempts to determine them by minute observation.\n\nThe rustic capitals, more practical than the square forms, soon came into general use. This was the standard form of writing, so far as books are concerned, until the 5th century, when it was replaced by a new type, the uncial, which is discussed below.\n\nWhile the set book-hand, in square or rustic capitals, was used for the copying of books, the writing of everyday life, letters and documents of all kinds, was in a cursive form, the oldest examples of which are provided by the graffiti on walls at Pompeii (\"CIL\", iv), a series of waxen tablets, also discovered at Pompeii (\"CIL\", iv, supplement), a similar series found at Verespatak in Transylvania (\"CIL\", iii) and a number of papyri. From a study of a number of documents which exhibit transitional forms, it appears that this cursive was originally simplified capital writing. The evolution was so rapid, however, that at quite an early date the \"scriptura epistolaris\" of the Roman world can no longer be described as capitals. By the 1st century, this kind of writing began to develop the principal characteristics of two new types: the uncial and the minuscule cursive. With the coming into use of writing surfaces which were smooth, or offered little resistance, the unhampered haste of the writer altered the shape, size and position of the letters. In the earliest specimens of writing on wax, plaster or papyrus, there appears a tendency to represent several straight strokes by a single curve. The cursive writing thus foreshadows the specifically uncial forms. The same specimens show great inequality in the height of the letters; the main strokes are prolonged upwards (= b; = d) or downwards (= q; = s). In this direction, the cursive tends to become a minuscule hand.\n\nAlthough the characteristic forms of the uncial type appear to have their origin in the early cursive, the two hands are nevertheless quite distinct. The uncial is a \"libraria\", closely related to the capital writing, from which it differs only in the rounding off of the angles of certain letters, principally . It represents a compromise between the beauty and legibility of the capitals and the rapidity of the cursive, and is clearly an artificial product. It was certainly in existence by the latter part of the 4th century, for a number of manuscripts of that date are written in perfect uncial hands (\"Exempla\", pl. XX). It presently supplanted the capitals and appears in numerous manuscripts which have survived from the 5th, 6th and 7th centuries, when it was at its height. By this time it had become an imitative hand, in which there was generally no room for spontaneous development. It remained noticeably uniform over a long period. It is difficult therefore to date the manuscripts by palaeographical criteria alone. The most that can be done is to classify them by centuries, on the strength of tenuous data. The earliest uncial writing is easily distinguished by its simple and monumental character from the later hands, which become progressively stiff and affected.\n\nIn the ancient cursive writing, from the 1st century onward, there are symptoms of transformation in the form of certain letters, the shape and proportions of which correspond more closely to the definition of minuscule writing than to that of majuscule. Rare and irregular at first, they gradually become more numerous and more constant and by degrees supplant the majuscule forms, so that in the history of the Roman cursive there is no precise boundary between the majuscule and minuscule periods.\n\nThe oldest example of minuscule cursive writing that has been discovered is a letter on papyrus, found in Egypt, dating from the 4th century. This marks a highly important date in the history of Latin writing, for with only one known exception, not yet adequately explained—two fragments of imperial rescripts of the 5th century—the minuscule cursive was consequently the only \"scriptura epistolaris\" of the Roman world. The ensuing succession of documents show a continuous improvement in this form of writing, characterised by the boldness of the strokes and by the elimination of the last lingering majuscule forms. The Ravenna deeds of the 5th and 6th centuries exhibit this hand at its perfection.\n\nAt this period, the minuscule cursive made its appearance as a \"book hand\", first as marginal notes, and later for the complete books themselves. The only difference between the book-hand and that used for documents is that the principal strokes are shorter and the characters thicker. This form of the hand is usually called \"semi-cursive\".\n\nThe fall of the Empire and the establishment of the barbarians within its former boundaries did not interrupt the use of the Roman minuscule cursive hand, which was adopted by the newcomers. But for gaps of over a century in the chronological series of documents which have been preserved, it would be possible to follow the evolution of the Roman cursive into the so-called \"national hands\", forms of minuscule writing which flourished after the barbarian invasions in Italy, France, Spain, England and Ireland, and which are still known as Lombardic, Merovingian, Visigothic, Anglo-Saxon and Irish. These names came into use at a time when the various national hands were believed to have been invented by the peoples who used them, but their connotation is merely geographical. Nevertheless, in spite of a close resemblance which betrays their common origin, these hands are specifically different, perhaps because the Roman cursive was developed by each nation in accordance with its artistic tradition.\n\n\nIn Italy, after the close of the Roman and Byzantine periods, the writing is known as Lombardic, a generic term which comprises several local varieties. These may be classified under four principal types: two for the \"scriptura epistolaris\", the old Italian cursive and the papal chancery hand, or \"littera romana\", and two for the \"libraria\", the old Italian book-hand and Lombardic in the narrow sense, sometimes known as \"Beneventana\" on account of the fact that it flourished in the principality of Benevento.\n\nThe oldest preserved documents written in the old Italian cursive show all the essential characteristics of the Roman cursive of the 6th century. In northern Italy, this hand began in the 9th century to be influenced by a minuscule book-hand which developed, as will be seen later, in the time of Charlemagne; under this influence it gradually disappeared, and ceased to exist in the course of the 12th century. In southern Italy, it persisted far on into the later Middle Ages. The papal chancery hand, a variety of Lombardic peculiar to the vicinity of Rome and principally used in papal documents, is distinguished by the formation of the letters \"a, e, q, t\". It is formal in appearance at first, but is gradually simplified, under the influence of the Carolingian minuscule, which finally prevailed in the bulls of Honorius II (1124–1130). The notaries public in Rome continued to use the papal chancery hand until the beginning of the 13th century. The old Italian book-hand is simply a semi-cursive of the type already described as in use in the 6th century. The principal examples are derived from \"scriptoria\" in northern Italy, where it was displaced by the Carolingian minuscule during the 9th century. In southern Italy, this hand persisted, developing into a calligraphic form of writing, and in the 10th century took on a very artistic angular appearance. The \"Exultet\" rolls provide the finest examples. In the 9th century, it was introduced in Dalmatia by the Benedictine monks and developed there, as in Apulia, on the basis of the archetype, culminating in a rounded \"Beneventana\" known as the \"Bari type\".\n\n\nThe offshoot of the Roman cursive which developed in Gaul under the first dynasty of kings is called Merovingian writing. It is represented by thirty-eight royal diplomas, a number of private charters and the authenticating documents of relics.\n\nThough less than a century intervenes between the Ravenna cursive and the oldest extant Merovingian document (AD 625), there is a great difference in appearance between the two writings. The facile flow of the former is replaced by a cramped style, in which the natural slope to the right gives way to an upright hand, and the letters, instead of being fully outlined, are compressed to such an extent that they modify the shape of other letters. Copyists of books used a cursive similar to that found in documents, except that the strokes are thicker, the forms more regular, and the heads and tails shorter. The Merovingian cursive as used in books underwent simplification in some localities, undoubtedly through the influence of the minuscule book-hand of the period. The two principal centres of this reform were Luxeuil and Corbie.\n\n\nIn Spain, after the Visigothic conquest, the Roman cursive gradually developed special characteristics. Some documents attributed to the 7th century display a transitional hand with straggling and rather uncouth forms. The distinctive features of Visigothic writing, the most noticeable of which is certainly the q-shaped g, did not appear until later, in the book-hand. The book-hand became set at an early date. In the 8th century it appears as a sort of semi-cursive; the earliest example of certain date is ms lxxxix in the Capitular Library in Verona. From the 9th century the calligraphic forms become broader and more rounded until the 11th century, when they become slender and angular. The Visigothic minuscule appears in a cursive form in documents about the middle of the 9th century, and in the course of time grows more intricate and consequently less legible. It soon came into competition with the Carolingian minuscule, which supplanted it as a result of the presence in Spain of French elements such as Cluniac monks and warriors engaged in the campaign against the Moors.\n\nThe Irish and Anglo-Saxon hands, which were not directly derived from the Roman minuscule cursive, will be discussed in a separate sub-section below.\n\nOne by one, the national minuscule cursive hands were replaced by a set minuscule hand which has already been mentioned and its origins may now be traced from the beginning.\n\nThe early cursive was the medium in which the minuscule forms were gradually evolved from the corresponding majuscule forms. Minuscule writing was therefore cursive in its inception. As the minuscule letters made their appearance in the cursive writing of documents, they were adopted and given calligraphic form by the copyists of literary texts, so that the set minuscule alphabet was constituted gradually, letter by letter, following the development of the minuscule cursive. Just as some documents written in the early cursive show a mixture of majuscule and minuscule forms, so certain literary papyri of the 3rd century, and inscriptions on stone of the 4th century yield examples of a mixed set hand, with minuscule forms side by side with capital and uncial letters. The number of minuscule forms increases steadily in texts written in the mixed hand, and especially in marginal notes, until by the end of the 5th century the majuscule forms have almost entirely disappeared in some manuscripts. This quasi-minuscule writing, known as the \"half-uncial\" thus derives from a long line of mixed hands which, in a synoptic chart of Latin scripts, would appear close to the oldest \"librariae\", and between them and the \"epistolaris\" (cursive), from which its characteristic forms were successively derived. It had a considerable influence on the continental \"scriptura libraria\" of the 7th and 8th centuries.\n\nThe half-uncial hand was introduced in Ireland along with Latin culture in the 5th century by priests and laymen from Gaul, fleeing before the barbarian invasions. It was adopted there to the exclusion of the cursive, and soon took on a distinct character. There are two well established classes of Irish writing as early as the 7th century: a large round half-uncial hand, in which certain majuscule forms frequently appear, and a pointed hand, which becomes more cursive and more genuinely minuscule. The latter developed out of the former. One of the distinguishing marks of manuscripts of Irish origin is to be found in the initial letters, which are ornamented by interlacing, animal forms, or a frame of red dots. The most certain evidence, however, is provided by the system of abbreviations and by the combined square and cuneiform appearance of the minuscule at the height of its development. The two types of Irish writing were introduced in the north of Great Britain by the monks, and were soon adopted by the Anglo-Saxons, being so exactly copied that it is sometimes difficult to determine the origin of an example. Gradually, however, the Anglo-Saxon writing developed a distinct style, and even local types, which were superseded after the Norman conquest by the Carolingian minuscule. Through St Columba and his followers, Irish writing spread to the continent, and manuscripts were written in the Irish hand in the monasteries of Bobbio Abbey and St Gall during the 7th and 8th centuries.\n\nJames J. John points out that the disappearance of imperial authority around the end of the 5th century in most of the Latin-speaking half of the Roman Empire does not entail the disappearance of the Latin scripts, but rather introduced conditions that would allow the various provinces of the West gradually to drift apart in their writing habits, a process that began around the 7th century.\n\nPope Gregory I (Gregory the Great, d. 604) was influential in the spread of Christianity to Britain and also sent Queens Theodelinde and Brunhilda, as well as Spanish bishops, copies of manuscripts. Furthermore, he sent the Roman monk Augustine of Canterbury to Britain on a missionary journey, on which Augustine may have brought manuscripts. Although Italy's dominance as a centre of manuscript production began to decline, especially after the Gothic War (535–554) and the invasions by the Lombards, its manuscripts—and more important, the scripts in which they were written—were distributed across Europe.\n\nFrom the 6th through the 8th centuries, a number of so-called 'national hands' were developed throughout the Latin-speaking areas of the former Roman Empire. By the late 6th century Irish scribes had begun transforming Roman scripts into Insular minuscule and majuscule scripts. A series of transformations, for book purposes, of the cursive documentary script that had grown out of the later Roman cursive would get under way in France by the mid-7th century. In Spain half-uncial and cursive would both be transformed into a new script, the Visigothic minuscule, no later than the early 8th century.\n\nBeginning in the 8th century, as Charlemagne began to consolidate power over a large area of western Europe, scribes developed a minuscule script (Caroline minuscule) that effectively became the standard script for manuscripts from the 9th to the 11th centuries. The origin of this hand is much disputed. This is due to the confusion which prevailed before the Carolingian period in the \"libraria\" in France, Italy and Germany as a result of the competition between the cursive and the set hands. In addition to the calligraphic uncial and half-uncial writings, which were imitative forms, little used and consequently without much vitality, and the minuscule cursive, which was the most natural hand, there were innumerable varieties of mixed writing derived from the influence of these hands on each other. In some, the uncial or half-uncial forms were preserved with little or no modification, but the influence of the cursive is shown by the freedom of the strokes; these are known as rustic, semi-cursive or cursive uncial or half-uncial hands. Conversely, the cursive was sometimes affected, in varying degrees, by the set \"librariae\"; the cursive of the \"epistolaris\" became a semi-cursive when adopted as a \"libraria\". Nor is this all. Apart from these reciprocal influences affecting the movement of the hand across the page, there were morphological influences at work, letters being borrowed from one alphabet for another. This led to compromises of all softs and of infinite variety between the uncial and half-uncial and the cursive. It will readily be understood that the origin of the Carolingian minuscule, which must be sought in this tangle of pre-Carolingian hands, involves disagreement. The new writing is admittedly much more closely related to the \"epistolaris\" than the primitive minuscule; this is shown by certain forms, such as the open a (), which recall the cursive, by the joining of certain letters, and by the clubbing of the tall letters b d h l, which resulted from a cursive \"ductus\". Most palaeographers agree in assigning the new hand the place shown in the following table:\n\nControversy turns on the question whether the Carolingian minuscule is the primitive minuscule as modified by the influence of the cursive or a cursive based on the primitive minuscule. Its place of origin is also uncertain: Rome, the Palatine school, Tours, Reims, Metz, Saint-Denis and Corbie have been suggested, but no agreement has been reached. In any case, the appearance of the new hand is a turning point in the history of culture. So far as Latin writing is concerned, it marks the dawn of modern times.\n\nIn the 12th century, Carolingian minuscule underwent a change in its appearance and adopted bold and broken Gothic letter-forms. This style remained predominant, with some regional variants, until the 15th century, when the Renaissance humanistic scripts revived a version of Carolingian minuscule. It then spread from the Italian Renaissance all over Europe.\n\nThese humanistic scripts are the base for the antiqua and the handwriting forms in western and southern Europe. In Germany and Austria, the \"Kurrentschrift\" was rooted in the cursive handwriting of the later Middle Ages. With the name of the calligrapher Ludwig Sütterlin, this handwriting counterpart to the blackletter typefaces was abolished by Hitler in 1941. After World War II, it was taught as an alternative script in some areas until the 1970s; it is no longer taught. Secretary hand is an informal business hand of the Renaissance.\n\nThere are undeniable points of contact between architecture and palaeography, and in both it is possible to distinguish a Romanesque and a Gothic period . The creative effort which began in the post-Carolingian period culminated at the beginning of the 12th century in a calligraphy and an architecture which, though still somewhat awkward, showed unmistakable signs of power and experience, and at the end of that century and in the first half of the 13th both arts reached their climax and made their boldest flights. The topography of later medieval writing is still being studied; national varieties can, of course, be' identified but the problem of distinguishing features becomes complicated as a result of the development of international relations, and the migration of clerks from one end of Europe to the other. During the later centuries of the Middle Ages the Gothic minuscule continued to improve within the restricted circle of \"de luxe\" editions and ceremonial documents. In common use, it degenerated into a cursive which became more and more intricate, full of superfluous strokes and complicated by abbreviations. In the first quarter of the 15th century an innovation took place which exercised a decisive influence on the evolution of writing in Europe. The Italian humanists were struck by the eminent legibility of the manuscripts, written in the improved Carolingian minuscule of the 10th and 11th centuries, in which they discovered the works of ancient authors, and carefully imitated the old writing. In Petrarch's compact book hand, the wider leading and reduced compression and round curves are early manifestations of the reaction against the crabbed Gothic secretarial minuscule we know today as \"blackletter\"; Petrarch was one of the few medieval authors to have written at any length on the handwriting of his time; in his essay on the subject, \"La scrittura\" he criticized the current scholastic hand, with its laboured strokes (\"artificiosis litterarum tractibus\") and exuberant (\"luxurians\") letter-forms amusing the eye from a distance, but fatiguing on closer exposure, as if written for other purpose than to be read. For Petrarch the gothic hand violated three principles: writing, he said, should be simple (\"castigata\"), clear (\"clara\") and orthographically correct. Boccaccio was a great admirer of Petrarch; from Boccaccio's immediate circle this post-Petrarchan \"semi-gothic\" revised hand spread to \"literati\" in Florence, Lombardy and the Veneto. A more thorough reform of handwriting than the Petrarchan compromise was in the offing. The generator of the new style (\"illustration\") was Poggio Bracciolini, a tireless pursuer of ancient manuscripts, who developed the new humanist script in the first decade of the 15th century. The Florentine bookseller Vespasiano da Bisticci recalled later in the century that Poggio had been a very fine calligrapher of \"lettera antica\" and had transcribed texts to support himself—presumably, as Martin Davies points out— before he went to Rome in 1403 to begin his career in the papal curia. Berthold Ullman identifies the watershed moment in the development of the new humanistic hand as the youthful Poggio's transcription of Cicero's \"Epistles to Atticus\". By the time the Medici library was catalogued in 1418, almost half the manuscripts were noted as in the \"lettera antica\". The new script was embraced and developed by the Florentine humanists and educators Niccolò de' Niccoli and Coluccio Salutati. The papal chancery adopted the new fashion for some purposes, and thus contributed to its diffusion throughout Christendom. The printers played a still more significant part in establishing this form of writing by using it, from the year 1465, as the basis for their types. The humanistic minuscule soon gave rise to a sloping cursive hand, known as the Italian, which was also taken up by printers in search of novelty and thus became the italic type. In consequence, the Italian hand became widely used, and in the 16th century began to compete with the Gothic cursive. In the 17th century, writing masters were divided between the two schools, and there was in addition a whole series of compromises. The Gothic characters gradually disappeared, except a few that survived in Germany. The Italian became universally used, brought to perfection in more recent times by English calligraphers.\n\n"}
{"id": "4574535", "url": "https://en.wikipedia.org/wiki?curid=4574535", "title": "Paremiology", "text": "Paremiology\n\nParemiology () is the collection and study of proverbs.\n\nParemiology can be dated back as far as Aristotle. Paremiography, on the other hand, is the collection of proverbs. The proverb scholar Wolfgang Mieder defines the term \"proverb\" as follows:\n\nAs well as actual proverbs, the following may be considered proverbial phrases:\n\nTypical stylistic features of proverbs (as Shirley Arora points out in her article, \"The Perception of Proverbiality\" (1984)) are:\n\n\nIn some languages, assonance, the repetition of a vowel, is also exploited in forming artistic proverbs, such as the following extreme example from Oromo, of Ethiopia.\nSimilarly, from Tajik:\nNotice that in all of these cases of complete assonance, the vowel is , the most common vowel in human languages.\n\nAlso in Amharic, complete assonance is not infrequent when verbs are in the 3rd person masculine singular, past tense. The vowel <ä> is the most frequent vowel in the language.\n\nInternal features that can be found quite frequently include:\n\nTo make the respective statement more general most proverbs are based on a metaphor. Further typical features of the proverb are its shortness and the fact that its author is generally unknown.\nIn the article \"Tensions in Proverbs: More Light on International Understanding\", Joseph Raymond comments on what common Russian proverbs from the 18th and 19th centuries portray: Potent antiauthoritarian proverbs reflected tensions between the Russian people and the Czar. The rollickingly malicious undertone of these folk verbalizations constitutes what might be labeled a \"paremiological revolt\". To avoid openly criticizing a given authority or cultural pattern, folk take recourse to proverbial expressions which voice personal tensions in a tone of generalized consent. Proverbs that speak to the political disgruntlement include: \"When the Czar spits into the soup dish, it fairly bursts with pride\"; \"If the Czar be a rhymester, woe be to the poets\"; and \"The hen of the Czarina herself does not lay swan's eggs\". While none of these proverbs state directly, \"I hate the Czar and detest my situation\" (which would have been incredibly dangerous), they do get their points across.\n\nProverbs are found in many parts of the world, but some areas seem to have richer stores of proverbs than others (such as West Africa), while others have hardly any (North and South America) (Mieder 2004b:108,109).\n\nProverbs are used by speakers for a variety of purposes. Sometimes they are used as a way of saying something gently, in a veiled way (Obeng 1996). Other times, they are used to carry more weight in a discussion, a weak person is able to enlist the tradition of the ancestors to support his position, or even to argue a legal case. Proverbs can also be used to simply make a conversation/discussion more lively. In many parts of the world, the use of proverbs is a mark of being a good orator.\n\nThe study of proverbs has application in a number of fields. Clearly, those who study folklore and literature are interested in them, but scholars from a variety of fields have found ways to profitably incorporate the study proverbs. For example, they have been used to study abstract reasoning of children, acculturation of immigrants, intelligence, the differing mental processes in mental illness, cultural themes, etc. Proverbs have also been incorporated into the strategies of social workers, teachers, preachers, and even politicians. (For the deliberate use of proverbs as a propaganda tool by Nazis, see Mieder 1982.)\n\nThere are collections of sayings that offer instructions on how to play certain games, such as dominoes (Borajo \"et al.\" 1990) and the Oriental board game go (Mitchell 2001). However, these are not prototypical proverbs in that their application is limited to one domain.\n\nOne of the most important developments in the study of proverbs (as in folklore scholarship more generally) was the shift to more ethnographic approaches in the 1960s. This approach attempted to explain proverb use in relation to the context of a speech event, rather than only in terms of the content and meaning of the proverb.\n\nAnother important development in scholarship on proverbs has been applying methods from cognitive science to understand the uses and effects of proverbs and proverbial metaphors in social relations.\n\n"}
{"id": "2847161", "url": "https://en.wikipedia.org/wiki?curid=2847161", "title": "Professional writing", "text": "Professional writing\n\nProfessional writing is writing for reward or as a profession, or it is any form of written communication produced in a workplace environment or context. Works produced with the professional writing style allow professionals (e.g. employers, lawyers, businesspeople, etc.) to make informed decisions. Professional writing involves the use of precise language to convey information in a way that is easily understood by its intended audience, and it may be directed to inform, persuade, instruct, stimulate debate, or encourage action. For example, in a business office, a memorandum (abbrev. memo) can be used to provide a solution to a problem, make a suggestion, or convey information. \n\nA professional writer may be freelance, meaning he or she works on a self-employed basis, or fully employed in an occupation where a professional writing standard is a prerequisite, such as journalism, marketing, advertising, public relations, the military, or technical writing. While not necessarily the practitioner's primary profession, professional writing skills are essential in many other fields such as law, medicine, business, engineering, and social work.\n\nProfessional writing is any type of writing that is written with the intention of communicating with others in a professional and courteous manner to facilitate work. Professional writing is either internal or external to a business or organization, which means that the audience of a written work is either an insider or outsider of the professional writer's organization; examples of internal business writing include email messages, memos, and reports while some examples of external business writing are letters and email messages. \n\nProfessional writing differs from other types of writing, such as academic and technical writing, because the term defines a general overview of writing that is done for profit in a workplace environment. Professional writing differs from academic writing due to the difference in purpose and readership between the two styles. Academic writing informs the audience through critical approach and directs further thinking by emphasizing clarity and thought while professional writing is applied to a business or setting (a hospital, a company, or a factory) and is meant to facilitate work through communication. The audience of academic writing is also limited in contrast to professional writing; specialized experts in specific fields make of the primary readership of academic writing while the amount and identities of readers of professional writing can be varied. When writing, professional writers must take into consideration the possibility of unexpected tertiary readers who can come across their document.\n\nProfessional writing differs from technical writing because of the type of content in technical writing. Technical writing could be identified as a concentration of the broad generalization of professional writing—technical writing is principally directed towards fields of interest. Both are similar in that they take place in professional workplace context and are primarily targeted to allow communication between experts; however, technical writing focuses on technical, specialized topics, such as science, technology, and engineering.\n\nThe audience of professional and business documents plays a significant role in the style of a professional document. Successful professional writers adapt their document to fulfill the needs of their audience. Four factors are taken into consideration when a professional writer creates a professional document:\nThe audience's pre-existing knowledge remains an important focus for a professional document because it would affect the audience's ability to read the document. For example, a general audience with little knowledge of a document's subject would be unable to read it if it contained specific, technical jargon. A professional writer would then have to minimize the amount technical jargon or define terms for the reader. \n\nExpectations of style and format are influential in the format and development of a professional document. Precedents created by earlier documents of the same genre of a professional writer's work heavily influence how the reader of his or her document will judge the credibility of both the writer and document. Documents belonging to a specific genre are expected to be written in a way that adheres to a format and style that defines that genre. If a professional writer were to produce a document that does not adhere to the precedented style, he or she and the document would lose credibility.\n\nRegarding business and professional writing, the relationship between writer and reader is key. The familiarity between the two influences the language used. For example, an employee might write more informally via email to a coworker of the same hierarchal level than he or she would write via email to his or her employer.\n\nThere is, particularly in business, a need for concise and unambiguous communication with colleagues, suppliers, clients, and the general public. Professional writing forestalls inattentiveness and criticism. \n\nPersuasive professional writing is connected to the concept of rhetoric, which focuses on informing or persuading and relies upon stimulating the interest of the audience through creating authoritative arguments. A professional writer uses rhetoric and persuasion when creating a document that is intended to suggest a solution to a problem or encourage action. A professional writer uses persuasive language when trying to influence the reader to do something as a result of reading a document. Professional writing Feasibility reports and economic justification reports are examples of documents that have such purposes. \n\nClear and concise professional writing is vital in many fields where misunderstanding could have serious consequences, such as in law, engineering, technical manuals, and product labels. Misunderstandings might also occur with international audiences; for this reason, a professional writer would have to take careful consideration of cultural differences. The use of language, style, and even color in a document could have detrimental effects because these elements, along with others, can change meaning when translated. For example, a picture of a woman wearing a swimsuit on the beach in an ad would be met with nonchalance in the United States; however, if the same picture was shown in an area with a prevalent Muslim populace, it would be met with outrage because of the religious beliefs that bar women from showing skin. This analysis and consideration of direct and possible audiences help create clear and concise writing and language that professional language requires.\n\nThe core skills required in professional writing are good communication, organized thought, a high standard of grammar and language, clarity, and conciseness. Skills may be acquired through practice or formal learning. While many practitioners of professional writing do so as a vocation rather than as full-time employment, the element of \"professionalism\" is what defines professional writing. Such is the importance of professional writing in the modern world, many academic institutions offer courses up to degree level on the subject, with some tailored to specific professions such as social work.\n\n"}
{"id": "9032902", "url": "https://en.wikipedia.org/wiki?curid=9032902", "title": "Readability survey", "text": "Readability survey\n\nA readability survey is a statistical survey of the ability of people to read given passages of text, written, formatted and/or laid-out in a variety of styles. The intent is to discover which are the preferable styles to use in order to maximise the ability of the reading audience to receive the intended message.\n\nTests may be performed by surveying real people reading the works, or an abbreviated test may be followed, where a number of works are surveyed using pre-determined scoring methodologies, which were themselves developed by systematically surveying real people's response to given texts.\n\nThese tests are commissioned or performed by entities like publishers, educators, design houses and governments, and have resulted in divination of a number of \"rules of thumb\" based on statistical analysis of results which demonstrate a level of consistency in certain parameters , such as the quantity and location of whitespace which obtains maximum readability (e.g. 20% whitespace in text body, margins should be around 40% of the width, for English textbooks, preferably including a column down the left side of the page, or split over both sides, or down the right side as a last resort if sufficient room on the left is not available).\n\nA readability survey implementation may use one or more readability tests in scoring the works.\n\n\nHere is one which is a survey of readability of surveys!\n\n\nMany more may be found by searching the literature, for example\n\n\n\n"}
{"id": "10511206", "url": "https://en.wikipedia.org/wiki?curid=10511206", "title": "Register complex", "text": "Register complex\n\nIn linguistics, a register complex is a combination of phonation type, pitch, length, vowel quality and/or other variants that function dependently as distinguishing features within a single phonological system. In languages employing register systems, differences in a distinguishing feature correlate relative to the quality of another distinguishing feature.\n\nFor instance, in a system where pitch, voice quality and stress timing were distinguishing features, the meaning of a vowel-consonant cluster like \"Pai\" (as in the English word \"Pie\") may depend on whether the cluster is voiced in a high, medium or low pitch relative to the clearness, breathiness and glotteral quality of speech, as well as relative to the duration of the cluster relative to other neighboring clusters. Thus, \"Pai\" voiced in a high tone with a breathy quality and unstressed along with other unstressed clusters would connote a different meaning if any of the three features changed relative to other features.\n\nMany register languages are tonal. However, tone is not a universal distinguishing feature among register systems and register complexes are not a component of most tonal languages. In fact, tonal languages can employ either a register tone systems or a contour tone systems. Mandarin has a contour tone system, where the distinguishing feature of the tones are their shifts in pitch (their pitch shapes or contours, such as rising, falling, dipping, or peaking) rather than simply their pitch relative to each other as in a register tone system. Register tone systems are found in Bantu languages and throughout Africa. In some register tone systems, there is a default tone, usually low in a two-tone system or mid in a three-tone system, that is more common and less salient than other tones. There are also languages that combine register and contour tones, such as the Kru languages, though in such cases the register tones may be analysed as being 'level' (unvarying pitch) contour tones. Furthermore, often the term 'register,' when not in the phrase 'register tone,' is used to indicate vowel phonation combined with tone in a single phonological system. Burmese, Khmer and possibly Vietnamese are register languages. Burmese is usually considered a tonal language and Khmer a vowel-phonation language, but in both cases differences in relative pitch or pitch contours are correlated with vowel phonation, so that neither exists independently.\n"}
{"id": "40979852", "url": "https://en.wikipedia.org/wiki?curid=40979852", "title": "Relativizer", "text": "Relativizer\n\nIn linguistics, a relativizer (abbreviated ) is a type of conjunction that introduces a relative clause. For example, in English, the conjunction \"that\" may be considered a relativizer in a sentence such as \"I have one that you can use.\" Relativizers do not appear, at least overtly, in all languages; even in languages that do have overt or pronounced relativizers, they do not necessarily appear all of the time. For these reasons it has been suggested that in some cases, a \"zero relativizer\" may be present, meaning that a relativizer is implied in the grammar but is not actually realized in speech or writing. For example, the word \"that\" can be omitted in the above English example, producing \"I have one you can use\", using (on this analysis) a zero relativizer.\n\nSince as far back as 1712, people have written about relativizers and what functions they have. They have been classified as conjunctions in earlier times, and have later been referred to as clause markers. They are known today as relativizers. Despite an agreement in nomenclature, there are multiple analyses which attempt to account for the grammatical function and distribution of relativizers.\n\nThe promotional analysis is a transformational analysis from 1973 depicting relative clauses in English, and how relative pronouns are introduced into the embedded clause. This analysis assumes that there is no overt head noun in the deep structure of the main clause. In order to form a relative construction, the noun phrase from the embedded clause is promoted to the empty head of the noun phrase of the main clause. From there, a corresponding relative pronoun leaves a trace in the space of the vacated noun phrase in the embedded clause. For example:\nThe Matching Analysis is another type of transformational analysis from the 1970s, which was in competition with the Promotional Analysis at that time. In this analysis the relative pronoun is introduced into the embedded clause by corresponding or matching to the head noun in the main clause. This is done by taking the noun phrase from the embedded sentence in the deep structure that matches the head noun in the noun phrase of the main clause, and replacing it with a relative pronoun. The relative pronoun thus co-references the head noun in the main clause. Finally, the relative pronoun is moved to the clause-initial position. For example:\n\nThere are two separate phrasal heads that relativizers can occupy. Cross-linguistically, relativizers may occupy either the head of a complementizer phrase (C-Type Relativizer) or the head of a determiner phrase (D-Type Relativizer). C-Type Relativizers can introduce a relative clause as an argument of a noun phrase, or they can introduce a relative clause as an argument of a verb phrase. D-Type Relativizers may only introduce a relative clause as an argument of a noun phrase. English is a language which uses a C-Type Relativizer, that, as a part of its relativization strategy because \"that\" can introduce a relative clause as either the argument to a Noun Phrase or the argument to a Complementizer Phrase. The following examples from English shows the same morpheme being used in both syntactic contexts.\nConversely, Arabic uses two phonologically distinct morphemes to account for these syntactic phenomena. In the same sentences in a D-Type language like Arabic, each example would employ the use of a different morpheme as shown in examples 1 & 2.\n\nThere are three types of relativizers used in English to introduce relative clauses: zero or null relativizers, \"wh\"-relativizers, and the \"that\"-relativizer.\n\nRelativizers have been analyzed to be optional in certain languages and are variably omitted in the English language. Such relativizer omission, or use of the null or zero variant of relativizers, does not pattern uniformly in English and has been predicted to be conditioned and constrained by a number of linguistic and social factors. These social factors and the potential influence of age, gender, and education have been minimally explored and seem to exhibit a lesser effect on relativizer omission. Linguistic constraints, such as sentence structure and syntactic position of the relativizer, main clause construction type, lexical specificity of the head NP, type of antecedent, and the adjacency, length, and grammatical subject of the relative clause have been implicated as having more significant influence on the patterning of relativizer omission in Canadian English. The omission of relativizers tends to occur more frequently in conversation than in formal writing.\n\nThe syntactic position or function of the relativizer in the relative clause is a major determiner for the choice of relative marker. The null relativizer variant is more common in object than subject relative clauses.\n\nThere is a preference for null relativizers when a main clause that is informationally light is directly \"adjacent\" to the relative clause. For example:\nIn this example, the main clause 'it's just kinda something' provides little semantic information and it is adjacent to the relative clause 'I noticed recently'. As such, it is thought that the main clause and the relative clause are processed together as a unitary processing chunk that is functioning like a single statement, which results in a null relativizer.\n\nEmpty head noun phrases, which are not lexically specific and which index generic groups or sets, have been correlated with the use of the null relativizer. Examples of empty noun phrases include words like \"all\", \"way\", \"time\", etc.\n\nUnique head noun phrases, which include superlatives and nouns with the words \"only\" and \"first\", also take the null relativizer. For example:\n\nLonger head noun phrases often co-occur with an overt relativizer, whereas shorter noun phrases are more likely to co-occur with a null relativizer. For example:\nIn these examples, the first sentence contains a longer noun phrase ('This pair of suede pants') in comparison to the second sentence, which contains a very short noun phrase ('The weight'). Thus, it is observed that the sentence containing the longer noun phrase also contains the relativizer 'that', whereas the sentence with the shorter noun phrase has a null relativizer.\n\nNull relativizers have been found to be correlated to the definiteness of the nominal antecedent. For example:\nThe first sentence contains a definite noun phrase, whereas the second sentence contains an indefinite noun phrase which co-occurs with the null relativizer.\n\nWhen the grammatical subject of a relative clause is a pronoun, it is more likely that the relativizer will be omitted. When the subject of a relative clause is a full noun phrase, the overt relativizer will be retained. For example:\n\nThe overt relativizers of Modern English include the words “which”, “what”, “when”, “where”, “who”, “whom” and “whose”, and these can be referred to within linguistics as \"\"wh\"-words\". These are officially classified as relative pronouns, but can be referred to as “\"wh\"-relativizers” in instances where their function is to introduce a relative clause.\nThe other overt relativizer of Modern English is the word “that”, which can be referred to as the “\"that\"-relativizer” where it introduces a relative clause. There is some debate as to whether to classify it as a relative pronoun - like the wh-words, a subordinating conjunction, or a complementizer. The distribution of the different types of English relativizers varies depending on several factors.\n\nFused relative clauses, sometimes referred to as “free” relative clauses, are different from most other types of relative clauses in that there is no nominal antecedent to which the relative clause refers. In many cases, the relativizers of English are relative pronouns, meaning that they are in coreference with a noun that precedes them in the sentence. This nominal function is “fused” with the relative clause in free relatives, and this leaves the relativizer without an overt entity to which it can refer. For example:\nThere is no noun preceding the relative clause in these cases, and that is why it is said that this noun’s function is “fused” with the relative clause.\n\nWhere there are different grammatical case forms of a relativizer, the case form that surfaces will depend on the grammatical function of the noun that appears previously in the sentence to which the relative clause refers, known as the nominal antecedent. The only examples in Modern English of this phenomenon are the forms “who” and “whom”. “Who” surfaces when it refers to a noun that is the subject of the main clause, and “whom” surfaces when it refers to a noun that is an object of the main clause. However, speaker judgments vary as to whether it is grammatical for \"who\" to surface when it is referring to an object of the main clause. Since, depending on speaker judgments, either only \"whom\" or both \"who and \"whom\" can grammatically introduce a relative clause referring to an object, there is an \"m\" in brackets on the end of the relativizer in example (21) below.\n 24) \"The person who visited Kim\"\n 25) \"The chairman listened to the student who(m) the professor gave a low grade to\"\n\nOnly certain relativizers can introduce clauses that refer to human antecedents, and similarly, only certain relativizers can introduce clauses that refer to non-human antecedents. “Who”, “whom”, and “whose” can only refer to human antecedents, “which”, and “what” can only refer to non-human antecedents. “That”, however, can refer to both human and non-human antecedents. To exemplify: \n 26) \"The Pat that I like is a genius\"\n 29) \"Every essay that she's written which I've read is on that pile\"\n\nRestrictive relative clauses add extraneous information that is not vital for the listener or reader’s understanding of which aforementioned noun is being referenced; or in other words, which noun is the nominal antecedent. Both \"wh\"-relativizers and the \"that\"-relativizer can be used to introduce restrictive relative clauses. Nonrestrictive relative clauses have semantic properties which make them necessary to prevent the sentence from being ambiguous. They are used in cases where the context that surrounds the sentence is not sufficient for the distinction between the potential nominal antecedents. Commas mark nonrestrictive relative clauses, and only the \"wh\"-relativizers can be used to introduce them. To exemplify:\n 31) \"He has four sons that became lawyers \"\n 33) \"He has four sons, who became lawyers\"\n\nIn non-finite clauses (clauses in which the verb is left unconjugated), the relativizer appears as an object of preposition, or in other words, directly after a preposition in the sentence. These relative clauses appear to be introduced by the preposition itself, but they are actually introduced by both the preposition and the relativizer, since these two grammatical particles form a “prepositional phrase”; and it is this phrase that introduces the clause. For example:\nNote that (37) is ungrammatical because the relativizer introduces a non-finite relative clause, but it is not contained within a propositional phrase.\n\nTeochew is a Chinese language originating from the Chaoshan region of the eastern Guangdong province. Indonesian Teochew refers to the Teochew dialect spoken in Indonesia. The most common way to form relative clauses in Indonesian Teochew is to use the relativizer \"kai\". These relative clauses can appear head-finally or head-initially.\n\nJambi Teochew is a variety of Indonesian Teochew that is spoken in the province of Jambi on the island of Sumatra. In this language, the relativizer \"kai\" is used to form relative clauses. In these relative clauses, the \"kai\" is obligatory. This relativizer comes from the Chinese language. The relativizer \"yang\" is optional, and is borrowed from Malaysian. The relativizer \"kai\" must always follow the modifying clause. If the optional relativizer \"yang\" is used, it would precede the modifying clause, as shown by example #43. If the relativizer \"kai\" is not present, the sentence becomes ungrammatical, regardless of whether \"yang\" is present or not. This is demonstrated in example #45.\n\nAnother way of forming relative clauses in Jambi Teochew is through using the classifier. The main difference between the \"kai\" and classifier relative clause is that there is the presence of a classifier in the classifier relative clause. The classifier in classifier relative clauses can only appear head-initially. The classifier agrees with the head noun type and is in the place of the relativizer \"kai\".\n\nHeadless relative clauses do not have a pronounced head. It is the equivalent of “the one” in English. Headless relative clauses are formed with the relativizer \"kai\". The Malaysian relativizer \"yang\" can be used optionally before the modifying clause.\n\nThe relativizer \"kai\" is obligatory. In addition, it is not possible to form a headless relative clause with a classifier in the place of the relativizer \"kai\".\n\nPontianak Teochew is a variety of Indonesian Teochew that is spoken in the capital city of Pontianak in the province of West Kalimantan. The relativizer \"kai\" is used to form relative clauses. It is obligatory in head-final relative clauses. If \"kai\" is not present in the sentence, the sentence becomes ungrammatical, as demonstrated by example #52. Pontianak Teochew does not allow the use of the Malaysian relativizer \"yang\". When this relativizer is present, the sentence becomes ungrammatical. This is shown in example #53.\n\nUte is a language that belongs to the Northern Division of the Uto-Aztecan language family that spans the distance from the Rocky Mountains to Popocatepetl volcano south of Mexico City.\n\nIn Ute, relative clauses that modify the subject are introduced in a different manner than those that modify the object. In both cases, there is no overt relativizer morpheme, rather nominalization and case morphology are used to introduce relative clauses.\n\nFor example, nominalizing suffixes are attached to verbal elements in subject relative clauses.\n\nIn relative clauses that are introduced as arguments to an object, the verbal elements are inflected with nominalizing morphology similar to that of their subject relative clause counterparts and the subject of the embedded clause is inflected with the genitive case.\n\n"}
{"id": "26975630", "url": "https://en.wikipedia.org/wiki?curid=26975630", "title": "Revision (writing)", "text": "Revision (writing)\n\nRevision is the stage in the writing process where the author reviews, alters, and amends their message, according to what has been written in the draft. Revision follows drafting and precedes editing. Drafting and revising often form a loop as a work moves back and forth between the two stages. It is not uncommon for professional writers to go through many drafts and revisions before successfully creating a written piece that is ready for the next stage: editing. Revision itself is not indicative of the quality of the text.\n\nIn their seminal book, \"The Elements of Style\", William Strunk, Jr. and E.B. White acknowledge the need for revision in the writing process: “Few writers are so expert that they can produce what they are after on the first try. Quite often you will discover, on examining the completed work, that there are serious flaws in the arrangement of the material, calling for transpositions... do not be afraid to experiment with your text.”\n\nFor an essay, a successful revision involves:\n\nIdentification of thesis. The purpose of the essay should be re-considered based on what has been written in the draft. If this purpose differs from the original thesis, the author must decide from which thesis to continue writing.\n\nConsideration of structure. The author should identify the strengths of the draft, then re-consider the order of those strengths, adjusting their placement as necessary so the work can build with auxesis to a crescendo.\n\nUncovering weakness in argument or presentation. Once the strengths of the draft have been identified and placed in the strongest order, the author can re-examine the work for weaknesses in argument or presentation. Faulty logic, missing transitions, and unsupported or poorly supported assertions are common weaknesses. Identifying these weaknesses during revision will inform the next draft.\n\nIn general, revision of written work can be guided by the following questions:\n\n\nSuccessful revision is not improving grammar or diction. Those will be the focus of later editing. However, there are instances when the writer or the author employs a strategy that involves a reviewing process. There are scholarship and theories citing this particular theme such as the case of the three-component model identified by Hayes and Flower and the James Britton's model of the writing process as a series of stages described in metaphors of linear growth, conception - incubation - production. Here, a review by the writer or a third party, which often entails corrective annotations, is part of the process that leads to the revision stage. Recently, due to the collaborative capabilities offered by the Internet, there are writers who \"crowdsource\" reviews from several people, who contribute digital annotations.\n"}
{"id": "29464167", "url": "https://en.wikipedia.org/wiki?curid=29464167", "title": "Section (typography)", "text": "Section (typography)\n\nIn books and documents, a section is a subdivision, especially of a chapter. \n\nSections are visually separated from each other with a section break, typically consisting of extra space between the sections, and sometimes also by a section heading for the latter section. They are a concern in the process of typography and pagination, where it may be desirable to have a page break follow a section break for the sake of aesthetics or readability.\n\nIn fiction, sections often represent scenes, and accordingly the space separating them is sometimes also called a scene break.\n\nIn written narrative such as fiction, sections are not usually numbered or named. Section breaks are used to signal various changes in a story, including changes in time, location, point-of-view character, mood, tone, emotion, and pace. As a fiction-writing mode, the section break can be considered a transition, similar to a chapter break.\n\nSome documents, especially legal documents, may have numbered sections, such as \"Section Two of the Canadian Charter of Rights and Freedoms\" or \"Internal Revenue Code section 183\". The section sign (§) may be used to reference sections and subsections. Subsections are often written in lowercase Roman numerals, e.g. Section 51(xxvi) of the Australian Constitution.\n\nA document may also be considered to be divided into sections by its headings and subheadings, which may be used for a table of contents. For example, the hierarchical sections used in Wikipedia can be compiled into a table of contents for an article. Many books, however, only have chapter headings in the table of contents.\n\nWhile a chapter may be divided by section breaks, a group of chapters is conventionally called a \"part\", often identified with a Roman numeral, e.g. \"Part II\".\n\nReference material may be divided into sections. The section headers of a Chinese dictionary are one example.\n\nSpace between paragraphs in a section break is sometimes accompanied by an asterism (either proper ⁂ or manual * * *), a horizontal rule, fleurons, or other ornamental symbols. An ornamental symbol used as section break does not have a generally accepted name. Such a typographic device can be specifically referred to as \"dinkus\", \"space break symbol\", \"paragraph separator\", \"paragraph divider\", \"horizontal divider\", \"thought break\", or as an instance of \"filigree\" or \"flourish\". Ornamental section breaks can be created using glyphs, rows of lozenges, dingbats, or other miscellaneous symbols. Fonts such as Webdings and Wingdings include many such glyphs.\n\nIn HTML, horizontal rules can be generated using the codice_1 tag, which generates a paragraph-level thematic break. For more ornate presentation, CSS can be used to replace the line with an image.\n\n"}
{"id": "143856", "url": "https://en.wikipedia.org/wiki?curid=143856", "title": "Space (punctuation)", "text": "Space (punctuation)\n\nIn writing, a space ( ) is a blank area that separates words, sentences, syllables (in syllabification) and other written or printed glyphs (characters). Conventions for spacing vary among languages, and in some languages the spacing rules are complex.\n\nTypesetting uses spaces of varying length for specific purposes. The typewriter, on the other hand, can accommodate only a limited number of keys. Most typewriters have only one width of space, obtained by pressing the space bar. Following widespread acceptance of the typewriter, some spacing and other typewriter conventions, which were based on the typewriter's mechanical limitations, have influenced professional typography other designers of printed works.\n\nComputer representation of text eliminates all mechanical and physical limitations in any sufficiently advanced character encoding environment (such as Unicode), where spaces of various widths, styles, or language characteristics (different \"space characters\") are indicated with unique code points. Whitespace characters include spaces of various width, including all those that professional typesetters employ.\n\nModern English uses a space to separate words, but not all languages follow this practice. Spaces were not used to separate words in Latin until roughly 600–800 CE. Ancient Hebrew and Arabic, while they didn't use spacing, used word-dividers partly to compensate in clarity for the lack of vowels. The earliest Greek script also used interpuncts to divide words, rather than spacing, although this practice was soon displaced by the \"scriptura continua\". The earliest signs of spacing between words appears in the Latin alphabet, where it was used extremely rarely in some manuscripts and then altogether forgotten. However, spacing was then reinvented into language through Irish and Anglo-Saxon scribes, and then with the creation of the Carolingian minuscule by Alcuin of York, where it originated and then spread to the rest of world, including modern Arabic and Hebrew. Indeed, the actions of these Irish and Anglo-Saxon scribes marked the dramatic shift for reading between antiquity and the modern period. Spacing would become standard in Renaissance Italy and France, and then Byzantium by the end of the 16th century; then entering into the Slavic languages in Cyrillic in the 17th century, and only in modern times entering modern Sanskrit. Traditionally, all CJK languages have no spaces: modern Chinese and Japanese (except when written with little or no kanji) do not; on the other hand, modern Korean uses spaces.\n\nRunic texts use either an interpunct-like or a colon-like punctuation mark to separate words. There are two Unicode characters dedicated for this: and .\n\nLanguages with a Latin-derived alphabet have used various methods of sentence spacing since the advent of movable type in the 15th century.\n\n\nThere has been some controversy regarding the proper amount of sentence spacing in typeset material. The \"Elements of Typographic Style\" states that only a single word space is required for sentence spacing.\" Psychological studies suggest \"readers benefit from having two spaces after periods.\"\n\nThe International System of Units (SI) prescribes inserting a space between a number and a unit of measurement (being regarded as a multiplication sign) and between units in compound units, but never between a prefix and a base unit.\n\nThe only exceptions to this rule is the traditional symbolic notation of angles: degree (e.g., 30°), minute of arc (e.g., 22′), and second of arc (e.g., 8″).\n\nThe SI also prescribes the use of thin space whenever thousands separators are used. Both a point or a comma are reserved as decimal markers.\n\n"}
{"id": "51460271", "url": "https://en.wikipedia.org/wiki?curid=51460271", "title": "Speaker types", "text": "Speaker types\n\nWithin the linguistic study of endangered languages, sociolinguists distinguish between different speaker types based on the type of competence they have acquired of the endangered language. Often in situations where a community is gradually shifting away from an endangered language to a majority language, not all speakers acquire full linguistic competence, but have varying degrees and types of competence depending on the kind of exposure to the minority language they experienced in their upbringing. Originally the relevance of speaker types in situations of language shift was noted by Nancy Dorian who coined the term semi-speaker to refer to those speakers of Sutherland Gaelic who were predominantly English-speaking and whose Gaelic competence was limited and showed considerable influence from English. Later studies added additional speaker types such as rememberers (who remember some words and phrases but have little or no grammatical competence and do not actively speak the language), passive speakers (who have nearly full comprehension competence but do not actively speak the language). In the context of language revitalization, new speakers who have learned the endangered language as a second language are sometimes distinguished.\n\nIn contexts of language acquisition and language teaching studies sometimes there is a distinction between native speakers and second language speakers, depending on whether language was learned as a language of primary socialisation or learned after already having fully acquired a first language. In contexts of multilingualism a bilingual speaker may also be described as a heritage speaker (A heritage language actually refers to a language whose speakers have moved from the original area where the language was spoken: e.g. Welsh is a heritage language in Patagonia, but not in Wales) if they have not been as fully exposed to one of their languages, leading to a diminished degree of confidence in themselves as speakers, and sometimes also limited competence in one of their languages.\n\nA rememberer is a person who knows individual words or phrases (sometimes entire texts) but cannot use the target language productively. Such persons are of particular interest when studying any endangered or dying language. Rememberers are contrasted with fluent or full speakers, who have a good command of the language, and semi-speakers, who have a partial command of it. The distinction between fluent speakers and rememberers is important in fieldwork, but accurately determining where a member of a language community falls on the speaker-rememberer continuum can be challenging.\n\nA passive speaker (also referred to as a \"receptive bilingual\" or \"passive bilingual\") is someone who has had enough exposure to a language in childhood to have a native-like comprehension of it, but has little or no active command of it. Such speakers are especially common in language shift communities where speakers of a declining language do not acquire active competence. Around 10% of the Ainu people, for example, who speak the language are considered passive speakers. Passive speakers are often targeted in language revival efforts to increase the number of speakers of a language quickly, as they are likely to gain active and near-native speaking skills more quickly than those with no knowledge of the language. They are also found in areas where people grow up hearing another language outside their family with no formal education.\n\nA semi-speaker is a speaker who has acquired a partial linguistic competence in a given language, but who does not generally use it regularly in conversation. Their speech can contain erroneous forms. Semi-speakers are often among the most motivated and engaged participants in language revitalization projects. As languages become obsolete and speech communities shift to other languages, they are spoken less frequently and in fewer social domains. Many speakers learn the language partially, often in a simplified way, with significant influence from the majority language. They are sometimes referred to as \"semi-speakers\", \"quasi-speakers\" or \"rememberers\". The word semi-speaker was introduced by linguist Nancy Dorian in describing the last speakers of the East Sutherland dialect of Scottish Gaelic. When semi-speakers form a significant part of the speech, community language contraction often ensues, as the linguistic norms are accommodated to the competences of the speakers.\n\nA terminal speaker is the last speaker of a language. A terminal speaker may still be alive, or may have been the last person speaking what is now an extinct language. In the process of language death, the remaining speakers begin to lose some of the vocabulary and grammar of the language, so when there is only a last terminal speaker, the person will not remember a complete form of the language as it had been spoken by a larger community using it in all language domains. Being a terminal speaker means that the person is bilingual, remembering their heritage language but interacting with their community in another language. The importance of this distinction is seen in the story of Dolly Pentreath of Cornwall. She is popularly named as the last fluent, first-language speaker of Cornish, but there were others who still spoke it for many years, though possibly incompletely. Terminal speakers are sometimes found by linguists doing language documentation on a language before it dies. A clear example of a terminal speaker being contacted by a linguist is the case of Abegaz, the last speaker of the Mesmes language in Ethiopia. He lived in an isolated hilly area and was about 80 years old when he was contacted by a team of sociolinguistic language surveyors; he has died since that contact. Ned Maddrell was the last speaker of the Manx language, having died in 1974. In 2008, Doris McLemore was reported to be the last speaker of the Wichita language as she worked with a team of linguists to document the language before it died completely. Many more terminal speakers are listed under the Wikipedia category Last known speakers of a language.\n"}
{"id": "861713", "url": "https://en.wikipedia.org/wiki?curid=861713", "title": "Sun Language Theory", "text": "Sun Language Theory\n\nThe Sun Language Theory () was a Turkish nationalist pseudoscientific linguistic hypothesis developed in Turkey in the 1930s that proposed that all human languages are descendants of one proto-Turkic primal language. The theory proposed that because this primal language had close phonemic resemblances to Turkish, all other languages can essentially be traced back to Turkic roots. According to the theory, the Central Asian worshippers, who wanted to salute the omnipotence of the sun and its life-giving qualities, had done so by transforming their meaningless blabbering into a coherent set of ritual utterings, and language was born, hence the name.\n\nInfluences on the theory included:\n\n\n\nThe founder and first president of the Republic of Turkey, Mustafa Kemal Atatürk, not only gave the theory official backing and material support but also was himself a very important contributor to its development, \"though clearly he did not do all the donkey work\".\n\nAs described in a 1936 \"New York Times\" article on the curriculum of the newly opened School of Language, History and Geography of Ankara University, the theoryclaims that the Sumerians, being Turks, originating in Central Asia, all languages also consequently originated there and first used by the Turks. The first language, in fact, came into being in this way: Prehistoric man, i.e., Turks in the most primitive stage, was so struck by the effects of the sun on life that he made of it a deity whence sprang all good and evil. Thence came to him light, darkness, warmth and fire, with it were associated all ideas of time: height, distance, movement, size, and give expression to his feelings. The sun was thus the first thing to which a name was given. It was \"ag\" (pronounced agh), and from this syllable all words in use today are derived. This, briefly, is the theory about the \"sun language,\" and with the new conception of Turkish history it will be taught in the new Angora school.In short, based upon a heliocentric view of the origin of civilization and human languages, the theory claimed that the Turkish language was the language which all civilized languages derived from.\n\nSome of the words provided with false Turkish etymologies through the practice of \"goropism\" were \"God\", attributed to the Turkish \"kut\" (blessing); \"Bulletin\" from \"belleten\" (to learn by heart); \"Electric\" from Uyghur \"yaltrık\" (shine). According to linguist Ghil'ad Zuckermann, \"it is possible that the Sun Language Theory was adopted by Atatürk in order to legitimize the Arabic and Persian words which the Turkish language authorities did not manage to uproot. This move compensated for the failure to provide a neologism for every foreignism/loanword.\"\n\n\n\n"}
{"id": "3221635", "url": "https://en.wikipedia.org/wiki?curid=3221635", "title": "True name", "text": "True name\n\nA true name is a name of a thing or being that expresses, or is somehow identical to, its true nature. The notion that language, or some specific sacred language, refers to things by their true names has been central to philosophical study as well as various traditions of magic, religious invocation and mysticism (mantras) since antiquity.\n\nThe true name of the Egyptian sun god Ra was revealed to Isis through an elaborate trick. This gave Isis complete power over Ra and allowed her to put her son Horus on the throne.\n\nSocrates in Plato's \"Cratylus\" considers, without taking a position, the possibility whether names are \"conventional\" or \"natural\" (\"true name\" [τῇ ἀληθείᾳ ὄνομα]), that is, whether language is a system of arbitrary signs or whether words have an intrinsic relation to the things they signify (this anti-conventionalist position is called Cratylism).\n\nHellenistic Judaism emphasized the divine nature of \"logos\", later adopted by the Gospel of John. The true name of God plays a central role in Kabbalism (see Gematria, Temurah, YHWH [the tetragrammaton]) and to some extent in Sufism (see 100th name of God).\nThe ancient Jews considered God's true name so potent that its invocation conferred upon the speaker tremendous power over His creations. To prevent abuse of this power, as well as to avoid blasphemy, the name of God was always taboo, and increasingly disused so that by the time of Jesus their High Priest was supposedly the only individual who spoke it aloud — and then only in the Holy of Holies upon the Day of Atonement.\n\nAlso in a Biblical context, in the tale of Jacob's nocturnal wrestling with an anonymous angel, the angel refuses to reveal his own name to Jacob even after the angel's submission at dawn. Thereafter Jacob obtains a new name which signifies his successful struggle to God and man, and names the place to commemorate his surviving an encounter with the Divine.\n\nContemporary pre-industrial peoples guard secret names which are only used in solemn rituals. These names are never mentioned and kept from general knowledge.\n\nIn Jewish tradition, when several children have died in a family the next that is born has no name given to it, but is referred to as \"Alter\" (, literally \"old\"), or \"Alterke\", the view being that the Angel of Death, not knowing the name of the child, will not be able to seize it. When such a child attains the marriageable age, a new name, generally that of one of the Patriarchs, is given to it. \n\nWhen captured by Polyphemus, Homer's Odysseus is careful not to reveal his name; when asked for it, Odysseus tells the giant that he is \"Οὖτις\", which means \"nobody\". But later, having escaped after blinding Polyphemus and thinking himself beyond Polyphemus' power, Odysseus boastfully reveals his real name, an act of hubris that was to cause enormous problems later. Knowing his name, Polyphemus was able to call upon Odysseus the revenge of his father, the god Poseidon. Many later episodes of the Odyssey depict Odysseus facing the relentless hostility of Poseidon - which he could have avoided had he persisted in keeping his real name secret. \nAccording to practises in folklore, referred to as 'the Law of Names'; knowledge of a true name allows one to affect another person or being magically. It is stated that knowing someone's, or something's, true name therefore gives the person (who knows the true name) power over them. This effect is used in many tales, such as in the German fairytale of \"Rumpelstiltskin\": within Rumpelstiltskin and all its variants, the girl can free herself from the power of a supernatural helper who demands her child by learning its name.\n\nA legend of Saint Olaf recounts how a troll built a church for the saint at a fantastic speed and price, but the saint was able to free himself by learning the troll's name during a walk in the woods. Similarly, the belief that children who were not baptised at birth were in particular danger of having the fairies kidnap them and leave changelings in their place may stem from their unnamed state. In the Scandinavian variants of the ballad \"Earl Brand\", the hero can defeat all his enemies until the heroine, running away with him, pleads with him by name to spare her youngest brother.\n\nIn Scandinavian beliefs, more magical beasts, such as the Nix, could be defeated by calling their name. For the same reason significant objects in Germanic mythology, which were considered to have some kind of intrinsic personality, had their own names too, for example the legendary Sword Balmung.\n\nMedieval beliefs about witchcraft indicated that the Devil baptized witches with a new, secret name.\n\nIn the folklore of Northern England, there was the belief that a boggart should never be named, for when the boggart was given a name, it would not be reasoned with nor persuaded, but would become uncontrollable and destructive.\n\nGiacomo Puccini used a similar theme in the opera Turandot. The plot turns on whether or not Princess Turandot could learn the name of her unwanted suitor. If she does, she could execute him; if she doesn't, she would have to marry him.\n\nThe term \"true name\" is sometimes used in cryptography and computer security to refer to a name that is assumed to uniquely identify a principal in a global namespace (for example, an X.500 or X.509 Distinguished name). This usage is often critical, with the implication that use of true names is difficult to enforce and unwise to rely on. \n\nIn fantasy where magic works by evoking true names, characters often go to great lengths to conceal their true names. In some settings, such as Ursula K. Le Guin's \"Earthsea\", this is true for all beings. In others, as in Larry Niven's \"The Magic Goes Away\", it applies only to those of magical inclination, as where a wizard is revived from the dead only by another who found his name, and even then only with great difficulty. Finding a true name may require arcane procedures. In \"Earthsea\", a wizard must listen for and give the hero his true name; this is performed in both Le Guin's \"A Wizard of Earthsea\" and \"The Tombs of Atuan\".\n\n\nA character remembering their true name may be an important means of maintaining mastery of their own life. In Hayao Miyazaki's movie \"Spirited Away\", the witch who runs the bathhouse, Yubaba, ensures loyalty by stealing the names of her subjects. For example, one of the witch's most loyal subjects, the spirit of the Kohaku River, has his name taken and is given a slave name: Haku. He forgets his name, and it is in this way 'taken' from him; he warns Chihiro Ogino against the dangers of forgetting her own name. She frees him when she recognises him and he then remembers and 'takes back' his name and is freed from the clutches of the witch.\n\nIn the cyberpunk genre following Vernor Vinge's 1981 \"True Names\" and the work of William Gibson, much of the plot involved interactions between people's virtual selves in cyberspace. Learning a fellow hacker's real-world name (i.e., their \"true name\") could allow you to turn them in to the government or otherwise blackmail them, conveying a kind of power that could be considered analogous to the equivalent concept of myth and legend.\n\n"}
{"id": "1214877", "url": "https://en.wikipedia.org/wiki?curid=1214877", "title": "Universal language", "text": "Universal language\n\nUniversal language may refer to a hypothetical or historical language spoken and understood by all or most of the world's population. In some contexts, it refers to a means of communication said to be understood by all living things, beings, and objects alike. It may be the idea of an international auxiliary language for communication between groups speaking different primary languages. In other conceptions, it may be the primary language of all speakers, or the only existing language. Some religious and mythological traditions state that there was once a single universal language among all people, or shared by humans and supernatural beings.\n\nIn other traditions, there is less interest in or a general deflection of the question. For example in Islam the Arabic language is the language of the Qur'an, and so universal for Muslims. The written Classical Chinese language was and is still read widely but pronounced differently by readers in China, Vietnam, Korea and Japan; for centuries it was a \"de facto\" universal \"literary\" language for a broad-based culture. In something of the same way Sanskrit in India and Nepal, Tamil in India and Sri Lanka and Pali in Sri Lanka and in Theravada countries of South-East Asia (Burma, Thailand, Cambodia), were literary languages for many for whom they were not their mother tongue.\n\nComparably, the Latin language (\"qua\" Medieval Latin) was in effect a universal language of literati in the Middle Ages, and the language of the Vulgate Bible in the area of Catholicism, which covered most of Western Europe and parts of Northern and Central Europe also.\n\nIn a more practical fashion, trade languages, such as ancient Koine Greek, may be seen as a kind of \"real\" universal language, that was used for commerce.\n\nIn historical linguistics, monogenesis refers to the idea that all spoken human languages are descended from a single ancestral language spoken many thousands of years ago.\n\nVarious religious texts, myths, and legends describe a state of humanity in which originally only one language was spoken.\n\nIn Jewish and Christian beliefs, the story of the Tower of Babel tells of a consequent \"confusion of tongues\" (the splintering of numerous languages from an original Adamic language) as a punishment from God.\n\nMyths exist in other cultures describing the creation of multiple languages as an act of a god as well, such as the destruction of a 'knowledge tree' by Brahma in Indic tradition, or as a gift from the God Hermes in Greek myth. Other myths describe the creation of different languages as concurrent with the creation of different tribes of people, or due to supernatural events.\n\nRecognizable strands in the contemporary ideas on universal languages took form only in Early Modern Europe. A \"lingua franca\" or trade language was nothing very new; but an international auxiliary language was a natural wish in light of the gradual decline of Latin. Literature in vernacular languages became more prominent with the Renaissance. Over the course of the 18th century, learned works largely ceased to be written in Latin. According to Colton Booth (\"Origin and Authority in Seventeenth-Century England\" (1994) p. 174) \"The Renaissance had no single view of Adamic language and its relation to human understanding.\" The question was more exactly posed in the work of Francis Bacon.\n\nIn the vast writings of Gottfried Leibniz can be found many elements relating to a possible universal language, specifically a constructed language, a concept that gradually came to replace that of a rationalized Latin as the natural basis for a projected universal language. Leibniz conceived of a \"characteristica universalis\" (also see \"mathesis universalis\"), an \"algebra\" capable of expressing all conceptual thought. This algebra would include rules for symbolic manipulation, what he called a \"calculus ratiocinator\" . His goal was to put reasoning on a firmer basis by reducing much of it to a matter of calculation that many could grasp. The \"characteristica\" would build on an alphabet of human thought.\n\nLeibniz's work is bracketed by some earlier mathematical ideas of René Descartes, and the satirical attack of Voltaire on Panglossianism. Descartes's ambitions were far more modest than Leibniz's, and also far more successful, as shown by his wedding of algebra and geometry to yield what we now know as analytic geometry. Decades of research on symbolic artificial intelligence have not brought Leibniz's dream of a \"characteristica\" any closer to fruition.\n\nOther 17th-century proposals for a 'philosophical' (i.e. universal) language include those by Francis Lodwick, Thomas Urquhart (possibly parodic), George Dalgarno (\"Ars signorum\", 1661), and John Wilkins (\"An Essay towards a Real Character and a Philosophical Language\", 1668). The classification scheme in Roget's Thesaurus ultimately derives from Wilkins's \"Essay\".\n\n\"Candide\", a satire written by Voltaire, took aim at Leibniz as Dr. Pangloss, with the choice of name clearly putting universal language in his sights, but satirizing mainly the optimism of the projector as much as the project. The argument takes the universal language itself no more seriously than the ideas of the speculative scientists and \"virtuosi\" of Jonathan Swift's Laputa. For the like-minded of Voltaire's generation, universal language was tarred as fool's gold with the same brush as philology with little intellectual rigour, and universal mythography, as futile and arid directions.\n\nIn the 18th century, some rationalist natural philosophers sought to recover a supposed Edenic language. It was assumed that education inevitably took people away from an innate state of goodness they possessed, and therefore there was an attempt to see what language a human child brought up in utter silence would speak. This was assumed to be the Edenic tongue, or at least the lapsarian tongue.\n\nOthers attempted to find a common linguistic ancestor to all tongues; there were, therefore, multiple attempts to relate esoteric languages to Hebrew (e.g. Basque and Irish), as well as the beginnings of comparative linguistics.\n\nAt the end of the 19th century, there was a large profusion of constructed languages intended as genuine, spoken language. There were created languages which don't belong to any country and can be learned by everyone. Among these are Solresol, Volapük, and Esperanto, the most spoken constructed language nowadays. At that time, those ideas were readily accepted. With the advent of World Wars and the Cold War, these successes were buried. \n\nThe constructed language movement produced such languages as Latino Sine Flexione (1903), Ido (1907), Occidental (1922), and Interlingua (1951).\n\nEnglish remains the dominant language of international business and global communication through the influence of global media and the former British Empire that had established the use of English in regions around the world such as North America, Africa, Australia and New Zealand. However, English is not the only language used in global organizations such as in the EU or the UN, because many countries do not recognize English as a universal language.\n\nThe early ideas of a universal language with complete conceptual classification by categories is still debated on various levels. Michel Foucault believed such classifications to be subjective, citing Borges' fictional Celestial Emporium of Benevolent Knowledge's Taxonomy as an illustrative example.\n"}
{"id": "37208440", "url": "https://en.wikipedia.org/wiki?curid=37208440", "title": "Verbal language in dreams", "text": "Verbal language in dreams\n\nVerbal language in dreams is the speech—most commonly in the form of a dialogue between the dreamer him/herself and other dream characters—which forms part of the overall (mostly imagistic) dream scenario. Historically, there have been abundant references to verbal language in dreams going back millennia. Early in the twentieth century German psychiatrist Emil Kraepelin presented a large corpus of dream speech, almost all from his own dreams and virtually all deviant, without any pretense that this was representative of dream speech in general. The first systematic elicitation of verbal language in dreams from a large subject pool under methodological protocols was presented beginning in the early 1980s, along with detailed analyses as well as theoretical consideration of the implications for various dream models, from the psychoanalytic approach to more recent theories.\n\nTraditionally, dreams have been defined predominantly in imagistic terms. Prominent dream theories of the modern era from Sigmund Freud's psychoanalytic model (1900) to the present have similarly placed emphasis of the visual aspects of dreams. Yet, even the earliest of written sources, such as the Hebrew Bible and The Odyssey make clear that dreams need not be \"silent movies\"; they may be \"talkies\" incorporating a \"sound track\" abounding in verbal dialogues or monologues.\n\nA survey by Heynick of several books containing over 300 dreams, both genuine reports and dreams incorporated into works of fiction, showed that some three-quarters contained verbal dialogue or explicit reference to speech in the dream. As a specimen of a dream with dialogue as part of a famous work of fiction, Heynick cites the dream of Charles Swann, the main character in Proust's \"Swann's Way\" (1913; italics added):\n\nThe painter remarked to Swann [the dreamer] \"that Napoleon III had eclipsed himself immediately after Odette. \"They had obviously arranged it between them,\"\" he added; \"they must have agreed to meet at the foot of the cliff, but they wouldn't say good-bye together, it might have looked odd. She is his mistress.\" The strange young man burst into tears. Swann endeavored to console him. \"After all, she is quite right,\" he said to the young man, drying his eyes for him and taking off his fez to make him feel more at ease. \"I've advised her to do that myself a dozen times. Why be so distressed? He was obviously the man to understand her.\"\n\nIn 1906 Kraepelin, a pioneer of the somatic approach to psychiatry and of the methodical classification of psychiatric disorders, published a 105-page monograph \"Über Sprachstörungen im Traume\" (On Speech Disorders in Dreams). As the title suggests, Kraepelin's declared aim was to analyze only deviant specimens of speech from dreams. Specimens reflecting correct speech processes were excluded from his study. To this end, Kraepelin assembled in the course of twenty years 286 specimens, the vast majority drawn from his own dreams, with no pretense to nonselectivity. He apparently drew in large measure from hypnagogic and occasionally hypnopompic dreamlets (experienced when falling asleep and waking up), which differ phenomologically from full-fledged dreams and are characterized by different neurological indices as well.\n\nKraepelin meticulously classified his collection of dreams according to the nature of the deviances from correct normal speech in wakefulness. Three-fifths of his specimens were grouped as disorders of word selection, including large numbers of neologisms (non-existing words, typically formed by combinations of existing words or their components); just over one-fifth as disorders of discourse (actaphasia, usually involving the incorrect choice of language dependency relations; and agrammatism, the faulty construction of complex sentences); and just under one fifth as disorders of thought. Although it was well known at the time that the speech of normal people in wakefulness is often fraught with errors, Kraepelin prized his corpus of deviant dream speech for the profound nature of many of the errors they contained, different from the common slips of the tongue made by mentally healthy people in everyday life. He likened various specimens of his dream speech corpus to the speech in waking life of patients with dementia praecox (schizophrenia), speech confusion, and aphasia. Kraeplin saw his dream experiences as affording him (a normal person) first-hand insight into these pathological processes. He further speculated on neurological concomitants involving the activities and interaction of areas of the brain—the cerebral cortex, Wernicke's coil, Broca's coil—which are different from in normal wakefulness. Although several of Kraepelin's dream speech specimens are amenable to interpretation for their latent sexual significance, he had no interest in the psychoanalytic approach and made no reference to his contemporary Freud in any of his writings.\n\nPrior to the 1980s, therefore, no indices or standards existed that were representative of the verbal language component of the dreams of a large general population. But beginning in 1983, Heynick reported in a series of publications the results of two experiments designed to evaluate, first, the linguistic competence and, subsequently, the pragmatic competence of the dreamer in the dreaming state, using large subject pools drawn from the general population (in this case in the Netherlands) and following careful protocols designed to avoid selectivity and maximize accuracy of recall.\n\nThe term \"linguistic performance,\" central to this experiment, derives from the transformational-generative (TG) revolution in linguistics in the second half of the twentieth century and the concomitant emergence of the field of formal psycholinguistics. The TG model of language generation assumes an ideal \"linguistic competence\" on the part of the speaker, theoretically enabling him or her to generate all the infinite number of well-formed sentences in the native language while generating none of the ill-formed sentences. That in actual use, i.e., linguistic performance, the speaker is limited in, among other things, the complexity or elaboration of the sentences he or she can generate, and often produces utterances which are ill-formed, is due to the limitations of the various auxiliary psychological mechanisms at the speaker's disposal, such as limited short-term memory, perceptual limitations, and defective feedback, as well as to factors such as deliberate changes in structure in mid-sentence, and unconscious interference of the Freudian type.\n\nAll the above has traditionally applied to the native speaker in the waking state. The experiment explored the linguistic performance of the native speaker in the dreaming state.\n\n78 Dutch subjects sleeping at home recorded on special forms following precise protocol instructions a total of 566 Dutch utterances directly recalled and transcribed from 566 dreams which had been in progress prior to alarm clock awakening in the morning. (For just over 80 percent of all awakenings, there was either no dream in progress at the time of awakening or no verbal material to be reported from the dream.) The subjects reported (as part of the questionnaire form) that 60 percent of the utterances were said by him or herself in the dream scenario; 40 percent by someone else, usually addressed to the dreamer.\nWord-count analysis showed the utterances in the corpus to have a wide range of length, with a mean utterance length of 7.5 words and a mean sentence length of 6.5 words. The declarative sentences in the corpus were analyzed for complexity (sentential elaboration) using in particular the number of subordinate clauses per unit as an index. The corpus when classed into three groups according to the education level of the subject-dreamers showed that those with most education had the highest sentential elaboration; those with the least, the lowest; with those with intermediate education scoring in between. (There were no standards available for absolute comparison with the sentential elaboration in spoken Dutch in waking life by the three education-level groups in the general population; however, the relative degree of sentential elaboration of the dream corpus of the three education-level groups accorded with the relative indices for written Dutch by the three educational-level groups, which were available for comparison.)\n\n72 of the 566 utterances were marked by their respective subjects (in response to one of the questions on the form) as deviating from wakeful usage, although analysis of the same specimens by two academic linguists deemed the large majority of those marked utterances to be fully acceptable Dutch. Fewer than 5 percent of all utterances clearly deviated from correct wakeful speech. These included semantic anomalies, faulty lexical substitutions, neologisms (word-blends), non-existent proper names, language mixing, and (in two instances) syntactic errors.\n\nThe subjects in the above experiment were not asked to report the full dream scenario during which the utterances were made. Excluded, therefore, from the analysis was any consideration of \"pragmatic competence,\" the \"knowledge of condition and manner or appropriate use [of his or her linguistic ability] in conformity with various purposes,\" which forms part of the broader field of language psychology (rather than formal psycholinguistics).\nDream utterances \"plus\" their dream scenario contexts were gathered in a second experiment, again following carefully defined protocols, this time using the telephone elicitation technique. (Volunteer subjects were awakened on random nights at random hours, on average no more than once a week.) 77 dream reports from 33 subjects were tape-recorded and transcribed. These contained 92 dream speech utterances elicited verbatim from the subjects immediately after provoked awakening (to minimize deterioration in memory recall) plus an additional 113 utterances (81 in direct verbatim form, 32 in indirect form) contained in the subsequent telling of the entire dream. 40 percent of the utterances were said by the dreamer; 60 percent by someone else in the dream scenario, usually addressed to the dreamer (a reversal of the proportions in the linguistic performance experiment).\n\nFive scorers, working independently but achieving a high degree of interscorer reliability, rated the appropriateness of each of the 205 dreams utterances to their situational context in the dream. Averaged out (and rounded off), 67% of the utterances were deemed to be \"entirely suitable to the narrative\"; 20% \"not entirely suitable\"; 7% \"largely unsuitable\"; and 4% \"entirely unsuitable.\"\n\nAs an example, in the following dream all lines of dialogue were deemed by all scorers (in the first instance, by four or the five scorers) to be fully suitable to the narrative:\n\nI was sitting in the garden and reading old magazines. [...] My son [...] took a little girlfriend along, also about five, and she started looking through the magazines. And they were getting wet, since she had such a cold that her nose ran. And I said \"that she, your little girlfriend, should rinse her nose with the lotah\", that's a jug used in yoga [instead of a handkerchief]. I use it myself, too. So I let her do it, but because she was so small, it didn't go right, and she started to cry. And then she walked home. So I said to my little boy, \"I'll go to her mother and tell her what happened, otherwise she'll think 'what a strange lady.'\" [...] Then that girl came out again, and my son had already told her mother that I had tried to help her get rid of her cold with the lotah. I simply knew that he had said that, but I didn't hear him say that literally. Then I said to that girl \"\"In the future, when you catch a cold, you should go to the doctor and have him write out a prescription for medicine.\"\n\nIn general, the dreamer as (to continue the film metaphor) \"script-writer\" appears to have at his or her command not just a \"story grammar\" capable of generating a reasonably, though not always, coherent overall scenario, but also the linguistic pragmatic competence to generate verbal dialogue which is usually, though again not always, appropriate to that scenario.\n\nThe abundance of verbal language in dreams—typically in the form of dialogue between the dreamer and other characters in the dream scenario—which generally shows a syntactic and lexical well-formedness comparable to speech in the waking state, and which in addition is, far more often than not, appropriate to the dream context, has, in Heynick's theoretical analysis, profound consequences for overall dream theories, past and present.\n\nFreud's psychoanalytic model of the mind and, in particular, his theory of dream generation and its function posit the existence of two global modes of mental functioning. The primary process, characterized by such mechanisms as condensations, displacements, and reversals (and the absence of any sense of negation) is theoretically characteristic of the infantile mode of thinking. It is superseded in the course of ontogeny by the development of the conscious part of the mind, which in the older child and adult is governed by the secondary process, adhering to the rules of grammar and logic. The verbal language which the developing child acquires is for Freud by definition a secondary process. The primary process in psychoanalytic theory is however not banished from the mind, but is contained in the unconscious, where it continues to characterize the mode of functioning of that part of the psyche.\n\nThe unconscious with its primary process mode—along with its repressed ideational content from early childhood, such as the Oedipus complex—provides, in Freud's theory, the driving force and initial input to dreams. Freud attributed the generation of a more or less coherent dream narrative to the process of \"secondary revision,\" which might (he vacillated on this issue) be \"a contribution on the part of waking thought to the construction of dreams\" rather than part of the \"dream work\" proper. The dream-like features which one experiences in dreams, such as condensations, displacements (symbolism), and reversals, are the manifestations of the primary-process input to the dream generation process.\n\nYet the dreams of Freud's own and those of his patients, which he provides in \"The Interpretation of Dreams\" and elsewhere, typically abound in verbal dialogue, which is always syntactically well-formed, often complex (containing subordinate clauses) and usually, though not always, semantically well-formed and appropriate to the context of the dream.\n\nAs an example, Heynick cites, among others, the \"dream of Irma's injection, which Freud himself considered to be of central importance to the development of his dream theory:\n\nI said to Irma: \"If you still get pains, it's really your own fault.\" She replied: \"If you only knew what pains I've got in my throat and stomach and abdomen—it's choking me.\" [...] I at once \"called in\" Dr. M. [who] looked quite different from usual; he was very pale, he walked with a limp and his chin was clean shaven. [...] My friend Leopold was [...] saying, \"She has a dull area down on the left.\" He also indicated that a portion of the skin of the left shoulder was infiltrated. (I noticed this, just as he did, in spite of her dress.) [...] M. said: \"\"There's no doubt it's an infection, but no matter, dysentery will supervene and the toxin will be eliminated.\"\n\nVirtually all the deep personal significance which Freud in his analysis attributed to his Irma dream would be lost without the verbal dialogue. Yet the utterances are fully grammatical; they contain properly embedded and conjoined clauses, two conditional phrases, a future tense, and two instances of negation. All words are proper German (in the original). Semantically most of the dialogue appears appropriate to the overall scenario, except the last which is medically absurd.\n\nFreud was apparently confronted with the enigma of how the generation of speech, a secondary process par excellence, could be functioning at such a apparently high level of adequacy during dreaming, the input and motive power of which involves a theoretically primary process. Freud attempted to resolve this problem by introducing his \"replay hypothesis\" (as Heynick terms it), according to which the dialogue in dreams is merely a repeating, in the same or similar words, of speech actually said or heard by the dreamer recently in waking life, usually on the day before the dream, only slightly modified (if at all) by the above-mentioned process of \"secondary revision.\" This theoretical regression to an infantile mimicking of speech thus denied to the dreaming process proper any substantial language functioning or (in modern terms) linguistic competence.\n\nWith a view to testing the validity of Freud's replay hypothesis, the subject-dreamers in the above-mentioned experiment on linguistic competence were asked with regard to each of their dream speech specimens whether they could recall having said the utterance prior to the dream in waking life in the same or similar words. The subjects deemed that to have been the case on the day before the dream for 8% of the utterances and for the week before the dream (including the day before the dream) for 14% of the utterances. Such low figures appear to invalidate Freud's replay hypothesis.\n\nAlthough no neo-Freudians treat the phenomenon of dream speech per se, proposed revisions to the psychoanalytic model of the mind by theorists such as Gill, Holt and Noy, beginning in the late 1960s, variously included the redefining of the place of speech generation (in wakefulness, but this would also apply to speech during dreaming) whereby it would no longer necessarily be classified as a strictly secondary process. Such revisions would, Heynick points out, potentially make the phenomenon of verbal language in dreams less anomalous in the psychoanalytic scheme, and without the need to resort, as Freud did, to the replay hypothesis as an \"ad hoc\" expedient.\n\nThe activation-synthesis hypothesis of dream generation first proposed by Harvard University psychiatrists John Allan Hobson and Robert McCarley in the late 1970s drew upon the discovery earlier in the decade of the ponto-geniculo-occipital (PGO) impulses which during the nightly REM periods (the physiological stage of sleep which is most associated with the phenomenological experience of vivid dreaming) are fired off from the brain stem and travel to the visual areas of the cortex.\n\nIn the visual area of the cortex, according to the model, the impulses call up largely at random a series of images (=activation). The dreamer's sleeping consciousness automatically tries to integrate these images into a more or less coherent story, while, for example, linking them with fragments of memory from the past (=synthesis). (Similar mechanisms involving the PGO impulses have been incorporated into other dream models, namely Francis Crick and Graeme Mitchison's reverse learning theory and Michael Jouvet's endogenous learning hypothesis.)\nThe activation-synthesis hypothesis, although based on neurological data more advanced than was ever available to Freud, is reminiscent of Freud's psychoanalytic model in that the initial input to the process is primitive and chaotic, onto which some order in imposed by a cognitively more advanced process. But as for the psychological interpretation of dreams the authors of the activation-synthesis hypothesis are outspokenly anti-Freudian, in that in their model the initial input to the dreaming process (the PGO-impulses) are devoid of any depth-psychology significance, Oedipal or whatever. (If the dream acquires any significance relating to the dreamer's personality. history, and present circumstances, this occurs in the manner in which the more or less randomly evoked images are integrated into a story during the synthesis stage, analogous to what a subject may \"see\" in the random ink-blots of a Rorschach test.)\n\nBe this as it may, the activation-synthesis hypothesis with its overwhelming emphasis on visual areas of the cortex is silent as to how the extensive verbal language may be generated and integrated into the dream scenario. Heynick cites as ironic a specimen dream which Hobson presents as exemplary of production by the activation-synthesis model:\n\nI am in Willianstown, Massachusetts, talking to a colleague, Van, who is wearing a white shirt (he usually wears blue) open at the neck (he is normally necktied, and even collar-clipped) and khakis (he usually sports flannels). Casual. Van says, as if by the way, \"that he attended the committee meeting that has yesterday considered my candidacy for an invited lecture series.\" (I know from his tone that he is going to deliver bad news.) The committee has decided against it because \"\"They don't feel that psychoanalysis should be confronted with laboratory data.\"\nI allowed at how bad this idea was. \"It's the wrong reason,\" I said. \"And their timing is off, because Adolf Grünbaum is just about to publish his important new book in which he insists that that is precisely what psychoanalysis must do.\" Van ignores this statement, appearing never to have heard of A.G.\nWe go out a door (which is on the corner of the building) to behold the beautiful Williams campus. A red-brick wall extends down a green lawn to the classic white Puritan buildings.\nVan says, \"They chose Mary\" (or seems to say that) \"reflecting their priorities to attract a speaker who might help them with their fund-raising efforts.\" \"\"That's why you have such beautiful buildings,\" I note, \"and why there is nothing in them.\"\"\n\nHobson presents this specimen as an example of how dreams can sometimes reflect personal concerns—in this case relating to his academic squabbles with his colleagues due to his anti-psychoanalytic stance within the psychiatric profession. As Heynick points out, the personal significance in this dream is in fact derived almost exclusively from the verbal dialogue, without which the dream would lose all its meaning.\n\nThe psychoneirics (from Greek; psycho = mind + oneiros = dream) model of dream generation, formulated by Foulkes from the late 1970s onwards, is cited as an exemplary model (though not necessarily the only possible type of model) of how the generation of verbal language can be incorporated into the generation of the overall dream scenario. Patterned on the psycholinguistic model of speech production (in wakefulness), the psychoneirics model is basically non-neurological. It views dream generation in humans as, like speech in humans, a skillful cognitive act, in fact possibly drawing upon the selfsame cognitive abilities.\n\nThe input to the dreaming process involves (not unlike the activation-synthesis hypothesis of dreaming) the diffuse activation of memory elements. Psychoneirics focuses on the midrange dream generation processes (regardless of whether or not the dream happens to include verbal dialogue) as involving \"schematic selection\" and \"element activation,\" analogous to the syntactic frame (sentence structure) selection and word selection of psycholinguistic models. (The dream-like features of dreams, such as condensations (composite images) and anomalous narrative shifts, are seen as residual flux of the dream production mechanism, which is otherwise doing a reasonably good job of developing the initial input into a coherent narrative.) Under this theoretical assumption that the human dream-generation ability and speech-generation ability (in wakefulness) derive from similar cognitive capacities, the psychoneirics model can claim to seamlessly account for the generation of verbal dialogue within dreams.\n\nThe data presented in the above experiments is considered as having implications not just for the evaluation of the various existing models of dream generation, but also for models of speech generation in general, that is in everyday life.\n\nDreaming is a state of consciousness different from the normal wakeful state. With regard to actions within the dream, dreaming consciousness is presumably characterized by a diminished capacity for deliberate intention on the part of the dreamer (including intention of what to say or, as scriptwriter, what to have other characters say) and a diminished attention to, or diminished ability to receive and monitor feedback from, the actions (including speech acts) as they are being carried out in the dream. The characteristics of dialogue in dreams indicate that despite the presumed diminished intention, attention and feedback on the part of the dreamer as speaker-listener and scriptwriter, the utterances generated, far more often than not, are semantically and syntactically well-formed and appropriate to the overall scenario. The implication is that the human capacity for language in general (that is, in everyday wakefulness) can largely rely on processes which, once they are triggered when the conditions match those required for their operation, can generate verbal utterances automatically, outside of awareness and without the need for intervention by the speaker except at points where some critical choice is made.\n\nNotes\nBibliography\n"}
{"id": "664811", "url": "https://en.wikipedia.org/wiki?curid=664811", "title": "Writer's block", "text": "Writer's block\n\nWriter's block is a condition, primarily associated with writing, in which an author loses the ability to produce new work, or experiences a creative slowdown. The condition ranges in difficulty from coming up with original ideas to being unable to produce a work for years. Throughout history, writer's block has been a documented problem.\n\nProfessionals who have struggled with the affliction include authors such as F. Scott Fitzgerald and Joseph Mitchell, comic strip cartoonist Charles M. Schulz, composer Sergei Rachmaninoff, and songwriter Adele. Research concerning this topic was done in the late 1970s and 1980s. During this time, researchers were influenced by the Process and Post-Process movements, and therefore focused specifically on the writer's processes. The condition was first described in 1947 by psychoanalyst Edmund Bergler. However, some great writers may have already suffered from writer’s block years before Bergler described it, such as Herman Melville, who quit writing novels a few years after writing \"Moby-Dick\".\n\nWriter's block may have several causes. Some are creative problems that originate within an author's work itself. A writer may run out of inspiration, or be distracted by other events. A fictional example can be found in George Orwell's novel \"Keep the Aspidistra Flying\", in which the protagonist Gordon Comstock struggles in vain to complete an epic poem describing a day in London: \"It was too big for him, that was the truth. It had never really progressed, it had simply fallen apart into a series of fragments.\"\n\nOther blocks may be produced by adverse circumstances in a writer's life or career: physical illness, depression, the end of a relationship, financial pressures, or a sense of failure. The pressure to produce work may in itself contribute to writer's block, especially if they are compelled to work in ways that are against their natural inclination (i.e. with a deadline or an unsuitable style or genre). Writer's block may also come from feeling intimidated by one's previous big successes. The writer Elizabeth Gilbert, reflecting on her post-bestseller prospects, proposed that such a pressure might be released by interpreting creative writers as \"having\" genius rather than \"being\" a genius.\n\nIt has been suggested that writer's block is more than just a mentality. Under stress, a human brain will \"shift control from the cerebral cortex to the limbic system\". The limbic system is associated with the instinctual processes, such as \"fight or flight\" response; and behavior that is based on \"deeply engrained training\". The limited input from the cerebral cortex hinders a person's creative processes, which are replaced by the behaviors associated with the limbic system. The person is often unaware of the change, which may lead them to believe they are creatively \"blocked\". In her 2004 book \"The Midnight Disease: The Drive to Write, Writer's Block, and the Creative Brain\" (), the writer and neurologist Alice W. Flaherty has argued that literary creativity is a function of specific areas of the brain, and that block may be the result of brain activity being disrupted in those areas.\n\nFor a composition perspective, Lawrence Oliver says, in his article, \"Helping Students Overcome Writer's Block\", \"Students receive little or no advice on how to generate ideas or explore their thoughts, and they usually must proceed through the writing process without guidance or corrective feedback from the teacher, who withholds comments and criticism until grading the final product.\" He says, students \"learn to write by writing\", and often they are insecure and/or paralyzed by rules.\n\nPhyllis Koestenbaum wrote in her article \"The Secret Climate the Year I Stopped Writing\" about her trepidation toward writing, claiming it was tied directly to her instructor's response. She says, \"I needed to write to feel, but without feeling I couldn't write.\" To contrast Koestenbaum experience, Nancy Sommers expressed her belief that papers do not end when students finish writing and that neither should instructors' comments. She urges a \"partnership\" between writers and instructors so that responses become a conversation.\n\nJames Adams notes in his book, \"Conceptual Blockbusting\", various reasons blocks occur include fear of taking a risk, \"chaos\" in the pre-writing stage, judging versus generating ideas, an inability to incubate ideas, or a lack of motivation.\n\nAs far as strategies for coping with writer's block Clark describes: class and group discussion, journals, free writing and brainstorming, clustering, list making, and engaging with the text. To overcome writing blocks, Oliver suggests asking writers questions to uncover their writing process. Then he recommends solutions such as systematic questioning, freewriting, and encouragement. A recent study of 2500 writers aimed to find techniques that writers themselves use to overcome writer's block. The research discovered a range of solutions from altering the time of day to write and setting deadlines to lowering expectations and using mindfulness meditation. \n\nGarbriele Lusser Rico's concern with the mind links to brain lateralization, also explored by Rose and Linda Flowers and John R. Hayes among others. Rico's book, \"Writing the Natural Way\" looks into invention strategies, such as clustering, which has been noted to be an invention strategy used to help writers overcome their blocks, and further emphasizes the solutions presented in works by Rose, Oliver, and Clark. Similar to Rico, James Adams discusses right brain involvement in writing. While Downey purposes that he is basing his approach in practical concerns, his concentration on right brain techniques speaks to cognitive theory approach similar to Rico's and a more practical advice for writers to approach their writer's block.\n\n\n"}
{"id": "5876717", "url": "https://en.wikipedia.org/wiki?curid=5876717", "title": "Yuantang (language game)", "text": "Yuantang (language game)\n\nYuantang is language game spoken by Hakka speakers at Yuantang (), a village in southern China. It is also known as the \"snake language\".\n\nExample : 食饭 → 手习花散 [sit fan] → [siu jit fa san]; eat (rice) → \"hand + learn + flower + separation\".\n\nClearly, the words 食 and 饭 are each split into two sounds, the initial and the rime, thus 食 [sit] is made up of the initial of 手 [s] and the rime of 习 [it], and similarly, 饭 [fan] is [f] from 花 and [an] from 散. This is similar to the traditional Chinese practice of representing sounds by two characters known as fanqie.\n<br>\nThis practice also resembles Jin, another Sinitic language, in its process of splitting a monosyllabic word into two syllables. A similar process is also found in Mandarin.\n\nThere is no solid evidence for the original of Yuantang. But it is believed to be an invention of a local intellectual in Qing dynasty.\n"}
{"id": "714599", "url": "https://en.wikipedia.org/wiki?curid=714599", "title": "ǂKx'ao-ǁ'ae", "text": "ǂKx'ao-ǁ'ae\n\nǂKxʼao-ǁʼae (ǂKxʼauǁʼein, Auen, Kaukau) is a southeastern variety of the !Xuun dialect continuum, spoken in Botswana (the settlements of Groote Laagte, East Hanahai, Kanagas and Ghanzi in Ghanzi District and on the commercial farms) and in Namibia (the city of Gobabis and settlements along the C22 road to Otjinene as far as Eiseb, Omaheke Region) by about 7,000 people. In Botswana, most speakers are bilingual in Naro or Tswana.\n\nThere are numerous spellings of the name, including \"ǁAuǁei, ǁX’auǁ’e,\" and \"Auen\". Endonyms are \"Juǀʼhoan, ǃXuun \"in Namibia and \"ǂKx'ao||'ae\" (predominantly in Botswana) meaning \"northern people\" in Naro. It also goes by the names \"Gobabis ǃKung\" and \"Kaukau \"(which can take the noun class prefixes in Tswana to give \"Mokaukau \"for one person, \"Bakaukau\" for the group and \"Sekaukau\" for the language).\n\nIn Namibia, ǂKx'ao||'ae\" \"tends to refer literally to the !Xuun speakers to the north in the Caprivi area. With the exception of a few cultural traits, speakers of ǂKx'ao||'ae\" \"in Botswana and those of Ju|'hoan in Namibia argue that they are one and the same people, speaking one language, with some dialectal attributes.\n\nThe non-Latin characters used by the language predominantly refer to click consonants and follow the orthography by Patrick Dickens for Juǀ'hoan.\n\nThe limited data on these dialects is poorly transcribed, but as of 2015 fieldwork is in progress.\n\n\n"}
