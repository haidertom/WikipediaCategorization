{"id": "4503405", "url": "https://en.wikipedia.org/wiki?curid=4503405", "title": "American Speech–Language–Hearing Association", "text": "American Speech–Language–Hearing Association\n\nThe American Speech–Language–Hearing Association (ASHA) is a professional association for speech–language pathologists, audiologists, and speech, language, and hearing scientists in the United States and internationally. It has more than 197,856 members and affiliates.\n\nThe mission of the American Speech–Language–Hearing Association is to promote the interests of and provide the highest quality services for professionals in audiology, speech–language pathology, and speech and hearing science, and to advocate for people with communication disabilities.\n\nThe association's national office is located at 2200 Research Boulevard, Rockville, Maryland. The organization also has an office on Capitol Hill.\n\nArlene Pietranton is currently serving as the association's executive director.\nASHA was founded in 1925 as the American Academy of Speech Correction. The current name was adopted in 1978.\n\nThe 2014 ASHA conference was held in Orlando, Florida from November 20–22.\n\nThe 2017 ASHA conference will be held in Los Angeles, California from November 9–11.\n\nThe Council for Academic Accreditation in Audiology and Speech-Language Pathology (CAA) is the accreditation unit of the ASHA. Founded over 100 years ago by American universities and secondary schools, CAA established standards for graduate program accreditation that meet entry-level preparation in the speech and hearing field. Accreditation is available for graduate programs with a master's degree in Speech-Language Pathology or clinical doctoral program in audiology.\n\nProfessionals of Communication Sciences and Disorders (CSD) can become members of ASHA. These professionals include audiologists, speech-language pathologists, and speech-language-hearing scientists. As of December 31, 2017, there are more than 197,856 members and affiliates of ASHA. Opportunities ASHA membership brings include access to publications associated with ASHA, to continuing education programs through ASHA, to a platform to network with other CSD professionals, to career-building tools, and to money-saving programs.\n\nASHA sponsors special interest groups (SIGS) within the organization as a means of promoting community and learning in more specialized topics. As of 2016, ASHA has 19 established Special Interest Groups (SIG). These have been added through the years. ASHA members can be a SIG Affiliate of any number of SIGS, with each affiliation requiring nominal yearly dues. The 19 SIGS are:\n\n"}
{"id": "52715832", "url": "https://en.wikipedia.org/wiki?curid=52715832", "title": "Anglophone problem (Cameroon)", "text": "Anglophone problem (Cameroon)\n\nThe Anglophone Problem, as it is commonly referred to in Cameroon, is a socio-political issue rooted in Cameroon's colonial legacies from the Germans, British, and the French.\n\nThe issue classically and principally opposes many Cameroonians from the Northwest and Southwest regions, many of whom consider themselves anglophones, to the Cameroon government. This is based on the fact that these two regions (formally British Southern Cameroons) were controlled by Britain as a mandated and trust territory of the League of Nations and the United Nations respectively.\" \n\nWhile many Northwesterners and Southwesterners believe there is an anglophone problem, some do not. In fact, the term \"anglophone\" today creates a lot of controversy, as many former French-speaking Cameroonians who are either bilingual or speak only English (most of whom have gone through the English sub-system of education) consider themselves as anglophones. The root of the Anglophone problem in Cameroon can be traced back to the Foumban Conference of 1961 that united the two territories, with different colonial legacies, into one state. The Anglophone Problem is increasingly dominating the political agenda of Cameroon. This problem has led to arguments and actions (protests, strikes, etc.) that argue for federalism or separation from the union by the Anglophones. Failure to address the Anglophone Problem threatens Cameroon's ability to create national unity between the two groups of people.\n\nThe roots of the Anglophone problem can be traced back to World War I, when Cameroon was known as German Kamerun. The German Empire first gained influence in Cameroon in 1845 when Alfred Saker of the Baptist Missionary Society introduced a mission station. In 1860, German merchants established a factory: the Woermann Company. On July 5, 1884, local tribes provided the Woermann Company with rights to control the Kamerun River, consequently setting the foundation for the later German colonization of Kamerun. In 1916, during World War I, France and Britain joined forces to attack and seize German Kamerun. Later, the Treaty of Versailles would award France and Britain mandates over Cameroon as punishment of the Germans who lost the war. Most of German Kamerun was given to the French, over 167,000 square miles of territory. The British were given Northern Cameroons, about 17,500 square miles of territory and Southern Cameroons, 16,580 square miles. Each colonizer would later influence the colonies with their European languages and cultures, thus rendering them as Anglophones and Francophones. The large difference in awarded territory has resulted in present-day Cameroon having a huge majority Francophone population and a very small minority Anglophone population.\n\nFollowing World War II, a wave of independence flowed rapidly throughout Africa. The United Nations obliged that Britain and France relinquish their colonies and guide them towards independence. There were three political options for British Southern Cameroons. They could become independent by uniting with Nigeria or with French Cameroun. No option of self-determination by becoming independent was given. The most desired option was independence with the least popular being unification with French Cameroun. However, during the British Plebiscite of 1961, the British argued that Southern Cameroons was not economically viable enough to sustain itself as an independent nation and could only survive by joining with Nigeria or La République du Cameroun (the Republic of Cameroon). Though documents on the United Nations' \"Non-Self-Governing Territories\" state, \"integration should be the result of the freely expressed wishes of the territory's peoples\", the United Nations would later reject Southern Cameroons' appeal to have independence as a sovereign nation placed on the ballot. The plebiscite questions were: \nThe United Nations documents defined the basis of integration as: \"Integration with an independent State should be on the basis of complete equality between the peoples of the erstwhile Non-Self-Governing Territory and those of the independent country with which it is integrated. The peoples of both territories should have equal status and rights of citizenship... at all levels in the executive, legislative and judicial organs of government.\" With this promise in mind, on February 1961, British Northern Cameroons voted to join Nigeria, while British Southern Cameroons voted to join La République du Cameroun.\n\nThe purpose of the Foumban Constitutional Conference was to create a constitution for the new Federal state of British Southern Cameroon and La République du Cameroun. The conference brought together representatives from La République du Cameroun, including Amadou Ahidjo, their president, with representatives from Southern Cameroons. Two weeks before the Foumban Conference, there were reports that more than one hundred people were killed by terrorists in Loum, Bafang, Ndom, and Douala. The reports worried unification advocates who wanted British Cameroon to unify with French Cameroun. For the conference, the location of Foumban had been carefully chosen to make Ahidjo, appear as if he had everything under control. Mr. Mbile, a Southern Cameroonian representative at the conference noted, \"Free from all the unrest that had scared Southern Cameroonians, the Francophone authorities had picked the place deliberately for the occasion. The entire town had been exquisitely cleaned up and houses splashed with whitewash. Food was good and receptions lavish. The climate in Foumban real or artificial went far to convince us that despite the stories of 'murder and fire,' there could be at least this island of peace, east of the Mungo.\"\n\nBefore the Foumban Conference, all the parties in Southern Cameroons, the Native Authority Councils and the traditional leaders attended the Bamenda Conference. This conference decided on a common proposal to present when negotiations with La République du Cameroun arrived. Among many things, the Bamenda Conference agreed on a non-centralized federation to ensure there was a distinction between the powers of the states and the powers of the federation. Most of the proposals from the Bamenda Conference were ignored by Ahidjo. Some of these proposals included having a bicameral legislature and decentralizing power, but instead a unicameral system was established with a centralized system of power.\n\nAt the Foumban conference, Ahidjo presented delegates with a draft constitution. By the end of the conference, instead of creating an entirely new constitution, the contributions of the Southern Cameroons delegates were reflected in suggestions made to the draft initially presented to them. John Ngu Foncha and Ahidjo intended for the Foumban Constitutional Conference to be brief, however delegates left the three day conference with the impression that there would be sequential conferences to continue the drafting of the constitution. Mbile later noted, \"We may have done more if we had spent five months instead of five days in writing our constitution at Foumban.\" The Constitution for the new Federal Republic was agreed in Yaoundé in August 1961, between Ahidjo and Foncha, pending approval by the House of Assembly of the two states. In the end, the West Cameroon House of Assembly never ratified the Constitution. However, on October 1, 1961, the Federal Republic of Cameroon nevertheless came to fruition.\n\nOn May 6, 1972, Ahidjo announces his decision to convert the Federal Republic into a unitary state, on the provision that the idea was supported via referendum. This suggestion violated the articles in the Foumban document that read: 'any proposal for the revision of the present constitution, which impairs the unity and integrity of the Federation shall be inadmissible,' and 'proposals for revision shall be adopted by simple majority vote of the members of the Federal Assembly, provided that such majority includes a majority of the representatives ... of each of the Federated States,'... not through referendum. Such violations easily allowed for the passing of the referendum that turned the Federal Republic into the United Republic of Cameroon. Taking into account these actions, the evidence shows that the Francophone's intentions may have not been to form a federal state, but rather to annex Southern Cameroons and not treat them as equals. In 1984, Ahidjo's successor, Paul Biya, replaced the name \"United Republic of Cameroon\" with \"La République du Cameroun,\" the same name the francophone Cameroon had before federation talks. With changes in the Constitution of 1996, reference to the existence of a territory called the British Southern Cameroons that had a \"functioning self-government and recognized international boundaries\" was essentially erased.\n\nDespite the non-acknowledgement/denial of the Anglophone problem from Francophone government leaders, there exists a discontent by Anglophones, both young and old, as to how Anglophones are treated. This discontent presents itself in calls for federation or separation with movements that are garnering strength. At the core of Anglophone grievances is the loss of the former West Cameroons as a \"distinct community defined by differences in official language and inherited colonial traditions of education, law, and public administration.\" On 22 December 2016, in a letter to Paul Biya, the Anglophone Archbishops of Southern Cameroons define the Anglophone problem as follows:\n\nMovements which advocate the separation of English-speaking Cameroon from French-speaking Cameroun exist, led by the Cameroon Action Group, the Southern Cameroons Youth League, the Southern Cameroons National Council, the Southern Cameroon Peoples Organization and the Ambazonia Movement.\n\nAdvocates of Federation want a return to the constitution agreed upon in the 1961 Foumban Conference that acknowledges the history and culture of the two regions while giving equal power to the two. This federation had been dismantled on 20 May 1972 by the larger French-speaking Cameroon and extended the latter's executive power throughout West Cameroon. Federation advocates include the instrumental Consortium of the leaders of three Cameroon-based trade unions: Lawyers, Teachers, and Transporters. It also includes some Cameroonians in the diaspora led by a well organized US-based Anglophone Action Group, Inc. (AAG). AAG was one of the first groups in the diaspora to endorse the Cameroon-based Consortium as a peaceful alternative to achieving a return to the pre-1972 federated system. Opponents of federation include the ruling Cameroon Peoples Democratic Movement.\n\nUnitarianism do not want Federation or Separation, but rather a decentralized unitary government; whereas, now the government is highly centralized in power. This violates the tenets of the 1996 Constitution as decentralization has yet to be implemented.\n\nIn March 1990, the Social Democratic Front (SDF) led by John Fru Ndi, was founded on the perception of widespread Anglophone alienation. The SDF was the first major opposition party to the People's Democratic Movement, led by Paul Biya.\n\nBelow are various reasons that Anglophones feel marginalized, systemically, by the government. \n\n\n, the Anglophone problem is still on-going. It has spiraled into violence with police officers and gendarmes shooting dead several civilians. Official sources have put the number at 17 dead, but local individuals and groups have talked of 50 or more. Radical members of some secessionist groups have killed several police officers and gendarmes. 15,000 refugees have fled Southern Cameroons into neighboring Nigeria, with the UNHCR expecting that number to grow to 40,000 if the situation continues.\n\nWithout clearly acknowledging the existence of the Anglophone problem, the President of Cameroon has attempted to appease tensions by making a number of announcements:\nSeveral separatist or secessionist groups have emerged or become more prominent as a result of the harsh response by the government to the Anglophone problem. These groups desire to see Southern Cameroons completely separate from \"La République du Cameroun\" and form its own state, sometimes referred to as \"Ambazonia\". Some groups such as the \"Southern Cameroon Ambazonia United Front\" (SCACUF) are using diplomatic means in an attempt to gain independence for the Anglophone regions, whereas other groups have begun to employ armed confrontation with artisan weapons against the deployed gendarmes and soldiers in those regions.\n\n"}
{"id": "15047743", "url": "https://en.wikipedia.org/wiki?curid=15047743", "title": "Authors' conference", "text": "Authors' conference\n\nAn authors' conference or writers' conference is a type of conference where writers gather to review their written works and suggest improvements. This process helps an author improve his or her work and learn to be a better writer for future works, both by receiving critiques of their own work and by mentoring the work of the other authors. Writers may also benefit from meeting and hearing from professionals in related fields, such as agents, editors, illustrators, publishers, and providers of other relevant services.\n\nUnlike most other conference styles, an authors' conference is very participatory. Most conferences are divided into presentations, each of which has a clear separation of roles among a one or more presenters and an audience. While authors' conferences may include some such presentations, writers' conferences also include numerous sessions wherein an author does not present his or her work but rather listens while the other participants discuss the work. In this way, the author gains an understanding of what readers learn by reading the work.\n\nAn authors' conference consists of two phases, shepherding and writers' workshops. Shepherding usually (but not always) occurs before the conference meeting, and the meeting itself is organized as a writers' workshop.\n\nOne popular series of authors' conferences is the Pattern Languages of Programming conferences, held to encourage and assist authors of software design patterns and pattern languages.\n\nThe shepherding process occurs before the conference meeting. Authors submit papers for the conference, then each paper is assigned to a shepherd, an experienced author who works with the submitter to improve his or her paper. The process often consists of three iterations in as many weeks where the shepherd makes suggestions for improvement and the submitter incorporates the suggestions. This one-on-one mentoring using the submitter's unfinished work as an example is very effective for teaching the submitter how to be a better writer, although its effectiveness is ultimately determined by the participants' dedication and their working relationship. At the end of the shepherding process, the shepherd recommends whether to accept the submission for review at the conference. An accepted submission should meet minimal quality standards for effective evaluation in the writers' workshop, and should have been improved significantly by the submitter during the shepherding process.\n\nThe writers' workshop process occurs during the conference meeting. The workshop consists of 6–10 sessions, one per submission to be reviewed. The workshop participants are the authors of those submissions and any other reviewers the authors choose to accept. In a session, the authors of the submission listen quietly while the other participants discuss what they liked about the submission and suggest improvements. This gives the authors of the submission insight into what information readers are learning from the work and ideas for improving the work. Reviewers discussing a work are careful to make productive comments, both because the author is listening and because in other sessions that author will become the reviewer and make comments.\n\n\n"}
{"id": "9469448", "url": "https://en.wikipedia.org/wiki?curid=9469448", "title": "Basic writing", "text": "Basic writing\n\nBasic writing, or developmental writing, is a discipline of composition studies which focuses on the writing of students sometimes otherwise called \"remedial\" or \"underprepared\", usually freshman college students.\n\nSometimes called “remedial” or “developmental” writing, basic writing (BW) was developed in the 1970s, generally under the constraints of open admissions policies. Basic writing courses are meant to help students come to a basic understanding and familiarity with formal written English. BW students can be categorized two ways: 1) students coming straight from high school, who did not develop a basic competency in formal written English before graduation and who placed below average on a college writing placement test, and 2) non-traditional students who are older than average college freshman and who are coming to college for the first time in order to further their education in the hopes of gaining the skills necessary for better employment and earning more money. These are generally students that may have full-time jobs, come to classes at night, and may have children, and perhaps be a single parent. In some cases non-native and ESL students are also considered basic writers, because of their unfamiliarity with the English language, let alone formal written English (sometimes identified as standard English).\n\nBW students are usually characterized by a lack of understanding of the rules of formal written English which may manifest itself in non-traditional syntax, grammar, spelling, punctuation, usage, mechanics, organization, and clarity. Mina Shaughnessy, a pioneer in the field of basic writing, characterized basic writers as “those that had been left so far behind the others in their formal education that they appeared to have little chance of catching up, students whose difficulty with the written language seemed of a different order from those of the other groups, as if they had come, you might say, from a different country, or at least through different schools, where even modest standards of high-school literacy had not been met.\" However, BW is also a relative term. What might be considered freshman-level writing at one university might be characterized as basic writing at another, or even advanced writing at another, depending on the ability of the general student population and university standards.\n\nBasic Writing is also a field of study. The research in the field probes into the concerns of teachers and students on the academic margins. Deborah Mutnick explains about Basic Writing:\n\n“It signifies struggles for inclusion, diversity and equal opportunity; debates over standards and linguistic hegemony; the exploitation of faculty and staff on the academic margins; and the policies that opened and not threatened to close higher education’s doors to masses of people. It has played a key role not only in providing opportunities for research on adult literacy but also in illuminating the politics of writing in terms of race, class, ethnicity, and other social structures that would have remained invisible in the mostly white, middle-class classrooms that have traditionally constituted the “mainstream.”\n\nThe creation of basic writing courses in colleges across the United States is largely the result of the creation of open-admissions policies that no longer required academic standards be set for entrance into college. The first to start such a program was the City University of New York (CUNY). Before opening their campus to all those who wanted higher education, regardless of previous academic performance, CUNY had instituted the SEEK program (Search for Education, Elevation, and Knowledge) which was designed as a pre-collegiate program that was meant to prepare students, who were not yet ready to enter the university, for full admission. However, with the advent of open admissions in 1970 there was no longer a need for pre-collegiate classes, so the program transformed into a course taken by those admitted to the university who did not place well on admissions placement tests. The writing program that stemmed from this transformation became known as a basic writing course because it dealt not with preparing highly literate students for upper-level course work, but with the teaching the very basics of written communication.\n\nSince the late seventies, many colleges and universities have created open admissions policies, and have in turn created BW programs across the country. However, from the very beginning there has been large opposition to open admissions policies. Open admissions detractors have prevailed at some colleges and universities, overturning open admissions policies. As a result, BW course have either been eliminated entirely from the curriculum or have been relegated to community colleges.\n\nMina P. Shaughnessy (pronounced MY-NA SHAWN-ES-EE), involved with the SEEK program at CUNY, was a proponent of open admissions for City College (part of the CUNY system) and became director of the BW program once City College opened its doors to all. Shaughnessy worked hard not only to design a curriculum for students that seemed alien to the professors that literally did not know what to do with students who seemed not to be able to put two words together, in some cases, but to understand and categorize the characteristics of basic writers in order to understand them better, and be able to teach them more effectively. For this purpose, Shaughnessy compiled four-thousand placement essays written by students as part of the entrance process into City College and classified the errors that she found, trying to understand the logic behind spelling, syntax, grammar, etc., that seemed, at best, scattered and, at worst, completely arbitrary. She published her results in the book Errors and Expectations (1977). Her main conclusion is that these writers are not scattered or arbitrary, but that they have created systems of written English based on misunderstood rules, half-understood lessons on punctuation, their own local or familial dialects, among others, and have logically created their own systems of written English. It is not that these students do not understand communication, but they simply have not been taught or have misunderstood the rules of written formal English. Shaughnessy’s work was considered groundbreaking and Errors and Expectations is still considered the seminal book in the field of BW. And although she died in 1978, and other scholars have made contributions to the field, Shaughnessy remains its leading figure today.\n\nMina P. Shaughnessy is arguably the most prominent name in the field of BW. She helped create the atmosphere of academic respectability BW needed to become recognized as a legitimate scholarly field. Her 1977 book, Errors and Expectations, set the tone for much (if not all) of the BW scholarship that followed. BW scholars, whether they agree with Shaughnessy or not, are still responding to her.\n\nThe ‘‘‘Bedford Bibliography for Teachers of Basic Writing’’’ includes this annotation for Errors and Expectations:\n\nAll in all, Shaughnessy saw disadvantaged students as being intelligent (though scholastically under-prepared). She resolutely held that such students could be taught how to effectively write. It is teachers of BW and not BW students that need to radically alter their views toward the teaching and learning of writing. In her 1976 speech, \"Diving In: An Introduction to Basic Writing\", she asserted that “teachers (need to) realize and accept the need to remediate themselves regarding the needs and learning styles of basic writers.” \n\nFrom the ‘‘‘Bedford Bibliography for Teachers of Basic Writing’’’:\n\nTo be fair, we should note that Lu’s critique applied to other big names in the field of BW including Mike Rose (see below).\n\nOther notable scholars of BW, however, like Laura Gray-Rosendale have claimed that such critiques of Shaughnessy do not hold much critical weight. “Shaughnessy’s works,” themselves, she claims, “render ambiguous if not outright defy many such negative characterizations.” . Also, David Bartholomae (see below) has defended Shaughnessy’s emphasis on error in BW students’ writing (though he still advocates refocusing BW to help students learn various academic dialects). Adjusting the focus a bit, “Bartholomae extends Mina Shaughnessy's hope that teachers, especially basic writing teachers, will examine how they view errors in student writing. For example, he suggests that teachers who cannot understand student prose do not read the prose as complex texts and thus do not find the logic at work in many errors.” \n\nDavid Bartholomae is Professor of English and Chair of the English Department at the University of Pittsburgh. Bartholomae’s most-referenced publication about BW is the book chapter “Inventing the University.” The following is a selection from that chapter:\n\nBartholomae asserts that (though important) error should not determine the efforts of or relationship between BW teachers and their students. Rather, BW teachers should recognize that the language they demand from their BW students (typically short, direct, non-abstruse sentences) is not the language that they (the teachers) typically write and publish in. Students experience such disconnect between what they learn from their writing classes and what their discipline specific course require of them that they are often left to their own devices to figure out how to write acceptably in any given discipline. To resolve this, Bartholomae believes that BW teachers should immerse their students with academic writing (peer reviewed journal articles, book chapters, etc.). BW students thus get healthy exposure to sufficient sums of “academic” language in a teacher-assisted environment. This, Bartholomae claims, should help BW students make the transition more quickly to start writing “academically.” The idea that academics might personally strive to exhibit a clear, cogent, and elegant style in their articles and chapters, and thus model good writing for their students to emulate, is not entertained.\n\nMike Rose is Professor of Social Research Methodology at UCLA. He is best known in the BW community for his part autobiographical/part pedagogically philosophical book, ‘‘Lives on the Boundary.’’\n\nThe \"Bedford Bibliography for Teachers of Basic Writing\" offers this annotation:\n\nRose’s main interests in the study of thinking and learning include the, “study of the factors – cognitive, linguistic, socio-historical, and cultural – that enhance or limit people's engagement with written language.” As well as, “The development of pedagogies and materials to enhance critical reading and writing, particularly at the secondary and post-secondary level, and particularly with ‘underprepared’ or ‘at risk’ populations.” \n\nAdditionally, Rose has argued for the term basic writing as opposed to the terms \"developmental\" or \"remedial\" which have the connotations of medical terminology:\n\n\"This atomistic, medical model of language is simply not supported by more recent research in language and cognition. But because the teaching of writing – particularly teaching designated remedial has been conceptually, and [...] administratively segmented from the rich theoretical investigation that characterizes other humanistic study, these assumptions have rarely been subjected to rigorous and comprehensive scrutiny.\"\n\n\nBasic Writing E-journal (BWe)\n"}
{"id": "38746191", "url": "https://en.wikipedia.org/wiki?curid=38746191", "title": "Bote-Darai language", "text": "Bote-Darai language\n\nBote (Bote-Majhi) and Darai are mutually intelligible tribal dialects of Nepal that are close to Danwar Rai but otherwise unclassified. Speakers are rapidly shifting to Nepali.\n"}
{"id": "10078977", "url": "https://en.wikipedia.org/wiki?curid=10078977", "title": "Broken English", "text": "Broken English\n\nBroken English is a poorly spoken or ill-written version of the English language, sometimes considered a pidgin. Under the strictest definition of the term, broken English consists of English vocabulary grafted onto the syntax of a non-English speaker's native language, including word order, other aspects of sentence structure, and the presence or absence of articles in the speaker's native language. Typically, the non-English speaker also strips English phrases of linguistic markings that are definite articles or certain verb tenses.\n\nIn some communities, young people may intentionally adopt versions of the English language that older people consider to be broken English. This has been documented, for example, among the Māori of New Zealand, where the younger generation was more proficient in English than the previous generation, but intentionally made modifications to the language to assert their own sense of cultural identity.\n\nIn literature, broken English is often used to depict the foreignness of a character, or that character's lack of intelligence or education. However, poets have also intentionally used broken English to create a desired artistic impression, or as a creative experiment writing somewhere between standard English and a local language or dialect.\n\nFor example, in \"Henry V\", William Shakespeare used broken English to convey the national pride of Scottish and Irish allies in the King's invasion of Normandy. When Henry himself last implores the French Princess Katherine to marry him, knowing that her command of the English language is limited, he says to her: \"Come, your answer in broken music; for thy voice is music and thy English broken; therefore, queen of all, Katherine, break thy mind to me in broken English.\"\n\n"}
{"id": "38164895", "url": "https://en.wikipedia.org/wiki?curid=38164895", "title": "Citizen, speak Turkish!", "text": "Citizen, speak Turkish!\n\nThe Citizen, speak Turkish! () campaign was an initiative created by law students but sponsored by the Turkish government which aimed to put pressure on non-Turkish speakers to speak Turkish in public in the 1930s. In some municipalities, fines were given to those speaking in any language other than Turkish. The campaign has been considered by some authors as a significant contribution to Turkey's sociopolitical process of Turkification.\n\nDuring the Ottoman Empire in 1911, the Committee of Union and Progress decided to employ the Turkish language in all the schools of the Empire, with the aim to denationalize all the non-Turkish communities and instil patriotism among Turks. The reformation of the state schooling system and of language by the compulsory use of demotic Turkish aimed for the linguistic homogenization of society. The standardization of the Turkish language aimed to sever the link with the Ottoman language and past in order to create a new sense of Turkish nationhood.\n\nWhen the Turkish Republic was founded, nationalism and secularism were two of the founding principals. Mustafa Kemal Atatürk, the leader of the early years of the Republic, aimed to create a nation state () from the Turkish remnants of the Ottoman Empire. Kemalist ideology defines the \"Turkish People\" as \"those who protect and promote the moral, spiritual, cultural and humanistic values of the Turkish Nation.\" Kemalist criteria for national \"identity\" or simply being a Turk also refers to a shared language. In 1931 in a speech in Adana, Atatürk was quoted during a speech as saying: \n\nMany Turkish politicians and intellectuals believed that in order to attain full rights as a Turkish citizen, one must learn and speak Turkish. One such intellect, Hamdullah Suphi Tanrıöver, believed especially that minorities could not be accepted as citizens of Turkey if they did not speak Turkish or accepted Turkish culture. Consequently, non-Turkish languages taught in minority schools were becoming less common, whereas in May 1923, the Turkish Ministry of Education made the teaching of the Turkish language, history, and geography compulsory in all non-Muslim schools. These subjects had to be taught in Turkish by \"pure Turks\" appointed by the Ministry. The \"pure Turk\" teachers received a salary set by the Ministry which was substantially higher than regular teachers, resulting in a heavy financial burden for minority schools.\n\nIn 1935, during a speech at the Republican Party's fourth congress, Prime Minister İsmet İnönü was quoted as saying, \"We will not remain silent. All citizens who live with us must speak Turkish!\"\n\nThe campaign went beyond the measures of mere policy of speaking Turkish, to an outright prevention and prohibition of any other language.\n\nOn January 13, 1928, the student union at the Darülfünun Law School in Istanbul started a campaign with the objective of preventing the use of languages other than Turkish to be used in public. Signs were held by campaign organizers that proclaimed, \"We cannot call Turk to those who do not speak Turkish\". Some campaigners also chanted, \"Speak Turkish or leave the country!\". The campaigners placed posters in the major cities of the country with the slogan \"Citizen, speak Turkish!\" and the message further spread into the mass media, press, and political circles throughout the country. Signs in theaters, restaurants, hotels, and ferries urged everyone to speak Turkish and many people were harassed in public or criminalized for using a language other than Turkish.\n\nCitizens found to be using a language other than Turkish may sometimes have been charged with violating Article 159 (now defunct) of the Turkish penal code for \"insulting Turkishness\" as a legal justification.\n\nIn the 1960s, the movement saw its revival as posters and signs were placed and hung throughout the country.\n\nPrior to the launch of the \"Citizen, speak Turkish!\" campaign, many initiatives were already taken by the government of Turkey to make Turkish the sole language of the public. In 1924 during a session of the Turkish National Assembly, a law was proposed to make Turkish a compulsory language and refusing to speak it resulted in a fine. Meanwhile, as the debates in the National Assembly were ongoing, the municipal government of Bursa took the first initiative and began to impose fines to those who spoke a non-Turkish language in public areas. This was followed by the cities of Balıkesir and Bergama in 1927.\n\nAfter the launch of the \"Citizen, speak Turkish!\" campaign of 1928, arrests were being made all throughout the country with full support of the government who encouraged provincial governors 'to incorporate Turks with foreign dialects into the Turkish community by making Turkish their mother tongue'. In 1933, in the town of Mersin, British citizens, speaking French, were reportedly attacked in public. It was later reported that hundreds were being arrested for speaking languages other than Turkish in public. In a specific case, a M. Chalfoun and a certain Jewish merchant were arrested for speaking Arabic and French to a merchant in town. The accused were released only after the mayor of Mersin pardoned them after visiting them in prison.\n\nNew laws were being promulgated throughout the country. In 1936, the municipal governments of Tekirdağ, Lüleburgaz, and Edirne passed decrees to fine those who spoke non-Turkish languages in public. Soon thereafter, cities and towns such as Diyarbakır, Adana, Ankara, and Kırklareli followed suit.\n\n\n"}
{"id": "3788502", "url": "https://en.wikipedia.org/wiki?curid=3788502", "title": "Classical Mongolian language", "text": "Classical Mongolian language\n\nClassical Mongolian is an extinct Mongolic language formerly used in Mongolia, China, and Russia. It is a standardized written language used in the 18th century and 20th centuries. Notable texts include the translation of the Kanjur and Tanjur and several chronicles roughly between 1700 and 1900. \n\n\"Classical Mongolian\" sometimes refers to any language documents in Mongolian script that are neither Pre-classical (i.e. Middle Mongol in Mongolian script) nor modern Mongolian.\n\n"}
{"id": "644223", "url": "https://en.wikipedia.org/wiki?curid=644223", "title": "Computer-assisted translation", "text": "Computer-assisted translation\n\nComputer-assisted translation,computer-aided translation or CAT is a form of language translation in which a human translator uses computer hardware to support and facilitate the translation process.\n\nComputer-assisted translation is sometimes called machine-assisted, or machine-aided, translation (not to be confused with machine translation).\n\nThe automatic machine translation systems available today are not able to produce high-quality translations unaided: their output must be edited by a human to correct errors and improve the quality of translation. Computer-assisted translation (CAT) incorporates that manual editing stage into the software, making translation an interactive process between human and computer.\n\nSome advanced computer-assisted translation solutions include controlled machine translation (MT). Higher priced MT modules generally provide a more complex set of tools available to the translator, which may include terminology management features and various other linguistic tools and utilities. Carefully customized user dictionaries based on correct terminology significantly improve the accuracy of MT, and as a result, aim at increasing the efficiency of the entire translation process.\n\nComputer-assisted translation is a broad and imprecise term covering a range of tools, from the fairly simple to the complicated. These can include:\n\nTranslation memory programs store previously translated source texts and their equivalent target texts in a database and retrieve related segments during the translation of new texts.\n\nSuch programs split the source text into manageable units known as \"segments\". A source-text sentence or sentence-like unit (headings, titles or elements in a list) may be considered a segment. Texts may also be segmented into larger units such as paragraphs or small ones, such as clauses. As the translator works through a document, the software displays each source segment in turn, and provides a previous translation for re-use if it finds a matching source segment in its database. If it does not, the program allows the translator to enter a translation for the new segment. After the translation for a segment is completed, the program stores the new translation and moves on to the next segment. In the dominant paradigm, the translation memory is, in principle, a simple database of fields containing the source language segment, the translation of the segment, and other information such as segment creation date, last access, translator name, and so on. Another translation memory approach does not involve the creation of a database, relying on aligned reference documents instead.\n\nSome translation memory programs function as standalone environments, while others function as an add-on or macro for commercially available word-processing or other business software programs. Add-on programs allow source documents from other formats, such as desktop publishing files, spreadsheets, or HTML code, to be handled using the TM program.\n\nTranslation memory technology is particularly useful to organizations translating text that contains specialized vocabulary related to a particular industry, such as automotive manufacturing.\n\nNew to the translation industry, Language search-engine software is typically an Internet-based system that works similarly to Internet search engines. Rather than searching the Internet, however, a language search engine searches a large repository of Translation Memories to find previously translated sentence fragments, phrases, whole sentences, even complete paragraphs that match source document segments.\n\nLanguage search engines are designed to leverage modern search technology to conduct searches based on the source words in context to ensure that the search results match the meaning of the source segments. Like traditional TM tools, the value of a language search engine rests heavily on the Translation Memory repository it searches against.\n\nTerminology management software provides the translator a means of automatically searching a given terminology database for terms appearing in a document, either by automatically displaying terms in the translation memory software interface window or through the use of hot keys to view the entry in the terminology database. Some programs have other hotkey combinations allowing the translator to add new terminology pairs to the terminology database on the fly during translation. Some of the more advanced systems enable translators to check, either interactively or in batch mode, if the correct source/target term combination has been used within and across the translation memory segments in a given project. Independent terminology management systems also exist that can provide workflow functionality, visual taxonomy, work as a type of term checker (similar to spell checker, terms that have not been used correctly are flagged) and can support other types of multilingual term facet classifications such as pictures, videos, or sound.\n\nAlignment programs take completed translations, divide both source and target texts into segments, and attempt to determine which segments belong together in order to build a translation memory or other reference resource with the content. Many alignment programs allow translators to manually realign mismatched segments. The resulting bitext (also known as parallel text) alignment can then be imported into a translation memory program for future translations or used as a reference document.\n\nInteractive machine translation is a paradigm in which the automatic system attempts to predict the translation the human translator is going to produce by suggesting translation hypotheses. These hypotheses may either be the complete sentence, or the part of the sentence that is yet to be translated.\n\nAugmented translation is a form of human translation carried out within an integrated technology environment that provides translators access to subsegment adaptive machine translation (MT) and translation memory (TM), terminology lookup (CAT), and automatic content enrichment (ACE) to aid their work, and that automates project management, file handling, and other ancillary tasks.\n\nBased on the concept of augmented reality, augmented translation seeks to make translators more productive by providing them with relevant information on an as-needed basis. This information adapts to the habits and style of individual translators in order to accelerate their work and increase productivity. It differs from classical postediting of MT, which has linguists revise entire texts translated by machines, in that it provides machine translation and information as suggestions that can be adopted in their entirety, edited, or ignored, as appropriate.\n\nAugmented translation extends principles first developed in the 1980s that made their way into CAT tools. However, it integrates several functions that have previously been discrete into one environment. For example, translators historically have had to leave their translation environments to do terminology research, but in an augmented environment, an ACE component would automatically provide links to information about terms and concepts found in the text directly within the environment.\n\nAs of May 2017, no full implementations of an augmented translation environment exist, although individual developers have created partial systems.\n\n\n"}
{"id": "30011719", "url": "https://en.wikipedia.org/wiki?curid=30011719", "title": "Consorzio ICoN", "text": "Consorzio ICoN\n\nThe Consorzio ICoN is an interuniversity consortium for Italian Studies established in 1999. It consists of 21 Italian universities and focuses on philology and cultural studies. The consortium is based and administrated at the University of Pisa and is supported by the Italian Ministry of University and Research (\"Ministero dell'Università e della Ricerca\"). It aims at diffusing Italian language, culture and literature.\n\nThe universities forming the consortium are\n\n\nThe University of Pisa, in collaboration with the consortium, offers a bachelor's degree program in Italian philology and cultural studies (Corso di laurea triennale in \"Lingua e cultura italiana per stranieri\") for foreigners and expatriates, based on distance education and e-learning. The program refers to the bachelor's degree class L-10 (literary studies) and offers four majors in\n\n\nExams are carried out at institutions in the country of residence (partner universities, Italian embassies). The title will be issued by the administrative university, actually Pisa, mentioning all participating universities. As Italy is part of European Higher Education Area, the program has a value of 180 ECTS points and permits to participate in a Master's degree program.\n\nThe ICoN consortium organizes three executive master's degrees (60 ECTS) in the field of Italian studies. They consist of phases of attendance, distance education and e-learning. The offer includes\n\n\nThe consortium ICoN also offers language courses and programs in written Italian at distinct levels. For native English speakers special packages have been designed in collaboration with the National Italian American Foundation and the University of California's department of Italian. The courses have been awarded with the European Commission's \"European Language Label\".\n\n\n"}
{"id": "1060739", "url": "https://en.wikipedia.org/wiki?curid=1060739", "title": "Counterword", "text": "Counterword\n\nA counterword (also spelled counter word and counter-word) is a word such as \"so\" that is frequently used to answer (\"counter\") in a reflex-like manner and that has due to this frequent use quickly taken on a new, much less specific or much looser meaning or is even almost meaningless or performs a completely new function. The word \"so\", for example, is frequently used to begin an answer in the sense of \"Well...\" or to function as an indirect way of saying \"Before answering that, I'd like to...\" or even instead of saying \"On the contrary...\" or \"No, I...\".\n\nIn a more general sense, the term is used for such words also when they are not used as a reflex-like answer and even for any widely used words that (due to a similar change) now have a broad and vague range of meanings in many very different situations (e.g. case, awfully, fix, job, payoff).\n\nSince such change due to very frequent use occurs much more rapidly than the change in meaning all words go through, and since such words are even sometimes still simultaneously used in their original sense, the new usage is often considered incorrect by some speakers. Other examples include \"nice\", \"terrific\", \"terrible\", \"awful\", \"tremendous\", \"swell\", \"hopefully\" and \"very fine\" (degrading the meaning of \"fine\" to \"OK\").\n\nThe \"Oxford English Dictionary\" does not support this and defines counter-word as \"countersign\", noting that its usage is military and obsolete with a single quotation from 1678.\n"}
{"id": "33934108", "url": "https://en.wikipedia.org/wiki?curid=33934108", "title": "Digital infinity", "text": "Digital infinity\n\nDigital infinity is a technical term in theoretical linguistics. Alternative formulations are \"discrete infinity\" and \"the infinite use of finite means\". The idea is that all human languages follow a simple logical principle, according to which a limited set of digits—irreducible atomic sound elements—are combined to produce an infinite range of potentially meaningful expressions. \n\nNoam Chomsky cites Galileo as perhaps the first to recognise the significance of digital infinity. This principle, notes Chomsky, is \"the core property of human language, and one of its most distinctive properties: the use of finite means to express an unlimited array of thoughts\". In his \"Dialogo\", Galileo describes with wonder the discovery of a means to communicate one's \"most secret thoughts to any other person ... with no greater difficulty than the various collocations of twenty-four little characters upon a paper.\" This is the greatest of all human inventions, Galileo continues, comparable to the creations of a Michelangelo...\n\n'Digital infinity' corresponds to Noam Chomsky's 'universal grammar' mechanism, conceived as a computational module inserted somehow into \"Homo sapiens\" otherwise 'messy' (non-digital) brain. This conception of human cognition—central to the so-called 'cognitive revolution' of the 1950s and 1960s—is generally attributed to Alan Turing, who was the first scientist to argue that a man-made machine might truly be said to 'think'. The idea of a thinking machine had previously been considered absurd, having been famously dismissed by René Descartes as \"theoretically\" impossible. Neither animals nor machines can think, insisted Descartes, since they lack a God-given soul. Turing was well aware of this traditional theological objection, and explicitly countered it. \n\nToday's digital computers are instantiations of Turing's theoretical breakthrough in conceiving the possibility of a man-made \"universal thinking machine\"—known nowadays as a 'Turing machine'. No physical mechanism can be intrinsically 'digital', Turing explained, since—examined closely enough—its possible states will vary without limit. But if most of these states can be profitably ignored, leaving only a limited set of relevant distinctions, then \"functionally\" the machine may be considered 'digital': \nAn implication is that 'digits' don't exist: they and their combinations are no more than convenient fictions, operating on a level quite independent of the material, physical world. In the case of a \"binary\" digital machine, the choice at each point is restricted to 'off' versus 'on'. Crucially, the intrinsic properties of the medium used to encode signals then have no effect on the message conveyed. 'Off' (or alternatively 'on') remains unchanged regardless of whether the signal consists of smoke, electricity, sound, light or anything else. In the case of analog (more-versus-less) gradations, this is not so because the range of possible settings is unlimited. Moreover, in the analog case it \"does\" matter which particular medium is being employed: equating a certain intensity of smoke with a corresponding intensity of light, sound or electricity is just not possible. In other words, only in the case of \"digital\" computation and communication can information be truly independent of the physical, chemical or other properties of the materials used to encode and transmit messages.\n\nDigital computation and communication operates, then, independently of the physical properties of the computing machine. As scientists and philosophers during the 1950s digested the implications, they exploited the insight to explain why 'mind' apparently operates on so different a level from 'matter'. Descartes celebrated distinction between immortal 'soul' and mortal 'body' was conceptualised, following Turing, as no more than the distinction between (digitally encoded) \"information\" on the one hand, and, on the other, the particular physical \"medium\"—light, sound, electricity or whatever—chosen to transmit the corresponding signals. Note that the Cartesian assumption of mind's independence of matter implied—in the human case at least—the existence of some kind of digital computer operating inside the human brain. \n\nTuring did not claim that the human mind really is a digital computer. More modestly, he proposed that digital computers might one day qualify in human eyes as machines endowed with \"mind\". However, it was not long before philosophers (most notably Hilary Putnam) took what seemed to be the next logical step—arguing that the human mind \"itself\" is a digital computer, or at least that certain mental \"modules\" are best understood that way.\n\nNoam Chomsky rose to prominence as one of the most audacious champions of this 'cognitive revolution'. Language, he proposed, is a computational 'module' or 'device' unique to the human brain. Previously, linguists had thought of language as learned cultural behaviour: chaotically variable, inseparable from social life and therefore beyond the remit of natural science. The Swiss linguist Ferdinand de Saussure, for example, had defined linguistics as a branch of 'semiotics', this in turn being inseparable from anthropology, sociology and the study of man-made conventions and institutions. By picturing language instead as the \"natural\" mechanism of 'digital infinity', Chomsky promised to bring scientific rigour to linguistics as a branch of strictly \"natural\" science.\n\nIn the 1950s, phonology was generally considered the most rigorously scientific branch of linguistics. For phonologists, \"digital infinity\" was made possible by the human vocal apparatus conceptualised as a kind of machine consisting of a small number of binary switches. For example, \"voicing\" could be switched 'on' or 'off', as could palatisation, nasalisation and so forth. Take the consonant [b], for example, and switch voicing to the 'off' position—and you get [p]. Every possible phoneme in any of the world's languages might in this way be generated by specifying a particular on/off configuration of the switches ('articulators') constituting the human vocal apparatus. This approach became celebrated as 'distinctive features' theory, in large part credited to the Russian linguist and polymath Roman Jakobson. The basic idea was that every phoneme in every natural language could in principle be reduced to its irreducible atomic components—a set of 'on' or 'off' choices ('distinctive features') allowed by the design of a digital apparatus consisting of the human tongue, soft palate, lips, larynx and so forth. \n\nChomsky's original work was in morphophonemics. During the 1950s, he became inspired by the prospect of extending Roman Jakobson's 'distinctive features' approach—now hugely successful—far beyond its original field of application. Jakobson had already persuaded a young social anthropologist—Claude Lévi-Strauss—to apply distinctive features theory to the study of kinship systems, in this way inaugurating 'structural anthropology'. Chomsky—who got his job at the Massachusetts Institute of Technology thanks to the intervention of Jakobson and his student, Morris Halle—hoped to explore the extent to which similar principles might be applied to the various sub-disciplines of linguistics, including syntax and semantics. If the phonological component of language was demonstrably rooted in a digital biological 'organ' or 'device', why not the syntactic and semantic components as well? Might not language as a whole prove to be a digital organ or device?\n\nThis led some of Chomsky's early students to the idea of 'generative semantics'—the proposal that the speaker generates word and sentence meanings by combining irreducible constituent elements of meaning, each of which can be switched 'on' or 'off'. To produce 'bachelor', using this logic, the relevant component of the brain must switch 'animate', 'human' and 'male' to the 'on' (+) position while keeping 'married' switched 'off' (-). The underlying assumption here is that the requisite conceptual primitives—irreducible notions such as 'animate', 'male', 'human', 'married' and so forth—are genetically determined internal components of the human language organ. While this idea would rapidly encounter intellectual difficulties—sparking controversies culminating in the so-called 'linguistics wars'—it attracted young and ambitious scholars impressed by the recent emergence of computer science and its promise of scientific parsimony and unification. If the theory worked, the simple principle of digital infinity would apply to language as a whole. Linguistics in its entirety might then lay claim to the coveted status of \"natural\" science. No part of the discipline—not even semantics—need be contaminated any longer by association with such 'un-scientific' disciplines as cultural anthropology or social science.\n\n\n"}
{"id": "2964506", "url": "https://en.wikipedia.org/wiki?curid=2964506", "title": "Double articulation", "text": "Double articulation\n\nDouble articulation, or duality of patterning is a concept used in linguistics and semiotics. It refers to the two-level structure inherent to a sign system, insofar as it is composed by two kinds of elements: 1) significant or meaningful, and 2) distinctive or meaningless.\n\n\"Double articulation\" refers to the two fold structure of the stream of speech, which can be primarily divided into \"meaningful\" signs (like words or morphemes), and then secondarily into \"distinctive\" elements (like letters or phonemes). For example, the meaningful English word \"cat\" is composed of the sounds [k], [æ], and [t], which are meaningless as separate individual sounds (and which can also be combined to form the separate words \"tack\" and \"act\", with distinct meanings). These sounds, called phonemes, represent the secondary and lowest level of articulation in the hierarchy of the organization of speech. Higher, primary, levels of organization (including morphology, syntax, and semantics) govern the combination of these individually meaningless phonemes into meaningful elements.\n\nThe French concept of \"double articulation\" was first introduced by the André Martinet, in 1949. The English calque \"double articulation\" has been criticised as inaccurate and is often replaced by \"duality of patterning\".\n\nAccording to Charles F. Hockett and other linguists, this duality is an important property of human languages, since it allows for the expression of a potentially infinite number of meaningful language sequences. Strictly speaking, however, such expressiveness follows from generativity or productivity (a finite number of components combining via rules to produce a potentially infinite arrangement of novel utterances), not of duality per se (one could have a system with 2 levels of the kind referred to as duality, and yet have only finite productivity. For further discussion, see figurae, as well as Hockett's_design_features, which treats productivity and duality as distinct essential properties of language.\n\nSign languages may have less double articulation because more gestures are possible than sound and able to convey more meaning without double articulation.\n\n"}
{"id": "16794300", "url": "https://en.wikipedia.org/wiki?curid=16794300", "title": "Economics of language", "text": "Economics of language\n\nThe economics of language is an emerging field of study concerning a range of topics such as the effect of language skills on income and trade, and the costs and benefits of language planning options, preservation of minority languages, etc. It is relevant to analysis of language policy.\n\nIn his book 'Language and economy', the German sociolinguist Florian Coulmas discusses \"the many ways in which language and economy interact, how economic developments influence the emergence, expansion, or decline of languages; how linguistic conditions facilitate or obstruct the economic process; how multilingualism and social affluence are interrelated; how and why language and money fulfill similar functions in modern societies; why the availability of a standard language is an economic advantage; how the unequal distribution of languages in multilingual societies makes for economic inequality; how the economic value of languages can be assessed; why languages have an internal economy and how they adapt to the demands of the external economy. Florian Coulmas shows that language is the medium of business, an asset in itself and sometimes a barrier to trade\".\n\nStates shoulder language costs, because it maintains themselves by means of it, as does business which needs communication competence. Florian Coulmas discusses the language-related expenditures of government and business in Language and economy. In the same book he also discusses the role of language as a commodity, because languages can behave like economic systems. That is why socio-economic ecologies are (dis)favorable to particular languages. The spread of languages depends in an essential way on economic conditions. Language can be an expression of symbolic power. However, changes in the linguistic map of the world show that these are also powerful linked to economic developments in the world. Assigning an economic value to a certain language in the linguistic market place means vesting it with some of the privileges and power related to that language. Most language communities in the world practice this policy without any concern about \"reciprocity\" in language learning investments, forgetting the \"pursuit of linguistic justice as parity of esteem and while linguistic regimes are sometimes very unjust\".\n\nLanguages are capital investments in a literal sense : language technology is the most important one. It requires substantial investments which, in the absence of profitability, only affluent countries and businesses can afford. In this respect, today \"English is seen as a consequence and an instrument of American imperial power, an appreciable asset for American anglophones in the twenty-first-century global contest for competitive advantage, prosperity, and power\". Though the best business language remains the language of the customers, meaning multilingual business practices, an \"ideal' global economy presupposes a single language for the whole world. But an \"ideal\" global language presupposes a common acceptable and fair language burden for all business partners. See in this respect language tax to counteract linguistic inequality, as also language for purposes of trade incurs costs to most countries and private entreprises, whereas governments of countries whose language occupies a leading position on the international language market refuse to subsidize the spread of other languages for which they believe they have no need. In his report \"L'enseignement des langues étrangères comme politique publique\", François Grin argues that 'though some languages would be more beneficial in terms of cost-benefit analysis' such as e.g. Esperanto (Esperanto business groups such as IKEF have been active for many years), the problem is that a shifting pattern in the valuation of languages is not always brought about by rationally culculable factors only. In addition to its economic potential, language is also a carrier of political, cultural and sociopsychological properties. In spite of the non-economic values attached to language, what prevails in matters of language is often that which is profitable and this can lead to the superiority of a dominant language as a means of production, with a high linguistic capital value. In this respect it is evident to see that the will (or necessity) to learn English in the last decades has grown so much and its range of action has been so wide that the economic necessity and other incentives of foreign-language study are generally perceived as unimportant. For similar reasons, former British prime minister Margaret Thatcher tried to torpedo the LINGUA program of the European Community, as from her point of view, Britain was asked to pay for a program which benefited her country least. Because of the enormous imbalance on current accounts of the major European languages in favour of English, the LINGUA programme called for an expansion and diversification of foreign-language education in the Member States. For the individual speaker the unequal linguistic balances imply that the first language is an economically exploitable qualification for some who can simply marketing their mother tongue skills, whereas others can not.\n\n\n\n"}
{"id": "38397907", "url": "https://en.wikipedia.org/wiki?curid=38397907", "title": "Ewondo Populaire", "text": "Ewondo Populaire\n\nEwondo Populair is a Beti-based pidgin of Cameroon, spoken in the area of the capital Yaoundé.\n"}
{"id": "44476602", "url": "https://en.wikipedia.org/wiki?curid=44476602", "title": "Extensionalism", "text": "Extensionalism\n\nExtensionalism, in the philosophy of language, in logic and semantics, is the view that all languages or at least all scientific languages should be extensional. Rudolf Carnap (in his earlier work) and Willard Van Orman Quine were prominent proponents of this view.\n\n"}
{"id": "2134287", "url": "https://en.wikipedia.org/wiki?curid=2134287", "title": "Feature (linguistics)", "text": "Feature (linguistics)\n\nIn linguistics, a feature is the assignment of binary or unary conditions which act as constraints.\n\nIn phonology, segments are categorized into natural classes on the basis of their distinctive features. Each feature is a quality or characteristic of the natural class, such as voice or manner. A unique combination of features defines a phoneme.\n\nExamples of phonemic or distinctive features are: [+/- voice ], [+/- ATR ] (binary features) and [ CORONAL ] (a unary feature; also a place feature).\n\nSurface representations can be expressed as the result of rules acting on the features of the underlying representation. These rules are formulated in terms of transformations on features.\n\nIn morphology and syntax, words are often organized into lexical categories or word classes, such as \"noun\", \"verb\", \"adjective\", and so on. These word classes have grammatical features (also called \"categories\" or \"inflectional categories\"), which can have one of a set of potential values (also called the \"property\", \"meaning\", or \"feature\" of the category).\n\nFor example, consider the pronoun in English. Pronouns are a \"lexical category\". Pronouns have the person \"feature\", which can have a \"value\" of \"first\", \"second\", or \"third\". English pronouns also have the number \"feature\", which can have a value of either \"singular\" or \"plural\". As a result, we can describe the English pronoun \"they\" as a pronoun with [person:3] and [number:plural].\n\nIn semantics, words are categorized into semantic classes. Intersecting semantic classes share the same semantic features.\n\n"}
{"id": "30210144", "url": "https://en.wikipedia.org/wiki?curid=30210144", "title": "Frame-based terminology", "text": "Frame-based terminology\n\nFrame-based terminology is a cognitive approach to terminology developed by Pamela Faber and colleagues at the University of Granada. One of its basic premises is that the conceptualization of any specialized domain is goal-oriented, and depends to a certain degree on the task to be accomplished. Since a major problem in modeling any domain is the fact that languages can reflect different conceptualizations and construals, texts as well as specialized knowledge resources are used to extract a set of domain concepts. Language structure is also analyzed to obtain an inventory of conceptual relations to structure these concepts.\n\nAs its name implies, frame-based terminology uses certain aspects of frame semantics to structure specialized domains and create non-language-specific representations. Such configurations are the conceptual meaning underlying specialized texts in different languages, and thus facilitate specialized knowledge acquisition.\n\nFrame-based terminology focuses on:\n\nIn frame-based terminology, conceptual networks are based on an underlying domain event, which generates templates for the actions and processes that take place in the specialized field as well as the entities that participate in them.\n\nAs a result, knowledge extraction is largely text-based. The terminological entries are composed of information from specialized texts as well as specialized language resources. Knowledge is configured and represented in a dynamic conceptual network that is capable of adapting to new contexts. At the most general level, generic roles of agent, patient, result, and instrument are activated by basic predicate meanings such as make, do, affect, use, become, etc. which structure the basic meanings in specialized texts. From a linguistic perspective, Aktionsart distinctions in texts are based on Van Valin's classification of predicate types. At the more specific levels of the network, the qualia structure of the generative lexicon is used as a basis for the systematic classification and relation of nominal entities.\n\nThe methodology of frame-based terminology derives the conceptual system of the domain by means of an integrated top-down and bottom-up approach. The bottom-up approach consists of extracting information from a corpus of texts in various languages, specifically related to the domain. The top-down approach includes the information provided by specialized dictionaries and other reference material, complemented by the help of experts in the field. \n\nIn a parallel way, the underlying conceptual framework of a knowledge-domain event is specified. The most generic or base-level categories of a domain are configured in a prototypical domain event or action-environment interface. This provides a template applicable to all levels of information structuring. In this way a structure is obtained which facilitates and enhances knowledge acquisition since the information in term entries is internally as well as externally coherent.\n\n"}
{"id": "35313027", "url": "https://en.wikipedia.org/wiki?curid=35313027", "title": "Geko Karen", "text": "Geko Karen\n\nGeko is a Karen language of Burma. Yinbaw is reportedly a variety. Speakers of Geko and Yinbaw are ethnically Kayan, as are speakers of Lahta and Padaung.\n\n\nYinbaw (population 7,300 as of 1983) is spoken in eastern Shan State and Kayah State.\n\n\n"}
{"id": "2836229", "url": "https://en.wikipedia.org/wiki?curid=2836229", "title": "Gibberish (language game)", "text": "Gibberish (language game)\n\nGibberish (sometimes Jibberish) is a language game that is played in the United States, Canada and Ireland. Similar games are played in many other countries. The name Gibberish refers to the nonsensical sound of words spoken according to the rules of this game.\n\nThere are several variations of Gibberish in the English-speaking world. They use \"-itherg-\", \"-uthug-\", \"-elag-\", \"-itug-\", \"-uthaga-\", \"-uvug-\", \"-idig-\", \"-atheg-\" (\"th\" in \"then\" and the two vowels are pronounced with a schwa), and \"-adeg-\". The dialects are given different names. Another form of gibberish known as allibi is spoken using the insertion \"-allib-\".\n\nThese four dialects of Gibberish are spoken by adding the infix to each syllable after the onset. Example:\nWhen a syllable starts with more than one consonant, the infix is added after the onset consonants. Example:\nWhen the syllable begins with a vowel, that vowel is used in place of the first \"i\" in the \"-ithieg-\" infix. Example:\nWords of more than one syllable repeat the rules for each syllable.\n\nThis dialect works in much the same way as the previous dialects, with an additional rule. When a single syllable word begins with a vowel, the infix acts as a prefix, with the initial \"a\" becoming like that vowel.\nThe sentence \"I hit the alarm clock when I woke up\" could be \"Ittiguy Hittigit thittagee addagalitigarm clidigock wittigen Ittiguy wittigoke uttigup\".\n\nAnother paradigm involves infixing \"(V)rV+g\" following the onset of a monosyllabic word, or less usually after each onset or nucleus of polysyllabic words. In words consisting of a single diphthong, the Gibberish morpheme breaks up the syllable into a sequence of vowels plus a glide. The vowels of the Gibberish morpheme typically harmonize for quality with the vowel of the syllable nucleus, but can be reduced if unstressed according to English stress rules. The syllabifies into a new onset. Examples:\n\n\nL can also be commonly used instead of r. Examples:\n\n\nThe term \"gibberish\" is used more generally to refer to all language games created by inserting a certain infix before the vowel in each syllable. For example, if the code infix were \"ob\", then \"Hello, Thomas\" would be translated as \"Hobellobo, Thobomobas\". While a relatively simple code, this can be difficult to understand when spoken swiftly and sounds merely like meaningless babble, which is how it received its name. The terms \"Double Talk\" and \"Double Dutch\" are alternate names for such codes. While any syllables could be used as code syllables, some syllables are more commonly used. These include:\n\n\nAnother variation consists in the code syllables not having a specific vowel, but repeating the vowel of the syllable in which it is being inserted. This variation is common in Switzerland, where the inserted syllable thus could be \"@n@f\", where \"@\" denotes the original vowel; e.g. \"Hallo, Chrige\" would be translated into \"Hanafallonofo, Chrinifigenefe\". Similarly, \"Lalafa\" replaces each occurrence of a vowel with \"@ləf@.\" In Gibberish as spoken in the United Kingdom, the infix code syllable is often \"@rag\".\n\nIn some variants only the first syllable of each word is modified. On the other hand, combining (or double-encoding) forms of Gibberish, or by further encoding with other languages games such as Pig Latin and Tutnese can result in increasingly hard to decipher (and pronounce) words. For instance, combining Pig Latin, Hard Gibberish and Openglopish might result in a phrase ('that sounds like this').\n\nLanguage games in the Gibberish family are not unique to English-speaking countries. Gibberish games in other languages include:\n\n"}
{"id": "8753001", "url": "https://en.wikipedia.org/wiki?curid=8753001", "title": "Glossophilia", "text": "Glossophilia\n\nGlossophilia is a love of language, be it foreign or native. The term refers to people with a love for language and the structure of language. Glossophiles also dedicate themselves to the learning of foreign languages and intensely study as many languages as possible. It is not uncommon for glossophiles to be proficient in many languages.\n\n\n"}
{"id": "29042638", "url": "https://en.wikipedia.org/wiki?curid=29042638", "title": "Inscrutability of reference", "text": "Inscrutability of reference\n\nThe inscrutability or indeterminacy of reference (also referential inscrutability) is a thesis propounded by 20th century analytic philosopher Willard Van Orman Quine in his book \"Word and Object\". The main claim of this theory is that any given sentence can be changed into a variety of other sentences where the parts of the sentence will change in what they reference, but they will nonetheless maintain the meaning of the sentence as a whole. The , because it is subject to the and of the speaker.\n\nAlong with the holophrastic indeterminacy, the inscrutability of reference is the second kind of indeterminacy that makes up Quine's thesis about the indeterminacy of (radical) translation. While the inscrutability of reference concerns itself with single words, Quine does not want it to be used for propositions, as he attacks those in another way. He challenges the translation or referential scrutability of whole sentences, proposing his idea of the indeterminacy of translation. In order to accomplish this, Quine makes the statement that there is a so-called holophrastic indeterminacy, which tells that there are always multiple translations of one sentence, . This theory, linked with the inscrutability of reference make up the main characteristics of the indeterminacy of translation.\nThe inscrutability of reference can also be used in a more extended way, in order to explain Quine's theory of ontological relativity. We are told that, if we try to determine what the referential object of a certain word is, our answer will always be relative to our own \"\". Now, as Quine sees it, this idea is not only limited to language, but applies also for scientific questions and philosophical ones. For example, if we are proposed a philosophical theory, we can never definitely . The most we can do, is to adapt this theory to our current \"background philosophy\", that is . Because of this theory, Quine was often regarded as a relativist, or even a scientific skepticist. He, however, insisted that he belongs in neither of these categories, and some authors see in the inscrutability of reference an underdetermination of relativism.\n\nIn his indeterminacy of translation theory Quine claims that, if one is to translate a language, there are always several alternative translations, of which none is more correct than the other. A radical translation is therefore impossible. As a special part of this theory the inscrutability of reference indicates that, in trying to find out to which object a certain word (also sentence, sign etc.) of a language refers, there is never only one single possibility. That is even the case, if the possibilities that come into consideration lie very close together. Quine's example of the word \"gavagai\" is used to illustrate this. Note that it is also applied at the indeterminacy of translation, but has traditionally been introduced to point up referential inscrutability. The \"gavagai\" thought experiment tells about a linguist, who tries to find out, what the expression \"gavagai\" means, when uttered by a speaker of a yet unknown, native language upon seeing a rabbit. At first glance, it seems that \"gavagai\" simply translates with \"rabbit\". Now, Quine points out that the background language and its referring devices might fool the linguist here, because he is misled in a sense that he always makes direct comparisons between the foreign language and his own. However, when shouting \"gavagai\", and pointing at a rabbit, the natives could as well refer to something like \"undetached rabbit-parts\", or \"rabbit-tropes\" and it would not make any observable difference. The behavioural data the linguist could collect from the native speaker would be the same in every case, or to reword it, several translation hypotheses could be built on the same sensoric stimuli. Hence, the reference between the term \"gavagai\" and its referring object is language- or framework-dependent, and therefore inscrutable. Quine regards this discovery as trivial, because it is already a widely accepted fact that all the different things one word might refer to can be switched out, because of their proxy functions.\nQuine does not want to show that those native speakers might speak in interestingly different ways and we cannot know about it, but rather that there is nothing to be known. Not only is it impossible to discern, by any method, the correct translation and referential relation of \"gavagai\", but, in fact, there is not even a correct answer to this question. To make sense of the word \"gavagai\" either way, the linguist simply has to assume that the native speaker does not refer to complicated terms like \"rabbits-tropes\". The finding, then, that \"gavagai\" means \"rabbit\" is not really a translation, but merely a common sense interpretation.\nIt is important to note that indeterminacy and inscrutability not only occur in the course of translating something from a native, unknown language into a familiar one, but among every language. This holds also for languages which are quite similar, like German and Dutch, and even for speakers of the same language. One cannot with certainty say, what exactly his/her conversational partner refers to, when that person is talking about a rabbit. We commonly use the homophonic rule in those cases, i.e., if one utters \"rabbit\", we assume s/he uses it in the same way we do. But, as has been shown, there are multiple possibilities which can be indistinguishable from one another. This also applies in our own case. We ourselves do not know what it is we are referring to in using the word \"rabbit\", that is because there is, in Quine's word, \"no fact of the matter\" at all. One must not, however, use different possible referential objects in the same translation, because they are incommensurable and the resulting translation hypothesis would contain logical fallacies.\n\nHilary Putnam uses Quine's thesis about the inscrutability of reference to challenge the traditional Realist's view that there is a mind-independent world to which our propositional attitudes refer (e.g. when we talk about or think of something, these things exist not in our minds, but in said mind-independent world). This traditional view implies a correspondence theory of truth and might simply be called Realism about Being. While Michael Dummett already tried to show that the correspondence theory fails to obtain in some particular cases, Hilary Putnam is far more radical, for he claims that this theory fails in every case it is tried to be applied. On Putnam's account, the idea that we refer with our sentences and statements to a mind-independent, nonlinguistic world is an illusion. Further he claims that the problem to deal with is a language philosophical one and uses Quine's inscrutability of reference theory to clarify his point of view. He suggests, that, because the referential objects of a language are always inscrutable, the Realist's idea of a mind-independent world is fallacious, because it presupposes distinct referential relations from language to objects in the mind-independent world.\n\nThe inscrutability of reference is also used in the sorites paradox. The classic example for the sorites paradox mentions a heap of wheat grains of which grains are taken away one by one, until at one time there's only a single grain left. This raises the question of where the line is to be drawn. How long does the heap remain a heap, are two grains still a heap? When one is talking about a heap s/he obviously no proper definition of it ready to hand. The referential object of \"heap\" is inscrutable, in the sense that there is no such thing and it is not even necessary for the use of the term \"heap\".\n\n\n"}
{"id": "333528", "url": "https://en.wikipedia.org/wiki?curid=333528", "title": "Japanese language and computers", "text": "Japanese language and computers\n\nIn relation to the Japanese language and computers many adaptation issues arise, some unique to Japanese and others common to languages which have a very large number of characters. The number of characters needed in order to write English is very small, and thus it is possible to use only one byte to encode one English character. However, the number of characters in Japanese is much more than 256, and hence Japanese cannot be encoded using only one byte, and Japanese is thus encoded using two or more bytes, in a so-called \"double byte\" or \"multi-byte\" encoding. Some problems relate to transliteration and romanization, some to character encoding, and some to the input of Japanese text.\n\nThere are several standard methods to encode Japanese characters for use on a computer, including JIS, Shift-JIS, EUC, and Unicode. While mapping the set of kana is a simple matter, kanji has proven more difficult. Despite efforts, none of the encoding schemes have become the de facto standard, and multiple encoding standards are still in use today.\n\nFor example, most Japanese emails are in ISO-2022-JP (\"JIS encoding\") and web pages in Shift-JIS and yet mobile phones in Japan usually use some form of Extended Unix Code. If a program fails to determine the encoding scheme employed, it can cause and thus unreadable text on computers.\n\nThe first encoding to become widely used was JIS X 0201, which is a single-byte encoding that only covers standard 7-bit ASCII characters with half-width katakana extensions. This was widely used in systems that were neither powerful enough nor had the storage to handle kanji (including old embedded equipment such as cash registers). This means that only katakana, not kanji, was supported using this technique. Some embedded displays still have this limitation.\n\nThe development of kanji encodings was the beginning of the split. Shift JIS supports kanji and was developed to be completely backward compatible with JIS X 0201, and thus is in much embedded electronic equipment. \n\nHowever, Shift JIS has the unfortunate property that it often breaks any parser (software that reads the coded text) that is not specifically designed to handle it. For example, a text search method can get false hits if it is not designed for Shift JIS. EUC, on the other hand, is handled much better by parsers that have been written for 7-bit ASCII (and thus EUC encodings are used on UNIX, where much of the file-handling code was historically only written for English encodings). But EUC is not backwards compatible with JIS X 0201, the first main Japanese encoding. Further complications arise because the original Internet e-mail standards only support 7-bit transfer protocols. Thus (\"ISO-2022-JP\", often simply called JIS encoding) was developed for sending and receiving e-mails.\n\nIn character set standards such as JIS, not all required characters are included, so gaiji ( \"external characters\") are sometimes used to supplement the character set. Gaiji may come in the form of external font packs, where normal characters have been replaced with new characters, or the new characters have been added to unused character positions. However, gaiji are not practical in Internet environments since the font set must be transferred with text to use the gaiji. As a result, such characters are written with similar or simpler characters in place, or the text may need to be encoded using a larger character set (such as Unicode) that supports the required character.\n\nUnicode was intended to solve all encoding problems over all languages. The UTF-8 encoding used to encode Unicode in web pages does not have the disadvantages that Shift-JIS has. Unicode is supported by international software, and it eliminates the need for gaiji. There are still controversies, however. For Japanese, the kanji characters have been unified with Chinese; that is, a character considered to be the same in both Japanese and Chinese is given a single number, even if the appearance is actually somewhat different, with the precise appearance left to the use of a locale-appropriate font. This process, called Han unification, has caused controversy. The previous encodings in Japan, Taiwan Area, Mainland China and Korea have only handled one language and Unicode should handle all. The handling of Kanji/Chinese have however been designed by a committee composed of representatives from all four countries/areas. Unicode is slowly growing because it is better supported by software from outside Japan, but still (as of 2011) most web pages in Japanese use Shift-JIS. The Japanese Wikipedia uses Unicode.\n\nWritten Japanese uses several different scripts: kanji (Chinese characters), 2 sets of \"kana\" (phonetic syllabaries) and roman letters. While kana and roman letters can be typed directly into a computer, entering kanji is a more complicated process as there are far more kanji than there are keys on most keyboards. To input kanji on modern computers, the reading of kanji is usually entered first, then an input method editor (IME), also sometimes known as a front-end processor, shows a list of candidate kanji that are a phonetic match, and allows the user to choose the correct kanji. More-advanced IMEs work not by word but by phrase, thus increasing the likelihood of getting the desired characters as the first option presented. Kanji readings inputs can be either via romanization (\"rōmaji nyūryoku,\" ) or direct kana input (\"kana nyūryoku,\" ). Romaji input is more common on PCs and other full-size keyboards (although direct input is also widely supported), whereas direct kana input is typically used on mobile phones and similar devices – each of the 10 digits (1–9,0) corresponds to one of the 10 columns in the gojūon table of kana, and multiple presses select the row.\n\nThere are two main systems for the romanization of Japanese, known as \"Kunrei-shiki\" and \"Hepburn\"; in practice, \"keyboard romaji\" (also known as \"wāpuro rōmaji\" or \"word processor romaji\") generally allows a loose combination of both. IME implementations may even handle keys for letters unused in any romanization scheme, such as \"L\", converting them to the most appropriate equivalent. With kana input, each key on the keyboard directly corresponds to one kana. The JIS keyboard system is the national standard, but there are alternatives, like the thumb-shift keyboard, commonly used among professional typists.\n\nJapanese can be written in two directions. \"Yokogaki\" style writes left-to-right, top-to-bottom, as with English. \"Tategaki\" style writes first top-to-bottom, and then moves right-to-left.\n\nAt present, handling of downward text is incomplete. For example, HTML has no support for \"tategaki\" and Japanese users must use HTML tables to simulate it. However, CSS level 3 includes a property \"writing-mode\" which can render \"tategaki\" when given the value \"vertical-rl\" (i.e. top to bottom, right to left). Word processors and DTP software have more complete support for it.\n\n\n"}
{"id": "3264882", "url": "https://en.wikipedia.org/wiki?curid=3264882", "title": "Javanais", "text": "Javanais\n\nJavanais () is a type of French slang where the extra syllable is infixed inside a word after every consonant that is followed by a vowel, in order to render it incomprehensible. Some common examples are \"gros\" (, \"fat\") which becomes \"gravos\" (); \"bonjour\" (, \"hello\"'), which becomes \"bavonjavour\" (); and \"pénible\" (, \"annoying\"), becomes \"pavénaviblave\" (). \"Paris\" () becomes \"Pavaravis\" ().\n\nJavanais is determined by the production rule: \"CV → CavV\". There are also many variations that can be made upon the same pattern such as: \"CabV, CalV, CanV\", etc.\n\nIn French the word \"Javanais\" is also used to refer to the Javanese language.\n\nAround 1957, Boris Vian wrote a song \"La Java Javanaise\". The lyrics are a didactical method to learn the javanais. Each verse is firstly articulated in regular French, then translated in slang. As the title suggests, the song is a Java, a Parisian dance craze. In 1962, Serge Gainsbourg wrote and sang a song called \"La Javanaise\", a pun playing on Javanese dancing and the javanais style of speaking. The song heavily employs unaltered French words that naturally have an sequence; thus the lyrics resemble the word game of javanais.\n\n"}
{"id": "8042329", "url": "https://en.wikipedia.org/wiki?curid=8042329", "title": "Jeolla dialect", "text": "Jeolla dialect\n\nThe Jeolla dialect of Korean (also known as Cholla; , ), or Southwestern Korean, are spoken in the Honam region of South Korea, including the city of Gwangju. This area was known as Jeolla Province during the Joseon era.\n\nLike the Chungcheong dialect, the Jeolla dialect is considered non-standard. Perhaps the most obvious difference comes from common verb endings. In place of the usual \"-seumnida\" (습니다 ) or \"-seyo\" (세요 ) endings, a southern Jeolla person will use \"-rau\" (라우 ) or \"-jirau\" (지라우 ) appended to the verb. For a causative verb ending, expressed in standard language with a \"-nikka\" (니까 ) ending, Jeolla people use \"-ngkkei\" (응게 ), so the past tense of the verb \"did\" (\"because someone did it\"), \"haesseunikka\" (했으니까 ), becomes \"haesseungkke\" (했승게 ). A similar sound is used for the quotative ending, \"somebody said...\". The usual verb endings are \"-dago\" (다고 ) and \"-rago\" (라고 ). Jeolla dialect prefers \"-dangkke\" (당게 ).\n\nRegarding pronunciation differences, there is often a tendency to pronounce only the second vowel in a diphthong. For example, the verb ending that indicates \"since\", \"-neundae\", becomes \"-neundi\" (는디). The name of the large city of Gwangju (광주) becomes \"Gangju\" (강주), and the verb 'to not have, to be absent', \"eopda\" 없다, becomes very close to \"upda\" (웂다). There are some words that are unique to the dialect as well: \"utjeseo\" (웆제서) for \"why\", \"sibang\" (시방) for \"now\", and \"dwitgan\" (뒷간) for \"outhouse\". Jeolla dialect speakers have a tendency to end their sentences with \"-ing\", (잉) especially when asking a favor. This can be compared to the word \"eh,\" as used by some Canadians.\n\nPansori texts are written in the Jeolla dialect.\n"}
{"id": "51704277", "url": "https://en.wikipedia.org/wiki?curid=51704277", "title": "Juggling terminology", "text": "Juggling terminology\n\nJuggling terminology, juggling (especially toss juggling) terms:\n\n"}
{"id": "52316501", "url": "https://en.wikipedia.org/wiki?curid=52316501", "title": "Kalagnanam", "text": "Kalagnanam\n\nKalagnanam is a Sanskrit language word, which means \"Knowledge of Times\". 'Prophecy' is the exact the meaning of the word .\n\nThe Kalagnanam has many authors, who prophesied the future of their times. Most famous among them are Sri Madvirat Pothuluri Veera Brahmendra Swamy and Saint Sarvajna. There are also other people who told Kalagnanam like Eswaridevi Matam (the grand daughter of Veera Brahmendra Swamy), Yogi Subbaraya Sharma, Swami Madhavacharya (also called Vidyaranya, the minister of Harihara I) and Dudekula Siddaiah (the disciple of Veera Brahmendra Swamy). \n\nA brief part of Kalagnanam written by Sri Veera Brahmendra Swamy, which was kept in his native place has been published in Telugu language as a book in 1970 for the first time, along with some other prophet's work. A large volume of the predictions made by Veera Brahmendra Swamy are said to be kept under the banyan tree of Banaganapalle village of Kurnool district of Andhra Pradesh. Out of the brief context of Kalagnanam said by Veera Brahmendra Swamy, 320 verses or poems are very popular with name \"Govinda Vakyas\". \n\nAnother prophet, Saint Sarvajna has prophesied since rule of King Bijjalaraya to emerge of Future Avatar, Veera Bhoga Vasanta Rayalu. His prophecies are in poetic way called 'Dwipada'. \n\nThe whole part of prophecies said by many saints from South India is included in the kalagnanam book which was published by Brahmamgarimatam. The most part of Kalagnanam alias prophecies collection, describes about the coming King-cum-Avatar, Veera Bhoga Vasanta Rayalu and his acts. About 70 percent of prophecies speak of him, while the other part describes the events to be happened mainly in South India.\n\nTelegu language version of Kalagnanam\n"}
{"id": "36367419", "url": "https://en.wikipedia.org/wiki?curid=36367419", "title": "Ku Klux Klan titles and vocabulary", "text": "Ku Klux Klan titles and vocabulary\n\nKu Klux Klan nomenclature has evolved over the order's nearly 160 years of existence. The titles and designations were first laid out in the original Klan's prescripts of 1867 and 1868, then revamped with William J. Simmons' Kloran of 1916. Subsequent Klans have made various modifications.\n\nThe sources of the rituals, titles and even the name of the original KKK may be found in antebellum college fraternities and secret societies such as the Kuklos Adelphon. John Lester, one of the original members of the group, stated that the Klan rituals were \"modeled on and embraced the leading features of the rituals of an order which has long been popular in many colleges and universities under various names\" such as the Sons of Confucius or Guiasticutus but always styled Ancient and Honorable and Mirth-Provoking. Walter L. Fleming stated in a footnote to Lester's text that the contemporary (early twentieth century) Southern college fraternity that most nearly mirrored the early Klan was Alpha Sigma Sigma and the institution of snipe hunting.\nThe original prescript of the Ku Klux Klan was adopted by a convention in Nashville, Tennessee in April 1867. A slightly revised edition appeared the next year.\n\nIn both prescripts there were four levels or \"departments\" of organization, above the basic level:\n\n\nIn the first prescript each officer is given the power to appoint Deputies to organize Realms, Dominions, Provinces and Dens until the latter can elect their own officers. The Grand Wizard was to be elected by a majority of Grand Dragons, and each lower level was elected by a majority of the next lower level of officers (Dragons elected by Titans, Titans by Giants etc.), as soon as three units had been formed at each level (three Dominions within a Realm for Grand Dragon, three Provinces within a Dominion for Grand Titan etc.). In the second prescript each officer appoints the lower officer with the approval of his superior.\n\nThe Dens were the basic level of organization for the Reconstruction Klan. In the original prescript, its chief officer was the Grand Cyclops, who appointed two Nighthawks, a Grand Turk, a Grand Sentinel, Grand Magi and a Grand Ensign in addition to his Grand Scribe. The Grand Cyclops, Grand Exchequer, Grand Magi (second officer) and Grand Monk (third officer) were elected by the body politic of the dens, identified as Ghouls In the second prescript the Grand Ensign is dispensed with, while the Grand Exchequer was appointed by the Grand Cyclops, who was now appointed by the Grand Giant. Only the Grand Magi and Grand Monk were elected by the Ghouls.\n\nThe exact function of these officers and the meaning of the titles varied. The two Nighthawks have been identified as couriers. The Grand Sentinel was in charge of the \"Grand Guard\", an organization which is not otherwise elaborated upon in the prescripts, but apparently served as the Den's security detail. The Grand Turk was the den's \"executive officer\" and was charged with informing Klansmen of \"all informal or irregular meetings\" and helping the Grand Cyclops and the Grand Magi maintain the \"control and government\" of the Den. The Grand Ensigns job was to take care of the Klan's flag.\n\nThe organ for initiation into the Klan was called the Investigating Committee, composed of the Grand Cyclops, Grand Magi and Grand Monk. Upon the nomination of a new member by a current member, the committee would investigate the candidate's \"antecedents and his past and present standings and connections\" and would then pronounce the candidate \"competent and worthy\" to become a member. The Grand Turk would escort the candidate to an \"outpost\" where he would question him and administer a preliminary oath. After this the Grand Turk would conduct the candidate to the Den, where the Grand Cyclops would administer the final oath. The second prescript elaborates that the candidate must have his left hand on the Bible and his right hand toward heaven and includes a ten-point \"interrogation\" that the candidate must answer satisfactorily in order to proceed with the final oath. The questions asked included: are you now or have ever been a member of the Radical Republican Party, Loyal Leagues or Grand Army of the Republic? Did you fight with the Union Army during the Civil War? Do you believe in Negro equality? among other things.\n\nA system of \"judiciary\" organs was created in each prescript. In the first the judiciary was divided into a Grand Council of Yahoos, to try officers of the Klan, and a Grand Council of Centaurs to try regular Ghouls. In Art.IV Sec.4, the Grand Giant is charged with conducting the Council of Yahoo, but Art.VI Sec.2 states that the Council will be composed of officers of the equivalent rank as the accused and presided over by an officer of the next higher rank. A trial of the Grand Wizard would be held by a meeting of all the Grand Dragons, the most senior Grand Dragon presiding It is unclear if these tribunals were ever functional.\n\nThe second prescript presented an entirely different judiciary, with officers of the first three levels tried by three judges, appointed by the chief officer of the given jurisdiction, and the officers and attaches of the headquarters (presumably the Genii, Hydras, and Furies). Trials of Den officers would be conducted at the provincial headquarters and include five judges, and ordinary ghouls to be tried at their dens with seven judges. As before a trial of the Grand Wizard would be conducted by the most senior Grand Dragon, this time with a quorum of seven dragons. All defendants had the right of appeal to the next higher court, and the proceeding was to be governed as \"ordinary court martials\".\n\nIn addition to the structure outlined above, there are documented cases of organs of slightly different nomenclature from Reconstruction. For instance, a group of twenty men who were arrested on April 6, 1868, at their \"den\" at the corners of Beale Street and Hernando street in Memphis, was called the Supreme Cyclopean Council. The constitution that the police captured outlined an organization with a Grand Cyclops, Vice-Grand Cyclops, and Secretary and openly advocated assassination of the \"murders and robbers\" now ruling the South. Members were bound to participate in the activities of the order, even if it meant leaving the \"embraces\" of their wife. A Grand Klan, composed of delegates from Spartanburg, York, Union, and Chester counties in South Carolina and a few from North Carolina met in Spartanburg, South Carolina and declared that no more raids or whippings would be conducted by members of the Klan except by their order and that the penalty for violating this order would be 100 lashes for the first offense and death for the second.\n\nAn attempt was apparently made to make Ghoul titles for other officers in Maury County, Tennessee in 1867 and early 1868, but the result was only confusion. Additionally a \"Grand Tycoon\" notified Klansmen in Lebanon, Tennessee to cease night riding and the order was apparently effective in restoring calm to Wilson County, Tennessee. In some early reconstruction Klan units there was also a Lictor, whose role as a guard of the den was later subsumed by the Nighthawks.\n\nThe Knights of the Ku Klux Klan, Inc. which existed from 1915 to 1944, elaborated on the original prescript in its Kloran and in the constitution and by-laws adopted in 1922. Some titles and jurisdictional designations were carried over from the Reconstruction prescripts intact or slightly modified, and others were original with Imperial Wizard Simmons.\n\n\nThere was a nearly identical set of subordinate officers at each level:\n\nThe officers at each level above the local Klan unit were designated by a unique prefix: Imperial at the Empire level; Grand at the Realm level; and Great at the Province level. Thus, for instance, an Imperial Kludd would be the chaplain for the whole organization, a Grand Kludd for the Realm, Great Kludd for a Province and simply Kludd for a local Klan.\n\nNot all offices were reproduced at each level. The Kladd, Klarogo, Klexter, and Klokard were not present at the Province level, and the Kladd was not included on the Realm level. The Klazik, second vice-president, and Klonsul, attorney, positions only existed at the Imperial level. The Klazik's duties included being head of the department of Realms and organizing new Realms and Provinces At the Province level there were three Klaliffs that served as an advisory board. A Klokann of three members, each one named a Klokan, filled this role at the local levels and a Klokann of four members at the Imperial level, in which they served as a \"Supreme Board of Auditors and Special Advisers and was led by a Chief Imperial Klokan.\n\nThe Imperial Wizards Genii constituted his Imperial Kloncilium; this was the Imperial Wizards supreme advisory board, as well as the Klans highest administrative organ in between Klonvokations. It met regularly every July, but could also be called when the Imperial Wizard or five Geniis petitioned him to do so. The Kloncilium was also the Supreme Tribunal of Justice of the organization with appellate jurisdiction and the right to finally determine disputes between Realms, Provinces, Klans and members in unorganized states\n\nGiant was adopted as an emeritus title: Imperial Giant for an ex-Imperial Wizard, Grand Giant for an ex-Grand Dragon, Great Giant for ex-Great Titan and Klan Giant for an ex-Exalted Cyclops.\n\nA Kleagle was a recruiter and was responsible for organizing local Klans.\n\nSince the dissolution of the Knights of the Ku Klux Klan, Inc. in 1944, there have been many Klan groups. Details of the nomenclature have varied, somewhat, among the different groups, but some terms have had more currency than others. Over time, the term klavern replaced klan for local groups. Imperial Wizard and Grand Dragon have still been generally used for the leader of a Klan organization and for state (realm) leaders. Exceptions included cases of one state Klans, such as the Association of Georgia Klans, whose leader Dr. Samuel Green, kept the title Grand Dragon until shortly before his death. The White Knights of the Ku Klux Klan created some innovations in their organization including a bicameral Klongress with an upper house Klonvokation and lower Klanburgess.\n\nIn the late 1970s David Duke's Knights of the Ku Klux Klan dropped the Imperial Wizard title, and the leader was called national director. That organization, now known as the Knights Party, no longer uses most of the traditional Klan titles, and the only fraternal titles used are Page, Squire and Knight for levels of membership.\n\nAside from titles and geographical designations, a distinctive vocabulary has grown around the Ku Klux Klan organizations. These include names for rituals, code words, and practices of the various Klans.\n\n\nThere are terms related to membership and non-membership:\n\nGroups of Klansmen commissioned for \"special activities\"\n\n\n\nTwo KKK codes for dates and times have been developed, the Ku Klux Register in the original prescripts and the Kalender developed by William J. Simmons.\n\nIn the original prescripts the register contained twelve designations, thought to correspond to months:\n\nThe second prescript had a slightly different scheme.\n\nColors were used for the days of the week:\n\nTwelve designations were used for the hours of the day:\n\nThe Kalender developed by W. J. Simmons included codes for days of the week, weeks, months and years. For months:\n\nFor weeks and days:\n\nYears were reckoned according to reigns and cycles. The Reign of Incarnation and Incantation was all time up to the American Revolutionary War. The First Reign of our Incarnation and Incantation was the period between the Revolution and the establishment of the original Klan, which was reckoned to May 6, 1866 in this scheme. The Reign of our Second Incarnation and Incantation was reckoned between 1866 and Grand Wizard Nathan Bedford Forrest's dissolution of the Klan, which is reckoned to 1872. The Reign of our Third Incarnation and Incantation began in 1915. The Klan year, Anno Klanslar, began in March of each year, and the cycle was reckoned from December of each calendar year.\n\n\n"}
{"id": "40408456", "url": "https://en.wikipedia.org/wiki?curid=40408456", "title": "Lanuvian language", "text": "Lanuvian language\n\nLanuvian was an archaic Latino-Faliscan language.\n"}
{"id": "29329807", "url": "https://en.wikipedia.org/wiki?curid=29329807", "title": "Linguistics in education", "text": "Linguistics in education\n\nLinguistics in education refers to a small but growing field of linguistics which advocates a greater use of linguistic theory and in primary and secondary education.\n\nDue to changes in national standards for K-12 education in the United States during the 1960s, grammar was largely dropped from English courses. The belief was that direct grammar instruction was not required for improved instruction in writing. Despite more recent research that has shown the positive effects of grammar instruction, the attitude that it is unnecessary persists today.\n\nBecause grammar has not been taught in many years, generations of teachers were never exposed to its direct instruction. Thus, even though many wish to teach it in their classrooms they do not find that they have adequate knowledge on the subject. Unfortunately this often perpetuates linguistic stereotypes that can sometimes be discriminatory to speakers on what are believed to be non standard dialects or languages.\nAnother issue is that the curriculum for teachers is already very broad, especially in comparison to other college students, so requiring further courses for would-be teachers is rather unpopular. The same problem exists in the K-12 classroom which already have difficult time constraints on their current curriculum. To add grammar to an already full curriculum is extremely difficult.\n\nStudies of how grammar and other linguistic theory can be incorporated into K-12 classrooms have been highly successful both in improving students' conscious knowledge of grammar and changing attitudes about non-standard English dialects. There is evidence that grammar instruction can be beneficial to students' writing such that replacing writing or vocabulary instruction with grammar can actually be a more productive use of class time.\n\nLinguists have also been involved in this field in attempts to change misconceptions about language. One common example is the definition of nouns. Traditionally a noun is defined as a \"person, place, or thing\". While this definition captures much of what nouns are it does not incorporate all possible definitions and uses. For example, mental concepts such as \"belief\" or \"idea\" are also nouns but do not neatly fit the traditional definition. This can be especially difficult for children to understand. A more comprehensive definition seeks to describe nouns through their features and uses. However this definition requires the teacher to have greater knowledge of English syntax.\n\n"}
{"id": "29644283", "url": "https://en.wikipedia.org/wiki?curid=29644283", "title": "List of linguistic rights in African constitutions", "text": "List of linguistic rights in African constitutions\n\nLinguistic rights in Africa are stated in constitutions which differ by country. These constitutions usually state the national language(s) and/or official language(s), and may or may not explicitly allow for other languages in the country. Most of the linguistic rights stated here are negative rights, which grant freedom of usage of own language and prevent discrimination based on language.\n\nConstitution as adopted on 19 November 1976.\n\nConstitution as adopted on 2 December 1990.\n\nConstitution as adopted on 30 September 1966.\n\nConstitution as adopted on 2 June 1991.\n\nConstitution as consolidated on 18 March 2005\n\nConstitution as adopted on 18 January 1996.\n\nConstitution as adopted on 25 September 1992.\n\nConstitution as adopted on 31 March 1996.\n\nConstitution as adopted on 15 March 1992.\n\nConstitution as adopted on 18 February 2006.\n\nConstitution as adopted on 4 September 1992.\n\nConstitution as adopted on 11 September 1971.\n\nConstitution as amended on 17 January 1995.\n\nConstitution as adopted on 23 May 1997.\n\nConstitution as adopted on 8 December 1994.\n\nConstitution as adopted on 28 April 1992.\n\nConstitution as proposed on 6 May 2010.\n\nConstitution as adopted on 2 April 1993.\n\nConstitution as adopted on 6 January 1986.\n\nConstitution as adopted on 16 May 1994.\n\nConstitution as adopted on 27 February 1992.\n\nConstitution as adopted on 12 March 1968.\n\nConstitution as adopted on 21 January 2005.\n\nConstitution as adopted on February 1990.\n\nConstitution as adopted on 18 July 1999.\n\nConstitution as adopted on 29 May 1999.\n\nConstitution as adopted on 26 May 2003.\n\nConstitution as adopted on 7 January 2001.\n\nConstitution as adopted on 18 June 1993.\n\nConstitution as amended on 1 October 1991.\n\nConstitution as adopted on 8 May 1996.\n\nConstitution as adopted on 1 July 1998.\n\nConstitution as adopted on 26 July 2005\n\nConstitution as adopted on 1 June 1959\n\nConstitution as adopted on 8 October 1995.\n\nChapter 1, Article 3 of the constitution of the Sahrawi Arab Democratic Republic stipulates that the sole official language of Western Sahara shall be standard Arabic. In practice, Spanish is used as a working language by some Sahrawi media. The vernacular language spoken by nearly all Sahrawis, however, is Hassaniya Arabic.\n\nConstitution as adopted on 24 August 1991.\n\nConstitution as amended on 1 February 2007\n\n"}
{"id": "24651272", "url": "https://en.wikipedia.org/wiki?curid=24651272", "title": "MOGUL framework", "text": "MOGUL framework\n\nThe MOGUL framework is a research framework aiming to provide a theoretical perspective on the nature of language. MOGUL (Modular On-line Growth and Use of Language) draws on the common ground underlying various related areas of cognitive science including psycholinguistics, theoretical linguistics, first- and second-language acquisition, neurolinguistics and cognitive psychology; it is designed to be applicable to all these fields of research.\n\nThe MOGUL framework's background assumption is that the mind is composed of expert systems which have evolved over time, one of which is responsible for human linguistic ability. Historically, scientific studies of language have been divided between many sub-disciplines; theoretical linguists focus on the abstract properties of language and researchers in other fields investigate how language is used and processed in real time: either in psychological terms or (in the case of neurolinguistics) through a study of the physical systems of neurons in the brain. Each field of study has developed its own research traditions and technical vocabulary, making it difficult to integrate insights across disciplines. The MOGUL Framework represents an attempt to identify common themes and compatible approaches in different (but related fields), and hence to facilitate integration.\n\nMOGUL is designed as a platform for the generation of new hypotheses, with a terminology and a set of interrelated concepts that can be used across more than one discipline. Although not a full-fledged theory in its own right, it sets out various theoretical claims based on the current literature in a range of research domains. As suggested below it is not connectionist in the most widely used sense of that term, although it employs terms and concepts familiar to connectionists such as competition, activation, activation levels and so forth. The same processing perspective also requires the long-established Chomskyan distinction between competence and performance to be viewed in a somewhat different way. Linguistic competence (knowledge) is not compartmentalised and placed in a separate box in the model, which then forms the object for a processor (or processors) to act on. Rather, it is instantiated and implicit within the processing system. The distinction between mental representation and processing is still valid, but is expressed in a way that ought to allow the abstract properties of linguistic systems in the mind to be more easily investigated in conjunction with real-time linguistic phenomena. The inspiration for this approach is the work of Ray Jackendoff. Here the architecture of the language faculty is formulated in such a way as to facilitate statements about competence-related, representational issues and their relationship to real-time language processing within the same model.\n\nEmpirical evidence for the claims and assumptions set out in MOGUL rests essentially on the experimental findings of the various fields of research on which it is based. The MOGUL framework queries current solutions and raises new questions based upon this body of evidence, in the hope that answers to such questions will be sought in the related areas by appropriate researchers.\n\nMore specifically, MOGUL represents an attempt to provide ways of investigating how first- or second-language processing relates to linguistic development and how language in the monolingual and multilingual language user is situated within human cognitive architecture in general. What exactly happens, even in milliseconds, when someone is exposed to the sights and sounds of a new language? How do we explain what gets noticed and what gets regularly ignored? What about the fixed stages that people seem to pass though? How is it that we have so little conscious control over what we are attempting to acquire? Why do children appear to be worse language learners in the short run and much better in the long run? How can two or more language systems cohabit in one mind? Answers to these questions require an interdisciplinary approach. Two major sources for inspiration for the approach described here are the ideas of generative linguist Ray Jackendoff on language faculty and modularity and Global Workspace Theory, as advanced by Bernard Baars. The first MOGUL publication was released in 2004 and was a keynote article in \"Bilingualism: Language and Cognition\" (Truscott and Sharwood Smith 2004).\n\nIn order to situate languages within a wider cognitive context, the MOGUL framework is designed to provide a basic architecture that purports to explain how (in general terms) the mind functions. This means that it is not an account of brain function, although it is set out in such a way that might facilitate such accounts.\n\nBriefly, MOGUL architecture follows Jackendoff in positing a generic cognitive module (or, in MOGUL, a processing unit) which consists of an integrative processor operating on structural elements (structures) in a memory store. Specifically, it works with activated elements that are raised into working memory from that memory store.\n\nModules are only somewhat generic in structure; the rest is very specific to each module, since each module operates with its own code. Visual memory contains elements that only the visual processor can access; syntactic memory contains elements that only the syntactic processor can deal with, and so on. While Jackendoff prefers to think of working memory as a separate blackboard onto which elements of long-term memory are written, in MOGUL the alternative view of the working-memory blackboard is adopted: it is part of the unified memory system, and elements are raised to the working memory \"surface\" within the same store. In both versions, elements must cross an activation threshold to be visible to the integrative processor.) Note that memory is modularised, so visual memory (including visual working memory) is separate from – and independent of – for example, phonological memory. Modules, useless if entirely autonomous and cut off from other parts of the system as a whole, require some way whereby they can link up and cooperate. This is done by means of interface processors matching structural elements in the memory stores of adjacent modules. The cognitive system as a whole copes with a multitude of different tasks like tying a knot, listening to a speech, driving through a busy city and salsa-dancing by forming appropriate coalitions of processors producing assemblies of structures (linked together via appropriate interfaces) from different modules. The generic structure of a MOGUL module (based on Jackendoff's model) is portrayed in the figure on the right, and also reflected in various places in the third figure in this article (see \"MOGUL architecture in a nutshell\" below).\n\nThis functional architecture may be neurally and anatomically instantiated in different ways: the design of systems such as that proposed by Jackendoff and others (including MOGUL) involves no specific claims about the precise nature and location of the neural system. These are two different levels of description. The auditory module (translated into cortical terms) involves several brain regions distributed across the supratemporal plane; activation in MOGUL terms (when translated into neurological terms) may involve both excitatory and inhibitory neurotransmitters. By the same token, functional auditory structure (as described within the MOGUL framework) must be distinguished from anatomical and physiological auditory structure.\n\nAn important portion of the MOGUL account is devoted to the claim that the acquired (and atrophied, or lost) linguistic structures are natural byproducts of online processing. This means that acquisitional mechanisms do not exist as such, but are embodied in the operations of the parser. This claim is made explicit in Acquisition by Processing Theory (APT). Processing concepts (activation levels, competition and so on), as exploited by other non-modular, domain-specific approaches, are realigned in MOGUL to explain how new cognitive systems emerge. APT is a hypothesis put forward by Sharwood Smith and Truscott as an integral part of MOGUL and is applicable to all kinds of cognitive development, not only language acquisition and language attrition (for a similar approach acquisition which is framed within an emergentist perspective, see O'Grady 2005).\n\nEssentially, the claim is that all development is the lingering effect of processing. As the mind attempts to build mental representations online, various structures at its disposal are activated. Structures compete with one another to be selected for the current representation. As a simple example, on hearing the word \"ship\" an English speaker's processing system will activate various candidate structures so (for instance) the phonological structures underlying \"sheep\" and \"shape\" will compete for selection (and other candidates as well, including phonological structures belonging to other languages known to the listener). The \"ship\" structure is normally selected as the best fit, and thereby its likelihood of being selected in the future is correspondingly strengthened by a small amount. In this way, a basic \"use it or lose it\" principle is invoked; this development principle works throughout the cognitive system as a whole.\n\nAs we match various types of cognitive structure available to us in order to find the best fit for unfamiliar input from the environment new connections are developed, initially with the relevant structures possessing a low resting level of activation. This means they will have a relatively poor chance of selection for future instances of the same input. However, the more they are selected the more they will show up in the observable behaviour of the individual concerned.\n\nAlthough the frequency in which we experience given phenomena influences our development, cognitive growth is not an automatic consequence of experience; our mind is modular, and each module is controlled by its own unique processing principles. This limits, for instance, what we can learn to see or hear or say; seeing, hearing and speaking each involve dedicated processing units, which control their own internal operations. In this way, high-frequency events in the environment may still not impact development because the relevant parts of our mind are incapable of processing them. This may be because a) the relevant modules (processing units) are simply not designed to do so, or b) they are not yet ready to process them, because their internal principles require some prerequisite state of affairs before the potential new input can be integrated. By the same token, an internal operation may be halted because although one processing unit has processed input, an adjacent processing unit (module) with which it is connected is not in a ready state to cooperate (find matching structures within its own memory store). Hence frequent events in the external environment may indeed get processed by some part(s) of the cognitive system but still not by every relevant part. MOGUL espouses both modularity of mind and a constrained form of connectionism with a complex web of expert systems, each with its own particular organising principles. In this way, MOGUL makes claims not only about language abilities but about the mind in general.\n\nThe core modules involved in language comprehension work in two directions. Phonological, syntactic and conceptual processing work in one direction in response to visual and auditory events in the environment (typically in listening to speech or reading a text), but they also work in the other direction to produce speech or writing (or messages in sign language). Production involves a physical response to internal events, the creation of a message to be conveyed. This requires articulation of different parts of the body, following the commands of motor structures. Meanings in the conceptual processor are matched with syntactic structures which in turn are matched with phonological structures; this structural chain continues to be built following different routes according to the selected mode of articulation. The required motor structures that drive the articulation of speech will be different from those involved in writing or signing.\n\nWhen the basic direction of processing is, say, from conceptual structure to speech, the assembling of an appropriate structural chain is not carried out in a rigid unidirectional sequence; at different levels, different options will be available and compete for selection. In production, some options may be partially formed and then be dropped in favour of a better-fitting rival structure. In this way the ultimate structural chain is built up incrementally, out of the more accessible candidate structures that happen to provide the best overall fit at the time. Comprehension works according to the same principles, with parallel processing and split-second editing and revising until a best-fit interpretation is arrived at. It may be the wrong interpretation but it will be the best fit, given the person's current processing resources at that particular moment in time.\n\nAs an example of what processing online means, let us examine speech production in a fluent speaker. The construction of a message will be initiated in the conceptual processor. Conceptual structures will be chosen, which then activate the interface between the conceptual and syntactic system. The conceptual structures are matched up with particular syntactic structures forming the first stage – in other words, a CS+SS (conceptual structure plus syntactic structure) chain. A semantic argument structure in CS code which specifies an action with an agent (the doer) and a patient (what is acted upon), as in \"a boy hit the ball\", is matched up with a syntactic argument structure with the requisite verb and noun phrases (determiner phrases), each in the appropriate case: one in nominative case and the other in objective case. The interface between SS and PS kicks in, causing various appropriate phonological structures to be activated; an SS/PS match is made, the outcome now being a CS+SS+PS chain. As is generally the case, more than one option may be selected in parallel before one particular option is settled on. Structural chains are formed incrementally; as more CS is built so more SS and PS are constructed with more context, earlier options that were provisionally selected are dropped as the representation develops and becomes more complex. The PS is matched up with motor structures responsible for the articulation of speech, and the utterance is produced. Each type of structure (AS, PS, SS, CS and so on) is constructed in its own particular module and by its own unique integrative processor, following its own particular set of principles. Comprehension is a similar process, with the general direction going in reverse; auditory structures are formed in response to acoustic stimuli in the environment. These auditory structures match PS and SS, finally culminating in the interpretation of the message (its conceptual structure).\n\nOne feature of the MOGUL approach is an attempt to spell out in coherent terms the role of perception and affect in issues of language development in the individual. In the earlier example of \"a boy hit the ball\" (where a simple example of a how a CS+SS+PS chain was built up), we assume that the conceptual structures evoked also have interfaces with various perceptual and affective structures to account for the associations individuals have with, for example, the concept \"boy\". In other words, the activation of a chain effectively becomes the activation of a whole network of associations contributed by different modular systems in the mind. The co-activation of structural networks of various kinds is important in explaining facts about attention and noticing. MOGUL follows the view that high levels of activation are strongly implicated in the phenomenon of conscious awareness. This is made explicit in the role of \"perceptual output structures\" (POpS).\n\nPOpS is a generic term covering the output of various perceptual systems corresponding to the five senses – or, more properly, the sensory systems currently believed to exist (which number more than the traditional five). Of greatest interest in MOGUL are \"visual structures\" (VS) and \"auditory structures\" (AS), given the fact that language is usually perceived in visual and auditory terms. Structures reside in the memory stores of the appropriate processing system (processing unit, or module). Exposed to the sound of the word \"dog\" or the sound of a creaking door, the auditory system activates a particular auditory structure (or set of auditory structures) in response to this sensory input. In this way, a given structure can be thought of as an auditory memory that may be re-activated even when there is no external sensory input (in hallucinations and dreams, for example) and grow stronger or weaker (less accessible) depending on the frequency with which it is activated successfully.\n\nPerceptual output structures are strongly interconnected. This enables a coordinated responses to events in the environment and assists the organism's chances of survival. This coordinating function of the POpS system may be related to the role of the global workspace in Baars' theory. At the same time, since POpS mediate between the external environment and the internal operations of the mind that are inaccessible to awareness, POpS conform closely with Jackendoff's Intermediate Theory of Consciousness. The rich interconnections between POpS permit very high levels of activation to be achieved, a hypothetical prerequisite for awareness to occur. It is also reflected in the condition known as synaesthesia: here the connection between two POpS reaches levels that result in an awareness whereby the two senses appear to merge; for example, a particular sound takes on the quality of a particular taste. Another example of connection is provided by the experimentally-induced McGurk effect. Here, two sensory signals are generated to produce a conflict; the subject's awareness is the result of an attempt to resolve the conflict, so that hearing one sound and simultaneously seeing the speaker pronounce a different sound will create the illusion of hearing a sound halfway between the actual sound and the sound suggested by the speaker's facial gestures (especially the movement of the lips).\n\nThe main point is that awareness (whether it can be classified as an illusion or not) is generated indirectly via POpS; we do not have direct access to the contents of any module or processing unit. While we can never become aware of the fine phonological and syntactic properties of the word \"dog\" - or its semantic and pragmatic properties, its \"conceptual structure\"(CS) - we can certainly become aware of its sound. In the same way, we can become aware of the sound of the creaking door. In both cases, it is auditory structure (one of the POpS) that gives rise to the conscious experience. As implied above, it is in the nature of linguistic structures (PS and SS) and its conceptual structures (CS) that they do \"not\" have the rich interconnectivity described above, and consequently do not (and \"cannot\") achieve the appropriate levels of activation for consciousness to occur. What we become conscious of arises directly from POpS activity. In other words, consciousness is always perceptual in nature.\n\nThe affective system works with \"affective structures\" (AfS) that are constructed using primitives such as \"fear\" and \"disgust\", which are discussed in affective neuroscience in the work of António Damásio. AfS are connected to POpS and contribute directly to appropriate responses of the organism to events in the immediate environment, to its chances of survival and to the experience of conscious awareness. Making these aspects of mental life more explicit helps researchers to understand and devise hypotheses about how consciousness and emotion affect online language performance and language acquisition. By the same token, it should help to show how certain aspects of language behaviour are (or might be) immune to such influence.\n\nLanguage is a vague term and is its widest sense involves a host of different processing units, only some of which are what makes human language unique to this species, that is, the phonological and syntactic expert systems that make up what, in generative linguistics, is called the language module and whose special properties are often subsumed under the name Universal Grammar. Closely associated with this core system, but not part of the language module per se, are a rich set of conceptual structures developed over the lifespan, as also b) auditory structures that have been matched up with phonological structures and c) visual structures created for interpreting writing and sign language. In addition, there are the various motor structures involved in the production of language in its different modes. One might lump all of this together and call it \"knowledge of language\". None of it, however, constitutes 'knowledge of language' if, by this term, we rather mean language that we can think and talk about and analyse, in other words \"metalinguistic knowledge\", which, as with all forms of metacognition, is constructed out of conceptual structure and various perceptual output structures. To take a simple example, the word \"horse\" can be discussed or pondered; all that is needed for this is an auditory structure (the sound of the word) and its visual structure (representing its orthographic, written form), both of which are matched up with its meaning as illustrated in the figure on the right. Not represented in the figure is the additional conceptual structure (over above the meaning of horse itself), consisting of metalinguistic concepts such as \"word, syllable, noun, definition\" and the like. These concepts are required for any analytic thinking about language and may vary widely in degree and complexity, depending on an individual's metalinguistic sophistication. In any case, the language module is not directly implicated in any explicit discussion (or explicit thinking) about what is actually a linguistic form. In the MOGUL version of metalinguistic processing, the essential chain of connections that are assembled completely bypasses the PS and SS systems. Processing within the language module will always kick in automatically on exposure of language stimuli, but metalinguistic (explicit) activity is processed elsewhere. Following the standard arguments from learnability theory as applied in generative linguistics, the language module is nevertheless crucial for all other types of linguistic activity and is vital for explaining the acquisition of language. No amount of activation of auditory and conceptual structure alone can explain the growth of language in the mind following repeated exposure to speech, most obviously in the case of little children and (arguably) adults as well.\n\nBecause of these two modes of knowing, we can appear to be very knowledgeable about the grammar of a particular language or grammar in general and yet be very poor users of that language. Or, like many native speakers, we can make metalinguistic assertions about the rules of our language which are not at all borne out by the way we actually speak and comprehend our mother tongue. Knowledge of a language, in this metalinguistic sense of the word, can also be \"right\" or \"wrong\". We can have misconceptions about language, or we can have a view of grammar that accords with the facts. By way of contrast, the system operated subconsciously within our language module can never be right or wrong: it is just the way it is, the way it as has developed in us over time. Metalinguistic knowledge is not useless, however. An essential part of education in many cultures is acquiring such consciously-accessible knowledge about language, and especially about the mother tongue. In MOGUL, however, this should not be confused with the implicit knowledge of language that drives language performance.\n\nIn this figure, the MOGUL version of the cognitive system is sketched in simplified form. The circles or ovals represent specialised processors, and the rectangular boxes each represent the particular memory store of structures which they are designed to handle. Affective structures (AfS) are not differentiated in this figure. Also, for simplicity's sake, only five \"senses\" are portrayed in the POpS system. All languages are handled by the same syntactic and phonological processors, but different languages will involve both shared and unique \"syntactic structures\" (SS) and \"phonological structures\" (PS) residing in (respectively) their syntactic and phonological memory stores. Mental activity is characterised by the formation of chains or networks of structures across different modules. This is achieved by means of interface processors (the dotted arrows) following Jackendoff (1987). These are bimodular, in that they match elements in two adjacent modules; they can only access and link certain structural elements in one or the other module. Moreover, the matching is not a \"translation\" of one element into terms (the code) of another system; there is no exchange of information, merely a chaining of elements that can otherwise only be processed within its own particular processing unit or module.\n\n\n\n"}
{"id": "34384656", "url": "https://en.wikipedia.org/wiki?curid=34384656", "title": "Metafunction", "text": "Metafunction\n\nThe term \"metafunction\" originates in systemic functional linguistics and is considered to be a property of all languages. Systemic functional linguistics is functional and semantic rather than formal and syntactic in its orientation. As a functional linguistic theory, it claims that both the emergence of grammar and the particular forms that grammars take should be explained “in terms of the functions that language evolved to serve”. While languages vary in how and what they do, and what humans do with them in the contexts of human cultural practice, all languages are considered to be shaped and organised in relation to three functions, or metafunctions. Michael Halliday, the founder of systemic functional linguistics, calls these three functions the \"ideational\", \"interpersonal\", and \"textual\". The ideational function is further divided into the \"experiential\" and \"logical\".\n\nMetafunctions are \"systemic clusters\"; that is, they are groups of semantic systems that make meanings of a related kind. The three metafunctions are mapped onto the structure of the clause. For this reason, systemic linguists analyse a clause from three perspectives. Halliday argues that the concept of metafunction is one of a small set of principles that are necessary to explain how language works; this concept of function in language is necessary to explain the organisation of the semantic system of language. Function is considered to be \"a fundamental property of language itself\".\n\nAccording to Ruqaiya Hasan, the metafunctions in SFL \"are not hierarchised; they have equal status, and each is manifested in every act of language use: in fact, an important task for grammatics is to describe how the three metafunctions are woven together into the same linguistic unit\". Hasan argues that this is one way in which Halliday's account of the functions of language is different from that of Karl Bühler, for example, for whom functions of language are hierarchically ordered, with the referential function the most important of all. For Buhler, the functions were considered to operate one at a time. In SFL, the metafunctions operate simultaneously, and any utterance is a harmony of choices across all three functions.\n\nThe ideational function is language concerned with building and maintaining a theory of experience. It includes the experiential function and the logical function.\n\nThe experiential function refers to the grammatical choices that enable speakers to make meanings about the world around us and inside us:\n\nHalliday argues that it was through this process of humans making meaning from experience that language evolved. Thus, the human species had to “make sense of the complex world in which it evolved: to classify, or group into categories, the objects and events within its awareness”. These categories are not given to us through our senses; they have to be “construed”. In taking this position on the active role of grammar in construing “reality”, Halliday was influenced by Whorf.\n\nHalliday describes the logical function as those systems “which set up logical–semantic relationships between one clausal unit and another” The systems which come under the logical function are and . When two clauses are combined, a speaker chooses whether to give both clauses equal status, or to make one dependent on the other. In addition, a speaker choose some meaning relation in the process of joining or binding clauses together. Halliday argues that the meanings we make in such processes are most closely related to the experiential function. For this reason, he puts the experiential and logical functions together into the ideational function.\n\nThe interpersonal function refers to the grammatical choices that enable speakers to enact their complex and diverse interpersonal relations. This tenet of systemic functional linguistics is based on the claim that a speaker not only talks about something, but is always talking to and with others. Language not only construes experience, but simultaneously acts out “the interpersonal encounters that are essential to our survival”. Halliday argues that these encounters:\n\nThe grammatical systems that relate to the interpersonal function include Mood, Modality, and Polarity.\n\nHalliday argues that both experiential and interpersonal functions are intricately organized, but that between the two “there is comparatively very little constraint”. This means that “by and large, you can put any interactional ‘spin’ on any representational content”. What allows meanings from these two modes to freely combine is the intercession of a third, distinct mode of meaning that Halliday refers to as the textual function. The term encompasses all of the grammatical systems responsible for managing the flow of discourse. These systems “create coherent text – text that coheres within itself and with the context of situation” They are both structural (involving choices relating to the ordering of elements in the clause), and non-structural (involving choices that create cohesive ties between units that have no structural bond). The relevant grammatical systems include Theme, Given and New, as well as the systems of cohesion, such as Reference, Substitution, and Ellipsis. Halliday argues that the textual function is distinct from both the experiential and interpersonal because its object is language itself. Through the textual function, language “creates a semiotic world of its own: a parallel universe, or ‘virtual reality’ in modern terms”.\n"}
{"id": "2375603", "url": "https://en.wikipedia.org/wiki?curid=2375603", "title": "National Accreditation Authority for Translators and Interpreters", "text": "National Accreditation Authority for Translators and Interpreters\n\nThe National Accreditation Authority for Translators and Interpreters Ltd (known as NAATI) is the national standards and accreditation body for translators and interpreters in Australia. NAATI's mission, as outlined in the NAATI Constitution, is to set and maintain high national standards in translating and interpreting to enable the existence of a pool of accredited translators and interpreters responsive to the changing needs and demography of the Australian community. The core focus of the company is issuing credentials (known as accreditations or recognitions) for practitioners who wish to work as translators and interpreters in Australia.\n\nNAATI is a profit company that is jointly owned by the Commonwealth, State and Territory governments of Australia. It is governed by a board of directors who are appointed by the members.\n\nThe members of NAATI are the nine ministers who are responsible for multicultural affairs and/or citizenship in the Commonwealth, State and Territory governments. Members may appoint a representative to exercise any of their powers in relation to NAATI. These Member Representatives are separate to the NAATI board of directors.\n\nThe current members of NAATI include:\n\nNAATI provides eight key services to assist individuals gain and maintain a credential to work as a translator or interpreter in Australia. These services include:\n\nThere are two types of NAATI credentials – accreditation and recognition.\n\nNAATI accreditation is an acknowledgement that an individual has demonstrated the ability to meet the professional standards required by the translation and interpreting industry. NAATI assesses practitioners and aspiring translators and interpreters against these standards so that English speaking and non-English speaking Australians can interact effectively with each other.\n\nThere are a couple of different ways you can gain NAATI accreditation, including:\n\nNAATI recognition is granted in emerging languages or languages with very low community demand for which NAATI does not offer accreditation. The granting of NAATI recognition is an acknowledgement that an individual has recent and regular experience as a translator and/or interpreter with no defined skill level.\n\nUnder NAATI's current system, there are ten different types of credentials. These are listed in the table below.\n\nNAATI translator accreditation (professional level or higher) is usually awarded in one of the following directions:\n\n\nNAATI interpreter accreditations (at all levels) are awarded in both directions.\n\nOccasionally, NAATI has awarded accreditation in a language combination that does not feature English at the Conference Interpreter or Advanced Translator level e.g. Advanced Translator French to German or Conference Interpreter (Senior) French to/from Russian. This sort of accreditation can only be awarded on the basis of a professional membership of an international association such as AIIC or AITC.\n\n\n"}
{"id": "11274197", "url": "https://en.wikipedia.org/wiki?curid=11274197", "title": "National Vocabulary Championship", "text": "National Vocabulary Championship\n\nThe National Vocabulary Championship (NVC) was the first-ever U.S.-wide vocabulary competition for high school students created by GSN, in association with The Princeton Review. Thirty thousand high school students from across the United States participated in the inaugural year (2006-2007).\n\nThe NVC aimed to inspire students to expand their vocabularies and narrow the achievement gap. The program offered free educational resources, created spirited competition through testing and game play, and awarded more than $100,000 annually in college tuition and other prizes.\n\nFifty finalists nationally received a trip to the NVC Finals, where they competed to win $40,000 toward college tuition in the form of a 529 plan and to be crowned the National Vocabulary Champion.\n\nThe host of the National Vocabulary Championship was GSN host Dylan Lane.\n\nThe NVC was discontinued after the 2007-2008 academic and competition year due to changes in GSN policy and administration.\n\nThe NVC was open to eligible high school students in the United States between the ages of 13 and 19 years old and in grades 9-12. Home-schooled students were also eligible to compete.\n\nThere were two ways to enter the competition:\n\nEligible students at participating schools in eight local markets could qualify to participate in a \"Citywide Championship\" for a chance to win $5,000 toward college tuition and other prizes, as well as a trip to the national finals.\n\nEligible students nationwide could also participate through a \"National Qualifying Competition\" via on-line and regional exams offered by The Princeton Review for a chance to compete in the NVC Finals.\n\nEight U.S. cities across the country were chosen each year to host Citywide Championship events. Eligible students who wanted to compete in one of the local Citywide Championships had to attend a participating high school, register with the designated NVC coach at their school, and take the NVC in-school qualifying exam. All public and private high schools within these markets and their surrounding areas were invited to participate and encourage their students to compete. Approximately 100 top scorers per market qualified to compete in each Citywide Championship, where one winner received $5,000 toward college tuition and other prizes as well as a trip to the national finals.\n\nBelow is a list of cities that the NVC visited in 2007-2008:\n\nEligible high school students who did not attend a participating school (listed under Citywide Championship) or who did not wish to take the in-school qualifying exam could compete in the National Qualifying Competition by following the below steps:\n1) The NVC offered an online national qualifying exam during November 2007 at winwithwords.com. For months leading up to the online exam, study tools and study modules were available at winwithwords.com\nOr:\n2) Top-scoring students on the online national qualifying exam had an opportunity to advance to the regional exams, which took place at designated The Princeton Review locations across the country. Forty-two top scorers from the National Qualifying Competition joined the eight citywide champions at the national finals.\n\n\n"}
{"id": "45375262", "url": "https://en.wikipedia.org/wiki?curid=45375262", "title": "Ngatikese language", "text": "Ngatikese language\n\nNgatikese, or Sapwuahfik, is a Micronesian language originating on Sapwuahfik atoll, Federated States of Micronesia. Of the 700 Ngatikese speakers, only about 450 live on Sapwuahfik. It was previously considered a dialect of Pohnpeian, but is not particularly intelligible with that language. It is currently considered vulnerable.\n\n"}
{"id": "26447533", "url": "https://en.wikipedia.org/wiki?curid=26447533", "title": "Official bilingualism in the public service of Canada", "text": "Official bilingualism in the public service of Canada\n\nBecause Canada has, for over two centuries, contained both English- and French-speakers, the question of the language used in the administration of public affairs has always been a sensitive issue.\n\nAmong the aspect of this issue that have excited public attention from time to time are: \n\nThe issue of proportional hiring and promotion of speakers of both official languages has been an issue in Canadian politics since before Confederation. Members of each linguistic group have, at different times in the country’s history, complained of injustice when their group have been represented, in public service hiring and promotion, in numbers less than would be justified by their proportion of the national population.\n\nFor the greater part of Canada’s history, French-speakers were underrepresented, and English-speakers were overrepresented in the ranks of the public service. This disproportion became more pronounced in the more senior ranks of the Public Service. However, this trend has reversed itself in recent decades. Today, French is the first official language of 23% of Canada's population, with 29.2% of Public Service of Canada employees identifying French as their first official language, including 32% of management-level jobs.\n\nThe first high-profile complaint of preferential hiring took place in 1834. One of the Ninety-Two Resolutions of the Lower Canadian House of Assembly drew attention to the fact that French Canadians, who at the time were 88% of the colony's population, held only 30% of the posts in the 157-member colonial civil service. Moreover, the resolution stated, French Canadians were, \"for the most part, appointed to the inferior and less lucrative offices, and most frequently only obtaining even them, by becoming the dependent of those [British immigrants] who hold the higher and the more lucrative offices...\"\n\nWith the advent of responsible government in the 1840s, the power to make civil service appointments was transferred to elected politicians, who had a strong incentive to ensure that French Canadian voters did not feel that they were being frozen out of hiring and promotions. Although no formal reform of the hiring and promotion process was ever undertaken, the patronage-driven hiring process seems to have produced a more equitable representation of the two language groups. In 1863, the public service of the united Province of Canada (which included both of the former colonies of Lower Canada and Upper Canada) had grown to include 450 officials at its administrative headquarters. Of these, 161 persons, or about 36%, were French Canadians. However, French-speaking civil servants accounted for only 20% of the total payroll, suggesting that they were mainly confined to the lower ranks of the colonial administration.\n\nIn the period between 1867 and the turn of the Twentieth Century, French-Canadians made up about one-third of the Canadian population, and seem also to have represented about one-third of civil service appointments at junior levels, although they had only about half that much representation at most senior level. In general, it seems to have been the case that throughout the entire period between Confederation and the introduction of merit-based hiring and promotion in 1919, the percentage of French-speakers in the civil service relatively closely matched their percentage of the Canadian population because French-Canadians occupied a more-or-less proportionate share of cabinet posts, and they appointed public servants who were personally loyal to them. This so-called \"spoils system\" ensured a rough equity between members of the two language groups. A quarter-century after this system had been abolished, it was summarized as follows in a submission to the 1946 Royal Commission on Administrative Classifications in the Civil Service:\n\nIn 1918, just before the reforms began, the percentage of francophones in the federal civil service stood at just under 22%. The drop in the percentage of French-speakers was in part due to the fact that the proportion of Canadians who were French-speaking was gradually declining; in the 1921 census, persons of French origin were down to 27.9% of the population. As well, the Unionist government then in power had almost no representation in Quebec, which meant that for several years there were no French-Canadian ministers appointing and promoting their fellow French-speakers.\n\nThe percentage of French-speakers would, however, drop much further as a result of the replacement of a patronage-based appointment system with a merit-based system that was largely based upon the ability to perform efficiently in English. By 1946, according to figures presented in the House of Commons, native speakers of French had declined to only 13% of public service posts. At the time, French-speakers represented slightly under 30% of the Canadian population. This seems to have represented the low point, in terms of inequitably low representation for Francophones within the Public Service; by 1964, French-speakers had risen to 21.5% of the Public Service (which itself had grown to 137,000 officials, meaning that 29,500 French-speakers were employed by the federal government). Although this represented a substantial improvement, French-speakers at this time represented about 28% of Canada’s population, and therefore continued to be underrepresented. Moreover, French-Canadians continued to be grouped at the bottom of the seniority scale: They represented 23.7% of junior employees earning less than $4,000 per year, but only 10.4% of senior officials with earnings in the highest category (over $15,000).\n\nBy the early 1960s, the issue of Canada’s seemingly perpetual inability to create an equitable distribution of jobs in the country’s rapidly expanding public service was becoming a key grievance underlying Quebec nationalism. In 1961, a Royal Commission studying the structure of the federal bureaucracy organized a special committee to study the issue of bilingualism within the Public Service. The Commission’s 1962 report included recommendations that the federal government “adopt active measures to develop bilingual capacities among its employees on a selective basis,” and that it more actively recruit qualified French Canadians who would have the potential to advance to the senior ranks of the federal administration.\n\nIn part on the basis of this recommendation, the government of Prime Minister Lester B. Pearson established a Royal Commission on Bilingualism and Biculturalism, which made language reform within the public service one of its highest priorities. Book III of the Royal Commission’s multi-volume report, published in 1969, recommended a radical redesign of the Public Service of Canada, in order to establish full equality between the two official languages in the federal administration, and a permanently equitable distribution of jobs, at all levels of seniority, between French-speaking and English-speaking Canadians.\n\nSince the report of the Royal Commission in 1969, the percentage of Francophones in the public service of Canada has increased, and the percentage of Anglophones has decreased. Both Francophones and Anglophones are overrepresented in the public service due to the number of Canadians who speak neither English nor French.\n\nWhen looking at public service positions in various regions of the country, the same trend has occurred; however it is much less pronounced outside of the national capital region surrounding Ottawa, Ontario and Gatineau, Quebec.\n\nFrancophone representation in management positions in the Public Service has increased from 10.4% in 1964 to 18% in 1978 and to 30% in 2007. Francophones are now overrepresented in all Public service categories, however the degree to which this representation has occurred varies by occupation.\nPositions in the public service are classified in four ways, depending on the required linguistic capacity of the candidate for the position: bilingual imperative, English essential, French essential or either French or English essential. The bilingual imperative category is further broken down into the level of both languages the candidate is required to speak. Since 1989 there has been a sharp increase in the number of positions classified as ‘bilingual imperative’ and a sharp decrease in the number of positions open to unilingual Canadians.\n\nToday, the built-in barriers to the hiring and promotion of francophones have been overcome. French-speakers ceased to be underrepresented in the Public Service in 1978, and the percentage of public servants who are francophones has been growing steadily ever since. At the management level, French-speakers ceased to be under-represented in 1995. There is now a growing over-representation of persons with French as their first official language in the Public Service. In 2007, francophones occupied 31.5% of positions in the core public administration, and 26.9% in institutions subject to the \"Official Languages Act\", overall. Bilingualism is desirable for politicians; John Turner's fluency in both English and French, for example, helped him become leader of the Liberal Party and Prime Minister in 1984.\n\nIn 2007, positions in the core public administration broke down as follows:\n\n\"Bilingual\" - 40.2%\n\"English Essential\" - 51.2%\n\"French Essential\" - 4.0%\n\"English or French Essential\" - 4.4%\n\nIn his 2008-09 annual report, the Commissioner for Official Languages wrote, \"A vast majority of federal institutions have yet to create a workplace where their employees feel comfortable using either official language and are encouraged to do so.\" He identified the following problem areas:\n\n\nA lack of language proficiency on the part of some managers may contribute to the situation. The Commissioner wrote in his report: \"Some managers hinder the use of the minority language, either because they are not proficient enough in the language or because they hesitate to use it. Similarly, Francophones fearing that their professional contribution will not be fully recognized tend to work in English when their superiors do not use French daily and do not stress the importance of using it.\"\n\n"}
{"id": "26039801", "url": "https://en.wikipedia.org/wiki?curid=26039801", "title": "On a white bus", "text": "On a white bus\n\nOn a white bus is a mnemonic device used to commit subordinating conjunctions and relative pronouns of the English language to memory. Subordinating conjunctions and relative pronouns can start a subordinating clause or a clause that describes a noun. Some letters in this phrase stand for more than one subordinating conjunction.\n\nOther subordinating conjunctions : provided , provided that , since , so that , rather than\n"}
{"id": "18894210", "url": "https://en.wikipedia.org/wiki?curid=18894210", "title": "Open-ended question", "text": "Open-ended question\n\nAn open-ended question cannot be answered with a \"yes\" or \"no\" response, or with a static response. Open-ended questions are phrased as a statement which requires a response. The response can be compared to information that is already known to the questioner.\n\nExamples of open-ended questions:\n\nThe received wisdom in education is that open questions are broadly speaking ‘good’ questions. They invite students to give longer responses that demonstrate their understanding. They are preferable to closed questions (i.e. one that demands a yes/no answer) because they are better for discussions or enquiries, whereas closed questions are only good for knowledge testing. \n\nPeter Worley argues that this is a false assumption. This is based on Worley’s central arguments that there are two different kinds of open and closed questions: grammatical and conceptual. He argues that educational practitioners should be aiming for questions that are “grammatically closed, but conceptually open”. For example, in standard parlance, ‘is it ever right to lie?’ would be regarded as a closed question: it elicits a yes/no response. Significantly, however, it is conceptually open. Any initial yes/no answer to it can be ‘opened up’ by the questioner (‘why do you think that?,’ ‘Could there be an instance where that’s not the case?), inviting elaboration and enquiry. \n\nThis grammatically closed but cognitively open style of questioning, Worley argues, “gives [educators] the best of both worlds: the focus and specificity of a closed question (this, after all, is why teachers use them) and the inviting, elaborating character of an open question”. Closed questions, simply require ‘opening up’ strategies to ensure that conceptually open questions can fulfil their educational potential. \n\nWorley's structural and semantic distinction between open and closed questions is integral to his pedagogical invention 'Open Questioning Mindset', or OQM. OQM refers to the development, in educators, of an open attitude towards the process of learning and the questioning at the heart of that process. It is a mind-set that is applicable to all subject areas and all pedagogical environments. Teachers who develop an Open Questioning Mindset listen openly for the cognitive content of student’s contributions and looks for ways to use what is given for learning opportunities, whether right, wrong, relevant or apparently irrelevant. OQM encourages a style of pedagogy that values genuine enquiry in the classroom. It provides teachers with the tools to move beyond what Worley calls ‘guess what’s in my head’ teaching, that relies on closed and leading questions..\n\n"}
{"id": "10417456", "url": "https://en.wikipedia.org/wiki?curid=10417456", "title": "Orality", "text": "Orality\n\nOrality is thought and verbal expression in societies where the technologies of literacy (especially writing and print) are unfamiliar to most of the population. The study of orality is closely allied to the study of oral tradition. \n\nThe term “orality” has been used in a variety of ways, often to describe, in a generalised fashion, the structures of consciousness found in cultures that do not employ, or employ minimally, the technologies of writing. \n\nWalter J. Ong’s work was foundational for the study of orality, and reminds us that despite the striking success and subsequent power of written language, the vast majority of languages are never written, and the basic orality of language is permanent. \n\nIn his later publications Ong distinguishes between two forms of orality: 'primary orality' and 'secondary orality'. Primary orality is thought and expression un-touched by the culture of writing of print; \"secondary orality\" is explained by Ong as oral culture defined by (implicitly influenced) by the written and printed word, and includes oral culture made possible by technology such as a newscaster reading a news report on television.\n\nIn addition, 'residual orality' is also defined - it is the remnants, legacy, or influence of a predominately oral culture carried over into the written realm - an example might include the use of dialogue as a philosophical or didactic tool in written literature, such as used by the Greek thinker Plato.\n\nBefore writing became a way for many cultures before we had orality. Unfortunately many of the retained orality has been lost or drastically changed. Those that were able to be preserved gives us insight to past cultures and just how much we have evolved since then. In \"Orality and Literacy\" (2nd ed. ), Ong sums up his own work over the previous three decades as well as the work of numerous other scholars. With regard to oral tradition and primary orality he draws on pioneering work by Milman Parry, Albert B. Lord, and Eric A. Havelock. Marshall McLuhan was among the first to fully appreciate the significance of the Ong's earlier work about print culture and the written and printed word as a technology. In his work \"The Gutenberg Galaxy\" McLuhan quotes and discusses works by Ong in the 1950s regarding print culture.; Orality gave us the stepping stones that allowed us to get where we are today, it was a necessity for the growth of civilization. But using his own examples to amplify Ong's thought, McLuhan shows how each stage in the development of this technology throughout the history of communication – from the invention of speech (primary orality), to pictograms, to the phonetic alphabet, to typography, to the electronic communications of today – restructures human consciousness, profoundly changing not only the frontiers of human possibility, but even the frontiers it is possible for humans to imagine.\n\n'Primary orality' refers to thought and its verbal expression within cultures \"totally untouched by any knowledge of writing or print.\"\n\nAll sound is inherently powerful. If a hunter kills a lion he can see it, touch it, feel it and smell it. But if he hears a lion he must act, fast, because the sound of the lion signals its presence and its power. Speech is a form of sound that shares this common power. Like other sounds, it comes from within a living organism. A text can be ignored; it is just writing on paper. But to ignore speech can be unwise; our basic instincts compel us to pay attention.\n\nWriting is powerful in a different way: it permits people to generate ideas, store them, and retrieve them as needed across time in a highly efficient and accurate way. The absence of this technology in oral societies limits the development of complex ideas and the institutions that depend on them. Instead, sustained thought in oral settings depends on interpersonal communication, and storing complex ideas over a long period of time requires packaging them in highly memorable ways, generally by using mnemonic tools.\n\nIn his studies of the Homeric Question, Milman Parry was able to show that the poetic metre found in the \"Iliad\" and the \"Odyssey\" had been 'packaged' by oral Greek society to meet its information management needs. These insights first opened the door to a wider appreciation of the sophistication of oral traditions, and their various methods of managing information. Later, ancient and medieval mnemonic tools were extensively documented by Frances Yates in her book \"The Art of Memory\".\n\n‘Residual orality’ refers to thought and its verbal expression in cultures that have been exposed to writing and print, but have not fully ‘interiorized’ (in McLuhan’s term) the use of these technologies in their daily lives. As a culture interiorizes the technologies of literacy, the ‘oral residue’ diminishes.\n\nBut the availability of a technology of literacy to a society is not enough to ensure its widespread diffusion and use. For example, Eric Havelock observed in \"A Preface to Plato\" that after the ancient Greeks invented writing they adopted a scribal culture that lasted for generations. Few people, other than the scribes, considered it necessary to learn to read or write. In other societies, such as ancient Egypt or medieval Europe, literacy has been a domain confined to political and religious elites.\n\nMany cultures have experienced an equilibrium state in which writing and mass illiteracy have co-existed for hundreds or even thousands of years.\n\nOral residue rarely disappears quickly and never vanishes completely. Speech is inherently an oral event, based on human relationships, unlike texts. Oral societies can mount strong resistance to literate technologies, as vividly shown in the arguments of Socrates against writing in Plato's \"Phaedrus\". Writing, Socrates argues, is inhuman. It attempts to turn living thoughts dwelling in the human mind into mere objects in the physical world. By causing people to rely on what is written rather than what they are able to think, it weakens the powers of the mind and of memory. True knowledge can only emerge from a relationship between active human minds. And unlike a person, a text can’t respond to a question; it will just keep saying the same thing over and over again, no matter how often it is refuted.\n\nThe Canadian communications scholar, Harold Innis argued that a balance between the spoken word and writing contributed to the cultural and intellectual vitality of ancient Greece in Plato's time. Plato conveyed his ideas by writing down the conversations of Socrates thus \"preserving the power of the spoken word on the written page.\" Aristotle, Innis wrote, regarded Plato's style as \"halfway between poetry and prose.\" Plato was able to arrive at new philosophical positions \"through the use of dialogues, allegories and illustrations.\"\n\nFurthermore, as McLuhan emphasizes, modernization attenuates some oral capabilities. For example, in medieval Europe silent reading was largely unknown. This tilted the readers' attention towards the poetic and other auditory aspects of the text. Educated modern adults may also occasionally long for something like \"the capacious medieval memory, which, untrammeled by the associations of print, could learn a strange language with ease and by the methods of a child, and could retain in memory and reproduce lengthy epic and elaborate lyric poems.\"\nMcLuhan and Ong also document the apparent re-emergence, in the electronic age, of a kind of 'secondary orality' that displaces written words with audio/visual technologies like radio, telephones, and television. Unlike primary oral modes of communication, these technologies depend on print for their existence. Mass Internet collaborations, such as Wikipedia, rely primarily on writing, but re-introduce relationships and responsiveness into the text.\n\nIt has been a habit of literate cultures to view oral cultures simply in terms of their lack of the technologies of writing. This habit, argues Ong, is dangerously misled. Oral cultures are living cultures in their own right. A 1971 study found that of 3,000 extant languages, only 78 had a written literature. While literacy extends human possibilities in both thought and action, all literate technologies ultimately depend on the ability of humans to learn oral languages and then translate sound into symbolic imagery.\n\nUnderstanding between nations may depend to some degree on understanding oral culture. Ong argues that \"many of the contrasts often made between 'western' and other views seem reducible to contrasts between deeply interiorized literacy and more or less residually oral states of consciousness.\"\n\nDrawing on hundreds of studies from anthropology, linguistics and the study of oral tradition, Ong summarizes ten key aspects of the 'psychodynamics of orality'. While these are subject to continuing debate, his list remains an important milestone. Ong draws his examples from both primary oral societies, and societies with a very high 'oral residue'.\n\nTo retain complex ideas requires that they be packaged memorably for easy recall.\n\nAnthropologist Marcel Jousse identifies a close linkage between rhythm and breathing patterns, gestures and the bilateral symmetry of the human body in several ancient verse traditions. This synergy between the body and the construction of oral thought further fuels memory.\n\nOral cultures avoid complex 'subordinative' clauses. Ong cites an example from the Douay-Rheims version of Genesis (1609–10), noting that this basic additive pattern (in \"italics\") has been identified in many oral contexts around the world:\n\nDemonstrating how oral modes of communication tend to evolve into literate ones, Ong additionally cites the New American Bible (1970), which offers a translation that is grammatically far more complex:\n\nOral expression brings words together in pithy phrases that are the product of generations of evolution: the 'sturdy oak tree', the 'beautiful princess' or 'clever Odysseus'. This does not apply specifically to poetry or song; rather the words are brought together out of habit during general communication. 'Analyzing' or breaking apart such expressions is risky: they represent the work of generations and \"there is nowhere outside the mind to store them.\"\n\nOng cites an American example, noting that in some parts of the United States with heavy oral residue, until the early twentieth-century it was still considered normal or even obligatory to use the adjective ‘glorious’ when referring to the 'Fourth of July'.\n\nSpeech that repeats earlier thoughts or thought-pictures, or shines a different light on them somehow, helps to keep both the speaker and the listener focused on the topic, and makes it easier for all to recall the key points later. \"Oral cultures encourage fluency, fulsomeness, volubility. Rhetoricians were to call this \"\n\nBecause oral societies have no effective access to writing and print technologies, they must invest considerable energy in basic information management. Storage of information, being primarily dependent on individual or collective recall, must be handled with particular thrift. It is possible to approximately measure oral residue \"from the amount of memorization the culture's educational procedures require.\"\n\nThis creates incentives to avoid exploring new ideas and particularly to avoid the burden of having to store them. It does not prevent oral societies from demonstrating dynamism and change, but there is a premium on ensuring that changes cleave to traditional formulas, and \"are presented as fitting the traditions of the ancestors.\"\n\nOral cultures take a practical approach to information storage. To qualify for storage, information must usually concern matters of immediate practical concern or familiarity to most members of the society.\n\nLong after the invention of writing, and often long after the invention of print, basic information on how to perform a society’s most important trades was left unwritten, passed from one generation to the next as it always had been: through apprenticeship, observation and practice.\n\nBy contrast, only literary cultures have launched phenomenological analyses, abstract classifications, ordered lists and tables, etc. Nothing analogous exists in oral societies.\n\n'Agonistic' means 'combative', but Ong actually advances a deeper thesis with this point. Writing and to an even greater extent print, he argues, disengage humans from direct, interpersonal struggle.\n\nProducts of \"the highly polarized, agonistic, oral world of good and evil, virtue and vice, villains and heroes,\" the great works of oral literature from Homer to Beowulf, from the Mwindo epic to the Old Testament, are extremely violent by modern standards. They are also punctuated by frequent and intense intellectual combat and tongue-lashings on the one hand, and effusive praise (perhaps reaching its height among African praise singers) on the other.\n\nIn an oral culture the most reliable and trusted technique for learning is to share a \"close, empathetic, communal association\" with others who know.\n\nOng cites a study of community decision-making from 12th Century England. Writing already had a long history in England, and it would have been possible to use texts to establish for example, the age of majority of the heir to an estate. But people were skeptical about texts, noting not only the cost of generating and managing them, but the problems involved in preventing tampering or frauds.\n\nAs a result, they retained the traditional solution: gathering together \"mature wise seniors of many years, having good testimony\", and publicly discussing the age of the heir with them, until agreement was reached. This hallmark principle of orality, that truth emerges best from communal process, resonates today in the jury system.\n\nOral societies conserve their limited capacity to store information, and retain the relevance of their information to the interest of their present members, by shedding memories that have lost their past significance.\n\nWhile many examples exist, the classic example was reported by . Written records prepared by the British in Ghana in the early 1900s show that Ndewura Jakpa, the seventeenth century founder of the state of the Gonja people, had seven sons, each of whom ruled a territorial division within the state. Six decades later two of the divisions had disappeared for various reasons. The myths of the Gonja had been revised to recount that Jakpa had five sons, and that five divisions were created. Since they had no practical, present purpose, the other two sons and divisions had evaporated.\n\nIn oral cultures, concepts are used in a way that minimizes abstraction, focusing to the greatest extent possible on objects and situations directly known by the speaker. A study by Alexander Luria, a psychologist who did extensive fieldwork comparing oral and literate subjects in remote areas of Uzbekistan and Kirghizia in 1931–2 documented the highly situational nature of oral thinking.\n\n\n\n"}
{"id": "4221065", "url": "https://en.wikipedia.org/wiki?curid=4221065", "title": "Passive speaker (language)", "text": "Passive speaker (language)\n\nA passive speaker (also referred to as a receptive bilingual or passive bilingual) is a category of speaker who has had enough exposure to a language in childhood to have a native-like comprehension of it, but has little or no active command of it. Such passively fluent individuals are often raised in an environment where the language was spoken but did not become native speakers.\n\nSuch speakers are especially common in language shift communities where speakers of a declining language do not acquire active competence. Around 10% of the Ainu people who speak the language are considered passive speakers. Passive speakers are often targeted in language revival efforts to increase the number of speakers of a language quickly, as they are likely to gain active and near-native speaking skills more quickly than those with no knowledge of the language. They are also found in areas where people grow up hearing another language outside their family with no formal education.\n\nA \"passive language\" is a related term used in interpreting or translating. It is the language or languages from which the interpreter works. For example if an interpreter's job is to translate from German, Dutch and Swedish into French, then French is the active language while the others are passive.\n\nA more common term for the phenomenon is 'passive bilingualism'. Grosjean argues that there has been a monolingual bias regarding who is considered a 'bilingual' in which people who do not have equal competence in all their languages are judged as not speaking properly. 'Balanced bilinguals' are, in fact, very rare. One's fluency of a bilingual in a languages is domain-specific: it depends on what each language is used for. That means that speakers may not admit to their fluency in their passive language although there are social (extralinguistic) factors that underlie their different competencies.\n\nKarlos Cid Abasolo discusses that passive bilingualism would be a minimum requirement for the co-official status of Basque and Spanish to become a working reality. As there are now many monolingual Spanish-speakers, and no monolingual Basque-speakers in the Basque Country, there is little reason for those fluent in Basque to speak it, regardless of the domain, circumstance or interlocutor.\n\n\nAAIC Glossary: Passive language\n"}
{"id": "1872910", "url": "https://en.wikipedia.org/wiki?curid=1872910", "title": "Performative contradiction", "text": "Performative contradiction\n\nA performative contradiction () arises when the propositional content of a statement contradicts the presuppositions of asserting it. An example of a performative contradiction is the statement \"I am dead\" because the very act of proposing it presupposes the actor is alive.\n\nSolipsism is often held to be a performative contradiction if stated. If not stated it is usually considered an example of the normative application of \"qui tacet consentire videtur carborundum\" (He who remains silent vigorously consents).\nJürgen Habermas and related philosophers point out that statements spoken during justificatory argumentation carry additional presuppositions and so certain statements are performative contradictions in this context. Habermas claims that post-modernism's epistemological relativism suffers from a performative contradiction. Hans-Hermann Hoppe claims in his theory of argumentation ethics that arguing for any political position other than free-market anarchism results in a performative contradiction.\n\nJaakko Hintikka more rigorously fleshed out the notion of performative contradiction in analyzing Descartes' famous \"cogito ergo sum\" argument, concluding that \"cogito ergo sum\" relies on performance rather than logical inference.\n\n\n"}
{"id": "18356461", "url": "https://en.wikipedia.org/wiki?curid=18356461", "title": "Production support", "text": "Production support\n\nProduction support is the practices and disciplines of supporting the IT systems/applications which are currently being used by the end users. A production support person/team is responsible for receiving incidents and requests from end-users, analyzing these and either responding to the end user with a solution or escalating it to the other IT teams. These teams may include developers, system engineers and database administrators.\n\nIn order to understand the importance of production support, one needs to take a few factors into account.\n\nFrom the factors listed above, one can see that the way in which production support is managed is extremely crucial.\n\nThe major steps for Production Support are as below. These Production Support steps are in context of the Batch processing.\n\nUsually a batch job or group of related batch jobs (schedule/stream) runs to accomplish one or more business functions. These batch jobs run unattended and normally complete without any errors or issues. However, sometimes the batch job can have a break/interruption/abend/abort. There could be several reasons why a job could abend.\n\nWhen a job abends, it can send out an automated alert notification via e-mail, page, text. Also, data center or operations team is also actively monitoring the jobs. They also send alert notification using e-mail, page, text or they can call the on call person responsible for the recovery of the abended job.\n\nThe on call person acknowledges the e-mail, page, text or phone call for the abended job. The on call person also records the abended job details in a production issue tracking system. Sometimes, the abended job automatically records the job abend details along with job standard list (job log) in a production issue tracking system. The abended job details (job standard list, error log files, etc.) are available in the production job scheduler tool. The Production issue tracking tool creates a request number and this request number is given to the support team. This request number is used to track the progress of the production support issue. The request is assigned to on call support team person.\n\nFor critical Production Errors (e.g. Production job is in critical path and is likely to delay the batch completion SLAs and if the Production error is impacting business data), an e-mail is sent to entire organization or impacted teams so that they are aware of the issue. They are also provided with the estimated time for Production error recovery.\n\nThe Production support team on call person collects all the necessary information about the Production error. This information is then recorded in the Production error tracking tool using the correct support request number previously assigned. All the details such as data, environment, process, program logic that failed is used in the investigation. Production batch job, program used or any tool/utility used is reviewed for any possible errors.\n\nIf similar Production error occurred in the past then the issue resolution steps are retrieved from the support knowledge base and error is resolved using those steps. If it is a new Production error then new Production error resolution steps are created and Production error is resolved. The new Production error resolution steps are recorded in the knowledge base for the future usage. For major Production errors (critical infrastructure or application failures), a phone conference call is initiated and all required support persons/teams join the call and they all work together to resolve the error. This is also called as an Incident Management. If a problem occurs repeatedly then it is recorded and tracked using appropriate tools and processes until it is resolved permanently. This is also called as Problem Management. The issue is closed only after the customer or end user agrees that the problem is resolved.\n\nIf the Production error occurred due to programming errors then a request is created for the Development team to correct programming errors. Problem is identified, defined and root cause analysis is performed. The programming error is fixed using normal SDLC process - analysis/design/programming/QA/testing/release. The new version of the Production job/program is deployed and verified/validated.\n\nIf the Production error occurred due to job/schedule dependency issues or sequence issues then further analysis is done to find the correct sequence/dependencies. The new sequence/dependencies are verified and validated in test environment before Production deployment.\n\nIf the Production error occurred due to infrastructure issues then the specific infrastructure team is notified. The infrastructure team then implements permanent fix for the issue and monitors the infrastructure to avoid same error again.\n\nIf the Production error occurred due to unexpected consequences of infrastructure changes then most often the infrastructure team is not able to bill the time spent in resolving of the issue at the full rate. In some cases hours are completely disqualified from being billed.\n\nThe Production error tracking system is used to review all issues periodically (daily, weekly and monthly) and reports are generated to monitor resolved issues, repeating issues, pending issues. Reports are also generated for the IT/IS management for improvement and management of Production jobs.\n\n\n"}
{"id": "54602557", "url": "https://en.wikipedia.org/wiki?curid=54602557", "title": "R-Principle", "text": "R-Principle\n\nIn the Neo-Gricean approach to semantics and pragmatics advanced by Yale linguist Laurence Horn, the R-Principle (\"R\" for \"Relation\") is a reformulation of Paul Grice's Maxim of Relation (\"see\" Gricean maxims) combining with the second sub-maxim of Quantity and the third and fourth sub-maxims of Manner. The R-Principle states: \"Say no more than you must (given Q).\" As such it interacts with the Q-principle, which states: \"Say as much as you can (given R).\" \n\nAccording to the R-Principle, there is no reason to make a stronger statement (say more) if the extra information can be contributed by implicature. For instance, the inference from \"He broke a finger\" to \"He broke a finger of his own\" is an R-based inference, i.e. deriving from the R-Principle, since the economy of expression implies that a more informative statement was not needed. \n"}
{"id": "54540850", "url": "https://en.wikipedia.org/wiki?curid=54540850", "title": "Ronde script (calligraphy)", "text": "Ronde script (calligraphy)\n\nRonde (\"round\" in French) is a kind of script in which the heavy strokes are nearly upright, giving the characters when taken together a round look.\n\nRonde script appeared in France at the end of the 16th century, and was popularized by writing masters such as Louis Barbedor in the 17th century. In its original form, it borrowed some of its letterforms from the round Gothic style. This style of writing was still in use (with some modifications) until the 20th century because it was used in French school manuals to teach the bases of cursive writing. It was also commonly used by the scribes of the French Ministry of Finance until right after World War II, which gave this style the name of \"écriture ronde finnancière\" (\"round financial writing\", not to be confused with the \"financière\" writing style).\n\nThe classic French rondes where also very present in the work of 18th century type founder and calligrapher Nicholas Gando, which has been revived for the digital medium by way of the French 111 font.\n"}
{"id": "1839894", "url": "https://en.wikipedia.org/wiki?curid=1839894", "title": "Sananmuunnos", "text": "Sananmuunnos\n\nSananmuunnos (\"Word transformation\"), sometimes kääntösana, is a sort of verbal play in the Finnish language, similar to spoonerisms in English. \n\nSpecial to Finnish is a narrow phoneme inventory and vowel harmony. As Finnish is a mora-divided language, it is morae that are exchanged, not syllables (often a mora is also a syllable in a Finnish word, but not always). Also, Finnish inflectional and derivational morphology is extensive, thus applying a suffix from another word often produces a valid word. This leads to large number of possible spoonerisms. Much of practical \"sananmuunnos\" wordplay revolves around obscene double entendre expressed by spoonerism.\n\nSeveral books have been written. Some have whole stories with multiple puns in one sentence, for example. Most have a \"vocabulary\" in the back, usually a hundred or more word pairs long.\n\nInitial morae of two adjacent words are exchanged, which is spoonerism by definition.\n\nThe \"extra length\" of a long vowel is a full mora, and thus stays in its original position, making the new vowel long.\n\nIf necessary, stilted diphthongs are converted into allowed diphthongs as per phonotactics. The first vowel is the determinant for choosing the diphthong. The process preserves opening and closing diphthongs, e.g. the opening 'ie' is reflected as an opening 'uo'. \n\nIf necessary, vowel harmony is applied. As per vowel harmony, the initial syllable controls the kind of vowel selected.\nThat is, transformation is A, U, O into Ä, Y, Ö, if the former do not begin the word. Notice that information may be lost in this step, making it irreversible.\n\nIt is possible (although not accepted by some \"orthodox\") to exchange only the initial consonants, if that is the only way to get a sensible result. E.g. palasokeri [p-ala/s-okeri] 'sugar lump' → salapokeri [s-ala/p-okeri] 'playing poker in secret' (*solapakeri would not mean anything).\n\nTypically presented, spoonerisms are a kind of double entendre. Appropriately, the very term \"sananmuunnos\" is one; it becomes \"munansaannos\", which can be understood as \"small yield of penis\".\n\n\n"}
{"id": "8722168", "url": "https://en.wikipedia.org/wiki?curid=8722168", "title": "Terminology extraction", "text": "Terminology extraction\n\nTerminology extraction (also known as term extraction, glossary extraction, term recognition, or terminology mining) is a subtask of information extraction. The goal of terminology extraction is to automatically extract relevant terms from a given corpus.\n\nIn the semantic web era, a growing number of communities and networked enterprises started to access and interoperate through the internet. Modeling these communities and their information needs is important for several web applications, like topic-driven web crawlers, web services, recommender systems, etc. The development of terminology extraction is also essential to the language industry.\n\nOne of the first steps to model the knowledge domain of a virtual community is to collect a vocabulary of domain-relevant terms, constituting the linguistic surface manifestation of domain concepts. Several methods to automatically extract technical terms from domain-specific document warehouses have been described in the literature.\n\nTypically, approaches to automatic term extraction make use of linguistic processors (part of speech tagging, phrase chunking) to extract terminological candidates, i.e. syntactically plausible terminological noun phrases, NPs (e.g. compounds \"credit card\", adjective-NPs \"local tourist information office\", and prepositional-NPs \"board of directors\" - in English, the first two constructs are the most frequent). Terminological entries are then filtered from the candidate list using statistical and machine learning methods. Once filtered, because of their low ambiguity and high specificity, these terms are particularly useful for conceptualizing a knowledge domain or for supporting the creation of a domain ontology or a terminology base. Furthermore, terminology extraction is a very useful starting point for semantic similarity, knowledge management, human translation and machine translation, etc.\n\nThe methods for terminology extraction can be applied to parallel corpora. Combined with e.g. co-occurrence statistics, candidates for term translations can be obtained. Bilingual terminology can be extracted also from comparable corpora (corpora containing texts within the same text type, domain but not translations of documents between each other).\n\n"}
{"id": "27404794", "url": "https://en.wikipedia.org/wiki?curid=27404794", "title": "The Interpreter (Kim novel)", "text": "The Interpreter (Kim novel)\n\nThe Interpreter (2003) is Suki Kim’s first novel. In \"The Interpreter\", Kim creates a twenty-nine-year-old Korean American court interpreter named Suzy Park who makes a startling and ominous discovery during one court case which ultimately reveals the mystery of her parents' homicide. The award winning novel, mainly a murder mystery, breaks through the stereotypical images of the happy immigrant experience with a story of pain, loss, and murder.\n\nSuzy Park is a young, attractive, and achingly alone Korean American woman who works as a court interpreter for the New York City court system. She has had two rocky relationships with married men, worked a series of unsatisfying jobs, and cut ties with her family before her parents were shot dead in an unsolved double murder. The life she has chosen as an interpreter is a reflection of Suzy's searching for her own identity and trying to bridge the two cultures to both of which she feels a detachment. During one court case she discovers that her parents were not murdered by random violence, as the police had indicated, but instead had been shot by political enemies. The discovery provides the glint of a new lead for Park, and the novel tracks her investigation into what really happened, which ultimately reveals the mystery of her parents' homicide.\n\n\n\n"}
{"id": "11270807", "url": "https://en.wikipedia.org/wiki?curid=11270807", "title": "Truth in Translation", "text": "Truth in Translation\n\nTruth in Translation is a stage play conceived and directed by Michael Lessac, with music by Hugh Masekela. It tells the story of the interpreters at South Africa's Truth and Reconciliation Commission. \n\nThe play was written in a collaboration between the interpreters who worked at the TRC, writer Paavo Tom Tammi and the company of South African actors. It premiered in Rwanda, has toured South Africa and is touring to international conflict zones such as Northern Ireland, Sierra Leone, the Balkans, Jerusalem/Ramallah, Sri Lanka, Peru, and Indonesia/Timor to tell the story of the South African experience. The project includes workshops with audiences, exhibitions (The Forgiveness Project and Jillian Edelstein's \"Truth and Lies\") and filming of the interaction between audiences and the company, and attempts to provoke a global dialogue around notions of healing and reconciliation.\n\n"}
{"id": "31143541", "url": "https://en.wikipedia.org/wiki?curid=31143541", "title": "Visual rhetoric and composition", "text": "Visual rhetoric and composition\n\nIn the field of composition studies, the place of visual rhetoric is often uncertain or contested. Proponents of its inclusion in composition typically point to the increasingly visual nature of society, and the increasing presence of visual texts. Literacy, they argue, can no longer be limited only to written text and must also include an understanding of the visual. The exact role of visual rhetoric in the composition classroom, however, is unclear.\n\nDespite this focus on new media, the inclusion of visual rhetoric in composition studies is distinct from a media theory of composition, though the two are obviously related. Visual rhetoric focuses on the rhetorical nature of all visual texts while new media tends to focus on electronic mediums.\n\n"}
