{"id": "5488986", "url": "https://en.wikipedia.org/wiki?curid=5488986", "title": "Alphabetic principle", "text": "Alphabetic principle\n\nAccording to the alphabetic principle, letters and combinations of letters are the symbols used to represent the speech sounds of a language based on systematic and predictable relationships between written letters, symbols, and spoken words. The alphabetic principle is the foundation of any alphabetic writing system (such as the English variety of the Roman alphabet), which is one of the more common types of writing systems in use today.\n\nAlphabetic writing systems that use an (in practice) almost perfectly phonemic orthography have a single letter for each individual speech sound and a one-to-one correspondence between sounds and the letters that represent them. Such systems are used, for example, in the modern languages Serbian (arguably, an example of perfect phonemic orthography), Macedonian, Estonian, Finnish, Italian, Romanian, Spanish, Georgian, Hungarian and Turkish. Such languages have a straightforward spelling system, enabling a writer to predict the spelling of a word given its pronunciation and similarly enabling a reader to predict the pronunciation of a word given its spelling. Ancient languages with such almost perfectly phonemic writing systems include Avestic, Latin, Tamil, Vedic, and Sanskrit (Devanāgarī— an abugida; see Vyakarana). On the other hand, French and English have a strong difference between sounds and symbols.\n\nThe alphabetic principle does not underlie logographic writing systems like Chinese or syllabic writing systems such as Japanese kana. Korean, along with Chinese and Japanese, is a member of the CJK group and shares origins for many of the symbols. Hangul, the Korean writing system, is actually strongly alphabetic while it may look logographic or syllabic to outsiders.\n\nMost orthographies that use the Latin writing system are imperfectly phonological and diverge from that ideal to a greater or lesser extent. This is because the ancient Romans designed the alphabet specifically for Latin. In the Middle Ages, it was adapted to the Romance languages, the direct descendants of Latin, as well as to the Celtic, Germanic, Baltic, and some Slavic languages, and finally to most of the languages of Europe.\n\nEnglish orthography is based on the alphabetic principle, but the acquisition of sounds and spellings from a variety of languages has made English spelling patterns confusing. Spelling patterns usually follow certain conventions but nearly every sound can be legitimately spelled with different letters or letter combinations. For example, the letters \"ee\" almost always represent , but the sound can also be represented by the letter \"y\" or the letters \"ie\".\n\nThe spelling systems for some languages, such as Spanish, are relatively simple because they adhere closely to the ideal one-to-one correspondence between sounds and the letter patterns that represent them. In English the spelling system is more complex and varies considerably in the degree to which it follows the stated pattern. There are several reasons for this, including: first, the alphabet has 26 letters, but the English language has 40 sounds that must be reflected in word spellings; second, English spelling began to be standardized in the 15th century, and most spellings have not been revised to reflect the long-term changes in pronunciation that are typical for all languages; and third, English frequently adopts foreign words without changing the spelling of those words.\n\nLearning the connection between written letters and spoken sounds has been viewed as a critical heuristic to word identification for decades. Understanding that there is a direct relationship between letters and sounds enables an emergent reader to decode the pronunciation of an unknown written word and associate it with a known spoken word. Typically, emergent readers identify the majority of unfamiliar printed words by sounding them out. Similarly, understanding the relationship of letters and sounds is also seen as a critical heuristic for learning to spell.\n\nTwo contrasting philosophies exist with regard to emergent readers learning to associate letters to speech sounds in English. Proponents of phonics argue that this relationship needs to be taught explicitly and to be learned to automaticity, in order to facilitate the rapid word recognition upon which comprehension depends. \nOthers, including advocates of whole-language who hold that reading should be taught holistically, assert that children can naturally intuit the relationship between letters and sounds.\n\n\n"}
{"id": "52715832", "url": "https://en.wikipedia.org/wiki?curid=52715832", "title": "Anglophone problem (Cameroon)", "text": "Anglophone problem (Cameroon)\n\nThe Anglophone Problem, as it is commonly referred to in Cameroon, is a socio-political issue rooted in Cameroon's colonial legacies from the Germans, British, and the French.\n\nThe issue classically and principally opposes many Cameroonians from the Northwest and Southwest regions, many of whom consider themselves anglophones, to the Cameroon government. This is based on the fact that these two regions (formally British Southern Cameroons) were controlled by Britain as a mandated and trust territory of the League of Nations and the United Nations respectively.\" \n\nWhile many Northwesterners and Southwesterners believe there is an anglophone problem, some do not. In fact, the term \"anglophone\" today creates a lot of controversy, as many former French-speaking Cameroonians who are either bilingual or speak only English (most of whom have gone through the English sub-system of education) consider themselves as anglophones. The root of the Anglophone problem in Cameroon can be traced back to the Foumban Conference of 1961 that united the two territories, with different colonial legacies, into one state. The Anglophone Problem is increasingly dominating the political agenda of Cameroon. This problem has led to arguments and actions (protests, strikes, etc.) that argue for federalism or separation from the union by the Anglophones. Failure to address the Anglophone Problem threatens Cameroon's ability to create national unity between the two groups of people.\n\nThe roots of the Anglophone problem can be traced back to World War I, when Cameroon was known as German Kamerun. The German Empire first gained influence in Cameroon in 1845 when Alfred Saker of the Baptist Missionary Society introduced a mission station. In 1860, German merchants established a factory: the Woermann Company. On July 5, 1884, local tribes provided the Woermann Company with rights to control the Kamerun River, consequently setting the foundation for the later German colonization of Kamerun. In 1916, during World War I, France and Britain joined forces to attack and seize German Kamerun. Later, the Treaty of Versailles would award France and Britain mandates over Cameroon as punishment of the Germans who lost the war. Most of German Kamerun was given to the French, over 167,000 square miles of territory. The British were given Northern Cameroons, about 17,500 square miles of territory and Southern Cameroons, 16,580 square miles. Each colonizer would later influence the colonies with their European languages and cultures, thus rendering them as Anglophones and Francophones. The large difference in awarded territory has resulted in present-day Cameroon having a huge majority Francophone population and a very small minority Anglophone population.\n\nFollowing World War II, a wave of independence flowed rapidly throughout Africa. The United Nations obliged that Britain and France relinquish their colonies and guide them towards independence. There were three political options for British Southern Cameroons. They could become independent by uniting with Nigeria or with French Cameroun. No option of self-determination by becoming independent was given. The most desired option was independence with the least popular being unification with French Cameroun. However, during the British Plebiscite of 1961, the British argued that Southern Cameroons was not economically viable enough to sustain itself as an independent nation and could only survive by joining with Nigeria or La République du Cameroun (the Republic of Cameroon). Though documents on the United Nations' \"Non-Self-Governing Territories\" state, \"integration should be the result of the freely expressed wishes of the territory's peoples\", the United Nations would later reject Southern Cameroons' appeal to have independence as a sovereign nation placed on the ballot. The plebiscite questions were: \nThe United Nations documents defined the basis of integration as: \"Integration with an independent State should be on the basis of complete equality between the peoples of the erstwhile Non-Self-Governing Territory and those of the independent country with which it is integrated. The peoples of both territories should have equal status and rights of citizenship... at all levels in the executive, legislative and judicial organs of government.\" With this promise in mind, on February 1961, British Northern Cameroons voted to join Nigeria, while British Southern Cameroons voted to join La République du Cameroun.\n\nThe purpose of the Foumban Constitutional Conference was to create a constitution for the new Federal state of British Southern Cameroon and La République du Cameroun. The conference brought together representatives from La République du Cameroun, including Amadou Ahidjo, their president, with representatives from Southern Cameroons. Two weeks before the Foumban Conference, there were reports that more than one hundred people were killed by terrorists in Loum, Bafang, Ndom, and Douala. The reports worried unification advocates who wanted British Cameroon to unify with French Cameroun. For the conference, the location of Foumban had been carefully chosen to make Ahidjo, appear as if he had everything under control. Mr. Mbile, a Southern Cameroonian representative at the conference noted, \"Free from all the unrest that had scared Southern Cameroonians, the Francophone authorities had picked the place deliberately for the occasion. The entire town had been exquisitely cleaned up and houses splashed with whitewash. Food was good and receptions lavish. The climate in Foumban real or artificial went far to convince us that despite the stories of 'murder and fire,' there could be at least this island of peace, east of the Mungo.\"\n\nBefore the Foumban Conference, all the parties in Southern Cameroons, the Native Authority Councils and the traditional leaders attended the Bamenda Conference. This conference decided on a common proposal to present when negotiations with La République du Cameroun arrived. Among many things, the Bamenda Conference agreed on a non-centralized federation to ensure there was a distinction between the powers of the states and the powers of the federation. Most of the proposals from the Bamenda Conference were ignored by Ahidjo. Some of these proposals included having a bicameral legislature and decentralizing power, but instead a unicameral system was established with a centralized system of power.\n\nAt the Foumban conference, Ahidjo presented delegates with a draft constitution. By the end of the conference, instead of creating an entirely new constitution, the contributions of the Southern Cameroons delegates were reflected in suggestions made to the draft initially presented to them. John Ngu Foncha and Ahidjo intended for the Foumban Constitutional Conference to be brief, however delegates left the three day conference with the impression that there would be sequential conferences to continue the drafting of the constitution. Mbile later noted, \"We may have done more if we had spent five months instead of five days in writing our constitution at Foumban.\" The Constitution for the new Federal Republic was agreed in Yaoundé in August 1961, between Ahidjo and Foncha, pending approval by the House of Assembly of the two states. In the end, the West Cameroon House of Assembly never ratified the Constitution. However, on October 1, 1961, the Federal Republic of Cameroon nevertheless came to fruition.\n\nOn May 6, 1972, Ahidjo announces his decision to convert the Federal Republic into a unitary state, on the provision that the idea was supported via referendum. This suggestion violated the articles in the Foumban document that read: 'any proposal for the revision of the present constitution, which impairs the unity and integrity of the Federation shall be inadmissible,' and 'proposals for revision shall be adopted by simple majority vote of the members of the Federal Assembly, provided that such majority includes a majority of the representatives ... of each of the Federated States,'... not through referendum. Such violations easily allowed for the passing of the referendum that turned the Federal Republic into the United Republic of Cameroon. Taking into account these actions, the evidence shows that the Francophone's intentions may have not been to form a federal state, but rather to annex Southern Cameroons and not treat them as equals. In 1984, Ahidjo's successor, Paul Biya, replaced the name \"United Republic of Cameroon\" with \"La République du Cameroun,\" the same name the francophone Cameroon had before federation talks. With changes in the Constitution of 1996, reference to the existence of a territory called the British Southern Cameroons that had a \"functioning self-government and recognized international boundaries\" was essentially erased.\n\nDespite the non-acknowledgement/denial of the Anglophone problem from Francophone government leaders, there exists a discontent by Anglophones, both young and old, as to how Anglophones are treated. This discontent presents itself in calls for federation or separation with movements that are garnering strength. At the core of Anglophone grievances is the loss of the former West Cameroons as a \"distinct community defined by differences in official language and inherited colonial traditions of education, law, and public administration.\" On 22 December 2016, in a letter to Paul Biya, the Anglophone Archbishops of Southern Cameroons define the Anglophone problem as follows:\n\nMovements which advocate the separation of English-speaking Cameroon from French-speaking Cameroun exist, led by the Cameroon Action Group, the Southern Cameroons Youth League, the Southern Cameroons National Council, the Southern Cameroon Peoples Organization and the Ambazonia Movement.\n\nAdvocates of Federation want a return to the constitution agreed upon in the 1961 Foumban Conference that acknowledges the history and culture of the two regions while giving equal power to the two. This federation had been dismantled on 20 May 1972 by the larger French-speaking Cameroon and extended the latter's executive power throughout West Cameroon. Federation advocates include the instrumental Consortium of the leaders of three Cameroon-based trade unions: Lawyers, Teachers, and Transporters. It also includes some Cameroonians in the diaspora led by a well organized US-based Anglophone Action Group, Inc. (AAG). AAG was one of the first groups in the diaspora to endorse the Cameroon-based Consortium as a peaceful alternative to achieving a return to the pre-1972 federated system. Opponents of federation include the ruling Cameroon Peoples Democratic Movement.\n\nUnitarianism do not want Federation or Separation, but rather a decentralized unitary government; whereas, now the government is highly centralized in power. This violates the tenets of the 1996 Constitution as decentralization has yet to be implemented.\n\nIn March 1990, the Social Democratic Front (SDF) led by John Fru Ndi, was founded on the perception of widespread Anglophone alienation. The SDF was the first major opposition party to the People's Democratic Movement, led by Paul Biya.\n\nBelow are various reasons that Anglophones feel marginalized, systemically, by the government. \n\n\n, the Anglophone problem is still on-going. It has spiraled into violence with police officers and gendarmes shooting dead several civilians. Official sources have put the number at 17 dead, but local individuals and groups have talked of 50 or more. Radical members of some secessionist groups have killed several police officers and gendarmes. 15,000 refugees have fled Southern Cameroons into neighboring Nigeria, with the UNHCR expecting that number to grow to 40,000 if the situation continues.\n\nWithout clearly acknowledging the existence of the Anglophone problem, the President of Cameroon has attempted to appease tensions by making a number of announcements:\nSeveral separatist or secessionist groups have emerged or become more prominent as a result of the harsh response by the government to the Anglophone problem. These groups desire to see Southern Cameroons completely separate from \"La République du Cameroun\" and form its own state, sometimes referred to as \"Ambazonia\". Some groups such as the \"Southern Cameroon Ambazonia United Front\" (SCACUF) are using diplomatic means in an attempt to gain independence for the Anglophone regions, whereas other groups have begun to employ armed confrontation with artisan weapons against the deployed gendarmes and soldiers in those regions.\n\n"}
{"id": "406551", "url": "https://en.wikipedia.org/wiki?curid=406551", "title": "Applied linguistics", "text": "Applied linguistics\n\nApplied linguistics is an interdisciplinary field of linguistics which identifies, investigates, and offers solutions to language-related real-life problems. Some of the academic fields related to applied linguistics are education, psychology, communication research, anthropology, and sociology.\n\nApplied linguistics is an interdisciplinary field of linguistics. Major branches of applied linguistics include bilingualism and multilingualism, conversation analysis, contrastive linguistics, sign linguistics, language assessment, literacies, discourse analysis, language pedagogy, second language acquisition, language planning and policy, interlinguistics, stylistics, pragmatics, forensic linguistics and translation.\n\nMajor journals of the field include \"Annual Review of Applied Linguistics\", \"Applied Linguistics\", \"Journal of Applied Linguistics\", \"International Review of Applied Linguistics\", \"International Journal of Applied Linguistics\", \"European Journal of Applied Linguistics\", \"Issues in Applied Linguistics\", \"Language Learning\", \"Language and Education, TESOL Quarterly\", and \"Linguistics and Education\".\n\nThe tradition of applied linguistics established itself in part as a response to the narrowing of focus in linguistics with the advent in the late 1950s of generative linguistics, and has always maintained a socially-accountable role, demonstrated by its central interest in language problems.\n\nAlthough the field of applied linguistics started from Europe and the United States, the field rapidly flourished in the international context.\n\nApplied linguistics first concerned itself with principles and practices on the basis of linguistics. In the early days, applied linguistics was thought as “linguistics-applied” at least from the outside of the field. In the 1960s, however, applied linguistics was expanded to include language assessment, language policy, and second language acquisition. As early as the 1970s, applied linguistics became a problem-driven field rather than theoretical linguistics, including the solution of language-related problems in the real world. By the 1990s, applied linguistics had broadened including critical studies and multilingualism. Research in applied linguistics was shifted to \"the theoretical and empirical investigation of real world problems in which language is a central issue.\"\n\nIn the United States, applied linguistics also began narrowly as the application of insights from structural linguistics—first to the teaching of English in schools and subsequently to second and foreign language teaching. The \"linguistics applied\" approach to language teaching was promulgated most strenuously by Leonard Bloomfield, who developed the foundation for the Army Specialized Training Program, and by Charles C. Fries, who established the English Language Institute (ELI) at the University of Michigan in 1941. In 1946, Applied linguistics became a recognized field of studies in the aforementioned university. In 1948, the Research Club at Michigan established \"Language Learning: A Journal of Applied Linguistics\", the first journal to bear the term \"applied linguistics.\" In the late 1960s, applied linguistics began to establish its own identity as an interdisciplinary field of linguistics concerned with real-world language issues. The new identity was solidified by the creation of the American Association for Applied Linguistics in 1977.\n\nThe International Association of Applied Linguistics was founded in France in 1964, where it is better known as Association Internationale de Linguistique Appliquée, or AILA. AILA has affiliates in more than thirty countries, some of which are listed below.\n\nAustralia\nAustralian applied linguistics took as its target the applied linguistics of mother tongue teaching and teaching English to immigrants. The Australia tradition shows a strong influence of continental Europe and of the USA, rather than of Britain. Applied Linguistics Association of Australia (ALAA) was established at a national congress of applied linguists held in August 1976. ALAA holds a joint annual conference in collaboration with the Association for Applied Linguistics in New Zealand (ALANZ).\n\nCanada\nThe Canadian Association of Applied Linguistics / L’Association canadienne de linguistique appliquée (CAAL/ACLA), is an officially bilingual (English and French) scholarly association with approximately 200 members. They produce the \"Canadian Journal of Applied Linguistics\" and hold an annual conference.\n\nIreland\nThe Irish Association for Applied Linguistics/Cumann na Teangeolaíochta Feidhmí (IRAAL) was founded in 1975. They produce the journal \"Teanga\", the Irish word for 'language'.\n\nJapan\nIn 1982, the Japan Association of Applied Linguistics (JAAL) was established in the Japan Association of College English Teachers (JACET) in order to engage in activities on a more international scale. In 1984, JAAL became an affiliate of the International Association of Applied Linguistics (AILA).\n\nNew Zealand\nThe Applied Linguistics Association of New Zealand (ALANZ) produces the journal \"New Zealand Studies in Applied Linguistics\" and has been collaborating with the Applied Linguistics Association of Australia in a combined annual conference since 2010, with the Association for Language Testing and Assessment of Australia and New Zealand (ALTAANZ) later joining the now three-way conference collaboration. \n\nSouth Africa\nThe Southern African Applied Linguistics Association (SAALA) was founded in 1980. There are currently four publications associated with SAALA including the \"Southern African Linguistics and Applied Language Studies Journal\" (SAJALS).\n\nUnited Kingdom\nThe British Association for Applied Linguistics (BAAL) was established in 1967. Its mission is \"the advancement of education by fostering and promoting, by any lawful charitable means, the study of language use, language acquisition and language teaching and the fostering of interdisciplinary collaboration in this study [...]\". BAAL hosts an annual conference, as well as many additional smaller conferences and events organised by its Special Interest Groups (SIGs). \n\nUnited States\nThe American Association for Applied Linguistics (AAAL) was founded in 1977. AAAL holds an annual conference, usually in March or April, in the United States or Canada.\n\n\n"}
{"id": "52477230", "url": "https://en.wikipedia.org/wiki?curid=52477230", "title": "Arabic WordNet", "text": "Arabic WordNet\n\nArabic WordNet is a WordNet for Arabic language, since its creation in 2006, it has been extended in 2015.\n"}
{"id": "6352447", "url": "https://en.wikipedia.org/wiki?curid=6352447", "title": "Artificial grammar learning", "text": "Artificial grammar learning\n\nArtificial grammar learning (AGL) is a paradigm of study within cognitive psychology and linguistics. Its goal is to investigate the processes that underlie human language learning by testing subjects' ability to learn a made-up grammar in a laboratory setting. It was developed to evaluate the processes of human language learning but has also been utilized to study implicit learning in a more general sense. The area of interest is typically the subjects' ability to detect patterns and statistical regularities during a training phase and then use their new knowledge of those patterns in a testing phase. The testing phase can either use the symbols or sounds used in the training phase or transfer the patterns to another set of symbols or sounds as surface structure.\n\nMany researchers propose that the rules of the artificial grammar are learned on an implicit level since the rules of the grammar are never explicitly presented to the participants. The paradigm has also recently been utilized for other areas of research such as language learning aptitude, structural priming and to investigate which brain structures are involved in syntax acquisition and implicit learning.\n\nApart from humans, the paradigm has also been used to investigate pattern learning in other species, e.g. cottontop tamarins and starlings.\n\nMore than half a century ago George A. Miller established the paradigm of artificial grammar learning in order to investigate the influence of explicit grammar structures on human learning, he designed a grammar model of letters with different sequences. His research demonstrated that it was easier to remember a structured grammar sequence than a random sequence of letters. His explanation was that learners could identify the common characteristics between learned sequences and accordingly encode them to a memory set. He predicted that subjects could identify which letters will most likely appear together as a sequence repeatedly and which letters would not and that the subjects would use this information to form memory sets. Those memory sets served participants as a strategy later on during their memory tests.\n\nReber doubted Miller's explanation. He claimed that if participants could encode the grammar rules as productive memory sets, then they should be able to verbalize their strategy in detail. He conducted research that led to the development of the modern AGL paradigm. This research used a synthetic grammar learning model to test implicit learning. AGL became the most used and tested model in the field. As in the original paradigm developed by Miller, participants were asked to memorize a list of letter strings which were created from an artificial grammar rule model. It was only during the test phase that participants were told that there was a set of rules behind the letter sequences they memorized. They were then instructed to categorize new letter strings based on the same set of rules which they had not previously been exposed to. They classified new letter strings as \"grammatical\" (constructed from the grammar rule), vs. \"randomly constructed\" sequences. If subjects correctly sorted the new strings above chance level, it could be inferred that subjects had acquired the grammatical rule structure without any explicit instruction of the rules. Reber found that participants sorted out new strings above chance level. While they reported using strategies during the sorting task, they could not actually verbalize those strategies. Subjects could identify which strings were grammatically correct but could not identify the rules that composed grammatical strings.\n\nThis research was replicated and expanded upon by many others. The conclusions of most of these studies were congruent with Reber's hypothesis: the implicit learning process was done with no intentional learning strategies. These studies also identified common characteristics for the implicitly acquired knowledge:\n\n\nThe modern AGL paradigm can be used to investigate explicit and implicit learning, although it is most often used to test implicit learning. In a typical AGL experiment, participants are required to memorize strings of letters previously generated by a specific grammar. The length of the strings usually ranges from 2-9 letters per string. An example of such a grammar is shown in figure 1.\n\nFigure 1: Example of an artificial grammar rule\n\nIn order to compose a grammatically \"ruleful\" string of letters, according to the predetermined grammar rule, a subject must follow the rules for the pairing of letters as represented in the model (figure 1). When observing a violation of the grammatical rule system that composes the string, it is considered an \"unruleful\" or randomly constructed string.\n\nIn the case of a standard AGL implicit learning task, subjects are not told that the strings are based on a specific grammar. Instead, they are simply given the task to memorize the letter strings for a memory. After the learning phase, subjects are told that the letter strings presented during the learning phase were based on specific rules, but are not explicitly told what the rules are. During a test phase, the subjects are instructed to categorize new letter strings as \"ruleful\" or \"unruleful\". The dependent variable usually measured is the percentage of correctly categorized strings. Implicit learning is considered to be successful when the percentage of correctly sorted strings is significantly higher than chance level. If this significant difference is found, it indicates the existence of a learning process that is more involved than memorizing the presented letter strings.\n\nThe mechanism behind the implicit learning that is hypothesized to occur while people engage in artificial grammar learning is statistical learning or, more specifically, Bayesian learning. Bayesian learning takes into account types of biases or \"prior probability distributions\" individuals have that contribute to the outcome of implicit learning tasks. These biases can be thought of as a probability distribution that contains the probability that each possible hypothesis is likely to be correct. Due to the structure of the Bayesian model, the inferences output by the model are in the form of a probability distribution rather than a single most probable event. This output distribution is a \"posterior probability distribution\". The posterior probability of each hypothesis in the original distribution is the probability of the hypothesis being true given the data and the probability of data given the hypothesis is true.\nThis Bayesian model for learning is fundamental for understanding the pattern detection process involved in implicit learning and, therefore, the mechanisms that underlie the acquisition of artificial grammar learning rules. It is hypothesized that the implicit learning of grammar involves predicting co-occurrences of certain words in a certain order. For example, \"the dog chased the ball\" is a sentence that can be learned as grammatically correct on an implicit level due to the high co-occurrence of \"chase\" being one of the words to follow \"dog\". A sentence like \"the dog cat the ball\" is implicitly recognized as grammatically incorrect due to the lack of utterances that contain those words paired in that specific order. This process is important for teasing apart thematic roles and parts of speech in grammatical processing (see grammar). While the labeling of the thematic roles and parts of speech is explicit, the identification of words and parts of speech is implicit.\n\nTraditional approaches to AGL claim that the stored knowledge obtained during the learning phase is abstract. Other approaches argue that this stored knowledge is concrete and consists of exemplars of strings encountered during the learning phase or \"chunks\" of these exemplars. In any case, it is assumed that the information stored in memory is retrieved in the test phase and is used to aid decisions about letter strings. \n3 main approaches attempt to explain the AGL phenomena:\n\nResearch with amnesia patients suggests the \"Dual Factor approach\" may be the most accurate model. A series of experiments with amnesiac patients support the idea that AGL involves both abstract concepts and concrete exemplars. Amnesiacs were able to classify stimuli as \"grammatical\" vs. \"randomly constructed\" just as well as participants in the control group. While able to successfully complete the task, amnesiacs were not able to explicitly recall grammatical \"chunks\" of the letter sequence while the control group was able to explicitly recall them. When performing the task with the same grammar rules but a different sequence of letters than those that they were previously tested on, both amnesiacs and the control group were able to complete the task (although performance was better when the task was completed using the same set of letters used for training). The results of the experiment support the dual factor approach to artificial grammar learning in that people use abstract information to learn rules for grammars and use concrete, exemplar-specific memory for chunks. Since the amnesiacs were unable to store specific \"chunks\" in memory, they completed the task using an abstract set of rules. The control group was able to store these specific chunks in memory and (as evidenced by recall) did store these examples in memory for later reference.\n\nAGL research has been criticized due to the \"automatic question\": Is AGL considered to be an automatic process? During encoding (see encoding (memory)), performance can be automatic in the sense of occurring without conscious monitoring (without conscious guidance by the performer’s intentions). In the case of AGL, it was claimed that implicit learning is an automatic process due to the fact that it is done with no intention of learning a specific grammar rule. This complies with the classic definition of an \"automatic process\" as a fast, unconscious, effortless process that may start unintentionally. When aroused, it continues until it is over without the ability to stop or ignore its consequences. This definition has been challenged many times. Alternative definitions for automatic process have been given. Reber's presumption that AGL is automatic could be problematic by implying that an unintentional process is an automatic process in its essence. \nWhen focusing on AGL tests, a few issues need to be addressed. The process is complex and contains encoding and recall or retrieval. Both encoding and retrieval could be interpreted as automatic processes since what was encoded during the learning stage is not necessary for the task intentionally performed during the test stage. Researchers need to differentiate between implicitness as referring to the process of learning or knowledge encoding and also as referring to performance during the test phase or knowledge retrieval. Knowledge encoded during training may include many aspects of the presented stimuli (whole strings, relations among elements, etc.). The contribution of the various components to performance depends on both the specific instruction in the acquisition phase and the requirements of the retrieval task. Therefore, the instructions on every phase are important in order to determine whether or not each stage will require automatic processing. Each phase should be evaluated for automaticity separately.\n\nOne hypothesis that contradicts the automaticity of AGL is the \"mere exposure effect\". The mere exposure effect is increased affect towards a stimulus that is the result of nonreinforced, repeated exposure to the stimulus. Results from over 200 experiments on this effect indicate that there is a positive relationship between mean \"goodness\" rating and frequency of stimulus exposure. Stimuli for these experiments included line drawings, polygons and nonsense words (which are types of stimuli used in AGL research). These experiments exposed participants to each stimulus up to 25 times. Following each exposure participants were asked to rate the degree to which each stimulus suggested \"good\" vs. \"bad\" affect on a 7-point scale. In addition to the main pattern of results, it was also found in several experiments that participants rated higher positive affect for previously exposed items than for novel items. Since implicit cognition should not reference previous study episodes, the effects on affect ratings should not have been observed if processing of this stimuli is truly implicit. The results of these experiments suggests that different categorization of the strings may occur due to differences in affect associated with the strings and not due to implicitly learned grammar rules.\n\nSince the advent of computers and artificial intelligence, computer programs have been adapted that attempt to simulate the implicit learning process observed in the AGL paradigm. The AI programs first adapted to simulate both natural and artificial grammar learning used the following basic structure:\n\n\n\nAn early model for AI grammar learning is Wolff's SNPR System. The program acquires a series of letters with no pauses or punctuation between words and sentences. The program then examines the string in subsets and looks for common sequences of symbols and defines \"chunks\" in terms of these sequences (these chunks are akin to the exemplar-specific chunks described for AGL). As the model acquires these chunks through exposure, the chunks begin to replace the sequences of unbroken letters. When a chunk precedes or follows a common chunk, then the model determines disjunctive classes in terms of the first set. For example, when the model encounters \"the-dog-chased\" and \"the-cat-chased\" it classifies \"dog\" and \"cat\" as being members of the same class since they both precede \"chase\". While the model sorts chunks into classes, it does explicitly define these groups (e.g., noun, verb). Early AI models of grammar learning such as these ignored the importance of negative instances of grammar's effect on grammar acquisition and were also lacking in the ability to connect grammatical rules to pragmatics and semantics. Newer models have attempted to factor these details in.\nThe Unified Model attempts to take both of these factors into account. The model breaks down grammar according to \"cues\". Languages mark case roles using five possible cue types: word order, case marking, agreement, intonation and verb-based expectation (see grammar). The influence that each cue has over a language's grammar is determined by its \"cue strength\" and \"cue validity\". Both of these values are determined using the same formula, except that cue strength is defined through experimental results and cue validity is defined through corpus counts from language databases. The formula for cue strength/validity is as follows:\n\nCue availability is the proportion of times that the cue is available over the times that it is needed. Cue reliability is the proportion of times that the cue is correct over the total occurrences of the cue. By incorporating cue reliability along with cue availability, The Unified Model is able to account for the effects of negative instances of grammar since it takes accuracy and not just frequency into account. As a result, this also accounts for the semantic and pragmatic information since cues that do not produce grammar in the appropriate context will have low cue strength and cue validity. While MacWhinney's model also simulates natural grammar learning, it attempts to model the implicit learning processes observed in the AGL paradigm.\n\nContemporary studies with AGL have attempted to identify which structures are involved in the acquisition of grammar and implicit learning. Agrammatic aphasic patients (see Agrammatism) were tested with the AGL paradigm. The results show that breakdown of language in agrammatic aphasia is associated with an impairment in artificial grammar learning, indicating damage to domain-general neural mechanisms sub serving both language and sequential learning.\nDe Vries, Barth, Maiworm, Knecht, Zwitserlood & Flöel found that electrical stimulation of Broca's area enhances implicit learning of an artificial grammar. Direct current stimulation may facilitate acquisition of grammatical knowledge, a finding of potential interest for rehabilitation of aphasia.\nPetersson, Vasiliki & Hagoort, examine the neurobiological correlates of syntax, the processing of structured sequences, by comparing fMRI results on artificial and natural language syntax. They argue that the \"Chomsky hierarchy\" is not directly relevant for neurobiological systems through AGL testing.\n\n"}
{"id": "20695536", "url": "https://en.wikipedia.org/wiki?curid=20695536", "title": "Bai (suffix)", "text": "Bai (suffix)\n\nBai or baisaheb is a suffix added to the name of female members of the Maratha and Rajput dynasties.e.g. Shantabai It is also used as an honorific for the elder sister amongst the Marathi-speaking people. This type of suffix is also used in several kshatriya castes and in some of the tribal castes, for example the Lambadi.\n"}
{"id": "5650079", "url": "https://en.wikipedia.org/wiki?curid=5650079", "title": "Binomial pair", "text": "Binomial pair\n\nIn linguistics, a binomial pair or binomial is a sequence of two or more words or phrases belonging to the same grammatical category, having some semantic relationship and joined by some syntactic device such as \"and\" or \"or\". Examples in English include \"through and through\", \"(without) let or hindrance\", and \"chalk and cheese\".\n\nSeveral legal terms are binomial pairs, often (but not necessarily) consisting of one Germanic word and one Romance word, such as \"(last) will and testament\" or \"cease and desist\".\n\nBinomials are often irreversible; that is, they occur only in one order. For example, one says \"bow and arrow\" but not \"*arrow and bow\"; one says \"here and there\" and \"that's neither here nor there\", but not \"*there and here\" or \"*that's neither there nor here\".\n\n\n"}
{"id": "3652290", "url": "https://en.wikipedia.org/wiki?curid=3652290", "title": "Block quotation", "text": "Block quotation\n\nA block quotation (also known as a long quotation or extract) is a quotation in a written document that is set off from the main text as a paragraph, or block of text, and typically distinguished visually using indentation and a different typeface or smaller size font. This is in contrast to setting it off with quotation marks in a \"run-in quote\". Block quotations are used for long quotations. \"The Chicago Manual of Style\" recommends using a block quotation when extracted text is 100 words or more, or approximately six to eight lines in a typical manuscript.\n\nIn the first centuries of typesetting, quotations were distinguished merely by indicating the speaker, and this can still be seen on some editions of the Bible. During the Renaissance, quotations were distinguished by setting in a typeface contrasting with the main body text (often Italic type with roman, or the other way round). Block quotations were set this way at full size and full measure.\n\nQuotation marks were first cut in type during the middle of the sixteenth century, and were used copiously by some printers by the seventeenth. In Baroque and Romantic-period books, they could be repeated at the beginning of every line of a long quotation. When this practice was abandoned, the empty margin remained, leaving an indented block quotation.\n\nApart from quotation marks not being used to enclose block quotations, there are no hard-and-fast rules for the exact formatting of block quotations. To a large extent the specific format may be dictated by the method of publication (e.g. handwritten text, typewritten pages, or electronic publishing) as well as the typeface being used.\n\nFor writers and editors, \"The Chicago Manual of Style\" (8th edition, 2007) recommends using a block quotation when cited text is five lines or longer.\nOther sources set the threshold at four or five lines.\nThe block quotation may also be used to distinguish shorter citations from original text, though strictly speaking this does not follow APA or MLA style guidelines. Use of the block quotation for shorter passages is a stylistic choice that may or may not be acceptable depending on the situation.\n\nSome guidelines suggest an indentation of five, ten, or fifteen spaces. However, five spaces in a proportional font may be much narrower than in a typewriter font of the same point size. In addition, setting an indent based on an exact number of spaces may not be technically possible in a given word processing or electronic publishing application. In these situations, a measurement of distance rather than a number of spaces may be prescribed instead (for example, a to 1 in or 1 to 2 cm indent). Some writers indent block quotations from the right margin as well. Block quotations are generally set off from the text that precedes and follows them by also adding extra space above and below the quotation and setting the text in smaller type. Barring specific requirements, the format of the block quotation will ultimately be determined by aesthetics, making the quotation pleasing to the eye, easy to read, and appropriate for the particular writing task.\n\nIn typesetting, block quotations can be distinguished from the surrounding text by variation in typeface (often italic vs. roman), type size, or by indentation. Often combinations of these methods are used, but are not necessary. Block quotations are also visually distinguished from preceding and following main text blocks by a white line or half-line space. For example:\n\n"}
{"id": "3788502", "url": "https://en.wikipedia.org/wiki?curid=3788502", "title": "Classical Mongolian language", "text": "Classical Mongolian language\n\nClassical Mongolian is an extinct Mongolic language formerly used in Mongolia, China, and Russia. It is a standardized written language used in the 18th century and 20th centuries. Notable texts include the translation of the Kanjur and Tanjur and several chronicles roughly between 1700 and 1900. \n\n\"Classical Mongolian\" sometimes refers to any language documents in Mongolian script that are neither Pre-classical (i.e. Middle Mongol in Mongolian script) nor modern Mongolian.\n\n"}
{"id": "9124629", "url": "https://en.wikipedia.org/wiki?curid=9124629", "title": "Cognitive philology", "text": "Cognitive philology\n\nCognitive philology is the science that studies written and oral texts as the product of human mental processes. Studies in cognitive philology compare documentary evidence emerging from textual investigations with results of experimental research, especially in the fields of cognitive and ecological psychology, neurosciences and artificial intelligence. \"The point is not the text, but the mind that made it\". Cognitive Philology aims to foster communication between literary, textual, philological disciplines on the one hand and researches across the whole range of the cognitive, evolutionary, ecological and human sciences on the other.\n\nCognitive philology: \n\nAmong the founding thinkers and noteworthy scholars devoted to such investigations are: \n\n\n"}
{"id": "21657133", "url": "https://en.wikipedia.org/wiki?curid=21657133", "title": "Contrastive linguistics", "text": "Contrastive linguistics\n\nContrastive linguistics is a practice-oriented linguistic approach that seeks to describe the differences and similarities between a pair of languages (hence it is occasionally called \"\"differential\" linguistics\").\n\nWhile traditional linguistic studies had developed comparative methods (comparative linguistics), chiefly to demonstrate family relations between cognate languages, or to illustrate the historical developments of one or more languages, modern contrastive linguistics intends to show in what ways the two respective languages differ, in order to help in the solution of practical problems. (Sometimes the terms diachronic linguistics and synchronic linguistics are used to refer to these two perspectives.) \n\nContrastive linguistics, since its inception by Robert Lado in the 1950s, has often been linked to aspects of applied linguistics, e.g., to avoid interference errors in foreign-language learning, as advocated by Di Pietro (1971) (see also contrastive analysis), to assist interlingual transfer in the process of translating texts from one language into another, as demonstrated by Vinay & Darbelnet (1958) and more recently by Hatim (1997) (see translation), and to find lexical equivalents in the process of compiling bilingual dictionaries, as illustrated by Heltai (1988) and Hartmann (1991) (see bilingual lexicography). \n\nContrastive descriptions can occur at every level of linguistic structure: speech sounds (phonology), written symbols (orthography), word-formation (morphology), word meaning (lexicology), collocation (phraseology), sentence structure (syntax) and complete discourse (textology). Various techniques used in corpus linguistics have been shown to be relevant in intralingual and interlingual contrastive studies, e.g. by 'parallel-text' analysis (Hartmann 1997).\n\nContrastive linguistic studies can also be applied to the differential description of one or more varieties within a language, such as styles (contrastive rhetoric), dialects, registers or terminologies of technical genres.\n\n\n"}
{"id": "36339441", "url": "https://en.wikipedia.org/wiki?curid=36339441", "title": "Cross-border language", "text": "Cross-border language\n\nA cross-border language or trans-border language is a language spoken by a population (ethnic group or nation) that lives in a geographical area in two or several internationally recognized countries that have common land or maritime borders. Cross-border languages are particularly common in Africa but are found on all other continents as well.\n\nExamples include German (spoken in Germany, Austria, Switzerland and other neighbouring countries), Catalan (spoken in Spain, Andorra and elsewhere), Hungarian (spoken in most countries bordering Hungary) and Gaelic (spoken in Ireland, Scotland and the Isle of Man).\n"}
{"id": "53961171", "url": "https://en.wikipedia.org/wiki?curid=53961171", "title": "CyraCom Language Solutions", "text": "CyraCom Language Solutions\n\nCyraCom International, Inc. is an American language services company that provides over the phone and video interpretation and language assessment services. It is the largest provider of telephone interpretation services in the United States.\nCyraCom was originally founded as Kevmark Industries in 1995, when co-founder Mark Myers invented the first dual-handset telephone. Myers and co-founder Kevin Carey built a prototype with two handsets connected to a single base, allowing both patients and providers to speak to an interpreter on a three-way call without passing a handset back and forth. The device later became known as the CyraPhone.\nThe company name was changed to CyraCom in 1997, named for Edmond Rostand's Cyrano de Bergerac. It operates call centers and offices in the United States with one office in the United Kingdom, and provides interpreters in hundreds of languages.\n"}
{"id": "10042066", "url": "https://en.wikipedia.org/wiki?curid=10042066", "title": "Developmental linguistics", "text": "Developmental linguistics\n\nDevelopmental linguistics is the study of the development of linguistic ability in an individual, particularly the acquisition of language in childhood. It involves research into the different stages in language acquisition, language retention, and language loss in both first and second languages, in addition to the area of bilingualism. Before infants can speak, the neural circuits in their brains are constantly being influenced by exposure to language.\nThe neurobiology of language contains a \"critical period\" in which children are most sensitive to language. The different aspects of language have varying \"critical periods\". Studies show that the critical period for phonetics is toward the end of the first year. At 18 months, a toddler's vocabulary vastly expands. The critical period for syntactic learning is 18-36 months. Infants of different mother languages can be differentiated at the age of 10 months. At 20 weeks they begin vocal imitation.\nBeginning when babies are about 12 months, they take on computational learning and social learning. Social interactions for infants and toddlers is important because it helps associate \"perception and action\". In-person social interaction rather than audio or video better facilitates learning in babies because they learn from how other people respond to them, especially their mothers.\nBabies have to learn to mimic certain syllables, which takes practice in manipulating tongue and lip movement. Sensory-motor learning in speech is linked to exposure to speech, which is very sensitive to language.\nInfants exposed to Spanish exhibit a different vocalization than infants exposed to English. One study took infants that were learning English and made them listen to Spanish in 12 sessions. The result showed consequent alterations in their vocalization, which demonstrated Spanish prosody. \nOne study used MEG to record activation in the brains of newborns, 6 months olds and 12 months olds while presenting them with syllables, harmonics and non-speech sounds. For the 6 month and 12 month old, the auditory and motor areas responded to speech. The newborn showed auditory activation but not motor activation. Another study presented 3 month olds with sentences and recorded their brain activity via fMRI motor speech areas did activate. These studies suggest that the link between perception and action begins to develop at 3 months.\nWhen babies are young, they are actually the most sensitive to distinguishing all phonetic units. During an infant’s 1st year of life, they have to differentiate between about 40 phonetic units. When they are older they have usually been exposed to their native language so much that they lose this ability and can only distinguish phonetic units in their native language. Even at 12 months babies exhibit a deficit in differentiated non-native sounds. However, their ability to distinguish sounds in their native language continues to improve and become more fine-tuned. For example, Japanese learning infants learn that there is no differentiation between /r/ and /l/. However, in English, \"rake\" and \"lake\" are two different words. Japanese babies eventually lose their ability to distinguish between /r/ and /l/.\nSimilarly, a Spanish learning infant cannot form words until they learn the difference between works like \"bano\" and \"pano\", because the /p/ sound is different than the /b/ sound. English learning babies do not learn to differentiate between the two.\n\nBabies learn early on to recognize prosodic cues. English speaking babies learn that the English language places a lot of stress on the first syllable, which is called trochaic or strong-weak prosody. The Polish prosodic pattern is the opposite and is called weak-strong or iambic prosody. At 7.5 months, English speaking infants can recognize their native pattern and segment words accordingly. They are able to identify the stress on words like \"pencil\" or \"stapler\". However, when they hear a phrase like \"guitar is\", they will perceive \"tar is\" as a unit since that is where the stress occurs.\nExposing babies to music can improve their listening skills. Music tunes their attention skills and helps them distinguish different sounds.\nA study was done that compared activity in the prefrontal cortex in a group of babies who had 12 sessions of music class compared to babies who took 12 sessions of a social class with no music. The study found that babies in the music classes had greater activity in the prefrontal cortex.\n\n"}
{"id": "39488790", "url": "https://en.wikipedia.org/wiki?curid=39488790", "title": "Display question", "text": "Display question\n\nA display question is a type of question where the questioner already knows the answer. Display questions are used in language education in order to elicit language practice. They are contrasted with referential questions, questions for which the answer is not yet known. The use of referential questions is generally preferred to the use of display questions in communicative language teaching.\n\nRichards and Schmidt give the following example:\n\n"}
{"id": "30856172", "url": "https://en.wikipedia.org/wiki?curid=30856172", "title": "Diwan-khane", "text": "Diwan-khane\n\nDivan-khane () is a Persian phrase from {\"divan\" = court} + {\"khane\" = house} to describe a guest house usually for prominent people in the Middle Eastern society.\n\nIn tribal Middle Eastern, Arab, Persian, and Kurdish societies, a guest house of the tribal chieftain is used mostly for discussing tribal affairs. This served as an institution dedicated to the political and social affairs of the tribe. A \"diwan\" or \"diwan-khane\" was a special room, or house, dedicated to the \"agha\" and his male guests, for sitting and drinking tea, discussing the political and social affairs of the tribe and other mundane subjects. \n\nThe \"agha\" and his guests would also listen to singers and story tellers (usually Jewish merchants or peddlers), who would entertain them. The common \"agha\" was in charge of several major tasks of the tribal society under his jurisdiction: He was the head of the political unit, the judge and arbitrator, the military leader and the finance minister responsible mainly for receiving dues/taxes from his subjects for their harvest and commercial transactions under his jurisdiction. \n\nOne of the best studies of \"aghas\"in the Kurdish society is by Mordechai Zaken: \"Jewish subjects and their tribal chieftains in Kurdistan\".\n"}
{"id": "21514247", "url": "https://en.wikipedia.org/wiki?curid=21514247", "title": "English Language Unity Act", "text": "English Language Unity Act\n\nThe English Language Unity Act was first introduced in 2005. It hoped to establish English as the official language of the federal government of the United States. If passed it would require that all official functions and proceedings of federal and state government be conducted in English. It would also require that applicants for naturalization be tested on their ability to read and generally understand the English language. They would be tested on the laws of the United States as well as other important documents that relate to the law, including the Declaration of Independence and Constitution. It would also require that all naturalization ceremonies be conducted in English. If a person became injured because of violations of this Act, they would be able to file suit in court. Before this act could be considered by the House of Representatives it had to be approved by the House Judiciary, and Education and the Workforce Committees.\n\nThe English Language Unity Act is based on a similar bill, \"The Bill Emerson English Language Empowerment Act\", which passed in the House of Representatives in 1999. However, it never became law. It tried to amend Federal law to declare English to be the official language of the U.S. Government. If it became a law, it would have required state representatives to conduct official business in English. It would have required that all officials conduct naturalization ceremonies entirely in English as well.\nConservative Republican lawmaker Representative Steve King introduced this act to the House of Representatives on March 1, 2005 it was known as the English Language Unity Act of 2005. Before the 109th session ended of congress, the bill accumulated 164 sponsors. The last action on this bill was the introductory remarks on the proposed bill on May 19, 2006. The bill did not come up for debate during this session of Congress, so it is said to have died.\n\nRepresentative King reintroduced the bill as the English Unity Act of 2007 on February 12, 2007. This time it gained the support of 153 cosponsors. The last action on this proposed bill was on June 5, 2007 when the House Committee on Education and Labor referred the bill to the subcommittee on Early Childhood Education, Elementary and Secondary Education. Congress adjourned before further action could be taken, so the bill died again in 2007.\n\nRepresentative King alongside Senator Jim Inhofe introduced the English Language Unity Act of 2011 on Friday, March 10, 2011. In a release King defended his proposition by saying \"A common language is the most powerful unifying force known throughout history. We need to encourage assimilation of all legal immigrants in each generation. A nation divided by language cannot pull together as effectively as a people.\" Inhofe added: \"This legislation will provide much-needed commonality among United States citizens, regardless of heritage. As a nation built by immigrants, it is important that we share one vision and one official language.\" \n\nRepeated attempts to make English the official language of the United States have failed time and time again, though the issue never fails to spark heated debate. While appearing to focus solely on language, these attempt trigger issues related to financial burden, discrimination, patriotism, and unity.\n\nThose against the legislation argue the bill deals with a non-issue. They claim Congress does not need to establish legislation in order to teach others the importance of knowing English since the language is already spoken by a majority of Americans. Opponents of the legislation question why there is a sudden need for an official language, given that the United States government has flourished without one for the past two hundred years. They argue that it is not an official language that binds Americans together as a country, but rather the freedoms and ideals that are enjoyed by its citizens. \n\nSome opponents of the bill also argue that the legislation is unconstitutional. They assert it would limit the government's ability to correspond with all its citizens, and that by restricting federal and state employees from communicating with citizens in a language other than English, the bill violates first amendment rights of free speech. Some further contend that the bill would call for changes to the Voting Rights Act by eliminating all non- English ballots, despite the fact that nothing in the bill's language modifies the Voting Rights Act.\n\nSupporters of the legislation take the position that accommodation of non-English speakers discourages assimilation. They acknowledge that the ability to speak a language other than English is valuable, and that its use in the home, church or private place of business should in no way be discouraged. At the same time, they argue that the government should not bear the responsibility of guaranteeing that non-English-speaking individuals can participate in government solely using their mother language. They contend that as more immigrants learn English, the language barriers that divide the country into separate groups will disintegrate and lead to a decrease in racial and ethnic problems. They also believe that by learning English, individuals can become more productive citizens and members of American society. They argue that immigrants who are fluent in English have better economic opportunities, and assert that non-English speakers tend to find themselves restricted to low-skilled, low-paying jobs. Supporters of the bill also posit that the ability of immigrant groups to speak English will give them an increased political voice and allow them to participate more fully and effectively in the democratic process.\nThirty - one states currently have adopted legislation similar to the English Language Unity Act. Alabama (1990) Alaska (1998) Arizona (2006) Arkansas (1987) California (1986) Colorado (1988) Florida (1988) Georgia (1986 & 1996) Hawaii (1978) Idaho (2007) Illinois (1969) Indiana (1984) Iowa (2002) Kansas (2007) Kentucky (1984) Louisiana (1812) Massachusetts (1975) Mississippi (1987) Missouri (1998 & 2008) Montana (1995) Nebraska (1920) New Hampshire (1995) North Carolina (1987) North Dakota (1987) Oklahoma (2010) South Carolina (1987) South Dakota (1995) Tennessee (1984) Utah (2000) Virginia (1981 & 1996) Wyoming (1996).\n"}
{"id": "3480573", "url": "https://en.wikipedia.org/wiki?curid=3480573", "title": "Fiction writing", "text": "Fiction writing\n\nFiction writing is the composition of non-factual prose texts. Fictional writing often is produced as a story meant to entertain or convey an author's point of view. The result of this may be a short story, novel, novella, screenplay, or drama, which are all types (though not the only types) of fictional writing styles. Different types of authors practice fictional writing, including novelists, playwrights, short story writers, dramatists and screenwriters.\n\nA genre is the subject matter or category that writers use. For instance, science fiction, fantasy and mystery fiction are genres. Genre fiction also known as popular fiction, is plot-driven fictional works written with the intent of fitting into a specific literary genre, in order to appeal to readers and fans already familiar with that genre.\n\nGenre fiction is storytelling driven by plot, as opposed to literary fiction, which focuses more on theme and character. Genre fiction, or popular fiction, is written to appeal to a large audience and it sells more primarily because it is more commercialised. An example is the \"Twilight\" series which may sell more than Herman Melville's \"Moby Dick\", because the \"Twilight\" novels deal with elements of pop culture – romance and vampires.\n\nLiterary fiction is fictional works that hold literary merit, that is to say, they are works that offer social commentary, or political criticism, or focus on aspect of the human condition.\n\nLiterary fiction is usually contrasted with, popular, commercial, or genre fiction. Some have described the difference between them in terms of analysing reality (literary) rather than escaping reality (popular). The contrast between these two categories of fiction is controversial among some critics and scholars.\n\nCharacterization is one of the five elements of fiction, along with plot, setting, theme, and writing style. A character is a participant in the story, and is usually a person, but may be any persona, identity, or entity whose existence originates from a fictional work or performance.\n\nCharacters may be of several types:\n\nThe plot, or storyline, is the rendering and ordering of the events and actions of a story. Starting with the initiating event, then the rising action, conflict, climax, falling action, and ending possibly with a resolution.\n\nPlot consists of action and reaction, also referred to as stimulus and response and has a beginning, a middle, and an ending.\n\nThe climax of the novel consists of a single action-packed sentence in which the conflict (problem) of the novel is resolved. This sentence comes towards the end of the novel. The main part of the action should come before the climax.\n\nPlot also has a mid-level structure: scene and sequel. A scene is a unit of drama—where the action occurs. Then, after a transition of some sort, comes the sequel—an emotional reaction and regrouping, an aftermath.\n\nSetting is the locale and time of a story. The setting is often a real place, but may be a fictitious city or country within our own world; a different planet; or an alternate universe, which may or may not have similarities with our own universe. Sometimes setting is referred to as milieu, to include a context (such as society) beyond the immediate surroundings of the story. It is basically where and when the story takes place.\n\nTheme is what the author is trying to tell the reader. For example, the belief in the ultimate good in people, or that things are not always what they seem. This is often referred to as the \"moral of the story.\" Some fiction contains advanced themes like morality, or the value of life, whereas other stories have no theme, or a very shallow one.\n\nStyle includes the multitude of choices fiction writers make, consciously or not, in the process of writing a story. It encompasses not only the big-picture, strategic choices such as point of view and choice of narrator, but also tactical choices of grammar, punctuation, word usage, sentence and paragraph length and structure, tone, the use of imagery, chapter selection, titles, etc. In the process of creating a story, these choices meld to become the writer's voice, his or her own unique style.\n\nFor each piece of fiction, the author makes many choices, consciously or subconsciously, which combine to form the writer's unique style. The components of style are numerous, but include point of view, choice of narrator, fiction-writing mode, person and tense, grammar, punctuation, word usage, sentence length and structure, paragraph length and structure, tone, imagery, chapter usage, and title selection.\n\nThe narrator is the story teller. The main character in the book can also be the narrator.\n\nPoint of view is the perspective (or type of personal or non-personal \"lens\") through which a story is communicated. Narrative point of view or narrative perspective describes the position of the narrator, that is, the character of the storyteller, in relation to the story being told.\n\nThe tone of a literary work expresses the writer's attitude toward or feelings about the subject matter and audience.\n\nSuspension of disbelief is the reader's temporary acceptance of story elements as believable, regardless of how implausible they may seem in real life.\n\nErnest Hemingway wrote \"Prose is architecture, not interior decoration.\"\n\nStephen King, in his non-fiction, part autobiographical, part self-help writing memoir, \"\", he gives readers helpful advice on honing their craft: \"Description begins in the writer's imagination, but should finish in the reader's.\"\n\nKurt Vonnegut the author of praised novels \"Cat's Cradle\", \"Slaughterhouse-Five\", and \"Breakfast of Champions\", has given his readers, from his short story collection, \"Bagombo Snuff Box\", eight rules on how to write a successful story. The list can be found in the Introduction of the collection.\n\n\"Now lend me your ears. Here is Creative Writing 101:\n\n\n\n"}
{"id": "39356537", "url": "https://en.wikipedia.org/wiki?curid=39356537", "title": "Hockett's design features", "text": "Hockett's design features\n\nIn the 1960s, linguistic anthropologist Charles F. Hockett defined a set of features that characterize human language and set it apart from animal communication. He called these characteristics the design features of language. Hockett originally believed there to be 13 design features. While primate communication utilizes the first 9 features, the final 4 features (displacement, productivity, cultural transmission, and duality) are reserved for humans. Hockett later added prevarication, reflexiveness, and learnability to the list as uniquely human characteristics. He asserted that even the most basic human languages possess these 16 features.\n\nCharles Hockett was an American linguist and anthropologist, who lived from 1916 to 2000. Hockett graduated from Yale in 1939, and later taught at both Cornell and Rice. Hockett made significant contributions to structural linguistics, as well as the study of Native American, Chinese, and Fijian languages. His work focused on detailed linguistic analysis, particularly morphology and phonology, and on the concepts and tools that facilitated such analysis.\nUp until the 1950s, language was largely viewed as a social-behavioral phenomenon. Hockett was challenged in this belief by Noam Chomsky, who suggested that language is biologically-based and innately learned. He believed that humans share a universal grammar that ties all languages together. Hockett staunchly opposed this \"Chomskyan\" concept of the nature of language. However, Hockett is most famous for defining what he called the design features of language, which demonstrate his beliefs about the commonalities between human languages.\n\nVocal-auditory channel.\nRefers to the idea that speaking/hearing is the mode humans use for language. When Hockett first defined this feature, it did not take sign language into account, which reflects the ideology of orality that was prevalent during the time (See, for instance, the argumentation of Christin, 1995) . This feature has since been modified to include other channels of language, such as tactile-visual or chemical-olfactory.\n\nBroadcast transmission and directional reception\nWhen humans speak, sounds are transmitted in all directions; however, listeners perceive the direction from which the sounds are coming. Similarly, signers broadcast to potentially anyone within the line of sight, while those watching see who is signing. This is characteristic of most forms of human and animal communication.\n\nTransitoriness\nAlso called rapid fading, transitoriness refers to the idea of temporary quality of language. Language sounds exist for only a brief period of time, after which they are no longer perceived. Sound waves quickly disappear once a speaker stops speaking. This is also true of signs. In contrast, other forms of communication such as writing and Inka khipus (knot-tying) are more permanent.\n\nInterchangeability\nRefers to the idea that humans can give and receive identical linguistic signals; humans are not limited in the types of messages they can say/hear. One can say \"I am a boy\" even if one is a girl. This is not to be confused with lying (prevarication). The importance is that a speaker can physically create any and all messages regardless of their truth or relation to the speaker. In other words, anything that one can hear, one can also say.\n\nNot all species possess this feature. For example, in order to communicate their status, queen ants produce chemical scents that no other ants can produce (see animal communication below).\n\nTotal feedback\nSpeakers of a language can hear their own speech and can control and modify what they are saying as they say it. Similarly, signers see, feel, and control their signing.\n\nSpecialization\nThe purpose of linguistic signals is communication and not some other biological function. When humans speak or sign, it is generally intentional.\n\nAn example of \"non\"-specialized communication is dog panting. When a dog pants, it often communicates to its owner that it is hot or thirsty; however, the dog pants in order to cool itself off. This is a biological function, and the communication is a secondary matter.\n\nSemanticity\nSpecific sound signals are directly tied to certain meanings.\n\nArbitrariness ** languages are generally made up of both arbitrary and iconic symbols. In spoken languages this takes the form of onomatopoeias. In English \"murmur\", in Mandarin \"māo\" (cat). In ASL \"cup\", \"me\" \"up/down\", etc. \nThere is no intrinsic or logical connection between a sound form (signal) and its meaning. Whatever name a human language attributes an object is purely arbitrary. The word \"car\" is nothing like an actual car. Spoken words are really nothing like the objects they represent. This is further demonstrated by the fact that different languages attribute very different names to the same object.\n\nSigned languages are transmitted visually and this allows for a certain degree of iconicity. For example, in the ASL sign HOUSE, the hands are flat and touch in a way that resembles the roof and walls of a house. However, many other signs are not iconic, and the relationship between form and meaning is arbitrary. Thus, while Hockett did not account for the possibility of non-arbitrary form-meaning relationships, the principle still generally applies. \n\nDiscreteness\nLinguistic representations can be broken down into small discrete units which combine with each other in rule-governed ways. They are perceived categorically, not continuously. For example, English marks number with the plural morpheme /s/, which can be added to the end of any noun. The plural morpheme is perceived categorically, not continuously: we cannot express smaller or larger quantities by varying how loudly we pronounce the /s/. \n\nDisplacement\nRefers to the idea that humans can talk about things that are not physically present or that do not even exist. Speakers can talk about the past and the future, and can express hopes and dreams. A human's speech is not limited to here and now. Displacement is one of the features that separates human language from other forms of primate communication.\n\nProductivity\nRefers to the idea that language-users can create and understand novel utterances. Humans are able to produce an unlimited amount of utterances. Also related to productivity is the concept of grammatical patterning, which facilitates the use and comprehension of language. Language is not stagnant, but is constantly changing. New idioms are created all the time and the meaning of signals can vary depending on the context and situation.\n\nTraditional transmission\nAlso called cultural transmission. While humans are born with innate language capabilities, language is learned after birth in a social setting. Children learn how to speak by interacting with experienced language users. Language and culture are woven together.\n\nDuality of patterning\nMeaningful messages are made up of distinct smaller meaningful units (words and morphemes) which themselves are made up of distinct smaller, meaningless units (phonemes).\n\nPrevarication\nPrevarication is the ability to lie or deceive. When using language, humans can make false or meaningless statements.\n\nReflexiveness\nHumans can use language to talk about language.\n\nLearnability\nLanguage is teachable and learnable. In the same way as a speaker learns their first language, the speaker is able to learn other languages. It is worth noting that young children learn language with competence and ease; however, language acquisition is constrained by a critical period such that it becomes more difficult once children pass a certain age.\n\nHockett distinguished language from communication. While almost all animals communicate in some way, a communication system is only considered language if it possesses \"all\" of the above characteristics. Some animal communication systems are impressively sophisticated.\n\nAnts make use of the chemical-olfactory channel of communication. Ants produce chemicals called pheromones, which are released through body glands and received by the tips of the antenna. Ants can produce up to twenty different pheromone scents, each a unique signal used to communicate things such as the location of food and danger, or even the need to defend or relocate the colony. When an ant is killed, it releases a pheromone that alerts others of potential danger. Pheromones also help ants distinguish family members from strangers. The queen ant has special pheromones which she uses to signal her status, orchestrate work, and let the colony know when they need to raise princesses or drones.\nAnts will even engage in warfare to protect the colony or a food source. This warfare involves tactics that resemble human warfare. Marauder ants will capture and hold down an enemy while another ant crushes it. Ants are loyal to their colony to the death; however, the queen will kill her own in order to be the last one standing. This level of \"planning\" among an animal species requires an intricate communication.\n\nBird communication demonstrates many features, including the vocal-auditory channel, broadcast transmission/directional reception, rapid fading, semanticity, and arbitrariness. Bird communication is divided into songs and calls. Songs are used primarily to attract mates, while calls are used to alert of food and danger and coordinate movement with the flock. Calls are acoustically simple, while songs are longer and more complex. Bird communication is both discrete and non-discrete. Birds use syntax to arrange their songs, where musical notes act as phonemes. The order of the notes is important to the meaning of the song, thus indicating that discreteness exists. Bird communication is also continuous in the sense that it utilizes duration and frequency. However, the fact that birds have \"phonemes\" does not necessarily mean that they can combine them in an infinite way. Birds have a limited number of songs that they can produce. The male indigo bunting only has one song, while the brown thrasher can sing over 2000 songs. Birds even have unique dialects, depending on where they are from.\n\nHoneybee communication is distinct from other forms of animal communication. Rather than vocal-auditory, bees use the space-movement channel to communicate. Honeybees use two kinds of dances to communicate—the round dance and the waggle dance. They use the round dance to communicate that food is 50–75 meters from the hive. They use the waggle dance when it is farther than this. To do the waggle dance, a bee moves in a zig-zag line and then does a loop back to the beginning of the line, forming a figure-eight. The direction of the line points to the food. The speed of the dance indicates the distance to the food. In this way, bee dancing is also continuous, rather than discrete. Their communication is also not arbitrary. They move in a direction and pattern that physically points out where food is located.\n\nHoneybee dancing also demonstrates displacement, which is generally considered a human characteristic. Most animals will only give a food-found call in the physical presence of food, yet bees can talk about food that is over 100 meters away.\n\n\n"}
{"id": "3770189", "url": "https://en.wikipedia.org/wiki?curid=3770189", "title": "Intensive journal method", "text": "Intensive journal method\n\nThe intensive journal method is a psychotherapeutic technique largely developed in 1966 at Drew University and popularized by Ira Progoff (1921–1998). It consists of a series of writing exercises using loose leaf notebook paper in a simple ring binder, divided into sections to help in accessing various areas of the writer's life. These include a dialogue section for the personification of things, a \"depth dimension\" to aid in accessing the subconscious and other places for recording remembrances and meditations.\n\nThe original Intensive Journal contained only 16 sections, but was later expanded to include five additional sections as part of Progoff's \"process meditation\" method. It has been the inspiration for many other \"writing therapies\" since then and is used in a variety of settings, including hospitals and prisons, by individuals as an aid to creativity or autobiography, and often as an adjunct to treatment in analytic, humanistic or cognitive therapy.\n\nThe intensive journal method is a registered trademark of Progoff and used under license by Dialogue House Associates, Inc of New York, who train facilitators and consultants in the use of the method and coordinate an ongoing series of public workshops using the method throughout the United States and elsewhere.\n\n\n"}
{"id": "43772546", "url": "https://en.wikipedia.org/wiki?curid=43772546", "title": "Interface hypothesis", "text": "Interface hypothesis\n\nThe interface hypothesis in adult second language acquisition is an attempt to explain non-target-like linguistic behavior that persists even among highly advanced speakers. The hypothesis was first put forward by Antonella Sorace.\n\nThe hypothesis posits that for adult second language learners, acquiring grammatical properties within a given linguistic area, such as phonology, syntax, or semantics, should not be problematic. Interfacing between those modules, such as communicating between the syntax and semantic systems, should likewise be feasible. However, grammatical operations where the speaker is required to interface between an internal component of the grammar, and an external component, such as pragmatics or discourse information, will prove to be very difficult, and will not be acquired completely by the second language learner, even at very advanced levels.\n\nExamples of phenomena argued to be influenced by the interface hypothesis include use of overt vs. null subjects, as well as use of subject placement before or after the verb to mark focus vs. using prosody, in languages like Italian by native English speakers.\n\n"}
{"id": "333528", "url": "https://en.wikipedia.org/wiki?curid=333528", "title": "Japanese language and computers", "text": "Japanese language and computers\n\nIn relation to the Japanese language and computers many adaptation issues arise, some unique to Japanese and others common to languages which have a very large number of characters. The number of characters needed in order to write English is very small, and thus it is possible to use only one byte to encode one English character. However, the number of characters in Japanese is much more than 256, and hence Japanese cannot be encoded using only one byte, and Japanese is thus encoded using two or more bytes, in a so-called \"double byte\" or \"multi-byte\" encoding. Some problems relate to transliteration and romanization, some to character encoding, and some to the input of Japanese text.\n\nThere are several standard methods to encode Japanese characters for use on a computer, including JIS, Shift-JIS, EUC, and Unicode. While mapping the set of kana is a simple matter, kanji has proven more difficult. Despite efforts, none of the encoding schemes have become the de facto standard, and multiple encoding standards are still in use today.\n\nFor example, most Japanese emails are in ISO-2022-JP (\"JIS encoding\") and web pages in Shift-JIS and yet mobile phones in Japan usually use some form of Extended Unix Code. If a program fails to determine the encoding scheme employed, it can cause and thus unreadable text on computers.\n\nThe first encoding to become widely used was JIS X 0201, which is a single-byte encoding that only covers standard 7-bit ASCII characters with half-width katakana extensions. This was widely used in systems that were neither powerful enough nor had the storage to handle kanji (including old embedded equipment such as cash registers). This means that only katakana, not kanji, was supported using this technique. Some embedded displays still have this limitation.\n\nThe development of kanji encodings was the beginning of the split. Shift JIS supports kanji and was developed to be completely backward compatible with JIS X 0201, and thus is in much embedded electronic equipment. \n\nHowever, Shift JIS has the unfortunate property that it often breaks any parser (software that reads the coded text) that is not specifically designed to handle it. For example, a text search method can get false hits if it is not designed for Shift JIS. EUC, on the other hand, is handled much better by parsers that have been written for 7-bit ASCII (and thus EUC encodings are used on UNIX, where much of the file-handling code was historically only written for English encodings). But EUC is not backwards compatible with JIS X 0201, the first main Japanese encoding. Further complications arise because the original Internet e-mail standards only support 7-bit transfer protocols. Thus (\"ISO-2022-JP\", often simply called JIS encoding) was developed for sending and receiving e-mails.\n\nIn character set standards such as JIS, not all required characters are included, so gaiji ( \"external characters\") are sometimes used to supplement the character set. Gaiji may come in the form of external font packs, where normal characters have been replaced with new characters, or the new characters have been added to unused character positions. However, gaiji are not practical in Internet environments since the font set must be transferred with text to use the gaiji. As a result, such characters are written with similar or simpler characters in place, or the text may need to be encoded using a larger character set (such as Unicode) that supports the required character.\n\nUnicode was intended to solve all encoding problems over all languages. The UTF-8 encoding used to encode Unicode in web pages does not have the disadvantages that Shift-JIS has. Unicode is supported by international software, and it eliminates the need for gaiji. There are still controversies, however. For Japanese, the kanji characters have been unified with Chinese; that is, a character considered to be the same in both Japanese and Chinese is given a single number, even if the appearance is actually somewhat different, with the precise appearance left to the use of a locale-appropriate font. This process, called Han unification, has caused controversy. The previous encodings in Japan, Taiwan Area, Mainland China and Korea have only handled one language and Unicode should handle all. The handling of Kanji/Chinese have however been designed by a committee composed of representatives from all four countries/areas. Unicode is slowly growing because it is better supported by software from outside Japan, but still (as of 2011) most web pages in Japanese use Shift-JIS. The Japanese Wikipedia uses Unicode.\n\nWritten Japanese uses several different scripts: kanji (Chinese characters), 2 sets of \"kana\" (phonetic syllabaries) and roman letters. While kana and roman letters can be typed directly into a computer, entering kanji is a more complicated process as there are far more kanji than there are keys on most keyboards. To input kanji on modern computers, the reading of kanji is usually entered first, then an input method editor (IME), also sometimes known as a front-end processor, shows a list of candidate kanji that are a phonetic match, and allows the user to choose the correct kanji. More-advanced IMEs work not by word but by phrase, thus increasing the likelihood of getting the desired characters as the first option presented. Kanji readings inputs can be either via romanization (\"rōmaji nyūryoku,\" ) or direct kana input (\"kana nyūryoku,\" ). Romaji input is more common on PCs and other full-size keyboards (although direct input is also widely supported), whereas direct kana input is typically used on mobile phones and similar devices – each of the 10 digits (1–9,0) corresponds to one of the 10 columns in the gojūon table of kana, and multiple presses select the row.\n\nThere are two main systems for the romanization of Japanese, known as \"Kunrei-shiki\" and \"Hepburn\"; in practice, \"keyboard romaji\" (also known as \"wāpuro rōmaji\" or \"word processor romaji\") generally allows a loose combination of both. IME implementations may even handle keys for letters unused in any romanization scheme, such as \"L\", converting them to the most appropriate equivalent. With kana input, each key on the keyboard directly corresponds to one kana. The JIS keyboard system is the national standard, but there are alternatives, like the thumb-shift keyboard, commonly used among professional typists.\n\nJapanese can be written in two directions. \"Yokogaki\" style writes left-to-right, top-to-bottom, as with English. \"Tategaki\" style writes first top-to-bottom, and then moves right-to-left.\n\nAt present, handling of downward text is incomplete. For example, HTML has no support for \"tategaki\" and Japanese users must use HTML tables to simulate it. However, CSS level 3 includes a property \"writing-mode\" which can render \"tategaki\" when given the value \"vertical-rl\" (i.e. top to bottom, right to left). Word processors and DTP software have more complete support for it.\n\n\n"}
{"id": "57760678", "url": "https://en.wikipedia.org/wiki?curid=57760678", "title": "Julia S. Falk", "text": "Julia S. Falk\n\nJulia S. Falk is Professor Emeritus, Linguistics, Michigan State University. She earned her PhD in Lingusitics from the University of Washington in 1968 with a dissertation entitled, \"Nominalizations in Spanish.\" She was apparently the first woman to receive her PhD in this department.\n\nJulia S. Falk is most known for her work on the history of linguistics, and in particular the history of linguistics in the United States from 1900-1950. She has written articles on the early history of the Linguistic Society of America, as well as a number of short intellectual biographies of particular linguists, in journal articles and for various reference works, including the American National Biography and the Encyclopedia of Linguistics. Her work has played an important role in drawing attention to the role of women linguists in the first half of the twentieth century through her book, \"Women, Language and Linguistics\", as well as in Falk (1994, 1995).\n\nA member of North American Association for the History of the Language Sciences (NAAHoLS), she served as president in the year 2000.\n\n\"History of linguistics\"\n\nFalk, Julia S. 1992. Otto Jespersen, Leonard Bloomfield, and American structural linguistics. \"Language\" 68:3.465-91.\n\nFalk, Julia S. 1995. Roman Jakobson and the history of Saussurean concepts in North American linguistics. \"Historiographia Linguistica\" 22:3.335-67.\n\nFalk, Julia S. 1995. Words without grammar: Linguists and the international auxiliary language movement in the United States. \"Language & Communication\" 15:3.241-59.\n\nFalk, Julia S. 1998. Defining linguistics: E.H. Sturtevant and the early years of the Linguistic Society of America. \"16th International Congress of Linguists\", ed. by Bernard Caron, paper no. 0029, CD-ROM. Oxford: Pergamon/Elsevier Science.\n\nFalk, Julia S. 1998. The American shift from historical to non-historical linguistics. \"Language & Communication\" 18:171-80.\n\nFalk, Julia S. 1999. 'Language as a living, cultural phenomenon' -- Gladys Amanda Reichard and the study of native American languages. In \"History of Linguistics\" 1996, ed. by David Cram et al., pp. 111-118. Amsterdam & Philadelphia: John Benjamins.\n\nFalk, Julia S. 2003. Turn to the history of linguistics: Noam Chomsky and Charles Hockett in the 1960s. \"Historiographia Linguistica\" 30:1/2.129-185.\n\nFalk, Julia S. 2004. Saussure and American linguistics. In \"The Cambridge Companion to Saussure\", ed. by Carol Sanders, pp. 107-123. Cambridge: Cambridge University Press. \n\nFalk, Julia S. 2004. Otto Jespersen. In \"The Encyclopedia of Linguistics\", ed. by Philip Strazny, pp. 562-565.\n\n\"Women linguists\"\n\nFalk, Julia S. 1994. The women Foundation Members of the Linguistic Society of America. \"Language\" 70:3.455-90(1994).\n\nFalk, Julia S. 1995. ‘Portraits of women linguists: Louise Pound, Edith Claflin, Adelaide Hahn’. In Kurt R. Jankowsky (ed.), \"History of linguistics 1993\" (pp. 313–320). Amsterdam / Philadelphia: John Benjamins.\n\nFalk, Julia S. 1997. Territoriality, relationships, and reputation: The case of Gladys A. Reichard. \"Southwest Journal of Linguistics\" 16:1/2.17-37.\n\nFalk, Julia S. 1999. \"Women, Language and Linguistics: Three American Stories from the First Half of the Twentieth Century\" [Alice Vanderbilt Morris, Gladys Amanda Reichard, E. Adelaide Hahn]. London and New York: Routledge, 1999.\n"}
{"id": "15779916", "url": "https://en.wikipedia.org/wiki?curid=15779916", "title": "Languages of Norfolk Island", "text": "Languages of Norfolk Island\n\nThere are two official languages of Norfolk Island, English and Norfuk. English, due to the influence of Great Britain and Australia, the two colonial powers who administered Norfolk Island, is the dominant language of the pair. Norfuk, a creole language based on English and Tahitian and brought to the island by the descendants of the Bounty mutineers from Pitcairn Island was spoken by 580 persons according to the 1989 census. It is closely related to Pitkern spoken on Pitcairn Island.\n\n"}
{"id": "20084909", "url": "https://en.wikipedia.org/wiki?curid=20084909", "title": "Languages of the Pitcairn Islands", "text": "Languages of the Pitcairn Islands\n\nThere are two languages of the Pitcairn Islands, English and Pitkern.\n\nPitkern is a creole language based on eighteenth-century English and Tahitian and spoken by about fifty people inland not to mention those outside Adamstown, mostly dozens of children leaving Pitcairn while becoming adults. It is partly derived from eighteenth-century English because Pitcairn Island was settled by the Bounty mutineers in the eighteenth century, and they brought some people from Taihiti with them.\n\nPitkern is closely related to Norfuk spoken on Norfolk Island, where some descendants of the mutineers subsequently settled.\n\n"}
{"id": "10288698", "url": "https://en.wikipedia.org/wiki?curid=10288698", "title": "Legibility", "text": "Legibility\n\nLegibility is the ease with which a reader can recognize individual characters in text. \"The legibility of a typeface is related to the characteristics inherent in its design … which relate to the ability to distinguish one letter from the other.\" Aspects of type design that affect legibility include \"x-height, character shapes, stroke contrast, the size of its counters, serifs or lack thereof, and weight.\"\n\nLegibility is different from readability. Readability is the ease with which a reader can recognize words, sentences, and paragraphs. Legibility is a component of readability. Other typographic factors that affect readability include font choice, point size, kerning, tracking, line length, leading, and justification.\n\n"}
{"id": "2496534", "url": "https://en.wikipedia.org/wiki?curid=2496534", "title": "List of English terms of venery, by animal", "text": "List of English terms of venery, by animal\n\nThis is a list of English terms of venery (\"venery\" being an archaic word for hunting), comprising terms from a tradition that arose in the Late Middle Ages, at least partly from the \"Book of Saint Albans\" of 1486, a historic list of \"company terms\". The present list also includes more common collective terms (such as \"herd\" and \"flock\") for some animals.\n\nStandard terms for particular groups are listed first in each group and shown in bold.\n\n"}
{"id": "33192017", "url": "https://en.wikipedia.org/wiki?curid=33192017", "title": "Mangaya language", "text": "Mangaya language\n\nMangaya (Buga) is a Ubangian language of South Sudan. The endonym is \"Bug\".\n"}
{"id": "4240949", "url": "https://en.wikipedia.org/wiki?curid=4240949", "title": "Manu propria", "text": "Manu propria\n\nIt is also found in several ancient documents in front of or after the writer's signature at the end of the document.\n\nRichly decorated \"manu propria\" signs were frequently used by medieval dignitaries and literates to verify the authenticity of handwritten documents.\n\n\"mppria\" was commonly used in the 18th century. However, it was not only used for Latin documents.\n\n\nLater, official documents were routinely accompanied with this abbreviation, for example declaration of war on Serbia by Emperor Franz Joseph from 1914 ends with \"m.p.\".\n\nOrdinary personal cheques frequently include the abbreviation at the end of the signature line.\n\nSome of the countries that still regularly use \"manu propria\" include:\n\n"}
{"id": "34384656", "url": "https://en.wikipedia.org/wiki?curid=34384656", "title": "Metafunction", "text": "Metafunction\n\nThe term \"metafunction\" originates in systemic functional linguistics and is considered to be a property of all languages. Systemic functional linguistics is functional and semantic rather than formal and syntactic in its orientation. As a functional linguistic theory, it claims that both the emergence of grammar and the particular forms that grammars take should be explained “in terms of the functions that language evolved to serve”. While languages vary in how and what they do, and what humans do with them in the contexts of human cultural practice, all languages are considered to be shaped and organised in relation to three functions, or metafunctions. Michael Halliday, the founder of systemic functional linguistics, calls these three functions the \"ideational\", \"interpersonal\", and \"textual\". The ideational function is further divided into the \"experiential\" and \"logical\".\n\nMetafunctions are \"systemic clusters\"; that is, they are groups of semantic systems that make meanings of a related kind. The three metafunctions are mapped onto the structure of the clause. For this reason, systemic linguists analyse a clause from three perspectives. Halliday argues that the concept of metafunction is one of a small set of principles that are necessary to explain how language works; this concept of function in language is necessary to explain the organisation of the semantic system of language. Function is considered to be \"a fundamental property of language itself\".\n\nAccording to Ruqaiya Hasan, the metafunctions in SFL \"are not hierarchised; they have equal status, and each is manifested in every act of language use: in fact, an important task for grammatics is to describe how the three metafunctions are woven together into the same linguistic unit\". Hasan argues that this is one way in which Halliday's account of the functions of language is different from that of Karl Bühler, for example, for whom functions of language are hierarchically ordered, with the referential function the most important of all. For Buhler, the functions were considered to operate one at a time. In SFL, the metafunctions operate simultaneously, and any utterance is a harmony of choices across all three functions.\n\nThe ideational function is language concerned with building and maintaining a theory of experience. It includes the experiential function and the logical function.\n\nThe experiential function refers to the grammatical choices that enable speakers to make meanings about the world around us and inside us:\n\nHalliday argues that it was through this process of humans making meaning from experience that language evolved. Thus, the human species had to “make sense of the complex world in which it evolved: to classify, or group into categories, the objects and events within its awareness”. These categories are not given to us through our senses; they have to be “construed”. In taking this position on the active role of grammar in construing “reality”, Halliday was influenced by Whorf.\n\nHalliday describes the logical function as those systems “which set up logical–semantic relationships between one clausal unit and another” The systems which come under the logical function are and . When two clauses are combined, a speaker chooses whether to give both clauses equal status, or to make one dependent on the other. In addition, a speaker choose some meaning relation in the process of joining or binding clauses together. Halliday argues that the meanings we make in such processes are most closely related to the experiential function. For this reason, he puts the experiential and logical functions together into the ideational function.\n\nThe interpersonal function refers to the grammatical choices that enable speakers to enact their complex and diverse interpersonal relations. This tenet of systemic functional linguistics is based on the claim that a speaker not only talks about something, but is always talking to and with others. Language not only construes experience, but simultaneously acts out “the interpersonal encounters that are essential to our survival”. Halliday argues that these encounters:\n\nThe grammatical systems that relate to the interpersonal function include Mood, Modality, and Polarity.\n\nHalliday argues that both experiential and interpersonal functions are intricately organized, but that between the two “there is comparatively very little constraint”. This means that “by and large, you can put any interactional ‘spin’ on any representational content”. What allows meanings from these two modes to freely combine is the intercession of a third, distinct mode of meaning that Halliday refers to as the textual function. The term encompasses all of the grammatical systems responsible for managing the flow of discourse. These systems “create coherent text – text that coheres within itself and with the context of situation” They are both structural (involving choices relating to the ordering of elements in the clause), and non-structural (involving choices that create cohesive ties between units that have no structural bond). The relevant grammatical systems include Theme, Given and New, as well as the systems of cohesion, such as Reference, Substitution, and Ellipsis. Halliday argues that the textual function is distinct from both the experiential and interpersonal because its object is language itself. Through the textual function, language “creates a semiotic world of its own: a parallel universe, or ‘virtual reality’ in modern terms”.\n"}
{"id": "871470", "url": "https://en.wikipedia.org/wiki?curid=871470", "title": "Mutual intelligibility", "text": "Mutual intelligibility\n\nIn linguistics, mutual intelligibility is a relationship between languages or dialects in which speakers of different but related varieties can readily understand each other without prior familiarity or special effort. It is sometimes used as an important criterion for distinguishing languages from dialects, although sociolinguistic factors are often also used.\n\nIntelligibility between languages can be asymmetric, with speakers of one understanding more of the other than speakers of the other understanding the first. When it is relatively symmetric, it is characterized as \"mutual\". It exists in differing degrees among many related or geographically proximate languages of the world, often in the context of a dialect continuum.\n\nLinguistic distance is the name for the concept of calculating a measurement for how different languages are from one another. The higher the linguistic distance, the lower the mutual intelligibility. One common metric used is the Levenshtein distance.\n\nFor individuals to achieve moderate proficiency or understanding in a language (called L2) other than their first language (L1) typically requires considerable time and effort through study and/or practical application. However, many groups of languages are partly mutually intelligible, i.e. most speakers of one language find it relatively easy to achieve some degree of understanding in the related language(s). Often the languages are genetically related, and they are likely to be similar to each other in grammar, vocabulary, pronunciation, or other features.\n\nIntelligibility among languages can vary between individuals or groups within a language population according to their knowledge of various registers and vocabulary in their own language, their exposure to additional related languages, their interest in or familiarity with other cultures, the domain of discussion, psycho-cognitive traits, the mode of language used (written vs. oral), and other factors.\n\nThere is no formal distinction between two distinct languages and two varieties of a single language, but some linguists use mutual intelligibility as one of the primary factors in deciding between the two cases.\n\nSome linguists claim that mutual intelligibility is, ideally at least, the primary criterion separating languages from dialects. On the other hand, speakers of closely related languages can often communicate with each other; thus there are varying degrees of mutual intelligibility, and often other criteria are also used. As an example, in the case of a linear dialect continuum that shades gradually between varieties, where speakers near the center can understand the varieties at both ends, but speakers at one end cannot understand the speakers at the other end, the entire chain is often considered a single language. If the central varieties then die out and only the varieties at both ends survive, they may then be reclassified as two languages, even though no actual language change has occurred.\n\nIn addition, political and social conventions often override considerations of mutual intelligibility. For example, the varieties of Chinese are often considered a single language even though there is usually no mutual intelligibility between geographically separated varieties. Another similar example would be varieties of Arabic. In contrast, there is often significant intelligibility between different Scandinavian languages, but as each of them has its own standard form, they are classified as separate languages. There is also significant intelligibility between Thai languages of different regions of Thailand.\n\nTo deal with the conflict in cases such as Arabic, Chinese and German, the term \"Dachsprache\" (a sociolinguistic \"umbrella language\") is sometimes seen: Chinese and German are languages in the sociolinguistic sense even though some speakers cannot understand each other without recourse to a standard or prestige form.\n\nAsymmetric intelligibility refers to two languages that are considered partially mutually intelligible, but where one group of speakers has more difficulty understanding the other language than the other way around. There can be various reasons for this. If, for example, one language is related to another but has simplified its grammar, the speakers of the original language may understand the simplified language, but less vice versa. For example, Dutch speakers tend to find it easier to understand Afrikaans than vice versa as a result of Afrikaans's simplified grammar.\n\nPerhaps the most common reason for apparent asymmetric intelligibility is that speakers of one variety have more exposure to the other than vice versa. For example, speakers of Scottish English have frequent exposure to standard American English through movies and TV programs, whereas speakers of American English have little exposure to Scottish English; hence, American English speakers often find it difficult to understand Scottish English or, especially, Scots (which differs significantly from standard Scottish English), whereas Scots tend to have few problems understanding standard American English.\n\nNorthern Germanic languages spoken in Scandinavia form a dialect continuum where two furthermost dialects have almost no mutual intelligibility. As such, spoken Danish and Swedish normally have low mutual intelligibility, but Swedes in the Öresund region (including Malmö and Helsingborg), across a strait from the Danish capital Copenhagen, understand Danish somewhat better, largely due to the proximity of the region to Danish-speaking areas (see \"Mutual intelligibility in North Germanic languages\"). While Norway was under Danish rule, the Bokmål written standard of Norwegian originates from Dano-Norwegian, a koiné that evolved among the urban elite in Norwegian cities during the later years of the union. Additionally, Norwegian assimilated a considerable amount of Danish vocabulary as well as traditional Danish expressions. As a consequence, spoken mutual intelligibility is not reciprocal.\n\nSimilarly, in Germany and Italy, standard German or Italian speakers may have great difficulty understanding the \"dialects\" from regions other than their own, but virtually all \"dialect\" speakers learn the standard languages in school and from the media.\n\nBelow is an \"incomplete\" list of fully and partially mutually intelligible varieties sometimes considered languages.\n\n\n\n\n\n\nBecause of the difficulty of imposing boundaries on a continuum, various counts of the Romance languages are given; in \"The Linguasphere register of the world’s languages and speech communities\" David Dalby lists 23 based on mutual intelligibility:\n\n\n\n"}
{"id": "14406411", "url": "https://en.wikipedia.org/wiki?curid=14406411", "title": "Nepal Bhasa movement", "text": "Nepal Bhasa movement\n\nNepal Bhasa movement (Nepal Bhasa: नेपालभाषा आन्दोलन) refers to the struggle for linguistic rights by its speakers in Nepal in the face of opposition from the government and hostile neighbors. The campaign aims to increase the use of Nepal Bhasa in the home, education, government and business. Despite a high level of development, Newar culture and language are both under threat.\n\nNewars have been fighting to save their language from the time of the repressive Rana regime till today, and activists have been jailed, exiled and tortured. Opponents have even petitioned the Supreme Court to have its use barred. The history of Nepal Bhasa since the late 18th century has been marked by constant struggle against state repression and a hostile environment.\n\nThe movement arose against the suppression of the language by the state that began with the rise of the Shah dynasty in 1768 AD, and intensified during the Rana regime (1846-1951) and Panchayat system (1960-1990). Moreover, hostility towards the language from neighbors grew following mass migration into the Kathmandu Valley, leading to the indigenous Newars becoming a minority in their homeland. During the period 1952 to 1991, the percentage of the valley population speaking Nepal Bhasa dropped from 74.95% to 43.93%. The language has been listed as being \"definitely endangered\" by UNESCO.\n\nThe language movement can be divided into the following eras.\n\n\nFollowing the advent of the Shahs, the Gorkha language became the court language of Nepal, and Nepal Bhasa was replaced as the language of administration. Open suppression began in 1906 with documents in Nepal Bhasa being declared not admissible in court. In the subsequent years, authors were fined, whipped, imprisoned or expelled and their books confiscated. It was illegal to sing hymns in Nepal Bhasa or speak it on the telephone. As a result, development of the language and literature was stifled.\n\nDuring the period 1909 to 1941 known as the Renaissance era, a few authors braved official disapproval and started writing, translating, educating and restructuring the language. Writers Nisthananda Bajracharya, Siddhidas Mahaju, Jagat Sundar Malla and Yogbir Singh Kansakar are honored as the Four Pillars of Nepal Bhasa. Shukraraj Shastri and Dharmaditya Dharmacharya were also at the forefront of the movement.\n\nIn 1909, Bajracharya published the first printed book using movable type. Shastri wrote a grammar of the language entitled \"Nepal Bhasa Vyakarana\" which was published from Kolkata, India in 1928. Dharmacharya published the first magazine in Nepal Bhasa \"Buddha Dharma wa Nepal Bhasa\" from Kolkata in 1925. Also, the Renaissance marked the beginning of the movement to get official recognition for the name \"Nepal Bhasa\" in place of the Khas imposed term \"Newari\".\n\nIn 1940, the government mounted a crackdown against democracy activists and writers in which Shukraraj Shastri was hanged. A large number of authors were imprisoned for their literary or political activities. Dharmacharya was jailed for three months. Chittadhar Hridaya was sentenced to six years, Siddhicharan Shrestha was sentenced to 18 years, Phatte Bahadur Singh was sentenced to life imprisonment and Dharma Ratna Yami was sentenced to 18 years. They were released in 1945 after serving five years. In prison, Hridaya produced his greatest work \"Sugata Saurabha\", an epic poem on the life of the Buddha. Shrestha wrote a collection of poems entitled \"Seeswan\" (\"Wax Flower\", published in 1948) among other works.\n\nTheravada Buddhist monks stood up to the Rana regime and published books on Buddhism from India and brought them into Nepal. This led to the Banishment of Buddhist monks from Nepal. In 1944, eight monks were expelled for refusing to stop teaching Theravada Buddhism and writing in Nepal Bhasa. In 1946, the ban on writing was lifted, and the monks were allowed to return to Nepal following international pressure. Their writings and public activities in Nepal Bhasa had a profound impact on the development of the language.\n\nNepal Bhasa lovers took the movement to India and Tibet where they formed associations to organize writers and bring out publications to escape government suppression in Nepal. Newar merchants based in Kolkata, Kalimpong and Lhasa were major patrons of the language movement.\n\n\"Buddha Dharma wa Nepal Bhasa\", the first ever magazine in Nepal Bhasa, was published from Kolkata in 1925 by Dharmaditya Dharmacharya. He also established a literary organization named Nepal Bhasa Sahitya Mandal (Nepal Bhasa Literature Organization) in 1926.\n\nIn 1944, the Buddhist monks expelled from Nepal went to Sarnath and formed an organization named Dharmodaya Sabha. In 1947, it launched \"Dharmodaya\" magazine from Kalimpong funded by trader Maniharsha Jyoti Kansakar. Monk Aniruddha Mahathera was the first editor.\n\nNewar traders based in Lhasa promoted \"Thaunkanhe\" magazine which launched in Kathmandu in 1951. It was the first Nepal Bhasa magazine to be published from Nepal, and the first editor was a former merchant Purna Kaji Tamrakar.\n\nIn 1950, a literary society named Chwasa Pasa (Pen Friend) was formed in Kolkata by Prem Bahadur Kansakar and another writer Madan Lochan Singh to bring together writers living in exile. The society moved to Nepal in 1951 after the advent of democracy.\n\nFollowing the fall of the Ranas and advent of democracy in 1951, there was greater linguistic freedom, and Nepal Bhasa struggled to catch up. Books, magazines and newspapers appeared. A daily newspaper \"Nepal Bhasa Patrika\" began publication in 1955.\n\nState-owned Radio Nepal began broadcasting the news in Nepal Bhasa once a day in 1951. In 1958, Kathmandu Municipality passed a resolution that it would accept applications and publish major decisions in Nepal Bhasa in addition to the Nepali language.\n\nNepal Bhasa entered a vibrant phase in the educational system. It was included in the curriculum, and Nepal Rastriya Vidhyapitha recognized it as an alternative medium of instruction in the schools and colleges affiliated to it. In 1953, the government recognized Nepal Bhasa as a spoken language and an oriental language subject. The Nepal Educational Council adopted it as an optional subject.\n\nFollowing lobbying by language lovers, Nepal Bhasa was included in the course of study at the high school level in 1954, at the intermediate level in 1960 and bachelor level in 1962. And for two decades, Nepal Bhasa was widely taught in schools and colleges in the Kathmandu Valley and other parts of Nepal with thousands of students studying it as an optional subject.\n\nIn 1960, parliament was abolished, political parties were forbidden and the Panchayat system was established. Under this autocratic system, the government followed a one-language policy, and Nepal Bhasa suffered another period of suppression. It was gradually pushed out of the media and the educational institutions, triggering protest movements.\n\nThe restrictive policy of Panchayat encouraged the formation of literary associations to provide a forum for writers to present their works. In 1962, Birat Nepal Bhasa Sahitya Sammelan Guthi (Grand Nepal Bhasa Literary Conference Trust) was formed in Bhaktapur. It organized annual literary meets which continue till today.\n\nIn 1960, Nepal Bhasa students at Tri-Chandra College in Kathmandu launched an annual magazine named \"Jah\" (ज:) (meaning \"light\" in Nepal Bhasa) after the college magazine \"Light\" refused to include Nepal Bhasa articles in it. The magazine led to the organization of students interested in Nepal Bhasa in the college. That same year, they started the annual Inter-College Nepal Bhasa Literary Conference and also produced a weekly radio program in Nepal Bhasa on Radio Nepal named \"Jeevan Dabu\" (\"Life's stage\"). In the successive years, \"Jah\" served as a breeding ground for new writers and language activists.\n\nIn 1963, Kathmandu Municipality's decision to recognize Nepal Bhasa was revoked. In 1965, the language was banned from being broadcast over Radio Nepal. The removal of Nepal Bhasa from the country's only radio station sparked a protest movement which became known as the Movement of 1965 (\"Bais Salya Andolan\"). The protest took the form of literary meets as other types of demonstrations were prohibited. Literary programs were held weekly in market squares and courtyards where participants recited poems and sang songs containing critical messages.\n\nThe government cracked down by putting a number of activists in jail including writers Mangal Man Shakya, Pushpa Gopal Shrestha and Shree Krishna Anu. Buddhist monk Bhikshu Sudarshan Mahasthavir was jailed for six months and six days. Other prominent campaigners like Durga Lal Shrestha, Hitkar Bir Singh Kansakar and Mangal Man Shakya (of Om Bahal) were forced to go underground. The movement was made leaderless, and it came to a stop after a year.\n\nThe ban on Nepal Bhasa remained, but the 1965 Movement succeeded in raising awareness about linguistic rights and arousing public opinion against the Panchayat regime. Its most important achievement was creating a new generation of writers and campaigners who would take over from the activists who fought for the language during the Rana regime and lead the movement in the subsequent decades. Nepal Bhasa Manka Khala, founded in 1979, is one of the organizations that emerged during this period to struggle for language rights.\n\nThe New Education System Plan brought out in 1971 disrupted the popular study of Nepal Bhasa in educational institutions. Under the plan, vernacular subjects were removed from the curriculum, and students were forced to choose between the mother tongue and technical subjects, leading to Nepal Bhasa being pushed into the background. Student enrolment in the course dropped, resulting in the subject being pulled out of many schools and a further decline in students.\n\nFrom 1977, students could study Nepal Bhasa at the Master's level; but the university degree was not counted for promotion by the government, and so there were few takers for the subject.\n\nIn addition to government suppression, Nepal Bhasa has faced opposition from hostile critics. After Sri Lanka Broadcasting Corporation began broadcasting a weekly half-hour program in Nepal Bhasa on 6 November 1983, opponents in Nepal pressured the radio station to shut it down. Similar opposition led to All India Radio - Kurseong stopping broadcasting Nepal Bhasa songs during its Nepali service in 1966.\n\nArbitrary actions increased resentment towards Panchayat. In 1988, police arrested participants in a procession marking the birth anniversary of Nepal Bhasa poet Siddhidas Mahaju.\nAfter the 1990 People's Movement that brought the Panchayat system to an end, the languages of Nepal enjoyed greater freedom. However, efforts to get Nepal Bhasa reincluded as an optional subject after a gap of two decades were unsuccessful due to lack of funding.\n\nThe 1990 constitution recognized Nepal as a multi-ethnic and multilingual country. It also said that each community shall have the right to operate schools up to the primary level in its own mother tongue. However, the government's responsibility was not stated and communities had to open such schools on their own. In 1991, Jagat Sundar Bwane Kuthi, the first Nepal Bhasa-medium school, opened in Kathmandu with the efforts of volunteers and the support of domestic and foreign donors.\n\nMeanwhile, years of efforts to gain official recognition for the standard name Nepal Bhasa finally succeeded. On 8 September 1995, the government decided that the name Nepal Bhasa should be used instead of Newari. However, the Central Bureau of Statistics has not been doing so.\n\nOn 25 July 1997, Kathmandu Metropolitan City declared that its policy to recognize Nepal Bhasa, which had been passed on 18 April 1958 and cancelled by the Panchayat regime in 1963, would be revived. The rest of the city governments in the Kathmandu Valley announced in quick succession that they too would officially recognize Nepal Bhasa -- Lalitpur Sub-Metropolitan City on 14 July, Bhaktapur Municipality on 23 July, Madhyapur Thimi Municipality on 10 September, and Kirtipur Municipality on 15 September 1997.\n\nWhile Newars hailed the move, its detractors went to court. A group consisting of Lal Bahadur Thapa, Yagyanidhi Dahal, Hari Prasad Pokhrel, Achyut Raman Adhikari and Dhruba Raj Thebe filed a joint petition at the Supreme Court of Nepal against their decision. And on 18 March 1998, the Supreme Court issued a stay order preventing Kathmandu Metropolitan City from using Nepal Bhasa pending a final verdict.\n\nSubsequently, on 1 June 1999, the Supreme Court quashed the decision of the local bodies as being unconstitutional. After the verdict was announced, demonstrators marched through Kathmandu in protest.\n\nNewars have been observing June 1 as Black Day with protest meets and public demonstrations recalling the day when the Supreme Court barred Nepal Bhasa from being used in local bodies. On some occasions, the protests have been marked by vandalism and arrest of protestors by the police.\n\nA second People's Movement in 2006 ousted the Shah dynasty and Nepal became a republic which gave the people greater linguistic freedom. The 2007 Interim Constitution stated that the use of one's mother tongue in a local body or office shall not be barred. However, subtle discrimination persists. Organizations with names in Nepal Bhasa are not registered, and municipality officials refuse to accept applications written in the language.\n\n"}
{"id": "53212790", "url": "https://en.wikipedia.org/wiki?curid=53212790", "title": "Norvegia transcription", "text": "Norvegia transcription\n\nNorvegia (Latin for \"Norway\") is a phonetic transcription system which was developed by Norwegian linguist Johan Storm in 1884. Norvegia is still employed in the teaching of Scandinavian studies at Norwegian universities.\n\n\n\n"}
{"id": "35653834", "url": "https://en.wikipedia.org/wiki?curid=35653834", "title": "Omok language", "text": "Omok language\n\nOmok is an extinct Yukaghir language of Siberia, part of a dialect continuum with the two surviving languages. It was last spoken perhaps as late as the 18th century.\n"}
{"id": "25231955", "url": "https://en.wikipedia.org/wiki?curid=25231955", "title": "Opening sentence", "text": "Opening sentence\n\nAt the beginning of a written work stands the opening sentence. The opening line is part or all of the opening sentence that may start the lead paragraph. For older texts the Latin term \"incipit\" (it begins) is in use for the very first words of the opening sentence.\n\nAs in speech, a personal document such as a letter normally starts with a salutation; this, however, tends not to be the case in documents, articles, essays, poetry, lyrics, and general works of fiction and nonfiction. In nonfiction, the opening sentence generally points the reader to the subject under discussion directly in a matter-of-fact style. In journalism, the opening line typically sets out the scope of the article.\n\nIn fiction, authors have much liberty in the way they can cast the beginning.\nTechniques to hold the reader's attention include keeping the opening sentence to the point, showing attitude, shocking, and being controversial. One of the most famous opening lines, \"It was the best of times, it was the worst of times ...\", starts a sentence of 118 words that draws the reader in by its contradiction; the first sentence of \"Yes\" even contains 477 words. \"Call me Ishmael\" is an example of a short opening sentence. Formulaic openings are generally eschewed, but expected in certain genres, such as fairy tales beginning \"Once upon a time...\".\n\nInspired by the opening, \"It was a dark and stormy night...\", the annual tongue-in-cheek Bulwer-Lytton Fiction Contest invites entrants to compose \"the opening sentence of the worst of all possible novels\", and its derivative, the Lyttle Lytton Contest, for its equivalent in brevity.\n\nThe opening sentence may sometimes be also used as the title for the work, e.g. \"Everything I Possess I Carry With Me\"; papal encyclicals and bulls are titled according to their incipits.\n\n"}
{"id": "27462322", "url": "https://en.wikipedia.org/wiki?curid=27462322", "title": "Partner-assisted scanning", "text": "Partner-assisted scanning\n\nPartner-assisted scanning or listener-assisted scanning is an augmentative and alternative communication technique used to enable a person with severe speech impairments to communicate. The approach is used with individuals who, due to sickness or disability, have severe motor impairments and good memory and attention skills. It is used as an alternative to direct access (e.g. pointing) to symbols, pictures, or speech generating devices when these are not used.\n\nPartner-assisted scanning is a technique used with children who have severe motor and communication impairments, and especially those with additional visual impairment, those who do not yet have an established alternative form of communication, or who are unable to use their usual method, perhaps because their electronic speech output device is being repaired. Adults may also use scanning with a partner when they are not using their more high-tech alternative communication device. Partner-assisted scanning can also be the main means of communication for adults in late stages of diseases such as Amyotrophic Lateral Sclerosis (ALS), multiple sclerosis (MS) or those in intensive care.\n\nIn partner-assisted scanning, the communication partner presents messages or letter choices in a sequential fashion to the individual who wishes to communicate something, and the individual then makes their selection. Scanning refers to the process of items presented one after the other, in the same patterns, until a choice is made as the desired item is reached. Items can be presented either visually, by pointing, or auditorily, by speaking.\n\nEfficiency may be increased in visual partner scanning by the partner first pointing to groups of items, such as rows of letters, and once a row has been selected, proceeding to point to all letters in that row until a choice is made. The communicator can spell words this way in order to express what they need to communicate. Similarly, the partner can point to groups of words. Visual scanning may also be accomplished by the partner pointing to pictures, such as those in a personal communication book, using an agreed upon pattern.\n\nThe selecting system can be divided into two categories: alphanumeric and choice making. These differ, because in the alphanumeric version you scan through letters and numbers. Choice-making is where you present multiple choices. For example, when asked \"Would you like a movie or book?\", the communicator indicates a \"yes\" response. They can do this in a variety of ways, like facial expressions, vocalizations, or body gestures. To determine the best way, one's team of caretakers has to look at the individual's abilities.\n\nAuditory scanning with a partner is often used when the communicator has very poor vision. Groups of letters can be represented by numbers, such as \"1=abcdef\". The partner lists off the numbers, and once a group is selected, he or she will name all letters in that group. Auditory scanning can also be accomplished with lists of novel messages. The partner orally lists the options and then repeats them so the communicator can make a selection. This could be a short list of emotions to inquire as to how someone is feeling (e.g. happy, sad, frustrated). The partner and communicator can memorize many lists, or \"menus\", with sub-lists covering different topics for communication. It is important that lists are repeated in the same order and with pauses after each item, so that there is time to respond.\n\nJean-Dominique Bauby, who had locked-in syndrome, used partner-assisted scanning to communicate and to write his book \"The Diving Bell and the Butterfly\".\n\n\n"}
{"id": "23264192", "url": "https://en.wikipedia.org/wiki?curid=23264192", "title": "Performative text", "text": "Performative text\n\nIn the philosophy of language, the notion of performance conceptualizes what a spoken or written text can bring about in human interactions.\n\nIn the 1950s the philosopher of language J. L. Austin introduced the term 'performative utterance' to make clear that 'to say something is to do something'. Developing this idea, scholars have theorized on the relation of a spoken or written text to its broader context, that is to say everything outside the text itself. The question whether a performative is separable from the situation it emerged in is relevant when one addresses for example the status of individual intentions or speech as a resource of power. There are two main theoretical strands in research today. One emphasizes the predetermined conventions surrounding a performative utterance and the clear distinction between text and context. Another emphasizes the active construction of reality through spoken and written texts and is related to theories of human agency and discourse. The ideas about performance and text have contributed to the performative turn in the social sciences and humanities, proving their methodological use for example in the interpretation of historical texts.\n\nEarly theories acknowledge that performance and text are both embedded in a system of rules and that the effects they can produce depend on convention and recurrence. In this sense, text is an instance of 'restored behaviour', a term introduced by Richard Schechner that sees performance as a repeatable ritual. The focus here is largely on individual sentences in the active first person voice, rather than on politics or discourse. The syntactical analyses are firmly anchored in analytical epistemology, as the distinction between the research object and its context is not conceived as problematic.\n\nJ. L. Austin introduced the performative utterance as an additional category to 'constatives', statements that can be either true or false. Language not only represents, but also can make something happen. Austin distinguishes between two types of performative speech acts. The illocutionary act is concerned with what an actor is doing in saying something (e.g. when someone says 'hello', he is greeting another person). The perlocutionary act involves the unintended consequences of an utterance and refers to that what an actor is doing by saying something (e.g. when someone says 'hello' and the greeted person is scared by it).\n\nEvery performative utterance has its own procedure and risks of failure that Austin calls 'infelicities'. He sees a sharp distinction between the individual text and the 'total speech act situation' surrounding it. According to Austin, in order to successfully perform an illocutionary act, certain conditions have to be met (e.g. a person who pronounces a marriage must be authorized to do so). Besides the context, the performative utterance itself is unambiguous as well. The words of an illocutionary act have to be expressed in earnest; if not, Austin discards them as a parasitic use of language.\n\nBuilding on Austin's thought, language philosopher John Searle tried to develop his own account of speech acts, suggesting that these acts are a form of rule-governed behaviour. On the one hand, Searle discerns rules that merely regulate language, such as referring and predicating. These rules account for the 'propositional content' of our sentences. On the other hand, he discerns rules that are constitutive in character and define behaviour (e.g. when we make a promise). These rules are the conventions underlying performative utterances and they enable us not only to represent and express ourselves, but also to communicate.\n\nThis focus on effect implies a conscious actor and Searle assumes that language stems from an intrinsic intentionality of the mind. These intentions set the prerequisites for the performance of speech acts and Searle sets out to map their necessary and sufficient conditions. Like Austin, he thinks in terms of demarcated contexts and transparent intentions, two issues that in the 1970s led him into polemics with postmodern thinker Jacques Derrida.\n\nThe second set of theories on performance and text diverged from the tradition represented by Austin and Searle. Bearing the stamp of postmodernism, it states that neither the meaning, nor the context of a text can be defined in its entirety. Instead of emphasizing linguistic rules, scholars within this strand stress that the performative utterance is intertwined with structures of power. Because a text inevitably changes a situation or discourse, the distinction between text and context is blurred.\n\nThe postmodern philosopher Jacques Derrida holds with Austin and Searle that by illocutionary force, language itself can transform and effect. However, he criticizes the notion of 'felicity conditions' and the idea that the success of a performative utterance is determined by conventions. Derrida values the distinctiveness of every individual speech act, because it has a specific effect in the particular situation in which it is performed. It is because of this effect or 'breaking force' that Derrida calls the possibility of repeating a text 'iterability', a word derived from Latin \"iterare\", to repeat.\n\nAccording to Derrida, the effects caused by a performative text are in a sense also part of it. In this way, the distinction between a text and that what is outside it dissolves. For this reason it is pointless to try to define the context of a speech act.\nBesides the consequential effects, the dissolution of the text-context divide is also caused by iterability. Due to the possibility of repetition, the intentions of an individual actor can never be fully present in a speech act. The core of a performative utterance is therefore not constituted by animating intentions, as Austin and Searle would have it, but by the structure of language.\n\nThe philosopher Judith Butler offers a political interpretation of the concept of the performative utterance. Power in the form of active censorship defines and regulates the domain of a certain discourse. Indebted to the work of Michel Foucault, Butler expounds how subjects are produced by their context, because the possibilities of speech are predetermined.\n\nNotwithstanding such social restraints, Butler underscores the possibility of agency. The boundaries of a discourse need continuous re-demarcation and this is where speech can escape its constriction. The emphasis on the limits of what is allowed to be said also frames that what is silenced.\nPerformativity has a political aspect that consists in what Derrida has described as the breaking force, by which an utterance changes its context. Butler assigns an important role to what Austin has called infelicities and parasitic uses of language. Quotations, parodies and other deviations from official discourse can become instruments of power that affect society.\n\nThe historian Quentin Skinner developed classical and postmodern theories on performative texts into a concrete research method. Using Austin's vocabulary, he seeks to recover what historical authors were doing in writing their texts, which corresponds with the performance of illocutionary acts. According to Skinner, philosophical ideas are intertwined with claims of power. Every text is an act of communication that positions itself in relation to the status quo it seeks to change.\n\nSkinner agrees with Derrida that contexts in their entirety are irretrievable but nevertheless states that there is a relevant context outside the text that can be described in a plausible way. Extensive research is required to relate historical texts to their contemporary discourses. According to Skinner 'there is a sense in which we need to understand why a certain proposition has been put forward if we wish to understand the proposition itself'. He values agency over structure and stresses the importance of authorial intentions. Skinner therefore proposes to study historical sources in order to retrieve the convictions the author held, reflect on their coherence and investigate possible motives for the illocutionary act. This practical method seeks to deal with the blurred distinction between text and context and offer a meaningful way of interpreting historical reality.\n\n\n"}
{"id": "23443371", "url": "https://en.wikipedia.org/wiki?curid=23443371", "title": "Plataforma per la Llengua", "text": "Plataforma per la Llengua\n\nLa Plataforma per la Llengua (the Pro-Language Platform) is a non-governmental organisation born in 1993 in Barcelona (Catalonia), in order to defend and promote Catalan language all over the Catalan-speaking territories in the European States where it is spoken: Spain, France, Andorra and Italy. Their activities include the development of sociolinguistic studies and a constant monitoring on the status of the Catalan language, in collaboration with other organizations, foundations and public agencies. Even it is based in Barcelona, it has delegations in Valencian Community, Alghero and in several Catalan regions. Moreover, La Plataforma per la Llengua works together with some Northern Catalonia, La Franja, Andorra and Balearic Islands organisations. There are so many aims; one of them is to guarantee the linguistic rights to Catalan-speaking people and the use of Catalan language as a connexion tool in all the fields and territories where it is spoken .\n\nAs of 2016, there were 12,000 associates, more than 55,000 followers on Facebook and more than 30,000 followers on Twitter. Also, every six months, the NGO publishes a magazine called Corbella whose print run is 9,000 copies. In 2008, this magazine was awarded a prize for being the best cultural association media. The prize was awarded by the Federació d’Ateneus de Catalunya.\n\nThe Association is supported by a consultative council consisting of Salvador Cardús, Jordi Font, Josep M. López Llaví, Albert Manent, Isidor Marí, Fèlix Martí, Jordi Porta, Jordi Sànchez, Josep Mª Terricabras, Miquel Sellarès, Vicenç Villatoro, Isabel-Clara Simó, Miquel Strubell, Patrícia Gabancho and Abelard Saragossà.\n\nOn 23 September 2012, Martí Gasull i Roig, one of the organisation's founders and the main Plataforma per la Llengua coordinator, died in an accident on a mountain (Manaslu), in Nepal. Martí Gasull i Roig work’s has been appreciated many times. On 9 October 2012, Generalitat de Catalunya awarded him the posthumous title of Creu de Sant Jordi. Among posthumous prizes there are: Premi d’Obra Cultural Balear Aina Moll i Marquès a la nit de la Cultura 2012, 'El Segador de l'any' by Reagrupament, UEC and Gràcia and Barcelona local government plens municipals recognition and the Barcelona local government Medalla d'Or al Mèrit Cultural. On 15 January 2013, Plataforma per la Llengua honoured Martí Gasull Roig in the Sala Gran del Teatre Nacional de Catalunya, in Barcelona. It approved the Premi Martí Gasull i Roig basis in order to award a person or an organisation that has highlighted in the defence of Catalan language once a year.\n\nThis is the mission as defined by the organisation:\n\n\nLa Plataforma per la Llengua was born in 1993. His first public action was collecting 18,000 Coca-Cola tins, which was a score, in Plaça de Catalunya, in Barcelona, 12 December 1993. The goal was asking for Catalan business tagging. A draw that said \"Etiquetem en català\" (Let’s tag in Catalan) was made with all the collected tins. Since then, some more campaigns took place in tagging area, cinema and in the Catalan recognition.\n\nBetween 1995 and 2000 some protestation acts took place in cinemas to ask for dubbing and Catalan subtitling. Also, there were campaigns to pressure the government and the multinationals with massive messages, certificates and public reports to media. In 2001, relevant multinationals like Warner made films in Catalan about the Story of Harry Potter. In 2011, La Plataforma per la Llengua made Catalan government write a cinema law that guarantee 50% of copies in Catalan. However, in 2013 it hasn’t been yet enforced. In 1997, La Plataforma per la Llengua played a relevant role in the discussion of other laws filed by the Generalitat de Catalunya, like the Act on linguistic policy, the Statute of Autonomy of Catalonia 2006, the Act on reception for immigrants and returnees to Catalonia 2010 or Catalan Consumer Code 2010. Some small changes were made in the documents that involved Catalan language, but most of them haven’t been yet applied.\n\nIn business area, there was a very unfavourable situation in 2000, and by intensive campaigns, some goals were achieved: Catalan language was introduced in some big multinationals like Apple, Microsoft, Vodafone, Orange, Telefónica, Carrefour, Nokia, Samsung, Siemens, Carrefour, Decathlon, Schlecker, Facebook, Google, Twitter or IKEA.\n\nFrom 2000 La Plataforma per la Llengua started growing. Before, it was an organisation supported by Col•lectiu l’Esbarzer, but then, an associates basis and a professional structured were created. A new work methodology was applied, reports were written and a consumer net appear. Some events were organised in order to make citizens realise about the linguistic points and claim a business improvement as well as an autonomic government improvement of Catalonia, Valencian Community, Aragon and Balearic Islands and Spanish, French and Italian States.\nIn 2004, the first Festa de la Joguina en Català (The Game Party in Catalan) was celebrated some days before Christmas. Since then, it has been celebrated once a year to claim children can play in Catalan. In 2005, the first Festa d’Acollida d’Estudiants Universitaris Erasmus (Welcome party for university students with Erasmus Programme) in Barcelona to increase Catalan language in university courses and make new arrival students use Catalan in university courses. In 2005, the first Dia de Sant Jordi act (Saint George's Day), Catalonia’s patron, was celebrated in Plaça de Catalunya, in Barcelona, with other immigrants organisations to reclaim the use of Catalan as a common language. Since then, the organisation has worked together with a 30 immigrant organisations to established the linguistic diversity of Catalan Community and the use of Catalan as a cohesion tool, by using a hundred linguistic partners formed by a Catalan and a person who wants to learn it, and by videos about social cohesion in Catalan that have had a thousand visitors.\n\nLa Plataforma per la Llengua put into groups a hundred local governments and Catalan institutions that had the compromise to use only the products tagged in Catalan. After having achieved tagging in Catalan in the products like wine, cava, water, milk and beer, finally, after strong campaigns towards the government, in 2010, the Catalan Consumer Code 2010 comprised the obligation of tagging in Catalan the products distributed in Catalonia.\n\nLa Plataforma per la Llengua has complained again by campaigns that Catalan is the only European language with so many million speakers that is neither official of European Union nor of a state of European Union. In 2009 a report was written, it complained more than 500 actions that Catalan people need to use Spanish Language and the valid Catalan banning in Spain.\n\nLa Plataforma per la Llengua has acted to answer to the Government of Spain attacks and to the some other Spanish Autonomic Government attacks. These attacks were against people who spoke Catalan, specially, in learning and against the recognition of the official language too. Complaints about failure to take action and about Spanish police aggressions, as well as the aggressions of other civil servants against citizens that express themselves in Catalan.\n\nThroughout 20 years, this organization has received several awards, including the 2008 National Prize for Culture for the social projection of the Catalan language, given by the Government of Catalonia: Generalitat de Catalunya; Premi Abacus, awarded in 2010; Premi de Voluntariat, in 2008, 2010 and 2012, and Premi al millor projecte associatiu del Consell Nacional de la Joventut de Catalunya, awarded in 2012.\n\n"}
{"id": "596795", "url": "https://en.wikipedia.org/wiki?curid=596795", "title": "Rigid designator", "text": "Rigid designator\n\nIn modal logic and the philosophy of language, a term is said to be a rigid designator or absolute substantial term when it designates (picks out, denotes, refers to) the same thing in \"all possible worlds\" in which that thing exists and does not designate anything else in those possible worlds in which that thing does \"not\" exist. A designator is \"persistently rigid\" if it designates the same thing in every possible world in which that thing exists and designates nothing in all other possible worlds. A designator is \"obstinately rigid\" if it designates the same thing in every possible world, period, whether or not that thing exists in that world. Rigid designators are contrasted with \"connotative terms\", \"non-rigid\" or \"flaccid designators\", which may designate different things in different possible worlds.\n\nThe Scholastic philosophers in the Middle Ages developed a theory of properties of terms in which different classifications of concepts feature prominently.\n\nConcepts, and the terms that signify them, can be divided into absolute or connotative, according to the mode in which they signify. If they signify something absolutely, that is, after the manner of substance, they are absolute, for example rock, lion, man, whiteness, wisdom, tallness. If they signify something connotatively, that is, with reference to a subject of inherence, i.e., after the manner of accidents, they are connotative, for example, white, wise, tall.\n\nBoth connotative and absolute concepts can be used to signify accidents, but since connotative concepts signify with a reference to a subject of inherence, they can refer to object with different definitions and properties (i.e. with different \"essences\"). For example, large, as a connotative concept, can signify objects with many distinct essences: a man, a lion, a triangle can be large.\n\nOn the other hand, absolute concepts signify objects that have the same definitions and properties. For example, the concept of gold, as an absolute concept, can signify only objects with the same definitions and properties (i.e. with the same \"essence\").\n\nThe notion of absolute concepts was then revived by Saul Kripke, with the name “rigid designation”, in the lectures that became \"Naming and Necessity\", in the course of his argument against descriptivist theories of reference, building on the work of Ruth Barcan Marcus. At the time of Kripke's lectures, the dominant theory of reference in analytic philosophy (associated with the theories of Gottlob Frege and Bertrand Russell) was that the meaning of sentences involving proper names could be given by substituting a contextually appropriate description for the name. Russell, for example, famously held that someone who had never met Otto von Bismarck might know of him as \"the first Chancellor of the German Empire\", and if so, his statement that (say) \"Bismarck was a ruthless politician\" should be understood to mean \"The first Chancellor of the German Empire was a ruthless politician\" (which could in turn be analysed into a series of more basic statements according to the method Russell introduced in his theory of definite descriptions). Kripke argued—against both the Russellian analysis and several attempted refinements of it—that such descriptions could not possibly \"mean the same thing\" as the name \"Bismarck,\" on the grounds that proper names such as \"Bismarck\" always designate \"rigidly\", whereas descriptions such as \"the first Chancellor of the German Empire\" do not. Thus, for example, it \"might have been the case\" that Bismarck died in infancy. If so, he would not have ever satisfied the description \"the first Chancellor of the German Empire,\" and (indeed) someone else probably would have. It does not follow that the first Chancellor of the German Empire may not have been the first Chancellor of the German Empire—that is (at least according to its surface-structure) a contradiction. Kripke argues that the way that proper names \"work\" is that when we make statements about what might or might not have been true of Bismarck, we are talking about what might or might not have been true of \"that particular person\" in various situations, whereas when we make statements about what might or might not have been true of, say, \"the first Chancellor of the German Empire\" we \"could\" be talking about what might or might not have been true of \"whoever\" would have happened to fill that office in those situations.\n\nThe \"could\" here is important to note: rigid designation is a property of the \"way terms are used\", not a property of \"the terms themselves\", and some philosophers, following Keith Donnellan, have argued that a phrase such as \"the first Chancellor of the German Empire\" \"could\" be used rigidly, in sentences such as \"the first Chancellor of the German Empire could have decided never to go into politics.\" Kripke himself doubted that there was any need to recognize rigid uses of definite descriptions, and argued that Russell's notion of scope offered all that was needed to account for such sentences. But in either case, Kripke argued, nothing important in his account depends on the question. Whether definite descriptions can be used rigidly or not, they can at least \"sometimes\" be used non-rigidly, but a proper name \"can only be used rigidly\"; the asymmetry, Kripke argues, demonstrates that no definite description could \"give the meaning\" of a proper name—although it might be used to explain \"who\" a name refers to (that is, to \"fix the referent\" of the name).\n\nIn \"Naming and Necessity\", Kripke argues that proper names and certain natural kind terms—including biological taxa and types of natural substances (most famously, \"water\" and \"HO\") designate rigidly. He argues for a form of scientific essentialism not unlike Aristotelian essentialism. Essential properties are common to an object in all possible worlds, and so they pick out the same objects in all possible worlds - they rigidly designate.\n\nProper names rigidly designate for reasons that differ from natural kinds terms. The reason 'Johnny Depp' refers to one particular person in all possible worlds is because some person initially gave the name to him by saying something like \"Let's call our baby 'Johnny Depp'\". This is called the initial baptism. This usage of 'Johnny Depp' for referring to some particular baby got passed on from person-to-person in a giant causal and historical chain of events. That is why everybody calls Johnny Depp 'Johnny Depp'. Johnny's mother passed it onto her friends who passed it onto their friends who passed it onto their friends, and so on.\n\nOne puzzling consequence of Kripke semantics is that identities involving rigid designators are necessary. If water is HO, then water is \"necessarily\" HO. Since the terms 'water' and 'HO' pick out the same object in every possible world, there is no possible world in which 'water' picks out something different from 'HO'. Therefore, water is necessarily HO. It is possible, of course, that we are mistaken about the chemical composition of water, but that does not affect the necessity of identities. What is not being claimed is that water is necessarily HO, but \"conditionally\", \"if\" water is HO (though we may not know this, it does not change the fact if it is true), then water is necessarily HO.\n\n"}
{"id": "1104379", "url": "https://en.wikipedia.org/wiki?curid=1104379", "title": "Rustic capitals", "text": "Rustic capitals\n\nRustic capitals () is an ancient Roman calligraphic script. Because the term is negatively connotated supposing an opposition to the more 'civilized' form of the Roman square capitals, Bernhard Bischoff prefers to call the script \"canonized capitals\".\n\nRustic capitals are similar to Roman square capitals, but are less rigid, influenced more by pen and ink writing on papyrus or parchment than the writing used for inscriptions. The letters are thinner and more compressed, use many more curved lines than do square capitals, and have descenders extending below the baseline.\n\nThe script was used between the 1st century and the 9th century, most often between the 4th and 6th centuries. After the 5th century, rustic capitals began to fall out of use, but they continued to be used as a display script in titles and headings, along with uncial as the script of the main text.\n\nAbout fifty manuscripts with rustic capitals survive, including four copies of works by Virgil (including the Vergilius Vaticanus and the Vergilius Romanus), one copy of a work by Terence, and one of a work by Prudentius. The script was usually used for \"de luxe\" copies of pagan authors; the only works by Christian authors which use this script are those by Prudentius and Sedulius.\n\n"}
{"id": "25948756", "url": "https://en.wikipedia.org/wiki?curid=25948756", "title": "Salva congruitate", "text": "Salva congruitate\n\nSalva congruitate is a Latin scholastic term in logic, which means \"without becoming ill-formed\", \"salva\" meaning \"rescue\", \"salvation\", \"welfare\" and \"congruitate\" meaning \"combine\", \"coincide\", \"agree\". Salva Congruitate is used in logic to mean that two terms may be substituted for each other while preserving grammaticality in all contexts.\n\nTimothy C. Potts describes \"salva congruitate\" as a form of replacement in the context of meaning. It is a replacement which preserves semantic coherence and should be distinguished from a replacement which preserves syntactic coherence but may yield an expression to which no meaning has been given. This means that supposing an original expression is meaningful, the new expression obtained by the replacement will also be meaningful, though it will not necessarily have the same meaning as the original one, nor, if the expression in question happens to be a proposition, will the replacement necessarily preserve the truth value of the original.\n\nBob Hale explains \"salva congruitate\", as applied to singular terms, as substantival expressions in natural language, which are able to replace singular terms without destructive effect on the grammar of a sentence. Thus the singular term 'Bob' may be replaced by the definite description 'the first man to swim the English Channel' \"salva congruitate\". Such replacement may shift both meaning and reference, and so, if made in the context of a sentence, may cause a change in truth-value. Thus terms which may be interchanged \"salva congruitate\" may not be interchangeable \"salva veritate\" (preserving truth). More generally, expressions of any type are interchangeable \"salva congruitate\" if and only if they can replace one another preserving grammaticality or well-formedness.\n\n"}
{"id": "44836", "url": "https://en.wikipedia.org/wiki?curid=44836", "title": "Speech act", "text": "Speech act\n\nA speech act in linguistics and the philosophy of language is an utterance that has performative function in language and communication. \nAccording to Kent Bach, \"almost any speech act is really the performance of several acts at once, distinguished by different aspects of the speaker's intention: there is the act of saying something, what one does in saying it, such as requesting or promising, and how one is trying to affect one's audience\".\nThe contemporary use of the term goes back to J. L. Austin's development of performative utterances and his theory of locutionary, illocutionary, and perlocutionary acts. Speech acts are commonly taken to include such acts as promising, ordering, greeting, warning, inviting and congratulating.\n\nSpeech acts can be analysed on three levels: \n\nThe concept of an illocutionary act is central to the concept of a speech act. Although there are numerous opinions regarding how to define 'illocutionary acts', there are some kinds of acts which are widely accepted as illocutionary, as for example promising or commanding.\n\nFollowing the usage of, for example, John R. Searle, \"speech act\" is often meant to refer just to the same thing as the term illocutionary act, which John L. Austin had originally introduced in \"How to Do Things with Words\" (published posthumously in 1962). Searle's work on speech acts is also commonly understood to refine Austin's conception. However, some philosophers have pointed out a significant difference between the two conceptions: whereas Austin emphasized the conventional interpretation of speech acts, Searle emphasized a psychological interpretation (based on beliefs, intentions, etc.).\n\nAccording to Austin's preliminary informal description, the idea of an \"illocutionary act\" can be captured by emphasizing that \"by saying something, we \"do\" something\", as when someone issues an order to someone to go by saying \"Go!\", or when a minister joins two people in marriage saying, \"I now pronounce you husband and wife.\" (Austin would eventually define the \"illocutionary act\" in a more exact manner.)\n\nAn interesting type of illocutionary speech act is that performed in the utterance of what Austin calls performatives, typical instances of which are \"I nominate John to be President\", \"I sentence you to ten years' imprisonment\", or \"I promise to pay you back.\" In these typical, rather explicit cases of performative sentences, the action that the sentence describes (nominating, sentencing, promising) is performed by the utterance of the sentence itself.\n\nWhile illocutionary acts relate more to the speaker, perlocutionary acts are centered around the listener. Perlocutionary acts always have a 'perlocutionary effect' which is the effect a speech act has on a listener. This could affect the listener's thoughts, emotions or even their physical actions. An example of this could be if someone uttered the sentence \"I'm hungry.\" The perlocutionary effect on the listener would persuade them to maybe make a sandwich for the speaker.\n\nIn the course of performing speech acts we ordinarily communicate with each other. The content of communication may be identical, or almost identical, with the content intended to be communicated, as when a stranger asks, \"What is your name?\"\n\nHowever, the meaning of the linguistic means used (if ever there are linguistic means, for at least some so-called \"speech acts\" can be performed non-verbally) may also be different from the content intended to be communicated. One may, in appropriate circumstances, request Peter to do the dishes by just saying, \"Peter ...!\", or one can promise to do the dishes by saying, \"Me!\" One common way of performing speech acts is to use an expression which indicates one speech act, and indeed performs this act, but also performs a further speech act, which is indirect. One may, for instance, say, \"Peter, can you close the window?\", thereby asking Peter whether he will be able to close the window, but also requesting that he does so. Since the request is performed indirectly, by means of (directly) performing a question, it counts as an indirect speech act.\n\nAn even more indirect way of making such a request would be to say, in Peter's presence in the room with the open window, \"I'm cold.\" The speaker of this request must rely upon Peter's understanding of several items of in-explicit information: that the window is open and is the cause of her being cold, that being cold is an uncomfortable sensation and she wishes it to be taken care of, and that Peter cares to rectify this situation by closing the window. This, of course, depends much on the relationship between the requester and Peter—he might understand the request differently if she were his boss at work than if she were his girlfriend at home. The more presumed information pertaining to the request, the more indirect the speech act may be considered to be.\n\nIndirect speech acts are commonly used to reject proposals and to make requests. For example, a speaker asks, \"Would you like to meet me for coffee?\" and another replies, \"I have class.\" The second speaker used an indirect speech act to reject the proposal. This is indirect because the literal meaning of \"I have class\" does not entail any sort of rejection.\n\nThis poses a problem for linguists because it is confusing (on a rather simple approach) to see how the person who made the proposal can understand that his proposal was rejected. Following substantially an account of H. P. Grice, Searle suggests that we are able to derive meaning out of indirect speech acts by means of a cooperative process out of which we are able to derive multiple illocutions; however, the process he proposes does not seem to accurately solve the problem. Sociolinguistics has studied the social dimensions of conversations. This discipline considers the various contexts in which speech acts occur.\n\nIn other words this means that one does not need to say the words apologize, pledge, or praise in order to show they are doing the action. All the examples above show how the actions and indirect words make something happen rather than coming out straightforward with specific words and saying it.\n\nFor much of the history of linguistics and the positivist philosophy of language, language was viewed primarily as a way of making factual assertions, and the other uses of language tended to be ignored, as Austin states at the beginning of Lecture 1, \"It was for too long the assumption of philosophers that the business of a 'statement' can only be to 'describe' some state of affairs, or to 'state some fact', which it must do either truly or falsely.\" Wittgenstein came up with the idea of \"don't ask for the meaning, ask for the use,\" showing language as a new vehicle for social activity. Speech act theory hails from Wittgenstein’s philosophical theories. Wittgenstein believed meaning derives from pragmatic tradition, demonstrating the importance of how language is used to accomplish objectives within specific situations. By following rules to accomplish a goal, communication becomes a set of language games. Thus, utterances do more than reflect a meaning, they are words designed to get things done. The work of J. L. Austin, particularly his \"How to Do Things with Words\", led philosophers to pay more attention to the non-declarative uses of language. The terminology he introduced, especially the notions \"locutionary act\", \"illocutionary act\", and \"perlocutionary act\", occupied an important role in what was then to become the \"study of speech acts\". All of these three acts, but especially the \"illocutionary act\", are nowadays commonly classified as \"speech acts\".\n\nAustin was by no means the first one to deal with what one could call \"speech acts\" in a wider sense. The term 'social act' and some of the theory of this \"sui generis\" type of linguistic action are to be found in the fifth of Thomas Reid's Essays on the Active Powers of the Human Mind (1788, chapter VI, Of the Nature of a Contract).\n\"A man may see, and hear, and remember, and judge, and reason; he may deliberate and form purposes, and execute them, without the intervention of any other intelligent being. They are solitary acts. But when he asks a question for information, when he testifies a fact, when he gives a command to his servant, when he makes a promise, or enters into a contract, these are social acts of mind, and can have no existence without the intervention of some other intelligent being, who acts a part in them. Between the operations of the mind, which, for want of a more proper name, I have called solitary, and those I have called social, there is this very remarkable distinction, that, in the solitary, the expression of them by words, or any other sensible sign, is accidental. They may exist, and be complete, without being expressed, without being known to any other person. But, in the social operations, the expression is essential. They cannot exist without being expressed by words or signs, and known to the other party.\"\n\nAdolf Reinach (1883–1917) and Stanislav Škrabec (1844–1918) have been both independently credited with a fairly comprehensive account of social acts as performative utterances dating to 1913, long before Austin and Searle. \n\nThe term \"Speech Act\" had also been already used by Karl Bühler.\n\nThe term metalocutionary act has also been used to indicate a speech act that refers to the forms and functions of the discourse itself rather than continuing the substantive development of the discourse, or to the configurational functions of prosody and punctuation.\n\nDore (1975) proposed that children's utterances were realizations of one of nine primitive speech acts:\n\nThere is no agreed formalization of Speech Act theory. A first attempt to give some grounds of an illocutionary logic has been given by John Searle and D. Vandervecken 1985. Other attempts have been proposed by Per Martin-Löf for a treatment of the concept of assertion inside intuitionistic type theory, and by Carlo Dalla Pozza, with a proposal of a formal pragmatics connecting propositional content (given with classical semantics) and illocutionary force (given by intuitionistic semantics). Up to now the main basic formal application of speech act theory are to be found in the field of human-computer interaction (in chatboxes and other tools: see below).\n\nComputational speech act models of human–computer conversation have been developed.\n\nSpeech act theory has been used to model conversations for automated classification and retrieval.\n\nAnother highly-influential view of Speech Acts has been in the 'Conversation for Action' developed by Terry Winograd and Fernando Flores in their 1987 text \"Understanding Computers and Cognition: A New Foundation for Design\". Arguably the most important part of their analysis lies in a state-transition diagram (in Chapter 5) that Winograd and Flores claim underlies the significant illocutionary (speech act) claims of two parties attempting to coordinate action with one another (no matter whether the agents involved might be human–human, human–computer, or computer–computer).\n\nA key part of this analysis is the contention that one dimension of the social domain-tracking the illocutionary status of the transaction (whether individual participants claim that their interests have been met, or not) is very readily conferred to a computer process, regardless of whether the computer has the means to adequately represent the real world issues underlying that claim. Thus a computer instantiating the 'conversation for action' has the useful ability to model the status of the current social reality independent of any external reality on which social claims may be based.\n\nThis transactional view of speech acts has significant applications in many areas in which (human) individuals have had different roles—for instance, a patient and a physician might meet in an encounter in which the patient makes a request for treatment, the physician responds with a counter-offer involving a treatment she feels is appropriate, and the patient might respond, etc. Such a \"Conversation for Action\" can describe a situation in which an external observer (such as a computer or health information system) may be able to track the ILLOCUTIONARY (or Speech Act) STATUS of negotiations between the patient and physician participants even in the absence of any adequate model of the illness or proposed treatments. The key insight provided by Winograd and Flores is that the state-transition diagram representing the SOCIAL (Illocutionary) negotiation of the two parties involved is generally much, much simpler than any model representing the world in which those parties are making claims; in short, the system tracking the status of the 'conversation for action' need not be concerned with modeling all of the realities of the external world. A conversation for action is critically dependent upon certain stereotypical CLAIMS about the status of the world made by the two parties. Thus a \"Conversation for Action\" can be readily tracked and facilitated by a device with little or no ability to model circumstances in the real world other than the ability to register claims by specific agents about a domain.\n\nIn making useful \"applications\" of technology to domains such as healthcare, it is helpful to discriminate between problems which are very, very hard (such as deep understanding of pathophysiology as it relates to genetic and various environmental influences) and problem which are relatively easier, such as following the status of negotiations between a patient and a health care provider. Speech Act (Illocutionary) Analysis allows for a useful understanding of the status of a negotiation between (for instance) a health care provider and a patient INDEPENDENT of any well-accepted credible and comprehensive understanding of a disease process as it might apply to that patient. For this reason, systems which track the status of PROMISES and REJECTED-PROPOSALS and ACCEPTED-PROMISES can help us to understand the situations in which (human or computer) AGENTS find themselves as they attempt to fulfill ROLES involving other agents, and such systems can facilitate both human and human–computer systems in achieving role-associated goals.\n\nIn the past, philosophy has discussed rules for when expressions are used. The two rules are \"constitutive\" and \"regulative rules\".\n\nThe concept of constitutive rules finds its origin in Wittgenstein and Rawls, and has been elaborated by G.C.J. Midgley, Max Black, G.H. von Wright, David Shwayder, and John Searle.\n\nWhereas \"regulative rules\" are prescriptions that regulate a pre-existing activity (whose existence is logically independent of the rules), \"constitutive rules\" constitute an activity the existence of which is logically dependent on the rules.\n\nFor example: traffic rules are \"regulative rules\" that prescribe certain behaviour in order to regulate the traffic. Without these rules however, the traffic would not cease to be. In contrast: the rules of chess are \"constitutive rules\" that constitute the game. Without these rules chess would not exist, since the game is logically dependent on the rules. \n\nMulti-agent systems sometimes use speech act labels to express the intent of an agent when it sends a message to another agent. For example the intent \"inform\" in the message \"inform(content)\" may be interpreted as a request that the receiving agent adds the item \"content\" to its knowledge-base; this is in contrast to the message \"query(content)\" which may be interpreted (depending on the semantics employed) as a request to see if the item content is currently in the receiving agents knowledge base. There are at least two standardisations of speech act labelled messaging KQML and FIPA.\n\nKQML and FIPA are based on the Searlian, that is, psychological semantics of speech acts. Munindar P. Singh has long advocated moving away from the psychological to a social semantics of speech acts—one that would be in tune with Austin's conception. Andrew Jones has also been a critic of the psychological conception. A recent collection of manifestos by researchers in agent communication reflects a growing recognition in the multiagent systems community of the benefits of a social semantics.\n\n\nIn political science, the Copenhagen School adopts speech act as a form of felicitous speech act (or simply 'facilitating conditions'), whereby the speaker, often politicians or players, act in accordance to the truth but in preparation for the audience to take action in the directions of the player that are driven or incited by the act. This forms an observable framework under a specified subject matter from the player, and the audience who are 'under-theorised [would] remain outside of the framework itself, and would benefit from being both brought in and drawn out.' It is because the audience would not be informed of the intentions of the player, except to focus on the display of the speech act itself. Therefore, in the perspective of the player, the truth of the subject matter is irrelevant except the result produced via the audience.\n\nThe study of Speech Acts is prevalent in legal theory since laws themselves can be interpreted as speech acts. Laws issue out a command to their constituents which can be realized as an action. When forming a legal contract, speech acts can be made when people are making or accepting an offer. Considering the theory of freedom of speech, some speech acts may not be legally protected. For example, a death threat is a type of speech act and is considered to exist outside of the protection of freedom of speech as it is treated as a criminal act.\n\nIn finance, it is possible to understand mathematical models as speech acts: the notion of \"financial Logos\" is defined in Walter (2016) as the speech act of mathematical financial risk models. The action of the financial Logos on financial practices is the following: the framing of financial decision-making by risk modelling.\n\n\n"}
{"id": "25490263", "url": "https://en.wikipedia.org/wiki?curid=25490263", "title": "Speech repetition", "text": "Speech repetition\n\nSpeech repetition is the saying by one individual of the spoken vocalizations made by another individual. This requires the ability in the person making the copy to map the sensory input they hear from the other person's vocal pronunciation into a similar motor output with their own vocal tract.\n\nSuch speech input output imitation often occurs independently of speech comprehension such as in speech shadowing when a person automatically says words heard in earphones, and the pathological condition of echolalia in which people reflexively repeat overheard words. This links to speech repetition of words being separate in the brain to speech perception. Speech repetition occurs in the dorsal speech processing stream while speech perception occurs in the ventral speech processing stream. Repetitions are often incorporated unawares by this route into spontaneous novel sentences immediately or after delay following storage in phonological memory.\n\nIn humans, the ability to map heard input vocalizations into motor output is highly developed due to this copying ability playing a critical role in a child's rapid expansion of their spoken vocabulary. In older children and adults it still remains important as it enables the continued learning of novel words and names and additional languages. Such repetition is also necessary for the propagation of language from generation to generation. It has also been suggested that the phonetic units out of which speech is made have been selected upon by the process of vocabulary expansion and vocabulary transmissions due to children preferentially copying words in terms of more easily imitated elementary units.\n\nVocal imitation happens quickly: words can be repeated within 250-300 milliseconds both in normals (during speech shadowing) and during echolalia. The imitation of speech syllables possibly happens even quicker: people begin imitating the second phone in the syllable [ao] earlier than they can identify it (out of the set [ao], [aæ] and [ai]). Indeed, \"...simply executing a shift to [o] upon detection of a second vowel in [ao] takes very little longer than does interpreting and executing it as a shadowed response\". Neurobiologically this suggests \"...that the early phases of speech analysis yield information which is directly convertible to information required for speech production\". Vocal repetition can be done immediately as in speech shadowing and echolalia. It can also be done after the pattern of pronunciation is stored in short-term memory or long-term memory. It automatically uses both auditory and where available visual information about how a word is produced.\n\nThe automatic nature of speech repetition was noted by Carl Wernicke, the late nineteenth century neurologist, who observed that \"The primary speech movements, enacted before the development of consciousness, are reflexive and mimicking in nature..\".\n\nVocal imitiation arises in development before speech comprehension and also babbling: 18-week-old infants spontaneously copy vocal expressions provided the accompanying voice matches. Imitation of vowels has been found as young as 12 weeks. It is independent of native language, language skills, word comprehension and a speaker's intelligence. Many autistic and some mentally disabled people engage in the echolalia of overheard words (often their only vocal interaction with others) without understanding what they echo. Reflex uncontrolled echoing of others words and sentences occurs in roughly half of those with Gilles de la Tourette syndrome. The ability to repeat words without comprehension also occurs in mixed transcortical aphasia where it links to the sparing of the short-term phonological store.\n\nThe ability to repeat and imitate speech sounds occurs separately to that of normal speech. Speech shadowing provides evidence of a 'privileged' input/output speech loop that is distinct to the other components of the speech system. Neurocognitive research likewise finds evidence of a direct (nonlexical) link between phonological analysis input and motor programming output.\n\nSpeech sounds can be imitatively mapped into vocal articulations in spite of vocal tract anatomy differences in size and shape due to gender, age and individual anatomical variability. Such variability is extensive making input output mapping of speech more complex than a simple mapping of vocal track movements. The shape of the mouth varies widely: dentists recognize three basic shapes of palate: trapezoid, ovoid, and triagonal; six types of malocclusion between the two jaws; nine ways teeth relate to the dental arch and a wide range of maxillary and mandible deformities. Vocal sound can also vary due to dental injury and dental caries. Other factors that do not impede the sensory motor mapping needed for vocal imitation are gross oral deformations such as hare-lips, cleft palates or amputations of the tongue tip, pipe smoking, pencil biting and teeth clinching (such as in ventriloquism). Paranasal sinuses vary between individuals 20-fold in volume, and differ in the presence and the degree of their asymmetry.\n\nVocal imitation occurs potentially in regard to a diverse range of phonetic units and types of vocalization. The world's languages use consonantal phones that differ in thirteen imitable vocal tract place of articulations (from the lips to the glottis). These phones can potentially be pronounced with eleven types of imitable manner of articulations (nasal stops to lateral clicks). Speech can be copied in regard to its social accent, intonation, pitch and individuality (as with entertainment impersonators). Speech can be articulated in ways which diverge considerably in speed, timbre, pitch, loudness and emotion. Speech further exists in different forms such as song, verse, scream and whisper. Intelligible speech can be produced with pragmatic intonation and in regional dialects and foreign accents. These aspects are readily copied: people asked to repeat speech-like words imitate not only phones but also accurately other pronunciation aspects such as fundamental frequency, schwa-syllable expression, voice spectra and lip kinematics, voice onset times, and regional accent.\n\nIn 1874 Carl Wernicke proposed that the ability to imitate speech plays a key role in language acquisition. This is now a widely researched issue in child development. A study of 17,000 one and two word utterances made by six children between 18 months to 25 months found that, depending upon the particular infant, between 5% and 45% of their words might be mimicked. These figures are minima since they concern only immediately heard words. Many words that may seem spontaneous are in fact delayed imitations heard days or weeks previously. At 13 months children who imitate new words (but not ones they already know) show a greater increase in noun vocabulary at four months and non noun vocabulary at eight months. A major predictor of vocabulary increase in both 20 months, 24 months, and older children between 4 and 8 years is their skill in repeating nonword phone sequences (a measure of mimicry and storage). This is also the case with children with Down's syndrome . The effect is larger than even age: in a study of 222 two-year-old children that had spoken vocabularies ranging between 3–601 words the ability to repeat nonwords accounted for 24% of the variance compared to 15% for age and 6% for gender (girls better than boys).\n\nImitation provides the basis for making longer sentences than children could otherwise spontaneously make on their own. Children analyze the linguistic rules, pronunciation patterns, and conversational pragmatics of speech by making monologues (often in crib talk) in which they repeat and manipulate in word play phrases and sentences previously overheard. Many proto-conversations involve children (and parents) repeating what each other has said in order to sustain social and linguistic interaction. It has been suggested that the conversion of speech sound into motor responses helps aid the vocal \"alignment of interactions\" by \"coordinating the rhythm and melody of their speech\". Repetition enables immigrant monolingual children to learn a second language by allowing them to take part in 'conversations'. Imitation related processes aids the storage of overheard words by putting them into speech based short- and long-term memory.\n\nThe ability to repeat nonwords predicts the ability to learn second-language vocabulary. A study found that adult polyglots performed better in short-term memory tasks such as repeating nonword vocalizations compared to nonpolyglots though both are otherwise similar in general intelligence, visuo-spatial short-term memory and paired-associate learning ability. Language delay in contrast links to impairments in vocal imitation.\n\nElectrical brain stimulation research upon the human brain finds that 81% of areas that show disruption of phone identification are also those in which the imitating of oral movements is disrupted and vice versa; Brain injuries in the speech areas show a 0.9 correlation between those causing impairments to the copying of oral movements and those impairing phone production and perception.\n\nSpoken words are sequences of motor movements organized around vocal tract gesture motor targets. Vocalization due to this is copied in terms of the motor goals that organize it rather than the exact movements with which it is produced. These vocal motor goals are auditory. According to James Abbs 'For speech motor actions, the individual articulatory movements would not appear to be controlled with regard to three- dimensional spatial targets, but rather with regard to their contribution to complex vocal tract goals such as resonance properties (e.g., shape, degree of constriction) and or aerodynamically significant variables'. Speech sounds also have duplicable higher-order characteristics such as rates and shape of modulations and rates and shape of frequency shifts. Such complex auditory goals (which often link—though not always—to internal vocal gestures) are detectable from the speech sound which they create.\n\nTwo cortical processing streams exist: a ventral one which maps sound onto meaning, and a dorsal one, that maps sound onto motor representations. The dorsal stream projects from the posterior Sylvian fissure at the temporoparietal junction, onto frontal motor areas, and is not normally involved in speech perception. \nCarl Wernicke identified a pathway between the left posterior superior temporal sulcus (a cerebral cortex region sometimes called the Wernicke's area) as a centre of the sound \"images\" of speech and its syllables that connected through the arcuate fasciculus with part of the inferior frontal gyrus (sometimes called the Broca's area) responsible for their articulation. This pathway is now broadly identified as the dorsal speech pathway, one of the two pathways (together with the ventral pathway) that process speech. The posterior superior temporal gyrus is specialized for the transient representation of the phonetic sequences used for vocal repetition. Part of the auditory cortex also can represent aspects of speech such as its consonantal features.\n\nMirror neurons have been identified that both process the perception and production of motor movements. This is done not in terms of their exact motor performance but an inference of the intended motor goals with which it is organized. Mirror neurons that both perceive and produce the motor movements of speech have been identified. According to the motor theory of speech imitation such speech mirror neurons in infants have selected for motor goals with vocal track gestures that are easy to imitate and this has shaped the nature of the phonetic units out of which spoken words are constructed. Speech is mirrored constantly into its articulations since speakers cannot know in advance that a word is unfamiliar and in need of repetition—which is only learnt after the opportunity to map it into articulations has gone. Thus, speakers if they are to incorporate unfamiliar words into their spoken vocabulary\nmust by default map all spoken input. The motor theory of speech imitation unlike that of motor theory of speech perception does not link mirror neurons with speech perception.\n\nHuman language is a vocabulary-based form of communication that unlike that of other animals employs tens of thousands of lexicals and names. This requires that young humans new to language have the ability to quickly learn both the pronunciations and use of many thousands of words. If children could not repeat speech without problems, human language could not exist. This makes the evolution of the capacity of speech repetition a critical innovation needed for the origin of speech The motor theory of speech imitation argues that this need for speech to be imitable not speech perception nor speech production moreover underlies the evolved nature of the vowel and consonant units of phonetics.\n\nWords in sign languages, unlike those in spoken ones, are made not of sequential units but of spatial configurations of subword unit arrangements, the spatial analogue of the sonic-chronological morphemes of spoken language. These words, like spoken ones, are learnt by imitation. Indeed, rare cases of compulsive sign-language echolalia exist in otherwise language-deficient deaf autistic individuals born into signing families. At least some cortical areas neurobiologically active during both sign and vocal speech, such as the auditory cortex, are associated with the act of imitation.\n\nBirds learn their songs from those made by other birds. In several examples, birds show highly developed repetition abilities: the Sri Lankan Greater racket-tailed drongo (\"Dicrurus paradiseus\") copies the calls of predators and the alarm signals of other birds Albert's lyrebird (\"Menura alberti\") can accurately imitate the satin bowerbird (\"Ptilonorhynchus violaceus\"),\n\nResearch upon avian vocal motor neurons finds that they perceive their song as a series of articulatory gestures as in humans. Birds that can imitate humans, such as the Indian hill myna (Gracula religiosa), imitate human speech by mimicking the various speech formants, created by changing the shape of the human vocal tract, with different vibration frequencies of its internal tympaniform membrane. Indian hill mynahs also imitate such phonetic characteristics as voicing, fundamental frequencies, formant transitions, nasalization, and timing, through their vocal movements are made in a different way from those of the human vocal apparatus.\n\n\nApes taught language show an ability to imitate language signs with chimpanzees such as Washoe who was able to learn with his arms a vocabulary of 250 American Sign Language gestures. However, such human trained apes show no ability to imitate human speech vocalizations.\n\n"}
{"id": "603325", "url": "https://en.wikipedia.org/wiki?curid=603325", "title": "Topic and comment", "text": "Topic and comment\n\nIn linguistics, the topic, or theme, of a sentence is what is being talked about, and the comment (rheme or focus) is what is being said about the topic. This opposition of the given/new information is called information structure. That the information structure of a clause is divided in this way is generally agreed on, but the boundary between topic/theme and comment/rheme/focus depends on grammatical theory. \n\nThe difference between \"topic\" and grammatical subject is that topic is used to describe the information structure, or pragmatic structure of a clause and how it coheres with other clauses, whereas the subject is a purely grammatical category. \"Topic\" and \"subject\" must also be distinguished from \"actor\" (or \"agent\"), the \"doer\". In English clauses with a verb in the passive voice, for instance, the topic is typically the subject, while the agent may be omitted or may follow the preposition \"by\". In some languages, word order and other syntactic phenomena are determined largely by the topic–comment (theme–rheme) structure. These languages are sometimes referred to as topic-prominent languages. Korean and Japanese are often given as examples of this.\n\nThe distinction was probably first suggested by Henri Weil in 1844. He established the\nconnection between information structure and word order. Georg von der Gabelentz distinguished psychological subject (roughly topic) and psychological object (roughly focus). In the Prague school, the dichotomy, termed topic–focus articulation, has been studied mainly by Vilém Mathesius, Jan Firbas, František Daneš, Petr Sgall and Eva Hajičová. They have been concerned mainly by its relation to intonation and word-order. Mathesius also pointed that the topic does not provide new information but connects the sentence to the context. The work of Michael Halliday in the 1960s is responsible for developing linguistic science through his systemic functional linguistics model for English.\n\nThe sentence- or clause-level \"topic\", or \"theme\", can be defined in a number of different ways. Among the most common are\n\nIn an ordinary English clause, the subject is normally the same as the topic/theme (example 1), even in the passive voice (where the subject is a patient, not an agent: example 2):\n\nThese clauses have different topics: the first is about \"the dog\", and the second about \"the little girl\".\n\nIn English it is also possible to use other sentence structures to show the topic of the sentence, as in the following:\n\nThe case of expletives is sometimes rather complex. Consider sentences with expletives (meaningless subjects), like:\n\n\nIn these examples the syntactic subject position (to the left of the verb) is manned by the meaningless expletive (\"it\" or \"there\"), whose sole purpose is satisfying the extended projection principle, and is nevertheless necessary. In these sentences the topic is never the subject, but is determined pragmatically. In all these cases, the whole sentence refers to the comment part.\n\nThe relation between topic/theme and comment/rheme/focus should not be confused with the topic-comment relation in Rhetorical structure theory Discourse Treebank (RST-DT corpus) where it is defined as \"a general statement or topic of discussion is introduced, after which a specific remark is made on the statement or topic\" (ex. \"[As far as the pound goes,] [some traders say a slide toward support at 1.5500 may be a favorable development for the dollar this week.]\") \n\nDifferent languages mark topics in different ways. Distinct intonation and word-order are the most common means. The tendency to place topicalized constituents sentence-initially (\"topic fronting\") is widespread. Topic fronting refers to placing the topic at the beginning\nof a clause regardless whether it is marked or not. Again, linguists disagree on many details. \n\nLanguages often show different kinds of grammar for sentences that introduce new topics and those that continue discussing previously established topics. \n\nWhen a sentence continues discussing a previously established topic, it is likely to use pronouns to refer to the topic. Such topics tend to be subjects. In many languages, pronouns referring to previously established topics will show pro-drop.\n\nThe topic/theme comes first in the clause, and is typically marked out by intonation as well.\n\n\nThe main application of the topic-comment structure is in the domain of speech technology, especially the design of embodied conversational agents (intonational focus assignment, relation between information structure and posture and gesture). There were some attempts to apply the theory of topic/comment for the information retrieval and the automatic summarization .\n\n\n\n"}
{"id": "54292359", "url": "https://en.wikipedia.org/wiki?curid=54292359", "title": "Traduire", "text": "Traduire\n\nTraduire is a 2011 French independent underground experimental documentary art film directed by Nurith Aviv. It was released on DVD by , as part of a boxset, also including \"Misafa Lesafa\" (2004) and \"Langue sacrée, langue parlée\" (2008).\n\nThe film, the third in a trilogy, containing \"Misafa Lesafa\" (2004) and \"Langue sacrée, langue parlée\" (2008), contains conversations with translators of Hebrew works into different languages. Among the interviewees are Brest, France-based Sandrick Le Mague, who translates theological texts into French, Boston-based Prof. Dr. Angel Sáenz-Badillos, who translates medieval poetry into Spanish, Acre-based Israeli-Arab novelist, screenwriter, and, journalist, , who translates the plays of Israeli playwright Hanoch Levin into Arabic, Malakoff-based Prof. Dr. , who compiles a Hebrew-Yiddish dictionary, Barcelona-based Prof. Dr. , who translates the contemporary Israeli poet Yehuda Amichai into Catalan, Tel Aviv-based Israeli poetess, , who translates the contemporary Israeli poetess Leah Goldberg into Russian and Lithuanian, and, Berkeley, California-based Prof. Dr. Chana Bloch, who translated into English the works of contemporary Israeli poets Yehuda Amichai and Dahlia Ravikovitch.\n\nCritic Jacques Mandelbaum opined that \"Aviv films these encounters carefully, taking time to listen to each translator in the half-light of their offices, bringing surprisingly passionate ideas to the surface\" and that the film \"finds room in its erudite enterprise to explore sensibilities.\"\n\n"}
{"id": "27133613", "url": "https://en.wikipedia.org/wiki?curid=27133613", "title": "Transcription (service)", "text": "Transcription (service)\n\nA transcription service is a business service basically based on Speech to Text process which converts speech (either live or recorded) into a written or electronic text document. Transcription services are often provided for business, legal, or medical purposes. The most common type of transcription is from a spoken-language source into text such as a computer file suitable for printing as a document such as a report. Common examples are the proceedings of a court hearing such as a criminal trial (by a court reporter) or a physician's recorded voice notes (medical transcription). Some transcription businesses can send staff to events, speeches, or seminars, who then convert the spoken content into text. Some companies also accept recorded speech, either on cassette, CD, VHS, or as sound files. For a transcription service, various individuals and organisations have different rates and methods of pricing. That can be per line, per word, per minute, or per hour, which differs from individual to individual and industry to industry. Transcription companies primarily serve private law firms, local, state and federal government agencies and courts, trade associations, meeting planners and nonprofits.\n\nBefore 1970, transcription was a difficult job, as secretaries had to write down the speech as they heard it using advanced skills, like shorthand. They also had to be at the location where the service was required. But with the introduction of tape cassettes and portable recorders in the late 1970s, the work became much easier and new possibilities emerged. Cassettes can travel through internal mail or external mail which meant for the first time, the transcribers could have the work brought to them in their own office which could be in a different location or business. For the first time, transcribers could work from home for many different businesses at their own convenience, provided they met the deadlines required by their clients.\n\nWith the birth of modern technology like speech recognition, transcription has become much easier. An MP3-based Dictaphone, for example, can be used to record the sound. Recordings for transcription can be in different media file types. The recording can then be uploaded to a PC, uploaded to a cloud storage, or emailed within minutes to someone who could be anywhere in the world. The transcriptionist can then replay the audio many times. The sound can also be filtered, equalised or have the tempo adjusted when the clarity is poor. The completed document can then be emailed back and printed out or incorporated into other documents – all within just a few hours of the original recording being made.\n\nThe industry standard for transcribing an audio file takes one hour for every 15 minutes of audio. For live usage, real-time text transcription services are available for captioning purposes, including Remote CART, Captioned Telephone, and live closed captioning for live broadcasts. Live transcripts are less accurate than offline transcripts, as there is no time for corrections and refinements. However, in a multistage subtitling process with a broadcast delay and access to a live audio feed it is possible to have several correction stages and for the text to be displayed at the same time as the \"live\" transmission.\n\nInterview transcription is a word-to-word written documentation of a taped or live interview. All types of interviews pertaining to legal cases, businesses, research, celebrity interviews and many more can be transcribed. While tapes need to be played and replayed to get the exact information one is looking for, transcribed copies allow easy lookup for the desired information. A written transcript is also important to identify key topics discussed in an interview. People with hearing imparity or deafness can also have access to the interview proceedings with accurately prepared interview transcripts. When transcribing interviews one needs to be aware and plan around conditions that ensure quality recording and transcribing.\n\nBy the early 1900s, the main responsibility of a doctor lay in treating the patient and other responsibilities such as creating a patient's medical record, keeping the files up to date and any other related paperwork eventually fell into the hands of hired medical stenographers. With the invention of typewriters, maintaining records became easier and with the invention of cassette players, it made way for the development of transcription machines. The initial versions available for purchase, offered the ability to record speech on cassette tapes. In fact, they were very popular for a long time although they did not offer much voice clarity at all. As soon as the usage of computers picked up in organizations and in other sectors, cassette tapes were replaced with better storage devices such as floppy discs and CD's. Today, the availability of highly sophisticated recording equipment ensures that multiple high clarity files can be created, stored and sent for medical transcription purposes.\n\nMedical transcription presents other challenges as a service to the transcriber. For example, working knowledge of medical terminology like ICD codes and an understanding of the rules and regulations regarding HIPPA compliance may be necessary to complete a medical transcription service.\n\nBusiness meetings, and professional recordings can contain sensitive data, so security is an important factor which should not be overlooked by a transcription company when providing services to its clients. Companies should therefore follow the various laws and industry best practice, especially so when serving law firms, government agencies or courts. Medical Transcription specifically is governed by HIPAA, which elaborates data security practices and compliance measures to be strictly followed, failure of which leads to legal action and penalties.\n\nTranscription security includes maintaining confidentiality of the data through information security practices including limiting access with passwords and ensuring a secure environment for data and appropriate methods of disposal of all materials and deletion of files. Personnel may be required to sign non-disclosure agreements on a regular basis as well as take various oaths regarding confidentiality and accuracy.\n"}
{"id": "910058", "url": "https://en.wikipedia.org/wiki?curid=910058", "title": "Truth-conditional semantics", "text": "Truth-conditional semantics\n\nTruth-conditional semantics is an approach to semantics of natural language that sees meaning (or at least the meaning of assertions) as being the same as, or reducible to, their truth conditions. This approach to semantics is principally associated with Donald Davidson, and attempts to carry out for the semantics of natural language what Tarski's semantic theory of truth achieves for the semantics of logic (Davidson 1967).\n\nTruth-conditional theories of semantics attempt to define the meaning of a given proposition by explaining when the sentence is true. So, for example, because 'snow is white' is true if and only if snow is white, the meaning of 'snow is white' is snow is white.\n\nThe first truth-conditional semantics was developed by Donald Davidson in \"Truth and Meaning\" (1967). It applied Tarski's semantic theory of truth to a problem it was not intended to solve, that of giving the meaning of a sentence.\n\nScott Soames has harshly criticized truth-conditional semantics on the grounds that it is either wrong or uselessly circular.\n\nUnder its traditional formulation, truth-conditional semantics gives every necessary truth precisely the same meaning, for all of them are true under precisely the same conditions (namely, all of them). And since the truth conditions of any unnecessarily true sentence are equivalent to the conjunction of those truth conditions and any necessary truth, any sentence means the same as its meaning plus a necessary truth. For example, if \"\"snow is white\" is true if snow is white\", then it is trivially the case that \"\"snow is white\" is true if snow is white and 2+2=4\", therefore under truth-conditional semantics \"snow is white\" means both that snow is white and that 2+2=4. That is wrong. Of course, it is not the case, trivially or otherwise, that such a consequence follows.\n\nSoames argues further that reformulations that attempt to account for this problem must beg the question. In specifying precisely \"which\" of the infinite number of truth-conditions for a sentence will count towards its meaning, one must take the meaning of the sentence as a guide. However, we wanted to specify meaning with truth-conditions, whereas now we are specifying truth-conditions with meaning, rendering the entire process fruitless.\n\nMichael Dummett (1975) has objected to Davidson's program on the grounds that such a theory of meaning will not explain what it is a speaker has to know in order for them to understand a sentence. Dummett believes a speaker must know three components of a sentence to understand its meaning: a theory of sense, indicating the part of the meaning that the speaker grasps; a theory of reference, which indicates what claims about the world are made by the sentence, and a theory of force, which indicates what kind of speech act the expression performs. Dummett further argues that a theory based on inference, such as Proof-theoretic semantics, provides a better foundation for this model than truth-conditional semantics does.\n\n\n"}
{"id": "22618868", "url": "https://en.wikipedia.org/wiki?curid=22618868", "title": "Velleity", "text": "Velleity\n\nVelleity is the lowest degree of volition, a slight wish or tendency.\n\nThe marketer Matt Bailey described it as \"a desire to see something done, but not enough desire to make it happen\".\n\nMatt Bailey expressed an attempt \"to bring it back, as it has more relevance now than ever.\" He writes that:\n\nFriedrich Nietzsche describes the velleity of an artist as a \"desire to \"be\" 'what he is able to represent, conceive, and express'...\" Nietzsche championed the will to power, which can be encapsulated as starting with velleity, in his free-will theorem.\n\nKeith David Wyma refers frequently to the \"concept of velleity\", citing Thomas Aquinas as a pioneer of introducing the idea into philosophy.\n\nPsychologist Avi Sion writes, \"\"Many psychological concepts may only be defined and explained with reference to velleity\".\" (\"Emphasis in original\".) An example he cites is that \"an ordinarily desirable object can only properly be called 'interesting' or 'tempting' to that agent at that time, if he manifests some velleity...\"\nHe distinguishes between the two types of velleity - \"\"to do\" something and one \"not to do\" something...\" Furthermore, he asserts, \"The concept of velleity is also important because it enables us to understand the co-existence of conflicting values.\" A person could thus have \"double velleity\" or \"a mix of velleity for something and a volition for its opposite: the latter dominates, of course, but that does not erase the fact of velleity.\"\n\nKathy Kolbe also lists velleity as a \"key concept of conation.\"\n\n"}
