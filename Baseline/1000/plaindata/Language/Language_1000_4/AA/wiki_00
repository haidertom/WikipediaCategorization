{"id": "53238020", "url": "https://en.wikipedia.org/wiki?curid=53238020", "title": "Abstract Meaning Representation", "text": "Abstract Meaning Representation\n\nAbstract Meaning Representation (AMR) is a semantic representation language. AMR graphs are rooted, labeled, directed, acyclic graphs (DAGs), comprising whole sentences. They are intended to abstract away from syntactic representations, in the sense that sentences which are similar in meaning should be assigned the same AMR, even if they are not identically worded. By nature, the AMR language is biased towards English – it is not meant to function as an international auxiliary language.\n"}
{"id": "5259993", "url": "https://en.wikipedia.org/wiki?curid=5259993", "title": "Accent reduction", "text": "Accent reduction\n\nAccent reduction, also known as accent modification or accent neutralization, is a systematic approach for learning or adopting a new speech accent. It is the process of learning the sound system (or phonology) and melodic intonation of a language so the non-native speaker can communicate with clarity to be understood by the general public of this second language.  \n\nAccent reduction training is not the same as ESL (English as a Second Language) classes. Accent reduction classes go beyond learning vocabulary and grammar and focus upon clarity of speech and fine tuning a specific accent or dialect. Foreign accent reduction training is typically appropriate for adult learners who have at least a moderate level of conversational proficiency in the second language. Non-native speakers from any background or profession can benefit from accent reduction training. That is not to say that every non-native speaker needs to modify their accent, however. The goal of accent reduction training is to improve speaking clarity so the non-native speaker is understood in the workplace as well as within their community; not necessarily to totally eliminate the accent. Business professionals, physicians, professors, researchers, telemarketers, etc. oftentimes request accent reduction training be provided by their employers so they can communicate more effectively with their colleagues, clients, and customers. \n\nForeign born students and professionals can benefit from accent modification training to improve their English intelligibility to be more competitive when interviewing for jobs.  Under U.S. labor law, employers can make job decisions based on accent if it interferes with work. The federal Equal Employment Opportunity Commission does receive a small number of complaints every year from individuals who believe they are victims of accent-related job discrimination.\n\nTwo distinct types of accent reduction training are available: self-study and instructor-led.  There are many types of self-study books, apps, cds, and software systems on the market.  Some of these products offer materials that are unique to individuals from specific language backgrounds.  The self-study learning methods can be helpful especially if audios are included so the user can hear the correct pronunciations.  Instructor-led training, although significantly more expensive than the self-study option, allows students to receive personalized instruction, obtain immediate feedback from the trainer, and typically make more timely progress.  Instructor-led training is available in a variety of ways:  1:1 with an instructor, small group training, seminars, or workshops.  Delivery can occur in person, web-based using webcams, or via telephone.\n\nAccent modification is offered by various certified speech-language trainers, linguists, and specialists in ESL. In the United States, they are promoted by various organizations including the Accent Freedom, the American Speech–Language–Hearing Association (ASHA), the Accent Reduction Training Association (ARTA), Corporate Speech Pathology Network (CORSPAN),  Voice, Speech Trainers Association (VASTA).\n\nInstructor-led courses typically start with a speech assessment to determine a student’s unique needs.  Speech production is a very complicated process involving coordinating movements of the lips, tongue, jaw, vocal cords, and respiratory system.  Speakers from different language backgrounds have different speech patterns when speaking English as they are attempting to implement their own language’s pronunciation rules while speaking English.  Even people from the same language background can have differing speech patterns based upon the age at which they learned English, the characteristics of their teacher’s speech, and influences of other languages they may speak.\n\nAreas of focus may include teaching students clear articulation of vowels and consonants as well as the intonation patterns that are unique to each language.  The learning sequence is typically broken down into progressive learning segments until they cumulate into using the newly learned skills in conversation. Additional areas might include linking, rate, or voice projection.  Instructor-led accent training will also frequently include conversational practice to help the student transfer these newly learned skills into everyday conversations.  \n\nTraining timelines can vary from a few days to several months depending upon the chosen model of instruction.  Outside practice time is essential for the participant to see significant changes in their speech.\n\n Although accents can be minimized through training, actually eliminating an accent is extremely difficult to master and could take years to accomplish.  It is unrealistic to expect total accent elimination in a short period of time.  \n\nAccent improvement focuses on teaching students how to pronounce difficult sounds such as , , , , and ; intonation, stress, and rhythm. Spanish and Portuguese speakers might add an before the vowel , as in \"his\" for \"is\". Therefore, vowel sounds are also covered in accent reduction training. Practicing of the vowel most commonly spelled \"i\" is done by reciting a few of the following differences: his versus is, hit versus it, hill versus ill. By not letting the back of their tongue touch the palate, native speakers of Asian languages (Chinese, etc.) can avoid adding a before the for example in speaking \"yin\" instead of \"in\".\nSpecialists also use activities, games, and printable workbooks to help students practice what they learn.\nAlthough the accents can be reduced through training, some linguists warn against giving students a false hope that they will lose their accents. According to Dennis Baron, a linguistics professor at the University of Illinois at Urbana-Champaign, eliminating an accent is difficult. Calming an accent, he said, takes years of interaction with native English speakers. Even so, under U.S. labor law, employers can make job decisions based on accent if it interferes with work. The federal Equal Employment Opportunity Commission does receive a small number of complaints every year from individuals who believe they are victims of accent-related job discrimination.\n\nThe actors Portia de Rossi, Tom Holland, Anthony La Paglia, Katherine Langford, and Charlize Theron are examples of notable people who received such training to lose their native accents and develop American accents, even in everyday speech.\n\nWith regard to English accent training, the two most distinct choices of accent reduction are the British or American pathways. However the Australian method of received pronunciation is increasingly preferred by Asian nations, given the two regions' geographic proximity; this is an important consideration given the rise of Asia's economic strength and choice in education.\n\n\n"}
{"id": "32642147", "url": "https://en.wikipedia.org/wiki?curid=32642147", "title": "Agarabi language", "text": "Agarabi language\n\nAgarabi, also called \"Bare\", is a Kainantu language spoken in Papua New Guinea.\n\n"}
{"id": "48315392", "url": "https://en.wikipedia.org/wiki?curid=48315392", "title": "Article structure", "text": "Article structure\n\nNews stories and features, whether in magazine writing or broadcast news, can be categorized in terms of article structures that define the order in which information is introduced to the story. Some writers deny consciously organizing articles according to specific structures, but may use them to describe the writing \"post hoc\". Others recognize a style within a developing story and use it to develop the narrative. Still others may be required to pursue a specific style from the beginning by publisher guidelines.\n\nSome of the structure types include:\n\nThe inverted pyramid structure begins with the latest or most important developments, then provides greater description and detail, tapering off with less significant events of relevance. Proponents of this structure, which is common in news reporting, may criticize other styles as \"burying the lead,\" though others criticize it as a dull style that \"tells the joke by starting at the punch line.\" Because this structure is designed to permit truncation by an editor, authors do not need to plan a specific ending as they might with other styles.\n\nA narrative structure is a straightforward, chronological description of events. For example, an article about a set of Murders may begin with the discovery of the first victim and end with the imprisonment or execution of a suspect.\n\nThe hourglass structure is a combination of inverted pyramid and narrative structures. The author begins with key details (who, what, when, where, and why), and adds details of increasingly lower importance as in the inverted pyramid structure. The story then abruptly \"turns,\" requiring a clear transition, to focus on a narrative, such as the story of a specific eyewitness or party, which addresses finer details and implications, before making its final conclusion. Like the inverted pyramid, it attempts to satisfy readers who don't complete the article, while continuing to engage readers with greater interest. In broadcast news reporting, the narrative portion may be provided by a reporter on the scene, while the beginning and end are told by an anchor at the studio. The simple narrative portion may be convenient for reporters writing under a deadline.\n\nAn article may begin with an anecdotal \"hook\" to catch the reader's attention. This is followed by a \"nut graph\" paragraph that summarizes the story as a whole. Body paragraphs then explore these ideas in greater detail, provide useful background, or explain conflicting opinions.\n\nThe diamond structure is similar to a nut graph, with anecdotal hook, nut graph, and a wealth of general detail, but then progressively narrows these issues and applies them to the anecdote introduced in the hook. The story may be considered as a \"quest\" to understand the situation of a single individual. A journalism class may require this structure for an opinion story.\n\nThe \"Christmas tree\" shape of this story broadens out from the introduction and a series of internal turning points or surprises within the narrative, before coming to a final conclusion. For example, the subject may be revealed to (a)HIV, (b) be unresponsive to existing drugs, (c) learn of an experimental study, but (d) doesn't know if it will work. Each of these turning points provides the basis for further development of the story.\n\nThe organic structure, as expounded by Jon Franklin, is composed at its most basic level of visual imagery that provides a cinematic feel. These are linked into \"foci\" that detail an action, which are in turn linked by \"transitions in time, mood, subject and character.\" A typical sequence of foci may be complication, development, and resolution.\n\nCredited to Rick Bragg, the \"five boxes\" of this structure are a standard progression: A hook that attracts the reader's attention with a specific image or detail; a nut graph summary; a second lead that introduces remaining facts; details of secondary importance; and a \"kicker:\" a strong image, comment, or quotation that provides a strong conclusion.\n\n"}
{"id": "744504", "url": "https://en.wikipedia.org/wiki?curid=744504", "title": "Circular reference", "text": "Circular reference\n\nA circular reference is a series of references where the last object references the first, resulting in a closed loop.\nA circular reference is not to be confused with the logical fallacy of a circular argument. Although a circular reference will often be unhelpful and reveal no information, such as two entries in a book index referring to each other, it is not necessarily so that a circular reference is of no use. Dictionaries, for instance, must always ultimately be a circular reference since all words in a dictionary are defined in terms of other words, but a dictionary nevertheless remains a useful reference. Sentences containing circular references can still be meaningful;\n\nis circular but not without meaning. Indeed, it can be argued that self-reference is a necessary consequence of Aristotle's Law of non-contradiction, a fundamental philosophical axiom. In this view, without self-reference, logic and mathematics become impossible, or at least, lack usefulness.\n\nCircular references can appear in computer programming when one piece of code requires the result from another, but that code needs the result from the first. For example:\n\nFunction A will show the time the sun last set based on the current date, which it can obtain by calling Function B. Function B will calculate the date based on the number of times the moon has orbited the earth since the last time Function B was called. So, Function B asks Function C just how many times that is. Function C doesn't know, but can figure it out by calling Function A to get the time the sun last set.\n\nThe entire set of functions is now worthless because none of them can return any useful information whatsoever. This leads to what is technically known as a livelock. It also appears in spreadsheets when two cells require each other's result. For example, if the value in Cell A1 is to be obtained by adding 5 to the value in Cell B1, and the value in Cell B1 is to be obtained by adding 3 to the value in Cell A1, no values can be computed. (Even if the specifications are A1:=B1+5 and B1:=A1-5, there is still a circular reference. It doesn't help that, for instance, A1=3 and B1=-2 would satisfy both formulae, as there are infinitely many other possible values of A1 and B1 that can satisfy both instances.)\n\nA circular reference represents a big problem in computing.\n\nIn ISO Standard SQL circular integrity constraints are implicitly supported within a single table. Between multiple tables circular constraints (e.g. foreign keys) are permitted by defining the constraints as deferrable (See CREATE TABLE for PostgreSQL and DEFERRABLE Constraint Examples for Oracle). In that case the constraint is checked at the end of the transaction not at the time the DML statement is executed. To update a circular reference two statements can be issued in a single transaction that will satisfy both references once the transaction is committed.\n\nA distinction should be made with processes containing a circular reference between those that are incomputable and those that are an iterative calculation with a final output. The latter may fail in spreadsheets not equipped to handle them but are nevertheless still logically valid.\n\nCircular reference in worksheets can be a very useful technique for solving implicit equations such as the Colebrook equation and many others, which might otherwise require tedious Newton-Raphson algorithms in VBA or use of macros.\n\n"}
{"id": "22696673", "url": "https://en.wikipedia.org/wiki?curid=22696673", "title": "Cognitive synonymy", "text": "Cognitive synonymy\n\nCognitive synonymy is a type of synonymy in which synonyms are so similar in meaning that they cannot be differentiated either denotatively or connotatively, that is, not even by mental associations, connotations, emotive responses, and poetic value. It is a stricter (more precise) technical definition of synonymy, specifically for theoretical (e.g., linguistic and philosophical) purposes. In usage employing this definition, synonyms with greater differences are often called near-synonyms rather than synonyms.\n\nIf a word is cognitively synonymous with another word, they refer to the same thing independently of context. Thus, a word is cognitively synonymous with another word if and only if all instances of both words express the same exact thing, and the referents are necessarily identical, which means that the words' interchangeability is not context-sensitive.\n\nWillard Van Orman Quine used the concept of cognitive synonymy extensively in his famous 1951 paper \"Two Dogmas of Empiricism\", where two words were cognitively synonymous if they were interchangeable in every possible instance.\n\nFor example,\n\nQuine notes that if one is referring to the word itself, this doesn't apply, as in,\n\nAs compared to the substitution which is obviously false,\n\n\n"}
{"id": "25071036", "url": "https://en.wikipedia.org/wiki?curid=25071036", "title": "Community interpreting", "text": "Community interpreting\n\nCommunity interpreting is a specific type of interpreting service which is found more in community-based than organisational situations. It is a particularly vital service in communities with large numbers of ethnic minorities, enabling those minorities to access services where, due to the language barrier, they would otherwise find it difficult. Situations where such interpreters are necessary are typical include medical, educational, housing, social security and legal areas.\n\nCommunity interpreters need not only to be fluent in the language that they are interpreting, but also with the public services involved, to be aware of the cultural and racial implications of the interpreting work. Interpreters are also expected to follow the Interpreter's Code of Ethics.\n\n"}
{"id": "51147036", "url": "https://en.wikipedia.org/wiki?curid=51147036", "title": "Constraint-based grammar", "text": "Constraint-based grammar\n\nConstraint-based grammars can perhaps be best understood in contrast to generative grammars. A generative grammar lists all the transformations, merges, movements, and deletions that can result in all well-formed sentences, while constraint-based grammars, take the opposite approach, allowing anything that is not otherwise constrained. \"The grammar is nothing but a set of constraints that structures are required to satisfy in order to be considered well-formed.\" \"A constraint-based grammar is more like a data base or a knowledge representation system than it is like a collection of algorithms.\"\n\nExamples of such grammars include\n"}
{"id": "590696", "url": "https://en.wikipedia.org/wiki?curid=590696", "title": "Definite description", "text": "Definite description\n\nA definite description is a denoting phrase in the form of \"the X\" where X is a noun-phrase or a singular common noun. The definite description is \"proper\" if X applies to a unique individual or object. For example: \"the first person in space\" and \"the 42nd President of the United States of America\", are proper. The definite descriptions \"the person in space\" and \"the Senator from Ohio\" are \"improper\" because the noun phrase X applies to more than one thing, and the definite descriptions \"the first man on Mars\" and \"the Senator from some Country\" are \"improper\" because X applies to nothing. Improper descriptions raise some difficult questions about the law of excluded middle, denotation, modality, and mental content.\n\nAs France is currently a republic, it has no king. Bertrand Russell pointed out that this raises a puzzle about the truth value of the sentence \"The present King of France is bald.\"\n\nThe sentence does not seem to be true: if we consider all the bald things, the present King of France isn't among them, since there is no present King of France. But if it is false, then one would expect that the negation of this statement, that is, \"It is not the case that the present King of France is bald,\" or its logical equivalent, \"The present King of France is not bald,\" is true. But this sentence doesn't seem to be true either: the present King of France is no more among the things that fail to be bald than among the things that are bald. We therefore seem to have a violation of the Law of Excluded Middle.\n\nIs it meaningless, then? One might suppose so (and some philosophers have; see below) since \"the present King of France\" certainly does fail to refer. But on the other hand, the sentence \"The present King of France is bald\" (as well as its negation) seem perfectly intelligible, suggesting that \"the Present King of France\" can't be meaningless.\n\nRussell proposed to resolve this puzzle via his theory of descriptions. A definite description like \"the present King of France\", he suggested, isn't a referring expression, as we might naively suppose, but rather an \"incomplete symbol\" that introduces quantificational structure into sentences in which it occurs. The sentence \"the present King of France is bald\", for example, is analyzed as a conjunction of the following three quantified statements:\n\n\nMore briefly put, the claim is that \"The present King of France is bald\" says that some x is such that x is currently King of France, and that any y is currently King of France only if y = x, and that x is bald: \n\nThis is \"false\", since it is \"not\" the case that some x is currently King of France.\n\nThe negation of this sentence, i.e. \"The present King of France is not bald\", is ambiguous. It could mean one of two things, depending on where we place the negation 'not'. On one reading, it could mean that there is no one who is currently King of France and bald:\n\nOn this disambiguation, the sentence is \"true\" (since there is indeed no x that is currently King of France).\n\nOn a second reading, the negation could be construed as attaching directly to 'bald', so that the sentence means that there is currently a King of France, but that this King fails to be bald:\n\nOn this disambiguation, the sentence is \"false\" (since there is no x that is currently King of France).\n\nThus, whether \"the present King of France is not bald\" is true or false depends on how it is interpreted at the level of logical form: if the negation is construed as taking wide scope (as in the first of the above)), it is true, whereas if the negation is construed as taking narrow scope (as in the second of the above]), it is false. In neither case does it lack a truth value.\n\nSo we do \"not\" have a failure of the Law of Excluded Middle: \"the present King of France is bald\" (i.e. formula_4) is false, because there is no present King of France. The negation of this statement is the one in which 'not' takes wide scope: formula_5. This statement is \"true\" because there does not exist anything which is currently King of France.\n\nStephen Neale, among others, has defended Russell's theory, and incorporated it into the theory of generalized quantifiers. On this view, 'the' is a quantificational determiner like 'some', 'every', 'most' etc. The definite description 'the' has the following denotation (using lambda notation):\n\nwe then get the Russellian truth conditions via two steps of function application: 'The present King of France is bald' is true if, and only if formula_4. On this view, definite descriptions like 'the present King of France' do have a denotation (specifically, definite descriptions denote a function from properties to truth values—they are in that sense not syncategorematic, or \"incomplete symbols\"); but the view retains the essentials of the Russellian analysis, yielding exactly the truth conditions Russell argued for.\n\nThe Fregean analysis of definite descriptions, implicit in the work of Frege and later defended by Strawson (1950) among others, represents the primary alternative to the Russellian theory. On the Fregean analysis, definite descriptions are construed as referring expressions rather than quantificational expressions. Existence and uniqueness are understood as a presupposition of a sentence containing a definite description, rather than part of the content asserted by such a sentence. The sentence 'The present King of France is bald', for example, isn't used to claim that there exists a unique present King of France who is bald; instead, that there is a unique present King of France is part of what this sentence \"presupposes\", and what it \"says\" is that this individual is bald. If the presupposition fails, the definite description fails to refer, and the sentence as a whole fails to express a proposition.\n\nThe Fregean view is thus committed to the kind of truth value gaps (and failures of the Law of Excluded Middle) that the Russellian analysis is designed to avoid. Since there is currently no King of France, the sentence 'The present King of France is bald' fails to express a proposition, and therefore fails to have a truth value, as does its negation, 'The present King of France is not bald'. The Fregean will account for the fact that these sentences are nevertheless \"meaningful\" by relying on speakers' knowledge of the conditions under which either of these sentences \"could\" be used to express a true proposition. The Fregean can also hold on to a restricted version of the Law of Excluded Middle: for any sentence whose presuppositions are met (and thus expresses a proposition), either that sentence or its negation is true.\n\nOn the Fregean view, the definite article 'the' has the following denotation (using lambda notation):\n\n(That is, 'the' denotes a function which takes a property f and yields the unique object z that has property f, if there is such a z, and is undefined otherwise.) The presuppositional character of the existence and uniqueness conditions is here reflected in the fact that the definite article denotes a partial function on the set of properties: it is only defined for those properties f which are true of exactly one object. It is thus undefined on the denotation of the predicate 'currently King of France', since the property of currently being King of France is true of no object; it is similarly undefined on the denotation of the predicate 'Senator of the US', since the property of being a US Senator is true of more than one object.\n\nFollowing the example of \"Principia Mathematica\", it is customary to use a definite description operator symbolized using the \"turned\" (rotated) Greek lower case iota character \"℩\". The notation ℩formula_15 means \"the unique formula_16 such that formula_17\", and\n\nis equivalent to \"There is exactly one formula_20 and it has the property\nformula_21\":\n\n"}
{"id": "7302228", "url": "https://en.wikipedia.org/wiki?curid=7302228", "title": "Distributional semantics", "text": "Distributional semantics\n\nDistributional semantics is a research area that develops and studies theories and methods for quantifying and categorizing semantic similarities between linguistic items based on their distributional properties in large samples of language data. The basic idea of distributional semantics can be summed up in the so-called Distributional hypothesis: \"linguistic items with similar distributions have similar meanings.\"\n\nThe distributional hypothesis in linguistics is derived from the semantic theory of language usage, i.e. words that are used and occur in the same contexts tend to purport similar meanings. \nThe underlying idea that \"a word is characterized by the company it keeps\" was popularized by Firth. \nThe Distributional Hypothesis is the basis for statistical semantics. Although the Distributional Hypothesis originated in linguistics,\nit is now receiving attention in cognitive science especially regarding the context of word use.\nIn recent years, the distributional hypothesis has provided the basis for the theory of similarity-based generalization in language learning: the idea that children can figure out how to use words they've rarely encountered before by generalizing about their use from distributions of similar words.\nThe distributional hypothesis suggests that the more semantically similar two words are, the more distributionally similar they will be in turn, and thus the more that they will tend to occur in similar linguistic contexts. Whether or not this suggestion holds has significant implications for both the data-sparsity problem in computational modeling, and for the question of how children are able to learn language so rapidly given relatively impoverished input (this is also known as the problem of the poverty of the stimulus).\n\nDistributional semantics favor the use of linear algebra as computational tool and representational framework. The basic approach is to collect distributional information in high-dimensional vectors, and to define distributional/semantic similarity in terms of vector similarity. Different kinds of similarities can be extracted depending on which type of distributional information is used to collect the vectors: topical similarities can be extracted by populating the vectors with information on which text regions the linguistic items occur in; paradigmatic similarities can be extracted by populating the vectors with information on which other linguistic items the items co-occur with. Note that the latter type of vectors can also be used to extract syntagmatic similarities by looking at the individual vector components.\n\nThe basic idea of a correlation between distributional and semantic similarity can be operationalized in many different ways. There is a rich variety of computational models implementing distributional semantics, including latent semantic analysis (LSA), Hyperspace Analogue to Language (HAL), syntax- or dependency-based models, random indexing, semantic folding and various variants of the topic model.\n\nDistributional semantic models differ primarily with respect to the following parameters:\n\n\nDistributional semantic models that use linguistic items as context have also been referred to as word space models.\n\nCompositional distributional semantic models are an extension of distributional semantic models that characterize the semantics of entire phrases or sentences. This is achieved by composing the distributional representations of the words that sentences contain. Different approaches to composition have been explored, and are under discussion at established workshops such as SemEval.\n\nSimpler non-compositional models fail to capture the semantics of larger linguistic units as they ignore grammatical structure and logical words, which are crucial for their understanding.\n\nDistributional semantic models have been applied successfully to the following tasks:\n\n\n\n\n\n"}
{"id": "16794300", "url": "https://en.wikipedia.org/wiki?curid=16794300", "title": "Economics of language", "text": "Economics of language\n\nThe economics of language is an emerging field of study concerning a range of topics such as the effect of language skills on income and trade, and the costs and benefits of language planning options, preservation of minority languages, etc. It is relevant to analysis of language policy.\n\nIn his book 'Language and economy', the German sociolinguist Florian Coulmas discusses \"the many ways in which language and economy interact, how economic developments influence the emergence, expansion, or decline of languages; how linguistic conditions facilitate or obstruct the economic process; how multilingualism and social affluence are interrelated; how and why language and money fulfill similar functions in modern societies; why the availability of a standard language is an economic advantage; how the unequal distribution of languages in multilingual societies makes for economic inequality; how the economic value of languages can be assessed; why languages have an internal economy and how they adapt to the demands of the external economy. Florian Coulmas shows that language is the medium of business, an asset in itself and sometimes a barrier to trade\".\n\nStates shoulder language costs, because it maintains themselves by means of it, as does business which needs communication competence. Florian Coulmas discusses the language-related expenditures of government and business in Language and economy. In the same book he also discusses the role of language as a commodity, because languages can behave like economic systems. That is why socio-economic ecologies are (dis)favorable to particular languages. The spread of languages depends in an essential way on economic conditions. Language can be an expression of symbolic power. However, changes in the linguistic map of the world show that these are also powerful linked to economic developments in the world. Assigning an economic value to a certain language in the linguistic market place means vesting it with some of the privileges and power related to that language. Most language communities in the world practice this policy without any concern about \"reciprocity\" in language learning investments, forgetting the \"pursuit of linguistic justice as parity of esteem and while linguistic regimes are sometimes very unjust\".\n\nLanguages are capital investments in a literal sense : language technology is the most important one. It requires substantial investments which, in the absence of profitability, only affluent countries and businesses can afford. In this respect, today \"English is seen as a consequence and an instrument of American imperial power, an appreciable asset for American anglophones in the twenty-first-century global contest for competitive advantage, prosperity, and power\". Though the best business language remains the language of the customers, meaning multilingual business practices, an \"ideal' global economy presupposes a single language for the whole world. But an \"ideal\" global language presupposes a common acceptable and fair language burden for all business partners. See in this respect language tax to counteract linguistic inequality, as also language for purposes of trade incurs costs to most countries and private entreprises, whereas governments of countries whose language occupies a leading position on the international language market refuse to subsidize the spread of other languages for which they believe they have no need. In his report \"L'enseignement des langues étrangères comme politique publique\", François Grin argues that 'though some languages would be more beneficial in terms of cost-benefit analysis' such as e.g. Esperanto (Esperanto business groups such as IKEF have been active for many years), the problem is that a shifting pattern in the valuation of languages is not always brought about by rationally culculable factors only. In addition to its economic potential, language is also a carrier of political, cultural and sociopsychological properties. In spite of the non-economic values attached to language, what prevails in matters of language is often that which is profitable and this can lead to the superiority of a dominant language as a means of production, with a high linguistic capital value. In this respect it is evident to see that the will (or necessity) to learn English in the last decades has grown so much and its range of action has been so wide that the economic necessity and other incentives of foreign-language study are generally perceived as unimportant. For similar reasons, former British prime minister Margaret Thatcher tried to torpedo the LINGUA program of the European Community, as from her point of view, Britain was asked to pay for a program which benefited her country least. Because of the enormous imbalance on current accounts of the major European languages in favour of English, the LINGUA programme called for an expansion and diversification of foreign-language education in the Member States. For the individual speaker the unequal linguistic balances imply that the first language is an economically exploitable qualification for some who can simply marketing their mother tongue skills, whereas others can not.\n\n\n\n"}
{"id": "7060322", "url": "https://en.wikipedia.org/wiki?curid=7060322", "title": "European Day of Languages", "text": "European Day of Languages\n\nThe European Day of Languages is 26 September, as proclaimed by the Council of Europe on 6 December 2001, at the end of the European Year of Languages (2001), which had been jointly organised by the Council of Europe and the European Union. Its aim is to encourage language learning across Europe.\n\nThe general objectives of the European Day of Languages are to:\n\nIn keeping with these rules, people, young and old, are encouraged to take up a language, or take special pride in their existing language skills. Also, those responsible for providing access to language learning are encouraged to make it easier for people to learn a range of languages, and to support policy initiatives to promote languages. There is also emphasis on learning a language other than English.\n\nOn the occasion of the day, a range of events are organised across Europe, including those for children, television and radio programmes, language classes and conferences. The events are not organised by the Council of Europe or the European Union nor do they allocate special funding (i.e. apart from their existing language programmes) for the day. Member states and potential partners are given a free hand to organise activities. To coordinate the activities organised at national level, the Council of Europe asks participating countries to nominate \"National Relay Persons\" for the day. The national relay in the UK used to be CILT, the National Centre for Languages.\n\nThere are about 225 indigenous languages in Europe – roughly 3% of the world's total. Most of the European languages are of Indo-European origin. Since the end of the 18th century, the most widespread language of Europe (both in terms of geography and the number of native speakers) has been Russian, which replaced French. Counting only native speakers, approximately 150 million Europeans speak Russian on a daily basis, followed by German (approx. 95 mil.), English and French (each by 65 mil.), Italian (60 mil.), Spanish and Polish (40 mil. each), Ukrainian (30 mil.) and Romanian (26 mil.). As far as foreign language studies are concerned, English is currently the most popular foreign language in Europe, followed by German, French, Italian, Russian and Spanish\n\nAccording to the European Union survey \"Europeans and their Languages\" (\"Special Eurobarometer 243\", February 2006), 56% of EU citizens (25 member states) speak a language other than their mother tongue, but 44% admit to not knowing any languages other than their native language. However, 28% have knowledge of two foreign languages. Among EU citizens, 38% indicate that they know English, followed by 14% knowing French or German, 7% Russian, 5% Spanish and 3% Italian. The typical multilingual European is a student or someone holding a managerial position or someone born in a country where the language of his/her parents is different from the main language of the country.\n\nWith greater numbers of immigrants and refugees, European cities have become more multilingual. For example: in Moscow and Saint Petersburg many recent immigrants speak Ukrainian, Romanian, Armenian, Tatar, Azeri, Tajik, Chinese or one of many other languages; in London some 300 languages are spoken (English, French, Chinese, Polish, Russian, Spanish, Portuguese, Arabic, Bengali, Turkish, Kurdish, Berber, Hindi, Urdu, Punjabi etc.).\n\nThe European Union adheres to a policy of multilingualism, both in its institutional workings and as an aim for its citizens. At the 2002 EU summit in Barcelona, it set a target for children to learn at least two foreign languages from an early age. Multilingualism for the EU is linked to worker mobility and the European economy. The European Union spends more than €30 million a year promoting language learning and linguistic diversity through the \"Socrates\" and \"Leonardo da Vinci\" programmes, a policy that began with the pioneering \"Lingua programme\" in 1990.\n\n\n"}
{"id": "3022991", "url": "https://en.wikipedia.org/wiki?curid=3022991", "title": "Even language", "text": "Even language\n\nThe Even language , also known as Lamut, Ewen, Eben, Orich, Ilqan (, earlier also ), is a Tungusic language spoken by the Evens in Siberia. It is spoken by widely scattered communities of reindeer herders from Kamchatka and the Sea of Okhotsk in the east to the River Lena in the west, and from the Arctic coast in the north to the River Aldan in the south. Even is an endangered language, with only some 5,700 speakers (Russian census, 2010). These speakers are specifically from the Magadan region, the Chukot region and the Koryak region. Dialects are Arman, Indigirka, Kamchatka, Kolyma-Omolon, Okhotsk, Ola, Tompon, Upper Kolyma, Sakkyryr, Lamunkhin.\n\nIn these regions where the Evens primarily reside, the Even language is generally implemented in pre-school and elementary school, alongside the national language, Russian. Where Even functioned primarily as an oral language communicated between reindeer herding brigades, textbooks began circulating throughout these educational institutions from around 1925 to 1995. \n\nThe syntax of the Even language follows the nominative case and SOV (subject-object-verb) word order, with the attribute preceding the dependent member.\n\nIn some remote Arctic villages, such as Russkoye Ustye, whose population descended from Russian-Even intermarriage, the language spoken into the 20th century was a dialect of Russian with a strong Even influence.\n\n/ɣ, q/ are allophones of /ɡ, k/.\n\n"}
{"id": "19785923", "url": "https://en.wikipedia.org/wiki?curid=19785923", "title": "Fay ki Boli", "text": "Fay ki Boli\n\nFay Kee Bolee (, lit. \"Fay Language\") is a language game used by Urdu-speaking people in Pakistan. It can be regarded as a trivial substitution cipher and it is sometimes used by adults to converse in (perceived) privacy in the presence of children. It is close to games found in other languages such as Farfallino alphabet (Italian), jeringonza (Spanish/Portuguese), and Pig Latin (English).\n\n\"\"Fay\" (), Urdu language Alphabet corresponding to the sound of \"F\" in English, can be replaced by most other consonants to form another variety. Pay ki Boli is another more complex variation of the language game.\n\nThe game is based on a simple substitution: adding an extra \"F\" before each vowel of a syllable. Insert \"fay\" in the middle of each syllable (usually before the vowel - splitting the syllable into two) in each word. In some monosyllabic words which would still be so obvious, \"yay\"\" (), Urdu alphabet for \"Y\", iyfa\n\n"}
{"id": "1025265", "url": "https://en.wikipedia.org/wiki?curid=1025265", "title": "Gender of connectors and fasteners", "text": "Gender of connectors and fasteners\n\nIn electrical and mechanical trades and manufacturing, each half of a pair of mating connectors or fasteners is conventionally assigned the designation male or female. The \"female\" connector is generally a receptacle that receives and holds the \"male\" connector. On occasion, the terms \"male\" and \"female\" are respectively referred to as the A and B ends, though the names of some standards conflict with this as they contain the letters A or B within the name; unambiguous, though rare, terms include plug and socket or jack.\n\nThe assignment is a direct analogy with genitalia and heterosexual sex; the part bearing one or more protrusions, or which fits inside the other, being designated male in contrast to the part containing the corresponding indentations, or fitting outside the other, being designated female. Extension of the analogy results in the verb to mate being used to describe the process of connecting two corresponding parts together. \n\nIn some cases (notably electrical power connectors), the gender of connectors is selected according to rigid rules, to enforce a sense of one-way directionality (e.g. a flow of power \"from\" one device \"to\" another). This gender distinction is implemented to enhance safety or ensure proper functionality by preventing unsafe or non-functional configurations from being set up.\n\nIn terms of mathematical graph theory, an electrical power distribution network made up of plugs and sockets is a directed tree, with the directionality arrows corresponding to the female-to-male transfer of electrical power through each mated connection. This is an example where male and female connectors have been deliberately designed and assigned to physically enforce a safe network topology.\n\nIn other contexts, such as plumbing, one-way flow is \"not\" enforced through connector gender assignment. Flows through piping networks can be bidirectional, as in underground water distribution networks which have designed-in redundancy. In plumbing situations where one-way flow \"is\" desired, it is implemented through other means (e.g. gravity flow, one-way check valves), and not through male-female gender schemes.\n\nIn mechanical design, the prototypical \"male\" component is a threaded bolt, but an alignment post, a mounting boss, or a sheet metal tab connector can also be considered as male. Correspondingly, a threaded nut, an alignment hole, a mounting recess, or sheet metal slot connector is considered to be female.\n\nWhile some mechanical designs are \"one-off\" custom setups not intended to be repeated, there is an entire fastener industry devoted to manufacturing mass-produced or semi-custom components. To avoid unnecessary confusion, conventional definitions of fastener gender have been defined and agreed upon.\n\nAlthough this aspect is not highlighted in their promotional literature, several common construction toys embody gendered (and in some cases, genderless) mechanical interconnections. This should not be surprising, since these toys feature the nearly infinite flexibility and versatility of shape that a modular interconnect architecture can enable. Mathematicians have begun to classify well-known construction sets using group theory to study the combinatoric possibilities of structures that can be built.\n\nFor example, the canonical LEGO plastic blocks have \"female\" indentations on the lower surface, and \"male\" bosses or protrusions on the upper surfaces. Meccano and Erector have many gendered connections, starting with the nut-and-bolt fasteners they use frequently.\n\nStickle bricks, using interlocking plastic protrusions, are effectively genderless. Lincoln logs use a very simple form of genderless connections. Kapla or KEVA planks are extremely simple genderless systems interconnected only by gravity.\n\nIn plumbing fittings, the \"M\" or \"F\" usually comes at the beginning rather than the end of the abbreviated designation. For example:\n\nA short length of pipe having an MIP thread at both ends is sometimes called a nipple. A short pipe fitting having an FIP thread at both ends is sometimes called a coupling.\n\nHermaphroditic connections, which include both male and female elements in a single unit, are used for some specialized tubing fittings, such as Storz fire hose connectors. A picture of such fittings appears in the \n\nDownspouts (downpipes, rain conductors or leaders) are used to convey rainwater from roof gutters to the ground through hollow pipes or tubes. These tubes usually come in sections, joined by inserting the male end (often crimped with a special tool to slightly reduce its size) into the female end of the next section. These connections are usually not sealed or caulked, instead relying on gravity to move the rainwater from the male end and into the receiving female connection located directly below.\n\nSheet metal ductwork for conveying air in HVAC systems typically uses gendered connections. Typically, the airflow through a ductwork connection is from male to female. However, since one-way flow is implemented by forced-air fans or blowers, \"backwards\" gendered connections can be seen frequently in some systems, since all connections are typically sealed with duct sealing mastic or tape to prevent leakage anyway. The flow convention is usually loosely adhered to for simplicity of design, and to reduce the number of gender changer fittings required, but exceptions are made whenever expedient.\n\nAlthough the gender of tubing and plumbing fittings is usually obvious, this may not be true of electrical connectors because of their more complex and varying constructions. Instead, connector gender is conventionalized and thus can be somewhat obscure to the uninitiated. For example, the female D-subminiature connector body projects outward from the mounting plane of the chassis, and this protrusion could be erroneously construed as male. Instead, the \"maleness\" of the D-subminiature connectors is defined by specific presence of \"male pins\", rather than by the protrusion of the connector, which is also true for many other pin-based connectors like XLR. The male/female distinction is more obvious with ring crimp lug connectors which are placed around a screw post, but again with spade or split ring crimp lug connectors the end alone is not obviously female.\n\nFurther confusion can be caused by the term \"jack\", which is used for both female and male connectors and typically refers to the fixed (panel) side of a connector pair. IEEE STD 100, IEEE-315-1975 and IEEE 200-1975 (replaced by ASME Y14.44-2008) define \"Plug\" and \"Jack\" by location or mobility, rather than gender.\n\nA connector in a fixed location is a jack and a moveable connector is a plug. The distinction is relative, so a portable radio is considered stationary compared to the cable from the headphones; the radio has a jack, and the headphone cable has a plug. Where the relationship is equal, such as when two flexible cables are connected, each is considered a plug. Jacks use the reference designator prefix of J and plugs use the reference designator prefix of P. It is possible in the case of box mounted connectors for the connector to be a receptacle with male pin contacts. In this case the connector is designated a jack (J ref des) regardless of the contact gender because the housing for the contacts is in fact configured as the receptacle even though its mate (the plug) goes around the receptacle. See MIL-STD-38999 and similar cases.\n\nIt is common practice to use female connectors for jacks, so the informal gender-based usage often happens to agree with the functional description of the technical standards. However, this is \"not\" always the case; often-seen exceptions include a computer's AC Power Inlet and EIA232 DE9 Serial Port, or the male coaxial power jacks for connecting external power adapters to portable equipment.\n\nTo summarize, it is considered best practice to use \"male\" and \"female\" for connector \"gender\", and \"plug\" and \"jack\" for connector \"function\" or \"mobility\".\n\nIn the UK, many Commonwealth countries and some non-English-speaking countries, the word \"jack\" may refer to the \"plug\" on the end of a removable cable. These connectors were originally referred to as \"jack plugs\", or plugs intended to be mated with fixed receptacles, or \"sockets\" (which North Americans would call \"jacks\"), but the second word was dropped. This variant usage is in direct contradiction to common usage and official standards in North America.\n\nIn the United Kingdom, for example, the connector on the end of a headphone lead is known as a \"jack\", that plugs into a \"socket\" on the main unit. The same generally occurs also in Italy, where the English word \"jack\" is commonly used to indicate the connector on the end of a headphone lead.\n\nIn Romania female connectors are known as \"mamă\" (mother) and male connectors are \"tată\" (father).\n\nThe standard letters \"M\" and \"F\" are commonly used in part numbers to designate connector gender. For example, in Switchcraft XLR microphone or hydrophone connectors, the part numbers are denoted as follows:\n\nThe terms plug, pin, and prong are also often used for \"male\" connectors, and receptacle, socket, and slot are used for \"female\" connectors. In many cases these terms are more common than \"male\" and \"female\", especially in documentation intended for the non-specialist. These nearly synonymous terms can cause a fair amount of confusion when the designations are shortened in labels.\n\nFor example, a female high-density D-subminiature connector with a size 1 shell can be named DE15F or DE15S (see accompanying pictures). Both terms mean the same thing but could be construed to be completely different items. Similarly, a male standard-density D-sub with a size 1 shell can be named DE9M or DE9P; a female standard-density D-sub with a size 2 shell can be named DA15F or DA15S; a male high-density D-sub with a size 3 shell can be named DB44M or DB44P; and so forth.\n\nElectronic designers often select female jack connectors for fixed mounting on electronic equipment they design. This is usually done because female connectors are more resistant to damage or contamination, by virtue of their concealed or recessed electrical contacts. A damaged motherboard connector can result in the scrapping of an expensive piece of electronic equipment. The risk of expensive damage is reduced by relegating the more exposed male contacts to connecting cables, which can be repaired or replaced at lower cost.\n\nWith an RS232 serial port, the male connector is more fragile than the female connector.\n\nSome people say the male coaxial connector is more prone to damage.\nOther people say the female coaxial connector is more prone to damage.\n\nSuch cost and reliability considerations probably drove the design decision to use female jack connectors on many computer terminals (and some personal computers) for the serial port, in direct violation of the connector gender convention specified in the RS-232 standard for \"DTE\" computer equipment. This confusing reversal of the RS-232 connector gender convention would cause many hours of frustration for ill-informed end users, as they tried to troubleshoot non-functional serial port equipment connections.\n\nIn the case of electrical power connections, designers do not reverse connector gender in such a casual fashion, because exposing live AC line power on male connectors is unsafe and generally illegal. Devices that need to be robust against mechanical damage use a special male IEC 60320 C14 connector (see Gallery above), which is recessed below the surface of a mounting panel, providing the desired physical protection while conforming to safety regulations.\n\nIn electrical connections where voltage or current is sufficient to cause injury, the part permanently connected to the power \"source\" is invariably female, with concealed contacts, to prevent inadvertent touching of live conductors. A male plug, with fully exposed protruding contacts, is installed on the cord of (or directly onto) the device \"receiving\" the power.\n\nIn the case of consumer-level AC power, connector gender is used to implicitly enforce safe use of power connectors. Because of this consideration, it is illegal under electrical code to make or use any \"gender changers\" to connect AC line power to consumer-level equipment.\n\nIn low-voltage use such as for data communications, electrical shock hazard is not an issue, and male or female connectors are used based on other engineering factors such as convenience of use, cost, or ease of manufacturing. For example, the common \"patch cables\" used for Ethernet (and the similar cords used for telephones) typically have male modular plugs on both ends, to connect to jacks on equipment or mounted in walls.\n\nAs an illustrative example of some design tradeoffs in power connector selection, consider the adjacent picture. A commonly seen coaxial power connector is usually set up so that power is fed \"from\" the female plug on the right \"into\" the male jack on the left (which is typically a part of the electronic device accepting the power). Although the plug is female, with a partially recessed center contact, it is still possible for casual accidental contact with a metallic object to short-circuit the power source. Depending on the design of the power adapter, it may react to a short circuit by shutting down temporarily, or instead by blowing out an internal safety fuse.\n\nIn this example, the marginal reliability of the connector choice was deemed to be acceptable by the equipment designer, since the power adapter supplies low voltage that does not pose an electric shock hazard. The potential fire hazard from accidental short-circuiting is addressed by the internal safety fuse, although this requires that a failed power adapter must be completely replaced. In a different design, if the power adapter were intended to supply a voltage sufficient to cause electrical shock, the semi-exposed center contact of the female plug would be considered unacceptably hazardous, requiring a different choice of power connector.\nSome electrical connectors are hermaphroditic because they include both male and female elements in a single unit intended to interconnect freely, without regard for gender. See the discussion of Genderless connectors elsewhere in this article for more detailed information.\n\nAs an additional complication, certain electronic connector designs may incorporate combinations of male and female pins in a \"single\" connector body, for mating with a complementary connector with opposite gender pins in corresponding positions. In these unusual cases, gender is often defined by the shape of the connector \"body\", rather than the mixed-gender connector \"pins\" and \"sockets\". These types of connectors are not strictly speaking hermaphroditic, since mating connectors are \"not\" freely interchangeable. An informal term that has been used for these connectors is \"bisexual\", in addition to the more official terminology, mixed-gender. Thus, for example, one can have a mixed-gender female plug that connects to a mixed-gender male jack (though a reversed gender assignment of connectors would be a more typical design choice in this example).\n\nMale connector pins are often protected by a shell (also called a shroud, surround, or shield), which may envelop the entire female connector when mated. RF connectors often have multiple layers of interlocking shells to properly connect the shields of coaxial and triaxial cable. In such cases, the gender is assigned based on the \"innermost\" connecting point. With the exception of reverse polarity BNC or TNC, where the outer shell determines the gender and the innermost connecting points are opposite to a standard connector, for example a female RP-TNC connector has a solid innermost pin.\n\nAnother ambiguous situation arises with the connectors used for USB, FireWire (IEEE-1394), HDMI, and Thunderbolt serial data bus connections. Close examination of these connectors reveals that the contact \"pins\" are not actually pins, but instead are conductive surfaces that slide past each other when they mate. Therefore, the traditional pin and socket nomenclature is not applicable. Instead, most computer hardware people fall back to referring to the wrap-around metal shield on the plug connector \"as if\" it were a connector \"pin\". By this convention, the connectors on serial bus cables are \"male plugs\", and the corresponding connectors on equipment are \"female jacks\". A unique connector configuration where the contacts are hermaphroditic is the ELCO Varicon where the contacts are bifurcated and nest with one another axial at a 90 degree rotation in cross-shaped wells. In this case the plugs had the contacts oriented transversely and the sockets longitudinally. \n\nA casual glance at a USB \"Type A\" plug connector may give the false impression that it is hermaphroditic. However, a physical attempt to mate two USB \"Type A\" cables with each other reveals the fact that the connectors will not interconnect. Classifying according to mathematical graph theory, USB buses are directed trees, whereas FireWire buses have a true bus network topology. This difference is reflected in the bus connectors used, in that USB cables are asymmetrical (one end Type A, other end Type B) while FireWire cables may have identical connectors at both ends.\n\nBy definition, a hermaphroditic connector includes mating surfaces having simultaneous male and female aspects, involving complementary paired identical parts each containing both protrusions and indentations. These mating surfaces are mounted into identical fittings which can freely mate with any other, without regard for gender (provided that the size and type are already matched). Alternative names include hermaphrodite, androgynous, genderless, sexless, combination (or combo), two-in-one, two-way, and other descriptive terms. Several of these latter alternate names are ambiguous in meaning, and should not be used unless carefully defined in context. True hermaphroditic connectors should not be confused with \"mixed gender\" connectors, which are described elsewhere in this article.\n\nAnother closely related type is the stackable connector for electronics, which typically has male pins on one surface, and complementary female sockets on the opposite surface, allowing multiple units to be stacked up like plastic milk crates. Examples of this include stackable banana plugs, and interconnect cables specified for the IEEE-488 instrumentation bus. Stackable mezzanine bus connectors are used on some modular microcomputer accessory boards for systems such as the Arduino add-on daughterboards called \"shields\". The older PC/104 embedded PC modules use a similar stackable format for interconnection. Stackable connectors are not classified as hermaphroditic in the strictest sense, but are often described as such in looser usage.\n\nThe hermaphroditic design is useful when multiple complex or lengthy components must be arbitrarily connected in various combinations. For example, if hoses have hermaphroditic fittings, they can be connected without having to pull a lengthy hose and reverse it because it has the wrong gender to connect to another hose. Some military fiber optical cables also have hermaphroditic connectors to prevent \"wrong gender\" connector problems in field deployments. In a similar fashion, railcars are usually equipped with hermaphroditic railway coupling mechanisms that allow either end of the vehicle to be connected to a train consist without having to turn the railcar around first.\nFor the same reason, several spacecraft docking mechanisms are designed to be \"androgynous\",\nincluding the Androgynous Peripheral Attach System, the NASA Docking System, and Chinese Docking Mechanism.\n\nIn the absence of genderless connectors, \"gender changer\" fittings might be used to enable certain connections. The designer of a connection system may use one or both schemes to allow arbitrary connectivity, or even combine both schemes into a single system.\n\nWhen an enforced sense of unidirectionality or \"one-way flow\" is required for safety or other reasons (for example, AC electrical power connections), a strict assignment of connector genders is implemented to prevent undesired configurations, and gender changers are banned.\n\nSome commonly seen examples of hermaphroditic connectors include the SAE connector for 12 V DC power, jackhammer air hose connectors, and the Anderson Powerpole series of modular high-current power connectors. The IBM token ring connector was another widespread example, but it has become obsolete and is being phased out. The General Radio Corporation (GenRad) developed a hermaphroditic coaxial radio frequency connector often called the \"GR connector\".\n\nSome audio multicore cables are fitted with hermaphroditic multipin quick-disconnect connectors for ease of use in the field. One style of this audio signal cable is fitted on both ends with connectors that are each populated half with pins and half with sockets. The advantage to the user is that it does not matter which end connects to the stage and which to the audio mixer, facilitating faster set up. Another style of connector uses hybrid male/female pins with a receiving slot fitted in the center of each two-tine pin, and relies on 90-degree rotation of the pin axes to mate. The connector housings themselves are sexed male and female.\n\nDevices used for mating two connectors of the same gender have a wide variety of terms, including for example: \"gender changer\", \"gender mender\", \"gender bender\", \"gender blender\", \"sex changer\", and \"homosexual adapter\". A specific gender changer can be referred to by either the gender of its connectors, or the gender which it is designed to connect to, resulting in a thoroughly ambiguous terminology. Thus a \"male gender changer\" might have female connectors to mate two male ends, or male connectors to mate two female ends.\n\nAdding to this potential for confusion, some gender changers also combine additional functions such as cross-over pin-outs or even embedding micro-controllers for performance, or for logic level or protocol adaptations, which would properly make them an adapter, but this nomenclature is sometimes neglected in marketing materials or common parlance.\n\n\n"}
{"id": "33526402", "url": "https://en.wikipedia.org/wiki?curid=33526402", "title": "German Orthographic Conference of 1901", "text": "German Orthographic Conference of 1901\n\nThe German Orthographic Conference of 1901 (the Berlin II Orthographic Conference; or \"\") took place in Berlin from 17th till 19 June 1901. The results of the conference became official in the German Empire in 1902. \nThe standardized German spelling that resulted from the conference was largely based on the Prussian school spelling, but also on the Orthographic Conference of 1876.\n\nThe conference results removed numerous existing variant forms.\nSoon after the conference, its results were criticized by people who believed there should be further changes.\n\nThe spelling was used in Germany, Austria and Switzerland, apart from the replacement of \"ß\" with \"ss\" in Switzerland in later years.\nThe \"Erziehungsrat des Kantons Zürich\" stopped the teaching of \"ß\" in schools in 1935 with the Kanton Zürich being the first to do so, and the \"Neue Zürcher Zeitung\" as last Swiss newspaper stopped using \"ß\" in 1974. However, some Swiss book publishers still use \"ß\".\n\nIt was not until 95 years later that the German spelling was changed with a reform in 1996.\n"}
{"id": "5841782", "url": "https://en.wikipedia.org/wiki?curid=5841782", "title": "Global Recordings Network", "text": "Global Recordings Network\n\nGlobal Recordings Network (GRN) was founded by Joy Ridderhof in Los Angeles, California in 1939 as \"Gospel Recordings.\" The mission of GRN is \"In partnership with the church, to effectively communicate the Good News of Jesus Christ by means of culturally appropriate audio and audio-visual materials in every language.\" This is accomplished by recording the stories of the Bible in the native language or dialect, by a mother tongue speaker and providing them in an audio format to the community. Often the languages do not have a written form. GRN has recorded over 6,000 languages or dialects. GRN has offices in more than 20 countries.\n\nThe language professor Alexander Arguelles notes that it is possible to use these recordings and the accompanying text in a language the learner knows, to start learning any of the languages. For many there is no other way to learn the language. 1300 of the languages accompany the stories with standardized pictures, shown for 10-20 seconds, which allow learners to find short parallel sections in the language they know and the one they want to learn.\n\nThe recordings have been used for linguistic research on rhythm and phonological characteristics, vowels, consonants, for comparative research on phonemes from hundreds of languages, for developing and testing computer systems to recognize languages, and for documenting and reviving rare languages.\n\n\n\n"}
{"id": "43293772", "url": "https://en.wikipedia.org/wiki?curid=43293772", "title": "Idiom (language structure)", "text": "Idiom (language structure)\n\nIdiom is the syntactical, grammatical, or structural form peculiar to a language. Idiom is the realized structure of a language, as opposed to possible but unrealized structures that could have developed to serve the same semantic functions but did not.\n\nLanguage grammar and syntax is often inherently arbitrary and peculiar to a particular language or a group of related languages. For example, although in English it is \"idiomatic\" (accepted as structurally correct) to say \"cats are associated with agility\", other forms could have developed, such as \"cats associate toward agility\" or \"cats are associated of agility\". Unidiomatic constructions sound wrong to fluent speakers, although they are often entirely comprehensible. For example, the title of the classic book \"English As She Is Spoke\" is easy to understand (its idiomatic counterpart is \"English As It Is Spoken\"), but it deviates from English idiom in the gender of the pronoun and the inflection of the verb. Lexical gaps are another key example of idiom.\n\nMonolingual native speakers in an insulated monolingual-native environment are mostly not conscious of \"idiomaticness\" (the quality or state of a construction matching the idiom of the given language), because in general their minds never reach for, or hear, other possible structures. The main exception is when they hear the natural experimentation of children acquiring the language, when they may encounter, for example, overregularization (for example, \"I seed two deers\" for \"I saw two deer\"). By this correlation, solecism to native-speaking monolingual minds often sounds childish. However, when adults study a foreign language, they become consciously aware of idiomaticness and the lack of it. For example, in English it is idiomatic to use an indefinite article when describing a person's occupation (\"I am a plumber\"; \"she is an engineer\"), but in Spanish and many other languages it is not (\"soy plomero\"; \"ella es ingeniera\"), and a native speaker of English learning Spanish must encounter and accept that fact to become fluent.\n\nThe count sense of the word \"idiom\", referring to a saying with a figurative meaning, is related to the present sense of the word by the arbitrariness and peculiarity aspects; the idiom \"she is pulling my leg\" (meaning \"she is humorously misleading me\") is idiomatic because it belongs, by convention, to the language, whether or not anyone can identify the original logic by which it was coined (arbitrariness), and regardless of whether it translates literally to any other language (peculiarity).\n\n"}
{"id": "56226456", "url": "https://en.wikipedia.org/wiki?curid=56226456", "title": "Interruption (speech)", "text": "Interruption (speech)\n\nAn interruption is a speech event when one person breaks in to interject while another person is talking. Linguists, social psychologists, anthropologists, and sociologists are among the social scientists who have studied and identified patterns of interruption that may differ by gender, social status, race/ethnicity, culture, and political orientation.\n\nHarvey Sacks, the sociologist who launched the field of conversation analysis, worked with linguist Emanuel Schegloff and Gail Jefferson in the 1970s to analyze how turn-taking was organized in speech events such as everyday conversations. Speech events are organized so that only one person speaks at a time and to provide for orderly ways to change speakers. Sacks et al. thought that the process of turn-taking is subconscious.\n\nOverlaps occur when two or more speakers talk simultaneously.\n\nCommunication analyst Julia A. Goldberg uses conversation analysis to define three types of conversational interruptions. Relationally neutral interruptions are interjections by the listener that seek to repair, repeat, or clarify something the speaker just said. During this type of interruption, the interrupter does not intend to exert power over the speaker, or to establish rapport with the speaker. The act of interruption itself is understood as neutral in this instance. Another type of interruption defined by Goldberg is the power interruption, where the interrupter breaks in and cuts off the speaker as a way to display some social power. Power interruptions are understood as acts of conflict and competition, and are viewed as rude, hostile, disrespectful, and/or uncaring about the speaker and/or what the speaker is saying. A rapport interruption is designed to display mutuality and generally conveys the impression that the interrupter understands and empathizes with the speaker and/or the content of the speech, and is interpreted as collaborative and cooperative.\n\nPower interruptions are also analyzed by Zimmerman and West, sociologists who note that the people who seek to be socially dominant exert their power over others through interrupting their speech. This can be seen in interactions between whites and racial/ethnic minorities, and between adults and children. Zimmerman and West also analyzed how sex roles shape interruption patterns.\n\nSince the late-1970s, social scientists have studied the effect gender has on interruption patterns and other components of verbal communication. The findings of these studies are mixed, with some finding gender differences, while others did not. Among those that found gender differences are sociologists Don Zimmerman and Candace West who used male dominance theory to claim that men interrupted women to assert their social dominance over females. Zimmerman and West's work discovered that interruptions were more evenly distributed in conversations involving same-sex speakers, while in cross-sex interactions, men were much more likely to interrupt women. Zhao and Gantz analyzed fictional TV shows to claim that male characters used disruptive interruptions more than female characters, while female characters more often used cooperative interruptions. They note, however, that the apparent gender differences in interruption patterns are affected by differences in social status among the TV characters. Goldberg notes that when conversational context and content are analyzed, interruptions may be seen as power displays, rapport displays, or as neutral acts that may or may not be shaped by the gender of the speaker. Linguist Makri-Tsilipakou discovered that men and women use \"simultaneous speech\" at about the same rate, but the sexes differ as to their interpretation of the meaning of the interruption. Women use simultaneous speech as a sign of support and agreement, while men use it either as support for the other's speech or to dissent from other speakers or from their viewpoint. Drass, a social psychologist, found that gender identity, as separate from biological sex, was an important variable, with persons who were more male-identified being more likely to interrupt than persons who were more female-identified.\n\nConversely, a study by Murray and Covelli used Zimmerman and West's coding strategies on their own dataset of conversations to find that women interrupted men more often than men interrupted women. According to James and Clarke, this pattern is especially evident in conversational situations where women felt more expertise, and thus may have felt that their interruptions were more legitimate.\n\nThe term, , was coined in early 2015 by Jessica Bennett in an article that appeared in \"Time.\" Bennett defines the term as \"[u]nnecessary interruption of a woman by a man.\" During the 2016 American presidential debates, the term was applied to candidate Donald Trump who interrupted Hillary Clinton dozens of times during the first and second debates. Since its introduction, the term has gained a significant presence on the internet as experts offer advice on how to avoid manterrupting and as feminist and postfeminist bloggers and commentators debate its applicability. In 2017, Woman Interrupted, an app to track the gender of conversational partners was introduced. The app, which is free, is designed to assist men and women in recognizing and addressing manterrupting.\n\nInterruptions work as a status-organizing cue. In other words, conversational participants use cues such as perceptions of prestige, power, social class, gender, race and age, to organize small group hierarchies. Interruption patterns differ by social status, with persons of higher social status, such as belonging to a social group who has more prestige or power, interrupting persons with lower status. Jacobi and Schweers analyzed transcripts of oral arguments made before the U.S. Supreme Court to find that senior justices interrupted their junior colleagues more frequently than the reverse. Kollock et al. studied conversations among couples, including male couples, female couples, and mixed sex couples. They found that partners who were considered to have more social power interrupted their partners more often, regardless of the gender composition of the dyad. In TV shows, characters who are lower in the status hierarchy are scripted to display a \"sense of defiance\" that allows them to interrupt more aggressively than persons who hold a mid-level status. A study of interviews between physicians and patients found that physicians, who are considered to hold higher status than their patients in terms of prestige, are much more likely to interrupt their patients, regardless of the sex of the patient or the physician. Patients interrupted senior physicians at a higher rate than they interrupted doctors who were in training, indicating that the senior physicians are regarded as having a higher status than the their junior colleagues. In contrast, a study of physician-patient interactions among six different statuses, from low to high, indicated that patients tended to interrupt physicians more than the reverse, and that high and low status physicians did not differ in the number of times that they interrupted their patients. This study, by Irish and Hall, noted that status thus appears to be less of an indicator of the likelihood of interruptions among physicians and patients.\n\nIn addition to social status affecting interruption patterns, interruptions also affect social status. In a study of mixed-sex and same-sex dyads, Farley discovered that the interrupters gained social status after they interrupted, while those who were interrupted lost social status. This study also found that people who interrupted also lost in terms of likeability.\n\nZimmerman and West note that whites interrupt blacks as a strategy to exert their power and dominance.\n\nInterruptions, and how people interpret interruptions, differ by culture and language. Makri-Tsilpakou notes that some languages and cultures have higher tolerance for simultaneous talk, and that interpretations of interruptions may differ depending on cultural context.\n\nPolitical orientation, e.g. where a person falls on the conservative to liberal political continuum, also shapes the likelihood that people will interrupt others or will be interrupted themselves. Jacobi and Schweers, in their study of transcripts of oral arguments made before the U.S. Supreme Court, found that conservative justices and advocates interrupt more often than liberals.\n"}
{"id": "14629", "url": "https://en.wikipedia.org/wiki?curid=14629", "title": "Inventor", "text": "Inventor\n\nAn inventor is a person who creates or discovers a new method, form, device or other useful means that becomes known as an invention. The word \"inventor\" comes from the Latin verb \"invenire\", \"invent-\", to find. The system of patents was established to encourage inventors by granting limited-term, limited monopoly on inventions determined to be sufficiently novel, non-obvious, and useful. Although inventing is closely associated with science and engineering, inventors are not necessarily engineers nor scientists.\n"}
{"id": "53966954", "url": "https://en.wikipedia.org/wiki?curid=53966954", "title": "Language deprivation in deaf and hard of hearing children", "text": "Language deprivation in deaf and hard of hearing children\n\nLanguage deprivation in deaf and hard of hearing children occurs when a child does not receive language exposure during their critical period. Language development can be severely delayed due to the lack of stimulation and socialization. This has been observed in such well known cases as Genie, Kaspar Hauser, Anna, and Isabelle, as well as cases of feral children such as Victor. (All of those children had typical hearing.) Similarly, language deprivation in deaf and hard of hearing children may occur when sufficient language exposure does not occur within the first few years of life, the critical period of language development. This is common because deaf children without hearing aids or cochlear implants cannot access the world around them through auditory means. These children often arrive at preschool or kindergarten with significant language delays that can greatly impact the rest of their education. Accommodations and specialized methods of instruction are required to meet the unique communication needs of deaf children, such as Auditory Verbal/Listening and Spoken Language therapy, cued speech, sign language, or a combination of approaches. Age of enrollment in early intervention services and strength of parental involvement are the strongest success indicators for language development in deaf children.\n\nMore than 90% of deaf and hard of hearing children are born to hearing parents, who may be unfamiliar with deafness and unsure about how to teach their children how to communicate. Many doctors and early intervention specialists have little experience with children with hearing loss and are not trained in best practices for their development. Audiologists, otolaryngologists, speech-language pathologists, teachers of the deaf, and sign-language interpreters are the specialists who are trained and qualified to work specifically with deaf and hard-of-hearing children and their families on language development and related issues.\n\nChildren with typical hearing in households where spoken language is the primary means of communication, or children with typical vision in signing households, are exposed to language at a young age. The critical period of language development occurs from birth and continues to age 5. During this period, the child develops language in structure such as syntax, social and brain development. Throughout a normal day, children with access to auditory signals are likely to continually receive input from TV, radio, surrounding conversations, and narration of events throughout their day. Similarly, children with typical vision in signing households are also exposed throughout the day to visual language. Through these avenues, children receive information about language structure, how we use language to interact with one another, and other cognitive and social cues. When a child is not exposed to language early, the child may develop language delays. This can be the case among deaf and hard of hearing children because this population can have limited access to language. Deaf children should be exposed to language early to avoid language delays.\n\nEarly intervention services that provide language interventions to deaf and hard of hearing children can result in those children achieving language milestones at the same rates as their hearing peers. Early access to language provides a foundation for developing and acquiring other languages regardless of modality. Deaf and hard of hearing children who attend early intervention programs have a higher fluency in language by age five.\n\nDeaf and hard of hearing children often arrive to kindergarten at age 5 already significantly behind their hearing peers (in vocabulary development, social interactions, and other cognitive processes). Due to hearing loss, some of these children cannot access language in the same way as their hearing peers. These approaches often assume that English is their first language (L1) and that these L1 foundations in English are already strong, having been set from birth to age 5. Educational placements designed for hearing students can prove to be unsuccessful for DHH students, especially if those students haven't been previously exposed to auditory stimuli. There are several types of educational placements for deaf and hard of hearing children, including: culturally deaf schools; mainstream general education classrooms; self-contained classrooms in public schools (a classroom that is designed for specifically for children with disabilities); Auditory Verbal, LSL, or OPTION schools for listening and spoken language; and co-enrollment classrooms that contain both deaf and hearing children who use sign language and spoken language to learn.\n\nThe Auditory Verbal/Listening and Spoken Language (LSL) approach is a communication option for most infants, toddlers, and young children with hearing loss. This approach is generally used by parents who want their deaf child to listen and talk in the primary language of the home. The main principles of AV/LSL promote early detection and diagnosis of hearing loss, use of hearing technology such as hearing aids and cochlear implants to help children access sounds and spoken language, and early intervention services to guide and coach parents and caregivers on how to teach a child with hearing loss to listen and talk. Eighty percent of deaf children who have early and high-quality AV/LSL interventions can learn to listen and speak with the same skill level and fluency as their hearing peers. Listening and Spoken Language Specialists must be certified in LSL in order to use the LSLS designation.\n\nThese include approaches that use foundations in American Sign Language (ASL) to develop proficient reading and writing skills in English. Bilingual-bicultural education can also address the needs of those students who may be able to access spoken language as well. When ASL functions as the child's first language, some believe it can support second language acquisition, although there is no specific evidence. Research done in bilingualism shows that if a child is fluent in a second language, it is due to proficiency in a first language. However, most bilingual research has been conducted on bilingual unimodal (speaking and hearing) communicators, not on bilingual bimodal (signing and speaking) communicators. There is evidence that bilingual bimodal communicators using sign language and spoken language do not enjoy the cognitive advantages of bilingualism that bilingual unimodals do.\n\nCued Speech is a visual communication system that uses eight handshapes in four different placements near the face, in combination with the mouth movements of speech, to make the sounds of spoken language look different from each other. It can be used in conjunction with any of 56 spoken languages or dialects.\n\nLEAD-K stands for Language Equality & Acquisition for Deaf Kids. It is a controversial effort in the United States intended to increase the number of deaf children who use American Sign Language. The LEAD-K initiative aims to \"end language deprivation\" by promoting American Sign Language, and by putting new laws in place to ensure that deaf and hard of hearing children begin learning both American Sign Language and English before kindergarten enrollment. The main goal of LEAD-K is to promote a foundation for American Sign Language within deaf and hard of hearing children. The organization has drafted state-level legislative bills which passed in California, Kansas, Hawaii, and Oregon in 2015-2016. Versions of the bill have been proposed in at least twelve states since 2016, but all have failed to pass.\n\n\n"}
{"id": "35928472", "url": "https://en.wikipedia.org/wiki?curid=35928472", "title": "Language policy in Ukraine", "text": "Language policy in Ukraine\n\nLanguage policy in Ukraine is based on its Constitution, international obligations, and from 2012 until February 2018 on the law \"On the principles of the state language policy\" (before 2012, the 1989 law \"On the languages in the Ukrainian SSR\" was in force).\n\nThe Ukrainian language is the state language of Ukraine. According to the article 10 of the Constitution of Ukraine, the State has to ensure the comprehensive development and functioning of the Ukrainian language in all spheres of social life throughout the entire territory of Ukraine.\nOther languages spoken in Ukraine are guaranteed constitutional protection. Russian is recognized as the language of a national minority.\n\nA 2012 law, called the law \"On the principles of the state language policy\" gave the status of regional language to Russian and other minority languages. It allowed the use of minority languages in courts, schools and other government institutions in areas of Ukraine where the national minorities exceed 10% of the population. The law was used mostly in Ukraine's southern and eastern regions, where predominant or significant parts of the population speak Russian as their first language. Three minor settlements did the same for Hungarian, Moldovan and Romanian. Ukrainian remained the only official country-wide language. Introduction of the law was supported by the governing Party of regions and opposed by the opposition parties. According to its opponents the law undermined and supplanted the role of the Ukrainian language, and violated Article 10 of the Ukrainian Constitution.\n\nThe bill was adopted amid fistfights in the Ukrainian Parliament building on 3 July 2012, and the opposition said that the procedure of adopting the law was not respected. The law came into force on 10 August 2012. Since then various cities and regions of Ukraine declared Russian a regional language in their jurisdictions. Other cities and regions declared their opposition to this law. Immediately after the 2014 Ukrainian revolution, on 23 February 2014, the Ukrainian Parliament voted to repeal the law. This decision was vetoed by the acting President Oleksandr Turchynov, who instead ordered drafting of a new law to \"accommodate the interests of both eastern and western Ukraine and of all ethnic groups and minorities.\" However, in October 2014 the Constitutional Court of Ukraine started reviewing the constitutionality of the law, and on 28 February 2018 it ruled the law unconstitutional.\n\nSince the fall of the Soviet Union and the independence of Ukraine, the Russian language has dwindled, but remains one of the two most used languages for business, legal proceedings, science, artistry, and many other spheres of everyday life. According to the 2001 census, 67.5% of the citizens of Ukraine regard Ukrainian as their native language, with Russian being considered the native language for another 29.6%. Various other languages constitute the remaining 2.9%. During the Soviet era, both Russian and Ukrainian had official status as state languages of the Ukrainian SSR.\n\nSupporters of the bill argued it would make life easier for Russian-speaking Ukrainians. Opponents fear adoption of Russian as a minority language could spread rapidly, challenging Ukrainian and causing splits between eastern and western Ukraine. In practice Russian is already used widely in official establishments in Ukraine.\n\nAccording to the article #27 (2nd part) it is necessary to translate Ukrainian place names into other languages using only Ukrainian transcription (the transcription of the state language).\n\nOn 9 February 2013 the authors of the 2012 language law, Serhiy Kivalov and Vadym Kolesnichenko, were awarded the \"Medal of Pushkin\" by Russian President Vladimir Putin for \"great contribution to the preservation and promotion of the Russian language and culture abroad\".\n\n\nV. Kolesnichenko, one of the authors of the law, refers to its support from various higher education bodies, scientists and NGOs.\n\nSome say that the bill contradicts the Constitution of Ukraine, violates the Budget Code, and aims to annihilate the Ukrainian language. It suffered a criticism in the conclusions of state authorities and their departments: the Main Scientific-Expert Bureau of the Ukrainian Parliament (23 May 2012), the Parliamentary Committee on Culture and Spirituality (September 23, 2011), the Parliamentary Committee on Budget (3 November 2011), Ministry of Finance (9 September 2011), the Ministry of Justice (27 September 2011). The bill also failed to obtain the support of the specialized institutions of the National Academy of Sciences of Ukraine: the Linguistics Institute, the Institute of the Ukrainian Language, the Institute of political and ethno-national researches, the Shevchenko Institute of Literature, the Institute of State and Law, the Ukrainian linguistic-informational Fund, the Philology Institute of Kiev University, and the Academy of Sciences of the High School of Ukraine.\n\nIn December 2011 the Venice Commission of the Council of Europe issued its Opinion on the draft law. According to Ukrayinska Pravda, the Venice Commission did not notice in the draft law of Kolesnichenko any guarantees of the protection of the Ukrainian language and later came to a decision that the bill is another \"pre-election tool\" for the Party of Regions. V. Kolesnichenko, one of the authors of the law, claimed the 2011 analysis of the Venice Commission was \"generally supportive\". The opponents noted that the analysis contained strong criticism, specifically about the failure to protect the role of Ukrainian as the State language.\n\nIn its Opinion, the Venice Commission stated, among other theses:\n\n\"It seems questionable to the Venice Commission that the parallel use of the State language and regional and minority languages, and in practice mostly the Russian language in large spheres of public life and not only on a local level, can still be considered to be in compliance with article 10 of the Constitution, as clarified by the Constitutional Court.\"\n\n\"the present draft does no longer formally focus on the Russian language, as the references to this language are almost always replaced by a reference to \"the regional or minority language\". This equalization of the treatment of the Russian language to the treatment of the regional or minority languages appears to be beneficial, in certain areas of public life, to other regional or minority languages\" (Para. 64)\n\n\"…the question remains whether... there are sufficient guarantees, in the current Draft Law, for the consolidation of the Ukrainian language as the sole State language, and of the role it has to play in the Ukrainian multilinguistic society. The Venice Commission can only reiterate its call... for a fair balance between the protection of the rights of minorities, on the one hand, and the preservation of the State language as a tool for integration within society, on the other hand.\" (Para. 66)\n\n\"The recognition of linguistic freedom in the media and in the cultural area could moreover, due to market considerations, result in the dominance of the Russian language.\"\n\nThe Opinion also made other observations and proposals of change. It was of the view that \"further improvements, increased guarantees and more substantial changes to the normative content of the Draft should be introduced...\"\n\nPrior to 24 May 2012 there were rumours that a revision of the legislation on languages would take place in parliament (the Verkhovna Rada) and that the Secretary of National Security and Defense would attend the session. Some 1,000 protesters gathered just outside the Verkhovna Rada building setting up another tent city. State law enforcement warned the protesters not to establish a tent city. The protesters were yelling in Ukrainian \"Get busy with work, not a tongue\" (implying the law draft on languages). Some posters carried the slogan: \"The problem is in poverty, not in language\".\n\nAt the evening session the parliamentary opposition in the Verkhovna Rada (BYuT and Our Ukraine) blocked the main tribune in parliament as some representatives from the Party of Regions surrounded the presidium. The speaker was forced to announce a break in the session. After the break Member of Parliament Vyacheslav Kyrylenko read a statement of the united opposition not to conduct any hearings regarding language issues. After the law draft #10154 \"On the state language of Ukraine\" was not adopted onto the daily agenda, Kyrylenko withdrew his draft #9059 \"Prohibition of narrowing the sphere of use of Ukrainian language\" from a revision, while Kolesnichenko gave a presentation on his draft #9073. The head of the Committee On Issues of Culture and Spirituality Volodymyr Yavorivsky disclosed the decision of the committee to reject the bill #9073 as it was the decision of the committee's majority. He pointed to the fact that the law draft in fact will introduce a bilingual situation in number of regions. However, after a review the bill was supported by the parliamentary majority which showed its support in adopting two state languages: Ukrainian and Russian. The parliamentary minority and the deputy group \"Reforms for the Future\" stayed in opposition to the bill. Parliament speaker Volodymyr Lytvyn was forced to hastily close the session as further discussion descended into another scandalous fight leaving some members of parliament injured.\n\nThe Party of Regions released a statement to the press where it accused the opposition of denying the adaptation of a bill that protects some constitutional rights of millions of citizens of Ukraine. PoR leader in parliament Yefremov promised to revisit the issue once everything is stable.\n\nThe bill was to come into force only after it was signed by Ukrainian President Viktor Yanukovych and the Chairman of Parliament. But the Chairman of Parliament Volodymyr Lytvyn tendered his resignation on 4 July 2012. However, the Verkhovna Rada twice held votes of confidence in the speaker, and did not accept his resignation. On 31 July Lytvyn signed the law. The bill was signed by President Yanukovych on 8 August 2012. The law came into force on 10 August 2012. Since then various Ukrainian cities and regions have declared Russian a regional language in their jurisdictions, these being the municipalities of Odessa, Kharkiv, Kherson, Mykolaiv, Zaporizhia, Sevastopol, Dnipropetrovsk, Luhansk and Krasny Luch; and the Oblasts of Odessa, Zaporizhia, Donetsk, Kherson, Mykolaiv and Dnipropetrovsk. Hungarian has been made a regional language in the town of Berehove in Zakarpattia Oblast, Moldovan in the village of Tarasivtsi (Chernivtsi Oblast), and Romanian in the village of Bila Tserkva; also in Zakarpattia Oblast. These languages will now be used in city/Oblast administrative office work and documents. As of September 2012 there were no plans for such bilingualism in Kiev. Chairmen of the Supreme Council of Crimea Volodomyr Konstantinov stated in March 2013 that the August 2012 law had changed nothing in Crimea.\n\nOn February 23, 2014, the second day after the flight of Viktor Yanukovich, while in a parliamentary session, a deputy from the \"Batkivshchina\" party, Vyacheslav Kyrylenko, moved to include in the agenda a bill to repeal the 2012 law \"On the principles of the state language policy\". The motion was carried with 86% of the votes in favour—232 deputies in favour vs 37 opposed against the required minimum of 226 of 334 votes. The bill was included in the agenda, immediately put to a vote with no debate and approved with the same 232 voting in favour. The bill would have made Ukrainian the sole state language at all levels. Still, all the minority languages (including Russian) remain explicitly protected under article 10 of the Ukrainian Constitution. The repeal would also bring back into force the previous law on languages, which was in place in Ukraine for 23 years before July 2012 and was regulating the use of the minority languages. According to Uilleam Blacker, the repeal bill contained no specific threat to the Russian language.\n\nHowever the move to repeal the 2012 law \"On the principles of the state language policy\" provoked negative reactions in Crimea and in some regions of Southern and Eastern Ukraine. It became one of the topics of the protests against the new government approved by the parliament after the flight of Viktor Yanukovich. In this context, the next major development was the Crimean crisis.\n\nPassage of the repeal bill was met with regret by the Secretary-General of the Council of Europe. The OSCE High Commissioner on National Minorities expressed concern over possible further unrest. He also proposed to give advice and facilitate discussions on new legislation, declaring that \"we must avoid the mistakes made last time [in 2012] when unbalanced legislation was adopted without a proper dialogue in the Verkhovna Rada.\" The bill was also criticized by the Ambassador for Human Rights of the Russian foreign ministry. Bulgarian and Romanian foreign ministers evaluated it as a step in the wrong direction, and the Greek foreign minister expressed disappointment. The Hungarian foreign ministry expressed serious concerns, noting that the decision \"could question the commitment of the new Ukrainian administration towards democracy\". The Polish foreign minister called it a mistake.\n\nAfter urgently ordering a working group to draft a replacement law on February 27, acting President Oleksandr Turchynov declared on 3 March that he will not be signing the repeal bill until a replacement law is adopted. Since then the repeal bill is not signed, but not vetoed by President, its current status is \"ready for sign\".\n\nOn 7 April 2014 former BYuT leader Yulia Tymoshenko stated she supported the 2012 language law.\n\nOn 3 November 2014, newly elected president Petro Poroshenko declared that the language policy in Ukraine will be amended.\n\nOn 10 July 2014 57 parliamentary deputies appealed the Constitutional Court of Ukraine to review the 2012 law \"On the principles of the state language policy\". On 10 October 2014 the court opened the proceedings on the constitutionality of the law. On 14 December 2016 the Constitutional Court ended the oral proceedings and on 13 January 2017 moved to the closed part of the process. On 28 February 2018 the Constitutional Court of Ukraine ruled the law unconstitutional.\n\n"}
{"id": "13503257", "url": "https://en.wikipedia.org/wiki?curid=13503257", "title": "Language survey", "text": "Language survey\n\nA language survey is conducted around the world for a variety of reasons.\n\n\nMethods used in language surveys depend on the questions that the survey is trying to answer. Methods used include collecting word lists (Bender 1971), playing recorded texts to assess comprehension (Casad 1974), sentence repetition tests (Radloff 1991), questionnaires (Hochstetler and Tillinghast 1996), group and individual interviews, retelling of stories (McKinnies and Priestly 2004), direct observation (Cooper and Carpenter 1976), and even internet surveys (tafesilafai.org).\n\nAs with any form of research, the methods used depend on the questions that the researchers are trying to answer. Also, the reliability of the results varies according to the method and the rigor with which it is applied, proper sampling technique, etc.\n\nThe results of language surveys are use for a variety of purposes. One of the most common is in making decisions for implementing educational programs. The results have also been used for making decision for language development work (Holbrook, 2001). And of course, academics are always interested in the results of any language survey.\n\nSurveys have also been conducted by ethnic associations (Saskatchewan 1991), government agencies (Statistics Canada 1993), NGO's (Toba, et al. 2002), foundations (Pew Hispanic Center 2004), etc. Often such groups work together (Clifton 2002). Some large and notable surveys include the Language Survey of India which was begun by George Abraham Grierson late in the 19th century (Sociolinguistics research in India) and the Survey of Language Use and Language Teaching in East Africa, sponsored by the Ford Foundation from the 1960s. Both resulted in a number of volumes describing locations of languages, patterns of multilingualism, language classification, and also included descriptions of languages, such as \"Language in Ethiopia\" (Bender, Bowen, Cooper, and Ferguson 1976). The single agency conducting the most language surveys around the world is SIL International (Summer Institute of Linguistics). The results of many of their surveys are posted on the web: http://www.sil.org/silesr.\n\nSurveys have usually been conducted among spoken languages. However, surveys have also been done among users of sign languages (Vasishta, Woodward, and Wilson 1978, Woodward 1991, 1993, 1996, Parkhurst & Parkhurst 1998, Al-Fityani & Padden 2008). As with surveys among spoken languages, surveys among sign languages have studied multilingualism, attitudes about various languages both spoken and signed (Ciupek-Reed 2012), differences and similarities between signed varieties (Aldersson and McEntee-Atalianis 2007, Bickford 1991, 2005, Parks 2011), and assessing the vitality of signed languages, and initial descriptions of undocumented sign languages.\n\n\n\n"}
{"id": "33944980", "url": "https://en.wikipedia.org/wiki?curid=33944980", "title": "Languages of Seychelles", "text": "Languages of Seychelles\n\nThe national languages of Seychelles are Seychellois Creole, English and French.\n\nSeychellois Creole, a French-based creole language, is by far the most commonly spoken language in the archipelago and is spoken natively by about 95% of the population. Nevertheless, the country was a British colony for over a century and a half, and the legacy of British Seychelles) made English remain the main language in government and business.\n\nFrench was introduced before the British rule. It has remained in use largely because it is used by the Franco-Seychellois minority and is similar to Seychellois Creole.\n\n\n"}
{"id": "18077", "url": "https://en.wikipedia.org/wiki?curid=18077", "title": "Lexicon", "text": "Lexicon\n\nA lexicon, word-hoard, wordbook, or word-stock is the vocabulary of a person, language, or branch of knowledge (such as nautical or medical). In linguistics, a lexicon is a language's inventory of lexemes. The word \"lexicon\" derives from the Greek (\"lexicon\"), neuter of (\"lexikos\") meaning \"of or for words.\"\n\nLinguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalogue of a language's words (its wordstock); and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. The lexicon is also thought to include bound morphemes, which cannot stand alone as words (such as most affixes). In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included.\n\nItems in the lexicon are called lexemes, or lexical items, or word forms. Lexemes are not atomic elements but contain both phonological and morphological components. When describing the lexicon, a reductionist approach is used, trying to remain general while using a minimal description. To describe the size of a lexicon, lexemes are grouped into lemmas. A lemma is a group of lexemes generated by inflectional morphology. Lemmas are represented in dictionaries by headwords which list the citation forms and any irregular forms, since these must be learned to use the words correctly. Lexemes derived from a word by derivational morphology are considered new lemmas. The lexicon is also organized according to open and closed categories. Closed categories, such as determiners or pronouns, are rarely given new lexemes; their function is primarily syntactic. Open categories, such as nouns and verbs, have highly active generation mechanisms and their lexemes are more semantic in nature.\n\nA central role of the lexicon is the documenting of established \"lexical norms and conventions\". Lexicalization is the process in which new words, having gained widespread usage, enter the lexicon. Since lexicalization may modify lexemes phonologically and morphologically, it is possible that a single etymological source may be inserted into a single lexicon in two or more forms. These pairs, called a doublet, are often close semantically. Two examples are \"aptitude\" versus \"attitude\" and \"employ\" versus \"imply\".\n\nThe mechanisms, not mutually exclusive, are:\n\nNeologisms are new lexeme candidates which, if they gain wide usage over time, become part of a language's lexicon. Neologisms are often introduced by children who produce erroneous forms by mistake. Another common source is slang and activities such as advertising and branding.\n\nThere are two types of borrowings (neologisms based on external sources) that retain the sound of the source language material:\n\nThe following are examples of external lexical expansion using the source language lexical item as the basic material for the neologization, listed in decreasing order of phonetic resemblance to the original lexical item (in the source language):\n\nThe following are examples of simultaneous external and internal lexical expansion using target language lexical items as the basic material for the neologization but still resembling the sound of the lexical item in the source language:\n\nAnother mechanism involves generative devices that combine morphemes according to a language's rules. For example, the suffix \"-able\" is usually only added to transitive verbs, as in \"readable\" but not \"cryable\".\n\nA compound word is a lexeme composed of several established lexemes, whose semantics is not the sum of that of their constituents. They can be interpreted through analogy, common sense and, most commonly, context. Compound words can have simple or complex morphological structures. Usually only the head requires inflection for agreement. Compounding may result in lexemes of unwieldy proportion. This is compensated by mechanisms that reduce the length of words. A similar phenomenon has been recently shown to feature in social media also where hashtags compound to form longer-sized hashtags that are at times more popular than the individual constituent hashtags forming the compound. Compounding is the most common of word formation strategies cross-linguistically. \n\nComparative historical linguistics studies the evolutions languages and takes a diachronic view of the lexicon. The evolution of lexicons in different languages occurs through parallel mechanism. Over time historical forces work to shape the lexicon, making it simpler to acquire and often creating an illusion of great regularity in language.\n\nThe term \"lexicon\" is generally used in the context of single language. Therefore, multi-lingual speakers are generally thought to have multiple lexicons. Speakers of language variants (Brazilian Portuguese and European Portuguese, for example) may be considered to possess a single lexicon. Thus a \"cash dispenser\" (British English) as well as an automatic teller machine or ATM in American English would be understood by both American and British speakers, despite each group using different dialects.\n\nWhen linguists study a lexicon, they consider such things as what constitutes a word; the word/concept relationship; lexical access and lexical access failure; how a word's phonology, syntax, and meaning intersect; the morphology-word relationship; vocabulary structure within a given language; language use (pragmatics); language acquisition; the history and evolution of words (etymology); and the relationships between words, often studied within philosophy of language.\n\nVarious models of how lexicons are organized and how words are retrieved have been proposed in psycholinguistics, neurolinguistics and computational linguistics.\n\n\n"}
{"id": "22760983", "url": "https://en.wikipedia.org/wiki?curid=22760983", "title": "Linguistics", "text": "Linguistics\n\nLinguistics is the scientific study of language, and it involves an analysis of language form, language meaning, and language in context. The earliest activities in the documentation and description of language have been attributed to the 6th century BC Indian grammarian Pāṇini who wrote a formal description of the Sanskrit language in his \"\".\n\nLinguists traditionally analyse human language by observing an interplay between sound and meaning. Phonetics is the study of speech and non-speech sounds, and delves into their acoustic and articulatory properties. The study of language meaning, on the other hand, deals with how languages encode relations between entities, properties, and other aspects of the world to convey, process, and assign meaning, as well as manage and resolve ambiguity. While the study of semantics typically concerns itself with truth conditions, pragmatics deals with how situational context influences the production of meaning.\n\nGrammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organisation of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Modern theories that deal with the principles of grammar are largely based within Noam Chomsky's framework of generative linguistics.\n\nIn the early 20th century, Ferdinand de Saussure distinguished between the notions of \"langue\" and \"parole\" in his formulation of structural linguistics. According to him, \"parole\" is the specific utterance of speech, whereas langue refers to an abstract phenomenon that theoretically defines the principles and system of rules that govern a language. This distinction resembles the one made by Noam Chomsky between competence and performance in his theory of transformative or generative grammar. According to Chomsky, competence is an individual's innate capacity and potential for language (like in Saussure's \"langue\"), while performance is the specific way in which it is used by individuals, groups, and communities (i.e., \"parole\", in Saussurean terms).\n\nThe study of \"parole\" (which manifests through cultural discourses and dialects) is the domain of sociolinguistics, the sub-discipline that comprises the study of a complex system of linguistic facets within a certain speech community (governed by its own set of grammatical rules and laws). Discourse analysis further examines the structure of texts and conversations emerging out of a speech community's usage of language. This is done through the collection of linguistic data, or through the formal discipline of corpus linguistics, which takes naturally occurring texts and studies the variation of grammatical and other features based on such corpora (or corpus data).\n\nStylistics also involves the study of written, signed, or spoken discourse through varying speech communities, genres, and editorial or narrative formats in the mass media. In the 1960s, Jacques Derrida, for instance, further distinguished between speech and writing, by proposing that written language be studied as a linguistic medium of communication in itself. Palaeography is therefore the discipline that studies the evolution of written scripts (as signs and symbols) in language. The formal study of language also led to the growth of fields like psycholinguistics, which explores the representation and function of language in the mind; neurolinguistics, which studies language processing in the brain; biolinguistics, which studies the biology and evolution of language; and language acquisition, which investigates how children and adults acquire the knowledge of one or more languages.\n\nLinguistics also deals with the social, cultural, historical and political factors that influence language, through which linguistic and language-based context is often determined. Research on language through the sub-branches of historical and evolutionary linguistics also focus on how languages change and grow, particularly over an extended period of time.\n\nLanguage documentation combines anthropological inquiry (into the history and culture of language) with linguistic inquiry, in order to describe languages and their grammars. Lexicography involves the documentation of words that form a vocabulary. Such a documentation of a linguistic vocabulary from a particular language is usually compiled in a dictionary. Computational linguistics is concerned with the statistical or rule-based modeling of natural language from a computational perspective. Specific knowledge of language is applied by speakers during the act of translation and interpretation, as well as in language education – the teaching of a second or foreign language. Policy makers work with governments to implement new plans in education and teaching which are based on linguistic research.\n\nRelated areas of study also includes the disciplines of semiotics (the study of direct and indirect language through signs and symbols), literary criticism (the historical and ideological analysis of literature, cinema, art, or published material), translation (the conversion and documentation of meaning in written/spoken text from one language or dialect onto another), and speech-language pathology (a corrective method to cure phonetic disabilities and dis-functions at the cognitive level).\n\nBefore the 20th century, the term \"philology\", first attested in 1716, was commonly used to refer to the study of language, which was then predominantly historical in focus. Since Ferdinand de Saussure's insistence on the importance of synchronic analysis, however, this focus has shifted and the term \"philology\" is now generally used for the \"study of a language's grammar, history, and literary tradition\", especially in the United States (where philology has never been very popularly considered as the \"science of language\").\n\nAlthough the term \"linguist\" in the sense of \"a student of language\" dates from 1641, the term \"linguistics\" is first attested in 1847. It is now the usual term in English for the scientific study of language, though \"linguistic science\" is sometimes used.\n\nLinguistics is a multi-disciplinary field of research that combines tools from natural sciences, social sciences, and the humanities. Many linguists, such as David Crystal, conceptualize the field as being primarily scientific. The term \"linguist\" applies to someone who studies language or is a researcher within the field, or to someone who uses the tools of the discipline to describe and analyse specific languages.\n\nWhile some theories on linguistics focus on the different varieties that language produces, among different sections of society, others focus on the universal properties that are common to all human languages. The theory of variation therefore would elaborate on the different usages of popular languages like French and English across the globe, as well as its smaller dialects and regional permutations within their national boundaries. The theory of variation looks at the cultural stages that a particular language undergoes, and these include the following.\n\nThe pidgin stage in a language is a stage when communication occurs through a grammatically simplified means, developing between two or more groups that do not have a language in common. Typically, it is a mixture of languages at the stage when there occurs a mixing between a primary language with other language elements.\n\nA creole stage in language occurs when there is a stable natural language developed from a mixture of different languages. It is a stage that occurs after a language undergoes its pidgin stage. At the creole stage, a language is a complete language, used in a community and acquired by children as their native language.\n\nA dialect is a variety of language that is characteristic of a particular group among the language speakers. The group of people who are the speakers of a dialect are usually bound to each other by social identity. This is what differentiates a dialect from a register or a discourse, where in the latter case, cultural identity does not always play a role. Dialects are speech varieties that have their own grammatical and phonological rules, linguistic features, and stylistic aspects, but have not been given an official status as a language. Dialects often move on to gain the status of a language due to political and social reasons. Differentiation amongst dialects (and subsequently, languages too) is based upon the use of grammatical rules, syntactic rules, and stylistic features, though not always on lexical use or vocabulary. The popular saying that \"a language is a dialect with an army and navy\" is attributed as a definition formulated by Max Weinreich.\n\nUniversal grammar takes into account general formal structures and features that are common to all dialects and languages, and the template of which pre-exists in the mind of an infant child. This idea is based on the theory of generative grammar and the formal school of linguistics, whose proponents include Noam Chomsky and those who follow his theory and work.\n\n\"We may as individuals be rather fond of our own dialect. This should not make us think, though, that it is actually any better than any other dialect. Dialects are not good or bad, nice or nasty, right or wrong – they are just different from one another, and it is the mark of a civilised society that it tolerates different dialects just as it tolerates different races, religions and sexes.\"\n\nDiscourse is language as social practice (Baynham, 1995) and is a multilayered concept. As a social practice, discourse embodies different ideologies through written and spoken texts. Discourse analysis can examine or expose these ideologies. Discourse influences genre, which is chosen in response to different situations and finally, at micro level, discourse influences language as text (spoken or written) at the phonological or lexico-grammatical level. Grammar and discourse are linked as parts of a system. A particular discourse becomes a language variety when it is used in this way for a particular purpose, and is referred to as a register. There may be certain lexical additions (new words) that are brought into play because of the expertise of the community of people within a certain domain of specialization. Registers and discourses therefore differentiate themselves through the use of vocabulary, and at times through the use of style too. People in the medical fraternity, for example, may use some medical terminology in their communication that is specialized to the field of medicine. This is often referred to as being part of the \"medical discourse\", and so on.\n\nWhen a dialect is documented sufficiently through the linguistic description of its grammar, which has emerged through the consensual laws from within its community, it gains political and national recognition through a country or region's policies. That is the stage when a language is considered a standard variety, one whose grammatical laws have now stabilised from within the consent of speech community participants, after sufficient evolution, improvisation, correction, and growth. The English language, besides perhaps the French language, may be examples of languages that have arrived at a stage where they are said to have become standard varieties.\n\nThe study of a language's universal properties, on the other hand, include some of the following concepts.\n\nThe lexicon is a catalogue of words and terms that are stored in a speaker's mind. The lexicon consists of words and bound morphemes, which are parts of words that can't stand alone, like affixes. In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included. Lexicography, closely linked with the domain of semantics, is the science of mapping the words into an encyclopedia or a dictionary. The creation and addition of new words (into the lexicon) is called coining or neologization, and the new words are called neologisms.\n\nIt is often believed that a speaker's capacity for language lies in the quantity of words stored in the lexicon. However, this is often considered a myth by linguists. The capacity for the use of language is considered by many linguists to lie primarily in the domain of grammar, and to be linked with competence, rather than with the growth of vocabulary. Even a very small lexicon is theoretically capable of producing an infinite number of sentences.\n\nAs constructed popularly through the Sapir–Whorf hypothesis, relativists believe that the structure of a particular language is capable of influencing the cognitive patterns through which a person shapes his or her world view. Universalists believe that there are commonalities between human perception as there is in the human capacity for language, while relativists believe that this varies from language to language and person to person. While the Sapir–Whorf hypothesis is an elaboration of this idea expressed through the writings of American linguists Edward Sapir and Benjamin Lee Whorf, it was Sapir's student Harry Hoijer who termed it thus. The 20th century German linguist Leo Weisgerber also wrote extensively about the theory of relativity. Relativists argue for the case of differentiation at the level of cognition and in semantic domains. The emergence of cognitive linguistics in the 1980s also revived an interest in linguistic relativity. Thinkers like George Lakoff have argued that language reflects different cultural metaphors, while the French philosopher of language Jacques Derrida's writings have been seen to be closely associated with the relativist movement in linguistics, especially through deconstruction and was even heavily criticized in the media at the time of his death for his theory of relativism.\n\nLinguistic structures are pairings of meaning and form. Any particular pairing of meaning and form is a Saussurean sign. For instance, the meaning \"cat\" is represented worldwide with a wide variety of different sound patterns (in oral languages), movements of the hands and face (in sign languages), and written symbols (in written languages). Linguistic patterns have proven their importance for the knowledge engineering field especially with the ever-increasing amount of available data.\n\nLinguists focusing on structure attempt to understand the rules regarding language use that native speakers know (not always consciously). All linguistic structures can be broken down into component parts that are combined according to (sub)conscious rules, over multiple levels of analysis. For instance, consider the structure of the word \"tenth\" on two different levels of analysis. On the level of internal word structure (known as morphology), the word \"tenth\" is made up of one linguistic form indicating a number and another form indicating ordinality. The rule governing the combination of these forms ensures that the ordinality marker \"th\" follows the number \"ten.\" On the level of sound structure (known as phonology), structural analysis shows that the \"n\" sound in \"tenth\" is made differently from the \"n\" sound in \"ten\" spoken alone. Although most speakers of English are consciously aware of the rules governing internal structure of the word pieces of \"tenth\", they are less often aware of the rule governing its sound structure. Linguists focused on structure find and analyze rules such as these, which govern how native speakers use language.\n\nLinguistics has many sub-fields concerned with particular aspects of linguistic structure. The theory that elucidates on these, as propounded by Noam Chomsky, is known as generative theory or universal grammar. These sub-fields range from those focused primarily on form to those focused primarily on meaning. They also run the gamut of level of analysis of language, from individual sounds, to words, to phrases, up to cultural discourse.\n\nSub-fields that focus on a grammatical study of language include the following.\n\n\nStylistics is the study and interpretation of texts for aspects of their linguistic and tonal style. Stylistic analysis entails the analysis of description of particular dialects and registers used by speech communities. Stylistic features include rhetoric, diction, stress, satire, irony, dialogue, and other forms of phonetic variations. Stylistic analysis can also include the study of language in canonical works of literature, popular fiction, news, advertisements, and other forms of communication in popular culture as well. It is usually seen as a variation in communication that changes from speaker to speaker and community to community. In short, Stylistics is the interpretation of text.\n\nOne major debate in linguistics concerns the very nature of language and how it should be understood. Some linguists hypothesize that there is a module in the human brain that allows people to undertake linguistic behaviour, which is part of the formalist approach. This \"universal grammar\" is considered to guide children when they learn language and to constrain what sentences are considered grammatical in any human language. Proponents of this view, which is predominant in those schools of linguistics that are based on the generative theory of Noam Chomsky, do not necessarily consider that language evolved for communication in particular. They consider instead that it has more to do with the process of structuring human thought (see also formal grammar).\n\nAnother group of linguists, by contrast, use the term \"language\" to refer to a communication system that developed to support cooperative activity and extend cooperative networks. Such theories of grammar, called \"functional\", view language as a tool that emerged and is adapted to the communicative needs of its users, and the role of cultural evolutionary processes are often emphasized over that of biological evolution.\n\nLinguistics is primarily descriptive. Linguists describe and explain features of language without making subjective judgments on whether a particular feature or usage is \"good\" or \"bad\". This is analogous to practice in other sciences: a zoologist studies the animal kingdom without making subjective judgments on whether a particular species is \"better\" or \"worse\" than another.\n\nPrescription, on the other hand, is an attempt to promote particular linguistic usages over others, often favouring a particular dialect or \"acrolect\". This may have the aim of establishing a linguistic standard, which can aid communication over large geographical areas. It may also, however, be an attempt by speakers of one language or dialect to exert influence over speakers of other languages or dialects (see Linguistic imperialism). An extreme version of prescriptivism can be found among censors, who attempt to eradicate words and structures that they consider to be destructive to society. Prescription, however, may be practised appropriately in the teaching of language, like in ELT, where certain fundamental grammatical rules and lexical terms need to be introduced to a second-language speaker who is attempting to acquire the language.\n\nThe objective of describing languages is often to uncover cultural knowledge about communities. The use of anthropological methods of investigation on linguistic sources leads to the discovery of certain cultural traits among a speech community through its linguistic features. It is also widely used as a tool in language documentation, with an endeavour to curate endangered languages. However, now, linguistic inquiry uses the anthropological method to understand cognitive, historical, sociolinguistic and historical processes that languages undergo as they change and evolve, as well as general anthropological inquiry uses the linguistic method to excavate into culture. In all aspects, anthropological inquiry usually uncovers the different variations and relativities that underlie the usage of language.\n\nMost contemporary linguists work under the assumption that spoken data and signed data are more fundamental than written data. This is because\n\nNonetheless, linguists agree that the study of written language can be worthwhile and valuable. For research that relies on corpus linguistics and computational linguistics, written language is often much more convenient for processing large amounts of linguistic data. Large corpora of spoken language are difficult to create and hard to find, and are typically transcribed and written. In addition, linguists have turned to text-based discourse occurring in various formats of computer-mediated communication as a viable site for linguistic inquiry.\n\nThe study of writing systems themselves, graphemics, is, in any case, considered a branch of linguistics.\n\nBefore the 20th century, linguists analysed language on a diachronic plane, which was historical in focus. This meant that they would compare linguistic features and try to analyse language from the point of view of how it had changed between then and later. However, with Saussurean linguistics in the 20th century, the focus shifted to a more synchronic approach, where the study was more geared towards analysis and comparison between different language variations, which existed at the same given point of time.\n\nAt another level, the syntagmatic plane of linguistic analysis entails the comparison between the way words are sequenced, within the syntax of a sentence. For example, the article \"the\" is followed by a noun, because of the syntagmatic relation between the words. The paradigmatic plane on the other hand, focuses on an analysis that is based on the paradigms or concepts that are embedded in a given text. In this case, words of the same type or class may be replaced in the text with each other to achieve the same conceptual understanding.\n\nThe formal study of language began in India with Pāṇini, the 6th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. Pāṇini's systematic classification of the sounds of Sanskrit into consonants and vowels, and word classes, such as nouns and verbs, was the first known instance of its kind. In the Middle East, Sibawayh, a non-Arab, made a detailed description of Arabic in AD 760 in his monumental work, \"Al-kitab fi al-nahw\" (, \"The Book on Grammar\"), the first known author to distinguish between sounds and phonemes (sounds as units of a linguistic system). Western interest in the study of languages began somewhat later than in the East, but the grammarians of the classical languages did not use the same methods or reach the same conclusions as their contemporaries in the Indic world. Early interest in language in the West was a part of philosophy, not of grammatical description. The first insights into semantic theory were made by Plato in his \"Cratylus\" dialogue, where he argues that words denote concepts that are eternal and exist in the world of ideas. This work is the first to use the word etymology to describe the history of a word's meaning. Around 280 BC, one of Alexander the Great's successors founded a university (see Musaeum) in Alexandria, where a school of philologists studied the ancient texts in and taught Greek to speakers of other languages. While this school was the first to use the word \"grammar\" in its modern sense, Plato had used the word in its original meaning as \"téchnē grammatikḗ\" (), the \"art of writing\", which is also the title of one of the most important works of the Alexandrine school by Dionysius Thrax. Throughout the Middle Ages, the study of language was subsumed under the topic of philology, the study of ancient languages and texts, practised by such educators as Roger Ascham, Wolfgang Ratke, and John Amos Comenius.\n\nIn the 18th century, the first use of the comparative method by William Jones sparked the rise of comparative linguistics. Bloomfield attributes \"the first great scientific linguistic work of the world\" to Jacob Grimm, who wrote \"Deutsche Grammatik\". It was soon followed by other authors writing similar comparative studies on other language groups of Europe. The study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt, of whom Bloomfield asserts:\n\nThis study received its foundation at the hands of the Prussian statesman and scholar Wilhelm von Humboldt (1767–1835), especially in the first volume of his work on Kavi, the literary language of Java, entitled \"Über die Verschiedenheit des menschlichen Sprachbaues und ihren Einfluß auf die geistige Entwickelung des Menschengeschlechts\" (\"On the Variety of the Structure of Human Language and its Influence upon the Mental Development of the Human Race\").\n\nEarly in the 20th century, Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them. By introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still foundational in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the langue- parole distinction, distinguishing language as an abstract system (\"langue\") from language as a concrete manifestation of this system (\"parole\"). Substantial additional contributions following Saussure's definition of a structural approach to language came from The Prague school, Leonard Bloomfield, Charles F. Hockett, Louis Hjelmslev, Émile Benveniste and Roman Jakobson.\n\nDuring the last half of the 20th century, following the work of Noam Chomsky, linguistics was dominated by the generativist school. While formulated by Chomsky in part as a way to explain how human beings acquire language and the biological constraints on this acquisition, in practice it has largely been concerned with giving formal accounts of specific phenomena in natural languages. Generative theory is modularist and formalist in character. Chomsky built on earlier work of Zellig Harris to formulate the generative theory of language. According to this theory the most basic form of language is a set of syntactic rules universal for all humans and underlying the grammars of all human languages. This set of rules is called Universal Grammar, and for Chomsky describing it is the primary objective of the discipline of linguistics. For this reason the grammars of individual languages are of importance to linguistics only in so far as they allow us to discern the universal underlying rules from which the observable linguistic variability is generated.\n\nIn the classic formalization of generative grammars first proposed by Noam Chomsky in the 1950s, a grammar \"G\" consists of the following components:\n\nA formal description of language attempts to replicate a speaker's knowledge of the rules of their language, and the aim is to produce a set of rules that is minimally sufficient to successfully model valid linguistic forms.\n\nFunctional theories of language propose that since language is fundamentally a tool, it is reasonable to assume that its structures are best analysed and understood with reference to the functions they carry out. Functional theories of grammar differ from formal theories of grammar, in that the latter seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, whereas the former defines the functions performed by language and then relates these functions to the linguistic elements that carry them out. This means that functional theories of grammar tend to pay attention to the way language is actually used, and not just to the formal relations between linguistic elements.\n\nFunctional theories describe language in term of the functions existing at all levels of language.\n\nCognitive linguistics emerged as a reaction to generativist theory in the 1970s and 1980s. Led by theorists like Ronald Langacker and George Lakoff, cognitive linguists propose that language is an emergent property of basic, general-purpose cognitive processes. In contrast to the generativist school of linguistics, cognitive linguistics is non-modularist and functionalist in character. Important developments in cognitive linguistics include cognitive grammar, frame semantics, and conceptual metaphor, all of which are based on the idea that form–function correspondences based on representations derived from embodied experience constitute the basic units of language.\n\nCognitive linguistics interprets language in terms of concepts (sometimes universal, sometimes specific to a particular tongue) that underlie its form. It is thus closely associated with semantics but is distinct from psycholinguistics, which draws upon empirical findings from cognitive psychology in order to explain the mental processes that underlie the acquisition, storage, production and understanding of speech and writing. Unlike generative theory, cognitive linguistics denies that there is an \"autonomous linguistic faculty\" in the mind; it understands grammar in terms of \"conceptualization\"; and claims that knowledge of language arises out of \"language use\". Because of its conviction that knowledge of language is learned through use, cognitive linguistics is sometimes considered to be a functional approach, but it differs from other functional approaches in that it is primarily concerned with how the mind creates meaning through language, and not with the use of language as a tool of communication.\n\nHistorical linguists study the history of specific languages as well as general characteristics of language change. The study of language change is also referred to as \"diachronic linguistics\" (the study of how one particular language has changed over time), which can be distinguished from \"synchronic linguistics\" (the comparative study of more than one language at a given moment in time without regard to previous stages). Historical linguistics was among the first sub-disciplines to emerge in linguistics, and was the most widely practised form of linguistics in the late 19th century. However, there was a shift to the synchronic approach in the early twentieth century with Saussure, and became more predominant in western linguistics with the work of Noam Chomsky.\n\nEcolinguistics explores the role of language in the life-sustaining interactions of humans, other species and the physical environment. The first aim is to develop linguistic theories which see humans not only as part of society, but also as part of the larger ecosystems that life depends on. The second aim is to show how linguistics can be used to address key ecological issues, from climate change and biodiversity loss to environmental justice.\n\nSociolinguistics is the study of how language is shaped by social factors. This sub-discipline focuses on the synchronic approach of linguistics, and looks at how a language in general, or a set of languages, display variation and varieties at a given point in time. The study of language variation and the different varieties of language through dialects, registers, and ideolects can be tackled through a study of style, as well as through analysis of discourse. Sociolinguists research on both style and discourse in language, and also study the theoretical factors that are at play between language and society.\n\nDevelopmental linguistics is the study of the development of linguistic ability in individuals, particularly the acquisition of language in childhood. Some of the questions that developmental linguistics looks into is how children acquire different languages, how adults can acquire a second language, and what the process of language acquisition is.\n\nNeurolinguistics is the study of the structures in the human brain that underlie grammar and communication. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and computer modelling. Amongst the structures of the brain involved in the mechanisms of neurolinguistics, the cerebellum which contains the highest numbers of neurons has a major role in terms of predictions required to produce language.\n\nLinguists are largely concerned with finding and describing the generalities and varieties both within particular languages and among all languages. Applied linguistics takes the results of those findings and \"applies\" them to other areas. Linguistic research is commonly applied to areas such as language education, lexicography, translation, language planning, which involves governmental policy implementation related to language use, and natural language processing. \"Applied linguistics\" has been argued to be something of a misnomer. Applied linguists actually focus on making sense of and engineering solutions for real-world linguistic problems, and not literally \"applying\" existing technical knowledge from linguistics. Moreover, they commonly apply technical knowledge from multiple sources, such as sociology (e.g., conversation analysis) and anthropology. (Constructed language fits under Applied linguistics.)\n\nToday, computers are widely used in many areas of applied linguistics. Speech synthesis and speech recognition use phonetic and phonemic knowledge to provide voice interfaces to computers. Applications of computational linguistics in machine translation, computer-assisted translation, and natural language processing are areas of applied linguistics that have come to the forefront. Their influence has had an effect on theories of syntax and semantics, as modelling syntactic and semantic theories on computers constraints.\n\nLinguistic analysis is a sub-discipline of applied linguistics used by many governments to verify the claimed nationality of people seeking asylum who do not hold the necessary documentation to prove their claim. This often takes the form of an interview by personnel in an immigration department. Depending on the country, this interview is conducted either in the asylum seeker's native language through an interpreter or in an international \"lingua franca\" like English. Australia uses the former method, while Germany employs the latter; the Netherlands uses either method depending on the languages involved. Tape recordings of the interview then undergo language analysis, which can be done either by private contractors or within a department of the government. In this analysis, linguistic features of the asylum seeker are used by analysts to make a determination about the speaker's nationality. The reported findings of the linguistic analysis can play a critical role in the government's decision on the refugee status of the asylum seeker.\n\nWithin the broad discipline of linguistics, various emerging sub-disciplines focus on a more detailed description and analysis of language, and are often organized on the basis of the school of thought and theoretical approach that they pre-suppose, or the external factors that influence them.\n\nSemiotics is the study of sign processes (semiosis), or signification and communication, signs, and symbols, both individually and grouped into sign systems, including the study of how meaning is constructed and understood. Semioticians often do not restrict themselves to linguistic communication when studying the use of signs but extend the meaning of \"sign\" to cover all kinds of cultural symbols. Nonetheless, semiotic disciplines closely related to linguistics are literary studies, discourse analysis, text linguistics, and philosophy of language. Semiotics, within the linguistics paradigm, is the study of the relationship between language and culture. Historically, Edward Sapir and Ferdinand De Saussure's structuralist theories influenced the study of signs extensively until the late part of the 20th century, but later, post-modern and post-structural thought, through language philosophers including Jacques Derrida, Mikhail Bakhtin, Michel Foucault, and others, have also been a considerable influence on the discipline in the late part of the 20th century and early 21st century. These theories emphasize the role of language variation, and the idea of subjective usage, depending on external elements like social and cultural factors, rather than merely on the interplay of formal elements.\n\nSince the inception of the discipline of linguistics, linguists have been concerned with describing and analysing previously undocumented languages. Starting with Franz Boas in the early 1900s, this became the main focus of American linguistics until the rise of formal structural linguistics in the mid-20th century. This focus on language documentation was partly motivated by a concern to document the rapidly disappearing languages of indigenous peoples. The ethnographic dimension of the Boasian approach to language description played a role in the development of disciplines such as sociolinguistics, anthropological linguistics, and linguistic anthropology, which investigate the relations between language, culture, and society.\n\nThe emphasis on linguistic description and documentation has also gained prominence outside North America, with the documentation of rapidly dying indigenous languages becoming a primary focus in many university programmes in linguistics. Language description is a work-intensive endeavour, usually requiring years of field work in the language concerned, so as to equip the linguist to write a sufficiently accurate reference grammar. Further, the task of documentation requires the linguist to collect a substantial corpus in the language in question, consisting of texts and recordings, both sound and video, which can be stored in an accessible format within open repositories, and used for further research.\n\nThe sub-field of translation includes the translation of written and spoken texts across mediums, from digital to print and spoken. To translate literally means to transmute the meaning from one language into another. Translators are often employed by organizations, such as travel agencies as well as governmental embassies to facilitate communication between two speakers who do not know each other's language. Translators are also employed to work within computational linguistics setups like Google Translate for example, which is an automated, programmed facility to translate words and phrases between any two or more given languages. Translation is also conducted by publishing houses, which convert works of writing from one language to another in order to reach varied audiences. Academic Translators, specialize and semi specialize on various other disciplines such as; Technology, Science, Law, Economics etc.\n\nBiolinguistics is the study of the biology and evolution of language. It is a highly interdisciplinary field, including linguists, biologists, neuroscientists, psychologists, mathematicians, and others. By shifting the focus of investigation in linguistics to a comprehensive scheme that embraces natural sciences, it seeks to yield a framework by which the fundamentals of the faculty of language are understood.\n\nClinical linguistics is the application of linguistic theory to the fields of Speech-Language Pathology. Speech language pathologists work on corrective measures to cure communication disorders and swallowing disorders\n\nChaika (1990) showed that people with schizophrenia who display speech disorders, like rhyming inappropriately, have attentional dysfunction, as when a patient, shown a colour chip and then asked to identify it, responded \"looks like clay. Sounds like gray. Take you for a roll in the hay. Heyday, May Day.\" The color chip was actually clay-colored, so his first response was correct.'\n\nHowever, most people suppress or ignore words which rhyme with what they've said unless they are deliberately producing a pun, poem or rap. Even then, the speaker shows connection between words chosen for rhyme and an overall meaning in discourse. People with schizophrenia with speech dysfunction show no such relation between rhyme and reason. Some even produce stretches of gibberish combined with recognizable words.\n\nComputational linguistics is the study of linguistic issues in a way that is \"computationally responsible\", i.e., taking careful note of computational consideration of algorithmic specification and computational complexity, so that the linguistic theories devised can be shown to exhibit certain desirable computational properties and their implementations. Computational linguists also work on computer language and software development.\n\nEvolutionary linguistics is the interdisciplinary study of the emergence of the language faculty through human evolution, and also the application of evolutionary theory to the study of cultural evolution among different languages. It is also a study of the dispersal of various languages across the globe, through movements among ancient communities.\n\nForensic linguistics is the application of linguistic analysis to forensics. Forensic analysis investigates on the style, language, lexical use, and other linguistic and grammatical features used in the legal context to provide evidence in courts of law. Forensic linguists have also contributed expertise in criminal cases.\n\n\n"}
{"id": "3318869", "url": "https://en.wikipedia.org/wiki?curid=3318869", "title": "Majhwar language", "text": "Majhwar language\n\nMajhwar is a poorly attested language, apparently related to or a dialect of Asuri, spoken in parts of Nepal and Sikkim.\n"}
{"id": "21161169", "url": "https://en.wikipedia.org/wiki?curid=21161169", "title": "Manduriano", "text": "Manduriano\n\nManduriano is a dialect of the Sicilian language, spoken by the majority of the inhabitants of Manduria, in southern Apulia; it is a sub-dialect of Salentino.\n"}
{"id": "25249677", "url": "https://en.wikipedia.org/wiki?curid=25249677", "title": "Marcion (software)", "text": "Marcion (software)\n\nMarcion is Coptic–English/Czech dictionary related to Crum's coptic dictionary, written in C++, based on MySQL, with Qt GUI. Contains many coptic texts, grammars, Greek texts, Liddell–Scott Greek–English lexicon, and others. Can be used as a bible study tool. Marcion is free software released under the GNU GPL.\n"}
{"id": "55746942", "url": "https://en.wikipedia.org/wiki?curid=55746942", "title": "Michaelis (dictionary)", "text": "Michaelis (dictionary)\n\nMichaelis is a brand of dictionaries of the portuguese language published in Brasil by Melhoramentos publishing company. There are also under this brand books about the grammar of a variety of foreign languages. The first Michaelis dictionary was created by the end of the 19th century by the German lexicographer Henriette Michaelis, in a partnership with her sister Carolina Michaelis de Vasconcelos.\n\nThe dictionary has versions in the following languages:\n\nIn July 2015, an online petition was created on Change.org demanding the dictionary to review the definition of \"marriage\". The definition was, among others, a \"legitimate union between man and woman\". The petition requested that the word \"people\" were used instead of \"man\" and \"woman\". The petition got more than three thousand endorsements. The director of Melhoramentos abided by and made the requested changes.\n\n"}
{"id": "55781608", "url": "https://en.wikipedia.org/wiki?curid=55781608", "title": "Mundo Lingo", "text": "Mundo Lingo\n\nMundo Lingo are free language social events that happen independently in various metropolises in different countries. They usually take place in the late evening during the week (7pm to midnight) in a specific bar. They gather many men and women of all ages and all nationalities that wish to learn or improve one or more languages.\n\nEach participant receive flags to stick to his clothes. They are supposed to indicate their native language at the top, and the rest below (generally the languages they mastered and below the one they wants to learn). Then, people are free to chat with each other while having drinks.\n\nThe first Mundo Lingo event was an initiative of UK born Benji Moreira when he was an immigrant in Buenos Aires (Argentina). It took place on July 7, 2011 and was aimed for local Argentines to learn foreign languages and for internationals to practice Spanish.\nAfter a while, around 50 people per week were attending and the flag system was introduced. In 2014 the event grew increasingly popular and was exported to Cologne, London, Montreal, and then Melbourne.\n\nAs of September 2018, Mundo Lingo events regularly take place in up to 30 cities, 24 countries and 5 continents.\n\nAs of September 2018, Mundo Lingo events regularly take place in Saigon and Hanoi (Vietnam), Bangkok and Chiang Mai (Thailand), Nanjing (China), Osaka (Japan), Singapore and Yangon (Myanmar) .\n\nAs of September 2018, Mundo Lingo events regularly take place in Melbourne, Sydney and Brisbane (Australia) and Wellington and Auckland (New Zealand).\n\nAs of September 2018, Mundo Lingo events regularly take place in Barcelona (Spain), Cologne, Stuttgart and Munich (Germany), Copenhagen (Denmark), Geneva (Switzerland), London (England), Paris (France) and Oslo (Norway).\n\nAs of September 2018, Mundo Lingo events regularly take place in Buenos Aires, Córdoba and La Plata (Argentina), Lima (Peru), Rio de Janeiro (Brazil), Montréal and Toronto (Canada), New York (United States).\n\n"}
{"id": "4804623", "url": "https://en.wikipedia.org/wiki?curid=4804623", "title": "Mythical origins of language", "text": "Mythical origins of language\n\nThere have been many accounts of the origin of language in the world's mythologies and other stories pertaining to the origin of language, the development of language and the reasons behind the diversity in languages today.\n\nThese myths have similarities, recurring themes, and differences, having been passed down through oral tradition. Some myths go further than just storytelling and are religious, with some even having a literal interpretation even today. Recurring themes in the myths of language dispersal are floods and catastrophes. Many stories tell of a great deluge or flood which caused the peoples of the Earth to scatter over the face of the planet. Punishment by a god or gods for perceived wrongdoing on the part of man is another recurring theme.\n\nMyths regarding the origins of language and languages are generally subsumed or footnoted into larger creation myths, although there are differences. Some tales say a creator endowed language from the beginning, others count language among later gifts, or curses.\n\nThe Hebrew Bible attributes the origin of language per se to humans, with Adam being asked to name the creatures that God had created.\n\nOne of the most well known examples in the West is the Tower of Babel passage from Genesis. \nIt tells of God punishing humanity for arrogance and disobedience by means of the confusion of tongues.\n\nThis became the standard account in the European Middle Ages, reflected in medieval literature such as the tale of Fénius Farsaid.\n\nVāc is the Hindu goddess of speech, or \"speech personified\". As \"brahman\" \"sacred utterance\", she has a cosmological role as the \"Mother of the Vedas\".\nShe is presented as the consort of Prajapati, who is likewise presented as the origin of the Veda.\nShe became conflated with Sarasvati in later Hindu mythology.\n\nIn common with the mythology of many other civilizations and cultures which tell of a Great Flood, certain Native American tribes tell of a deluge which came over the Earth. After the water subsides, various explanations are given for the new diversity in speech.\n\nThe Aztecs' story maintains that only a man, Coxcox, and a woman, Xochiquetzal, survive, having floated on a piece of bark. They found themselves on land and begot many children who were at first born unable to speak, but subsequently, upon the arrival of a dove were endowed with language, although each one was given a different speech such that they could not understand one another.\n\nA similar flood is described by the Kaska people from North America, however, like with the story of Babel, the people were now \"widely scattered over the world\". The narrator of the story adds that this explains the many different centres of population, the many tribes and the many languages, \"Before the flood, there was but one centre; for all the people lived together in one country, and spoke one language.\"\n\nAn Iroquois story tells of the god Taryenyawagon (\"Holder of the Heavens\") guiding his people on a journey and directing them to settle in different places whence their languages changed.\n\nA Salishan myth tells how an argument led to the divergence of languages. Two people were arguing whether the high-pitched humming noise that accompanies ducks in flight is from air passing through the beak or from the flapping of wings. The argument is not settled by the chief, who then calls a council of all the leading people from nearby villages. This council breaks down in argument when nobody can agree, and eventually the dispute leads to a split where some people move far away. Over time they slowly began to speak differently, and eventually other languages were formed.\n\nIn the mythology of the Yuki, indigenous people of California, a creator, accompanied by Coyote creates language as he creates the tribes in various localities. He lays sticks which will transform into people upon daybreak.\n\nThe Ticuna people of the Upper Amazon tell that all the peoples were once a single tribe, speaking the same language until two hummingbird eggs were eaten, it is not told by whom. Subsequently the tribe split into groups and dispersed far and wide.\n\nIn Ancient Greece there was a myth which told that for ages men had lived without law under the rule of Zeus and speaking one language, gifted to them by the god and goddess of ingenuity, Philarios and Philarion. The god Hermes brought diversity in speech and along with it separation into nations and discord ensued. Zeus then resigned his position, yielding it to the first king of men, Phoroneus.\n\nIn Norse mythology, the faculty of speech is a gift from the third son of Borr, Vé, who gave also hearing and sight.\n\nThe Wa-Sania, a Bantu people of East African origin have a tale that in the beginning, the peoples of the earth knew only one language, but during a severe famine, a madness struck the people, causing them to wander in all directions, jabbering strange words, and this is how different languages came about.\n\nA god who speaks all languages is a theme in African mythology, two examples being Eshu of the Yoruba, a trickster who is a messenger of the gods. Eshu has a parallel in Legba from the Fon people of Benin. Another Yoruba god who speaks all the languages of the world is Orunmila, the god of divination.\n\nA group of people on the island of Hao in Polynesia tell a very similar story to the Tower of Babel, speaking of a God who, \"in anger chased the builders away, broke down the building, and changed their language, so that they spoke diverse tongues\".\n\nIn South Australia, a people of Encounter Bay tell a story of how diversity in language came about from cannibalism:\n\nAnother group of Australian Aboriginal people, the Gunwinggu, tell of a goddess in dreamtime giving each of her children a language of their own to play with.\n\nThe traditional beliefs of the indigenous inhabitants of the Andaman Islands in the Bay of Bengal describe language as being given by the god Pūluga to the first man and woman at their union following a great deluge. The language given was called \"bojig-yâb-\", which is the language spoken to this day, according to their belief, by the tribe inhabiting the south and south-eastern portion of middle Andaman. This language is described by the inhabitants as the \"mother tongue\" from which all other dialects have been made.\n\nTheir beliefs hold that even before the death of the first man,\n\nThus explaining the diversity of language.\n\n\n"}
{"id": "14473198", "url": "https://en.wikipedia.org/wiki?curid=14473198", "title": "National Center for Voice and Speech", "text": "National Center for Voice and Speech\n\nThe National Center for Voice and Speech (NCVS), is a multi-site research and teaching organization dedicated to studying the characteristics, limitations and enhancement of human voice and speech. The NCVS is located in Salt Lake City, Utah with the Lead Institution located at the University of Utah. NCVS is also a Center at the University of Iowa where it has laboratories in the Department of Speech Pathology and Audiology. In addition, the NCVS has collaborators in Denver and at many institutions around the United States. Its focus is vocology, or the science and practice of voice habilitation.\n\nInitially conceived as a \"center without walls,\" the NCVS was formally organized in 1990 with the assistance of a grant from the National Institute on Deafness and Other Communication Disorders (NIDCD), an institute of the National Institutes of Health (NIH). The NCVS was organized on the premise that a consortium of institutions (including the Wilber James Gould Voice Center at the DCPA, University of Iowa, University of Utah, University of Wisconsin–Madison) would be better able to conduct and disseminate research than a single organization. NCVS members, although geographically separate, were linked by a common desire to fully understand the characteristics, limitations and enhancement of human voice and speech.\n\nIn 1999, NIDCD discontinued the Multi-Purpose Research and Training Center funding mechanism for the entire institute focusing instead on single-project research awards (R01s). In a July 2000 meeting, however, NCVS investigators voted unanimously to continue the concept of a national resource center for voice and speech, to be driven by a variety of single-project research awards (R01s), as well as health communication, core, and training grants. In 2001, the NCVS moved its central location to Denver, where the otolaryngologist Dr. Wilbur James Gould had founded a center to study the voice and speech patterns of stage performers.\n\nThe NCVS team of investigators, led by Ingo Titze, studies the powers, limitations and enhancement of human voice and speech. The investigators are scientists, clinicians, educators, engineers and musicians who use diverse backgrounds (i.e., speech-language pathology, physics, computer science, acoustics, vocal performance, biology, medicine and engineering) to work together on voice and speech investigations. As a direct outgrowth of their work, NCVS members also teach other investigators and practitioners who work with voice, as well as speech clients and the general public. One example is the Summer Vocology Institute, which trains voice coaches and vocal health professionals in the study of Vocology.\n\n\n\n"}
{"id": "1699819", "url": "https://en.wikipedia.org/wiki?curid=1699819", "title": "Note-taking", "text": "Note-taking\n\nNote-taking (sometimes written as notetaking or note taking) is the practice of recording information captured from another source. By taking notes, the writer records the essence of the information, freeing their mind from having to recall everything. Notes are commonly drawn from a transient source, such as an oral discussion at a meeting, or a lecture (notes of a meeting are usually called minutes), in which case the notes may be the only record of the event.\nNote taking is a form of self discipline.\n\nNote-taking has been an important part of human history and scientific development. The Ancient Greeks developed hypomnema, personal records on important subjects. In the Renaissance and early modern period, students learned to take notes in schools, academies and universities, oftentimes producing beautiful volumes that served as reference works after they finished their studies. In predigital times there were many kinds of notebooks used by adults, some of which included accounting waste books, marginalia, and commonplace books. Philosopher John Locke developed an indexing system which served as a model for commonplace books; for example, it inspired another book, \"Bell’s Common-Place Book, Formed generally upon the Principles Recommended and Practised by Mr Locke\" nearly a century later. Note-taking is something that many people haven't yet noticed how much it's changed over the course of history.\n\nNote-taking is a central aspect of a complex human behavior related to information management involving a range of underlying mental processes and their interactions with other cognitive functions. The person taking notes must acquire and filter the incoming sources, organize and restructure existing knowledge structures, comprehend and write down their explanation of the information, and ultimately store and integrate the freshly processed material. The result is a knowledge representation, and a memory storage. Studies comparing the performance of students who took handwritten notes to students who typed their notes found that students who took handwritten notes performed better on examinations, hypothetically due to the deeper processing of learned material through selective rephrasing instead of word-for-word transcription which is common when typing notes.\n\nMany different formats are used to structure information and make it easier to find and to understand, later. The format of the initial record may often be informal and/or unstructured. One common format for such notes is shorthand, which can allow large amounts of information to be put on paper very quickly. Note-taking is an important skill for students, especially at the college level. Many students gain skills as they go through High School and most grasp onto the best and easiest note-taking techniques. Practice in note-taking may lead to great skills in it that may also lead to success in the future. In some contexts, such as college lectures, the main purpose of taking notes may be to implant the material in the mind; the written notes themselves being of secondary importance. Historically, note-taking was an analog process, written in notebooks, or other paper methods like Post-It notes. In the digital age, computers, tablet PCs and personal digital assistants (PDAs) are common.\n\nNote-taking is a race against time. The note taker typically is under severe time pressure, and different note-taking styles and techniques try to make the best use of time. The average rate of speech is 2–3 words per second, but the average handwriting speed as only 0.2–0.3 words per second.\n\nRegardless of the medium (paper, computer), note-taking can be broadly divided into linear and nonlinear methods, which can be combined.\n\nPeople tend to write down the most important information to prevent losing time and be ready to note subsequent information provided.\n\nIt's also always important to go over the notes you write down on a loop to easily remember any of the information you went over.\n\nLinear note-taking is the process of writing down information in the order in which you receive it. Paper is itself two-dimensional so linear notes follow the natural succession of time 1,2, and so on, beginning, middle and end. However, the human brain is thought to be multi-dimensional: the more connections one makes to current knowledge, the greater the likelihood of understanding, remembering and applying the information.\n\nOutlining is one of the most common but still one of the best note-taking systems. Notes and thoughts are organised in a structured, logical manner, reducing the time needed to edit and review, allowing a lot of information to be digested in a short period of time. Outlining is less effective for classes that involve many formulas and graphs, like mathematics or chemistry. In these situations, a system such as Cornell notes may be superior.\n\nOutlines tend to proceed down a page, using headings and bullets to structure information. A common system consists of headings that use Roman numerals, letters of the alphabet, and Arabic numerals at different levels. A typical structure would be:\n\nHowever, this sort of structure has limitations in written form since it is difficult to go back and insert more information. Adaptive systems are used for paper-and-pen insertions, such as using the reverse side of the preceding page in a spiral notebook to make insertions. Or one can simply leave large spaces in between items, to enable more material to be inserted. The above method is effective for most people, but you can be creative in making your own method. (See for more about application software that supports outlining).\n\nHowever, computerized note-taking, whether with a word processor, an outliner like Workflowy, or a digital notebook program such as OneNote, Evernote or TiddlyWiki, provides the opportunity to revise easily and add more entries or rows to the outline.\n\nNon-linear note-taking involves using mind maps and spidergrams that start with notes in the middle of a page, commonly in an oval representing the topic. Non-linear note taking may require additional sheets of paper extending the notes at the top, bottom or sides, giving an holistic overview of the information. The many types of non-linear note-taking include clustering, concept mapping, the Cornell system, idea mapping, instant replays, Ishikawa diagrams, knowledge maps, learning maps, mind mapping, model maps, the pyramid principle, semantic networks, and SmartWisdom.\n\nThis method of note taking is useful for subject matter that can be broken into categories, such as similarities, differences, date, event, impact, etc. Charting works best if students are able to identify categories and draw a table prior to the lecture. This method is also useful as an editing tool. Students may review and rewrite notes using the charting method. The method may work well for students who like to organize information neatly and who learn by recognizing patterns.\n\nMapping is one of the many forms of note-taking that employs geographic organizers and diagrams to assemble information. This specific method encourages students to continually practice and also to create flash cards because mapping method doesn't come naturally to everyone in the beginning, so practice is essential. Here, ideas are written in a tree structure, with lines connecting them together. Mind maps, also referred to as brain-storming are commonly drawn from a central point, purpose or goal in the center of the page and then branching outward to identify all the ideas connected to that goal. Colors, small graphics and symbols are often used to help to visualize the information more easily. This note-taking method is most common among visual learners and is a core practice of many accelerated learning techniques. It is also used for planning and writing essays.\n\nThis method are also for people that have a visual mindset when they take notes better on drawing pictures with different shapes and diagrams. Those who feel comfortable using this method tend to use a lot of colors in their diagrams to make them focused and concentrated in what they are writing. For instance, many who use this method might feel more focused when their teacher or professor who use a lot of colors in their power point slides since, the person themselves might like to use colors and use different colors to categorize different kinds of information.\n\nSentence note taking is simply writing down each topic as a short, simple sentence. This method works well for fast paced lesson where a lot of information is being covered. Everyone should record every new thought, fact or topic on a separate line. All information is recorded but is not organized into major and minor topics. Every new thought is written as a new line. Speed is essential, because not much thought about formatting is needed to create space for more notes. Notes can be numbered or set off with bullets showing where a new thought begins. This strategy is especially helpful when a professor or teacher asks to read the notes. It also saves the time required to formulate and write long, complex sentences.\n\nSQ3R is a method of taking notes from written material, though it might be better classed as a method of reading and gaining understanding. Material is skimmed to produce a list of headings, that are then converted into questions. These questions are then considered whilst the text is read to provide motivation for what is being covered. Notes are written under sections headed by the questions as each of the material's sections is read. One then makes a summary from memory, and reviews the notes. (SQ3R—Survey, Question, Read, Recite, Review.). However, there is another updated version called SQ4R, which has been used by students since the early 60's. It's probably worth your time to try the steps first, then choose and apply only those that really work effectively. It provides a systematic way of comprehending and studying the content that is being spoken. Now students are able to monitor their own comprehension through review. Current research shows that students who use the SQ3R strategy retain more information and achieve higher test scores.\n\nSometimes lecturers may provide handouts of guided notes, which provide a \"map\" of the lecture content with key points or ideas missing. Students then fill in missing items as the lecture progresses. Guided notes may assist students in following lectures and identifying the most important ideas from a lecture. This format provides students with a framework, yet requires active listening (as opposed to providing copies of power point slides in their entirety), and promotes active engagement during lecture or independent reading, provide full and accurate notes for use as a study guide, helping the student to identify the most important information covered. Research suggests that guided notes improve student recording of critical points in lecture, as well as quiz scores on related content. In addition, an investigation carried out on students with learning problems showed that the use of the guided notes is an effective strategy to improve the performance of these students.\n\nThe growing ubiquity of laptops in universities and colleges has led to a rise in electronic note-taking. Many students write their notes in word processors or prepare digital hand-written notes using a graphics tablet or tablet computer and styli or digital pens, with the aid of note-taking software. Online applications are receiving growing attention from students who can forward notes using email, or otherwise make use of collaborative features in these applications and can also download the texts as a file (txt, rtf...) in a local computer. It has also become common for lecturers to deliver lectures using these and similar technologies, including electronic whiteboards, especially at institutes of technology.\n\nOnline note-taking has created problems for teachers who must balance educational freedom with copyright and intellectual property concerns regarding course content.\n\nElectronic note-taking has shown ineffectiveness when compared to traditional methods of note-taking. A study done by Pam A. Mueller of Princeton University and Daniel M. Oppenheimer of the University of California have shown that students who take notes digitally against students who take notes on paper retain less information and have difficulties remembering what they've typed down. Electronic note-taking has created computer-aided distractions in class as multitasking on laptops is very easy to accomplish. However these researches are only about typing notes on laptops, not writing on tablets.\n\nLaptops are usually a controlled device in classrooms and students may or not be able to take notes on their digital devices when required.\n\nThe Cornell Method is a systematical structure that help organize your notes, actively involve you in the creation of knowledge, improve your study expertness, and lead to academic success. The Cornell method of taking notes was developed by Dr. Walter Pauk of Cornell University in 1940's. This effective system for taking notes was made prestigious by Pauk's best selling book \"How To Study In College\" and is commonly used at many universities today. The Cornell method consists of dividing a single page into three sections: Notes, Cues and a Summary, its own system is characterized by a right hand column for notes, a left hand column for provoking key words or some type of questions that might help remember key aspects of the topic. Cornell Method also help students worldwide combat the effects of forgetting, which can be instantaneous and complete. It is recommended to read the notes at least 3 times in order for the content to sink in. Otherwise, with time it will be forgotten. For more information please refer to the article Forgetting Curve.\n\nProfessional notetakers provide access to information for people who cannot take their own notes, in particular the deaf and hearing impaired. Professional Notetakers most frequently work in colleges and universities, but are also used in workplace meetings, appointments, conferences, and training sessions. They are usually educated to degree level. In the UK they are increasingly expected to have a professional note-taking qualification, such as that offered by the Council for the Advancement of Communication with Deaf People (CACDP). Professional freelance notetakers can also be supplied by transcription services companies, such as Take Note\n\n\n\nTake Note Typing, Professional Notetaking & Transcription Services \n"}
{"id": "31507057", "url": "https://en.wikipedia.org/wiki?curid=31507057", "title": "Online speech therapy", "text": "Online speech therapy\n\nOnline speech therapy or telepractice is the use of technology to provide speech therapy via high speed internet, webcam, headset with microphone or any other form of communication. Online therapy is a clinical arrangement where the patient and a speech-language certified pathologist communicate and interact face-to-face over the Internet. The session involves a suite of therapeutic exercises including listening, speaking, reading and writing. The recorded videos are assessed by the pathologist to generate an activity report for evaluating progress and usage.\n\nTelepractice is a method of reaching students and individuals in any distant location. Telepractice is defined by the American Speech–Language–Hearing Association (ASHA) as the use of technology to provide speech therapy services to remote regions. Janet Brown, the Director of Healthcare services in SLP at ASHA has stated \"research shows that with telepractice a speech-language pathologist can provide speech therapy services, with the same results, as being there in person.\"\n\nAssessment for online speech therapy consists of an informal oral exam by a licensed trained professional through video conferencing or a web application. Patients are initially screened for communication disorders with diagnosis and consultation for provision counseling including cognitive aspects of communication, syntax, hypophonia and upper aerodigestive functions. The therapist and patient communicate via telecommunication technology where they can interact in real time. Therapy may cover speech sound production, fluency, language, cognition and written language.\n\nThe therapists create an assessment document or report that is updated after every session. The document is reported to the parents or the referral source in compliance with HIPAA and FERPA. The Health Insurance Portability and Accountability Act also known as HIPAA is a federal law that protects patient medical records. HIPAA specifically protects “individually identifiable health information.” The Family Educational Rights and Privacy Act known as FERPA is a federal law that protects student education records. FERPA gives parents certain rights with respect to their children’s education records until they turn 18 or transfer to a school higher than the high school level, thus making them “eligible students.” Clinical departments in Universities also offer speech and language therapy services where they keep recorded video of sessions between clinicians and patients secured through password security services.\n\nOnline speech therapy courses are offered by professional associations, Universities and cyber schools for treatment of communication disorders. The minimum eligibility for an online speech therapy course is a bachelor's degree for Master's entry-level field and master's degree for CScD offered by professional associations and Universities in the United States. The speech therapist course is not available for Bachelors or under graduate degree level in the United States. The pathologists are required to attain a minimum of 400 clinical hours to complete the course. In Asian countries, the course is open for Bachelors level degree program offered by Universities.\n\nSpeech Pathologist are also required to complete 400 hours clinical hours and pass the KASA exams in the United States.\n"}
{"id": "53828", "url": "https://en.wikipedia.org/wiki?curid=53828", "title": "Ontological commitment", "text": "Ontological commitment\n\nAn ontological commitment refers to a relation between a language and certain objects postulated to be extant by that language. The 'existence' referred to need not be 'real', but exist only in a universe of discourse. As an example, legal systems use vocabulary referring to 'legal persons' that are collective entities that have rights. One says the legal doctrine has an \"ontological commitment\" to non-singular individuals. In information systems and artificial intelligence, where an ontology refers to a specific vocabulary and a set of explicit assumptions about the meaning and usage of these words, then an ontological commitment is an agreement to use the shared vocabulary in a coherent and consistent manner within a specific context. In philosophy a \"theory is ontologically committed to an object only if that object occurs in \"all\" the ontologies of that theory\"\n\nThe sentence “Napoleon is one of my ancestors” apparently commits us only to the existence of two individuals (i.e., Napoleon and the speaker) and a line of ancestry between them. The fact that no other people or objects are mentioned seems to limit the “commitment” of the sentence. However, it is well known that sentences of this kind cannot be interpreted in first-order logic, where individual variables stand for individual things. Instead, they must be represented in some second-order form. In ordinary language, such second-order forms use either grammatical plurals or terms such as “set of” or “group of”.\n\nFor example, the sentence involving Napoleon can be rewritten as “any group of people that includes me and the parents of each person in the group must also include Napoleon,” which is easily interpreted as a statement in second-order logic (one would naturally start by assigning a name, such as \"G\", to the group of people under consideration). Formally, collective noun forms such as “a group of people” are represented by second-order variables, or by first-order variables standing for sets (which are well-defined objects in mathematics and logic). Since these variables do not stand for individual objects, it seems we are “ontologically committed” to entities other than individuals — sets, classes, and so on. As Quine puts it,\n\nthe general adoption of class variables of quantification ushers in a theory whose laws were not in general expressible in the antecedent levels of logic. The price paid for this increased power is ontological: objects of a special and abstract kind, viz. classes, are now presupposed. Formally it is precisely in allowing quantification over class variables α, β, etc., that we assume a range of values for these variables to refer to. To be assumed as an entity is to be assumed as a value of a variable. (\"Methods of Logic\" p. 228)\n\nAnother statement about individuals that appears “ontologically innocent” is the well-known Geach–Kaplan sentence: \"Some critics admire only one another.\"\n\nWillard Van Orman Quine provided an early and influential formulation of ontological commitment:\nThe purpose of Quine's strategy is to determine just how the \"ontological commitment\" of a theory is to be found. Quine argued that the only ontologically committing expressions are variables bound by a first-order existential quantifier, and natural language expressions which were formalized using variables bound by first-order existential quantifiers.\n\nAttempts have been made to argue that predicates are also ontologically committing, and thus that subject-predicate sentences bear additional ontological commitment to abstract objects such as universals, sets, or classes. It has been suggested that the use of meaningful names in nonexistence statements such as “Pegasus does not exist” brings with it an ontological commitment to fictional objects like Pegasus, a quandary referred to as Plato's beard and escaped by using quantifiers.\n\nThis discussion has a connection to the Carnap-Quine argument over analytic and synthetic objects. Although Quine refers to 'ontological commitment' in this connection, in his rejection of the analytic/synthetic distinction he does not rely upon the formal translation of any particular theory along the lines he has suggested. Instead, Quine argues by using examples that although there are tautological statements in a formal theory, like \"all squares are rectangles\", a formal theory necessarily contains references to objects that are not tautological, but have external connections. That is, there is an \"ontological commitment\" to such external objects. In addition, the terms used to interpret the application of the theory are not simply descriptions of sensory input, but are statements in a context. That is, inversely, there is an \"ontological commitment\" of these observational objects to the formal theory. As Ryan puts it: \"Rather than being divided between contingent synthetic claims and indubitable analytic propositions, our beliefs constitute a continuous range from a periphery of sense-reports to interior concepts that are comparatively theory-laden and general.\" Thus we end up with Quine's 'flat' ontology that does not see a distinction between analytic and synthetic objects.\n\nWhatever process one uses to determine the ontological commitments of a theory, that does not prescribe what ontological commitments one should have. Quine regarded this as a matter of epistemology, which theory one should accept. \"Appeal is made to [concerns of] explanatory power, parsimony, conservatism, precision, and so on\".\n\nOntological parsimony can be defined in various ways, and often is equated to versions of Occam's razor, a \"rule of thumb, which obliges us to favor theories or hypotheses that make the fewest unwarranted, or \"ad hoc\", assumptions about the data from which they are derived.\" Glock regards 'ontological parsimony' as one of the 'five main points' of Quine's conception of ontology.\n\nFollowing Quine, Baker states that a theory, \"T\", is \"ontologically committed\" to items \"F\" if and only if \"T\" entails that \"F′\"s exist. If two theories, \"T\" and \"T\", have the same ontological commitments except that \"T\" is ontologically committed to \"F′\"s while \"T\" is not, then \"T\" is more parsimonious than \"T\". More generally, a sufficient condition for \"T\" being more parsimonious than \"T\" is for the ontological commitments of \"T\" to be a proper subset of those of \"T\".\n\nThese ideas lead to the following particular formulation of Occam's razor: 'Other things being equal, if \"T\" is more ontologically parsimonious than \"T\" then it is rational to prefer \"T\" to \"T\".' While a common formulation stipulates only that entities should not be multiplied beyond necessity, this version by contrast, states that entities should not be multiplied \"other things being equal\", and this is compatible with parsimony being a comparatively weak theoretical virtue.\n\nThe standard approach to ontological commitment has been that, once a theory has been regimented and/or \"paraphrased\" into an agreed \"canonical\" version, which may indeed be in formal logical notation rather than the original language of the theory, ontological commitments can be read off straightforwardly from the presence of certain ontologically committing expressions (e.g. bound variables of existential quantification). Although there is substantial debate about which expressions are ontologically committing, parties to that debate generally agree that the expressions they prefer are reliable bearers of ontological commitment, imparting ontological commitment to all regimented sentences in which they occur. This assumption has been challenged.\n\nInwagen has taken issue with Quine's methodology, claiming that this process did not lead to a unique set of fundamental objects, but to several possible sets, and one never could be certain that all the possible sets had been found. He also took issue with Quine's notion of a theory, which he felt was tantamount to suggesting a 'theory' was just a collection of sentences. Inwagen suggested that Quine's approach provided useful tools for discovering what entities were ontological commitments, but that he had not been successful. His attempts are comparable to an \"attempt to reach the moon by climbing ever higher trees...\"\n\nIt has been suggested that the ontological commitments of a theory cannot be discerned by analysis of the syntax of sentences, looking for ontologically committing expressions, because the true ontological commitments of a sentence (or theory) are restricted to the entities needed to serve as truthmakers for that sentence, and the syntax of even a regimented or formalized sentence is not a reliable guide to what entities are needed to make it true. However, this view has been attacked by Jonathan Shaffer, who has argued that truthmaking is not an adequate test for ontological commitment: at best, the search for the truthmakers of our theory will tell us what is \"fundamental\", but not what our theory is ontologically committed to, and hence will not serve as a good way of deciding what exists.\n\nIt also has been argued that the syntax of sentences is not a reliable guide to their ontological commitments because English has no form of words which reliably functions to make an existence-claim in every context in which it is used. For example, Jody Azzouni suggests that \"There is\" does not make any kind of genuine existence-claim when it is used in a sentence such as \"There are mice that talk\". Since the meaning of the existential quantifier in formal notation is usually explained in terms of its equivalence to English expressions such as \"there is\" and \"there exist\", and since these English expressions are not reliably ontologically committing, it comes to seem that we cannot be sure of our theory's ontological commitments even after it has been regimented into a canonical formulation. This argument has been attacked by Howard Peacock, who suggests that Azzouni's strategy conflates two different kinds of ontological commitment – one which is intended as a measure of what a theory explicitly claims to exist, and one which is intended as a measure of what is required for the theory to be true; what the ontological costs of the theory are. If ontological commitment is thought of as a matter of the ontological costs of a theory, then it is possible that a sentence may be ontologically committed to an entity even though competent speakers of the language do not recognize the sentence as asserting the existence of that entity. Ontological commitment is not a matter of what commitments one explicitly recognizes, but rather a matter of what commitments are actually incurred.\n\n\n"}
{"id": "24566", "url": "https://en.wikipedia.org/wiki?curid=24566", "title": "Palaeography", "text": "Palaeography\n\nPalaeography (UK) or paleography (US; ultimately from , \"palaiós\", \"old\", and , \"graphein\", \"to write\") is the study of ancient and historical handwriting (that is to say, of the forms and processes of writing; not the textual content of documents). Included in the discipline is the practice of deciphering, reading, and dating historical manuscripts, and the cultural context of writing, including the methods with which writing and books were produced, and the history of scriptoria.\n\nThe discipline is important for understanding, authenticating, and dating ancient texts. However, it cannot in general be used to pinpoint dates with high precision.\n\nPalaeography can be an essential skill for historians and philologists, as it tackles two main difficulties. First, since the style of a single alphabet in each given language has evolved constantly, it is necessary to know how to decipher its individual characters as they existed in various eras. Second, scribes often used many abbreviations, usually so as to write more quickly and sometimes to save space, so the specialist-palaeographer must know how to interpret them. Knowledge of individual letter-forms, ligatures, punctuation, and abbreviations enables the palaeographer to read and understand the text. The palaeographer must know, first, the language of the text (that is, one must become expert in the relevant earlier forms of these languages); and second, the historical usages of various styles of handwriting, common writing customs, and scribal or notarial abbreviations. Philological knowledge of the language, vocabulary, and grammar generally used at a given time or place can help palaeographers identify ancient or more recent forgeries versus authentic documents.\n\nKnowledge of writing materials is also essential to the study of handwriting and to the identification of the periods in which a document or manuscript may have been produced. An important goal may be to assign the text a date and a place of origin: this is why the palaeographer must take into account the style and formation of the manuscript and the handwriting used in it.\n\nPalaeography can be used to provide information about the date at which a document was written. However, \"paleography is a last resort for dating\" and, \"for book hands, a period of 50 years is the least acceptable spread of time\" with it being suggested that \"the 'rule of thumb' should probably be to avoid dating a hand more precisely than a range of at least seventy or eighty years\". In a 2005 e-mail addendum to his 1996 \"The Paleographical Dating of P-46\" paper Bruce W. Griffin stated \"Until more rigorous methodologies are developed, it is difficult to construct a 95% confidence interval for NT manuscripts without allowing a century for an assigned date.\" William M Schniedewind went even further in the abstract to his 2005 paper \"Problems of Paleographic Dating of Inscriptions\" and stated that \"The so-called science of paleography often relies on circular reasoning because there is insufficient data to draw precise conclusion about dating. Scholars also tend to oversimplify diachronic development, assuming models of simplicity rather than complexity\".\n\n\nThe Aramaic language was the international trade language of the Ancient Middle East, originating in what is modern-day Syria, between 1000 and 600 BC. It spread from the Mediterranean coast to the borders of India, becoming extremely popular and being adopted by many people, both with or without any previous writing system. The Aramaic script was written in a consonantal form with a direction from right to left. The Aramaic alphabet, a modified form of Phoenician, was the ancestor of the modern Arabic and Hebrew scripts, as well as the Brāhmī script, the parent writing system of most modern abugidas in India, Southeast Asia, Tibet, and Mongolia. Initially, the Aramaic script did not differ from the Phoenician, but then the Aramaeans simplified some of the letters, thickened and rounded their lines: a specific feature of its letters is the distinction between d and r. One innovation in Aramaic is the \"matres lectionis\" system to indicate certain vowels. Early Phoenician-derived scripts did not have letters for vowels, and so most texts recorded just consonants. Most likely as a consequence of phonetic changes in North Semitic languages, the Aramaeans reused certain letters in the alphabet to represent long vowels. The letter \"aleph\" was employed to write /ā/, \"he\" for /ō/, \"yod\" for /ī/, and \"vav\" for /ū/.\n\nAramaic writing and language supplanted Babylonian cuneiform and Akkadian language, even in their homeland in Mesopotamia. The wide diffusion of Aramaic letters led to its writing being used not only in monumental inscriptions, but also on papyrus and potsherds. Aramaic papyri have been found in large numbers in Egypt, especially at Elephantine—among them are official and private documents of the Jewish military settlement in 5 BC. In the Aramaic papyri and potsherds, words are separated usually by a small gap, as in modern writing. At the turn of the 3rd to 2nd centuries BC, the heretofore uniform Aramaic letters developed new forms, as a result of dialectal and political fragmentation in several subgroups. The most important of these is the so-called square Hebrew block script, followed by Palmyrene, Nabataean, and the much later Syriac script.\n\nAramaic is usually divided into three main parts:\n\nThe term Middle Aramaic refers to the form of Aramaic which appears in pointed texts and is reached in the 3rd century AD with the loss of short unstressed vowels in open syllables, and continues until the triumph of Arabic.\n\nOld Aramaic appeared in the 11th century BC as the official language of the first Aramaean states. The oldest witnesses to it are inscriptions from northern Syria of the 10th to 8th centuries BC, especially extensive state treaties (c. 750 BC) and royal inscriptions. The early Old Ancient should be classified as \"Ancient Aramaic\" and consists of two clearly distinguished and standardised written languages, the Early Ancient Aramaic and the Late Ancient Aramaic. Aramaic was influenced at first principally by Akkadian, then from the 5th century BC by Persian and from the 3rd century BC onwards by Greek, as well as by Hebrew, especially in Palestine. As Aramaic evolved into the imperial language of the Neo-Assyrian Empire, the script used to write it underwent a change into something more cursive. The best examples of this script come from documents written on papyrus from Egypt. About 500 BC, Darius I (522–486) made the Aramaic used by the Achaemenid imperial administration into the official language of the western half of the Persian Empire. This so-called \"Imperial Aramaic\" (the oldest dated example, from Egypt, belonging to 495 BC) is based on an otherwise unknown written form of Ancient Aramaic from Babylonia. In orthography, Imperial Aramaic preserves historical forms—alphabet, orthography, morphology, pronunciation, vocabulary, syntax and style are highly standardised. Only the formularies of the private documents and the Proverbs of Ahiqar have maintained an older tradition of sentence structure and style. Imperial Aramaic immediately replaced Ancient Aramaic as a written language and, with slight modifications, it remained the official, commercial and literary language of the Near East until gradually, beginning with the fall of the Persian Empire (331 BC) and ending in the 4th century AD, it was replaced by Greek, Persian, the eastern and western dialects of Aramaic and Arabic, though not without leaving its traces in the written form of most of these. In its original Achaemenid form, Imperial Aramaic is found in texts of the 5th to 3rd centuries BC. These come mostly from Egypt and especially from the Jewish military colony of Elephantine, which existed at least from 530 to 399 BC.\n\n\n\nA history of Greek handwriting must be incomplete owing to the fragmentary nature of evidence. If one rules out the inscriptions on stone or metal, which belong to the science of , we are practically dependent for the period preceding the 4th or 5th century AD on the papyri from Egypt (cf. ), the earliest of which take back our knowledge only to the end of the 4th century BC. This limitation is less serious than might appear, since the few manuscripts not of Egyptian origin which have survived from this period, like the parchments from Avroman or Dura, the Herculaneum papyri, and a few documents found in Egypt but written elsewhere, reveal a uniformity of style in the various portions of the Greek world; but some differences can be discerned, and it is probable that, were there more material, distinct local styles could be traced.\n\nFurther, any given period several types of hand may exist together. There was a marked difference between the hand used for literary works (generally called \"uncials\" but, in the papyrus period, better styled \"book-hand\") and that of documents (\"cursive\") and within each of these classes several distinct styles were employed side by side; and the various types are not equally well represented in the surviving papyri.\n\nThe development of any hand is largely influenced by the materials used. To this general rule the Greek script is no exception. Whatever may have been the period at which the use of papyrus or leather as a writing material began in Greece (and papyrus was employed in the 5th century BC), it is highly probable that for some time after the introduction of the alphabet the characters were incised with a sharp tool on stones or metal far oftener than they were written with a pen. In cutting a hard surface, it is easier to form angles than curves; in writing the reverse is the case; hence the development of writing was from angular letters (\"capitals\") inherited from epigraphic style to rounded ones (\"uncials\"). But only certain letters were affected by this development, in particular E (uncial ε), Σ (c), Ω (ω), and to a lesser extent A (α).\n\nThe earliest Greek papyrus yet discovered is probably that containing the \"Persae\" of Timotheus, which dates from the second half of the 4th century BC and its script has a curiously archaic appearance. E, Σ, and Ω have the capital form, and apart from these test letters the general effect is one of stiffness and angularity. More striking is the hand of the earliest dated papyrus, a contract of 311 BC. Written with more ease and elegance, it shows little trace of any development towards a truly cursive style; the letters are not linked, and though the uncial c is used throughout, E and Ω have the capital forms. A similar impression is made by the few other papyri, chiefly literary, dating from about 300 BC; E may be slightly rounded, Ω approach the uncial form, and the angular Σ occurs as a letter only in the Timotheus papyrus, though it survived longer as a numeral (= 200), but the hands hardly suggest that for at least a century and a half the art of writing on papyrus had been well established. Yet before the middle of the 3rd century BC, one finds both a practised book-hand and a developed and often remarkably handsome cursive.\n\nThese facts may be due to accident, the few early papyri happening to represent an archaic style which had survived along with a more advanced one; but it is likely that there was a rapid development at this period, due partly to the opening of Egypt, with its supplies of papyri, and still more to the establishment of the great Alexandrian Library, which systematically copied literary and scientific works, and to the multifarious activities of Hellenistic bureaucracy. From here onward, the two types of script were sufficiently distinct (though each influenced the other) to require separate treatment. Some literary papyri, like the roll containing Aristotle's \"Constitution of Athens\", were written in cursive hands, and, conversely, the book-hand was occasionally used for documents. Since the scribe did not date literary rolls, such papyri are useful in tracing the development of the book-hand.\n\nThe documents of the mid-3rd century BC show a great variety of cursive hands. There are none from chancelleries of the Hellenistic monarchs, but some letters, notably those of Apollonius, the finance minister of Ptolemy II, to this agent, Zeno, and those of the Palestianian sheikh, Toubias, are in a type of script which cannot be very unlike the Chancery hand of the time, and show the Ptolemaic cursive at its best. These hands have a noble spaciousness and strength, and though the individual letters are by no means uniform in size there is a real unity of style, the general impression being one of breadth and uprightness. H, with the cross-stroke high, Π, Μ, with the middle stroke reduced to a very shallow curve, sometimes approaching a horizontal line, Υ, and Τ, with its cross-bar extending much further to the left than to the right of the up-stroke, Γ and Ν, whose last stroke is prolonged upwards above the line, often curving backwards, are all broad; ε, c, θ and β, which sometimes takes the form of two almost perpendicular strokes joined only at the top, are usually small; ω is rather flat, its second loop reduced to a practically straight line. Partly by the broad flat tops of the larger letters, partly by the insertion of a stroke connecting those (like H, Υ) which are not naturally adapted to linking, the scribes produced the effect of a horizontal line along the top of the writing, from which the letters seem to hang. This feature is indeed a general characteristic of the more formal Ptolemaic script, but it is specially marked in the 3rd century BC.\n\nBesides these hand of Chancery type, there are numerous less elaborate examples of cursive, varying according to the writer's skill and degree of education, and many of them strikingly easy and handsome. In some cursiveness is carried very far, the linking of letters reaching the point of illegibility, and the characters sloping to the right. A is reduced to a mere acute angle (∠), T has the cross-stroke only on the left, ω becomes an almost straight line, H acquires a shape somewhat like h, and the last stroke of N is extended far upwards and at times flattened out until it is little more than a diagonal stroke to the right. The attempt to secure a horizontal line along the top is here abandoned. This style was not due to inexpertness, but to the desire for speed, being used especially in accounts and drafts, and was generally the work of practised writers. How well established the cursive hand had now become is shown in some wax tablets of this period, the writing on which, despite the difference of material, closely resemble the hands of papyri.\n\nDocuments of the late 3rd and early 2nd centuries BC show, perhaps partly by the accident of survival (there is nothing analogous to the Apollonius letters, a loss of breadth and spaciousness. In the more formal types the letters stand rather stiffly upright, often without the linking strokes, and are more uniform in size; in the more cursive they are apt to be packed closely together. These features are more marked in the hands of the 2nd century. The less cursive often show am approximation to the book-hand, the letters growing rounder and less angular than in the 3rd century; in the more cursive linking was carried further, both by the insertion of coupling strokes and by the writing of several letters continuously without raising the pen, so that before the end of the century an almost current hand was evolved. A characteristic letter, which survived into the early Roman period, is T, with its cross-stroke made in two portions (variants:). In the 1st century, the hand tended, so far as can be inferred from surviving examples, to disintegrate; one can recognise the signs which portend a change of style, irregularity, want of direction, and the loss of the feeling for style. A fortunate accident has preserved two Greek parchments written in Parthia, one dated 88 BC, in a practically unligatured hand, the other, 22/21 BC, in a very cursive script of Ptolemaic type; and though each has non-Egyptian features the general character indicates a uniformity of style in the Hellenistic world.\n\nThe development of the Ptolemaic book-hand is difficult to trace, as there are few examples, mostly not datable on external grounds. Only for the 3rd century BC have we a secure basis. The hands of that period have an angular appearance; there is little uniformity in the size of individual letters, and though sometimes, notably in the Petrie papyrus containing the \"Phaedo\" of Plato, a style of considerable delicacy is attained, the book-hand in general shows less mastery than the contemporary cursive. In the 2nd century the letters grew rounder and more uniform in size, but in the 1st century there is perceptible, here as in the cursive hand, a certain disintegration. Probably at no time did the Ptolemaic book-hand acquire such unity of stylistic effect as the cursive.\n\nPapyri of the Roman period are far more numerous and show greater variety. The cursive of the 1st century has a rather broken appearance, part of one character being often made separately from the rest and linked to the next letter. A form characteristic of the 1st and 2nd century and surviving after that only as a fraction sign (=⅛) is η in the shape . By the end of the 1st century, there had been developed several excellent types of cursive, which, though differing considerably both in the forms of individual letters and in general appearance, bear a family likeness to one another. Qualities which are specially noticeable are roundness in the shape of letters, continuity of formation, the pen being carried on from character to character, and regularity, the letters not differing strikingly in size and projecting strokes above or below the line being avoided. Sometimes, especially in tax-receipts and in stereotyped formulae, cursiveness is carried to an extreme. In a letter of the prefect, dated in 209, we have a fine example of the Chancery hand, with tall and laterally compressed letters, ο very narrow and α and ω often written high in the line. This style, from at least the latter part of the 2nd century, exercised considerable influence on the local hands, many of which show the same characteristics less pronounced; and its effects may be traced into the early part of the 4th century. Hands of the 3rd century uninfluenced by it show a falling off from the perfection of the 2nd century; stylistic uncertainty and a growing coarseness of execution mark a period of decline and transition.\n\nSeveral different types of book-hand were used in the Roman period. Particularly handsome is a round, upright hand seen, for example, in a British Museum papyrus containing \"Odyssey\" III. The cross-stroke of ε is high, Μ deeply curved and Α has the form α. Uniformity of size is well attained, and a few strokes project, and these but slightly, above or below the line. Another type, well called by palaeographer Schubart the \"severe\" style, has a more angular appearance and not infrequently slopes to the right; though handsome, it has not the sumptuous appearance of the former. There are various classes of a less pretentious style, in which convenience rather than beauty was the first consideration and no pains were taken to avoid irregularities in the shape and alignment of the letters. Lastly may be mentioned a hand which is of great interest as being the ancestor of the type called (from its later occurrence in vellum codices of the Bible) the biblical hand. This, which can be traced back at least the late 2nd century, has a square, rather heavy appearance; the letters, of uniform size, stand upright, and thick and thin strokes are well distinguished. In the 3rd century the book-hand, like the cursive, appears to have deteriorated in regularity and stylistic accomplishment.\n\nIn the charred rolls found at Herculaneum and dating from about the beginning of our era, are specimens of Greek literary hands from outside Egypt; and a comparison with the Egyptian papyri reveals great similarity in style and shows that conclusions drawn from the henads of Egypt may, with caution, be applied to the development of writing in the Greek world generally.\n\nThe cursive hand of the 4th century shows some uncertainty of character. Side by side with the style founded on the Chancery hand, regular in formation and with tall and narrow letters, which characterised the period of Diocletian, and lasted well into the century, we find many other types mostly marked by a certain looseness and irregularity. A general progress towards a florid and sprawling hand is easily recognisable, but a consistent and deliberate style was hardly evolved before the 5th century, from which unfortunately few dated documents have survived. Byzantine cursive tends to an exuberant hand, in which the long strokes are excessively extended and individual letters often much enlarged. But not a few hands of the 5th and 6th centuries are truly handsome and show considerable technical accomplishment. Both an upright and a sloping type occur and there are many less ornamental hands, but there gradually emerged towards the 7th century two general types, one (especially used in letters and contracts) a current hand, sloping to the right, with long strokes in such characters at τ, ρ, ξ, η (which has the h shape), ι, and κ, and with much linking of letters, and another (frequent in accounts), which shows, at least in essence, most of the forms of the later minuscule. (cf. below.) This is often upright, though a slope to the right is quite common, and sometimes, especially in one or two documents of the early Arab period, it has an almost calligraphic effect.\n\nIn the Byzantine period, the book-hand, which in earlier times had more than once approximated to the contemporary cursive, diverged widely from it.\n\nThe change from papyrus to vellum involved no such modification in the forms of letters as followed that from metal to papyrus. The justification for considering the two materials separately is that after the general adoption of vellum, the Egyptian evidence is first supplemented and later superseded by that of manuscripts from elsewhere, and that during this period the hand most used was one not previously employed for literary purposes.\n\nThe prevailing type of book-hand during what in papyrology is called the Byzantine period, that is, roughly from AD 300 to 650, is known as the biblical hand. It went back to at least the end of the 2nd century and had had originally no special connection with Christian literature. In manuscripts, whether vellum or paper, of the 4th century found in Egypt are met other forms of script, particularly a sloping, rather inelegant hand derived from the literary hand of the 3rd century, which persisted to at least the 5th century; but the three great early codices of the Bible are all written in uncials of the biblical type. In the Vaticanus, placed in the 4th century, the characteristics of the hand are least strongly marked; the letters have the forms characteristic of the type but without the heavy appearance of later manuscripts, and the general impression is one of greater roundness. In the Sinaiticus, which is not much later, the letters are larger and more heavily made; and in the Alexandrinus (5th century) a later development is seen, with emphatic distinction of thick and thin strokes. By the 6th century, alike in vellum and in papyrus manuscripts, the heaviness had become very marked, though the hand still retained, in its best examples, a handsome appearance; but after this it steadily deteriorated, becoming ever more mechanical and artificial. The thick strokes grew heavier; the cross strokes of T and Θ and the base of Δ were furnished with drooping spurs. The hand, which is often singularly ugly, passed through various modifications, now sloping, now upright, though it is not certain that these variations were really successive rather than concurrent. A different type of uncials, derived from the Chancery hand and seen in two papyrus examples of the Festal letters despatched annually by the Patriarch of Alexandria, was occasionally used, the best known example being the Codex Marchalianus (6th or 7th century). A combination of this hand with the other type is also known.\n\nThe uncial hand lingered on, mainly for liturgical manuscripts, where a large and easily legible script was serviceable, as late as the 12th century, but in ordinary use it had long been superseded by a new type of hand, the minuscule, which originated in the 8th century, as an adaptation to literary purposes of the second of the types of Byzantine cursive mentioned above. A first attempt at a calligraphic use of this hand, seen in one or two manuscripts of the 8th or early 9th century, in which it slopes to the right and has a narrow, angular appearance, did not find favour, but by the end of the 9th century a more ornamental type, from which modern Greek script descended, was already established. It has been suggested that it was evolved in the Monastery of Stoudios at Constantinople. In its earliest examples it is upright and exact but lacks flexibility; accents are small, breathings square in formation, and in general only such ligatures are used as involve no change in the shape of letters. The single forms have a general resemblance (with considerable differences in detail) both to the minuscule cursive of late papyri, and to those used in modern Greek type; uncial forms were avoided.\n\nIn the course of the 10th century the hand, without losing its beauty and exactness, gained in freedom. Its finest period was from the 9th to the 12th century, after which it rapidly declined. The development was marked by a tendency\nBut from the first there were several styles, varying from the formal, regular hands characteristic of service books to the informal style, marked by numerous abbreviations, used in manuscripts intended only for a scholar's private use. The more formal hands were exceedingly conservative, and there are few classes of script more difficult to date than the Greek minuscule of this class. In the 10th, 11th and 12th centuries a sloping hand, less dignified than the upright, formal type, but often very handsome, was especially used for manuscripts of the classics.\n\nHands of the 11th century are marked in general (though there are exceptions) by a certain grace and delicacy, exact but easy; those of the 12th by a broad, bold sweep and an increasing freedom, which readily admits uncial forms, ligatures and enlarged letters but has not lost the sense of style and decorative effect. In the 13th and still more in the 14th centuries there was a steady decline; the less formal hands lost their beauty and exactness, becoming ever more disorderly and chaotic in their effect, while formal style imitated the precision of an earlier period without attaining its freedom and naturalness, and often appears singularly lifeless. In the 15th century, especially in the West, where Greek scribes were in request to produce manuscripts of the classical authors, there was a revival, and several manuscripts of this period, though markedly inferior to those of the 11th and 12th centuries, are by no means without beauty.\n\nIn the book-hand of early papyri, neither accents nor breathings were employed. Their use was established by the beginning of the Roman period, but was sporadic in papyri, where they were used as an aid to understanding, and therefore more frequently in poetry than prose, and in lyrical oftener than in other verse. In the cursive of papyri they are practically unknown, as are marks of punctuation. Punctuation was effected in early papyri, literary and documentary, by spaces, reinforced in the book-hand by the paragraphos, a horizontal stroke under the beginning of the line. The coronis, a more elaborate form of this, marked the beginning of lyrics or the principal sections of a longer work. Punctuation marks, the comma, the high, low and middle points, were established in the book-hand by the Roman period; in early Ptolemaic papyri, a double point (:) is found.\n\nIn vellum and paper manuscripts, punctuation marks and accents were regularly used from at least the 8th century, though with some differences from modern practice. At no period down to the invention of printing did Greek scribes consistently separate words. The book-hand of papyri aimed at an unbroken succession of letters, except for distinction of sections; in cursive hands, especially where abbreviations were numerous, some tendency to separate words may be recognised, but in reality it was phrases or groups of letters rather than words which were divided. In the later minuscule word-division is much commoner but never became systematic, accents and breathings serving of themselves to indicate the proper division.\n\nThe view that the art of writing in India developed gradually, as in other areas of the world, by going through the stages of pictographic, ideographic and transitional phases of the phonetic script, which in turn developed into syllabic and alphabetic scripts was challenged by Falk and others in the early 1990s. In the new paradigm, Indian alphabetic writing, called Brāhmī, was discontinuous with earlier, undeciphered, glyphs, and was invented specifically by King Ashoka for application in his royal edicts. In the subcontinent, three scripts like Indus, Kharoṣṭhī and Brāhmī became prevalent. In addition, Greek and Arabic scripts were also added to the Indian context after their penetration in the early centuries of the common era (CE). The decipherment and subsequent development of Indus glyphs is also a matter for continuing research and discussion. After a lapse of a few centuries the Kharoṣṭhī script became obsolete; the Greek script in India went through a similar fate and disappeared. But the Brāhmī and Arabic scripts endured for a much longer period. Moreover, there was a change and development in the Brāhmī script which may be traced in time and space through the Maurya, Kuṣāṇa, Gupta and early medieval periods. The present day Nāgarī script is derived from Brāhmī. The Brāhmī is also the ancestral script of many other Indian scripts, in northern and southern South Asia. Legends and inscriptions in Brāhmī are engraved upon leather, wood, terracotta, ivory, stone, copper, bronze, silver and gold. Arabic got an important place, particularly in the royalty, during the medieval period and it provides rich material for history writing.\n\nMost of the available inscriptions and manuscripts written in the above scripts—in languages like Prākrita, Pāḷi, Saṃskṛta, Apabhraṃśa, Tamil and Persian—have been read and exploited for history writing, but numerous inscriptions preserved in different museums still remain undeciphered for lack of competent palaeographic Indologists, as there is a gradual decline in the subcontinent of such disciplines as palaeography, epigraphy and numismatics. The discipline of ancient Indian scripts and the languages they are written needs new scholars who, by adopting traditional palaeographic methods and modern technology, may decipher, study and transcribe the various types of epigraphs and legends still extant today.\n\nThe language of the earliest written records, that is, the Edicts of Ashoka, is Prakrit. Besides Prakrit, the Ashokan edicts are also written in Greek and Aramaic. Moreover, all the edicts of Ashoka engraved in the Kharoshthi and Brahmi scripts are in the Prakrit language: thus, originally the language employed in the inscriptions was Prakrit, with Sanskrit adopted at a later stage. Past the period of the Maurya Empire, the use of Prakrit continued in inscriptions for a few more centuries. In north India, Prakrit was replaced by Sanskrit by the end of the 3rd century, while this change took place about a century later in south India. Some of the inscriptions though written in Prakrit, were influenced by Sanskrit and vice versa. The epigraphs of the Kushana kings are found in a mixture of Prakrit and Sanskrit, while the Mathura inscriptions of the time of Sodasa, belonging to the first quarter of the 1st century, contain verses in classical Sanskrit. From the 4th century onwards, the Guptas came to power and made Sanskrit flourish by supporting it in language and literature.\n\nIn western India and also in some regions of Andhra Pradesh and Karnataka, Prakrit was used till the 4th century, mostly in the Buddhist writings though in a few contemporary records of the Ikshvakus of Nagarjunakonda, Sanskrit was applied. The inscription of Yajna Sri Satakarni (2nd century) from Amaravati is considered to be the earliest so far. The earlier writings (4th century) of Salankayanas of the Telugu region are in Prakrit, while their later records (belonging to the 5th century) are written in Sanskrit. In the Kannada speaking area, inscriptions belonging to later Satavahanas and Chutus were written in Prakrit. From the 4th century onwards, with the rise of the Guptas, Sanskrit became the predominant language of India and continued to be employed in texts and inscriptions of all parts of India along with the regional languages in the subsequent centuries. The copper-plate charters of the Pallavas, the Cholas and the Pandyas documents are written in both Sanskrit and Tamil. Kannada is used in texts dating from about the 5th century and the Halmidi inscription is considered to be the earliest epigraph written in the Kannada language. Inscriptions in Telugu began to appear from the 6th or 7th century. Malayalam made its beginning in writings from the 15th century onwards.\n\nIn north India, the Brahmi script was used over a vast area; however, Ashokan inscriptions are also found using Kharoshthi, Aramaic and Greek scripts. With the advent of the Saka-Kshatrapas and the Kushanas as political powers in north India, the writing system underwent a definite change due to the use of new writing tools and techniques. Further development of the Brahmi script and perceivable changes in its evolutionary trend can be discerned during the Gupta period: in fact, the Gupta script is considered to be the successor of the Kushana script in north India.\n\nFrom the 6th to about the 10th century of the common era, the inscriptions in north India were written in a script variously named, e.g., Siddhamatrika and Kutila (\"Rañjanā script\"). From the 8th century, Siddhamatrika developed into the Śāradā script in Kashmir and Punjab, into Proto-Bengali or Gaudi in Bengal and Orissa, and into Nagari in other parts of north India. Nāgarī script was used widely in northern India from the 10th century onwards. The use of Nandinagari, a variant of Nagari script, is mostly confined to the Karnataka region.\n\nIn central India, mostly in Madhya Pradesh, the inscriptions of the Vakatakas, and the kings of Sarabhapura and Kosala were written in what are known as \"box-headed\" and \"nail-headed\" characters. It may be noted that the early Kadambas of Karnataka also employed \"nail-headed\" characters in some of their inscriptions. During the 3rd–4th century, the script used in the inscriptions of Ikshvakus of Nagarjunakonda developed a unique style of letter-forms with elongated verticals and artistic flourishes, which did not continue after their rule.\n\nThe earliest attested form of writing in South India is represented by inscriptions found in caves, associated with the Chalukya and Chera dynasties. These are written in variants of what is known as the Cave character, and their script differs from the Northern version in being more angular. Most of the modern scripts of South India have evolved from this script, with the exception of Vatteluttu, the exact origins of which are unknown, and Nandinagari, which is a variant of Devanagari that developed due to later Northern influence. In south India from the 7th century of the common era onwards, a number of inscriptions belonging to the dynasties of Pallava, Chola and Pandya are found. These records are written in three different scripts known as Tamil, Vattezhuttu and Grantha scripts, the last variety being used to write Sanskrit inscriptions. In the Kerala region, the Vattezhuttu script developed into a still more cursive script called Kolezhuthu during the 14th and 15th centuries. At the same time, the modern Malayalam script developed out of the Grantha script. The early form of the Telugu-Kannada script is found in the inscriptions of the early Kadambas of Banavasi and the early Chalukyas of Badami in the west, and Salankayana and the early Eastern Chalukyas in the east who ruled the Kannada and Telugu speaking areas respectively, during the 4th to 7th centuries.\n\nAttention should be drawn at the outset to certain fundamental definitions and principles of the science. The original characters of an alphabet are modified by the material and the implements used. When stone and chisel are discarded for papyrus and reed-pen, the hand encounters less resistance and moves more rapidly. This leads to changes in the size and position of the letters, and then to the joining of letters, and, consequently, to altered shapes. We are thus confronted at an early date with quite distinct types. The majuscule style of writing, based on two parallel lines, ADPL, is opposed to the minuscule, based on a system of four lines, with letters of unequal height, adpl. Another classification, according to the care taken in forming the letters, distinguishes between the set book-hand and the cursive script. The difference in this case is determined by the subject matter of the text; the writing used for books (\"scriptura libraria\") is in all periods quite distinct from that used for letters and documents (\"epistolaris, diplomatica\"). While the set book-hand, in majuscule or minuscule, shows a tendency to stabilise the forms of the letters, the cursive, often carelessly written, is continually changing in the course of years and according to the preferences of the writers.\n\nThis being granted, a summary survey of the morphological history of the Latin alphabet shows the zenith of its modifications at once, for its history is divided into two very unequal periods, the first dominated by majuscule and the second by minuscule writing.\n\nJean Mabillon, a French Benedictine monk, scholar and antiquary, whose work \"De re diplomatica\" was published in 1681, is widely regarded as the founder of the twin disciplines of palaeography and diplomatics. However, the actual term \"palaeography\" was coined (in Latin) by Bernard de Montfaucon, a Benedictine monk, in the title of his \"Palaeographia Graeca\" (1708), which remained a standard work in the specific field of Greek palaeography for more than a century. With their establishment of palaeography, Mabillon and his fellow Benedictines were responding to the Jesuit Daniel Papebroch, who doubted the authenticity of some of the documents which the Benedictines offered as credentials for the authorisation of their monasteries. In the 19th century such scholars as Wilhelm Wattenbach, Leopold Delisle and Ludwig Traube contributed greatly to making palaeography independent from diplomatic. In the 20th century, the 'New French School' of palaeographers, especially Jean Mallon, gave a new direction to the study of scripts by stressing the importance of ductus (the shape and order of the strokes used to compose letters) in studying the historical development of scripts.\n\nThe Latin alphabet first appears in the epigraphic type of majuscule writing, known as capitals. These characters form the main stem from which developed all the branches of Latin writing. On the oldest monuments (the \"inscriptiones bello Hannibalico antiquiores\" of the \"Corpus Inscriptionum Latinarum = CIL\"), it is far from showing the orderly regularity of the later period. Side by side with upright and square characters are angular and sloping forms, sometimes very distorted, which seem to indicate the existence of an early cursive writing from which they would have been borrowed. Certain literary texts clearly allude to such a hand. Later, the characters of the cursive type were progressively eliminated from formal inscriptions, and capital writing reached its perfection in the Augustan Age.\n\nEpigraphists divide the numerous inscriptions of this period into two quite distinct classes: \"tituli\", or formal inscriptions engraved on stone in elegant and regular capitals, and \"acta\", or legal texts, documents, etc., generally engraved on bronze in cramped and careless capitals. Palaeography inherits both these types. Reproduced by scribes on papyrus or parchment, the elegant characters of the inscriptions become the square capitals of the manuscripts, and the \"actuaria\", as the writing of the \"acta\" is called, becomes the rustic capital.\n\nOf the many books written in square capitals, the \"éditions de luxe\" of ancient times, only a few fragments have survived, the most famous being pages from manuscripts of Virgil. The finest examples of rustic capitals, the use of which is attested by papyri of the 1st century, are to be found in manuscripts of Virgil and Terence. Neither of these forms of capital writing offers any difficulty in reading, except that no space is left between the words. Their dates are still uncertain, in spite of attempts to determine them by minute observation.\n\nThe rustic capitals, more practical than the square forms, soon came into general use. This was the standard form of writing, so far as books are concerned, until the 5th century, when it was replaced by a new type, the uncial, which is discussed below.\n\nWhile the set book-hand, in square or rustic capitals, was used for the copying of books, the writing of everyday life, letters and documents of all kinds, was in a cursive form, the oldest examples of which are provided by the graffiti on walls at Pompeii (\"CIL\", iv), a series of waxen tablets, also discovered at Pompeii (\"CIL\", iv, supplement), a similar series found at Verespatak in Transylvania (\"CIL\", iii) and a number of papyri. From a study of a number of documents which exhibit transitional forms, it appears that this cursive was originally simplified capital writing. The evolution was so rapid, however, that at quite an early date the \"scriptura epistolaris\" of the Roman world can no longer be described as capitals. By the 1st century, this kind of writing began to develop the principal characteristics of two new types: the uncial and the minuscule cursive. With the coming into use of writing surfaces which were smooth, or offered little resistance, the unhampered haste of the writer altered the shape, size and position of the letters. In the earliest specimens of writing on wax, plaster or papyrus, there appears a tendency to represent several straight strokes by a single curve. The cursive writing thus foreshadows the specifically uncial forms. The same specimens show great inequality in the height of the letters; the main strokes are prolonged upwards (= b; = d) or downwards (= q; = s). In this direction, the cursive tends to become a minuscule hand.\n\nAlthough the characteristic forms of the uncial type appear to have their origin in the early cursive, the two hands are nevertheless quite distinct. The uncial is a \"libraria\", closely related to the capital writing, from which it differs only in the rounding off of the angles of certain letters, principally . It represents a compromise between the beauty and legibility of the capitals and the rapidity of the cursive, and is clearly an artificial product. It was certainly in existence by the latter part of the 4th century, for a number of manuscripts of that date are written in perfect uncial hands (\"Exempla\", pl. XX). It presently supplanted the capitals and appears in numerous manuscripts which have survived from the 5th, 6th and 7th centuries, when it was at its height. By this time it had become an imitative hand, in which there was generally no room for spontaneous development. It remained noticeably uniform over a long period. It is difficult therefore to date the manuscripts by palaeographical criteria alone. The most that can be done is to classify them by centuries, on the strength of tenuous data. The earliest uncial writing is easily distinguished by its simple and monumental character from the later hands, which become progressively stiff and affected.\n\nIn the ancient cursive writing, from the 1st century onward, there are symptoms of transformation in the form of certain letters, the shape and proportions of which correspond more closely to the definition of minuscule writing than to that of majuscule. Rare and irregular at first, they gradually become more numerous and more constant and by degrees supplant the majuscule forms, so that in the history of the Roman cursive there is no precise boundary between the majuscule and minuscule periods.\n\nThe oldest example of minuscule cursive writing that has been discovered is a letter on papyrus, found in Egypt, dating from the 4th century. This marks a highly important date in the history of Latin writing, for with only one known exception, not yet adequately explained—two fragments of imperial rescripts of the 5th century—the minuscule cursive was consequently the only \"scriptura epistolaris\" of the Roman world. The ensuing succession of documents show a continuous improvement in this form of writing, characterised by the boldness of the strokes and by the elimination of the last lingering majuscule forms. The Ravenna deeds of the 5th and 6th centuries exhibit this hand at its perfection.\n\nAt this period, the minuscule cursive made its appearance as a \"book hand\", first as marginal notes, and later for the complete books themselves. The only difference between the book-hand and that used for documents is that the principal strokes are shorter and the characters thicker. This form of the hand is usually called \"semi-cursive\".\n\nThe fall of the Empire and the establishment of the barbarians within its former boundaries did not interrupt the use of the Roman minuscule cursive hand, which was adopted by the newcomers. But for gaps of over a century in the chronological series of documents which have been preserved, it would be possible to follow the evolution of the Roman cursive into the so-called \"national hands\", forms of minuscule writing which flourished after the barbarian invasions in Italy, France, Spain, England and Ireland, and which are still known as Lombardic, Merovingian, Visigothic, Anglo-Saxon and Irish. These names came into use at a time when the various national hands were believed to have been invented by the peoples who used them, but their connotation is merely geographical. Nevertheless, in spite of a close resemblance which betrays their common origin, these hands are specifically different, perhaps because the Roman cursive was developed by each nation in accordance with its artistic tradition.\n\n\nIn Italy, after the close of the Roman and Byzantine periods, the writing is known as Lombardic, a generic term which comprises several local varieties. These may be classified under four principal types: two for the \"scriptura epistolaris\", the old Italian cursive and the papal chancery hand, or \"littera romana\", and two for the \"libraria\", the old Italian book-hand and Lombardic in the narrow sense, sometimes known as \"Beneventana\" on account of the fact that it flourished in the principality of Benevento.\n\nThe oldest preserved documents written in the old Italian cursive show all the essential characteristics of the Roman cursive of the 6th century. In northern Italy, this hand began in the 9th century to be influenced by a minuscule book-hand which developed, as will be seen later, in the time of Charlemagne; under this influence it gradually disappeared, and ceased to exist in the course of the 12th century. In southern Italy, it persisted far on into the later Middle Ages. The papal chancery hand, a variety of Lombardic peculiar to the vicinity of Rome and principally used in papal documents, is distinguished by the formation of the letters \"a, e, q, t\". It is formal in appearance at first, but is gradually simplified, under the influence of the Carolingian minuscule, which finally prevailed in the bulls of Honorius II (1124–1130). The notaries public in Rome continued to use the papal chancery hand until the beginning of the 13th century. The old Italian book-hand is simply a semi-cursive of the type already described as in use in the 6th century. The principal examples are derived from \"scriptoria\" in northern Italy, where it was displaced by the Carolingian minuscule during the 9th century. In southern Italy, this hand persisted, developing into a calligraphic form of writing, and in the 10th century took on a very artistic angular appearance. The \"Exultet\" rolls provide the finest examples. In the 9th century, it was introduced in Dalmatia by the Benedictine monks and developed there, as in Apulia, on the basis of the archetype, culminating in a rounded \"Beneventana\" known as the \"Bari type\".\n\n\nThe offshoot of the Roman cursive which developed in Gaul under the first dynasty of kings is called Merovingian writing. It is represented by thirty-eight royal diplomas, a number of private charters and the authenticating documents of relics.\n\nThough less than a century intervenes between the Ravenna cursive and the oldest extant Merovingian document (AD 625), there is a great difference in appearance between the two writings. The facile flow of the former is replaced by a cramped style, in which the natural slope to the right gives way to an upright hand, and the letters, instead of being fully outlined, are compressed to such an extent that they modify the shape of other letters. Copyists of books used a cursive similar to that found in documents, except that the strokes are thicker, the forms more regular, and the heads and tails shorter. The Merovingian cursive as used in books underwent simplification in some localities, undoubtedly through the influence of the minuscule book-hand of the period. The two principal centres of this reform were Luxeuil and Corbie.\n\n\nIn Spain, after the Visigothic conquest, the Roman cursive gradually developed special characteristics. Some documents attributed to the 7th century display a transitional hand with straggling and rather uncouth forms. The distinctive features of Visigothic writing, the most noticeable of which is certainly the q-shaped g, did not appear until later, in the book-hand. The book-hand became set at an early date. In the 8th century it appears as a sort of semi-cursive; the earliest example of certain date is ms lxxxix in the Capitular Library in Verona. From the 9th century the calligraphic forms become broader and more rounded until the 11th century, when they become slender and angular. The Visigothic minuscule appears in a cursive form in documents about the middle of the 9th century, and in the course of time grows more intricate and consequently less legible. It soon came into competition with the Carolingian minuscule, which supplanted it as a result of the presence in Spain of French elements such as Cluniac monks and warriors engaged in the campaign against the Moors.\n\nThe Irish and Anglo-Saxon hands, which were not directly derived from the Roman minuscule cursive, will be discussed in a separate sub-section below.\n\nOne by one, the national minuscule cursive hands were replaced by a set minuscule hand which has already been mentioned and its origins may now be traced from the beginning.\n\nThe early cursive was the medium in which the minuscule forms were gradually evolved from the corresponding majuscule forms. Minuscule writing was therefore cursive in its inception. As the minuscule letters made their appearance in the cursive writing of documents, they were adopted and given calligraphic form by the copyists of literary texts, so that the set minuscule alphabet was constituted gradually, letter by letter, following the development of the minuscule cursive. Just as some documents written in the early cursive show a mixture of majuscule and minuscule forms, so certain literary papyri of the 3rd century, and inscriptions on stone of the 4th century yield examples of a mixed set hand, with minuscule forms side by side with capital and uncial letters. The number of minuscule forms increases steadily in texts written in the mixed hand, and especially in marginal notes, until by the end of the 5th century the majuscule forms have almost entirely disappeared in some manuscripts. This quasi-minuscule writing, known as the \"half-uncial\" thus derives from a long line of mixed hands which, in a synoptic chart of Latin scripts, would appear close to the oldest \"librariae\", and between them and the \"epistolaris\" (cursive), from which its characteristic forms were successively derived. It had a considerable influence on the continental \"scriptura libraria\" of the 7th and 8th centuries.\n\nThe half-uncial hand was introduced in Ireland along with Latin culture in the 5th century by priests and laymen from Gaul, fleeing before the barbarian invasions. It was adopted there to the exclusion of the cursive, and soon took on a distinct character. There are two well established classes of Irish writing as early as the 7th century: a large round half-uncial hand, in which certain majuscule forms frequently appear, and a pointed hand, which becomes more cursive and more genuinely minuscule. The latter developed out of the former. One of the distinguishing marks of manuscripts of Irish origin is to be found in the initial letters, which are ornamented by interlacing, animal forms, or a frame of red dots. The most certain evidence, however, is provided by the system of abbreviations and by the combined square and cuneiform appearance of the minuscule at the height of its development. The two types of Irish writing were introduced in the north of Great Britain by the monks, and were soon adopted by the Anglo-Saxons, being so exactly copied that it is sometimes difficult to determine the origin of an example. Gradually, however, the Anglo-Saxon writing developed a distinct style, and even local types, which were superseded after the Norman conquest by the Carolingian minuscule. Through St Columba and his followers, Irish writing spread to the continent, and manuscripts were written in the Irish hand in the monasteries of Bobbio Abbey and St Gall during the 7th and 8th centuries.\n\nJames J. John points out that the disappearance of imperial authority around the end of the 5th century in most of the Latin-speaking half of the Roman Empire does not entail the disappearance of the Latin scripts, but rather introduced conditions that would allow the various provinces of the West gradually to drift apart in their writing habits, a process that began around the 7th century.\n\nPope Gregory I (Gregory the Great, d. 604) was influential in the spread of Christianity to Britain and also sent Queens Theodelinde and Brunhilda, as well as Spanish bishops, copies of manuscripts. Furthermore, he sent the Roman monk Augustine of Canterbury to Britain on a missionary journey, on which Augustine may have brought manuscripts. Although Italy's dominance as a centre of manuscript production began to decline, especially after the Gothic War (535–554) and the invasions by the Lombards, its manuscripts—and more important, the scripts in which they were written—were distributed across Europe.\n\nFrom the 6th through the 8th centuries, a number of so-called 'national hands' were developed throughout the Latin-speaking areas of the former Roman Empire. By the late 6th century Irish scribes had begun transforming Roman scripts into Insular minuscule and majuscule scripts. A series of transformations, for book purposes, of the cursive documentary script that had grown out of the later Roman cursive would get under way in France by the mid-7th century. In Spain half-uncial and cursive would both be transformed into a new script, the Visigothic minuscule, no later than the early 8th century.\n\nBeginning in the 8th century, as Charlemagne began to consolidate power over a large area of western Europe, scribes developed a minuscule script (Caroline minuscule) that effectively became the standard script for manuscripts from the 9th to the 11th centuries. The origin of this hand is much disputed. This is due to the confusion which prevailed before the Carolingian period in the \"libraria\" in France, Italy and Germany as a result of the competition between the cursive and the set hands. In addition to the calligraphic uncial and half-uncial writings, which were imitative forms, little used and consequently without much vitality, and the minuscule cursive, which was the most natural hand, there were innumerable varieties of mixed writing derived from the influence of these hands on each other. In some, the uncial or half-uncial forms were preserved with little or no modification, but the influence of the cursive is shown by the freedom of the strokes; these are known as rustic, semi-cursive or cursive uncial or half-uncial hands. Conversely, the cursive was sometimes affected, in varying degrees, by the set \"librariae\"; the cursive of the \"epistolaris\" became a semi-cursive when adopted as a \"libraria\". Nor is this all. Apart from these reciprocal influences affecting the movement of the hand across the page, there were morphological influences at work, letters being borrowed from one alphabet for another. This led to compromises of all softs and of infinite variety between the uncial and half-uncial and the cursive. It will readily be understood that the origin of the Carolingian minuscule, which must be sought in this tangle of pre-Carolingian hands, involves disagreement. The new writing is admittedly much more closely related to the \"epistolaris\" than the primitive minuscule; this is shown by certain forms, such as the open a (), which recall the cursive, by the joining of certain letters, and by the clubbing of the tall letters b d h l, which resulted from a cursive \"ductus\". Most palaeographers agree in assigning the new hand the place shown in the following table:\n\nControversy turns on the question whether the Carolingian minuscule is the primitive minuscule as modified by the influence of the cursive or a cursive based on the primitive minuscule. Its place of origin is also uncertain: Rome, the Palatine school, Tours, Reims, Metz, Saint-Denis and Corbie have been suggested, but no agreement has been reached. In any case, the appearance of the new hand is a turning point in the history of culture. So far as Latin writing is concerned, it marks the dawn of modern times.\n\nIn the 12th century, Carolingian minuscule underwent a change in its appearance and adopted bold and broken Gothic letter-forms. This style remained predominant, with some regional variants, until the 15th century, when the Renaissance humanistic scripts revived a version of Carolingian minuscule. It then spread from the Italian Renaissance all over Europe.\n\nThese humanistic scripts are the base for the antiqua and the handwriting forms in western and southern Europe. In Germany and Austria, the \"Kurrentschrift\" was rooted in the cursive handwriting of the later Middle Ages. With the name of the calligrapher Ludwig Sütterlin, this handwriting counterpart to the blackletter typefaces was abolished by Hitler in 1941. After World War II, it was taught as an alternative script in some areas until the 1970s; it is no longer taught. Secretary hand is an informal business hand of the Renaissance.\n\nThere are undeniable points of contact between architecture and palaeography, and in both it is possible to distinguish a Romanesque and a Gothic period . The creative effort which began in the post-Carolingian period culminated at the beginning of the 12th century in a calligraphy and an architecture which, though still somewhat awkward, showed unmistakable signs of power and experience, and at the end of that century and in the first half of the 13th both arts reached their climax and made their boldest flights. The topography of later medieval writing is still being studied; national varieties can, of course, be' identified but the problem of distinguishing features becomes complicated as a result of the development of international relations, and the migration of clerks from one end of Europe to the other. During the later centuries of the Middle Ages the Gothic minuscule continued to improve within the restricted circle of \"de luxe\" editions and ceremonial documents. In common use, it degenerated into a cursive which became more and more intricate, full of superfluous strokes and complicated by abbreviations. In the first quarter of the 15th century an innovation took place which exercised a decisive influence on the evolution of writing in Europe. The Italian humanists were struck by the eminent legibility of the manuscripts, written in the improved Carolingian minuscule of the 10th and 11th centuries, in which they discovered the works of ancient authors, and carefully imitated the old writing. In Petrarch's compact book hand, the wider leading and reduced compression and round curves are early manifestations of the reaction against the crabbed Gothic secretarial minuscule we know today as \"blackletter\"; Petrarch was one of the few medieval authors to have written at any length on the handwriting of his time; in his essay on the subject, \"La scrittura\" he criticized the current scholastic hand, with its laboured strokes (\"artificiosis litterarum tractibus\") and exuberant (\"luxurians\") letter-forms amusing the eye from a distance, but fatiguing on closer exposure, as if written for other purpose than to be read. For Petrarch the gothic hand violated three principles: writing, he said, should be simple (\"castigata\"), clear (\"clara\") and orthographically correct. Boccaccio was a great admirer of Petrarch; from Boccaccio's immediate circle this post-Petrarchan \"semi-gothic\" revised hand spread to \"literati\" in Florence, Lombardy and the Veneto. A more thorough reform of handwriting than the Petrarchan compromise was in the offing. The generator of the new style (\"illustration\") was Poggio Bracciolini, a tireless pursuer of ancient manuscripts, who developed the new humanist script in the first decade of the 15th century. The Florentine bookseller Vespasiano da Bisticci recalled later in the century that Poggio had been a very fine calligrapher of \"lettera antica\" and had transcribed texts to support himself—presumably, as Martin Davies points out— before he went to Rome in 1403 to begin his career in the papal curia. Berthold Ullman identifies the watershed moment in the development of the new humanistic hand as the youthful Poggio's transcription of Cicero's \"Epistles to Atticus\". By the time the Medici library was catalogued in 1418, almost half the manuscripts were noted as in the \"lettera antica\". The new script was embraced and developed by the Florentine humanists and educators Niccolò de' Niccoli and Coluccio Salutati. The papal chancery adopted the new fashion for some purposes, and thus contributed to its diffusion throughout Christendom. The printers played a still more significant part in establishing this form of writing by using it, from the year 1465, as the basis for their types. The humanistic minuscule soon gave rise to a sloping cursive hand, known as the Italian, which was also taken up by printers in search of novelty and thus became the italic type. In consequence, the Italian hand became widely used, and in the 16th century began to compete with the Gothic cursive. In the 17th century, writing masters were divided between the two schools, and there was in addition a whole series of compromises. The Gothic characters gradually disappeared, except a few that survived in Germany. The Italian became universally used, brought to perfection in more recent times by English calligraphers.\n\n"}
{"id": "230752", "url": "https://en.wikipedia.org/wiki?curid=230752", "title": "Paragraph", "text": "Paragraph\n\nA paragraph (from the Ancient Greek παράγραφος \"paragraphos\", \"\"to write beside\" or \"written beside\"\") is a self-contained unit of a discourse in writing dealing with a particular point or idea. A paragraph consists of one or more sentences. Though not required by the syntax of any language, paragraphs are usually an expected part of formal writing, used to organize longer prose.language\n\nThe oldest classical Greek and Latin writing had little or no space between words and could be written in boustrophedon (alternating directions). Over time, text direction (left to right) became standardized, and word dividers and terminal punctuation became common. The first way to divide sentences into groups was the original \"paragraphos\", similar to an underscore at the beginning of the new group. The Greek \"paragraphos\" evolved into the pilcrow (¶), which in English manuscripts in the Middle Ages can be seen inserted inline between sentences. The hedera leaf (e.g. ☙) has also been used in the same way.\nIn ancient manuscripts, another means to divide sentences into paragraphs was a line break (newline) followed by an initial at the beginning of the next paragraph. An initial is an oversized capital letter, sometimes outdented beyond the margin of the text. This style can be seen, for example, in the original Old English manuscript of Beowulf. Outdenting is still used in English typography, though not commonly.\nModern English typography usually indicates a new paragraph by indenting the first line. This style can be seen in the (handwritten) United States Constitution from 1787. For additional ornamentation, a hedera leaf or other symbol can be added to the inter-paragraph whitespace, or put in the indentation space.\n\nA second common modern English style is to use no indenting, but add vertical white space to create \"block paragraphs.\" On a typewriter, a double carriage return produces a blank line for this purpose; professional typesetters (or word processing software) may put in an arbitrary vertical space by adjusting leading. This style is very common in electronic formats, such as on the World Wide Web and email.\n\nWidows and orphans occur when the first line of a paragraph is the last line in a column or page, or when the last line of a paragraph is the first line of a new column or page.\n\nProfessionally printed material in English typically does not indent the first paragraph, but indents those that follow. For example, Robert Bringhurst states that we should \"Set opening paragraphs flush left.\" Bringhurst explains as follows:\n\nThe function of a paragraph is to mark a pause, setting the paragraph apart from what precedes it. If a paragraph is preceded by a title or subhead, the indent is superfluous and can therefore be omitted.\n\n\"The Elements of Typographic Style\" states that \"at least one en [space]\" should be used to indent paragraphs after the first, noting that that is the \"practical minimum\". An em space is the most commonly used paragraph indent. Miles Tinker, in his book \"Legibility of Print\", concluded that indenting the first line of paragraphs increases readability by 7%, on the average.\n\nIn word processing and desktop publishing, a hard return or paragraph break indicates a new paragraph, to be distinguished from the soft return at the end of a line internal to a paragraph.\nThis distinction allows word wrap to automatically re-flow text as it is edited, without losing paragraph breaks. The software may apply vertical whitespace or indenting at paragraph breaks, depending on the selected style.\n\nHow such documents are actually stored depends on the file format. For example, HTML uses the <p> tag as a paragraph container. In plaintext files, there are two common formats. Pre-formatted text will have a newline at the end of every physical line, and two newlines at the end of a paragraph, creating a blank line. An alternative is to only put newlines at the end of each paragraph, and leave word wrapping up to the application that displays or processes the text.\n\nA line break that is inserted manually, and preserved when re-flowing, may still be distinct from a paragraph break, although this is typically not done in prose. HTML's <br /> tag produces a line break without ending the paragraph; the W3C recommends using it only to separate lines of verse (where each \"paragraph\" is a stanza), or in a street address.\n\nParagraphs are commonly numbered using the decimal system, where (in books) the integral part of the decimal represents the number of the chapter and the fractional parts are arranged in each chapter in order of magnitude. Thus in Whittaker and Watson's 1921 \"A Course of Modern Analysis\", chapter 9 is devoted to Fourier Series; within that chapter §9.6 introduces Riemann's theory, the following section §9.61 treats an associated function, following §9.62 some properties of that function, following §9.621 a related lemma, while §9.63 introduces Riemann's main theorem, and so on. Whittaker and Watson attribute this system of numbering to Giuseppe Peano on their \"Contents\" page, although this attribution does not seem to be widely credited elsewhere.\n\nMany published books use a device to separate certain paragraphs further when there is a change of scene or time. This extra space, especially when co-occurring at a page or section break, may contain an asterisk, three asterisks, a special stylistic dingbat, or a special symbol known as an asterism.\n\nA common English usage misconception is that a paragraph has three to five sentences; single-word paragraphs can be seen in some professional writing, and journalists often use single-sentence paragraphs.\n\nThe crafting of clear, coherent paragraphs is the subject of considerable stylistic debate. Forms generally vary among types of writing. For example, newspapers, scientific journals, and fictional essays have somewhat different conventions for the placement of paragraph breaks.\n\nEnglish students are sometimes taught that a paragraph should have a topic sentence or \"main idea\", preferably first, and multiple \"supporting\" or \"detail\" sentences which explain or supply evidence. One technique of this type, intended for essay writing, is known as the Schaffer paragraph. For example, the following excerpt from Dr. Samuel Johnson's \"Lives of the English Poets\", the first sentence is the main idea: that Joseph Addison is a skilled \"describer of life and manners\". The succeeding sentences are details that support and explain the main idea in a specific way.\n\nThis advice differs from stock advice for the construction of paragraphs in Japanese (translated as \"danraku\" 段落).\n\n\n"}
{"id": "41602973", "url": "https://en.wikipedia.org/wiki?curid=41602973", "title": "Parinama-vada (Hindu thought)", "text": "Parinama-vada (Hindu thought)\n\nPariṇāma-vāda (), or theTransformation theory is that which pre-supposes the cause to be continually transforming itself into its effects, and it has three variations – the Satkarya-vada of the Samkhyas, the Prakrti Parinama-vada of the Saiva Siddhanta and the Brahma-Parinama-vada of the Vishishtadvaita Vedanta School of Thought.\n\nIn Indian philosophy, there are basically three major cosmological theories of origination – 1) Arambha-vada (the theory of atomic agglomeration, based on the theory of Asatkarya-vada that the effect, which is something newly produced, does not exist in the cause), 2) Parinama-vada (the theory of real transformation, based on satkarya-vada that the effect, though phenomenally different, is substantially identical with the cause, and pre-exists latently in it), and 3) Vivartavada (the theory of apparent transformation or of false appearance). There is also the fourth, Pratityasamutpada-vada, the theory of dependent origination of Buddhism.\n\nAccording to the Sat-Karya-vada of the Samkhya School of Vedanta, also accepted by the Vishishtadvaitavada Vedanta, causation is the manifestation of what is in the latent condition in the cause. The Prakrti Parnama-vada is based on the premise that the world is a transformation of the primordial Nature or Prakrti. According to the Brahman Parinama-vada, the world is a transformation of Brahman.\nParināma-vāda is the term that refers to the Monotheistic Schools’ theory of ‘actual transformation’ different from Vivartavada, the Monistic Schools’ theory of apparent transformation. It is the theory that the effect is a real transformation of the cause. According to the Brahman-parinama, this universe is a real transformation of Brahman.\n\nThe Arambha-vada theory of causation is advocated by the Nyaya School, which is the creationistic view of causation and implies new creation as the effect that puts an end to its antecedent nonexistence and marks a new beginning. According to this school the effect, being the counter-entity of its prior nonexistence, must be held to be nonexistent before its appearance as an effect although it arose out of a previously existent cause. This theory is the reverse of Parinama-vada.\n\nThe general Vedanta view is that Brahman is both, the Material and the Efficient, cause of the entire universe. There is nothing outside the Omnipresent Brahman. Brahman is the only being which contains the elements of cit and a-cit which are transformed into the forms of individual souls and material objects. There is no external world of souls and matter produced out of external material causes, and the very concept of Pradhana or Primal Matter, outside Brahman, involves contradiction.\n\nAccording to this philosophy, which follows from Sat-Karya-vada, the cause first, potentially contains the effect in it as its Shakti (power), in an un-manifest way; then through the instrumentality of the efficient cause, that potential, latent, un-manifest effect is made actual, patent and manifest. Creation is not a new beginning but the manifestation of the already present un-manifest. The world, as the effect arisen from the pure cause, cannot be impure and imperfect because Brahman, the pure essence, merely transforms itself and does not change, and therefore, remains the same always, whereas the effects are mere names, due to words, for knowing and identifying the effects.\n\nPrakrti is orderly. The Ṛta (order) that makes Prakrti appear to be composed of sub-systems arranged hierarchically with each sub-system being progressively inclusive, co-ordinating and interdependent is traditionally held to be the main basis of the doctrine of pre-existent effect or Sat-Karya-vada or the doctrine of real transformation or Parinama-vada, which R.A.Sinari states is “the earliest and epistemologically the most valuable attempt made in Indian Philosophy to set up a theory of causal order”. All phenomena, belonging to the surface and/or the deeper structure of Prakrti, are parinama i.e. transformation, of one and the same substratum.\n\nThe Svetasvatara Upanishad says: मायां तु प्रकृतिं – “Know Maya to be Prakrti”. But, both the Samkhya School and the Brahma Sutras base their understandings of the process of transformation for origination of things on the will of the creator. Badarayana by stating – नासतोऽदृष्टत्वात् | (Brahma Sutra 2.2.26), declares that Existence does not come out of non-existence. The entire creation is the result of Brahman’s will – अभिध्योपदेशाच्च | (Brahma Sutra 1.4.24), and that all transmigratory existence has no beginning - उपपद्यते चाप्युलभ्यते च | (Brahma Sutra 2.1.36).\n\nTantra has influenced the Hindu, the Buddhist and the Jain traditions. According to the Srividya and the Saivite texts, the thirty-six tattvas covering the entire range of the un-manifest and manifest world, from the gross to the most subtle known as siva, pure illumination. Parinama-vada called Sakti parinama-vada, along with the doctrine of Abhasavada or Pratibimbavada, explains the relationship between samvit or Tripura and the world; Tripura refers to the totality of the three-folds – sthula (gross), suksma (subtle) and para (transcendent), it represents. According to Abhasa-vada, samvit is like a mirror and the universe is a reflection appearing on it. But the universe cannot be outside the mirror i.e. citi or samvit. According to Parinama-vada, citi (consciousness) manifests in the form of the universe without losing its pristine nature.\n\nGaudapada treats creation as an imaginary event even though it seems to follow a sequential order. Badarayana also states that creation for Brahman is a mere pastime out of his spontaneity without any extraneous motive. But, Gaudapada, who was aware of the concepts of the real and apparent transformations, develops the doctrine of creation as an illusory transformation of Brahman without recourse to ‘vivarta’ terminology. The followers of Advaita School promoted by Adi Sankara, to whom owing to Maya the world appears as if it is real i.e. distortion or false apprehension of the all encompassing unity of Brahman, use this term ‘vivarta’ to support the principle of the immutability of reality. Vidyaranya reminds us – एकमेवाद्वितीयं सन्नामरूपविवर्जितम् | - that before the creation there existed the Reality, one only, without a second, and without name and form (Panchadasi 5.5), this after explaining (in verse 2.59) that with Brahman as its basis, Maya creates the various objects of the world, just as a variety of pictures are drawn on a wall by the use of different colours, in other words, “Maya makes it possible for the imagination to attribute different changes to the unchangeable”. it is, therefore, said that “Maya resembles avidya, the source of common illusions, and described as the principle of cosmic illusion, thus differing from Prakrti of the Samkhyas which is real in the full sense of the term”.\n\nBoth, Parnama-vada and Vivartavada, have their own critics. Madhava rejects Bhaskara on the ground that it is not possible for Brahman to transform at the loss of original nature, and there cannot exist unbridgeable gulf between Cit (Spirit) and Jada (Matter).239 A perfect being of pure intelligence and bliss cannot evolve out of itself an effect that is inert and wholly lacking in intelligence. Ramanuja accepts the material causality of Brahman which is absolutely without personal modification and which has the transforming Prakrti as its body. Vivarta concept is rejected because Brahman is not the constituent cause of Maya, therefore Brahman cannot be the constituent cause of the world through Maya.\n"}
{"id": "11916916", "url": "https://en.wikipedia.org/wiki?curid=11916916", "title": "Phonological Awareness for Literacy", "text": "Phonological Awareness for Literacy\n\nThe Phonological Awareness for Literacy (PAL) Program (Burrows, Allison, Barnett, and Savina, 2007) is a commercial literacy therapy program for use by speech therapists designed to improve phonological awareness skills required for literacy in children aged 8 – 12. It aims to create/strengthen awareness of the relationship between phonological awareness skills to reading and writing.\n\nAdapted from Auditory Discrimination in Depth (Lindamood & Lindamood, 1975), which is now known as the Lindamood Phoneme Sequencing (LiPS) Program.\n\nThe PAL introduces identification, segmentation, blending and manipulation of speech sounds in syllables. It does not encourage reading using the whole-word approach instead teaching children to break written words up into individual graphemes and matching letters with their corresponding phonemes before reassembling the phonemes back into words to read.\n\nDeveloping an awareness of linguistic terms: Checks child's understanding of literacy terminology used and teaches the child how to talk about language (metalinguistic skills).\n\nSound–symbol association: Determines child's knowledge of how letters and sounds correspond, and that can be several representations of each sound.\n\nBlock representation of Consonant or vowel sequences: This component facilitates the child's ability to segment words into individual phonemes through developing auditory analysis skills. A single block represents an individual sound, and a row of blocks represent a string of sounds; so that the number of blocks directly correlates to the number of sounds in the sequence.\n\nBlock representation of syllables: Once the child understands that syllables consist of sounds, they then have to count the number of sounds, the order and distinguish between phonetic features. N.B. all block representation tasks deal only with non-words; this is to prevent the child from using pre-learned spelling patterns to respond to the tasks.\n\nReading and spelling non-words: This builds on previously learnt skills by using block representation to read and spell non-words. The child is encouraged to employ metalinguistic knowledge to describe changes.\n\nReading and spelling real words: Children learn to transfer the aforementioned foundation skills to simple/regular real words, which do not require specific spelling rules.\n\nsimple words\n\ncomplex words\n\nmultisyllabic words\n\nEach level consists of subsections of teaching, auditory analysis, decoding and encoding. Children progress from non-words to real words within each level, prior to commencing the next level. The design of the program ensures that the child is not exposed to more difficult tasks before acquiring the necessary skills at preceding levels.\n\nLevel 1 (simple words): Simple syllables with structures up to a consonant–vowel–consonant (CVC) level. The focus of this level is the ability to decode and encode CVC syllables, before applying this skill in reading and spelling. This incorporates the learning of long and short vowels in addition to consonants. Words used at this level have consistent grapheme-phoneme correspondences (GPCs).\n\nLevel 2 (complex words): Consonant clusters are introduced, in addition to application of specific rules (e.g. borrower c and g, and Magic E).\n\nLevel 3 (multisyllabic words): Specific teaching on how to break words up into syllables and the introduction of grammatical morphemes. This level builds on the previously learned abilities of segmenting and manipulating sounds by transferring these skills to syllables within multisyllabic words.\n\nThe manual\nColoured blocks, letter tiles\nRecord sheets and file/folder\nSuitable reinforcement (stickers, stamps etc.)\n\nTherapy should be delivered by a suitably qualified speech therapist.\n\nReading Freedom Remedial Reading Program (Calder, 1992)\n\nSounds Abound. Listening, Rhyming & Reading (Catts, 1993)\n\nAuditory Discrimination in Depth (Lindamood and Lindamood, 1975)\n\nA Sound Way (Love and Reily, 1995)\n"}
{"id": "477912", "url": "https://en.wikipedia.org/wiki?curid=477912", "title": "Rhema", "text": "Rhema\n\nRhema (ῥῆμα in Greek) literally means an \"utterance\" or \"thing said\" in Greek. It is a word that signifies the action of utterance.\n\nIn philosophy, it was used by both Plato and Aristotle to refer to propositions or sentences.\n\nIn Christianity, it is used in reference to the concept of \"Rhematos Christou\"; Jesus Christ's sayings.\n\nThe Greek noun ῥῆμα \"saying, utterance, word, verb\" is analyzed as consisting of the root ἐρ-/ῥε- (er-/rhe-) \"say\" (cf. εἴρω \"I say\"; ἐρῶ \"I will say\") and the suffix -μα (-ma), a suffix used to form nouns from verbs.\n\nBoth Plato (c. 428–347 BC) and Aristotle (384–322 BC) used the terms \"logos\", \"rhema\" and \"onoma\". In Plato's usage, a logos (often translatable as a \"sentence\") is a sequence in which verbs are mingled with nouns and every logos must have an onoma and rhema. For Plato, every logos was either true or false and in a logos, names included rhema \"which denotes actions\" and onoma a \"mark set on those who do the actions\". Aristotle identified three components as central to the proposition: \"onoma\", \"rhema\" and \"logos\". These terms are translated differently depending on the context of the discussion—grammar or logic, as in the table on the right. But it was only in the 12th century that grammarians began to think in terms of units we understand as \"subject\" and \"predicate\".\n\nThe Septuagint translation of the Hebrew Bible into Greek uses the terms \"rhema\" and \"logos\" as equivalents and uses both for the Hebrew word \"dabar\", as the Word of God.\n\nIn Christianity, \"rhema\" is used in Bible study to signify Jesus Christ's utterances. The Greek word \"rhema\" is useful to distinguish between two meanings of \"word\". While both \"rhema\" and \"logos\" are translated into the English \"word\", in the original Greek there was a substantial distinction.\n\nSome modern usage distinguishes \"rhema\" from \"logos\" in Christian theology, with \"rhema\" at times called \"spoken word\", referring to the revelation received by disciples when the Holy Spirit \"speaks\" to them. In this usage, \"Logos\" refers to Christ.\n"}
{"id": "909888", "url": "https://en.wikipedia.org/wiki?curid=909888", "title": "Semantics of logic", "text": "Semantics of logic\n\nIn logic, the semantics of logic is the study of the semantics, or interpretations, of formal and (idealizations of) natural languages usually trying to capture the pre-theoretic notion of entailment.\n\nThe truth conditions of various sentences we may encounter in arguments will depend upon their meaning, and so logicians cannot completely avoid the need to provide some treatment of the meaning of these sentences. The semantics of logic refers to the approaches that logicians have introduced to understand and determine that part of meaning in which they are interested; the logician traditionally is not interested in the sentence as uttered but in the proposition, an idealised sentence suitable for logical manipulation.\n\nUntil the advent of modern logic, Aristotle's \"Organon\", especially \"De Interpretatione\", provided the basis for understanding the significance of logic. The introduction of quantification, needed to solve the problem of multiple generality, rendered impossible the kind of subject-predicate analysis that governed Aristotle's account, although there is a renewed interest in term logic, attempting to find calculi in the spirit of Aristotle's syllogistic but with the generality of modern logics based on the quantifier.\n\nThe main modern approaches to semantics for formal languages are the following:\n\n\n"}
{"id": "44991176", "url": "https://en.wikipedia.org/wiki?curid=44991176", "title": "Serbian language in Croatia", "text": "Serbian language in Croatia\n\nThe Serbian language is one of the officially recognized minority languages in Croatia. It is primarily used by the Serbs of Croatia. The Croatian Constitution, Croatian Constitutional law on national minorities rights, Law on Education in language and script of national minorities and Law on Use of Languages and Scripts of National Minorities define the public co-official usage of Serbian in Croatia. Serbian and Croatian are two standardized varieties of the pluricentric Serbo-Croatian language. Majority of Serbs of Croatia use Ijekavian pronunciation of Proto-Slavic vowel jat except in the Podunavlje region in Vukovar-Srijem and Osijek-Baranja Counties where local Serb population use Ekavian pronunciation. Post-World War II and Croatian War of Independence settlers in Podunavlje which have come from Bosnia, Dalmatia or Western Slavonia either use their original Ijekavian pronunciation, adopted Ekavian pronunciation or both of them depending on context. In 2011 Census majority of Serbs of Croatia declared Croatian standardized variety as their first language with Ijekavian pronunciation always being required standard form in Croatian. While Serbian variety recognizes both pronunciations as standard Ekavian is the more common one as it is the dominant one in Serbia, with Ijekavian being mostly limited to Republika Srpska in Bosnia, Montenegro and Croatia.\n\nThe Orthodox liturgical book Varaždin Apostol from 1454 represents the oldest preserved text in Cyrillic from the territory of today's Croatia. Croatian Constitutional law on national minorities rights, one of only two constitutional laws in country, entered into force on 23 December 2002.\n\nIn April 2015 the United Nations Human Rights Committee urged the Croatian government to ensure the right of minorities to use their language and alphabet. The report noted the use of Serbian Cyrillic in Vukovar and municipalities concerned. Serbian Foreign Minister Ivica Dačić said that his country welcomes the UN Human Rights Committee's report.\n\nMost schools with instruction in Serbian are located in Vukovar-Srijem and Osijek-Baranja County in the area of former Eastern Slavonia, Baranja and Western Syrmia where rights on education in minority languages were provided during the United Nations Transitional Administration for Eastern Slavonia, Baranja and Western Sirmium based on the Erdut Agreement. Today with those schools there is also Kantakuzina Katarina Branković Serbian Orthodox Secondary School in Zagreb.\n\nIn the school year 2010-2011, 3.742 students attended kindergartens, primary and secondary schools in Serbian. 59 educational institutions offered Serbian language education that year and 561 educators and teachers worked in them. In school year 2011-2012 the total number of students was 4.059 in 63 educational institutions and 563 educators and teachers worked in them. Number of classes or groups in this period increased from 322 to 353.\n\nAs a chair at the Department of South Slavic languages, Faculty of Humanities and Social Sciences at the University of Zagreb has a The Chair of Serbian and Montenegrin literature. Among the others, lecturers of Serbian literature at the university over the time included Antun Barac, Đuro Šurmin and Armin Pavić.\n\nVarious minority organizations use Serbian in their work. One of them, Association for Serbian language and literature in Croatia from Vukovar is a nonprofit professional organization that brings together scientists and technical workers in the Republic of Croatia engaged in studying and teaching of Serbian language and literature.\n\nThe Law on Use of Languages and Scripts of National Minorities provides for a mandatory co-official use of minority languages in municipalities of Croatia with at least one third of members of ethnic minoritiy. Municipalities Dvor, Gvozd, Jagodnjak, Šodolovci, Borovo, Trpinja, Markušica, Negoslavci, Biskupija, Ervenik, Kistanje, Gračac, Udbina, Vrbovsko, Donji Kukuruzari, Erdut and Vukovar, according to the provisions of law, are obliged to grant equal co-official use of Serbian language and Serbian Cyrillic alphabet. Law enforcement is facing great resistance in the part of the majority population, most notably in the case of Vukovar where it led to 2013 Anti-Cyrillic protests in Croatia.\n\n"}
{"id": "246918", "url": "https://en.wikipedia.org/wiki?curid=246918", "title": "Singular term", "text": "Singular term\n\nA singular term is a paradigmatic referring device in a language. Singular terms are of philosophical importance for philosophers of language, because they \"refer\" to things in the world, and the ability of words to refer calls for scrutiny.\n\nSingular terms are defined as expressions that purport to denote or designate particular individual people, places, or other objects. They contrast with \"general terms\" (such as \"car\" or \"chair\") which can apply to more than one thing. \n\nThere are various kinds of singular terms: proper names (eg. \"Matthew\"), definite descriptions (eg. \"the second fisherman in the boat\"), singular personal pronouns (eg. \"she\"), demonstrative pronouns (eg. \"this\"), etc.\n\nHistorically, various definitions for \"singular term\" have been offered:\n\n\n"}
{"id": "40758290", "url": "https://en.wikipedia.org/wiki?curid=40758290", "title": "Truth-apt", "text": "Truth-apt\n\nIn philosophy, to say that a statement is truth-apt is to say that it could be uttered in some context (without its meaning being altered) and would then express a true or false proposition. \n\nTruth-apt sentences are capable of being true or false, unlike questions or commands. Whether paradoxical sentences, prescriptions (especially moral claims), or attitudes are truth-apt is sometimes controversial.\n\n"}
{"id": "1313388", "url": "https://en.wikipedia.org/wiki?curid=1313388", "title": "Umbrella term", "text": "Umbrella term\n\nAn umbrella term is a word or phrase that covers a wide range of concepts belonging to a common category. For example, \"cryptology\" is an umbrella term that encompasses cryptography and cryptanalysis, among other fields. Similarly, an umbrella organization is a central and coordinating body representing a number of smaller, separate bodies.\n\nA blanket term is a closely related word or phrase that is used to describe multiple groups of related things. The degree of relation may vary or have a minimal relationship, but blanket terms often trade specificity for ease of use. In other words, a blanket term, by itself, gives little detail about the things that it describes or the relationships between them, but it is easy to say and remember. Blanket terms may originate as slang but eventually become integrated into the general vocabulary.\n\n\n\n\n"}
{"id": "26260610", "url": "https://en.wikipedia.org/wiki?curid=26260610", "title": "Universal grinder", "text": "Universal grinder\n\nIn linguistics, the term \"universal grinder\" refers to an idea that in some languages, most count nouns can be used as if they were mass nouns, which causes a slight change in their meaning. The term \"universal grinder\" was first used in print by F. Jeffry Pelletier in 1975, after a personal suggestion by David Lewis.\n\nRelated concepts, the universal sorter and universal packager refer to similar processes that allow mass nouns to be used as count nouns.\n\nThe idea of the \"universal grinder\" is that while count nouns usually denote whole, distinct objects (such as \"a steak\", \"two steaks\"), the equivalent mass noun connotes a non-distinct quantity of whatever constitutes these objects (\"some steak\"). The universal grinder suggests that most count nouns can be used as mass nouns, and the distinct thing named by the noun will be thought of as a general quantity.\n\nWhile the \"universal grinder\" can be used in many Indo-European languages, most particularly English and Dutch, it does not apply in all languages; there is considerable cross-linguistic variation in the morphology and semantics of the mass/count and singular/plural distinctions. For example, bare count nouns in Mandarin Chinese do not receive mass interpretations: a sentence such as \"Qiáng-shang dōu shì gǒu\" can only be interpreted as 'There are dogs all over the wall,' not as 'There is dog all over the wall.'\n\nThe \"universal sorter\" describes one way in which mass nouns are understood when they are used in the plural. Harry Bunt suggested the universal sorter in his 1981 doctoral dissertation. When an ordinarily uncountable noun such as \"wine\" appears with plural form (\"several wines\"), it can be understood as referring to various abstract kinds (for example, varieties of wine).\n\nThe \"universal packager\" likewise describes how mass nouns are understood when they are used as countable nouns. In this case, the plural noun may be understood as naming individual servings or \"packages\" (e.g. \"two coffees\" may mean \"two cups of coffee\"), or as naming abstract kinds (\"two coffees\" can also refer to e.g. Colombian coffee and Indonesian coffee). Some scholars use the phrase \"universal packager\" to refer to both the concrete and abstract understanding of plural nouns; others use it to refer only to the concrete sense, and use \"universal sorter\" for abstract meaning.\n"}
{"id": "92200", "url": "https://en.wikipedia.org/wiki?curid=92200", "title": "Utterance", "text": "Utterance\n\nIn spoken language analysis, an utterance is the smallest unit of speech. It is a continuous piece of speech beginning and ending with a clear pause. In the case of oral languages, it is generally but not always bounded by silence. Utterances do not exist in written language, only their representations do. They can be represented and delineated in written language in many ways.\n\nIn oral/spoken language utterances have several features including paralinguistic features which are aspects of speech such as facial expression, gesture, and posture. Prosodic features include stress, intonation, and tone of voice, as well as ellipsis, which are words that the listener inserts in spoken language to fill gaps. Moreover, other aspects of utterances found in spoken languages are non-fluency features including: voiced/un-voiced pauses (like \"umm\"), tag questions, and false starts when someone begins their utterances again to correct themselves. Other features include: fillers (\"and stuff\"); accent/dialect; deictic expressions, which are utterances like \"over there!\" which need further explanation to be understood; simple conjunctions (\"and,\" \"but,\" etc.); and colloquial lexis which are everyday informal words.\n\nUtterances that are portrayed in writing are planned, in contrast to utterances in improvised spoken language. In written language there are frameworks that are used to portray this type of language. Discourse structure (which can also be found in spoken language) is how the conversation is organized, in which adjacency pairs - an utterance and the answer to that utterance - are used. Discourse markers are used to organize conversation (\"first,\" \"secondly,\" etc.). Lexis denotes the words being used in a text or spoken; these words can create a semantic field. For example, a semantic field of love can be created with lexical choices such as adore, admire, and care. Grammar/syntax is another feature of language in general but also utterances, and pragmatics means that when utterances are spoken or written the meaning is not literal, as in sarcasm.\n\nAn utterance which is found in spoken and written language as in a script has several characteristics. These include paralinguistic features which is a feature of communication that doesn't involve words but is added around an utterance to give meaning. Examples of paralinguistic features include facial expressions, laughter, eye contact, and gestures. Prosodic features refer to the sound of someone's voice as they speak: pitch, intonation and stress. Ellipsis can be used in either written or spoken language, when an utterance is conveyed and the speaker omits words because they are already understood in the situation. For example: A: Juice? B: Please. A: Room temperature? B: Cold.\n\nNon-fluency features also occur when producing utterances. As people think about what to say to while speaking, there are errors and corrections in speech. For example, voiced/un-voiced pauses which are \"umm,\" \"erm,\" etc. in voiced pauses and in transcripts un-voiced pauses are denoted as (.) or (1) relating to the amount of time of the pause. Tag questions are also a part of non-fluency features; these are used by the speaker to check if the listener understands what the speaker is saying. An example is \"Do you know what I mean?\" False alerts occur when the speaker is voicing an utterance but stops and starts again, usually to correct themselves.\n\nFillers usually give the speaker time to think and gather their thoughts in order to continue their utterance; these include lexis such as, \"like,\" \"and stuff,\" Accent/dialect is also a characteristic included in utterances which is the way the words are voiced, the pronunciation and the different types of lexis used in different parts of the world. Deictic expressions are utterances that need more explanation in order to be understood, like: \"Wow! Look over there!\" Simple conjunctions in speech are words that connect other words like \"and,\" \"but,\" etc. Colloquial lexis is a type of speech that is casual in which the utterance is usually more relaxed.\n\nThe development of utterances in children is facilitated by parents, adults, or any other guardian the child has growing up. Studies have indicated that this development of utterances is affected by the parent, adult, or guardian's socioeconomic status (SES). It has been shown that children have larger vocabularies and learn new words more quickly during early childhood from parents with a high education and higher SES status, while children with less educated parents and lower SES status have a smaller vocabulary and a slower growth in their vocabulary skills (Arriaga, Fenson, Cronan & Pethick, 1998; Hart & Risley, 1995; Hoff, Laursen & Tardif, 2002; Hoff-Ginsberg, 1991; Lawrence & Shipley, 1996; Ninio, 1980). This correlation is due to the fact that more educated parents use more lexis when speaking to their children as opposed to parents that are less educated (Hart & Risley, 1995; Hoff, 2003 a; Huttenlocher, Vasilyeva, Waterfall, Vevea & Hedges, in press). Hoff conducted an analysis that shows support for this correlation in 2003 which shows that the mean length of utterance and vocabulary of mothers who talk to their children is related to their SES status and thus child vocabulary development. High-SES mothers use longer utterances when talking to their children and a wider variety of words. They also spend more time talking to their children. Low-SES mothers use shorter utterances and a smaller vocabulary. As a result, children with more educated parents have larger vocabularies (Hoff, 2003).\n\nIn child-directed speech, utterances have several additional features. For example, the phonology in child-directed speech is different: Utterances are spoken more slowly, with longer pauses in between utterances, higher pitches, etc. The lexis and semantics differ, and a speaker uses words suited for children, \"doggie\" instead of \"dog,\" for example. The grammar is simpler, repetitive, with less use of verbs and adjectives. There is a greater use of one word utterances and the pragmatics uses supportive language like expansions and re-casting.\n\nPaul Grice (1989) came up with four maxims necessary in order to have a collegial conversation in which utterances are understood:\n\n\nAccording to philosopher Mikhail Bakhtin, there are four accepted properties that utterances should have:\n\nBakhtin also emphasizes that an utterance and a sentence are not the same thing. According to Bakhtin, sentences do not indicate a change of speech subject, and thus do not automatically satisfy one of the four properties of utterances. According to him, the sentence as a language unit is grammatical in nature, while an utterance is \"ethical\".\n\n\n"}
{"id": "1404732", "url": "https://en.wikipedia.org/wiki?curid=1404732", "title": "Vocabulary development", "text": "Vocabulary development\n\nVocabulary development is a process by which people acquire words. Babbling shifts towards meaningful speech as infants grow and produce their first words around the age of one year. In early word learning, infants build their vocabulary slowly. By the age of 18 months, infants can typically produce about 50 words and begin to make word combinations.\n\nIn order to build their vocabularies, infants must learn about the meanings that words carry. The mapping problem asks how infants correctly learn to attach words to referents. Constraints theories, domain-general views, social-pragmatic accounts, and an emergentist coalition model have been proposed to account for the mapping problem...\n\nFrom an early age, infants use language to communicate. Caregivers and other family members use language to teach children how to act in society. In their interactions with peers, children have the opportunity to learn about unique conversational roles. Through pragmatic directions, adults often offer children cues for understanding the meaning of words.\n\nThroughout their school years, children continue to build their vocabulary. In particular, children begin to learn abstract words. Beginning around age 3–5, word learning takes place both in conversation and through reading. Word learning often involves physical context, builds on prior knowledge, takes place in social context, and includes semantic support. The phonological loop and serial order short-term memory may both play an important role in vocabulary development.\n\nInfants begin to understand words such as \"Mommy\", \"Daddy\", \"hands\" and \"feet\" when they are approximately 6 months old. Initially, these words refer to their own mother or father or hands or feet. Infants begin to produce their first words when they are approximately one year old. Infants' first words are normally used in reference to things that are of importance to them, such as objects, body parts, people, and relevant actions. Also, the first words that infants produce are mostly single-syllabic or repeated single syllables, such as \"no\" and \"dada\". By 12 to 18 months of age, children's vocabularies often contain words such as \"kitty\", \"bottle\", \"doll\", \"car\" and \"eye\". Children's understanding of names for objects and people usually precedes their understanding of words that describe actions and relationships. \"One\" and \"two\" are the first number words that children learn between the ages of one and two. Infants must be able to hear and play with sounds in their environment, and to break up various phonetic units to discover words and their related meanings.\n\nStudies related to vocabulary development show that children's language competence depends upon their ability to hear sounds during infancy. Infants' perception of speech is distinct. Between six and ten months of age, infants can discriminate sounds used in the languages of the world. By 10 to 12 months, infants can no longer discriminate between speech sounds that are not used in the language(s) to which they are exposed. Among six-month-old infants, seen articulations (i.e. the mouth movements they observe others make while talking) actually enhance their ability to discriminate sounds, and may also contribute to infants' ability to learn phonemic boundaries. Infants' phonological register is completed between the ages of 18 months and 7 years.\n\nChildren's phonological development normally proceeds as follows:\n\n6–8 weeks: Cooing appears\n\n16 weeks: Laughter and vocal play appear\n\n6–9 months: Reduplicated (canonical) babbling appears\n\n12 months: First words use a limited sound repertoire\n\n18 months: Phonological processes (deformations of target sounds) become systematic\n\n18 months–7 years: Phonological inventory completion\n\nAt each stage mentioned above, children play with sounds and learn methods to help them learn words. There is a relationship between children's prelinguistic phonetic skills and their lexical progress at age two: failure to develop the required phonetic skills in their prelinguistic period results in children's delay in producing words. Environmental influences may affect children's phonological development, such as hearing loss as a result of ear infections. Deaf infants and children with hearing problems due to infections are usually delayed in the beginning of vocal babbling.\n\nBabbling is an important aspect of vocabulary development in infants, since it appears to help practice producing speech sounds. Babbling begins between five and seven months of age. At this stage, babies start to play with sounds that are not used to express their emotional or physical states, such as sounds of consonants and vowels. Babies begin to babble in real syllables such as \"ba-ba-ba, neh-neh-neh, and dee-dee-dee,\" between the ages of seven and eight months; this is known as canonical babbling. Jargon babbling includes strings of such sounds; this type of babbling uses intonation but doesn't convey meaning. The phonemes and syllabic patterns produced by infants begin to be distinctive to particular languages during this period (e.g., increased nasal sounds in French and Japanese babies) though most of their sounds are similar. There is a shift from babbling to the use of words as the infant grows.\n\nAs children get older their rate of vocabulary growth increases. Children probably understand their first 50 words before they produce them. By the age of eighteen months, children typically attain a vocabulary of 50 words in production, and between two and three times greater in comprehension. A switch from an early stage of slow vocabulary growth to a later stage of faster growth is referred to as the \"vocabulary spurt\". Young toddlers acquire one to three words per month. A vocabulary spurt often occurs over time as the number of words learned accelerates. It is believed that most children add about 10 to 20 new words a week. Between the ages of 18 to 24 months, children learn how to combine two words such as \"no bye-bye\" and \"more please\". Three-word and four-word combinations appear when most of the child's utterances are two-word productions. In addition, children are able to form conjoined sentences, using \"and\". This suggests that there is a vocabulary spurt between the time that the child's first word appears, and when the child is able to form more than two words, and eventually, sentences. However, there have been arguments as to whether or not there is a spurt in acquisition of words. In one study of 38 children, only five of the children had an inflection point in their rate of word acquisition as opposed to a quadratic growth. This study suggests that most children do not have a vocabulary spurt.\n\nIn word learning, the mapping problem refers to the question of how infants attach the forms of language to the things that they experience in the world. There are infinite objects, concepts, and actions in the world that words could be mapped onto. Many theories have been proposed to account for the way in which the language learner successfully maps words onto the correct objects, concepts, and actions.\n\nWhile domain-specific accounts of word learning argue for innate constraints that limit infants' hypotheses about word meanings, domain-general perspectives argue that word learning can be accounted for by general cognitive processes, such as learning and memory, which are not specific to language. Yet other theorists have proposed social pragmatic accounts, which stress the role of caregivers in guiding infants through the word learning process. According to some research, however, children are active participants in their own word learning, although caregivers may still play an important role in this process. Recently, an emergentist coalition model has also been proposed to suggest that word learning cannot be fully attributed to a single factor. Instead, a variety of cues, including salient and social cues, may be utilized by infants at different points in their vocabulary development.\n\nTheories of word-learning constraints argue for biases or default assumptions that guide the infant through the word learning process. Constraints are outside of the infant's control and are believed to help the infant limit their hypotheses about the meaning of words that they encounter daily. Constraints can be considered domain-specific (unique to language).\n\nCritics argue that theories of constraints focus on how children learn nouns, but ignore other aspects of their word learning. Although constraints are useful in explaining how children limit possible meanings when learning novel words, the same constraints would eventually need to be overridden because they are not utilized in adult language. For instance, adult speakers often use several terms, each term meaning something slightly different, when referring to one entity, such as a family pet. This practice would violate the mutual exclusivity constraint.\n\nBelow, the most prominent constraints in the literature are detailed:\n\nDomain-general views of vocabulary development argue that children do not need principles or constraints in order to successfully develop word-world mappings. Instead, word learning can be accounted for through general learning mechanisms such as salience, association, and frequency. Children are thought to notice the objects, actions, or events that are most salient in context, and then to associate them with the words that are most frequently used in their presence. Additionally, research on word learning suggests that fast mapping, the rapid learning that children display after a single exposure to new information, is not specific to word learning. Children can also successfully fast map when exposed to a novel fact, remembering both words and facts after a time delay.\n\nDomain-general views have been criticized for not fully explaining how children manage to avoid mapping errors when there are numerous possible referents to which objects, actions, or events might point. For instance, if biases are not present from birth, why do infants assume that labels refer to whole objects, instead of salient parts of these objects? However, domain-general perspectives do not dismiss the notion of biases. Rather, they suggest biases develop through learning strategies instead of existing as built-in constraints. For instance, the whole object bias could be explained as a strategy that humans use to reason about the world; perhaps we are prone to thinking about our environment in terms of whole objects, and this strategy is not specific to the language domain. Additionally, children may be exposed to cues associated with categorization by shape early in the word learning process, which would draw their attention to shape when presented with novel objects and labels. Ordinary learning could, then, lead to a shape bias.\n\nSocial pragmatic theories, also in contrast to the constraints view, focus on the social context in which the infant is embedded. According to this approach, environmental input removes the ambiguity of the word learning situation. Cues such as the caregiver's gaze, body language, gesture, and smile help infants to understand the meanings of words. Social pragmatic theories stress the role of the caregiver in talking about objects, actions, or events that the infant is already focused-in upon.\n\nJoint attention is an important mechanism through which children learn to map words-to-world, and vice versa. Adults commonly make an attempt to establish joint attention with a child before they convey something to the child. Joint attention is often accompanied by physical co-presence, since children are often focused on what is in their immediate environment. As well, conversational co-presence is likely to occur; the caregiver and child typically talk together about whatever is taking place at their locus of joint attention. Social pragmatic perspectives often present children as covariation detectors, who simply associate the words that they hear with whatever they are attending to in the world at the same time. The co-variation detection model of joint attention seems problematic when we consider that many caregiver utterances do not refer to things that occupy the immediate attentional focus of infants. For instance, caregivers among the Kaluli, a group of indigenous peoples living in New Guinea, rarely provide labels in the context of their referents. While the covariation detection model emphasizes the caregiver's role in the meaning-making process, some theorists argue that infants also play an important role in their own word learning, actively avoiding mapping errors. When infants are in situations where their own attentional focus differs from that of a speaker, they seek out information about the speaker's focus, and then use that information to establish correct word-referent mappings. Joint attention can be created through infant agency, in an attempt to gather information about a speaker's intent.\n\nFrom early on, children also assume that language is designed for communication. Infants treat communication as a cooperative process. Specifically, infants observe the principles of conventionality and contrast. According to conventionality, infants believe that for a particular meaning that they wish to convey, there is a term that everyone in the community would expect to be used. According to contrast, infants act according to the notion that differences in form mark differences in meaning. Children's attention to conventionality and contrast is demonstrated in their language use, even before the age of 2 years; they direct their early words towards adult targets, repair mispronunciations quickly if possible, ask for words to relate to the world around them, and maintain contrast in their own word use.\n\nThe emergentist coalition model suggests that children make use of multiple cues to successfully attach a novel label to a novel object. The word learning situation may offer an infant combinations of social, perceptual, cognitive, and linguistic cues. While a range of cues are available from the start of word learning, it may be the case that not all cues are utilized by the infant when they begin the word learning process. While younger children may only be able to detect a limited number of cues, older, more experienced word learners may be able to make use of a range of cues. For instance, young children seem to focus primarily on perceptual salience, but older children attend to the gaze of caregivers and use the focus of caregivers to direct their word mapping. Therefore, this model argues that principles or cues may be present from the onset of word learning, but the use of a wide range of cues develops over time.\n\nSupporters of the emergentist coalition model argue that, as a hybrid, this model moves towards a more holistic explanation of word learning that is not captured by models with a singular focus. For instance, constraints theories typically argue that constraints/principles are available to children from the onset of word learning, but do not explain how children develop into expert speakers who are not limited by constraints. Additionally, some argue that domain-general perspectives do not fully address the question of how children sort through numerous potential referents in order to correctly sort out meaning. Lastly, social pragmatic theories claim that social encounters guide word learning. Although these theories describe how children become more advanced word learners, they seem to tell us little about children's capacities at the start of word learning. According to its proponents, the emergentist coalion model incorporates constraints/principles, but argues for the development and change in these principles over time, while simultaneously taking into consideration social aspects of word learning alongside other cues, such as salience.\n\nBoth linguistic and socio-cultural factors affect the rate at which vocabulary develops. Children must learn to use their words appropriately and strategically in social situations. They have flexible and powerful social-cognitive skills that allow them to understand the communicative intentions of others in a wide variety of interactive situations. Children learn new words in communicative situations. Children rely on pragmatic skills to build more extensive vocabularies. Some aspects of pragmatic behaviour can predict later literacy and mathematical achievement, as children who are pragmatically skilled often function better in school. These children are also generally better liked.\n\nChildren use words differently for objects, spatial relations and actions. Children ages one to three often rely on general purpose deictic words such as \"here\", \"that\" or \"look\" accompanied by a gesture, which is most often pointing, to pick out specific objects. Children also stretch already known or partly known words to cover other objects that appear similar to the original. This can result in word \"overextension\" or misuses of words. Word overextension is governed by the perceptual similarities children notice among the different referents. Misuses of words indirectly provide ways of finding out which meanings children have attached to particular words. When children come into contact with spatial relations, they talk about the location of one object with respect to another. They name the object located and use a deictic term, such as \"here\" or \"there\" for location, or they name both the object located and its location. They can also use a general purpose locative marker, which is a preposition, postposition or suffix depending on the language that is linked in some way to the word for location. Children's earliest words for actions usually encode both the action and its result. Children use a small number of general purpose verbs, such as \"do\" and \"make\" for a large variety of actions because their resources are limited. Children acquiring a second language seem to use the same production strategies for talking about actions. Sometimes children use a highly specific verb instead of a general purpose verb. In both cases children stretch their resources to communicate what they want to say.\n\nInfants use words to communicate early in life and their communication skills develop as they grow older. Communication skills aid in word learning. Infants learn to take turns while communicating with adults. While preschoolers lack precise timing and rely on obvious speaker cues, older children are more precise in their timing and take fewer long pauses. Children get better at initiating and sustaining coherent conversations as they age. Toddlers and preschoolers use strategies such as repeating and recasting their partners' utterances to keep the conversation going. Older children add new relevant information to conversations. Connectives such as \"then\", \"so\", and \"because\" are more frequently used as children get older. When giving and responding to feedback, preschoolers are inconsistent, but around the age of six, children can mark corrections with phrases and head nods to indicate their continued attention. As children continue to age they provide more constructive interpretations back to listeners, which helps prompt conversations.\n\nCaregivers use language to help children become competent members of society and culture. From birth, infants receive pragmatic information. They learn structure of conversations from early interactions with caregivers. Actions and speech are organized in games, such as peekaboo to provide children with information about words and phrases. Caregivers find many ways to help infants interact and respond. As children advance and participate more actively in interactions, caregivers adapt their interactions accordingly. Caregivers also prompt children to produce correct pragmatic behaviours. They provide input about what children are expected to say, how to speak, when they should speak, and how they can stay on topic. Caregivers may model the appropriate behaviour, using verbal reinforcement, posing a hypothetical situation, addressing children's comments, or evaluating another person.\n\nFamily members contribute to pragmatic development in different ways. Fathers often act as secondary caregivers, and may know the child less intimately. Older siblings may lack the capacity to acknowledge the child's needs. As a result, both fathers and siblings may pressure children to communicate more clearly. They often challenge children to improve their communication skills, therefore preparing them to communicate with strangers about unfamiliar topics. Fathers have more breakdowns when communicating with infants, and spend less time focused on the same objects or actions as infants. Siblings are more directive and less responsive to infants, which motivates infants to participate in conversations with their older siblings. There are limitations to studies that focus on the influences of fathers and siblings, as most research is descriptive and correlational. In reality, there are many variations of family configurations, and context influences parent behaviour more than parent gender does. The majority of research in this field is conducted with mother/child pairs.\n\nPeers help expose children to multi-party conversations. This allows children to hear a greater variety of speech, and to observe different conversational roles. Peers may be uncooperative conversation partners, which pressures the children to communicate more effectively. Speaking to peers is different from speaking to adults, but children may still correct their peers. Peer interaction provides children with a different experience filled with special humour, disagreements and conversational topics.\n\nCulture and context in infants’ linguistic environment shape their vocabulary development. English learners have been found to map novel labels to objects more reliably than to actions compared to Mandarin learners. This early noun bias in English learners is caused by the culturally reinforced tendency for English speaking caregivers to engage in a significant amount of ostensive labelling as well as noun-friendly activities such as picture book reading. Adult speech provides children with grammatical input. Both Mandarin and Cantonese languages have a category of grammatical function word called a noun classifier, which is also common across many genetically unrelated East Asian languages. In Cantonese, classifiers are obligatory and specific in more situations than in Mandarin. This accounts for the research found on Mandarin-speaking children outperforming Cantonese-speaking children in relation to the size of their vocabulary.\n\nPragmatic directions provide children with additional information about the speaker's intended meaning. Children's learning of new word meanings is guided by the pragmatic directions that adults offer, such as explicit links to word meanings. Adults present young children with information about how words are related to each other through connections, such as \"is a part of\", \"is a kind of\", \"belongs to\", or \"is used for\". These pragmatic directions provide children with essential information about language, allowing them to make inferences about possible meanings for unfamiliar words. This is also called inclusion. When children are provided with two words related by inclusion, they hold on to that information. When children hear an adult say an incorrect word, and then repair their mistake by stating the correct word, children take into account the repair when assigning meanings to the two words.\n\nVocabulary development during the school years builds upon what the child already knows, and the child uses this knowledge to broaden his or her vocabulary. Once children have gained a level of vocabulary knowledge, new words are learned through explanations using familiar, or \"old\" words. This is done either explicitly, when a new word is defined using old words, or implicitly, when the word is set in the context of old words so that the meaning of the new word is constrained. When children reach school-age, context and implicit learning are the most common ways in which their vocabularies continue to develop. By this time, children learn new vocabulary mostly through conversation and reading. Throughout schooling and adulthood, conversation and reading are the main methods in which vocabulary develops. This growth tends to slow once a person finishes schooling, as they have already acquired the vocabulary used in everyday conversation and reading material and generally are not engaging in activities that require additional vocabulary development.\n\nDuring the first few years of life, children are mastering concrete words such as \"car\", \"bottle\", \"dog\", \"cat\". By age 3, children are likely able to learn these concrete words without the need for a visual reference, so word learning tends to accelerate around this age. Once children reach school-age, they learn abstract words (e.g. \"love\", \"freedom\", \"success\"). This broadens the vocabulary available for children to learn, which helps to account for the increase in word learning evident at school age. By age 5, children tend to have an expressive vocabulary of 2,100–2,200 words. By age 6, they have approximately 2,600 words of expressive vocabulary and 20,000–24,000 words of receptive vocabulary. Some claim that children experience a sudden acceleration in word learning, upwards of 20 words per day, but it tends to be much more gradual than this. From age 6 to 8, the average child in school is learning 6–7 words per day, and from age 8 to 12, approximately 12 words per day.\n\nExposure to conversations and engaging in conversation with others help school-age children develop vocabulary. Fast mapping is the process of learning a new concept upon a single exposure and is used in word learning not only by infants and toddlers, but by preschool children and adults as well. This principle is very useful for word learning in conversational settings, as words tend not to be explained explicitly in conversation, but may be referred to frequently throughout the span of a conversation.\n\nReading is considered to be a key element of vocabulary development in school-age children. Before children are able to read on their own, children can learn from others reading to them. Learning vocabulary from these experiences includes using context, as well as explicit explanations of words and/or events in the story. This may be done using illustrations in the book to guide explanation and provide a visual reference or comparisons, usually to prior knowledge and past experiences. Interactions between the adult and the child often include the child's repetition of the new word back to the adult. When a child begins to learn to read, their print vocabulary and oral vocabulary tend to be the same, as children use their vocabulary knowledge to match verbal forms of words with written forms. These two forms of vocabulary are usually equal up until grade 3. Because written language is much more diverse than spoken language, print vocabulary begins to expand beyond oral vocabulary. By age 10, children's vocabulary development through reading moves away from learning concrete words to learning abstract words.\n\nGenerally, both conversation and reading involve at least one of the four principles of context that are used in word learning and vocabulary development: physical context, prior knowledge, social context and semantic support.\n\nPhysical context involves the presence of an object or action that is also the topic of conversation. With the use of physical context, the child is exposed to both the words and a visual reference of the word. This is frequently used with infants and toddlers, but can be very beneficial for school-age children, especially when learning rare or infrequently used words. Physical context may include props such as in toy play. When engaging in play with an adult, a child's vocabulary is developed through discussion of the toys, such as naming the object (e.g. \"dinosaur\") or labeling it with the use of a rare word (e.g., \"stegosaurus\"). These sorts of interactions expose the child to words they may not otherwise encounter in day-to-day conversation.\n\nPast experiences or general knowledge is often called upon in conversation, so it is a useful context for children to learn words. Recalling past experiences allows the child to call upon their own visual, tactical, oral, and/or auditory references. For example, if a child once went to a zoo and saw an elephant, but did not know the word \"elephant\", an adult could later help the child recall this event, describing the size and color of the animal, how big its ears were, its trunk, and the sound it made, then using the word \"elephant\" to refer to the animal. Calling upon prior knowledge is used not only in conversation, but often in book reading as well to help explain what is happening in a story by relating it back to the child's own experiences.\n\nSocial context involves pointing out social norms and violations of these norms. This form of context is most commonly found in conversation, as opposed to reading or other word learning environments. A child's understanding of social norms can help them to infer the meaning of words that occur in conversation. In an English-speaking tradition, \"please\" and \"thank you\" are taught to children at a very early age, so they are very familiar to the child by school-age. For example, if a group of people is eating a meal with the child present and one person says, \"give me the bread\" and another responds with, \"that was rude. What do you say?\", and the person responds with \"please\", the child may not know the meaning of \"rude\", but can infer its meaning through social context and understanding the necessity of saying \"please\".\n\nSemantic support is the most obvious method of vocabulary development in school-age children. It involves giving direct verbal information of the meaning of a word. By the time children are in school, they are active participants in conversation, so they are very capable and willing to ask questions when they do not understand a word or concept. For example, a child might see a zebra for the first time and ask, \"what is that?\" and the parent might respond, \"that is a zebra. It is like a horse with stripes and it is wild so you cannot ride it\".\n\nPictures support involves two memory techniques – association and visualization. Associating an image with a word helps a user learn word in a more effective way. Anshul Agarwal, Founder of dailyvocab.com mentioned in his interview to Career360 – \"memory aid for each word help student learn words more faster and effectively\".\n\nMemory plays an important role in vocabulary development, however the exact role that it plays is disputed in the literature. Specifically, short-term memory and how its capacities work with vocabulary development is questioned by many researchers.\n\nThe phonology of words has proven to be beneficial to vocabulary development when children begin school. Once children have developed a vocabulary, they utilize the sounds that they already know to learn new words. The phonological loop encodes, maintains and manipulates speech-based information that a person encounters. This information is then stored in the phonological memory, a part of short term memory. Research shows that children's capacities in the area of phonological memory are linked to vocabulary knowledge when children first begin school at age 4–5 years old. As memory capabilities tend to increase with age (between age 4 and adolescence), so does an individual's ability to learn more complex vocabulary.\n\nSerial-order short-term memory may be critical to the development of vocabulary. As lexical knowledge increases, phonological representations have to become more precise to determine the differences between similar sound words (i.e. \"calm\", \"come\"). In this theory, the specific order or sequence of phonological events is used to learn new words, rather than phonology as a whole.\n\n\n"}
