{"id": "10402117", "url": "https://en.wikipedia.org/wiki?curid=10402117", "title": "Ancient language", "text": "Ancient language\n\nAn ancient language is any language originating in times that may be referred to as ancient. There is no formal criterion for deeming a language ancient, but a traditional convention is to demarcate as \"ancient\" those languages that existed prior to the 5th century. Linguist Roger Woodward has said that \"[p]erhaps, then, what makes an ancient language different is our awareness that it has outlived those for whom it was an intimate element of the psyche\".\n\nBy this definition, the term includes languages attested from ancient times in the list of languages by first written accounts, and described in historical linguistics, and particularly the languages of classical antiquity, such as Tamil language, Ancient Greek, Hebrew language, Old Persian, Avestan, Middle Persian, Sanskrit language, Chinese language, Latin, Arabic language. The term may also encompass other classical languages and various extinct languages.\n\nThe description of fictional races and realms having their own \"ancient languages\" adds depth and richness to storytelling, even if the vocabulary and grammar of the languages themselves is never provided. Examples of this include a fictional language in the \"Inheritance Cycle\" young-adult novels, the language of the race called Ancients in the Mythology of Stargate, and various languages in writings by J.R.R. Tolkien.\n\n"}
{"id": "15047743", "url": "https://en.wikipedia.org/wiki?curid=15047743", "title": "Authors' conference", "text": "Authors' conference\n\nAn authors' conference or writers' conference is a type of conference where writers gather to review their written works and suggest improvements. This process helps an author improve his or her work and learn to be a better writer for future works, both by receiving critiques of their own work and by mentoring the work of the other authors. Writers may also benefit from meeting and hearing from professionals in related fields, such as agents, editors, illustrators, publishers, and providers of other relevant services.\n\nUnlike most other conference styles, an authors' conference is very participatory. Most conferences are divided into presentations, each of which has a clear separation of roles among a one or more presenters and an audience. While authors' conferences may include some such presentations, writers' conferences also include numerous sessions wherein an author does not present his or her work but rather listens while the other participants discuss the work. In this way, the author gains an understanding of what readers learn by reading the work.\n\nAn authors' conference consists of two phases, shepherding and writers' workshops. Shepherding usually (but not always) occurs before the conference meeting, and the meeting itself is organized as a writers' workshop.\n\nOne popular series of authors' conferences is the Pattern Languages of Programming conferences, held to encourage and assist authors of software design patterns and pattern languages.\n\nThe shepherding process occurs before the conference meeting. Authors submit papers for the conference, then each paper is assigned to a shepherd, an experienced author who works with the submitter to improve his or her paper. The process often consists of three iterations in as many weeks where the shepherd makes suggestions for improvement and the submitter incorporates the suggestions. This one-on-one mentoring using the submitter's unfinished work as an example is very effective for teaching the submitter how to be a better writer, although its effectiveness is ultimately determined by the participants' dedication and their working relationship. At the end of the shepherding process, the shepherd recommends whether to accept the submission for review at the conference. An accepted submission should meet minimal quality standards for effective evaluation in the writers' workshop, and should have been improved significantly by the submitter during the shepherding process.\n\nThe writers' workshop process occurs during the conference meeting. The workshop consists of 6–10 sessions, one per submission to be reviewed. The workshop participants are the authors of those submissions and any other reviewers the authors choose to accept. In a session, the authors of the submission listen quietly while the other participants discuss what they liked about the submission and suggest improvements. This gives the authors of the submission insight into what information readers are learning from the work and ideas for improving the work. Reviewers discussing a work are careful to make productive comments, both because the author is listening and because in other sessions that author will become the reviewer and make comments.\n\n\n"}
{"id": "50451161", "url": "https://en.wikipedia.org/wiki?curid=50451161", "title": "Boeraans", "text": "Boeraans\n\nBoeraans is a dialect of Afrikaans which is one of the 11 official languages in South Africa, and is one of the youngest Germanic languages with official status.\n\nBoeraans was also known as \"Die Taal\" (meaning \"The Language). The Border Boers moved from the Western Cape to the Eastern Cape, and stayed there.\n\nIn 1820, the Boers moved to the region known as the Free State Province in South Africa and Transvaal (now the provinces of Limpopo, Mpumalanga, Gauteng and a part of North West) in South Africa.\n\n"}
{"id": "54865111", "url": "https://en.wikipedia.org/wiki?curid=54865111", "title": "Co-construction (linguistics)", "text": "Co-construction (linguistics)\n\nIn linguistics, a co-construction is a grammatical or semantic entity which has been uttered by more than one speaker. It is a technical term for the notion of one person finishing another person's thought. For example:\n"}
{"id": "22696673", "url": "https://en.wikipedia.org/wiki?curid=22696673", "title": "Cognitive synonymy", "text": "Cognitive synonymy\n\nCognitive synonymy is a type of synonymy in which synonyms are so similar in meaning that they cannot be differentiated either denotatively or connotatively, that is, not even by mental associations, connotations, emotive responses, and poetic value. It is a stricter (more precise) technical definition of synonymy, specifically for theoretical (e.g., linguistic and philosophical) purposes. In usage employing this definition, synonyms with greater differences are often called near-synonyms rather than synonyms.\n\nIf a word is cognitively synonymous with another word, they refer to the same thing independently of context. Thus, a word is cognitively synonymous with another word if and only if all instances of both words express the same exact thing, and the referents are necessarily identical, which means that the words' interchangeability is not context-sensitive.\n\nWillard Van Orman Quine used the concept of cognitive synonymy extensively in his famous 1951 paper \"Two Dogmas of Empiricism\", where two words were cognitively synonymous if they were interchangeable in every possible instance.\n\nFor example,\n\nQuine notes that if one is referring to the word itself, this doesn't apply, as in,\n\nAs compared to the substitution which is obviously false,\n\n\n"}
{"id": "55750276", "url": "https://en.wikipedia.org/wiki?curid=55750276", "title": "Confusion network", "text": "Confusion network\n\nA confusion network is a natural language processing method that combines outputs from multiple machine translation systems. The defining characteristic of confusion networks is that they allow multiple ambiguous inputs, deferring committal translation decisions until later stages of processing. This approach is used in the open source machine translation software Moses and the proprietary translation API in IBM Bluemix Watson.\n"}
{"id": "58447333", "url": "https://en.wikipedia.org/wiki?curid=58447333", "title": "Conjunct consonant", "text": "Conjunct consonant\n\nConjunct consonants are a type of letters, used for example in Brahmi or modern Devanagari, to write consonant clusters such as or . Although most of the time, letters are formed by using a simple consonant with the inherent value vowel \"a\" (as with \"k\" , pronounced \"ka\" in Brahmi), or by combining a consonant with an vowel in the form of an diacritic (as with \"ki\" in Brahmi), the usage of conjunct consonant permits the creation of more sophisticated sounds (as with \"kya\" , formed with the consonants k and y assembled verticaly). Conjuncts are often used with loan words. Native words typically use the basic consonant and native speakers know to suppress the vowel.\n\nIn modern Devanagari the components of a conjunct are written left to right when possible (when the first consonant has a vertical stem that can be removed at the right), whereas in Brahmi characters are joined vertically downwards. \n\nSome simple examples of conjunct consonants in Devanagari are: त + व = त्व tva, ण + ढ = ण्ढ ṇḍha, स + थ = स्थ stha, where the vertical stroke of the first letter is simply lost in the combination. Sometimes, conjunct consonants are not clearly derived from the letters making up their components: the conjunct for kṣ is क्ष (क् + ष) and for jñ it is ज्ञ (ज् + ञ). \n\nConjunct consonants are used in many other scripts as well, usually derived from the Brahmi script. In Balinese, conjunct consonants are called \"Haksara Wrehastra\".\n\nConjunct consonant are not limited to Brahmic languages, and can be seen in Navajo for example.\n"}
{"id": "1060739", "url": "https://en.wikipedia.org/wiki?curid=1060739", "title": "Counterword", "text": "Counterword\n\nA counterword (also spelled counter word and counter-word) is a word such as \"so\" that is frequently used to answer (\"counter\") in a reflex-like manner and that has due to this frequent use quickly taken on a new, much less specific or much looser meaning or is even almost meaningless or performs a completely new function. The word \"so\", for example, is frequently used to begin an answer in the sense of \"Well...\" or to function as an indirect way of saying \"Before answering that, I'd like to...\" or even instead of saying \"On the contrary...\" or \"No, I...\".\n\nIn a more general sense, the term is used for such words also when they are not used as a reflex-like answer and even for any widely used words that (due to a similar change) now have a broad and vague range of meanings in many very different situations (e.g. case, awfully, fix, job, payoff).\n\nSince such change due to very frequent use occurs much more rapidly than the change in meaning all words go through, and since such words are even sometimes still simultaneously used in their original sense, the new usage is often considered incorrect by some speakers. Other examples include \"nice\", \"terrific\", \"terrible\", \"awful\", \"tremendous\", \"swell\", \"hopefully\" and \"very fine\" (degrading the meaning of \"fine\" to \"OK\").\n\nThe \"Oxford English Dictionary\" does not support this and defines counter-word as \"countersign\", noting that its usage is military and obsolete with a single quotation from 1678.\n"}
{"id": "39582405", "url": "https://en.wikipedia.org/wiki?curid=39582405", "title": "Darbar (title)", "text": "Darbar (title)\n\nDarbar (Persian: دربار, Urdu: دربار, Pashto: دربار, Hindi: दरबार, Bengali: দরবার, Nepali: दरबार) is a South Asian word mainly derived from Persian language and also equally common in all South Asian languages. It was the term used for the court or levee of the prince; an audience chamber or a place where Muslim Kings and other rulers had their formal and informal meetings, i.e. in European context, equivalent to a king's court.\n\nDarbar is a Persian-derived term (from Persian: دربار - darbār) meaning the court or place of discussion of king, Prince, ruler or noble court. It was later used in Pakistan, India and Nepal for a ruler's court or feudal levee as the latter came to be ruled and later administered by foreigners. A darbar may be either a feudal state council for administering the affairs of a princely state, or a Muslim shrine like data darbar in Lahore.\n"}
{"id": "13698825", "url": "https://en.wikipedia.org/wiki?curid=13698825", "title": "Dialog act", "text": "Dialog act\n\nIn linguistics and in particular in natural language understanding, a dialog act is an utterance, in the context of a conversational dialog, that serves a function in the dialog. Types of dialog acts include a question, a statement, or a request for action. Dialog acts are a type of speech act. \n\nDialog act recognition, also known as spoken utterance classification, is an important part of spoken language understanding. AI inference models or statistical models are used to recognize and classify dialog acts.\n\nA dialog system typically includes a taxonomy of dialog types or \"tags\" that classify the different functions dialog acts can play. One study had 42 types of dialog act in their taxonomy. Examples of types in this study include \"STATEMENT\", \"OPINION\", \"AGREEMENT/ACCEPT\", and \"YES-NO-QUESTION\". \n\nThe research on dialog acts have increased since 1999, after spoken dialog systems became commercial reality.\n"}
{"id": "1156719", "url": "https://en.wikipedia.org/wiki?curid=1156719", "title": "Direction of fit", "text": "Direction of fit\n\nThe technical term direction of fit is used to describe the distinctions that are offered by two related sets of opposing terms:\n\nIn philosophy of mind, a belief has a mind-to-world direction of fit. A belief (that p, say) depicts the world as being in a state of affairs such that p is true. Beliefs, some philosophers have argued, aim at the truth and so aim to fit the world. A belief is satisfied when it fits the world.\n\nA desire, on the other hand, normally expresses a yet to be realized state of affairs and so has a world-to-mind direction of fit. A desire that p, unlike a belief, doesn't depict the world as being in the state that p; rather it expresses a desire that the world be such that p is true. Desire is a state that is satisfied when the world fits it.\n\nA way to account for the difference is that a (rational) person that holds the belief that p when confronted with evidence that not-p, will revise his belief, whereas a person that desires that p can retain his desire that p in the face of evidence that not-p.\n\nTo a philosopher of language a word-to-world fit occurs when, say, a sports journalist correctly names Jones as a goal scorer; while if the journalist mistakenly names Smith as the goal scorer, the printed account does \"not\" display a word-to-world fit, and must be altered such that it matches the real world. Conversely, a world-to-word fit occurs when a fan of Smith's team opines that they deserved to win the match, even though they lost. In this case, the world would have to change to make the sports fan's wish become true.\n\nHowever, in the case of, say, a judge delivering a death sentence to a criminal declared guilty by a jury, the utterances of the judge \"alter\" the world, through the fact of that utterance; and, in this case, the judge is generating a world-to-word-to-world fit (see below). So, if the judge's opinion is upheld, the world \"must\" be altered to match the content of the judge's utterance (i.e., the criminal must be executed).\n\nAccording to Thomas Aquinas (\"Summa Theologica\", Part I, Question 21, Article 2), there are two kinds of \"truth\" (\"veritas\"), both understood as correspondence between mind (\"intellectus\") or words (\"oratio\") and world (\"things\", \"res\"):\nPerhaps the first to speak of a \"direction of fit\" was the philosopher J. L. Austin. Austin did not use the distinction between different directions of fit to contrast commands or expressions of intention to assertions, or desires to beliefs. He rather distinguishes different ways of asserting that an item is of a certain type.\n\nIn an extensive discussion of the issues involved with the differences between, say, (a) wrongly calling a triangle a square (something which, he said, was committing an act of violence to the language) and (b) wrongly describing a triangular object as being a square (something which, he said, was committing an act of violence to the facts), Austin distinguished between what he termed:\n\nThe concept of direction of fit can also apply to speech acts: e.g., statements, guesses and conjectures have word-to-world direction of fit, while commands and promises have a world-to-word direction of fit.\n\nJohn Searle and Daniel Vanderveken assert that there are only four possible \"directions of fit\" in language:\n\nSearle used this notion of \"direction of fit\" to create a taxonomy of illocutionary acts.\nAlthough Elizabeth Anscombe never employed the term \"the direction of fit\", Searle has strongly argued that the following passage from her work Intention was, by far, \"the best illustration\" of the distinction between the tasks of \"[getting] the words (more strictly their propositional content) to match the world… [and that of getting] the world to match the words\":\n\nAccording to Velleman, when used in the domain of the philosophy of mind, the concept direction of fit represents the distinguishing feature between two types of intentional mental states:\n\nIn some forms of mind-body dualism, a matching \"factum\" and \"faciendum\" must be present in a person's mind in order for him to act intentionally. If a person has the belief that action (A) will lead to state (S), and has the desire that state (S) obtain, then he will perform action (A). The action is directly caused by simultaneous presence of the two mental states; no further explanation is needed.\n\nAccording to Velleman:\n\n"}
{"id": "10160405", "url": "https://en.wikipedia.org/wiki?curid=10160405", "title": "Egbert Richter-Ushanas", "text": "Egbert Richter-Ushanas\n\nEgbert Richter (also publishing under the pen name of Richter-Ushanas, after \"\", a Vedic rishi) is a German freelance writer and lecturer, author of self-published (\"Egbert Richter Verlag\", registered in Bremen) treatises on Yoga, Vedanta, Esotericism and mythology, translations of some Upanishads, the Bhagavad Gita and the Yoga Sutras of Patanjali as well as original poetry. Richter has also published various pseudo-scholarly claims, including decipherment of the Rongorongo, Phaistos Disc and Indus scripts.\n\nIn his 1992 decipherment claim of the Indus script, he argues that the script is \"based very largely on intuition, and this quality is also required for reading it\", likening the process of \"decipherment\" to meditation, concluding that the Rbhus, who Richter alleges were priests of the Harappan civilization, invented the Indus script under the influence of Sumerian cuneiform. Werner (1999) confesses himself \"at a loss how to evaluate\" Richter's work, admitting the author has thorough knowledge of the various sources he uses, but at the same time completely lacks academic method, discipline or experience.\n\n\n\n"}
{"id": "10374", "url": "https://en.wikipedia.org/wiki?curid=10374", "title": "Essay", "text": "Essay\n\nAn essay is, generally, a piece of writing that gives the author's own argument — but the definition is vague, overlapping with those of a paper, an article, a pamphlet, and a short story. Essays have traditionally been sub-classified as formal and informal. Formal essays are characterized by \"serious purpose, dignity, logical organization, length,\" whereas the informal essay is characterized by \"the personal element (self-revelation, individual tastes and experiences, confidential manner), humor, graceful style, rambling structure, unconventionality or novelty of theme,\" etc.\n\nEssays are commonly used as literary criticism, political manifestos, learned arguments, observations of daily life, recollections, and reflections of the author. Almost all modern essays are written in prose, but works in verse have been dubbed essays (e.g., Alexander Pope's \"An Essay on Criticism\" and \"An Essay on Man\"). While brevity usually defines an essay, voluminous works like John Locke's \"An Essay Concerning Human Understanding\" and Thomas Malthus's \"An Essay on the Principle of Population\" are counterexamples.\n\nIn some countries (e.g., the United States and Canada), essays have become a major part of formal education. Secondary students are taught structured essay formats to improve their writing skills; admission essays are often used by universities in selecting applicants, and in the humanities and social sciences essays are often used as a way of assessing the performance of students during final exams.\n\nThe concept of an \"essay\" has been extended to other media beyond writing. A film essay is a movie that often incorporates documentary filmmaking styles and focuses more on the evolution of a theme or idea. A photographic essay covers a topic with a linked series of photographs that may have accompanying text or captions.\n\nAn essay has been defined in a variety of ways. One definition is a \"prose composition with a focused subject of discussion\" or a \"long, systematic discourse\".\nIt is difficult to define the genre into which essays fall. Aldous Huxley, a leading essayist, gives guidance on the subject. He notes that \"the essay is a literary device for saying almost everything about almost anything\", and adds that \"by tradition, almost by definition, the essay is a short piece\". Furthermore, Huxley argues that \"essays belong to a literary species whose extreme variability can be studied most effectively within a three-poled frame of reference\". \nThese three poles (or worlds in which the essay may exist) are:\nHuxley adds that the most satisfying essays \"...make the best not of one, not of two, but of all the three worlds in which it is possible for the essay to exist.\"\n\nThe word \"essay\" derives from the French infinitive \"essayer\", \"to try\" or \"to attempt\". In English \"essay\" first meant \"a trial\" or \"an attempt\", and this is still an alternative meaning. The Frenchman Michel de Montaigne (1533–1592) was the first author to describe his work as essays; he used the term to characterize these as \"attempts\" to put his thoughts into writing, and his essays grew out of his commonplacing. Inspired in particular by the works of Plutarch, a translation of whose \"Œuvres Morales\" (\"Moral works\") into French had just been published by Jacques Amyot, Montaigne began to compose his essays in 1572; the first edition, entitled \"Essais\", was published in two volumes in 1580. For the rest of his life, he continued revising previously published essays and composing new ones. Francis Bacon's essays, published in book form in 1597, 1612, and 1625, were the first works in English that described themselves as \"essays\". Ben Jonson first used the word \"essayist\" in English in 1609, according to the \"Oxford English Dictionary\".\n\nEnglish essayists included Robert Burton (1577–1641) and Sir Thomas Browne (1605–1682). In France, Michel de Montaigne's three volume \"Essais\" in the mid 1500s contain over 100 examples widely regarded as the predecessor of the modern essay. In Italy, Baldassare Castiglione wrote about courtly manners in his essay \"Il Cortigiano\". In the 17th century, the Jesuit Baltasar Gracián wrote about the theme of wisdom. During the Age of Enlightenment, essays were a favored tool of polemicists who aimed at convincing readers of their position; they also featured heavily in the rise of periodical literature, as seen in the works of Joseph Addison, Richard Steele and Samuel Johnson. In the 18th and 19th centuries, Edmund Burke and Samuel Taylor Coleridge wrote essays for the general public. The early 19th century, in particular, saw a proliferation of great essayists in English – William Hazlitt, Charles Lamb, Leigh Hunt and Thomas de Quincey all penned numerous essays on diverse subjects. In the 20th century, a number of essayists tried to explain the new movements in art and culture by using essays (e.g., T.S. Eliot). Whereas some essayists used essays for strident political themes, Robert Louis Stevenson and Willa Cather wrote lighter essays. Virginia Woolf, Edmund Wilson, and Charles du Bos wrote literary criticism essays.\n\nAs with the novel, essays existed in Japan several centuries before they developed in Europe with a genre of essays known as \"zuihitsu\" — loosely connected essays and fragmented ideas. Zuihitsu have existed since almost the beginnings of Japanese literature. Many of the most noted early works of Japanese literature are in this genre. Notable examples include \"The Pillow Book\" (c. 1000), by court lady Sei Shōnagon, and \"Tsurezuregusa\" (1330), by particularly renowned Japanese Buddhist monk Yoshida Kenkō. Kenkō described his short writings similarly to Montaigne, referring to them as \"nonsensical thoughts\" written in \"idle hours\". Another noteworthy difference from Europe is that women have traditionally written in Japan, though the more formal, Chinese-influenced writings of male writers were more prized at the time.\n\nThis section describes the different forms and styles of essay writing. These forms and styles are used by an array of authors, including university students and professional essayists.\n\nThe defining features of a \"cause and effect\" essay are causal chains that connect from a cause to an effect, careful language, and chronological or emphatic order. A writer using this rhetorical method must consider the subject, determine the purpose, consider the audience, think critically about different causes or consequences, consider a thesis statement, arrange the parts, consider the language, and decide on a conclusion.\n\nClassification is the categorization of objects into a larger whole while division is the breaking of a larger whole into smaller parts.\n\nCompare and contrast essays are characterized by a basis for comparison, points of comparison, and analogies. It is grouped by the object (chunking) or by point (sequential). The comparison highlights the similarities between two or more similar objects while contrasting highlights the differences between two or more objects. When writing a compare/contrast essay, writers need to determine their purpose, consider their audience, consider the basis and points of comparison, consider their thesis statement, arrange and develop the comparison, and reach a conclusion. Compare and contrast is arranged emphatically.\n\nExpository essay is used to inform, describe or explain a topic, using important facts and teaching reader about the topic. Mostly written in third-person, using \"it\", \"he\", \"she\", \"they\". Expository essay uses formal language to discuss someone or something. Examples of expository essays are: a medical or biological condition, social or technological process, life or character of a famous person. Writing of expository essay often consists of following next steps: organizing thoughts (brainstorming), researching a topic, developing a thesis statement, writing the introduction, writing the body of essay, writing the conclusion. Expository essays are often assigned as a part of SAT and other standardized testings or as a homework for high school and college students.\n\nDescriptive writing is characterized by sensory details, which appeal to the physical senses, and details that appeal to a reader's emotional, physical, or intellectual sensibilities. Determining the purpose, considering the audience, creating a dominant impression, using descriptive language, and organizing the description are the rhetorical choices to consider when using a description. A description is usually arranged spatially but can also be chronological or emphatic. The focus of a description is the scene. Description uses tools such as denotative language, connotative language, figurative language, metaphor, and simile to arrive at a dominant impression. One university essay guide states that \"descriptive writing says what happened or what another author has discussed; it provides an account of the topic\".\nLyric essays are an important form of descriptive essays.\n\nIn the dialectic form of the essay, which is commonly used in philosophy, the writer makes a thesis and argument, then objects to their own argument (with a counterargument), but then counters the counterargument with a final and novel argument. This form benefits from presenting a broader perspective while countering a possible flaw that some may present. This type is sometimes called an ethics paper.\n\nAn exemplification essay is characterized by a generalization and relevant, representative, and believable examples including anecdotes. Writers need to consider their subject, determine their purpose, consider their audience, decide on specific examples, and arrange all the parts together when writing an exemplification essay.\nAn essayist writes a \"familiar essay\" if speaking to a single reader, writing about both themselves, and about particular subjects. Anne Fadiman notes that \"the genre's heyday was the early nineteenth century,\" and that its greatest exponent was Charles Lamb. She also suggests that while critical essays have more brain than the heart, and personal essays have more heart than brain, familiar essays have equal measures of both.\n\nA history essay sometimes referred to as a thesis essay describes an argument or claim about one or more historical events and supports that claim with evidence, arguments, and references. The text makes it clear to the reader why the argument or claim is as such.\n\nA narrative uses tools such as flashbacks, flash-forwards, and transitions that often build to a climax. The focus of a narrative is the plot. When creating a narrative, authors must determine their purpose, consider their audience, establish their point of view, use dialogue, and organize the narrative. A narrative is usually arranged chronologically.\n\nAn argumentative essay is a critical piece of writing, aimed at presenting objective analysis of the subject matter, narrowed down to a single topic. The main idea of all the criticism is to provide an opinion either of positive or negative implication. As such, a critical essay requires research and analysis, strong internal logic and sharp structure. Its structure normally builds around introduction with a topic's relevance and a thesis statement, body paragraphs with arguments linking back to the main thesis, and conclusion. In addition, an argumentative essay may include a refutation section where conflicting ideas are acknowledged, described, and criticized. Each argument of argumentative essay should be supported with sufficient evidence, relevant to the point.\n\nA process essay is used for an explanation of making or breaking something. Often, it is written in chronological order or numerical order to show step-by-step processes. It has all the qualities of a technical document with the only difference is that it is often written in descriptive mood, while a technical document is mostly in imperative mood.\n\nAn economic essay can start with a thesis, or it can start with a theme. It can take a narrative course and a descriptive course. It can even become an argumentative essay if the author feels the need. After the introduction, the author has to do his/her best to expose the economic matter at hand, to analyze it, evaluate it, and draw a conclusion. If the essay takes more of a narrative form then the author has to expose each aspect of the economic puzzle in a way that makes it clear and understandable for the reader\n\nA \"reflective essay\" is an analytical piece of writing in which the writer describes a real or imaginary scene, event, interaction, passing thought, memory, or form — adding a personal reflection on the meaning of the topic in the author's life. Thus, the focus is not merely descriptive. The writer doesn’t just describe the situation, but revisits the scene with more detail and emotion to examine what went well, or reveal a need for additional learning — and may relate what transpired to the rest of the author's life.\n\nThe logical progression and organizational structure of an essay can take many forms. Understanding how the movement of thought is managed through an essay has a profound impact on its overall cogency and ability to impress. A number of alternative logical structures for essays have been visualized as diagrams, making them easy to implement or adapt in the construction of an argument.\n\nIn countries like the United States and the United Kingdom, essays have become a major part of a formal education in the form of free response questions. Secondary students in these countries are taught structured essay formats to improve their writing skills, and essays are often used by universities in these countries in selecting applicants (\"see\" admissions essay). In both secondary and tertiary education, essays are used to judge the mastery and comprehension of the material. Students are asked to explain, comment on, or assess a topic of study in the form of an essay. In some courses, university students must complete one or more essays over several weeks or months. In addition, in fields such as the humanities and social sciences, mid-term and end of term examinations often require students to write a short essay in two or three hours.\n\nIn these countries, so-called academic essays also called \"papers\", are usually more formal than literary ones. They may still allow the presentation of the writer's own views, but this is done in a logical and factual manner, with the use of the first person often discouraged. Longer academic essays (often with a word limit of between 2,000 and 5,000 words) are often more discursive. They sometimes begin with a short summary analysis of what has previously been written on a topic, which is often called a literature review.\n\nLonger essays may also contain an introductory page that defines words and phrases of the essay's topic. Most academic institutions require that all substantial facts, quotations, and other supporting material in an essay be referenced in a bibliography or works cited page at the end of the text. This scholarly convention helps others (whether teachers or fellow scholars) to understand the basis of facts and quotations the author uses to support the essay's argument and helps readers evaluate to what extent the argument is supported by evidence, and to evaluate the quality of that evidence. The academic essay tests the student's ability to present their thoughts in an organized way and is designed to test their intellectual capabilities.\n\nOne of the challenges facing universities is that in some cases, students may submit essays purchased from an essay mill (or \"paper mill\") as their own work. An \"essay mill\" is a ghostwriting service that sells pre-written essays to university and college students. Since plagiarism is a form of academic dishonesty or academic fraud, universities and colleges may investigate papers they suspect are from an essay mill by using plagiarism detection software, which compares essays against a database of known mill essays and by orally testing students on the contents of their papers.\n\nEssays often appear in magazines, especially magazines with an intellectual bent, such as \"The Atlantic\" and \"Harpers\". Magazine and newspaper essays use many of the essay types described in the section on forms and styles (e.g., descriptive essays, narrative essays, etc.). Some newspapers also print essays in the op-ed section.\nEmployment essays detailing experience in a certain occupational field are required when applying for some jobs, especially government jobs in the United States. Essays known as Knowledge Skills and Executive Core Qualifications are required when applying to certain US federal government positions.\n\nA KSA, or \"Knowledge, Skills, and Abilities,\" is a series of narrative statements that are required when applying to Federal government job openings in the United States. KSAs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The knowledge, skills, and abilities necessary for the successful performance of a position are contained on each job vacancy announcement. KSAs are brief and focused essays about one's career and educational background that presumably qualify one to perform the duties of the position being applied for.\n\nAn Executive Core Qualification, or ECQ, is a narrative statement that is required when applying to Senior Executive Service positions within the US Federal government. Like the KSAs, ECQs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The Office of Personnel Management has established five executive core qualifications that all applicants seeking to enter the Senior Executive Service must demonstrate.\n\nA film essay (or \"cinematic essay\") consists of the evolution of a theme or an idea rather than a plot per se, or the film literally being a cinematic accompaniment to a narrator reading an essay. From another perspective, an essay film could be defined as a documentary film visual basis combined with a form of commentary that contains elements of self-portrait (rather than autobiography), where the signature (rather than the life story) of the filmmaker is apparent. The cinematic essay often blends documentary, fiction, and experimental film making using tones and editing styles.\n\nThe genre is not well-defined but might include propaganda works of early Soviet parliamentarians like Dziga Vertov, present-day filmmakers including Chris Marker, Michael Moore (\"Roger & Me\" (1989), \"Bowling for Columbine\" (2002) and \"Fahrenheit 9/11\" (2004)), Errol Morris (\"The Thin Blue Line\" (1988)), Morgan Spurlock (\"Supersize Me: A Film of Epic Portions\") and Agnès Varda. Jean-Luc Godard describes his recent work as \"film-essays\". Two filmmakers whose work was the antecedent to the cinematic essay include Georges Méliès and Bertolt Brecht. Méliès made a short film (\"The Coronation of Edward VII\" (1902)) about the 1902 coronation of King Edward VII, which mixes actual footage with shots of a recreation of the event. Brecht was a playwright who experimented with film and incorporated film projections into some of his plays. Orson Welles made an essay film in his own pioneering style, released in 1974, called \"F for Fake\", which dealt specifically with art forger Elmyr de Hory and with the themes of deception, \"fakery,\" and authenticity in general. These are often published online on video hosting services.\n\nDavid Winks Gray's article \"The essay film in action\" states that the \"essay film became an identifiable form of filmmaking in the 1950s and '60s\". He states that since that time, essay films have tended to be \"on the margins\" of the filmmaking the world. Essay films have a \"peculiar searching, questioning tone ... between documentary and fiction\" but without \"fitting comfortably\" into either genre. Gray notes that just like written essays, essay films \"tend to marry the personal voice of a guiding narrator (often the director) with a wide swath of other voices\". The University of Wisconsin Cinematheque website echoes some of Gray's comments; it calls a film essay an \"intimate and allusive\" genre that \"catches filmmakers in a pensive mood, ruminating on the margins between fiction and documentary\" in a manner that is \"refreshingly inventive, playful, and idiosyncratic\".\n\nIn the realm of music, composer Samuel Barber wrote a set of \"Essays for Orchestra,\" relying on the form and content of the music to guide the listener's ear, rather than any extra-musical plot or story.\n\nA photographic essay strives to cover a topic with a linked series of photographs. Photo essays range from purely photographic works to photographs with captions or small notes to full-text essays with a few or many accompanying photographs. Photo essays can be sequential in nature, intended to be viewed in a particular order — or they may consist of non-ordered photographs viewed all at once or in an order that the viewer chooses. All photo essays are collections of photographs, but not all collections of photographs are photo essays. Photo essays often address a certain issue or attempt to capture the character of places and events.\nIn the visual arts, an essay is a preliminary drawing or sketch that forms a basis for a final painting or sculpture, made as a test of the work's composition (this meaning of the term, like several of those following, comes from the word \"essay\"'s meaning of \"attempt\" or \"trial\").\n\n\n\n"}
{"id": "34008425", "url": "https://en.wikipedia.org/wiki?curid=34008425", "title": "European Association for the Teaching of Academic Writing", "text": "European Association for the Teaching of Academic Writing\n\nThe European Association for the Teaching of Academic Writing (EATAW) is an academic association supporting scholarly activity in academic writing. The association was first established in 1999 with the first conference being held in 2001. The Europe-wide association has three main activities: a bi-annual conference, an Internet forum and \"the Journal of Academic Writing\".\n\nThe EATAW conference is held every two years in a European University. It was first held in 2001 in Groningen. The occasion of the bi-annual conference is when the EATAW board is elected for a term of two years.\n\n\"The Journal of Academic Writing\" is a peer reviewed journal established by EATAW.\n\n\n"}
{"id": "30864082", "url": "https://en.wikipedia.org/wiki?curid=30864082", "title": "Gender differences in spoken Japanese", "text": "Gender differences in spoken Japanese\n\nThe Japanese language has some words and some grammatical constructions that are associated with men or boys, while others are associated with women or girls. Such differences are sometimes called \"gendered language\". In Japanese, speech patterns associated with women are referred to as or .\n\nIn general, the words and speech patterns associated with men are perceived as rough, vulgar, or abrupt, while those associated with women are considered more polite, more deferential, or \"softer\". Some linguists consider the description of \"rough–soft continuum\" more accurate than the description of \"male–female continuum\". For example, Eleanor Harz Jorden in \"\" refers to the styles as \"blunt/gentle\", rather than male/female.\n\nThere are no gender differences in written Japanese (except in quoted speech), and almost no differences in polite speech (\"teineigo\").\n\nThe word , which is usually translated as \"ladylike\" or \"feminine,\" refers to the behaviour expected of a typical Japanese woman. As well as behaving in particular ways, being \"onnarashii\" means conforming to particular styles of speech. Some of the features of women’s speech include speaking in a higher register, using more polite forms and using polite speech or honorifics in more situations, and referring to themselves and those whom they address more formally.\n\nSome linguistic features commonly associated with women include omission of the copula \"da\", the use of personal pronouns such as \"watashi\" or \"atashi\" among others, use of feminine sentence-final particles such as \"wa\", \"na no\", \"kashira\", and \"mashō\", and the more frequent use of the honorific prefixes \"o\" and \"go\".\n\nActual language used by Japanese-speaking women differs from these ideals. Such \"onnarashii\" speech is a social norm that institutions such as education and media encourage women to adopt. Similarly, these forms may be prescribed for women learners by Japanese textbooks and other materials. There are, however, various deviations from these norms in conversation.\n\nAlthough Japanese women may not follow the gender norm in speech, some linguistic studies indicate that Japanese women tend to use more honorific language than men do, which reinforces the idea of \"onnarashii\" and traditional gender roles.\n\nJust as there are modes of speaking and behaviour that are considered intrinsically feminine, there are also those that are considered . Based on men's reports of their own speech, as well as prescriptive advice for language use, men's language is thought of as using fewer polite forms, distinct pronouns and sentence-final particles, and some reduced vowels.\n\nSome words associated with men's speech include the informal \"da\" in place of the copula \"desu\", personal pronouns such as \"ore\" and \"boku\", and sentence-final particles such as \"yo\", \"ze\", \"zo\", and \"kana\". Masculine speech also features less frequent use of honorific prefixes and fewer \"aizuchi\" response tokens.\n\nResearch on Japanese men's speech shows greater use of \"neutral\" forms, forms not strongly associated with masculine or feminine speech, than is seen in Japanese women's speech.\n\nSome studies of conversation between Japanese men and women show neither gender taking a more dominant position in interaction. Men, however, tend to show a \"self-oriented conversation style\", telling stories and expressing their expertise on topics being discussed more than is typical of women in these studies.\n\nSince the late twentieth century observers have noted that individual Japanese men and women do not necessarily speak in the ways attributed to their gender. Scholars have described considerable variation within each gender; some individuals use traditional characteristics of gendered speech, while others do not. Upper-class women who do not conform to traditional expectations of gendered speech are sometimes criticized for failing to maintain \"traditional Japanese culture\".\n\nRegional dialect may often play a role in the expression and perception masculinity or femininity of speech in Japanese.\n\nAnother recent phenomenon influencing gender norms in speech is the popularity of entertainers, typically men who enact very feminine speech, dress, and other gender markers. The word \"okama\" originally referred to male homosexuals, but its usage has expanded to refer to gay men and cross-dressers as well as trans women, among other uses. Entertainers who identify as okama sometimes use a form of speech called , literally \"older sister speech\", a speaking style that combines the formal aspects of women's speech described above with blunt or crude words and topics. For example:\nThe pronoun \"atashi\" and the sentence-final \"da wa\" is typical of women's speech, while the verb \"kuttara\" is typical of men's speech and the topic itself is very blunt.\n\n\n"}
{"id": "1025265", "url": "https://en.wikipedia.org/wiki?curid=1025265", "title": "Gender of connectors and fasteners", "text": "Gender of connectors and fasteners\n\nIn electrical and mechanical trades and manufacturing, each half of a pair of mating connectors or fasteners is conventionally assigned the designation male or female. The \"female\" connector is generally a receptacle that receives and holds the \"male\" connector. On occasion, the terms \"male\" and \"female\" are respectively referred to as the A and B ends, though the names of some standards conflict with this as they contain the letters A or B within the name; unambiguous, though rare, terms include plug and socket or jack.\n\nThe assignment is a direct analogy with genitalia and heterosexual sex; the part bearing one or more protrusions, or which fits inside the other, being designated male in contrast to the part containing the corresponding indentations, or fitting outside the other, being designated female. Extension of the analogy results in the verb to mate being used to describe the process of connecting two corresponding parts together. \n\nIn some cases (notably electrical power connectors), the gender of connectors is selected according to rigid rules, to enforce a sense of one-way directionality (e.g. a flow of power \"from\" one device \"to\" another). This gender distinction is implemented to enhance safety or ensure proper functionality by preventing unsafe or non-functional configurations from being set up.\n\nIn terms of mathematical graph theory, an electrical power distribution network made up of plugs and sockets is a directed tree, with the directionality arrows corresponding to the female-to-male transfer of electrical power through each mated connection. This is an example where male and female connectors have been deliberately designed and assigned to physically enforce a safe network topology.\n\nIn other contexts, such as plumbing, one-way flow is \"not\" enforced through connector gender assignment. Flows through piping networks can be bidirectional, as in underground water distribution networks which have designed-in redundancy. In plumbing situations where one-way flow \"is\" desired, it is implemented through other means (e.g. gravity flow, one-way check valves), and not through male-female gender schemes.\n\nIn mechanical design, the prototypical \"male\" component is a threaded bolt, but an alignment post, a mounting boss, or a sheet metal tab connector can also be considered as male. Correspondingly, a threaded nut, an alignment hole, a mounting recess, or sheet metal slot connector is considered to be female.\n\nWhile some mechanical designs are \"one-off\" custom setups not intended to be repeated, there is an entire fastener industry devoted to manufacturing mass-produced or semi-custom components. To avoid unnecessary confusion, conventional definitions of fastener gender have been defined and agreed upon.\n\nAlthough this aspect is not highlighted in their promotional literature, several common construction toys embody gendered (and in some cases, genderless) mechanical interconnections. This should not be surprising, since these toys feature the nearly infinite flexibility and versatility of shape that a modular interconnect architecture can enable. Mathematicians have begun to classify well-known construction sets using group theory to study the combinatoric possibilities of structures that can be built.\n\nFor example, the canonical LEGO plastic blocks have \"female\" indentations on the lower surface, and \"male\" bosses or protrusions on the upper surfaces. Meccano and Erector have many gendered connections, starting with the nut-and-bolt fasteners they use frequently.\n\nStickle bricks, using interlocking plastic protrusions, are effectively genderless. Lincoln logs use a very simple form of genderless connections. Kapla or KEVA planks are extremely simple genderless systems interconnected only by gravity.\n\nIn plumbing fittings, the \"M\" or \"F\" usually comes at the beginning rather than the end of the abbreviated designation. For example:\n\nA short length of pipe having an MIP thread at both ends is sometimes called a nipple. A short pipe fitting having an FIP thread at both ends is sometimes called a coupling.\n\nHermaphroditic connections, which include both male and female elements in a single unit, are used for some specialized tubing fittings, such as Storz fire hose connectors. A picture of such fittings appears in the \n\nDownspouts (downpipes, rain conductors or leaders) are used to convey rainwater from roof gutters to the ground through hollow pipes or tubes. These tubes usually come in sections, joined by inserting the male end (often crimped with a special tool to slightly reduce its size) into the female end of the next section. These connections are usually not sealed or caulked, instead relying on gravity to move the rainwater from the male end and into the receiving female connection located directly below.\n\nSheet metal ductwork for conveying air in HVAC systems typically uses gendered connections. Typically, the airflow through a ductwork connection is from male to female. However, since one-way flow is implemented by forced-air fans or blowers, \"backwards\" gendered connections can be seen frequently in some systems, since all connections are typically sealed with duct sealing mastic or tape to prevent leakage anyway. The flow convention is usually loosely adhered to for simplicity of design, and to reduce the number of gender changer fittings required, but exceptions are made whenever expedient.\n\nAlthough the gender of tubing and plumbing fittings is usually obvious, this may not be true of electrical connectors because of their more complex and varying constructions. Instead, connector gender is conventionalized and thus can be somewhat obscure to the uninitiated. For example, the female D-subminiature connector body projects outward from the mounting plane of the chassis, and this protrusion could be erroneously construed as male. Instead, the \"maleness\" of the D-subminiature connectors is defined by specific presence of \"male pins\", rather than by the protrusion of the connector, which is also true for many other pin-based connectors like XLR. The male/female distinction is more obvious with ring crimp lug connectors which are placed around a screw post, but again with spade or split ring crimp lug connectors the end alone is not obviously female.\n\nFurther confusion can be caused by the term \"jack\", which is used for both female and male connectors and typically refers to the fixed (panel) side of a connector pair. IEEE STD 100, IEEE-315-1975 and IEEE 200-1975 (replaced by ASME Y14.44-2008) define \"Plug\" and \"Jack\" by location or mobility, rather than gender.\n\nA connector in a fixed location is a jack and a moveable connector is a plug. The distinction is relative, so a portable radio is considered stationary compared to the cable from the headphones; the radio has a jack, and the headphone cable has a plug. Where the relationship is equal, such as when two flexible cables are connected, each is considered a plug. Jacks use the reference designator prefix of J and plugs use the reference designator prefix of P. It is possible in the case of box mounted connectors for the connector to be a receptacle with male pin contacts. In this case the connector is designated a jack (J ref des) regardless of the contact gender because the housing for the contacts is in fact configured as the receptacle even though its mate (the plug) goes around the receptacle. See MIL-STD-38999 and similar cases.\n\nIt is common practice to use female connectors for jacks, so the informal gender-based usage often happens to agree with the functional description of the technical standards. However, this is \"not\" always the case; often-seen exceptions include a computer's AC Power Inlet and EIA232 DE9 Serial Port, or the male coaxial power jacks for connecting external power adapters to portable equipment.\n\nTo summarize, it is considered best practice to use \"male\" and \"female\" for connector \"gender\", and \"plug\" and \"jack\" for connector \"function\" or \"mobility\".\n\nIn the UK, many Commonwealth countries and some non-English-speaking countries, the word \"jack\" may refer to the \"plug\" on the end of a removable cable. These connectors were originally referred to as \"jack plugs\", or plugs intended to be mated with fixed receptacles, or \"sockets\" (which North Americans would call \"jacks\"), but the second word was dropped. This variant usage is in direct contradiction to common usage and official standards in North America.\n\nIn the United Kingdom, for example, the connector on the end of a headphone lead is known as a \"jack\", that plugs into a \"socket\" on the main unit. The same generally occurs also in Italy, where the English word \"jack\" is commonly used to indicate the connector on the end of a headphone lead.\n\nIn Romania female connectors are known as \"mamă\" (mother) and male connectors are \"tată\" (father).\n\nThe standard letters \"M\" and \"F\" are commonly used in part numbers to designate connector gender. For example, in Switchcraft XLR microphone or hydrophone connectors, the part numbers are denoted as follows:\n\nThe terms plug, pin, and prong are also often used for \"male\" connectors, and receptacle, socket, and slot are used for \"female\" connectors. In many cases these terms are more common than \"male\" and \"female\", especially in documentation intended for the non-specialist. These nearly synonymous terms can cause a fair amount of confusion when the designations are shortened in labels.\n\nFor example, a female high-density D-subminiature connector with a size 1 shell can be named DE15F or DE15S (see accompanying pictures). Both terms mean the same thing but could be construed to be completely different items. Similarly, a male standard-density D-sub with a size 1 shell can be named DE9M or DE9P; a female standard-density D-sub with a size 2 shell can be named DA15F or DA15S; a male high-density D-sub with a size 3 shell can be named DB44M or DB44P; and so forth.\n\nElectronic designers often select female jack connectors for fixed mounting on electronic equipment they design. This is usually done because female connectors are more resistant to damage or contamination, by virtue of their concealed or recessed electrical contacts. A damaged motherboard connector can result in the scrapping of an expensive piece of electronic equipment. The risk of expensive damage is reduced by relegating the more exposed male contacts to connecting cables, which can be repaired or replaced at lower cost.\n\nWith an RS232 serial port, the male connector is more fragile than the female connector.\n\nSome people say the male coaxial connector is more prone to damage.\nOther people say the female coaxial connector is more prone to damage.\n\nSuch cost and reliability considerations probably drove the design decision to use female jack connectors on many computer terminals (and some personal computers) for the serial port, in direct violation of the connector gender convention specified in the RS-232 standard for \"DTE\" computer equipment. This confusing reversal of the RS-232 connector gender convention would cause many hours of frustration for ill-informed end users, as they tried to troubleshoot non-functional serial port equipment connections.\n\nIn the case of electrical power connections, designers do not reverse connector gender in such a casual fashion, because exposing live AC line power on male connectors is unsafe and generally illegal. Devices that need to be robust against mechanical damage use a special male IEC 60320 C14 connector (see Gallery above), which is recessed below the surface of a mounting panel, providing the desired physical protection while conforming to safety regulations.\n\nIn electrical connections where voltage or current is sufficient to cause injury, the part permanently connected to the power \"source\" is invariably female, with concealed contacts, to prevent inadvertent touching of live conductors. A male plug, with fully exposed protruding contacts, is installed on the cord of (or directly onto) the device \"receiving\" the power.\n\nIn the case of consumer-level AC power, connector gender is used to implicitly enforce safe use of power connectors. Because of this consideration, it is illegal under electrical code to make or use any \"gender changers\" to connect AC line power to consumer-level equipment.\n\nIn low-voltage use such as for data communications, electrical shock hazard is not an issue, and male or female connectors are used based on other engineering factors such as convenience of use, cost, or ease of manufacturing. For example, the common \"patch cables\" used for Ethernet (and the similar cords used for telephones) typically have male modular plugs on both ends, to connect to jacks on equipment or mounted in walls.\n\nAs an illustrative example of some design tradeoffs in power connector selection, consider the adjacent picture. A commonly seen coaxial power connector is usually set up so that power is fed \"from\" the female plug on the right \"into\" the male jack on the left (which is typically a part of the electronic device accepting the power). Although the plug is female, with a partially recessed center contact, it is still possible for casual accidental contact with a metallic object to short-circuit the power source. Depending on the design of the power adapter, it may react to a short circuit by shutting down temporarily, or instead by blowing out an internal safety fuse.\n\nIn this example, the marginal reliability of the connector choice was deemed to be acceptable by the equipment designer, since the power adapter supplies low voltage that does not pose an electric shock hazard. The potential fire hazard from accidental short-circuiting is addressed by the internal safety fuse, although this requires that a failed power adapter must be completely replaced. In a different design, if the power adapter were intended to supply a voltage sufficient to cause electrical shock, the semi-exposed center contact of the female plug would be considered unacceptably hazardous, requiring a different choice of power connector.\nSome electrical connectors are hermaphroditic because they include both male and female elements in a single unit intended to interconnect freely, without regard for gender. See the discussion of Genderless connectors elsewhere in this article for more detailed information.\n\nAs an additional complication, certain electronic connector designs may incorporate combinations of male and female pins in a \"single\" connector body, for mating with a complementary connector with opposite gender pins in corresponding positions. In these unusual cases, gender is often defined by the shape of the connector \"body\", rather than the mixed-gender connector \"pins\" and \"sockets\". These types of connectors are not strictly speaking hermaphroditic, since mating connectors are \"not\" freely interchangeable. An informal term that has been used for these connectors is \"bisexual\", in addition to the more official terminology, mixed-gender. Thus, for example, one can have a mixed-gender female plug that connects to a mixed-gender male jack (though a reversed gender assignment of connectors would be a more typical design choice in this example).\n\nMale connector pins are often protected by a shell (also called a shroud, surround, or shield), which may envelop the entire female connector when mated. RF connectors often have multiple layers of interlocking shells to properly connect the shields of coaxial and triaxial cable. In such cases, the gender is assigned based on the \"innermost\" connecting point. With the exception of reverse polarity BNC or TNC, where the outer shell determines the gender and the innermost connecting points are opposite to a standard connector, for example a female RP-TNC connector has a solid innermost pin.\n\nAnother ambiguous situation arises with the connectors used for USB, FireWire (IEEE-1394), HDMI, and Thunderbolt serial data bus connections. Close examination of these connectors reveals that the contact \"pins\" are not actually pins, but instead are conductive surfaces that slide past each other when they mate. Therefore, the traditional pin and socket nomenclature is not applicable. Instead, most computer hardware people fall back to referring to the wrap-around metal shield on the plug connector \"as if\" it were a connector \"pin\". By this convention, the connectors on serial bus cables are \"male plugs\", and the corresponding connectors on equipment are \"female jacks\". A unique connector configuration where the contacts are hermaphroditic is the ELCO Varicon where the contacts are bifurcated and nest with one another axial at a 90 degree rotation in cross-shaped wells. In this case the plugs had the contacts oriented transversely and the sockets longitudinally. \n\nA casual glance at a USB \"Type A\" plug connector may give the false impression that it is hermaphroditic. However, a physical attempt to mate two USB \"Type A\" cables with each other reveals the fact that the connectors will not interconnect. Classifying according to mathematical graph theory, USB buses are directed trees, whereas FireWire buses have a true bus network topology. This difference is reflected in the bus connectors used, in that USB cables are asymmetrical (one end Type A, other end Type B) while FireWire cables may have identical connectors at both ends.\n\nBy definition, a hermaphroditic connector includes mating surfaces having simultaneous male and female aspects, involving complementary paired identical parts each containing both protrusions and indentations. These mating surfaces are mounted into identical fittings which can freely mate with any other, without regard for gender (provided that the size and type are already matched). Alternative names include hermaphrodite, androgynous, genderless, sexless, combination (or combo), two-in-one, two-way, and other descriptive terms. Several of these latter alternate names are ambiguous in meaning, and should not be used unless carefully defined in context. True hermaphroditic connectors should not be confused with \"mixed gender\" connectors, which are described elsewhere in this article.\n\nAnother closely related type is the stackable connector for electronics, which typically has male pins on one surface, and complementary female sockets on the opposite surface, allowing multiple units to be stacked up like plastic milk crates. Examples of this include stackable banana plugs, and interconnect cables specified for the IEEE-488 instrumentation bus. Stackable mezzanine bus connectors are used on some modular microcomputer accessory boards for systems such as the Arduino add-on daughterboards called \"shields\". The older PC/104 embedded PC modules use a similar stackable format for interconnection. Stackable connectors are not classified as hermaphroditic in the strictest sense, but are often described as such in looser usage.\n\nThe hermaphroditic design is useful when multiple complex or lengthy components must be arbitrarily connected in various combinations. For example, if hoses have hermaphroditic fittings, they can be connected without having to pull a lengthy hose and reverse it because it has the wrong gender to connect to another hose. Some military fiber optical cables also have hermaphroditic connectors to prevent \"wrong gender\" connector problems in field deployments. In a similar fashion, railcars are usually equipped with hermaphroditic railway coupling mechanisms that allow either end of the vehicle to be connected to a train consist without having to turn the railcar around first.\nFor the same reason, several spacecraft docking mechanisms are designed to be \"androgynous\",\nincluding the Androgynous Peripheral Attach System, the NASA Docking System, and Chinese Docking Mechanism.\n\nIn the absence of genderless connectors, \"gender changer\" fittings might be used to enable certain connections. The designer of a connection system may use one or both schemes to allow arbitrary connectivity, or even combine both schemes into a single system.\n\nWhen an enforced sense of unidirectionality or \"one-way flow\" is required for safety or other reasons (for example, AC electrical power connections), a strict assignment of connector genders is implemented to prevent undesired configurations, and gender changers are banned.\n\nSome commonly seen examples of hermaphroditic connectors include the SAE connector for 12 V DC power, jackhammer air hose connectors, and the Anderson Powerpole series of modular high-current power connectors. The IBM token ring connector was another widespread example, but it has become obsolete and is being phased out. The General Radio Corporation (GenRad) developed a hermaphroditic coaxial radio frequency connector often called the \"GR connector\".\n\nSome audio multicore cables are fitted with hermaphroditic multipin quick-disconnect connectors for ease of use in the field. One style of this audio signal cable is fitted on both ends with connectors that are each populated half with pins and half with sockets. The advantage to the user is that it does not matter which end connects to the stage and which to the audio mixer, facilitating faster set up. Another style of connector uses hybrid male/female pins with a receiving slot fitted in the center of each two-tine pin, and relies on 90-degree rotation of the pin axes to mate. The connector housings themselves are sexed male and female.\n\nDevices used for mating two connectors of the same gender have a wide variety of terms, including for example: \"gender changer\", \"gender mender\", \"gender bender\", \"gender blender\", \"sex changer\", and \"homosexual adapter\". A specific gender changer can be referred to by either the gender of its connectors, or the gender which it is designed to connect to, resulting in a thoroughly ambiguous terminology. Thus a \"male gender changer\" might have female connectors to mate two male ends, or male connectors to mate two female ends.\n\nAdding to this potential for confusion, some gender changers also combine additional functions such as cross-over pin-outs or even embedding micro-controllers for performance, or for logic level or protocol adaptations, which would properly make them an adapter, but this nomenclature is sometimes neglected in marketing materials or common parlance.\n\n\n"}
{"id": "14223847", "url": "https://en.wikipedia.org/wiki?curid=14223847", "title": "Gestures in language acquisition", "text": "Gestures in language acquisition\n\nGestures are a form of non-verbal communication that include movements of the hands, arms, and/or other parts of the body. Children can use gesture to communicate before they have the ability to use spoken words and phrases. In this way gestures can prepare children to learn a spoken language, creating a bridge from pre-verbal communication to speech. The onset of gesture has also been shown to predict and facilitate children's spoken language acquisition. Once children begin to use spoken words their gestures can be used in conjunction with these words to form phrases and eventually to express thoughts and complement vocalized ideas.\n\nGestures not only complement language development but also enhance the child’s ability to communicate. Gestures allow the child to convey a message or thought that they would not be able to easily express using their limited vocabulary. Children's gestures are classified into different categories occurring in different stages of development. The categories of children's gesture include deictic and representational gestures.\n\nGestures are distinct from manual signs in that they do not belong to a complete language system. For example, pointing through the extension of a body part, especially the index finger to indicate interest in an object is a widely used gesture that is understood by many cultures On the other hand, manual signs are conventionalized—they are gestures that have become a lexical element in a language. A good example of manual signing is American Sign Language (ASL)–when individuals communicate via ASL, their signs have meanings that are equivalent to words (e.g., two people communicating using ASL both understand that forming a fist with your right hand and rotating this fist using clockwise motions on the chest carries the lexical meaning of the word \"sorry\").\n\nTypically, the first gestures children show around 10 to 12 months of age are deictic gestures. These gestures are also known as pointing where children extend their index finger, although any other body part could also be used, to single out an object of interest. Deictic gestures occur across cultures and indicate that infants are aware of what other people pay attention to. Pre-verbal children use pointing for many different reasons, such as responding to or answering questions and/or sharing their interests and knowledge with others.\n\nThere are three main functions to infant's pointing:\n\nThe existence of deictic gestures that are declarative and epistemic in nature reflects another important part of children's development, the development of joint visual attention. Joint visual attention occurs when a child and an adult are both paying attention to the same object. Joint attention through the use of pointing is considered a precursor to speech development because it reveals that children want to communicate with another person. Furthermore, the amount of pointing at 12 months old predicts speech production and comprehension rates at 24 months old.\n\nOnce children can produce spoken words they often use deictic gestures to create sentence-like phrases. These phrases occur when a child, for example, says the word \"eat\" and then points to a cookie. The incidence of these gesture-word combinations predicts the transition from one-word to two-word speech. This shows that gesture can maximize the communicative opportunities that children can have before their speech is fully developed facilitating their entrance into lexical and syntactic development.\n\nRepresentational gesture refers to an object, person, location, or event with hand movement, body movement, or facial expression. Representational gestures can be divided into iconic and conventional gestures. Unlike deictic gestures, representational gestures communicate a specific meaning. Children start to produce representational gestures at 10 to 24 months of age. Young American children will produce more deictic gestures than representational gestures, but Italian children will produce almost equal amounts of representational and deictic gestures.\n\nIconic gestures have visually similar relationship to the action, object, or attribute they portray. There is an increase in iconic gesturing after the two-word utterance stage at 26 months. Children are able to create novel iconic gestures when they were attempting to inform the listener of information they think the listener does not know. Iconic gestures aided language development after the two-word utterance stage, whereas deictic gestures did not. Iconic gestures are the most common form of representational gesture in Italian children. Children will copy the iconic gestures they see their parents using, therefore including iconic gestures when measuring representational vocabularies increases Italian children's vocabularies. Even though the Italian children produced more iconic gestures, the two-word utterance stage did not arrive earlier than American children who produce fewer iconic gestures.\n\nConventional gestures are culture-bound emblems that do not translate across different cultures. Culture-specific gestures such as shaking your head \"no\" or waving \"goodbye\" are considered conventional gestures. Although American children do not typically produce many representational gestures in general, conventional gestures are the most frequently used in the representational gesture category.\n\nLike most developmental timelines it is important to consider that no two children develop at the same pace. Infant gesture is thought to be an important part of the prelinguistic period and prepares a child for the emergence of language. It has been suggested that language and gesture develop in interaction with one another. It is believed that gestures are easier to produce for both infants and adults; this is supported by the fact that infants begin to communicate with gestures before they can produce words. The first type of gestures that appear in infants are deictic gestures. Deictic gestures include pointing, which is often the most common gesture produced at ten months of age. At eleven months of age children can produce a sequence of 2 gestures, usually a deictic gesture with a conventional or representational gesture. and by twelve months of age children can begin to produce 3-gestures in sequence usually a representational or conventional gesture that is preceded and followed by a deictic gesture. Around twelve months of age, infants begin to use representational gestures. In relation to language acquisition, representational gestures appear around the same time as first words. At age 18 months children produce more deictic gestures than representational gestures. Between the first and second year of life, children begin to learn more words and use gestures less. At 26 months of age, there is an increase in iconic gesture use and comprehension. Gestures become more complex as children get older. Between age 4-6 children can use whole body gestures when describing a route. A whole body gesture occurs in three-dimensional space and is used when the speaker is describing a route as if they are on it. At ages 5–6, children also describe a route from a bird's eye view and use representational gestures from this point of view. The ways in which gestures are used are an indication of the developmental or conceptual ability of children.\n\nNot only do gestures play an important role in the natural development of spoken language, but they also are a major factor in augmentative and alternative communication (AAC). AAC refers to the methods, tools, and theories to use non-standard linguistic forms of communication by and with individuals without or with limited functional speech. Means used to communicate in AAC can span from high-tech computer-based communication devices, to low-tech means such as one-message switches, to non-tech means such as picture cards, manual signs, and gestures. It is only within the last two decades that the importance of gestures in the cognitive and linguistic development processes has been examined, and in particular the gesture's functionality for individuals with communication disorders, especially AAC users.\n\n"}
{"id": "55251262", "url": "https://en.wikipedia.org/wiki?curid=55251262", "title": "Gildersleeve Prize", "text": "Gildersleeve Prize\n\nThe Gildersleeve Prize is an annual award of $1,000 to the author of \"the best article of the year\" published in the \"American Journal of Philology\". It is awarded by The Johns Hopkins University Press and is named after the classical scholar Basil Lanneau Gildersleeve who founded the journal.\n\nPrevious winners are:\n"}
{"id": "38293879", "url": "https://en.wikipedia.org/wiki?curid=38293879", "title": "Idea networking", "text": "Idea networking\n\nIdea networking is a qualitative method of doing a cluster analysis of any collection of statements, developed by Mike Metcalfe at the University of South Australia. Networking lists of statements acts to reduce them into a handful of clusters or categories. The statements might be source from interviews, text, web sites, focus groups, SWOT analysis or community consultation. Idea networking is inductive as it does not assume any prior classification system to cluster the statements. Rather keywords or issues in the statements are individually linked (paired). These links can then be entered into network software to be displayed as a network with clusters. When named, these clusters provide emergent categories, meta themes, frames or concepts which represent, structure or sense-make the collection of statements.\n\nAn idea network can be constructed in the following way:\nThe number of links per statement should be from 1 to 7; many more will result in a congested network diagram. This means choosing why the statements are linked may need grading as strong or weak, or by sub sets. For example, statements linked as being about weather conditions may be further subdivided into those about good weather, wet weather or bad weather, etc.). This linking is sometimes called 'coding' in thematic analysis which highlights that the statements can be linked for several and different reasons (source, context, time, etc.). There maybe many tens of reasons why statements are linked. The same statements may be linked for different reasons. The number of reasons should not be restricted to low number as so anticipate the resultant clustering. \n\n\nIn his book \"Notes on the Synthesis of Form\", the pragmatist Christopher Alexander suggested networking the ideas of clients as means to identifying the major facets of an architectural design. This is still used modern design work usually using cluster analysis. Modern social network analysis software provides a useful tool for how these ideas can be networked. \nThis simply adds ideas to the list of computers, power stations, people and events that can be networked (see Network theory). The links between ideas can be represented in a matrix or network. Modern network diagramming software, with node repulsion algorithms, allows useful visual representation of these networks revealing clusters of nodes.\n\nWhen networking peoples' statements or ideas, these become the nodes and the links are provided by an analyst linking those statements thought to be similar. Keywords, synonyms, experience or context might be used to provide this linking. For example, the statements: (1) \"That war is economics progressed by other means, might be considered linked to the statement\"; (2) \"That progress unfortunately needs the innovation which is a consequence of human conflict\". \nLinguistic pragmatism argues we use our conceptions to interpret our perceptions (sensory inputs). These conceptions might be represented by words as conceptual ideas or concepts. For example, if we use the conceptual idea or concepts of justice to interpret the actions of people, we get a different interpretation (or meaning) compared to using the conceptual idea of personal power. Using the conceptual idea of justice makes certain action ideas seem reasonable. These may include due process, legal representation, hearing both sides, have norms or regulations for comparison. Therefore, there is a relationship between conceptual ideas and related apparently rational action ideas.\n\nIf the statements gathered at a consultative meeting are considered action ideas, then clusters of these similar actions ideas might be considered to examples of a meta idea or conceptual idea. These are also called themes, and frames. Modern research extending Miller's Magic number 7 plus or minus 2, to idea handling, suggests a five-part classification is appropriate for humans.\n\nUsing networking to cluster statements is considered useful because:\n\n"}
{"id": "59114506", "url": "https://en.wikipedia.org/wiki?curid=59114506", "title": "Influence of French on English", "text": "Influence of French on English\n\nThere has been a long-standing influence of French on English, in terms of syntax, grammar, lexicon, spelling and pronunciation.\n\nMost French vocabulary that entered English occurred after the Norman conquest of England in 1066 and the establishment of a French-speaking administration. French became the language of the court, the administration and the elites for several centuries, until after the Hundred Years War. English has been constantly influenced by French from that time till present day.\nAccording to Laura K. Lawless, more than a third of the current English vocabulary is of French origin. According to linguist Henriette Walter, words of French origin represent more than two thirds of the English vocabulary. It is estimated by linguist Anthony Lacoudre that over 40 000 English words are directly French and may be understood without orthographical change by French speakers.\n\nAt the beginning of XIth century, the English language did not have a well-defined status. Indeed, the inhabitants of what would become Great Britain did not have a language that allowed them to communicate with each other. There were many different dialectal forms. Great Britain, in which various Celtic idioms had coexisted since the IVth century, had experienced partial Roman occupation since the 1st century A.D., and this for four centuries.\n\nFrom 450 onwards, the Saxons, the Angles and the Jutes, who came from the continent, settled in the south and east. Germanic dialects would prevail in these regions, supplanting Celtic dialects, which would remain in the west and north of the island (Wales, Cornwall, Scotland) and Ireland. In the VIIIth century, Vikings from Scandinavia settled on the island. Their languages, also Germanic, in turn influence the languages already present on the island. Thus, at the dawn of XIth century, the country was made up of a series of peoples with significantly different speeches, most of them Germanic, with multiple influences.\n\nIt is therefore a linguistically disunited people that the Normans will get massively in contact with, from 1066. William II of Normandy, supported by his King, Philip I of France, and his blood legitimacy to the throne of England, landed at Hastings, in Sussex, on 29 September 1066. His men are deployed around the city waiting for the king Harold II's troops. On October 14, exhausted by the long journey to Hastings, Harold II's troops lost the battle after a day. Following the defeat of the English, Duke William II of Normandy became King of England on December 25, 1066, crowned under the name of William I of England, also known as William the Conqueror. This date marks the beginning of a long period of ties between the peoples and languages.\n\nIn fact, these links already existed before the Battle of Hastings. Indeed, the geographical location of Normandy, facing the English Channel, favoured commercial contacts with England. These ties will be further strengthened at the beginning of the XIth century when the daughter of the Duke Richard II of Normandy, Emma, marries King Æthelred II of England. But it is really from the 1066 conquest that proto-English becomes massively impregnated with Old French, then modern French. It should be noted, however, that only French will influence English in the centuries following the conquest. The reverse contribution of English to French will only be real from the XVIIIth century.\n\nThe arrival of William the Conqueror and his barons significantly changed the linguistic situation in England. Norman is essentially imposed in the upper layers of society. The Anglo-Saxon dialects were supplanted by Norman in the circles of the court and aristocracy, justice and the Church. The influential circles, who came from Normandy and settled in England, kept their Norman mother tongue, while the more modest rural and urban strata continued to speak English.\n\nNorman is a particular variety of the Gallo-Roman language, spoken in Normandy. It is one of the Oil languages alongside, among others, the Picard and the Walloon. The Norman language is modified in contact with the Anglo-Saxon language. It then integrates words and phrases from English and will give birth to a dialect, Anglo-Norman, still spoken on the Anglo-Norman isles. Anglo-Norman can be described as a vernacular language, on English soil in the XIth century, in the field of literature, culture, court and among the clergy. French was therefore, at first, spoken in England under the form of this Anglo-Norman dialect.\n\nDuring the XIIth century, continental French has a greater influence on Old English. It acquires great prestige in England, especially within the aristocracy and the clergy. It becomes the language of law and justice nationwide. Rich and noble families, most of them of Norman origin, teach their children French or send them to study in France. The expansion of the French language in England was also encouraged by royal marriages. From Henry II Plantagenet and Eleanor of Aquitaine at the beginning of the century, to Henri VI and Marguerite in the XVth century, all kings of England married French princesses. These marriages made French the language of the English court for several centuries and were decisive in strengthening the use of French in England. This period (XIIth-XVth centuries) is characterized by a massive influx of French words into Old English vocabulary.\n\nIn 1204, Philippe Auguste Normandy is officially annexed to the kingdom of France, politically isolating England from the continent. Normans who choose to stay in England move further away from France and, therefore, from the French language. Keeping its status as the language of justice and the language of power, England saw the first teaching manuals for teaching French to the English. These manuals were intended for English nobles who wish to perfect their knowledge of French and teach it to their children. Two types of French spoken in the higher spheres of English society can be distinguished during the XIIIth century : the Anglo-Norman dialect, which was the aristocrats' mother tongue, and a more prestigious type of French as a second language. Knowing \"parisian\" French was a mark of social distinction. As a language of culture, French supplanted Latin from the XIIth century onward as the language of diplomacy and worldly relations throughout Europe. The mass and influence of French literature reinforced its reputation and appeal.\n\nThe XVIth century, that of the Renaissance, is a decisive century for French since king François I of France, through the Ordonnance de Villers-Cotterêts (1539), makes French the official language of administration in the whole kingdom.\nAlthough troubled by the European wars of religion, the Italian Wars, the language is marked by intellectual, technical and scientific effervescence. It ushered in an era of prosperity that would also spread to England through French.\n\nThe XVIIth century announces the apogee of the Kingdom of France. This period was characterized by the political, literary and artistic prestige of France and the French language. Peace restored and unity ensured in the country, economy grew considerably. Personalities such as the King Henri IV, the Cardinal of Richelieu or the Sun King contribute to fixing and enhancing the French language in Europe, the Americas, India and Oceania.\n\nThe creation of the Académie française by Richelieu in 1635, under Louis XIII, was a step that led to the standardization of French in continental Europe and abroad, including England. French is then the second language of all the elites in Europe, from Turkey to Ireland and from Moscow to Lisbon. The greatest scholars and intellectuals, writers and scientists, express themselves and correspond in this new standardised French. French is considered a perfect language, whose beauty and elegance are determined by the development of scientific logic, aided by dictionaries and grammars.\n\nThe geographical use of French has continuously and greatly diversified in the last five hundred years, with countries and states like New-Brunswick, Quebec, Ivory Coast, Benin, Togo, Guinea, Cameroon, Congo, Democratic Republic of the Congo, Madagascar, Mauritius, Tchad, Djibouti, Senegal, Morocco, Algeria, Tunisia, Lebanon, France, Belgium, Switzerland, Luxembourg, Monaco, Aosta Valley, French Polynesia, New Caledonia, and Vanuatu adopting it as their official language. This geographical diversity has led to many different contacts with vernacular dialects, regional and international languages, from which French has often been enriched locally.\nIn a number of countries and regions where French shares co-officiality with English (Cameroon, Canada, Jersey, Mauritius, Rwanda, Vanuatu), particular lexical regionalisms are observed where French and English terms are used interchangeably.\n\nSeveral elements must be observed.\n\n\nThe following French glossary in English is in no way exhaustive. These words come as examples to illustrate the countless French words that are part of the English language.\n\nIn this section, examples of French-to-English lexical contributions are classified by field and in chronological order. The periods during which these words were used in the English language are specified as much as possible. It is not always possible to state with certainty the precise period from which a word was borrowed or integrated.\n\nThe English word is on the left, with its current French equivalent in brackets, then comes its Old French origin in bold and the century of its introduction on the right.\n\n\n\n\n\n\n\n\n\n"}
{"id": "1610290", "url": "https://en.wikipedia.org/wiki?curid=1610290", "title": "Interlanguage", "text": "Interlanguage\n\nAn interlanguage is an idiolect that has been developed by a learner of a second language (or L2) which preserves some features of their first language (or L1), and can also overgeneralize some L2 writing and speaking rules. These two characteristics of an interlanguage result in the system's unique linguistic organization.\n\nAn interlanguage is idiosyncratically based on the learners' experiences with the L2. It can \"fossilize\", or cease developing, in any of its developmental stages. The interlanguage rules are claimed to be shaped by several factors, including L1-transfer, previous learning strategies, strategies of L2 acquisition (i.e., simplification), L2 communication strategies (i.e., circumlocution), and overgeneralization of L2 language patterns.\n\nInterlanguage is based on the theory that there is a dormant psychological framework in the human brain that is activated when one attempts to learn a second language. Interlanguage theory is often credited to Larry Selinker, who coined the terms \"interlanguage\" and \"fossilization.\" Uriel Weinreich is credited with providing the foundational information that was the basis of Selinker's research. Selinker (1972) noted that in a given situation, the utterances produced by a learner are different from those native speakers would produce had they attempted to convey the same meaning. This comparison suggests the existence of a separate linguistic system. This system can be observed when studying the utterances of the learner who attempts to produce meaning in their L2 speech; it is not seen when that same learner performs form-focused tasks, such as oral drills in a classroom.\n\nInterlanguage can be variable across different contexts; for example, it may be more accurate, complex and fluent in one domain than in another.\n\nTo study the psychological processes involved one can compare the interlanguage utterances of the learner with two things:\n\nIt is possible to apply an interlanguage perspective to a learner's underlying knowledge of the target language sound system (interlanguage phonology), grammar (morphology and syntax), vocabulary (lexicon), and language-use norms found among learners (interlanguage pragmatics).\n\nBy describing the ways in which learner language conforms to universal linguistic norms, interlanguage research has contributed greatly to our understanding of linguistic universals in second-language acquisition.\n\nBefore the interlanguage hypothesis rose to prominence, the principal theory of second-language (L2) development was contrastive analysis. This theory assumed that learners' errors were caused by the difference between their L1 and L2. This approach was deficit-focused, in the sense that speech errors were thought to arise randomly and should be corrected. A further assumption followed that a sufficiently thorough analysis of the differences between learners' first and second languages could predict all of the difficulties they would face. This assumption was not based in rigorous analysis of learner language but rather was often anecdotal, and researchers' claims were prone to confirmation bias.\n\nRobert Lado (1957) held that the claims of contrastive analysis should be viewed as hypothetical unless and until they were based on systematic analyses of learner speech data. Around this time, second-language acquisition research shifted from hypotheses of language learning and the development of language-teaching materials to the systematic analysis of learner speech and writing with the practice of error analysis. Although this was initially done to validate the claims of contrastive analysis, researchers found that many learner behaviours could not be easily explained by transfer from learners' L1 to their L2.\n\nThe idea that language learners' linguistic systems were different from both their L1 and L2 was developed independently at around the same time by several different researchers. William Nemser called it an \"approximative system\" and Pit Corder called it \"transitional competence\".\n\nInterlanguage is claimed to be a language in its own right. Learner language varies much more than native-speaker language. Selinker noted that in a given situation the utterances produced by the learner are different from those native speakers would produce had they attempted to convey the same meaning. This comparison reveals a separate linguistic system.\n\nInterlanguage can be observed to be variable across different contexts. For example, it may be more accurate, complex and fluent in one discourse domain than in another. Variability is observed when comparing the utterances of the learner in conversation to form-focused tasks, such as memory-based oral drills in a classroom. Spontaneous conversation is more likely to involve the use of interlanguage. A learner may produce a target-like variant (e.g. 'I don't') in one context and a non-target like variant (e.g. 'me no') in another. Scholars from different traditions have taken opposing views on the importance of this phenomenon. Those who bring a Chomskyan perspective to second-language acquisition typically regard variability as nothing more than performance errors, and not worthy of systematic inquiry. On the other hand, those who approach it from a sociolinguistic or psycholinguistic orientation view variability as an inherent feature of the learner's interlanguage. In these approaches, a learner's preference for one linguistic variant over another can depend on social (contextual) variables such as the status or role of the person the learner is speaking to. Preference can also be based on linguistic variables such as the phonological environment or neighboring features marked for formality or informality.\n\nVariability in learner language distinguishes between \"free variation\", which has not been shown to be systematically related to accompanying linguistic or social features, and \"systematic variation\", which has.\n\nFree variation in the use of a language feature is usually taken as a sign that it has not been fully acquired. The learner is still trying to figure out what rules govern the use of alternate forms. This type of variability seems to be most common among beginning learners, and may be entirely absent among the more advanced.\n\nSystematic variation is brought about by changes in the linguistic, psychological, and social context. Linguistic factors are usually extremely local. For example, in earlier stages of acquisition, a learner will often display systematic constraints on their ability to use the correct tense. They may say \"Last year we travel to the ocean\" rather than \"Last year we travelled to the ocean.\" They also tend to make more mistakes when the word following a tensed word begins with a consonant (e.g., burned bacon). But they will show higher accuracy when the word following the tensed word begins with a nonconsonant (e.g., burned eggs).\n\nSocial factors may include a change in register or the familiarity of interlocutors. In accordance with communication accommodation theory, learners may adapt their speech to either converge with, or diverge from, their interlocutor's usage. For example, they may deliberately choose to address a non-target form like \"me no\" to an English teacher in order to assert identity with a non-mainstream ethnic group.\n\nThe most important psychological factor is usually regarded as attention to form, which is related to planning time. The more time that learners have to plan, the more target-like their production may be. Thus, literate learners may produce much more target-like forms in a writing task for which they have 30 minutes to plan, than in conversation where they must produce language with almost no planning at all. The impact of alphabetic literacy level on an L2 learner's ability to pay attention to form is as yet unclear.\n\nAffective factors also play an important role in systematic variation. For example, learners in a stressful situation (such as a formal exam) may produce fewer target-like forms than they would in a comfortable setting. This clearly interacts with social factors, and attitudes toward the interlocutor and topic also play important roles.\n\nIndividuals learning a second language may not always hear spoken L2 words as separate units. Some words might blend together and become a single unit in the learner's L2 system. The blended words are called \"prefabricated patterns\" or \"chunks\". These chunks are often not immediately obvious to the learner or anyone that listens to them speak, but may be noticed as the learner's L2 system becomes more developed and they use the chunk in a context where it does not apply. For example, if an English learner hears sentences beginning with \"do you\", they may associate it with being an indicator of a question but not as two separate words. To them, the word is \"doyou\". They may happen to say \"What do you doing?\" instead of \"What are you doing?\" Eventually the learner will learn to break the chunk up in to its component words and use them correctly.\n\nWhen learners experience significant restructuring in their L2 systems, they sometimes show a U-shaped learning pattern. For instance, a group of English language learners moved, over time, from accurate usage of the \"-ing\" present progressive morpheme, to incorrectly omitting it, and finally, back to correct usage. Occasionally the period of incorrect usage is seen as a learning regression. However, it is likely that when the learners first acquired the new \"-ing\" morpheme or \"chunk\", they were not aware of all of the rules that apply to its use. As their knowledge of tense in English expanded, this disrupted their correct usage of the morpheme. They eventually returned to correct usage when they gained greater understanding of the tense rules in English. These data provide evidence that the learners were initially producing output based on rote memory of individual words containing the present progressive \"-ing\" morpheme. However, in the second stage their systems contained the rule that they should use the bare infinitive form to express present action, without a separate rule for the use of \"-ing\". Finally, they learned the rule for appropriate use of \"-ing\".\n\nThe \"chunking\" method enables a learner to practice speaking their L2 before they correctly break the chunk up in to its component parts. According to interlanguage theory, this seeming progression and regression of language learning is an important and positive manifestation of the learner's developing understanding of the grammar of the target language.\n\nAn interlanguage can fossilize, or cease developing, in any of its developmental stages. Fossilization is the process of 'freezing' of the transition between the L1 and L2, and is regarded as the final stage of interlanguage development. It can occur even in motivated learners who are continuously exposed to their L2 or have adequate learning support. Reasons for this phenomenon may be due to complacency or inability to overcome the obstacles to acquiring native proficiency in the L2. Fossilization occurs often in adult language learners. It can also occur when a learner succeeds in conveying messages with their current L2 knowledge. The need to correct the form/structure is therefore not present. The learner fossilizes the form instead of correcting it.\n\nResearch on universal grammar (UG) has had a significant effect on second-language acquisition (SLA) theory. In particular, scholarship in the interlanguage tradition has sought to show that learner languages conform to UG at all stages of development.\n\nInterlanguage UG differs from native UG in that interlanguage UGs vary greatly in mental representations from one L2 user to another. This variability arises from differing relative influences on the interlanguage UG, such as existing L1 knowledge and UG constraints. An example of a UG constraint is an \"island constraint,\" where the \"wh\"-phrase in a question has a finite number of possible positions. Island constraints are based on the concept that there are certain syntactical domains within a sentence that act as phrase boundaries. It is theorized that the same constraints that act on a native UG are also often present in an interlanguage UG.\n\nThe concept of interlanguage is closely related to other types of language, especially creoles and pidgins. Each of these languages has its own grammar and phonology. The difference is mostly one of variability, as a learner's interlanguage is spoken only by the learner and changes frequently as they become more proficient in the language. In contrast, creoles and pidgins are generally the product of groups of people in contact with another language, and therefore may be more stable.\n\n\n"}
{"id": "51704277", "url": "https://en.wikipedia.org/wiki?curid=51704277", "title": "Juggling terminology", "text": "Juggling terminology\n\nJuggling terminology, juggling (especially toss juggling) terms:\n\n"}
{"id": "34869823", "url": "https://en.wikipedia.org/wiki?curid=34869823", "title": "Kanite language", "text": "Kanite language\n\nKanite is a Papuan language spoken in Eastern Highlands Province, Papua New Guinea.\n\n"}
{"id": "31692010", "url": "https://en.wikipedia.org/wiki?curid=31692010", "title": "Kaskian language", "text": "Kaskian language\n\nKaskian (Kaskean) was a non-Indo-European language of the Kaskians of northeastern Bronze Age Anatolia, in the mountains along the Black Sea coast.\n\nIt is sometimes suspected that Kaskian was related to the pre-Hittite Hattic language, based on toponyms and personal names. There may also be connections to the Northwest Caucasian languages; the name \"Kaskian\" may be cognate with an old name for Circassia, and the name of one of the tribes in the Kaskian confederation, the Abešla, may be cognate with the endonym of the Abkhaz people and some Circassian people, suggesting the Kaskians proper and Abešla might have been the ancestors of the Circassians and other Caucasian peoples. It has also been conjectured that Kaskian might belong to the Zan family of languages, and have affinities to Megrelian or Laz.\n"}
{"id": "373867", "url": "https://en.wikipedia.org/wiki?curid=373867", "title": "Language policy", "text": "Language policy\n\nMany countries have a language policy designed to favor or discourage the use of a particular language or set of languages. Although nations historically have used language policies most often to promote one official language at the expense of others, many countries now have policies designed to protect and promote \nregional and ethnic languages whose viability is threatened. Indeed, whilst the existence of linguistic minorities within their jurisdiction has often been considered to be a potential threat to internal cohesion, States also understand that providing language rights to minorities may be more in their long term interest, as a means of gaining citizens' trust in the central government.\n\nLanguage policy is what a government does either officially through legislation, court decisions or policy to determine how languages are used, cultivate language skills needed to meet national priorities or to establish the rights of individuals or groups to use and maintain languages. The scope of language policy varies in practice from State to State. This may be explained by the fact that language policy is often based on contingent historical reasons. Likewise, States also differ as to the degree of explicitness with which they implement a given language policy. The French Toubon law is a good example of explicit language policy. The same may be said for the Charter of the French Language in Quebec.\n\nThe preservation of cultural and linguistic diversity in today's world is a major concern to many scientists, artists, writers, politicians, leaders of linguistic communities, and defenders of linguistic human rights. More than half of the 6000 languages currently spoken in the world are estimated to be in danger of disappearing during the 21st century. Many factors affect the existence and usage of any given human language, including the size of the native speaking population, its use in formal communication, and the geographical dispersion and the socio-economic weight of its speakers. National language policies can either mitigate or exacerbate the effects of some of these factors.\n\nFor example, according to Ghil'ad Zuckermann, \"Native tongue title and language rights should be promoted. The government ought to define Aboriginal and Torres Strait Islander vernaculars as official languages of Australia. We must change the linguistic landscape of Whyalla and elsewhere. Signs should be in both English and the local indigenous language. We ought to acknowledge intellectual property of indigenous knowledge including language, music and dance.\"\n\nThere are many ways in which language policies can be categorized. It was elaborated by Université Laval sociolinguist Jacques Leclerc for the French-language Web site \"L'aménagement linguistique dans le monde\" put on line by the CIRAL in 1999. The collecting, translating and classifying of language policies started in 1988 and culminated in the publishing of \"Recueil des législations linguistiques dans le monde\" (vol. I to VI) at Presses de l'Université Laval in 1994. The work, containing some 470 language laws, and the research leading to publication, were subsidised by the Office québécois de la langue française. In April 2008, the Web site presented the linguistic portrait and language policies in 354 States or autonomous territories in 194 recognised countries.\n\n\nDirections of language policies:\n\n\nSome case studies:\n\n\n"}
{"id": "65373", "url": "https://en.wikipedia.org/wiki?curid=65373", "title": "Lingua franca", "text": "Lingua franca\n\nA lingua franca (; ), also known as a bridge language, common language, trade language, auxiliary language, vehicular language, or link language is a language or dialect systematically used to make communication possible between people who do not share a native language or dialect, particularly when it is a third language that is distinct from both of the speakers' native languages.\n\nLingua francas have developed around the world throughout human history, sometimes for commercial reasons (so-called \"trade languages\" facilitated trade) but also for cultural, religious, diplomatic and administrative convenience, and as a means of exchanging information between scientists and other scholars of different nationalities. The term is taken from the medieval Mediterranean Lingua Franca, a Romance-based pidgin language used by European merchants and sailors during the 2nd millennium. A world language – a language spoken internationally and learned and spoken by a large number of people – is a language that may function as a global lingua franca.\n\nLingua Franca refers to any language used for communication between people who do not share a native language. It can refer to hybrid languages such as pidgins and creoles used for communication between language groups. It can also refer to languages which are native to one nation (often a colonial power) but used as a second language for communication between groups. Finally, lingua franca can refer to a third language which allows inter-comprehension among people speaking different mother tongues as a neutral language or jargon of which nobody can claim ownership. Lingua Franca is a functional term, independent of any linguistic history or language structure. \n\nWhereas a vernacular language is the native language of a specific geographical community, a lingua franca is used beyond the boundaries of its original community, for trade, religious, political or academic reasons. For example, English is a vernacular in the United Kingdom but is used as a lingua franca in the Philippines. Arabic, French, Mandarin Chinese, Spanish, Portuguese, and Russian, serve a similar purpose as industrial/educational lingua francas, across regional and national boundaries.\n\nInternational auxiliary languages such as Esperanto and Lingua Franca Nova have not had a great degree of adoption globally so they cannot be described as global lingua francas.\n\nThe term \"lingua franca\" derives from Mediterranean Lingua Franca, the \"language of the Francs\", \"Francs\" being the term used to refer to all Western Europeans as opposed to the Greeks — is considered to be a translation of the Arab lûghat al-Ifranj, which appears for the first time in an Arabic text from the 9th century. However, Western speakers and their contemporaries only began to use the term \"lingua franca\" to describe this phenomenon at the beginning of the 16th century and the language that people around the Levant and the eastern Mediterranean Sea used as the main language of commerce and diplomacy from late medieval times, especially during the Renaissance era, to the 18th century and allowed communication between the people living, fighting in the area without becoming the vernacular language of any of them. At that time, Italian-speakers dominated seaborne commerce in the port cities of the Ottoman Empire and a simplified version of Italian, including many loan words from Greek, Old French, Portuguese, Occitan, and Spanish as well as Arabic and Turkish came to be widely used as the \"lingua franca\" (in the generic sense used) of the region.\n\nIn Lingua Franca (the specific language), \"lingua\" means a language, as in Portuguese and Italian, and \"franca\" is related to \"phrankoi\" in Greek and \"faranji\" in Arabic as well as the equivalent Italian. In all three cases, the literal sense is \"Frankish\", but the name actually applied to all Western Europeans during the late Byzantine Empire.\n\nThe Douglas Harper Etymology Dictionary states that the term \"Lingua Franca\" (as the name of the particular language) was first recorded in English during the 1670s, although an even earlier example of the use of \"Lingua Franca\" in English is attested from 1632, where it is also referred to as \"Bastard Spanish\".\n\nAs recently as the late 20th century, some restricted the use of the generic term to mean only hybrid languages that are used as vehicular languages, its original meaning, but it now refers to any vehicular language.\n\nThe term is well established in its naturalization to English, which is why major dictionaries do not italicize it as a \"foreign\" term. Its plurals in English are lingua francas and linguae francae, with the first of those being first-listed or only-listed in major dictionaries.\n\nThe use of lingua francas has existed since antiquity. The first example of lingua franca in the ancient world, before Greek and Latin, was Aramaic. Latin and Koine Greek were the lingua francas of the Roman Empire and the Hellenistic culture. Akkadian (died out during Classical antiquity) and then Aramaic remained the common languages of a large part of Western Asia from several earlier empires. \n\nIn certain countries, the lingua franca is also the national language. Indonesian has the same function in Indonesia, although Javanese has more native speakers. Still, Indonesian is the sole official language and is spoken throughout the country. Also Persian is both the lingua franca of Iran and its national language.\n\nThe Hindustani language (Hindi-Urdu) is the lingua franca of Pakistan and Northern India. Many Indian states have adopted the Three-language formula in which students in Hindi speaking states are taught: \"(a) Hindi (with Sanskrit as part of the composite course); (b) Urdu or any other modern Indian language and (c) English or any other modern European language.\" The order in non-Hindi speaking states is: \"(a) the regional language; (b) Hindi; (c) Urdu or any other modern Indian language excluding (a) and (b); and (d) English or any other modern European language.\" Hindi has also emerged as a lingua franca for the locals of Arunachal Pradesh, a linguistically diverse state in Northeast India.It is estimated that 90 percent of the state's population knows Hindi.\n\nThe only documented sign language used as a lingua franca is Plains Indian Sign Language, used across much of North America. It was used as a second language across many indigenous peoples. Alongside or a derivation of Plains Indian Sign Language was Plateau Sign Language, now extinct. Inuit Sign Language could be a similar case in the Arctic among the Inuit for communication across oral language boundaries, but little research exists.\n\nIn the European Union, the use of English as a lingua franca has led to the emergence of a new dialect called Euro English.\n\n\n\n"}
{"id": "2496534", "url": "https://en.wikipedia.org/wiki?curid=2496534", "title": "List of English terms of venery, by animal", "text": "List of English terms of venery, by animal\n\nThis is a list of English terms of venery (\"venery\" being an archaic word for hunting), comprising terms from a tradition that arose in the Late Middle Ages, at least partly from the \"Book of Saint Albans\" of 1486, a historic list of \"company terms\". The present list also includes more common collective terms (such as \"herd\" and \"flock\") for some animals.\n\nStandard terms for particular groups are listed first in each group and shown in bold.\n\n"}
{"id": "3025505", "url": "https://en.wikipedia.org/wiki?curid=3025505", "title": "List of language subsystems", "text": "List of language subsystems\n\nIn linguistics, languages are often studied in terms of six major subsystems, which relate to major subfields within linguistics. In addition, particular subfields of linguistic inquiry may divide their subject matter into more specific subsystems. This list contains links to commonly studied language subsystems.\n\nLinguists recognize seven major language subsystems:\n\n\nThis division varies among linguists and authors. For example, phonetics and phonology are occasionally merged into one subsystem. Morphology and lexicology can also be merged.\n\n"}
{"id": "14200266", "url": "https://en.wikipedia.org/wiki?curid=14200266", "title": "List of municipalities of Finland in which Finnish is not the sole official language", "text": "List of municipalities of Finland in which Finnish is not the sole official language\n\nThere are 53 municipalities of Finland in which Finnish is not the sole official language. In Finland, as of December 31, 2013, 89.3% of the population speak Finnish, 5.3% Swedish and 0.04% Sami languages. Both Finnish and Swedish are official languages of Finland. Officially, a municipality is bilingual if the minority language group consists of at least 8% of the population, or at least 3,000 speakers. A previously bilingual municipality remains so if the linguistic minority proportion drops below 8%, up to 6%. If it drops below 6%, it is possible for the municipality to remain bilingual by government decree, on the recommendation of the municipal council, for a further ten years. Municipalities that make use of the 3,000-speaker rule include the national capital Helsinki and the cultural center of Swedish Finns, Turku. On the Åland archipelago, where Finnish is almost absent from daily life, the language law does not apply. On the mainland, the highest proportion of Swedish-speakers is found on the western coast, in Ostrobothnia.\n\nOf the 317 Finnish municipalities, 16 are monolingually Swedish. 33 municipalities are bilingually Finnish and Swedish; of these, 15 have a Swedish-speaking majority and 18 a Finnish-speaking one. Four municipalities, all located in Lapland, have a Finnish-speaking majority and a Sami-speaking minority: Enontekiö, Inari, Sodankylä and Utsjoki. Initially, only Swedish was accorded official bilingualism, through a language act of 1922; similar provisions were extended to Sami through a 1991 law. The 1922 law was replaced by new but largely similar legislation in 2003.\n\n"}
{"id": "2530829", "url": "https://en.wikipedia.org/wiki?curid=2530829", "title": "Maxim (philosophy)", "text": "Maxim (philosophy)\n\nA maxim is a concise expression of a fundamental moral rule or principle, whether considered as objective or subjective contingent on one's philosophy. A maxim is often pedagogical and motivates specific actions. The \"Oxford Dictionary of Philosophy\" defines it as:\n\nIn deontological ethics, mainly in Kantian ethics, maxims are understood as subjective principles of action. A maxim is thought to be part of an agent's thought process for every rational action, indicating in its standard form: (1) the action, or type of action; (2) the conditions under which it is to be done; and (3) the end or purpose to be achieved by the action, or the motive. The maxim of an action is often referred to as the agent's intention. In Kantian ethics, the categorical imperative provides a test on maxims for determining whether the actions they refer to are right, wrong, or permissible.\n\nThe categorical imperative is stated canonically as: \"Act only according to that maxim whereby you can, at the same time, will that it should become a universal law.\"\n\nIn his \"Critique of Practical Reason\", Immanuel Kant provided the following example of a maxim and of how to apply the test of the categorical imperative:\nI have, for example, made it my maxim to increase my wealth by any safe means. Now I have a deposit in my hands, the owner of which has died and left no record of it. . . . I therefore apply the maxim to the present case and ask whether it could indeed take the form of a law, and consequently whether I could through my maxim at the same time give such a law as this: that everyone may deny a deposit which no one can prove has been made. I at once become aware that such a principle, as a law, would annihilate itself since it would bring it about that there would be no deposits at all.\n\nAlso, an action is said to have \"moral worth\" if the maxim upon which the agent acts cites the purpose of conforming to a moral requirement. That is, a person's action has moral worth when he does his duty purely for the sake of duty, or does the right thing for the right reason. Kant himself believed that it is impossible to know whether anyone's action has ever had moral worth. It might appear to someone that he has acted entirely \"from duty\", but this could always be an illusion of self-interest: of wanting to see oneself in the best, most noble light. This indicates that agents are not always the best judges of their own maxims or motives.\n\nMichael Polanyi in his account of tacit knowledge stressed the importance of the maxim in focusing both explicit and implicit modes of understanding. “Maxims are rules, the correct application of which is part of the art they govern...Maxims can only function within a framework of personal (i.e., experiential) knowledge”.\n\n\n"}
{"id": "20837956", "url": "https://en.wikipedia.org/wiki?curid=20837956", "title": "Mitigated speech", "text": "Mitigated speech\n\nMitigated speech is a linguistic term describing deferential or indirect speech inherent in communication between individuals of perceived High Power Distance which has been in use for at least two decades with many published references. \n\nThe term was popularized by Malcolm Gladwell in his book, \"Outliers\", where he defines mitigated speech as \"any attempt to downplay or sugarcoat the meaning of what is being said\". He continues with reference to Fischer and Orasanu, to describe 6 degrees of mitigation with which we make suggestions to authority: \n\n\nGladwell brings up the concept in the context of how crews relate to each other in the cockpit of a commercial airliner, graphically illustrating the degree to which mitigated speech can be detrimental in high-risk situations which require clear communication.\n\n\n"}
{"id": "14473198", "url": "https://en.wikipedia.org/wiki?curid=14473198", "title": "National Center for Voice and Speech", "text": "National Center for Voice and Speech\n\nThe National Center for Voice and Speech (NCVS), is a multi-site research and teaching organization dedicated to studying the characteristics, limitations and enhancement of human voice and speech. The NCVS is located in Salt Lake City, Utah with the Lead Institution located at the University of Utah. NCVS is also a Center at the University of Iowa where it has laboratories in the Department of Speech Pathology and Audiology. In addition, the NCVS has collaborators in Denver and at many institutions around the United States. Its focus is vocology, or the science and practice of voice habilitation.\n\nInitially conceived as a \"center without walls,\" the NCVS was formally organized in 1990 with the assistance of a grant from the National Institute on Deafness and Other Communication Disorders (NIDCD), an institute of the National Institutes of Health (NIH). The NCVS was organized on the premise that a consortium of institutions (including the Wilber James Gould Voice Center at the DCPA, University of Iowa, University of Utah, University of Wisconsin–Madison) would be better able to conduct and disseminate research than a single organization. NCVS members, although geographically separate, were linked by a common desire to fully understand the characteristics, limitations and enhancement of human voice and speech.\n\nIn 1999, NIDCD discontinued the Multi-Purpose Research and Training Center funding mechanism for the entire institute focusing instead on single-project research awards (R01s). In a July 2000 meeting, however, NCVS investigators voted unanimously to continue the concept of a national resource center for voice and speech, to be driven by a variety of single-project research awards (R01s), as well as health communication, core, and training grants. In 2001, the NCVS moved its central location to Denver, where the otolaryngologist Dr. Wilbur James Gould had founded a center to study the voice and speech patterns of stage performers.\n\nThe NCVS team of investigators, led by Ingo Titze, studies the powers, limitations and enhancement of human voice and speech. The investigators are scientists, clinicians, educators, engineers and musicians who use diverse backgrounds (i.e., speech-language pathology, physics, computer science, acoustics, vocal performance, biology, medicine and engineering) to work together on voice and speech investigations. As a direct outgrowth of their work, NCVS members also teach other investigators and practitioners who work with voice, as well as speech clients and the general public. One example is the Summer Vocology Institute, which trains voice coaches and vocal health professionals in the study of Vocology.\n\n\n\n"}
{"id": "21173", "url": "https://en.wikipedia.org/wiki?curid=21173", "title": "Natural language", "text": "Natural language\n\nIn neuropsychology, linguistics, and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.\n\nThough the exact definition varies between scholars, natural language can broadly be defined in contrast to artificial or constructed languages (such as computer programming languages and international auxiliary languages) and to other communication systems in nature. Such examples include bees' waggle dance and whale song, to which researchers have found or applied the linguistic cognates of dialect and even syntax. \n\nAll language varieties of world languages are natural languages, although some varieties are subject to greater degrees of published prescriptivism or language regulation than others. Thus nonstandard dialects can be viewed as a wild type in comparison with standard languages. But even an official language with a regulating academy, such as Standard French with the French Academy, is classified as a natural language (for example, in the field of natural language processing), as its prescriptive points do not make it either constructed enough to be classified as a constructed language or controlled enough to be classified as a controlled natural language.\n\nControlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind the development and implementation of a controlled natural language typically is to aid non-native speakers of a natural language in understanding it, or to ease computer processing of a natural language. An example of a widely used controlled natural language is Simplified English, which was originally developed for aerospace industry maintenance manuals.\n\nConstructed international auxiliary languages such as Esperanto and Interlingua (even those that have native speakers) are not generally considered natural languages. Natural languages have been used to communicate and have evolved in a natural way, whereas Esperanto was designed by L.L. Zamenhof selecting elements from natural languages, not grown from natural fluctuations in vocabulary and syntax. Some natural languages have become naturally \"standardized\" by children's natural tendency to correct for illogical grammatical structures in their parents' speech, which can be seen in the development of pidgin languages into creole languages (as explained by Steven Pinker in \"The Language Instinct\"), but this is not the case in many languages, including constructed languages such as Esperanto, where strict rules are in place as an attempt to consciously remove such irregularities. The possible exception to this are true native speakers of such languages. More substantive basis for this designation is that the vocabulary, grammar, and orthography of Interlingua are natural; they have been standardized and presented by a linguistic research body, but they predated it and are not themselves considered a product of human invention. Most experts, however, consider Interlingua to be naturalistic rather than natural. Latino Sine Flexione, a second naturalistic auxiliary language, is also naturalistic in content but is no longer widely spoken.\n\n\n"}
{"id": "53212790", "url": "https://en.wikipedia.org/wiki?curid=53212790", "title": "Norvegia transcription", "text": "Norvegia transcription\n\nNorvegia (Latin for \"Norway\") is a phonetic transcription system which was developed by Norwegian linguist Johan Storm in 1884. Norvegia is still employed in the teaching of Scandinavian studies at Norwegian universities.\n\n\n\n"}
{"id": "4532008", "url": "https://en.wikipedia.org/wiki?curid=4532008", "title": "Opaque context", "text": "Opaque context\n\nAn opaque context or referentially opaque context is a linguistic context in which it is not always possible to substitute \"co-referential\" expressions (expressions referring to the same object) without altering the truth of sentences. The expressions involved are usually grammatically singular terms. So, substitution of co-referential expressions into an opaque context does not always preserve truth. For example, \"Lois believes x is a hero\" is an opaque context because \"Lois believes Superman is a hero\" is true while \"Lois believes Clark Kent is a hero\" is false, even though 'Superman' and 'Clark Kent' are co-referential expressions.\n\nThe term is used in philosophical theories of reference, and is to be contrasted with \"referentially transparent context\". In rough outline:\n\n\nSimilar usage of the term applies for artificial languages such as programming languages and logics. The Cicero-Tully example above can be easily adapted. Use the notation formula_1 as a quotation that mentions a term formula_2. Define a predicate formula_3 to the effect that the letters making up a term number six. Then formula_4 induces an opaque context, or is referentially opaque, because formula_5 is true while formula_6 is false. Programming languages often have richer semantics than logics' semantics of truth and falsity, and so an operator such as formula_4 may fail to be referentially transparent for other reasons as well.\n\n"}
{"id": "976531", "url": "https://en.wikipedia.org/wiki?curid=976531", "title": "Outline (list)", "text": "Outline (list)\n\nAn outline, also called a hierarchical outline, is a list arranged to show hierarchical relationships and is a type of tree structure. An outline is used to present the main points (in sentences) or topics (terms) of a given subject. Each item in an outline may be divided into additional sub-items. If an organizational level in an outline is to be sub-divided, it shall have at least two subcategories, as advised by major style manuals in current use. An outline may be used as a drafting tool of a document, or as a summary of the content of a document or of the knowledge in an entire field. It is not to be confused with the general context of the term \"outline\", which a summary or overview of a subject, presented verbally or written in prose (for example, \"The Outline of History\" is not an outline of the type presented below). The outlines described in this article are lists, and come in several varieties.\n\nA sentence outline is a tool for composing a document, such as an essay, a paper, a book, or even an encyclopedia. It is a list used to organize the facts or points to be covered, and their order of presentation, by section. Topic outlines list the subtopics of a subject, arranged in levels, and while they can be used to plan a composition, they are most often used as a summary, such as in the form of a table of contents or the topic list in a college course's syllabus.\n\nOutlines are further differentiated by the index prefixing used, or lack thereof. Many outlines include a numerical or alphanumerical prefix preceding each entry in the outline, to provide a specific path for each item, to aid in referring to and discussing the entries listed. An alphanumerical outline uses alternating letters and numbers to identify entries. A decimal outline uses only numbers as prefixes. An outline without prefixes is called a \"bare outline\".\n\nSpecialized applications of outlines also exist. A reverse outline is a list of sentences or topics that is created from an existing work, as a revision tool; it may show the gaps in the document's coverage so that they may be filled, and may help in rearranging sentences or topics to improve the structure and flow of the work. An integrated outline is a composition tool for writing scholastic works, in which the sources, and the writer's notes from the sources, are integrated into the outline for ease of reference during the writing process.\n\nA software program designed for processing outlines is called an outliner.\n\nOutlines are differentiated by style, the inclusion of prefixes, and specialized purpose. There are also hand-written outlines (which are highly limited in utility), and digitized outlines, such as those contained within an outliner (which are much more useful).\n\nThere are two main styles of outline: sentence outlines and topic outlines.\n\nPropædia is the historical attempt of the Encyclopædia Britannica to present a hierarchical \"Outline of Knowledge\" in a separate volume in the 15th edition of 1974. The \"Outline of Knowledge\" was a project by Mortimer Adler. Propædia had three levels, 10 \"Parts\" at the top level, 41 \"Divisions\" at the middle level and 167 \"Sections\" at the bottom level, numbered, for example, \"1. Matter and Energy\", \"1.1 Atoms\", \"1.1.1. Structure and Properties of Atoms\".\n\nA feature included in many outlines is prefixing. Similar to section numbers, an outline prefix is a label (usually alphanumeric or numeric) placed at the beginning of an outline entry to assist in referring to it.\n\nBare outlines include no prefixes.\n\nAn \"alphanumeric outline\" includes a prefix at the beginning of each topic as a reference aid. The prefix is in the form of Roman numerals for the top level, upper-case letters (in the alphabet of the language being used) for the next level, Arabic numerals for the next level, and then lowercase letters for the next level. For further levels, the order is started over again. Each numeral or letter is followed by a period, and each item is capitalized, as in the following sample:\n\nSome call the Roman numerals \"A-heads\" (for \"A-level headings\"), the upper-case letters, \"B-heads\", and so on. Some writers also prefer to insert a blank line between the A-heads and B-heads, while often keeping the B-heads and C-heads together.\n\nIf more levels of outline are needed, lower-case Roman numerals and numbers and lower-case letters, sometimes with single and double parenthesis can be used, although the exact order is not well defined, and usage varies widely.\n\nThe scheme recommended by the \"MLA Handbook\", and the \"Purdue Online Writing Lab\", among others, uses the usual five levels, as described above, then repeats the Arabic numerals and lower-case letter surrounded by parentheses (round brackets) – I. A. 1. a. i. (1) (a) – and does not specify any lower levels, though \"(i)\" is usually next. In common practice, lower levels yet are usually Arabic numerals and lowercase letters again, and sometimes lower-case Roman again, with single parentheses – 1) a) i) – but usage varies. MLA style is sometimes incorrectly referred to as APA style, but the \"APA Publication Manual\" does not address outline formatting at all.\n\nA very different style recommended by \"The Chicago Manual of Style\", based on the practice of the United States Congress in drafting legislation, suggests the following sequence, from the top to the seventh level (the only ones specified): I. A. 1. \"a\") (1) (\"a\") \"i\") – capital Roman numerals with a period, capital letters with a period, Arabic numerals with a period, italic lowercase letters with a single parenthesis, Arabic numerals with a double parenthesis, italic lowercase letters with a double parenthesis, and italic lowercase Roman numerals with a single parentheses, though the italics are not required). Because of its use in the US Code and other US law books, Many American lawyers consequently use this outline format.\n\nAnother alternative scheme repeats all five levels with a single parenthesis for the second five – I) A) 1) a) i) – and then again with a double parenthesis for the third five – (I) (A) (1) (a) (i).\n\nMany oft-cited style guides besides the \"APA Publication Manual\", including the \"AP Stylebook\", the \"NYT Manual\", Fowler, \"The Guardian\" Style Guide, and Strunk & White, are curiously silent on the topic.\n\nOne side effect of the use of both Roman numerals and uppercase letters in all of these styles of outlining is that in most alphabets, \"I.\" may be an item at both the top (A-head) and second (B-head) levels. This is usually not problematic because lower level items are usually referred to hierarchically. For example, the third sub-sub-item of the fourth sub-item of the second item is item II. D. 3. So, the ninth sub-item (letter-I) of the first item (Roman-I) is item I. I., and only the top level one is item I.\n\nThe \"decimal outline\" format has the advantage of showing how every item at every level relates to the whole, as shown in the following sample outline:\n\nSpecial types of outlines include reverse outlines and integrated outlines.\n\nA \"reverse outline\" is an outline made from an existing work. Reverse outlining is like reverse engineering a document. The points or topics are extracted from the work, and are arranged in their order of presentation, by section, in the outline. Once completed, the outline can be filled in and rearranged as a plan for a new improved version of the document.\n\nAn \"integrated outline\" is a helpful step in the process of organizing and writing a scholarly paper (literature review, research paper, thesis or dissertation). When completed the integrated outline contains the relevant scholarly sources (author's last name, publication year, page number if quote) for each section in the outline. An integrated outline is generally prepared after the scholar has collected, read and mastered the literature that will be used in the research paper. Shields and Rangarajan (2013) recommend that new scholars develop a system to do this. Part of the system should contain a systematic way to take notes on the scholarly sources. These notes can then be tied to the paper through the integrated outline. This way the scholar reviews all of the literature before the writing begins.\n\nAn integrated outline can be a helpful tool for people with writer's block because the content of the paper is organized and identified prior to writing. The structure and content is combined and the author can write a small section at a time. The process is less overwhelming because it can be separated into manageable chunks. The first draft can be written using smaller blocks of time.\n\nFor a comparison, see Outliners, below.\n\nOutlines are used for composition, summarization, and as a development and storage medium.\n\n\"Merriam-Webster's manual for writers and editors\" (1998, p. 290) recommends that the section headings of an article should when read in isolation, combine to form an outline of the article content. Garson (2002) distinguishes a 'standard outline', presented as a regular table of contents from a refined tree-like 'hierarchical outline', stating that \"such an outline might be appropriate, for instance, when the purpose is taxonomic (placing observed phenomena into an exhaustive set of categories). ... hierarchical outlines are rare in quantitative writing, and the researcher is well advised to stick to the standard outline unless there are compelling reasons not to.\"\n\nWriters of fiction and creative nonfiction, such as Jon Franklin, may use outlines to establish plot sequence, character development and dramatic flow of a story, sometimes in conjunction with free writing.\n\nPreparation of an outline is an intermediate step in the process of writing a scholarly research paper, literature review, thesis or dissertation. A special kind of outline (integrated outline) incorporates scholarly sources into the outline before the writing begins.\n\nIn addition to being used as a composition tool during the drafting process, outlines can also be used as a publishing format. Outlines can presented as work's table of contents, but they can also be used as the body of a work. The \"Outline of Knowledge\" from the 15th edition of the \"Encyclopedia Britannica\" is an example of this. Wikipedia includes outlines that summarize subjects (for example, see Outline of chess, Outline of Mars, and Outline of knowledge).\n\nProfessors often hand out to their students at the beginning of a term, a summary of the subjects to be covered throughout the course in the form of a topic outline. It may also be included as part of a larger course synopsis.\n\nOutlines are also used to summarize talking points for a speech or lecture.\n\nOutlines, especially those used within an outliner, can be used for planning, scheduling, and recording.\n\nAn outliner (or \"outline processor\") is a specialized type of word processor used to view, create, build, modify, and maintain outlines. It is a computer program, or part of one, used for displaying, organizing, and editing hierarchically arranged text in an outline's tree structure. Textual information is contained in discrete sections called \"nodes\", which are arranged according to their topic-subtopic (parent-child) relationships, sort of like the members of a family tree. When loaded into an outliner, an outline may be collapsed or expanded to display as few or as many levels as desired.\n\nOutliners are used for storing and retrieving textual information, with terms, phrases, sentences, or paragraphs attached to a tree. So rather than being arranged by document, information is arranged by topic or content. An outline in an outliner may contain as many topics as desired. This eliminates the need to have separate documents, as outlines easily include other outlines just by adding to the tree.\n\nThe main difference between a hand-written outline and a digital one, is that the former is usually limited to a summary or blueprint of a planned document, while the latter may easily include all of the content of the entire document and many more. In other words, as a hand-written work an outline is a writing tool, but on a computer, it is a general purpose format supported by a robust development and display medium capable of handling knowledge from its creation to its end use.\n\nOutliners may be used in content creation instead of general word processors for capturing, organizing, editing, and displaying knowledge or general textual information. Outliners are ideal for managing lists, organizing facts and ideas, and for writing computer programs. They are also used for goal and task management (including personal information management and project management), and for writing books and movie scripts.\n\nThe graphical counterpart to outliners are mind mappers.\n\n\n"}
{"id": "723531", "url": "https://en.wikipedia.org/wiki?curid=723531", "title": "Palmyrene dialect", "text": "Palmyrene dialect\n\nPalmyrene or Palmyrenean was a Western Aramaic dialect spoken in the city of Palmyra, Syria, in the early centuries AD. The development of cursive versions of Aramaic led to the creation of the Palmyrene alphabet.\n\nOther West Aramaic dialects include Nabataean and Judeo-Aramaic. West Aramaic dialects continue to be spoken in three villages in Syria: Maaloula, Bakh`a and Jubb`adin.\n\n\n"}
{"id": "785639", "url": "https://en.wikipedia.org/wiki?curid=785639", "title": "Polyglotism", "text": "Polyglotism\n\nPolyglotism or polyglottism is knowledge of several languages, consisting of the ability to understand, speak, read, or write these languages. The word is a synonym of multilingualism, but in recent usage polyglot is sometimes used to refer to a person who learns multiple languages as an avocation. The term \"hyperpolyglot\" was coined in 2008 by linguist Richard Hudson to describe individuals who speak—to some degree—dozens of languages.\n\nMultilingualism, including multilingual societies as well as individuals who speak more than one language, is common. Individual polyglots or hyperpolyglots speak, study, or use large numbers of languages. In rare cases, polyglot savants have mental disabilities, but are able to learn many languages.\n\nMost nation states are home to multiple ethnic groups and multiple languages, and most of the world's population is bilingual or multilingual. In other words, only a minority of the world's peoples are monolingual Informal surveys suggest that the number of languages spoken in multilingual societies is typically between two and six languages. However, there is a lack of consensus on such numbers due to the lack of shared definitions of multilingualism and the difficulty of differentiating among language varieties.\n\nSome individuals speak, study, or otherwise use large numbers of languages. One notable hyperpolyglot was Cardinal Giuseppe Caspar Mezzofanti, an Italian priest reputed to have spoken anywhere from 30 to 72 languages. Individuals who consider themselves polyglots generally speak, read, or otherwise use five or more languages. In some cases, the number can be as high as several dozen.\n\nMany polyglots and hyperpolyglots become multilingual by studying languages. Language proficiency and learning success vary among individuals. Neuroscience of multilingualism points to possible differences among learners. One theory suggests that a spike in a baby's testosterone levels while in the uterus can increase brain asymmetry, which may relate to music and language ability, among other effects.\n\nStudy and motivation also affect learning. People learning multiple languages may optimize their learning strategies, making it easier to learn subsequent languages. They may also experience positive transfer – the process by which it becomes easier to learn additional languages if the grammar or vocabulary of the new language is similar to those of languages already spoken. On the other hand, students may also experience negative transfer – interference from languages learned at an earlier stage of development while learning a new language later in life. The International Association of Hyperpolyglots represents those with demonstrable hyperpolyglotic language proficiency.\n\nHyperpolyglots speak a larger number of languages and tend to show an above average ability to learn and speak languages. Linguistic expert Michael Erard says that while there are many polyglots who know six languages, the number drops off above this number. Based on the numbers, he says that when people know eleven languages fluently, it puts them in the hyperpolyglot category.\n\nWhile the term \"savant\" generally refers to an individual with a natural and/or innate talent for a particular field, people diagnosed with savant syndrome are typically individuals with significant mental disabilities who demonstrate profound and prodigious capacities and/or abilities far in excess of what would be considered normal, occasionally including the capacity for languages. The condition is associated with an increased memory capacity, which would aid in the storage and retrieval of knowledge of a language.\n\nIn 1991, for example, Neil Smith and Ianthi-Maria Tsimpli described Christopher, a man with non-verbal IQ scores between 40 and 70, who learned sixteen languages. Christopher was born in 1962 and approximately six months after his birth was diagnosed with brain damage. Despite being institutionalized because he was unable to take care of himself, Christopher had a verbal IQ of 89, was able to speak English with no impairment, and could learn subsequent languages with apparent ease. This facility with language and communication is considered unusual among savants.\n\n\n"}
{"id": "2969423", "url": "https://en.wikipedia.org/wiki?curid=2969423", "title": "Positive statement", "text": "Positive statement\n\nIn the social sciences and philosophy, a positive or descriptive statement concerns what \"is\", \"was\", or \"will be\", and contains no indication of approval or disapproval (what should be). Positive statements are thus the opposite of normative statements. Positive statement is based on empirical evidence. For examples, \"An increase in taxation will result in less consumption\" and \"A fall in supply of petrol will lead to an increase in its price\". However, positive statement can be factually incorrect: \"The moon is made of green cheese\" is empirically false, but is still a positive statement, as it is a statement about what is, not what should be.\n\nPositive statements are distinct from normative statements. Positive statements are based on empirical evidence, can be tested, and involve no value judgements. Positive statements refer to what is and contain no indication of approval or disapproval. When values or opinions come into the analysis, then it is in the realm of normative economics. A normative statement expresses a judgment about whether a situation is desirable or undesirable, which can carry value judgements. These refer to what ought to be.\n\nPositive statements are widely used to describe something measurable, like the rate of inflation in an economy. They are mainly used in explanations of theories and concepts. Using a positive statement does not mean you can't have your own opinions on issues. However, when you are writing academic essays it's important to use positive statements to support an argument, since it can be verified by evidence.\n\n\n"}
{"id": "25948756", "url": "https://en.wikipedia.org/wiki?curid=25948756", "title": "Salva congruitate", "text": "Salva congruitate\n\nSalva congruitate is a Latin scholastic term in logic, which means \"without becoming ill-formed\", \"salva\" meaning \"rescue\", \"salvation\", \"welfare\" and \"congruitate\" meaning \"combine\", \"coincide\", \"agree\". Salva Congruitate is used in logic to mean that two terms may be substituted for each other while preserving grammaticality in all contexts.\n\nTimothy C. Potts describes \"salva congruitate\" as a form of replacement in the context of meaning. It is a replacement which preserves semantic coherence and should be distinguished from a replacement which preserves syntactic coherence but may yield an expression to which no meaning has been given. This means that supposing an original expression is meaningful, the new expression obtained by the replacement will also be meaningful, though it will not necessarily have the same meaning as the original one, nor, if the expression in question happens to be a proposition, will the replacement necessarily preserve the truth value of the original.\n\nBob Hale explains \"salva congruitate\", as applied to singular terms, as substantival expressions in natural language, which are able to replace singular terms without destructive effect on the grammar of a sentence. Thus the singular term 'Bob' may be replaced by the definite description 'the first man to swim the English Channel' \"salva congruitate\". Such replacement may shift both meaning and reference, and so, if made in the context of a sentence, may cause a change in truth-value. Thus terms which may be interchanged \"salva congruitate\" may not be interchangeable \"salva veritate\" (preserving truth). More generally, expressions of any type are interchangeable \"salva congruitate\" if and only if they can replace one another preserving grammaticality or well-formedness.\n\n"}
{"id": "4106285", "url": "https://en.wikipedia.org/wiki?curid=4106285", "title": "Self-referential encoding", "text": "Self-referential encoding\n\nEvery day, people are presented with endless amounts of information, and in an effort to help keep track and organize this information, people must be able to recognize, differentiate and store information. One way to do that is to organize information as it pertains to the self. The overall concept of self-reference suggests that people interpret incoming information in relation to themselves, using their self-concept as a background for new information. Examples include being able to attribute personality traits to oneself or to identify recollected episodes as being personal memories of the past. The implications of self-referential processing are evident in many psychological phenomena. For example, the \"cocktail party effect\" notes that people attend to the sound of their names even during other conversation or more prominent, distracting noise. Also, people tend to evaluate things related to themselves more positively (This is thought to be an aspect of implicit self-esteem). For example, people tend to prefer their own initials over other letters. The self-reference effect (SRE) has received the most attention through investigations into memory. The concepts of self-referential encoding and the SRE rely on the notion that relating information to the self during the process of encoding it in memory facilitates recall, hence the effect of self-reference on memory. In essence, researchers have investigated the potential mnemonic properties of self-reference.\n\nResearch includes investigations into self-schema, self-concept and self-awareness as providing the foundation for self-reference's role in memory. Multiple explanations for the self-reference effect in memory exist, leading to a debate about the underlying processes involved in the self-reference effect. In addition, through the exploration of the self-reference effect, other psychological concepts have been discovered or supported, including simulation theory and the effect.\nAfter researchers developed a concrete understanding of the self-reference effect, many expanded their investigations to consider the self-reference effect in particular groups like those with autism spectrum disorders or those experiencing depression.\n\nSelf-knowledge can be categorized by structures in memory or schemata. A self-schema is a set of facts or beliefs that one has about themselves. For any given trait, an individual may or may not be \"schematic\"; that is, the individual may or may not think about themselves as to where they stand on that trait. For example, people who think of themselves as very overweight or who identify themselves to a greater extent based on their body weight would be considered \"schematic\" on the attribute of body weight. Thus, many everyday events, such as going out for a meal or discussing a friend's eating habits, could induce thoughts about the self. When people relate information to something that has to do with the self, it facilitates memory. Self-descriptive adjectives that fit into one's self-schema are easier to remember than adjectives not viewed as related to the self. Thus, the self-schema is an aspect of oneself that is used as an encoding structure that brings upon memory of information consistent with one's self-schema. Memories that are elaborate and well encoded are usually the result of self-referent correlations during the process of remembering. During the process of encoding, trait representations are encoded in long term memory either directly or indirectly. When they are directly encoded, it is in terms of relating to the self, and when it is indirectly encoded it is done through spouts of episodic information instead of information about the self.\n\nSelf-schema is often used as somewhat of a database for encoding personal data. The self-schema is also used by paying selective attention to outside information and internalizing that information more deeply in one's memory depending on how much that information relates to their schema. When self-schema is engaged, traits that go along with one's view of themselves are better remembered and recalled. These traits are also often recalled much better when processed with respect to the self. Similarly, items that are encoded with the self are based on one's self-schema. Processing the information should balance out when recalled for individuals who have a self-schema that goes along with the information.\n\nSelf-schemas do not necessarily only involve individual traits. People self-categorize at different levels that range from more personal to more social. Self-schemas have three main categories which play a role: the personal self, the relational self, and the collective self. The personal self deals with individual level characteristics, the relational self deals with intimate relationship partners, and the collective self deals with group identities, relating to self-important social groups to which one belongs (e.g., one's family or university). Information that is related to any type of self-schema, including group-related knowledge structures facilitates memory.\n\nIn order for the self to be an effective encoding mechanism, it must be a uniform, consistent, well-developed schema. It has been shown that identity exploration leads to the development of self-knowledge which facilitates self-judgments. Identity exploration led to shorter decision times, higher confidence ratings and more intrusions in memory tasks. Previous researchers hypothesized that words compatible with a person's self-schema are easily accessible in memory and are more likely than incompatible words to intrude on a schema-irrelevant memory task. In one experiment, when participants were asked to decide if certain adjectives were \"like me\" or \"not like me,\" they made the decisions faster when the words were compatible with their self-schema.\n\nHowever, despite the existence of the self-reference effect when considering schemata consistent adjectives, the connection between the self and memory can lead to a larger number of mistakes in recognition, commonly referred to as false alarms. Rogers et al. (1979) found that people are more likely to falsely recognize adjectives they had previously designated to be self-descriptive. Expanding on this, Strube et al. (1986) found that false alarms occurred more for self-schema consistent content, presumably because the presence of such words in the schema makes them more accessible in memory.\n\nIn addition to investigating the self-reference effect in regards to schemata consistent information, Strube et al. discussed how counter schemata information relates to this framework. They noted that the pattern of making correct decisions more rapidly did not hold when considering words that countered a person's self-schema, presumably because they were difficult to integrate into memory due to lack of a preexisting structure. That is, they lacked the organizational structure of encoding because they did not fall into the \"like me\" category, and elaboration would not work because prior connections to the adjective did not exist.\n\nTwo of the most common functions of the self receiving significant attention in research are the self-acting to organize the individual's understanding of the social environment, and the self functioning to regulate behavior through self-evaluation. The concept of self-awareness is considered to be the foundational principle for both functions of the self. Some research presents self-awareness in terms of self-focused attention whereas Hull and Levy suggest that self-awareness refers to the encoding of information based on its relevance to the self. Based on the latter interpretation of self-awareness, individuals must identify the aspects of situations that are relevant to themselves and their behavior will be shaped accordingly. Hull and Levy suggest that self-awareness corresponds to the encoding of information cued by self-symbolic stimuli, and examine the idea of self-awareness as a method of encoding. They structured an investigation that examined self-referent encoding in individuals with different levels of self-awareness, predicting that individuals with higher levels of self-consciousness would encode self-relevant information more deeply than other information, and that they would encode it more deeply than individuals with low levels of self-consciousness. The results of their investigation supported their hypothesis that self-focused attention is not enough to explain the role of self-awareness on attribution. Their results suggest that self-awareness leads to increased sensitivity to the situationally defined meanings of behavior, and therefore organizes the individual's understanding of the social environment. The research presented by Hull and Levy led to future research on the encoding of information associated with self-awareness.\n\nIn later research, Hull and colleagues examined the associations between self-referential encoding, self-consciousness and the extent to which a stimulus is consistent with self-knowledge. They first assumed that the encoding of a stimulus is facilitated if an individual's working memory already contains information consistent with the stimulus, and suggested that self-consciousness as an encoding mechanism relies on an individual's self-knowledge. It is known that situational and dispositional factors may activate certain pools of knowledge, moving them into working memory, and guiding the processing of certain stimulus information.\n\nIn order to better understand the idea of activating information in memory, Hull et al. presented an example of how information is activated. They referred to the sentence \"The robber took the money from the bank\". In English, the word bank has two applicable meanings in the context of this sentence (monetary institution and river shore). However, the monetary institution meaning of the word is more highly activated in this context due to the addition of the words robber and money to the sentence, because they are associatively relevant and therefore pull the monetary institution definition for bank into working memory. Once information is added to working memory, meanings and associations are more easily drawn. Therefore, the meaning of this example sentence is almost universally understood.\n\nIn reference to self-consciousness and self-reference, the connection between self-consciousness and self-referent encoding relies on such information activation. Research suggests that self-consciousness activates knowledge relating to the self, thereby guiding the processing of self-relevant information. Three experiments conducted by Hull and colleagues provided evidence that a manipulation of accessible self-knowledge impacts self-referent encoding based on the self-relevance of such information, individual differences in the accessibility of self-knowledge (self-consciousness) impacts perception, and a mediation relationship exists between self-consciousness and individual differences in self-referential encoding.\n\nSimilar to how self-awareness impacts the availability of self-knowledge and the encoding of self-relevant information, through the development of the self-schema, people develop and maintain certain personality characteristics leading to a variety of behavior patterns. Research has been done on the differences between Type A and Type B behavior patterns, focusing on how people in each group respond to environmental information and their interpretation of the performance of others and themselves. It has been found that Type A behavior is characterized by competitive achievement striving, time urgency and hostility, whereas Type B is usually defined as an absence of Type A characteristics. When investigating causal attributions for hypothetical positive and negative outcomes, Strube et al. found that Type A individuals were more self-serving, in that they took greater responsibility for positive than negative effects. Strube and colleagues argued that this could be a result of the fact that schema-consistent information is more easily remembered and the ease with which past successes and failures are recalled, determined by self-schema, would impact attributions. It is reasonable to believe that Type A's might recall successes more easily and hence be more self-serving.\n\nInfluential psychologists Craik and Lockhart laid the groundwork for research focused on self-referential encoding and memory. In 1972 they proposed their Depth of Processing framework which suggests that memory retention depends on how the stimulus material was encoded in memory. Their original research considered structural, phonemic, and semantic encoding tasks, and showed that semantic encoding is the best method to aid in recall. They asked participants to rate 40 descriptive adjectives on one of four tasks; Structural (Big font or small font?), Phonemic (Rhymes with xxx?), Semantic (Means same as xxx?), or Self-reference (Describes you?). This was then followed by an \"incidental recall task\". This is where participants are asked, without prior warning, to recall as many of the words they had seen as possible within a given time limit. Craik and Tulving's original experiment showed that structural and phonemic tasks lead only to \"shallow\" encoding, while the semantic tasks lead to \"deep\" encoding and resulted in better recall.\n\nHowever, in 1977, it was shown that self-relevant or self-descriptive encoding leads to even better recall than semantic tasks. Experts suggest that the call on associative memory required by semantic tasks is what provides the advantage over structural or phonemic tasks, but is not enough to surpass the benefit provided by self-referential encoding. The fact that self-reference was shown to be a stronger memory encoding method than semantic tasks is what led to more significant interest in the field One early and significant experiment aimed to place self-reference on Craik and Lockhart's depth of processing hierarchy, and suggested that self-reference was a more beneficial encoding method than semantic tasks. In this experiment, participants filled out self-ratings on 84 adjectives. Months later, these participants were revisited and were randomly shown 42 of those words. They then had to select the group of 42 \"revisited\" words out of the total original list. The researchers argued that if the \"self\" was involved in memory retrieval, participants would incorrectly recognize words that were more self-descriptive In another experiment, subjects answered yes or no to cue questions about 40 adjective in 4 tasks (structural, phonemic, semantic and self-referential) and later had to recall the adjectives. This experiment validated the strength of self-reference as an encoding method, and indicated it developed a stronger memory trace than the semantic task.\n\nResearchers are implementing a new strategy by developing different encoding tasks that enhance memory very similarly to self-referential encoding. Symons (1990) had findings that went against the norm when he was unable to find evidence of self-schematicity in the self-reference effect. Another finding was that when referencing gender and religion, there was a low memory recall when compared with referencing the self. A meta-analysis by Symons and Johnson (1997) showed self-reference resulting in better memory in comparison to tasks relying on semantic encoding or other-referent encoding. According to Symons and Johnson, self-referencing questions elicit elaboration and organization in memory, both of which creating a deeper encoding and thus facilitate memory.\n\nTheorists that favor the view that the self has a special role believe that the self leads to more in depth processing, leading to easier recall during self-reference tasks. Theorists also promote the self-schema as being one of the sole inhibitors that allow for recall from deep memory. Thorndyke and Hayes-Roth had the goal of focusing on the process made by the active memory schemata. Sex-typed individuals recall trait adjectives that go along with their sex role more quickly than trait adjectives that are not. During the process of free recall, these individuals also showed more patterns for gender clustering than other sexually typed individuals.\n\nAs research on self-referential encoding became more prolific, some psychologists took an opportunity to delineate specific self-referential encoding tasks. It is noted that descriptive tasks are those that require participants to determine if a stimulus word can be classified as \"self-descriptive.\" Autobiographical tasks are those that require participants to use the stimulus word as a cue to recall an autobiographical memory. Results from experiments that differentiated between these types of self-referential encoding found that they both produced better recall than semantic tasks, and neither was more advantageous than the other. However, research does suggest that the two types of self-referential encoding do rely on different processes to facilitate memory. In most experiments discussed, these types of self- referential encoding were not differentiated.\n\nIn a typical self-reference task, adjectives are presented and classified as either self-descriptive or not. For example, in a study by Dobson and Shaw, adjectives about the self that were preselected were given to the participants and they decide whether or not the adjectives are self-descriptive. The basis for making certain judgments, decisions, inferences and decisions is a self-referent encoding task. If two items are classified as self-descriptive there is no reason one trait would not be equally as easy to retrieve as the other on a self-reference task.\n\nWhile a significant amount of research supports the existence of the self-reference effect, the processes behind it are not well understood. However, multiple hypotheses have been introduced, and two main arguments have been developed: the elaborative processing hypothesis and the organizational processing hypothesis. Encodings in reference to the self are so elaborate because of the information one has about the self. Information encoded with the self is better remembered than information encoded with reference to something else.\n\nElaboration refers to the encoding of a single word by forming connections between it and other material already stored in memory. By creating these connections between the stimulus word and other material already in memory, multiple routes for retrieval of the stimulus word are formed. Based on the depth of processing framework, memory retention increases as elaboration during encoding increases. The Elaborative Processing Hypothesis would suggest that any encoding task that leads to the development of the most trace elaboration or associations is the best for memory retention. Additional research on the depth of processing hierarchy suggests that self-reference is the superior method of information encoding. The elaborative hypothesis would suggest this is because self-reference creates the most elaborate trace, due to the many links that can be made between the stimulus and information about the self already in memory.\n\nThe organizational processing hypothesis was proposed by Klein and Kihlstrom. This hypothesis suggests that encoding is best prompted by considering stimulus words in relation to one another. This thought process and relational thinking creates word to word associations. These inter-item associations are paths in memory that can be used during retrieval. Also, the category labels that define the relations between stimulus items can be used as item cues. Evidence of the organizational component of encoding is demonstrated through the clustering of words during recall. Word clustering during recall indicates that relational information was used to store the words in memory. Rogers, Kuiper and Kirker showed that self-referential judgments were more likely to encourage organization than semantic ones. Therefore, they suggested the self-reference effect was likely due to the organizational processing endured by self-referential encoding.\n\nStructural, phonemic and semantic tasks within the depth of processing paradigm require words to be considered individually, and lend themselves to an elaborative approach. As such, it can be argued that self-referential encoding is superior because it leads to an indirect division of words into categories: words that describe me versus words that do not. Due to this connection between self-reference and organizational processing, further research has been done on this area. Klein and Kihlstrom's research suggests first that, like previous research, self-reference led to better recall than semantic and structural encoding. Second, they found that self-referentially encoded words were more clustered in recall than words from other tasks, suggesting higher levels of organizational processing. From this they concluded that the organization, not encoding task, is what makes self-referential encoding superior \n\nPsychologists Einstein and Hunt showed that both elaborative processing and organizational processing facilitate recall. However, their research argues that the effectiveness of either approach depends on how related the stimulus words are to one another. A list of highly related stimulus words would be better encoded using the elaborative method. The relations between the words would be evident to subjects; therefore, they would not gain any additional pathways for retrieval by encoding the words based on their categorical membership. Instead, the other information gained through elaborative processing would be more beneficial. On the other hand, a list of stimulus words with little relation would be better stored to memory through the organizational method. Since the words have no obvious connection to one another, subjects would likely encode them individually, using an elaborative approach. Since relational information wouldn't be readily detected, focusing on it would add to memory by creating new traces for retrieval. Superior recall was better explained by a combination of elaboration and organization.\nUltimately, the exact processes behind self-referential encoding that makes it superior to other encoding tasks are still under debate. Research suggests that if elaborative processing is behind self-referential encoding, a self-referential task should have the same effect as an elaborative task, whereas if organizational processing underlies the self-reference effect self-referential encoding tasks should function like organizational tasks. To test this, Klein and Loftus ran a 3x2 study testing organizational, elaborative and self-referential encoding with lists of 30 related or unrelated words. When participants were asked to memorize the unrelated list, recall and clustering were higher for the organizational task, which produced almost equal results to the self-referential task, suggesting that has an organizational basis. For the list of related words, the elaborative task led to better recall and had matched results to the self-reference task, suggesting an elaborative basis. This research, then, suggests that the self-reference effect cannot be explained by a single type of processing. Instead, self-referential encoding must lead to information in memory that incorporates item specific and relational information.\n\nOverall, the SRE relies on the unique mnemonic aspects of the self. Ultimately, if the research is suggesting that the self has superior elaborative or organizational properties, information related to the self should be more easily remembered and recalled. The research presented suggests that self-referential encoding is superior because it promotes organization and elaboration simultaneously, and provides self-relevant categories that promote recall.\n\nThe field of social brain science is aimed at examining the neural foundations of social behavior. Neuroimaging and neuropsychology have led to the examination of neuroanatomy and its connection to psychological topics. Through this research, neuropsychologists have found a connection between social cognitive functioning and the medial prefrontal cortex (mPFC). In addition, the mPFC has been connected to reflection and introspection about personal mental states. Supporting these findings, it has been shown that damage to the mPFC is connected to impairments with self-reflection, introspection and daydreaming, as well as social competence, but not other areas of functioning. As such, the mPFC has been connected to self-referential processing.\n\nThe research discussed by those focusing on the neuroanatomy of self-referential processing included similar tasks to the memory and depth of processing research discussed previously. When participants were asked to judge adjectives based in whether or not they were self-descriptive, it was noted that the more self-relevant the trait, the stronger the activation of the mPFC. In addition, it was shown that the mPFC was activated during the appraisal of one's own personality traits, as well as during trait retrieval. One study showed that the more activity in the mPFC during self-referential judgments, the more likely the word was to be remembered on a subsequent surprise memory test. These results suggest that the mPFC is involved in both self-referential processing and in creating self-relevant memories.\n\nMedial prefrontal cortex (mPFC) activation occurs during processing of self-relevant information. When self-referent judgment is more relatable and less negative, the mFPC is activated. Finding support clear cut circuits that have high levels of activation when cognitive and emotional aspects of self-reflection are present. The caudate nucleus has not been associated with self-reference before, however, Fossati and colleagues found activity while participants were retrieving self-relevant trait adjectives. The ventral anterior cingulate cortex (vACC) is also a part of the brain that becomes activated when there are signs of self-referencing and processing. The vACC is activated when self-descriptive information is negative. There is also pCC (posterior cingulate cortex) activity seen in neuroimaging studies during self-referential processing.\n\nGiven all of the neurological support for the effect of self-reference on encoding and memory, there is still a debate in the psychological community about whether or not the self-reference effect signifies a special functional role played by the self in cognition. Generally, this question is met by people that have two opposing views on the processes behind self-reference. On one side of the debate, people believe that the self has special mnemonic abilities because it is a unique cognitive structure. On the other side, people support the arguments described above that suggest there is no special structure, but instead, the self-reference effect is simply a part of the standard depth of processing hierarchy. Since the overall hypothesis is the same for both sides of the debate, that self-relevant material leads to enhanced memory, it is difficult to test them using strictly behavioral measures. Therefore, PET and fMRI scans have been used to see the neural marker of self-referential mental activity.\n\nPrevious studies have shown that areas of the left prefrontal cortex are activated during semantic encoding. Therefore, if the self-reference effect works the same way, as part of the depth of processing hierarchy, the same brain region should be activated when judging traits related to the self. However, if the self has unique mnemonic properties, then self-referential tasks should activate brain regions distinct from those activated during semantic tasks. The field is still at is infancy, but future work on this hypothesis might help to settle the debate about the underlying processes of self-referential encoding.\n\nWhile not able to completely settle the debate over the foundation of self-referential processing, studies on the neurological aspect of personality trait judgments did lead to a related, significant result. It has been shown that judging personality traits about oneself and a close friend activated overlapping brain regions, and the activated regions have all been implicated in self-reference. Noting the similarity between making self-judgments and judgments about close others led to the introduction of the simulation theory of empathy. Simulation theory rests on the idea that one can make inferences about others by using the knowledge they have about themselves. In essence, the theory suggests that people use self-reflection to understand or predict the mental state of others. The more similar a person perceives another to be, the more active the mPFC has shown to be, suggesting more deep or intricate self-reference. However, this effect can cause people to make inaccurate judgments about others or to believe that their own opinions are representative of others in general. This misrepresentation is referred to as the false-consensus effect.\n\nIn addition to simulation theory, other expansions of the self-reference effect have been examined. Through studying the self, researchers have found that the self consists of many independent cognitive representations. For example, the personal self composed of individual characteristics is separate from the relational self which is based on relationships with significant others. These two forms of self are again separate from the collective self which corresponds to a particular group identity. Noting the existence of the collective self and the different group identities that combine to form such a self-representation led researchers to question if information stored in reference to a social group identity has the same effects in memory as information stored in reference to the individual self. In essence, researchers questioned if the self-reference effect can be extended to include situations where the self is more socially defined, producing a group-reference effect.\n\nPrevious research supports the idea that the group-reference effect should exist from a theoretical standpoint. First, the self-expansion model argues that individuals incorporate characteristics of their significant others (or other in-group members into the development of their self-concept. From this model, it is reasonable to conclude that characteristics that are common to both oneself and their significant others (or in-group members) would be more accessible. Second, the previous research discussed suggests that the self-reference effect is due to some combination of organizational, elaborative, mental cueing or evaluative properties of self-referential encoding tasks. Given that we have significant stores of knowledge about our social identities, and such collective identities provide an organizational framework, it is reasonable to assume that a group-reference task would operate similar to that of a self-reference task.\n\nIn order to test these claims, Johnson and colleagues aimed to test whether the self-reference effect generalized to group level identities. Their first study was structured to simply assess if group-reference influenced subsequent memory. In their experiment, they used membership at a particular university as the group of reference. They included group-reference, self-reference and semantic tasks. The experiment replicated the self-reference effect, consistent with previous research. In addition, evidence for a group-reference effect was found. Group-referenced encoding produced better recall than the semantic tasks, and the level of recall from the group-referenced task was not significantly different from the self-referenced task.\n\nDespite finding evidence of a group-reference effect, Johnson and colleagues pointed out that people identify with numerous groups, each with unique characteristics. Therefore, in order to reach conclusive evidence of a group-reference effect, alternative group targets need to be considered. In a second experiment by Johnson et al., the group of reference was modified to be the family of the individual. This group has fewer exemplars than the pool of university students, and affective considerations of the family as a group should be strong. No specific instructions or definitions were provided for family, allowing individuals to consider either the group as a whole (prototype) or specific exemplars (group). When the experiment was repeated using family as the group of reference, group-reference produced recall as much as self-reference. The mean number of recall for the group-reference was higher than self-reference. Participants indicated that they considered both the prototype and individual exemplars when responding to the questions, suggesting that the magnitude of the group-reference effect might not be dependent on the number of exemplars in the target group.\nBoth experiments presented by Johnson et al. found evidence for the group-reference effect. However, these conclusions are limited to the target groups of university students and family. Other research included gender (males and females) and religion (Jewish) as the reference groups and the group-reference effect on memory was not as evident. The group-reference recall for these two groups was not significantly more advantageous than the semantic task. Questioning what characteristics of reference groups that lead to the group-reference effect, a meta-analysis of all four group-reference conditions was performed. This analysis found that self-reference emerged as the most powerful encoding device; however, evidence was found to support the existence of a group-reference effect. The size of the reference groups and number of specific, individual exemplars was hypothesized to influence the existence of the group-reference effect. In addition, accessibility and level of knowledge about group members may also impact such an effect. So, while university students is a much larger group than family, individual exemplars may be more readily accessible than those in a religious group. Similarly, different cognitive representations were hypothesized to influence the group-reference effect. When a larger group is considered, people may be more likely to consider a prototype which may lead to fewer elaborations and cues later on. Smaller groups may lead to relying on the prototype and specific exemplars. Finally, desirability judgments that influence later processing may be influenced by self-reference and certain group-reference tasks. Individuals may be more sensitive to evaluative implications for the personal self and some group identities, but not others.\n\nGroups are also a major part of the self; therefore we attribute the role that different groups play in our self-concept also play a role in the self-reference effect. We process information about group members similarly to how we process for ourselves. Recall of remarks referencing our home and our self and group to familiarity of those aspects of our self. Reference to the self and social group and the identity that comes along with being a part of a social group are equally affective for memory. This is especially true when the groups are small, rather than large.\n\nUltimately, the group-reference effect provides evidence to explain the tendency to notice or pay attention to and remember statements made in regard to our home when traveling in a foreign place. Considering the proposal that groups form part of the self, this phenomenon can be considered an extension of the self-reference effect. Similar to the memorable nature of references to a person's individual self, references to social identities are seemed to be privileged in memory as well.\n\nOnce the foundation of research on self-referential encoding was established, psychologists began to explore how the concept applied to different groups of people, and connected to different phenomena.\n\nIndividuals diagnosed with autism spectrum disorders (ASDs) can display a wide range of symptoms. Some of the most common characteristics of individuals with ASDs include impairments with social functioning, language and communication difficulties, repetitive behaviors and restricted interests. In addition, it is often noted that these individuals are more \"self-focused.\" That is, they have difficulty seeing things from another's perspective. Despite being self-focused, though, research has shown that individuals with ASD's often have difficulty identifying or describing their emotions or the emotions of others. When asked to describe their daily experiences, responses from individuals on the autism spectrum tended to focus more on physical descriptions rather than mental and emotional states. In regards to their social interactions and behavior differences, it is thought that these individuals lack top down control, and therefore, their bottom up decisions remain unchecked. This simply suggests that these individuals cannot use their prior knowledge and memory to make sense of new input, but instead react to each new input individually, compiling them to make a whole picture \n\nNoting the difficulty individuals with ASDs experience with self-awareness, it was thought that they might have difficulty with self-related memory processes. Psychologists questioned if these individuals would show the typical self-reference effect in memory. In one Depth of Processing Study, participants were asked questions about the descriptiveness of certain stimulus words. However, unlike previous DOP studies that focused on phonemic, structural, semantic and self-referential tasks, the tasks were altered for this experiment. To test the referential abilities of individuals with ASD's, the encoding tasks were divided into: \"the self,\" asking to what extent a stimulus word described oneself, \"similar close other,\" asking to what extent a stimulus word was descriptive of one's best friend, \"dissimilar non-close other,\" asking to what extent a stimulus word was descriptive of Harry Potter, and a control group that was asked to determine the number of syllables in each word. Following these encoding tasks, participants were given thirty minutes before a surprise memory task. It was found that individuals with ASD's had no impairment in memory for words encoded in the syllable or dissimilar non-close other condition. However, they had decreased memory for words related to the self.\n\nTherefore, while research suggests that self-referentially encoded information is encoded more deeply than other information, the research on individuals with ASD's showed no advantage for memory recognition with self-reference tasks over semantic encoding tasks. This suggests that individuals with ASD's don't preferentially encode self-relevant information. Psychologists have investigated the biological basis for the decreased self-reference effect among individuals with Autism Spectrum Disorders and have suggested that it may be due to less specialized neural activity in the mPFC for those individuals. However, while individuals with ASD's showed smaller self-reference effects than the control group, some evidence of a self-reference effect was evident in some cases. This indicates that self-referent impairments are a matter of degree, not total absence.\n\nLombardo and his colleagues measured empathy among individuals with ASD's, and showed that these individuals scored lower than the control group on all empathy measures. This may be a result of the difficulty for these individuals to understand or take the perspective of others, in conjunction with their difficulty identifying emotions. This has implications for simulation theory, because these individuals are unable to use their self-knowledge to make conclusions about similar others.\n\nUltimately, the research suggests that people with ASD's might benefit from being more self-focused. The better their ability to reflect on themselves, the better the can mentalize with others.\n\nThere are three possible relations between cognitive processes and anxiety and depression. The first is whether cognitive processes are actually caused by the onset of clinically diagnosed symptoms of major depression or just generalized sadness or anxiousness. The second is whether emotional disorders such as depression and anxiety are able to be considered as caused by cognitions. And the third is whether different specific cognitive processes are able to be considered associates of different disorders. Kovacs and Beck (1977) posited a schematic model of depression where an already depressed self was primed by outside prompts that negatively impacted cognitive illusions of the world in the eye of oneself. These prompts only led participants to a more depressive series of emotions and behavior. The results from the study done by Derry and Kuiper supported Beck's theory that a negative self-schema is present in people, especially those with depressive disorder. Depressed individuals attribute depressive adjectives to themselves more than nondepressive adjectives. Those suffering from a more mild case of depression have trouble deciphering between the traits of themselves and others which results in a loss of their self-esteem and their negative self-evaluation. A depressive schema is what causes the negativity reported by those suffering from depression. Kuiper and Derry found that self-referent recall enhancement was limited only to nondepressed content.\n\nGenerally, self-focus is association with negative emotions. In particular private self-focus is more strongly associated with depression than public self-focus. Results from brain-imaging studies shows\nthat during self-referential processing, those with major depressive disorder show greater activation in the medial prefrontal cortex, suggesting that depressed individuals may be exhibiting greater cognitive control than\nnon-depressed individuals when processing self-relevant information.\n"}
{"id": "58071816", "url": "https://en.wikipedia.org/wiki?curid=58071816", "title": "Skolt of the Year Award", "text": "Skolt of the Year Award\n\nThe Skolt of the Year Award (, ) is an annual award founded in 2007. It is awarded to people, groups, organizations, and institutions individually or collectively in recognition of their outstanding linguistic and cultural contributions for the good of the Skolt community. In spite of its name, it is not a requirement that the recipient be a Skolt. The award is administered and voted on by the Skolt Sámi Language and Culture Association Saaʹmi Nueʹtt and the Skolt community council.\n"}
{"id": "40833540", "url": "https://en.wikipedia.org/wiki?curid=40833540", "title": "Social network (sociolinguistics)", "text": "Social network (sociolinguistics)\n\nIn the field of sociolinguistics, social network describes the structure of a particular speech community. Social networks are composed of a \"web of ties\" (Lesley Milroy) between individuals, and the structure of a network will vary depending on the types of connections it is composed of. Social network theory (as used by sociolinguists) posits that social networks, and the interactions between members within the networks, are a driving force behind language change.\n\nThe key participant in a social network is the \"anchor\", or center individual. From this anchor, ties of varying strengths radiate outwards to other people with whom the anchor is directly linked. These people are represented by \"points\". Participants in a network, regardless of their position, can also be referred to as \"actors \"or \"members\".\n\nThere are multiple ways to describe the structure of a social network. Among them are \"density, member closeness centrality, multiplexity, \"and\" orders\". These metrics measure the different ways of connecting within of a network, and when used together they provide a complete picture of the structure of a particular network.\n\nA social network is defined as either \"loose\" or \"tight\" depending on how connected its members are with each other, as measured by factors like density and multiplexity. This measure of tightness is essential to the study of socially motivated language change because the tightness of a social network correlates with lack of innovation in the population's speech habits. Conversely, a loose network is more likely to innovate linguistically.\n\nThe density of a given social network is found by dividing the number of all existing links between the actors by the number of potential links within the same set of actors. The higher the resulting number, the more dense a network is. Dense networks are most likely to be found in small, stable communities with few external contacts and a high degree of social cohesion. Loose social networks, by contrast, are more liable to develop in larger, unstable communities that have many external contacts and exhibit a relative lack of social cohesion.\n\nMember closeness centrality is the measurement of how close an individual actor is to all the other actors in the community. An actor with high closeness centrality is a central member, and thus has frequent interaction with other members of the network. A central member of a network tends to be under pressure to maintain the norms of that network, while a peripheral member of the network (one with a low closeness centrality score) does not face such pressure. Therefore, central members of a given network are typically not the first members to adopt a linguistic innovation because they are socially motivated to speak according to pre-existing norms within the network.\n\nMultiplexity is the number of separate social connections between any two actors. It has been defined as the \"interaction of exchanges within and across relationships\". A single tie between individuals, such as a shared workplace, is a uniplex relationship. A tie between individuals is multiplex\" \"when those individuals interact in multiple social contexts. For instance, A is B's boss, and they have no relationship outside of work, so their relationship is uniplex. However, C is both B's coworker and neighbor, so the relationship between B and C is multiplex, since they interact with each other in a variety of social roles.\n\nOrders are a way of defining the place of a speaker within a social network. Actors are classified into three different zones\nA \"first order zone\" is composed of all individuals that are directly linked to any given individual. The first order zone can also be referred to as the \"interpersonal environment\" or \"neighborhood\". A first order member of a network is an actor who has a large number of direct connections to the center of the network.\nA \"second order zone\" is a grouping of any individuals who are connected to at least one actor within the first order zone. However, actors in the second order zone are not directly connected to the central member of the network. A second order member has a loose or indirect connection to the network, and may only be connected to a certain network member.\nA \"third order zone\" is made up of newly observed individuals not directly connected to the first order zone. Third order members may be connected to actors in the second order zone, but not the first. They are peripheral members of the network, and are often the actors with the lowest member closeness centrality, since they may not have frequent contact with other members of the network.\n\nSocial networks are used in sociolinguistics to explain linguistic variation in terms of community norms, rather than broad categories like gender or race. Instead of focusing on the social characteristics of speakers, social network analysis concentrates on the relationships between speakers, then considers linguistic change in the light of those relationships. In an effort to depart from variationist sociolinguistics, the concept of the social network has been used to examine the links between the strength of network ties and the use of a linguistic variant. This allows researchers to create an accurate picture of a community's language use without resorting to stereotypical classification.\n\nThe concept of social networks is applicable at both the macro and micro levels. Social networks are at work in communities as large as nation-states or as small as an online dating service. They can also be applied to intimate social groups such as a friendship, family unit, or neighborhood. Because even the smallest of networks contains an enormous number of potential connections between actors, sociolinguists usually only study small networks so that the fieldwork is manageable. In fact, even when studying small networks, sociolinguists rely on the metrics outlined in the previous section, rather than mapping the network out, one connection at a time. One way of mapping the general structure of a network is to assign a \"strength scale\" to each speaker. For example, in Lesley Milroy's study of social networks in Belfast, Northern Ireland, the researchers measured five social variables, which together generated a strength scale for each member of the network: \nThe allocation of a network strength score allows the network patterns of individuals to be measured and possible links with linguistic patterns to be tested.\n\nIn recent years, computer simulation and modeling have been used to study social networks from a broader perspective. Because previous social network studies were focused on individual connections, the size of the networks were limited so that the researcher could work personally with subjects. With the rise of advanced computer modeling techniques, sociolinguists have been able to study the linguistic behavior of large networks of individuals over long periods of time without the inconvenience of individually working with thousands of subjects.\n\nAdvances in computer simulation and modeling technology have been used to study social networks on a larger scale, both with more participants and over a greater span of time. Previous social network studies had to examine individual connections in great detail, and so had to limit the size of the networks involved. Linguists working in the field were also unable to accurately pinpoint the causes of linguistic change because it tends to occur slowly over a long period of time, on a scale beyond the scope of a single research project. With the rise of computer modeling, sociolinguists have been able to study the linguistic behavior of large networks without the huge expenditure of time required to individually work with thousands of subjects long-term. The pioneering study in this field was Fagyal et al. in 2011.\n\nBecause social networks investigate the forces that impact individual behavior, rather than simply attributing linguistic difference to social class, a theory of language change based on social networks is able to explain linguistic behavior more deeply than variationist sociolinguistics. The two major findings of social network theory are that dense (highly interconnected) networks are resistant to change, and that most linguistic change is initiated by weak links—people who are not centrally connected to the network in question. Though most sociolinguistics working on social networks agree on these findings, there has been extended debate about which actors in the network are the primary drivers of linguistic change. The results of this debate are two theories, the strong-tie theory, and the weak-tie theory.\n\nThis study demonstrated that actors chose to imitate other (more prestigious) actors who embodied desirable social attributes, especially \"toughness\" as exemplified by urban students. This imitation of desirable qualities indicates that strongly connected agents lead change by spreading norms through network members. In Eckert's study of speech norms in Detroit high schools, she notes that suburban youth adopted the speech traits of urban youth (including a diphthongized and lowered [i]).\n\nLabov's 1986 study of Philadelphia speech communities (a term used before \"social networks\" became widespread) demonstrated that the agents of linguistic change were the leaders of the speech communities. Actors with high levels of prestige in the linguistic led the use of these forms, and enforced them as norms within the community. Members of this network then used the forms normalized within the network outside of the network, and continuous usage led to wide adoption of these speech norms.\n\nTakeshi Sibata's 1960 study of elementary school children provides strong support for the view that insiders, or leaders, in a social network facilitate language change. He interviewed several elementary school children, and taught them some invented words he created for the purpose of this study. After teaching the students these words, and telling them to teach the other students these words, he came back a week later to observe the results. A few children, those who were popular, friendly, cheerful, active in class activities, were the main people who spread those words. As the centers of their respective networks, these children functioned as strong-tie leaders of linguistic change.\n\nLabov's 1966 study of African American Vernacular English in South Harlem, revealed that second-order actors in African American social networks were the initiators of linguistic change in their communities. Though these second-order actors, or \"lames\" were not held in high regard by the leaders of the speech network, they had connections to other networks, and were sources of new linguistic variables. This study served as the basis of the \"Weak Tie Theory\" proposed by Milroy and Milroy.\n\nThis Milroy and Milroy study examined vernacular English as it was spoken in inner-city Belfast in the 1970s, in three working class communities in Belfast: those in the Ballymacarrett area, the Hammer area, and the Clonard area. Milroy took part in the life of each community as an acquaintance, or 'friend of a friend', investigating the correlation between the integration of individuals in the community and the way those individuals speak.\n\nEach individual studied was given a network strength score based on the person's knowledge of other people in the community, the workplace and at leisure activities to give a score of 1 to 5, with 5 being the highest network 'strength score'. Out of the five variables, one measured density, while the other four measured multiplexity.\n\nEach person's use of phonological variables, (ai), (a), (l), (th), (ʌ), (e), which were clearly indexical of the Belfast urban speech community, were then measured. The independent variables for this study were age, sex and location. These linguistic variables made up the dependent variable of the study, and were analyzed in relation to the network structure and background of each individual speaker. Deviation from the regional standard was determined by density and multiplicity of the social networks into which speakers are integrated.\n\nThe researchers found that a high network strength score was correlated with the use of vernacular forms, and therefore that the use of vernacular variants was strongly influenced by the level of integration into a network. The conclusion of the study was that close-knit networks are important for dialect maintenance.\n\nThis 1987 study, also conducted by Milroy, examined the variable [u], and its relationship to working class identity. The researchers found that actors with the weakest tie to this community identity were most likely to use the variable [u], possibly as a way to strengthen their ties to the network.\n\nIn Ballymacarrett, one of the villages the researchers surveyed, unrounded [u] was most often used by young males and females, who had weak ties to the working class networks, but use the variables frequently to project an image of working-class toughness. These young people often interacted with members of other social networks, and thus spread the [u] realization through their own social networks, which resulted in the adoption of unrounded [u] in most of Belfast. These results provide support for the weak tie theory of language change, because it was the actors on the peripheries of social networks who were responsible for spreading linguistic change. \n\nOne key study that employed computer simulations was Fagyal, Swarup, Escobar, Gasser, and Lakkarajud's work on the roles of group insiders (leaders) and outsiders (loners) in language change. The researchers found that both first-order and second-order network members (also known as \"leaders\" and \"loners\") were both needed in order for changes to spread predictably within the network.\n\nIn this study, the researchers simulated a social network of 900 participants, called nodes, which were connected into a network using a matrix algorithm. They then randomly assigned a linguistic variant to each node. On each cycle of the algorithm, every node interacted with another node, and the variant assigned to each node changed randomly depending on which variant the other node had. This cycle was repeated 40,000 times, and at the end of each cycle, the variant connected to each node was recorded.\n\nThe results of the Fagyal et al. study indicated that \"in a large, socially heterogenous population\", one linguistic variant eventually became the community norm, though other variants were not entirely eliminated. However, when the researchers manipulated the network to remove either loners or leaders, the results changed: without loners, one variant rapidly caused the loss of all other variants; and without leaders, no single variant became the norm for a majority of speakers.\n\nThese findings allowed the researchers to address the major debate in social network theory: whether it is leaders (or centers) or loners who are responsible for language change. In their findings, the presence of both leaders and loners was essential, though the two types of agents played different roles in the process of change.\n\nRather than introducing entirely new forms, leaders accelerate the adoption of forms that already exist within the network. Conversely, the researchers describe the loners' role this way: \"when loners are a part of a population structure that allows their influence to reach centrally-connected hubs, they can have a decisive impact on the linguistic system over time.\"\n\nPreviously, researchers had posited that loners preserved old forms that had been neglected by the larger community. Fagyal et al. complicate this claim by suggesting that the role of loners in a network is to safeguard old features, then reintroduce them to the community.\n\nThe researchers in Berg's 2006 study of digital social networks as linguistic social networks note the value of social networks as both linguistic corpuses and linguistic networks.\n\nIn Carmen Perez-Sabater's 2012 study of Facebook users, she discusses the use of English by native and non-native speakers on university Facebook pages. The researchers categorize these posts as a model of \"computer-mediated communication\", a new communication style that combines features of writing and speech. Facebook posts generally have a degree of informality, whether the users are native or nonnative English speakers, but native English speakers often have a higher degree of informality. For example, non-native speakers cited in the study use separated letter-style greetings and salutations, indicating linguistic insecurity. The conclusions of the study were that \"computer-mediated communication\" do not always tend toward informality, and that online social networks pattern similarly to non-virtual social networks.\n\n\n"}
{"id": "30314827", "url": "https://en.wikipedia.org/wiki?curid=30314827", "title": "Speech Pathology Australia", "text": "Speech Pathology Australia\n\nSpeech Pathology Australia (SPA) is the national peak body for the speech pathology profession in Australia.\n\nEstablished in 1949, SPA began as the Australian College of Speech Therapists, set up to regulate and maintain the qualifications and standards of the profession. The Australian Branch of the British Medical Association \"granted the Australian College of Speech Therapists full professional recognition as the examining, qualifying and representative body for speech therapy within the Commonwealth\". \n\nThe new organisation combined the Victorian Council of Speech Therapy, the Australian Association of Speech Therapists (New South Wales), the South Australian Council of Speech Science and Speech Therapy, and the Council for Speech Therapy (Western Australia), and granted members the right to practice in the United Kingdom.\nIn 1974-5, the organisation became the Australian Association of Speech & Hearing. The association no longer conducted examinations nor granted licentiates to graduates, a responsibility that was taken over by tertiary institutions.\nIn 1996 the organisation became the Speech Pathology Association of Australia Ltd and adopted the public name of Speech Pathology Australia.\n\nSydney-born Elinor Wray undertook speech therapy training at the Central School of Speech and Drama and St Thomas’s Hospital in London, as well as observing speech therapy practice at St Bartholomew’s Hospital and King’s College Hospital. She then spent three months observing at the London County Council Stammering Centres before returning to Sydney in 1929 to establish the first Australian speech therapy service. \n\nAppointed in an honorary capacity at the Royal Alexandra Hospital for Children at Camperdown, Sydney (now located at Westmead) on the recommendation of surgeon (and later president of the hospital) Sir Robert Wade in 1931, Wray voluntarily conducted three clinics weekly for the next seven years, initially working with Sir Robert’s cleft palate patients. The resulting positive outcomes led to the creation of the first hospital speech therapy clinic and the speech pathology profession in Australia.\n\nWray continued to work as a speech therapist until she retired and Speech Pathology Australia’s outstanding contribution to the profession award is named in her honour. Wray died at the age of 98 in 1992.\n\nAssociation membership is available to all applicants with approved qualifications in speech pathology from an Australian accredited university course and students enrolled in a recognised Australian accredited course by the Association.\n\nAll members are bound by the Association’s Code of Ethics to ensure the responsible and ethical practice of speech pathologists. The Code of Ethics outlines in detail the values, principles and standards of practice all members must follow.\n\nAs the recognised national professional standards organisation for speech pathologists in Australia, the Association is recognised by the Department of Education, Employment and Workplace Relations (DEEWR) as the assessing authority for speech pathologists intending to apply for skilled migration to Australia. Most employers in Australia require prospective employees to be eligible for practising membership of Speech Pathology Australia. \n\nA mutual recognition agreement has been in place since 2004 and now includes the American Speech-Language-Hearing Association (United States), Speech-Language and Audiology Canada, the Irish Association of Speech and Language Therapists, the New Zealand Speech-Language Therapists’ Association and the Royal College of Speech and Language (United Kingdom).\n\nThere are a number of university courses throughout Australia which offer entry-level training for speech pathologists. Courses may be an undergraduate (Bachelor) or graduate entry (Masters) degree. \n\nThe Federal Government of Australia, Department of Education and the Department of Employment recognises Speech Pathology Australia as the professional body representing speech pathologists in Australia and for their role in assessing qualifications required for practice in Australia.\n\nSpeech Pathology Australia has the further important role of accrediting university programs that offer training courses at both undergraduate and postgraduate levels. The Competency Based Occupational Standards for Speech Pathologists (CBOS 2011) details the knowledge, skills and attributes required for graduate entry into the profession of speech pathology. All entry level degree programs in Australia, whether undergraduate or post graduate, are assessed to the same CBOS 2011 standards in the accreditation process. University speech pathology programs that are accredited by Speech Pathology Australia have demonstrated that their graduates have attained CBOS entry level competencies and are thus eligible for practising membership to Speech Pathology Australia.\n\nCourses are invited to undergo initial accreditation or re-accreditation evaluation. New courses are encouraged to complete accreditation prior to the graduation of their first cohort of students. (See below for new courses who have initiated an accreditation evaluation).\n\nSpeech pathology courses holding accreditation with Speech Pathology Australia can be seen on the Association's website.\n\nThe annual awareness week, Speech Pathology Week, is held in August to raise awareness of the speech pathology profession and the work done by speech pathologist with the more than 1.1 million Australians who have a communication or swallowing disorder that impacts on their daily life.\n\nCommunication is a basic human right and Speech Pathology Week seeks to promote this fact.\n\nSince 2003, Speech Pathology Australia annually awards three Australian authors the “Best Book for Language and Literacy Development” in the following categories: \n\nBirth to 3 years - 3 to 5 years - 5 to 8 years - 8 to 10 years - Indigenous Children.\n\nEach award is based on the book’s appeal to children, interactive quality and ability to assist speech pathologists and parents in communication and literacy development.\n\nBook of the Year Awards encourage reading and engage young minds.\n\n"}
{"id": "39590024", "url": "https://en.wikipedia.org/wiki?curid=39590024", "title": "Tautology (language)", "text": "Tautology (language)\n\nIn literary criticism and rhetoric, a tautology is a statement which repeats the same idea, using near-synonymous morphemes, words, or phrases, that is, \"saying the same thing twice\". Tautology and pleonasm are not consistently differentiated in the literature.\n\nLike pleonasm, it is often considered a fault of style when unintentional. On the other hand, an intentional repetition may be an effective way to emphasize a thought, or help the listener or reader understand a point. \n\nSometimes logical tautologies like \"Boys will be boys\" are conflated with language tautologies, but in general, a rhetorical tautology is not inherently true.\n\nThe word was coined in Hellenistic Greek from ταὐτός (\"the same\") plus λόγος (\"word/idea\"), and transmitted through 3rd-century Latin \"tautologia\" and French \"tautologie\". It first appears in English in the 16th century. The use in the term logical tautology was introduced in English by Wittgenstein in 1919, perhaps following Auguste Comte's usage in 1835.\n\n\nIntentional repetition of meaning intends to amplify or emphasize a particular, usually significant, fact about what is being discussed. For example, a gift is, by definition, free of charge; using the phrase \"free gift\" might emphasize that there are no hidden conditions or fine print, be it the expectation of money or reciprocation, or that the gift is being given by volition.\n\nThis is related to the rhetorical device of hendiadys, where one concept is expressed through the use of two descriptive words or phrases. For example, \"goblets and gold\" meaning wealth, or \"this day and age\" meaning the present time. Superficially these expressions may seem tautological, but they are stylistically sound because the repeated meaning is just a way to emphasize the same idea.\n\nThe use of tautologies is, however, usually unintentional. For example, the phrases \"mental telepathy\", \"planned conspiracies\", and \"small dwarfs\" imply that there are such things as \"physical telepathy, spontaneous conspiracies, and giant dwarfs.\")\n\nParallelism is not tautology, but rather a particular stylistic device. Much Old Testament poetry is based on parallelism: the same thing said twice, but in slightly different ways (Fowler puts it as pleonasm). However, modern biblical study emphasizes that there are subtle distinctions and developments between the two lines, such that they are usually not truly the \"same thing.\" Parallelism can be found wherever there is poetry in the Bible: Psalms, the Books of the Prophets, and in other areas as well.\n\n"}
{"id": "4436310", "url": "https://en.wikipedia.org/wiki?curid=4436310", "title": "Theta criterion", "text": "Theta criterion\n\nThe theta-criterion (also named θ-criterion) is a constraint on x-bar theory that was first proposed by as a rule within the system of principles of the government and binding theory, called theta-theory (θ-theory). As theta-theory is concerned with the distribution and assignment of theta-roles (a.k.a. thematic roles), the theta-criterion describes the specific match between arguments and theta-roles (θ-roles) in logical form (LF):\nBeing a constraint on x-bar theory, the criterion aims to parse out ill-formed sentences. Thus, if the number or categories of arguments in a sentence does not meet the theta-role assigner's requirement in any given sentence, that sentence will be deemed ungrammatical. . In other words, theta-criterion sorts sentences into grammatical and ungrammatical bins based on c-selection and s-selection.\n\nA theta-role is a status of thematic relation . In other words, a theta-role describes the connection of meaning between a predicate or a verb and a constituent selected by this predicate. The number, types and positions of theta-roles that a lexicon assigns is encoded in its lexical entry and must be satisfied in syntactic structure following Projection Principle. The selection of a constituent by a head based on meaning is called s-selection (semantic-selection) and those based on grammatical categories are called c-selection. Such information can be expressed with a theta grid.\n\nIn the example below the verb 'love' has two theta-roles to assign: agent (the entity who loves) and theme (the entity being loved). In accordance with the theta-criterion, each theta-role must have its argument counterpart.\n\nIn Example 1a, Megan and Kevin are the arguments that the verb assigns the agent and theme theta-roles to, respectively. Because there is a one-to-one mapping of argument to theta-role, the theta-criterion is satisfied and the sentence is deemed grammatical . Below are two examples where the theta-criterion has not been fulfilled and are thus ungrammatical.\nExample 1b is ungrammatical (marked with *) because there are more theta-roles available than there are arguments. The theta-role theme does not have an argument matched to it. On the other hand, in example (1c), there are more arguments than theta-roles. Both theta-roles are matched to arguments (Megan with Agent and Jason with theme), but there is an argument left without a corresponding theta-role (Kevin has no theta-role) . Thus for reasons of inequality in number between theta-roles and arguments, with either having more than the other, the result will be ungrammatical.\n\nSince trace transmits theta-role, movements resulting in non-local relations between theta-role assigners and receivers in surface structure don't violate theta-criterion. This allows us to generate sentences with DP-raising, head movement, wh-movement, etc. However, if a phrase occupies a theta-position (complement or selected subject) in D-structure, it can no longer move to another theta-position or it will receive two theta-roles .\n\nVerbs that can be either transitive or intransitive at the first glance could present a problem for the theta-criterion. For a transitive verb, such as \"hit,\" we assign the theta-roles agent and theme to the arguments, as shown in (2b), (2c), and (2d):\n\nThe action of hitting here requires an animate subject, an agent, carry out the action. The theme is then someone or something that undergoes the action.\nFor an intransitive verb, such as \"arrive,\" we assign the theta-role theme to the sole argument, since \"Mary\" is the one the undergoes the action:\nThe theta-criterion assigns the theta-role in the underlying structure, as shown by (3c). The past-tense morpheme then requires a subject at the spec-TP position and forces the movement of \"Mary,\" as shown by (3d).\nA verb like \"eat\" can choose to take an object, as shown in (4):\n\nFor this type of verb, the potential object is usually semantically limited and therefore can be inferred from the verb at a default value . For instance, for (4a), the listener/reader automatically assumes that John ate \"something.\" What necessitates the object in (4c) is the distinction from the default meaning achieved by specifying what John ate . As a result, this type of verbs can be treated the same as transitive verbs. The theta-roles of \"agent\" and \"theme\" can be assigned:\nIn summary, by assigning the correct theta-roles, theta-criterion is able to tell the real intransitive verbs, such as \"arrive\" apart from verbs that can appear intransitive, such as \"eat.\"\n\nPRO (pronounced 'big pro') is a null pronoun phrase that occurs in a position where it does not get case (or gets null case) but takes the theta-role assigned by the non-finite verb to its subject. PRO's meaning is determined by the precedent DP that controls it . As theta criterion states that each argument is assigned a theta-role, and those theta-roles must consist of a syntactic category that the verb selects even when there is no overt subject. This is where PRO comes in to help satisfy theta-criterion by appearing as the null subject attaining the appropriate theta role .\nBelow is an example containing PRO in a sentence:\n\nExample (5a) is a raising sentence,and in contrast, (5b) is a control sentence, meaning it does not involve any DP movement. The PRO, which is a \"null DP\" is in the subject position of the embedded clause.\n\nSimilarly, example (6a) is a raising-to-object sentence; \"Brian\" raises to the object position of the verb \"want.\" In contrast, (6b) is an object control sentence. The verb \"persuade\" has three theta-roles to assign: \"agent\" to \"Jean\", \"theme\" to \"Brian\", and \"proposition\" to the clause [\"PRO to leave\"]. There is no raising, but there is a PRO in the subject position of the embedded clause that takes the verb leave's only theta-role, \"agent\". Since Brian does not receive theta-role from \"leave\", it only bears one theta-role, nor does PRO receive a second theta-role from \"persuade\". Every argument only receives one theta-role, and every theta-role of the two predicates is assigned to only one argument. The sentence is thus grammatical.\n\npro, also known as little pro, is an empty category that occurs in a subject or object position of a finite clause (finite clauses must contain a verb which shows tense) in languages like Italian, Spanish, Portuguese, Chinese, and some Arabic dialects (; ). pro differs from PRO in that it contains case. The meaning of pro is determined not by its antecedent but by verb agreement in the sentence. The DP is 'dropped' from a sentence if its reference can be recovered from the context. For example:\n\nThe verb \"restituire\" 'give back' assigns three theta-roles, but there are only two overt arguments in the sentence. It ultimately satisfies theta-criterion because the role, theme, is taken by a pro, whose existence can be proved by the properly bound reflexive pronoun \"se stessi\". Compare (7a) with (7b) below:\nWhen the reflexive pronoun \"se stessi\" 'themselves' doesn't have a proper antecedent to co-refer to, the sentence can't be grammatical. This indicates that in (7a) \"se stessi\" must have a proper antecedent in the sentence—the pro that takes the theme role.\n\nCognate objects are nominal complements of their cognate verbs that are normally intransitive. For example,\nSuch a structure posed a problem for theta-criterion because normally the verb assigns only one theta-role, theme, which is already taken by the DP, \"John.\" The sentence should be thus predicted ill-formed. To explain the phenomenon, one way is to re-categorize such a verb as \"die\" so as to change the way it assigns theta-roles. For that purpose, (8) can be interpreted as follows:\nOr John \"underwent\" a gruesome death. If the verb \"die\" is essentially similar to the operation-verb \"meet,\" the cognate objects should be assigned a theta-role—one restricted to the nominal form of the verb head . In other words, \"die\" is now classified as a potentially transitive verb, assigning two theta-roles, agent to \"John\" and theme to \"a gruesome death.\" Such a possibility is falsified, however, because cognate object constructions cannot be passivized .\n\nAs we can no longer consider verbs that take cognate objects the same as potentially transitive verbs, argues, based on the framework of , that cognate objects are adjuncts rather than arguments, having the same meaning and structure as the manner adverbs in (12b).\nSuch an analysis restores cognate objects to the group of arguments satisfying the theta-criterion, as adjuncts, by definition, are not counted as arguments and therefore need not be restricted by theta criterion. The tree form (11c) shows the adjunct DP in its relative position.\n\nDeverbal nouns are derived from verbs and thus assign theta-roles as their verb stems do. For example,\n\nAccording to , the constructions in (10) are analogous to \"the barbarians destroyed Rome\" and \"destruction\" needs to assign theta-roles in line with theta-criterion. It assigns \"agent\" to \"the barbarians\" and \"theme\" to \"Rome\" so (i) is fine. The verb \"destroy\" alone doesn't obligatorily assign theta-role to its subject so (ii) and (iii) is well-formed, too. However, \"destroy\" must assign a \"theme\", so (iv) is ruled out.\n\nTheta-criterion experienced its golden age in the 1980s when people discussed its application to various languages and structures and developed many other theories from it. However, after the minimalist program challenged some cornerstones of government and binding theory, people started to question the validity of this criterion, especially the number of theta-roles allowed to be taken by an argument. Hornstein and Boeckx, for example, proposed that there is no upper limit on the number of theta-roles an argument can receive during derivation. In their theory, the function of selecting correct number of arguments is shouldered by case theory, and theta-roles are just features on verbs that needs to be checked .\n\n"}
{"id": "1617429", "url": "https://en.wikipedia.org/wiki?curid=1617429", "title": "Tutnese", "text": "Tutnese\n\nTutnese or Double Dutch is a language game primarily used in English, although the rules can be easily modified to apply to almost any language. Tutnese is usually used by children, who use it to converse in privacy from adults; by members of historically marginalized minority groups for the same reason when in the presence of authority figures such as police (\"pupolulisus\" or \"pizolizice\"); or simply for amusement and humor.\n\nIn Tutnese, vowels are pronounced normally, but each consonant is replaced with a syllable from the following table:\n\nDouble letters in a word, rather than being repeated, are preceded by the syllable \"squa\" to indicate doubling. For doubled vowels, the prefix becomes \"squat\" instead—thus, \"OO\" would be spoken as \"squat-oh\".\n\nWord Example: \"Tree\" becomes \"Tutrugsquatee\". \nSentence Example: \"I took a walk to the park yesterday\" becomes \"I tutsquatohkuck a wackalulkuck tuto tuthashe pubarugkuck yubesustuterugdudayub\".\n\nWhile spaces between words are always ignored, at least one \"dialect\" requires that the first syllable of the name of any given punctuation mark be spoken, thus a full stop (period) is 'Per', a question mark is 'Que' ('Kway' or 'Kay', varies), and a comma is 'Com'.\n\nThis game appears to have been invented and used by black slaves in the American south, to teach spelling and conceal what they said, at a time when literacy among slaves was forbidden.\n\nMaya Angelou mentions learning Tutnese as a child in the first volume of her autobiography, \"I Know Why the Caged Bird Sings\". She and her friend Louise \"spent tedious hours teaching ourselves the Tut language. You (yack oh you) know (kack nug oh wug) what (wack hash a tut). Since all the other children spoke Pig Latin, we were superior because Tut was hard to speak and even harder to understand.\"\n\nThere is a version used in some parts of the US called Yuckish or Yukkish, which uses more or less the same constructs.\n\n"}
{"id": "797617", "url": "https://en.wikipedia.org/wiki?curid=797617", "title": "Universal pragmatics", "text": "Universal pragmatics\n\nUniversal pragmatics (UP), more recently placed under the heading of formal pragmatics, is the philosophical study of the necessary conditions for reaching an understanding through communication. The philosopher Jürgen Habermas coined the term in his essay \"What is Universal Pragmatics?\" where he suggests that human competition, conflict, and strategic action are attempts to achieve understanding that have failed because of modal confusions. The implication is that coming to terms with how people understand or misunderstand one another could lead to a reduction of social conflict.\n\nBy coming to an \"understanding,\" he means at the very least, when two or more social actors share the same meanings about certain words or phrases; and at the very most, when these actors are confident that those meanings fit relevant social expectations (or a \"mutually recognized normative background\").\n\nFor Habermas, the goal of coming to an understanding is \"intersubjective mutuality ... shared knowledge, mutual trust, and accord with one another\". In other words, the underlying goal of coming to an understanding would help to foster the enlightenment, consensus, and good will necessary for establishing socially beneficial norms. Habermas' goal is not primarily for subjective feeling alone, but for development of shared (intersubjective) norms which in turn establish the social coordination needed for practical action in pursuit of shared and individual objectives (a form of action termed \"communicative action\").\n\nAs an interdisciplinary subject, universal pragmatics draws upon material from a large number of fields, from pragmatics, semantics, semiotics, informal logic, and the philosophy of language, through social philosophy, sociology, and symbolic interactionism, to ethics, especially discourse ethics, and on to epistemology and the philosophy of mind.\n\nUniversal pragmatics (UP) seeks to overcome three dichotomies: the dichotomy between \"body and mind\", between \"theory and practice\", and between \"analytic and continental philosophy\". It is part of a larger project to rethink the relationship between philosophy and the individual sciences during a period of social crisis. The project is within the tradition of Critical Theory, a program that traces back to the work of Max Horkheimer.\n\nThe term \"universal pragmatics\" includes two different traditions that Habermas and his collaborator, colleague, and friend Karl-Otto Apel have attempted to reconcile. On the one hand, ideas are drawn from the tradition of Plato, Aristotle, and Kant, wherein words and concepts are regarded as universally valid idealizations of shared meanings. And, on the other hand, inspiration is drawn from the American Pragmatist tradition (feat. Charles Sanders Peirce, George Herbert Mead, Charles W. Morris), for whom words are usually arbitrary signs devoid of intrinsic meaning, and whose function is to denote the things and processes in the objective world that surrounds the speakers. \n\nUP shares with speech act theory, semiotics, and linguistics an interest in the details of language use and communicative action. However, unlike those fields, it insists on a difference between the linguistic data that we \"observe\" in the 'analytic' mode, and the \"rational reconstruction of the rules of symbol systems\" that each reader/listener possesses intuitively when interpreting strings of words. In this sense, it is an examination of the two ways that language usage can be analyzed: as an object of scientific investigation, and as a 'rational reconstruction' of intuitive linguistic 'know-how'.\n\nUniversal pragmatics is associated with the philosophical method of \"rational reconstruction\".\n\nThe basic concern in universal pragmatics is \"utterances\" (or \"speech acts\") in general. This is in contrast to most other fields of linguistics, which tend to be more specialized, focusing exclusively on very specific sorts of utterances such as \"sentences\" (which in turn are made up of \"words\", \"morphemes\", and \"phonemes\").\n\nFor Habermas, the most significant difference between a sentence and an utterance is in that sentences are judged according to how well they make sense \"grammatically\", while utterances are judged according to their \"communicative validity\" (see section 1). (1979:31)\n\nUniversal pragmatics is also distinct from the field of sociolinguistics, because universal pragmatics is only interested in the meanings of utterances \"if they have to do with claims about truth or rightness\", while sociolinguistics is interested in all utterances in their social contexts. (1979:31, 33)\n\nThere are three ways to evaluate an utterance, according to UP. There are \"theories that deal with elementary propositions\", \"theories of first-person sentences\", and \"theories of speech acts\".\n\nA theory of elementary propositions investigates those things in the real world that are being \"referenced\" by an utterance, and the things that are implied by an utterance, or \"predicate\" it. For example, the utterance \"The first Prime Minister of Canada\" refers to a man who went by the name of Sir John A. Macdonald. And when a speaker delivers the utterance, \"My husband is a lawyer\", it implies that the speaker is married to a man.\n\nA theory of first-person sentences examines the expression of the \"intentions\" of the actor(s) through language and in the first-person.\n\nFinally, a theory of speech acts examines the setting of standards for interpersonal relations through language. The basic goal for speech act theory is to explain how and when utterances in general are \"performative\". (1979:34) Central to the notion of speech acts are the ideas of illocutionary force and perlocutionary force, both terms coined by philosopher J.L. Austin. \"Illocutionary force\" describes the intent of the speaker, while \"perlocutionary force\" means \"the effect an utterance has in the world\", or more specifically, the effect on others.\n\nA performative utterance is a sentence where an action being performed is done by the utterance itself. For example: \"I inform you that you have a moustache\", or \"I promise you I will not burn down the house\". In these cases, the words are also taken as significant actions: the act of informing and promising (respectively).\n\nHabermas adds to this the observation that speech acts can either succeed or fail, depending on whether or not they succeed on \"influencing\" another person in the intended way. (1979:35)\n\nThis last method of evaluation—the theory of speech acts—is the domain that Habermas is most interested in developing as a \"theory of communicative action\".\n\nThere are a number of ways to approach Habermas's project of developing a formal pragmatic analysis of communication. Because Habermas developed it in order to have a normative and philosophical foundation for his critical social theory, most of the inroads into formal pragmatics start from sociology, specifically with what is called action theory. Action theory concerns the nature of human action, especially the manner in which collective actions are coordinated in a functioning society.\n\nThe coordination and integration of social action has been explained in many ways by many theories. Rational choice theory and game theory are two examples, which describe the integration of individuals into social groups by detailing the complex manner in which individuals motivated only by self-interest will form mutually beneficial and cooperative social arrangements. In contrast to these, Habermas has formulated a theory of \"communicative action\". (Habermas 1984; 1987) This theory and the project of developing a formal pragmatic analysis of communication are inseparable.\n\nHabermas makes a series of distinctions in the service of explaining social action. The first major differentiation he makes is between two social realms, \"the system\" and \"the lifeworld\". These designate two distinct modes of social integration:\n\nThus, communicative action is an indispensable facet of society. It is at the heart of the lifeworld and is, Habermas claims, responsible for accomplishing several fundamental social functions: reaching understanding, cultural reproduction, coordinating action-plans, and socializing individuals.\n\nHowever, Habermas is quick to note, different modes of interaction can (in some ways) facilitate these social functions and achieve integration within the lifeworld. This points towards the second key distinction Habermas makes, which differentiates \"communicative action\" from \"strategic action\". The coordination of action plans, which constitutes the social integration of the lifeworld, can be accomplished either through consensus or influence.\n\nStrategic action is action oriented towards success, while communicative action is action oriented towards understanding. Both involve the symbolic resources of the lifeworld and occur primarily by way of linguistic interaction. On the one hand, actors employing communicative actions draw on the uniquely impelling force of mutual understanding to align the orientation of their action plans. It is this subtle but insistent binding force of communicative interactions that opens the door to an understanding of their meanings. On the other hand, actors employing strategic actions do not exploit the potential of communication that resides in the mutual recognition of a shared action-oriented understanding. Instead strategic actors relate to others with no intention of reaching consensus or mutual understanding, but only the intention of accomplishing pre-determined ends unrelated to reaching an understanding. Strategic action often involves the use of communicative actions to achieve the isolated intentions of individuals, manipulating shared understanding in the service of private interests. Thus, Habermas claims, strategic action is parasitic on communicative action, which means communicative action is the primary mode of linguistic interaction. Reaching a reciprocally defined understanding is communication's basic function.\n\nKeeping in mind this delineation of the object domain, the formal pragmatics of communication can be more readily laid out. The essential insight has already been mentioned, which is that communication is responsible for irreplaceable modes of social integration, and this is accomplished through the unique binding force of a shared understanding. This is, in a sense, the pragmatic piece of formal pragmatics: communication does something in the world. What needs to be explained are the conditions for the possibility of what communication already does. This is, in a sense, the formal piece of formal pragmatics: a rational reconstruction of the deep generative structures that are the universal conditions for the possibility of a binding and compelling mutual understanding.\n\nFrom here, Habermas heads the analysis in two directions. In one direction is a kind of linguistic analysis (of speech acts), which can be placed under the heading of the validity dimensions of communication. The other direction entails a categorization of the idealized presuppositions of communication.\n\nHabermas argues that when speakers are communicating successfully, they will have to defend their meaning by using these four claims.\n\n\nHabermas is emphatic that these claims are universal—no human communication oriented at achieving mutual understanding could possibly fail to raise all of these validity claims. Additionally, to illustrate that all other forms of communication are derived from that which is oriented toward mutual understanding, he argues that there are no other kinds of validity claims whatsoever. This is important, because it is the basis of Habermas' critique of postmodernism.\n\nThe fundamental orientation toward mutual understanding is at the heart of universal pragmatics, as Habermas explains:\n\"The task of universal pragmatics is to identify and reconstruct universal conditions of possible mutual understanding... other forms of social action—for example, conflict, competition, strategic action in general—are derivatives of action oriented toward reaching understanding. Furthermore, since language is the specific medium of reaching understanding at the sociocultural stage of evolution, I want to go a step further and single out explicit speech actions from other forms of communicative action.\"\n\nAny meaning that meets the above criteria, and is recognized by another as meeting the criteria, is considered \"vindicated\" or \"communicatively competent\".\n\nIn order for anyone to speak validly — and therefore, to have his or her comments vindicated, and therefore reach a genuine consensus and understanding — Habermas notes that a few more fundamental commitments are required. First, he notes, actors \"have to treat this formulation of validity so seriously that it might be a precondition for any communication at all\". Second, he asserts that \"all actors must believe that their claims are able to meet these standards of validity\". And third, he insists that there must be a common conviction among actors that all validity claims \"are either already vindicated or could be vindicated\".\n\nHabermas claims that communication rests upon a non-egoistic understanding of the world, which is an idea he borrowed from thinkers like Jean Piaget. A subject capable of a de-centered understanding can take up three fundamentally different attitudes to the world. Habermas refers to such attitudes as \"dimensions of validity\". Specifically, this means individuals can recognize different standards for validity—i.e., that the validation of an empirical truth claim requires different methods and procedures than the validation of subjective truthfulness, and that both of those require different methods and procedures of validation than claims to normative rightness.\n\nThese dimensions of validity can be summarized as claims to \"truth (IT), truthfulness (I),\" and \"rightness (WE)\". So the ability to differentiate between the attitudes (and their respective \"worlds\") mentioned above should be understood as an ability to distinguish between types of validity claims.\n\nM. Cooke provided the only book length treatment of Habermas's communication theory. Cooke explains:\n\nThis is fundamental to Habermas's analysis of communication. He maintains that the performance of any speech act necessarily makes reference to these dimensions of validity, by raising at least three validity claims.\n\nOne way to grasp this idea is to take an inventory of the ways in which an attempt at communication can misfire, the ways a speech act can fail. A hearer may reject the offering of a speech act on the grounds that it is invalid because it:\n\nOf course, from this it follows that a hearer who accepts the offering of a speech act does so on the grounds that it is valid because it:\n\nThis means that when engaging in communication the speaker and hearer are inescapably oriented to the validity of what is said. A speech act can be understood as an offering, the success or failure of which depends upon the hearer's response of either accepting or rejecting the validity claims it raises. The three dimensions of validity pointed out above are implicated in any attempt at communication.\n\nThus, communication relies on its being embedded within relations to various dimensions of validity. Any and every speech act is infused with inter-subjectively recognized claims to be valid. This implicitly ties communication to argumentation and various discursive procedures for the redemption of validity claims. This is true because to raise a validity claim in communication is to simultaneously imply that one is able to show, if challenged, that one's claim is justified. Communication is possible because speakers are accountable for the validity of what they say. This assumption of responsibility on the part of the speaker is described by Habermas as a \"warranty\", because in most cases the validity claims raised during communication are taken as justified, and communication proceeds on that basis. Similarly, the hearer is accountable for the stance he or she takes up in relation to the validity claims raised by the speaker. Both speaker and hearer are bound to the validity claims raised by the utterances they share during communication. They are bound by the weak obligations inherent in pursuing actions oriented towards reaching an understanding. Habermas would claim that this obligation is a rational one:\n\nThis begins to point towards the idea of communicative rationality, which is the potential for rationality that is implicit in the validity basis of everyday communication, the shape of reason that can be extracted from Habermas's formal-pragmatic analyses.\n\nHowever, before the idea of communicative rationality can be described, the other direction of Habermas's formal pragmatic analyses of communication needs to be explained. This direction looks towards the idealized presuppositions of communication.\n\nWhen individuals pursue actions oriented towards reaching an understanding, the speech acts they exchange take on the weight of a mutually recognized validity. This means each actor involved in communication takes the other as accountable for what they have said, which implies that good reasons could be given by all to justify the validity of the understanding that is being achieved. Again, in most situations the redemption of validity claims is not an explicit undertaking (except in \"discourses\", see below). Instead, each actor issues a \"warranty\" of accountability to the other, which only needs to be redeemed if certain validity claims are thrown into question. This suggests that the validity claims raised in every communicative interaction implicitly tie communication to argumentation.\n\nIt is here that the idealized presuppositions of communication arise. Habermas claims that all forms of argumentation, even implicit and rudimentary ones, rest upon certain \"idealizing suppositions,\" which are rooted in the very structures of action oriented towards understanding. These \"strong idealizations\" are always understood as at least approximately satisfied by participants in situations where argumentation (and communication) is thought to be taking place. Thus, when during communication it is discovered that the belief that these presuppositions are satisfied is not justified it is always taken as problematic. As a result, steps are usually taken to reestablish and maintain the belief that they are approximately satisfied, or communication is simply called off.\n\n\nIn sum, all these presuppositions must be assumed to be approximately satisfied in any situation of communication, despite their being necessarily counterfactual. Habermas refers to the positing of these idealized presuppositions as the \"simultaneously unavoidable and trivial accomplishments that sustain communicative action and argumentation\".\n\nHabermas calls \"discourses\" those forms of communication that come sufficiently close to actually satisfying these presuppositions. Discourses often occur within institutionalized forms of argumentation that self-reflectively refine their procedures of communication, and as a result, have a more rigorous set of presuppositions in addition to the ones listed above.\n\nA striking feature of discourse is that validity claims tend to be explicitly thematized and there is the presupposition that all possible interlocutors would agree to the universal validity of the conclusions reached. Habermas especially highlights this in what he calls \"theoretical discourses\" and \"practical discourses\". These are tied directly to two of the three dimensions of validity discussed above: theoretical discourse being concerned with validity claims thematized regarding objective states of affairs (IT); practical discourse being concerned with validity claims thematized concerning the rightness of norms governing social interactions (WE).\n\nHabermas understands presupposition (5) to be responsible for generating the self-understanding and continuation of theoretical and practical discourses. Presupposition (5) points out that the validity of an understanding reached in theoretical or practical discourse, concerning some factual knowledge or normative principle, is always expanded beyond the immediate context in which it is achieved. The idea is that participants in discourses such as these presuppose that any understanding reached could attain universal agreement concerning its universal validity if these discourses could be relieved of the constraints of time and space. This idealized presupposition directs discourses concerning truth and normative certainty beyond the contingencies of specific communicative situations and towards the idealized achievements of universal consensus and universal validity. It is a rational reconstruction of the conditions for the possibility of earnest discourses concerning facts and norms. Recall that, for Habermas, rational reconstructions aim at offering the most acceptable account of what allows for the competencies already mastered by a wide range of subjects. In order for discourse to proceed, the existence of facts and norms must be presupposed, yet the certainty of an absolute knowledge of them must be, in a sense, postponed.\n\nStriking a Piagetian and Peircean chord, Habermas understands the deep structures of collective inquiry as developmental. Thus, the presupposition shared by individuals involved in discourse is taken to reflect this. The pursuit of truth and normative certainty is taken to be motivated and grounded, not in some objective or social world that is treated as a \"given\", but rather in a learning process. Indeed, Habermas himself is always careful to formulate his work as a research project, open to refinement.\n\nIn any case, reconstructing the presuppositions and validity dimensions inherent to communication is valuable because it brings into relief the inescapable foundations of everyday practices. Communicative action and the rudimentary forms of argumentation that orient the greater part of human interaction cannot be left behind. By reconstructing the deep structures of these Habermas has discovered a seed of rationality planted in the very heart of the lifeworld. Everyday practices, which are common enough to be trivial, such as reaching an understanding with another, or contesting the reasons for pursuing a course of action, contain an implicit and idealized rationality.\n\nIn other words, communication is always somewhat rational. Communication could not occur if the participants thought that the speech acts exchanged did not carry the weight of a validity for which those participating could be held accountable. Nor would anyone feel that a conclusion was justified if it was achieved by any other means than the uncoerced force of the better argument. Nor could the specialized discourses of law, science and morality continue if the progress of knowledge and insight was denied in favor of relativism.\n\nIt is a question how appropriate it is to speak of \"communication\" tenselessly, and of \"everyday practices\" as though they cut across all times and cultures. That they do cannot be assumed, and anthropology provides evidence of significant difference. It is possible to ignore these facts by limiting the scope of universal pragmatics to current forms of discourse, but this runs the risk of contradicting Habermas's own demand for (5). Moreover, the initial unease with the classical and liberal views of rationality had to do precisely with their ahistorical character and refusal, or perhaps inability, to acknowledge their own origins in circumstances of the day. Their veneer of false universality torn off by the likes of Foucault, it remains to be seen whether \"universal\" pragmatics can stand up to the same challenges posed by deconstruction and skepticism.\n\n\n"}
{"id": "126900", "url": "https://en.wikipedia.org/wiki?curid=126900", "title": "Vocal music", "text": "Vocal music\n\nVocal music is a type of music performed by one or more singers, either with instrumental accompaniment, or without instrumental accompaniment (a cappella), in which singing provides the main focus of the piece. Music which employs singing but does not feature it prominently is generally considered instrumental music (e.g. the wordless women's choir in the final movement of Holst's The Planets) as is music without singing. Music without any non-vocal instrumental accompaniment is referred to as \"a cappella\".\n\nVocal music typically features sung words called lyrics, although there are notable examples of vocal music that are performed using non-linguistic syllables, sounds, or noises, sometimes as musical onomatopoeia. A short piece of vocal music with lyrics is broadly termed a song.\n\nVocal music is probably the oldest form of music, since it does not require any instrument besides the human voice. All musical cultures have some form of vocal music.\n\n\nSolfege, a vocalized musical scale, assigns various syllables such as ‘‘Do-Re-Mi‘‘ to each note. A variety of similar tools are found in \ntraditional Indian music, and scat singing of jazz.\n\nHip hop music has a very distinct form of vocal percussion known as beatboxing. It involves creating beats, rhythms, and scratching.\n\nThe singer of the Icelandic group Sigur Rós, Jón Þór Birgisson, often uses vocals without words, as does Icelandic singer/songwriter, Björk. Her album Medúlla is composed entirely of processed and acoustic vocal music, including beatboxing, choral arrangements, and throat singing.\n\nSinger Bobby McFerrin has recorded a number of albums using only his voice and body, sometimes consisting of a texted melody supported by untexted vocalizations.\n\nThe Second Viennese School, especially Alban Berg and Arnold Schoenberg, pioneered a technique called Sprechstimme in which singers are half-talk, half-sing, and only approximate pitches.\n\n\n\n\n"}
