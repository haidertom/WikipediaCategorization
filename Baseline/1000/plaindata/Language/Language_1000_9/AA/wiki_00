{"id": "4503405", "url": "https://en.wikipedia.org/wiki?curid=4503405", "title": "American Speech–Language–Hearing Association", "text": "American Speech–Language–Hearing Association\n\nThe American Speech–Language–Hearing Association (ASHA) is a professional association for speech–language pathologists, audiologists, and speech, language, and hearing scientists in the United States and internationally. It has more than 197,856 members and affiliates.\n\nThe mission of the American Speech–Language–Hearing Association is to promote the interests of and provide the highest quality services for professionals in audiology, speech–language pathology, and speech and hearing science, and to advocate for people with communication disabilities.\n\nThe association's national office is located at 2200 Research Boulevard, Rockville, Maryland. The organization also has an office on Capitol Hill.\n\nArlene Pietranton is currently serving as the association's executive director.\nASHA was founded in 1925 as the American Academy of Speech Correction. The current name was adopted in 1978.\n\nThe 2014 ASHA conference was held in Orlando, Florida from November 20–22.\n\nThe 2017 ASHA conference will be held in Los Angeles, California from November 9–11.\n\nThe Council for Academic Accreditation in Audiology and Speech-Language Pathology (CAA) is the accreditation unit of the ASHA. Founded over 100 years ago by American universities and secondary schools, CAA established standards for graduate program accreditation that meet entry-level preparation in the speech and hearing field. Accreditation is available for graduate programs with a master's degree in Speech-Language Pathology or clinical doctoral program in audiology.\n\nProfessionals of Communication Sciences and Disorders (CSD) can become members of ASHA. These professionals include audiologists, speech-language pathologists, and speech-language-hearing scientists. As of December 31, 2017, there are more than 197,856 members and affiliates of ASHA. Opportunities ASHA membership brings include access to publications associated with ASHA, to continuing education programs through ASHA, to a platform to network with other CSD professionals, to career-building tools, and to money-saving programs.\n\nASHA sponsors special interest groups (SIGS) within the organization as a means of promoting community and learning in more specialized topics. As of 2016, ASHA has 19 established Special Interest Groups (SIG). These have been added through the years. ASHA members can be a SIG Affiliate of any number of SIGS, with each affiliation requiring nominal yearly dues. The 19 SIGS are:\n\n"}
{"id": "33113250", "url": "https://en.wikipedia.org/wiki?curid=33113250", "title": "Carinthian dialect group", "text": "Carinthian dialect group\n\nThe Carinthian dialect group (\"koroška narečna skupina\", \"koroščina\") is a group of closely related dialects of Slovene. The Carinthian dialects are spoken by Carinthian Slovenes in Austria, in Slovenian Carinthia, and in the northwestern parts of Slovenian Styria along the upper Drava Valley, in the westernmost areas of Upper Carniola on the border with Italy, and in some villages in the Province of Udine in Italy.\n\nAmong other features, this group is characterized by late denasalization of *\"ę\" and *\"ǫ\", a close reflex of long yat and open reflex of short yat, lengthening of old acute syllables and short neo-acute syllables, and an \"e\"-like reflex of the long semivowel and \"ə\"-like reflex of the short semivowel.\n\n"}
{"id": "42243433", "url": "https://en.wikipedia.org/wiki?curid=42243433", "title": "Chanchala", "text": "Chanchala\n\nChanchala is a Sanskrit adjective basically referring to the unsteady vacillating nature of human mind and actions which need to be stilled, neutralized or controlled for gaining right speech and vision.\n\nChanchala (Sanskrit: चञ्चल) means - 'inconsiderate', 'nimble', 'shaking', 'inconstant', 'moveable', 'flickering', 'moving', 'unsteady', 'fortune', 'wind', 'long pepper', 'goddess of fortune' \n\n\"Chanchala\" is the good word for 'vacillation' in Sanskrit language; in poetry the girl with the dancing eyes is called \"chanchalakshi\", which is considered to be rare attribute. However, as part of the literary evidence of Kusana period, the word \"Chanchala\", like \"Dhavani\" and \"Rodini\", indicates the nature or action of Mother goddess. In the Bhagavad Gita(Sloka VI.26):\nthe word Chanchala used in the first line refers to the restless and the unsteady mind that wanders away.\n\nDasam Granth, which like the Guru Granth Sahib is an important book of Sikhism, it is not composed in ragas (its first composition dates 1684 A.D.) tells us that \"Chanchala\" is the name a \"chhand\" or metre of sixteen syllables having \"ragan\", \"jagan\", \"ragan\", \"jagan\" and \"laghu\" consecutively in each quarter, this metre is also known as \"Chitra\", \"Biraj and \"Brahmrupak\", and has been used twice in \"Choubis Autar\".\n\n\"Chanchala\", meaning, 'the fickle-fortune', is one of the many names of Lakshmi. There is no mention of \"Lakshmi\" in the Rig Veda. Sri of the Rig Veda is deified as a personified being in the Yajurveda, and in the Atharvaveda (I.18) she is prayed to secure prosperity. \"Jatavedas Agni\" is repeatedly asked to make the goddess come to the votary; the epithet \"anapagamini\" reflects the \"chanchala\" i.e. fleet or fickle aspect of the goddess. \"Lakshmi\" or \"Chanchala\" as the mobile one associates only with the rich and the dynamic, no matter what their caste, creed or colour. Because \"Lakshmi\" is \"chanchala\" i.e. quick on her feet, to make her \"achala\" i.e. 'immobile', she needs to be worshipped quietly so that she does not get distracted.\n\nIn Yoga, \"vritti\" indicates the contents of mental awareness that are disturbances in the medium of consciousness. The \"vrittis\" of the \"gunas\" are ever-active and swift, the \"gunas\" serve as parts of \"buddhi\", their habitual conduct is fickle, restless, tremulous (\"chanchala\") activity, which activity can be controlled through \"Abhyasa\", \"Vairagya\" and \"Ishvarapranidhana\". \"Sri Narada Pancharatnam\" (Sloka VIII.15) tells us that \"Chanchala\" is the \"nadi\" which along with \"Medhya\" resides in the \"Visuddha Chakra\" on the throat.\n"}
{"id": "5295", "url": "https://en.wikipedia.org/wiki?curid=5295", "title": "Character encoding", "text": "Character encoding\n\nCharacter encoding is used to represent a repertoire of characters by some kind of encoding system. Depending on the abstraction level and context, corresponding code points and the resulting code space may be regarded as bit patterns, octets, natural numbers, electrical pulses, etc. A character encoding is used in computation, data storage, and transmission of textual data. \"Character set\", \"character map\", \"codeset\" and \"code page\" are related, but not identical, terms.\n\nEarly character codes associated with the optical or electrical telegraph could only represent a subset of the characters used in written languages, sometimes restricted to upper case letters, numerals and some punctuation only. The low cost of digital representation of data in modern computer systems allows more elaborate character codes (such as Unicode) which represent most of the characters used in many written languages. Character encoding using internationally accepted standards permits worldwide interchange of text in electronic form.\n\nEarly binary repertoires include Bacon's cipher, Braille, International maritime signal flags, and the 4-digit encoding of Chinese characters for a Chinese telegraph code (Hans Schjellerup, 1869). Common examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode.\n\nMorse code was introduced in the 1840s and is used to encode each letter of the Latin alphabet, each Arabic numeral, and some other characters via a series of long and short presses of a telegraph key. Representations of characters encoded using Morse code varied in length.\n\nThe Baudot code, a five-bit encoding, was created by Émile Baudot in 1870, patented in 1874, modified by Donald Murray in 1901, and standardized by CCITT as International Telegraph Alphabet No. 2 (ITA2) in 1930.\n\nFieldata, a six-or seven-bit code, was introduced by the U.S. Army Signal Corps in the late 1950s.\n\nIBM's Binary Coded Decimal (BCD) was a six-bit encoding scheme used by IBM in as early as 1959 in its 1401 and 1620 computers, and in its 7000 Series (for example, 704, 7040, 709 and 7090 computers), as well as in associated peripherals. BCD extended existing simple four-bit numeric encoding to include alphabetic and special characters, mapping it easily to punch-card encoding which was already in widespread use. It was the precursor to EBCDIC.\n\nASCII was introduced in 1963 and is a seven-bit encoding scheme used to encode letters, numerals, symbols, and device control codes as fixed-length codes using integers.\n\nIBM's Extended Binary Coded Decimal Interchange Code (usually abbreviated as EBCDIC) is an eight-bit encoding scheme developed in 1963.\n\nThe limitations of such sets soon became apparent, and a number of \"ad hoc\" methods were developed to extend them. The need to support more writing systems for different languages, including the CJK family of East Asian scripts, required support for a far larger number of characters and demanded a systematic approach to character encoding rather than the previous \"ad hoc\" approaches. \n\nIn trying to develop universally interchangeable character encodings, researchers in the 1980s faced the dilemma that on the one hand, it seemed necessary to add more bits to accommodate additional characters, but on the other hand, for the users of the relatively small character set of the Latin alphabet (who still constituted the majority of computer users), those additional bits were a colossal waste of then-scarce and expensive computing resources (as they would always be zeroed out for such users).\n\nThe compromise solution that was eventually found and developed into Unicode was to break the assumption (dating back to telegraph codes) that each character should always directly correspond to a particular sequence of bits. Instead, characters would first be mapped to a universal intermediate representation in the form of abstract numbers called code points. Code points would then be represented in a variety of ways and with various default numbers of bits per character (code units) depending on context. To encode code points higher than the length of the code unit, such as above 256 for 8-bit units, the solution was to implement variable-width encodings where an escape sequence would signal that subsequent bits should be parsed as a higher code point.\n\nTerminology related to character encoding:\n\"Example:\" The Latin character set is used by English and most European languages, though the Greek character set is used only by the Greek language.\n\n\nThe character repertoire is an abstract set of more than one million characters found in a wide variety of scripts including \"Latin, Cyrillic, Chinese, Korean, Japanese, Hebrew, and Aramaic\".\n\nOther symbols such as musical notation are also included in the character repertoire. Both the Unicode and GB18030 standards have a character repertoire. As new characters are added to one standard, the other standard also adds those characters, to maintain parity.\n\nThe code unit size is equivalent to the bit measurement for the particular encoding:\n\n\"Example of a code unit:\" Consider a \"string\" of the letters \"abc\" followed by (represented with 1 char32_t, 2 char16_t or 4 char8_t). That string contains:\n\nTo express a character in Unicode, the hexadecimal value is prefixed with the string 'U+'. The range of valid code points for the Unicode standard is U+0000 to U+10FFFF, inclusive, divided in 17 planes, identified by the numbers 0 to 16. Characters in the range U+0000 to U+FFFF are in the plane 0, called the \"Basic Multilingual Plane\" (BMP). This plane contains most commonly-used characters. Characters in the range U+10000 to U+10FFFF in the other planes are called \"supplementary characters\".\n\nThe following table shows examples of code point values:\nA code point is represented by a sequence of code units. The mapping is defined by the encoding. Thus, the number of code units required to represent a code point depends on the encoding:\n\nUnicode and its parallel standard, the ISO/IEC 10646 Universal Character Set, together constitute a modern, unified character encoding. Rather than mapping characters directly to octets (bytes), they separately define what characters are available, corresponding natural numbers (code points), how those numbers are encoded as a series of fixed-size natural numbers (code units), and finally how those units are encoded as a stream of octets. The purpose of this decomposition is to establish a universal set of characters that can be encoded in a variety of ways. To describe this model correctly requires more precise terms than \"character set\" and \"character encoding.\" The terms used in the modern model follow:\n\nA character repertoire is the full set of abstract characters that a system supports. The repertoire may be closed, i.e. no additions are allowed without creating a new standard (as is the case with ASCII and most of the ISO-8859 series), or it may be open, allowing additions (as is the case with Unicode and to a limited extent the Windows code pages). The characters in a given repertoire reflect decisions that have been made about how to divide writing systems into basic information units. The basic variants of the Latin, Greek and Cyrillic alphabets can be broken down into letters, digits, punctuation, and a few \"special characters\" such as the space, which can all be arranged in simple linear sequences that are displayed in the same order they are read. But even with these alphabets, diacritics pose a complication: they can be regarded either as part of a single character containing a letter and diacritic (known as a precomposed character), or as separate characters. The former allows a far simpler text handling system but the latter allows any letter/diacritic combination to be used in text. Ligatures pose similar problems. Other writing systems, such as Arabic and Hebrew, are represented with more complex character repertoires due to the need to accommodate things like bidirectional text and glyphs that are joined together in different ways for different situations.\n\nA coded character set (CCS) is a function that maps characters to \"code points\" (each code point represents one character). For example, in a given repertoire, the capital letter \"A\" in the Latin alphabet might be represented by the code point 65, the character \"B\" to 66, and so on. Multiple coded character sets may share the same repertoire; for example ISO/IEC 8859-1 and IBM code pages 037 and 500 all cover the same repertoire but map them to different code points.\n\nA character encoding form (CEF) is the mapping of code points to \"code units\" to facilitate storage in a system that represents numbers as bit sequences of fixed length (i.e. practically any computer system). For example, a system that stores numeric information in 16-bit units can only directly represent code points 0 to 65,535 in each unit, but larger code points (say, 65,536 to 1.4 million) could be represented by using multiple 16-bit units. This correspondence is defined by a CEF.\n\nNext, a character encoding scheme (CES) is the mapping of code units to a sequence of octets to facilitate storage on an octet-based file system or transmission over an octet-based network. Simple character encoding schemes include UTF-8, UTF-16BE, UTF-32BE, UTF-16LE or UTF-32LE; compound character encoding schemes, such as UTF-16, UTF-32 and ISO/IEC 2022, switch between several simple schemes by using byte order marks or escape sequences; compressing schemes try to minimise the number of bytes used per code unit (such as SCSU, BOCU, and Punycode).\n\nAlthough UTF-32BE is a simpler CES, most systems working with Unicode use either UTF-8, which is backward compatible with fixed-width ASCII and maps Unicode code points to variable-width sequences of octets, or UTF-16BE, which is backward compatible with fixed-width UCS-2BE and maps Unicode code points to variable-width sequences of 16-bit words. See comparison of Unicode encodings for a detailed discussion.\n\nFinally, there may be a higher level protocol which supplies additional information to select the particular variant of a Unicode character, particularly where there are regional variants that have been 'unified' in Unicode as the same character. An example is the XML attribute xml:lang.\n\nThe Unicode model uses the term character map for historical systems which directly assign a sequence of characters to a sequence of bytes, covering all of CCS, CEF and CES layers.\n\nHistorically, the terms \"character encoding\", \"character map\", \"character set\" and \"code page\" were synonymous in computer science, as the same standard would specify a repertoire of characters and how they were to be encoded into a stream of code units – usually with a single character per code unit. But now the terms have related but distinct meanings, due to efforts by standards bodies to use precise terminology when writing about and unifying many different encoding systems. Regardless, the terms are still used interchangeably, with \"character set\" being nearly ubiquitous.\n\nA \"code page\" usually means a byte-oriented encoding, but with regard to some suite of encodings (covering different scripts), where many characters share the same codes in most or all those code pages. Well-known code page suites are \"Windows\" (based on Windows-1252) and \"IBM\"/\"DOS\" (based on code page 437), see Windows code page for details. Most, but not all, encodings referred to as code pages are single-byte encodings (but see octet on byte size.)\n\nIBM's Character Data Representation Architecture (CDRA) designates with coded character set identifiers (CCSIDs) and each of which is variously called a \"charset\", \"character set\", \"code page\", or \"CHARMAP\".\n\nThe term \"code page\" does not occur in Unix or Linux where \"charmap\" is preferred, usually in the larger context of locales.\n\nContrasted to CCS above, a \"character encoding\" is a map from abstract characters to code words. A \"character set\" in HTTP (and MIME) parlance is the same as a character encoding (but not the same as CCS).\n\n\"Legacy encoding\" is a term sometimes used to characterize old character encodings, but with an ambiguity of sense. Most of its use is in the context of Unicodification, where it refers to encodings that fail to cover all Unicode code points, or, more generally, using a somewhat different character repertoire: several code points representing one Unicode character, or versa (see e.g. code page 437). Some sources refer to an encoding as \"legacy\" only because it preceded Unicode. All Windows code pages are usually referred to as legacy, both because they antedate Unicode and because they are unable to represent all 2 possible Unicode code points.\n\nAs a result of having many character encoding methods in use (and the need for backward compatibility with archived data), many computer programs have been developed to translate data between encoding schemes as a form of data transcoding. Some of these are cited below.\n\nCross-platform:\nUnix-like: \n\nWindows:\n\n\n"}
{"id": "1648223", "url": "https://en.wikipedia.org/wiki?curid=1648223", "title": "Cluttering", "text": "Cluttering\n\nCluttering (also called tachyphemia or tachyphrasia) is a speech and communication disorder characterized by a rapid rate of speech, erratic rhythm, and poor syntax or grammar, making speech difficult to understand.\n\nCluttering is a speech and communication disorder that has also been described as a fluency disorder.\n\nIt is defined as:\"Cluttering is a fluency disorder characterized by a rate that is perceived to be abnormally rapid, irregular, or both for the speaker (although measured syllable rates may not exceed normal limits). These rate abnormalities further are manifest in one or more of the following symptoms: (a) an excessive number of disfluencies, the majority of which are not typical of people with stuttering; (b) the frequent placement of pauses and use of prosodic patterns that do not conform to syntactic and semantic constraints; and (c) inappropriate (usually excessive) degrees of coarticulation among sounds, especially in multisyllabic words\".\n\nStuttering is often misapplied as a common term referring to any dysfluency. It is also often incorrectly applied to normal dysfluency rather than dysfluency from a disorder. Cluttered speech is exhibited by normal speakers, and is often referred to as stuttering. This is especially true when the speaker is nervous, where nervous speech more closely resembles cluttering than stuttering.\n\nCluttering is sometimes confused with stuttering. Both communication disorders break the normal flow of speech, but they are distinct. A stutterer has a coherent pattern of thoughts, but may have a difficult time vocally expressing those thoughts; in contrast, a clutterer has no problem putting thoughts into words, but those thoughts become disorganized during speaking. Cluttering affects not only speech, but also thought patterns, writing, typing, and conversation.\n\nStutterers are usually dysfluent on initial sounds, when beginning to speak, and become more fluent towards the ends of utterances. In contrast, clutterers are most clear at the start of utterances, but their speaking rate increases and intelligibility decreases towards the end of utterances.\n\nStuttering is characterized by struggle behavior, such as overtense speech production muscles. Cluttering, in contrast, is effortless. Cluttering is also characterized by slurred speech, especially dropped or distorted /r/ and /l/ sounds; and monotone speech that starts loud and trails off into a murmur.\n\nA clutterer described the feeling associated with a clutter as:\nCluttering can often be confused with language delay, language disorder, learning disabilities, and attention deficit hyperactivity disorder (ADHD). Clutterers often have reading and writing disabilities, especially sprawling, disorderly handwriting, which poorly integrate ideas and space.\n\nBecause clutterers have poor awareness of their disorder, they may be indifferent or even hostile to speech-language pathologists. Delayed auditory feedback (DAF) is usually used to produce a more deliberate, exaggerated oral-motor response pattern. Other treatment components include improving narrative structure with story-telling picture books, turn-taking practice, pausing practice, and language therapy.\n\nBattaros was a legendary Libyan king who spoke quickly and in a disorderly fashion. Others who spoke as he did were said to suffer from \"battarismus\". This is the earliest record of the speech disorder of cluttering.\n\nIn the 1960s, cluttering was called \"tachyphemia\", a word derived from the Greek for \"fast speech\". This word is no longer used to describe cluttering because fast speech is not a required element of cluttering.\n\nDeso Weiss described cluttering as the outward manifestation of a \"central language imbalance\".\n\nThe First World Conference on Cluttering was held in May 2007 in Razlog, Bulgaria. It had over 60 participants from North America, Europe, the Middle East and Asia.\n\nWeiss claimed that Battaros, Demosthenes, Pericles, Justinian, Otto von Bismarck, and Winston Churchill were clutterers. He says about these people, \"Each of these contributors to world history viewed his world holistically, and was not deflected by exaggerated attention to small details. Perhaps then, they excelled because of, rather than in spite of, their \n[cluttering].\"\n\n\n\n"}
{"id": "29819979", "url": "https://en.wikipedia.org/wiki?curid=29819979", "title": "Cognitive hearing science", "text": "Cognitive hearing science\n\nCognitive hearing science is an interdisciplinary science field concerned with the physiological and cognitive basis of hearing and its interplay with signal processing in hearing aids. The field includes genetics, physiology, medical and technical audiology, cognitive neuroscience, cognitive psychology, linguistics and social psychology.\n\nTheoretically the research in cognitive hearing science combines a physiological model for the information transfer from the outer auditory organ to the auditory cerebral cortex, and a cognitive model for how language comprehension is influenced by the interplay between the incoming language signal and the individual's cognitive skills, especially the long-term memory and the working memory.\n\nResearchers examine the interplay between type of hearing impairment or deafness, type of signal processing in different hearing aids, type of listening environment and the individual's cognitive skills.\n\nResearch in cognitive hearing science has importance for the knowledge about different types of hearing impairment and its effects, as for the possibilities to determine which individuals can make use of certain type of signal processing in hearing aid or cochlear implant and thereby adapt hearing aid to the individual.\n\nCognitive hearing science has been introduced by researchers at the Linköping University research centre Linnaeus Centre HEAD (HEaring And Deafness) in Sweden, created in 2008 with a major 10-year grant from the Swedish Research Council.\n\n"}
{"id": "2267781", "url": "https://en.wikipedia.org/wiki?curid=2267781", "title": "Discourse ethics", "text": "Discourse ethics\n\nDiscourse ethics refers to a type of argument that attempts to establish normative or ethical truths by examining the presuppositions of discourse. Variations of this argument have been used in the establishment of egalitarian ethics, as well as libertarian ethics.\n\nGerman philosophers Jürgen Habermas and Karl-Otto Apel are considered the originators of modern discourse ethics. Habermas's discourse ethics is his attempt to explain the implications of communicative rationality in the sphere of moral insight and normative validity. It is a complex theoretical effort to reformulate the fundamental insights of Kantian deontological ethics in terms of the analysis of communicative structures. This means that it is an attempt to explain the universal and obligatory nature of morality by evoking the universal obligations of communicative rationality. It is also a cognitivist moral theory, which means it holds that justifying the validity of moral norms can be done in a manner analogous to the justification of facts. However, the entire project is undertaken as a rational reconstruction of moral insight. It claims only to reconstruct the implicit normative orientations that guide individuals and it claims to access these through an analysis of communication.\n\nThis type of ethics consists of conversations about ideas in civic or community contexts marked by diversity of perspectives requiring thoughtful public engagement. This discourse is made up of differing insights that helps to shape the public's engagement with one another. This type of discourse is meant to protect and to promote the public good. For public discourse ethics to be successful there must be an effective level of civility between people or persons involved. It was Sigmund Freud who once said, \"civilization began the first time an angry person cast a word instead of a rock\" and that statement is something that continues to be seen in society today. The Harvard Law Review accurately examines public discourse and explains it in a manner that is appropriate and conceptually accurate. \"Every man who publishes a book commits himself to the judgement of the public, and anyone may comment upon his performance... [W]hatever their merits, others have a right to pass their judgement upon them-to censure them if they be censurable, and to turn them into ridicule if they be ridiculous\". For the public discourse ethics to be productive there must be accountability on the public stage as the Harvard Law Review calls into question. Without any act of accountability the ethicality of the discourse is no longer valid and cannot go on. Public accountability consists of three basic factors. The factors are a diversity of ideas, an engagement of public decision making, and finally an account for continuing a practice or way of doing something or a means or reason for changing the practice. Finally, public discourse ethics puts a great responsibility on the individual. They must continually be asking questions and finding answers. They will not always be right; and that is okay. As long as they are able to make a positive decision in the end.\n\nHabermas maintains that normative validity cannot be understood as separate from the argumentative procedures used in everyday practice, such as those used to resolve issues concerning the legitimacy of actions and the validity of the norms governing interactions. He makes this claim by making reference to the validity dimensions attached to speech acts in communication and the implicit forms of argumentation they imply (see Universal pragmatics). The basic idea is that the validity of a moral norm cannot be justified in the mind of an isolated individual reflecting on the world. The validity of a norm is justified only intersubjectively in processes of argumentation between individuals; in a dialectic. The validity of a claim to normative rightness depends upon the mutual understanding achieved by individuals in argument.\n\nFrom this it follows that the presuppositions of argumentation would become important. Kant extracted moral principles from the necessities forced upon a rational subject reflecting on the world. Habermas extracts moral principles from the necessities forced upon individuals engaged in the discursive justification of validity claims, from the inescapable presuppositions of communication and argumentation. These presuppositions were the kinds of idealization that individuals had to make in order for communication and argumentation to even begin. For example:\n\nThere were also presuppositions unique to discourse:\n\nThese are all at the center of Habermas's moral theory. Habermas's discourse ethics attempts to distill the idealized moral point of view that accompanies a perfectly rational process of argumentation (also idealized), which would be the moral principle implied by the presuppositions listed above. The key point is that the presuppositions of argumentation and communication that have been rationally reconstructed by Habermas are both factual and normative. This can be said about his entire project because it is explicitly attempting to bridge the gap between the \"is\" and the \"ought.\" Habermas speaks of the mutual recognition and exchanging of roles and perspectives that are demanded by the very structural condition of rational argumentation. He maintains that what is implied in these factual presuppositions of communication is the deep structure of moral norms, the conditions that every valid norm must fulfill.\n\nThe presuppositions of communication express a universal obligation to maintain impartial judgment in discourse, which constrains all affected to adopt the perspectives of all others in the exchange of reasons. From this Habermas extracts the following principle of universalization (U), which is the condition every valid norm has to fulfill:\n\nThis can be understood as the deep structure of all acceptable moral norms, and should not be confused with the principle of discourse ethics (D), which presupposes that norms exist that satisfy the conditions specified by (U).\n\n(D) Only those norms can claim to be valid that meet (or could meet) with the approval of all affected in their capacity as participants in a practical discourse.\n\nThe implications of (U) and (D) are quite profound. (U) claims to be a rational reconstruction of the impartial moral point of view at the heart of all cognitivist moral theories. According to moral cognitivists (e.g. Kant, Rawls etc.), it is only from such a moral point of view that insight into the actual (quasi-factual) impersonal obligations of a general will can be gained, because this perspective relieves decisions from the inaccuracies of personal interests. Of course, Habermas's reconstruction is different because it is intersubjective. That is, Habermas (unlike Kant or Rawls) formulates the moral point of view as it arises out of the multiple perspectives of those affected by a norm under consideration. The moral point of view explicated in (U) is not the property of an individual subject but the property of a community of interlocutors, the results of a complex dialogical process of role taking and perspective exchanging. Furthermore, (U) is deduced from a rational reconstruction of the presupposition of communication, which downgrades the strong transcendentalism of Kantian ethics by establishing a foundation in inner-worldly processes of communication.\n\n(D) on the other hand is a principle concerning the manner in which norms conforming to (U) must be justified through discourse. Again, Habermas takes the task of moral reflection out of the isolated individual's head and gives it to intersubjective processes of communication. What (D) proposes is that moral principles must be validated in actual discourse and that those to be affected by a norm must be able to participate in argumentation concerning its validity. No number of thought experiments can replace a communicative exchange with others regarding moral norms that will affect them. Moreover, this general prescription concerning the type of discourse necessary for the justification of moral norms opens the process of moral deliberation to the kind of learning that accompanies a fallibilistic orientation. (U) and (D) are catalysts for a moral learning process, which although fallible is not relative. The flesh and blood insights of participants in communicative exchange are refracted through the universal guidelines explicated from the deep structures of communication and argumentation. This spawns discourses with a rational trajectory, which are grounded in the particular circumstances of those involved but aimed at a universal moral validity.\n\nThe practical applications of discourse ethics have taken a significant turn after the publication of Habermas' book Between Facts and Norms (1992), where its application to democracy and the legislative process was substantially refined and expanded. Prior to this book, Habermas had left open the question of the various applications of discourse theory to almost any type of consensus oriented group ranging from highly visible political and governmental groups, such as Parliament in Great Britain and Congressional debate in the United States, and other consensus oriented activities as found in public and private institutions such as those supported on various international websites and Wikipedia.\n\n\n"}
{"id": "45229059", "url": "https://en.wikipedia.org/wiki?curid=45229059", "title": "Enactment effect", "text": "Enactment effect\n\nThe enactment effect, also called self-performed task effect (SPT effect) is a term that was created in the early 80's to describe the fact that verb phrases are memorized better if a learner performs the described action during learning, compared to just getting the verbal information or seeing someone else perform the action. The use of gestures improves the quantity of phrases that can be recalled, the phrases can be recalled for a longer amount of time, and they can be accessed easier. Knowing that enacting improves memory performance can be useful in education and treatment of patients with memory disorders. \nIn their study, Engelkamp and Krumnacker (1980) gave participants verbal phrases like “brush the teeth” or \"shuffle the cards\". In a recognition task and a free recall task, the memory of participants was tested under four learning conditions: one group performed the action, a second group was supposed to imagine the action, the third group watched someone perform the action, and the last group just heard the phrases. The group that had enacted the gestures performed best in both tasks.\nAround the same time, Cohen examined recall capacity of verbal phrases in participants under three conditions: one group performed the action on a concrete object, a second group saw the experimenter perform the action on an object, and a third group only got verbal instructions. The self-performed task led to the best results, supporting the claim that the enactment effect exists.\nSince then, the effect has been reduplicated in many studies.  Recall after enactment tasks was shown to be superior to recall after verbal tasks in children as well as in adults. Furthermore, the enactment effect is existent in elderly people and people with moderate dementia of the Alzheimer type.\n\nGestures have been shown to be a useful tool in teaching a foreign language. The enactment effect can be used in second language teaching in order to learn a language more efficiently, faster, and to prevent forgetting. Various studies have shown that the use of gestures while learning new words improves recall and retention. The enactment effect in second language learning could be shown in children as well as adults. Not only does enactment improve memorization, but also the use of words in speech production. Using gestures while learning is helpful in learning concrete words as well as abstract words such as “change” or “difference”. Studies supported this claim in showing that abstract words were better remembered when a gesture was used while encoding.\n\nIn more recent studies, researchers have been trying to find the neurological explanation of the enactment effect, and the reason why memory is enhanced after enactment. It was shown that not mere physical motor information leads to the enactment effect, but that the semantic content of the gesture plays a role as well. Iconic gestures enhance memory compared to meaningless gestures which have no positive effect on memorization. Event-related potentials showed that enactment leads to deeper processing of new information, eliciting the assumption that by using gestures, the meaning of the new word is connected with an already existing concept in one’s native language.\n"}
{"id": "22526518", "url": "https://en.wikipedia.org/wiki?curid=22526518", "title": "Eskimo–Uralic languages", "text": "Eskimo–Uralic languages\n\nThe Eskimo–Uralic hypothesis posits that the Uralic and Eskimo–Aleut language families belong to a common language family of which they are the two branches. Although substantial arguments for the hypothesis have been made, it is not generally accepted by linguists. The best-known advocate of the Eskimo–Uralic hypothesis is Knut Bergsland. The hypothesis dates back to the pioneering Danish linguist Rasmus Rask in 1818, upon noticing similarities between Greenlandic Eskimo and Finnish.\n\n\n"}
{"id": "1104562", "url": "https://en.wikipedia.org/wiki?curid=1104562", "title": "Fast mapping", "text": "Fast mapping\n\nIn cognitive psychology, fast mapping is the term used for the hypothesized mental process whereby a new concept is learned (or a new hypothesis formed) based only on a single exposure to a given unit of information. Fast mapping is thought by some researchers to be particularly important during language acquisition in young children, and may serve (at least in part) to explain the prodigious rate at which children gain vocabulary. In order to successfully use the fast mapping process, a child must possess the ability to use \"referent selection\" and \"referent retention\" of a novel word. There is evidence that this can be done by children as young as two years old, even with the constraints of minimal time and several distractors. Previous research in fast mapping has also shown that children are able to retain a newly learned word for a substantial amount of time after they are subjected to the word for the first time (Carey and Bartlett, 1978). Further research by Markson and Bloom (1997), showed that children can remember a novel word a week after it was presented to them even with only one exposure to the novel word. While children have also displayed the ability to have equal recall for other types of information, such as novel facts, their ability to extend the information seems to be unique to novel words. This suggests that fast mapping is a specified mechanism for word learning. The process was first formally articulated and the term 'fast mapping' coined by Harvard researchers Susan Carey and Elsa Bartlett in 1978.\n\nToday, there is evidence to suggest that children do not learn words through 'fast mapping' but rather learn probabilistic, predictive relationships between objects and sounds that develop over time. Evidence for this comes, for example, from children's struggles to understand color words: although infants can distinguish between basic color categories, many sighted children use color words in the same way that blind children do up until the fourth year. Typically, words such as \"blue\" and \"yellow\" appear in their vocabularies and they produce them in appropriate places in speech, but their application of individual color terms is haphazard and interchangeable. If shown a blue cup and asked its color, typical three-year-olds seem as likely to answer \"red\" as \"blue.\" These difficulties persist up until around age four, even after hundreds of explicit training trials. The inability for children to understand color stems from the cognitive process of whole object constraint. Whole object constraint is the idea that a child will understand that a novel word represents the entirety of that object. Then, if the child is presented with further novel words, they attach inferred meanings to the object. However, color is the last attribute to be considered because it explains the least about the object itself. Children's behavior clearly indicates that they have knowledge of these words, but this knowledge is far from complete; rather it appears to be predictive, as opposed to all-or-none.\n\nAn alternate theory of deriving the meaning of newly learned words by young children during language acquisition stems from John Locke's \"associative proposal theory\". Compared to the \"intentional proposal theory\", associative proposal theory refers to the deduction of meaning by comparing the novel object to environmental stimuli. A study conducted by Yu & Ballard (2007), introduced cross-situational learning, a method based on Locke's theory. Cross-situtational learning theory is a mechanism in which the child learns meaning of words over multiple exposures in varying contexts in an attempt to eliminate uncertainty of the word's true meaning on an exposure-by-exposure basis.\n\nOn the other hand, more recent studies suggest that some amount of fast mapping does take place, questioning the validity of previous laboratory studies that aim to show that probabilistic learning does occur. A critique to the theory of fast mapping is how can children connect the meaning of the novel word with the novel word after just one exposure? For example, when showing a child a blue ball and saying the word \"blue\" how does the child know that the word blue explains the color of the ball, not the size, or shape? If children learn words by fast mapping than they must use inductive reasoning to understand the meaning associated with the novel word. A popular theory to explain this inductive reasoning is that children apply word-learning constraints to the situation where a novel word is introduced. There are speculations as to why this is; Markman and Wachtel (1988) conducted a study that helps explain the possible underlying principles of fast mapping. They claim children adhere to the theories of whole-object bias, the assumption that a novel label refers to the entire object rather than its parts, color, substance or other properties, and mutual exclusivity bias, the assumption that only one label applies to each object. In their experiment, children were presented with an object that they either were familiar with or was presented with a whole object term. Markman and Watchel concluded that the mere juxtaposition between familiar and novel terms may assist in part term acquisition. In other words, children will put constraints on themselves and assume the novel term refers to the whole object in view rather than to its parts. There have been six lexical constraints proposed (reference, extendibility, object scope, categorical scope, novel name, conventionality) that guide a child's learning of a novel word. When learning a new word children apply these constraints. However, this purposed method of constraints is not flawless. If children use these constraints there are many words that children will never learn such as actions, attributes, and parts. Studies have found that both toddlers and adults were more likely to categorize an object by its shape than its size or color.\n\nThe next question in fast mapping theory is how exactly is the meaning of the novel word learned? An experiment performed in October 2012 by the Department of Psychology by University of Pennsylvania, researchers attempted to determine if fast mapping occurs via cross-situational learning or by another method, \"Propose but verify\". In cross-situational learning, listeners hear a novel word and store multiple conjectures of what the word could mean based on its situational context. Then after multiple exposures the listener is able to target the meaning of the word by ruling out conjectures. In propose but verify, the learner makes a single conjecture about the meaning of the word after hearing the word used in context. The learner then carries that conjecture forward to be reevaluated and modified for consistency when the word is used again. The results of the experiment seems to support that propose but verify is the way by which learners fast map new words.\n\nThere is also controversy over whether words learned by fast mapping are retained or forgotten. Previous research has found that generally, children retain a newly learned word for a period of time after learning. In the aforementioned Carey and Bartlett study (1978), children who were taught the word \"chromium\" were found to keep the new lexical entry in working memory for several days, illustrating a process of gradual lexical alignment known as \"extended mapping.\" Another study, performed by Markson and Bloom (1997), showed that children remembered words up to 1 month after the study was conducted. However, more recent studies have shown that words learned by fast mapping tend to be forgotten over time. In a study conducted by Vlach and Sandhofer (2012), memory supports, which had been included in previous studies, were removed. This removal appeared to result in a low retention of words over time. This is a possible explanation for why previous studies showed high retention of words learned by fast mapping.\n\nSome researchers are concerned that experiments testing for fast mapping are produced in artificial settings. They feel that fast mapping doesn't occur as often in more real life, natural situations. They believe that testing for fast mapping should focus more on the actual understanding of a word instead of just its reproduction. For some, testing to see if the child can use the new word in a different situation constitutes true knowledge of a word, rather than simply identifying the new word.\n\nWhen learning novel words, it is believed that early exposure to multiple linguistic systems facilitates the acquisition of new words later in life. This effect was referred to by Kaushanskaya and Marian (2009) as the bilingual advantage. That being said, a bilingual individual's ability to fast map can vary greatly throughout their life.\n\nDuring the language acquisition process, a child may require a greater amount of time to determine a correct referent than a child who is a monolingual speaker. By the time a bilingual child is of school age, they perform equally on naming tasks when compared to monolingual children. By the age of adulthood, bilingual individuals have acquired word-learning strategies believed to be of assistance on fast mapping tasks. One example is speech practice, a strategy where the participant listens and reproduces the word in order to assist in remembering and decrease the likelihood of forgetting .\nBilingualism can increase an individual's cognitive abilities and contribute to their success in fast mapping words, even when they are using a nonnative language.\n\nChildren growing up in a low-socioeconomic status environment receive less attention than those in high-socioeconomic status environments. As a result, these children may be exposed to less words and therefore their language development may suffer. On norm-references vocabulary tests, children from low- socioeconomic homes tend to score lower than same-age children from a high-socioeconomic environment. However, when examining their fast mapping abilities there were no significant differences observed in their ability to learn and remember novel words. Children from low SES families were able to use multiple sources of information in order to fast map novel words. When working with children from low SES homes, providing a context of the word that attributes meaning, is a linguistic strategy that can benefit the child's word knowledge development.\n\nThree learning supports that have been proven to help with the fast mapping of words are saliency, repetition and generation of information. The amount of face-to-face interaction a child has with their parent affects his or her ability to fast map novel words. Interaction with a parent leads to greater exposure to words in different contexts, which in turn promotes language acquisition. Face to face interaction cannot be replaced by educational shows because although repetition is used, children do not receive the same level of correction or trial and error from simply watching. When a child is asked to generate the word it promotes the transition to long-term memory to a larger extent.\n\nIt appears that fast mapping is not only limited to humans, but can occur in dogs as well. In an experiment, a dog, named Rico, was able to learn the labels of over 200 various items. He was also able to identify novel objects simply by exclusion learning. Exclusion learning occurs when one learns the name of a novel object because one is already familiar with the names of other objects belonging to the same group. The researchers, who conducted the experiment, mention the possibility that a language acquisition device specific to humans does not control fast mapping. They believe that fast mapping is possibly directed by simple memory mechanisms.\n\nA study by Lederberg et al., was performed to determine if deaf and hard of hearing children fast map to learn novel words. In the study, when the novel word was introduced, the word was both spoken and signed. Then the children were asked to identify the referent object and even extend the novel word to identify a similar object. The results of the study indicated that deaf and hard of hearing children do perform fast mapping to learn novel words. However, compared to children with normal hearing (aging toddlers to 5 years old) the deaf and hard of hearing children did not fast map as accurately and successfully. The results showed a slight delay which disappeared as the children were a maximum of 5 years old. The conclusion that was drawn from the study is that the ability to fast map has a relationship to the size of the lexicon. The children with normal hearing had a larger lexicon and therefore were able to more accurately fast map compared to deaf and hard of hearing children who did not have as large of a lexicon. It is by around age 5 that deaf and hard of hearing children have a similar size lexicon to 5 year old children of normal hearing. This evidence supports the idea that fast mapping requires inductive reasoning so the larger the lexicon (number of known words) the easier it is for the child to reason out the accurate meaning for the novel word.\n\nIn the area of cochlear implants (CIs), there are variegated opinions on whether cochlear implants impact a child's ability to become a more successful fast mapper. In 2000, a study by Kirk, Myomoto, and others determined that there was a general correlation between the age of Cochlear Implant implementation and improved lexical skills (e.g. fast mapping and other vocabulary growth skills). They believed that children given implants prior to two years of age yielded higher success rates than older children between five and seven years of age. With that said, researchers at the University of Iowa wish to amend that very generalization. In 2013, \"Word Learning Processes in Children with Cochlear Implants\" by Elizabeth Walker and others indicated that although there may be a some levels of increased vocabulary acquisition in CI individuals, many post-implantees generally were slower developers of his/her own lexicon. Walker bases her claims on another research study in 2007 (Tomblin et al.) One of the purposes of this study was to note a CI child's ability to comprehend and retain novel words with related referents. When compared with non-deaf children, the CI children had lower success scores in retention. This finding was based on scorings obtained from their test: from 0 to 6 (0 the worst, 6 the best), CI children averaged a score around a 2.0 whereas non-deaf children scored higher (roughly 3.86).\n\nAn experiment was performed to assess fast mapping in adults with typical language abilities, disorders of spoken/written language (hDSWL), and adults with hDSWL and ADHD. \nThe conclusion draw from the experiment revealed that adults with ADHD were the least accurate at \"mapping semantic features and slower to respond to lexical labels.\" \nThe article reasoned that the tasks of fast mapping requires high attentional demand and so \"a lapse in attention could lead to diminished encoding of the new information.\"\n\nFast mapping in individuals with aphasia has gained research attention due to its effect on speaking, listening, reading, and writing. Research done by Blumstein makes an important distinction between those with Broca's aphasia, who are limited in physical speech, as compared to those with Wernicke's aphasia, who cannot link words with meaning. In Broca's aphasia, Blumstein found that whereas individuals with Wernicke's aphasia performed at the same level as the normal control group, those with Broca's aphasia showed slower reaction times for word presentations after reduced voice onset time stimuli. In short, when stimuli were acoustically altered, individuals with Broca's aphasia experienced difficulty recognizing the novel stimuli upon second presentation. Bloomstein's findings reinforce the crucial difference between one's ability to retain novel stimuli versus the ability to express novel stimuli. Because individuals with Wernicke's aphasia are only limited in their understanding of semantic meaning, it makes sense that the participant's novel stimulus recall would not be affected. On the other hand, those with Broca's aphasia lack the ability to produce speech, in effect hindering their ability to recall novel stimuli. Although individuals with Broca's aphasia are limited in their speech production, it is not clear whether they simply cannot formulate the physical speech or if they actually did not process the stimuli.\n\nResearch has also been done investigating fast mapping abilities in children with language deficits. One study done by Dollaghan compared children with normal language to those with expressive syntactic deficits, a type of specific language impairment characterized by simplified speech. The study found that normal and language impaired children did not differ in their ability to connect the novel word to referent or to comprehend the novel word after a single exposure. The only difference was that the language-impaired children were less successful in their production of the novel word. This implies that expressive language deficits are unrelated to the ability to connect word and referent in a single exposure. The problem for children with those deficits arises only when trying to convert that mental representation into verbal speech.\n\nA few researchers looked at fast mapping abilities in boys with autistic spectrum disorders (ASD), also referred to as autism spectrum, and boys with fragile X syndrome (FXS). The experimental procedure consisted of a presentation phase where two objects were presented, one of which was a novel object with a nonsense word name. This was followed by a comprehension testing phase, which assessed the boys' ability to remember and correctly select the novel objects. Even though all groups in the study had fast mapping performances above chance levels, in comparison to boys showing typical development, those with ASD and FXS demonstrated much more difficulty in comprehending and remembering names assigned to the novel objects. The authors concluded that initial processes involved in associative learning, such as fast mapping, are hindered in boys with FXS and ASD.\n\nResearch in artificial intelligence and machine learning to reproduce computationally this ability, termed one-shot learning. This is pursued to reduce the learning curve, as other models like reinforcement learning need thousand of exposures to a situation to learn it.\n\n"}
{"id": "15864296", "url": "https://en.wikipedia.org/wiki?curid=15864296", "title": "Fluency", "text": "Fluency\n\nFluency (also called volubility and eloquency) is the property of a person or of a system that delivers information quickly and with expertise.\n\nLanguage fluency is one of a variety of terms used to characterize or measure a person's language ability, often used in conjunction with accuracy and complexity. Although there are no widely agreed-upon definitions or measures of language fluency, someone is typically said to be fluent if their use of the language appears \"fluid\", or natural, coherent, and easy as opposed to slow, halting use. In other words, fluency is often described as the ability to produce language on demand and be understood.\n\nVarying definitions of fluency characterize it by the language user’s automaticity, their speed and coherency of language use, or the length and rate of their speech output. Theories of automaticity postulate that more fluent language users can manage all of the components of language use without paying attention to each individual component of the act. In other words, fluency is achieved when one can access language knowledge and produce language unconsciously, or automatically. Theories that focus on speed or length and rate of speech typically expect fluent language users to produce language in real time without unusual pauses, false starts, or repetitions (recognizing that some presence of these elements are naturally part of speech). Fluency is sometimes considered to be a measure of performance rather than an indicator of more concrete language knowledge, and thus perception and understandability are often key ways that fluency is understood.\n\nLanguage fluency is sometimes contrasted with accuracy (or correctness of language use, especially grammatical correctness) and complexity (or a more encompassing knowledge of vocabulary and discourse strategies). Fluency, accuracy, and complexity are distinct but interrelated components of language acquisition and proficiency.\n\nThere are four commonly discussed types of fluency: reading fluency, oral fluency, oral-reading fluency, and written or compositional fluency. These types of fluency are interrelated, but do not necessarily develop in tandem or linearly. One may develop fluency in certain type(s) and be less fluent or nonfluent in others.\n\nIn the sense of proficiency, \"fluency\" encompasses a number of related but separable skills:\n\nBecause an assessment of fluency is typically a measure or characterization of one's language ability, determining fluency may be a more challenging task when the speaker is acquiring a second language. It is generally thought that the later in life a learner approaches the study of a foreign language, the harder it is to acquire receptive (auditory) comprehension and fluent production (speaking) skills. For adults, once their mother tongue has already been established, the acquisition of a second language can come more slowly and less completely, ultimately affecting fluency. However, the critical period hypothesis is a hotly debated topic, with some scholars stating that adults can in fact become fluent in acquiring a second language. For instance, reading and writing skills in a foreign language can be acquired more easily even after the primary language acquisition period of youth is over.\n\nSo although it is often assumed that young children learn languages more easily than adolescents and adults, the reverse is in fact true; older learners are faster. The only exception to this rule is in pronunciation. Young children invariably learn to speak their second language with native-like pronunciation, whereas learners who start learning a language at an older age only rarely reach a native-like level.\n\nSince childhood is a critical period, widespread opinion holds that it is easier for young children to learn a second language than it is for adults. Children can even acquire native fluency when exposed to the language on a consistent basis with rich interaction in a social setting. In addition to capacity, factors like; 1) motivation, 2) aptitude, 3) personality characteristics, 4) age of acquisition 5) first language typology 6) socio-economic status and 7) quality and context of L2 input play a role in L2 acquisitions rate and building fluency. Second language acquisition (SLA) has the ability to influence children’s cognitive growth and linguistic development.\n\nSkill that consists of ability to produce words in target language develops until adolescence. Natural ability to acquire a new language with a deliberate effort may begin to diminish around puberty i.e. 12–14 years of age. Learning environment, comprehensible instructional materials, teacher, and the learner are indispensable elements in SLA and developing fluency in children. Extensive reading in L2 can offer twofold benefits in foreign language learning i.e. \"reading to comprehend English and reading to learn English\".\n\nParadis (2006) study on childhood language acquisition and building fluency examines how first and second language acquisition patterns are generally similar including vocabulary and morphosyntax. Phonology of first language is usually apparent in SLA and initial L1 influence can be lifelong, even for child L2 learners.\n\nChildren can acquire a second language simultaneously (learn L1 and L2 at the same time) or sequentially (learn L1 first and then L2). In the end, they develop fluency in both with one dominant language which is spoken largely by the community they live in.\n\nAccording to one source, there are five stages of SLA and developing fluency:\n\nThe process of learning a second language or \"L2,\" among older learners differs from younger learners because of their working memory. Working memory, also connected to fluency because it deals with automatic responses, is vital to language acquisition. This happens when information is stored and manipulated temporarily. During working memory, words are filtered, processed, and rehearsed, and information is stored while focusing on the next piece of interaction. These false starts, pauses or repetitions found in fluency assessments, can also be found within one's working memory as part of communication.\n\nThose with education at or below a high school level are least likely to take language classes. It has also been found that women and young immigrants are more likely to take language classes. Further, highly educated immigrants who are searching for skilled jobs – which require interpersonal and intercultural skills that are difficult to learn – are the most affected by lower fluency in the L2.\n\nFluency is a speech language pathology term that means the smoothness or flow with which sounds, syllables, words and phrases are joined together when speaking quickly. \"Fluency disorders\" is used as a collective term for cluttering and stuttering. Both disorders have breaks in the fluidity of speech, and both have the fluency breakdown of repetition of parts of speech.\n\nStudies in the assessment of creativity list fluency as one of the four primary elements in creative thinking, the others being flexibility, originality and elaboration. Fluency in creative thinking is seen as the ability to think of many diverse ideas quickly.\n\n\n"}
{"id": "47548689", "url": "https://en.wikipedia.org/wiki?curid=47548689", "title": "Gangwon dialect", "text": "Gangwon dialect\n\nThe Gangwon dialect is spoken in South Korea's Gangwon Province and in North Korea's Kangwŏn Province. Although they are large provinces by area, relatively few people lived in the Gangwon Province. As a result, people living in the western side of Gangwon (Yeongseo) did not develop a highly distinctive dialect. However, the part of Gangwon that stretches on the eastern coast of Korea (Yeongdong), did develop a distinctive dialect. This is because the Taebaek Mountains bisect the Gangwon Province, and the people on eastern Gangwon are isolated from the high mountains. The Gangwon dialect uses tones distinguish homophonic words, much like Chinese or Vietnamese do.\n"}
{"id": "51467184", "url": "https://en.wikipedia.org/wiki?curid=51467184", "title": "Indo-Portuguese Creole of Bombay", "text": "Indo-Portuguese Creole of Bombay\n\nThe Indo-Portuguese Creole of Bombay was a creole language based on Portuguese, which grew out of the long contact between the Portuguese and local languages. Currently this language is extinct. It was spoken in Bombay (now Mumbai) and northern India: Bassein, Salsette, Thana, Chevai, Mahim, Tecelaria, Dadar, Parel, Cavel, Bandora-Badra, Govai, Marol, Andheri, Versova, Malvan, Manori, Mazagaon. This language was, after the Ceylon creole dialect of Indo-Portuguese, the most important. In 1906 there were still close to 5,000 people who spoke Portuguese Creole as their mother tongue, 2,000 in Mumbai and Mahim, 1000 in Bandora, 500 in Thana, 100 in Curla (now Kurla), 50 in Bassein and 1,000 in other towns. There were, at that time, schools that taught Creole and the richest classes, which were replaced by the English language.\n\n"}
{"id": "10956960", "url": "https://en.wikipedia.org/wiki?curid=10956960", "title": "Information and media literacy", "text": "Information and media literacy\n\nInformation and media literacy (IML) enables people to interpret and make informed judgments as users of information and media, as well as to become skillful creators and producers of information and media messages in their own right.\n\nPrior to the 1990s, the primary focus of information literacy was research skills. Media literacy, a study that emerged around the 1970s, traditionally focuses on the analysis and the delivery of information through various forms of media. Nowadays, the study of information literacy has been extended to include the study of media literacy in many countries like the UK, Australia and New Zealand. The term \"information and media literacy\" is used by UNESCO to differentiate the combined study from the existing study of information literacy. It is also referred to as information and communication technologies (ICT) in the United States. Educators such as Gregory Ulmer have also defined the field as electracy.\n\nIML is a combination of information literacy and media literacy. The purpose of being information and media literate is to engage in a digital society; one needs to be able to understand, inquire, create, communicate and think critically. It is important to effectively access, organize, analyze, evaluate, and create messages in a variety of forms. The transformative nature of IML includes creative works and creating new knowledge; to publish and collaborate responsibly requires ethical, cultural and social understanding.\n\nThe IML learning capacities prepare students to be 21st century literate. According to Jeff Wilhelm (2000), \"technology has everything to do with literacy. And being able to use the latest electronic technologies has everything to do with being literate.\" He supports his argument with J. David Bolter's statement that \"if our students are not reading and composing with various electronic technologies, then they are illiterate. They are not just unprepared for the future; they are illiterate right now, in our current time and context\".\n\nWilhelm's statement is supported by the 2005 Wired World Phase II (YCWW II) survey conducted by the Media Awareness Network of Canada on 5000 Grade 4 – 11 students. The key findings of the survey were:\n\nMarc Prensky (2001) uses the term \"digital native\" to describe people who have been brought up in a digital world. The Internet has been a pervasive element of young people's home lives. 94% of kids reported that they had Internet access at home, and a significant majority (61%) had a high-speed connection.\n\nBy the time kids reach Grade 11, half of them (51 percent) have their own Internet-connected computer, separate and apart from the family computer. The survey also showed that young Canadians are now among the most wired in the world. Contrary to the earlier stereotype of the isolated and awkward computer nerd, today's wired kid is a social kid.\n\nIn general, many students are better networked through the use of technology than most teachers and parents, who may not understand the abilities of technology. Students are no longer limited to desktop computera. They may use mobile technologies to graph mathematical problems, research a question for social studies, text message an expert for information, or send homework to a drop box. Students are accessing information by using MSN, personal Web pages, Weblogs and social networking sites.\n\nMany teachers continue the tradition of teaching of the past 50 years. Traditionally teachers have been the experts sharing their knowledge with children. Technology, and the learning tools it provides access to, forces us to change to being facilitators of learning. We have to change the stereotype of teacher as the expert who delivers information, and students as consumers of information, in order to meet the needs of digital students. Teachers not only need to learn to speak digital, but also to embrace the language of digital natives.\n\nLanguage is generally defined as a system used to communicate in which symbols convey information. Digital natives can communicate fluently with digital devices and convey information in a way that was impossible without digital devices. People born prior to 1988 are sometimes referred to as \"digital immigrants.\" They experience difficulty programming simple devices like a VCR. Digital immigrants do not start pushing buttons to make things work.\n\nLearning a language is best done early in a child's development.\n\nIn acquiring a second language, Hyltenstam (1992) found that around the age of 6 and 7 seemed to be a cut-off point for bilinguals to achieve native-like proficiency. After that age, second language learners could get near-native-like-ness but their language would, while consisting of very few actual errors, have enough errors that would set them apart from the first language group.\n\nKindergarten and grades 1 and 2 are critical to student success as digital natives because not all students have a \"digital\"-rich childhood. Students learning technological skills before Grade 3 can become equivalently bilingual. \"Language-minority students who cannot read and write proficiently in English cannot participate fully in American schools, workplaces, or society. They face limited job opportunities and earning power.\" Speaking \"digital\" is as important as being literate in order to participate fully in North American society and opportunities.\n\nMany students are considered illiterate in media and information for various reasons. They may not see the value of media and information literacy in the 21st-century classroom. Others are not aware of the emergence of the new form of information. Educators need to introduce IML to these students to help them become media and information literate. Very little changes will be made if the educators are not supporting information and media literacy in their own classrooms.\n\nPerformance standards, the foundation to support them, and tools to implement them are readily available. Success will come when there is full implementation and equitable access are established. Shared vision and goals that focus on strategic actions with measurable results are also necessary.\n\nWhen the staff and community, working together, identify and clarify their values, beliefs, assumptions, and perceptions about what they want children to know and be able to do, an important next step will be to discover which of these values and expectations will be achieved. Using the capacity tools to assess IML will allow students, staff and the community to reflect on how well students are meeting learning needs as related to technology.\n\nThe IML Performance standards allow data collection and analysis to evidence that student-learning needs are being met. After assessing student IML, three questions can be asked:\n\n\nTeachers can use classroom assessment for learning to identify areas that might need increased focus and support. Students can use classroom assessment to set learning goals for themselves.\n\nThis integration of technology across the curriculum is a positive shift from computers being viewed as boxes to be learned to computers being used as technical communication tools. In addition, recent learning pedagogy recognizes the inclusion for students to be creators of knowledge through technology. International Society for Technology in Education (ISTE) has been developing a standard IML curriculum for the US and other countries by implementing the National Educational Technology Standards.\n\nIn the UK, IML has been promoted among educators through an information literacy website developed by several organizations that have been involved in the field.\n\nIML is included in the Partnership for the 21st Century program sponsored by the US Department of Education. Special mandates have been provided to Arizona, Iowa, Kansas, Maine, New Jersey, Massachusetts, North Carolina, South Dakota, West Virginia and Wisconsin. Individual school districts, such as the Clarkstown Central School District, have also developed their own information literacy curriculum. ISTE has also produced the National Educational Technology Standards for Students, Teachers and Administrators.\n\nIn British Columbia, Canada, the Ministry of Education has de-listed the Information Technology K to 7 IRP as a stand-alone course. It is still expected that all the prescribed learning outcomes continue to be integrated.\n\nThis integration of technology across the curriculum is a positive shift from computers being viewed as boxes to be learned to computers being used as technical communication tools. In addition, recent learning pedagogy recognizes the inclusion for students to be creators of knowledge through technology. Unfortunately, there has been no clear direction to implement IML.\n\nThe BC Ministry of Education published the Information and Communications Technology Integration Performance Standards, Grades 5 to 10 ICTI in 2005. These standards provide performance standards expectations for Grade 5 to 10; however, they do not provide guidance for other grades, and the expectation for a Grade 5 and Grade 10 student are the same.\n\nIn the Arab region, media and information literacy was largely ignored up until 2011, when the Media Studies Program at the American University of Beirut, the Open Society Foundations and the Arab-US Association for Communication Educators (AUSACE) launched a regional conference themed \"New Directions: Digital and Media Literacy\". The conference attracted significant attention from Arab universities and scholars, who discussed obstacles and needs to advance media literacy in the Arab region, including developing curricula in Arabic, training faculty and promoting the field. \n\nFollowing up on that recommendation, the Media Studies Program at AUB and the Open Society Foundations in collaboration with the Salzburg Academy on Media and Global Change launched in 2013 the first regional initiative to develop, vitalize, and advance media literacy education in the Arab region. The Media and Digital Literacy Academy of Beirut (MDLAB) offered an annual two-week summer training program in addition to working year-round to develop media literacy curricula and programs. The academy is conducted in Arabic and English and brings pioneering international instructors and professionals to teach advanced digital and media literacy concepts to young Arab academics and graduate students from various fields. MDLAB hopes that the participating Arab academics will carry what they learned to their countries and institutions and offers free curricular material in Arabic and English, including media literacy syllabi, lectures, exercises, lesson plans, and multi-media material, to assist and encourage the integration of digital and media literacy into Arab university and school curricula. \n\nIn recognition of MDLAB's accomplishments in advancing media literacy education in the Arab region, the founder of MDLAB received the 2015 UNESCO-UNAOC International Media and Information Literacy Award. \n\nPrior to 2013, only two Arab universities offered media literacy courses: the American University of Beirut (AUB) and the American University of Sharjah (AUS). Three years after the launch of MDLAB, over two dozen Arab universities incorporated media literacy education into their curricula, both as stand-alone courses or as modules injected into their existing media courses. Among the universities who have full-fledged media literacy courses (as of 2015) are Lebanese American University (Lebanon), Birzeit University (Palestine), University of Balamand (Lebanon), Damascus University (Syria), Rafik Hariri University (Lebanon), Notre Dame University (Lebanon), Ahram Canadian University (Egypt), American University of Beirut (Lebanon), American University of Sharjah (UAE), and Al Azm University (Lebanon). The first Arab school to adopt media literacy as part of its strategic plan is the International College (IC) in Lebanon. Efforts to introduce media literacy to the region's other universities and schools continues with the help of other international organizations, such as UNESCO, UNAOC, AREACORE, DAAD, and OSF.\n\nIn Singapore and Hong Kong, information literacy or information technology was listed as a formal curriculum.\n\nOne barrier to learning to read is the lack of books, while a barrier to learning IML is the lack of technology access. Highlighting the value of IML helps to identify existing barriers within school infrastructure, staff development, and support systems. While there is a continued need to work on the foundations to provide a sustainable and equitable access, the biggest obstacle is school climate.\n\nMarc Prensky identifies one barrier as teachers viewing digital devices as distractions: \"Let's admit the real reason that we ban cell phones is that, given the opportunity to use them, students would vote with their attention, just as adults would 'vote with their feet' by leaving the room when a presentation is not compelling.\"\n\nThe mindset of banning new technology, and fearing the bad things that can happen, can affect educational decisions. The decision to ban digital devices impacts students for the rest of their lives.\n\nAny tool that is used poorly or incorrectly can be unsafe. Safety lessons are mandatory in industrial technology and science. Yet safety or ethical lessons are not mandatory to use technology.\n\nNot all decisions in schools are measured by common ground beliefs. One school district in Ontario banned digital devices from their schools. Local schools have been looking at doing the same. These kinds of reactions are often about immediate actions and not about teaching, learning or creating solutions. Many barriers to IML exist.\n\n"}
{"id": "1549080", "url": "https://en.wikipedia.org/wiki?curid=1549080", "title": "Koryak language", "text": "Koryak language\n\nKoryak () is a Chukotko-Kamchatkan language spoken by about 1,700 people in the easternmost extremity of Siberia, mainly in Koryak Okrug. It is mostly spoken by Koryaks. Its close relative, the Chukchi language, is spoken by about three times that number. The language together with Chukchi, Kerek, Alutor and Itelmen forms the Chukotko-Kamchatkan language family. Its native name in Koryak is нымылан \"nymylan\", but the Russian name is more common.\n\nThe Chukchi and Koryaks form a cultural unit with an economy based on reindeer herding and both have autonomy within the Russian Federation.\n\n"}
{"id": "35094700", "url": "https://en.wikipedia.org/wiki?curid=35094700", "title": "Kosena language", "text": "Kosena language\n\nKosena is a Kainantu language of Papua New Guinea.\n"}
{"id": "17524", "url": "https://en.wikipedia.org/wiki?curid=17524", "title": "Language", "text": "Language\n\nLanguage is a system that consists of the development, acquisition, maintenance and use of complex systems of communication, particularly the human ability to do so; and a language is any specific example of such a system.\n\nThe scientific study of language is called linguistics. Questions concerning the philosophy of language, such as whether words can represent experience, have been debated at least since Gorgias and Plato in ancient Greece. Thinkers such as Rousseau have argued that language originated from emotions while others like Kant have held that it originated from rational and logical thought. 20th-century philosophers such as Wittgenstein argued that philosophy is really the study of language. Major figures in linguistics include Ferdinand de Saussure and Noam Chomsky.\n\nEstimates of the number of human languages in the world vary between 5,000 and 7,000. However, any precise estimate depends on a partly arbitrary distinction between languages and dialects. Natural languages are spoken or signed, but any language can be encoded into secondary media using auditory, visual, or tactile stimuli – for example, in whistling, signed, or braille. This is because human language is modality-independent. Depending on philosophical perspectives regarding the definition of language and meaning, when used as a general concept, \"language\" may refer to the cognitive ability to learn and use systems of complex communication, or to describe the set of rules that makes up these systems, or the set of utterances that can be produced from those rules. All languages rely on the process of semiosis to relate signs to particular meanings. Oral, manual and tactile languages contain a phonological system that governs how symbols are used to form sequences known as words or morphemes, and a syntactic system that governs how words and morphemes are combined to form phrases and utterances.\n\nHuman language has the properties of productivity and displacement, and relies entirely on social convention and learning. Its complex structure affords a much wider range of expressions than any known system of animal communication. Language is thought to have originated when early hominins started gradually changing their primate communication systems, acquiring the ability to form a theory of other minds and a shared intentionality. This development is sometimes thought to have coincided with an increase in brain volume, and many linguists see the structures of language as having evolved to serve specific communicative and social functions. Language is processed in many different locations in the human brain, but especially in Broca's and Wernicke's areas. Humans acquire language through social interaction in early childhood, and children generally speak fluently by approximately three years old. The use of language is deeply entrenched in human culture. Therefore, in addition to its strictly communicative uses, language also has many social and cultural uses, such as signifying group identity, social stratification, as well as social grooming and entertainment.\n\nLanguages evolve and diversify over time, and the history of their evolution can be reconstructed by comparing modern languages to determine which traits their ancestral languages must have had in order for the later developmental stages to occur. A group of languages that descend from a common ancestor is known as a language family. The Indo-European family is the most widely spoken and includes languages as diverse as English, Russian and Hindi; the Sino-Tibetan family includes Mandarin, Bodo and the other Chinese languages, and Tibetan; the Afro-Asiatic family includes Arabic, Somali, and Hebrew; the Bantu languages include Swahili, and Zulu, and hundreds of other languages spoken throughout Africa; and the Malayo-Polynesian languages include Indonesian, Malay, Tagalog, and hundreds of other languages spoken throughout the Pacific. The languages of the Dravidian family, spoken mostly in Southern India, include Tamil Telugu and Kannada. Academic consensus holds that between 50% and 90% of languages spoken at the beginning of the 21st century will probably have become extinct by the year 2100.\n\nThe English word \"language\" derives ultimately from Proto-Indo-European \"\" \"tongue, speech, language\" through Latin \"lingua\", \"language; tongue\", and Old French \"language\". The word is sometimes used to refer to codes, ciphers, and other kinds of artificially constructed communication systems such as formally defined computer languages used for computer programming. Unlike conventional human languages, a formal language in this sense is a system of signs for encoding and decoding information. This article specifically concerns the properties of natural human language as it is studied in the discipline of linguistics.\n\nAs an object of linguistic study, \"language\" has two primary meanings: an abstract concept, and a specific linguistic system, e.g. \"French\". The Swiss linguist Ferdinand de Saussure, who defined the modern discipline of linguistics, first explicitly formulated the distinction using the French word \"langage\" for language as a concept, \"langue\" as a specific instance of a language system, and \"parole\" for the concrete usage of speech in a particular language.\n\nWhen speaking of language as a general concept, definitions can be used which stress different aspects of the phenomenon. These definitions also entail different approaches and understandings of language, and they also inform different and often incompatible schools of linguistic theory. Debates about the nature and origin of language go back to the ancient world. Greek philosophers such as Gorgias and Plato debated the relation between words, concepts and reality. Gorgias argued that language could represent neither the objective experience nor human experience, and that communication and truth were therefore impossible. Plato maintained that communication is possible because language represents ideas and concepts that exist independently of, and prior to, language.\n\nDuring the Enlightenment and its debates about human origins, it became fashionable to speculate about the origin of language. Thinkers such as Rousseau and Herder argued that language had originated in the instinctive expression of emotions, and that it was originally closer to music and poetry than to the logical expression of rational thought. Rationalist philosophers such as Kant and Descartes held the opposite view. Around the turn of the 20th century, thinkers began to wonder about the role of language in shaping our experiences of the world – asking whether language simply reflects the objective structure of the world, or whether it creates concepts that it in turn imposes on our experience of the objective world. This led to the question of whether philosophical problems are really firstly linguistic problems. The resurgence of the view that language plays a significant role in the creation and circulation of concepts, and that the study of philosophy is essentially the study of language, is associated with what has been called the linguistic turn and philosophers such as Wittgenstein in 20th-century philosophy. These debates about language in relation to meaning and reference, cognition and consciousness remain active today.\n\nOne definition sees language primarily as the mental faculty that allows humans to undertake linguistic behaviour: to learn languages and to produce and understand utterances. This definition stresses the universality of language to all humans, and it emphasizes the biological basis for the human capacity for language as a unique development of the human brain. Proponents of the view that the drive to language acquisition is innate in humans argue that this is supported by the fact that all cognitively normal children raised in an environment where language is accessible will acquire language without formal instruction. Languages may even develop spontaneously in environments where people live or grow up together without a common language; for example, creole languages and spontaneously developed sign languages such as Nicaraguan Sign Language. This view, which can be traced back to the philosophers Kant and Descartes, understands language to be largely innate, for example, in Chomsky's theory of Universal Grammar, or American philosopher Jerry Fodor's extreme innatist theory. These kinds of definitions are often applied in studies of language within a cognitive science framework and in neurolinguistics.\n\nAnother definition sees language as a formal system of signs governed by grammatical rules of combination to communicate meaning. This definition stresses that human languages can be described as closed structural systems consisting of rules that relate particular signs to particular meanings. This structuralist view of language was first introduced by Ferdinand de Saussure, and his structuralism remains foundational for many approaches to language.\n\nSome proponents of Saussure's view of language have advocated a formal approach which studies language structure by identifying its basic elements and then by presenting a formal account of the rules according to which the elements combine in order to form words and sentences. The main proponent of such a theory is Noam Chomsky, the originator of the generative theory of grammar, who has defined language as the construction of sentences that can be generated using transformational grammars. Chomsky considers these rules to be an innate feature of the human mind and to constitute the rudiments of what language is. By way of contrast, such transformational grammars are also commonly used to provide formal definitions of language are commonly used in formal logic, in formal theories of grammar, and in applied computational linguistics. In the philosophy of language, the view of linguistic meaning as residing in the logical relations between propositions and reality was developed by philosophers such as Alfred Tarski, Bertrand Russell, and other formal logicians.\n\nYet another definition sees language as a system of communication that enables humans to exchange verbal or symbolic utterances. This definition stresses the social functions of language and the fact that humans use it to express themselves and to manipulate objects in their environment. Functional theories of grammar explain grammatical structures by their communicative functions, and understand the grammatical structures of language to be the result of an adaptive process by which grammar was \"tailored\" to serve the communicative needs of its users.\n\nThis view of language is associated with the study of language in pragmatic, cognitive, and interactive frameworks, as well as in sociolinguistics and linguistic anthropology. Functionalist theories tend to study grammar as dynamic phenomena, as structures that are always in the process of changing as they are employed by their speakers. This view places importance on the study of linguistic typology, or the classification of languages according to structural features, as it can be shown that processes of grammaticalization tend to follow trajectories that are partly dependent on typology. In the philosophy of language, the view of pragmatics as being central to language and meaning is often associated with Wittgenstein's later works and with ordinary language philosophers such as J.L. Austin, Paul Grice, John Searle, and W.O. Quine.\n\nA number of features, many of which were described by Charles Hockett and called design features set human language apart from other known systems of communication, such as those used by non-human animals.\n\nCommunication systems used by other animals such as bees or apes are closed systems that consist of a finite, usually very limited, number of possible ideas that can be expressed. In contrast, human language is open-ended and productive, meaning that it allows humans to produce a vast range of utterances from a finite set of elements, and to create new words and sentences. This is possible because human language is based on a dual code, in which a finite number of elements which are meaningless in themselves (e.g. sounds, letters or gestures) can be combined to form an infinite number of larger units of meaning (words and sentences). However, one study has demonstrated that an Australian bird, the chestnut-crowned babbler, is capable of using the same acoustic elements in different arrangements to create two functionally distinct vocalizations. Additionally, pied babblers have demonstrated the ability to generate two functionally distinct vocalisations composed of the same sound type, which can only be distinguished by the number of repeated elements.\n\nSeveral species of animals have proved to be able to acquire forms of communication through social learning: for instance a bonobo named Kanzi learned to express itself using a set of symbolic lexigrams. Similarly, many species of birds and whales learn their songs by imitating other members of their species. However, while some animals may acquire large numbers of words and symbols, none have been able to learn as many different signs as are generally known by an average 4 year old human, nor have any acquired anything resembling the complex grammar of human language.\n\nHuman languages also differ from animal communication systems in that they employ grammatical and semantic categories, such as noun and verb, present and past, which may be used to express exceedingly complex meanings. Human language is also unique in having the property of recursivity: for example, a noun phrase can contain another noun phrase (as in \"<nowiki>the chimpanzee]'s lips]</nowiki>\") or a clause can contain another clause (as in \"<nowiki>[I see [the dog is running</nowiki>\"). Human language is also the only known natural communication system whose adaptability may be referred to as \"modality independent\". This means that it can be used not only for communication through one channel or medium, but through several. For example, spoken language uses the auditive modality, whereas sign languages and writing use the visual modality, and braille writing uses the tactile modality.\n\nHuman language is also unique in being able to refer to abstract concepts and to imagined or hypothetical events as well as events that took place in the past or may happen in the future. This ability to refer to events that are not at the same time or place as the speech event is called \"displacement\", and while some animal communication systems can use displacement (such as the communication of bees that can communicate the location of sources of nectar that are out of sight), the degree to which it is used in human language is also considered unique.\n\nTheories about the origin of language differ in regard to their basic assumptions about what language is. Some theories are based on the idea that language is so complex that one cannot imagine it simply appearing from nothing in its final form, but that it must have evolved from earlier pre-linguistic systems among our pre-human ancestors. These theories can be called continuity-based theories. The opposite viewpoint is that language is such a unique human trait that it cannot be compared to anything found among non-humans and that it must therefore have appeared suddenly in the transition from pre-hominids to early man. These theories can be defined as discontinuity-based. Similarly, theories based on the generative view of language pioneered by Noam Chomsky see language mostly as an innate faculty that is largely genetically encoded, whereas functionalist theories see it as a system that is largely cultural, learned through social interaction.\n\nChomsky is one prominent proponent of a discontinuity-based theory of human language origins. He suggests that for scholars interested in the nature of language, \"talk about the evolution of the language capacity is beside the point.\" Chomsky proposes that perhaps \"some random mutation took place [...] and it reorganized the brain, implanting a language organ in an otherwise primate brain.\" Though cautioning against taking this story literally, Chomsky insists that \"it may be closer to reality than many other fairy tales that are told about evolutionary processes, including language.\"\n\nContinuity-based theories are held by a majority of scholars, but they vary in how they envision this development. Those who see language as being mostly innate, for example psychologist Steven Pinker, hold the precedents to be animal cognition, whereas those who see language as a socially learned tool of communication, such as psychologist Michael Tomasello, see it as having developed from animal communication in primates: either gestural or vocal communication to assist in cooperation. Other continuity-based models see language as having developed from music, a view already espoused by Rousseau, Herder, Humboldt, and Charles Darwin. A prominent proponent of this view is archaeologist Steven Mithen. Stephen Anderson states that the age of spoken languages is estimated at 60,000 to 100,000 years and that: Researchers on the evolutionary origin of language generally find it plausible to suggest that language was invented only once, and that all modern spoken languages are thus in some way related, even if that relation can no longer be recovered ... because of limitations on the methods available for reconstruction.\n\nBecause language emerged in the early prehistory of man, before the existence of any written records, its early development has left no historical traces, and it is believed that no comparable processes can be observed today. Theories that stress continuity often look at animals to see if, for example, primates display any traits that can be seen as analogous to what pre-human language must have been like. And early human fossils can be inspected for traces of physical adaptation to language use or pre-linguistic forms of symbolic behaviour. Among the signs in human fossils that may suggest linguistic abilities are: the size of the brain relative to body mass, the presence of a larynx capable of advanced sound production and the nature of tools and other manufactured artifacts.\n\nIt was mostly undisputed that pre-human australopithecines did not have communication systems significantly different from those found in great apes in general. However, a 2017 study on Ardipithecus ramidus challenges this belief. Scholarly opinions vary as to the developments since the appearance of the genus \"Homo\" some 2.5 million years ago. Some scholars assume the development of primitive language-like systems (proto-language) as early as \"Homo habilis\" (2.3 million years ago) while others place the development of primitive symbolic communication only with \"Homo erectus\" (1.8 million years ago) or \"Homo heidelbergensis\" (0.6 million years ago), and the development of language proper with Anatomically Modern \"Homo sapiens\" with the Upper Paleolithic revolution less than 100,000 years ago.\n\nThe study of language, linguistics, has been developing into a science since the first grammatical descriptions of particular languages in India more than 2000 years ago, after the development of the Brahmi script. Modern linguistics is a science that concerns itself with all aspects of language, examining it from all of the theoretical viewpoints described above.\n\nThe academic study of language is conducted within many different disciplinary areas and from different theoretical angles, all of which inform modern approaches to linguistics. For example, descriptive linguistics examines the grammar of single languages, theoretical linguistics develops theories on how best to conceptualize and define the nature of language based on data from the various extant human languages, sociolinguistics studies how languages are used for social purposes informing in turn the study of the social functions of language and grammatical description, neurolinguistics studies how language is processed in the human brain and allows the experimental testing of theories, computational linguistics builds on theoretical and descriptive linguistics to construct computational models of language often aimed at processing natural language or at testing linguistic hypotheses, and historical linguistics relies on grammatical and lexical descriptions of languages to trace their individual histories and reconstruct trees of language families by using the comparative method.\n\nThe formal study of language is often considered to have started in India with Pāṇini, the 5th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. However, Sumerian scribes already studied the differences between Sumerian and Akkadian grammar around 1900 BC. Subsequent grammatical traditions developed in all of the ancient cultures that adopted writing.\n\nIn the 17th century AD, the French Port-Royal Grammarians developed the idea that the grammars of all languages were a reflection of the universal basics of thought, and therefore that grammar was universal. In the 18th century, the first use of the comparative method by British philologist and expert on ancient India William Jones sparked the rise of comparative linguistics. The scientific study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt. Early in the 20th century, Ferdinand de Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them.\n\nBy introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still fundamental in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the Langue-parole distinction, distinguishing language as an abstract system (\"langue\"), from language as a concrete manifestation of this system (\"parole\").\n\nIn the 1960s, Noam Chomsky formulated the generative theory of language. According to this theory, the most basic form of language is a set of syntactic rules that is universal for all humans and which underlies the grammars of all human languages. This set of rules is called Universal Grammar; for Chomsky, describing it is the primary objective of the discipline of linguistics. Thus, he considered that the grammars of individual languages are only of importance to linguistics insofar as they allow us to deduce the universal underlying rules from which the observable linguistic variability is generated.\n\nIn opposition to the formal theories of the generative school, functional theories of language propose that since language is fundamentally a tool, its structures are best analyzed and understood by reference to their functions. Formal theories of grammar seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, while functional theories seek to define the functions performed by language and then relate them to the linguistic elements that carry them out. The framework of cognitive linguistics interprets language in terms of the concepts (which are sometimes universal, and sometimes specific to a particular language) which underlie its forms. Cognitive linguistics is primarily concerned with how the mind creates meaning through language.\n\nSpeaking is the default modality for language in all cultures. The production of spoken language depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus, the ability to acoustically decode speech sounds, and the neurological apparatus required for acquiring and producing language. The study of the genetic bases for human language is at an early stage: the only gene that has definitely been implicated in language production is FOXP2, which may cause a kind of congenital language disorder if affected by mutations.\n\n The brain is the coordinating center of all linguistic activity; it controls both the production of linguistic cognition and of meaning and the mechanics of speech production. Nonetheless, our knowledge of the neurological bases for language is quite limited, though it has advanced considerably with the use of modern imaging techniques. The discipline of linguistics dedicated to studying the neurological aspects of language is called neurolinguistics.\n\nEarly work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with fluency, articulation, word-finding, word repetition, and producing and comprehending complex grammatical sentences, both orally and in writing. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.\n\nWith technological advances in the late 20th century, neurolinguists have also incorporated non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electrophysiology to study language processing in individuals without impairments.\n\nSpoken language relies on human physical ability to produce sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract – the throat, the mouth, and the nose. By controlling the different parts of the speech apparatus, the airstream can be manipulated to produce different speech sounds.\n\nThe sound of speech can be analyzed into a combination of segmental and suprasegmental elements. The segmental elements are those that follow each other in sequences, which are usually represented by distinct letters in alphabetic scripts, such as the Roman script. In free flowing speech, there are no clear boundaries between one segment and the next, nor usually are there any audible pauses between words. Segments therefore are distinguished by their distinct sounds which are a result of their different articulations, and they can be either vowels or consonants. Suprasegmental phenomena encompass such elements as stress, phonation type, voice timbre, and prosody or intonation, all of which may have effects across multiple segments.\n\nConsonants and vowel segments combine to form syllables, which in turn combine to form utterances; these can be distinguished phonetically as the space between two inhalations. Acoustically, these different segments are characterized by different formant structures, that are visible in a spectrogram of the recorded sound wave (See illustration of Spectrogram of the formant structures of three English vowels). Formants are the amplitude peaks in the frequency spectrum of a specific sound.\n\nVowels are those sounds that have no audible friction caused by the narrowing or obstruction of some part of the upper vocal tract. They vary in quality according to the degree of lip aperture and the placement of the tongue within the oral cavity. Vowels are called \"close\" when the lips are relatively closed, as in the pronunciation of the vowel (English \"ee\"), or \"open\" when the lips are relatively open, as in the vowel (English \"ah\"). If the tongue is located towards the back of the mouth, the quality changes, creating vowels such as (English \"oo\"). The quality also changes depending on whether the lips are rounded as opposed to unrounded, creating distinctions such as that between (unrounded front vowel such as English \"ee\") and (rounded front vowel such as German \"ü\").\n\nConsonants are those sounds that have audible friction or closure at some point within the upper vocal tract. Consonant sounds vary by place of articulation, i.e. the place in the vocal tract where the airflow is obstructed, commonly at the lips, teeth, alveolar ridge, palate, velum, uvula, or glottis. Each place of articulation produces a different set of consonant sounds, which are further distinguished by manner of articulation, or the kind of friction, whether full closure, in which case the consonant is called \"occlusive\" or \"stop\", or different degrees of aperture creating \"fricatives\" and \"approximants\". Consonants can also be either \"voiced or unvoiced\", depending on whether the vocal cords are set in vibration by airflow during the production of the sound. Voicing is what separates English in \"bus\" (unvoiced sibilant) from in \"buzz\" (voiced sibilant).\n\nSome speech sounds, both vowels and consonants, involve release of air flow through the nasal cavity, and these are called \"nasals\" or \"nasalized\" sounds. Other sounds are defined by the way the tongue moves within the mouth: such as the l-sounds (called \"laterals\", because the air flows along both sides of the tongue), and the r-sounds (called \"rhotics\") that are characterized by how the tongue is positioned relative to the air stream.\n\nBy using these speech organs, humans can produce hundreds of distinct sounds: some appear very often in the world's languages, whereas others are much more common in certain language families, language areas, or even specific to a single language.\n\nWhen described as a system of symbolic communication, language is traditionally seen as consisting of three parts: signs, meanings, and a code connecting signs with their meanings. The study of the process of semiosis, how signs and meanings are combined, used, and interpreted is called semiotics. Signs can be composed of sounds, gestures, letters, or symbols, depending on whether the language is spoken, signed, or written, and they can be combined into complex signs, such as words and phrases. When used in communication, a sign is encoded and transmitted by a sender through a channel to a receiver who decodes it.\nSome of the properties that define human language as opposed to other communication systems are: the arbitrariness of the linguistic sign, meaning that there is no predictable connection between a linguistic sign and its meaning; the duality of the linguistic system, meaning that linguistic structures are built by combining elements into larger structures that can be seen as layered, e.g. how sounds build words and words build phrases; the discreteness of the elements of language, meaning that the elements out of which linguistic signs are constructed are discrete units, e.g. sounds and words, that can be distinguished from each other and rearranged in different patterns; and the productivity of the linguistic system, meaning that the finite number of linguistic elements can be combined into a theoretically infinite number of combinations.\n\nThe rules by which signs can be combined to form words and phrases are called syntax or grammar. The meaning that is connected to individual signs, morphemes, words, phrases, and texts is called semantics. The division of language into separate but connected systems of sign and meaning goes back to the first linguistic studies of de Saussure and is now used in almost all branches of linguistics.\n\nLanguages express meaning by relating a sign form to a meaning, or its content. Sign forms must be something that can be perceived, for example, in sounds, images, or gestures, and then related to a specific meaning by social convention. Because the basic relation of meaning for most linguistic signs is based on social convention, linguistic signs can be considered arbitrary, in the sense that the convention is established socially and historically, rather than by means of a natural relation between a specific sign form and its meaning.\n\nThus, languages must have a vocabulary of signs related to specific meaning. The English sign \"dog\" denotes, for example, a member of the species \"Canis familiaris\". In a language, the array of arbitrary signs connected to specific meanings is called the lexicon, and a single sign connected to a meaning is called a lexeme. Not all meanings in a language are represented by single words. Often, semantic concepts are embedded in the morphology or syntax of the language in the form of grammatical categories.\n\nAll languages contain the semantic structure of predication: a structure that predicates a property, state, or action. Traditionally, semantics has been understood to be the study of how speakers and interpreters assign truth values to statements, so that meaning is understood to be the process by which a predicate can be said to be true or false about an entity, e.g. \"<nowiki>[x [is y]]\" or \"[x [does y]]</nowiki>\". Recently, this model of semantics has been complemented with more dynamic models of meaning that incorporate shared knowledge about the context in which a sign is interpreted into the production of meaning. Such models of meaning are explored in the field of pragmatics.\n\nDepending on modality, language structure can be based on systems of sounds (speech), gestures (sign languages), or graphic or tactile symbols (writing). The ways in which languages use sounds or signs to construct meaning are studied in phonology. The study of how humans produce and perceive vocal sounds is called phonetics. In spoken language, meaning is produced when sounds become part of a system in which some sounds can contribute to expressing meaning and others do not. In any given language, only a limited number of the many distinct sounds that can be created by the human vocal apparatus contribute to constructing meaning.\n\nSounds as part of a linguistic system are called phonemes. Phonemes are abstract units of sound, defined as the smallest units in a language that can serve to distinguish between the meaning of a pair of minimally different words, a so-called minimal pair. In English, for example, the words \"bat\" and \"pat\" form a minimal pair, in which the distinction between and differentiates the two words, which have different meanings. However, each language contrasts sounds in different ways. For example, in a language that does not distinguish between voiced and unvoiced consonants, the sounds and (if they both occur) could be considered a single phoneme, and consequently, the two pronunciations would have the same meaning. Similarly, the English language does not distinguish phonemically between aspirated and non-aspirated pronunciations of consonants, as many other languages like Korean and Hindi do: the unaspirated in \"spin\" and the aspirated in \"pin\" are considered to be merely different ways of pronouncing the same phoneme (such variants of a single phoneme are called allophones), whereas in Mandarin Chinese, the same difference in pronunciation distinguishes between the words 'crouch' and 'eight' (the accent above the á means that the vowel is pronounced with a high tone).\n\nAll spoken languages have phonemes of at least two different categories, vowels and consonants, that can be combined to form syllables. As well as segments such as consonants and vowels, some languages also use sound in other ways to convey meaning. Many languages, for example, use stress, pitch, duration, and tone to distinguish meaning. Because these phenomena operate outside of the level of single segments, they are called suprasegmental. Some languages have only a few phonemes, for example, Rotokas and Pirahã language with 11 and 10 phonemes respectively, whereas languages like Taa may have as many as 141 phonemes. In sign languages, the equivalent to phonemes (formerly called cheremes) are defined by the basic elements of gestures, such as hand shape, orientation, location, and motion, which correspond to manners of articulation in spoken language.\n\nWriting systems represent language using visual symbols, which may or may not correspond to the sounds of spoken language. The Latin alphabet (and those on which it is based or that have been derived from it) was originally based on the representation of single sounds, so that words were constructed from letters that generally denote a single consonant or vowel in the structure of the word. In syllabic scripts, such as the Inuktitut syllabary, each sign represents a whole syllable. In logographic scripts, each sign represents an entire word, and will generally bear no relation to the sound of that word in spoken language.\n\nBecause all languages have a very large number of words, no purely logographic scripts are known to exist. Written language represents the way spoken sounds and words follow one after another by arranging symbols according to a pattern that follows a certain direction. The direction used in a writing system is entirely arbitrary and established by convention. Some writing systems use the horizontal axis (left to right as the Latin script or right to left as the Arabic script), while others such as traditional Chinese writing use the vertical dimension (from top to bottom). A few writing systems use opposite directions for alternating lines, and others, such as the ancient Maya script, can be written in either direction and rely on graphic cues to show the reader the direction of reading.\n\nIn order to represent the sounds of the world's languages in writing, linguists have developed the International Phonetic Alphabet, designed to represent all of the discrete sounds that are known to contribute to meaning in human languages.\n\nGrammar is the study of how meaningful elements called \"morphemes\" within a language can be combined into utterances. Morphemes can either be \"free\" or \"bound\". If they are free to be moved around within an utterance, they are usually called \"words\", and if they are bound to other words or morphemes, they are called affixes. The way in which meaningful elements can be combined within a language is governed by rules. The rules for the internal structure of words are called morphology. The rules of the internal structure of phrases and sentences are called \"syntax\".\n\nGrammar can be described as a system of categories and a set of rules that determine how categories combine to form different aspects of meaning. Languages differ widely in whether they are encoded through the use of categories or lexical units. However, several categories are so common as to be nearly universal. Such universal categories include the encoding of the grammatical relations of participants and predicates by grammatically distinguishing between their relations to a predicate, the encoding of temporal and spatial relations on predicates, and a system of grammatical person governing reference to and distinction between speakers and addressees and those about whom they are speaking.\n\nLanguages organize their parts of speech into classes according to their functions and positions relative to other parts. All languages, for instance, make a basic distinction between a group of words that prototypically denotes things and concepts and a group of words that prototypically denotes actions and events. The first group, which includes English words such as \"dog\" and \"song\", are usually called nouns. The second, which includes \"run\" and \"sing\", are called verbs. Another common category is the adjective: words that describe properties or qualities of nouns, such as \"red\" or \"big\". Word classes can be \"open\" if new words can continuously be added to the class, or relatively \"closed\" if there is a fixed number of words in a class. In English, the class of pronouns is closed, whereas the class of adjectives is open, since an infinite number of adjectives can be constructed from verbs (e.g. \"saddened\") or nouns (e.g. with the -like suffix, as in \"noun-like\"). In other languages such as Korean, the situation is the opposite, and new pronouns can be constructed, whereas the number of adjectives is fixed.\n\nWord classes also carry out differing functions in grammar. Prototypically, verbs are used to construct predicates, while nouns are used as arguments of predicates. In a sentence such as \"Sally runs\", the predicate is \"runs\", because it is the word that predicates a specific state about its argument \"Sally\". Some verbs such as \"curse\" can take two arguments, e.g. \"Sally cursed John\". A predicate that can only take a single argument is called \"intransitive\", while a predicate that can take two arguments is called \"transitive\".\n\nMany other word classes exist in different languages, such as conjunctions like \"and\" that serve to join two sentences, articles that introduce a noun, interjections such as \"wow!\", or ideophones like \"splash\" that mimic the sound of some event. Some languages have positionals that describe the spatial position of an event or entity. Many languages have classifiers that identify countable nouns as belonging to a particular type or having a particular shape. For instance, in Japanese, the general noun classifier for humans is \"nin\" (人), and it is used for counting humans, whatever they are called:\n\nFor trees, it would be:\n\nIn linguistics, the study of the internal structure of complex words and the processes by which words are formed is called morphology. In most languages, it is possible to construct complex words that are built of several morphemes. For instance, the English word \"unexpected\" can be analyzed as being composed of the three morphemes \"un-\", \"expect\" and \"-ed\".\n\nMorphemes can be classified according to whether they are independent morphemes, so-called roots, or whether they can only co-occur attached to other morphemes. These bound morphemes or affixes can be classified according to their position in relation to the root: \"prefixes\" precede the root, suffixes follow the root, and infixes are inserted in the middle of a root. Affixes serve to modify or elaborate the meaning of the root. Some languages change the meaning of words by changing the phonological structure of a word, for example, the English word \"run\", which in the past tense is \"ran\". This process is called \"ablaut\". Furthermore, morphology distinguishes between the process of inflection, which modifies or elaborates on a word, and the process of derivation, which creates a new word from an existing one. In English, the verb \"sing\" has the inflectional forms \"singing\" and \"sung\", which are both verbs, and the derivational form \"singer\", which is a noun derived from the verb with the agentive suffix \"-er\".\n\nLanguages differ widely in how much they rely on morphological processes of word formation. In some languages, for example, Chinese, there are no morphological processes, and all grammatical information is encoded syntactically by forming strings of single words. This type of morpho-syntax is often called isolating, or analytic, because there is almost a full correspondence between a single word and a single aspect of meaning. Most languages have words consisting of several morphemes, but they vary in the degree to which morphemes are discrete units. In many languages, notably in most Indo-European languages, single morphemes may have several distinct meanings that cannot be analyzed into smaller segments. For example, in Latin, the word \"bonus\", or \"good\", consists of the root \"bon-\", meaning \"good\", and the suffix -\"us\", which indicates masculine gender, singular number, and nominative case. These languages are called \"fusional languages\", because several meanings may be fused into a single morpheme. The opposite of fusional languages are agglutinative languages which construct words by stringing morphemes together in chains, but with each morpheme as a discrete semantic unit. An example of such a language is Turkish, where for example, the word \"evlerinizden\", or \"from your houses\", consists of the morphemes, \"ev-ler-iniz-den\" with the meanings \"house-plural-your-from\". The languages that rely on morphology to the greatest extent are traditionally called polysynthetic languages. They may express the equivalent of an entire English sentence in a single word. For example, in Persian the single word \"nafahmidamesh\" means \"I didn't understand it\" consisting of morphemes \"na-fahm-id-am-esh\" with the meanings, \"negation.understand.past.I.it\". As another example with more complexity, in the Yupik word \"tuntussuqatarniksatengqiggtuq\", which means \"He had not yet said again that he was going to hunt reindeer\", the word consists of the morphemes \"tuntu-ssur-qatar-ni-ksaite-ngqiggte-uq\" with the meanings, \"reindeer-hunt-future-say-negation-again-third.person.singular.indicative\", and except for the morpheme \"tuntu\" (\"reindeer\") none of the other morphemes can appear in isolation.\n\nMany languages use morphology to cross-reference words within a sentence. This is sometimes called \"agreement\". For example, in many Indo-European languages, adjectives must cross-reference the noun they modify in terms of number, case, and gender, so that the Latin adjective \"bonus\", or \"good\", is inflected to agree with a noun that is masculine gender, singular number, and nominative case. In many polysynthetic languages, verbs cross-reference their subjects and objects. In these types of languages, a single verb may include information that would require an entire sentence in English. For example, in the Basque phrase \"ikusi nauzu\", or \"you saw me\", the past tense auxiliary verb \"n-au-zu\" (similar to English \"do\") agrees with both the subject (you) expressed by the \"n\"- prefix, and with the object (me) expressed by the – \"zu\" suffix. The sentence could be directly transliterated as \"see you-did-me\"\n\nAnother way in which languages convey meaning is through the order of words within a sentence. The grammatical rules for how to produce new sentences from words that are already known is called syntax. The syntactical rules of a language determine why a sentence in English such as \"I love you\" is meaningful, but \"*love you I\" is not. Syntactical rules determine how word order and sentence structure is constrained, and how those constraints contribute to meaning. For example, in English, the two sentences \"the slaves were cursing the master\" and \"the master was cursing the slaves\" mean different things, because the role of the grammatical subject is encoded by the noun being in front of the verb, and the role of object is encoded by the noun appearing after the verb. Conversely, in Latin, both \"Dominus servos vituperabat\" and \"Servos vituperabat dominus\" mean \"the master was reprimanding the slaves\", because \"servos\", or \"slaves\", is in the accusative case, showing that they are the grammatical object of the sentence, and \"dominus\", or \"master\", is in the nominative case, showing that he is the subject.\n\nLatin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word order in questions, which exists in many languages. This rule explains why when in English, the phrase \"John is talking to Lucy\" is turned into a question, it becomes \"Who is John talking to?\", and not \"John is talking to who?\". The latter example may be used as a way of placing special emphasis on \"who\", thereby slightly altering the meaning of the question. Syntax also includes the rules for how complex sentences are structured by grouping words together in units, called phrases, that can occupy different places in a larger syntactic structure. Sentences can be described as consisting of phrases connected in a tree structure, connecting the phrases to each other at different levels. To the right is a graphic representation of the syntactic analysis of the English sentence \"the cat sat on the mat\". The sentence is analyzed as being constituted by a noun phrase, a verb, and a prepositional phrase; the prepositional phrase is further divided into a preposition and a noun phrase, and the noun phrases consist of an article and a noun.\n\nThe reason sentences can be seen as being composed of phrases is because each phrase would be moved around as a single element if syntactic operations were carried out. For example, \"the cat\" is one phrase, and \"on the mat\" is another, because they would be treated as single units if a decision was made to emphasize the location by moving forward the prepositional phrase: \"[And] on the mat, the cat sat\". There are many different formalist and functionalist frameworks that propose theories for describing syntactic structures, based on different assumptions about what language is and how it should be described. Each of them would analyze a sentence such as this in a different manner.\n\nLanguages can be classified in relation to their grammatical types. Languages that belong to different families nonetheless often have features in common, and these shared features tend to correlate. For example, languages can be classified on the basis of their basic word order, the relative order of the verb, and its constituents in a normal indicative sentence. In English, the basic order is SVO: \"The snake(S) bit(V) the man(O)\", whereas for example, the corresponding sentence in the Australian language Gamilaraay would be \"d̪uyugu n̪ama d̪ayn yiːy\" (snake man bit), SOV. Word order type is relevant as a typological parameter, because basic word order type corresponds with other syntactic parameters, such as the relative order of nouns and adjectives, or of the use of prepositions or postpositions. Such correlations are called implicational universals. For example, most (but not all) languages that are of the SOV type have postpositions rather than prepositions, and have adjectives before nouns.\n\nAll languages structure sentences into Subject, Verb, and Object, but languages differ in the way they classify the relations between actors and actions. English uses the nominative-accusative word typology: in English transitive clauses, the subjects of both intransitive sentences (\"I run\") and transitive sentences (\"I love you\") are treated in the same way, shown here by the nominative pronoun \"I\". Some languages, called ergative, Gamilaraay among them, distinguish instead between Agents and Patients. In ergative languages, the single participant in an intransitive sentence, such as \"I run\", is treated the same as the patient in a transitive sentence, giving the equivalent of \"me run\". Only in transitive sentences would the equivalent of the pronoun \"I\" be used. In this way the semantic roles can map onto the grammatical relations in different ways, grouping an intransitive subject either with Agents (accusative type) or Patients (ergative type) or even making each of the three roles differently, which is called the tripartite type.\n\nThe shared features of languages which belong to the same typological class type may have arisen completely independently. Their co-occurrence might be due to universal laws governing the structure of natural languages, \"language universals\", or they might be the result of languages evolving convergent solutions to the recurring communicative problems that humans use language to solve.\n\nWhile humans have the ability to learn any language, they only do so if they grow up in an environment in which language exists and is used by others. Language is therefore dependent on communities of speakers in which children learn language from their elders and peers and themselves transmit language to their own children. Languages are used by those who speak them to communicate and to solve a plethora of social tasks. Many aspects of language use can be seen to be adapted specifically to these purposes. Due to the way in which language is transmitted between generations and within communities, language perpetually changes, diversifying into new languages or converging due to language contact. The process is similar to the process of evolution, where the process of descent with modification leads to the formation of a phylogenetic tree.\n\nHowever, languages differ from biological organisms in that they readily incorporate elements from other languages through the process of diffusion, as speakers of different languages come into contact. Humans also frequently speak more than one language, acquiring their first language or languages as children, or learning new languages as they grow up. Because of the increased language contact in the globalizing world, many small languages are becoming endangered as their speakers shift to other languages that afford the possibility to participate in larger and more influential speech communities.\n\nThe semantic study of meaning assumes that meaning is in a relation between signs and meanings that are firmly established through social convention. However, semantics does not study the way in which social conventions are made and affect language. Rather, when studying the way in which words and signs are used, it is often the case that words have different meanings, depending on the social context of use. An important example of this is the process called deixis, which describes the way in which certain words refer to entities through their relation between a specific point in time and space when the word is uttered. Such words are, for example, the word, \"I\" (which designates the person speaking), \"now\" (which designates the moment of speaking), and \"here\" (which designates the position of speaking). Signs also change their meanings over time, as the conventions governing their usage gradually change. The study of how the meaning of linguistic expressions changes depending on context is called pragmatics. Deixis is an important part of the way that we use language to point out entities in the world. Pragmatics is concerned with the ways in which language use is patterned and how these patterns contribute to meaning. For example, in all languages, linguistic expressions can be used not just to transmit information, but to perform actions. Certain actions are made only through language, but nonetheless have tangible effects, e.g. the act of \"naming\", which creates a new name for some entity, or the act of \"pronouncing someone man and wife\", which creates a social contract of marriage. These types of acts are called speech acts, although they can also be carried out through writing or hand signing.\n\nThe form of linguistic expression often does not correspond to the meaning that it actually has in a social context. For example, if at a dinner table a person asks, \"Can you reach the salt?\", that is, in fact, not a question about the length of the arms of the one being addressed, but a request to pass the salt across the table. This meaning is implied by the context in which it is spoken; these kinds of effects of meaning are called conversational implicatures. These social rules for which ways of using language are considered appropriate in certain situations and how utterances are to be understood in relation to their context vary between communities, and learning them is a large part of acquiring communicative competence in a language.\n\nAll healthy, normally developing human beings learn to use language. Children acquire the language or languages used around them: whichever languages they receive sufficient exposure to during childhood. The development is essentially the same for children acquiring sign or oral languages. This learning process is referred to as first-language acquisition, since unlike many other kinds of learning, it requires no direct teaching or specialized study. In \"The Descent of Man\", naturalist Charles Darwin called this process \"an instinctive tendency to acquire an art\".\n\nFirst language acquisition proceeds in a fairly regular sequence, though there is a wide degree of variation in the timing of particular stages among normally developing infants. From birth, newborns respond more readily to human speech than to other sounds. Around one month of age, babies appear to be able to distinguish between different speech sounds. Around six months of age, a child will begin babbling, producing the speech sounds or handshapes of the languages used around them. Words appear around the age of 12 to 18 months; the average vocabulary of an eighteen-month-old child is around 50 words. A child's first utterances are holophrases (literally \"whole-sentences\"), utterances that use just one word to communicate some idea. Several months after a child begins producing words, he or she will produce two-word utterances, and within a few more months will begin to produce telegraphic speech, or short sentences that are less grammatically complex than adult speech, but that do show regular syntactic structure. From roughly the age of three to five years, a child's ability to speak or sign is refined to the point that it resembles adult language. Studies published in 2013 have indicated that unborn fetuses are capable of language acquisition to some degree.\n\nAcquisition of second and additional languages can come at any age, through exposure in daily life or courses. Children learning a second language are more likely to achieve native-like fluency than adults, but in general, it is very rare for someone speaking a second language to pass completely for a native speaker. An important difference between first language acquisition and additional language acquisition is that the process of additional language acquisition is influenced by languages that the learner already knows.\n\nLanguages, understood as the particular set of speech norms of a particular community, are also a part of the larger culture of the community that speaks them. Languages differ not only in pronunciation, vocabulary, and grammar, but also through having different \"cultures of speaking.\" Humans use language as a way of signalling identity with one cultural group as well as difference from others. Even among speakers of one language, several different ways of using the language exist, and each is used to signal affiliation with particular subgroups within a larger culture. Linguists and anthropologists, particularly sociolinguists, ethnolinguists, and linguistic anthropologists have specialized in studying how ways of speaking vary between speech communities.\n\nLinguists use the term \"varieties\" to refer to the different ways of speaking a language. This term includes geographically or socioculturally defined dialects as well as the jargons or styles of subcultures. Linguistic anthropologists and sociologists of language define communicative style as the ways that language is used and understood within a particular culture.\n\nBecause norms for language use are shared by members of a specific group, communicative style also becomes a way of displaying and constructing group identity. Linguistic differences may become salient markers of divisions between social groups, for example, speaking a language with a particular accent may imply membership of an ethnic minority or social class, one's area of origin, or status as a second language speaker. These kinds of differences are not part of the linguistic system, but are an important part of how people use language as a social tool for constructing groups.\n\nHowever, many languages also have grammatical conventions that signal the social position of the speaker in relation to others through the use of registers that are related to social hierarchies or divisions. In many languages, there are stylistic or even grammatical differences between the ways men and women speak, between age groups, or between social classes, just as some languages employ different words depending on who is listening. For example, in the Australian language Dyirbal, a married man must use a special set of words to refer to everyday items when speaking in the presence of his mother-in-law. Some cultures, for example, have elaborate systems of \"social deixis\", or systems of signalling social distance through linguistic means. In English, social deixis is shown mostly through distinguishing between addressing some people by first name and others by surname, and in titles such as \"Mrs.\", \"boy\", \"Doctor\", or \"Your Honor\", but in other languages, such systems may be highly complex and codified in the entire grammar and vocabulary of the language. For instance, in languages of east Asia such as Thai, Burmese, and Javanese, different words are used according to whether a speaker is addressing someone of higher or lower rank than oneself in a ranking system with animals and children ranking the lowest and gods and members of royalty as the highest.\n\nThroughout history a number of different ways of representing language in graphic media have been invented. These are called writing systems.\n\nThe use of writing has made language even more useful to humans. It makes it possible to store large amounts of information outside of the human body and retrieve it again, and it allows communication across distances that would otherwise be impossible. Many languages conventionally employ different genres, styles, and registers in written and spoken language, and in some communities, writing traditionally takes place in an entirely different language than the one spoken. There is some evidence that the use of writing also has effects on the cognitive development of humans, perhaps because acquiring literacy generally requires explicit and formal education.\n\nThe invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age in the late 4th millennium BC. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered to be the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 3400–3200 BC with the earliest coherent texts from about 2600 BC. It is generally agreed that Sumerian writing was an independent invention; however, it is debated whether Egyptian writing was developed completely independently of Sumerian, or was a case of cultural diffusion. A similar debate exists for the Chinese script, which developed around 1200 BC. The pre-Columbian Mesoamerican writing systems (including among others Olmec and Maya scripts) are generally believed to have had independent origins.\n\nAll languages change as speakers adopt or invent new ways of speaking and pass them on to other members of their speech community. Language change happens at all levels from the phonological level to the levels of vocabulary, morphology, syntax, and discourse. Even though language change is often initially evaluated negatively by speakers of the language who often consider changes to be \"decay\" or a sign of slipping norms of language usage, it is natural and inevitable.\n\nChanges may affect specific sounds or the entire phonological system. Sound change can consist of the replacement of one speech sound or phonetic feature by another, the complete loss of the affected sound, or even the introduction of a new sound in a place where there had been none. Sound changes can be \"conditioned\" in which case a sound is changed only if it occurs in the vicinity of certain other sounds. Sound change is usually assumed to be \"regular\", which means that it is expected to apply mechanically whenever its structural conditions are met, irrespective of any non-phonological factors. On the other hand, sound changes can sometimes be \"sporadic\", affecting only one particular word or a few words, without any seeming regularity. Sometimes a simple change triggers a chain shift in which the entire phonological system is affected. This happened in the Germanic languages when the sound change known as Grimm's law affected all the stop consonants in the system. The original consonant * became /b/ in the Germanic languages, the previous * in turn became /p/, and the previous * became /f/. The same process applied to all stop consonants and explains why Italic languages such as Latin have \"p\" in words like pater\" and pisces\", whereas Germanic languages, like English, have father\" and fish\".\n\nAnother example is the Great Vowel Shift in English, which is the reason that the spelling of English vowels do not correspond well to their current pronunciation. This is because the vowel shift brought the already established orthography out of synchronization with pronunciation. Another source of sound change is the erosion of words as pronunciation gradually becomes increasingly indistinct and shortens words, leaving out syllables or sounds. This kind of change caused Latin \"mea domina\" to eventually become the French \"madame\" and American English \"ma'am\".\n\nChange also happens in the grammar of languages as discourse patterns such as idioms or particular constructions become grammaticalized. This frequently happens when words or morphemes erode and the grammatical system is unconsciously rearranged to compensate for the lost element. For example, in some varieties of Caribbean Spanish the final /s/ has eroded away. Since Standard Spanish uses final /s/ in the morpheme marking the second person subject \"you\" in verbs, the Caribbean varieties now have to express the second person using the pronoun \"tú\". This means that the sentence \"what's your name\" is \"¿como te llamas?\" in Standard Spanish, but in Caribbean Spanish. The simple sound change has affected both morphology and syntax. Another common cause of grammatical change is the gradual petrification of idioms into new grammatical forms, for example, the way the English \"going to\" construction lost its aspect of movement and in some varieties of English has almost become a full-fledged future tense (e.g. \"I'm gonna\").\n\nLanguage change may be motivated by \"language internal\" factors, such as changes in pronunciation motivated by certain sounds being difficult to distinguish aurally or to produce, or through patterns of change that cause some rare types of constructions to drift towards more common types. Other causes of language change are social, such as when certain pronunciations become emblematic of membership in certain groups, such as social classes, or with ideologies, and therefore are adopted by those who wish to identify with those groups or ideas. In this way, issues of identity and politics can have profound effects on language structure.\n\nOne important source of language change is contact and resulting diffusion of linguistic traits between languages. Language contact occurs when speakers of two or more languages or varieties interact on a regular basis. Multilingualism is likely to have been the norm throughout human history and most people in the modern world are multilingual. Before the rise of the concept of the ethno-national state, monolingualism was characteristic mainly of populations inhabiting small islands. But with the ideology that made one people, one state, and one language the most desirable political arrangement, monolingualism started to spread throughout the world. Nonetheless, there are only 250 countries in the world corresponding to some 6000 languages, which means that most countries are multilingual and most languages therefore exist in close contact with other languages.\n\nWhen speakers of different languages interact closely, it is typical for their languages to influence each other. Through sustained language contact over long periods, linguistic traits diffuse between languages, and languages belonging to different families may converge to become more similar. In areas where many languages are in close contact, this may lead to the formation of language areas in which unrelated languages share a number of linguistic features. A number of such language areas have been documented, among them, the Balkan language area, the Mesoamerican language area, and the Ethiopian language area. Also, larger areas such as South Asia, Europe, and Southeast Asia have sometimes been considered language areas, because of widespread diffusion of specific areal features.\n\nLanguage contact may also lead to a variety of other linguistic phenomena, including language convergence, borrowing, and relexification (replacement of much of the native vocabulary with that of another language). In situations of extreme and sustained language contact, it may lead to the formation of new mixed languages that cannot be considered to belong to a single language family. One type of mixed language called pidgins occurs when adult speakers of two different languages interact on a regular basis, but in a situation where neither group learns to speak the language of the other group fluently. In such a case, they will often construct a communication form that has traits of both languages, but which has a simplified grammatical and phonological structure. The language comes to contain mostly the grammatical and phonological categories that exist in both languages. Pidgin languages are defined by not having any native speakers, but only being spoken by people who have another language as their first language. But if a Pidgin language becomes the main language of a speech community, then eventually children will grow up learning the pidgin as their first language. As the generation of child learners grow up, the pidgin will often be seen to change its structure and acquire a greater degree of complexity. This type of language is generally called a creole language. An example of such mixed languages is Tok Pisin, the official language of Papua New-Guinea, which originally arose as a Pidgin based on English and Austronesian languages; others are Kreyòl ayisyen, the French-based creole language spoken in Haiti, and Michif, a mixed language of Canada, based on the Native American language Cree and French.\n\n\"SIL Ethnologue\" defines a \"living language\" as \"one that has at least one speaker for whom it is their first language\". The exact number of known living languages varies from 6,000 to 7,000, depending on the precision of one's definition of \"language\", and in particular, on how one defines the distinction between languages and dialects. As of 2016, \"Ethnologue\" cataloged 7,097 living human languages. The \"Ethnologue\" establishes linguistic groups based on studies of mutual intelligibility, and therefore often includes more categories than more conservative classifications. For example, the Danish language that most scholars consider a single language with several dialects is classified as two distinct languages (Danish and Jutish) by the \"Ethnologue\".\n\nAccording to the \"Ethnologue\", 389 languages (nearly 6%) have more than a million speakers. These languages together account for 94% of the world's population, whereas 94% of the world's languages account for the remaining 6% of the global population. To the right is a table of the world's 10 most spoken languages with population estimates from the \"Ethnologue\" (2009 figures).\n\nThere is no clear distinction between a language and a dialect, notwithstanding a famous aphorism attributed to linguist Max Weinreich that \"a language is a dialect with an army and navy\". For example, national boundaries frequently override linguistic difference in determining whether two linguistic varieties are languages or dialects. Hakka, Cantonese and Mandarin are, for example, often classified as \"dialects\" of Chinese, even though they are more different from each other than Swedish is from Norwegian. Before the Yugoslav civil war, Serbo-Croatian was considered a single language with two dialects, but now Croatian and Serbian are considered different languages and employ different writing systems. In other words, the distinction may hinge on political considerations as much as on cultural differences, distinctive writing systems, or degree of mutual intelligibility.\n\nThe world's languages can be grouped into language families consisting of languages that can be shown to have common ancestry. Linguists recognize many hundreds of language families, although some of them can possibly be grouped into larger units as more evidence becomes available and in-depth studies are carried out. At present, there are also dozens of language isolates: languages that cannot be shown to be related to any other languages in the world. Among them are Basque, spoken in Europe, Zuni of New Mexico, Purépecha of Mexico, Ainu of Japan, Burushaski of Pakistan, and many others.\n\nThe language family of the world that has the most speakers is the Indo-European languages, spoken by 46% of the world's population. This family includes major world languages like English, Spanish, Russian, and Hindustani (Hindi/Urdu). The Indo-European family achieved prevalence first during the Eurasian Migration Period (c. 400–800 AD), and subsequently through the European colonial expansion, which brought the Indo-European languages to a politically and often numerically dominant position in the Americas and much of Africa. The Sino-Tibetan languages are spoken by 20% of the world's population and include many of the languages of East Asia, including Hakka, Mandarin Chinese, Cantonese, and hundreds of smaller languages.\n\nAfrica is home to a large number of language families, the largest of which is the Niger-Congo language family, which includes such languages as Swahili, Shona, and Yoruba. Speakers of the Niger-Congo languages account for 6.9% of the world's population. A similar number of people speak the Afroasiatic languages, which include the populous Semitic languages such as Arabic, Hebrew language, and the languages of the Sahara region, such as the Berber languages and Hausa.\n\nThe Austronesian languages are spoken by 5.5% of the world's population and stretch from Madagascar to maritime Southeast Asia all the way to Oceania. It includes such languages as Malagasy, Māori, Samoan, and many of the indigenous languages of Indonesia and Taiwan. The Austronesian languages are considered to have originated in Taiwan around 3000 BC and spread through the Oceanic region through island-hopping, based on an advanced nautical technology. Other populous language families are the Dravidian languages of South Asia (among them Kannada Tamil and Telugu), the Turkic languages of Central Asia (such as Turkish), the Austroasiatic (among them Khmer), and Tai–Kadai languages of Southeast Asia (including Thai).\n\nThe areas of the world in which there is the greatest linguistic diversity, such as the Americas, Papua New Guinea, West Africa, and South-Asia, contain hundreds of small language families. These areas together account for the majority of the world's languages, though not the majority of speakers. In the Americas, some of the largest language families include the Quechumaran, Arawak, and Tupi-Guarani families of South America, the Uto-Aztecan, Oto-Manguean, and Mayan of Mesoamerica, and the Na-Dene, Iroquoian, and Algonquian language families of North America. In Australia, most indigenous languages belong to the Pama-Nyungan family, whereas New Guinea is home to a large number of small families and isolates, as well as a number of Austronesian languages.\n\nLanguage endangerment occurs when a language is at risk of falling out of use as its speakers die out or shift to speaking another language. Language loss occurs when the language has no more native speakers, and becomes a \"dead language\". If eventually no one speaks the language at all, it becomes an \"extinct language\". While languages have always gone extinct throughout human history, they have been disappearing at an accelerated rate in the 20th and 21st centuries due to the processes of globalization and neo-colonialism, where the economically powerful languages dominate other languages.\n\nThe more commonly spoken languages dominate the less commonly spoken languages, so the less commonly spoken languages eventually disappear from populations. The total number of languages in the world is not known. Estimates vary depending on many factors. The consensus is that there are between 6,000 and 7,000 languages spoken as of 2010, and that between 50–90% of those will have become extinct by the year 2100. The top 20 languages, those spoken by more than 50 million speakers each, are spoken by 50% of the world's population, whereas many of the other languages are spoken by small communities, most of them with less than 10,000 speakers.\n\nThe United Nations Educational, Scientific and Cultural Organization (UNESCO) operates with five levels of language endangerment: \"safe\", \"vulnerable\" (not spoken by children outside the home), \"definitely endangered\" (not spoken by children), \"severely endangered\" (only spoken by the oldest generations), and \"critically endangered\" (spoken by few members of the oldest generation, often semi-speakers). Notwithstanding claims that the world would be better off if most adopted a single common \"lingua franca\", such as English or Esperanto, there is a consensus that the loss of languages harms the cultural diversity of the world. It is a common belief, going back to the biblical narrative of the tower of Babel in the Old Testament, that linguistic diversity causes political conflict, but this is contradicted by the fact that many of the world's major episodes of violence have taken place in situations with low linguistic diversity, such as the Yugoslav and American Civil War, or the genocide of Rwanda, whereas many of the most stable political units have been highly multilingual.\n\nMany projects aim to prevent or slow this loss by revitalizing endangered languages and promoting education and literacy in minority languages. Across the world, many countries have enacted specific legislation to protect and stabilize the language of indigenous speech communities. A minority of linguists have argued that language loss is a natural process that should not be counteracted, and that documenting endangered languages for posterity is sufficient.\n\n"}
{"id": "52354973", "url": "https://en.wikipedia.org/wiki?curid=52354973", "title": "Language Icon", "text": "Language Icon\n\nThe language icon is a specific icon designed for people to select a specific language to use when they face multi-lingual, multi-national websites.\n\nThe Language icon was designed by Onur Mustak Cobanli and his team in 2008 when they planned to build a multi-lingual, multi-national website. After the first language icons were rejected, the design contest was organized by A’ Design Award & Competition. Farhat Datta designed the winning entry, “Turnstile Language Icon.”\n\nIts website says it is under an unnamed \"Creative Commons\" license, but its license is not one of the licenses released by the Creative Commons. Its license is semi-noncommercial, so it is not a Free license and does not provide the all freedoms of the Creative Commons Share-Alike licenses (such as CC-BY-SA).\n\nInstead, the license for the license icon has the following conditions:\n\n\nThe language icon was adopted into the widely-used \"Font Awesome\" icon package in Font Awesome version 4. Font Awesome version 5 abandoned the language icon, replacing it with a completely different symbol for language selection.\n"}
{"id": "2383086", "url": "https://en.wikipedia.org/wiki?curid=2383086", "title": "Language development", "text": "Language development\n\nLanguage development is a process starting early in human life. Infants start without knowing a language, yet by 10 months, babies can distinguish speech sounds and engage in babbling. Some research has shown that the earliest learning begins in utero when the fetus starts to recognize the sounds and speech patterns of its mother's voice and differentiate them from other sounds after birth.\n\nTypically, children develop receptive language abilities before their verbal or expressive language develops. Receptive language is the internal processing and understanding of language. As receptive language continues to increase, expressive language begins to slowly develop.\n\nUsually, productive language is considered to begin with a stage of pre-verbal communication in which infants use gestures and vocalizations to make their intents known to others. According to a general principle of development, new forms then take over old functions, so that children learn words to express the same communicative functions they had already expressed by proverbial means.\n\nLanguage development is thought to proceed by ordinary processes of learning in which children acquire the forms, meanings, and uses of words and utterances from the linguistic input. Children often begin reproducing the words that they are repetitively exposed to. The method in which we develop language skills is universal; however, the major debate is how the rules of syntax are acquired. There are two major approaches to syntactic development, an empiricist account by which children learn all syntactic rules from the linguistic input, and a nativist approach by which some principles of syntax are innate and are transmitted through the human genome.\n\nThe nativist theory, proposed by Noam Chomsky, argues that language is a unique human accomplishment, and can be attributed to either \"millions of years of evolution\" or to \"principles of neural organization that may be even more deeply grounded in physical law\". Chomsky says that all children have what is called an innate language acquisition device (LAD). Theoretically, the LAD is an area of the brain that has a set of universal syntactic rules for all languages. This device provides children with the ability to make sense of knowledge and construct novel sentences with minimal external input and little experience. Chomsky's claim is based upon the view that what children hear—their linguistic input—is insufficient to explain how they come to learn language. He argues that linguistic input from the environment is limited and full of errors. Therefore, nativists assume that it is impossible for children to learn linguistic information solely from their environment. However, because children possess this LAD, they are in fact, able to learn language despite incomplete information from their environment. Their capacity to learn language is also attributed to the theory of universal grammar (UG), which posits that a certain set of structural rules are innate to humans, independent of sensory experience. This view has dominated linguistic theory for over fifty years and remains highly influential, as witnessed by the number of articles in journals and books.\n\nThe empiricist theory suggests, contra Chomsky, that there is enough information in the linguistic input children receive and therefore, there is no need to assume an innate language acquisition device exists (see above). Rather than a LAD evolved specifically for language, empiricists believe that general brain processes are sufficient enough for language acquisition. During this process, it is necessary for the child to actively engage with their environment. For a child to learn language, the parent or caregiver adopts a particular way of appropriately communicating with the child; this is known as child-directed speech (CDS). CDS is used so that children are given the necessary linguistic information needed for their language. Empiricism is a general approach and sometimes goes along with the interactionist approach. Statistical language acquisition, which falls under empiricist theory, suggests that infants acquire language by means of pattern perception.\n\nOther researchers embrace an interactionist perspective, consisting of social-interactionist theories of language development. In such approaches, children learn language in the interactive and communicative context, learning language forms for meaningful moves of communication. These theories focus mainly on the caregiver's attitudes and attentiveness to their children in order to promote productive language habits.\n\nAn older empiricist theory, the behaviorist theory proposed by B. F. Skinner suggested that language is learned through operant conditioning, namely, by imitation of stimuli and by reinforcement of correct responses. This perspective has not been widely accepted at any time, but by some accounts, is experiencing a resurgence. New studies use this theory now to treat individuals diagnosed with autism spectrum disorders. Additionally, Relational Frame Theory is growing from the behaviorist theory, which is important for Acceptance and Commitment Therapy. Some empiricist theory accounts today use behaviorist models.\n\nOther relevant theories about language development include Piaget's theory of cognitive development, which considers the development of language as a continuation of general cognitive development and Vygotsky's social theories that attribute the development of language to an individual's social interactions and growth.\n\nEvolutionary biologists are skeptical of the claim that syntactic knowledge is transmitted in the human genome. However, many researchers claim that the ability to acquire such a complicated system is unique to the human species. Non-biologists also tend to believe that our ability to learn spoken language may have been developed through the evolutionary process and that the foundation for language may be passed down genetically. The ability to speak and understand human language requires speech production skills and abilities as well as multisensory integration of sensory processing abilities.\n\nOne hotly debated issue is whether the biological contribution includes capacities specific to language acquisition, often referred to as universal grammar. For fifty years, linguist Noam Chomsky has argued for the hypothesis that children have innate, language-specific abilities that facilitate and constrain language learning. In particular, he has proposed that humans are biologically prewired to learn language at a certain time and in a certain way, arguing that children are born with a language acquisition device (LAD). However, since he developed the minimalist program, his latest version of theory of syntactic structure, Chomsky has reduced the elements of universal grammar, which he believes are prewired in humans to just the principle of recursion, thus voiding most of the nativist endeavor.\n\nResearchers who believe that grammar is learned rather than innate, have hypothesized that language learning results from general cognitive abilities and the interaction between learners and their human interactants. It has also recently been suggested that the relatively slow development of the prefrontal cortex in humans may be one reason that humans are able to learn language, whereas other species are not. Further research has indicated the influence of the FOXP2 gene.\n\nLanguage development and processing begins before birth. Evidence has shown that there is language development occurring antepartum. DeCasper and Spence performed a study in 1986 by having mothers read aloud during the last few weeks of pregnancy. When the infants were born, they were then tested. They were read aloud a story while sucking on a pacifier; the story was either the story read by the mother when the infant was in utero or a new story. The pacifier used was able to determine the rate of sucking that the infant was performing. When the story that the mother had read before was heard, the sucking of the pacifier was modified. This did not occur during the story that the infant had not heard before. The results for this experiment had shown that the infants were able to recognize what they had heard in utero, providing insight that language development had been occurring in the last six weeks of pregnancy.\n\nThroughout the first year of life, infants are unable to communicate with language. Instead, infants communicate with gestures. This phenomenon is known as prelinguistic gestures, which are nonverbal ways that infants communicate that also had a plan backed with the gesture. Examples of these could be pointing at an object, tugging on the shirt of a parent to get the parent's attention, etc. Harding, 1983, devised the major criteria that come along with the behavior of prelinguistic gestures and their intent to communicate. There are three major criteria that go along with a prelinguistic gesture: waiting, persistence, and ultimately, development of alternative plans. This process usually occurs around 8 months of age, where an appropriate scenario may be of a child tugging on the shirt of a parent to wait for the attention of the parent who would then notice the infant, which causes the infant to point to something they desire. This would describe the first two criteria. The development of alternative plans may arise if the parent does not acknowledge what the infant wants, the infant may entertain itself to satisfy the previous desire.\n\nWhen children reach about 15–18 months of age, language acquisition flourishes. There is a surge in word production resulting from the growth of the cortex. Infants begin to learn the words that form a sentence and within the sentence, the word endings can be interpreted. Elissa Newport and colleagues (1999) found that humans learn first about the sounds of a language, and then move on to how to speak the language. This shows how infants learn the end of a word and know that a new word is being spoken. From this step, infants are then able to determine the structure of a language and word.\n\nIt appears that during the early years of language development females exhibit an advantage over males of the same age. When infants between the age of 16 to 22 months were observed interacting with their mothers, a female advantage was obvious. The females in this age range showed more spontaneous speech production than the males and this finding was not due to mothers speaking more with daughters than sons. In addition, boys between 2 and 6 years as a group did not show higher performance in language development over their girl counterparts on experimental assessments. In studies using adult populations, 18 and over, it seems that the female advantage may be task dependent. Depending on the task provided, a female advantage may or may not be present. Similarly, one study found that out of the 5.5% of American children with language impairments, 7.2% are male, and 3.8% are female. There are many different suggested explanations for this gender gap in language impairment prevalence.\n\nIt is currently believed that in regards to brain lateralization males are left-lateralized, while females are bilateralized. Studies on patients with unilateral lesions have provided evidence that females are in fact more bilateralized with their verbal abilities. It seems that when a female has experienced a lesion to the left hemisphere, she is better able to compensate for this damage than a male can. If a male has a lesion in the left hemisphere, his verbal abilities are greatly impaired in comparison to a control male of the same age without that damage. However, these results may also be task-dependent as well as time-dependent.\n\nShriberg, Tomblin, and McSweeny (1999) suggest that the fine motor skills necessary for correct speech may develop more slowly in males. This could explain why some of the language impairments in young males seems to spontaneously improve over time.\n\nIt is also suggested that the gender gap in language impairment prevalence could also be explained by the clinical over diagnosis of males. Males tend to be clinically over diagnosed with a variety of disorders.\n\nThe study by Shriber et al. (1999) further explains that this gap in the prevalence of language impairment could be because males tend to be more visible. These researchers reveal that male children tend to act out behaviorally when they have any sort of disorder, while female children tend to turn inward and develop emotional disorders as well. Thus, the high ratio of males with language impairments may be connected with the fact that males are more visible, and thus more often diagnosed.\n\nResearch in writing development has been limited in psychology. In the research that has been conducted, focus has generally centred on the development of written and spoken language and their connection. Spoken and written skills could be considered linked. Researchers believe that children's spoken language influences their written language. When a child learns to write they need to master letter formation, spelling, punctuation and they also have to gain an understanding of the structure and the organisational patterns involved in written language.\n\nKroll's theory is one of the most significant on children's writing development. He proposed that children's writing development is split into 4 phases. Kroll explicitly states that these phases are 'artificial' in the sense that the boundaries between the phases are imprecise and he recognises that each child is different, thus their development is unique. The phases of writing development have been highlighted to give the reader a broad outline of what phases a child goes through during writing development; however when studying an individual's development in depth, the phases may be disregarded to an extent.\n\nThe first of Kroll's phases is the preparation for writing phase. In this phase the child is believed to grasp the technical skills needed for writing, allowing them to create the letters needed to write the words the children say. In this initial phase children experience many opportunities to extend their spoken language skills. Speaking and writing are considered fairly separate processes here, as children's writing is less well developed at this stage, whereas their spoken language is becoming more skilled.\n\nKroll considers the second phase in writing development to be consolidation. Here, children begin to consolidate spoken and written language. In this phase children's writing skills rely heavily on their spoken language skills, and their written and spoken language becoming integrated. Children's written language skills become stronger as they use their spoken language skills to improve their writing. Then in turn, when a development in children's written language skills is seen, their spoken language skills have also improved. A child's written language in this phase mirrors their spoken language.\n\nIn the third phase, differentiation, children begin to learn that written language regularly differs in structure and style from spoken language. The growth from consolidation to differentiation can be challenging for some children to grasp. Children can 'struggle with the transformation from the basically overt language of speech to the essentially covert activity of writing'. In this phase, the child learns that writing is generally considered more formal than spoken language, which is thought to be casual and conversational. Here, it is believed that children begin to understand that writing serves a purpose.\n\nKroll considers the last phase to be the systematic integration phase. A differentiation and integration between the child's speaking and writing can be seen in this phase. This means that speaking and writing have 'well-articulated forms and functions'; however, they are also integrated in the sense that they use the same system. As a result of the individual being aware of the audience, context and reason they are communicating, both written and spoken language are able to overlap and take several forms at this stage.\n\nKroll used the four phases to give an explanation and generalise about the development of these two aspects of language. The highest significance is placed on the second and third phase, consolidation and differentiation respectively. It could be concluded that children's written and spoken language, in certain respects, become more similar to age, maturation, and experience; however, they are also increasingly different in other respects. The content of the skills are more similar, but the approach used for both writing and speaking are different. When writing and speaking development is looked at more closely it can be seen that certain elements of written and spoken language are differentiating and other elements are integrating, all in the same phase.\n\nPerera conducted a survey and her view mirrors that of Kroll to the extent that she used Kroll's four phases. When a child undergoes initial learning of the written language, they have not yet fully mastered the oral language. It is clear that their written language development is aided by their spoken language; it can also be said that their spoken language development is aided by the development of their written language skills. Kantor and Rubin believe that not all individuals successfully move into the final stage of integration. Perera is also aware that it is hard to assign chronological ages to each phase of writing development, because each child is an individual, and also the phases are 'artificial'.\n\nOther than Kroll's theory, there are four principles on early patterns in writing development discussed by Marie Clay in her book \"What Did I Write?\". The four principles are recurring principle, the generative principle, the sign principle, and the inventory principle. The recurring principle involves patterns and shapes in English writing that develop throughout writing development. The generative principle incorporates the idea that a writer can create new meanings by organizing units of writing and letters of the alphabet. The sign principle is understanding that the word print also involves paper arrangement and word boundaries. And lastly, the inventory principle is the fact that children have the urge to list and name items that they are familiar with, and because of this they can practice their own writing skills.\n\nMore recent research has also explored writing development. Myhill concentrated on the development of written language skills in adolescents aged 13 to 15. Myhill discovered that the more mature writer was aware of the shaping of text, and used non-finite clauses, which mirrored Perera's results (1984). Other researchers focused on writing development up until late adolescence, as there has been a limited research in this area. Chrisite and Derewianke recognize that the survey conducted by Perera (1984) is still one of the most significant research studies in the writing development field and believe Perera's study is similar to theirs. Chrisite and Derewianke (2010) again propose four phases of writing development. The researchers believe that the process of writing development does not stop when an individual leaves formal education, and again, the researchers highlight that these phases are flexible in their onset. The first phase focuses on spoken language as the main aid for writing development, and the development then takes its course reaching the fourth phase, which continues beyond formal education.\n\nThe environment a child develops in has influences on language development. The environment provides language input for the child to process. Speech by adults to children help provide the child with correct language usage repetitively. Environmental influences on language development are explored in the tradition of social interactionist theory by such researchers as Jerome Bruner, Alison Gopnik, Andrew Meltzoff, Anat Ninio, Roy Pea, Catherine Snow, Ernest Moerk and Michael Tomasello. Jerome Bruner who laid the foundations of this approach in the 1970s, emphasized that adult \"scaffolding\" of the child's attempts to master linguistic communication is an important factor in the developmental process.\n\nOne component of the young child's linguistic environment is child-directed speech (also known as baby talk or motherese), which is language spoken in a higher pitch than normal with simple words and sentences. Although the importance of its role in developing language has been debated, many linguists think that it may aid in capturing the infant's attention and maintaining communication. When children begin to communicate with adults, this motherese speech allows the child the ability to discern the patterns in language and to experiment with language.\n\nThroughout existing research, it is concluded that children exposed to extensive vocabulary and complex grammatical structures more quickly develop language and also have a more accurate syntax than children raised in environments without complex grammar exposed to them. With motherese, the mother talks to the child and responds back to the child, whether it be a babble the child made or a short sentence. While doing this, the adult prompts the child to continue communicating, which may help a child develop language sooner than children raised in environments where communication is not fostered.\n\nChild-directed speech concentrates on small core vocabulary, here and now topics, exaggerated facial expressions and gestures, frequent questioning, para-linguistic changes, and verbal rituals. An infant is least likely to produce vocalizations when changed, fed, or rocked. The infant is more likely to produce vocalizations in response to a nonverbal behavior such as touching or smiling.\n\nChild-directed speech also catches the child's attention, and in situations where words for new objects are being expressed to the child, this form of speech may help the child recognize the speech cues and the new information provided. Data shows that children raised in highly verbal families had higher language scores than those children raised in low verbal families. Continuously hearing complicated sentences throughout language development increases the child's ability to understand these sentences and then to use complicated sentences as they develop. Studies have shown that students enrolled in high language classrooms have two times the growth in complex sentences usage than students in classrooms where teachers do not frequently use complex sentences.\n\nAdults use strategies other than child-directed speech like recasting, expanding, and labeling:\nSome language development experts have characterized child directed speech in stages. Primarily, the parents use repetition and also variation to maintain the infant's attention. Secondly, the parent simplifies speech to help in language learning. Third, any speech modifications maintain the responsiveness of the child. These modifications develop into a conversation that provides context for the development.\n\nWhile most children throughout the world develop language at similar rates and without difficulty, cultural and socioeconomic differences have been shown to influence development. An example of cultural differences in language development can be seen when comparing the interactions of mothers in the United States with their infants with mothers in Japan. Mothers in the United States use more questions, are more information-oriented, and use more grammatically correct utterances with their 3-month-olds. Mothers in Japan, on the other hand, use more physical contact with their infants, and more emotion-oriented, nonsense, and environmental sounds, as well as baby talk, with their infants. These differences in interaction techniques reflect differences in \"each society's assumptions about infants and adult-to-adult cultural styles of talking.\"\n\nSpecifically in North American culture, maternal race, education, and socioeconomic class influence parent-child interactions in the early linguistic environment. When speaking to their infants, mothers from middle class \"incorporate language goals more frequently in their play with their infants,\" and in turn, their infants produce twice as many vocalizations as lower class infants. Mothers from higher social classes who are better educated also tend to be more verbal, and have more time to spend engaging with their infants in language. Additionally, lower class infants may receive more language input from their siblings and peers than from their mothers.\n\nIt is crucial that children are allowed to socially interact with other people who can vocalize and respond to questions. For language acquisition to develop successfully, children must be in an environment that allows them to communicate socially in that language. Children who have learnt sound, meaning and grammatical system of language that can produce clear sentence may still not have the ability to use language effectively in various social circumstance. Social interaction is the footing stone of language.\n\nThere are a few different theories as to why and how children develop language. The most popular—and yet heavily debated—explanation is that language is acquired through imitation. This theory has been challenged by Lester Butler, who argues that children do not use the grammar that an adult would use. Furthermore, \"children's language is highly resistant to alteration by adult intervention\", meaning that children do not use the corrections given to them by an adult. The two most accepted theories in language development are psychological and functional. Psychological explanations focus on the mental processes involved in childhood language learning. Functional explanations look at the social processes involved in learning the first language.\n\n\nEach component has its own appropriate developmental periods.\n\nBabies can recognize their mother's voice from as early as few weeks old. It seems like they have a unique system that is designed to recognize speech sound. Furthermore, they can differentiate between certain speech sounds. A significant first milestone in phonetic development is the babbling stage (around the age of six months). This is the baby's way of practicing his control over that apparatus. Babbling is independent from the language. Deaf children for instance, babble the same way as hearing ones.\nAs the baby grows older, the babbling increases in frequency and starts to sound more like words (around the age of twelve months). Although every child is an individual with different pace of mastering speech, there is a tendency to an order of which speech sounds are mastered:\n\nAs the children's ability to produce sound develops, their ability to perceive the phonetic contrast of their language develops. The better they get in mastering the sound, the more sensitive they become to the changes in those sounds in their language once they get exposed to it. They learn to isolate individual phenomes while speaking which also serves as the basis of reading.\n\nSome processes that occur in early age:\n\n\nFrom shortly after birth to around one year, the baby starts to make speech sounds. \nAt around two months, the baby engages in cooing, which mostly consists of vowel sounds. At around four to six months, cooing turns into babbling, which is the repetitive consonant-vowel combinations. Babies understand more than they are able to say. In this 0–8 months range, the child is engaged in vocal play of vegetative sounds, laughing, and cooing.\n\nOnce the child hits the 8–12 month, range the child engages in canonical babbling, i.e. dada as well as variegated babbling. This jargon babbling with intonational contours the language being learned.\n\nFrom 12–24 months, babies can recognize the correct pronunciation of familiar words. Babies also use phonological strategies to simplify word pronunciation. Some strategies include repeating the first consonant-vowel in a multisyllable word ('TV' → 'didi') or deleting unstressed syllables in a multisyllable word ('banana' → 'nana'). Within this first year, two word utterances and two syllable words emerge. This period is often called the holophrastic stage of development, because one word conveys as much meaning as an entire phrase. For instance, the simple word \"milk\" can imply that the child is requesting milk, noting spilled milk, sees a cat drinking milk, etc. One study found that children at this age were capable of comprehending 2-word sentences, producing 2–3 word sentences, and naming basic colors.\n\nBy 24–30 months awareness of rhyme emerges as well as rising intonation. One study concludes that children between the ages of 24–30 months typically can produce 3–4 word sentence, create a story when prompted by pictures, and at least 50% of their speech is intelligible.\n\nBy 36–60 months, phonological awareness continues to improve as well as pronunciation. At this age, children have a considerable experience with language and are able to form simple sentences that are 3 words in length. They use basic prepositions, pronouns, and plurals. They become immensely creative in their language use and learn to categorize items such as recognizing that a shoe is not a fruit. At this age, children also learn to ask questions and negate sentences to develop these questions. Over time, their syntax gets more and more unique and complex. A study reveals that at this age, a child's speech should be at least 75% intelligible.\n\nBy 6–10 years, children can master syllable stress patterns, which helps distinguish slight differences between similar words.\n\nThe average child masters about fifty words by the age of eighteen months. These might include words such as, milk, water, juice and apple (noun-like words). Afterwards they acquire 12 to 16 words a day. By the age of six, they master about 13 to 14 thousand words.\n\nThe most frequent words include adjective-like expressions for displeasure and rejection such as 'no'. They also include social interaction words, such as \"please\" and \"bye\".\n\nThere are three stages for learning the meaning of new words:\n\n\nIn other words, when the child hears the word \"sheep\" he overgeneralizes it to other animals that look like sheep by the external appearance, such as white, wooly and four-legged animal.\n\nContextual clues are a major factor in the child's vocabulary development.\n\nThe child uses contextual clues to draw inferences about the category and meaning of new words. By doing so, the child distinguishes between names and ordinary nouns.\n\nFor example, when an object is presented to the child with the determiner \"a\" (a cat, a dog, a bottle) he perceives it as an ordinary noun.\n\nHowever, when the child hears a noun without the determiner, he perceives it as a name, for instance \"this is Mary\".\n\nChildren usually make correct meaning associations with the words that the adults say. However, sometimes they make semantic errors.\n\nThere are a few types of semantic errors:\n\nOverextension: When a child says or hears a word, they might associate what they see or hear as more generalized concept than the real meaning of the word. For example, if they say \"cat\", they might overextend it to other animals with same features.\n\nUnderextension: It involves the use of lexical items in an overly restrictive fashion. In other words, the child focuses on core members of a certain category. For example: 'cat' may only refer to the family cat and no other cat, or 'dog' may refer to certain kinds of dogs that the child is exposed to.\n\nVerb meaning: when a pre-school child hears the verb 'fill', he understands it as the action 'pour' rather than the result, which is 'make full'.\n\nDimensional terms: the first dimensional adjectives acquired are big and small because they belong to the size category. The size category is the most general one. Later children acquire the single dimension adjectives, such as, tall-short, long-short, high-low. Eventually they acquire the adjectives that describe the secondary dimension, such as thick-thin, wide-narrow and deep-shallow.\n\nFrom birth to one year, comprehension (the language we understand) develops before production (the language we use). There is about a 5-month lag in between the two. Babies have an innate preference to listen to their mother's voice. Babies can recognize familiar words and use preverbal gestures.\n\nWithin the first 12–18 months semantic roles are expressed in one word speech including agent, object, location, possession, nonexistence and denial. Words are understood outside of routine games but the child still needs contextual support for lexical comprehension.\n\n18–24 months Prevalent relations are expressed such as agent-action, agent-object, action-location.\nAlso, there is a vocabulary spurt between 18–24 months, which includes fast mapping. Fast mapping is the babies' ability to learn a lot of new things quickly. The majority of the babies' new vocabulary consists of object words (nouns) and action words (verbs).\n\n30–36 months The child is able to use and understand why question and basic spatial terms such as in, on or under.\n\n36–42 months There is an understanding of basic color words and kinship terms. Also, the child has an understanding of the semantic relationship between adjacent and conjoined sentences, including casual and contrastive.\n\n42–48 months When and how questions are comprehended as well as basic shape words such as circle, square and triangle.\n\n48–60 months Knowledge of letter names and sounds emerges, as well as numbers.\n\nBy 3–5 years, children usually have difficulty using words correctly. Children experience many problems such as underextensions, taking a general word and applying it specifically (for example, 'cartoons' specifically for 'Mickey Mouse') and overextensions, taking a specific word and applying it too generally (example, 'ant' for any insect). However, children coin words to fill in for words not yet learned (for example, someone is a cooker rather than a chef because a child may not know what a chef is). Children can also understand metaphors.\n\nFrom 6–10 years, children can understand meanings of words based on their definitions. They also are able to appreciate the multiple meanings of words and use words precisely through metaphors and puns. Fast mapping continues.\nWithin these years, children are now able to acquire new information from written texts and can explain relationships between multiple meaning words. Common idioms are also understood.\n\nThe development of syntactic structures follows a particular pattern and reveals much on the nature of language acquisition, which has several stages. According to O'Grady and Cho (2011), the first stage, occurring between the ages of 12–18 months, is called \"one-word stage.\" In this stage, children cannot form syntactic sentences and therefore use one-word utterances called \"holophrases\" that express an entire sentence. In addition, children's comprehension is more advanced than their production abilities. For example, a child who wants candy may say \"candy\" instead of expressing a full sentence.\n\nThe following stage is the \"two-word stage\" in which children begin to produce \"mini-sentences\" that are composed of two words, such as \"doggy bark\" and \"Ken water\" (O'Grady & Cho, 2011, p. 346). At this stage, it is unclear whether children have an understanding of underlying rules of the language such as syntactic categories, since their \"mini-sentences\" often lack distinction between the categories. However, children do exhibit sensitivity to sentence structures and they frequently use appropriate word order.\n\nAfter several months of speech that is restricted to short utterances, children enter the \"telegraphic stage\" and begin to produce longer and more complex grammatical structures (O'Grady & Cho, 2011, p. 347). This stage is characterized by production of complex structures as children begin to form phrases consisting of a subject and a complement in addition to use of modifiers and composition of full sentences. Children use mostly content words and their sentences lack function words. For example, a child may say \"fill cup water,\" instead of saying, \"Fill my cup with water.\" Subsequently, language acquisition continues to develop rapidly and children begin to acquire complex grammar that shows understanding of intricate linguistic features, such as the ability to switch the position of words in sentences.\n\nThroughout the process of syntactic development, children acquire and develop question structures. According to O'Grady and Cho (2011), at the early stages of language acquisition, children ask yes-no questions by rising intonation alone as they develop awareness to auxiliary verbs only at a later stage. When auxiliary verbs make their appearance, it takes children a few months before they are able to use inversion in yes-no questions. The development of WH- questions occurs between the ages of two and four, when children acquire auxiliary verbs that then leads to the ability to use inversion in questions. However, some children find inversion easier in yes-no questions than in WH- questions, since the position of the WH- word and the auxiliary verb both must changed (e.g., \"You are going where?\" instead of \"Where are you going?\").\n\nMorphological structures development occurs over a period of several years. Before language is acquired, children lack any use of morphological structures.\n\nThe morphological structures that children acquire during their childhood, and even up to the early school years, are: determiners (a, the), -ing inflection, plural –s, auxiliary be, possessive –s, third person singular –s, past tense –ed).\n\nWhen children start using them they tend to overgeneralize the rules and project them upon all the words in the language, including irregulars. For example: if a child knows the –ed (past tense) there is a possibility that they'll say \"I eated\"( Man-mans cat-cats). These errors result from overgeneralization of rules.\n\nThis overgeneralization is very noticeable, in fact, children do that in less than 25 percent of the time at any point before reducing that percentage. Then they improve their mastery, which can be tested in various ways, such as the \"wug test\" (Berko, 1958).\n\nChildren often figure out quickly frequent irregular verbs, such as go and buy rather than less common ones, such as win. This suggests that children must hear the word several hundred times before they are able to use it correctly.\n\nThis development of bound morphemes is similar in order among children, for example: -\"ing\" is acquired before the article \"the\". The interesting part though is that parents tend to use a different order while speaking to their kids, for example, parents use the article 'the' more frequently than -ing. Meaning, other factors determine the order of acquisition, such as:\n\n\nAs it comes to word formation processes such as derivation and compounding, the earliest derivational suffixes to show up in children's speech are those they hear adults use most frequently. (-er as the *'doer' of the action such as walker.) When it comes to compounds, children first make up names for agents and instruments that they don't know by a pattern (N-N), though some of them do not follow the pattern (*cutter grass for grass cutter). Then, they might have the right structure but the words are inappropriate since English already has words with the intended meaning such as car-smoke = exhaust. This process points to a preference for building words from other words, thus place less demand on memory than learning an entirely new word for each concept.\n\nGrammar describes the way sentences of a language are constructed, which includes both syntax and morphology.\n\nSyntactic development involves the ways that various morphemes are ordered or combined in sentences.Morphemes, which are basic units of meaning in language, get added as children learn to produce simple sentences and become more precise speakers. Morphemes can be whole words (like \"happy\") or parts of words that change meaning of words (\"un\"happy). Brown proposed a stage model that describes the various types of morphological structures that are developed in English and the age range within which they are normally acquired.\n\nStage I: From 15–30 months, children start using telegraphic speech, which are two word combinations, for example 'wet diaper'. Brown (1973) observed that 75% of children's two-word utterances could be summarized in the existence of 11 semantic relations:\nStage II: At around 28–36 months, children begin to engage in the production of simple sentences, usually 3 word sentences. These simple sentences follow syntactic rules and are refined gradually as development continues. The morphological developments seen in this age range include use of present progressive (-ing endings), the prepositions \"in\" and \"on\", and regular plurals (-s endings).\n\nStage III: Around 36–42 months, children continue to add morphemes and gradually produce complex grammatical structures. The morphemes that are added at this age include irregular past tense, possessive ('s), and use of the verb 'to be' (It is, I am, etc.).\n\nStage IV:Around 40–46 months children continue to add to their morphological knowledge. This range is associated with use of articles (a or the), regular past tense (-ed endings), and regular third person speech (He likes it).\n\nStage V: Around 42-52+ months children refine the complex grammatical structures and increase their use of morphemes to convey more complex ideas. Children in this stage use irregular third-person speech, the verb 'to be' as an auxiliary verb (She was not laughing), and in its contraction forms (It's, She's, etc.).\n\nFrom birth to one year, babies can engage in joint attention (sharing the attention of something with someone else). Infants also can engage in turn taking activities on the basis of their sensitivity to reactive contingency, which can elicit social responses in the babies from very early on.\n\n\nThere is a large debate regarding whether or not bilingualism is truly beneficial to children. Parents of children often view learning a second language throughout elementary and high school education beneficial to the child. Another perspective dictates that the second language just confuses the child and prevents them from mastering their primary language. Studies have shown that American bilingual children have greater cognitive flexibility, better perceptual skills and tend to be divergent thinkers than monolingual children between the ages of five to ten. Better executive functioning skills are likely because bilingual children have to choose one language to speak while actively suppressing the other. This builds stronger selective attention and cognitive flexibility because these skills are being exercised more. In addition, bilingual children have a better understanding of universal language concepts, such as grammar, because these concepts are applied in multiple languages. However, studies comparing Swedish-Finnish bilingual children and Swedish monolingual children between the ages of five to seven have also shown that the bilingual children have a smaller vocabulary than monolingual children. In another study throughout America, elementary school English-monolingual children performed better in mathematics and reading activities than their non-English-dominant bilingual and non-English monolingual peers from kindergarten to grade five. Learning two languages simultaneously can be beneficial or a hindrance to a child's language and intellectual development. Further research is necessary to continue to shed light on this debate.\n\nIn addition to the study of bilingualism in children, similar research is being conducted in adults. Research findings show that although bilingual benefits are muted in middle adulthood, they are more profound in older age when those who develop dementia experience onset about 4.5 years later in bilingual subjects. The increased attentional control, inhibition, and conflict resolution developed from bilingualism may be accountable for the later onset of dementia.\n\nThere is some research that demonstrates that bilingualism may often be misdiagnosed as a language impairment. A subtopic of bilingualism in the literature is nonstandard varieties of English. While bilingualism and nonstandard varieties of English cannot be considered a true language impairment, they are misrepresented in the population of those receiving language interventions.\n\nA language disorder is the impaired comprehension and or use of a spoken, written, and/or other symbol system. A disorder may involve problems in the following areas: \n\nOlswang and colleagues have identified a series of behaviors in children in the 18–36 month range that are predictors for the need of language intervention.\n\nThese predictors include: \n\nSome of the many conditions that cause language development problems include:\n\n\n\n\n"}
{"id": "37506954", "url": "https://en.wikipedia.org/wiki?curid=37506954", "title": "Linguistic boundary of Moselle", "text": "Linguistic boundary of Moselle\n\nThe linguistic boundary in the French department of Moselle (Lorraine region) is a subset of the wider Romance-Germanic language border that stretches through Belgium, France, Switzerland and Italy.\n\nAt the end of the nineteenth century:\n\n"}
{"id": "1723892", "url": "https://en.wikipedia.org/wiki?curid=1723892", "title": "Linguistic philosophy", "text": "Linguistic philosophy\n\nLinguistic philosophy is the view that philosophical problems are problems which may be solved (or dissolved) either by reforming language, or by understanding more about the language we presently use. The former position is that of ideal language philosophy, the latter the position of ordinary language philosophy.\n\n\n"}
{"id": "192821", "url": "https://en.wikipedia.org/wiki?curid=192821", "title": "List of official languages", "text": "List of official languages\n\nThis is a list of official languages of sovereign countries.\n\nDifferent organisations sometimes refer to their principal languages of administration and communication as \"working languages\", whilst others refer to these as being \"official\".\n\nAfar:\n\nAfrikaans:\n\nAja-Gbe:\n\nAkan (Akuapem Twi, Ashante Twi, Fante):\n\nAlbanian:\nAmharic:\n\nAnii:\n\nArabic (see also List of countries where Arabic is an official language):\n\nArmenian:\n\nAssamese:\n\nAymara:\n\nAzerbaijani:\n\nBalanta:\n\nBambara:\n\nBariba:\n\nBasque:\n\nBassari:\n\nBedik:\n\nBelarusian:\n\nBengali:\n\nBerber:\n\nBiali:\n\nBislama:\n\nBoko:\n\nBomu:\n\nBosnian:\n\nBozo:\n\nBuduma:\n\nBulgarian:\n\nBurmese:\n\nCantonese:\nCatalan:\n\nChinese, Mandarin:\n\nChichewa:\n\nChirbawe (Sena):\n\nComorian\n\nCroatian:\n\nCzech:\n\nDagaare:\n\nDagbani:\n\nDangme\n\nDanish:\n\nDari:\n\nDendi:\n\nDhivehi:\n\nDioula:\n\nDogon:\n\nDutch:\n\nDzongkha:\n\nEnglish (see also List of countries where English is an official language):\n\nEstonian:\n\nEwe-Gbe:\n\nFijian:\n\nFilipino:\n\nFinnish:\n\nFon-Gbe:\n\nFoodo:\n\nFormosan:\n\nFrench (see also List of countries where French is an official language):\n\nFula:\n\nGa:\n\nGàidhlig:\n\nGbe:\n\nGen-Gbe:\n\nGeorgian:\n\nGerman:\n\nGonja:\n\nGourmanché\n\nGreek:\n\nGuaraní:\n\nGujarati:\n\nHaitian Creole:\n\nHakka:\n\nHassaniya:\n\nHausa:\n\nHebrew:\n\nHindi:\n\nHiri Motu:\n\nHungarian:\n\nIgbo:\n\nIcelandic:\n\nIndonesian:\n\nIrish:\n\nItalian:\n\nJapanese:\n\nJola:\n\nKabye:\n\nKalanga:\n\nKannada:\n\n\nKanuri:\n\nKasem:\n\nKazakh:\n\nKhmer:\n\nKinyarwanda:\n\nKirundi:\n\nKissi\n\nKhoisan:\n\nKorean:\n\nKorean Sign Language:\n\nKpelle:\n\nKurdish:\n\nKyrgyz:\n\nLao:\n\nLatvian:\n\nLithuanian:\n\nLukpa:\n\nLuxembourgish:\n\nMacedonian:\n\nMalagasy:\n\nMalay:\n\nMalinke:\n\nMaltese:\n\nMamara:\n\nManding (Mandinka, Malinke):\n\nMandinka:\n\nMandjak:\n\nMankanya:\n\nManx Gaelic:\n\nMāori:\n\nMarshallese:\n\nMauritian Creole \n\nMbelime:\n\nMoldovan\n\nMongolian:\n\nMontenegrin:\n\nMossi:\n\nNambya:\n\nNateni:\n\nNauruan\n\nNdau:\n\nNdebele (Northern):\n\nNdebele (Southern):\n\nNepali:\n\nNew Zealand Sign Language:\n\nNoon:\n\nNorth Korean:\n\nNorthern Sotho:\n\nNorwegian:\n\nNzema:\n\nOniyan:\n\nOssetian:\n\nPalauan:\n\nPapiamento:\n\nPashto:\n\nPersian:\n\nPolish:\n\nPortuguese:\n\nPunjabi:\n\nQuechua:\n\nRomanian:\n\nRomansh:\n\nRussian:\n\nSafen:\n\nSamoa\n\nSango\n\nSena:\n\nSerbian:\n\nSerer:\n\nSeychellois Creole\n\nShona:\n\nSinhala:\n\nSlovak:\n\nSlovene:\n\nSomali:\n\nSonghay-Zarma:\n\nSoninke:\n\nSotho:\n\nSpanish:\n\nSusu:\n\nSwahili:\n\nSwati:\n\nSwedish:\n\nSyenara:\n\nTajik:\n\nTagalog:\n\nTamasheq:\n\nTamil:\n\nTammari:\n\nTasawaq:\n\nTebu:\n\nTelugu:\n\nTetum:\n\nThai:\n\nTigrinya:\n\nTok Pisin:\n\nToma:\n\nTonga:\n\nTongan\n\nTsonga:\n\nTswana:\n\nTurkish:\n\nTurkmen:\n\nTuvaluan\n\nUkrainian:\n\nUrdu:\n\nUzbek:\n\nVenda:\n\nVietnamese:\n\nWaama:\n\nWaci-Gbe:\n\nWamey:\n\nWelsh:\n\nWolof:\n\nXhosa:\n\nXwela-Gbe:\n\nYobe:\n\nYom:\n\nYoruba:\n\nZimbabwean sign language:\n\nZulu:\n\nThis is a ranking of languages by number of sovereign countries in which they are \"de jure\" or \"de facto\" official (or with a \"national language\" status).\n\n\nAbaza:\n\nAdyghe:\n\nAghul:\n\nAklanon:\n\nAlbanian:\n\n\nAltay:\n\nArabic:\n\nAranese \"see Occitan\"\n\nArmenian:\n\nAssamese:\n\nAvar:\n\nAzeri:\n\nBalkar:\n\nBashkir:\n\nBasque:\n\nBengali:\n\nBikol:\n\nBosnian:\n\nBuryat:\n\nCantonese Chinese:\n\nCatalan:\n\nCebuano:\n\nChavacano:\n\nChechen:\n\nCherkess:\n\nCherokee:\n\nChipewyan:\n\nChukchi:\n\nChuvash\n\nCree:\n\nCrimean Tatar\n\nCroatian:\n\nDargwa:\n\nDolgan:\n\nDutch:\n\nEnglish:\n\n\nErzya:\n\nEven:\n\nEvenki:\n\nFaroese:\n\nFinnish:\n\nFrench:\n\n\nFrisian (West):\n\nGagauz:\n\nGalician:\n\nGerman:\n\nGreek:\n\nGuaraní:\n\nGujarati:\n\nGwich'in:\n\nHawaiian:\n\nHiligaynon:\nHindi:\nHungarian:\n\nIbanag:\n\nIlocano:\n\nIngush:\n\nInuinnaqtun:\n\nInuktitut:\n\nInuvialuktun:\n\nIrish:\n\nItalian:\n\nIvatan:\n\nJapanese:\n\nKabardian\n\nKalaallisut:\n\nKalmyk:\n\nKannada:\n\nKapampangan:\n\nKarachay:\n\nKarelian:\n\nKashmiri:\n\nKazakh:\n\nKhakas:\n\nKhanty:\n\nKinaray-a:\n\nKomi:\n\nKomi-Permyak:\n\nKorean:\n\nKumyk:\n\nKyrgyz:\n\nLak:\n\nLezgian\n\nMacedonian:*part of Albania\n\nMaguindanao:\n\nMalayalam:\n\nMansi:\n\nMaranao:\n\nMarathi:\n\nMari (Hill and Meadow):\n\nMayan:\n\nMoksha:\n\nMongolian:\n\nNáhuatl:\n\nNenets:\n\nNepali:\n\nNogai:\n\nOccitan:\n\nOdia:\n\nOssetic (Digor and Iron dialects):\n\nPangasinan:\n\nPortuguese:*part of the People's Republic of China\n\nPunjabi:\n\nRomanian:\n\nRussian. Russian is fixed as a state language in the Constitutions of the republics of the Russian Federation:\n\nRusyn:\n\nRutul:\n\nSakha:\n\nSambal:\n\nSami:\n\nSanskrit:\n\nSaraiki\n\nSarikoli:\n\nScottish Gaelic:\n\nScots:\n\nSelkup:\n\nSerbian:\n\nSindhi:\n\nNorth and South Slavey:\n\nSlovak:\n\nSlovene:\n\nSpanish:\n\nSurigaonon:\n\nTabasaran:\n\nTagalog:\n\nTahitian:\n\nTamil:\n\nTat:\n\nTatar:\n\nTausug:\n\nTelugu:\n\nTibetan:\n\nTłįchǫ:\n\nTsakhur:\n\nTswana:\n\nTurkish:\n\nTuvan:\n\nUdmurt:\n\nUrdu:\n\nUyghur:\n\nVeps:\n\nVietnamese:\n\nWaray:\n\nWelsh:\n\nYakan:\n\nYiddish:\n\nYukaghir:\n\nZhuang:\n\n"}
{"id": "13056435", "url": "https://en.wikipedia.org/wiki?curid=13056435", "title": "Lithuanian press ban", "text": "Lithuanian press ban\n\nThe Lithuanian press ban () was a ban on all Lithuanian language publications printed in the Latin alphabet in force from 1865 to 1904 within the Russian Empire, which controlled Lithuania at the time. Lithuanian-language publications that used Cyrillic were allowed and even encouraged. \n\nThe concept arose after the failed January Uprising of 1863, taking the form of an administrative order in 1864, and was not lifted until 26 April 1904. The Russian courts reversed two convictions in press ban cases in 1902 and 1903, and the setbacks of the Russo-Japanese War in early 1904 brought about a loosened Russian policy towards minorities.\n\nUnder the ban, it was illegal to print, import, distribute, or possess any publications in the Latin alphabet. Tsarist authorities hoped that this measure, part of a larger Russification plan, would decrease Polish influence on Lithuanians and would return them to what were considered their ancient historical ties with Russia. However, Lithuanians organized printing outside the Empire, largely in Lithuania Minor (East Prussia), and in the United States. \n\nKnygnešiai (Lithuanian book smugglers) smuggled illegal books and periodicals across the border. The number of such publications kept increasing despite strict sanctions and persecution of the activists. The ban created a well-defined and organized opposition to Russian rule and culture—the opposite of its original intent. The Lithuanian historian Edvardas Gudavičius has described the ban as a test of the concept of Lithuania: had there been no resistance, the language would have become a historical footnote, and the modern nation would never have been created.\n\nThe first book published in print in the Lithuanian language was Lutheran Catechism of Martynas Mažvydas in 1547. Other milestone publications included Daniel Klein's \"Grammar\" in 1653, a publication of the Bible in 1735, and the first work of imaginative literature, Kristijonas Donelaitis' \"Metai\" (The Seasons), in 1818. During the years of the Polish–Lithuanian Commonwealth, which lasted from 1569 to 1781, the Polish language gained ground as the written \"lingua franca\" of greater Lithuania, although the Prussian areas of Lithuania Minor continued to issue publications in Lithuanian.\n\nAt the beginning of the 19th century, use of the Lithuanian language was largely limited to Lithuanian rural areas, apart from its use in Prussia; it was, however, retained by some members of the minor nobility, especially in the Samogitian region. Several factors contributed to its subsequent revival: the language drew attention from scholars of the emerging science of comparative linguistics; after the abolition of serfdom in the Russian Empire in 1861, social mobility increased, and Lithuanian intellectuals arose from the ranks of the rural populace; and language became associated with identity in Lithuania, as elsewhere across Europe. Within the Catholic Church, the barriers that had earlier prevented commoners from entering the priesthood were eased. A closer relationship developed between the educated clergy, who were increasingly of ethnic Lithuanian stock, and their parishioners, including a sympathy with their wish to use the Lithuanian language. The emerging national movement sought to distance itself from both Polish and Russian influences, and the use of the Lithuanian language was seen as an important aspect of this movement.\n\nAccording to the bibliographer Vaclovas Biržiška, between 1800 and 1864, when the press ban was enacted, 926 book titles were published in the Lithuanian language using its Latin alphabet. The orthography of the language was not standardized; this problem was used by the Russian authorities as a rationale for the change to Cyrillic.\n\nAfter the partitions of the Polish-Lithuanian Commonwealth in the late 18th century, significant portions of Lithuania and Poland were incorporated into the Russian Empire. The uprising of 1863, seeking to re-establish the Commonwealth, convinced many Russian politicians that Polish cultural and political influence was the main obstacle hindering the Russification of Lithuania. They believed that if the Lithuanian peasantry were distanced from the Polonized nobility and the Catholic Church, Lithuanians would naturally come under Russian cultural influence, as they had allegedly been during previous eras. The Russian politician Nikolai Miliutin wrote that \"Russian letters will finish that which was begun with the Russian sword.\"\n\nOn 13 May 1863 Tsar Alexander II of Russia appointed Mikhail Nikolayevich Muravyov as the Governor General of the Vilna Governorate. His duties included both suppression of the uprising, and implementation of the Russification policy. Because the situation was perceived as critical, Muravyov was temporarily granted extremely wide powers. Muravyov and Ivan Petrovich Kornilov, the newly appointed director of the Vilnius educational district, prepared a radical long-term Russification program that became known as the \"Program of Restoration of Russian Beginnings\" (). Its stated goals were to:\n\nOn 22 May 1864 Tsar Alexander II approved this program. A few days later Muravyov issued an administrative order that forbade printing Lithuanian language textbooks written in the Latin alphabet. This order was developed into a comprehensive ban on September 6, 1865 by Konstantin Petrovich von Kaufman, Muravyov's successor. Kaufman issued an order to six neighboring governorates declaring a full ban on all publications and demanding that censorship committees enforce it without hesitation. A week later the order was extended to the entire Empire by Pyotr Valuev, Minister of the Interior. In 1866 the ban was further extended to include all academic books.\n\nDespite its strict and widespread enforcement, none of the ban's supporting measures were ever actually codified into law. The ban was enforced based solely on administrative orders and the tsar's approval. When the special temporary powers of the Governor General were revoked in 1871, these administrative orders lost any legal value. From that point on the ban had no legal basis, but it was still strictly enforced.\n\nAt first the Russian authorities encouraged and sponsored the publication of Lithuanian-language works in the Cyrillic alphabet. The idea of replacing the Latin alphabet with Cyrillic was first elaborated by the well-known Pan-Slavist Alexander Hilferding in his 1863 book \"Lithuania and Samogitia\". \n\nThe first experiments with the conversion of Lithuanian writings into Cyrillic were conducted by a Lithuanian linguist, Jonas Juška. He showed some samples of adapted texts to both Muravyov and Kornilov in February 1864. However, Juška discontinued his work and Kornilov formed a committee to work on publishing Lithuanian books in Cyrillic. The committee had four members: the Polish librarian Stanisław Mikucki from Warsaw, Russian Jonas Kerčinskis, a Lithuanian Catholic priest who had converted to Eastern Orthodoxy, Antanas Petkevičius, and the well-known Lithuanian educator and publisher Laurynas Ivinskis. Ivinskis soon withdrew from the committee. The first such book was a primer intended for use in the new Russian schools that were replacing the Lithuanian parish schools. It appeared during the summer of 1864. The committee also published a prayer book, a calendar, and other religious literature.\n\nAbout 55 Lithuanian-Cyrillic titles were published during the 40 years of the ban; about half of these were published during its first decade. Seeing that the Lithuanian people were unwilling to accept these books, even when they were offered for free, the Russian government shifted its attention to eliminating the illegal publications.\n\nThe Russian Ministry of Education issued a report in May 1898 recommending that the press ban be repealed. The report stated that the ban had produced adverse and unforeseen results, including the development of Lithuanian nationalism. Other Russian officials had opined that the interests of the Russian state would be better served by the presence of a legal Lithuanian press that could be censored.\n\nDuring the years of the ban, 3,047 people (829 smugglers, 859 distributors, and 1,359 persons possessing banned books) were arrested in connection with the ban.\n\nAnti-Russian sentiment and distrust of the tsarist authorities had arisen after the 1863 revolt. The ban was also perceived as a threat to the Catholic Church; the Eastern Orthodox Church used the Cyrillic alphabet, and loyalty to the Latin alphabet was also a symbolic loyalty to Catholicism. Attempts were made to circumvent the ban by using Gothic script. However, that was also banned in 1872. A number of problems arose with the various Lithuanian-Cyrillic orthographies, which were all criticized as ill-adapted to the Lithuanian language. Within Russia, the ban was opposed by scholarly, liberal, and pro-democratic groups, which served to mitigate the punishments.\n\nThe organized resistance to the ban, both legal and illegal, was largely initiated by bishop Motiejus Valančius, who petitioned the government to exempt prayer books from the ban. He then moved towards sponsoring the illegal flow of books from outside Lithuania. The period from Valančius death in 1875 to 1883 saw the establishment of the Lithuanian-language newspaper \"Auszra\" (The Dawn), and the resistance at this time is associated with bishop Antanas Baranauskas. The resistance intensified towards the end of the 19th century, after another major newspaper, \"Varpas\" (The Bell), edited by Vincas Kudirka, was established in 1889. Between 1891 and 1893, 31,718 publications were confiscated and destroyed; between 1900 and 1902 this number increased to 56,182, reflecting their increased flow.\n\nThe period from 1890 to 1904 saw the publication of about 2,500 book titles in the Lithuanian Latin alphabet. The majority of these were published in Tilsit, a city in East Prussia, although some publications reached Lithuania from the United States. A largely standardized written version of the language was achieved by the turn of the twentieth century, based on historical and Aukštaitijan (highland) usages; the letters -č- and -š- were taken from Czech orthography. The widely accepted \"Lithuanian Grammar\", by Jonas Jablonskis, appeared in 1901.\n\nA number of challenges to the ban's legal basis were made, and the use of this venue intensified at the end of the 19th century, along with an increasing number of letters, petitions, and protests from Lithuanians. In 1902 and 1903 the Russian Supreme Court reversed two press ban convictions that had been brought against Antanas Macijauskas and Povilas Višinskis. The court's decisions stated that the original executive decree creating the ban was illegal. The outbreak of the Russo-Japanese War in February 1904 contributed to the Russian government's perception that its minorities needed to be accommodated. The ban was officially lifted on 26 April 1904.\n\nPublic and private education in Lithuanian was adversely affected by the press ban. The level of pent-up demand for schooling in the 19th century is illustrated by the increase in literacy in the Rietavas area; between 1853 and 1863, just before the ban, the number of literate persons rose from 11,296 to 24,330. The subsequent ban is thought to have contributed to illiteracy in 19th-century Lithuania.\n\nIn the wake of the ruling, parish schools were closed. A shortage of teachers led to the closure of a number of state schools as well, in spite of population growth. Parents began to withdraw their children from the state schools, since they were associated with the policy of Russification; students were not allowed to speak Lithuanian among themselves, and a discouraging atmosphere was created by the system of searches, inspections, and spying. Many students were schooled at home or in small secret groups instead, although this practice also resulted in sanctions.\n\nA census of the Kovno Governorate in 1897 showed that a higher proportion of older people than younger had received formal education: of persons age 30 to 39, 61.87% had experienced some level of formal education, compared to only 54.68% of persons aged 10 to 19.\n\nAfter the ban was lifted, printing presses and their supporting social and cultural infrastructure needed to be established. The first issue of a Lithuanian newspaper after the ban, \"Vilniaus žinios\", appeared on December 23, 1904; the Great Seimas of Vilnius, which took place in November 1905, was now able to issue its announcements and publications in Lithuanian.\n\nThe publishing houses of Martynas Kukta, Saliamonas Banaitis, and the Society of St. Casimir in Kaunas were responsible for many of the publications issued between the end of the ban in 1904 and the restoration of Lithuanian independence in 1918. The businessman Petras Vileišis installed a printing press at his palace, commissioned in 1904. During this period 4,734 Lithuanian-language titles in the Latin alphabet were published in Lithuania and abroad. After Lithuanian independence was established, the rate of publication increased steadily; 16,721 book titles were printed from 1918 to 1939. Between 1925 and 1939 about 800 to 900 book titles were printed annually.\n\nA standard Lithuanian orthography and grammar were established during the ban, despite the fact that the co-ordination of this process, involving competing dialects, was forced to take place in several countries. The ban is widely felt to have stimulated the Lithuanian national movement, rather than discouraging it. In 2004, the 100th anniversary of the ban's end was noted in UNESCO's events calendar, and the Lithuanian Seimas declared the \"Year of the Lithuanian Language and Book.\" \n"}
{"id": "319762", "url": "https://en.wikipedia.org/wiki?curid=319762", "title": "Logos", "text": "Logos\n\nLogos (, ; ; from , , ) is a term in Western philosophy, psychology, rhetoric, and religion derived from a Greek word variously meaning \"ground\", \"plea\", \"opinion\", \"expectation\", \"word\", \"speech\", \"account\", \"reason\", \"proportion\", and \"discourse\", but it became a technical term in Western philosophy beginning with Heraclitus (), who used the term for a principle of order and knowledge. \"Logos\" is the logic behind an argument. \"Logos\" tries to persuade an audience using logical arguments and supportive evidence. \"Logos\" is a persuasive technique often used in writing and rhetoric.\n\nAncient Greek philosophers used the term in different ways. The sophists used the term to mean discourse; Aristotle applied the term to refer to \"reasoned discourse\" or \"the argument\" in the field of rhetoric, and considered it one of the three modes of persuasion alongside \"ethos\" and \"pathos\". Stoic philosophers identified the term with the divine animating principle pervading the Universe. Within Hellenistic Judaism, Philo of Alexandria () adopted the term into Jewish philosophy. The Gospel of John identifies the Logos, through which all things are made, as divine (\"theos\"), and further identifies Jesus Christ as the incarnate Logos. The term is also used in Sufism, and the analytical psychology of Carl Jung.\n\nDespite the conventional translation as \"word\", it is not used for a word in the grammatical sense; instead, the term \"lexis\" (, ) was used. However, both \"logos\" and \"lexis\" derive from the same verb (), meaning \"(I) count, tell, say, speak\".\n\nAuthor and Professor Jeanne Fahnestock describes logos as a \"premise\". She states that, to find the reason behind a rhetor's backing of a certain position or stance, one must acknowledge the different \"premises\" that the rhetor applies via his or her chosen diction. The rhetor's success, she argues, will come down to \"certain objects of agreement...between arguer and audience\". \"Logos is logical appeal, and the term logic is derived from it. It is normally used to describe facts and figures that support the speaker's topic.\" Furthermore, logos is credited with appealing to the audience's sense of logic, with the definition of \"logic\" being concerned with the thing as it is known.\nFurthermore, one can appeal to this sense of logic in two ways. The first is through inductive reasoning, providing the audience with relevant examples and using them to point back to the overall statement. The second is through deductive enthymeme, providing the audience with general scenarios and then indicating commonalities among them.\n\nPhilo distinguished between \"logos prophorikos\" (\"the uttered word\") and the \"logos endiathetos\" (\"the word remaining within\"). The Stoics also spoke of the \"logos spermatikos\" (the generative principle of the Universe) which foreshadows related concepts in Neoplatonism. Early translators of the Greek New Testament such as Jerome (in the 4th century AD) were frustrated by the inadequacy of any single Latin word to convey the meaning of the word \"Logos\" as used to describe Jesus Christ in the Gospel of John. The Vulgate Bible usage of was thus constrained to use the (perhaps inadequate) noun for \"word\", but later Romance language translations had the advantage of nouns such as in French. Reformation translators took another approach. Martin Luther rejected (verb) in favor of (word), for instance, although later commentators repeatedly turned to a more dynamic use involving \"the living word\" as felt by Jerome and Augustine.\n\nThe writing of Heraclitus () was the first place where the word \"logos\" was given special attention in ancient Greek philosophy, although Heraclitus seems to use the word with a meaning not significantly different from the way in which it was used in ordinary Greek of his time. For Heraclitus, \"logos\" provided the link between rational discourse and the world's rational structure.\n\nWhat \"logos\" means here is not certain; it may mean \"reason\" or \"explanation\" in the sense of an objective cosmic law, or it may signify nothing more than \"saying\" or \"wisdom\". Yet, an independent existence of a universal \"logos\" was clearly suggested by Heraclitus.\n\nAristotle identifies two specific types of persuasion methods: artistic and inartistic. He defines artistic proofs as arguments that the rhetor generates and creates on their own. Examples of these include relationships, testimonies, and conjugates. He defines inartistic proofs as arguments that the rhetor quotes using information from a non-self-generated source. Examples of these include laws, contracts, and oaths.\n\nFollowing one of the other meanings of the word, Aristotle gave \"logos\" a different technical definition in the \"Rhetoric\", using it as meaning argument from reason, one of the three modes of persuasion. The other two modes are \"pathos\" (, ), which refers to persuasion by means of emotional appeal, \"putting the hearer into a certain frame of mind\"; and \"ethos\" (, ), persuasion through convincing listeners of one's \"moral character\". According to Aristotle, \"logos\" relates to \"the speech itself, in so far as it proves or seems to prove\". In the words of Paul Rahe:\n\n\"Logos\", \"pathos\", and \"ethos\" can all be appropriate at different times. Arguments from reason (logical arguments) have some advantages, namely that data are (ostensibly) difficult to manipulate, so it is harder to argue against such an argument; and such arguments make the speaker look prepared and knowledgeable to the audience, enhancing \"ethos\". On the other hand, trust in the speaker—built through \"ethos\"—enhances the appeal of arguments from reason.\n\nRobert Wardy suggests that what Aristotle rejects in supporting the use of \"logos\" \"is not emotional appeal per se, but rather emotional appeals that have no 'bearing on the issue', in that the \"pathē\" [, ] they stimulate lack, or at any rate are not shown to possess, any intrinsic connection with the point at issue—as if an advocate were to try to whip an antisemitic audience into a fury because the accused is Jewish; or as if another in drumming up support for a politician were to exploit his listeners's reverential feelings for the politician's ancestors\".\n\nAristotle comments on the three modes by stating: \nStoic philosophy began with Zeno of Citium , in which the \"logos\" was the active reason pervading and animating the Universe. It was conceived as material and is usually identified with God or Nature. The Stoics also referred to the \"seminal logos\" (\"\"logos spermatikos\"), or the law of generation in the Universe, which was the principle of the active reason working in inanimate matter. Humans, too, each possess a portion of the divine \"logos\".\n\nThe Stoics took all activity to imply a \"logos\" or spiritual principle. As the operative principle of the world, the \"logos\" was \"anima mundi\" to them, a concept which later influenced Philo of Alexandria, although he derived the contents of the term from Plato. In his Introduction to the 1964 edition of Marcus Aurelius' \"Meditations\", the Anglican priest Maxwell Staniforth wrote that \"Logos\" ... had long been one of the leading terms of Stoicism, chosen originally for the purpose of explaining how deity came into relation with the universe\".\n\nPublic discourse on ancient Greek rhetoric has historically emphasized Aristotle's appeals to \"logos\", \"pathos\", and \"ethos\", while less attention has been directed to Isocrates' teachings about philosophy and \"logos\", and their partnership in generating an ethical, mindful \"polis\". Isocrates does not provide a single definition of \"logos\" in his work, but Isocratean \"logos\" characteristically focuses on speech, reason, and civic discourse. He was concerned with establishing the \"common good\" of Athenian citizens, which he believed could be achieved through the pursuit of philosophy and the application of \"logos\".\n\nIn the Septuagint the term \"logos\" is used for the word of God in the creation of heaven in Psalm 33:6, and in some related contexts.\n\nPhilo (), a Hellenized Jew, used the term Logos to mean an intermediary divine being or demiurge. Philo followed the Platonic distinction between imperfect matter and perfect Form, and therefore intermediary beings were necessary to bridge the enormous gap between God and the material world. The \"Logos\" was the highest of these intermediary beings, and was called by Philo \"the first-born of God\".\nPhilo also wrote that \"the Logos of the living God is the bond of everything, holding all things together and binding all the parts, and prevents them from being dissolved and separated\".\n\nPlato's Theory of Forms was located within the Logos, but the Logos also acted on behalf of God in the physical world. In particular, the Angel of the Lord in the Hebrew Bible (Old Testament) was identified with the Logos by Philo, who also said that the Logos was God's instrument in the creation of the Universe.\n\nIn Christology, the \"Logos\" () is a name or title of Jesus Christ, seen as the pre-existent second person of the Trinity. The concept derives from , which in the Douay–Rheims, King James, New International, and other versions of the Bible, reads:\nThe Koine Greek reads, \"Ἐν ἀρχῇ ἦν ὁ λόγος, καὶ ὁ λόγος ἦν πρὸς τὸν θεόν, καὶ θεὸς ἦν ὁ λόγος.(En arkhêi ên ho lógos, kaì ho lógos ên \"pròs tòn theón, kaì theòs ên ho lógos\".) The definite article is used with 'theos/god', making it 'God', and the second usage is indefinite (without an article), rendering it 'god'. Thus David Bentley Hart translates it as 'In the origin there was the Logos, and the Logos was present with GOD, and the Logos was god'.\n\nHowever, many ancient Greek scholars, including Dr. Bruce M. Metzger of Princeton (Professor of New Testament Language and Literature) explains that concepts of definite articles in English and Koine Greek are not similar: \"In the New World Translation the opening verse of the Gospel according to John is mistranslated as follows: “Originally the Word was, and the Word was with God, and the Word was a god.” A footnote which is added to the first word, “Originally,” reads, “Literally, In (At) a beginning.” By using here the indefinite article “a” the translators have overlooked the well-known fact that in Greek grammar nouns may be definite for various reasons, whether or not the Greek definite article is present. A prepositional phrase, for example, where the definite article is not expressed, can be quite definite in Greek, [in Hebrew 10:31, \"εις χειρας θεου ζωντος\" is properly rendered (even by the New World Translation) with the definite article expressed twice, “into the hands of the living God.”] as in fact it is in John 1:1\"\n\nThe word \"logos\" has been used in different senses along with \"rhema\". Both Plato and Aristotle used the term \"logos\" along with \"rhema\" to refer to sentences and propositions.\n\nThe Septuagint translation of the Hebrew Bible into Greek uses the terms \"rhema\" and \"logos\" as equivalents and uses both for the Hebrew word \"dabar\", as the Word of God.\n\nSome modern usage in Christian theology distinguishes \"rhema\" from \"logos\" (which here refers to the written scriptures) while \"rhema\" refers to the revelation received by the reader from the Holy Spirit when the Word (\"logos\") is read, although this distinction has been criticized.\n\nNeoplatonist philosophers such as Plotinus (270 AD) used the term \"Logos\" in ways that drew on Plato and the Stoics, but the term Logos was interpreted in different ways throughout Neoplatonism, and similarities to Philo's concept of Logos appear to be accidental. The Logos was a key element in the meditations of Plotinus regarded as the first Neoplatonist. Plotinus referred back to Heraclitus and as far back as Thales in interpreting Logos as the principle of meditation, existing as the interrelationship between the hypostases—the soul, the intellect (\"nous\"), and the One.\n\nPlotinus used a trinity concept that consisted of \"The One\", the \"Spirit\", and \"Soul\". The comparison with the Christian Trinity is inescapable, but for Plotinus these were not equal and \"The One\" was at the highest level, with the \"Soul\" at the lowest. For Plotinus, the relationship between the three elements of his trinity is conducted by the outpouring of Logos from the higher principle, and \"eros\" (loving) upward from the lower principle. Plotinus relied heavily on the concept of Logos, but no explicit references to Christian thought can be found in his works, although there are significant traces of them in his doctrine. Plotinus specifically avoided using the term Logos to refer to the second person of his trinity. However, Plotinus influenced Gaius Marius Victorinus, who then influenced Augustine of Hippo. Centuries later, Carl Jung acknowledged the influence of Plotinus in his writings.\n\nVictorinus differentiated between the Logos interior to God and the Logos related to the world by creation and salvation.\n\nAugustine of Hippo, often seen as the father of medieval philosophy, was also greatly influenced by Plato and is famous for his re-interpretation of Aristotle and Plato in the light of early Christian thought. A young Augustine experimented with, but failed to achieve ecstasy using the meditations of Plotinus. In his \"Confessions\", Augustine described Logos as the \"Divine Eternal Word\", by which he, in part, was able to motivate the early Christian thought throughout the Hellenized world (of which the Latin speaking West was a part) Augustine's Logos \"had taken body\" in Christ, the man in whom the \"logos\" (i.e. or ) was present as in no other man.\n\nThe concept of the \"logos\" also exists in Islam, where it was definitively articulated primarily in the writings of the classical Sunni mystics and Islamic philosophers, as well as by certain Shi'a thinkers, during the Islamic Golden Age. In Sunni Islam, the concept of the \"logos\" has been given many different names by the denomination's metaphysicians, mystics, and philosophers, including \"ʿaql\" (\"Intellect\"), \"al-insān al-kāmil\" (\"Universal Man\"), \"kalimat Allāh\" (\"Word of God\"), \"haqīqa muḥammadiyya\" (\"The Muhammadan Reality\"), and \"nūr muḥammadī\" (\"The Muhammadan Light\").\n\nOne of the names given to a concept very much like the Christian Logos by the classical Muslim metaphysicians is \"ʿaql\", which is the \"Arabic equivalent to the Greek ('intellect').\" In the writings of the Islamic Neoplatonist philosophers, such as al-Farabi () and Avicenna (d. 1037), the idea of the \"ʿaql\" was presented in a manner that both resembled \"the late Greek doctrine\" and, likewise, \"corresponded in many respects to the Logos Christology.\"\n\nThe concept of Logos in Sufism is used to relate the \"Uncreated\" (God) to the \"Created\" (humanity). In Sufism, for the Deist, no contact between man and God can be possible without the Logos. The Logos is everywhere and always the same, but its personification is \"unique\" within each region. Jesus and Muhammad are seen as the personifications of the Logos, and this is what enables them to speak in such absolute terms.\n\nOne of the boldest and most radical attempts to reformulate the Neoplatonic concepts into Sufism arose with the philosopher Ibn Arabi, who traveled widely in Spain and North Africa. His concepts were expressed in two major works \"The Ringstones of Wisdom\" (\"Fusus al-Hikam\") and \"The Meccan Illuminations\" (\"Al-Futūḥāt al-Makkiyya\"). To Ibn Arabi, every prophet corresponds to a reality which he called a Logos (Kalimah), as an aspect of the unique Divine Being. In his view the Divine Being would have for ever remained hidden, had it not been for the prophets, with Logos providing the link between man and divinity.\n\nIbn Arabi seems to have adopted his version of the Logos concept from Neoplatonic and Christian sources, although (writing in Arabic rather than Greek) he used more than twenty different terms when discussing it. For Ibn Arabi, the Logos or \"Universal Man\" was a mediating link between individual human beings and the divine essence.\n\nOther Sufi writers also show the influence of the Neoplatonic Logos. In the 15th century Abd al-Karīm al-Jīlī introduced the \"Doctrine of Logos and the Perfect Man\". For al-Jīlī, the \"perfect man\" (associated with the Logos or the Holy Prophet) has the power to assume different forms at different times and to appear in different guises.\n\nIn Ottoman Sufism, Şeyh Gâlib (d. 1799) articulates Sühan (Logos-\"Kalima\") in his \"Hüsn ü Aşk\" (\"Beauty and Love\") in parallel to Ibn Arabi's Kalima. In the romance, \"Sühan\" appears as an embodiment of Kalima as a reference to the Word of God, the Perfect Man, and the Reality of Muhammad.\n\nCarl Jung contrasted the critical and rational faculties of logos with the emotional, non-reason oriented and mythical elements of \"eros\". In Jung's approach, logos vs eros can be represented as \"science vs mysticism\", or \"reason vs imagination\" or \"conscious activity vs the unconscious\".\n\nFor Jung, logos represented the masculine principle of rationality, in contrast to its female counterpart, \"eros\":\n\nJung attempted to equate logos and eros, his intuitive conceptions of masculine and feminine consciousness, with the alchemical Sol and Luna. Jung commented that in a man the lunar anima and in a woman the solar animus has the greatest influence on consciousness. Jung often proceeded to analyze situations in terms of \"paired opposites\", e.g. by using the analogy with the eastern yin and yang and was also influenced by the Neoplatonists.\n\nIn his book \"Mysterium Coniunctionis\" Jung made some important final remarks about anima and animus:\n\nIn so far as the spirit is also a kind of \"window on eternity\"... it conveys to the soul a certain influx divinus... and the knowledge of a higher system of the world, wherein consists precisely its supposed animation of the soul.\n\nAnd in this book Jung again emphasized that the animus compensates eros, while the anima compensates logos.\n\n"}
{"id": "1013868", "url": "https://en.wikipedia.org/wiki?curid=1013868", "title": "Lost in Translation (novel)", "text": "Lost in Translation (novel)\n\nLost in Translation is a novel written by Nicole Mones, published by Bantam Dell in 1999. It is the story of an American woman trying to lose her past by living as a translator in China. Emotionally charged and erotic, this widely translated bestseller has been universally praised for its authoritative portrayal of a China rarely captured in contemporary fiction. The novel’s accolades include the Kafka Prize for the year’s best work of fiction by any American woman, the Pacific Northwest Bookseller’s Association Book Award for the year’s best novel from the five northwestern states, and the New York Times Book Review’s \"Notable Book\" and \"Editor’s Choice\".\n\nExpatriate translator Alice Mannegan spends her nights in Beijing’s smoky bars, seeking fleeting encounters with Chinese men to blot out the shame of her racist father back in Texas. But when she signs on to an archaeological expedition searching for the missing bones of Peking Man in China’s remote Northwest deserts, her world cracks open. As the group follows the trail of the Jesuit philosopher/paleontologist Teilhard de Chardin to close in on one of archaeology’s greatest mysteries, Alice finds herself increasingly drawn to a Chinese professor who is shackled by his own painful memories. Love in all its forms–human, sexual, divine, between a nation and its history, a man and his past, a father and his daughter–drives the story to its breathtaking finish.\n\n\nIn a blog interview discussing her works, Mones described Alice, the main character, as \"one of the few characters who really seemed to write herself\". Mones also recalls her past working in China as an inspiration for Alice.\n\n\"When I was young, I was close to a young man whose father was suspected of having committed a racially motivated murder during the Civil Rights movement in the U.S. in the early 1960s. The burden of second-hand guilt lay heavy on my friend, and he moved to Hong Kong and refused to return to America. Then, he was killed in an accident. Looking back, I think I started writing the novel a few years after his death in order to play out his story and give it an ending.\"\n\n"}
{"id": "25249677", "url": "https://en.wikipedia.org/wiki?curid=25249677", "title": "Marcion (software)", "text": "Marcion (software)\n\nMarcion is Coptic–English/Czech dictionary related to Crum's coptic dictionary, written in C++, based on MySQL, with Qt GUI. Contains many coptic texts, grammars, Greek texts, Liddell–Scott Greek–English lexicon, and others. Can be used as a bible study tool. Marcion is free software released under the GNU GPL.\n"}
{"id": "2084219", "url": "https://en.wikipedia.org/wiki?curid=2084219", "title": "Master of Professional Writing Program", "text": "Master of Professional Writing Program\n\nThe Master of Professional Writing Program is a graduate degree program in professional writing. Chatham University in Pennsylvania has an online MPW program. The University of Southern California's MPW program will be closing in May 2016.\n\n\n\n"}
{"id": "34384656", "url": "https://en.wikipedia.org/wiki?curid=34384656", "title": "Metafunction", "text": "Metafunction\n\nThe term \"metafunction\" originates in systemic functional linguistics and is considered to be a property of all languages. Systemic functional linguistics is functional and semantic rather than formal and syntactic in its orientation. As a functional linguistic theory, it claims that both the emergence of grammar and the particular forms that grammars take should be explained “in terms of the functions that language evolved to serve”. While languages vary in how and what they do, and what humans do with them in the contexts of human cultural practice, all languages are considered to be shaped and organised in relation to three functions, or metafunctions. Michael Halliday, the founder of systemic functional linguistics, calls these three functions the \"ideational\", \"interpersonal\", and \"textual\". The ideational function is further divided into the \"experiential\" and \"logical\".\n\nMetafunctions are \"systemic clusters\"; that is, they are groups of semantic systems that make meanings of a related kind. The three metafunctions are mapped onto the structure of the clause. For this reason, systemic linguists analyse a clause from three perspectives. Halliday argues that the concept of metafunction is one of a small set of principles that are necessary to explain how language works; this concept of function in language is necessary to explain the organisation of the semantic system of language. Function is considered to be \"a fundamental property of language itself\".\n\nAccording to Ruqaiya Hasan, the metafunctions in SFL \"are not hierarchised; they have equal status, and each is manifested in every act of language use: in fact, an important task for grammatics is to describe how the three metafunctions are woven together into the same linguistic unit\". Hasan argues that this is one way in which Halliday's account of the functions of language is different from that of Karl Bühler, for example, for whom functions of language are hierarchically ordered, with the referential function the most important of all. For Buhler, the functions were considered to operate one at a time. In SFL, the metafunctions operate simultaneously, and any utterance is a harmony of choices across all three functions.\n\nThe ideational function is language concerned with building and maintaining a theory of experience. It includes the experiential function and the logical function.\n\nThe experiential function refers to the grammatical choices that enable speakers to make meanings about the world around us and inside us:\n\nHalliday argues that it was through this process of humans making meaning from experience that language evolved. Thus, the human species had to “make sense of the complex world in which it evolved: to classify, or group into categories, the objects and events within its awareness”. These categories are not given to us through our senses; they have to be “construed”. In taking this position on the active role of grammar in construing “reality”, Halliday was influenced by Whorf.\n\nHalliday describes the logical function as those systems “which set up logical–semantic relationships between one clausal unit and another” The systems which come under the logical function are and . When two clauses are combined, a speaker chooses whether to give both clauses equal status, or to make one dependent on the other. In addition, a speaker choose some meaning relation in the process of joining or binding clauses together. Halliday argues that the meanings we make in such processes are most closely related to the experiential function. For this reason, he puts the experiential and logical functions together into the ideational function.\n\nThe interpersonal function refers to the grammatical choices that enable speakers to enact their complex and diverse interpersonal relations. This tenet of systemic functional linguistics is based on the claim that a speaker not only talks about something, but is always talking to and with others. Language not only construes experience, but simultaneously acts out “the interpersonal encounters that are essential to our survival”. Halliday argues that these encounters:\n\nThe grammatical systems that relate to the interpersonal function include Mood, Modality, and Polarity.\n\nHalliday argues that both experiential and interpersonal functions are intricately organized, but that between the two “there is comparatively very little constraint”. This means that “by and large, you can put any interactional ‘spin’ on any representational content”. What allows meanings from these two modes to freely combine is the intercession of a third, distinct mode of meaning that Halliday refers to as the textual function. The term encompasses all of the grammatical systems responsible for managing the flow of discourse. These systems “create coherent text – text that coheres within itself and with the context of situation” They are both structural (involving choices relating to the ordering of elements in the clause), and non-structural (involving choices that create cohesive ties between units that have no structural bond). The relevant grammatical systems include Theme, Given and New, as well as the systems of cohesion, such as Reference, Substitution, and Ellipsis. Halliday argues that the textual function is distinct from both the experiential and interpersonal because its object is language itself. Through the textual function, language “creates a semiotic world of its own: a parallel universe, or ‘virtual reality’ in modern terms”.\n"}
{"id": "871470", "url": "https://en.wikipedia.org/wiki?curid=871470", "title": "Mutual intelligibility", "text": "Mutual intelligibility\n\nIn linguistics, mutual intelligibility is a relationship between languages or dialects in which speakers of different but related varieties can readily understand each other without prior familiarity or special effort. It is sometimes used as an important criterion for distinguishing languages from dialects, although sociolinguistic factors are often also used.\n\nIntelligibility between languages can be asymmetric, with speakers of one understanding more of the other than speakers of the other understanding the first. When it is relatively symmetric, it is characterized as \"mutual\". It exists in differing degrees among many related or geographically proximate languages of the world, often in the context of a dialect continuum.\n\nLinguistic distance is the name for the concept of calculating a measurement for how different languages are from one another. The higher the linguistic distance, the lower the mutual intelligibility. One common metric used is the Levenshtein distance.\n\nFor individuals to achieve moderate proficiency or understanding in a language (called L2) other than their first language (L1) typically requires considerable time and effort through study and/or practical application. However, many groups of languages are partly mutually intelligible, i.e. most speakers of one language find it relatively easy to achieve some degree of understanding in the related language(s). Often the languages are genetically related, and they are likely to be similar to each other in grammar, vocabulary, pronunciation, or other features.\n\nIntelligibility among languages can vary between individuals or groups within a language population according to their knowledge of various registers and vocabulary in their own language, their exposure to additional related languages, their interest in or familiarity with other cultures, the domain of discussion, psycho-cognitive traits, the mode of language used (written vs. oral), and other factors.\n\nThere is no formal distinction between two distinct languages and two varieties of a single language, but some linguists use mutual intelligibility as one of the primary factors in deciding between the two cases.\n\nSome linguists claim that mutual intelligibility is, ideally at least, the primary criterion separating languages from dialects. On the other hand, speakers of closely related languages can often communicate with each other; thus there are varying degrees of mutual intelligibility, and often other criteria are also used. As an example, in the case of a linear dialect continuum that shades gradually between varieties, where speakers near the center can understand the varieties at both ends, but speakers at one end cannot understand the speakers at the other end, the entire chain is often considered a single language. If the central varieties then die out and only the varieties at both ends survive, they may then be reclassified as two languages, even though no actual language change has occurred.\n\nIn addition, political and social conventions often override considerations of mutual intelligibility. For example, the varieties of Chinese are often considered a single language even though there is usually no mutual intelligibility between geographically separated varieties. Another similar example would be varieties of Arabic. In contrast, there is often significant intelligibility between different Scandinavian languages, but as each of them has its own standard form, they are classified as separate languages. There is also significant intelligibility between Thai languages of different regions of Thailand.\n\nTo deal with the conflict in cases such as Arabic, Chinese and German, the term \"Dachsprache\" (a sociolinguistic \"umbrella language\") is sometimes seen: Chinese and German are languages in the sociolinguistic sense even though some speakers cannot understand each other without recourse to a standard or prestige form.\n\nAsymmetric intelligibility refers to two languages that are considered partially mutually intelligible, but where one group of speakers has more difficulty understanding the other language than the other way around. There can be various reasons for this. If, for example, one language is related to another but has simplified its grammar, the speakers of the original language may understand the simplified language, but less vice versa. For example, Dutch speakers tend to find it easier to understand Afrikaans than vice versa as a result of Afrikaans's simplified grammar.\n\nPerhaps the most common reason for apparent asymmetric intelligibility is that speakers of one variety have more exposure to the other than vice versa. For example, speakers of Scottish English have frequent exposure to standard American English through movies and TV programs, whereas speakers of American English have little exposure to Scottish English; hence, American English speakers often find it difficult to understand Scottish English or, especially, Scots (which differs significantly from standard Scottish English), whereas Scots tend to have few problems understanding standard American English.\n\nNorthern Germanic languages spoken in Scandinavia form a dialect continuum where two furthermost dialects have almost no mutual intelligibility. As such, spoken Danish and Swedish normally have low mutual intelligibility, but Swedes in the Öresund region (including Malmö and Helsingborg), across a strait from the Danish capital Copenhagen, understand Danish somewhat better, largely due to the proximity of the region to Danish-speaking areas (see \"Mutual intelligibility in North Germanic languages\"). While Norway was under Danish rule, the Bokmål written standard of Norwegian originates from Dano-Norwegian, a koiné that evolved among the urban elite in Norwegian cities during the later years of the union. Additionally, Norwegian assimilated a considerable amount of Danish vocabulary as well as traditional Danish expressions. As a consequence, spoken mutual intelligibility is not reciprocal.\n\nSimilarly, in Germany and Italy, standard German or Italian speakers may have great difficulty understanding the \"dialects\" from regions other than their own, but virtually all \"dialect\" speakers learn the standard languages in school and from the media.\n\nBelow is an \"incomplete\" list of fully and partially mutually intelligible varieties sometimes considered languages.\n\n\n\n\n\n\nBecause of the difficulty of imposing boundaries on a continuum, various counts of the Romance languages are given; in \"The Linguasphere register of the world’s languages and speech communities\" David Dalby lists 23 based on mutual intelligibility:\n\n\n\n"}
{"id": "38551934", "url": "https://en.wikipedia.org/wiki?curid=38551934", "title": "Niwer Mil language", "text": "Niwer Mil language\n\nThe Niwer Mil language is spoken by 9,033 people on Boang Island, Malendok Island, Lif Island and Tefa Island in the Tanga Islands, Namatanai District of New Ireland Province in Papua New Guinea. It was split from the Tangga language in 2013. It is one of the languages that form the St George linkage group of Meso-Melanesian languages.\n"}
{"id": "15367144", "url": "https://en.wikipedia.org/wiki?curid=15367144", "title": "Old Hindi", "text": "Old Hindi\n\nOld Hindi () was the earliest stage of the Khariboli dialect of the Hindi language, and so the ancestor of Modern Standard Hindi. It was spoken by the peoples of the Hindi belt, especially around Delhi, in roughly the 13th–15th centuries. It is attested in only a handful of literature, including some works by the poet Amir Khusrau and some verses by the Sufi saint Baba Farid in the \"Adi Granth\". The works of Kabir also may be included, as they use a Khariboli-like dialect.\n\nThe language ultimately gave way to Hindustani.\n"}
{"id": "976531", "url": "https://en.wikipedia.org/wiki?curid=976531", "title": "Outline (list)", "text": "Outline (list)\n\nAn outline, also called a hierarchical outline, is a list arranged to show hierarchical relationships and is a type of tree structure. An outline is used to present the main points (in sentences) or topics (terms) of a given subject. Each item in an outline may be divided into additional sub-items. If an organizational level in an outline is to be sub-divided, it shall have at least two subcategories, as advised by major style manuals in current use. An outline may be used as a drafting tool of a document, or as a summary of the content of a document or of the knowledge in an entire field. It is not to be confused with the general context of the term \"outline\", which a summary or overview of a subject, presented verbally or written in prose (for example, \"The Outline of History\" is not an outline of the type presented below). The outlines described in this article are lists, and come in several varieties.\n\nA sentence outline is a tool for composing a document, such as an essay, a paper, a book, or even an encyclopedia. It is a list used to organize the facts or points to be covered, and their order of presentation, by section. Topic outlines list the subtopics of a subject, arranged in levels, and while they can be used to plan a composition, they are most often used as a summary, such as in the form of a table of contents or the topic list in a college course's syllabus.\n\nOutlines are further differentiated by the index prefixing used, or lack thereof. Many outlines include a numerical or alphanumerical prefix preceding each entry in the outline, to provide a specific path for each item, to aid in referring to and discussing the entries listed. An alphanumerical outline uses alternating letters and numbers to identify entries. A decimal outline uses only numbers as prefixes. An outline without prefixes is called a \"bare outline\".\n\nSpecialized applications of outlines also exist. A reverse outline is a list of sentences or topics that is created from an existing work, as a revision tool; it may show the gaps in the document's coverage so that they may be filled, and may help in rearranging sentences or topics to improve the structure and flow of the work. An integrated outline is a composition tool for writing scholastic works, in which the sources, and the writer's notes from the sources, are integrated into the outline for ease of reference during the writing process.\n\nA software program designed for processing outlines is called an outliner.\n\nOutlines are differentiated by style, the inclusion of prefixes, and specialized purpose. There are also hand-written outlines (which are highly limited in utility), and digitized outlines, such as those contained within an outliner (which are much more useful).\n\nThere are two main styles of outline: sentence outlines and topic outlines.\n\nPropædia is the historical attempt of the Encyclopædia Britannica to present a hierarchical \"Outline of Knowledge\" in a separate volume in the 15th edition of 1974. The \"Outline of Knowledge\" was a project by Mortimer Adler. Propædia had three levels, 10 \"Parts\" at the top level, 41 \"Divisions\" at the middle level and 167 \"Sections\" at the bottom level, numbered, for example, \"1. Matter and Energy\", \"1.1 Atoms\", \"1.1.1. Structure and Properties of Atoms\".\n\nA feature included in many outlines is prefixing. Similar to section numbers, an outline prefix is a label (usually alphanumeric or numeric) placed at the beginning of an outline entry to assist in referring to it.\n\nBare outlines include no prefixes.\n\nAn \"alphanumeric outline\" includes a prefix at the beginning of each topic as a reference aid. The prefix is in the form of Roman numerals for the top level, upper-case letters (in the alphabet of the language being used) for the next level, Arabic numerals for the next level, and then lowercase letters for the next level. For further levels, the order is started over again. Each numeral or letter is followed by a period, and each item is capitalized, as in the following sample:\n\nSome call the Roman numerals \"A-heads\" (for \"A-level headings\"), the upper-case letters, \"B-heads\", and so on. Some writers also prefer to insert a blank line between the A-heads and B-heads, while often keeping the B-heads and C-heads together.\n\nIf more levels of outline are needed, lower-case Roman numerals and numbers and lower-case letters, sometimes with single and double parenthesis can be used, although the exact order is not well defined, and usage varies widely.\n\nThe scheme recommended by the \"MLA Handbook\", and the \"Purdue Online Writing Lab\", among others, uses the usual five levels, as described above, then repeats the Arabic numerals and lower-case letter surrounded by parentheses (round brackets) – I. A. 1. a. i. (1) (a) – and does not specify any lower levels, though \"(i)\" is usually next. In common practice, lower levels yet are usually Arabic numerals and lowercase letters again, and sometimes lower-case Roman again, with single parentheses – 1) a) i) – but usage varies. MLA style is sometimes incorrectly referred to as APA style, but the \"APA Publication Manual\" does not address outline formatting at all.\n\nA very different style recommended by \"The Chicago Manual of Style\", based on the practice of the United States Congress in drafting legislation, suggests the following sequence, from the top to the seventh level (the only ones specified): I. A. 1. \"a\") (1) (\"a\") \"i\") – capital Roman numerals with a period, capital letters with a period, Arabic numerals with a period, italic lowercase letters with a single parenthesis, Arabic numerals with a double parenthesis, italic lowercase letters with a double parenthesis, and italic lowercase Roman numerals with a single parentheses, though the italics are not required). Because of its use in the US Code and other US law books, Many American lawyers consequently use this outline format.\n\nAnother alternative scheme repeats all five levels with a single parenthesis for the second five – I) A) 1) a) i) – and then again with a double parenthesis for the third five – (I) (A) (1) (a) (i).\n\nMany oft-cited style guides besides the \"APA Publication Manual\", including the \"AP Stylebook\", the \"NYT Manual\", Fowler, \"The Guardian\" Style Guide, and Strunk & White, are curiously silent on the topic.\n\nOne side effect of the use of both Roman numerals and uppercase letters in all of these styles of outlining is that in most alphabets, \"I.\" may be an item at both the top (A-head) and second (B-head) levels. This is usually not problematic because lower level items are usually referred to hierarchically. For example, the third sub-sub-item of the fourth sub-item of the second item is item II. D. 3. So, the ninth sub-item (letter-I) of the first item (Roman-I) is item I. I., and only the top level one is item I.\n\nThe \"decimal outline\" format has the advantage of showing how every item at every level relates to the whole, as shown in the following sample outline:\n\nSpecial types of outlines include reverse outlines and integrated outlines.\n\nA \"reverse outline\" is an outline made from an existing work. Reverse outlining is like reverse engineering a document. The points or topics are extracted from the work, and are arranged in their order of presentation, by section, in the outline. Once completed, the outline can be filled in and rearranged as a plan for a new improved version of the document.\n\nAn \"integrated outline\" is a helpful step in the process of organizing and writing a scholarly paper (literature review, research paper, thesis or dissertation). When completed the integrated outline contains the relevant scholarly sources (author's last name, publication year, page number if quote) for each section in the outline. An integrated outline is generally prepared after the scholar has collected, read and mastered the literature that will be used in the research paper. Shields and Rangarajan (2013) recommend that new scholars develop a system to do this. Part of the system should contain a systematic way to take notes on the scholarly sources. These notes can then be tied to the paper through the integrated outline. This way the scholar reviews all of the literature before the writing begins.\n\nAn integrated outline can be a helpful tool for people with writer's block because the content of the paper is organized and identified prior to writing. The structure and content is combined and the author can write a small section at a time. The process is less overwhelming because it can be separated into manageable chunks. The first draft can be written using smaller blocks of time.\n\nFor a comparison, see Outliners, below.\n\nOutlines are used for composition, summarization, and as a development and storage medium.\n\n\"Merriam-Webster's manual for writers and editors\" (1998, p. 290) recommends that the section headings of an article should when read in isolation, combine to form an outline of the article content. Garson (2002) distinguishes a 'standard outline', presented as a regular table of contents from a refined tree-like 'hierarchical outline', stating that \"such an outline might be appropriate, for instance, when the purpose is taxonomic (placing observed phenomena into an exhaustive set of categories). ... hierarchical outlines are rare in quantitative writing, and the researcher is well advised to stick to the standard outline unless there are compelling reasons not to.\"\n\nWriters of fiction and creative nonfiction, such as Jon Franklin, may use outlines to establish plot sequence, character development and dramatic flow of a story, sometimes in conjunction with free writing.\n\nPreparation of an outline is an intermediate step in the process of writing a scholarly research paper, literature review, thesis or dissertation. A special kind of outline (integrated outline) incorporates scholarly sources into the outline before the writing begins.\n\nIn addition to being used as a composition tool during the drafting process, outlines can also be used as a publishing format. Outlines can presented as work's table of contents, but they can also be used as the body of a work. The \"Outline of Knowledge\" from the 15th edition of the \"Encyclopedia Britannica\" is an example of this. Wikipedia includes outlines that summarize subjects (for example, see Outline of chess, Outline of Mars, and Outline of knowledge).\n\nProfessors often hand out to their students at the beginning of a term, a summary of the subjects to be covered throughout the course in the form of a topic outline. It may also be included as part of a larger course synopsis.\n\nOutlines are also used to summarize talking points for a speech or lecture.\n\nOutlines, especially those used within an outliner, can be used for planning, scheduling, and recording.\n\nAn outliner (or \"outline processor\") is a specialized type of word processor used to view, create, build, modify, and maintain outlines. It is a computer program, or part of one, used for displaying, organizing, and editing hierarchically arranged text in an outline's tree structure. Textual information is contained in discrete sections called \"nodes\", which are arranged according to their topic-subtopic (parent-child) relationships, sort of like the members of a family tree. When loaded into an outliner, an outline may be collapsed or expanded to display as few or as many levels as desired.\n\nOutliners are used for storing and retrieving textual information, with terms, phrases, sentences, or paragraphs attached to a tree. So rather than being arranged by document, information is arranged by topic or content. An outline in an outliner may contain as many topics as desired. This eliminates the need to have separate documents, as outlines easily include other outlines just by adding to the tree.\n\nThe main difference between a hand-written outline and a digital one, is that the former is usually limited to a summary or blueprint of a planned document, while the latter may easily include all of the content of the entire document and many more. In other words, as a hand-written work an outline is a writing tool, but on a computer, it is a general purpose format supported by a robust development and display medium capable of handling knowledge from its creation to its end use.\n\nOutliners may be used in content creation instead of general word processors for capturing, organizing, editing, and displaying knowledge or general textual information. Outliners are ideal for managing lists, organizing facts and ideas, and for writing computer programs. They are also used for goal and task management (including personal information management and project management), and for writing books and movie scripts.\n\nThe graphical counterpart to outliners are mind mappers.\n\n\n"}
{"id": "18356461", "url": "https://en.wikipedia.org/wiki?curid=18356461", "title": "Production support", "text": "Production support\n\nProduction support is the practices and disciplines of supporting the IT systems/applications which are currently being used by the end users. A production support person/team is responsible for receiving incidents and requests from end-users, analyzing these and either responding to the end user with a solution or escalating it to the other IT teams. These teams may include developers, system engineers and database administrators.\n\nIn order to understand the importance of production support, one needs to take a few factors into account.\n\nFrom the factors listed above, one can see that the way in which production support is managed is extremely crucial.\n\nThe major steps for Production Support are as below. These Production Support steps are in context of the Batch processing.\n\nUsually a batch job or group of related batch jobs (schedule/stream) runs to accomplish one or more business functions. These batch jobs run unattended and normally complete without any errors or issues. However, sometimes the batch job can have a break/interruption/abend/abort. There could be several reasons why a job could abend.\n\nWhen a job abends, it can send out an automated alert notification via e-mail, page, text. Also, data center or operations team is also actively monitoring the jobs. They also send alert notification using e-mail, page, text or they can call the on call person responsible for the recovery of the abended job.\n\nThe on call person acknowledges the e-mail, page, text or phone call for the abended job. The on call person also records the abended job details in a production issue tracking system. Sometimes, the abended job automatically records the job abend details along with job standard list (job log) in a production issue tracking system. The abended job details (job standard list, error log files, etc.) are available in the production job scheduler tool. The Production issue tracking tool creates a request number and this request number is given to the support team. This request number is used to track the progress of the production support issue. The request is assigned to on call support team person.\n\nFor critical Production Errors (e.g. Production job is in critical path and is likely to delay the batch completion SLAs and if the Production error is impacting business data), an e-mail is sent to entire organization or impacted teams so that they are aware of the issue. They are also provided with the estimated time for Production error recovery.\n\nThe Production support team on call person collects all the necessary information about the Production error. This information is then recorded in the Production error tracking tool using the correct support request number previously assigned. All the details such as data, environment, process, program logic that failed is used in the investigation. Production batch job, program used or any tool/utility used is reviewed for any possible errors.\n\nIf similar Production error occurred in the past then the issue resolution steps are retrieved from the support knowledge base and error is resolved using those steps. If it is a new Production error then new Production error resolution steps are created and Production error is resolved. The new Production error resolution steps are recorded in the knowledge base for the future usage. For major Production errors (critical infrastructure or application failures), a phone conference call is initiated and all required support persons/teams join the call and they all work together to resolve the error. This is also called as an Incident Management. If a problem occurs repeatedly then it is recorded and tracked using appropriate tools and processes until it is resolved permanently. This is also called as Problem Management. The issue is closed only after the customer or end user agrees that the problem is resolved.\n\nIf the Production error occurred due to programming errors then a request is created for the Development team to correct programming errors. Problem is identified, defined and root cause analysis is performed. The programming error is fixed using normal SDLC process - analysis/design/programming/QA/testing/release. The new version of the Production job/program is deployed and verified/validated.\n\nIf the Production error occurred due to job/schedule dependency issues or sequence issues then further analysis is done to find the correct sequence/dependencies. The new sequence/dependencies are verified and validated in test environment before Production deployment.\n\nIf the Production error occurred due to infrastructure issues then the specific infrastructure team is notified. The infrastructure team then implements permanent fix for the issue and monitors the infrastructure to avoid same error again.\n\nIf the Production error occurred due to unexpected consequences of infrastructure changes then most often the infrastructure team is not able to bill the time spent in resolving of the issue at the full rate. In some cases hours are completely disqualified from being billed.\n\nThe Production error tracking system is used to review all issues periodically (daily, weekly and monthly) and reports are generated to monitor resolved issues, repeating issues, pending issues. Reports are also generated for the IT/IS management for improvement and management of Production jobs.\n\n\n"}
{"id": "2938373", "url": "https://en.wikipedia.org/wiki?curid=2938373", "title": "Proxy statement", "text": "Proxy statement\n\nA proxy statement is a statement required of a firm when soliciting shareholder votes. This statement is filed in advance of the annual meeting. The firm needs to file a proxy statement, otherwise known as a Form DEF 14A (Definitive Proxy Statement), with the U.S. Securities and Exchange Commission. This statement is useful in assessing how management is paid and potential conflict-of-interest issues with auditors. The statement includes:\n\nSEC proxy rules: The term \"proxy statement\" means the statement required by Section 240.14a-3(a) whether or not contained in a single document.\n\nIn many cases, shareholder votes—particularly institutional shareholder votes—are determined by proxy firms which advise the shareholders...\n\nTraditionally, broker-dealers have been permitted to vote for \"routine\" proposals on behalf of their shareholders if the shareholders do not return the proxy statement. This has been controversial, and in 2006 the NYSE Proxy Working Group recommended that the rules be modified so that uncontested director elections were not considered routine. The SEC approved the rule on July 1, 2009.\n\nIn July 2010, the SEC announced that it was seeking public comment on the efficiency of the proxy system.\n\nThere has been some controversy over \"proxy access\" which is a method to allow shareholders to nominate candidates which appear on the proxy statement. Currently, only the nominating board can place candidates on the proxy statement. The United States Dodd–Frank Wall Street Reform and Consumer Protection Act specifically allowed the SEC to rule on this issue. In 2010, the SEC passed a rule which allowed certain shareholders to place candidates on the proxy statement; however, the rule was struck down by the United States Court of Appeals for the District of Columbia Circuit in 2011.\n\n"}
{"id": "31614392", "url": "https://en.wikipedia.org/wiki?curid=31614392", "title": "Reallocation (media)", "text": "Reallocation (media)\n\nReallocation is a term in the media industry used to describe the practice of relocating an unsuccessful series that was originally developed for a broadcast network onto a cable network in hopes of gaining the attention and interest of a niche audience as well as growing a larger audience.\n\nReasons for reallocation are not always due to cancellation. Reallocation is also used to regain expenses lost due to production fees on under performing content. In some cases reallocation is used in order to promote an unsuccessful series.\n\nAlthough not a common practice,\n\n Lotz, Amanda D. (2007) \"The Television Will Be Revolutionized\". New York, NY: New York University Press. p. 126-128\n"}
{"id": "25252953", "url": "https://en.wikipedia.org/wiki?curid=25252953", "title": "Shinmura Izuru Prize", "text": "Shinmura Izuru Prize\n\nThe is an award presented by the for contributions to linguistics. The prize has been awarded since 1982 and is named after Shinmura Izuru, known for his many contributions to Japanese linguistics and lexicography.\n\n\n"}
{"id": "32481043", "url": "https://en.wikipedia.org/wiki?curid=32481043", "title": "Skill-based theories of second-language acquisition", "text": "Skill-based theories of second-language acquisition\n\nSkill-based theories of second-language acquisition are theories of second-language acquisition based on models of skill acquisition in cognitive psychology. These theories conceive of second-language acquisition as being learned in the same way as any other skill, such as learning to drive a car or play the piano. That is, they see practice as the key ingredient of language acquisition. The most well-known of these theories is based on John Anderson's adaptive control of thought model.\n\nThe adaptive control of thought model assumes a distinction between declarative knowledge, knowledge that is conscious and consists of facts, and procedural knowledge, knowledge of how an activity is done. In this model, skill acquisition is seen as a progression from declarative to procedural knowledge. Adaptive control of thought is a general model of cognition, and second-language acquisition is just one application of a wide area of research in cognitive psychology. Second-language acquisition is seen as a progression through three stages, declarative, procedural, and autonomous.\n\n"}
{"id": "34432421", "url": "https://en.wikipedia.org/wiki?curid=34432421", "title": "Social Media Language Learning", "text": "Social Media Language Learning\n\nSocial Media Language Learning (SMLL) links interactive social media channels to language learning. This enables students to develop communication and language skills. Social media consist of interactive forms of media that allow users to interact with and publish to each other, generally by means of the internet. Daily observations and recent scholarly traditions suggest that a certain amount of learning takes place beyond the confines of the individual mind. Research has shown that language acquisition and learning is socially constructed and interactive in nature. According to the theory of language socialization, language learning is interwoven with cultural interaction and \"mediated by linguistic and other symbolic activity\". From this perspective, the use of technologies that facilitate communication and connection, particularly social media applications and programs, makes a lot of sense. Language learners are able to enhance their language skills due to the different avenues in which new social media have created. Social media provides the learner with the possibility of participating in actual, real-time, relevant conversations taking place online, and practicing the target language with or without the help of an experienced teacher by his or her side.\n\nThe Social Media Language Learning (SMLL) method was originally created by a Barcelona, Spain, based company called Idiomplus. It consists of applying interactive social media channels to language learning, which will in turn enable the student to develop communication skills while using these social networks.\n\nThe method provides the learner with the possibility of participating in actual, real-time, relevant conversations taking place online, and practicing the target language with the help of an experienced teacher by his or her side.\n\nThe Communicative Language Teaching (CLT) method provided the basis for the development of the SMLL method, given both emphasise the importance of teaching within a great scope of contexts with the objective of developing a functional knowledge of the language. Perfect grammar and pronunciation are not essential to the process, rather setting the focus on the communicative competence of the student and the ability to understand and make him/herself understood.\n\nThe Social Media Language Learning is based upon three tenets:\n\n\nThe student is therefore invited to emerge as much as possible in activities which require the use of language, given that all of them will result in learning. In-class and out-of-class communication are equally important. It combines the benefits of another method, known as blended learning, which allows the student to learn autonomously, whenever and wherever he wants, with all the required material available online, and at the same time have the support of an experienced teacher who eases the process and provides a professional and live explanation of the subjects at hand.\n\nOn-site classes with the teacher are intertwined with the ongoing online conversations with other relevant people. Learning is considered to be a constant, ever-flowing, indivisible part of everyday life, thus making the target language a part of it.\n\nA range of social media can be used to facilitate language learning, including blogs, online role-playing games, fan fiction writing, and so on.\n\nWeb logs or blogs are one of many forms of social media. A blog is an online platform that people can use to express issues related to their lives and different viewpoints they may have. Blogs address a wide range of topics and are used in many different ways for diverse purposes. As blogging architecture became more user-friendly the popularity of blogs increased.\n\nBlogs can be used for language learning. The ability to comment on other peoples blogs as well as have people post comments on your own is key to blogging. The development of a language is not normally the primary goal of the person who is participating in a blog but a blog is a place that can provide a foundation for reflecting on the language that is being written within it. Blogs can increase a users language competence as well \"blogs are egalitarian learning and teaching tools par excellence\". When writing in a blog it is done to express feelings and to be creative but as a result language learning can be achieved.\n\nSteven Thorne has reported a number of trends with respect to blogging and language learning. Blogs used within language classrooms he has observed produce encouraging written production along with increased scores on standardized assessment measures showing significant language development. The use of blogs also enable teachers and instructors to assess written language learning in a relatively accessible way. Blogs, it is argued, allow students to create fluent sets of sentences from sentences that previously would stand alone. It also allows students to write in paragraphs and to use different tenses with more confidence to discuss topics within their blogs.\n\nHui-Ju Wu and Pai-Lu Wu finds trends with blogging and language learning. Blogs help with language learning. Blogs help to develop vocabulary, increase reading speed, develop proper use of grammar and enhanced reading comprehension. Blogs produced better sentence fluency, a higher vocabulary, better sentence fluency and an awareness of looking for grammar mistakes in their writing. Blogs can give the confidence to write more sentences and to use different and more tenses.\n\nA growing body of research is documenting the ways in which primarily young people are learning languages via their social media, on their own, outside of formal language learning classes or programs. Social media studied include: online role-playing games, fan fiction writing, instant messaging, fan websites, virtual worlds, chat, and the like.\n\nMany MMO (massively multiplayer online) games cross national and linguistic boundaries. They often have built-in chat functions and enable participants to chat with players from all over the world who speak various languages. This can provide players with the opportunity to learn a new language—at least at a basic level—so they can participate more fully in the game with the other players. The social interaction these players engage in while playing the game helps with their understanding of the grammar constructions and conversational ways of the language they are using. In short, gamers may well be using a language they are learning much sooner than they otherwise might, given the highly contetxualised nature of the talk typically found within game-play situations and the text-based medium of interaction. Without this social interaction, many students may be less willing to practice their new skills which would enhance their abilities in the language they are learning. Rankin, Morrison, Mckenzie, McNeal, Gooch and Shute found found that English as a second Language (ESL) students were able to acquire more language skills through the social communication that they had with the native speakers of English.\n\nLearning language from video games, it is argued, is a contextual process. Gamers playing a game that has text and audio in a language other than their own (e.g., an English-speaking player playing a Japanese game) can draw on the context of the game to help them understand what is being said or written within the game. Many video games use repetition in their commands and this enables the player to recognize these words and come to understand what they mean and represent over time. Games transform the learning process from a passive task to one in which individuals engage actively in the experience of learning by focussing first on meaning. Computer games, researchers' argue, supply authentic environments for language learning, complete with ample opportunities for students to develop and test their emerging target language knowledge.\n\nIt is important to note that not all people who play these MMOs or video games necessarily start out wanting to learn a new language. Individuals playing MMOs typically want to be able to maintain social relationships with people who speak another language.\n\nIn Wan Shun Eva Lam's research, she studies how forms of social networking in electronic media have provided alternative contexts of language development for young immigrants in the US. As a result, the two students' experiences with English in an Internet chat room can be seen as a process of language socialization through which they acquired a particular linguistic variety of English to construct ethnic identifications with other young people of Chinese descent around the world. Lam explains that by studying closely how people navigate across contexts of socialization in the locality of the nation-state and the virtual environments of the Internet, we may discover how practices of English in the global sphere articulate with local practices of English in constituting the identities and life trajectories of people. Through such studies, the different kinds of constraints as well as opportunities are made evident through what the internet offers language learners in various forms.\n\nOnline interests groups influence language learning. According to Thorne, Black and Sykes's research, participating in Internet interest communities has the potential to propel language learners beyond the confines of the institutional identity. Participation in these semiotically mediated communities may help to strengthen the ecological linkages between forms of language use and identity dispositions.\n\nA number of social networking sites help individuals engage in language learning and interest. Facebook, for example, allows language barriers to be broken down. Livemocha is a social networking site in which allows members to communicate and learn language skills. According to the site, it has approximately 12 million registered members from 196 countries around the globe (although this includes members who have signed up for 1 day and never returned). Over 400,000 users visit the site daily.\n\nThe use of social media tools in language education is important because it creates a community for language teachers. The difference between social media as professional development tools and other professional development tools is that social media gives the teachers a community to participate with. The US Department of Education stresses that teachers should not only be connected to resources but they should have communities of practice that provide career-long personal learning opportunities for educators within and across schools, pre-service preparation and in-service education institutions, and professional organizations (Office of Educational Technology, 2010). Communities of practice are \"groups of people informally bound together by shared expertise and passion for a joint enterprise\" (Wenger & Snyder, 2000, p. 139). The social media tools help the language teachers to stay connected with their peers around the world and updated about their fields. Moreover, these online buttons give the language teachers the chance to help others in their fields, find solutions to their problems and improve their teaching language careers.\n\n\n"}
{"id": "13688163", "url": "https://en.wikipedia.org/wiki?curid=13688163", "title": "Special weather statement", "text": "Special weather statement\n\nA Special Weather Statement (Codename SPS with FEMA and National Weather Service) is a form of weather advisory. Special Weather Statements are issued by the National Weather Service of the United States (the NWS) and the Meteorological Service of Canada (the MSC). There are no set criteria for special weather statements in either country.\n\nA Special Weather Statement may be issued by the NWS for hazards that have not yet reached warning or advisory status or that do not have a specific code of their own, such as widespread funnel clouds. They are also occasionally used to clear counties from severe weather watches. A common form of special weather statement is a significant weather alert. Occasionally special weather statements appear as heat advisories. An EAS activation can and may be requested on very rare occasions.\n\nSpecial Weather Statements may also be issued for possible fire weather conditions, such as an enhanced risk. They even can be issued for rain/snow that may hit Advisory criteria such as snow being one inch that isn't going to likely pose a threat. Sometimes Special Weather Statements may be issued to update on current weather conditions, but that is sometimes issued by a Short Term Forecast.\n\nSpecial Weather Statements are issued by the MSC for weather events that are unusual or those that cause general inconvenience or public concern and cannot adequately be described in a weather forecast. These may include widespread events such as Arctic outflows, Alberta clippers, coastal fog banks, areas of possible thunderstorm development, and strong winds such as chinooks and les suetes winds. They are written in a free style and may also reflect warnings in effect near the United States border.\n\n\n"}
{"id": "34989090", "url": "https://en.wikipedia.org/wiki?curid=34989090", "title": "Termbase", "text": "Termbase\n\nA termbase, or term base (a contraction of terminology and database), is a database consisting of concept-oriented terminological entries (or ‘concepts’) and related information, usually in multilingual format. Entries may include any of the following additional information:\n\nA termbase allows for the systematic management of approved or verified terms and is a powerful tool for promoting consistency in terminology.\n\n\n\n"}
{"id": "57232858", "url": "https://en.wikipedia.org/wiki?curid=57232858", "title": "The Inner World of Aphasia", "text": "The Inner World of Aphasia\n\nThe Inner World of Aphasia is a 1968 medical training film by co-directors Edward Feil and Naomi Feil of Edward Feil Productions. It portrays patients suffering from aphasia and follows their experiences through their recoveries. Notable for its innovative artistic direction and empathetic approach to patients with aphasia, the film was added to the National Film Registry in 2015.\n\nThe film follows Marge Nelson (portrayed by Naomi Feil), a nurse who treats patients suffering from aphasia. Marge is overworked and unsympathetic towards her patients, experiencing frustration at their behavior and apparent inability to communicate.\n\nUpon having an accident and suffering brain damage, Marge herself experiences aphasia and undergoes treatment at the same hospital where she previously worked. While struggling to deal with the emotions around her trauma and new issues with processing information and communicating, Marge also discovers a startling lack of empathy from hospital staff and therapists treating her.\n\nMarge finds a new friend in a second patient who is suffering from aphasia. The film shows us that, like Marge, this second patient has also experienced a lack of empathy from others, except in his case it is his family members who are unable to understand his illness.\n\nThrough this new friendship and her own determination, Marge changes the emotional direction of her life and takes the first steps towards her recovery.\n\n\"The Inner World of Aphasia\" was praised by contemporary sources for its attention to patient experience, particularly for drawing attention to the ways in which patients treated for aphasia in the 1960s were dehumanized by hospital staff and family members.\n\nCompared to other media portrayals of aphasia the time, \"The Inner World of\" \"Aphasia\" was unique for its focus on portraying \"the complex subjective feel of what it is like to be an aphasic.\" Its use of flashback, optical distortions, and audio editing techniques were noted by medical professionals as accurately conveying the experience of a person suffering from aphasia (such as hemiplegia and hemianopsia).\n\nIn 1968, \"The Inner World of Aphasia\" received the Golden Eagle from The Council on International Non-Theatrical Events (CINE), the organization's highest honor.\n\nIn 1969, \"The American Journal of Nursing\" included \"The Inner World of Aphasia\" on its list of films screened by the American Nurses Association / National League of Nurses Film Committee and recommended for screening to nurses and nursing students.\n\nIn 2015, \"The Inner World of Aphasia\" was added to the National Film Registry, a selection of films noted as worthy of preservation by the United States National Film Preservation Board.\n"}
{"id": "30483175", "url": "https://en.wikipedia.org/wiki?curid=30483175", "title": "Threshold hypothesis", "text": "Threshold hypothesis\n\nThe threshold hypothesis is a hypothesis concerning second language acquisition set forth in a study by Cummins (1976) that stated that a minimum threshold in language proficiency must be passed before a second-language speaker can reap any benefits from language. It also states that, in order to gain proficiency in a second language, the learner must also have passed a certain and age appropriate level of competence in his or her first language.\n"}
{"id": "11270807", "url": "https://en.wikipedia.org/wiki?curid=11270807", "title": "Truth in Translation", "text": "Truth in Translation\n\nTruth in Translation is a stage play conceived and directed by Michael Lessac, with music by Hugh Masekela. It tells the story of the interpreters at South Africa's Truth and Reconciliation Commission. \n\nThe play was written in a collaboration between the interpreters who worked at the TRC, writer Paavo Tom Tammi and the company of South African actors. It premiered in Rwanda, has toured South Africa and is touring to international conflict zones such as Northern Ireland, Sierra Leone, the Balkans, Jerusalem/Ramallah, Sri Lanka, Peru, and Indonesia/Timor to tell the story of the South African experience. The project includes workshops with audiences, exhibitions (The Forgiveness Project and Jillian Edelstein's \"Truth and Lies\") and filming of the interaction between audiences and the company, and attempts to provoke a global dialogue around notions of healing and reconciliation.\n\n"}
{"id": "12575688", "url": "https://en.wikipedia.org/wiki?curid=12575688", "title": "Twilight language", "text": "Twilight language\n\nTwilight language is a rendering of the Sanskrit term ' (written also ', ', '; , THL \"gongpé ké\") or of their modern Indic equivalents (especially in Bengali, Odia, Assamese, Maithili, Hindi, Nepali, Braj Bhasha and Khariboli).\n\nAs popularized by Roderick Bucknell and Martin Stuart-Fox in \"The Twilight Language: Explorations in Buddhist Meditation and Symbolism\" in 1986, the notion of \"twilight language\" is a supposed polysemic language and communication system associated with tantric traditions in Vajrayana Buddhism and Hinduism. It includes visual communication, verbal communication and nonverbal communication. Tantric texts are often written in a form of the twilight language that is incomprehensible to the uninitiated reader. As part of an esoteric tradition of initiation, the texts are not to be employed by those without an experienced guide and the use of the twilight language ensures that the uninitiated do not easily gain access to the knowledge contained in these works.\n\nThe phrase \"twilight language\" has subsequently been adopted by some other Western writers. For example, according to Judith Simmer-Brown:\n\nAs Bucknell and Stuart-Fox state:\n\nNumbers, numerology and the spirituality of numerals is key to the twilight language and endemic to Vajrayana, as it is throughout Indian religions. Numbers that are particularly frequent in classification are three, five and nine. As Bucknell and Stuart-Fox state:\n\nAlthough twilight language is primarily a feature of esoteric traditions such as the Vajrayana, Bucknell and Stuart-Fox cite the Thai \"bhikkhu\" Buddhadasa as having explored \"the importance of symbolic language in the Pali Canon ... in a number of lectures and publications.\"\n\nNayak holds the great fertile locality of Sonepur, Odisha and its literature is championed by such as Charyapada, Matsyendranath, Daripada and other Naths:\n\nIn 1970, Mircea Eliade presented evidence that the concept of \"twilight\" (or \"crepuscular\") language is based on a translation error. According to Eliade, in 1916 Haraprasād Shāstri proposed the translation of \"twilight language\". However in 1928 Vidhushekar Shāstri debunked that translation, showing that the term is based on a shortened form of the word \"sandhāya\", which can be translated as \"having in view\", \"intending\", or \"with regard to\". Eliade concludes that: \"Hence there is no reference to the idea of a 'twilight language'.\" He continues by speculating on how the term came to be corrupted by scribes who read the familiar word \"sandhyā\" (\"crepuscular\") for the original \"sandhā\". Eliade therefore translates the phrase as \"Intentional Language\". Staal explains, \"sandhā means esoteric meaning, as contrasted with \"prima facie\" or superficial meaning,\" and suggests to translate sandhābhāsā as \"secret language\".\n\n\n\n"}
{"id": "797617", "url": "https://en.wikipedia.org/wiki?curid=797617", "title": "Universal pragmatics", "text": "Universal pragmatics\n\nUniversal pragmatics (UP), more recently placed under the heading of formal pragmatics, is the philosophical study of the necessary conditions for reaching an understanding through communication. The philosopher Jürgen Habermas coined the term in his essay \"What is Universal Pragmatics?\" where he suggests that human competition, conflict, and strategic action are attempts to achieve understanding that have failed because of modal confusions. The implication is that coming to terms with how people understand or misunderstand one another could lead to a reduction of social conflict.\n\nBy coming to an \"understanding,\" he means at the very least, when two or more social actors share the same meanings about certain words or phrases; and at the very most, when these actors are confident that those meanings fit relevant social expectations (or a \"mutually recognized normative background\").\n\nFor Habermas, the goal of coming to an understanding is \"intersubjective mutuality ... shared knowledge, mutual trust, and accord with one another\". In other words, the underlying goal of coming to an understanding would help to foster the enlightenment, consensus, and good will necessary for establishing socially beneficial norms. Habermas' goal is not primarily for subjective feeling alone, but for development of shared (intersubjective) norms which in turn establish the social coordination needed for practical action in pursuit of shared and individual objectives (a form of action termed \"communicative action\").\n\nAs an interdisciplinary subject, universal pragmatics draws upon material from a large number of fields, from pragmatics, semantics, semiotics, informal logic, and the philosophy of language, through social philosophy, sociology, and symbolic interactionism, to ethics, especially discourse ethics, and on to epistemology and the philosophy of mind.\n\nUniversal pragmatics (UP) seeks to overcome three dichotomies: the dichotomy between \"body and mind\", between \"theory and practice\", and between \"analytic and continental philosophy\". It is part of a larger project to rethink the relationship between philosophy and the individual sciences during a period of social crisis. The project is within the tradition of Critical Theory, a program that traces back to the work of Max Horkheimer.\n\nThe term \"universal pragmatics\" includes two different traditions that Habermas and his collaborator, colleague, and friend Karl-Otto Apel have attempted to reconcile. On the one hand, ideas are drawn from the tradition of Plato, Aristotle, and Kant, wherein words and concepts are regarded as universally valid idealizations of shared meanings. And, on the other hand, inspiration is drawn from the American Pragmatist tradition (feat. Charles Sanders Peirce, George Herbert Mead, Charles W. Morris), for whom words are usually arbitrary signs devoid of intrinsic meaning, and whose function is to denote the things and processes in the objective world that surrounds the speakers. \n\nUP shares with speech act theory, semiotics, and linguistics an interest in the details of language use and communicative action. However, unlike those fields, it insists on a difference between the linguistic data that we \"observe\" in the 'analytic' mode, and the \"rational reconstruction of the rules of symbol systems\" that each reader/listener possesses intuitively when interpreting strings of words. In this sense, it is an examination of the two ways that language usage can be analyzed: as an object of scientific investigation, and as a 'rational reconstruction' of intuitive linguistic 'know-how'.\n\nUniversal pragmatics is associated with the philosophical method of \"rational reconstruction\".\n\nThe basic concern in universal pragmatics is \"utterances\" (or \"speech acts\") in general. This is in contrast to most other fields of linguistics, which tend to be more specialized, focusing exclusively on very specific sorts of utterances such as \"sentences\" (which in turn are made up of \"words\", \"morphemes\", and \"phonemes\").\n\nFor Habermas, the most significant difference between a sentence and an utterance is in that sentences are judged according to how well they make sense \"grammatically\", while utterances are judged according to their \"communicative validity\" (see section 1). (1979:31)\n\nUniversal pragmatics is also distinct from the field of sociolinguistics, because universal pragmatics is only interested in the meanings of utterances \"if they have to do with claims about truth or rightness\", while sociolinguistics is interested in all utterances in their social contexts. (1979:31, 33)\n\nThere are three ways to evaluate an utterance, according to UP. There are \"theories that deal with elementary propositions\", \"theories of first-person sentences\", and \"theories of speech acts\".\n\nA theory of elementary propositions investigates those things in the real world that are being \"referenced\" by an utterance, and the things that are implied by an utterance, or \"predicate\" it. For example, the utterance \"The first Prime Minister of Canada\" refers to a man who went by the name of Sir John A. Macdonald. And when a speaker delivers the utterance, \"My husband is a lawyer\", it implies that the speaker is married to a man.\n\nA theory of first-person sentences examines the expression of the \"intentions\" of the actor(s) through language and in the first-person.\n\nFinally, a theory of speech acts examines the setting of standards for interpersonal relations through language. The basic goal for speech act theory is to explain how and when utterances in general are \"performative\". (1979:34) Central to the notion of speech acts are the ideas of illocutionary force and perlocutionary force, both terms coined by philosopher J.L. Austin. \"Illocutionary force\" describes the intent of the speaker, while \"perlocutionary force\" means \"the effect an utterance has in the world\", or more specifically, the effect on others.\n\nA performative utterance is a sentence where an action being performed is done by the utterance itself. For example: \"I inform you that you have a moustache\", or \"I promise you I will not burn down the house\". In these cases, the words are also taken as significant actions: the act of informing and promising (respectively).\n\nHabermas adds to this the observation that speech acts can either succeed or fail, depending on whether or not they succeed on \"influencing\" another person in the intended way. (1979:35)\n\nThis last method of evaluation—the theory of speech acts—is the domain that Habermas is most interested in developing as a \"theory of communicative action\".\n\nThere are a number of ways to approach Habermas's project of developing a formal pragmatic analysis of communication. Because Habermas developed it in order to have a normative and philosophical foundation for his critical social theory, most of the inroads into formal pragmatics start from sociology, specifically with what is called action theory. Action theory concerns the nature of human action, especially the manner in which collective actions are coordinated in a functioning society.\n\nThe coordination and integration of social action has been explained in many ways by many theories. Rational choice theory and game theory are two examples, which describe the integration of individuals into social groups by detailing the complex manner in which individuals motivated only by self-interest will form mutually beneficial and cooperative social arrangements. In contrast to these, Habermas has formulated a theory of \"communicative action\". (Habermas 1984; 1987) This theory and the project of developing a formal pragmatic analysis of communication are inseparable.\n\nHabermas makes a series of distinctions in the service of explaining social action. The first major differentiation he makes is between two social realms, \"the system\" and \"the lifeworld\". These designate two distinct modes of social integration:\n\nThus, communicative action is an indispensable facet of society. It is at the heart of the lifeworld and is, Habermas claims, responsible for accomplishing several fundamental social functions: reaching understanding, cultural reproduction, coordinating action-plans, and socializing individuals.\n\nHowever, Habermas is quick to note, different modes of interaction can (in some ways) facilitate these social functions and achieve integration within the lifeworld. This points towards the second key distinction Habermas makes, which differentiates \"communicative action\" from \"strategic action\". The coordination of action plans, which constitutes the social integration of the lifeworld, can be accomplished either through consensus or influence.\n\nStrategic action is action oriented towards success, while communicative action is action oriented towards understanding. Both involve the symbolic resources of the lifeworld and occur primarily by way of linguistic interaction. On the one hand, actors employing communicative actions draw on the uniquely impelling force of mutual understanding to align the orientation of their action plans. It is this subtle but insistent binding force of communicative interactions that opens the door to an understanding of their meanings. On the other hand, actors employing strategic actions do not exploit the potential of communication that resides in the mutual recognition of a shared action-oriented understanding. Instead strategic actors relate to others with no intention of reaching consensus or mutual understanding, but only the intention of accomplishing pre-determined ends unrelated to reaching an understanding. Strategic action often involves the use of communicative actions to achieve the isolated intentions of individuals, manipulating shared understanding in the service of private interests. Thus, Habermas claims, strategic action is parasitic on communicative action, which means communicative action is the primary mode of linguistic interaction. Reaching a reciprocally defined understanding is communication's basic function.\n\nKeeping in mind this delineation of the object domain, the formal pragmatics of communication can be more readily laid out. The essential insight has already been mentioned, which is that communication is responsible for irreplaceable modes of social integration, and this is accomplished through the unique binding force of a shared understanding. This is, in a sense, the pragmatic piece of formal pragmatics: communication does something in the world. What needs to be explained are the conditions for the possibility of what communication already does. This is, in a sense, the formal piece of formal pragmatics: a rational reconstruction of the deep generative structures that are the universal conditions for the possibility of a binding and compelling mutual understanding.\n\nFrom here, Habermas heads the analysis in two directions. In one direction is a kind of linguistic analysis (of speech acts), which can be placed under the heading of the validity dimensions of communication. The other direction entails a categorization of the idealized presuppositions of communication.\n\nHabermas argues that when speakers are communicating successfully, they will have to defend their meaning by using these four claims.\n\n\nHabermas is emphatic that these claims are universal—no human communication oriented at achieving mutual understanding could possibly fail to raise all of these validity claims. Additionally, to illustrate that all other forms of communication are derived from that which is oriented toward mutual understanding, he argues that there are no other kinds of validity claims whatsoever. This is important, because it is the basis of Habermas' critique of postmodernism.\n\nThe fundamental orientation toward mutual understanding is at the heart of universal pragmatics, as Habermas explains:\n\"The task of universal pragmatics is to identify and reconstruct universal conditions of possible mutual understanding... other forms of social action—for example, conflict, competition, strategic action in general—are derivatives of action oriented toward reaching understanding. Furthermore, since language is the specific medium of reaching understanding at the sociocultural stage of evolution, I want to go a step further and single out explicit speech actions from other forms of communicative action.\"\n\nAny meaning that meets the above criteria, and is recognized by another as meeting the criteria, is considered \"vindicated\" or \"communicatively competent\".\n\nIn order for anyone to speak validly — and therefore, to have his or her comments vindicated, and therefore reach a genuine consensus and understanding — Habermas notes that a few more fundamental commitments are required. First, he notes, actors \"have to treat this formulation of validity so seriously that it might be a precondition for any communication at all\". Second, he asserts that \"all actors must believe that their claims are able to meet these standards of validity\". And third, he insists that there must be a common conviction among actors that all validity claims \"are either already vindicated or could be vindicated\".\n\nHabermas claims that communication rests upon a non-egoistic understanding of the world, which is an idea he borrowed from thinkers like Jean Piaget. A subject capable of a de-centered understanding can take up three fundamentally different attitudes to the world. Habermas refers to such attitudes as \"dimensions of validity\". Specifically, this means individuals can recognize different standards for validity—i.e., that the validation of an empirical truth claim requires different methods and procedures than the validation of subjective truthfulness, and that both of those require different methods and procedures of validation than claims to normative rightness.\n\nThese dimensions of validity can be summarized as claims to \"truth (IT), truthfulness (I),\" and \"rightness (WE)\". So the ability to differentiate between the attitudes (and their respective \"worlds\") mentioned above should be understood as an ability to distinguish between types of validity claims.\n\nM. Cooke provided the only book length treatment of Habermas's communication theory. Cooke explains:\n\nThis is fundamental to Habermas's analysis of communication. He maintains that the performance of any speech act necessarily makes reference to these dimensions of validity, by raising at least three validity claims.\n\nOne way to grasp this idea is to take an inventory of the ways in which an attempt at communication can misfire, the ways a speech act can fail. A hearer may reject the offering of a speech act on the grounds that it is invalid because it:\n\nOf course, from this it follows that a hearer who accepts the offering of a speech act does so on the grounds that it is valid because it:\n\nThis means that when engaging in communication the speaker and hearer are inescapably oriented to the validity of what is said. A speech act can be understood as an offering, the success or failure of which depends upon the hearer's response of either accepting or rejecting the validity claims it raises. The three dimensions of validity pointed out above are implicated in any attempt at communication.\n\nThus, communication relies on its being embedded within relations to various dimensions of validity. Any and every speech act is infused with inter-subjectively recognized claims to be valid. This implicitly ties communication to argumentation and various discursive procedures for the redemption of validity claims. This is true because to raise a validity claim in communication is to simultaneously imply that one is able to show, if challenged, that one's claim is justified. Communication is possible because speakers are accountable for the validity of what they say. This assumption of responsibility on the part of the speaker is described by Habermas as a \"warranty\", because in most cases the validity claims raised during communication are taken as justified, and communication proceeds on that basis. Similarly, the hearer is accountable for the stance he or she takes up in relation to the validity claims raised by the speaker. Both speaker and hearer are bound to the validity claims raised by the utterances they share during communication. They are bound by the weak obligations inherent in pursuing actions oriented towards reaching an understanding. Habermas would claim that this obligation is a rational one:\n\nThis begins to point towards the idea of communicative rationality, which is the potential for rationality that is implicit in the validity basis of everyday communication, the shape of reason that can be extracted from Habermas's formal-pragmatic analyses.\n\nHowever, before the idea of communicative rationality can be described, the other direction of Habermas's formal pragmatic analyses of communication needs to be explained. This direction looks towards the idealized presuppositions of communication.\n\nWhen individuals pursue actions oriented towards reaching an understanding, the speech acts they exchange take on the weight of a mutually recognized validity. This means each actor involved in communication takes the other as accountable for what they have said, which implies that good reasons could be given by all to justify the validity of the understanding that is being achieved. Again, in most situations the redemption of validity claims is not an explicit undertaking (except in \"discourses\", see below). Instead, each actor issues a \"warranty\" of accountability to the other, which only needs to be redeemed if certain validity claims are thrown into question. This suggests that the validity claims raised in every communicative interaction implicitly tie communication to argumentation.\n\nIt is here that the idealized presuppositions of communication arise. Habermas claims that all forms of argumentation, even implicit and rudimentary ones, rest upon certain \"idealizing suppositions,\" which are rooted in the very structures of action oriented towards understanding. These \"strong idealizations\" are always understood as at least approximately satisfied by participants in situations where argumentation (and communication) is thought to be taking place. Thus, when during communication it is discovered that the belief that these presuppositions are satisfied is not justified it is always taken as problematic. As a result, steps are usually taken to reestablish and maintain the belief that they are approximately satisfied, or communication is simply called off.\n\n\nIn sum, all these presuppositions must be assumed to be approximately satisfied in any situation of communication, despite their being necessarily counterfactual. Habermas refers to the positing of these idealized presuppositions as the \"simultaneously unavoidable and trivial accomplishments that sustain communicative action and argumentation\".\n\nHabermas calls \"discourses\" those forms of communication that come sufficiently close to actually satisfying these presuppositions. Discourses often occur within institutionalized forms of argumentation that self-reflectively refine their procedures of communication, and as a result, have a more rigorous set of presuppositions in addition to the ones listed above.\n\nA striking feature of discourse is that validity claims tend to be explicitly thematized and there is the presupposition that all possible interlocutors would agree to the universal validity of the conclusions reached. Habermas especially highlights this in what he calls \"theoretical discourses\" and \"practical discourses\". These are tied directly to two of the three dimensions of validity discussed above: theoretical discourse being concerned with validity claims thematized regarding objective states of affairs (IT); practical discourse being concerned with validity claims thematized concerning the rightness of norms governing social interactions (WE).\n\nHabermas understands presupposition (5) to be responsible for generating the self-understanding and continuation of theoretical and practical discourses. Presupposition (5) points out that the validity of an understanding reached in theoretical or practical discourse, concerning some factual knowledge or normative principle, is always expanded beyond the immediate context in which it is achieved. The idea is that participants in discourses such as these presuppose that any understanding reached could attain universal agreement concerning its universal validity if these discourses could be relieved of the constraints of time and space. This idealized presupposition directs discourses concerning truth and normative certainty beyond the contingencies of specific communicative situations and towards the idealized achievements of universal consensus and universal validity. It is a rational reconstruction of the conditions for the possibility of earnest discourses concerning facts and norms. Recall that, for Habermas, rational reconstructions aim at offering the most acceptable account of what allows for the competencies already mastered by a wide range of subjects. In order for discourse to proceed, the existence of facts and norms must be presupposed, yet the certainty of an absolute knowledge of them must be, in a sense, postponed.\n\nStriking a Piagetian and Peircean chord, Habermas understands the deep structures of collective inquiry as developmental. Thus, the presupposition shared by individuals involved in discourse is taken to reflect this. The pursuit of truth and normative certainty is taken to be motivated and grounded, not in some objective or social world that is treated as a \"given\", but rather in a learning process. Indeed, Habermas himself is always careful to formulate his work as a research project, open to refinement.\n\nIn any case, reconstructing the presuppositions and validity dimensions inherent to communication is valuable because it brings into relief the inescapable foundations of everyday practices. Communicative action and the rudimentary forms of argumentation that orient the greater part of human interaction cannot be left behind. By reconstructing the deep structures of these Habermas has discovered a seed of rationality planted in the very heart of the lifeworld. Everyday practices, which are common enough to be trivial, such as reaching an understanding with another, or contesting the reasons for pursuing a course of action, contain an implicit and idealized rationality.\n\nIn other words, communication is always somewhat rational. Communication could not occur if the participants thought that the speech acts exchanged did not carry the weight of a validity for which those participating could be held accountable. Nor would anyone feel that a conclusion was justified if it was achieved by any other means than the uncoerced force of the better argument. Nor could the specialized discourses of law, science and morality continue if the progress of knowledge and insight was denied in favor of relativism.\n\nIt is a question how appropriate it is to speak of \"communication\" tenselessly, and of \"everyday practices\" as though they cut across all times and cultures. That they do cannot be assumed, and anthropology provides evidence of significant difference. It is possible to ignore these facts by limiting the scope of universal pragmatics to current forms of discourse, but this runs the risk of contradicting Habermas's own demand for (5). Moreover, the initial unease with the classical and liberal views of rationality had to do precisely with their ahistorical character and refusal, or perhaps inability, to acknowledge their own origins in circumstances of the day. Their veneer of false universality torn off by the likes of Foucault, it remains to be seen whether \"universal\" pragmatics can stand up to the same challenges posed by deconstruction and skepticism.\n\n\n"}
{"id": "12236006", "url": "https://en.wikipedia.org/wiki?curid=12236006", "title": "Verse (poetry)", "text": "Verse (poetry)\n\nIn the countable sense, a verse is formally a single metrical line in a poetic composition. However, verse has come to represent any division or grouping of words in a poetic composition, with groupings traditionally having been referred to as stanzas.\n\nIn the uncountable (mass noun) sense verse refers to \"poetry\" as contrasted to prose. Where the common unit of verse is based on meter or rhyme, the common unit of prose is purely grammatical, such as a sentence or paragraph.\n\nIn the second sense verse is also used pejoratively in contrast to poetry to suggest work that is too pedestrian or too incompetent to be classed as poetry.\n\nBlank verse is poetry written in regular, metrical, but unrhymed, lines, almost always composed of iambic pentameters.\n<poem>\n</poem>\n\nFree verse is usually defined as having no fixed meter and no end rhyme. Although free verse may include end rhyme, it commonly does not.\n<poem>\n\n</poem>\n"}
{"id": "4068416", "url": "https://en.wikipedia.org/wiki?curid=4068416", "title": "Welsh Language Act 1967", "text": "Welsh Language Act 1967\n\nThe Welsh Language Act 1967, is an Act of the Parliament of the United Kingdom which gave some rights to use the Welsh language in legal proceedings in Wales (including Monmouthshire) and gave the relevant Minister the right to authorise the production of a Welsh version of any documents required or allowed by the Act. \n\nThe Welsh Language Act 1967 is a short Act, consisting of a Preamble and five sections. The Preamble states that \"it is proper that the Welsh language should be freely used by those who so desire in the hearing of legal proceedings in Wales\". The first section gives the right to use Welsh orally in court proceedings in Wales provided that the person who wishes to do so has notified the court in advance. The second and third sections allowed but did not require Ministers to provide Welsh versions of forms or wordings. The fourth section of the Act repealed the provision in Part 3 of the Wales and Berwick Act 1746 that the term \"England\" should include Wales. \n\nThe Act was based on the Hughes Parry report into the status of Welsh, published in 1965, which advocated equal validity for Welsh in speech and in written documents, both in the courts and in public administration in Wales. However, the Act did not include all of the report's recommendations. \n\nThe Laws in Wales Acts 1535–1542 had made English the only language of the law courts and other aspects of public administration in Wales, even though most of the population spoke Welsh and few spoke English. The 1967 Act was the first alteration to this situation, but the Welsh Language Act 1993, was the first to put Welsh on an equal basis with English in public life.\n"}
{"id": "712947", "url": "https://en.wikipedia.org/wiki?curid=712947", "title": "ǃGãǃne language", "text": "ǃGãǃne language\n\nǃGãǃne (!Gã!nge) is an extinct language of the ǃKwi family, once spoken near Tsolo and in Umtata District in South Africa, south of Lesotho. The only material on the language is 140 words collected from two semi-speakers (rememberers) in 1931.\n\nLike ǁXegwi, ǃGãǃne is considered an \"outlier\" among the !Kwi languages by Güldemann (2005, 2011). \"Ethnologue\" and \"Glottolog\" count it as a dialect of Seroa, though the two have no demonstrable connection apart from being in the !Kwi family.\n"}
