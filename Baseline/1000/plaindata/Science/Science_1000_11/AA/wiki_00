{"id": "1349840", "url": "https://en.wikipedia.org/wiki?curid=1349840", "title": "A Letter to a Friend", "text": "A Letter to a Friend\n\nA Letter to a Friend (written 1656; published posthumously in 1690), by Sir Thomas Browne, the 17th century philosopher and physician, is a medical treatise of case-histories and witty speculations upon the human condition.\n\nIt is believed to be the source of a term Mary Leitao found in 2001 to describe her son's skin condition. She chose the name \"Morgellons disease\" from a skin condition described by Browne in \"Letter to a Friend\", thus:\n\nThere is, however, no suggestion that the symptoms described by Browne are linked to the alleged modern cases of Morgellons.\n\n"}
{"id": "38751", "url": "https://en.wikipedia.org/wiki?curid=38751", "title": "ArXiv", "text": "ArXiv\n\narXiv (pronounced \"archive\") is a repository of electronic preprints (known as e-prints) approved for publication after moderation, that consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, and quantitative finance, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository. Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, and had hit a million by the end of 2014. By October 2016 the submission rate had grown to more than 10,000 per month.\n\narXiv was made possible by the low-bandwidth TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side. Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory which could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993. The term e-print was quickly adopted to describe the articles.\n\nIt began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org. It is now hosted principally by Cornell, with eight mirrors around the world.\n\nIts existence was one of the precipitating factors that led to the current movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv.\n\nThe annual budget for arXiv is approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions. This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Each member institution pledges a five-year funding commitment to support arXiv. Based on institutional usage ranking, the annual fees are set in four tiers from $1,000 to $4,400. Cornell's goal is to raise at least $504,000 per year through membership fees generated by approximately 220 institutions.\n\nIn September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv's operation and development. Ginsparg was quoted in the \"Chronicle of Higher Education\" as saying it \"was supposed to be a three-hour tour, not a life sentence\". However, Ginsparg remains on the arXiv Scientific Advisory Board and on the arXiv Physics Advisory Committee.\n\nAlthough arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic, or reject submissions that are not scientific papers. The lists of moderators for many sections of arXiv are publicly available, but moderators for most of the physics sections remain unlisted.\n\nAdditionally, an \"endorsement\" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines. Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors, but to check whether the paper is appropriate for the intended subject area. New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.\n\nA majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston's geometrization conjecture, including the Poincaré conjecture as a particular case, uploaded by Grigori Perelman in November 2002. Perelman appears content to forgo the traditional peer-reviewed journal process, stating: \"If anybody is interested in my way of solving the problem, it's all there let them go and read about it\". Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.\n\nPapers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.\n\nThe standard access route is through the arXiv.org website or one of several mirrors. Several other interfaces and access routes have also been created by other un-associated organisations. These include the University of California, Davis's \"front\", a web portal that offers additional search functions and a more self-explanatory interface for arXiv.org, and is referred to by some mathematicians as (the) Front. A similar function used to be offered by eprintweb.org, launched in September 2006 by the Institute of Physics, and was switched off on June 30, 2014. Carnegie Mellon provides Table, a search engine for tables extracted from arXiv publications. Google Scholar and Live Search Academic (now defunct) can also be used to search for items in arXiv. A full text and author search engine for arXiv is provided by Scientillion. Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them.\n\nFiles on arXiv can have a number of different copyright statuses:\n\nWhile arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat's Last Theorem using only high-school mathematics, they are \"surprisingly rare\". arXiv generally re-classifies these works, e.g. in \"General mathematics\", rather than deleting them; however, some authors have voiced concern over the lack of transparency\nin the arXiv screening process.\n\n\n"}
{"id": "26998311", "url": "https://en.wikipedia.org/wiki?curid=26998311", "title": "BSSN formalism", "text": "BSSN formalism\n\nThe BSSN formalism is a formalism of general relativity that was developed by Thomas W. Baumgarte, Stuart L. Shapiro, Masaru Shibata and Takashi Nakamura between 1987 and 1999. It is a modification of the ADM formalism developed during the 1950s.\n\nThe ADM formalism is a Hamiltonian formalism that does not permit stable and long-term numerical simulations. In the BSSN formalism, the ADM equations are modified by introducing auxiliary variables. The formalism has been tested for a long-term evolution of linear gravitational waves and used for a variety of purposes such as simulating the non-linear evolution of gravitational waves or the evolution and collision of black holes.\n\n"}
{"id": "3279786", "url": "https://en.wikipedia.org/wiki?curid=3279786", "title": "Comparison of the imperial and US customary measurement systems", "text": "Comparison of the imperial and US customary measurement systems\n\nBoth the imperial and United States customary systems of measurement derive from earlier English systems used in the Middle Ages, that were the result of a combination of the local Anglo-Saxon units inherited from German tribes and Roman units brought by William the Conqueror after the Norman Conquest of England in 1066.\n\nHaving this shared heritage, the two systems are quite similar, but there are differences. The US customary system is based on English systems of the 18th century, while the Imperial system was defined in 1824, after American independence.\n\nVolume may be measured either in terms of units of cubic length or with specific volume units. The units of cubic length (the cubic inch, cubic foot, cubic mile, etc.) are the same in the imperial and US customary systems but with the specific units of volume (the bushel, gallon, fluid ounce, etc.) they differ. The US customary system has one set of units for fluids and another set for dry goods. The imperial system has only one set defined independently of and subdivided differently from its US counterparts.\n\nBy the end of the eighteenth century various systems of volume measurement were in use throughout the British Empire. Wine was measured with units based on the Queen Anne's gallon of 231 cubic inches (3.785 L). Beer was measured with units based on an ale gallon of 282 cubic inches (4.621 L). Grain was measured with the Winchester measure with a gallon of approximately 268.8 cubic inches (one eighth of a Winchester bushel or 4.405 L). In 1824 these were replaced with a single system based on the imperial gallon. \nOriginally defined as the volume of of distilled water (under certain conditions), then redefined by the Weights and Measures Act 1985 to be exactly (277.4 cu in), the imperial gallon is close in size to the old ale gallon.\n\nThe Winchester measure was made obsolete in the British Empire but remained in use in the US. The Winchester bushel was replaced with an imperial bushel of 8 imperial gallons. The subdivisions of the bushel were maintained. As with US dry measures the imperial system divides the bushel into 4 pecks, 8 gallons, 32 quarts or 64 pints. Thus all of these imperial measures are about 3% larger than their US dry measure counterparts.\n\nFluid measure is not as straightforward. The American colonists adopted a system based on the 231-cubic-inch wine gallon for all fluid purposes. This became the US fluid gallon. Both the imperial and US fluid gallon are divided into 4 quarts, 8 pints or 32 gills. However, whereas the US gill is divided into 4 US fluid ounces, the imperial gill is divided into 5 imperial fluid ounces. So whilst the imperial gallon, quart, pint and gill are about 20% larger than their US fluid measure counterparts, the fluid ounce is about 4% smaller. Note that one avoirdupois ounce of water has an approximate volume of one imperial fluid ounce at 62 °F (16.67 °C). This convenient fluid-ounce-to-avoirdupois-ounce relation does not exist in the US system.\n\nOne noticeable comparison between the imperial system and the US system is between some Canadian and American beer bottles. Many Canadian brewers package beer in a 12-imperial-fluid-ounce bottles, which are 341 mL each. American brewers package their beer in 12-US-fluid-ounce bottles, which are 355 mL each. This results in the Canadian bottles being labelled as 11.5 fl oz in US units when imported into the United States. Because Canadian beer bottles predate the adoption of the Metric System in that country, they are still sold and labelled in Canada as 341 mL. Canned beer in Canada is sold and labelled in 355 mL cans, and when exported to the US are labelled as 12 fl oz.\n\nThe international yard is defined as exactly 0.9144 metres. This definition was agreed by the United States, Canada, the United Kingdom, South Africa, Australia and New Zealand through the international yard and pound agreement of 1959.\n\nThe US survey foot and survey mile have been maintained as separate units for surveying purposes to avoid the accumulation of error that would follow replacing them with the international versions, particularly with State Plane Coordinate Systems. (The choice of unit for surveying purposes is based on the unit used when the overall framework or geodetic datum for the region was established, so that - for example - much of the former British empire still uses the Clarke foot for surveying.) The US survey foot is defined so that 1 metre is exactly 39.37 inches, making the international foot of 0.3048 metres exactly two parts per million shorter. This is a difference of just over 3.2 mm or a little over one eighth of an inch per mile.\n\nThe main units of length (inch, foot, yard and international mile) were the same in the US, though the US rarely uses some of the intermediate units, such as the (surveyor's) chain (22 yards) and the furlong (220 yards).\n\nAt one time the definition of the nautical mile was based on the sphere whose surface is the same as the Clarke Ellipsoid. In the US, the full value of 1853.256 metres was used, but in the Commonwealth, this was rounded to 6080 feet (1853.184 m). These have been replaced by the international version, which rounds the sixtieth part of the 45° to the nearest metre, as 1852 metres.\n\nTraditionally, both Britain and the US used three different weight systems: troy weight for precious metals, avoirdupois weight for most other purposes and apothecaries' weight for medicines. However, apothecaries' weight has now been superseded by the metric system.\n\nOne important difference is the widespread use in Britain of the stone of 14 pounds () for body weight. This unit is not used in the United States, although its influence was seen in the practice, until World War II, of selling flour by a barrel of 196 pounds (14 stone). Another difference arose when Britain abolished the \"troy pound\" () on January 6, 1879, leaving only the \"troy ounce\" () and its decimal subdivisions, whereas the troy pound (of 12 troy ounces) and pennyweight are still legal in the United States, although no longer widely used.\n\nIn all the systems, the fundamental unit is the pound (lb), and all other units are defined as fractions or multiples of it. The tables of imperial troy mass and apothecaries' mass are the same as the corresponding United States tables, except for the British spelling \"drachm\" in the table of apothecaries' mass. The table of imperial avoirdupois mass is the same as the United States table up to 1 pound, but above that point the tables differ.\n\nThe imperial system has a hundredweight, defined as eight stone or 112 lb (), whereas a US hundredweight is 100 lb (). In both systems, 20 hundredweights make a ton. In the US, the terms \"long ton\" (, ) and \"short ton\" (; ) are used to distinguish them. The term \"metric ton\" is also used to denote a tonne (, ), which is about 2% less than the long ton.\n\n"}
{"id": "838846", "url": "https://en.wikipedia.org/wiki?curid=838846", "title": "Concept inventory", "text": "Concept inventory\n\nA concept inventory is a criterion-referenced test designed to help determine whether a student has an accurate working knowledge of a specific set of concepts. Historically, concept inventories have been in the form of multiple-choice tests in order to aid interpretability and facilitate administration in large classes. Unlike a typical, teacher-authored multiple-choice test, questions and response choices on concept inventories are the subject of extensive research. The aims of the research include ascertaining (a) the range of what individuals think a particular question is asking and (b) the most common responses to the questions. Concept inventories are evaluated to ensure test reliability and validity. In its final form, each question includes one correct answer and several distractors.\n\nIdeally, a score on a criterion-referenced test reflects the amount of content knowledge a student has mastered. Criterion-referenced tests differ from norm-referenced tests in that (in theory) the former is not used to compare an individual's score to the scores of the group. Ordinarily, the purpose of a criterion-referenced test is to ascertain whether a student mastered a predetermined amount of content knowledge; upon obtaining a test score that is at or above a cutoff score, the student can move on to study a body of content knowledge that follows next in a learning sequence. In general, item difficulty values ranging between 30% and 70% are best able to provide information about student understanding.\n\nThe distractors are incorrect or irrelevant answers that are usually (but not always) based on students' commonly held misconceptions. Test developers often research student misconceptions by examining students' responses to open-ended essay questions and conducting \"think-aloud\" interviews with students. The distractors chosen by students help researchers understand student thinking and give instructors insights into students' prior knowledge (and, sometimes, firmly held beliefs). This foundation in research underlies instrument construction and design, and plays a role in helping educators obtain clues about students' ideas, scientific misconceptions, and didaskalogenic (\"teacher-induced\" or \"teaching-induced\") confusions and conceptual lacunae that interfere with learning.\n\nConcept inventories are education-related diagnostic tests. In 1985 Halloun and Hestenes introduced a \"multiple-choice mechanics diagnostic test\" to examine students' concepts about motion. It evaluates student understanding of basic concepts in classical (macroscopic) mechanics. A little later, the Force Concept Inventory (FCI), another concept inventory, was developed. The FCI was designed to assess student understanding of the Newtonian concepts of force. Hestenes (1998) found that while \"nearly 80% of the [students completing introductory college physics courses] could state Newton's Third Law at the beginning of the course. FCI data showed that less than 15% of them fully understood it at the end\".These results have been replicated in a number of studies involving students at a range of institutions (see sources section below). That said, there remains questions as what exactly the FCI measures. Results from Hake (1998) using the FCI have led to greater recognition in the science education community of the importance of students' \"interactive engagement\" with the materials to be mastered. .\n\nSince the development of the FCI, other physics instruments have been developed. These include the Force and Motion Conceptual Evaluation developed by Thornton and Sokoloff and the Brief Electricity and Magnetism Assessment developed by Ding et al. For a discussion of how a number of concept inventories were developed see Beichner. Information about physics concept tests can be found at the NC State Physics Education Research Group website (see the external links below).\n\nIn addition to physics, concept inventories have been developed in statistics, chemistry, astronomy, basic biology, natural selection, genetics, engineering, geoscience. and computer science.\n\nIn many areas, foundational scientific concepts transcend disciplinary boundaries. An example of an inventory that assesses knowledge of such concepts is an instrument developed by Odom and Barrow (1995) to evaluate understanding of diffusion and osmosis. In addition, there are non-multiple choice conceptual instruments, such as the essay-based approach suggested by Wright et al. (1998) and the essay and oral exams used by Nehm and Schonfeld (2008). and Cooper et al to measure student understanding of Lewis structures in chemistry.\n\nSome concept inventories are problematic. The concepts tested may not be fundamental or important in a particular discipline, the concepts involved may not be explicitly taught in a class or curriculum, or answering a question correctly may require only a superficial understanding of a topic. It is therefore possible to either over-estimate or under-estimate student content mastery. While concept inventories designed to identify trends in student thinking may not be useful in monitoring learning gains as a result of pedagogical interventions, disciplinary mastery may not be the variable measured by a particular instrument. Users should be careful to ensure that concept inventories are actually testing conceptual understanding, rather than test-taking ability, language skills, or other abilities that can influence test performance.\n\nThe use of multiple-choice exams as concept inventories is not without controversy. The very structure of multiple-choice type concept inventories raises questions involving the extent to which complex, and often nuanced situations and ideas must be simplified or clarified to produce unambiguous responses. For example, a multiple-choice exam designed to assess knowledge of key concepts in natural selection does not meet a number of standards of quality control. One problem with the exam is that the two members of each of several pairs of parallel items, with each pair designed to measure exactly one key concept in natural selection, sometimes have very different levels of difficulty. Another problem is that the multiple-choice exam overestimates knowledge of natural selection as reflected in student performance on a diagnostic essay exam and a diagnostic oral exam, two instruments with reasonably good construct validity. Although scoring concept inventories in the form of essay or oral exams is labor-intensive, costly, and difficult to implement with large numbers of students, such exams can offer a more realistic appraisal of the actual levels of students' conceptual mastery as well as their misconceptions. Recently, however, computer technology has been developed that can score essay responses on concept inventories in biology and other domains (Nehm, Ha, & Mayfield, 2011), promising to facilitate the scoring of concept inventories organized as (transcribed) oral exams as well as essays.\n\n"}
{"id": "36737017", "url": "https://en.wikipedia.org/wiki?curid=36737017", "title": "Crystal Palace School", "text": "Crystal Palace School\n\nCrystal Palace School of Art, Science, and Literature, which opened in 1854, was set up by the Crystal Palace Company as a new enterprise to occupy part of its buildings when it re-erected the Crystal Palace in suburban Sydenham in 1853. Civil engineer and later first director of the Royal College of Music, George Grove was appointed secretary. It was a part of the great movements for educational and social reform of the nineteenth century.\n\nThe main buildings were destroyed by fire in 1936.\n\nThe overwhelming majority of classes were for women:\n\n\n\n\n\n\nClasses for gentlemen were limited to the School of Engineering\n\n\nThe South Tower also contained John Logie Baird's transmitter and studios.\n\nLetter to The Editor, \"The Times\", Monday Dec. 07, 1936. page 10, issue 47551<br>\nThe end of the Crystal Palace brings to mind memories of the School of Engineering which was housed in the South Tower, fortunately still standing firm as a rock. This school, founded by J. W. Wilson, M.I.M.E., an engineer who helped to build the Great Exhibition of 1851, sent many of its students to the four quarters of the globe. The curriculum of the school included mechanical and civil courses and about five of the circular rooms were used. There was a fitting shop, pattern shop, and drawing office. Those in the mechanical section built a 4 h.p. vertical engine which was generally exhibited at the head of the stairs on the south side of the Palace. In the Civil Engineering section we surveyed the whole of the grounds, and drew plans and made estimates for an imaginary railway which extended from one side to the other. This entailed all the necessary estimates for embankments, a cantilever bridge etc. Then there was the Colonial section presided over by a most congenial superintendent, who had no doubt seen much of a pioneer's life and infused his enthusiasm into those who belonged to his section. Concerning the rigidity of the South Tower, I was working in a high storey soon after joining the school when there was a strong wind, and, feeling giddy, I mentioned it to the superintendent. He informed me that it rocked several inches at the top, which made it safer than absolute rigidity.–Mr F. C. Bell, 74, Berners Street, Ipswich.\n\nThe school was a centre for the examinations of the Oxford and Cambridge syndicates.\n\nPolytechnic (United Kingdom)\n\n\n"}
{"id": "8758352", "url": "https://en.wikipedia.org/wiki?curid=8758352", "title": "Developmentally appropriate practice", "text": "Developmentally appropriate practice\n\nDevelopmentally appropriate practice (or DAP) is a perspective within early childhood education whereby a teacher or child caregiver nurtures a child's social/emotional, physical, and cognitive development by basing all practices and decisions on (1) theories of child development, (2) individually identified strengths and needs of each child uncovered through authentic assessment, and (3) the child's cultural background as defined by his community, family history, and family structure.\n\nDevelopmentally Appropriate Practice (DAP) is the practice of an excellent educator who knows and changes his/her teaching practices to meet the students where they are developmentally. In order to adjust teaching practices to meet these students' needs, the teacher must research and be educated on child development and a variety of approaches in order to teach the children. Every approach or teaching strategy should be appropriate for the students in the classroom. Teachers can use multiple strategies in one lesson to ensure all children are learning. To ensure each students' needs are being met the educator must know each student and know where he/she is developmentally and what works for him/her. Meeting the students' needs and adjusting the way a skill or concept is being taught does not mean the educator makes the task easier for the student. Instead, the educator challenges the student by setting a goal that is attainable without frustrating the student. Along with adjusting to meet the students where they are developmentally the educator must take into consideration the students' cultural and social contexts. These practices and this knowledge base are a foundation or a starting point for educators as they make decisions in their classrooms.[1]\n\nThe National Association for the Education of Young Children (NAEYC) wrote a position statement to outline best practices for teachers and educators of all kinds. The position statement was adopted in 2009 and is written solely based on research about child development, child learning, and effective educational practices. Educators have the responsibility to narrow the achievement gap for students who come from all kinds of backgrounds and give them learning opportunities they may not have the ability to experience outside of the school. Giving the child experiences that are both led by them and led by the teacher are important for the child's development and learning. Child-led experiences can come from play, which is an extremely important way to learn and spend time as a child. Children may not have the opportunity at home to have well developed or engaging play where they will learn social skills, cognitive skills, or emotional skills. These are very important skills for children to develop. Teacher-led experiences should be well developed, well thought out, consist of a variety of approaches and be completely focused on the children. This position statement gives educators and teachers guidance and principles to keep in mind in decision making, classroom community, enhancing planning curriculum for development, assessing child development and building relationships with families.[2]\n\nChildren ages 3-5 are referred to as preschoolers. The preschool years are a vitally important period of learning and development in all areas of human functioning- physical, social and emotional, cognitive (including perception, reasoning, memory, and other aspects of academic and intellectual development), and language. It is also established that best development and learning during these years is most likely to occur when children establish positive and caring relationships with adults and other children; receive carefully planned, intentional adult guidance and assistance; and explore interesting environments with many things to do and learn.\n\nEarly childhood teachers, in collaboration with families, are responsible for ensuring that that program promotes the development and enhances the learning of each individual child served; in other words, professionals must ensure that the program is developmentally appropriate. Ensuring this requires that teachers have a great deal of knowledge, skill, and training. Children entering preschool vary significantly in what they know and can do.\n\nConsiderable growth and change occur in children during the preschool years in all areas of development. To function most effectively, preschool teachers need to know about the goals, sequences, and trajectories of development in all of those areas- to avoid a scaled-down version of curriculum intended for older children and to understand the importance of communicating with kindergarten and other teachers and aligning the curriculum accordingly.\n\nThree-year-olds are no longer toddlers, but they behave like toddlers at times, and they are not steady in their gains. Children's social skills are still uncertain, they are still working on how to regulate and appropriately express their emotions, and they are not yet able to communicate their ideas and feelings in skilled, complex ways. They believe in fairies and monsters and have trouble with logical sequences that seem basic to adults- which is why adults tend to underestimate their abilities. Yet at other times, their language ability, motor skills, reasoning abilities, and other behaviors make them seem older than they are.\n\nThe challenge for the preschool teacher is to maintain appropriate expectations, providing each child with the right amount of challenge, support, sensitivity, and stimulation that promotes development and learning- all of which can only happen within the context of a close, nurturing teacher-child relationship.\n\nPreschoolers revel in their increasing coordination, using their bodies cheerfully and enthusiastically. They thrive in environments that encourage them to experiment with new materials, roles, and ideas through various projects and especially through play. They have great interest in feelings and are better able to express and label their emotions and identify others' emotions. They make some important gains in cognition, allowing them the pleasure of representing their world in pretend play, symbols, objects, drawings, and words. And, given a rich language environment, they show impressive gains in language skills.\n\nPreschoolers are an enchanting, enthusiastic, curious, and inherently playful and imaginative bunch, providing the adults who work with them entry to a world of great charm and delight!\n\nCopple, C. & Bredekamp, S. (2009). Developmentally appropriate practice in early childhood programs: Serving children from birth through age 8. 3d ed. Washington, DC: National Association for the Education of Young Children.\n\nIn the Kindergarten Year, children are 5-6 years old, learning through play, community, and social interaction with peers. As a kindergarten teacher it is developmentally appropriate to be modeling positive social interactions in the classroom, fostering positive relationships between students of different cultures and ethnicities, give children leadership roles in the classroom, and teach in whole and small group settings, with peer interaction during instruction. It is important in this year that children spend the majority of the day moving, with little time sitting down and still. They should be working and developing their fine and gross motor skills through this daily movement. Children should also be spending time playing outside everyday, with a regular physical education schedule.\n\nIn kindergarten children should be learning body and spatial awareness, as well as key movements like balancing, jumping and catching objects of varied size. Children should have time daily to exercise their fine motor skills, such as through writing with utensils of varied size, using scissors, play dough, puzzles, etc. Along with physical development, kindergarten is a time when students should be developing self-help skills, which should be promoted by teachers through giving students daily activities that teach them to care for themselves. These activities would look like washing their hands, putting on jackets and cleaning up after their own classroom activity.\n\nTeachers should be giving regular time in the classroom for students to be conversing with others, and working in small groups on projects that teach communication, listening and understanding skills. Teachers should provide opportunities for students to verbally respond to questions, describe scenarios, retell stories, and give directions. It is important that English Second Language students are integrated and involved in these discussions and peer work time, in order to develop their listening, speaking and understanding skills.\n\nAcademically students should be introducing phonological and phonemic awareness by implementing songs, poems, books, etc. Students should have books read to them on a daily basis. Books should be available in a classroom library at all times and contain a variety of story types. It is appropriate at this age that students be taught the sounds of letters, and writing should be encouraged throughout. Math is taught through manipulatives, block play, games, etc., and at this age it is natural for students to begin to try and make sense of the world through mathematics. Science is based off students natural curiosity and experiences. Science should be a hands on activity.\n\nTechnology should be integrated into the classroom through computers, and any other available technology in the classroom, but should foster problem solving and thinking, and by teachers to document children.\n\nCopple, C. & Bredekamp, S. (2009). Developmentally appropriate practice in early childhood programs: Serving children from birth through age 8. 3d ed. Washington, DC: National Association for the Education of Young Children.\n\nDuring the early grades, children gain increasing mastery in every area, both in their development and their learning. The exploring, problem solving, communicating, etc. that children do during this time period enthuses and interests them, which teachers can use at this age to create a lifelong love for learning. In order for primary children to prosper, they need warm and sensitive teaching, integrated learning, authentic assessment, and a mix of child and teacher-guided activities.\n\nDuring the primary grades, it is essential for children to learn how to read. It, however, is equally important that the children develop the desire to read. This sets the tone for almost everything in the classroom and, most often times, determines what type of student the child will be. Teacher's build such enthusiasm by showing interest in each individual child's needs and show genuine excitement for their learning. Teachers should provide a variety of ways for students to learn being that every individual learns in their own way. This will encourage student effort, despite the type of learner the child is.\n\nPhysically, children in the primary grades grow around 2-3 inches in height, with their face maturing a good bit as well. It is during this time that the child's face starts to elongate to accommodate permanent teeth. The child's major gross motor skills have typically developed by this time. As far as fine motor skills, some children have started to develop them by this age, while some are still refining these skills.\n\nSocially, primary aged children need an atmosphere characterized by good conversation, laughter, and excitement over accomplishments. It is important that a teacher creates this type of atmosphere in the classroom in order to promote what is developmentally appropriate for these grade levels.\n\nAs far as emotionally, children in the primary grades are largely guided by their emotions and they are the main force guiding behavior as well as learning from infancy through adulthood. The ability to infer other's thoughts, feelings, and intentions is a key component to help kids develop and maintain relationships. It is during the primary grades that students become increasingly able to consider the feelings and emotions of others.\n\nCognitively, primary aged children are making great strides. A significant, yet gradual, change occurs in most children between the ages of five and seven. This being because they become able to think about things more dimensionally, which allows them to solve a wider range of problems. Due to this cognitive growth, children in the primary grades enjoy challenges that test their growing skills. That is, as long as they are able to experience success following these challenges. This is something that teachers should keep in mind in the primary grades when they are creating material.\n\nAs far as language goes, children experience a drastic growth in the primary grades following the already substantial growth that happens in the preschool years. It is during this time period that children start to develop actual writing and reading skills, as opposed to the preschool and kindergarten years where listening, speaking, and reading skills are only just emerging. \n\nCopple, C. & Bredekamp, S. (2009). Developmentally appropriate practice in early childhood programs: Serving children from birth through age 8. 3d ed. Washington, DC: National Association for the Education of Young Children. \n"}
{"id": "265006", "url": "https://en.wikipedia.org/wiki?curid=265006", "title": "Dialogue Concerning the Two Chief World Systems", "text": "Dialogue Concerning the Two Chief World Systems\n\nThe Dialogue Concerning the Two Chief World Systems (\"Dialogo sopra i due massimi sistemi del mondo\") is a 1632 Italian-language book by Galileo Galilei comparing the Copernican system with the traditional Ptolemaic system. It was translated into Latin as \"Systema cosmicum\" () in 1635 by Matthias Bernegger. The book was dedicated to Galileo's patron, Ferdinando II de' Medici, Grand Duke of Tuscany, who received the first printed copy on February 22, 1632.\n\nIn the Copernican system, the Earth and other planets orbit the Sun, while in the Ptolemaic system, everything in the Universe circles around the Earth. The \"Dialogue\" was published in Florence under a formal license from the Inquisition. In 1633, Galileo was found to be \"vehemently suspect of heresy\" based on the book, which was then placed on the \"Index of Forbidden Books\", from which it was not removed until 1835 (after the theories it discussed had been permitted in print in 1822). In an action that was not announced at the time, the publication of anything else he had written or ever might write was also banned in Catholic countries.\n\nWhile writing the book, Galileo referred to it as his \"Dialogue on the Tides\", and when the manuscript went to the Inquisition for approval, the title was \"Dialogue on the Ebb and Flow of the Sea\". He was ordered to remove all mention of tides from the title and to change the preface because granting approval to such a title would look like approval of his theory of the tides using the motion of the Earth as proof. As a result, the formal title on the title page is \"Dialogue\", which is followed by Galileo's name, academic posts, and followed by a long subtitle. The name by which the work is now known was extracted by the printer from the description on the title page when permission was given to reprint it with an approved preface by a Catholic theologian in 1744. This must be kept in mind when discussing Galileo's motives for writing the book. Although the book is presented formally as a consideration of both systems (as it needed to be in order to be published at all), there is no question that the Copernican side gets the better of the argument.\n\nThe book is presented as a series of discussions, over a span of four days, among two philosophers and a layman:\n\n\nThe discussion is not narrowly limited to astronomical topics, but ranges over much of contemporary science. Some of this is to show what Galileo considered good science, such as the discussion of William Gilbert's work on magnetism. Other parts are important to the debate, answering erroneous arguments against the Earth's motion.\n\nA classic argument against earth motion is the lack of speed sensations of the earth surface, though it moves, by the earth's rotation, at about 1700 km/h at the equator. In this category there is a thought experiment in which a man is below decks on a ship and cannot tell whether the ship is docked or is moving smoothly through the water: he observes water dripping from a bottle, fish swimming in a tank, butterflies flying, and so on; and their behavior is just the same whether the ship is moving or not. This is a classic exposition of the Inertial frame of reference and refutes the objection that if we were moving hundreds of kilometres an hour as the Earth rotated, anything that one dropped would rapidly fall behind and drift to the west.\n\nThe bulk of Galileo's arguments may be divided into three classes:\n\n\nGenerally, these arguments have held up well in terms of the knowledge of the next four centuries. Just how convincing they ought to have been to an impartial reader in 1632 remains a contentious issue.\n\nGalileo attempted a fourth class of argument:\n\n\nAs an account of the causation of tides or a proof of the Earth's motion, it is a failure. The fundamental argument is internally inconsistent and actually leads to the conclusion that tides do not exist. But, Galileo was fond of the argument and devoted the \"Fourth Day\" of the discussion to it. The degree of its failure is—like nearly anything having to do with Galileo—a matter of controversy. On the one hand, the whole thing has recently been described in print as \"cockamamie.\" On the other hand, Einstein used a rather different description:\n\nIt was Galileo's longing for a mechanical proof of the motion of the earth which misled him into formulating a wrong theory of the tides. The \"fascinating arguments in the last conversation\" would hardly have been accepted as proof by Galileo, had his temperament not got the better of him. [Emphasis added]\nThe \"Dialogue\" does not treat the Tychonic system, which was becoming the preferred system of many astronomers at the time of publication and which was ultimately proven incorrect. The Tychonic system is a motionless Earth system but not a Ptolemaic system; it is a hybrid system of the Copernican and Ptolemaic models. Mercury and Venus orbit the Sun (as in the Copernican system) in small circles, while the sun in turn orbits a stationary Earth; Mars, Jupiter, and Saturn orbit the Sun in much larger circles, which means they also orbit the earth. The Tychonian system is mathematically equivalent to the Copernican system, except that the Copernican system predicts a stellar parallax, while the Tychonian system predicts no stellar parallax. Stellar parallax was not measurable until the 19th century, and therefore there was at the time no valid disproof of the Tychonic system on empirical grounds, nor any decisive observational evidence for the Copernican system.\n\nGalileo never took Tycho's system seriously, as can be seen in his correspondence, regarding it as an inadequate and physically unsatisfactory compromise. A reason for the absence of Tycho's system (in spite of many references to Tycho and his work in the book) may be sought in Galileo's theory of the tides, which provided the original title and organizing principle of the \"Dialogue\". For, while the Copernican and Tychonic systems are equivalent geometrically, they are quite different dynamically. Galileo's tidal theory entailed the actual, physical movement of the Earth; that is, if true, it would have provided the kind of proof that Foucault's pendulum apparently provided two centuries later. With reference to Galileo's tidal theory, there would be no difference between the Ptolemaic and Tychonic systems.\n\nGalileo also fails to discuss the possibility of non-circular orbits, although Kepler's proposal of an elliptical orbit for Mars had been published in 1609. Prince Cesi's letter to Galileo of 1612 treated the two laws of 1609 as common knowledge. Kepler's third law was published in 1619. A heliocentric system with planets in approximately elliptical orbits is deducible from Newton's laws of motion and gravity, but these were not published until 1687.\n\nPreface: To the Discerning Reader\nrefers to the ban on the \"Pythagorean opinion that the earth moves\" and says that the author \"takes the Copernican side with a pure mathematical hypothesis\". He introduces the friends Sagredo and Salviati with whom he had had discussions as well as the peripatetic philosopher Simplicio.\n\nHe starts with Aristotle's proof of the completeness and perfection of the world (i.e. the universe) because of its three dimensions. Simplicio points out that three was favoured by the Pythagoreans whereas Salviati can't understand why three legs are better than two or four. He suggests that the numbers were \"trifles which later spread among the vulgar\" and that their definitions, such as those of straight lines and right angles, were more useful in establishing the dimensions. Simplicio's response was that Aristotle thought that in physical matters mathematical demonstration wasn't always needed. \n\nSalviati attacks Aristotle's definition of the heavens as incorruptible and unchanging whilst only the lunar-bound zone shows change. He points to the changes seen in the skies: the new stars of 1572 and 1604 and sunspots, seen through the new telescope. There is a discussion about Aristotle's use of \"a priori\" arguments. Salviati suggests that he uses his experience to choose an appropriate argument to prove just as others do and that he would change his mind in the present circumstances.\n\nSimplicio argues that sunspots could simply be small opaque objects passing in front of the sun, but Salviati points out that some appear or disappear randomly and those at the edge are flattened, unlike separate bodies. Therefore, \"it is better Aristotelian philosophy to say 'Heaven is alterable because my senses tell me' than 'Heaven is unalterable because Aristotle was so persuaded by reasoning'\".\n\nExperiments with a mirror are used to show that the moon's surface must be opaque and not a perfect crystal sphere as Simplicio believes. He refuses to accept that mountains on the moon cause shadows, or that reflected light from the earth is responsible for the faint outline in a crescent moon.\n\nSagredo holds that he considers the earth noble because of the changes in it whereas Simplicio says that change in the moon or stars would be useless because they do not benefit man. Salviati points out that days on the moon are a month long and despite the varied terrain that the telescope has disclosed, it wouldn't sustain life. Humans acquire mathematical truths slowly and hesitantly, whereas God knows the full infinity of them intuitively. And when one looks into the marvellous things men have understood and contrived, then clearly the human mind is one of the most excellent of God's works.\n\nSalviati starts by repeating that Aristotle would be changing his opinions if he saw what they were seeing. \"It is the followers of Aristotle who have crowned him with authority, not he who has usurped or appropriated it to himself.\"\n\nThere is one supreme motion—that by which the sun, moon, planets and fixed stars appear to be moved from east to west in the space of 24 hours. This may as logically belong to the earth alone as to the rest of the universe. Aristotle and Ptolemy, who understood this, do not argue against any other motion than this diurnal one.\n\nMotion is relative: the position of the sacks of grain on a ship can be identical at the end of the voyage despite the movement of the ship. Why should we believe that nature moves all these extremely large bodies with inconceivable velocities rather than simply moving the moderately sized earth? If the earth is removed from the picture, what happens to all the movement?\n\nThe movement of the skies from east to west is the opposite of all the other motions of the heavenly bodies which are from west to east; making the earth rotate brings it into line with all the others. Although Aristotle argues that circular motions are not contraries, they could still lead to collisions.\n\nThe great orbits of the planets take longer than the shorter: Saturn and Jupiter take many years, Mars two, whereas the moon takes only a month. Jupiter's moons take even less. This is not changed if the earth rotates every day, but if the earth is stationary then we suddenly find that the sphere of the fixed stars rotates in 24 hours. Given the distances, that would more reasonably be thousands of years.\n\nIn addition some of these stars have to travel faster than others: if the Pole Star was precisely at the axis, then it would be entirely stationary whereas those of the equator have unimaginable speed. The solidity of this supposed sphere is incomprehensible. Make the earth the primum mobile and the need for this extra sphere disappears.\nThey consider three main objections to the motion of the earth: that a falling body would be left behind by the earth and thus fall far to the west of its point of release; that a cannonball fired to the west would similarly fly much further than one fired to the east; and that a cannonball fired vertically would also land far to the west. Salviati shows that these do not take account of the impetus of the cannon.\n\nHe also points out that attempting to prove that the earth doesn't move by using vertical fall commits the logical fault of paralogism (assuming what is to be proved), because if the earth is moving then it is only in appearance that it is falling vertically; in fact it is falling at a slant, as happens with a cannonball rising through the cannon (illustrated).\n\nIn rebutting a work which claims that a ball falling from the moon would take six days to arrive, the odd-number rule is introduced: a body falling 1 unit in an interval would fall 3 units in the next interval, 5 units in the subsequent one, etc. This gives rise to the rule by which the distance fallen is according to the square of the time. Using this he calculates the time is really little more than 3 hours. He also points out that density of the material doesn't make much difference: a lead ball might only accelerate twice as fast as one of cork.\n\nIn fact, a ball falling from such a height wouldn't fall behind but ahead of the vertical because the rotational motion would be in ever-decreasing circles. What makes the earth move is similar to whatever moves Mars or Jupiter and is the same as that which pulls the stone to earth. Calling it gravity doesn't explain what it is.\n\nSalviati starts by dismissing the arguments of a book against the novas he has been reading overnight. Unlike comets, these were stationary and their lack of parallax easily checked and thus could not have been in the sublunary sphere.\n\nSimplicio now gives the greatest argument against the annual motion of the earth that if it moves then it can no longer be the center of the zodiac, the world. Aristotle gives proofs that the universe is finite bounded and spherical. Salvatius points out that these disappear if he denies him the assumption that it is movable, but allows the assumption initially in order not to multiply disputes.\nHe points out that if anything is the centre, it must be the sun not the earth, because all the planets are closer or further away from the earth at different times, Venus and Mars up to eight times. He encourages Simplicio to make a plan of the planets, starting with Venus and Mercury which are easily seen to rotate about the sun. Mars must also go about the sun (as well as the earth) since it is never seen horned, unlike Venus now seen through the telescope; similarly with Jupiter and Saturn. Earth, which is between Mars with a period of two years and Venus with nine months, has a period of a year which may more elegantly be attributed to motion than a state of rest.\n\nSagredo brings up two other common objections. If the earth rotated, the mountains would soon be in a position that one would have to descend them rather than ascend. Secondly, the motion would be so rapid that someone at the bottom of a well would have only a brief instance to glimpse a star as it traversed. Simplicio can see that the first is no different from travelling over the globe, as any who have circumnavigated but though he realises the second is the same as if the heavens were rotating, he still doesn't understand it. Salviati says the first is no different from those who deny the antipodes. For the second, he encourages Simplicio to decide what fraction of the sky can be seen from down the well.\n\nSalviati brings up another problem, which is that Mars and Venus are not as variable as the theory would suggest. He explains that the size of a star to the human eye is affected by the brightness and the sizes are not real. This is resolved by use of the telescope which also shows the crescent shape of Venus. A further objection to the movement of the earth, the unique existence of the moon, has been resolved by the discovery of the moons of Jupiter, which would appear like our moon to any Jovian.\nCopernicus has succeeded in reducing some of the uneven motions of Ptolemy who had to deal with motions that sometimes go fast, sometimes slow, and sometimes backwards, by means of vast epicycles. Mars, above the sun's sphere, often falls far below it, then soars above it. These anomalies are cured by the annual movement of the earth. This is explained by a diagram in which the varying motion of Jupiter is shown using the earth's orbit.\n\nSimplicio produces another booklet in which theological arguments are mixed with astronomic, but Salviati refuses to address the issues from Scripture. So he produces the argument that the fixed stars must be at an inconceivable distance with the smallest larger than the whole orbit of the earth. Salviati explains that this all comes from a misrepresentation of what Copernicus said, resulting in a huge over-calculation of the size of a sixth magnitude star. But many other famous astronomers over-estimated the size of stars by ignoring the brightness factor. Not even Tycho, with his accurate instruments, set himself to measure the size of any star except the sun and moon. But Salviati (Galileo) was able to make a reasonable estimate simply by hanging a cord to obscure the star and measuring the distance from eye to cord.\n\nBut still many cannot believe that the fixed stars can individually be as big or bigger than the sun. To what end are these? Salviati maintains that \"it is brash for our feebleness to attempt to judge the reasons for God's actions, and to call everything in the universe vain and superfluous which does not serve us\".\n\nHas Tycho or any of his disciples tried to investigate in any way phenomena that might affirm or deny the movement of the earth? Do any of them know how much variation is needed in the fixed stars? Simplicio objects to conceding that the distance of the fixed stars is too great for it to be detectable. Salviati points out how difficult it is even to detect the varying distances of Saturn. Many of the positions of the fixed stars are not known accurately and far better instruments than Tycho's are needed: say using a sight with a fixed position 60 miles away.\n\nSagredo then asks Salviati to explain how the Copernican system explains the seasons and inequalities of night and day. This he does with the aid of a diagram showing the position of the earth in the four seasons. He points out how much simpler it is than the Ptolemaic system. But Simplicio thinks Aristotle was wise to avoid too much geometry. He prefers Aristotle's axiom to avoid more than one simple motion at a time.\n\nThey are in Sagredo's house in Venice, where tides are an important issue, and Salviati wants to show the effect of the earth's movement on the tides. He first points out the three periods of the tides: \"daily (diurnal)\", generally with intervals of 6 hours of rising and six more of falling; \"monthly\", seemingly from the moon, which increases or decreases these tides; and \"annual\", leading to different sizes at the equinoxes.\n\nHe considers first the daily motion. Three varieties are observed: in some places the waters rise and fall without any forward motion; in others they move towards the east and back to the west without rising or falling; in still others there is a combination of both—this happens in Venice where the waters rise on entering and fall on leaving. In the Straits of Messina there are very swift currents between Scylla and Charybdis. In the open Mediterranean the alteration of height is small but the currents are noticeable.\n\nSimplicio counters with the peripatetic explanations, which are based on the depths of the sea, and the dominion of the moon over the water, though this doesn't explain the risings when the moon is below the horizon. But he admits it could be a miracle.\n\nWhen the water in Venice rises, where does it come from? There is little rise in Corfu or Dubrovnik. From the ocean through the Straits of Gibraltar? It's much too far away and the currents are too slow.\n\nSo could the movement of the container cause the disturbance? Consider the barges that bring water into Venice. When they hit an obstacle, the water rushes forward; when they speed up it will go to the back. For all this disturbance there is no need for new water and the level in the middle stays largely constant though the water there rushes backwards and forwards.\n\nConsider a point on the earth under the joint action of the annual and diurnal movements. At one time these are added together and 12 hours later they act against each other, so there is an alternate speeding up and slowing down. So the ocean basins are affected in the same way as the barge particularly in an east-west direction. Note also that the length of the barge makes a difference to the speed of oscillations, just as the length of a plumb bob changes its speed. The depth of water also makes a difference to the size of vibrations.\n\nThe primary effect only explains tides once a day; one must look elsewhere for the six-hour change, to the oscillation periods of the water. In some places, such as the Hellespont and the Aegean the periods are briefer and variable. But a north-south sea like the Red Sea has very little tide whereas the Messina Strait carries the pent up effect of two basins.\n\nSimplicio objects that if this accounts for the water, should it not even more be seen in the winds? Salviati suggests that the containing basins are not so effective and the air does not sustain its motion. Nevertheless, these forces are seen by the steady winds from east to west in the oceans in the torrid zone.\n\nIt seems that the moon also is taking part in the production of the daily effects, but that is repugnant to his mind. The motions of the moon have caused great difficulty to astronomers. It's impossible to make a full account of these things given the irregular nature of the sea basins.\n\n\n"}
{"id": "10923902", "url": "https://en.wikipedia.org/wiki?curid=10923902", "title": "Dream Pool Essays", "text": "Dream Pool Essays\n\nThe Dream Pool Essays or Dream Torrent Essays (Pinyin: \"Mèng Xī Bǐ Tán\"; Wade-Giles: \"Meng⁴ Hsi¹ Pi³-t'an²\"; Chinese: 夢溪筆談／梦溪笔谈) was an extensive book written by the Han Chinese polymath, genius, scientist and statesman Shen Kuo (1031–1095) by 1088 AD, during the Song dynasty (960–1279) of China. Although Shen was previously a highly renowned government official and military general, he compiled this enormous written work while virtually isolated on his lavish garden estate near modern-day Zhenjiang, Jiangsu province. He named the book after the name he gave to his estate, the \"Dream Brook\". The literal translated meaning is \"Brush Talks from a Dream Brook\", and Shen Kuo is quoted as saying:\n\nAs the historian Chen Dengyuan points out, much of Shen Kuo's written work was probably purged under the leadership of minister Cai Jing (1046–1126). For example, only six of Shen's books remain, and four of these have been significantly altered since the time they were penned by the author. The \"Dream Pool Essays\" was first quoted in a Chinese written work of 1095 AD, showing that even towards the end of Shen's life his final book was becoming widely printed. The book was originally 30 chapters long, yet an unknown Chinese author's edition of 1166 AD edited and reorganized the work into 26 chapters. There is one surviving copy of this 1166 edition housed now in Japan, while a Chinese reprint was produced in 1305 as well. In 1631 another edition was printed, but it was heavily reorganized into three broad chapters.\n\n\nWith Shen's writings on fossils, geomorphology, and shifting geographical climates, he states in the following passages:\nIn the Zhi-ping reign period [1064–67 AD] a man of Zezhou was digging a well in his garden, and unearthed something shaped like a squirming serpent, or dragon. He was so frightened by it that he dared not touch it, but after some time, seeing that it did not move, he examined it and found it to be stone. The ignorant country people smashed it, but Zheng Boshun, who was magistrate of Jincheng at the time, got hold of a large piece of it on which scale-like markings were to be seen exactly like those on a living creature. Thus a serpent or some kind of marine snake (\"chhen\") had certainly been turned to stone, as happens with the 'stone-crabs'.\n\nIn recent years [cca. 1080] there was a landslide on the bank of a large river in Yong-ning Guan near Yanzhou. The bank collapsed, opening a space of several dozens of feet, and under the ground a forest of bamboo shoots was thus revealed. It contained several hundred bamboo with their roots and trunks all complete, and all turned to stone...Now bamboos do not grow in Yanzhou. These were several dozens of feet below the present surface of the ground, and we do not know in what dynasty they could possibly have grown. Perhaps in very ancient times the climate was different so that the place was low, damp, gloomy, and suitable for bamboos. On the Jin-hua Shan in Wuzhou there are stone pine-cones, and stones formed from peach kernels, stone bulrush roots, stone fishes, crabs, and so on, but as these are all (modern) native products of that place, people are not very surprised at them. But these petrified bamboos appeared under the ground so deep, though they are not produced in that place today. This is a very strange thing.\n\nWhen the Director of the Astronomical Observatory asked Shen Kuo if the shapes of the sun and moon were round like balls or flat like fans, Shen Kuo explained his reasoning for the former:\nIf they were like balls they would surely obstruct each other when they met. I replied that these celestial bodies were certainly like balls. How do we know this? By the waxing and waning of the moon. The moon itself gives forth no light, but is like a ball of silver; the light is the light of the sun (reflected). When the brightness is first seen, the sun(-light passes almost) alongside, so the side only is illuminated and looks like a crescent. When the sun gradually gets further away, the light shines slanting, and the moon is full, round like a bullet. If half of a sphere is covered with (white) powder and looked at from the side, the covered part will look like a crescent; if looked at from the front, it will appear round. Thus we know that the celestial bodies are spherical.\nWhen the director of the astronomical observatory asked Shen Kuo why eclipses occurred only on an occasional basis while in conjunction and opposition once a day, Shen Kuo wrote:\nI answered that the ecliptic and the moon's path are like two rings, lying one over the other, but distant by a small amount. (If this obliquity did not exist), the sun would be eclipsed whenever the two bodies were in conjunction, and the moon would be eclipsed whenever they were exactly in position. But (in fact) though they may occupy the same degree, the two paths are not (always) near (each other), and so naturally the bodies do not (intrude) upon one another.\nOn the use of the sighting tube to fix the position of the pole star, Shen Kuo wrote:\nBefore Han times it was believed that the pole star was in the center of the sky, so it was called Jixing (Summit star). Zu Geng(-zhi) found out with the help of the sighting tube that the point in the sky which really does not move was a little more than 1 degree away from the summit star. In the Xining reign-period (1068–1077) I accepted the order of the emperor to take charge of the Bureau of the Calendar. I then tried to find the true pole by means of the tube. On the very first night I noticed that the star which could be seen through the tube moved after a while outside the field of view. I realized, therefore, that the tube was too small, so I increased the size of the tube by stages. After three months' trials I adjusted it so that the star would go round and round within the field of view without disappearing. In this way I found that the pole star was distant from the true pole somewhat more than 3 degrees. We used to make the diagrams of the field, plotting the positions of the star from the time when it entered the field of view, observing after nightfall, at midnight, and early in the morning before dawn. Two hundred of such diagrams showed that the 'pole star' was really a circumpolar star. And this I stated in my detailed report to the emperor.\n\nOn the methods of Bi Sheng's invention of movable type printing between the years 1041 to 1048 AD, Shen Kuo wrote:\n[Bi Sheng] took sticky clay and cut in it characters as thin as the edge of a coin. Each character formed, as it were, a single type. He baked them in the fire to make them hard. He had previously prepared an iron plate and he had covered his plate with a mixture of pine resin, wax, and paper ashes. When he wished to print, he took an iron frame and set it on the iron plate. In this he placed the types, set close together. When the frame was full, the whole made one solid block of type. He then placed it near the fire to warm it. When the paste [at the back] was slightly melted, he took a smooth board and pressed it over the surface, so that the block of type became as even as a whetstone. If one were to print only two or three copies, this method would be neither simple nor easy. But for printing hundreds or thousands of copies, it was marvelously quick. As a rule he kept two forms going. While the impression was being made from the one form, the type was being put in place on the other. When the printing of the one form was finished, the other was then ready. In this way the two forms alternated and the printing was done with great rapidity.\n\nOf Daoism and the inability of empirical science to explain everything in the world, Shen Kuo wrote:\nThose in the world who speak of the regularities underlying the phenomena, it seems, manage to apprehend their crude traces. But these regularities have their very subtle aspect, which those who rely on mathematical astronomy cannot know of. Still even these are nothing more than traces. As for the spiritual processes described in the [\"Book of Changes\"] that \"when they are stimulated, penetrate every situation in the realm,\" mere traces have nothing to do with them. This spiritual state by which foreknowledge is attained can hardly be sought through changes, of which in any case only the cruder sort are attainable. What I have called the subtlest aspect of these traces, those who discuss the celestial bodies attempt to know by depending on mathematical astronomy; but astronomy is nothing more than the outcome of conjecture.\n\nBelow are two passages from Shen's book outlining the basics contained in Yu Hao's \"Timberwork Manual\". Yu Hao was a Chinese architect of the earlier 10th, and Kuo was one to praise his work. In the first quote, Shen Kuo describes a scene where Yu Hao gives advice to another artisan architect about slanting struts for diagonal wind bracing:\nWhen Mr. Qian (Wei-yan) was Governor of the two Zhejiang provinces, he authorized the building of a wooden pagoda at the Fan-tian Si (Brahma-Heaven Temple) in Hangzhou with a design of twice three stories. While it was under construction General Chhien went up to the top and was worried because it swayed a little. But the Master-Builder explained that as the tiles had not yet been put on, the upper part was still rather light, hence the effect. So then they put on all the tiles, but the sway continued as before. Being at a loss what to do, he privately sent his wife to see the wife of Yu Hao with a present of golden hair pins, and enquire about the cause of the motion. (Yu) Hao laughed and said: 'That's easy, just fit in struts (pan) to settle the work, fixed with (iron) nails, and it will not move any more.' The Master-Builder followed his advice, and the tower stood quite firm. This is because the nailed struts filled in and bound together (all the members) up and down so that the six planes (above and below, front and back, left and right) were mutually linked like the cage of the thorax. Although people might walk on the struts, the six planes grasped and supported each other, so naturally there could be no more motion. Everybody acknowledged the expertise thus shown.\nIn this next quote, Shen Kuo describes the dimensions and types of architecture outlined in Yu Hao's book:\nMethods of building construction are described in the \"Timberwork Manual\", which, some say, was written by Yu Hao. (According to that book), buildings have three basic units of proportion, what is above the cross-beams follows the Upperwork Unit, what is above the ground floor follows the Middlework Unit, and everything below that (platforms, foundations, paving, etc.) follows the Lowerwork Unit. The length of the cross-beams will naturally govern the lengths of the uppermost cross-beams as well as the rafters, etc. Thus for a (main) cross-beam of (8 ft) length, an uppermost cross-beam of (3.5 ft) length will be needed. (The proportions are maintained) in larger and smaller halls. This (2/28) is the Upperwork Unit. Similarly, the dimensions of the foundations must match the dimensions of the columns to be used, as also those of the (side-) rafters, etc. For example, a column (11 ft) high will need a platform (4.5 ft) high. So also for all the other components, corbelled brackets, projecting rafters, other rafters, all have their fixed proportions. All these follow the Middlework Unit (2/24). Now below of ramps (and steps) there are three kinds, steep, easy-going, and intermediate. In places these gradients are based upon a unit derived from the imperial litters. Steep ramps are ramps for ascending which the leading and trailing bearers have to extend their arms fully down and up respectively (ratio 3/35). Easy-going ramps are those for which the leaders use elbow length and the trailers shoulder height (ratio 1/38); intermediate ones are negotiated by the leaders with downstretched arms and trailers at shoulder height (ratio 2/18). These are the Lowerwork Units. The book (of Yu Hao) had three chapters. But builders in recent years have become much more precise and skillful (yen shan) than formerly. Thus for some time past the old Timberwork Manual has fallen out of use. But (unfortunately) there is hardly anybody capable of writing a new one. To do that would be a masterpiece in itself!\n\nShen Kuo described the natural predator insect similarly shaped to the \"gou-he\" (\"dog-grubs\") which preyed upon the agricultural pest infestation of \"zi-fang\", the moth \"Leucania separata\":\nIn the Yuan-Feng reign period (1078–1085), in the Qingzhou region, an outbreak of \"zi-fang\" insects caused serious damage to the crops in the fields in autumn. Suddenly another insect appeared in swarms of thousands and tens of thousands, covering the entire ground area. It was shaped like earth-burrowing \"gou-he\" (dog grubs), and its mouth was flanked by pincers. Whenever it met a \"zi-fang\", it would seize it with the pincers and break the poor beast into two bits. Within ten days all the \"zi-fang\" had disappeared, so the locality had an abundant harvest. Such kinds of insects have been known since antiquity and the local people call them \"pang-bu-ken\" (\"not allowing other [insects] to be\").\n\nAround 1078, Shen Kuo wrote an accurate description of the damaging effects of lightning to buildings and to the specific materials of objects within. Taking an objective and speculative viewpoint, he stated:\nA house belonging to Li Shunju was struck by lightning. Brilliant sparkling light was seen under the eaves. Everyone thought that the hall would be burnt, and those who were inside rushed out. After the thunder had abated, the house was found to be alright, though its walls and the paper on the windows were blackened. On certain wooden shelves, certain lacquered vessels with silver mouths had been struck by the lightning, so that the silver had melted and dropped to the ground, but the lacquer was not even scorched. Also, a valuable sword made of strong steel had been melted to liquid, without the parts of the house nearby being affected. One would have thought that the thatch and wood would have been burnt up first, yet here were metals melted and no injury to thatch and wood. This is beyond the understanding of ordinary people. There are Buddhist books which speak of 'dragon fire' which burns more fiercely when it meets with water instead of being extinguished by water like 'human' fire. Most people can only judge of things by the experiences of ordinary life, but phenomena outside the scope of this are really quite numerous. How insecure it is to investigate natural principles using only the light of common knowledge, and subjective ideas.\nA passage called \"Strange Happenings\" contains a peculiar account of an unidentified flying object. Shen wrote that, during the reign of Emperor Renzong (1022–1063), an object as bright as a pearl occasionally hovered over the city of Yangzhou at night, but described first by local inhabitants of eastern Anhui and then in Jiangsu. Shen wrote that a man near Xingkai Lake observed this curious object; allegedly it:\n...opened its door and a flood of intense light like sunbeams darted out of it, then the outer shell opened up, appearing as large as a bed with a big pearl the size of a fist illuminating the interior in silvery white. The intense silver-white light, shot from the interior, was too strong for human eyes to behold; it cast shadows of every tree within a radius of ten miles. The spectacle was like the rising Sun, lighting up the distant sky and woods in red. Then all of a sudden, the object took off at a tremendous speed and descended upon the lake like the Sun setting.\nShen went on to say that Yibo, a poet of Gaoyou, wrote a poem about this \"pearl\" after witnessing it. Shen wrote that since the \"pearl\" often made an appearance around Fanliang in Yangzhou, the people there erected a \"Pearl Pavilion\" on a wayside, where people came by boat in hopes to see the mysterious flying object.\n\nAround 1065 Shen Kuo wrote about the assembly methods for swords, and the patterns produced in the steel:\nAncient people use \"chi kang\", (combined steel), for the edge, and \"jou thieh\" (soft iron) for the back, otherwise it would often break. Too strong a weapon will cut and destroy its own edge; that is why it is advisable to use nothing but combined steel. As for the \"yu-chhang\" (fish intestines) effect, it is what is now called the 'snake-coiling' steel sword, or alternatively, the 'pine tree design'. If you cook a fish fully and remove its bones, the shape of its guts will be seen to be like the lines on a 'snake-coiling sword'.\n\nShen Kuo observed that the Chinese since some centuries prior had entirely adopted barbarian fashions.\n中國衣冠，自北齊以來，乃全用胡服。窄袖、緋綠短衣、長靿靴、有鞢帶，皆胡服也。窄袖利於馳射，短衣、長靿皆便於涉草。胡人樂茂草，常寢處其間，予使北時皆見之。雖王庭亦在深荐中。予至胡庭日，新雨過，涉草，衣褲皆濡，唯胡人都無所沾。帶衣所垂蹀躞，蓋欲佩帶弓劍、帨、算囊、刀勵之類。\n\nThe clothing of China since the Northern Qi [550–557] onward has been entirely made barbarian. Narrow sleeves, short dark red or green robes, tall boots and metal girdle ornaments are all barbarian garb. The narrow sleeves are useful when shooting while galloping. The short robes and tall boots are convenient when passing through tall grass. The barbarians all enjoy thick grass as they always sleep in it. I saw them all do it when I was sent north. Even the king's court is in the deep grasses. On the day I had arrived at the barbarian court the new rains had passed and I waded through the grass. My robes and trousers were all soaked, but the barbarians were not at all wet. With things hanging from robe and belt they walk about. One perhaps might want to hang items like a bow and blade, handkerchief, coin purse or knife from the belt.\n\nOn the humanities:\n\nOn natural sciences:\n\nHumanistic sciences:\n\n\n\n\n\n"}
{"id": "2791047", "url": "https://en.wikipedia.org/wiki?curid=2791047", "title": "Egg drop competition", "text": "Egg drop competition\n\nThe egg drop contest is an experiment usually performed by college or primary school students. Competitors typically attempt to create a device that can keep a raw chicken egg intact when dropped from a height. Students are asked to build a device made from a limited or not limited amount of materials to support an egg when dropped from various heights.\n\nA variation of the egg drop competition is the naked egg drop, in which an exposed raw egg is dropped into a container below that must catch the egg and keep it from breaking. \n\nAnother common variation on the egg drop competition is the egg hurl competition, where the containers are hurled by a device such as a trebuchet or air cannon. This variation is often used by schools that lack tall structures from which to drop the containers. The egg hurl variation adds additional difficulties to the design of the container, since it is initially hurled at high speed and has to cope with horizontal as well as vertical velocities upon landing.\n\nOften schools work together to make larger competitions that pit more students against each other. One of the larger regional egg drop competitions is the Winston-Salem / Forsyth County Egg Drop Competition that takes place during Engineers Week (late February) each year. \n\nEgg drop is one of 18 events in the North Carolina Science Olympiad elementary competition. More than 100 teams compete in this annual competition. \n\n"}
{"id": "12891588", "url": "https://en.wikipedia.org/wiki?curid=12891588", "title": "Experimental design diagram", "text": "Experimental design diagram\n\nExperimental Design Diagram (EDD) is a diagram used in science classrooms to design an experiment. This diagram helps to identify the essential components of an experiment. It includes a title, the research hypothesis and null hypothesis, the independent variable, the levels of the independent variable, the number of trials, the dependent variable, the operational definition of the dependent variable and the constants.\n\n[science way of controlling the data]\n"}
{"id": "1110558", "url": "https://en.wikipedia.org/wiki?curid=1110558", "title": "Five Billion Years of Change", "text": "Five Billion Years of Change\n\nFive Billion Years of Change: A History of the Land (2003, ) is a book by Denis Wood that attempts a holistic view of reality that ranges from the Big Bang to the World Wide Web. Specifically, this books deals with the formation of various structures:\n\nA key theme is repeated through this book: humans have a tendency to divide our understandings into \"history\" and \"prehistory\". People are shocked when some event from prehistory intrudes upon their current lives; Wood likens the shock of this intrusion to an expulsion from the Garden of Eden. This division is a metaphor for various artificial divisions; for example:\nInstead of thinking in terms of artificial divisions of \"now\" and \"back then\", readers should develop an intuitive mindset of graduated changes in which the \"fossils\" of the ancient past are intermingled with contemporary objects; for instance, the \"oxygen holocaust\" of the paleoproterozoic eon exists in today's oxygen-rich atmosphere.\n\nAlso, the heroic saga induces another faulty thinking style that obscures the true nature of the world. To understand the real story of humanity, Wood argues that people must focus on the mass actions of people or of large impersonal forces rather than a few heroes or kings. Hollywood movies dealing with ecological threats are especially misleading; rather than imparting an accurate image of ecological issues, movies present a villain such as a mad scientist or a greedy, evil business person. Instead, such entertainment and much news reporting distracts us from our individual actions that are at the heart of ecological problems.\n\nInaccurate ways of thinking induce a passive helplessness. Instead, by presenting a sweeping story of successive, interlinked, long term trends, the author hopes to give readers a flexible, authentic model of the world. With that model, readers will be capable of understanding (and possibly dealing with) current global challenges.\n"}
{"id": "1890042", "url": "https://en.wikipedia.org/wiki?curid=1890042", "title": "Five Ws", "text": "Five Ws\n\nThe Five Ws (sometimes referred to as Five Ws and How, 5W1H, or Six Ws) are questions whose answers are considered basic in information gathering or problem solving. They are often mentioned in journalism (\"cf.\" news style), research and police investigations. They constitute a formula for getting the complete story on a subject. According to the principle of the Five Ws, a report can only be considered complete if it answers these questions starting with an interrogative word:\n\nSome authors add a sixth question, \"how\", to the list: \n\nEach question should have a factual answer — facts necessary to include for a report to be considered complete. Importantly, none of these questions can be answered with a simple \"yes\" or \"no\".\n\nIn the United Kingdom (excluding Scotland), the Five Ws are used in Key Stage 2 and Key Stage 3 lessons.\n\nThe Five Ws and How were long attributed to Hermagoras of Temnos. But in 2010, Sloan established Aristotle's Nicomachean Ethics as the source of the elements of circumstance or \"Septem Circumstantiae\". Thomas Aquinas had much earlier acknowledged Aristotle as the originator of the elements of circumstances, providing a detailed commentary on Aristotle 's system in his “Treatise on human acts” and specifically in part one of two Q7 “Of the Circumstances of Human Acts”. Thomas Aquinas examines the concept of Aristotle's voluntary and involuntary action in his \"Summa Theologia\" as well as a further set of questions about the elements of circumstance. Primarily he asks \"Whether a circumstance is an accident of a human act\" (Article 1), \"Whether Theologians should take note of the circumstances of human acts?\" (Article 2), \"Whether the circumstances are properly set forth (in Aristotle's) third book of Ethics\" (Article 3) and \"Whether the most important circumstances are 'Why' and 'In What the act consists'? (Article 4). For in acts we must take note of \"who\" did it, by what aids or instruments he did it (\"with\"), \"what\" he did, \"where\" he did it, \"why\" he did it, \"how\" and \"when\" he did it.For Aristotle, the elements are used in order to distinguish voluntary or involuntary action. Because Aristotle employs this schema as a primordial crucible for defining the difference between voluntary and involuntary agents (a topic of incalculable importance in the works of Aristotle), the benefits of locating this schema within Aristotle, and ultimately providing clarification of the passage, \"may prove helpful to a number of disciplines\"(Sloan 2010, 236).These elements of circumstances are used by Aristotle as a framework to describe and evaluate moral action in terms of What was/should be done, Who did it, How it was done, Where it happened, and most importantly for what reason (Why), and so on for all the other elements. He outlines them as follows in the Ethics as translated by Sloan.Therefore it is not a pointless endeavor to divide these circumstances by kind and number; (1) the \"Who\", (2) the \"What\", (3) around what place (\"Where\") or (4) in which time something happens (\"When\"), and sometimes (5) with what, such as an instrument (\"With\"), (6) for the sake of what (\"Why\"), such as saving a life, and (7) the (\"How\"), such as gently or violently…And it seems that the most important circumstances are those just listed, including the \"Why\".For Aristotle (in Sloan), ignorance of any of these elements can imply involuntary action.Thus, with ignorance as a possibility concerning all these things, that is, \"the circumstances of the act\", the one who acts in ignorance of any of them seems to act involuntarily, and especially regarding the most important ones. And it seems that the most important circumstances are those just listed, including the \"Why\"In the Politics, Aristotle illustrates why the elements are important in terms of human (moral) action.I mean, for instance (a particular circumstance or movement or action), How could we advise the Athenians whether they should go to war or not, if we did not know their strength (\"How much\"), whether it was naval or military or both (\"What kind\"), and how great it is (\"How many\"), what their revenues amount to (\"With\"), Who their friends and enemies are (\"Who\"), what wars, too they have waged (\"What\"), and with what success; and so on.Essentially, these elements of circumstances provide a theoretical framework that can be used to particularize, explain or predict \"any\" given set of circumstances of action. Hermagoras went so far as to claim that \"all\" hypotheses are derived from these seven circumstances.In other words, no hypothetical question, or question involving particular persons and actions, can arise without reference to these circumstances, and no demonstration of such a question can be made without using them.In any particular act or situation, one needs to interrogate these questions in order to determine the actual circumstances of the action.It is necessary for students of virtue to differentiate between the Voluntary and Involuntary; such a distinction should even prove useful to the lawmaker for assigning honors and punishments. This aspect is encapsulated by Aristotle in Rhetoric as \"forensic speech\" and is used to determine \"\"The characters and circumstances which lead men to commit wrong, or make them the victims of wrong”\" in order to accuse or defend. It is this application of the elements of circumstances that was emphasised by latter rhetoricians.\n\nEven though the classical origin of these questions as situated in ethics had long been lost, they have been a standard way of formulating or analyzing rhetorical questions since antiquity. The rhetor Hermagoras of Temnos, as quoted in pseudo-Augustine's \"De Rhetorica\", applied Aristotle's \"elements of circumstances\" (μόρια περιστάσεως) as the loci of an issue:\n\nSt. Thomas Aquinas also refers to the elements as used by Cicero in De Inventione (Chap. 24 DD1, 104) as:\"Quis, quid, ubi, quibus auxiliis, cur, quomodo, quando.\"Similarly, Quintilian discussed \"loci argumentorum\", but did not put them in the form of questions.\n\nVictorinus explained Cicero's application of the elements of circumstances by putting them into correspondence with Hermagoras's questions:\nJulius Victor also lists circumstances as questions.\n\nBoethius \"made the seven circumstances fundamental to the arts of prosecution and defense\":\n\nThe question form was taken up again in the 12th century by Thierry de Chartres and John of Salisbury.\n\nTo administer suitable penance to sinners, the 21st canon of the Fourth Lateran Council (1215) enjoined confessors to investigate both sins and the circumstances of the sins. The question form was popular for guiding confessors, and it appeared in several different forms:\n\nThe method of questions was also used for the systematic exegesis of a text.\n\nIn the 16th century, Thomas Wilson wrote in English verse:\nWho, what, and where, by what helpe, and by whose:<br>\nWhy, how, and when, doe many things disclose.\n\nIn the United States in the 19th century, Prof. William Cleaver Wilkinson popularized the \"Three Ws\" – What? Why? What of it? – as a method of Bible study in the 1880s, although he did not claim originality. This would also became the \"Five Ws\", but the application was rather different from that in journalism:\n\n\"What? Why? What of it?\" is a plan of study of alliterative methods\nfor the teacher emphasized by Professor W.C. Wilkinson not as original\nwith himself but as of venerable authority. \"It is, in fact,\" he says,\n\"an almost immemorial orator's analysis. First the facts, next the\nproof of the facts, then the consequences of the facts. This analysis\nhas often been expanded into one known as \"The Five Ws:\" \"When? Where? Who?\nWhat? Why?\" Hereby attention is called, in the study of any lesson: to the\ndate of its incidents; to their place or locality; to the person\nspeaking or spoken to, or to the persons introduced, in the narrative; to\nthe incidents or statements of the text; and, finally, to the\napplications and uses of the lesson teachings.\n\nThe \"Five Ws\" (and one H) were memorialized by Rudyard Kipling in his \"Just So Stories\" (1902), in which a poem, accompanying the tale of \"The Elephant's Child\", opens with:\n\nBy 1917, the \"Five Ws\" were being taught in high-school journalism classes, and by 1940, the tendency of journalists to address all of the \"Five Ws\" within the lead paragraph of an article was being characterized as old-fashioned and fallacious:\n\nStarting in the 2000s, the Five W's were sometimes misattributed to Kipling, especially in the management and quality literature, and contrasted with the 5 Whys.\n\nIn each of English and Latin, most of the interrogative words begin with the same sound, \"wh\" in English, \"qu\" in Latin. This is not a coincidence, as they both come from the Proto-Indo-European root \"ko-\", reflected in Proto-Germanic as \"χa-\" or \"kha-\" and in Latin as \"qu-\".\n\n"}
{"id": "543667", "url": "https://en.wikipedia.org/wiki?curid=543667", "title": "Flood geology", "text": "Flood geology\n\nFlood geology (also creation geology or diluvial geology) is the attempt to interpret and reconcile geological features of the Earth in accordance with a literal belief in the global flood described in Genesis . In the early 19th century, diluvial geologists hypothesized that specific surface features were evidence of a worldwide flood which had followed earlier geological eras; after further investigation they agreed that these features resulted from local floods or glaciers. In the 20th century, young Earth creationists revived flood geology as an overarching concept in their opposition to evolution, assuming a recent six-day Creation and cataclysmic geological changes during the Biblical Deluge, and incorporating creationist explanations of the sequence of rock strata.\n\nIn the early stages of development of the science of geology, fossils were interpreted as evidence of past flooding. The \"theories of the Earth\" of the 17th century proposed mechanisms based on natural laws, within a timescale set by the biblical chronology. As modern geology developed, geologists found evidence of an ancient Earth, and evidence inconsistent with the notion that the Earth had developed in a series of cataclysms, the most recent of which could be attributed to the Genesis flood. In early 19th-century Britain, \"Diluvialism\" attributed landforms and surface features such as beds of gravel and erratic boulders to the destructive effects of this supposed global Deluge, but by 1830 geologists increasingly found that the evidence only showed relatively local floods. Attempts were made by so-called scriptural geologists to give primacy to literal Biblical explanations, but they lacked background in geology and were marginalised by the scientific community, as well as having little influence on the church.\n\nFlood geology was revived as a field of study within creation science, which is a part of young Earth creationism.\nProponents hold to a literal reading of and view its passages to be historically accurate, using the Bible's internal chronology to place the Flood and the story of Noah's Ark within the last five thousand years.\n\nThe key tenets of flood geology are refuted by scientific analysis. Flood geology contradicts the scientific consensus in geology, stratigraphy, geophysics, physics, paleontology, biology, anthropology, and archeology. Modern geology, its sub-disciplines and other scientific disciplines utilize the scientific method. In contrast, flood geology does not adhere to the scientific method, and it is, therefore, a pseudoscience.\n\nIn pre-Christian times, fossils found on land were thought by Greek philosophers, including Xenophanes, Xanthus and Aristotle, to be evidence that the sea had in past ages covered the land. Their concept of vast time periods in an eternal cosmos was rejected by early Christian writers as incompatible with their belief in Creation by God. Among the church fathers, Tertullian spoke of fossils demonstrating that mountains had been overrun by water without explicitly saying when. Chrysostom and Augustine believed that fossils were the remains of animals that were killed and buried during the brief duration of the Biblical Genesis Flood, and later Martin Luther viewed fossils as having resulted from the Flood.\n\nOther scholars, including Avicenna, thought fossils were produced in the rock by \"petrifying virtue\" acting on \"seeds\" of plants and animals. In 1580 Bernard Palissy speculated that fossils had formed in lakes, and natural historians subsequently disputed the alternatives. Robert Hooke made empirical investigations, and doubted that the numbers of fossil shells or depth of shell beds could have formed in the one year of Noah's Flood. In 1616 Nicolas Steno showed how chemical processes changed organic remains into stone fossils. His fundamental principles of stratigraphy published in 1669 established that rock strata formed horizontally and were later broken and tilted, though he assumed these processes would occur within 6,000 years including a worldwide Flood.\n\nIn his influential \"Principles of Philosophy\" of 1644, René Descartes applied his mechanical physical laws to envisage swirling particles forming the Earth as a layered sphere. This natural philosophy was recast in Biblical terms by the theologian Thomas Burnet, whose \"Sacred Theory of the Earth\" published in the 1680s proposed complex explanations based on natural laws, and explicitly rejected the simpler approach of invoking miracles as incompatible with the methodology of natural philosophy (the precursor to science). Burnet maintained that less than 6,000 years ago the Earth had emerged from chaos as a perfect sphere, with paradise on land over a watery abyss. This crust had dried out and cracked, and its collapse caused the Biblical Deluge, forming mountains as well as underground caverns where the water retreated. He made no mention of fossils, but inspired other diluvial theories that did.\n\nIn 1695, John Woodward's \"An Essay Toward a Natural History of the Earth\" viewed the Genesis Flood as dissolving rocks and earth into a thick slurry which caught up all living things, and when the waters settled formed strata according to the specific gravity of these materials, including fossils of the organisms. When it was pointed out that lower layers were often less dense and forces that shattered rock would destroy organic remains, he resorted to the explanation that a divine miracle had temporarily suspended gravity. William Whiston's \"New Theory of the Earth\" of 1696 combined scripture with Newtonian physics to propose that the original chaos was the atmosphere of a comet with the days of Creation each taking a year, and the Genesis Flood had resulted from a second comet. His explanation of how the Flood caused mountains and the fossil sequence was similar to Woodward's. Johann Jakob Scheuchzer wrote in support of Woodward's ideas in 1708, describing some fossil vertebrae as bones of sinners who had perished in the Flood. A skeleton found in a quarry was described by him in 1726 as \"Homo diluvii testis\", a giant human testifying to the Flood. This was accepted for some time, but in 1812 it was shown to be a prehistoric salamander.\n\nThe modern science of geology developed in the 18th century, the term \"geology\" itself was popularised by the \"Encyclopédie\" of 1751. Steno's categorisation of strata was expanded by several geologists, including Johann Gottlob Lehmann who believed that the oldest mountains had formed early in the Creation, and categorised as \"Flötz-Gebürge\" stratified mountains with few ore deposits but with thin layers containing fossils, overlain by a third category of superficial deposits. In his 1756 publication he identified 30 different layers in this category which he attributed to the action of the Genesis Deluge, possibly including debris from the older mountains. Others including Giovanni Arduino attributed secondary strata to natural causes: Georg Christian Füchsel said that geologists had to take as standard the processes in which nature currently produces solids, \"we know no other way\", and only the most recent deposits could be attributed to a great Flood.\n\nLehman's classification was developed by Abraham Gottlob Werner who thought that rock strata had been deposited from a primeval global ocean rather than by Noah's Flood, a doctrine called Neptunism. The idea of a young Earth was further undermined in 1774 by Nicolas Desmarest, whose studies of a succession of extinct volcanoes in Europe showed layers which would have taken long ages to build up. The fact that these layers were still intact indicated that any later Flood had been local rather than universal. Against Neptunism, James Hutton proposed an indefinitely old cycle of eroded rocks being deposited in the sea, consolidated and heaved up by volcanic forces into mountains which in turn eroded, all in natural processes which continue to operate.\n\nThe first professional geological society, the Geological Society of London, was founded in 1807. By this time, geologists were convinced that an immense time had been needed to build up the huge thickness of rock strata visible in quarries and cliffs, implying extensive pre-human periods. Most accepted a basic time scale classifying rocks as primitive, transition, secondary, or tertiary. Several researchers independently found that strata could be identified by characteristic fossils: secondary strata in southern England were mapped by William Smith from 1799 to 1815.\n\nGeorges Cuvier, working with Alexandre Brongniart, examined tertiary strata in the region around Paris. Cuvier found that fossils identified rock formations as alternating between marine and terrestrial deposits, indicating \"repeated irruptions and retreats of the sea\" which he identified with a long series of sudden catastrophes which had caused extinctions. In his 1812 \"Discours préliminaire\" to his \"Recherches sur les ossemens fossiles de quadrupeds\" put forward a synthesis of this research into the long prehistoric period, and a historical approach to the most recent catastrophe. His historical approach tested empirical claims in the Biblical text of Genesis against other ancient writings to pick out the \"real facts\" from \"interested fictions\". In his assessment, Moses had written the account around 3,300 years ago, long after the events described. Cuvier only discussed the Genesis Flood in general terms, as the most recent example of \"an event of an universal catastrophe, occasioned by an irruption of the waters\" not set \"much further back than five or six thousand years ago\". The historical texts could be loosely related to evidence such as overturned strata and \"heaps of \"debris\" and rounded pebbles\". An English translation was published in 1813 with a preface and notes by Robert Jameson, Regius Professor of Natural History at the University of Edinburgh. He began the preface with a sentence which ignored Cuvier's historical approach and instead deferred to revelation: \n\nThis sentence was removed after the second edition, and Jameson's position changed as shown by his notes in successive editions, but it influenced British views of Cuvier's concept. In 1819 George Bellas Greenough, first president of The Geological Society, issued \"A Critical Examination of the First Principles of Geology\" stating that unless erratic boulders deposited hundreds of miles from their original sources had been moved by seas, rivers, or collapsing lakes, \"the only remaining cause, to which these effects can be ascribed, is a Debacle or Deluge.\"\n\nConservative geologists in Britain welcomed Cuvier's theory to replace Werner's Neptunism, and the Church of England clergyman William Buckland became the foremost proponent of Flood geology as he sought to get the new science of geology accepted on the curriculum of the University of Oxford. In 1818 he was visited by Cuvier, and in his inaugural speech in 1819 as the first professor of geology at the university he defended the subject against allegations that it undermined religion. His speech, published as \"Vindiciae Geologicae; or, The Connexion of Geology with Religion Explained\", equated the last of a long series of catastrophes with the Genesis Flood, and said that \"the grand fact of an universal deluge at no very remote period is proved on grounds so decisive and incontrovertible, that, had we never heard of such an event from Scripture, or any other, authority, Geology of itself must have called in the assistance of some such catastrophe, to explain the phenomena of diluvian action which are universally presented to us, and which are unintelligible without recourse to a deluge exerting its ravages at a period not more ancient than that announced in the Book of Genesis.\" The evidence he proposed included erratic boulders, extensive areas of gravel, and landforms which appeared to have been scoured by water.\n\nThis inaugural address influenced the geologists William Conybeare and William Phillips. In their 1822 book on \"Outlines of the Geology of England and Wales\" Conybeare referred to the same features in an introduction about the relationship between geology and religion, describing how a deluge causing \"the last great geological change to which the surface of our planet appears to have been exposed\" left behind the debris (which he named in Latin \"Diluvium\") as evidence for \"that great and universal catastrophe to which it seems most properly assignable\". In 1823 Buckland published his detailed account of \"Relics of the Flood\", \"Reliquiae Diluvianae;\" or, \"Observations on the Organic Remains Contained in Caves, Fissures, and Diluvial Gravel and on Other Geological Phenomena Attesting the Action of an Universal Deluge\", incorporating his research suggesting that animal fossils had been dragged into the Kirkdale Cave by hyenas then covered by a layer of red mud washed in by the Deluge.\n\nBuckland's views were supported by other Church of England clergymen naturalists: his Oxford colleague Charles Daubeny proposed in 1820 that the volcanoes of the Auvergne showed a sequence of lava flows from before and after the Flood had cut valleys through the region. In an 1823 article \"On the deluge\", John Stevens Henslow, professor of mineralogy at the University of Cambridge, affirmed the concept and proposed that the Flood had originated from a comet, but this was his only comment on the topic. Adam Sedgwick, Woodwardian Professor of Geology at Cambridge, presented two supportive papers in 1825, \"On the origin of alluvial and diluvial deposits\", and \"On diluvial formations\". At this time, most of what Sedgwick called \"The English school of geologists\" distinguished superficial deposits which were \"diluvial\", showing \"great irregular masses of sand, loam, and coarse gravel, containing through its mass rounded blocks sometimes of enormous magnitude\" and supposedly caused by \"some great irregular inundation\", from \"alluvial\" deposits of \"comminuted gravel, silt, loam, and other materials\" attributed to lesser events, the \"propelling force\" of rivers, or \"successive partial inundations\".\n\nIn America, Benjamin Silliman at Yale College spread the concept, and in an 1833 essay dismissed the earlier idea that most stratified rocks had been formed in the Flood, while arguing that surface features showed \"wreck and ruin\" attributable to \"mighty floods and rushing torrents of water\". He said that \"we must charge to moving waters the undulating appearance of stratified sand and gravel, often observed in many places, and very conspicuously in the plain of New Haven, and in other regions of Connecticut and New England\", while both \"bowlder stones\" and sandy deserts across the world could be attributed to \"diluvial agency\".\n\nOther naturalists were critical of Diluvialism: the Church of Scotland pastor John Fleming published opposing arguments in a series of articles from 1823 onwards. He was critical of the assumption that fossils resembling modern tropical species had been swept north \"by some violent means\", which he regarded as absurd considering the \"unbroken state\" of fossil remains. For example, fossil mammoths demonstrated adaptation to the same northern climates now prevalent where they were found. He criticized Buckland's identification of red mud in the Kirkdale cave as diluvial, when near identical mud in other caves had been described as fluvial. While Cuvier had reconciled geology with a loose reading of the Biblical text, Fleming argued that such a union was \"indiscreet\" and turned to a more literal view of Genesis:\nWhen Sedgwick visited Paris at the end of 1826 he found hostility to Diluvialism: Alexander von Humboldt ridiculed it \"beyond measure\", and Louis-Constant Prévost \"lectured against it\". In the summer of 1827 Sedgwick and Roderick Murchison travelled to investigate the geology of the Scottish Highlands, where they found \"so many indications of \"local diluvial\" operations\" that Sedgwick began to change his mind about it being worldwide. When George Poulett Scrope published his investigations into the Auvergne in 1827, he did not use the term \"diluvium\". He was followed by Murchison and Charles Lyell whose account appeared in 1829. All three agreed that the valleys could well have been formed by rivers acting over a long time, and a deluge was not needed. Lyell, formerly a pupil of Buckland, put strong arguments against diluvialism in the first volume of his \"Principles of Geology\" published in 1830, though suggesting the possibility of a deluge affecting a region such as the low-lying area around the Caspian Sea. Sedgwick responded to this book in his presidential address to the Geological Society in February 1830, agreeing that diluvial deposits had formed at differing times. At the society a year later, when retiring from the presidency, Sedgwick described his former belief that \"vast masses of diluvial gravel\" had been scattered worldwide in \"one violent and transitory period\" as \"a most unwarranted conclusion\", and therefore thought \"it right, as one of my last acts before I quit this Chair, thus publicly to read my recantation.\" However, he remained convinced that a flood as described in Genesis was not excluded by geology.\n\nOne student had seen the gradual abandonment of diluvialism: Charles Darwin had attended Jameson's geology lectures in 1826, and at Cambridge became a close friend of Henslow before learning geology from Sedgwick in 1831. At the outset of the \"Beagle\" voyage Darwin was given a copy of Lyell's \"Principles of Geology\", and at the first landfall began his career as a geologist with investigations which supported Lyell's concept of slow uplift while also describing loose rocks and gravel as \"part of the long disputed Diluvium\". Debates continued over the part played by repeated exceptional catastrophes in geology, and in 1832 William Whewell dubbed this view catastrophism, while naming Lyell's insistence on explanations based on current processes uniformitarianism.\n\nBuckland, too, gradually modified his views on the Deluge. In 1832 a student noted Buckland's view on cause of diluvial gravel, \"whether is Mosaic inundation or not, will not say\". In a footnote to his \"Bridgewater Treatise\" of 1836, Buckland backed down from his former claim that the \"violent inundation\" identified in his \"Reliquiae Diluvianae\" was the Genesis flood: \nFor a while, Buckland had continued to insist that \"some\" geological layers were related to the Great Flood, but grew to accept the idea that they represented multiple inundations which occurred well before humans existed. In 1840 he made a field trip to Scotland with the Swiss geologist Louis Agassiz, and became convinced that the \"diluvial\" features which he had attributed to the Deluge had, in fact, been produced by ancient ice ages. Buckland became one of the foremost champions of Agassiz's theory of glaciations, and diluvialism went out of use in geology. Active geologists no longer posited sudden ancient catastrophes with unknown causes, and instead increasingly explained phenomena by observable processes causing slow changes over great periods.\n\nScriptural geologists were a heterogeneous group of writers in the early nineteenth century, who claimed \"the primacy of literalistic biblical exegesis\" and a short Young Earth time-scale. Their views were marginalised and ignored by the scientific community of their time. They generally lacked any background in geology, and had little influence even in church circles.\n\nMany quoted out of date geological writings. Among the most prominent, Granville Penn argued in 1822 that \"mineral geology\" rejected revelation, while true \"Mosaical geology\" showed that God had created primitive rock formations directly, in correspondence with the laws which God then made to produce subsequent effects. A first revolution on the third day of creation deepened the oceans so water rushed in, and in the Deluge 1,656 years afterwards a second revolution sank land areas and raised the sea bed to cause a swirling flood which moved soil and fossil remains into stratified layers, after which God created new vegetation. As Genesis appeared to show that the rivers of Eden had survived this catastrophe, he argued that the verses concerned were an added \"parenthesis\" which should be disregarded. In 1837 George Fairholme expressed disappointment about disappearing belief in the deluge, and about Sedgwick and Buckland recanting diluvialism, while putting forward his own \"New and Conclusive Physical Demonstrations\" which ignored geological findings to claim that strata had been deposited in a quick continuous process while still moist.\n\nGeology was popularized by several authors. John Pye Smith's lectures published in 1840 reconciled an extended time frame with Genesis by the increasingly common gap theology or day-age theology, and said it was likely that the gravel and boulder formations were not \"diluvium\", but had taken long ages predating the creation of humans. He reaffirmed that the Flood was historical as a local event, something which the 17th century theologians Edward Stillingfleet and Matthew Poole had already suggested on a purely Biblical basis. Smith also denounced the \"fanciful\" writings of the scriptural geologists. Edward Hitchcock sought to ensure that geological findings could be corroborated by scripture, and dismissed the scriptural geology of Penn and Fairholme as misrepresenting both scripture and the facts of geology. He noted the difficulty of equating a violent deluge with the more tranquil Genesis account. Hugh Miller supported similar points with considerable detail.\n\nLittle attention was paid to Flood geology over the rest of the 19th century, its few supporters included the author Eleazar Lord in the 1850s and the Lutheran scholar Carl Friedrich Keil in 1860 and 1878. The visions of Ellen G. White published in 1864 formed Seventh-day Adventist Church views, and influenced 20th century creationism.\n\nThe Seventh-day Adventist Church, led by Ellen G. White, took a six-day creation literally, and put her prolific \"inspired\" writings on a level with the Bible. Her visions of the flood and its aftermath, published in 1864, described a catastrophic deluge which reshaped the entire surface of the Earth, followed by a powerful wind which piled up new high mountains, burying the bodies of men and beasts. Buried forests became coal and oil, and where God later caused these to burn, they reacted with limestone and water to cause \"earthquakes, volcanoes and fiery issues\".\n\nEllen G. White's visions prompted several books by one of her followers, George McCready Price, leading to the 20th-century revival of flood geology. After years selling White's books door-to-door, Price took a one-year teacher-training course and taught in several schools. When shown books on evolution and the fossil sequence which contradicted his beliefs, he found the answer in White's \"revealing word pictures\" which suggested how the fossils had been buried. He studied textbooks on geology and \"almost tons of geological documents\", finding \"how the actual facts of the rocks and fossils, \"stripped of mere theories\", splendidly refute this evolutionary theory of the invariable order of the fossils, \"which is the very backbone of the evolution doctrine\"\". In 1902 he produced a manuscript for a book proposing geology based on Genesis, in which the sequence of fossils resulted from the different responses of animals to the encroaching flood. He agreed with White on the origins of coal and oil, and conjectured that mountain ranges (including the Alps and Himalaya) formed from layers deposited by the flood which had then been \"folded and elevated to their present height by the great lateral pressure that accompanied its subsidence\". He then found a report describing paraconformities and a paper on thrust faults. He concluded from these \"providential discoveries\" that it was impossible to prove the age or overall sequence of fossils, and included these points in his self-published paperback of 1906, \"Illogical Geology: The Weakest Point in the Evolution Theory\". His arguments continued this focus on disproving the sequence of strata, and he ultimately sold more than 15,000 copies of his 1923 college textbook \"The New Geology\".\n\nPrice increasingly gained attention outside Adventist groups, and in the creation–evolution controversy other leading Christian fundamentalists praised his opposition to evolution - though none of them followed his young Earth arguments, retaining their belief in the gap or in the day-age interpretation of Genesis. Price corresponded with William Jennings Bryan and was invited to be a witness in the Scopes Trial of 1925, but declined as he was teaching in England and opposed to teaching Genesis in public schools as \"it would be an infringement on the cardinal American principle of separation of church and state\". Price returned from England in 1929 to rising popularity among fundamentalists as a scientific author. In the same year his former student Harold W. Clark self-published the short book \"Back to Creationism\", which recommended Price's flood geology as the new \"science of creationism\", introducing the label \"creationism\" as a replacement for \"anti-evolution\" of \"Christian Fundamentals\".\n\nIn 1935 Price and Dudley Joseph Whitney (a rancher who had co-founded the Lindcove Community Bible Church, and now followed Price) founded the \"Religion and Science Association\" (RSA). They aimed to resolve disagreements among fundamentalists with \"a harmonious solution\" which would convert them all to flood geology. Most of the organising group were Adventists, others included conservative Lutherans with similarly literalist beliefs. Bryon C. Nelson of the Norwegian Lutheran Church of America had included Price's geological views in a 1927 book, and in 1931 published \"The Deluge Story in Stone: A History of the Flood Theory of Geology\", which described Price as the \"one very outstanding advocate of the Flood\" of the century. The first public RSA conference in March 1936 invited various fundamentalist views, but opened up differences between the organisers on the antiquity of creation and on life before Adam. The RSA went defunct in 1937, and a dispute continued between Price and Nelson, who now viewed Creation as occurring over 100,000 years previously.\n\nIn 1938 Price, with a group of Adventists in Los Angeles, founded what became the \"Deluge Geology Society\" (DGS), with membership restricted to those believing that the creation week comprised \"six literal days, and that the Deluge should be studied as the cause of the major geological changes since creation\". Not all DGS-adherents were Adventists; early members included the Independent Baptist Henry M. Morris and the Missouri Lutheran Walter E. Lammerts. The DGS undertook field-work: in June 1941 their first \"Bulletin\" hailed the news that the Paluxy River dinosaur trackways in Texas appeared to include human footprints. Though Nelson had advised Price in 1939 that this was \"absurd\" and that the difficulty of human footprints forming during the turmoil of the deluge would \"knock the Flood theory all to pieces\", in 1943 the DGS began raising funds for \"actual excavation\" by a Footprint Research Committee of members including the consulting geologist Clifford L. Burdick. Initially they tried to keep their research secret from \"unfriendly scientists\". Then in 1945, to encourage backing, they announced giant human footprints, allegedly defeating \"at a single stroke\" the theory of evolution. The revelation that locals had carved the footprints, and an unsuccessful field trip that year, failed to dampen their hopes. However, by then doctrinal arguments had riven the DGS. The most extreme dispute began in late 1938 after Harold W. Clark observed deep drilling in oil fields and had discussions with practical geologists which dispelled the belief that the fossil sequence was random, convincing him that the evidence of thrust faults was \"almost incontrovertible\". He wrote to Price, telling his teacher that the \"rocks do lie in a much more definite sequence than we have ever allowed\", and proposing that the fossil sequence was explained by ecological zones before the flood. Price reacted with fury, and despite Clark emphasising their shared belief in literal recent Creation, the dispute continued. In 1946 Clark set out his views in a book, \"The New Diluvialism\", which Price denounced as \"Theories of Satanic Origin\".\n\nIn 1941 F. Alton Everest co-founded the American Scientific Affiliation (ASA) as a less confrontational forum for evangelical scientists. Some deluge geologists, including Lammerts and Price, urged close cooperation with the DGS, but Everest began to see their views as presenting an \"insurmountable problem\" for the ASA. In 1948 he requested J. Laurence Kulp, a geologist in fellowship with the Plymouth Brethren, to explore the issue. At the convention that year, Kulp examined hominid antiquity demonstrated by radiocarbon dating. At the 1949 convention a paper by Kulp was presented, giving a detailed critique of \"Deluge Geology\", which he said had \"grown and infiltrated the greater portion of fundamental Christianity in America primarily due to the absence of trained Christian geologists\". Kulp demonstrated that \"major propositions of the theory are contraindicated by established physical and chemical laws\". He focused on \"four basic errors\" commonly made by flood geologists:\n\n\nKulp accused Price of ignorance and deception, and concluded that \"this unscientific theory of flood geology has done and will do considerable harm to the strong propagation of the gospel among educated people\". Price said nothing during the presentation and discussion. When invited to speak, he \"said something very brief which missed what everyone was waiting for\". Further publications made the ASA's opposition to flood geology clear.\n\nIn 1942, Henry M. Morris was persuaded by Irwin A. Moon's \"Sermons from Science\" of the importance of harmonising science and the Bible, and was introduced to the concepts of a vapor canopy causing the Flood, and its geological effects. About a year later he found George McCready Price's \"New Geology\" a \"life-changing experience\", and joined the \"Deluge Geology Society\". His book \"That You Might Believe\" for college students included Price's flood geology, and was published in 1946.\n\nMorris had joined the American Scientific Affiliation (ASA) in 1949, and in the summer of 1953 he made a presentation on \"The Biblical Evidence for a Recent Creation and Universal Deluge\" at their annual conference, held at the Grace Theological Seminary's campus. He impressed a graduate student there, John C. Whitcomb, Jr. who was teaching Old Testament and Hebrew. To Whitcomb's distress, Morris was \"politely denounced\" by the ASA members at the presentation.\n\nIn 1955 the ASA held a joint meeting with the Evangelical Theological Society (ETS) at the same campus, and there was considerable discussion about \"The Christian View of Science and Scripture\" (1954) by theologian Bernard Ramm. This book dismissed flood geology as typifying the \"ignoble tradition\" of fundamentalism, and said that Price could not be taken seriously, as he lacked the necessary competence, training and integrity. Instead, Ramm proposed what he called progressive creationism in which the Genesis days were pictorial images revealing a process that had taken place over millions of years. Ramm's views were praised by ASA scientists, but the ETS theologians were unwilling to follow Ramm.\n\nThis encouraged Whitcomb to make his doctoral dissertation a response to Ramm and a defence of Price's position. He systematically asked evangelical professors of apologetics, archaeology and the Old Testament about creation and the flood, and in October told Morris that Ramm's book had been sufficient incentive for him to devote his dissertation to the topic. In 1957 Whitcomb's 450 page dissertation, \"The Genesis Flood\", was completed, and he promptly began summarising it for a book. Moody Publishers responded positively and agreed with him that chapters on scientific aspects should be carefully checked or written by someone with a PhD in science, but Whitcomb's attempts to find someone with a doctorate in geology were unsuccessful. Morris gave helpful advice, expressing concern that sections were too closely based on Price and Velikovsky who were \"both considered by scientists generally as crackpots\". Morris produced an outline of his planned three chapters, and in December 1957 agreed to co-author the book.\n\nMorris completed his draft in early 1959. His intended 100 pages grew to almost 350, around twice the length of Whitcomb's eventual contribution. Whitcomb raised concerns that \"For many people, our position would be somewhat discredited\" by prominent references to \"Price and Seventh-Day Adventism\", and these were deleted. By early 1960 they were impatient at delays when Moody Publishers had misgivings about the length and literal views of the book, and they went along with Rousas Rushdoony's recommendation of a small Philadelphia publishers.\n\n\"The Genesis Flood\" by Whitcomb and Morris was published by the Presbyterian and Reformed Publishing Company of Philadelphia in February 1961. Their premise was that the Bible is infallible, \"the basic argument of this volume is that the Scriptures are true.\" For Whitcomb, Genesis described a worldwide Flood which covered all the high mountains, Noah's ark with capacity equivalent to eight freight trains, flood waters from a canopy and the deeps, and subsequent dispersal of animals from Ararat to all the continents via land bridges. He disputed the views published by Arthur Custance and Bernard Ramm. Morris then confronted readers with the dilemma of whether to believe Scripture or accept the interpretations of trained geologists, and instead of the latter proposed \"a new scheme of historical geology\" true both to Scripture and to God's work revealed in nature. This was essentially Price's \"The New Geology\" updated for the 1960s, though with few direct references to Price.\n\nLike Price before him, Morris argued that most fossil bearing strata had been formed during the global Deluge, disputing uniformitarianism, multiple ice ages, and the geologic column. He explained the apparent fossil sequence as the outcome of marine organisms dying in the slurry of sediments in early stages of the Flood, moving currents sorting object by size and shape, and the mobility of vertebrates allowing them to initially escape the floodwaters. He cited Lammerts in support of Price's views about the thrust fault at Chief Mountain disproving the sequence. \nThe book went beyond Price in some areas. Morris extended the six day creation from the Earth to the entire universe, and said that death and decay had only begun with the Fall of Man, which had therefore introduced entropy and the second law of thermodynamics. He proposed that a vapor canopy, before providing water for the Flood, created a mild even climate and shielded the Earth from cosmic rays so radiocarbon dating would not work. He cited Clifford L. Burdick's testimony that some of the Paluxy River dinosaur trackways overlapped human footprints, but Burdick failed to confirm this and the section was removed from the third edition.\n\nIn a 1957 discussion with Whitcomb, Walter E. Lammerts suggested an \"informal association\" to exchange ideas, and possibly research, on flood geology. Morris was unavailable to get things started, then around 1961 Wiliam J. Tinkle got in touch, and they set about recruiting others. They had difficulty in finding supporters with scientific qualifications. The Creation Research Committee of ten they put together on 9 February 1962 had varying views on the age of the Earth, but all opposed evolution. They then succeeded in recruiting others into what became the Creation Research Society (CRS) in June 1963, and grew rapidly. Getting an agreed statement of belief was problematic, they affirmed that the Bible was \"historically and scientifically true in the original autographs\" so that \"the account of origins in Genesis is a factual presentation of simple historical truths\" and \"The great flood described in Genesis, commonly referred to as the Noachian Flood, was an historic event worldwide in its extent and effect\", but to Morris's disappointment they did not make flood geology mandatory. They lacked a qualified geologist, and Morris persuaded the group to appoint Clifford L. Burdick as their only Earth scientist, overcoming initial concerns raised by Lammerts. The CRS grew rapidly, with an increasing proportion of the membership adhering to strict young Earth flood geology.\n\nThe resources of the CRS for its first decade went into publication of the CRS \"Quarterly\", and a project to publish a creationist school book. Since the 1920s most U.S. schools had not taught pupils about evolution, but Sputnik exposed apparent weaknesses of U.S. science education and the Biological Sciences Curriculum Study produced textbooks in 1963 which included the topic. When the Texas Education Agency held a hearing in October 1964 about adopting these textbooks, creationist objectors were unable to name suitable creationist alternatives. Lammerts organised a CRS textbook committee which lined up a group of authors, with John N. Moore as senior editor bringing their contributions together into a suitable textbook.\n\nThe teaching of evolution, reintroduced in 1963 by the Biological Sciences Curriculum Study textbooks, was prohibited by laws in some states. These bans were contested; the \"Epperson v. Arkansas\" case which began late in 1965 was decided in 1968 by the United States Supreme Court ruling that such laws violated the Establishment Clause of the First Amendment to the United States Constitution.\n\nSome creationists thought a legal decision requiring religious neutrality in schools should shield their children from teachings hostile to their religion; Nell. J. Segraves and Jean E. Sumrall (a friend of Lammerts who was also associated with the Creation Research Society and the Bible-Science Association) petitioned the California State Board of Education to require that school biology texts designate evolution a theory. In 1966 Max Rafferty as California State Superintendent of Public Instruction suggested that they demand equal time for creation, as the Civil Rights Act of 1964 allowed teachers to mention religion as long as they did not promote specific doctrines. Their first attempt failed, but in 1969 controversy arose over a proposed \"Science Framework for California Schools\". Anticipating success, they and others in the Bible-Science Association formed \"Creation Science, Inc.\", to produce textbooks. A compromise acceptable to Segraves, Sumrall and the Board was suggested by Vernon L. Grose, and the revised 1970 \"Framework\" included \"While the Bible and other philosophical treatises also mention creation, science has independently postulated the various theories of creation. Therefore, creation in scientific terms is not a religious or philosophical belief.\" The result kept school texts free of creationism, but downgraded evolution to mere speculative theory.\n\nCreationists reacted to the California developments with a new confidence that they could introduce their ideas into schools by minimizing biblical references. Henry M. Morris declared that \"Creationism is on the way back, this time not primarily as a religious belief, but as an alternative scientific explanation of the world in which we live.\" In 1970 \"Creation Science, Inc.\", combined with a planned studies center at Christian Heritage College as the Creation-Science Research Center. Morris moved to San Diego to become director of the center and academic vice-president of the college. In the fall he presented a course at the college on \"Scientific Creationism\", the first time he is known to have used the term in public. (Two years later, the Creation-Science Research Center split with part becoming the Institute for Creation Research (ICR) led by Morris.)\n\nThe Creation Research Society (CRS) had found schoolbook publishers reluctant to take on their textbook, and eventually the Christian publishing company Zondervan brought out \"Biology: A Search for Order in Complexity\" in 1970. The ten thousand copies printed sold out within a year, and they produced 25,000 as the second impression, but hardly any public schools adopted the book. A preface by Morris claimed that there were two philosophies of creation, \"the doctrine of evolution and the doctrine of special creation\", attempting to give both equal validity. The book mostly covered uncontroversial details of biology, but asserted that these were correctly seen as \"God's creation\" or \"divine creation\", and presented biblical creation as the correct scientific view. A chapter on \"Weaknesses of Geologic Evidence\" disputed evolutionary theories while asserting a \"fact that most fossil material was laid down by the flood in Noah's time\". Another chapter disputed evolutionary theory.\n\nIn the \"Creation Research Society Quarterly\" for September 1971 Morris introduced the \"two-model approach\" asserting that evolution and creation were both equally scientific and equally religious, and soon afterwards he said they were \"competing scientific hypotheses\". For the third printing of \"Biology: A Search for Order in Complexity\" in 1974, the editor John N. Moore added a preface setting out this approach as \"the two basic viewpoints of origins\", the \"evolution model\" and the \"creation model\". When an Indiana school decided to use the book as their biology text, the \"Hendren v. Campbell\" district court case banned its use in public schools as infringing the Establishment Clause. Judge Michael T. Dugan, II, described it as \"a text obviously designed to present \"only\" the view of Biblical Creationism in a favorable light\", contravening the constitution by promotion of a specific sectarian religious view.\n\nAs a tactic to gain the same scientific status as evolution, flood geology proponents had effectively relabeled the Bible-based flood geology of George McCready Price as \"creation science\" or \"scientific creationism\" by the mid 1970s. At the CRS board meeting in the Spring of 1972, members were told to start using \"scientific creationism\", a phrase used interchangeably with \"creation science\"; Morris explained that preferences differed, though neither was ideal as \"one simple term\" could not \"identify such a complex and comprehensive subject.\" In the 1974 ICR handbook for high-school teachers titled \"Scientific Creationism\", Morris used the two-model approach to support his argument that creationism could \"be taught without reference to the book of Genesis or to other religious literature or to religious doctrines\", and in public schools only the \"basic scientific creation model\" should be taught, rather than biblical creationism which \"would open the door to wide interpretations of Genesis\" or to non-Christian cosmogonies. He did not deny having been influenced by the Bible. In his preface to the book dated July 1974, Morris as editor outlined how the \"Public School Edition\" of the book evaluated evidence from a \"strictly scientific point of view\" without \"reference to the Bible or other religious literature\", while the \"General Edition\" was \"essentially identical\" except for an additional chapter on \"Creation according to Scripture\" that \"places the scientific evidence in its proper biblical and theological context.\"\n\nThe main ideas in creation science are: the belief in \"creation \"ex nihilo\"\" (Latin: out of nothing); the conviction that the Earth was created within the last 6,000 years; the belief that mankind and other life on Earth were created as distinct fixed \"baraminological\" \"kinds\"; and the idea that fossils found in geological strata were deposited during a cataclysmic flood which completely covered the entire Earth. As a result, creation science also challenges the commonly accepted geologic and astrophysical theories for the age and origins of the Earth and Universe, which creationists acknowledge are irreconcilable to the account in the Book of Genesis.\n\nThe geologic column and the fossil record are used as major pieces of evidence in the modern scientific explanation of the development and evolution of life on Earth as well as a means to establish the age of the Earth. Young Earth Creationists such as Morris and Whitcomb in their 1961 book, \"The Genesis Flood\", say that the age of the fossils depends on the amount of time credited to the geologic column, which they ascribe to be about one year. Some flood geologists dispute geology's assembled global geologic column since index fossils are used to link geographically isolated strata to other strata across the map. Fossils are often dated by their proximity to strata containing index fossils whose age has been determined by its location on the geologic column. Oard and others say that the identification of fossils as index fossils has been too error-prone for index fossils to be used reliably to make those correlations, or to date local strata using the assembled geologic scale.\n\nOther creationists accept the existence of the geological column and believe that it indicates a sequence of events that might have occurred during the global flood. Institute for Creation Research creationists such as Andrew Snelling, Steven A. Austin and Kurt Wise take this approach, as does Creation Ministries International. They cite the Cambrian explosion — the appearance of abundant fossils in the upper Ediacaran (Vendian) Period and lower Cambrian Period — as the pre-Flood/Flood boundary, the presence in such sediments of fossils that do not occur later in the geological record as part of a pre–flood biota that perished and the absence of fossilized organisms that appear later (such as angiosperms and mammals) as due to erosion of sediments deposited by the flood as waters receded off the land. Creationists say that fossilization can only take place when the organism is buried quickly to protect the remains from destruction by scavengers or decomposition. They say that the fossil record provides evidence of a single cataclysmic flood and not of a series of slow changes accumulating over millions of years.\n\nFlood geologists have proposed numerous hypotheses to reconcile the sequence of fossils evident in the fossil column with the literal account of Noah's flood in the Bible. Whitcomb and Morris proposed three possible factors:\n\n\nSome creationists believe that oil and coal deposits formed rapidly in sedimentary layers as volcanoes or flood waters flattened forests and buried the debris. They believe the vegetation decomposed rapidly into oil or coal due to the heat of the subterranean waters as they were unleashed from the Earth during the flood or by the high temperatures created as the remains were compressed by water and sediment.\n\nCreationists continue to search for evidence in the natural world that they consider consistent with the above description, such as evidence of rapid formation. For example, there have been claims of raindrop marks and water ripples at layer boundaries, sometimes associated with the claimed fossilized footprints of men and dinosaurs walking together. Such footprint evidence has been debunked and some have been shown to be fakes.\n\nProponents of Flood Geology state that \"native global flood stories are documented as history or legend in almost every region on earth\". \"These flood tales are frequently linked by common elements that parallel the biblical account including the warning of the coming flood, the construction of a boat in advance, the storage of animals, the inclusion of family, and the release of birds to determine if the water level had subsided.\" They suggest that \"the overwhelming consistency among flood legends found in distant parts of the globe indicates they were derived from the same origin, but oral transcription has changed the details through time\".\n\nAnthropologist Patrick Nunn rejects this view and highlights the fact that much of the human population lives near water sources such as rivers and coasts, where unusually severe floods can be expected to occur occasionally and will be recorded in tribal mythology.\n\nGeorge McCready Price attempted to fit a great deal of earth's geological history into a model based on a few accounts from the Bible. Price's simple model was used by Whitcomb and Morris initially but they did not build on the model in the 60s and 70s. However, a rough sketch of a creationist model could be constructed from creationist publications and debate material. Recent creationist efforts attempt to build complex models that incorporate as much scientific evidence as possible into the Biblical narrative. Some scientific evidence used for these models was formerly rejected by creationists. These models attempt to explain continental movements in a short time frame, the order of the fossil record, and the Pleistocene ice age.\n\nIn the 60s and 70s a simple creationist model proposed that, \"The Flood split the land mass into the present continents.\" Steve Austin and other creationists proposed a preliminary model of catastrophic plate tectonics (CPT) in 1994. Their work built on earlier papers by John Baumgardner and Russell Humphreys in 1986. Baumgardner proposed a model of mantle convection that allows for runaway subduction and Humphrey associated mantle convection with rapid magnetic reversals in earth history. Baumgardner's proposal holds that the rapid plunge of former oceanic plates into the mantle (caused by an unknown trigger-mechanism) increased local mantle pressures to the point that its viscosity dropped several magnitudes according to known properties of mantle silicates. Once initiated, sinking plates caused the spread of low viscosity throughout the mantle resulting in runaway mantle-convection and catastrophic tectonic motion which dragged continents across the surface of the earth. Once the former ocean plates, which are thought to be denser than the mantle, reached the bottom of the mantle an equilibrium resulted. Pressures dropped, viscosity increased, runaway mantle-convection stopped, leaving the surface of the earth rearranged. Proponents point to subducted slabs in the mantle which are still relatively cool, which they regard as evidence that they have not been there for millions of years which would result in temperature equilibration.\n\nGiven that conventional plate tectonics accounts for much of the geomorphic features of continents and oceans, it is natural that creationists would seek to develop a high speed version of the same process. CPT explains many geological features, provides mechanisms for the Biblical flood, and minimizes appeals to miracles.\n\nSome prominent creationists (Froede, Oard, Read) oppose CPT for various technical reasons. One main objection is that the model assumes the super continent Pangaea was intact at the initiation of the year-long flood. The CPT process then tore Pangaea apart creating the current configuration of the continents. But the breakup of Pangaea started early in the Mesozoic, meaning that CPT only accounts for part of the entire Phanerozoic geological record. CPT in this form only explains part of the geological column that flood geology normally explains. Modifying the CPT model to account for the entire Phanerozoic including multiple Wilson Cycles would complicate the model considerably.\n\nOther objections of CPT include the amount of heat produced for the rapid plate movements, and the fact that the cooling of hot oceanic plates and the raising of continental plates would take a great deal of time and require multiple small scale catastrophes after the flood ended. The original CPT proposal of Austin and others in 1994 was admittedly preliminary but the major issues have not been solved.\n\nThe vast majority of geologists regard the hypothesis of catastrophic plate tectonics as pseudoscience; they reject it in favor of the conventional geological theory of plate tectonics. It has been argued that the tremendous release of energy necessitated by such an event would boil off the Earth's oceans, making a global flood impossible. Not only does catastrophic plate tectonics lack any plausible geophysical mechanism by which its changes might occur, it also is contradicted by considerable geological evidence (which is in turn consistent with conventional plate tectonics), including:\n\n\nConventional plate tectonics accounts for the geological evidence already, including innumerable details that catastrophic plate tectonics cannot, such as why there is gold in California, silver in Nevada, salt flats in Utah, and coal in Pennsylvania, without requiring any extraordinary mechanisms to do so.\n\nIsaac Vail (1840–1912), a Quaker schoolteacher, in his 1912 work \"The Earth's Annular System\", extrapolated from the nebular hypothesis what he called the annular system of earth history, with the earth being originally surrounded by rings resembling those of Saturn, or \"canopies\" of water vapor. Vail hypothesised that, one by one, these canopies collapsed on the Earth, resulting in fossils being buried in a \"succession of stupendous cataclysms, separated by unknown periods of time\". The Genesis flood was thought to have been caused by \"the last remnant\" of this vapor. Although this final flood was geologically significant, it was not held to account for as much of the fossil record as George McCready Price had asserted. \n\nVail's ideas about geology appeared in Charles Taze Russell's \"The Photo-Drama of Creation\" and subsequently in Joseph Franklin Rutherford's \"Creation\" of 1927 and later publications. The Seventh-day Adventist physicist Robert W. Woods also proposed a vapor canopy, before \"The Genesis Flood\" gave it prominent and repeated mention in 1961.\n\nThough the vapor-canopy theory has fallen into disfavour among most creationists, Dillow in 1981 and Vardiman in 2003 attempted to defend the idea. Among its more vocal adherents, controversial Young Earth Creationist Kent Hovind uses it as the basis for his eponymous \"Hovind Theory\". Jehovah's Witnesses propose as the water source of the deluge a \"heavenly ocean\" that was over the earth from the second creative day until the Flood.\n\nModern geology, its sub-disciplines and other scientific disciplines utilize the scientific method to analyze the geology of the earth. The key tenets of flood geology are refuted by scientific analysis and do not have any standing in the scientific community. Modern geology relies on a number of established principles, one of the most important of which is Charles Lyell's principle of uniformitarianism. In relation to geological forces it states that the shaping of the Earth has occurred by means of mostly slow-acting forces that can be seen in operation today. By applying these principles, geologists have determined that the Earth is approximately 4.54 billion years old. They study the lithosphere of the Earth to gain information on the history of the planet. Geologists divide Earth's history into eons, eras, periods, epochs, and faunal stages characterized by well-defined breaks in the fossil record (see Geologic time scale). In general, there is a lack of any evidence for any of the above effects proposed by flood geologists and their claims of fossil layering are not taken seriously by scientists.\n\nThe global flood cannot explain geological formations such as angular unconformities, where sedimentary rocks have been tilted and eroded then more sedimentary layers deposited on top, needing long periods of time for these processes. There is also the time needed for the erosion of valleys in sedimentary rock mountains. In another example, the flood, had it occurred, should also have produced large-scale effects spread throughout the entire world. Erosion should be evenly distributed, yet the levels of erosion in, for example, the Appalachians and the Rocky Mountains differ significantly.\n\nGeochronology is the science of determining the absolute age of rocks, fossils, and sediments by a variety of techniques. These methods indicate that the Earth as a whole is about 4.54 billion years old, and that the strata that, according to flood geology, were laid down during the Flood some 6,000 years ago, were actually deposited gradually over many millions of years.\n\nIf the flood were responsible for fossilization, then all the animals now fossilized must have been living together on the Earth just before the flood. Based on estimates of the number of remains buried in the Karoo fossil formation in Africa, this would correspond to an abnormally high density of vertebrates worldwide, close to 2100 per acre.\nCreationists argue that evidence for the geological column is fragmentary, and all the complex layers of chalk occurred in the approach to the 150th day of Noah's flood. However, the entire geologic column is found in several places, and shows multiple features, including evidence of erosion and burrowing through older layers, which are inexplicable on a short timescale. Carbonate hardgrounds and the fossils associated with them show that the so-called flood sediments include evidence of long hiatuses in deposition that are not consistent with flood dynamics or timing.\n\nProponents of Flood Geology are also unable to account for the alternation between calcite seas and aragonite seas through the Phanerozoic. The cyclical pattern of carbonate hardgrounds, calcitic and aragonitic ooids, and calcite-shelled fauna has apparently been controlled by seafloor spreading rates and the flushing of seawater through hydrothermal vents which changes its Mg/Ca ratio.\n\nPhil Senter's 2011 article, \"The Defeat of Flood Geology by Flood Geology\", in the journal \"Reports of the National Center for Science Education\", discusses \"sedimentologic and other geologic features that Flood geologists have identified as evidence that particular strata cannot have been deposited during a time when the entire planet was under water ... and distribution of strata that predate the existence of the Ararat mountain chain.\" These include continental basalts, terrestrial tracks of animals, and marine communities preserving multiple in-situ generations included in the rocks of most or all Phanerozoic periods, and the basalt even in the younger Precambrian rocks. Others, occurring in rocks of several geologic periods, include lake deposits and eolian (wind) deposits. Using their own words, Flood geologists find evidence in every Paleozoic and Mesozoic period, and in every epoch of the Cenozoic period, indicating that a global flood could not have occurred during that interval.\n\n\n\n\n\n\n\n"}
{"id": "26629519", "url": "https://en.wikipedia.org/wiki?curid=26629519", "title": "Folk epidemiology of autism", "text": "Folk epidemiology of autism\n\nFolk epidemiology of autism refers to the popular beliefs about the origin of autism. Without direct knowledge of autism, a rare and complex disease, members of the public are influenced by rumors and misinformation presented in the mass media and repeated on social media and the internet. These misinformed beliefs persist even when contradicted by scientific evidence. Folk epidemiology persists because people seek, receive, and preferentially believe information that is consistent with their existing views; misjudge the reliability of their sources of information, and are misled by anecdotal evidence; and tend not to revise their opinions even when their original sources of information are shown to be wrong.\n\nThe scientific consensus is the MMR vaccine has no link to the development of autism, and that the vaccine's benefits greatly outweigh its risks. Folk beliefs that the MMR vaccine causes autism led to a sharp drop in rates of vaccination in the UK and Ireland after 1998 and corresponding increases in the incidence of vaccine-preventable childhood illnesses, disability, and death.\n\nIn 1998 Andrew Wakefield published a fraudulent article in a \"The Lancet\", a prominent British medical journal, claiming that the MMR vaccines caused autism. Although the article was later retracted, the idea that vaccines cause autism became a major news story. By the time that scientists had shown the narrative to be false, it had become part of the folk epidemiology of autism. The narrative was easy to understand and apparently consistent with anecdotal evidence of children receiving autism diagnoses shortly after having been vaccinated. Doctors were highly critical of the media coverage for triggering a decline in vaccination rates. The false belief has persisted despite a public information campaign aimed at making parents aware that by refusing vaccinations they are putting their children at risk of contracting infectious diseases that are frequently fatal.\n\nObservers have criticized the involvement of mass media in the controversy alleging that the media provided Wakefield's study with more credibility than it deserved. Endorsements by celebrities such as Jenny McCarthy and Robert F. Kennedy Jr. were highly publicized. Later commentary faulted the coverage for giving a misleading impression of the degree to which the evidence supported a link between the MMR vaccine and autism.\n\nThe scientific consensus is that the MMR vaccine has no link to the development of autism, and that the vaccine's benefits greatly outweigh its risks. The false belief that the MMR vaccine causes autism led to a sharp drop in rates of vaccination in the UK and Ireland after 1998, creating a major public health risk. Lower vaccination rates were followed by increases in the incidence of measles and mumps, and the resulting cases of permanent disability and death.\n\nPublic fears about vaccination have consumed resources that might otherwise have advanced research into the real causes of autism. There have been thirteen studies that properly followed the scientific method and contained large numbers of participants that failed to connect autism to the MMR vaccine. There have been seven well-constructed studies that have attempted to link autism to thiomersal in vaccines and were unsuccessful. Special interest groups continue to push for more research on the link between vaccines and autism. Further research motivated by the folk epidemiology of autism would represent a lost opportunity to investigate autism's true causes.\n\n"}
{"id": "1508301", "url": "https://en.wikipedia.org/wiki?curid=1508301", "title": "Futures studies", "text": "Futures studies\n\nFutures studies (also called futurology) is the study of postulating possible, probable, and preferable futures and the worldviews and myths that underlie them. In general, it can be considered as a branch of the social sciences and parallel to the field of history. Futures studies (colloquially called \"futures\" by many of the field's practitioners) seeks to understand what is likely to continue and what could plausibly change. Part of the discipline thus seeks a systemic and pattern-based understanding of past and present, and to determine the likelihood of future events and trends.\n\nUnlike the physical sciences where a narrower, more specified system is studied, futures studies concerns a much bigger and more complex world system. The methodology and knowledge are much less proven as compared to natural science or even social science like sociology and economics. There is a debate as to whether this discipline is an art or science and sometimes described by scientists as pseudoscience.\n\nFutures studies is an interdisciplinary field, studying past and present changes, and aggregating and analyzing both lay and professional strategies and opinions with respect to future. It includes analyzing the sources, patterns, and causes of change and stability in an attempt to develop foresight and to map possible futures. Around the world the field is variously referred to as futures studies, strategic foresight, futuristics, futures thinking, futuring, and futurology. Futures studies and strategic foresight are the academic field's most commonly used terms in the English-speaking world.\n\nForesight was the original term and was first used in this sense by H.G. Wells in 1932. \"Futurology\" is a term common in encyclopedias, though it is used almost exclusively by nonpractitioners today, at least in the English-speaking world. \"Futurology\" is defined as the \"study of the future.\" The term was coined by German professor Ossip K. Flechtheim in the mid-1940s, who proposed it as a new branch of knowledge that would include a new science of probability. This term may have fallen from favor in recent decades because modern practitioners stress the importance of alternative and plural futures, rather than one monolithic future, and the limitations of prediction and probability, versus the creation of possible and preferable futures.\n\nThree factors usually distinguish futures studies from the research conducted by other disciplines (although all of these disciplines overlap, to differing degrees). First, futures studies often examines not only possible but also probable, preferable, and \"wild card\" futures. Second, futures studies typically attempts to gain a holistic or systemic view based on insights from a range of different disciplines, generally focusing on the STEEP categories of Social, Technological, Economic, Environmental and Political. Third, futures studies challenges and unpacks the assumptions behind dominant and contending views of the future. The future thus is not empty but fraught with hidden assumptions. For example, many people expect the collapse of the Earth's ecosystem in the near future, while others believe the current ecosystem will survive indefinitely. A foresight approach would seek to analyze and highlight the assumptions underpinning such views.\n\nAs a field, futures studies expands on the research component, by emphasizing the communication of a strategy and the actionable steps needed to implement the plan or plans leading to the preferable future. It is in this regard, that futures studies evolves from an academic exercise to a more traditional business-like practice, looking to better prepare organizations for the future.\n\nFutures studies does not generally focus on short term predictions such as interest rates over the next business cycle, or of managers or investors with short-term time horizons. Most strategic planning, which develops operational plans for preferred futures with time horizons of one to three years, is also not considered futures. Plans and strategies with longer time horizons that specifically attempt to anticipate possible future events are definitely part of the field. As a rule, futures studies is generally concerned with changes of transformative impact, rather than those of an incremental or narrow scope.\n\nThe futures field also excludes those who make future predictions through professed supernatural means.\n\nJohan Galtung and Sohail Inayatullah argue in \"Macrohistory and Macrohistorians\" that the search for grand patterns of social change goes all the way back to Ssu-Ma Chien (145-90BC) and his theory of the cycles of virtue, although the work of Ibn Khaldun (1332–1406) such as \"The Muqaddimah\" would be an example that is perhaps more intelligible to modern sociology. Early western examples include Sir Thomas More’s “Utopia,” published in 1516, and based upon Plato’s “Republic,” in which a future society has overcome poverty and misery to create a perfect model for living. This work was so powerful that utopias have come to represent positive and fulfilling futures in which everyone’s needs are met.\n\nSome intellectual foundations of futures studies appeared in the mid-19th century. Isadore Comte, considered the father of scientific philosophy, was heavily influenced by the work of utopian socialist Henri Saint-Simon, and his discussion of the metapatterns of social change presages futures studies as a scholarly dialogue.\n\nThe first works that attempt to make systematic predictions for the future were written in the 18th century. \"Memoirs of the Twentieth Century\" written by Samuel Madden in 1733, takes the form of a series of diplomatic letters written in 1997 and 1998 from British representatives in the foreign cities of Constantinople, Rome, Paris, and Moscow. However, the technology of the 20th century is identical to that of Madden's own era - the focus is instead on the political and religious state of the world in the future. Madden went on to write \"The Reign of George VI, 1900 to 1925\", where (in the context of the boom in canal construction at the time) he envisioned a large network of waterways that would radically transform patterns of living - \"Villages grew into towns and towns became cities\".\n\nIn 1845, Scientific American, the oldest continuously published magazine in the U.S., began publishing articles about scientific and technological research, with a focus upon the future implications of such research. It would be followed in 1872 by the magazine Popular Science, which was aimed at a more general readership.\n\nThe genre of science fiction became established towards the end of the 19th century, with notable writers, including Jules Verne and H. G. Wells, setting their stories in an imagined future world.\n\nAccording to W. Warren Wagar, the founder of future studies was H. G. Wells. His \"Anticipations of the Reaction of Mechanical and Scientific Progress Upon Human Life and Thought: An Experiment in Prophecy\", was first serially published in \"The Fortnightly Review\" in 1901. Anticipating what the world would be like in the year 2000, the book is interesting both for its hits (trains and cars resulting in the dispersion of population from cities to suburbs; moral restrictions declining as men and women seek greater sexual freedom; the defeat of German militarism, the existence of a European Union, and a world order maintained by \"English-speaking peoples\" based on the urban core between Chicago and New York) and its misses (he did not expect successful aircraft before 1950, and averred that \"my imagination refuses to see any sort of submarine doing anything but suffocate its crew and founder at sea\").\n\nMoving from narrow technological predictions, Wells envisioned the eventual collapse of the capitalist world system after a series of destructive total wars. From this havoc would ultimately emerge a world of peace and plenty, controlled by competent technocrats.\n\nThe work was a bestseller, and Wells was invited to deliver a lecture at the Royal Institution in 1902, entitled \"The Discovery of the Future\". The lecture was well-received and was soon republished in book form. He advocated for the establishment of a new academic study of the future that would be grounded in scientific methodology rather than just speculation. He argued that a scientifically ordered vision of the future \"will be just as certain, just as strictly science, and perhaps just as detailed as the picture that has been built up within the last hundred years to make the geological past.\" Although conscious of the difficulty in arriving at entirely accurate predictions, he thought that it would still be possible to arrive at a \"working knowledge of things in the future\".\n\nIn his fictional works, Wells predicted the invention and use of the atomic bomb in \"The World Set Free\" (1914). In \"The Shape of Things to Come\" (1933) the impending World War and cities destroyed by aerial bombardment was depicted. However, he didn't stop advocating for the establishment of a futures science. In a 1933 BBC broadcast he called for the establishment of \"Departments and Professors of Foresight\", foreshadowing the development of modern academic futures studies by approximately 40 years.\n\nAt the beginning of the 20th century future works were often shaped by political forces and turmoil. The WWI era led to adoption of futures thinking in institutions throughout Europe. The Russian Revolution led to the 1921 establishment of the Soviet Union’s Gosplan, or State Planning Committee, which was active until the dissolution of the Soviet Union. Gosplan was responsible for economic planning and created plans in five year increments to govern the economy. One of the first Soviet dissidents, Yevgeny Zamyatin, published the first dystopian novel, \"We\", in 1921. The science fiction and political satire featured a future police state and was the first work censored by the Soviet censorship board, leading to Zamyatin’s political exile.\n\nIn the United States, President Hoover created the Research Committee on Social Trends, which produced a report in 1933. The head of the committee, William F. Ogburn, analyzed the past to chart trends and project those trends into the future, with a focus on technology. Similar technique was used during The Great Depression, with the addition of alternative futures and a set of likely outcomes that resulted in the creation of Social Security and the Tennessee Valley development project.\n\nThe WWII era emphasized the growing need for foresight. The Nazis used strategic plans to unify and mobilize their society with a focus on creating a fascist utopia. This planning and the subsequent war forced global leaders to create their own strategic plans in response. The post-war era saw the creation of numerous nation states with complex political alliances and was further complicated by the introduction of nuclear power.\n\nProject RAND was created in 1946 as joint project between the United States Army Air Forces and the Douglas Aircraft Company, and later incorporated as the non-profit RAND corporation. Their objective was the future of weapons, and long-range planning to meet future threats. Their work has formed the basis of US strategy and policy in regard to nuclear weapons, the Cold War, and the space race.\n\nFutures studies truly emerged as an academic discipline in the mid-1960s. First-generation futurists included Herman Kahn, an American Cold War strategist for the RAND Corporation who wrote \"On Thermonuclear War\" (1960), \"Thinking about the unthinkable\" (1962) and \"The Year 2000: a framework for speculation on the next thirty-three years\" (1967); Bertrand de Jouvenel, a French economist who founded Futuribles International in 1960; and Dennis Gabor, a Hungarian-British scientist who wrote \"Inventing the Future\" (1963) and \"The Mature Society. A View of the Future\" (1972).\n\nFuture studies had a parallel origin with the birth of systems science in academia, and with the idea of national economic and political planning, most notably in France and the Soviet Union. In the 1950s, the people of France were continuing to reconstruct their war-torn country. In the process, French scholars, philosophers, writers, and artists searched for what could constitute a more positive future for humanity. The Soviet Union similarly participated in postwar rebuilding, but did so in the context of an established national economic planning process, which also required a long-term, systemic statement of social goals. Future studies was therefore primarily engaged in national planning, and the construction of national symbols.\n\nBy contrast, in the United States, futures studies as a discipline emerged from the successful application of the tools and perspectives of systems analysis, especially with regard to quartermastering the war-effort. The Society for General Systems Research, founded in 1955, sought to understand cybernetics and the practical application of systems sciences, greatly influencing the U.S. foresight community. These differing origins account for an initial schism between futures studies in America and futures studies in Europe: U.S. practitioners focused on applied projects, quantitative tools and systems analysis, whereas Europeans preferred to investigate the long-range future of humanity and the Earth, what might constitute that future, what symbols and semantics might express it, and who might articulate these.\n\nBy the 1960s, academics, philosophers, writers and artists across the globe had begun to explore enough future scenarios so as to fashion a common dialogue. Several of the most notable writers to emerge during this era include: sociologist Fred L. Polak, whose work \"Images of the Future\" (1961) discusses the importance of images to society’s creation of the future; Marshall McLuhan, whose \"The Gutenberg Galaxy\" (1962) and \"Understanding Media: The Extensions of Man\" (1964) put forth his theories on how technologies change our cognitive understanding; and Rachel Carson’s \"The Silent Spring\" (1962) which was hugely influential not only to future studies but also the creation of the environmental movement.\n\nInventors such as Buckminster Fuller also began highlighting the effect technology might have on global trends as time progressed.\n\nBy the 1970s there was an obvious shift in the use and development of futures studies; it’s focus was no longer exclusive to governments and militaries. Instead, it embraced a wide array of technologies, social issues, and concerns. This discussion on the intersection of population growth, resource availability and use, economic growth, quality of life, and environmental sustainability – referred to as the \"global problematique\" – came to wide public attention with the publication of \"Limits to Growth\", a study sponsored by the Club of Rome which detailed the results of a computer simulation of the future based on economic and population growth. Public investment in the future was further enhanced by the publication of Alvin Toffler’s bestseller \"Future Shock\" (1970), and its exploration of how great amounts of change can overwhelm people and create a social paralysis due to “information overload.”\n\nInternational dialogue became institutionalized in the form of the World Futures Studies Federation (WFSF), founded in 1967, with the noted sociologist, Johan Galtung, serving as its first president. In the United States, the publisher Edward Cornish, concerned with these issues, started the World Future Society, an organization focused more on interested laypeople.\n\nThe first doctoral program on the Study of the Future, was founded in 1969 at the University Of Massachusetts by Christoper Dede and Billy Rojas.The next graduate program (Master's degree) was also founded by Christopher Dede in 1975 at the University of Houston–Clear Lake. Oliver Markley of SRI (now SRI International) was hired in 1978 to move the program into a more applied and professional direction. The program moved to the University of Houston in 2007 and renamed the degree to Foresight. The program has remained focused on preparing professional futurists and providing high-quality foresight training for individuals and organizations in business, government, education, and non-profits. In 1976, the M.A. Program in Public Policy in Alternative Futures at the University of Hawaii at Manoa was established. The Hawaii program locates futures studies within a pedagogical space defined by neo-Marxism, critical political economic theory, and literary criticism. In the years following the foundation of these two programs, single courses in Futures Studies at all levels of education have proliferated, but complete programs occur only rarely. In 2012, the Finland Futures Research Centre started a master's degree Programme in Futures Studies at Turku School of Economics, a business school which is part of the University of Turku in Turku, Finland.\n\nAs a transdisciplinary field, futures studies attracts generalists. This transdisciplinary nature can also cause problems, owing to it sometimes falling between the cracks of disciplinary boundaries; it also has caused some difficulty in achieving recognition within the traditional curricula of the sciences and the humanities. In contrast to \"Futures Studies\" at the undergraduate level, some graduate programs in strategic leadership or management offer masters or doctorate programs in \"strategic foresight\" for mid-career professionals, some even online. Nevertheless, comparatively few new PhDs graduate in Futures Studies each year.\n\nThe field currently faces the great challenge of creating a coherent conceptual framework, codified into a well-documented curriculum (or curricula) featuring widely accepted and consistent concepts and theoretical paradigms linked to quantitative and qualitative methods, exemplars of those research methods, and guidelines for their ethical and appropriate application within society. As an indication that previously disparate intellectual dialogues have in fact started converging into a recognizable discipline, at least six solidly-researched and well-accepted first attempts to synthesize a coherent framework for the field have appeared: 's \"Why Futures Studies?\", James Dator's \"Advancing Futures Studies\", Ziauddin Sardar's \"Rescuing all of our Futures\", Sohail Inayatullah's \"Questioning the future\", Richard A. Slaughter's \"The Knowledge Base of Futures Studies\", a collection of essays by senior practitioners, and Wendell Bell's two-volume work, \"The Foundations of Futures Studies\".\n\nSome aspects of the future, such as celestial mechanics, are highly predictable, and may even be described by relatively simple mathematical models. At present however, science has yielded only a special minority of such \"easy to predict\" physical processes. Theories such as chaos theory, nonlinear science and standard evolutionary theory have allowed us to understand many complex systems as contingent (sensitively dependent on complex environmental conditions) and stochastic (random within constraints), making the vast majority of future events unpredictable, \"in any specific case\".\n\nNot surprisingly, the tension between predictability and unpredictability is a source of controversy and conflict among futures studies scholars and practitioners. Some argue that the future is essentially unpredictable, and that \"the best way to predict the future is to create it.\" Others believe, as Flechtheim, that advances in science, probability, modeling and statistics will allow us to continue to improve our understanding of probable futures, while this area presently remains less well developed than methods for exploring possible and preferable futures.\n\nAs an example, consider the process of electing the president of the United States. At one level we observe that any U.S. citizen over 35 may run for president, so this process may appear too unconstrained for useful prediction. Yet further investigation demonstrates that only certain public individuals (current and former presidents and vice presidents, senators, state governors, popular military commanders, mayors of very large cities, etc.) receive the appropriate \"social credentials\" that are historical prerequisites for election. Thus with a minimum of effort at formulating the problem for statistical prediction, a much reduced pool of candidates can be described, improving our probabilistic foresight. Applying further statistical intelligence to this problem, we can observe that in certain election prediction markets such as the Iowa Electronic Markets, reliable forecasts have been generated over long spans of time and conditions, with results superior to individual experts or polls. Such markets, which may be operated publicly or as an internal market, are just one of several promising frontiers in predictive futures research.\n\nSuch improvements in the predictability of individual events do not though, from a complexity theory viewpoint, address the unpredictability inherent in dealing with entire systems, which emerge from the interaction between multiple individual events.\n\nFuturology is sometimes described by scientists as pseudoscience.\n\nIn terms of methodology, futures practitioners employ a wide range of approaches, models and methods, in both theory and practice, many of which are derived from or informed by other academic or professional disciplines [https://books.google.com/books?id=ILJ_pfMgLqsC&pg=PA242&lpg=PA242&dq=futurists+borrow+from+other+disciplines&source=bl&ots=ycoWOEgcIS&sig=J-Nv6SzqAv-DLsGc7608ZuPyV5I&hl=en&sa=X&ved=0ahUKEwi31u3GgMHSAhXpj1QKHUxBCOAQ6AEIIzAA#v=onepage&q=futurists%20borrow%2 <nowiki>[1]</nowiki>], including social sciences such as economics, psychology, sociology, religious studies, cultural studies, history, geography, and political science; physical and life sciences such as physics, chemistry, astronomy, biology; mathematics, including statistics, game theory and econometrics; applied disciplines such as engineering, computer sciences, and business management (particularly strategy).\n\nThe largest internationally peer-reviewed collection of futures research methods (1,300 pages) is Futures Research Methodology 3.0. Each of the 37 methods or groups of methods contains: an executive overview of each method’s history, description of the method,\nprimary and alternative usages, strengths and weaknesses, uses in combination with other methods, and speculation about future evolution of the method. Some also contain appendixes with applications, links to software, and sources for further information.\n\nGiven its unique objectives and material, the practice of futures studies only rarely features employment of the scientific method in the sense of controlled, repeatable and verifiable experiments with highly standardized methodologies. However, many futurists are informed by scientific techniques or work primarily within scientific domains. Borrowing from history, the futurist might project patterns observed in past civilizations upon present-day society to model what might happen in the future, or borrowing from technology, the futurist may model possible social and cultural responses to an emerging technology based on established principles of the diffusion of innovation. In short, the futures practitioner enjoys the synergies of an interdisciplinary laboratory.\n\nAs the plural term “futures” suggests, one of the fundamental assumptions in futures studies is that the future is plural not singular.[http://www.futures.hawaii.edu/publications/futures-studies/DatorFuturesStudies.pdf <nowiki>[2]</nowiki>] That is, the future consists not of one inevitable future that is to be “predicted,” but rather of multiple alternative futures of varying likelihood which may be derived and described, and about which it is impossible to say with certainty which one will occur. The primary effort in futures studies, then, is to identify and describe alternative futures in order to better understand the driving forces of the present or the structural dynamics of a particular subject or subjects. The exercise of identifying alternative futures includes collecting quantitative and qualitative data about the possibility, probability, and desirability of change. The plural term \"futures\" in futures studies denotes both the rich variety of alternative futures, including the subset of preferable futures (normative futures), that can be studied, as well as the tenet that the future is many.\n\nAt present, the general futures studies model has been summarized as being concerned with \"three Ps and a W\", or possible, probable, and preferable futures, plus wildcards, which are low probability but high impact events (positive or negative). Many futurists, however, do not use the wild card approach. Rather, they use a methodology called Emerging Issues Analysis. It searches for the drivers of change, issues that are likely to move from unknown to the known, from low impact to high impact.\n\nIn terms of technique, futures practitioners originally concentrated on extrapolating present technological, economic or social trends, or on attempting to predict future trends. Over time, the discipline has come to put more and more focus on the examination of social systems and uncertainties, to the end of articulating scenarios. The practice of scenario development facilitates the examination of worldviews and assumptions through the causal layered analysis method (and others), the creation of preferred visions of the future, and the use of exercises such as backcasting to connect the present with alternative futures. Apart from extrapolation and scenarios, many dozens of methods and techniques are used in futures research (see below).\n\nThe general practice of futures studies also sometimes includes the articulation of normative or preferred futures, and a major thread of practice involves connecting both extrapolated (exploratory) and normative research to assist individuals and organizations to model preferred futures amid shifting social changes. Practitioners use varying proportions of collaboration, creativity and research to derive and define alternative futures, and to the degree that a “preferred” future might be sought, especially in an organizational context, techniques may also be deployed to develop plans or strategies for directed future shaping or implementation of a preferred future.\n\nWhile some futurists are not concerned with assigning probability to future scenarios, other futurists find probabilities useful in certain situations, such as when probabilities stimulate thinking about scenarios within organizations [https://pdfs.semanticscholar.org/2870/f9cef0b618c480312802fbb78e49bd69fa83.pdf <nowiki>[3]</nowiki>]. When dealing with the three Ps and a W model, estimates of probability are involved with two of the four central concerns (discerning and classifying both probable and wildcard events), while considering the range of possible futures, recognizing the plurality of existing alternative futures, characterizing and attempting to resolve normative disagreements on the future, and envisioning and creating preferred futures are other major areas of scholarship. Most estimates of probability in futures studies are normative and qualitative, though significant progress on statistical and quantitative methods (technology and information growth curves, cliometrics, predictive psychology, prediction markets, crowdvoting forecasts, etc.) has been made in recent decades.\n\nFutures techniques or methodologies may be viewed as “frameworks for making sense of data generated by structured processes to think about the future”. There is no single set of methods that are appropriate for all futures research. Different futures researchers intentionally or unintentionally promote use of favored techniques over a more structured approach. Selection of methods for use on futures research projects has so far been dominated by the intuition and insight of practitioners; but can better identify a balanced selection of techniques via acknowledgement of foresight as a process together with familiarity with the fundamental attributes of most commonly used methods.\n\nFuturists use a diverse range of forecasting methods including:\n\n\nFuturists use scenarios – alternative possible futures – as an important tool. To some extent, people can determine what they consider probable or desirable using qualitative and quantitative methods. By looking at a variety of possibilities one comes closer to shaping the future, rather than merely predicting it. Shaping alternative futures starts by establishing a number of scenarios. Setting up scenarios takes place as a process with many stages. One of those stages involves the study of trends. A trend persists long-term and long-range; it affects many societal groups, grows slowly and appears to have a profound basis. In contrast, a fad operates in the short term, shows the vagaries of fashion, affects particular societal groups, and spreads quickly but superficially.\n\nSample predicted futures range from predicted ecological catastrophes, through a utopian future where the poorest human being lives in what present-day observers would regard as wealth and comfort, through the transformation of humanity into a posthuman life-form, to the destruction of all life on Earth in, say, a nanotechnological disaster.\n\nFuturists have a decidedly mixed reputation and a patchy track record at successful prediction. For reasons of convenience, they often extrapolate present technical and societal trends and assume they will develop at the same rate into the future; but technical progress and social upheavals, in reality, take place in fits and starts and in different areas at different rates.\n\nMany 1950s futurists predicted commonplace space tourism by the year 2000, but ignored the possibilities of ubiquitous, cheap computers. On the other hand, many forecasts have portrayed the future with some degree of accuracy. Current futurists often present multiple scenarios that help their audience envision what \"may\" occur instead of merely \"predicting the future\". They claim that understanding potential scenarios helps individuals and organizations prepare with flexibility.\n\nMany corporations use futurists as part of their risk management strategy, for horizon scanning and emerging issues analysis, and to identify wild cards – low probability, potentially high-impact risks. Every successful and unsuccessful business engages in futuring to some degree – for example in research and development, innovation and market research, anticipating competitor behavior and so on.\n\nIn futures research \"weak signals\" may be understood as advanced, noisy and socially situated indicators of change in trends and systems that constitute raw informational material for enabling anticipatory action. There is some confusion about the definition of weak signal by various researchers and consultants. Sometimes it is referred as future oriented information, sometimes more like emerging issues. The confusion has been partly clarified with the concept 'the future sign', by separating signal, issue and interpretation of the future sign.\n\nA weak signal can be an early indicator of coming change, and an example might also help clarify the confusion. On May 27, 2012, hundreds of people gathered for a “Take the Flour Back” demonstration at Rothamsted Research in Harpenden, UK, to oppose a publicly funded trial of genetically modified wheat. This was a weak signal for a broader shift in consumer sentiment against genetically modified foods. When Whole Foods mandated the labeling of GMOs in 2013, this non-GMO idea had already become a trend and was about to be a topic of mainstream awareness.\n\n\"Wild cards\" refer to low-probability and high-impact events, such as existential risks. This concept may be embedded in standard foresight projects and introduced into anticipatory decision-making activity in order to increase the ability of social groups adapt to surprises arising in turbulent business environments. Such sudden and unique incidents might constitute turning points in the evolution of a certain trend or system. Wild cards may or may not be announced by weak signals, which are incomplete and fragmented data from which relevant foresight information might be inferred.\nSometimes, mistakenly, wild cards and weak signals are considered as synonyms, which they are not. One of the most often cited examples of a wild card event in recent history is 9/11. Nothing had happened in the past that could point to such a possibility and yet it had a huge impact on everyday life in the United States, from simple tasks like how to travel via airplane to deeper cultural values. Wild card events might also be natural disasters, such as Hurricane Katrina, which can force the relocation of huge populations and wipe out entire crops to completely disrupt the supply chain of many businesses. Although wild card events can’t be predicted, after they occur it is often easy to reflect back and convincingly explain why they happened.\n\nA long-running tradition in various cultures, and especially in the media, involves various spokespersons making predictions for the upcoming year at the beginning of the year. These predictions sometimes base themselves on current trends in culture (music, movies, fashion, politics); sometimes they make hopeful guesses as to what major events might take place over the course of the next year.\n\nSome of these predictions come true as the year unfolds, though many fail. When predicted events fail to take place, the authors of the predictions often state that misinterpretation of the \"signs\" and portents may explain the failure of the prediction.\n\nMarketers have increasingly started to embrace futures studies, in an effort to benefit from an increasingly competitive marketplace with fast production cycles, using such techniques as trendspotting as popularized by Faith Popcorn.\n\nTrends come in different sizes. A mega-trend extends over many generations, and in cases of climate, mega-trends can cover periods prior to human existence. They describe complex interactions between many factors. The increase in population from the palaeolithic period to the present provides an example.\n\nPossible new trends grow from innovations, projects, beliefs or actions that have the potential to grow and eventually go mainstream in the future.\n\nVery often, trends relate to one another the same way as a tree-trunk relates to branches and twigs. For example, a well-documented movement toward equality between men and women might represent a branch trend. The trend toward reducing differences in the salaries of men and women in the Western world could form a twig on that branch.\n\nWhen a potential trend gets enough confirmation in the various media, surveys or questionnaires to show that it has an increasingly accepted value, behavior or technology, it becomes accepted as a bona fide trend. Trends can also gain confirmation by the existence of other trends perceived as springing from the same branch. Some commentators claim that when 15% to 25% of a given population integrates an innovation, project, belief or action into their daily life then a trend becomes mainstream.\nBecause new advances in technology have the potential to reshape our society, one of the jobs of a futurist is to follow these developments and consider their implications. However, the latest innovations take time to make an impact. Every new technology goes through its own life cycle of maturity, adoption, and social application that must be taken into consideration before a probable vision of the future can be created.\n\nGartner created their Hype Cycle to illustrate the phases a technology moves through as it grows from research and development to mainstream adoption. The unrealistic expectations and subsequent disillusionment that virtual reality experienced in the 1990s and early 2000s is an example of the middle phases encountered before a technology can begin to be integrated into society.\n\nEducation in the field of futures studies has taken place for some time. Beginning in the United States of America in the 1960s, it has since developed in many different countries. Futures education encourages the use of concepts, tools and processes that allow students to think long-term, consequentially, and imaginatively. It generally helps students to:\nThorough documentation of the history of futures education exists, for example in the work of Richard A. Slaughter (2004), David Hicks, Ivana Milojević to name a few.\n\nWhile futures studies remains a relatively new academic tradition, numerous tertiary institutions around the world teach it. These vary from small programs, or universities with just one or two classes, to programs that offer certificates and incorporate futures studies into other degrees, (for example in planning, business, environmental studies, economics, development studies, science and technology studies). Various formal Masters-level programs exist on six continents. Finally, doctoral dissertations around the world have incorporated futures studies. A recent survey documented approximately 50 cases of futures studies at the tertiary level.\n\nThe largest Futures Studies program in the world is at Tamkang University, Taiwan. Futures Studies is a required course at the undergraduate level, with between three and five thousand students taking classes on an annual basis. Housed in the Graduate Institute of Futures Studies is an MA Program. Only ten students are accepted annually in the program. Associated with the program is the \"Journal of Futures Studies\".\n\nThe longest running Future Studies program in North America was established in 1975 at the University of Houston–Clear Lake. It moved to the University of Houston in 2007 and renamed the degree to Foresight. The program was established on the belief that if history is studied and taught in an academic setting, then so should the future. Its mission is to prepare professional futurists. The curriculum incorporates a blend of the essential theory, a framework and methods for doing the work, and a focus on application for clients in business, government, nonprofits, and society in general.\n\nAs of 2003, over 40 tertiary education establishments around the world were delivering one or more courses in futures studies. The World Futures Studies Federation has a comprehensive survey of global futures programs and courses. The Acceleration Studies Foundation maintains an annotated list of primary and secondary graduate futures studies programs.\n\nOrganizations such as Teach The Future also aim to promote future studies in the secondary school curriculum in order to develop structured approaches to thinking about the future in public school students. The rationale is that a sophisticated approach to thinking about, anticipating, and planning for the future is a core skill requirement that every student should have, similar to literacy and math skills.\n\nSeveral corporations and government agencies utilize foresight products to both better understand potential risks and prepare for potential opportunities. Several government agencies publish material for internal stakeholders as well as make that material available to broader public. Examples of this include the US Congressional Budget Office long term budget projections, the National Intelligence Center, and the United Kingdom Government Office for Science. Much of this material is used by policy makers to inform policy decisions and government agencies to develop long term plan. Several corporations, particularly those with long product development lifecycles, utilize foresight and future studies products and practitioners in the development of their business strategies. The Shell Corporation is one such entity. Foresight professionals and their tools are increasingly being utilized in both the private and public areas to help leaders deal with an increasingly complex and interconnected world.\n\nDesign and futures studies have many synergies as interdisciplinary fields with a natural orientation towards the future. Both incorporate studies of human behavior, global trends, strategic insights, and anticipatory solutions.\n\nDesigners have adopted futures methodologies including scenarios, trend forecasting, and futures research. Design thinking and specific techniques including ethnography, rapid prototyping, and critical design have been incorporated into in futures as well. In addition to borrowing techniques from one another, futurists and designers have joined to form agencies marrying both competencies to positive effect. The continued interrelation of the two fields is an encouraging trend that has spawned much interesting work.\n\nThe Association for Professional Futurists has also held meetings discussing the ways in which Design Thinking and Futures Thinking intersect and benefit one another.\n\nImperial cycles represent an \"expanding pulsation\" of \"mathematically describable\" macro-historic trend. The List of largest empires contains imperial record progression in terms of territory or percentage of world population under single imperial rule.\n\nChinese philosopher K'ang Yu-wei and French demographer Georges Vacher de Lapouge in the late 19th century were the first to stress that the trend cannot proceed indefinitely on the definite surface of the globe. The trend is bound to culminate in a world empire. K'ang Yu-wei estimated that the matter will be decided in the contest between Washington and Berlin; Vacher de Lapouge foresaw this contest between the United States and Russia and estimated the chance of the United States higher. Both published their futures studies before H. G. Wells introduced the science of future in his \"Anticipations\" (1901).\n\nFour later anthropologists—Hornell Hart, Raoul Naroll, Louis Morano, and Robert Carneiro—researched the expanding imperial cycles. They reached the same conclusion that a world empire is not only pre-determined but close at hand and attempted to estimate the time of its appearance.\n\nAs foresight has expanded to include a broader range of social concerns all levels and types of education have been addressed, including formal and informal education. Many countries are beginning to implement Foresight in their Education policy. A few programs are listed below:\n\nBy the early 2000s, educators began to independently institute futures studies (sometimes referred to as futures thinking) lessons in K-12 classroom environments. To meet the need, non-profit futures organizations designed curriculum plans to supply educators with materials on the topic. Many of the curriculum plans were developed to meet common core standards. Futures studies education methods for youth typically include age-appropriate collaborative activities, games, systems thinking and scenario building exercises.\n\nWendell Bell and Ed Cornish acknowledge science fiction as a catalyst to future studies, conjuring up visions of tomorrow. Science fiction’s potential to provide an “imaginative social vision” is its contribution to futures studies and public perspective. Productive sci-fi presents plausible, normative scenarios. Jim Dator attributes the foundational concepts of “images of the future” to Wendell Bell, for clarifying Fred Polak’s concept in Images of the Future, as it applies to futures studies. Similar to futures studies’ scenarios thinking, empirically supported visions of the future are a window into what the future could be. Pamela Sargent states, “Science fiction reflects attitudes typical of this century.” She gives a brief history of impactful sci-fi publications, like The Foundation Trilogy, by Isaac Asimov and Starship Troopers, by Robert A. Heinlein. Alternate perspectives validate sci-fi as part of the fuzzy “images of the future.” However, the challenge is the lack of consistent futures research based literature frameworks. Ian Miles reviews The New Encyclopedia of Science Fiction,” identifying ways Science Fiction and Futures Studies “cross-fertilize, as well as the ways in which they differ distinctly.” Science Fiction cannot be simply considered fictionalized Futures Studies. It may have aims other than “prediction, and be no more concerned with shaping the future than any other genre of literature.” It is not to be understood as an explicit pillar of futures studies, due to its inconsistency of integrated futures research. Additionally, Dennis Livingston, a literature and Futures journal critic says, “The depiction of truly alternative societies has not been one of science fiction’s strong points, especially” preferred, normative envisages.\n\nSeveral governments have formalized strategic foresight agencies to encourage long range strategic societal planning, with most notable are the governments of Singapore, Finland, and the United Arab Emirates. Other governments with strategic foresight agencies include Canada's Policy Horizons Canada and the Malaysia's Malaysian Foresight Institute.\n\nThe Singapore government's Centre for Strategic Futures (CSF) is part of the Strategy Group within the Prime Minister's Office. Their mission is to position the Singapore government to navigate emerging strategic challenges and harness potential opportunities. Singapore’s early formal efforts in strategic foresight began in 1991 with the establishment of the Risk Detection and Scenario Planning Office in the Ministry of Defence. In addition to the CSF, the Singapore government has established the Strategic Futures Network, which brings together deputy secretary-level officers and foresight units across the government to discuss emerging trends that may have implications for Singapore.\n\nSince the 1990s, Finland has integrated strategic foresight within the parliament and Prime Minister’s Office. The government is required to present a “Report of the Future” each parliamentary term for review by the parliamentary Committee for the Future. Led by the Prime Minister’s Office, the Government Foresight Group coordinates the government’s foresight efforts. Futures research is supported by the Finnish Society for Futures Studies (established in 1980), the Finland Futures Research Centre (established in 1992), and the Finland Futures Academy (established in 1998) in coordination with foresight units in various government agencies.\n\nIn the United Arab Emirates, Sheikh Mohammed bin Rashid, Vice President and Ruler of Dubai, announced in September 2016 that all government ministries were to appoint Directors of Future Planning. Sheikh Mohammed described the UAE Strategy for the Future as an \"integrated strategy to forecast our nation’s future, aiming to anticipate challenges and seize opportunities\". The Ministry of Cabinet Affairs and Future(MOCAF) is mandated with crafting the UAE Strategy for the Future and is responsible for the portfolio of the future of UAE.\n\nForesight is also applied when studying potential risks to society and how to effectively deal with them. These risks may arise from the development and adoption of emerging technologies and/or social change. Special interest lies on hypothetical future events that have the potential to damage human well-being on a global scale - global catastrophic risks. Such events may cripple or destroy modern civilization or, in the case of existential risks, even cause human extinction. Potential global catastrophic risks include but are not limited to hostile artificial intelligence, nanotechnology weapons, climate change, nuclear warfare, total war, and pandemics.\n\nSeveral authors have become recognized as futurists. They research trends, particularly in technology, and write their observations, conclusions, and predictions. In earlier eras, many futurists were at academic institutions. John McHale, author of \"The Future of the Future\", published a 'Futures Directory', and directed a think tank called \"The Centre For Integrative Studies\" at a university. Futurists have started consulting groups or earn money as speakers, with examples including Alvin Toffler, John Naisbitt and Patrick Dixon. Frank Feather is a business speaker that presents himself as a pragmatic futurist. Some futurists have commonalities with science fiction, and some science-fiction writers, such as Arthur C. Clarke, are known as futurists. In the introduction to \"The Left Hand of Darkness\", Ursula K. Le Guin distinguished futurists from novelists, writing of the study as the business of prophets, clairvoyants, and futurists. In her words, \"a novelist's business is lying\".\n\nA survey of 108 futurists found that they share a variety of assumptions, including in their description of the present as a critical moment in an historical transformation, in their recognition and belief in complexity, and in their being motivated by change and having a desire for an active role bringing change (versus simply being involved in forecasting).\n\nThe Association for Professional Futurists recognizes the Most Significant Futures Works for the purpose of identifying and rewarding the work of foresight professionals and others whose work illuminates aspects of the future.\n\n\n\n\n"}
{"id": "23428417", "url": "https://en.wikipedia.org/wiki?curid=23428417", "title": "GPoT Center", "text": "GPoT Center\n\nGPoT Center (Turkish: \"Küresel Siyasal Eğilimler Merkezi\") officially the Global Political Trends Center is a research unit, which was established by Mensur Akgun and Sylvia Tiryaki at the Istanbul Kültür University in Turkey in 2008. The mission of the Center is to conduct research, projects, to produce innovative publications analyzing the latest issues in the international relations, to formulate viable political recommendations and to boost the dialogue between civil society, academia and media.\n\nGPoT Center, as one of the leading Turkish think tanks, runs projects in various areas and has organized numerous conferences, workshops and round-table meetings focused on: the EU - Turkey relations, the Cyprus question, the Turkey - Armenia relations, Turkish - Arab dialogue, NATO, second track diplomacy between Syria and Israel, Arab-Israeli conflict and other issues related to the international agenda in general and the agenda of Turkish foreign policy in particular.\n\nThe projects implemented by GPoT Center often involve participation of high-level politicians and opinion makers, however the Center is also active in organization of second-track diplomacy meetings aiming at effective conflict solution. For the purposes of second-track diplomacy, GPoT Center brings together representatives of conflicting parties, i.e. civil society activists, opinion leaders, as well as academics to discuss issues pertaining to solution of the relevant problem. The meetings are usually conducted in friendly atmosphere and under Chatam House rules.\n\nThe Center publishes various types of publications available in the electronic as well as printed form. It has published a number of policy briefs analyzing issues in international relations and shorter essays, i.e. GPoT Briefs focusing on one topic from the current global issues. GPoT Center published its first two books on Cyprus issue in the summer 2009 and has since then published other books on developments in Cyprus, Israeli-Turkish relations, as well as democratization of the Middle East.\n\nThe Center cooperates with institutions from Sweden - SIIA, Lebanon - CAUS, Qatar - ADF, United Kingdom - LSE, Czech Republic - EUROPEUM, Slovak Republic - SFPA, Armenia - EPF, YPC and Internews, Egypt - Ibn-Khaldun Center, and France - ENS. GPoT Center is also part of larger research and academic networks such as the United Nations's Alliance of Civilizations, International Research and Security Network, United Nations' Committee on the Exercise of the Inalienable Rights of the Palestinian People and Anna Lindh Euro-Mediterranean Foundation for the Dialogue Between Cultures.\n\n"}
{"id": "31071816", "url": "https://en.wikipedia.org/wiki?curid=31071816", "title": "Geoprofessions", "text": "Geoprofessions\n\nGeoprofessions is a term coined by the Geoprofessional Business Association to connote various technical disciplines that involve engineering, earth and environmental services applied to below-ground (“subsurface”), ground-surface, and ground-surface-connected conditions, structures, or formations. The principal disciplines include, as major categories:\n\nEach discipline involves specialties, many of which are recognized through professional designations that governments and societies or associations confer based upon a person’s education, training, experience, and educational accomplishments. In the United States, engineers must be licensed in the state or territory where they practice engineering. Most states license geologists and several license environmental “site professionals.” Several states license engineering geologists and recognize geotechnical engineering through a geotechnical-engineering titling act.\n\nAlthough geotechnical engineering is applied for a variety of purposes, it is essential to foundation design. As such, geotechnical engineering is applicable to every existing or new structure on the planet; every building and every highway, bridge, tunnel, harbor, airport, water line, reservoir, or other public work. Commonly, the geotechnical-engineering service comprises a study of subsurface conditions using various sampling, in-situ testing, and/or other site-characterization techniques. The instrument of professional service in those cases typically is a report through which geotechnical engineers relate the information they have been retained to provide, typically: their findings; their opinions about subsurface materials and conditions; their judgment about how the subsurface materials and conditions assumed to exist probably will behave when subjected to loads or used as building material; and their preliminary recommendations for materials usage or appropriate foundation systems, the latter based on their knowledge of a structure’s size, shape, weight, etc., and the subsurface/structure interactions likely to occur. Civil engineers, structural engineers, and architects, feasibly among other members of the project team, apply the geotechnical findings and preliminary recommendations to take the structure’s design forward. They realize these preliminary recommendations are subject to change, however, because – as a matter of practical necessity related to the observational method inherent to geotechnical engineering – geotechnical engineers base their recommendations on the composition of samples taken from a tiny portion of a site whose actual subsurface conditions are unknowable before excavation, because they are hidden by earth and/or rock and/or water. For this reason, as a key component of a complete geotechnical engineering service, geotechnical engineers employ construction-materials engineering and testing (CoMET) to observe subsurface materials as they are exposed through excavation. To help achieve economies on their clients’ behalf, geotechnical engineers assign their field representatives – specially educated and trained paraprofessionals – to observe the excavated materials and the excavations themselves in light of conditions the geotechnical engineers opined to exist. When differences are discovered, the geotechnical engineers evaluate the new findings and, when necessary, modify their design and construction recommendations. Because such changes could require other members of the design and construction team to modify their designs, specifications, and proposed methods, many owners have their geotechnical engineers serve as active members of the project team from project inception to conclusion, working with others to help ensure appropriate application of geotechnical information and judgments.\n\nIn other cases, geotechnical engineering goes beyond a study and construction recommendations to include design of soil and rock structures. The most common of these are the pavements that make up our streets and highways, airport runways, and bridge and tunnel decks, among other paved improvements. Geotechnical engineers design the pavements in terms of the subgrade, subbase, and base layers of materials to be used, and the thickness and composition of each. Geotechnical engineers also design the earth-retention walls associated with structures such as levees, earthen dams, reservoirs, and landfills. In other cases, the design is applied to contain earth, via structures such as excavation-support systems and retaining walls. Sometimes referred to as geostructural engineering or geostructural design, these services are also intrinsic to hydraulic engineering, hydrogeologic engineering, coastal engineering, geologic engineering and water-resources engineering. Geotechnical-engineering design is also applied for structures such as tunnels, bridges, dams, and other structures beneath, on, or connected to the surface of the earth. Geotechnical engineering, like geology, engineering geology, and geologic engineering, also involves the specialties of rock mechanics and soil mechanics, and often requires knowledge of geotextiles and geosynthetics, as well as an array of instrumentation and monitoring equipment, to help ensure specified conditions are achieved and maintained.\n\nEarthquake engineering and landslide detection, remediation, and prevention are geoprofessional services associated with specialized types of geotechnical engineering (as well as geophysics; see below), as is forensic geotechnical engineering, a geoprofessional service applied to determine why a certain applicable type of event – usually a failure of some sort – occurred. (Virtually all geoprofessional services can be performed for forensic purposes, commonly as litigation-support/expert witness services.) Railway-systems engineering is another type of specialized geotechnical engineering, as are the design of piers and bulkheads, drydocks, on-shore and off-shore wind-turbine systems, and systems that stabilize oil platforms and other marine structures to the sea floor.\n\nGeotechnical engineers have long been involved in sustainability initiatives, including (among many others) the use of excavated materials; the safe application of contaminated subsurface materials; the recycling of asphalt, concrete, and building rubble and debris; and the design of permeable pavements.\n\nAll civil-engineering specialties and projects – roads and highways, bridges, rail systems, ports and other waterfront structures, airport terminals, etc. – require the involvement of geotechnical engineers and engineering, meaning that many civil-engineering pursuits are geoprofessional pursuits to a greater or lesser degree. However, geotechnical engineering has for centuries also been associated with military engineering; sappers (in general) and miners (whose tunneling design services (known as landmining and undermining) were used in military-siege operations).\n\nEngineering geologist.\n(a) Elements of the engineering geologist specialty. \nThe practice of engineering geology involves the interpretation, evaluation, analysis, and application of geological information and data to civil works. Geotechnical soil and rock units are designated, characterized, and classified, using standard engineering soil and rock classification systems. Relationships are interpreted between landform development, current and past geologic processes, ground and surface water, and the strength characteristics of soil and rock. Processes evaluated include both surficial processes (for example, slope, fluvial, and coastal processes), and deep-seated processes (for example, volcanic activity and seismicity). Geotechnical zones or domains are designated based on soil and rocked geological strength characteristics, common landforms, related geologic processes, or other pertinent factors. Proposed developmental modifications are evaluated and, where appropriate, analyzed to predict potential or likely changes in types and rates of surficial geologic processes. Proposed modifications may include such things as vegetation removal, using various types of earth materials in construction, applying loads to shallow or deep foundations, constructing cut or fill slopes and other grading, and modifying ground and surface water flow. The effects of surficial and deep-seated geologic processes are evaluated and analyzed to predict their potential effect on public health, public safety, land use, or proposed development.\n(b) Typical engineering geologic applications and types of projects. Engineering geology is applied during all project phases, from conception through planning, design, construction, maintenance, and, in some cases, reclamation and closure. Planning-level engineering geologic work is commonly conducted in response to forest practice regulations, critical areas ordinances, and the State Environmental Policy Act. Typical planning-level engineering geologic applications include timber harvest planning, proposed location of residential and commercial developments and other buildings and facilities, and alternative route selection for roads, rail lines, trails, and utilities. Site-specific engineering geologic applications include cuts, fills, and tunnels for roads, trails, railroads, and utility lines; foundations for bridges and other drainage structures, retaining walls and shoring, dams, buildings, water towers, slope, channel and shoreline stabilization facilities, fish ladders and hatcheries, ski lifts and other structures; landings for logging and other work platforms; airport landing strips; rock bolt systems; blasting; and other major earthwork projects such as for aggregate sources and landfills.\n\nWhile engineering geology is applicable principally to planning, design and construction activities, other specialties of geology are applied in a variety of geoprofessional specialty fields, such as mining geology, petroleum geology, and environmental geology. Note that mining geology and mining engineering are different geoprofessional fields.\n\nGeological engineering is a hybrid discipline that comprises elements of civil engineering, mining engineering, petroleum engineering, and earth sciences. Geological engineers often become licensed as both engineers and geologists. There are thirteen geological-engineering (or geoengineering) programs in the United States that are accredited by the Engineering Accreditation Commission (EAC) of ABET: (1) Colorado School of Mines, (2) Michigan Technological University, (3) Missouri University of Science and Technology, (4) Montana Tech of the University of Montana, (5) South Dakota School of Mines and Technology, (6) University of Alaska-Fairbanks, (7) University of Minnesota Twin Cities, (8) University of Mississippi, (9) University of Nevada, Reno (10) University of North Dakota, (11) University of Texas at Austin, (12) University of Utah, and (13) University of Wisconsin-Madison.\n\nOther schools offer programs or classes in geological engineering, including the University of Arizona.\n\nGeoengineering or geological engineering, engineering geology, and geotechnical engineering deal with the discovery, development, and production and use of subsurface earth resources, as well as the design and construction of earthworks. Geoengineering is the application of geosciences, where mechanics, mathematics, physics, chemistry, and geology are used to understand and shape our interaction with the earth.\n\nGeoengineers work in areas of\n\nProfessional geoscience organizations such as the American Rock Mechanics Association or the Geo-Institute and academic degrees such as the bachelor of geoengineering accredited by ABET acknowledge the broad scope of work practiced by geoengineers and stress fundamentals of science and engineering methods for the solution of complex problems. Geoengineers study the mechanics of rock, soil, and fluids to improve the sustainable use of earth’s finite resources, where problems appear with competing interests, for example, groundwater and waste isolation, offshore oil drilling and risk of spills, natural gas production and induced seismicity.\n\nGeophysics is the study of the physical properties of the earth using quantitative physical methods to determine what lies beneath the earth's surface. The physical properties of concern include the propagation of elastic waves (seismic), magnetism, gravity, electrical resistivity/conductivity, and electromagnetism. Geophysics has historically been most commonly used in oil exploration and mining, but its popularity in non-destructive investigative work has flourished since the early 1990s. It is also used in groundwater exploration and protection, geo-hazard studies (e.g., faults and landslides), alignment studies (e.g., proposed roadway, underground utilities, and pipelines), foundation studies, contamination characterization and remediation, landfill investigations, unexploded-ordnance investigations, vibration monitoring, dam-safety evaluation, location of underground storage tanks, identification of subsurface voids, and assisting in archeological investigations. (definition from Association of Environmental & Engineering Geologists)\n\nGeophysical engineering is the application of geophysics to the engineering design of facilities including roads, tunnels, and mines.\n\nEnvironmental science and environmental engineering are the geoprofessions commonly associated with the identification, remediation, and prevention of environmental contamination. These services range from phase-one and phase-two environmental site-assessments – research designed to assess the likelihood that a property is contaminated and subsurface exploration conducted to identify the nature and extent of contamination, respectively – up through the design of processes and systems to remediate contaminated sites for the protection of human health and the environment.\n\nEnvironmental geology is one of the principal geoprofessions engaged in assessing and remediating contaminated sites. Environmental geologists help identify the subsurface stratigraphy in which contaminants are located and through which they migrate. Environmental chemistry is the geoprofession that encompasses the study of chemical compounds in the soil. These compounds are categorized as pollutants or contaminants when introduced into the environment by human factors (e.g., waste, mining processes, radioactive release) and are not of natural origin. Environmental chemistry assesses interactions or these compounds with soil, rock, and water to determine their fate and transport, the techniques to measure the levels of contaminants in the environment, and technologies to destroy or reduce the toxicity of contaminants in wastes or compounds that have been released to the environment. Environmental engineering is often applied to assess contaminated sites, but more often is used in the design of systems to remediate contaminated soil and groundwater.\n\nHydrogeology is the geoprofession involved when environmental studies involve subsurface water. Hydrogeology applications range from securing safe, plentiful underground drinking-water sources to identifying the nature of groundwater contamination in order to facilitate remediation. Environmental toxicology is a geoprofession when used to identify the source, fate, transformation, effects, and risks of pollutants on the environment, including soil, water, and air. Wetlands science is a geoprofessional pursuit that incorporates several scientific disciplines, such as botany, biology, and limnology. It involves, among other activities, the delineation, conservation, restoration, and preservation of wetlands. These services are sometimes conducted by geoprofessional specialists called wetlands scientists. Ecology is a closely related environmental geoprofession involving studies into the distribution of organisms and biodiversity within an environmental context.\nNumerous geoprofessional disciplines contribute to the redevelopment of brownfields, sites (typically urban) that are underused or abandoned because they are or are assumed to be contaminated by hazardous materials. Geoprofessionals are engaged to evaluate the degree to which such sites are contaminated and the steps that can be taken to achieve the sites’ safe reuse. Environmental engineers and scientists work with developers to identify and design remediation strategies and exposure-barrier designs that protect future site users from unacceptable exposure to environmental contamination resulting from previous uses of the site. Because these previous uses often resulted in degraded soil conditions and the presence of abandoned, underground structures, geotechnical engineers often are needed to design special foundations for the new structures.\n\nConstruction-materials engineering and testing (CoMET) comprises an array of licensed-engineer-directed professional services applied principally for purposes of construction quality assurance and quality control. CoMET services commonly are provided as a separate discipline by firms that also practice geotechnical engineering, possibly among other geoprofessional disciplines. The geoprofessional-service industry has evolved in this manner because geotechnical engineering employs the observational method. Karl von Terzaghi and Ralph B. Peck – the creators of modern geotechnical engineering – used the observational method and multiple working hypotheses to expedite and economize the subsurface-exploration process, by using sampling and testing to form a judgment about subsurface conditions, and then observing excavated conditions and materials to confirm or modify those judgments and related recommendations, and then finalize them. To economize still further, geoprofessionals educated and trained paraprofessionals to represent them on site (hence the term “field representative”), especially to apply their judgment (much as a geotechnical engineer would) in comparing observed conditions with those the geotechnical engineer believed would exist. Over time, geotechnical engineers expanded their CoMET services by providing the additional education and training their field representatives needed to evaluate constructors’ attainment of conditions commonly specified by geoprofessionals; e.g., subsurface preparation for foundations of buildings, roadways, and other structures; materials used for subgrade, subbase, and base purposes; site grading; construction of earthen structures (earth dams, levees, reservoirs, landfills, et al.) and earth-retaining structures (e.g., retaining walls); and so on. Because many of the materials involved, such as concrete, are used in other elements of construction projects and structures, geoprofessional firms expanded their field representatives’ skill sets still more, to encompass observation and testing of numerous additional materials (e.g., reinforced concrete, structural steel, masonry, wood, and fireproofing), processes (e.g., cutting and filling and rebar placement), and outcomes (e.g., the effectiveness of welds). Laboratory services are a common element of many CoMET operations. Also operating under the direction of a licensed engineer, they are applied in geotechnical engineering to evaluate subsurface-material samples. In overall CoMET operations, laboratories operate with the equipment and personnel required to evaluate a variety of construction materials.\n\nCoMET services applied to evaluate the actual composition of a site’s subsurface are part of a complete geotechnical engineering service. For purposes of short-term economy, however, some owners select a firm not associated with the geotechnical engineer of record to provide these and all other CoMET services. This approach precludes the geotechnical engineer of record from providing a complete service. It also aggravates risk, because the individuals engaged to evaluate actual subsurface conditions are not “briefed” by the geotechnical engineer of record before they go to the project site and seldom communicate with the geotechnical engineer of record when they discern differences, in large part because the firm associated with the geotechnical engineer of record is regarded as a competitor of the firm employing the field representatives. In some cases, the field representatives in question lack the specific project background information and/or the education and training required to discern those differences.\n\nCoMET services applied to evaluate constructor’s attainment of specified conditions take the form of quality-assurance (QA) or quality-control (QC) services. QA services are performed directly or indirectly for the owner. The owner specifies the nature and extent of QA services that the owner believes is appropriate. Some owners specify none at all or only those that may be required by law. Those required by law are imposed via a jurisdiction’s building code. Almost all U.S. jurisdictions base their building codes on “model codes” developed by associations of building officials. The International Code Council (ICC) is the most prominent of these groups and its International Building Code (IBC) is the most commonly used model. As a result, many jurisdictions now require IBC “Special Inspection,” a term defined by the IBC as “the required examination of the materials, installation, fabrication, erection, or placement of components and connections requiring special expertise to ensure compliance with approved construction documents and referenced standards.” Special Inspection requirements vary from jurisdiction to jurisdiction based on the provisions adopted by the local building official. While some of the services involved may be similar to or the same as conventional CoMET services, Special Inspection is handled differently. Most commonly, the owner or the owner’s agent is required to retain a building-official-approved Special Inspection-services provider. Special Inspection is often required to obtain a certificate of occupancy.\n\nQC services are those applied by or on behalf of a constructor to ensure the constructor has attained conditions the constructor has contractually agreed to attain. Most CoMET consultants are engaged far more to provide QA services than QC services.\n\nMany CoMET procedures are specified in standards developed by standards-developing organizations (SDOs) such as the American Society of Civil Engineers (ASCE), ASTM International, and American Concrete Institute (ACI), using standards-development protocols approved by the American National Standards Institute (ANSI) and/or the International Organization for Standardization (ISO). All such standards identify what is minimally required to conform. Likewise, several organizations have developed programs to accredit CoMET field and laboratory services to perform certain types of testing and inspection. Some of these programs are more comprehensive than others; e.g., requiring regular calibration of equipment, participation in proficiency testing programs, and implementation and documentation of a (quality) management system to demonstrate technical competence. As with all such programs, of course, accreditation identifies what is least acceptable. Many CoMET laboratories go far beyond minimum requirements in an effort to attain higher levels of quality.\n\nA variety of organizations – including local building departments – have developed personnel-certification protocols and requirements. In many jurisdictions, only appropriately certified individuals are permitted to perform certain evaluations. Individuals typically are required to meet certain prerequisites for certification and must pass examinations, in some cases involving performance observation in the field. The prerequisite for higher degrees of certification often include a requirement that the individual has met requirements for a lower degree of certification (e.g., Soils Technician I is in some cases a prerequisite for Soils Technician II). It should be noted that field representatives are sometimes referred to as “soil testers,” “technicians,” “technicians/technologists,” or “engineering technicians.” The Geoprofessional Business Association developed the term “field representative” to encompass all the many types of paraprofessionals involved (e.g., those involved with specific types of materials, such as reinforced concrete, soil, or steel; those who observe or inspect processes or conditions, such as welding inspectors, caisson inspectors, and foundation inspectors), and especially to underscore their significant, mutual responsibility, that purpose titles such as “technician” fail to signify. In fact, the engineers who direct CoMET operations are personally and professionally responsible and liable for their field representatives’ acts and statements while representing the engineer on site.\n\nEspecially because CoMET consultants have more hands-on experience with construction activities than many other design-team members, many owners involve them (among other geoprofessionals) from the outset of a project, during the design phase, to help the owner and/or design team members develop technical specifications and establish testing and inspection requirements, instrumentation requirements and procedures, and observation programs. Geotechnical engineers employ CoMET services during the earliest stages of a project, to oversee subsurface sampling procedures, such as drilling.\n\nMany of the CoMET services performed for construction projects are performed for environmental projects as well, but requirements tend to be less rigid because they involve fewer licensing and related requirements. For example, individuals may perform federally mandated all-appropriate inquiries – typically a phase-one environmental site assessment – without a license of any kind.\n\nTo the extent that archeology and paleontology require systematic subsurface excavation to recover artifacts, they, too, are considered geoprofessions. Many geoprofessional-services firms offer these services to those of their clients that need to satisfy federal and/or state regulations that require paleontological and/or archeological inquiry before site development or redevelopment activities can proceed.\n\n\n"}
{"id": "22541326", "url": "https://en.wikipedia.org/wiki?curid=22541326", "title": "House of Numbers: Anatomy of an Epidemic", "text": "House of Numbers: Anatomy of an Epidemic\n\nHouse of Numbers: Anatomy of an Epidemic is a 2009 documentary film directed, produced, and hosted by Brent Leung and described by him as an objective examination of the idea that HIV causes AIDS. The film argues that human immunodeficiency virus (HIV) is harmless and does not cause acquired immune deficiency syndrome (AIDS), a position known as AIDS denialism. The film's claims of impartiality have been widely rejected by scientists, and the film's claims about HIV/AIDS have been dismissed as pseudoscience and conspiracy theory masquerading as even-handed examination.\n\nA group of scientists interviewed for the film later complained that their comments had been misrepresented and taken out of context, and that the film promotes pseudoscience. The film also interviews Christine Maggiore, a prominent AIDS denialist who later died after suffering from AIDS-related conditions.\n\nLeung has declined to discuss funding for the film except to state that funders came from \"all over the world\". In the film, Leung interviews a range of scientists and AIDS denialists, most notably Christine Maggiore. At the time of filming, Maggiore was HIV-positive and appeared healthy, despite her refusal to take anti-retroviral medication, which mainstream medicine uses to slow down the rate at which HIV destroys CD4+ T-cells. As she said in the film, she refused to take the medication, and had not had her daughter, Eliza Jane Scovill, tested, or provided her with medication, because she believed HIV did not cause AIDS. Rather, she believed that the medication itself caused AIDS. Maggiore's relative health, despite years of infection, is used by the film to support the idea that anti-retrovirals are unnecessary to combat, and may themselves cause, AIDS.\n\nThe film was screened at the multiple small film festivals, including the London Raindance film festival A panel discussion of the film at a Boston film festival was disrupted by Leung and other AIDS denialists in the audience, who attempted to shout down members of the panel with whom they disagreed.\n\nBoth Maggiore and her daughter died of AIDS-related complications before the film's release, although their deaths are mentioned only in small print during the closing credits along with a claim that Maggiore's death was \"unrelated to HIV.\" Maggiore's daughter died in September 2005 of AIDS-related infections, although Maggiore rejected the cause of death and argued that the coroner's report was politically motivated. Maggiore herself died in December 2008 from AIDS-related opportunistic infections.\n\nEighteen scientists interviewed in the film stated that their answers to Leung's questions were selectively edited to convey a false sense that the scientific community disagrees on basic facts about HIV/AIDS. Two interviewees, Neil Constantine and Robin Weiss, cite examples supporting the allegation that Leung misrepresented their words in a \"surely intentional\" manner. Brent Leung denied taking quotes out of context.\n\nThe film's promotion of AIDS denialism, a pseudoscientific movement implicated in thousands of deaths, drew criticism and anger. The \"New York Times\" characterized the film as \"a weaselly support pamphlet for AIDS denialists\", \"willfully ignorant\", and \"a globe-trotting pseudo-investigation that should raise the hackles of anyone with even a glancing knowledge of the basic rules of reasoning.\" \"The Wall Street Journal\" cited the film as part of \"this season's fashion in conspiracy theories.\" The \"Portland Oregonian\" criticized Leung for \"not being entirely honest with viewers,\" and decried the film's reliance on \"selective editing, anomalies and anecdotes, unsupported conclusions... and suppression of inconvenient facts.\"\n\nReaction from the scientific community was similarly negative. \"Lancet Infectious Diseases\" criticized the film's arguments, calling them a \"toxic combination of misrepresentation and sophistry.\" AIDSTruth.org, a website created by HIV researchers to address AIDS denialism, criticized the film for concealing its \"agenda behind a false veneer of honest inquiry\", and published a rebuttal to some of the film's claims. Ben Goldacre, writing in \"The Guardian\", described \"House of Numbers\" as \"a dreary and pernicious piece of Aids denialist propaganda.\"\n\nIn February 2014 several people involved with the film filed DMCA notices against a YouTube science blogger named Myles Power, who had made a video series debunking claims made in the film. Power argued that the film was fair use as criticism and education. Several commentators described the notices as attempted censorship. The videos were restored several days later.\n\n\n"}
{"id": "5040", "url": "https://en.wikipedia.org/wiki?curid=5040", "title": "Inedia", "text": "Inedia\n\nInedia (Latin for \"fasting\") or breatharianism is the belief that it is possible for a person to live without consuming food. Breatharians claim that food, and in some cases water, are not necessary for survival, and that humans can be sustained solely by \"prana\", the vital life force in Hinduism. According to Ayurveda, sunlight is one of the main sources of \"prana\", and some practitioners believe that it is possible for a person to survive on sunlight alone. The terms \"breatharianism\" or \"inedia\" may also refer to this philosophy when it is practiced as a lifestyle in place of the usual diet.\n\nBreatharianism is considered a deadly pseudoscience by scientists and medical professionals, and several adherents of these practices have died from starvation and dehydration. Though it is common knowledge that biological entities require sustenance to survive, breatharianism continues.\n\nNutritional science proves that fasting for extended periods leads to starvation, dehydration, and eventual death. In the absence of food intake, the body normally burns its own reserves of glycogen, body fat, and muscle. Breatharians claim that their bodies do not consume these reserves while fasting.\n\nSome breatharians have submitted themselves to medical testing, including a hospital's observation of Indian mystic Prahlad Jani appearing to survive without food or water for 15 days, and an Israeli breatharian appearing to survive for eight days on a television documentary. In a handful of documented cases, individuals attempting breatharian fasting have died. Among the claims in support of Inedia investigated by the Indian Rationalist Association, all were found to be fraudulent. In other cases, people have attempted to survive on sunlight alone, only to abandon the effort after losing a large percentage of their body weight.\n\nThe 1670 Rosicrucian text \"Comte de Gabalis\" attributed the practice to the physician and occultist Paracelsus (1493–1541) who was described as having lived \"several years by taking only one half scrupule of Solar Quintessence\". In this book, it is also stated that, \"Paracelsus affirms that He has seen many of the Sages fast twenty years without eating anything whatsoever.\"\n\nRam Bahadur Bomjon is a young Nepalese Buddhist monk who lives as an ascetic in a remote area of Nepal. Bomjon appears to go for periods of time without ingesting either food or water. One such period was chronicled in a 2006 Discovery Channel documentary titled \"The Boy With Divine Powers\", which reported that Bomjon neither moved, ate, nor drank anything during 96 hours of filming.\n\nPrahlad Jani is an Indian sadhu who says he has lived without food and water for more than 70 years. His claims were investigated by doctors at Sterling Hospital, Ahmedabad, Gujarat in 2003 and 2010. The study concluded that Prahlad Jani was able to survive under observation for two weeks without either food or water, and had passed no urine or stool, with no need for dialysis. Interviews with the researchers speak of strict observation and relate that round-the-clock observation was ensured by multiple CCTV cameras. Jani was subjected to multiple medical tests, and his only contact with any form of fluid was during bathing and gargling, with the fluid spat out measured by the doctors. The research team could not comment on his claim of having been able to survive in this way for decades.\n\nThe case has attracted criticism, both after the 2003 tests and the 2010 tests. Sanal Edamaruku, president of the Indian Rationalist Association, criticized the 2010 experiment for allowing Jani to move out of a certain CCTV camera's field of view, meet devotees and leave the sealed test room to sunbathe. Edamaruku stated that the regular gargling and bathing activities were not sufficiently monitored, and accused Jani of having had some \"influential protectors\" who denied Edamaruku permission to inspect the project during its operation.\n\nJasmuheen (born Ellen Greve) was a prominent advocate of breatharianism in the 1990s. She said \"I can go for months and months without having anything at all other than a cup of tea. My body runs on a different kind of nourishment.\" Interviewers found her house stocked with food; Jasmuheen claimed the food was for her husband and daughter. In 1999, she volunteered to be monitored closely by the Australian television program \"60 Minutes\" for one week without eating to demonstrate her methods. Jasmuheen stated that she found it difficult on the third day of the test because the hotel room in which she was confined was located near a busy road, causing stress and pollution that prevented absorption of required nutrients from the air. \"I asked for fresh air. Seventy percent of my nutrients come from fresh air. I couldn’t even breathe,\" she said. The third day the test was moved to a mountainside retreat where her condition continued to deteriorate. After Jasmuheen had fasted for four days, Berris Wink, president of the Queensland branch of the Australian Medical Association, urged her to stop the test.\n\nAccording to Wink, Jasmuheen’s pupils were dilated, her speech was slow, and she was \"quite dehydrated, probably over 10%, getting up to 11%\". Towards the end of the test, she said, \"Her pulse is about double what it was when she started. The risks if she goes any further are kidney failure. \"60 Minutes\" would be culpable if they encouraged her to continue. She should stop now.\" The test was stopped. Wink said, \"Unfortunately there are a few people who may believe what she says, and I'm sure it's only a few, but I think it's quite irresponsible for somebody to be trying to encourage others to do something that is so detrimental to their health.\" Jasmuheen challenged the results of the program, saying, \"Look, 6,000 people have done this around the world without any problem.\"\n\nJasmuheen was awarded the Bent Spoon Award by Australian Skeptics in 2000 (\"presented to the perpetrator of the most preposterous piece of paranormal or pseudoscientific piffle\"). She also won the 2000 Ig Nobel Prize for Literature for \"Living on Light\". Jasmuheen claims that their beliefs are based on the writings and \"more recent channelled material\" from St. Germain. She stated that some people's DNA has expanded from 2 to 12 strands, to \"absorb more hydrogen\". When offered $30,000 to prove her claim with a blood test, she said that she didn't understand the relevance as she was not referring to herself.\n\nIn the documentary \"No Way to Heaven\" the Swiss chemist Michael Werner claims to have followed the directions appearing on Jasmuheen's books, living for several years without food. The documentary also describes two attempts at scientific verification of his claims. , five deaths had been directly linked to breatharianism as a result of Jasmuheen's publications. Jasmuheen has denied any responsibility for the deaths.\n\nWiley Brooks is the founder of the Breatharian Institute of America. He was first introduced to the public in 1980 when he appeared on the TV show \"That's Incredible!\". Brooks stopped teaching recently to \"devote 100% of his time on solving the problem as to why he needed to eat some type of food to keep his physical body alive and allow his light body to manifest completely.\" Brooks claims to have found \"four major deterrents\" which prevented him from living without food: \"people pollution\", \"food pollution\", \"air pollution\" and \"electro pollution\".\n\nIn 1983 he was reportedly observed leaving a Santa Cruz 7-Eleven with a Slurpee, a hot dog, and Twinkies. He told \"Colors\" magazine in 2003 that he periodically breaks his fasting with a cheeseburger and a cola, explaining that when he's surrounded by junk culture and junk food, consuming them adds balance.\n\nWiley Brooks later claimed that Diet Coke and McDonald's cheeseburgers have special \"5D\" properties. The idea of separate but interconnected 5D and 3D worlds is a major part of Wiley Brooks' ideology, and Wiley Brooks encourages his followers to only eat these special 5D foods, as well as meditate on a set of magical 5D words.\n\nBrooks's institute has charged varying fees to prospective clients who wished to learn how to live without food, which have ranged from US$100,000 with an initial deposit of $10,000 to one billion dollars, to be paid via bank wire transfer with a preliminary deposit of $100,000, for a session called \"Immortality workshop\". A payment plan was also offered. These charges have typically been presented as limited time offers exclusively for billionaires.\n\nHira Ratan Manek (born 12 September 1937) claims that since 18 June 1995 he has lived on water and occasionally tea, coffee, and buttermilk. Manek states that Sungazing is the key to his health citing yogis, ancient Egyptians, Aztecs, Mayans and Native Americans as practitioners of the art. While he and his proponents state that medical experts have confirmed his ability to draw sustenance by gazing at the sun, he was caught on camera eating a big meal in a San Francisco restaurant in the 2011 documentary \"Eat the Sun\".\n\nIn a television documentary produced by the Israeli television investigative show \"The Real Face\" (פנים אמיתיות) hosted by Amnon Levy, Israeli practitioner of Inedia Ray Maor (ריי מאור) who appeared to survive without food or water for eight days and eight nights. According to the documentary, he was restricted to a small villa and placed under constant video surveillance, with medical supervision that included daily blood testing. The documentary claimed Maor was in good spirits throughout the experiment, lost 17 lb after eight days, blood tests showed no change before, during or after the experiment, and Cardiologist Ilan Kitsis from Tel Aviv Sourasky Medical Center was \"baffled.\"\n\n\nHindu religious texts contain account of saints and hermits practicing what would be called inedia, breatharianism or Sustenance through Light in modern terms. In Valmiki's Ramayana, Book III, Canto VI, an account of anchorites and holy men is given, who flocked around Rama when he came to Śarabhanga's hermitage. These included, among others, the \"...saints who live on rays which moon and daystar give\" and \"those ... whose food the wave of air supplies\". In Canto XI of the same book a hermit named Māṇḍakarṇi is mentioned: \"For he, great votarist, intent – On strictest rule his stern life spent – ... – Ten thousand years on air he fed...\" (English quotations are from Ralph T. H. Griffith's translation).\n\nParamahansa Yogananda's \"Autobiography of a Yogi\" details two alleged historical examples of breatharianism, Hari Giri Bala and Therese Neumann.\n\nThere are claims that Devraha Baba lived without food.\n\n\n\n"}
{"id": "11336178", "url": "https://en.wikipedia.org/wiki?curid=11336178", "title": "Knudsen cell", "text": "Knudsen cell\n\nIn crystal growth, a Knudsen cell is an effusion evaporator source for relatively low partial pressure elementary sources (e.g. Ga, Al, Hg, As). Because it is easy to control the temperature of the evaporating material in Knudsen cells, they are commonly used in molecular-beam epitaxy.\n\nThe Knudsen effusion cell was developed by Martin Knudsen (1871-1949). A typical Knudsen cell contains a crucible (made of pyrolytic boron nitride, quartz, tungsten or graphite), heating filaments (often made of metal tantalum), water cooling system, heat shields, and an orifice shutter.\n\nThe Knudsen cell is used to measure the vapor pressures of a solid with very low vapor pressure. Such a solid forms a vapor at low pressure by sublimation. The vapor slowly effuses through the pinhole, and the loss of mass is proportional to the vapor pressure and can be used to determine this pressure. The heat of sublimation can also be determined by measuring the vapor pressure as a function of temperature, using the Clausius–Clapeyron relation.\n"}
{"id": "44389561", "url": "https://en.wikipedia.org/wiki?curid=44389561", "title": "LEAP Science and Maths Schools", "text": "LEAP Science and Maths Schools\n\nLanga Education Assistance Program (LEAP), also known as LEAP Science and Maths Schools, is a collection of six non-fee payment secondary education schools located in three provinces in South Africa. The first LEAP school opened in 2004 in rented premises in Observatory, Cape Town and served mainly, learners from the township of Langa. As an independent school, LEAP is mainly founded by South African Corporates with limited subsidies from the Department of Basic Education.\n\nJohn Gilmour was a teacher at Pinelands High School in Cape Town in 1987 when he decided to respond to a call from individuals within the South African business community, to contribute to the redress of the devastation of the Bantu Education Act, a segregation law imposed in the education sector by the Apartheid system in 1953.\n\n“Africa Week” was then introduced by a team led by John Gilmour to bring black learners under the “Bantu Education” system to spend a week at Pinelands High School, which was then a white’s only school. The program became the precursor for the Langa Education Assistance Program (or LEAP) which aimed at providing one hundred black students from the Langa township, with support tuition from Pinelands High teachers in English, Maths and Science, three afternoons a week.\n\nThe prohibitive transport cost of bringing learners from the township schools to Pinelands High School forced the model to be revised. In 1996, it was then decided that instead of learners being bussed in to Pinelands High, teachers will be transported to meet learners in township schools.\n\nCommunity members in Langa perceived the new model of LEAP as an attempt of white teachers to “save” black children. Teachers from the community felt that they were as able as other teachers to provide extra lessons that would address the inadequacies in the students' educational foundations. The uneasiness within the community obliged the leadership of LEAP to change the model and include community teachers in the program.\n\nWhen John Gilmour realised that despite best efforts of LEAP and similar programs in the country there was no increase in the proportion of black learners entering university, especially science-based disciplines, he sought for an alternative model. LEAP Science and Maths school was the alternative model.\n\nLEAP Science and Maths school was a deliberate decision of John Gilmour seeking to increase the number of black learners who take science and maths-based modules at high school in order to increase the chance of being accepted at University, particular in disciplines where these modules are a prerequisite.\n\nJohn resigned as Headmaster of Abbot’s College in 2004, where he has been since he left Pinelands in 1997, to focus on LEAP Science and Maths School. In January 2004, the first LEAP Science and Maths School opened its doors in the suburb of Observatory, Cape Town, with seventy-two learners, seven teachers and one administrative staff member.\n\nThe LEAP Science and Maths school, which started in Observatory later moved to Pinelands and become known as LEAP 1. It serves learners from community of Langa Township. It is the first and oldest of the LEAP Science and Maths schools. The school is headed by its Operation Leader Oscar Dlodlo.\n\nLEAP 2 opened in 2007 also in Pinelands. It serves learners from the townships of Gugulethu and Crossroads. In 2013, the school opened satellite classes to serve grade 9 and 10 learners from the township of Delft. Learners of the school are taught in English and isiXhosa. The head of the school is Mr. Babele Emedi, who also assumes the role of Operations Leader.\n\nThe school opened its doors in 2008 to serve learners from the township of Alexandra in Johannesburg. Learners are taught in English, isiZulu, Sepedi and Sesotho under the leadership of the Operations leader, Mr. Mark Fletcher.\n\nThis school is situated in the township of Diepsloot just outside Johannesburg. The school opened in 2011 through a partnership with the South Africa corporate Aveng Group. LEAP 4 teaches learners in isiZulu and Sepedi as home languages, along with English. Paul Mumba, is the Operations Leader and head of LEAP 4.\n\nIn 2012, LEAP 5 opened its doors in Jane Furse, Sekhukhune in Limpopo province. The school is under the leadership of its Operation Leader, Raphael Mukachi.\n\nLEAP 6 is situated in Ga-Rankuwa near Pretoria. The school opened in 2012 to respond to the request of the Anglican Diocese of Pretoria, concerned with the community’s poor educational results and high unemployment rate, particularly among young people. The school is led by its Operation Leader, Lawrence Manyesa.\n\n“We strive for the positive transformation of communities through the meaningful education of children from those communities”\n\n“LEAP schools provide student-centered, math’s and science-focused education to economically disadvantaged students from grades 8 to 12. We focus on enabling the self-awareness necessary for each student’s growth to healthy adulthood, and to ensuring optimal academic results which will allow for choices for lifelong learning and a fulfilling future.”\n\nAlthough LEAP schools focus on Maths and Sciences; it puts a lot of emphasis on the emotional development of learners. John Gilmour acknowledged the importance of the emotional development in a country where “eight million children come from single-parent households and a further 4.3 million reside with neither biological parent”. In his article “A tale of two systems”, he speaks of LEAP’s response to this problem “has been to recognise that social transformation starts with personal transformation: one person at a time, one room at a time, one school at a time and one community at a time.”\n\nThe school cherishes the following values:\n\nIn a report that analyses the performance of the school between 2005 and 2013, LEAP reported that:\n\n\nThe school claims to outperform national statistics when it compares itself with national averages of learners who write maths and science in 2013. All LEAP learners wrote these modules compared to only 43% who took maths and 33% who took physical science nationally.\n\nBonisanani Mtshekisa is LEAP’s first university graduate. He graduated with a bachelor in Bachelor of Commerce in Finance from the University of the Western Cape. In its 2012 annual report, the school reported to have over 30 university graduates ranging from physiotherapist, engineer to accountants.\n\nIn 2014, LEAP launched a series of celebratory adverts to showcase its work and celebrate the achievements of its past students. Qondisa Nxganga, a Master’s graduate from the prestigious African Institute for Mathematical Sciences (AIMS), is featured in these adverts alongside a number of other LEAP alumni.\n\nLEAP’s model is based on strong engagement with the broader community and building beneficial partnerships and collaboration with other organizations, particular schools. It is a three-way collaboration approach where each LEAP school partners with a more privileged school as well as a less privileged school (a township school) in the community where it operates to promote a culture of shared resources and cultural exchange\n\nLEAP has also been successful in building strong partnerships with both commercial and non-profit organizations. Most of its funding comes from South African corporates, trusts and Foundations.\n\nLEAP, is Teach with Africa’s partner on the continent and together they have established the Teacher Institute which seeks to develop teachers and supports individuals wanting to enter the teaching profession\n\n"}
{"id": "18576196", "url": "https://en.wikipedia.org/wiki?curid=18576196", "title": "Lariat chain", "text": "Lariat chain\n\nA Lariat chain is a loop of chain that hangs off, and is spun by a wheel. It is often used as a science exhibit or a toy. \n\nThe original Lariat Chain was created in 1986 by Norman Tuck, as an Artist-in-Residence project at the Exploratorium in San Francisco. \n\nLariat Chain was developed from an earlier Tuck piece entitled Chain Reaction (1984). Chain Reaction was hand cranked, and utilized a heavy chain attached by magnets onto an iron flywheel. As in Lariat Chain, Chain Reaction used a brush to disrupt the motion of the traveling chain. \n\nThe speed of the chain is arranged to equal the wave speed of transverse waves, so that waves moving against the motion of the chain appear to be standing still.\n\n\n"}
{"id": "4352963", "url": "https://en.wikipedia.org/wiki?curid=4352963", "title": "List of Balzan Prize recipients", "text": "List of Balzan Prize recipients\n\nThis is a list of recipients of the Balzan Prize, one of the world's most prestigious academic awards.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2056943", "url": "https://en.wikipedia.org/wiki?curid=2056943", "title": "List of IARC Group 3 carcinogens", "text": "List of IARC Group 3 carcinogens\n\nSubstances, mixtures and exposure circumstances in this list have been classified by the International Agency for Research on Cancer (IARC) as \"Group 3: The agent (mixture or exposure circumstance) is not classifiable as to its carcinogenicity to humans.\" This category is used most commonly for agents, mixtures and exposure circumstances for which the evidence of carcinogenicity is inadequate in humans and inadequate or limited in experimental animals. Exceptionally, agents (mixtures) for which the evidence of carcinogenicity is inadequate in humans but sufficient in experimental animals may be placed in this category when there is strong evidence that the mechanism of carcinogenicity in experimental animals does not operate in humans. Agents, mixtures and exposure circumstances that do not fall into any other group are also placed in this category.\n\nFurther details can be found in the IARC Monographs\n\n\n"}
{"id": "12494249", "url": "https://en.wikipedia.org/wiki?curid=12494249", "title": "List of vineyard soil types", "text": "List of vineyard soil types\n\nThe soil composition of vineyards is one of the most important viticultural considerations when planting grape vines. The soil supports the root structure of the vine and influences the drainage levels and amount of minerals and nutrients that the vine is exposed to. The ideal soil condition for a vine is a layer of thin topsoil and subsoil that sufficiently retains water but also has good drainage so that the roots do not become overly saturated. The ability of the soil to retain heat and/or reflect it back up to the vine is also an important consideration that affects the ripening of the grapes.\n\nThere are several minerals that are vital to the health of vines that all good vineyard soils have. These include calcium which helps to neutralize the Soil pH levels, iron which is essential for photosynthesis, magnesium which is an important component of chlorophyll, nitrogen which is assimilated in the form of nitrates, phosphates which encourages root development, and potassium which improves the vine metabolisms and increases its health for next year's crop.\n\n\"Unless otherwise noted the primary reference for this list is Sotheby's Wine Encyclopedia 2005\"\n\n\n\n\n"}
{"id": "41885308", "url": "https://en.wikipedia.org/wiki?curid=41885308", "title": "Living educational theory", "text": "Living educational theory\n\nLiving educational theory (LET) is a research method in educational research.\n\nThe idea of action research as a living practice entered the mainstream of action research from the book, \"Action Research as a Living Practice\" by Terrance Carson and Dennis Sumara in 1997. Carson and Sumara transformed the concept of traditional action research with the idea that, ...\" participation in action research practices are particular ways of living and understanding that require more of the researcher than the \"application\" of research methods. Rather, action research is a lived practice that requires that the researcher not only investigate the subject at hand but, as well, provide some account of the way in which the investigation both shapes and is shaped by the investigator . This requires what Martin Buber called an \"I-Thou\" approach toward other and this approach applied to action research as well. To make Buber's language more modern and accessible, LET translated Buber's \"I-Thou\" approach toward another human being to an \"I/you/we\" approach to action research. This differs greatly from an approach to living theory action research imagined by Jack Whitehead (2002) where he imagines living theory action research as forming an \"I-theory\" of knowledge. Director of the Philosophy for Children Project at Notre Dame de Namur University William Barry proposes LET focuses on the connections between the researcher and the other person or subject where the lives of action researchers are inextricable linked in a profound manner with the individuals and communities involved in the subject of study. LET from a Barryian perspective is a critical theory and emancipatory action research approach which seeks the dialectic, not debate and battles of [discourse].\n\nA major difference of William Barry's version of living educational theory, which was the focus of his successful completion of a Ph.D. thesis at Nottingham Trent University, UK, is the essential question behind the living educational theory approach to action research (2012b). The question is not \"How can I generate a living legacy for myself through an I-It theory approach toward knowledge and other forms of life?\" Rather the essential question is, \"How does one conduct a life that includes the practice of educational action research?\" The theory/practice problem disappears when honesty about one's biases regarding spiritual, existential, and emotional intelligence are made clear in the action research process.\n\nThe phraseology \"educational theory\" originated with the work of Jack Whitehead, a former lecturer at the University of Bath, and it was further developed and greatly improved methodologically by Jean McNiff in 2009 because of her willingness to be transparent about her values and intentions. Whitehead's main emphasis for conducting research is to promote the individual under the guise of collaboration and research outcomes must be captured on video for authentic validation. Whitehead's view of action research promotes that living educational theory (he uses living theory and living educational theory interchangeably so it is difficult for a reader to know what he is writing about) should be aimed at the bringing of energy-flowing values as explanatory principles and standards of judgments into the Academy for the legitimation of living educational theories (Whitehead 2008). In the eyes of Whitehead radical constructivism is at the core of living educational theory research. In 2013, Whitehead and McNiff separated as collaborators as McNiff saw spiritual, emotional intelligence as key to action research while Whitehead disagreed and believed that media accounts (primarily video tapping people) of action research could provide clues to virtues which held the future of humanity though energy flowing examples of collaboration. McNiff stated in a May 2013 conference she San Francisco, California (ARNA Conference) that she would never appear or work with Whitehead again. She repeated this message again months later at a UK conference in York. American William Barry believed the concept of LET was too important and found a dialectic between McNiff and Whitehead and he created a new understanding of LET which was presented at a three-day international conference in 2013 at Liverpool Hope University titled, \"Researching Our Own Practice\" .\n\nLiving educational theory was first clearly defined and developed by California Professor of Philosophy William Barry (2012b) in Liz Atkins and Susan Wallace's book , \"Qualitative Research in Education\", co-published by Sage and the British Educational Research Association (BERA). This book was one of four in a series sponsored by the BERA regarding best practice progressive research methods in educational research. The originality and uniqueness of Barry's development of living educational theory (LET) action research is the importance of gaining \"ontological weight\" through the action research process. Ontological weight empowers the researcher's ability, and the ability of other people involved in the action research project, to have the research experience and focus of the research be transformational and add, or at least reinforce, a sense of meaning in learning and life. Barry was influenced to use the concept \"ontological weight\" by the existentialist Catholic philosopher Gabriel Marcel (1963).\n\nThe idea of action research as a living practice entered the mainstream of action research from the book, \"Action Research as a Living Practice\" by Terrance Carson and Dennis Sumara in 1997. The term \"educational theory\" originated with the work of Jack Whitehead, a former lecturer at the University of Bath, and it was further developed and greatly improved methodologically by Jean McNiff in 2009. Whitehead's main emphasis for conducting research is to promote the individual under the guise of collaboration and research outcomes must be captured on video for authentic validation. Whitehead's view of action research promotes that living educational theory (he uses living theory and living educational theory interchangeably so it is difficult for a reader to know what he is writing about) should be aimed at the bringing of energy-flowing values as explanatory principles and standards of judgments into the Academy for the legitimation of living educational theories (Whitehead 2008). In the eyes of Whitehead, radical constructivism is at the core of living educational theory research.\n\nBarry was asked by the BERA sponsored authors to reflect on the nature of living educational theory (LET) because there existed no clear definition of LET in the literature. Barry was asked because he had successfully used LET in an innovative fashion, and was the first to clearly define LET as based in critical theory which embraced transpersonal psychology through his earned 2012 PhD thesis at Nottingham Trent University Nottingham, UK. He proposed LET as a way of challenging the oppressive use of power using critical theory in a need fulfilling way (Glasser 1998). Barry proposed the following definition and approach to action research he calls living educational theory and his approach has been used as an action research method in undergraduate and graduate courses and research at Notre Dame de Namur University in Silicon Valley, California as well as by other researchers around the world.\n\nBarry explained that living educational theory \"[is] a critical and transformational approach to action research. It confronts the researcher to challenge the status quo of their educational practice and to answer the question, 'How can I improve that I'm doing?' Researchers who use this approach must be willing to recognize and assume responsibility for being a 'living contradiction' in their professional practice – thinking one way and acting in another. The mission of the LET action researcher is to overcome workplace norms and self – behavior which contradict the researcher's values and beliefs. The vision of the LET researcher is to make an original contribution to knowledge through generating an educational theory proven to improve the learning of people within a social learning space. The standard of judgment for theory validity is evidence of workplace reform, transformational growth of the researcher, and improved learning by the people researcher claimed to have influenced...\" .\n\nBarry's LET approach to action research was heavily influenced by action researchers focused on emancipatory social change, collaboration, and liberation theology (2012a). Prominent developers of LET, without whose work LET would most likely never had been developed by Barry, are notable action researchers, educators, and philosophers such as Martin Buber's (1970) conception of 'I and Thou' and Krishnamurti's (1953) liberation pedagogy emphasizing education as significant to leading a quality filled life; Paulo Freire (1998 and 1970) and his concept of participatory action research and the need to be politically aware; the work of Carr and Kremmis (1986) and Habermas (1992) and their concept and building critical educational knowledge; Professor Manheimer of UNC (1999) and his challenge to his readers to enfold the past into the living present in order to become historical to oneself and then strive to linking life times with each other; Apple (1982) and Michel Foucault (1990) and the role of power and politics in education and Joe Kincheloe (2008), Henry Giroux (1997), and Peter McLaren (1989) and their promotion of critical pedagogy.\n\nProf. Barry was the first Ph.D. researcher to successfully use living educational theory in conjunction with neuro-linguistic programming (NLP), spiral dynamics and autoethnography (based on a multiple intelligences model which includes spiritual and emotional intelligence and embraces transpersonal knowing) as valid methods of research working under the methodological umbrella of phenomenology and hermeneutics. His LET approach to Ph.D.level action research led to the unique use of fictional storytelling as a vehicle by which to replace the traditional literature review chapter in Ph.D. research but in a more rigorous and creative fashion. The process of storytelling allows the researcher to exercise their emotional intelligence in a superior manner than the traditional research literature review allows.\n\nLiving educational theory as defined and created by Barry is part of the curriculum of multiple courses at Notre Dame de Namur University located in Silicon Valley, California in their credentialing program for teacher education. Barry's Living Educational Theory Action Research Method is based on a six step process based on research questions that normally start from the format, \" How can I influence the transformation of...?\" or \"How can I contribute to the improvement of...?\" The research is dialectical in nature:\n\n\n\n\n"}
{"id": "435331", "url": "https://en.wikipedia.org/wiki?curid=435331", "title": "Lysenkoism", "text": "Lysenkoism\n\nLysenkoism () was a political campaign conducted by Trofim Lysenko, his followers and Soviet authorities against genetics and science-based agriculture. Lysenko served as the director of the Soviet Union's Lenin All-Union Academy of Agricultural Sciences. Lysenkoism began in the late 1920s and formally ended in 1964.\n\nThe pseudo-scientific ideas of Lysenkoism assumed the heritability of acquired characteristics (Lamarckism). Lysenko's theory rejected Mendelian inheritance and the concept of the \"gene\"; it departed from Darwinian evolutionary theory by rejecting natural selection. Proponents falsely claimed to have discovered, among many other things, that rye could transform into wheat and wheat into barley, that weeds could spontaneously transmute into food grains, and that \"natural cooperation\" was observed in nature as opposed to \"natural selection\". Lysenkoism promised extraordinary advances in breeding and in agriculture that never came about.\n\nJoseph Stalin supported the campaign. More than 3,000 mainstream biologists were fired or even sent to prison, and numerous scientists were executed as part of a campaign instigated by Lysenko to suppress his scientific opponents. The president of the Agriculture Academy, Nikolai Vavilov, was sent to prison and died there, while scientific research in the field of genetics was effectively destroyed until the death of Stalin in 1953. Research and teaching in the fields of neurophysiology, cell biology, and many other biological disciplines was also negatively affected or banned.\n\nIn modern usage, the term lysenkoism has become distinct from normal pseudoscience. Where pseudoscience pretends to be science, lysenkoism aims at attacking the legitimacy of science itself, usually for political reasons. It is the rejection of the universality of scientific truth, and the purposeful defamation of the scientific method to the level of politics. \n\nIn 1928, Trofim Lysenko, a previously unknown agronomist, claimed to have developed an agricultural technique, termed vernalization, which tripled or quadrupled crop yield by exposing wheat seed to high humidity and low temperature. While cold and moisture exposure are a normal part of the life cycle of autumn-seeded winter cereals, the vernalization technique claimed to increase yields by increasing the intensity of exposure, in some cases planting soaked seeds directly into the snow cover of frozen fields. In reality, the technique was neither new (it had been known since 1854, and was extensively studied during the previous twenty years), nor did it produce the yields he promised, although some increase in production did occur.\n\nWhen Lysenko began his fieldwork in the Soviet Union of the 1930s, the agriculture of the Soviet Union was in a massive crisis due to the forced collectivisation of farms, and the extermination of the kulaks. The resulting famine provoked the people and the government alike to search for any possible solution to the critical lack of food. Lysenko's vernalization practices yielded marginally greater food production on the farms, and he was quickly accepted as the hero of Soviet agriculture.\n\nMany agronomists were educated before the revolution, and even many of those educated afterwards did not agree with the forced collectivization policies. Furthermore, among biologists of the day, the most popular topic was not agriculture at all, but the new genetics that was emerging out of studies of \"Drosophila melanogaster\", commonly known as fruit flies. Drosophilid flies made experimental verification of genetics theories, such as Mendelian ratios and heritability, much easier.\n\nIsaak Izrailevich Prezent, a main Lysenko theorist, presented Lysenko in Soviet mass-media as a genius who had developed a new, revolutionary agricultural technique. In this period, Soviet propaganda often focused on inspirational stories of peasants who, through their own canny ability and intelligence, came up with solutions to practical problems. Lysenko's widespread popularity provided him a platform to denounce theoretical genetics and to promote his own agricultural practices. He was, in turn, supported by the Soviet propaganda machine, which overstated his successes and omitted mention of his failures. This was accompanied by fake experimental data supporting Lysenkoism from scientists seeking favor and the destruction of counter-evidence to Lysenko's theories. Instead of performing controlled experiments, Lysenko claimed that vernalization increased wheat yields by 15%, solely based upon questionnaires taken of farmers.\n\nLysenko's political success was mostly due to his appeal to the Communist Party and Soviet ideology. Following the disastrous collectivization efforts of the late 1920s, Lysenko's \"new\" methods were seen by Soviet officials as paving the way to an \"agricultural revolution.\" Lysenko himself was from a peasant family, and was an enthusiastic advocate of Leninism. During a period which saw a series of man-made agricultural disasters, he was also extremely fast in responding to problems, although not with real solutions. Whenever the Party announced plans to plant a new crop or cultivate a new area, Lysenko had immediate practical suggestions on how to proceed.\n\nSo quickly did he develop his prescriptions—from the cold treatment of grain, to the plucking of leaves from cotton plants, to the cluster planting of trees, to unusual fertilizer mixes—that academic biologists did not have time to demonstrate that one technique was valueless or harmful before a new one was adopted. The Party-controlled newspapers applauded Lysenko's \"practical\" efforts and questioned the motives of his critics. Lysenko's \"revolution in agriculture\" had a powerful propaganda advantage over the academics, who urged the patience and observation required for science.\n\nLysenko was admitted into the hierarchy of the Communist Party, and was put in charge of agricultural affairs. He used his position to denounce biologists as \"fly-lovers and people haters\", and to decry the \"wreckers\" in biology, who he claimed were trying to purposely disable the Soviet economy and cause it to fail. Furthermore, he denied the distinction between theoretical and applied biology.\n\nLysenko presented himself as a follower of Ivan Vladimirovich Michurin, a well-known and well-liked Soviet horticulturist. However, unlike Michurin, he advocated a form of Lamarckism, insisting on using only hybridization and grafting, as non-genetic techniques. With this came, most importantly, the implication that \"acquired\" characteristics of an organism—for example, the state of being leafless as a result of having been plucked—could be inherited by that organism's descendants. This is why Lysenko claimed vernalization would give greater productivity than it did; he believed the ability of his vernalized seeds to flower faster and produce more wheat would be passed on to the next generation of wheat seeds, thus causing vernalization to further amplify the process.\n\nSupport from Joseph Stalin gave Lysenko even more momentum and popularity. In 1935, Lysenko compared his opponents in biology to the peasants who still resisted the Soviet government's collectivization strategy, saying that by opposing his theories the traditional geneticists were setting themselves against Marxism. Stalin was in the audience when this speech was made, and he was the first one to stand and applaud, calling out \"Bravo, Comrade Lysenko. Bravo.\" This event emboldened Lysenko and gave him and his ally Prezent free rein to slander the geneticists who still spoke out against him. Many of Lysenkoism's opponents, such as his former mentor Nikolai Ivanovich Vavilov, were imprisoned or even executed because of Lysenko's and Prezent's denunciations.\n\nOn August 7, 1948, the V.I. Lenin Academy of Agricultural Sciences announced that from that point on Lysenkoism would be taught as \"the only correct theory\". Soviet scientists were forced to denounce any work that contradicted Lysenko. Criticism of Lysenko was denounced as \"bourgeois\" or \"fascist\", and analogous \"non-bourgeois\" theories also flourished in other fields in the Soviet academy at this time (see Japhetic theory; socialist realism). Perhaps the only opponents of Lysenkoism during Stalin's lifetime to escape liquidation were from the small community of Soviet nuclear physicists: as Tony Judt has observed, \"It is significant that Stalin left his nuclear physicists alone and never presumed to second guess \"their\" calculations. Stalin may well have been mad but he was not stupid.\"\n\nMany other countries of the Eastern Bloc accepted Lysenkoism as the official \"new biology\" as well; however the acceptance of Lysenkoism was not uniform in communist countries. In Poland, all geneticists except for Wacław Gajewski followed Lysenkoism. Even though Gajewski was not allowed contact with students, he was allowed to continue his scientific work at the Warsaw botanical garden. Lysenkoism was then rapidly rejected starting from 1956 and modern genetics research departments were formed, including the first department of genetics headed by Wacław Gajewski, which was started at the Warsaw University in 1958.\n\nCzechoslovakia adopted Lysenkoism in 1949. Jaroslav Kříženecký (1896–1964) was one of the prominent Czechoslovak geneticists opposing Lysenkoism, and when he criticized Lysenkoism in his lectures, he was dismissed from the Agricultural University in 1949 for \"serving the established capitalistic system, considering himself superior to the working class, and being hostile to the democratic order of the people\", and imprisoned in 1958. In 1963, he was appointed head of the newly established Gregor Mendel department in the Moravian Museum in Brno, the city in which Gregor Mendel pursued his early experiments on inheritance and formulated the laws of Mendelian inheritance.\n\nIn the German Democratic Republic, although Lysenkoism was taught at some of the universities, it had very little impact on science due to the actions of a few scientists (for example, the geneticist and fierce critic of Lysenkoism, Hans Stubbe) and an open border to West Berlin research institutions. Nonetheless, Lysenkoist theories were found in schoolbooks until the dismissal of Nikita Khrushchev in 1964.\n\nLysenkoism dominated Chinese science from 1949 until 1956, particularly during the Great Leap Forward, when, during a genetics symposium opponents of Lysenkoism were permitted to freely criticize it and argue for Mendelian genetics. In the proceedings from the symposium, Tan Jiazhen is quoted as saying \"Since [the] USSR started to criticize Lysenko, we have dared to criticize him too\". For a while, both schools were permitted to coexist, although the influence of the Lysenkoists remained large for several years.\n\nFrom 1934 to 1940, under Lysenko's admonitions and with Stalin's approval, many geneticists were executed (including Isaak Agol, Solomon Levit, Grigorii Levitskii, Georgii Karpechenko and Georgii Nadson) or sent to labor camps. The famous Soviet geneticist and president of the Agriculture Academy, Nikolai Vavilov, was arrested in 1940 and died in prison in 1943. Hermann Joseph Muller (and his teachings about genetics) was criticized as a bourgeois, capitalist, imperialist, and promoting fascism so he left the USSR, to return to the US via Republican Spain. In 1948, genetics was officially declared \"a bourgeois pseudoscience\"; all geneticists were fired from their jobs (some were also arrested), and all genetic research was discontinued.\n\nOver 3,000 biologists were imprisoned, fired, or executed for attempting to oppose Lysenkoism at one time and overall, scientific research in genetics was effectively destroyed until the death of Stalin in 1953. Due to Lysenkoism, crop yields in the USSR actually declined as well.\n\nAt the end of 1952, the situation started changing, possibly due to Stalin taking offense to Lysenko's growing influence. Articles criticizing Lysenkoism were published in newspapers. However, the process of return to regular genetics slowed down in Nikita Khrushchev's times, due to Lysenko showing him the supposed successes of an experimental agricultural complex. It once again became forbidden to criticize Lysenkoism, though it was now possible to express different views, and all geneticists were released or rehabilitated posthumously. The ban was only waived in the mid-1960s.\n\nAlmost alone among Western scientists, John Desmond Bernal, Professor of Physics at Birkbeck College, University of London, and a Fellow of the Royal Society, made an aggressive public defense of Lysenko and some years later gave an obituary of \"Stalin as a Scientist\". However, despite Bernal's endorsement, other members of Britain's scientific community retreated from open support of the Soviet Union.\n\nThe word \"Neo-Lysenkoism\" has occasionally been used by hereditarian researchers, such as Roger Pearson and Bernard Davis, as a pejorative term in the debates over race and intelligence and sociobiology to describe scientists supposedly minimizing the role of genes in shaping human behavior, such as Leon Kamin, Richard Lewontin, Stephen Jay Gould and Barry Mehler.\n\nThe use of the term \"neo-Lysenkoism\" to describe criticism of evolutionary psychology have drawn criticism for misapplying the term \"Lysenkoism\" to things unrelated to actual Lysenkoism. For instance, Lysenko's views on inheritance of acquired characteristics presumed that individuals were born with characteristics acquired by their parents, not tabula rasa. This was integrated into Stalin's purges by presuming that simply having biological parents who were capitalists constituted a risk factor for becoming a dissident due to inheritance of acquired capitalist interest even if the children had not been postnatally raised capitalist. This is argued to show that the definition of \"nature\" as \"anything before birth\" that is used by evolutionary psychologists when they classify ideas of hormones before birth on sexuality and empathy as \"biological\" would also classify Lysenkoism as \"biological\" due to all three being prenatalist but not Mendelian genetic, remarking that accusations of social engineering in \"nurture\" models can simply transform into accusations of prenatal environment engineering in \"womb environment\" models. The usage of the term \"neo-Lysenkoism\" for models of evolution that contradict the Darwinian model of gradual and maximized reproductive success is criticized for missing the point that while it is uncertain whether or not Darwin read about Mendel's genetic discoveries, it is clear that Darwin did not integrate Mendelian genetics into his model of evolution and that Mendel read The Origin of Species and accepted evolution but did not consider his discoveries a solution to any problems with Darwinian evolution, that Mendelian genetics was the primary subject of suppression of modern biology under Lysenkoism, and that Mendelian genetics is actually a source of modern criticism of the maximization of reproductive success model of evolution as Mendelian models show that short term maximized reproductive success of specific genes deplete genetic variation and increase the long term risk of extinction (explaining higher extinction risks in more sexually dimorphic species as a functional similarity between Bateman's principle and asexual reproduction) during environmental change which could not be predicted by premodern models of genetics such as those of Darwin and Lysenko alike. Contrary to the allegation of anti-psychiatry being neo-Lysenkoist, it is remarked that psychiatric medication was not suppressed in the Soviet Union under Lysenkoism, but the use of psychiatric drugs in the Soviet Union actually increased and was approved by the regime during the Lysenko era. It is cited that Communists considered chemical imbalance models of \"mental illnesses\" including but not limited to anti-communism in \"working class\" people in Communist countries more materialistic and in accordance with Marxist dialectical materialism than the idea of other cultural factors than class interest influencíng behavior that was considered idealist, and that this was used as an argument by officials in the Soviet Union for approving increased psychiatric medication.\n\n\n\n"}
{"id": "18912788", "url": "https://en.wikipedia.org/wiki?curid=18912788", "title": "Malawi Liverpool Wellcome Trust", "text": "Malawi Liverpool Wellcome Trust\n\nMLW was established in 1995 to conduct internationally excellent science to benefit human health with a focus on sub-Saharan Africa. MLW is built around excellent laboratories, strategically located in the largest hospital in Malawi, Queen Elizabeth Central Hospital. It is closely linked with the community and is an integral part of the College of Medicine. These relationships provide a unique opportunity replicated in few centres in Africa to study major health issues spanning both community and hospital.\n\nSince its inception, MLW has maintained partnership with the Liverpool School of Tropical Medicine, the University of Liverpool and the Wellcome Trust. MLW is based in Blantyre with field research sites around urban Blantyre, Thyolo, Chikhwawa, Zomba, Mangochi and Machinga.\n\n"}
{"id": "11817317", "url": "https://en.wikipedia.org/wiki?curid=11817317", "title": "Mechanical explanations of gravitation", "text": "Mechanical explanations of gravitation\n\nMechanical explanations of gravitation (or kinetic theories of gravitation) are attempts to explain the action of gravity by aid of basic mechanical processes, such as pressure forces caused by pushes, without the use of any action at a distance. These theories were developed from the 16th until the 19th century in connection with the aether. However, such models are no longer regarded as viable theories within the mainstream scientific community and general relativity is now the standard model to describe gravitation without the use of actions at a distance. Modern \"quantum gravity\" hypotheses also attempt to describe gravity by more fundamental processes such as particle fields, but they are not based on classical mechanics.\n\nThis theory is probably the best-known mechanical explanation, and was developed for the first time by Nicolas Fatio de Duillier in 1690, and re-invented, among others, by Georges-Louis Le Sage (1748), Lord Kelvin (1872), and Hendrik Lorentz (1900), and criticized by James Clerk Maxwell (1875), and Henri Poincaré (1908).\n\nThe theory posits that the force of gravity is the result of tiny particles or waves moving at high speed in all directions, throughout the universe. The intensity of the flux of particles is assumed to be the same in all directions, so an isolated object A is struck equally from all sides, resulting in only an inward-directed pressure but no net directional force. With a second object B present, however, a fraction of the particles that would otherwise have struck A from the direction of B is intercepted, so B works as a shield, so-to-speak—that is, from the direction of B, A will be struck by fewer particles than from the opposite direction. Likewise, B will be struck by fewer particles from the direction of A than from the opposite direction. One can say that A and B are \"shadowing\" each other, and the two bodies are pushed toward each other by the resulting imbalance of forces.\nThis shadow obeys the inverse square law, because the imbalance of momentum flow over an entire spherical surface enclosing the object is independent of the size of the enclosing sphere, whereas the surface area of the sphere increases in proportion to the square of the radius. To satisfy the need for mass proportionality, the theory posits that a) the basic elements of matter are very small so that gross matter consists mostly of empty space, and b) that the particles are so small, that only a small fraction of them would be intercepted by gross matter. The result is, that the \"shadow\" of each body is proportional to the surface of every single element of matter.\n\n\"Criticism\": This theory was declined primarily for thermodynamic reasons because a shadow only appears in this model if the particles or waves are at least partly absorbed, which should lead to an enormous heating of the bodies. Also drag, \"i.e.\" the resistance of the particle streams in the direction of motion, is a great problem too. This problem can be solved by assuming superluminal speeds, but this solution largely increases the thermal problems and contradicts special relativity.\n\nBecause of his philosophical beliefs, René Descartes proposed in 1644 that no empty space can exist and that space must consequently be filled with matter. The parts of this matter tend to move in straight paths, but because they lie close together, they can not move freely, which according to Descartes implies that every motion is circular, so the aether is filled with vortices. Descartes also distinguishes between different forms and sizes of matter in which rough matter resists the circular movement more strongly than fine matter. Due to centrifugal force, matter tends towards the outer edges of the vortex, which causes a condensation of this matter there. The rough matter cannot follow this movement due to its greater inertia—so due to the pressure of the condensed outer matter those parts will be pushed into the center of the vortex. According to Descartes, this inward pressure is nothing else than gravity. He compared this mechanism with the fact that if a rotating, liquid filled vessel is stopped, the liquid goes on to rotate. Now, if one drops small pieces of light matter (e.g. wood) into the vessel, the pieces move to the middle of the vessel.\n\nFollowing the basic premises of Descartes, Christiaan Huygens between 1669 and 1690 designed a much more exact vortex model. This model was the first theory of gravitation which was worked out mathematically. He assumed that the aether particles are moving in every direction, but were thrown back at the outer borders of the vortex and this causes (as in the case of Descartes) a greater concentration of fine matter at the outer borders. So also in his model the fine matter presses the rough matter into the center of the vortex. Huygens also found out that the centrifugal force is equal to the force, which acts in the direction of the center of the vortex (centripetal force). He also posited that bodies must consist mostly of empty space so that the aether can penetrate the bodies easily, which is necessary for mass proportionality. He further concluded that the aether moves much faster than the falling bodies. At this time, Newton developed his theory of gravitation which is based on attraction, and although Huygens agreed with the mathematical formalism, he said the model was insufficient due to the lack of a mechanical explanation of the force law. Newton's discovery that gravity obeys the inverse square law surprised Huygens and he tried to take this into account by assuming that the speed of the aether is smaller in greater distance.\n\n\"Criticism\": Newton objected to the theory because drag must lead to noticeable deviations of the orbits which were not observed. Another problem was that moons often move in different directions, against the direction of the vortex motion. Also, Huygens' explanation of the inverse square law is circular, because this means that the aether obeys Kepler's third law. But a theory of gravitation has to explain those laws and must not presuppose them.\n\nIn a 1675 letter to Henry Oldenburg, and later to Robert Boyle, Newton wrote the following: [Gravity is the result of] “a condensation causing a flow of ether with a corresponding thinning of the ether density associated with the increased velocity of flow.” He also asserted that such a process was consistent with all his other work and Kepler's Laws of Motion. Newtons' idea of a pressure drop associated with increased velocity of flow was mathematically formalised as Bernoulli's principle published in Daniel Bernoulli's book \"Hydrodynamica\" in 1738.\n\nHowever, although he later proposed a second explanation (see section below), Newton's comments to that question remained ambiguous. In the third letter to Bentley in 1692 he wrote:\n\nIt is inconceivable that inanimate brute matter should, without the mediation of something else which is not material, operate upon and affect other matter, without mutual contact, as it must do if gravitation in the sense of Epicurus be essential and inherent in it. And this is one reason why I desired you would not ascribe 'innate gravity' to me. That gravity should be innate, inherent, and essential to matter, so that one body may act upon another at a distance, through a vacuum, without the mediation of anything else, by and through which their action and force may be conveyed from one to another, is to me so great an absurdity, that I believe no man who has in philosophical matters a competent faculty of thinking can ever fall into it. Gravity must be caused by an agent acting constantly according to certain laws; but whether this agent be material or immaterial, I have left to the consideration of my readers.\n\nOn the other hand, Newton is also well known for the phrase Hypotheses non fingo, written in 1713:\n\nI have not as yet been able to discover the reason for these properties of gravity from phenomena, and I do not feign hypotheses. For whatever is not deduced from the phenomena must be called a hypothesis; and hypotheses, whether metaphysical or physical, or based on occult qualities, or mechanical, have no place in experimental philosophy. In this philosophy particular propositions are inferred from the phenomena, and afterwards rendered general by induction.\n\nAnd according to the testimony of some of his friends, such as Nicolas Fatio de Duillier or David Gregory, Newton thought that gravitation is based directly on divine influence.\n\nSimilar to Newton, but mathematically in greater detail, Bernhard Riemann assumed in 1853 that the gravitational aether is an incompressible fluid and normal matter represents sinks in this aether. So if the aether is destroyed or absorbed proportionally to the masses within the bodies, a stream arises and carries all surrounding bodies into the direction of the central mass. Riemann speculated that the absorbed aether is transferred into another world or dimension.\n\nAnother attempt to solve the energy problem was made by Ivan Osipovich Yarkovsky in 1888. Based on his aether stream model, which was similar to that of Riemann, he argued that the absorbed aether might be converted into new matter, leading to a mass increase of the celestial bodies.\n\n\"Criticism\": As in the case of Le Sage's theory, the disappearance of energy without explanation violates the energy conservation law. Also some drag must arise, and no process which leads to a creation of matter is known.\n\nNewton updated the second edition of \"Optics\" (1717) with another mechanical-ether theory of gravity. Unlike his first explanation (1675 - see Streams), he proposed a stationary aether which gets thinner and thinner nearby the celestial bodies. On the analogy of the lift, a force arises, which pushes all bodies to the central mass. He minimized drag by stating an extremely low density of the gravitational aether.\n\nLike Newton, Leonhard Euler presupposed in 1760 that the gravitational aether loses density in accordance with the inverse square law. Similarly to others, Euler also assumed that to maintain mass proportionality, matter consists mostly of empty space.\n\n\"Criticism\": Both Newton and Euler gave no reason why the density of that static aether should change. Furthermore, James Clerk Maxwell pointed out that in this \"hydrostatic\" model \"the state of stress... which we must suppose to exist in the invisible medium, is 3000 times greater than that which the strongest steel could support\".\n\nRobert Hooke speculated in 1671 that gravitation is the result of all bodies emitting waves in all directions through the aether. Other bodies, which interact with these waves, move in the direction of the source of the waves. Hooke saw an analogy to the fact that small objects on a disturbed surface of water move to the center of the disturbance.\n\nA similar theory was worked out mathematically by James Challis from 1859 to 1876. He calculated that the case of attraction occurs if the wavelength is large in comparison with the distance between the gravitating bodies. If the wavelength is small, the bodies repel each other. By a combination of these effects, he also tried to explain all other forces.\n\n\"Criticism\": Maxwell objected that this theory requires a steady production of waves, which must be accompanied by an infinite consumption of energy.\nChallis himself admitted, that he hadn't reached a definite result due to the complexity of the processes.\n\nLord Kelvin (1871) and Carl Anton Bjerknes (1871) assumed that all bodies pulsate in the aether. This was in analogy to the fact that, if the pulsation of two spheres in a fluid is in phase, they will attract each other; and if the pulsation of two spheres is \"not\" in phase, they will repel each other. This mechanism was also used for explaining the nature of electric charges. Among others, this hypothesis has also been examined by George Gabriel Stokes and Woldemar Voigt.\n\n\"Criticism\" : To explain universal gravitation, one is forced to assume that all pulsations in the universe are in phase—which appears very implausible. In addition, the aether should be incompressible to ensure that attraction also arises at greater distances. And Maxwell argued that this process must be accompanied by a permanent new production and destruction of aether.\n\nIn 1690, Pierre Varignon assumed that all bodies are exposed to pushes by aether particles from all directions, and that there is some sort of limitation at a certain distance from the Earth's surface which cannot be passed by the particles. He assumed that if a body is closer to the Earth than to the limitation boundary, then the body would experience a greater push from above than from below, causing it to fall toward the Earth.\n\nIn 1748, Mikhail Lomonosov assumed that the effect of the aether is proportional to the complete surface of the elementary components of which matter consists (similar to Huygens and Fatio before him). He also assumed an enormous penetrability of the bodies. However, no clear description was given by him as to how exactly the aether interacts with matter so that the law of gravitation arises.\n\nIn 1821, John Herapath tried to apply his co-developed model of the kinetic theory of gases on gravitation. He assumed that the aether is heated by the bodies and loses density so that other bodies are pushed to these regions of lower density.\nHowever, it was shown by Taylor that the decreased density due to thermal expansion is compensated for by the increased speed of the heated particles; therefore, no attraction arises.\n\nThese mechanical explanations for gravity never gained widespread acceptance, although such ideas continued to be studied occasionally by physicists until the beginning of the twentieth century, by which time it was generally considered to be conclusively discredited. However, some researchers outside the scientific mainstream still try to work out some consequences of those theories.\n\nLe Sage's theory was studied by Radzievskii and Kagalnikova (1960), Shneiderov (1961), Buonomano and Engels (1976), Adamut (1982), Jaakkola (1996), Tom Van Flandern (1999), and Edwards (2007). A variety of Le Sage models and related topics are discussed in Edwards, et al.\n\nGravity due to static pressure was recently studied by Arminjon.\n\n"}
{"id": "34976715", "url": "https://en.wikipedia.org/wiki?curid=34976715", "title": "Mien Shiang", "text": "Mien Shiang\n\nMien Shiang is a 3,000-year-old Taoist practice that means literally face (mien) reading (shiang). In just moments, one can supposedly determine anyone’s \"Wu Xing\" — Five Element personality type — their character, behavior, and health potential — by analyzing their face. The Taoist Five Elements, Wood, Fire, Earth, Metal and Water, are metaphors devised by the ancient Taoist philosophers to explain the relationship, interaction, and ongoing change of everything in the Universe. In recent times the art of Face Reading is becoming more and more popular. Schools that teach Mien Shiang are becoming more wide spread.\n"}
{"id": "13182486", "url": "https://en.wikipedia.org/wiki?curid=13182486", "title": "Molecular Spectra and Molecular Structure IV. Constants of Diatomic Molecules", "text": "Molecular Spectra and Molecular Structure IV. Constants of Diatomic Molecules\n\nMolecular Spectra and Molecular Structure IV. Constants of Diatomic Molecules, by K. P. Huber and Gerhard Herzberg (Van nostrand Reinhold company, New York, 1979, ), is a classic comprehensive multidisciplinary reference text contains a critical compilation of available data for all diatomic molecules and ions known at the time of publication - over 900 diatomic species in all - including electronic energies, vibrational and rotational constants, and observed transitions. Extensive footnotes discuss the reliability of these data and additional detailed informationon potential energy curves, spin-coupling constants, /\\-type doubling, perturbations between electronic states, hyperfine structure, rotational g factors, dipole moments, radiative lifetimes, oscillator strengths, dissociation energies and ionization potentials when available, and other aspects. Herzberg received the 1971 Nobel Prize in Chemistry; both authors are world-renowned highly respected scientists.\n\nPhysics, engineering, mathematics, kinetics, spectroscopy, astronomy, astrophysics, aeronautics, astronautics, radiation, optics, energy, photometry, spectrometry, electromagnetics, oscillators, thermochemistry, thermodynamics, ionization, X-rays, ESR, photoelectrons, electronics, industry, science and technology.\n"}
{"id": "24861242", "url": "https://en.wikipedia.org/wiki?curid=24861242", "title": "My Science Career", "text": "My Science Career\n\nThe My Science Career website is an Irish online resource for career information in science, technology, engineering and mathematics (STEM).\n\nThe website has a famous Irish scientists section, science related articles, a science career glossary and a video interviews section with scientists about their work. “A day in the life” section looks at the everyday working loves of Irish scientists and science broadcasters, ranging from a professor of biochemistry to a marine photographer.\n\nA “Science Ambassadors” section profiles Irish scientists on what it’s like working in various fields and the qualifications they have. The Science Ambassadors range from newly qualified graduates to well established researchers.\n\nMyScienceCareer.ie is an initiative of Ireland’s national integrated awareness programme Discover Science & Engineering (DSE), a government initiative. DSE runs numerous other initiatives, including Science.ie, Science Week Ireland and Discover Primary Science.\n\n"}
{"id": "166380", "url": "https://en.wikipedia.org/wiki?curid=166380", "title": "Natural history", "text": "Natural history\n\nNatural history is a domain of inquiry involving organisms including animals, fungi and plants in their environment; leaning more towards observational than experimental methods of study. A person who studies natural history is called a naturalist or natural historian.\n\nNatural history encompasses scientific research but is not limited to it. It involves the systematic study of any category of natural objects or organisms. So while it dates from studies in the ancient Greco-Roman world and the mediaeval Arabic world, through to European Renaissance naturalists working in near isolation, today's natural history is a cross discipline umbrella of many specialty sciences; e.g., geobiology has a strong multi-disciplinary nature.\n\nThe meaning of the English term \"natural history\" (a calque of the Latin \"historia naturalis\") has narrowed progressively with time; while, by contrast, the meaning of the related term \"nature\" has widened (see also History below).\n\nIn antiquity, \"natural history\" covered essentially anything connected with nature, or which used materials drawn from nature, such as Pliny the Elder's encyclopedia of this title, published circa 77 to 79 AD, which covers astronomy, geography, humans and their technology, medicine, and superstition, as well as animals and plants.\n\nMedieval European academics considered knowledge to have two main divisions: the humanities (primarily what is now known as classics) and divinity, with science studied largely through texts rather than observation or experiment. The study of nature revived in the Renaissance, and quickly became a third branch of academic knowledge, itself divided into descriptive natural history and natural philosophy, the analytical study of nature. In modern terms, natural philosophy roughly corresponded to modern physics and chemistry, while natural history included the biological and geological sciences. The two were strongly associated. During the heyday of the gentleman scientists, many people contributed to both fields, and early papers in both were commonly read at professional science society meetings such as the Royal Society and the French Academy of Sciences – both founded during the seventeenth century.\n\nNatural history had been encouraged by practical motives, such as Linnaeus' aspiration to improve the economic condition of Sweden. Similarly, the Industrial Revolution prompted the development of geology to help find useful mineral deposits.\n\nModern definitions of natural history come from a variety of fields and sources, and many of the modern definitions emphasize a particular aspect of the field, creating a plurality of definitions with a number of common themes among them. For example, while natural history is most often defined as a type of observation and a subject of study, it can also be defined as a body of knowledge, and as a craft or a practice, in which the emphasis is placed more on the observer than on the observed.\n\nDefinitions from biologists often focus on the scientific study of individual organisms in their environment, as seen in this definition by Marston Bates: \"Natural history is the study of animals and Plants – of organisms. ... I like to think, then, of natural history as the study of life at the level of the individual – of what plants and animals do, how they react to each other and their environment, how they are organized into larger groupings like populations and communities\" and this more recent definition by D.S. Wilcove and T. Eisner: \"The close observation of organisms—their origins, their evolution, their behavior, and their relationships with other species\".\n\nThis focus on organisms in their environment is also echoed by H.W. Greene and J.B. Losos: \"Natural history focuses on where organisms are and what they do in their environment, including interactions with other organisms. It encompasses changes in internal states insofar as they pertain to what organisms do\".\n\nSome definitions go further, focusing on direct observation of organisms in their environment, both past and present, such as this one by G.A. Bartholomew: \"A student of natural history, or a naturalist, studies the world by observing plants and animals directly. Because organisms are functionally inseparable from the environment in which they live and because their structure and function cannot be adequately interpreted without knowing some of their evolutionary history, the study of natural history embraces the study of fossils as well as physiographic and other aspects of the physical environment\".\n\nA common thread in many definitions of natural history is the inclusion of a descriptive component, as seen in a recent definition by H.W. Greene: \"Descriptive ecology and ethology\". Several authors have argued for a more expansive view of natural history, including S. Herman, who defines the field as \"the scientific study of plants and animals in their natural environments. It is concerned with levels of organization from the individual organism to the ecosystem, and stresses identification, life history, distribution, abundance, and inter-relationships.\n\nIt often and appropriately includes an esthetic component\", and T. Fleischner, who defines the field even more broadly, as \"A practice of intentional, focused attentiveness and receptivity to the more-than-human world, guided by honesty and accuracy\". These definitions explicitly include the arts in the field of natural history, and are aligned with the broad definition outlined by B. Lopez, who defines the field as the \"Patient interrogation of a landscape\" while referring to the natural history knowledge of the Eskimo (Inuit).\n\nA slightly different framework for natural history, covering a similar range of themes, is also implied in the scope of work encompassed by many leading natural history museums, which often include elements of anthropology, geology, paleontology and astronomy along with botany and zoology, or include both cultural and natural components of the world.\n\nThe plurality of definitions for this field has been recognized as both a weakness and a strength, and a range of definitions have recently been offered by practitioners in a recent collection of views on natural history.\n\nNatural history begins with Aristotle and other ancient philosophers who analyzed the diversity of the natural world. Natural history was understood by Pliny the Elder to cover anything that could be found in the world, including living things, geology, astronomy, technology, art and humanity.\n\n\"De Materia Medica\" was written between 50 and 70 AD by Pedanius Dioscorides, a Roman physician of Greek origin. It was widely read for more than 1,500 years until supplanted in the Renaissance, making it one of the longest-lasting of all natural history books.\n\nFrom the ancient Greeks until the work of Carl Linnaeus and other 18th century naturalists, a major concept of natural history was the \"scala naturae\" or Great Chain of Being, an arrangement of minerals, vegetables, more primitive forms of animals, and more complex life forms on a linear scale of supposedly increasing perfection, culminating in our species.\n\nNatural history was basically static through the Middle Ages in Europe – although in the Arabic and Oriental world it proceeded at a much brisker pace. From the thirteenth century, the work of Aristotle was adapted rather rigidly into Christian philosophy, particularly by Thomas Aquinas, forming the basis for natural theology. During the Renaissance, scholars (herbalists and humanists, particularly) returned to direct observation of plants and animals for natural history, and many began to accumulate large collections of exotic specimens and unusual monsters. Leonhart Fuchs was one of the three founding fathers of botany, along with Otto Brunfels and Hieronymus Bock. Other important contributors to the field were Valerius Cordus, Konrad Gesner (\"Historiae animalium\"), Frederik Ruysch, or Gaspard Bauhin. The rapid increase in the number of known organisms prompted many attempts at classifying and organizing species into taxonomic groups, culminating in the system of the Swedish naturalist Carl Linnaeus.\n\nA significant contribution to English natural history was made by parson-naturalists such as Gilbert White, William Kirby, John George Wood, and John Ray, who wrote about plants, animals, and other aspects of nature. Many of these men wrote about nature to make the natural theology argument for the existence or goodness of God.\n\nIn modern Europe, professional disciplines such as botany, geology, mycology, palaeontology, physiology and zoology were formed. \"Natural history\", formerly the main subject taught by college science professors, was increasingly scorned by scientists of a more specialized manner and relegated to an \"amateur\" activity, rather than a part of science proper. In Victorian Scotland it was believed that the study of natural history contributed to good mental health. Particularly in Britain and the United States, this grew into specialist hobbies such as the study of birds, butterflies, seashells (malacology/conchology), beetles and wildflowers; meanwhile, scientists tried to define a unified discipline of biology (though with only partial success, at least until the modern evolutionary synthesis). Still, the traditions of natural history continue to play a part in the study of biology, especially ecology (the study of natural systems involving living organisms and the inorganic components of the Earth's biosphere that support them), ethology (the scientific study of animal behavior), and evolutionary biology (the study of the relationships between life-forms over very long periods of time), and re-emerges today as integrative organismal biology.\n\nAmateur collectors and natural history entrepreneurs played an important role in building the world's large natural history collections, such as the Natural History Museum, London, and the National Museum of Natural History in Washington D.C.\n\nThree of the greatest English naturalists of the nineteenth century, Henry Walter Bates, Charles Darwin, and Alfred Russel Wallace—who all knew each other—each made natural history travels that took years, collected thousands of specimens, many of them new to science, and by their writings both advanced knowledge of \"remote\" parts of the world—the Amazon basin, the Galápagos Islands, and the Malay archipelago, among others—and in so doing helped to transform biology from a descriptive to a theory based science.\n\nThe understanding of \"Nature\" as \"an organism and not as a mechanism\" can be traced to the writings of Alexander von Humboldt (Prussia, 1769–1859). Humboldt's copious writings and research were seminal influences for Charles Darwin, Simón Bolívar, Henry David Thoreau, Ernst Haeckel, and John Muir.\n\nNatural history museums, which evolved from cabinets of curiosities, played an important role in the emergence of professional biological disciplines and research programs. Particularly in the 19th century, scientists began to use their natural history collections as teaching tools for advanced students and the basis for their own morphological research.\n\nThe term \"natural history\" alone, or sometimes together with archaeology, forms the name of many national, regional and local natural history societies that maintain records for animals (including birds (ornithology), insects (entomology) and mammals (mammalogy)), fungi (mycology), plants (botany) and other organisms. They may also have geological and microscopical sections.\n\nExamples of these societies in Britain include the Natural History Society of Northumbria founded in 1829, London Natural History Society (1858), Birmingham Natural History Society (1859), British Entomological and Natural History Society founded in 1872, Glasgow Natural History Society, Manchester Microscopical and Natural History Society established in 1880, Whitby Naturalists' Club founded in 1913, Scarborough Field Naturalists' Society and the Sorby Natural History Society, Sheffield, founded in 1918. The growth of natural history societies was also spurred due to the growth of British colonies in tropical regions with numerous new species to be discovered. Many civil servants took an interest in their new surroundings, sending specimens back to museums in Britain. (See also: Indian natural history)\n\nSocieties in other countries include the American Society of Naturalists and Polish Copernicus Society of Naturalists.\n\n"}
{"id": "3265837", "url": "https://en.wikipedia.org/wiki?curid=3265837", "title": "Noise-equivalent target", "text": "Noise-equivalent target\n\nIn detection systems, the noise-equivalent target (NET) is the intensity of a target measured by a system when the signal-to-noise ratio of the system is 1.\n\nNoise-equivalent temperature is an example for noise-equivalent target.\n"}
{"id": "22649", "url": "https://en.wikipedia.org/wiki?curid=22649", "title": "Observation", "text": "Observation\n\nObservation is the active acquisition of information from a primary source. In living beings, observation employs the senses. In science, observation can also involve the recording of data via the use of scientific instruments. The term may also refer to any data collected during the scientific activity. Observations can be qualitative, that is, only the absence or presence of a property is noted, or quantitative if a numerical value is attached to the observed phenomenon by counting or measuring.\n\nThe scientific method requires observations of natural phenomena to formulate and test hypotheses. It consists of the following steps:\n\nObservations play a role in the second and fifth steps of the scientific method. However, the need for reproducibility requires that observations by different observers can be comparable. Human sense impressions are subjective and qualitative, making them difficult to record or compare. The use of measurement developed to allow recording and comparison of observations made at different times and places, by different people. Measurement consists of using observation to compare the phenomenon being observed to a standard unit. The standard unit can be an artifact, process, or definition which can be duplicated or shared by all observers. In measurement the number of standard units which is equal to the observation is counted. Measurement reduces an observation to a number which can be recorded, and two observations which result in the same number are equal within the resolution of the process.\n\nHuman senses are limited and subject to errors in perception, such as optical illusions. Scientific instruments were developed to aid human abilities of observation, such as weighing scales, clocks, telescopes, microscopes, thermometers, cameras, and tape recorders, and also translate into perceptible form events that are unobservable by the senses, such as indicator dyes, voltmeters, spectrometers, infrared cameras, oscilloscopes, interferometers, geiger counters, and radio receivers.\n\nOne problem encountered throughout scientific fields is that the observation may affect the process being observed, resulting in a different outcome than if the process was unobserved. This is called the \"observer effect\". For example, it is not normally possible to check the air pressure in an automobile tire without letting out some of the air, thereby changing the pressure. However, in most fields of science it is possible to reduce the effects of observation to insignificance by using better instruments.\n\nConsidered as a physical process itself, all forms of observation (human or instrumental) involve amplification and are thus thermodynamically irreversible processes, increasing entropy.\n\nIn some specific fields of science the results of observation differ depending on factors which are not important in everyday observation. These are usually illustrated with \"paradoxes\" in which an event appears different when observed from two different points of view, seeming to violate \"common sense\".\n\nThe human senses do not function like a video camcorder, impartially recording all observations. Human perception occurs by a complex, unconscious process of abstraction, in which certain details of the incoming sense data are noticed and remembered, and the rest forgotten. What is kept and what is thrown away depends on an internal model or representation of the world, called by psychologists a \"schema\", that is built up over our entire lives. The data is fitted into this schema. Later when events are remembered, memory gaps may even be filled by \"plausible\" data the mind makes up to fit the model; this is called \"reconstructive memory\". How much attention the various perceived data are given depends on an internal value system, which judges how important it is to the individual. Thus two people can view the same event and come away with entirely different perceptions of it, even disagreeing about simple facts. This is why eyewitness testimony is notoriously unreliable.\n\nSeveral of the more important ways observations can be affected by human psychology are given below.\n\nHuman observations are biased toward confirming the observer's conscious and unconscious expectations and view of the world; we \"see what we expect to see\". In psychology, this is called confirmation bias. Since the object of scientific research is the discovery of new phenomena, this bias can and has caused new discoveries to be overlooked; one example is the discovery of x-rays. It can also result in erroneous scientific support for widely held cultural myths, on the other hand, as in the scientific racism that supported ideas of racial superiority in the early 20th century. Correct scientific technique emphasizes careful recording of observations, separating experimental observations from the conclusions drawn from them, and techniques such as blind or double blind experiments, to minimize observational bias.\n\nAnother bias, which has become more prevalent with the advent of \"big science\" and the large rewards of new discoveries, is bias in favor of the researcher's desired hypothesis or outcome; we \"see what we want to see\". Called pathological science and cargo cult science, this is different from deliberate falsification of results, and can happen to good-faith researchers. Researchers with a great incentive or desire for a given outcome can misinterpret or misjudge results, or even persuade themselves they have seen something they haven't. Possible examples of mistaken discoveries caused by this bias are Martian \"canals\", N rays, polywater, cold fusion, and perpetual motion machines. Recent decades have seen scientific scandals caused by researchers playing \"fast and loose\" with observational methods in order to get their pet theories published. This type of bias is rampant in pseudoscience, where correct scientific techniques are not followed. The main defense against this bias, besides correct research techniques, is peer review and repetition of the experiment, or the observation, by other researchers with no incentive to bias. For example, an emerging practice in the competitive field of biotechnology is to require the physical results of experiments, such as serums and tissue cultures, be made available to competing laboratories for independent testing.\n\nModern scientific instruments can extensively process \"observations\" before they are presented to the human senses, and particularly with computerized instruments, there is sometimes a question as to where in the data processing chain \"observing\" ends and \"drawing conclusions\" begins. This has recently become an issue with digitally enhanced images published as experimental data in papers in scientific journals. The images are enhanced to bring out features that the researcher wants to emphasize, but this also has the effect of supporting the researcher's conclusions. This is a form of bias that is difficult to quantify. Some scientific journals have begun to set detailed standards for what types of image processing are allowed in research results. Computerized instruments often keep a copy of the \"raw data\" from sensors before processing, which is the ultimate defense against processing bias, and similarly scientific standards require preservation of the original unenhanced \"raw\" versions of images used as research data.\n\nAn observational bias occurs when researchers only look where they think they will find positive results, or where it is easy to record observations. This is called the \"streetlight effect\".\n\nObservation in philosophical terms is the process of filtering sensory information through the thought process. Input is received via hearing, sight, smell, taste, or touch and then analyzed through either rational or irrational thought.\nFor example, let us suppose that an observer \"sees\" a parent beat their child; and consequently \"observes\" that such an action is either good or bad. Deductions about what behaviors are good or bad may be based on preferences about building relationships, or study of the consequences resulting from the observed behavior. With the passage of time, impressions stored in the consciousness about many, together with the resulting relationships and consequences, permit the individual to build a construct about the moral implications of behavior.\n"}
{"id": "48177158", "url": "https://en.wikipedia.org/wiki?curid=48177158", "title": "Physician-scientist", "text": "Physician-scientist\n\nA physician-scientist is a holder of a degree in medicine and science who invests significant time and professional effort in scientific research and spends correspondingly less time in direct clinical practice compared to other physicians. Physician-scientists are often employed by academic or research institutions and may focus their clinical practices on very specialized patient populations, such as those with rare genetic diseases or rare cancers. Although they are a minority of both practicing physicians and active research scientists, physician-scientists are often cited as playing a critical role in translational medicine and clinical research, connecting biomedical research findings to health care applications. The United States National Institutes of Health includes holders of other clinical degrees - such as nurses, dentists, and veterinarians - in its studies of the physician-scientist workforce (PSW).\n\nPhysician-scientists by definition hold degrees in medicine; they may also hold additional graduate degrees such as a Ph.D. In the United States and Canada, some universities run specialized dual degree MD-PhD programs, and a small number of D.O.-granting institutions also offer dual degree options as D.O.-Ph.D. In the United States the NIH supports competitive university programs called Medical Scientist Training Programs that aim to train physician-scientists, originally established in 1964 and present at 45 institutions as of 2015. Similar programs were established in the United Kingdom in the 1980s, although with relatively less funding support. Although this dual-degree pathway is not necessary to establish a physician-scientist career, most do receive some form of explicit research training in addition to their clinical education.\n\nMost physician-scientists are employed by universities and medical schools, or by research institutions such as the National Institutes of Health. As of 2014, the NIH counted around 9,000 NIH-funded physician-scientists; this count does not include those whose work is funded by sources other than the NIH - typically meaning those who work in industry, such as at pharmaceutical companies or medical device companies.\n\nAt many medical schools, physician-scientist faculty are expected to obtain significant fractions of their nominal salary in the form of competitive research grants, which are also requirements for the award of tenure. This \"up or out\" system has been described as developed for a primarily male workforce with homemaker wives, incompatible with the work-life balance needs of the current workforce. Uncertainty about stable careers in academic medicine and the long initial training phase are often cited as concerns by aspiring entrants to the field. Data from the NIH on physician-scientist grant awardees suggests that women and minorities are often underrepresented in the population, even in fields like veterinary science where the majority of students are women.\n\nThe American Physician Scientists Association (APSA) is a professional association dedicated to physician-scientists, founded in 2003. The American Society for Clinical Investigation introduced Young Physician-Scientist Awards in 2013 to support productive early-career researchers.\n\nThe concept of the physician-scientist is often attributed to Samuel Meltzer's work in the early 1900s. Concern has often been displayed at declining interest or participation in the field, with James Wyngaarden - who would later go on to become the director of the NIH - describing physician-scientists as an \"endangered species\" in 1979. Among U.S. biomedical researchers, physician-scientists have declined over time as a share of the total researcher population, although their absolute numbers have been stable since the 1970s.\n"}
{"id": "4576518", "url": "https://en.wikipedia.org/wiki?curid=4576518", "title": "Plant collecting", "text": "Plant collecting\n\nPlant collecting is the acquisition of plant specimens for the purposes of research, cultivation, or as a hobby. Plant specimens may be kept alive, but are more commonly dried and pressed to preserve the quality of the specimen. Plant collecting is an ancient practice with records of a Chinese botanist collecting roses over 5000 years ago.\n\nHerbaria are collections of preserved plants samples and their associated data for scientific purposes. The largest herbarium in the world exist at the Muséum National d'Histoire Naturelle, in Paris, France. Plant samples in herbaria typically include a reference sheet with information about the plant and details of collection. This detailed and organized system of filing provides horticulturist and other researchers alike with a way to find information about a certain plant, and a way to add new information to an existing plant sample file.\n\nThe collection of live plant specimens from the wild, sometimes referred to as plant hunting, is an activity that has occurred for centuries. The earliest recorded evidence of plant hunting was in 1495 BC when botanists were sent to Somalia to collect incense trees for Queen Hatshepsut. The Victorian era saw a surge in plant hunting activity as botanical adventurers explored the world to find exotic plants to bring home, often at considerable personal risk. These plants usually ended up in botanical gardens or the private gardens of wealthy collectors. Prolific plant hunters in this period included William Lobb and his brother Thomas Lobb, George Forrest, Joseph Hooker, Charles Maries and Robert Fortune.\nProper pressing and mounting techniques are critical to the longevity of a collected plant sample. Properly preserved plant samples can last for hundreds of years. The New York Botanical Garden itself holds plant samples that date back to the Lewis and Clark expedition of 1804–1806.\n\nThe first step of plant pressing begins with the collection of the sample. When collecting a sample it is important to first make sure that land you are collecting on allows for the removal of natural specimens. The next step after finding a suitable plant for collection is to assign it with a number for record keeping purposes. This number system is up to the individual collector, but usually involves the date of collection or the sequential number of that collection. Along with an assigned number, observations about the plant's location and live appearance should be noted in detail. These field notes will accompany the finished sample to provide supplementary information about the plant.\n\nAfter cleaning the sample of any mud or dirt, the next step is to press the sample. Some samples may press better if they have been left to wilt for a few days. However plants should never be allowed to spoil or decompose before pressing, as this will impact the quality of the dried product. Plant presses are most commonly constructed with two flat smooth pieces of wood, and some type of compression mechanism. Compression may be accomplished with tightened nuts and bolts on the corners of the press, with tightened straps around the press, or by placing weights on top of the press. When placing in the press, plant samples should be sandwiched between a few layers of absorbent blotter. Newspaper and cardboard are two common choices for blotter material. When arranging the plant in the press it is important to remember that the dried plant product will be fragile and inflexible, so position the plant exactly as you wish the final product to appear. Tighten the press and wait approximately a day to check up on the plant. The blotter should be taken out and replaced with dry blotter roughly every 24 hours. Complete drying time will vary depending on the type of plant, but is generally 7–10 days. Fleshier plants such as succulents will take longer.\n\nAfter the plant sample has been sufficiently dried and pressed, it must be mounted. The quality of mounting not only impacts the appearance of the plant sample, but also determines the rate of deterioration the sample will experience. Herbarium quality mounts use specialized paper for the best protection against deterioration. This paper can either be 100% alpha cellulose paper or cotton “rag” paper. These types of paper are ideal for preserving plant samples because they are acid free and pH neutral. Samples can be strapped to the paper with linen tape, or glued onto the sheet. If glue is needed, it is recommended that Grade A methyl cellulose mixed with water be used for optimal deterioration resistance.\n\nIn order for plant specimens to be kept in good condition for future reference, adequate storage conditions are needed. The storage space should be kept in a low light, low humidity environment. The temperature of the storage space should be kept cool, between 50 – 65 degrees Fahrenheit.\n\nIt is important to keep the storage space free of harmful pest. It is recommended to protect the specimens by sheathing the sheets in sealed plastic bags. Various pesticides may also be used to protect the storage space from pest infestation. If pest infestation has already occurred, the samples should be frozen for three to four days. Freezing new additions of plant samples is a suggested preventative measure against the introduction of pest to the storage space.\n\nHerbarium specimens of plants are collected for a number of different uses. They can assist in accurate identification and provide a species record for a time and place that can be used in distribution maps. They can also provide biological material for researchers, a reference point to document scientific names and vouchers for research and seed collections. DNA barcoding, a new method of identification of plant vouchers, is being used in herbaria across the world. The Smithsonian National Museum of Natural History creates their barcodes from a short sequence of plant DNA, which can be easily identified from all healthy specimens of the species. This barcode is then printed and placed onto the plant mount. By creating these DNA barcodes, the process of organizing and loaning plant specimens becomes more streamlined and can be mechanized.\n\nVoucher specimens are select herbarium specimens. What distinguishes these specimens from others is that a voucher specimen is a \"representative sample of an expertly identified organism.\" These specimens are usually associated with a professional research article and are considered to be more official references than a typical herbarium specimen. Voucher specimens can be useful in many ways such as use for comparison when scientists think they have found a new species or when dichotomous keys have narrowed the possible species down to a few that have minute differences.\n\nPlant collecting may also refer to a hobby, in which the hobbyist takes identifiable samples of plant species found in nature, dries them, and stores them in a paper sheet album, a simple herbarium, along with the information of the finding location, finding date, etc. necessary scientific information. As in many collecting hobbies, rarer specimens have been valued. However, when collecting living organisms, the conservation aspects must precede the collector's ambitions. This has led in some cases to a collector voluntarily taking part, helping scientists, in some research areas, provided he can store the \"collectible\". In fact, historically, many species have initially been found within a collection of a collector.\n\nUsually, a plant can be identified in nature, since they are stationary.\nThe advent of digital cameras has led many plant collectors to switch totally to photography. Some have switched to collecting live specimens of various plant species in their gardens, building a sort of \"private botanical garden\". Some have specialized in a specific group, the orchids and the roses and their cultivars are among the most collected.\n\nIllegal collection of plants is known as plant poaching. A report on the risk of rare plant poaching has provided data showing possible connections between geography and the rate of poaching in the Shenandoah National Park, Virginia, USA. The openings for poaching were found to be increased in locations with easy accessibility, such as roads, trails, and developed areas. The condition of the environment can determine the levels of poaching, with regions of higher quality receiving more attention from poachers.\n\nThe hobby and practice of plant collecting is known to have been the cause of declines in certain plant populations. This can be the result of hobbyists being oblivious to the status of a particular species, collectors of valuable species for profit, or researchers over collecting to fill slots in herbaria. This issue can be solved with proper research on the status of species before a plant is collected and taking the smallest sample possible.\n\nWhile plant collecting may seem like a very safe and harmless practice, there is a few things collectors should keep in mind to protect themselves. First collectors should always be aware of the land where they are collecting. As in hiking there will be certain limitations to whether or not public access is granted on a plot of land and if collection from that land is allowed. For example, in a National park of the United States, plant collection is not allowed unless given special permission. Collecting internationally will involve some logistics, such as official permits which will most likely be required to bring plants both from the country of collection and to the destination country. The major herbaria can be useful to the average hobbyist in aiding them in acquiring these permits.\n\nIf traveling to a remote location to access samples, it is safe practice to inform someone of your whereabouts and planned time of return. If traveling in hot weather, collectors should bring adequate water to avoid dehydration. Forms of sun protection such as sunscreen and wide brimmed hats may be essential depending on location. Travel to remote locations will most likely involve walking measurable distances in wild terrain, so precautions synonymous with those related to hiking should be taken.\n\nPlant \"discovery\" means the first time that a new plant was recorded for science, often in the form of dried and pressed plants (a herbarium specimen) being sent to a botanical establishment such as Kew Gardens in London, where it would be examined, classified and named.\n\nPlant \"introduction\" means the first time that living matter – seed, cuttings or a whole plant – was brought back to Europe. Thus, the Handkerchief tree (\"Davidia involucrata\") was discovered by Père David in 1869 but introduced to Britain by Ernest Wilson in 1901.\n\nOften, the two happened simultaneously: thus Sir Joseph Hooker discovered and introduced his Himalayan rhododendrons between 1849 and 1851.\n\n\n"}
{"id": "16064155", "url": "https://en.wikipedia.org/wiki?curid=16064155", "title": "Project NEXUS", "text": "Project NEXUS\n\nProject NEXUS is a National Science Foundation (NSF) grant funded project based at the University of Maryland, College Park and headed by Principal Investigator Dr. J. Randy McGinnis. Its purpose is to investigate an innovative model of teacher preparation.\n\nOriginally a collaboration among University of Maryland, College Park (UM), (a large, primarily majority population university); Bowie State University (BSU), Maryland (a smaller, historically black university); and Hands On Science Outreach, Inc. (a national organization developing and administering informal science education classes for children), it has evolved into a collaborative with Coppin State University (CSU), (Maryland) and available informal science education internships. The award was made in 2005 and ran until 2012.\n\nThe key assumption was that science educational practices require systemic reform within the undergraduate science subject matter and education classes, prospective teachers’ field-based experiences, and professional development during new teachers induction years (NSF, 1993; NRC, 1997; Sunal, Wright, & Day, 2004.)\n\nProject NEXUS was supported by the National Science Foundation under Grant No. 0455752.\n"}
{"id": "22055657", "url": "https://en.wikipedia.org/wiki?curid=22055657", "title": "Psychological research", "text": "Psychological research\n\nPsychological research refers to research that psychologists conduct to systematic study and analyse of the experiences and behaviours of individuals or groups. Their research can have educational, occupational and clinical applications.\n\nPsychologists use many research methods, and categorical distinctions of these methods have emerged. Methods can be categorized by the kind of data they produce: qualitative or quantitative—and both these are used for pure or applied research.\n\nPsychology tends to be eclectic, applying knowledge from other fields. Some of its methods are used within other areas of research, especially in the social and behavioural sciences.\n\nThe field of psychology commonly uses experimental methods in what is known as experimental psychology. Researchers design experiments to test specific hypotheses (the deductive approach), or to evaluate functional relationships (the inductive approach).\n\nThe method of experimentation involves an experimenter changing some influence—the \"independent variable(IV)\"— on the research subjects, and studying the effects it produces on an expected aspect—the \"dependent variable (DV)\"— of the subjects behaviour or experience. Other variables researchers consider in experimentation are known as the \"extraneous variables\", and are either \"controllable\" or \"confounding\" (more than one variable at play).\n\nConfounding variables are external variables that are not taken into account when conducting an experiment. Because they are not controlled for, they can skew experiments results and provide a false or unreliable conclusion. For example, the psychologist Seymour Feshbach conducted an experiment to see how violence on television (the independent variable), affected aggression in adolescent boys (the dependent variable). He published his results in a paper called \"Television and Aggression\" in 1971. The paper showed that, in some cases, the lack of violence on television made the boys \"more\" aggressive. This was due to a confounding variable, which in this case was frustration. This means that extraneous variables are important to consider when designing experiments, and many methods have emerged to scientifically control them. For this reason, many experiments in psychology are conducted in laboratory conditions where they can be more strictly regulated.\n\nAlternatively, some experiments are less controlled. Quasi-experiment's are those that a researcher sets up in a controlled environment, but does not control the independent variable. For example, Michael R. Cunningham used a quasi-experiment to \"...measure the physical in physical attractiveness.\" On the other hand, in field experiments the experimenter controls an independent variable (making it the control variable), but does not control the environment where the experiment takes place. Experimenters sometimes apply fewer controls, as a way to lessen potential biases. In a \"true experiment\", participants are randomly chosen to remove the chance of experimenter's bias.\n\n\"Observational research,\" (a type of non-experimental, correlational research), involves the researcher observing the ongoing behaviour of their subjects. There are multiple methods of observational research such as \"participant observations\", \"non-participant observations\" and \"naturalistic observations\".\n\nParticipant observations are methods that involve a researcher joining the particular social group they are studying. For example, the social psychologist, Leon Festinger and his associates, joined a group called \"The Seekers\" in order to observe them. The Seekers believed they were in touch with aliens, and that the aliens had told them the world was about to end. When the foretasted event didn't happen, Festinger and his associates observed how the attitudes of the group members changed. They published their results in a 1956 book called \"When Prophecy Fails\". David Rosenhan in 1973 published a journal that involved research by participant observations.\" see: on being sane in insane places\".\n\nThe other method of observational research is non-participant observation. In particular naturalistic methods are methods that simply study behaviours that occur naturally in natural environments—with no manipulation by the observer. The events studied \"must\" be natural and not staged. This fact gives naturalistic observational research a high ecological validity. During naturalistic observations, researchers can avoid interfering with the behavior they are observing by using unobtrusive methods, if needed.\n\nBoth types of observational methods are designed to be as reliable as possible. Reliability can be estimated using \"inter-observer reliability\", that is, by comparing observations conducted by different researchers. \"Intra-observer reliability\" means estimating the reliability of an observation using a comparison of observations conducted by the same researcher. The reliability of conducted studies is important in any field of science.\n\n\"For a statistical perspective of reliability, see also\" Reliability (statistics).\n\nAll scientific processes begin with a description based on observation. Theories may develop later to explain these observations or classify associated phenomena. In scientific methodology, the conceptualizing of descriptive research precedes the hypotheses of \"explanatory research\".\n\nAn example of a descriptive device used in psychological research is the \"diary\", which is used to record observations. There is a history of use of diaries within clinical psychology. Examples of psychologists that used them include B.F. Skinner (1904–1990) and Virginia Axline (1911–1988). A special case of a diary in this context, that has particular importance in development psychology, is known as the \"baby biography\", and was used by psychologists such as Jean Piaget.\n\nOther recording methods can include video or audio. For example, forensic psychologists record custodial interrogations to aid law enforcement.\n\nA \"case study\"—or \"case report\"—is an intensive analysis of a person, group, or event that stresses developmental factors related to the context. Case studies may be descriptive or explanatory. Explanatory case studies explore causation to identify underlying principles. However, there is a debate to whether case studies count as a scientific research method. Clinical psychologists use case studies most often, especially to describe abnormal events and conditions, which are particularly important in clinical research. Sigmund Freud made extensive use of case studies to formulate his theory of psychoanalysis.\n\nFamous case studies include: Anna O. and Rat Man of Freud's Genie, who is one of the most severe cases of social isolation ever recorded, and Washoe, a chimpanzee who was the first non-human that had learned to communicate using American Sign Language.\n\nInterviews and questionnaires intrude as a foreign element into the\nsocial setting they would describe, they create as well as measure atti-\ntudes, they elicit atypical role and response, they are limited to those\nwho are accessible and who will cooperate, and the responses obtained\nare produced in part by dimensions of individual differences irrelevant\nto the topic at hand.\n\nBradburn et al. (1979) found a tendency for survey respondents to over-\nreport socially desirable behaviours when interviewed using less anonymous methods.\n\nPsychometrics is a field of study concerned with the theory and technique of psychological measurement. One part of the field is concerned with the objective measurement of skills and knowledge, abilities, attitudes, personality traits, and educational achievement.\n\nArchival research can be defined as the study of existing data. The existing data is collected to answer research questions. Existing data sources may include statistical records, survey archives, and written records.\n\nCross-sectional research is a research method often used in developmental psychology, but also utilized in many other areas including social science and education. This type of study utilizes different groups of people who differ in the variable of interest, but share other characteristics such as socioeconomic status, educational background, and ethnicity.\n\nFor example, researchers studying developmental psychology might select groups of people who are remarkably similar in most areas, but differ only in age.\n\nLongitudinal research is a type of research method used to discover relationships between variables that are not related to various background variables. This observational research technique involves studying the same group of individuals over an extended period of time.\n\nData is first collected at the outset of the study, and may then be gathered repeatedly throughout the length of the study. In some cases, longitudinal studies can last several decades.\n\nCross-cultural psychology is a branch of psychology that looks at how cultural factors influence human behavior.\n\nEssentially, cohort refers to people who are approximately the same age. When researchers conduct different types of studies (for example, developmental/cross sectional studies), they use cohorts to see how people of different ages compare on some topic at one point in time. For example, a researcher may compare the effects of a new study aid in three different cohorts: 10th graders, 11th graders, and 12th graders. In this way, you can examine the study aid across three different grade levels.\n\nA discipline lying on the border between artificial intelligence and psychology. It is concerned with building computer models of human cognitive processes and is based on an analogy between the human mind and computer programs. The brain and computer are viewed as general-purpose symbol-manipulation systems, capable of supporting software processes, but no analogy is drawn at a hardware level.\n\nThe term \"unobtrusive measures\" was first coined by Eugene Webb, Campbell, Schwartz, and Sechrest in a 1966 book, \"Unobtrusive methods: Nonreactive research in the social science\", in which they described methods that don't involve direct induction of data from research subjects. For example, the evidence people leave behind as they traverse their physical environment is unobtrusive. Unobtrusive methods get around biases, such as the selection bias and the experimenter's bias, that result from the researcher and his intrusion. Consequently, however, these methods reduce the researcher's control over the type of data collected.\n\nWeb and others regard these methods as an additional tools to use with the more common \"reactive\" and \"intrusive methods\".\n\n\n"}
{"id": "26762483", "url": "https://en.wikipedia.org/wiki?curid=26762483", "title": "Science 2.0", "text": "Science 2.0\n\nScience 2.0 is a suggested new approach to science that uses information-sharing and collaboration made possible by network technologies. It is similar to the open research and open science movements and is inspired by Web 2.0 technologies. Science 2.0 stresses the benefits of increased collaboration between scientists. Science 2.0 uses collaborative tools like wikis, blogs and video journals to share findings, raw data and \"nascent theories\" online. Science 2.0 benefits from openness and sharing, regarding papers and research ideas and partial solutions.\n\nA general view is that Science 2.0 is gaining traction with websites beginning to proliferate, yet at the same time there is considerable resistance within the scientific community about aspects of the transition as well as discussion about what, exactly, the term means. There are several views that there is a \"sea change\" happening in the status quo of scientific publishing, and substantive change regarding how scientists share research data. There is considerable discussion in the scientific community about whether scientists should embrace the model and exactly how Science 2.0 might work, as well as several reports that many scientists are slow to embrace collaborative methods and are somewhat \"inhibited and slow to adopt a lot of online tools.\"\n\nThe term has many meanings and continues to evolve in scientific parlance. It not only describes what is currently happening in science, but describes a direction in which proponents believe science should move towards, as well as a growing number of websites which promote free scientific collaboration.\nThe term \"Science 2.0\" suggests a contrast between traditional ways of doing science, often denoted \"Science 1.0\", with more collaborative approaches, and suggests that the new forms of science will work with Web 2.0 technologies. One description from \"Science\" is that Science 2.0 uses the \"networking power of the internet to tackle problems with multiple interacting variables - the problems, in other words, of everyday life.\" A different and somewhat controversial view is that of Ben Shneiderman, who suggested that Science 2.0 combines hypothesis-based inquiries with social science methods, partially for the purpose of improving those new networks.\n\nWhile the term describes websites for sharing scientific knowledge, it also includes efforts by existing science publishers to embrace new digital tools, such as offering areas for discussions following published online articles. Sometimes it denotes open access which, according to one view, means that the author continues to hold the copyright but that others can read it and use it for reasonable purposes, provided that the attribution is maintained. Most online scientific literature is behind paywalls, meaning that a person can find the title of an article on Google but they can not read the actual article. People who can access these articles are generally affiliated with a university or secondary school or library or other educational institution, or who pay on a per-article or subscription basis.\n\nOne view is that \"Science 2.0\" should include an effort by scientists to offer papers in non-technical language, as a way of reaching out to non-scientists. For others, it includes building vast databases of case histories. There is a sense in which \"Science 2.0\" indicates a general direction for scientific collaboration, although there is little clarity about how exactly this might happen. One aim is to \"make scientific collaboration as easy as sharing videos of trips home from the dentist,\" according to one view.\n\nClosely related terms are \"cyberscience\" focussing on scientists communicating in the cyberspace and \"cyberscience 2.0\" expanding the notion to the emerging trend of academics using Web 2.0 tools.\n\nThe rise of the Internet has transformed many activities such as retailing and information searching. In journalism, Internet technologies such as blogging, tagging and social networking have caused many existing media sources such as newspapers to \"adopt whole new ways of thinking and operating,\" according to a report in \"Scientific American\" in 2008. The idea is that while the Internet has transformed many aspects of life, it has not changed scientific research as much as it could. While firms such as eBay, Amazon and Netflix have changed consumer retailing, and online patient-centered medical data has enabled better health care, Science 2.0 advocate Ben Shneiderman said:\n\nAccording to one view, a similar web-inspired transformation that has happened to other areas is now happening to science. The general view is that science has been slower than other areas to embrace the web technology, but that it is beginning to catch up.\nBefore the Internet, scientific publishing has been described as a \"highly integrated and controlled process.\" Research was done in private. Next, it was submitted to scientific publications and reviewed by editors and gatekeepers and other scientists. Last, it was published. This has been the traditional pathway of scientific advancement, sometimes dubbed \"Science 1.0\".\n\nEstablished journals provided a \"critical service\", according to one view. Publications such as \"Science\" and \"Nature\" have large editorial staffs to manage the peer-review process as well as have hired fact-checkers and screeners to look over submissions. These publications get revenue from subscriptions, including online ones, as well as advertising revenue and fees paid by authors. According to advocates of Science 2.0, however, this process of paper-submission and review was rather long. Detractors complained that the system is \"hidebound, expensive and elitist\", sometimes \"reductionist\", as well as being slow and \"prohibitively costly\". Only a select group of gatekeepers—those in charge of the traditional publications—limited the flow of information. Proponents of open science claimed that scientists could learn more and learn faster if there is a \"friction-free collaboration over the Internet.\"\n\nYet there is considerable resistance within the scientific community to a change of approach. The act of publishing a new finding in a major journal has been at the \"heart of the career of scientists,\" according to one view, which elaborated that many scientists would be reluctant to sacrifice the \"emotional reward\" of having their discoveries published in the usual, traditional way. Established scientists are often loath to switch to an open-source model, according to one view.\n\nTimo Hannay explained that the traditional publish-a-paper model, sometimes described as \"Science 1.0\", was a workable one but there need to be other ways for scientists to make contributions and get credit for their work:\n\nIn 2008, a scientist at the University of Maryland named Ben Shneiderman wrote an editorial entitled \"Science 2.0\". Shneiderman argued that Science 2.0 was about studying social interactions in the \"real world\" with study of e-commerce, online communities and so forth. A writer in \"Wired Magazine\" criticized Shneiderman's view, suggesting that Shneiderman's call for more collaboration, more real-world tests, and more progress should not be called \"Science 2.0\" or \"Science 1.0\" but simply science.\n\nThere are reports that established journals are moving towards greater openness. Some help readers network online; others enable commenters to post links to websites; others make papers accessible after a certain period of time has elapsed. But it remains a \"hotly debated question\", according to one view, whether the business of scientific research can move away from the model of \"peer-vetted, high-quality content without requiring payment for access.\" The topic has been discussed in a lecture series at the California Institute of Technology. Proponent Adam Bly thinks that the key elements needed to transform Science 2.0 are \"vision\" and \"infrastructure\":\n\nThere are numerous examples of more websites offering opportunities for scientific collaboration.\n\n\n\n"}
{"id": "1174316", "url": "https://en.wikipedia.org/wiki?curid=1174316", "title": "Science by press conference", "text": "Science by press conference\n\nScience by press conference (or science by press release) is the practice by which scientists put an unusual focus on publicizing results of research in the media. The term is usually used disparagingly. It is intended to associate the target with people promoting scientific \"findings\" of questionable scientific merit who turn to the media for attention when they are unlikely to win the approval of the professional scientific community.\n\nPremature publicity violates a cultural value of most of the scientific community, which is that findings should be subjected to independent review with a \"thorough examination by the scientific community\" before they are widely publicized. The standard practice is to publish a paper in a peer-reviewed scientific journal. This idea has many merits, including that the scientific community has a responsibility to conduct itself in a deliberative, non-attention seeking way; and that its members should be oriented more towards the pursuit of insight than fame. Science by press conference in its most egregious forms can be undertaken on behalf of an individual researcher seeking fame, a corporation seeking to sway public opinion or investor perception, or a political or ideological movement.\n\nThe phrase was coined by Spyros Andreopoulos, a public affairs officer at Stanford University Medical School, in a 1980 letter which appeared in the \"New England Journal of Medicine\". Andreopoulos was commenting specifically on the publicity practices of biotechnology startups, including Biogen and Genentech. The journal in which it appeared had implemented a long-standing policy under editor Franz J. Ingelfinger which prohibited seeking publicity for research prior to its submission or publication, informally called the Ingelfinger Rule.\n\n\nThese cases became notorious examples of \"science by press conference\" precisely because they were widely reported in the press, but were later either rebuffed, debunked, or found to be outright fraud.\n\nCompetition for publicity, between scientific institutions or just individual researchers, is considered a driving force behind premature press conferences. Pressure to announce research findings quickly enough to \"avoid losing credit\" for any scientific advances may be enhanced by limited or highly competitive funding.\n\nScience by press conference does not have to involve a groundbreaking announcement. A manufacturer may desire to publicize results of research that suggest their product is safe. Science by press conference does not necessarily have to be directed at the general public. In some cases, it may be directed at a target market like opinion leaders, a specific industry, potential investors, or a specific group of consumers. Biotechnology companies, for example, have financial incentives to utilize premature press conferences to gain favorable media coverage.\n\nIn recent years, sociologists of science have recast discussion about \"science by press conference\". They point to the increasing presence of media conversation across all aspects of culture, and argue that science is subject to many of the same social forces as other aspects of culture. They have described the increased \"medialization\" of science, and suggest that both science and society are changed by this process.\n\nWhile the phrase tends to criticize scientists involved in creating the publicity, it has also been used to assert that the media bear responsibility in many instances. Even well-intentioned scientists can sometimes unintentionally create truth-distorting media firestorms because of journalists' difficulty in remaining critical and balanced, the media's interest in controversy, and the general tendency of science reporting to focus on apparent \"groundbreaking findings\" rather than on the larger context of a research field. Further, when results are released with great fanfare and limited peer review, basic journalism skills require skepticism and further investigation; the fact that they often do not can be seen as a problem with the media as much as with scientists who seek to exploit their power.\n\nA common example of science by press conference occurs when the media report that a certain product or activity affects health or safety. For instance, the media frequently report findings that a certain food causes or prevents a disease. These reports sometimes contradict earlier reports. In some cases, it is later learned that a group interested in influencing opinion had a hand in publicizing a specific report.\n\nThe phrase also condemns different behavior in different fields. For instance, scientists working in fields that put an emphasis on the value of fast dissemination of research, like HIV treatment research, often first and most visibly disseminate research results via conferences or talks rather than through printed publication. In these areas of science, printed publication occurs later in the process of dissemination of results than in some other fields. In the case of HIV, this is partly the result of AIDS activism in which people with AIDS and their allies criticized the slow pace of research. In particular, they characterized researchers who kept quiet before publication as being more interested in their careers than in the well-being of people with AIDS. On the other hand, over-hyped early findings can inspire activists' ire and even their direct and critical use of the phrase \"science by press conference\". AIDS denialist groups have claimed that press conferences announcing findings in HIV and AIDS research, particularly Robert Gallo's April 23, 1984, announcement of the discovery of the probable AIDS virus, inhibited research into non-HIV etiologies of AIDS.\n\nSimilarly, clinical trials and other kinds of important medical research may release preliminary results to the media before a journal article is printed. In this case, the justification can be that clinicians and patients will benefit from the information even knowing that the data are preliminary and require further review. For instance, researchers did not wait to publish journal articles about the SARS outbreak before notifying the media about many of their findings, for obvious reasons.\n\nAnother example might be the termination of a clinical trial because it has yielded early benefit. Publicizing this kind of result has obvious value; a delay of a few months might have terrible consequences when the results concern life-threatening conditions. On the other hand, the latter practice is especially vulnerable to abuse for self-serving ends and thus has drawn criticism similar to that implied by the phrase \"science by press conference\".\n\nThese examples illustrate that the derision in the term \"science by press conference\" does not necessarily reflect an absolute rule to publish before publicizing. Rather, it illustrates the value that publicity should be a byproduct of science rather than its objective.\n"}
{"id": "13432082", "url": "https://en.wikipedia.org/wiki?curid=13432082", "title": "Science communication", "text": "Science communication\n\nScience communication is the public communication of science-related topics to non-experts. This often involves professional scientists (called \"outreach\" or \"popularization\"), but has also evolved into a professional field in its own right. It includes science exhibitions, journalism, policy or media production. Science communication also includes communication between scientists (for instance through scientific journals), as well as between scientists and non-scientists (especially during public controversies over science and in citizen science initiatives).\nScience communication may generate support for scientific research or study, or to inform decision making, including political and ethical thinking. There is increasing emphasis on explaining methods rather than simply findings of science. This may be especially critical in addressing scientific misinformation, which spreads easily because it is not subject to the constraints of scientific method.\nScience communicators can use entertainment and persuasion including humour, storytelling and metaphors. Scientists can be trained in some of the techniques used by actors to improve their communication.\n\nPartly due to a market for professional training, science communication is also an academic discipline. Journals include \"Public Understanding of Science\" and \"Science Communication\". Researchers in this field are often linked to Science and Technology Studies, but may also come from history of science, mainstream media studies, psychology or sociology. As a reflection of growth in this field, academic departments, such as the \"Department of Life Sciences Communication\" at the University of Wisconsin-Madison, have been established to focus on applied and theoretical communication issues. Agricultural communication is considered a subset of science communication from an academic and professional standpoint relating to agriculture-related information among agricultural and non-agricultural stakeholders. Health communication is a related discipline.\nWriting in 1987, Geoffery Thomas and John Durant advocated various reasons to increase public understanding of science, or scientific literacy. If the public enjoyed science more, they suggested there would presumably be more funding, progressive regulation, and trained scientists. More trained engineers and scientists could allow a nation to be more competitive economically.\nScience can also benefit individuals. Science can simply have aesthetic appeal (e.g. popular science or science fiction). Living in an increasingly technological society, background scientific knowledge can help to negotiate it. The science of happiness is an example of a field whose research can have direct and obvious implications for individuals.\nGovernments and societies might also benefit from more scientific literacy, since an informed electorate promotes a more democratic society. Moreover, science can inform moral decision making (e.g. answering questions about whether animals can feel pain, how human activity influences climate, or even a science of morality).\nBernard Cohen points out potential pitfalls in improving scientific literacy. He explains first that we must avoid 'scientific idolatry'. In other words, science education must allow the public to respect science without worshiping it, or expecting infallibility. Ultimately scientists are humans, and neither perfectly altruistic, nor perfectly competent. Science communicators must also appreciate the distinction between understanding science and possessing a transferable skill of scientific thinking. Indeed, even trained scientists do not always manage to transfer the skill to other areas of their life.\n\nCommunicating science to the public is increasingly important in today's society. However according to some research, some scientists do not have the skills necessary to do so effectively. There has been some research done over why this is, and it has been found that the stereotype of scientists is the main reason they will not communicate to the public often. The \"Draw a Scientist\" experiment proves that from a young age, most people assume that scientists are unsocial, so scientists use that as a reason to not communicate.\n\nCohen is critical of what has been called \"Scientism\" – the claim that science is the best or only way to solve all problems. He also criticizes the teaching of 'miscellaneous information' and doubts that much of it will ever be of any use, (e.g. the distance in light years from the Earth to various stars, or the names of minerals). Much of scientific knowledge, particularly if it is not the subject of public debate and policy revision, may never really translate to practical changes for the lives of the learners.\n\nMany criticisms of academic research in public understanding of science come from scholars in Science and Technology Studies. For example, Steven Hilgartner (1990) argues that what he calls 'the dominant view' of science popularization tends to imply a tight boundary around those who can articulate true, reliable knowledge. By defining a deficient public as recipients of knowledge, the scientists get to contrast their own identity as experts. The process of popularization is a form of boundary work. Understood in this way, science communication may explicitly exist to connect scientists with the rest of society, but its very existence only acts to emphasise it: as if the scientific community only invited the public to play in order to reinforce its most powerful boundary (according to work by Massimiano Bucchi or Brian Wynne).\n\nBiologist Randy Olson adds that anti-science groups can often be so motivated, and so well funded, that the impartiality of science organizations in politics can lead to crises of public understanding of science. He cites examples of denialism (for instance of global warming) to support this worry. Journalist Robert Krulwich likewise argues that the stories scientists tell are invariably competing with the efforts of people like Adnan Oktar. Krulwich explains that attractive, easy to read, and cheap creationist textbooks were sold by the thousands to schools in Turkey (despite their strong secular tradition) due to the efforts of Oktar. Astrobiologist David Morrison has spoken of repeated disruption of his work by popular anti-scientific phenomena, having been called upon to assuage public fears of an impending cataclysm involving an unseen planetary object—first in 2008, and again in 2012 and 2017.\n\nMarine biologist and film-maker Randy Olson published \"Don't Be Such a Scientist: Talking Substance in an Age of Style\". In the book he describes how there has been this unproductive negligence when it comes to teaching scientists to communicate. \"Don't be Such a Scientist\" is written to his fellow scientists, and he says they need to \"lighten up\". He adds that scientists are ultimately the most responsible for promoting and explaining science to the public and media. This, Olson says, should be done according to a good grasp of social science; scientists must use persuasive and effective means like story telling. Olson acknowledges that the stories told by scientists need not only be compelling but also accurate to modern science - and says this added challenge must simply be confronted. He points to figures like Carl Sagan as effective popularizers, partly because such figures actively cultivate a likeable image.\n\nScience popularization figures such as Carl Sagan and Neil Degrasse Tyson are partly responsible for the view of science or a specific science discipline within the general public. However, the degree of knowledge and experience a science popularizer has can vary greatly. Because of this, some can depend on sensationalism. As a Forbes contributor put it, \"The main job of physics popularizers is the same as it is for any celebrity: get more famous.\" Because of this variation in experience, research scientists can sometimes question the credibility of science popularizers. Another point in the controversy of popular science is the idea of how public debate can affect public opinion. A relevant and highly public example of this is climate change. A science communication study appearing in the New York Times proves that \"even a fractious minority wields enough power to skew a reader’s perception of a [science news] story” and that even “firmly worded (but not uncivil) disagreements between commenters affected readers’ perception of science.” This causes some to worry that the popularizing of science in the public, questioning whether the further popularization of science will cause pressure towards generalization or sensationalism. Unfortunately, this question will fall to time for an answer.\n\nAs his commencement address to Caltech students, journalist Robert Krulwich delivered a speech entitled \"Tell me a story\". Krulwich says that scientists are actually given many opportunities to explain something interesting about science or their work, and that they must seize such opportunities. He says scientists must resist shunning the public, as Sir Isaac Newton did in his writing, and instead embrace metaphors the way Galileo did; Krulwich suggests that metaphors only become more important as the science gets more difficult to understand. He adds that telling stories of science in practice, of scientists' success stories and struggles, helps convey that scientists are real people. Finally, Krulwich advocates for the importance of scientific values in general, and helping the public to understand that scientific views are not mere opinions, but hard-won knowledge.\n\nActor Alan Alda helps scientists and PhD students get more comfortable with communication with the help of drama coaches (they use the acting techniques of Viola Spolin).\n\nMatthew Nisbet describes the use of opinion leaders as intermediaries between scientists and the public as a way to reach the public via trained individuals who are more closely engaged with their communities, such as \"teachers, business leaders, attorneys, policymakers, neighborhood leaders, students, and media professionals.\" Examples of initiatives that take this approach include Science & Engineering Ambassadors, sponsored by the National Academy of Sciences, and Science Booster Clubs, coordinated by the National Center for Science Education.\n\nIn the preface of \"The Selfish Gene\", Richard Dawkins wrote: \"Three imaginary readers looked over my shoulder while I was writing, and I now dedicate the book to them. [...] First the general reader, the layman [...] second the expert [and] third the student\".\n\nMany criticisms of the public understanding of science movement have emphasized that this thing they were calling the public was somewhat of an (unhelpful) black box. Approaches to the public changed with the move away from the public understanding of science. Science communication researchers and practitioners now often showcase their desire to listen to non-scientists as well as acknowledging an awareness of the fluid and complex nature of (post/late) modern social identities. At the very least, people will use plurals: publics or audiences. As the editor of Public Understanding of Science put it in a special issue on publics:\nWe have clearly moved from the old days of the deficit frame and thinking of publics as monolithic to viewing publics as active, knowledgeable, playing multiple roles, receiving as well as shaping science. (Einsiedel, 2007: 5)\nHowever, Einsiedel goes on to suggest both views of the public are \"monolithic\" in their own way; they both choose to declare what something called the public is. Public understanding of science might have ridiculed publics for their ignorance, but an alternative \"public engagement with science and technology\" romanticizes its publics for their participatory instincts, intrinsic morality or simple collective wisdom. As Susanna Hornig Priest (2009) concludes in her recent introduction essay on science’s contemporary audiences, the job of science communication might be to help non-scientists feel they are not excluded as opposed to always included; that they can join in if they want, rather than that there is a necessity to spend their lives engaging.\n\nThe process of quantifiably surveying public opinion of science is now largely associated with the public understanding of science movement (some would say unfairly). In the US, Jon Miller is the name most associated with such work and well known for differentiating between identifiable ‘attentive’ or ‘interested’ publics (that is to say science fans) and those who do not care much about science and technology. Miller’s work questioned whether the American public had the following four attributes of scientific literacy:\n\nIn some respects, John Durant’s work surveying British public applied similar ideas to Miller. However, they were slightly more concerned with attitudes to science and technology, rather than just how much knowledge people had. They also looked at public confidence in their knowledge, considering issues such as the gender of those ticking \"don’t know\" boxes. We can see aspects of this approach, as well as a more \"public engagement with science and technology\" influenced one, reflected within the Eurobarometer studies of public opinion. These have been running since 1973 to monitor public opinion in the member states, with the aim of helping the preparation of policy (and evaluation of policy). They look at a host of topics, not just science and technology but also defence, the euro, enlargement of the European Union, and culture. Eurobarometer’s recent study of Europeans’ Attitudes to Climate Change is a good example. It focuses on respondents’ \"subjective level of information\"; asking \"personally, do you think that you are well informed or not about…?\" rather than checking what people knew.\n\nScience communication can be analysed through frame analysis, a research method used to analyse how people understand situations and activities.\n\nSome features of this analysis are listed below.\n\nPeople make an enormous number of decisions every day, and to approach all of them in a careful, methodical manner is impractical. They therefore often use mental shortcuts known as \"heuristics\" to quickly arrive at acceptable inferences. Tversky and Kahneman originally proposed three heuristics, listed below, although there are many others that have been discussed in later research. \nThe most effective science communication efforts take into account the role that heuristics play in everyday decision-making. Many outreach initiatives focus solely on increasing the public's knowledge, but studies (e.g. Brossard et al. 2012) have found that there is little – if any – correlation between knowledge levels and attitudes towards scientific issues.\n\nThe simulation heuristic is used to judge how likely certain outcomes are based on the ease with which one can imagine a particular ending. This heuristic can be used for many tasks, including prediction (Will the Jets win this football game?) and causality (Did Jim eat the last slice of pizza?). An application of this heuristic is to the case of near misses. Consider the following example from Kahneman & Tversky:Mr. Crane and Mr. Tees were scheduled to leave the airport on different flights, at the same time. They traveled from town in the same limousine, were caught in a traffic jam, and arrived at the airport thirty minutes after the scheduled departure time of their flights.Mr. Crane is told his flight left on time.Mr. Tees is told that his flight was delayed, and just left five minutes ago.Who is more upset?Mr. Crane or Mr. Tees?Almost everyone says, \"Mr. Tees\", because they cannot imagine how Mr. Crane could have caught his flight, while Mr. Tees might have made it if not for that slow pedestrian, or the exceptionally long security line. The simulation heuristic has this ability to generate \"if only\" conditions, which can be used to understand the negative feelings of frustration, indignation, etc. that arise from near misses such as that of Mr. Tees.\n\nThis simulation of how events \"might\" have occurred is referred to as counterfactual thinking, and can be used to try to identify a unique or unusual circumstance that lead to a dramatic outcome. For example, consider a man who is shot during a robbery while shopping at a convenience store. Subjects will award more damages to a man who was shopping at a store far from his house than they will to a man who was shopping at a store near his home that he commonly visits.\n\nRegarding simulations of future events, simply imagining hypothetical events makes them seem more likely to occur. This phenomenon can be extended to a person's own behavior, as imagining oneself performing or refusing to perform an action causes changes in expectations about one's future behavior. Simulation is \"more likely to increase the perceived likelihood of a potential outcome...than to reduce perceived likelihood of a potential consequence\". Thus, the implications of research on the simulation heuristic are particularly intriguing when designing outreach efforts intended to change behaviors, such as increasing recycling or decreasing fast food consumption.\n\nWhile scientific study began to emerge as a popular discourse following the Renaissance and the Enlightenment, science was not widely funded or exposed to the public until the nineteenth century. Most science prior to this was funded by individuals under private patronage and was studied in exclusive groups, like the Royal Society. Public science emerged due to a gradual social change, resulting from the rise of the middle class in the nineteenth century. As scientific inventions, like the conveyor belt and the steam locomotive entered and enhanced the lifestyle of people in the nineteenth century, scientific inventions began to be widely funded by universities and other public institutions in an effort to increase scientific research. Since scientific achievements were beneficial to society, the pursuit of scientific knowledge resulted in science as a profession. Scientific institutions, like the National Academy of Sciences or the British Association for the Advancement of Science are examples of leading platforms for the public discussion of science. David Brewster, founder of the British Association for the Advancement of Science, believed in regulated publications in order to effectively communicate their discoveries, \"so that scientific students may know where to begin their labours.\" As the communication of science reached a wider audience, due to the professionalization of science and its introduction to the public sphere, the interest in the subject increased.\n\nThere was a change in media production in the nineteenth century. The invention of the steam-powered printing press enabled more pages to be printed per hour, which resulted in cheaper texts. Book prices gradually dropped, which gave the working classes the ability to purchase them. No longer reserved for the elite, affordable and informative texts were made available to a mass audience. Historian Aileen Fyfe noted that, as the nineteenth century experienced a set of that sought to improve the lives of those in the working classes, the availability of public knowledge was valuable for intellectual growth. As a result, there were reform efforts to further the knowledge of the less educated. The Society for the Diffusion of Useful Knowledge, led by Henry Brougham, attempted to organize a system for widespread literacy for all classes. Additionally, weekly periodicals, like the \"Penny Magazine\", were aimed to educate the general public on scientific achievements in a comprehensive manner.\n\nAs the audience for scientific texts expanded, the interest in public science did as well. 'Extension lectures' were installed in some universities, like Oxford and Cambridge, which encouraged members of the public to attend lectures. In America, travelling lectures were a common occurrence in the nineteenth century and attracted hundreds of viewers. These public lectures were a part of the lyceum movement and demonstrated basic scientific experiments, which advanced scientific knowledge for both the educated and uneducated viewers.\n\nNot only did the popularization of public science enlighten the general public through mass media, but it also enhanced communication within the scientific community. Although scientists had been communicating their discoveries and achievements through print for centuries, publications with a variety of subjects decreased in popularity. Alternatively, publications in discipline-specific journals were crucial for a successful career in the sciences in the nineteenth century. As a result, scientific journals such as \"Nature\" or \"National Geographic\" possessed a large readership and received substantial funding by the end of the nineteenth century as the popularization of science continued.\n\nScience can be communicated to the public in many different ways. According to Karen Bultitude, a science communication lecturer at University College London, these can be broadly categorised into three groups: traditional journalism, live or face-to-face events, and online interaction. Traditional journalism (for example, newspapers, magazines, television and radio) has the advantage of reaching large audiences; in the past, this is way most people regularly accessed information about science. Traditional media is also more likely to produce information that is high quality (well written or presented), as it will have been produced by professional journalists. Traditional journalism is often also responsible for setting agendas and having an impact on government policy. The traditional journalistic method of communication is one-way, so there can be no dialogue with the public, and science stories can often be reduced in scope so that there is a limited focus for a mainstream audience, who may not be able to comprehend the bigger picture from a scientific perspective. However, there is new research now available on the role of newspapers and television channels in constituting 'scientific public spheres' which enable participation of a wide range of actors in public deliberations.\n\nAnother disadvantage of traditional journalism is that, once a science story is taken up by mainstream media, the scientist(s) involved no longer has any direct control over how his or her work is communicated, which may lead to misunderstanding or misinformation. Research in this area demonstrates how the relationship between journalists and scientists has been strained in some instances. On one hand scientists have reported being frustrated with things like journalists oversimplifying or dramatizing of their work, while on the other hand journalists find scientists difficult to work with and ill-equipped to communicate their work to a general audience. Despite this potential tension, a comparison of scientists from several countries has shown that many scientists are pleased with their media interactions and engage often.\n\nHowever, it is important to note the use of traditional media sources, like newspapers and television, has steadily declined as primary sources for science information, while the internet has rapidly increased in prominence. In 2016, over half of Americans (55 percent) reported using the internet as their primary source to learn about science and technology, compared to 24 percent reporting TV and 4 percent reporting newspapers were their primary sources. Additionally, traditional media outlets have dramatically decreased the number of, or in some cases eliminated, science journalists and the amount of science-related content they publish.\n\nThe second category is live or face-to-face events, such as public lectures (for example, UCL's public lunch hour lectures – museums, debates, science busking, sci-art, science cafes and science festivals. Citizen Science or crowd-sourced science (scientific research conducted, in whole or in part, by amateur or nonprofessional scientists), which can be done with a face-to-face approach, online, or as a combination of the two to engage in science communication. Research has shown that members of the public seek out science information that is entertaining, but also helping citizens to critically participate in risk regulation and S&T governance. Therefore it is important to bear this aspect in mind when communicating scientific information to the public (for example, through events combining science communication and comedy, such as Festival of the Spoken Nerd or during scientific controversies). The advantages of this approach are that it is more personal and allows scientists to interact with the public, allowing for two-way dialogue. Scientists are also better able to control content using this method. Disadvantages of this method include the limited reach, it can also be resource-intensive and costly and also, it may be that only audiences with an existing interest in science will be attracted.\n\nThe third category is online interaction, for example, websites, blogs, wikis and podcasts can also be used for science communication, as can other social media. Online methods of communicating science have the potential to reach huge audiences, can allow direct interaction between scientists and the public, and the content is always accessible and can be somewhat controlled by the scientist. Additionally, online communication of science can help boost scientists' reputation through increased citations, better circulation of articles, and establishing new collaborations. Online communication also allows for both one-way and two-way communication, depending on the audience’s and the author's preferences. However, there are disadvantages in that it is difficult to control how content is picked up by others, and regular attention and updating is needed.\n\nWhen considering whether or not to engage in science communication online, scientists should review what science communication research has shown to be the potential positive and negative outcomes. Online communication has given rise to movements like open science, which advocates for making science more accessible. However, when engaging in communication about science online, scientists should consider not publicizing or reporting findings from their research until it has been peer-reviewed and published, as journals may not accept the work after it has been circulated under the \"Ingelfinger rule\".\n\nOther considerations revolve around how scientsts will be perceived by other scientists for engaging in communication. For example, some scholars have criticized engaged, popular scholars using concepts like the Sagan effect or Kardashian Index. Despite these criticisms, many scientists are taking to communicating their work on online platforms, a sign of potentially changing norms in the field.\n\nBy using Twitter, researchers and academics can discuss and communicate scientific topics with many types of audiences based on various points of view. Some studies indicate that the use of Twitter can positively impact the number of times a scientific article is cited. These studies show that articles that are highly tweeted about are eleven times more likely to be highly cited than those that who few people tweeted.\n\nAs noted in studies by Gunther Eysenbach, research has shed light on how Twitter has a direct link to the advances in the science community. Alison Burt, Editor in chief of Elsevier Connect and author of the article \"How to use social media for science,\" states the potential drawbacks to sharing their research on Twitter.\n\nKimberly Collins of PLOS explains reasons how some scientists are hesitant to join Twitter. Some scientists are hesitant to use social media outlets such as Twitter due to lack of knowledge of the platform, and inexperience with how to make meaningful posts. Some scientists do not see the meaning in using Twitter as a platform to share their research or have the time to add the information into the accounts themselves.\n\nScientists also believe that Twitter is not professional enough for them to put out information as well as receive relevant suggestions and comments back. Scientists did give a positive to using Twitter by (28%) of the scientists who participated in the study said communicating science on Twitter can benefit because of the size and diverse audience it reaches. BoingBoing science editor and New York Times columnist Maggie Koerth-Baker commented on the importance of keeping public and private personas on social media separate in order to maintain professionalism online. According to these findings, posting academic research on a personal social media accounts could potentially send mixed messages to Twitter users.\n\nThere have been occasions where scientific outreach on Twitter has been met with positive results. In September 2017, an 8 year old bug lover was teased at her school for her passion for bugs. This led to the Entomological society of Canada posting a tweet defending her love for bugs called #BugsR4Girls. The ESC’s use of twitter was able to make a statement saying, \"A young girl who loves insects is being bullied & needs our support. DM your email & we'll connect you! #BugsR4Girls\".\n\nIn 2017, a study done by the Pew Research Center of Journalism and Media found that \"About a quarter of social media users follow science related pages and accounts. This group places both more importance and comparatively more trust on science news that comes to them through social media\".\n\nKaren Peterson, director of Scientific Career Development at Fred Hutchinson Cancer research Center stresses the \"importance of using social networks such as Facebook and Twitter to engage in intercommunication\" for establishing an online presence as well. Online presence is necessary for career development. No matter your personality type, career advisors recommend that postdocs use online networking tools to make connections, exchange scientific ideas, and advance a career.\n\nAccording to Nature, \"more than 3,000 scientists and engineers told Nature about their awareness of various giant social media networks and research-profiling sites\". Elena Milani created the SciHashtag project which is a condensed collection of Twitter hashtags regarding science communication and science. Twitter has become a part of researchers’ lives.\n\nThe public understanding of science, public awareness of science and public engagement with science and technology are all terms coined with a movement involving governments and societies in the late 20th century. During the late 19th century, science became a professional subject and influenced by governmental suggestions. Prior to this, public understanding of science was very low on the agenda. However, some well-known figures such as Michael Faraday ran lectures aimed at the non-expert public, his being the famous Christmas Lectures which began in 1825. The 20th century saw groups founded on the basis they could position science in a broader cultural context and allow scientists to communicate their knowledge in a way that could reach and be understood by the general public. In the UK, \"The Bodmer Report\" (or \"The Public Understanding of Science\" as it is more formally known) published in 1985 by The Royal Society changed the way scientists communicated their work to the public. The report was designed to \"review the nature and extent of the public understanding of science in the United Kingdom and its adequacy for an advanced democracy.\". Chaired by the geneticist Sir Walter Bodmer alongside famous scientists such as broadcaster Sir David Attenborough, the report was evidenced by all of the major sectors concerned; scientists, politicians, journalists and industrialists but not the general public. One of the main assumptions drawn from the report was everybody should have some grasp of science and this should be introduced from a young age by teachers who are suitably qualified in the subject area. The report also asked for further media coverage of science including via newspapers and television which has ultimately led to the establishment of platforms such as the Vega Science Trust.\n\nIn both the UK and the United States following the second world war, public views of scientists swayed from great praise to resentment. Therefore, the Bodmer Report highlighted concerns from the scientific community that their withdrawal from society was causing scientific research funding to be weak. Bodmer promoted the communication of science to a wider more general public by expressing to British scientists that it was their responsibility to publicise their research. An upshot of the publication of the report was the creation of the Committee on the Public Understanding of Science (COPUS), a collaboration between the British Association for the Advancement of Science, the Royal Society and the Royal Institution. The engagement between these individual societies caused the necessity for a public understanding of science movement to be taken seriously. COPUS also awarded grants for specific outreach activities allowing the public understanding to come to the fore. Ultimately leading to a cultural shift in the way scientists publicised their work to the wider non-expert community. Although COPUS no longer exists within the UK the name has been adopted in the US by the Coalition for the Public Understanding of Science. An organisation which is funded by the US National Academy of Sciences and the National Science Foundation and focuses on popular science projects such as science cafes, festivals, magazines and citizen science schemes.\n\n\n\n"}
{"id": "10466072", "url": "https://en.wikipedia.org/wiki?curid=10466072", "title": "Scientific literacy", "text": "Scientific literacy\n\nScientific literacy or Science literacy encompasses written, numerical, and digital literacy as they pertain to understanding science, its methodology, observations, and theories.\n\nThe OECD PISA Framework (2015) defines scientific literacy as \"the ability to engage with science-related issues, and with the ideas of science, as a reflective citizen.\" A scientifically literate person, therefore, is willing to engage in reasoned discourse about science \nand technology which requires the competencies to:\n\nAccording to the United States National Center for Education Statistics, \"scientific literacy is the knowledge and understanding of scientific concepts and processes required for personal decision making, participation in civic and cultural affairs, and economic productivity\". A scientifically literate person is defined as one who has the capacity to:\n\nScientific literacy may also be defined in language similar to the definitions of ocean literacy, Earth science literacy and Climate Literacy. Thus a scientifically literate person can:\n\nFinally, scientific literacy may involve particular attitudes toward learning and using science. A scientifically-literate citizen is capable of researching matters of fact for him or herself.\n\nReforms in science education in the United States have often been driven by strategic challenges such as the launch of the Sputnik satellite in 1957 and the Japanese economic boom in the 1980s. By contrast, scientific literacy is now taken to mean that everyone should have a working knowledge of science and its role in society. Science literacy is seen as a right of every person and a requirement for responsible members of society, one that helps average people to make better decisions and enrich their lives. The shift occurred in the late 1980s and early 1990s, with the publication of \"Science for All Americans\" and \"Benchmarks for Science Literacy\".\n\nInitial definitions of science literacy included elaborations of the actual content that people should understand, and this content often followed somewhat traditional lines (biology, chemistry, physics). Earth science was somewhat narrowly defined as expanded geological processes. In the decade after those initial documents, ocean scientists and educators revised the notion of science literacy to include more contemporary, systems-oriented views of the natural world, leading to scientific literacy programs for the ocean, climate, earth science, and so on. This shift has ensured that educators' views of science literacy stay in sync with the directions and advances of real science in the real world.\n\nThe interdependence of humans and our natural environment is at the heart of scientific literacy in the Earth systems. As defined by nationwide consensus among scientists and educators, this literacy has two key parts. First, a literate person is defined, in language that echoes the above definition of scientific literacy. Second, a set of concepts are listed, organized into six to nine big ideas or essential principles. This defining process was undertaken first for ocean literacy, then for the Great Lakes, estuaries, the atmosphere, and climate.\nEarth science literacy is one of the types of literacy defined for Earth systems; the qualities of an Earth science literate person are representative of the qualities for all the Earth system literacy definitions.\n\nAccording to the Earth Science Literacy Initiative, an Earth-science-literate person:\n\nAll types of literacy in Earth systems have a definition like the above. Ocean literacy is further defined as \"understanding our impact on the ocean and the ocean's impact on us\".\nSimilarly, the climate literacy website includes a guiding principle for decision making; \"humans can take action to reduce climate change and its impacts\". Each type of Earth systems literacy then defines the concepts students should understand upon graduation from high school. Current educational efforts in Earth systems literacy tend to focus more on the scientific concepts than on the decision-making aspect of literacy, but environmental action remains as a stated goal.\n\nThe theme of science in a socially-relevant context appears in many discussions of scientific literacy. Ideas that turn up in the life sciences include an allusion to ecological literacy, the \"well-being of earth\". Robin Wright, a writer for \"Cell Biology Education\", laments \"will [undergraduates'] misunderstandings or lack of knowledge about science imperil our democratic way of life and national security?\" A discussion of physics literacy includes energy conservation, ozone depletion and global warming.\nThe mission statement of the Chemistry Literacy Project includes environmental and social justice.\nTechnological literacy is defined in a three-dimensional coordinate space; on the knowledge axis, it is noted that technology can be risky, and that it \"reflects the values and culture of society\".\nEnergy Literacy boasts several websites, including one associated with climate literacy\nand two advocacy organizations.\n\nAttitudes about science can have a significant effect on scientific literacy. In education theory, understanding of content lies in the cognitive domain, while attitudes lie in the affective domain. Thus, negative attitudes, such as fear of science, can act as an affective filter and an impediment to comprehension and future learning goals. Studies of college students' attitudes about learning physics suggest that these attitudes may be divided into categories of real world connections, personal connections, conceptual connections, student effort and problem-solving.\n\nThe decision-making aspect of science literacy suggests further attitudes about the state of the world, one's responsibility for its well-being and one's sense of empowerment to make a difference. These attitudes may be important measures of science literacy, as described in the case of ocean literacy.\n\nProponents of scientific literacy tend to focus on what is learned by the time a student graduates from high school. Science literacy has always been an important element of the standards movement in education. All science literacy documents have been drafted with the explicit intent of influencing educational standards, as a means to drive curriculum, teaching, assessment, and ultimately, learning nationwide.\n\nPrograms to promote scientific literacy among students abound, including several programs sponsored by technology companies, as well as quiz bowls and science fairs. A partial list of such programs includes the Global Challenge Award, the National Ocean Sciences Bowl and Action Bioscience.\n\nSome organizations have attempted to compare the scientific literacy of adults in different countries. The Organisation for Economic Co-operation and Development found that scientific literacy in the United States is not measurably different from the OECD average. Science News reports \"The new U.S. rate, based on questionnaires administered in 2008, is seven percentage points behind Sweden, the only European nation to exceed the Americans. The U.S. figure is slightly higher than that for Denmark, Finland, Norway and the Netherlands. And it’s double the 2005 rate in the United Kingdom (and the collective rate for the European Union).\" \n\nUniversity educators are attempting to develop reliable instruments to measure scientific literacy, and the use of concept inventories is increasing in the fields of physics, astronomy, chemistry, biology\nand earth science.\n\n\n\n"}
{"id": "12987835", "url": "https://en.wikipedia.org/wiki?curid=12987835", "title": "The Ambidextrous Universe", "text": "The Ambidextrous Universe\n\nThe Ambidextrous Universe is a popular science book by Martin Gardner, covering aspects of symmetry and asymmetry in human culture, science and the wider universe.\n\nOriginally published in 1964, it underwent revisions in 1969, 1979, 1990 and 2005 (the last two are known as the \"Third, revised edition\"). Originally titled \" The Ambidextrous Universe: Mirror Asymmetry and Time-Reversed Worlds\", subsequent editions are known as \"The New Ambidextrous Universe: Symmetry and Asymmetry from Mirror Reflections to Superstrings\".\n\nThe book begins with the subject of mirror reflection, and from there passes through symmetry in geometry, poetry, art, music, galaxies, suns, planets and living organisms. It then moves down into the molecular scale and looks at how symmetry and asymmetry have evolved from the beginning of life on Earth. There is a chapter on carbon and its versatility and on chirality in biochemistry. Chapter 18 (and subsequent chapters) deals with a conundrum called the Ozma Problem (see below). The second half of the book concerns various aspects of atomic and subatomic physics and how they relate to mirror asymmetry and the related concepts of chirality, antimatter, magnetic and electrical polarity, parity, charge and spin. Time invariance (and reversal) is discussed. Implications for particle physics, theoretical physics and cosmology are covered and brought up to date (in later editions of the book) with regard to GUTs, TOEs, superstring theory and M-theory.\n\nThe 18th chapter, \"The Ozma Problem\", poses a problem that Gardner claims would arise if Earth should ever enter into communication with life on another planet through Project Ozma. This is the problem of how to communicate the meaning of left and right, where the two communicants are conditionally not allowed to view any one object in common. The problem was first implied in Immanuel Kant's discussion of left and right, and William James mentioned it in his chapter on \"The Perception of Space\" in \"The Principles of Psychology\" (1890). It is also mentioned by Charles Howard Hinton. Gardner follows the thread of several false leads on the road to the solution of the Ozma Problem, in each case presenting an apparent solution which, on closer examination, turns out to be a false one.\n\nThe solution to the Ozma Problem was finally embodied in the famous \"Wu experiment\", conducted in 1956 by Chinese-American physicist Chien-Shiung Wu (1912–1997), involving the beta decay of cobalt-60. This experiment was the first to disprove the conservation of parity. At long last, according to Gardner, it is believed that one could carefully describe the Wu experiment to a distant extraterrestrial intelligence and thereby convey the exact meaning of left/right.\n\nW. H. Auden alludes to \"The Ambidextrous Universe\" in his poem \"Josef Weinheber\" (1965).\n\nPale Fire<br>\nIn the original 1964 edition of \"The Ambidextrous Universe\", Gardner quoted two lines of poetry from Vladimir Nabokov's 1962 novel \"Pale Fire\" which are supposed to have been written by a poet, \"John Shade\", who is actually fictional. As a joke, Gardner credited the lines only to Shade and put Shade's name in the index as if he were a real person. In his 1969 novel \"\", Nabokov returned the favor by having the character Van Veen \"quote\" the Gardner book along with the two lines of verse:\n\"Space is a swarming in the eyes, and Time a singing in the ears,\" says John Shade, a modern poet, as quoted by an invented philosopher (\"Martin Gardiner\" ) in \"The Ambidextrous Universe\", page 165 .\n\nLook at the Harlequins!<br>\nNabokov's 1974 novel \"Look at the Harlequins!\", about a man who can't distinguish left from right, was heavily influenced by his reading of \"The Ambidextrous Universe\".\n"}
{"id": "16225496", "url": "https://en.wikipedia.org/wiki?curid=16225496", "title": "The Science Network", "text": "The Science Network\n\nThe Science Network (TSN) is a non-profit virtual forum dedicated to science and its impact on society. Initially conceived in 2003 by Roger Bingham and Terry Sejnowski as a cable science network, TSN would soon become a global digital platform. TSN currently offers free access to over 1100 videos of lectures from scientific meetings and long form one-on-one conversations with prominent scientists and communicators of science, including Neil deGrasse Tyson, V.S. Ramachandran, Helen S. Mayberg, and Barbara Landau, on topics including education, aging, neuroscience, and stem cells. As part of its mission, TSN has also sponsored and co-sponsored scientific forums, such as the landmark Symposium and Town Hall meeting, \"Stem cells: science, ethics and politics at the crossroads\", held at the Salk Institute in 2004 and the \"Beyond Belief\" conference series.\n\nTSN's signature series \"Beyond Belief\" was conceived to bring together a community of scientists, philosophers, scholars from the humanities, and social commentators. Speakers at these meetings have included Steven Weinberg, Richard Dawkins, Sam Harris, Harry Kroto, Neil deGrasse Tyson, and Stuart Kauffman. So far, the following three \"Beyond Belief\" conferences were organized:\n\n\"\", the first of The Science Network's annual Beyond Belief symposia, held from November 5 to November 7, 2006, was described by \"The New York Times\", as \"a free-for-all on science and religion,\" which seemed at times like \"the founding convention for a political party built on a single plank: in a world dangerously charged with ideology, science needs to take on an evangelical role, vying with religion as teller of the greatest story ever told.\" According to participant Melvin Konner, however, the event came to resemble a \"den of vipers” debating the issue, \"Should we bash religion with a crowbar or only with a baseball bat?”\n\nNew Scientist summed up the topics to be discussed as a list of three questions:\n\n\nSpeakers included physicists Steven Weinberg, Lawrence Krauss, author Sam Harris, biologist Joan Roughgarden, and astrophysicist Neil deGrasse Tyson.\n\n\"Beyond Belief: Enlightenment 2.0\" was the second annual symposium and was held from 31 October to 2 November 2007 at the Frederic de Hoffmann Auditorium of the Salk Institute for Biological Studies.\n\n\"Beyond Belief: Candles in the Dark\" was the third annual Beyond Belief symposium. This event was organized by The Science Network and held from 3 October to 6 October 2008 in La Jolla, CA.\n"}
{"id": "55269375", "url": "https://en.wikipedia.org/wiki?curid=55269375", "title": "University technology transfer offices", "text": "University technology transfer offices\n\nUniversity technology transfer offices (TTOs), or Technology Licensing Offices (TLOs), are responsible for technology transfer and other aspects of the commercialization of research that takes place in a university. TTOs engage in a variety of commercial activities that are meant to facilitate the process of bringing research developments to market, often acting as a channel between academia and industry. Most major research universities have established TTOs in the past decades in an effort to increase the impact of university research and provide opportunities for financial gain. While TTOs are commonplace, many studies have questioned their financial benefit to the university.\n\nThe history of technology transfer is intimately linked with the history of the science policy of the United States. The foundation for modern American science policy laid way out in Vannevar Bush's letter in response to President Roosevelt's query about whether the US should maintain the high level of research funding it had been pouring into the Office of Scientific Research and Development, which had coordinated large private-public partnership research projects as part of the war effort, including the Manhattan Project. Bush's answer was \"Science - the Endless Frontier\". In that letter, Bush advocated that the US should continue to fund basic research at high levels, arguing that while the US no longer had a geographic frontier, extending the boundaries of science would allow the creation of new technologies, which in turn would spur new industries, create jobs, generate wealth, and maintain US power. As the US worked out its approach to funding science in the 1950s, Congress decided that the federal government should maintain ownership of patents on inventions funded by the federal government.\n\nFederal research funding drove the growth of the research university. Many universities in the early 20th century did not engage in patenting and licensing, since the government owned most inventions, and out of fear of interfering with their missions of supporting the growth of knowledge and objective inquiry. Prior to the postwar period, universities relied mostly on external patent management organizations such as the Research Corporation, while few set up their own research foundations that were independent from but affiliated to the university. Some universities, such as Stanford University and the University of Wisconsin, had active licensing programs of their own. There was a shift in universities' approaches to technology transfer between 1970-1980. During this period, universities began taking commercialization efforts into their own hands and setting up TTOs.\n\nThe Bayh–Dole Act of 1980 led many US universities to set up tech transfer offices. The Act was created to try to spur the stagnant US economy of the 1970s, harking back to Vannevar Bush's vision of the role of federal research funding in the US economy. The Act decentralized ownership of inventions funded with federal grants, allowing universities that received federal grant funding to maintain ownership of such inventions, obligating them to try to patent and license the inventions to US companies, and requiring universities to share license income with inventors.\n\nWhile the broad goal of TTOs is to commercialize university research, they engage in numerous activities that not only bring these developments to market but also encourage and support faculty and students in the entire technology transfer process. Such encouragement may increase the chances of faculty and students creating research developments that can be commercialized. Some of the major functions of TTOs include:\n\nAn important task of many TTOs is to create and maintain industry partnerships that may be crucial for collaboration and bringing technologies to market. Some universities such as MIT and Northwestern have separate offices for industry and corporate relations which typically work in conjunction with the TTO of the institution. In this case, TTOs often exploit the relationships developed by the corporate relations office, focusing more specifically on the technology transfer process itself. TTOs often employ two methods when engaging with industry partners: 1) the \"pull\" method, in which TTOs \"receive\" interest from industry partners in bringing specific technologies at the university to market, and 2) the \"push\" method, in which TTOs actively \"seek\" industry partners for this purpose.\n\nThe Bayh-Dole Act obligated universities to seek patent protection, when appropriate, for inventions to which they elect title; after passage of the Bayh-Dole Act many US universities created intellectual property policies that obligated faculty to assign inventions to the university. Universities typically license the patent to a company that will invest money in developing the invention into a product, which it will then be able to sell at a premium, recouping its investment and making profit before the patent expires.\n\nTTOs at many universities often provide general business and legal counseling to foster entrepreneurship among faculty and students. By providing resources, funding, and connections to university spin-off companies, TTOs attempt to increase the chances of startup success, which may result in financial gain if the university owns the intellectual property of the invention or has an equity stake in the company. Hence, many TTOs establish business incubators and programs for faculty and students in an attempt to enhance the entrepreneurial atmosphere among researchers at the university. Some examples of such incubators and programs include the Blavatnik Biomedical Accelerator as well as the Physical Sciences and Engineering Accelerator at Harvard University, and Fab Lab MSI, affiliated with the University of Chicago. Research has suggested that incubators at TTOs have not had a high incidence of technology transfer, despite this being one of the reasons they were established, and may even negatively impact the success of TTOs and technology transfer at the university.\n\nThe structure and organization of TTOs can affect its overall performance and can vary among universities. Since TTOs deal with both academic research and industry, they consist of a diverse set of individuals, including scientists, lawyers, analysts, licensing experts, and business managers. By having individuals (particularly different scientists, engineers, and analysts) with varying sets of expertise in research, TTOs attempt to more effectively assess, protect, and profit from the research developments taking place in multiple disciplines throughout the university.\n\nTTOs can by classified into three different types: \nAs of 2012 the \"internal\" type was most common in the US.\n\nTTOs attempt to capitalize on the research developments made at the university by employing strategies focused on providing the university with opportunities for financial gain and increased research impact. A common strategy that TTOs engage in is licensing their inventions, either to an industry partner or back to the university inventor if the inventor started a company (i.e. a university spin-off). Through this approach, TTOs can bring university technologies to market without having to engage in production and distribution themselves. TTOs can also take an equity stake in the spin-off company rather than licensing the technology. Some research has suggested that equity in spin-off companies may provide higher returns than licensing, but this strategy seems to be more common with TTOs that are financially independent from the parent university (i.e. external TTO structure). While these strategies vary greatly among TTOs at different universities, a majority of them employ some combination of licensing and equity stakes, with licensing being a more standard practice.\n\nAs many major research universities across the US began to adopt TTOs, institutions outside the US became attracted to the idea of taking control of their commercialization activities as well. Prior to the 2000s, many German-speaking and Scandinavian countries had a policy of \"professor's privilege\", in which faculty retain the right to control the intellectual property of their inventions. In addition, in recent years many OECD and EU nations have created legislation that emulates Bayh-Dole, in an attempt to increase the commercialization activities and impact of their respective research universities. Denmark was among the first to abolish professor's privilege, followed by Germany, Austria, Norway and Finland between 2000-2007. Countries such as France and the UK, which already had policies in place that grant intellectual property rights to universities during this period, began heavily encouraging and enforcing these institutional ownership rights. As of 2011, most European countries grant universities the rights to the intellectual property of inventions developed by faculty researchers, yet a few countries such as Italy and Sweden still employ professor's privilege. Hence, there has been a marked increase in the commercialization activities of universities and creation of TTOs in Europe.\n\nSeveral Asian countries such as Japan, China, and India have also shifted towards a Bayh-Dole type legislation, although some countries such as Malaysia have a shared ownership model. Moreover, there has been a general shift towards increased commercialization and the establishment of TTOs across higher education institutions in Asian countries. \n\nAlthough universities created TTOs with hopes of financial gain, many TTOs have retained losses in their commercialization activities and have not generated significant local economic development. It has been argued that protecting intellectual property and patenting is a costly process, and of all the patents and licenses a university issues, there may be a limited number of inventions that actually yield enough revenue to cover or surpass these costs. Research has shown that larger, more established TTOs are sufficiently profitable, whereas many smaller, more recent TTOs are not, and that an estimated half of TTOs retain losses in their commercialization activities (of those that do not have losses, a majority do no better than to cover their costs). Even the most profitable TTOs only produce revenue that amounts to 1-3% of the total research expenditures at the university. Moreover, less than 1% of licensed technologies actually yield over $1M in revenue. Another criticism of TTOs is its role in the research atmosphere of the university, with many scholars arguing that its presence and purpose of engaging in commercialization activities conflicts with a university's mission of furthering knowledge and objective academic inquiry.\n\nRebecca Eisenberg and Michael Heller have argued that the Bayh-Dole Act spurred university tech transfer offices to become too aggressive in patenting, creating patent thickets and a tragedy of the anticommons especially in the field of biomedical research. As of 2012, evidence for such an anticommons effect in the practice of biomedical science was lacking.\n"}
{"id": "46418619", "url": "https://en.wikipedia.org/wiki?curid=46418619", "title": "Women in climate change", "text": "Women in climate change\n\nThe contributions of women in climate change have received increasing attention in the early 21st century. Feedback from women and the issues faced by women have been described as \"imperative\" by the United Nations and \"critical\" by the Population Reference Bureau. A report by the World Health Organization concluded that incorporating gender-based analysis would \"provide more effective climate change mitigation and adaptation.\"\n\nWomen have made major contributions to climate change research and policy and to broader analysis of global environmental issues. They include many women scientists as well as policy makers and activists. Women researchers have made significant contributions to major scientific assessments such as those of the Intergovernmental Panel on Climate Change and the Millennium Ecosystem Assessment and are reasonably well represented on key global change committees of the International Council for Science (ICSU) and US National Academy of Sciences. Women have played important leadership roles in international climate policy. For example, Christiana Figueres leads the international climate negotiations as the Executive Secretary of the UN Framework Convention on Climate Change (UNFCCC) and former Irish President Mary Robinson is the UN Special Envoy on Climate Change. Susan Solomon chaired the climate science working group 1 of the Intergovernmental Panel on Climate Change Fourth Assessment in 2007.\n\nWomen are generally underrepresented in science and have faced many barriers to their success and recognition. Following the scientific revolution in the 17th century European women became involved in observational science, including astronomy, natural history and weather observations although many universities would not admit women until the late 19th century.\n\nThe latest report from the US National Science Foundation shows that while women are now earning half of the undergraduate degrees in science and engineering, most of these are in the biosciences (especially pre-med) compared to physics, computer sciences and engineering (20%). In terms of doctorates, women are also only 20% of the engineering and physics PhDs. Although the proportion of women full professors in the US has doubled since 1993 women occupy less than 1/4 of senior faculty positions in science and engineering and women earn less than men at the same level.\n\nIt has been noted that women of color, indigenous women and women from the global south are even more likely to be overlooked, to be poorly represented in the academy and leadership. This is associated with a legacy of discrimination, lack of educational opportunities, language barriers, and a lack of effort to identify and cite them.\n\nWomen are underrepresented in key disciplines for the study of climate change. For example, women are a minority in the earth sciences where surveys reveal that less than 20% of meteorologists and geoscientists are women. A recent analysis of US atmospheric science doctoral programs reveals that women were 17% of tenure track and tenured faculty, with even smaller proportions at higher rank, and 53% of departments had two or fewer women faculty. Women are slightly better represented in the ecological sciences. One study reports that women are 55% of graduate students in ecology but only 1/3 of tenured faculty are women and that 3/4 of the articles in the flagship international journal - Ecology - are written by men. Women received proportionally less research funding and were less likely to be cited by their colleagues. Women members of the Ecological Society of America increased from 23% in 1992 to 37% in 2010.\n\nThe United Nations Educational, Scientific and Cultural Organization publishes data on women in science worldwide. Overall women are better represented as a share of total scientific researchers in Latin America, Oceania and Europe (30%+) and least in Asia (19%).\n\nIt is argued that when women are overlooked as scholars and decision makers the world fails to take advantage of its full human capacity, which is needed for issues as urgent as climate change. Women may also take more collaborative approaches, especially in negotiations, and may pay more attention to disadvantaged groups and to the natural environment.\n\nGender has become an issue because of women's essential roles in managing resources such as water, forests and energy and as women lead fights for environmental protection.\n\nA general concern has been expressed about the need to highlight the work of women and to include more women in major committees in order to provide gender balance, social justice, and inspiration to young women to enter careers in science. This reflects more general arguments about the barriers to women's advancement and the need for women to 'Lean in' to leadership positions (e.g. Lean In). \nThe outcome document of the Rio+20 Conference on Sustainable Development - the Future we Want - recognized the need to remove barriers to the full and equal participation of women in decision making and management and the need to increase women in leadership positions. A report prepared by UN Women, the Mary Robinson Foundation - Climate Justice, the Global Gender and Climate Alliance and the UNFCCC recognizes the structural inequalities that impede the representation of women in climate science, negotiations and policies and recommends greater gender balance in the UNFCCC and national delegations. The report argues that the 'challenges of climate change cannot be solved without empowering women' and that women have been marginalized in international negotiations. It reports data that show weak representation of women in the institutions of the UNFCCC including the Adaptation Committee (25%), the GEF Council (19%) and the Expert Group (15%) and that overall women constitute less than 20% of delegation heads and less than 30% of delegation members at UNFCCC conferences.\n\nA call for international science to pay greater attention to the inclusion of women scholars was made by Kate Raworth on Twitter and then in her article \"Must the Anthropocene be the Manthropocene?\" She pointed out that the working group of 36 scientists and scholars who convened in Berlin in 2014 to begin assessing evidence humanity was entering a new epoch, the Anthropocene, was composed almost entirely of men. She stated: \"Leading scientists may have the intellect to recognize that our planetary era is dominated by human activity, but they still seem oblivious to the fact that their own intellectual deliberations are bizarrely dominated by white northern male voices\".\n\nThere are a variety of ways to identify women who have made major contributions to climate change. The first is the list of authors of the high level international assessments for the UN and other organizations such as the Intergovernmental Panel on Climate Change, and the UN Framework Convention on Climate Change (UNFCCC). The second is to examine women who have been invited to join the editorial boards of climate change refereed journals. A third is to look at the membership of the global change committees of the International Council for Science (ICSU). And a fourth is to recognize women that are members of their National Academy of Sciences who work on climate change. Many of them are IPCC or other report authors, and also members of ICSU committees, members of their National Academy and other marks of accomplishment.\n\n\n\n\n"}
