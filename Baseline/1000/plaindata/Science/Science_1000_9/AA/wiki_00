{"id": "47313457", "url": "https://en.wikipedia.org/wiki?curid=47313457", "title": "Aperiodic frequency", "text": "Aperiodic frequency\n\nAperiodic frequency is the rate of incidence or occurrence of non-cyclic phenomena, including random processes such as radioactive decay. It is expressed in units of measurement of reciprocal seconds (s) or, in the case of radioactivity, becquerels. \n\nIt is defined as a ratio, \"f\" = \"N\"/\"T\", involving the number of times an event happened (\"N\") during a given time duration (\"T\"); it is a physical quantity of type temporal rate.\n\n"}
{"id": "1137568", "url": "https://en.wikipedia.org/wiki?curid=1137568", "title": "Artificial gravity", "text": "Artificial gravity\n\nArtificial gravity (sometimes referred to as pseudogravity) is the creation of an inertial force that mimics the effects of a gravitational force, usually by rotation. \nArtificial gravity, or rotational gravity, is thus the appearance of a centrifugal force in a rotating frame of reference (the transmission of centripetal acceleration via normal force in the non-rotating frame of reference), as opposed to the force experienced in linear acceleration, which by the equivalence principle is indistinguishable from gravity.\nIn a more general sense, \"artificial gravity\" may also refer to the effect of linear acceleration, e.g. by means of a rocket engine.. \n\nRotational simulated gravity has been used in simulations to help astronauts train for extreme conditions. \nRotational simulated gravity has been proposed as a solution in manned spaceflight to the adverse health effects caused by prolonged weightlessness. \nHowever, there are no current practical outer space applications of artificial gravity for humans due to concerns about the size and cost of a spacecraft necessary to produce a useful centripetal acceleration comparable to the gravitational field strength on Earth (g).\n\nArtificial gravity can be created using a centripetal force. A centripetal force directed towards the center of the turn is required for any object to move in a circular path. \nIn the context of a rotating space station it is the normal force provided by spacecraft's hull that acts as centripetal force. Thus, the \"gravity\" force felt by an object the centrifugal force perceived in the rotating frame of reference as pointing \"downwards\" towards the hull.\nIn accordance with Newton's Third Law the value of little \"g\" (the perceived \"downward\" acceleration) is equal in magnitude and opposite in direction\nto the centripetal acceleration.\n\nFrom the point of view of people rotating with the habitat, artificial gravity by rotation behaves in some ways similarly to normal gravity but with the following differences:\n\nThis form of artificial gravity has additional engineering issues:\n\nThe engineering challenges of creating a rotating spacecraft are comparatively modest to any other proposed approach. Theoretical spacecraft designs using artificial gravity have a great number of variants with intrinsic problems and advantages. The formula for the centripetal force implies that the radius of rotation grows with the square of the rotating spacecraft period, so a doubling of the period requires a fourfold increase in the radius of rotation. For example, to produce standard gravity, = with a rotating spacecraft period of 15 s, the radius of rotation would have to be , while a period of 30 s would require it to be . To reduce mass, the support along the diameter could consist of nothing but a cable connecting two sections of the spaceship. Among the possible solutions include a habitat module and a counterweight consisting of every other part of the spacecraft, alternatively two habitatable modules of similar weight could be attached to one another.\n\nWhatever design is chosen, it would be necessary for the spacecraft to possess some means to quickly transfer ballast from one section to another, otherwise even small shifts in mass could cause a substantial shift in the spacecraft's axis, which would result in a dangerous \"wobble.\" One possible solution would be to engineer the spacecraft's plumbing system to serve this purpose, using drinking water and/or waste water as the ballast. \n\nIt is not yet known whether exposure to high gravity for short periods of time is as beneficial to health as continuous exposure to normal gravity. It is also not known how effective low levels of gravity would be at countering the adverse effects on health of weightlessness. Artificial gravity at 0.1\"g\" and a rotating spacecraft period of 30 s would require a radius of only . Likewise, at a radius of 10 m, a period of just over 6 s would be required to produce standard gravity (at the hips; gravity would be 11% higher at the feet), while 4.5 s would produce 2\"g\". If brief exposure to high gravity can negate the harmful effects of weightlessness, then a small centrifuge could be used as an exercise area.\n\nThe Gemini 11 mission attempted to produce artificial gravity by rotating the capsule around the Agena Target Vehicle to which it was attached by a 36-meter tether. They were able to generate a small amount of artificial gravity, about 0.00015 g, by firing their side thrusters to slowly rotate the combined craft like a slow-motion pair of bolas.\nThe resultant force was too small to be felt by either astronaut, but objects were observed moving towards the \"floor\" of the capsule.\n\nIt should be pointed out that the Gemini 8 mission achieved artificial gravity for a few minutes. This, however, was due to an accident. The acceleration forces upon the crew were so high (~ 4g's) that the mission had to be urgently terminated.\n\nArtificial gravity has been suggested as a solution to the various health risks associated with spaceflight. In 1964, the Soviet space program believed that a human could not survive more than 14 days in space due to a fear that the heart and blood vessels would be unable to adapt to the weightless conditions. This fear was eventually discovered to be unfounded as spaceflights have now lasted up to 438 consecutive days, with missions aboard the international space station commonly lasting 6 months. However, the question of human safety in space did launch an investigation into the physical effects of prolonged exposure to weightlessness. In June 1991, a Spacelab Life Sciences 1 flight performed 18 experiments on two men and two women over a period of nine days. In an environment without gravity, it was concluded that the response of white blood cells and muscle mass decreased. Additionally, within the first 24 hours spent in a weightless environment, blood volume decreased by 10%. Upon return to earth, the effects of prolonged weightlessness continue to affect the human body as fluids pool back to the lower body, the heart rate rises, a drop in blood pressure occurs and there is a reduced ability to exercise.\n\nArtificial gravity, due to its ability to mimic the behavior of gravity on the human body has been suggested as one of the most encompassing manners of combating the physical effects inherent with weightless environments. Other measures that have been suggested as symptomatic treatments include exercise, diet and penguin suits. However, criticism of those methods lays in the fact that they do not fully eliminate the health problems and require a variety of solutions to address all issues. Artificial gravity, in contrast, would remove the weightlessness inherent with space travel. By implementing artificial gravity, space travelers would never have to experience weightlessness or the associated side effects. Especially in a modern-day six-month journey to Mars, exposure to artificial gravity is suggested in either a continuous or intermittent form to prevent extreme debilitation to the astronauts during travel.\n\nA number of proposals have incorporated artificial gravity into their design:\n\n\nSome of the reasons that artificial gravity remains unused today in spaceflight trace back to the problems inherent in implementation. One of the realistic methods of creating artificial gravity is a centripetal force pulling a person towards a relative floor. In that model, however, issues arise in the size of the spacecraft. As expressed by John Page and Matthew Francis, the smaller a spacecraft, the more rapid the rotation that is required. As such, to simulate gravity, it would be more ideal to utilize a larger spacecraft that rotates very slowly. The requirements on size in comparison to rotation are due to the different magnitude of forces the body can experience if the rotation is too tight. Additionally, questions remain as to what the best way to initially set the rotating motion in place without disturbing the stability of the whole spacecraft's orbit. At the moment, there is not a ship massive enough to meet the rotation requirements, and the costs associated with building, maintaining, and launching such a craft are extensive.\n\nIn general, with the limited health effects present in shorter spaceflights, as well as the high cost of research, application of artificial gravity is often stunted and sporadic.\n\nSeveral science fiction novels, films and series have featured artificial gravity production. In the movie , a rotating centrifuge in the \"Discovery\" spacecraft provides artificial gravity. \nIn the novel \"The Martian\", the \"Hermes\" spacecraft achieves artificial gravity by design; it employs a ringed structure, at whose periphery forces around 40% of Earth's gravity are experienced, similar to Mars's gravity. The movie \"Interstellar\" features a spacecraft called the \"Endurance\" that can rotate on its center axis to create artificial gravity, controlled by retro thrusters on the ship.\n\nHigh-G training is done by aviators and astronauts who are subject to high levels of acceleration ('G') in large-radius centrifuges. It is designed to prevent a \"g-induced loss Of consciousness\" (abbreviated G-LOC), a situation when \"g\"-forces move the blood away from the brain to the extent that consciousness is lost. Incidents of acceleration-induced loss of consciousness have caused fatal accidents in aircraft capable of sustaining high-\"g\" for considerable periods.\n\nIn amusement parks, pendulum rides and centrifuges provide rotational force. Roller coasters also do, whenever they go over dips, humps, or loops. When going over a hill, time in which zero or negative gravity is felt is called air time, or \"airtime\", which can be divided into \"floater air time\" (for zero gravity) and \"ejector air time\" (for negative gravity).\n\nLinear acceleration, even at a low level, can provide sufficient g-force to provide useful benefits. A spacecraft under constant acceleration in a straight line would give the appearance of a gravitational pull in the direction opposite of the acceleration. This \"pull\" that would cause a loose object to \"fall\" towards the hull of the spacecraft is actually a manifestation of the inertia of the objects inside the spacecraft, in accordance with Newton's first law. \nFurther, the \"gravity\" felt by an object pressed against the hull of the spacecraft is simply the reaction force of the object on the hull reacting to the acceleration force of the hull on the object, in accordance with Newton's Third Law and somewhat similar to the effect on an object pressed against the hull of a spacecraft rotating as outlined above. Unlike an artificial gravity based on rotation, linear acceleration gives the appearance of a gravity field which is both uniform throughout the spacecraft and without the disadvantage of additional fictitious forces.\n\nSome chemical reaction rockets can at least temporarily provide enough acceleration to overcome Earth's gravity and could thus provide linear acceleration to emulate Earth's g-force. However, since all such rockets provide this acceleration by expelling reaction mass such an acceleration would only be temporary, until the limited supply of rocket fuel had been spent.\n\nNevertheless, constant linear acceleration is desirable since in addition to providing artificial gravity it could theoretically provide relatively short flight times around the solar system. For example, if a propulsion technique able to support 1\"g\" of acceleration continuously were available, a spaceship accelerating (and then decelerating for the second half of the journey) at 1\"g\" would reach Mars within a few days. Similarly, a hypothetical space travel using constant acceleration of 1\"g\" for one year would reach relativistic speeds and allow for a round trip to the nearest star, Proxima Centauri.\n\nAs such, low-impulse but long-term linear acceleration has been proposed for various interplanetary missions. For example, even heavy (100 ton) cargo payloads to Mars could be transported to Mars in and retain approximately 55 percent of the LEO vehicle mass upon arrival into a Mars orbit, providing a low-gravity gradient to the spacecraft during the entire journey.\n\nA propulsion system with a very high specific impulse (that is, good efficiency in the use of reaction mass that must be carried along and used for propulsion on the journey) could accelerate more slowly producing useful levels of artificial gravity for long periods of time. A variety of electric propulsion systems provide examples. Two examples of this long-duration, low-thrust, high-impulse propulsion that have either been practically used on spacecraft or are planned in for near-term in-space use are Hall effect thrusters and Variable Specific Impulse Magnetoplasma Rockets (VASIMR). Both provide very high specific impulse but relatively low thrust, compared to the more typical chemical reaction rockets. They are thus ideally suited for long-duration firings which would provide limited amounts of, but long-term, milli-g levels of artificial gravity in spacecraft.\n\nIn a number of science fiction plots, acceleration is used to produce artificial gravity for interstellar spacecraft, propelled by as yet theoretical or hypothetical means.\n\nThis effect of linear acceleration is well understood, and is routinely used for 0\"g\" cryogenic fluid management for post-launch (subsequent) in-space firings of upper stage rockets.\n\nRoller coasters, especially launched roller coasters or those that rely on electromagnetic propulsion, can provide linear acceleration \"gravity\", and so can relatively high acceleration vehicles, such as sports cars. Linear acceleration can be used to provide air-time on roller coasters and other thrill rides.\n\nA similar effect to gravity can be created through diamagnetism. It requires magnets with extremely powerful magnetic fields. Such devices have been able to levitate at most a small mouse, producing a 1 \"g\" field to cancel that of the Earth's. \n\nSufficiently powerful magnets require either expensive cryogenics to keep them superconductive or several megawatts of power.\n\nWith such extremely strong magnetic fields, safety for use with humans is unclear. In addition, it would involve avoiding any ferromagnetic or paramagnetic materials near the strong magnetic field that is required for diamagnetism to be evident.\n\nFacilities using diamagnetism may prove workable for laboratories simulating low gravity conditions here on Earth. A mouse has been levitated against Earth's gravity, creating a condition similar to microgravity. Lower forces may also be generated to simulate a condition similar to lunar or Martian gravity with small model organisms.\n\n\"Weightless Wonder\" is the nickname for the NASA aircraft that flies parabolic trajectories and briefly provides a nearly weightless environment in which to train astronauts, conduct research, and film motion pictures. The parabolic trajectory creates a vertical linear acceleration which matches that of gravity, giving zero-g for a short time, usually 20–30 seconds, followed by approximately 1.8g for a similar period. The nickname Vomit Comet is also used to refer to motion sickness that is often experienced by the aircraft passengers during these parabolic trajectories. Such reduced gravity aircraft are nowadays operated by several organizations worldwide.\n\nA Neutral Buoyancy Laboratory (NBL) is an astronaut training facility, such as the Sonny Carter Training Facility at the NASA Johnson Space Center in Houston, Texas. The NBL is a large indoor pool of water, the largest in the world, in which astronauts may perform simulated EVA tasks in preparation for space missions. The NBL contains full-sized mock-ups of the Space Shuttle cargo bay, flight payloads, and the International Space Station (ISS).\n\nThe principle of neutral buoyancy is used to simulate the weightless environment of space. The suited astronauts are lowered into the pool using an overhead crane and their weight is adjusted by support divers so that they experience no buoyant force and no rotational moment about their center of mass. The suits worn in the NBL are down-rated from fully flight-rated EMU suits like those in use on the space shuttle and International Space Station.\n\nThe NBL tank is in length, wide, and deep, and contains 6.2 million gallons (23.5 million litres) of water. Divers breathe nitrox while working in the tank.\n\nNeutral buoyancy in a pool is not weightlessness, since the balance organs in the inner ear still sense the up-down direction of gravity. Also, there is a significant amount of drag presented by water. Generally, drag effects are minimized by doing tasks slowly in the water. Another difference between neutral buoyancy simulation in a pool and actual EVA during spaceflight is that the temperature of the pool and the lighting conditions are maintained constant.\n\nIn science fiction, artificial gravity (or cancellation of gravity) or \"paragravity\" is sometimes present in spacecraft that are neither rotating nor accelerating. At present, there is no confirmed technique that can simulate gravity other than actual mass or acceleration. There have been many claims over the years of such a device. Eugene Podkletnov, a Russian engineer, has claimed since the early 1990s to have made such a device consisting of a spinning superconductor producing a powerful \"gravitomagnetic field\", but there has been no verification or even negative results from third parties. In 2006, a research group funded by ESA claimed to have created a similar device that demonstrated positive results for the production of gravitomagnetism, although it produced only 0.0001\"g\". This result has not been replicated. String theory predicts that gravity and electromagnetism unify in hidden dimensions and that extremely short photons can enter those dimensions.\n\n"}
{"id": "664", "url": "https://en.wikipedia.org/wiki?curid=664", "title": "Astronaut", "text": "Astronaut\n\nAn astronaut or cosmonaut is a person trained by a human spaceflight program to command, pilot, or serve as a crew member of a spacecraft. Although generally reserved for professional space travelers, the terms are sometimes applied to anyone who travels into space, including scientists, politicians, journalists, and tourists.\n\nUntil 2002, astronauts were sponsored and trained exclusively by governments, either by the military or by civilian space agencies. With the suborbital flight of the privately funded SpaceShipOne in 2004, a new category of astronaut was created: the commercial astronaut.\n\nThe criteria for what constitutes human spaceflight vary. The Fédération Aéronautique Internationale (FAI) Sporting Code for astronautics recognizes only flights that exceed an altitude of. In the United States, professional, military, and commercial astronauts who travel above an altitude of are awarded astronaut wings.\n\n, a total of 552 people from 36 countries have reached or more in altitude, of which 549 reached low Earth orbit or beyond.\nOf these, 24 people have traveled beyond low Earth orbit, either to lunar orbit, the lunar surface, or, in one case, a loop around the Moon. Three of the 24–Jim Lovell, John Young and Eugene Cernan–did so twice. The three current astronauts who have flown without reaching low Earth orbit are spaceplane pilots Joe Walker, Mike Melvill, and Brian Binnie, who participated in suborbital missions.\n\n, under the U.S. definition, 558 people qualify as having reached space, above altitude. Of eight X-15 pilots who exceeded in altitude, only one exceeded 100 kilometers (about 62 miles). Space travelers have spent over 41,790 man-days (114.5 man-years) in space, including over 100 astronaut-days of spacewalks. , the man with the longest cumulative time in space is Gennady Padalka, who has spent 879 days in space. Peggy A. Whitson holds the record for the most time in space by a woman, 377 days.\n\nIn 1959, when both the United States and Soviet Union were planning, but had yet to launch humans into space, NASA Administrator T. Keith Glennan and his Deputy Administrator, Dr. Hugh Dryden, discussed whether spacecraft crew members should be called \"astronauts\" or \"cosmonauts\". Dryden preferred \"cosmonaut\", on the grounds that flights would occur in the \"cosmos\" (near space), while the \"astro\" prefix suggested flight to the stars. Most NASA Space Task Group members preferred \"astronaut\", which survived by common usage as the preferred American term. When the Soviet Union launched the first man into space, Yuri Gagarin in 1961, they chose a term which anglicizes to \"cosmonaut\".\n\nIn English-speaking nations, a professional space traveler is called an \"astronaut\". The term derives from the Greek words \"ástron\" (ἄστρον), meaning \"star\", and \"nautes\" (ναύτης), meaning \"sailor\". The first known use of the term \"astronaut\" in the modern sense was by Neil R. Jones in his 1930 short story \"The Death's Head Meteor\". The word itself had been known earlier; for example, in Percy Greg's 1880 book \"Across the Zodiac\", \"astronaut\" referred to a spacecraft. In \"Les Navigateurs de l'Infini\" (1925) by J.-H. Rosny aîné, the word \"astronautique\" (astronautic) was used. The word may have been inspired by \"aeronaut\", an older term for an air traveler first applied in 1784 to balloonists. An early use of \"astronaut\" in a non-fiction publication is Eric Frank Russell's poem \"The Astronaut\", appearing in the November 1934 \"Bulletin of the British Interplanetary Society\".\n\nThe first known formal use of the term astronautics in the scientific community was the establishment of the annual International Astronautical Congress in 1950, and the subsequent founding of the International Astronautical Federation the following year.\n\nNASA applies the term astronaut to any crew member aboard NASA spacecraft bound for Earth orbit or beyond. NASA also uses the term as a title for those selected to join its Astronaut Corps. The European Space Agency similarly uses the term astronaut for members of its Astronaut Corps.\n\nBy convention, an astronaut employed by the Russian Federal Space Agency (or its Soviet predecessor) is called a \"cosmonaut\" in English texts. The word is an anglicisation of the Russian word \"kosmonavt\" (, ), one who works in space outside the Earth's atmosphere, a space traveler, which derives from the Greek words \"kosmos\" (κόσμος), meaning \"universe\", and \"nautes\" (ναύτης), meaning \"sailor\". Other countries of the former Eastern Bloc use variations of the Russian word \"kosmonavt\", such as the Polish \"kosmonauta\".\n\nCoinage of the term \"kosmonavt\" has been credited to Soviet aeronautics pioneer Mikhail Tikhonravov (1900–1974). The first cosmonaut was Soviet Air Force pilot Yuri Gagarin, also the first person in space. Valentina Tereshkova, a Russian factory worker, was the first woman in space, as well as the first civilian among the Soviet cosmonaut or NASA astronaut corps to make a spaceflight. On March 14, 1995, Norman Thagard became the first American to ride to space on board a Russian launch vehicle, and thus became the first \"American cosmonaut\".\n\n\"Yǔ háng yuán\" (, \"Space-universe navigating personnel\") is used for astronauts and cosmonauts in general, while \"Hángtiān yuán\" (, \"navigating outer space personnel\") is used for Chinese astronauts. Here, \"Hángtiān\" () is strictly defined as the navigation of outer space within the local star system, i.e. Solar system. The phrase \"tài kōng rén\" (, \"spaceman\") is often used in Hong Kong and Taiwan.\n\nThe term \"taikonaut\" is used by some English-language news media organizations for professional space travelers from China. The word has featured in the Longman and Oxford English dictionaries, the latter of which describes it as \"a hybrid of the Chinese term \"taikong\" (space) and the Greek \"naut\" (sailor)\"; the term became more common in 2003 when China sent its first astronaut Yang Liwei into space aboard the \"Shenzhou 5\" spacecraft. This is the term used by Xinhua News Agency in the English version of the Chinese \"People's Daily\" since the advent of the Chinese space program. The origin of the term is unclear; as early as May 1998, Chiew Lee Yih () from Malaysia, used it in newsgroups.\n\nWith the rise of space tourism, NASA and the Russian Federal Space Agency agreed to use the term \"spaceflight participant\" to distinguish those space travelers from professional astronauts on missions coordinated by those two agencies.\n\nWhile no nation other than the Russian Federation (and previously the former Soviet Union), the United States, and China have launched a manned spacecraft, several other nations have sent people into space in cooperation with one of these countries. Inspired partly by these missions, other synonyms for astronaut have entered occasional English usage. For example, the term \"spationaut\" (French spelling: \"spationaute\") is sometimes used to describe French space travelers, from the Latin word \"spatium\" for \"space\", the Malay term \"angkasawan\" was used to describe participants in the Angkasawan program, and the Indian Space Research Organisation hope to launch a spacecraft in 2022 that would carry \"vyomanauts\", coined from the Sanskrit word for space. In Finland, the NASA astronaut Timothy Kopra, a Finnish American, has sometimes been referred to as \"sisunautti\", from the Finnish word \"sisu\".\n\nThe first human in space was Soviet Yuri Gagarin, who was launched on April 12, 1961, aboard Vostok 1 and orbited around the Earth for 108 minutes. The first woman in space was Soviet Valentina Tereshkova, who launched on June 16, 1963, aboard Vostok 6 and orbited Earth for almost three days.\n\nAlan Shepard became the first American and second person in space on May 5, 1961, on a 15-minute sub-orbital flight. The first American to orbit the Earth was John Glenn, aboard Friendship 7 on February 20, 1962. The first American woman in space was Sally Ride, during Space Shuttle Challenger's mission STS-7, on June 18, 1983. In 1992 Mae Jemison became the first African American woman to travel in space aboard STS-47.\n\nCosmonaut Alexei Leonov was the first person to conduct an extravehicular activity (EVA), (commonly called a \"spacewalk\"), on March 18, 1965, on the Soviet Union's Voskhod 2 mission. This was followed two and a half months later by astronaut Ed White who made the first American EVA on NASA's Gemini 4 mission.\n\nThe first manned mission to orbit the Moon, \"Apollo 8\", included American William Anders who was born in Hong Kong, making him the first Asian-born astronaut in 1968.\n\nThe Soviet Union, through its Intercosmos program, allowed people from other \"socialist\" (i.e. Warsaw Pact and other Soviet-allied) countries to fly on its missions, with the notable exception of France participating in Soyuz TM-7. An example is Czechoslovak Vladimír Remek, the first cosmonaut from a country other than the Soviet Union or the United States, who flew to space in 1978 on a Soyuz-U rocket.\n\nOn July 23, 1980, Pham Tuan of Vietnam became the first Asian in space when he flew aboard Soyuz 37. Also in 1980, Cuban Arnaldo Tamayo Méndez became the first person of Hispanic and black African descent to fly in space, and in 1983, Guion Bluford became the first African American to fly into space. In April 1985, Taylor Wang became the first ethnic Chinese person in space. The first person born in Africa to fly in space was Patrick Baudry (France), in 1985. In 1985, Saudi Arabian Prince Sultan Bin Salman Bin AbdulAziz Al-Saud became the first Arab Muslim astronaut in space. In 1988, Abdul Ahad Mohmand became the first Afghan to reach space, spending nine days aboard the Mir space station.\n\nWith the increase of seats on the Space Shuttle, the U.S. began taking international astronauts. In 1983, Ulf Merbold of West Germany became the first non-US citizen to fly in a US spacecraft. In 1984, Marc Garneau became the first of 8 Canadian astronauts to fly in space (through 2010).\nIn 1985, Rodolfo Neri Vela became the first Mexican-born person in space. In 1991, Helen Sharman became the first Briton to fly in space.\nIn 2002, Mark Shuttleworth became the first citizen of an African country to fly in space, as a paying spaceflight participant. In 2003, Ilan Ramon became the first Israeli to fly in space, although he died during a re-entry accident.\n\nOn October 15, 2003, Yang Liwei became China's first astronaut on the Shenzhou 5 spacecraft.\n\nThe youngest person to fly in space is Gherman Titov, who was 25 years old when he flew Vostok 2. (Titov was also the first person to suffer space sickness).\nThe oldest person who has flown in space is John Glenn, who was 77 when he flew on STS-95.\n\n438 days is the longest time spent in space, by Russian Valeri Polyakov.\nAs of 2006, the most spaceflights by an individual astronaut is seven, a record held by both Jerry L. Ross and Franklin Chang-Diaz. The farthest distance from Earth an astronaut has traveled was , when Jim Lovell, Jack Swigert, and Fred Haise went around the Moon during the Apollo 13 emergency.\n\nThe first civilian in space was Valentina Tereshkova aboard Vostok 6 (she also became the first woman in space on that mission).\nTereshkova was only honorarily inducted into the USSR's Air Force, which did not accept female pilots at that time. A month later, Joseph Albert Walker became the first American civilian in space when his X-15 Flight 90 crossed the line, qualifying him by the international definition of spaceflight. Walker had joined the US Army Air Force but was not a member during his flight. \nThe first people in space who had never been a member of any country's armed forces were both Konstantin Feoktistov and Boris Yegorov aboard Voskhod 1.\n\nThe first non-governmental space traveler was Byron K. Lichtenberg, a researcher from the Massachusetts Institute of Technology who flew on STS-9 in 1983. In December 1990, Toyohiro Akiyama became the first paying space traveler as a reporter for Tokyo Broadcasting System, a visit to Mir as part of an estimated $12 million (USD) deal with a Japanese TV station, although at the time, the term used to refer to Akiyama was \"Research Cosmonaut\". Akiyama suffered severe space sickness during his mission, which affected his productivity.\n\nThe first self-funded space tourist was Dennis Tito on board the Russian spacecraft Soyuz TM-3 on April 28, 2001.\n\nThe first person to fly on an entirely privately funded mission was Mike Melvill, piloting SpaceShipOne flight 15P on a suborbital journey, although he was a test pilot employed by Scaled Composites and not an actual paying space tourist. Seven others have paid the Russian Space Agency to fly into space:\n\n\nThe first NASA astronauts were selected for training in 1959. Early in the space program, military jet test piloting and engineering training were often cited as prerequisites for selection as an astronaut at NASA, although neither John Glenn nor Scott Carpenter (of the Mercury Seven) had any university degree, in engineering or any other discipline at the time of their selection. Selection was initially limited to military pilots. The earliest astronauts for both America and the USSR tended to be jet fighter pilots, and were often test pilots.\n\nOnce selected, NASA astronauts go through twenty months of training in a variety of areas, including training for extravehicular activity in a facility such as NASA's Neutral Buoyancy Laboratory. Astronauts-in-training (astronaut candidates) may also experience short periods of weightlessness (microgravity) in an aircraft called the \"Vomit Comet,\" the nickname given to a pair of modified KC-135s (retired in 2000 and 2004, respectively, and replaced in 2005 with a C-9) which perform parabolic flights. Astronauts are also required to accumulate a number of flight hours in high-performance jet aircraft. This is mostly done in T-38 jet aircraft out of Ellington Field, due to its proximity to the Johnson Space Center. Ellington Field is also where the Shuttle Training Aircraft is maintained and developed, although most flights of the aircraft are conducted from Edwards Air Force Base.\n\nAstronauts is training must learn how to control and fly the Space Shuttle and, it is vital that they are familiar with the International Space Station so they know what they must do when they get there.\n\n\n\n\nMission Specialist Educators, or \"Educator Astronauts\", were first selected in 2004, and as of 2007, there are three NASA Educator astronauts: Joseph M. Acaba, Richard R. Arnold, and Dorothy Metcalf-Lindenburger.\nBarbara Morgan, selected as back-up teacher to Christa McAuliffe in 1985, is considered to be the first Educator astronaut by the media, but she trained as a mission specialist.\nThe Educator Astronaut program is a successor to the Teacher in Space program from the 1980s.\n\nAstronauts are susceptible to a variety of health risks including decompression sickness, barotrauma, immunodeficiencies, loss of bone and muscle, loss of eyesight, orthostatic intolerance, sleep disturbances, and radiation injury. A variety of large scale medical studies are being conducted in space via the National Space and Biomedical Research Institute (NSBRI) to address these issues. Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity Study in which astronauts (including former ISS commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts to diagnose and potentially treat hundreds of medical conditions in space. This study's techniques are now being applied to cover professional and Olympic sports injuries as well as ultrasound performed by non-expert operators in medical and high school students. It is anticipated that remote guided ultrasound will have application on Earth in emergency and rural care situations, where access to a trained physician is often rare.\n\nA 2006 Space Shuttle experiment found that \"Salmonella typhimurium\", a bacterium that can cause food poisoning, became more virulent when cultivated in space. More recently, in 2017, bacteria were found to be more resistant to antibiotics and to thrive in the near-weightlessness of space. Microorganisms have been observed to survive the vacuum of outer space.\n\nOn December 31, 2012, a NASA-supported study reported that manned spaceflight may harm the brain and accelerate the onset of Alzheimer's disease.\n\nIn October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.\n\nOver the last decade, flight surgeons and scientists at NASA have seen a pattern of vision problems in astronauts on long-duration space missions. The syndrome, known as visual impairment intracranial pressure (VIIP), has been reported in nearly two-thirds of space explorers after long periods spent aboard the International Space Station (ISS).\n\nOn November 2, 2017, scientists reported that significant changes in the position and structure of the brain have been found in astronauts who have taken trips in space, based on MRI studies. Astronauts who took longer space trips were associated with greater brain changes.\n\nBeing in space can be physiologically deconditioning on the body. It can affect the otolith organs and adaptive capabilities of the central nervous system. Zero gravity and cosmic rays can cause many implications for astronauts.\n\nIn October 2018, NASA-funded researchers found that lengthy journeys into outer space, including travel to the planet Mars, may substantially damage the gastrointestinal tissues of astronauts. The studies support earlier work that found such journeys could significantly damage the brains of astronauts, and age them prematurely.\n\nResearchers in 2018 reported, after detecting the presence on the International Space Station (ISS) of five \"Enterobacter bugandensis\" bacterial strains, none pathogenic to humans, that microorganisms on ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts.\n\nAn astronaut on the International Space Station requires about 0.83 kilograms (1.83 pounds) weight of food inclusive of food packaging per meal each day. (The packaging for each meal weighs around 0.12 kilograms - 0.27 pounds) Longer-duration missions require more food.\n\nShuttle astronauts worked with nutritionists to select menus that appeal to their individual tastes. Five months before flight, menus are selected and analyzed for nutritional content by the shuttle dietician. Foods are tested to see how they will react in a reduced gravity environment. Caloric requirements are determined using a basal energy expenditure (BEE) formula.\nOn Earth, the average American uses about 35 gallons (132 liters) of water every day. On board the ISS astronauts limit water use to only about three gallons (11 liters) per day.\n\nIn Russia, cosmonauts are awarded Pilot-Cosmonaut of the Russian Federation upon completion of their missions, often accompanied with the award of Hero of the Russian Federation. This follows the practice established in the USSR where cosmonauts were usually awarded the title Hero of the Soviet Union.\n\nAt NASA, those who complete astronaut candidate training receive a silver lapel pin. Once they have flown in space, they receive a gold pin. U.S. astronauts who also have active-duty military status receive a special qualification badge, known as the Astronaut Badge, after participation on a spaceflight. The United States Air Force also presents an Astronaut Badge to its pilots who exceed in altitude.\n\nEighteen astronauts (fourteen men and four women) have lost their lives during four space flights. By nationality, thirteen were American (including one born in India), four were Russian (Soviet Union), and one was Israeli.\n\nEleven people (all men) have lost their lives training for spaceflight: eight Americans and three Russians. Six of these were in crashes of training jet aircraft, one drowned during water recovery training, and four were due to fires in pure oxygen environments.\n\nThe Space Mirror Memorial, which stands on the grounds of the John F. Kennedy Space Center Visitor Complex, commemorates the lives of the men and women who have died during spaceflight and during training in the space programs of the United States. In addition to twenty NASA career astronauts, the memorial includes the names of a U.S. Air Force X-15 test pilot, a U.S. Air Force officer who died while training for a then-classified military space program, and a civilian spaceflight participant.\n\n"}
{"id": "4483284", "url": "https://en.wikipedia.org/wiki?curid=4483284", "title": "Body memory", "text": "Body memory\n\nBody memory (BM) is a hypothesis that the body itself is capable of storing memories, as opposed to only the brain. While experiments have demonstrated the possibility of cellular memory there are currently no known means by which tissues other than the brain would be capable of storing memories.\n\nModern usage of BM tends to frame it exclusively in the context of traumatic memory and ways in which the body responds to recall of a memory. In this regard, it has become relevant in treatment for PTSD.\n\nPeter Levine calls BM \"implicit memory\" or more specifically procedural memory, things that the body is capable of doing automatically and not in one's consciousness. He clarifies 3 types of BM and frames his work in terms of traumatic memory consequence and resolution:\n\nNicola Diamond elaborates on the opinion of philosopher Merleau-Ponty and asserts that BM is formed by doing. Whether practicing a bodily activity or forming a reaction to a traumatic memory.\n\nEdward Casey speaks of BM as, \"memory intrinsic to the body, how we remember by and through the body\", rather than what is remembered about the body.\n\nThomas Fuchs defines 6 different types of BM: procedural, situational, intercorporeal, incorporative, pain, and traumatic memory. He notes that they are not strictly separable from one another but \"derived from different dimensions of bodily experience. Michelle Summa further refines this definition as an implicit memory. A pre-thematic, operative consciousness of the past expressed through the body.\n\nAntonio Damasio calls these reactions to memories \"somatic markers\" or emotions that are expressed primarily as physical feelings.\n\nThese memories are often associated with phantom pain in a part or parts of the body – the body appearing to remember the past trauma. The idea of body memory is a belief frequently associated with the idea of repressed memories, in which memories of incest or sexual abuse can be retained and recovered through physical sensations. It may also be associated with phantom limb sensation but this is less common.\n\nIn 1993, Susan E. Smith, presented a paper relating the idea of \"Survivor Psychology\" at a false memory syndrome conference, stated about BM that, \"body memories are thought to literally be emotional, kinesthetic, or chemical recordings stored at the cellular level and retrievable by returning to or recreating the chemical, emotional, or kinesthetic conditions under which the memory recordings are filed. She went on in the abstract of the paper, \"one of the most commonly used theories to support the ideology of repressed memories or incest and sexual abuse amnesia is body memories.\" and \"The belief in these pseudoscientific concepts appears to be related to scientific illiteracy, gullibility, and a lack of critical thinking skills and reasoning abilities in both the mental health community and in society at large\"\n\nA 2017 systematic review of cross-disciplinary research in body memory found that the available data neither largely support or refute the claim that memories are stored outside of the brain and more research is needed.\n\nIn the \"Encyclopedia of Phenomenology\" Embree notes that, \"To posit body memory is to open up a Pandora's Box\", and links the idea to physical associations of memory rather than as a memory stored in a bodily manner.\n\nCellular memory (CM) is a parallel hypothesis to BM positing that memories can be stored outside the brain in all cells. The idea that non-brain tissues can have memories is believed by some who have received organ transplants, though this is considered impossible. The author said the stories are intriguing though and may lead to some serious scientific investigation in the future. In his book \"TransplantNation\" Douglas Vincent suggests that atypical newfound memories, thoughts, emotions and preferences after an organ transplant are more suggestive of immunosuppressant drugs and the stress of surgery on perception than of legitimate memory transference. In other words, \"as imaginary as a bad trip on LSD or other psychotropic drug.\"\n\nBiologists at Tufts University have been able to train flatworms despite the loss of the brain and head. This may show memory stored in other parts of the body in some animals. A worm reduced to 1/279th of the original can be regrown within a few weeks and be trained much quicker to head towards light and open space for food, an unnatural behavior for a flatworm. With each head removed training times appear reduced. This may just be a sign of epigenetics showing the appearance of memory.\n\nHowever, in the 1950s and 1960s James McConnell flatworm experiments measured how long it took to learn a maze. McConnell trained some to move around a maze and then chopped them up and fed them to untrained worms. The untrained group learned faster compared to a control that had not been fed trained worms. McConnell believed the experiment indicated cellular memory. The training involved stressing the worms with electric shock. This kind of stress releases persistent hormones and shows no evidence for memory transfer. Similar experiments with mice being trained and being fed to untrained mice showed improved learning. It was not a memory that was transferred but hormone enriched tissue.\n\nIn epigenetics there are various mechanisms for cells to pass on \"memories\" of stressors to their progeny. Strategies include Msn2 nucleo-cytoplasmic shuttling, changes in chromatin, partitioning of anti-stress factors, and damaged macromolecules between mother and daughter cells.\n\nIn adaptive immunity there is a functional CM that enables he immune system to learn to react to pathogens through mechanisms such as, cytoxic memory mediation in bone marrow, innate immune memory in stromal cells, fungal mediation of innate and inherited immunological response, and T and B-cell immune training. In this regard CM is essential for vaccine and immunity research.\n\n"}
{"id": "10914387", "url": "https://en.wikipedia.org/wiki?curid=10914387", "title": "Computational-representational understanding of mind", "text": "Computational-representational understanding of mind\n\nComputational representational understanding of mind (CRUM) is a hypothesis in cognitive science which proposes that thinking is performed by computations operating on representations. This hypothesis assumes that the mind has mental representations analogous to data structures and computational procedures analogous to algorithms, such that computer programs using algorithms applied to data structures can model the mind and its processes.\n\nCRUM takes into consideration several theoretical approaches of understanding human cognition, including logic, rule, concept, analogy, image, and connection based systems. These serve as the representation aspects of CRUM theory which are then acted upon to simulate certain aspects of human cognition, such as the use of rule-based systems in neuroeconomics. \n\nThere is much disagreement on this hypothesis, but CRUM has high regard among some researchers . Philosopher Paul Thagard called it \"the most theoretically and experimentally successful approach to mind ever developed\".\n\n\n"}
{"id": "199772", "url": "https://en.wikipedia.org/wiki?curid=199772", "title": "Concept testing", "text": "Concept testing\n\nConcept testing (to be distinguished from pre-test markets and test markets which may be used at a later stage of product development research) is the process of using surveys (and sometimes qualitative methods) to evaluate consumer acceptance of a new product idea prior to the introduction of a product to the market. It is important not to confuse concept testing with advertising testing, brand testing and packaging testing; as is sometimes done. Concept testing focuses on the basic product idea, without the embellishments and puffery inherent in advertising.\n\nIt is important that the instruments (questionnaires) to test the product have a high quality themselves. Otherwise, results from data gathered surveys may be biased by measurement error. That makes the design of the testing procedure more complex. Empirical tests provide insight into the quality of the questionnaire. This can be done by:\n\nConcept testing in the new product development (NPD) process is the concept generation stage. The concept generation stage of concept testing can take on many forms. Sometimes concepts are generated incidentally, as the result of technological advances. At other times concept generation is deliberate: examples include brain-storming sessions, problem detection surveys and qualitative research. While qualitative research can provide insights into the range of reactions consumers may have, it cannot provide an indication of the likely success of the new concept; this is better left to quantitative concept-test surveys.\n\nIn the early stages of concept testing, a large field of alternative concepts might exist, requiring concept-screening surveys. Concept-screening surveys provide a quick means to narrow the field of options; however they provide little depth of insight and cannot be compared to a normative database due to interactions between concepts. For greater insight and to reach decisions on whether or not pursue further product development, monadic concept-testing surveys must be conducted.\n\nFrequently concept testing surveys are described as either monadic, sequential monadic or comparative. The terms mainly refer to how the concepts are displayed:\n\n1.) Monadic. The concept is evaluated in isolation.\n2.) Sequential monadic. Multiple concepts are evaluated in sequence (often randomized order).\n3.) Comparative. Concepts are shown next to each other. \n4.) Proto-monadic. Concepts are first shown in sequence, and then next to each other.\n\n\"Monadic testing is the recommended method for most concept testing. Interaction effects and biases are avoided. Results from one test can be compared to results from previous monadic tests. A normative database can be constructed.\" However, each has its specific uses and it depends on the research objectives. The decision as to which method to use is best left to experience research professionals to decide, as there are numerous implications in terms of how the results are interpreted.\n\nTraditionally concept-test survey results are compared to 'norms databases'. These are databases of previous new-product concept tests. These must be 'monadic' concept tests, to prevent interaction effects. To be fair, it is important that these databases contain 'new' concept test results, not ratings of old products that consumers are already familiar with; since once consumers become familiar with a product the ratings often drop. Comparing new concept ratings to the ratings for an existing product already on the market would result in an invalid comparison, unless special precautions are taken by researchers to reduce or adjust for this effect quantitatively. Additionally, the concept is usually only compared to norms from the same product category, and the same country.\n\nCompanies that specialize in this area, tend to have developed their own unique systems, each with its own standards. Keeping to these standards consistently is important to preventing contamination of the results.\n\nPerhaps one of the famous concept-test systems is the Nielsen Bases system, which comes in different versions. Other well-known products include Decision Analyst's 'Concept Check', Acupoll's 'Concept Optimizer', Ipsos Innoquest and GFK. Examples of smaller players include Skuuber and Acentric Express Test.\n\nThe simplest approach to determining attribute importance is to ask direct open-ended questions. Alternatively checklists or ratings of the importance of each product attribute may be used.\n\nHowever, various debates have existed over whether or not consumers could be trusted to directly indicate the level of importance of each product attribute. As a result, correlation analysis and various forms of multiple regression have often been used for identifying importance - as an alternative to direct questions.\n\nA complementary technique to concept testing, is conjoint analysis (also referred to as discrete choice modelling). Various forms of conjoint analysis and discrete choice modelling exist. While academics stress the differences between the two, in practice there is often little difference. \nThese techniques estimate the importance of product attributes indirectly, by creating alternative products according to an experimental design, and then using consumer responses to these alternatives (usually ratings of purchase likelihood or choices made between alternatives) to estimate importance. The results are often expressed in the form of a 'simulator' tool which allows clients to test alternative product configurations and pricing.\n\nVolumetric concept testing\n\nVolumetric concept testing falls somewhere between traditional concept testing and pre-test market models (simulated test market models are similar but emphasize greater realism) in terms of the level of complexity. The aim is to provide 'approximate' sales volume forecasts for the new concept prior to launch. They incorporate other variables beyond just input from the concept test survey itself, such as the distribution strategy.\n\nExamples of volumetric forecasting methodologies include 'Acupoll Foresight' and Decision Analyst's 'Conceptor'.\n\nSome models (more properly referred to as 'pre-test market models' or 'simulated test markets') gather additional data from a follow-up product testing survey (especially in the case of consumer packaged goods as repeat purchase rates need to be estimated). They may also include advertisement testing component that aims to assess advertising quality. Some such as Decision Analyst, include discrete choice models / conjoint analysis.\n\n"}
{"id": "36737017", "url": "https://en.wikipedia.org/wiki?curid=36737017", "title": "Crystal Palace School", "text": "Crystal Palace School\n\nCrystal Palace School of Art, Science, and Literature, which opened in 1854, was set up by the Crystal Palace Company as a new enterprise to occupy part of its buildings when it re-erected the Crystal Palace in suburban Sydenham in 1853. Civil engineer and later first director of the Royal College of Music, George Grove was appointed secretary. It was a part of the great movements for educational and social reform of the nineteenth century.\n\nThe main buildings were destroyed by fire in 1936.\n\nThe overwhelming majority of classes were for women:\n\n\n\n\n\n\nClasses for gentlemen were limited to the School of Engineering\n\n\nThe South Tower also contained John Logie Baird's transmitter and studios.\n\nLetter to The Editor, \"The Times\", Monday Dec. 07, 1936. page 10, issue 47551<br>\nThe end of the Crystal Palace brings to mind memories of the School of Engineering which was housed in the South Tower, fortunately still standing firm as a rock. This school, founded by J. W. Wilson, M.I.M.E., an engineer who helped to build the Great Exhibition of 1851, sent many of its students to the four quarters of the globe. The curriculum of the school included mechanical and civil courses and about five of the circular rooms were used. There was a fitting shop, pattern shop, and drawing office. Those in the mechanical section built a 4 h.p. vertical engine which was generally exhibited at the head of the stairs on the south side of the Palace. In the Civil Engineering section we surveyed the whole of the grounds, and drew plans and made estimates for an imaginary railway which extended from one side to the other. This entailed all the necessary estimates for embankments, a cantilever bridge etc. Then there was the Colonial section presided over by a most congenial superintendent, who had no doubt seen much of a pioneer's life and infused his enthusiasm into those who belonged to his section. Concerning the rigidity of the South Tower, I was working in a high storey soon after joining the school when there was a strong wind, and, feeling giddy, I mentioned it to the superintendent. He informed me that it rocked several inches at the top, which made it safer than absolute rigidity.–Mr F. C. Bell, 74, Berners Street, Ipswich.\n\nThe school was a centre for the examinations of the Oxford and Cambridge syndicates.\n\nPolytechnic (United Kingdom)\n\n\n"}
{"id": "31889797", "url": "https://en.wikipedia.org/wiki?curid=31889797", "title": "Cycles of Time", "text": "Cycles of Time\n\nCycles of Time: An Extraordinary New View of the Universe is a science book by mathematical physicist Roger Penrose published by The Bodley Head in 2010. The book outlines Penrose's Conformal Cyclic Cosmology (CCC) model, which is an extension of general relativity but opposed to the widely supported multidimensional string theories and cosmological inflation following the Big Bang.\n\nPenrose examines implications of the Second Law of Thermodynamics and its inevitable march toward a maximum entropy state of the universe. Penrose illustrates entropy in terms of information state phase space (with 1 dimension for every degree of freedom) where particles end up moving through ever larger grains of this phase space from smaller grains over time due to random motion. He disagrees with Stephen Hawking's back-track over whether information is destroyed when matter enters black holes. Such information loss would non-trivially lower total entropy in the universe as the black holes wither away due to Hawking radiation, resulting in a loss in phase space degrees of freedom.\n\nPenrose goes on further to state that over enormous scales of time (beyond 10 years), distance ceases to be meaningful as all mass breaks down into extremely red-shifted photon energy, whereupon time has no influence, and the universe continues to expand without event . This period from Big Bang to infinite expansion Penrose defines as an aeon. The smooth “hairless” infinite oblivion of the previous aeon becomes the low-entropy Big Bang state of the next aeon cycle. Conformal geometry preserves the angles but not the distances of the previous aeon, allowing the new aeon universe to appear quite small at its inception as its phase space starts anew.\n\nPenrose cites concentric rings found in the WMAP cosmic microwave background survey as preliminary evidence for his model, as he predicted black hole collisions from the previous aeon would leave such structures due to ripples of gravitational waves.\n\nMost nonexpert critics (nonscientists) have found the book a challenge to fully comprehend; a few such as \"Kirkus Reviews\" and Doug Johnstone for \"The Scotsman\" appreciate the against the grain innovative ideas Penrose puts forth. Manjit Kumar reviewing for \"The Guardian\" admires the Russian doll geometry play of the CCC concept, framing it as an idea of which M. C. Escher \"would have approved\". Graham Storrs for the \"New York Journal of Books\" concedes that this is not the book that an unambitious lay person should plunge into. The American fiction writer Anthony Doerr in \"The Boston Globe\" writes \"Penrose has never shied away from including mathematics in his texts, and kudos to his publisher for honoring that wish. That said, the second half of \"Cycles of Time\" offers some seriously hard sledding\"; \"If you'll forgive a skiing metaphor, \"Cycles of Time\" is a black diamond of a book.\"\n"}
{"id": "3961885", "url": "https://en.wikipedia.org/wiki?curid=3961885", "title": "Daffy's Elixir", "text": "Daffy's Elixir\n\nDaffy's Elixir (also sometimes known as Daffey's Elixir or Daffye's Elixir) is a name that has been used by several patent medicines over the years. It was originally designed for diseases of the stomach, but was later marketed as a universal cure. It remained a popular remedy in Britain and later the United States of America throughout the eighteenth and nineteenth centuries.\n\nDaffy's Elixir was one of the most popular and frequently advertised patent medicines in Britain during the 18th century. It is reputed to have been invented by clergyman Thomas Daffy rector of Redmile, Leicestershire, in 1647. He named it elixir salutis (lit. \"elixir of health\") and promoted as a generic cure-all.\n\nAn early recipe for \"True Daffy\" from 1700 lists the following ingredients: aniseed, brandy, cochineal, elecampane, fennel seed, jalap, manna, parsley seed, raisin, rhubarb, saffron, senna and spanish liquorice. Chemical analysis has shown this to be a laxative made mostly from alcohol. Other recipes include Guiuacum wood chips, caraway, Salt of Tartar, and scammony.\n\nAccording to an early nineteenth century advertisement it was used for the following ailments: The Stone in Babies and Children; Convulsion fits; Consumption and Bad Digestives; Agues; Piles; Surfeits; Fits of the Mother and Vapours from the Spleen; Green Sickness; Children's Distempers, whether the Worms, Rickets, Stones, Convulsions, Gripes, King's Evil, Joint Evil or any other disorder proceeding from Wind or Crudities; Gout and Rheumatism; Stone or Gravel in the Kidnies; Cholic and Griping of the Bowels; the Phthisic (both as cure and preventative provided always that the patient be moderate in drinking, have a care to prevent taking cold and keep a good diet; Dropsy and Scurvy. The frequent use of the medicine to treat Colic, gripes or fret in horses was deplored in early veterinary manuals.\n\nAfter Daffy's death in 1680 the recipe was left to his daughter Catherine, and his kinsmen Anthony and Daniel who were apothecaries in Nottingham. Anthony Daffy moved to London in the 1690s and began to exploit the product issuing pamphlets such as \"Directions for taking elixir salutis or, the famous purging cordial, known by the name of Daffy's elixir salutis\" [London], [1690?]. His widow Elleanor Daffy continued from about 1693 and (their daughter?) Katharine from about 1707. During the early 18th century the product was advertised widely in the emerging national and local newspapers. The success attracted several counterfeit copies, using inferior alcohol rather than brandy.\n\nThe medicine was later produced by William and Cluer Dicey & Co. of Bow Church yard c.1775 who claimed the sole rights of manufacture of the True Daffy's Elixir, although the recipe was not subject to any patent. Proprietorship was also then claimed by Peter Swinton of Salisbury Court and his son Anthony Daffy Swinton who may have been descended from the inventor. Dicey and Co. and their successors marketed it in the United States of America.\n\nIt then passed to Dicey and Sutton, and later to Messrs W. Sutton & Co. of Enfield Middlesex who continuing to market it throughout the nineteenth century. The use of Daffy's elixir is referred to in Anthony Trollope's novel Barchester Towers, 1857.\n\nDaffy's elixir is also mentioned on several occasions in Thomas Pynchon's novel Mason & Dixon, particularly by Jeremiah Dixon, who attempts to procure large quantities before beginning his surveying trip with Charles Mason. Dixon is warned by Benjamin Franklin, however, that imported Daffy's Elixir is extremely expensive, and he would be better off ordering a customized version from the apothecary. During the same visit, Dixon also orders laudanum, a well-known constipating agent.\n\nDaffy's elixir is also mentioned in the Charles Dickens book, Oliver Twist, Ch. II, where it is referred to as Daffy, in the sentence: 'Why, it's what I'm obliged to keep a little of in the house, to put into the blessed infants' Daffy, when they ain't well, Mr. Bumble,(the Parish Beadle)' replied Mrs. Mann as she opened a corner cupboard, and took down a bottle and glass. 'It's gin. I'll not deceive you, Mr. B. It's gin.'\n\nDaffy’s Elixir is also mentioned in the Works of William Makepeace Thackeray book, Vanity Fair, Chapter XXXVIII A Family In a Small Way, where it is referenced in the sentence ‘..and there found Mrs. Sedley in the act of surreptitiously administering Daffy’s Elixir to the infant.’\n\n\"Daffy’s original elixir salutis, vindicated against all counterfeits, &c. or, An advertisement by mee, Anthony Daffy, of London, citizen and student in physick, By way of vindication of my famous and generally approved cordial drink, (called elixir salutis) from the notoriously false suggestions of one Tho. Witherden of Bear-steed in the county of Kent, Gent. (as pretended;) Jane White, Robert Brooke, apothecary, and Edward Willet; all new upstatrt counterfitors of my elixir, and Ape-like imitators of my long since printed Books and Directions, (some of them, nigh verbatim, or word for word) and that to the jeopardy of many good, (but mis-in-formed) Peoples Healths, and Lives too; as also, from the false pretentions of other more sneaking Cub-Quacks, not yet lickt into form, but remaining Moon-blind brats, (still in swadling-clouts) I mean the numerous crew of libellous pamphleteeirs, which are (if possible) more dangerous counterfeiters of my Elixer\" . . . Advertisement by mee, Anthony Daffy s.n., 1690?].\n\n\"Daffy’s original and famous elixir salutis: the choice drink of health: or, health-bringing drink. Being a famous cordial drink, found out by the providence of the Almighty, and (for above twenty years) experienced by himself, and divers persons (whose names are at most of their desires here inserted) a most excellent preservative of man-kind. A secret far beyond any medicament yet known, and is found so agreeable to nature, that it effects all its operations, as nature would have it, and as a virtual expedient proposed by her, for reducing all her extreams unto an equal temper; the same being fitted unto all ages, sexes, complexions, and constitutions, and highly fortifying nature against any noxious humour, invading or offending the noble parts. Never published by any but by Anthony Daffy, student in physick, and since continued by his widow Elleanor Daffy\", London : printed with allowance, for the author, by Tho. Milbourn dwelling in Jewen-Street, 1693.\n\n\n"}
{"id": "415674", "url": "https://en.wikipedia.org/wiki?curid=415674", "title": "Ear candling", "text": "Ear candling\n\nEar candling, also called ear coning or thermal-auricular therapy, is an alternative medicine practice claimed to improve general health and well-being by lighting one end of a hollow candle and placing the other end in the ear canal. Medical research has shown that the practice is both dangerous and ineffective and does not help remove earwax or toxicants.\n\nEdzard Ernst has published critically on the subject of ear candles, noting, \"There is no data to suggest that it is effective for any condition. Furthermore, ear candles have been associated with ear injuries. The inescapable conclusion is that ear candles do more harm than good. Their use should be discouraged.\"\n\nAccording to the US Food and Drug Administration (US FDA), ear candling is sometimes promoted with claims that the practice can \"purify the blood\" or \"cure\" cancer, but that Health Canada has determined the candles have no effect on the ear, and no health benefit; instead they create risk of injury, especially when used on children. In October 2007, US FDA issued an alert identifying ear candles (also known as ear cones or auricular candles) as \"dangerous to health when used in the dosage or manner, or with the frequency or duration, prescribed, recommended, or suggested in the labeling thereof\" ... \"since the use of a lit candle in the proximity of a person's face would carry a high risk of causing potentially severe skin/hair burns and middle ear damage.\"\n\nA 2007 paper in the journal \"Canadian Family Physician\" concludes:\n\nA 2007 paper in \"American Family Physician\" said:\n\nThe Spokane Ear, Nose, and Throat Clinic conducted a research study in 1996 which concluded that ear candling does not produce negative pressure and was ineffective in removing wax from the ear canal. Several studies have shown that ear candles produce the same residue when burnt without ear insertion and that the residue is simply candle wax and soot.\n\n, there are at least two cases in which people have set their houses on fire while ear candling, one of which resulted in death.\n\nA survey of ENT surgeons found some who had treated people with complications from ear candling. Burns were the most common.\n\nOne end of a cylinder or cone of waxed cloth is lit, and the other is placed into the subject's ear. The flame is cut back occasionally with scissors and extinguished between five and ten centimeters (two to four inches) from the subject.\n\nThe subject is lying on one side with the treated ear uppermost and the candle vertical. The candle can be stuck through a paper plate or aluminium pie tin to protect against any hot wax or ash falling onto the subject. Another way to perform ear candling involves the subject lying face up with the ear candle extending out to the side with a forty-five-degree upward slant. A dish of water is placed next to the subject under the ear candle.\n\nProponents claim that the flame creates negative pressure, drawing wax and debris out of the ear canal, which appears as a dark residue.\n\nAn ear candling session lasts up to one hour, during which one or two ear candles may be burned for each ear.\n\nIn Europe, some ear candles bear the CE mark (93/42/EEC), though they are mostly self-issued by the manufacturer. This mark indicates that the device is designed and manufactured so as not to compromise the safety of patients, but no independent testing is required as proof.\n\nWhile ear candles are widely available in the U.S., selling or importing them with medical claims is illegal. This means that one cannot market ear candles as products that \"Diagnose, cure, treat, or prevent any disease\".\n\nIn a report, Health Canada states \"There is no scientific proof to support claims that ear candling provides medical benefits. ... However, there is plenty of proof that ear candling is dangerous.\" It says that while some people claim to be selling the candles \"for entertainment purposes only\", the Canadian government maintains that there is no reasonable non-medical use, and hence any sale of the devices is illegal in Canada.\n\nIn a paper published by Edzard Ernst in \"Journal of Laryngology & Otology\", the cost of practicing ear candling according to the recommended frequency of use is estimated. As each candles costs $3.15 USD (adjusted for inflation), the annual cost of the treatment would amount to $982.00 USD (also adjusted for inflation). The author calls the continued practice of the treatment \"a triumph of ignorance over science ... or perhaps a triumph of commercial interests over medical reasoning.\"\n\nAlthough Biosun, a manufacturer of ear candles, refers to them as \"Hopi\" ear candles, there is no such treatment within traditional Hopi healing practices. Vanessa Charles, public relations officer for the Hopi Tribal Council, has stated that ear candling \"is not and has never been a practice conducted by the Hopi tribe or the Hopi people.\" The Hopi tribe has repeatedly asked Biosun, the manufacturer of 'Hopi Ear Candles', to stop using the Hopi name. Biosun has not complied with this request and continues to claim that ear candles originated within the Hopi tribe.\n\nMany advocates of ear candles claim that the treatment originates from traditional Chinese, Egyptian, or North American medicine. The mythical city of Atlantis is also reported to be the origin of this practice.\n\n\n"}
{"id": "1059396", "url": "https://en.wikipedia.org/wiki?curid=1059396", "title": "Emotional Freedom Techniques", "text": "Emotional Freedom Techniques\n\nEmotional Freedom Techniques (EFT) is a form of counseling intervention that draws on various theories of alternative medicine including acupuncture, neuro-linguistic programming, energy medicine, and Thought Field Therapy (TFT). It is best known through Gary Craig's \"EFT Handbook\", published in the late 1990s, and related books and workshops by a variety of teachers. EFT and similar techniques are often discussed under the umbrella term \"energy psychology\".\n\nAdvocates claim that the technique may be used to treat a wide variety of physical and psychological disorders, and as a simple form of self-administered therapy. The \"Skeptical Inquirer\" describes the foundations of EFT as \"a hodgepodge of concepts derived from a variety of sources, [primarily] the ancient Chinese philosophy of chi, which is thought to be the 'life force' that flows throughout the body.\" The existence of this life force is \"not empirically supported\".\n\nEFT has no benefit as a therapy beyond the placebo effect or any known-effective psychological techniques that may be provided in addition to the purported \"energy\" technique. It is generally characterized as pseudoscience and it has not garnered significant support in clinical psychology.\n\nDuring a typical EFT session, the person will focus on a specific issue while tapping on \"end points of the body's energy meridians\".\n\nAccording to the EFT manual, the procedure consists of the participant rating the emotional intensity of their reaction on a Subjective Units of Distress Scale (SUDS) (a Likert scale for subjective measures of distress, calibrated 0-10) then repeating an orienting affirmation while rubbing or tapping specific points on the body. Some practitioners incorporate eye movements or other tasks. The emotional intensity is then rescored and repeated until no changes are noted in the emotional intensity.\n\nProponents of EFT and other similar treatments believe that tapping/stimulating acupuncture points provide the basis for significant improvement in psychological problems. However, the theory and mechanisms underlying the supposed effectiveness of EFT have \"no evidentiary support\" \"in the entire history of the sciences of biology, anatomy, physiology, neurology, physics, or psychology.\" Researchers have described the theoretical model for EFT as \"frankly bizarre\" and \"pseudoscientific.\" One review noted that one of the highest quality studies found no evidence that the location of tapping points made any difference, and attributed effects to well-known psychological mechanisms, including distraction and breathing therapy.\n\nAn article in the \"Skeptical Inquirer\" argued that there is no plausible mechanism to explain how the specifics of EFT could add to its effectiveness, and they have been described as unfalsifiable and therefore pseudoscientific. Evidence has not been found for the existence of meridians.\n\nEFT has no useful effect as a therapy beyond the placebo effect or any known-effective psychological techniques that may be used with the purported \"energy\" technique, but proponents of EFT have published material claiming otherwise. Their work, however, is flawed and hence unreliable: high-quality research has never confirmed that EFT is effective.\n\nA 2009 review found \"methodological flaws\" in research studies that had reported \"small successes\" for EFT and the related Tapas Acupressure Technique. The review concluded that positive results may be \"attributable to well-known cognitive and behavioral techniques that are included with the energy manipulation. Psychologists and researchers should be wary of using such techniques, and make efforts to inform the public about the ill effects of therapies that advertise miraculous claims.\"\n\nA 2016 systematic review found that EFT was effective in reducing anxiety compared to controls, but also called for more research comparing its effectiveness to that of established treatments.\n\nA Delphi poll of an expert panel of psychologists rated EFT on a scale describing how discredited EFT has been in the field of psychology. On average, this panel found EFT had a score of 3.8 on a scale from 1.0 to 5.0, with 3.0 meaning \"possibly discredited\" and a 4.0 meaning \"probably discredited.\" A book examining pseudoscientific practices in psychology characterized EFT as one of a number of \"fringe psychotherapeutic practices\", and a psychiatry handbook states EFT has \"all the hallmarks of pseudoscience\".\n\nEFT, along with its predecessor, Thought Field Therapy, has been dismissed with warnings to avoid their use by publications such as \"The Skeptic's Dictionary\" and Quackwatch.\n\nProponents of EFT and other energy psychology therapies have been \"particularly interested\" in seeking \"scientific credibility\" despite the implausible proposed mechanisms for EFT. A 2008 review by energy psychology proponent David Feinstein concluded that energy psychology was a potential \"rapid and potent treatment for a range of psychological conditions.\" However, this work by Feinstein has been widely criticized. One review criticized Feinstein's methodology, noting he ignored several research papers that did not show positive effects of EFT, and that Feinstein did not disclose his conflict of interest as an owner of a website that sells energy psychology products such as books and seminars, contrary to the best practices of research publication. Another review criticized Feinstein's conclusion, which was based on research of weak quality and instead concluded that any positive effects of EFT are due to the more traditional psychological techniques rather than any putative \"energy\" manipulation. A book published on the subject of evidence-based treatment of substance abuse called Feinstein's review \"incomplete and misleading\" and an example of a poorly performed evidence-based review of research.\n\nFeinstein published another review in 2012, concluding that energy psychology techniques \"consistently demonstrated strong effect sizes and other positive statistical results that far exceed chance after relatively few treatment sessions\". This review was also criticized, where again it was noted that Feinstein dismissed higher quality studies which showed no effects of EFT, in favor of methodologically weaker studies which did show a positive effect.\n"}
{"id": "5186688", "url": "https://en.wikipedia.org/wiki?curid=5186688", "title": "Energy (esotericism)", "text": "Energy (esotericism)\n\nThe term \"energy\" is used by writers and practitioners of various esoteric forms of spirituality and alternative medicine to refer to a variety of phenomena. There is no scientific evidence for the existence of such energy.\n\nTherapies that purport to use, modify, or manipulate unknown energies are thus among the most contentious of all complementary and alternative medicines. Claims related to energy therapies are most often anecdotal (from single stories), rather than being based on repeatable empirical evidence.\n\nConcepts such as \"life force\" and \"élan vital\" existed from antiquity and emerged from the debate over vitalism in the 18th and 19th centuries with Mesmer and the magnetism. They continued to be discussed in the 20th century by some thinkers and practitioners in the modern New Age movement.\n\nAs biologists studied embryology and developmental biology, particularly before the discovery of genes, a variety of organisational forces were posited to account for their observations. German biologist Hans Driesch (1867–1941), proposed entelechy, an energy which he believed controlled organic processes. However such ideas are discredited and modern science has all but abandoned the attempt to associate additional energetic properties with life.\n\nAccording to Brian Dunning, the scientific term energy is, in fact, misused in the context of spirituality and alternative medicine:\n\nDespite the lack of scientific support, spiritual writers and thinkers have maintained ideas about energy and continue to promote them either as useful allegories or as fact. The field of energy medicine purports to manipulate energy, but there is no credible evidence to support this.\n\nThe concept of \"qi\" (energy) appears throughout traditional East Asian culture, such as in the art of feng shui and Chinese martial arts. Qi philosophy also includes the notion of \"negative qi\", typically understood as introducing negative moods like outright fear or more moderate expressions like social anxiety or awkwardness. Deflecting this negative qi through geomancy is a preoccupation in feng shui. The traditional explanation of acupuncture states that it works by manipulating the circulation of qi through a network of meridians.\n\nThere are various sacred natural sites that people of various belief systems find numinous or having an \"energy\" with significance to humans. The idea that some kind of \"negative energy\" is responsible for creating or attracting ghosts or demons appears in contemporary paranormal culture and beliefs as exemplified in the TV shows \"Paranormal State\" and \"Ghost Hunters\".\n"}
{"id": "53686244", "url": "https://en.wikipedia.org/wiki?curid=53686244", "title": "Exhibition of citriculture", "text": "Exhibition of citriculture\n\nThe International Exhibition of Citriculture was a Specialised Expo recognised by the Bureau International des Expositions. The Expo took place from 21 May to 20 June 1956 in Beit Dagan, Israel and was organised within the framework of the fourth International Congress of Mediterranean Citrus Growers. \n\n"}
{"id": "1878393", "url": "https://en.wikipedia.org/wiki?curid=1878393", "title": "Fads and Fallacies in the Name of Science", "text": "Fads and Fallacies in the Name of Science\n\nFads and Fallacies in the Name of Science (1957)—originally published in 1952 as In the Name of Science: An Entertaining Survey of the High Priests and Cultists of Science, Past and Present—was Martin Gardner's second book. A survey of what it described as pseudosciences and cult beliefs, it became a founding document in the nascent scientific skepticism movement. Michael Shermer said of it: \"Modern skepticism has developed into a science-based movement, beginning with Martin Gardner's 1952 classic\".\n\nThe book debunks what it characterises as pseudo-science and the pseudo-scientists who propagate it.\n\n\"Fads and Fallacies in the Name of Science\" starts with a brief survey of the spread of the ideas of \"cranks\" and \"pseudo-scientists\", attacking the credulity of the popular press and the irresponsibility of publishing houses in helping to propagate these ideas. Cranks often cite historical cases where ideas were rejected which are now accepted as right. Gardner acknowledges that such cases occurred, and describes some of them, but says that times have changed: \"If anything, scientific journals err on the side of permitting \"questionable\" theses to be published\". Gardner acknowledges that \"among older scientists ... one may occasionally meet with irrational prejudice against a new point of view\", but adds that \"a certain degree of dogma ... is both necessary and desirable\" because otherwise \"science would be reduced to shambles by having to examine every new-fangled notion that came along.\"\n\nGardner says that cranks have \"two\" common characteristics. The first \"and most important\" is that they work in almost total isolation from the scientific community. Gardner defines the community as an efficient network of communication within scientific fields, together with a co-operative process of testing new theories. This process allows for apparently bizarre theories to be published — such as Einstein's theory of relativity, which initially met with considerable opposition; it was never dismissed as the work of a crackpot, and it soon met with almost universal acceptance. But the crank \"stands entirely outside the closely integrated channels through which new ideas are introduced and evaluated. He does not send his findings to the recognized journals or, if he does, they are rejected for reasons which in the vast majority of cases are excellent.\"\n\nThe second characteristic of the crank (which also contributes to his or her isolation) is the tendency to paranoia. There are \"five\" ways in which this tendency is likely to be manifested.\n\n\nThese psychological traits are in varying degrees demonstrated throughout the remaining chapters of the book, in which Gardner examines particular \"fads\" he labels pseudo-scientific. His writing became the source book from which many later studies of pseudo-science were taken (e.g. \"Encyclopedia of Pseudo-science\").\n\nAs per the subtitle of the book, \"The curious theories of modern pseudoscientists and the strange, amusing and alarming cults that surround them\" are discussed in the chapters as listed.\n\n\nThe 1957 Dover publication is a revised and expanded version of \"In the Name of Science\", which was published by G. P. Putnam's Sons in 1952. The subtitle boldly states the book's theme: \"The curious theories of modern pseudoscientists and the strange, amusing and alarming cults that surround them. A study in human gullibility\". As of 2005, it had been reprinted at least 30 times.\n\nThe book was expanded from an article first published in the \"Antioch Review\" in 1950, and in the preface to the first edition, Gardner thanks the Review for allowing him to develop the article as the starting point of his book. Not all material in the article is carried over to the book. For example, in the article, Gardner writes:\nThe reader may wonder why a competent scientist does not publish a detailed refutation of Reich's absurd biological speculations. The answer is that the informed scientist doesn't care, and would, in fact, damage his reputation by taking the time to undertake such a thankless task.\nAnd comments in a footnote:\nIt is not within the scope of this paper, however, to discuss technical criteria by which hypotheses are given high, low, or negative degrees of confirmation. Our purpose is simply to glance at several examples of a type of scientific activity which fails completely to conform to scientific standards, but at the same time is the result of such intricate mental activity that it wins temporary acceptance by many laymen insufficiently informed to recognize the scientist's incompetence. Although there obviously is no sharp line separating competent from incompetent research, and there are occasions when a scientific \"orthodoxy\" may delay the acceptance of novel views, the fact remains that the distance between the work of competent scientists and the speculations of a Voliva or Velikovsky is so great that a qualitative difference emerges which justifies the label of \"pseudo-science.\" Since the time of Galileo the history of pseudo-science has been so completely outside the history of science that the two streams touch only in the rarest of instances.\nWhile in the book, Gardner writes:\nIf someone announces that the moon is made of green cheese, the professional astronomer cannot be expected to climb down from his telescope and write a detailed refutation. “A fairly complete textbook of physics would be only part of the answer to Velikovsky,” writes Prof. Laurence J. Lafleur, in his excellent article on “Cranks and Scientists” (\"Scientific Monthly\", Nov., 1951), “and it is therefore not surprising that the scientist does not find the undertaking worth while.”\nAnd in the wrap-up of the chapter:\nJust as an experienced doctor is able to diagnose certain ailments the instant a new patient walks into his office, or a police officer learns to recognize criminal types from subtle behavior clues which escape the untrained eye, so we, perhaps, may learn to recognize the future scientific crank when we first encounter him.\nA contemporary review in the \"Pittsburgh Post-Gazette\" particularly welcomed Gardner's critical remarks about Hoxsey Therapy and about Krebiozen, both of which were being advanced as anti-cancer measures at that time. The review concluded that the book \"should help to counteract some amusing and some positively harmful cults, the existence of which is all too often promoted by irresponsible journalism.\"\n\nThe work has often been mentioned in subsequent books and articles. Louis Lasagna, in his book \"The Doctors' Dilemmas\", considered it to be a \"superb account of scientific cults, fads, and frauds\" and wrote that \"This talented writer combines solid fact with a pleasing style.\"\n\nSociologist of religion Anson D. Shupe took in general a positive attitude, and praises Gardner for his humor. But he says\nIf there is a single criticism to be made of Gardner ... it is that he accepts too comfortably the conventional wisdom, or accepted social reality, of current twentieth-century science and middle-class American Christianity. Somehow it is evident (to me at least) that he is implicitly making a pact with the reader to evaluate these fringe groups in terms of their own shared presumptions about what is \"normal\". Thus he is quite confident throwing around labels like \"quack\", \"crank\" and \"preposterous\". In science the use of such value judgments can be quite time-bound; likewise in religions where today's heresy may become tomorrow's orthodoxy. The odds of course are always on the side of the writer criticizing fringe groups because statistically speaking so few of them survive. However, when a group does weather its infancy and go on to prosper, invariably its original detractors look a bit more arbitrary than they did initially, and then the shoe is on the other foot.\n\nIn the 1980s a fierce interchange took place between Gardner and Colin Wilson. In \"The Quest for Wilhelm Reich\" Wilson wrote of this book(Gardner) writes about various kinds of cranks with the conscious superiority of the scientist, and in most cases one can share his sense of the victory of reason. But after half a dozen chapters this non-stop superiority begins to irritate; you begin to wonder about the standards that make him so certain \"he\" is always right. He asserts that the scientist, unlike the crank, does his best to remain open-minded. So how can he be so \"sure\" that no sane person has ever seen a flying saucer, or used a dowsing rod to locate water? And that all the people he disagrees with are unbalanced fanatics? A colleague of the positivist philosopher A. J. Ayer once remarked wryly \"I wish I was as certain of anything as he seems to be about everything\". Martin Gardner produces the same feeling. By Wilson's own account, up to that time he and Gardner had been friends, but Gardner took offence. In February 1989 Gardner wrote a letter published in \"The New York Review of Books\" describing Wilson as \"England’s leading journalist of the occult, and a firm believer in ghosts, poltergeists, levitations, dowsing, PK (psychokinesis), ESP, and every other aspect of the psychic scene\". Shortly afterwards, Wilson replied, defending himself and adding \"What strikes me as so interesting is that when Mr. Gardner—and his colleagues of CSICOP—begin to denounce the 'Yahoos of the paranormal,' they manage to generate an atmosphere of such intense hysteria ...\". Gardner in turn replied quoting his own earlier description of Wilson: \"The former boy wonder, tall and handsome in his turtleneck sweater, has now decayed into one of those amiable eccentrics for which the land of Conan Doyle is noted. They prowl comically about the lunatic fringes of science ...\"\n\nIn a review of a subsequent Gardner work, Paul Stuewe of the \"Toronto Star\" called \"Fads and Fallacies\" a \"hugely enjoyable demolition of pseudo-scientific nonsense\". Ed Regis, writing in \"The New York Times\", considered the book to be \"the classic put-down of pseudoscience\". Fellow skeptic Michael Shermer called the book \"\"the\" skeptic classic of the past half-century.\" He noted that the mark of popularity for the book came when John W. Campbell denounced the chapter on dianetics over the radio.\n\nMark Erickson, author of \"Science, culture and society: understanding science in the twenty-first century\", noted that Gardner's book provided \"a flavour of the immense optimism surrounding science in the 1950s\" and that his choice of topics were \"interesting\", but also that his attacks on \"osteopathy, chiropractice, and the Bates method for correcting eyesight would raise eyebrows amongst medical practitioners today\".\n\nGardner’s own response to criticism is given in his preface:\n\n\n"}
{"id": "12829161", "url": "https://en.wikipedia.org/wiki?curid=12829161", "title": "Free parameter", "text": "Free parameter\n\nA free parameter is a variable in a mathematical model which cannot be predicted precisely or constrained by the model and must be estimated experimentally or theoretically. A mathematical model, theory, or conjecture is more likely to be right and less likely to be the product of wishful thinking if it relies on few free parameters and is consistent with large amounts of data.\n\n"}
{"id": "12459101", "url": "https://en.wikipedia.org/wiki?curid=12459101", "title": "Gemmotherapy", "text": "Gemmotherapy\n\nGemmotherapy [from Lat. \"gemma\", bud, and New Lat. \"therapīa\", Grk. \"therapeia\", medical treatment] is a form of herbal medicine that uses remedies made principally from the embryonic tissue of various trees and shrubs (the buds and emerging shoots), but also from the reproductive parts (the seeds and catkins) and from newly grown tissue (the rootlets and the cortex of rootlets). In two instances, remedies are also made from the sap.\n\nThis raw material is taken at the peak time of the tree or shrub’s annual germination, in the spring for buds or the autumn for seeds. Certain plant hormones and enzymes are released during this process, and in some cases are only present in the plant at this time.\n\nThe therapeutic effects of remedies made from the embryonic material of plants were first investigated in the late 1950s by a Belgian homeopath, Pol Henry (1918–88), working with a group of French homeopaths and biotherapists including Max Tétau (1927-2012) and O.A. Julian (1910–84). They conducted the first experiments as well as human and animal clinical trials that elucidated the effects of gemmotherapy and summarized their clinical findings.\n\nHenry initially called the new type of medicine, phytoembryotherapy, but it was Tetau that later coined the phrase gemmotherapy.\n\nGemmotherapy was included in herbal therapies in France in the \"Pharmacopée Francaise\" in 1965.\n"}
{"id": "2149716", "url": "https://en.wikipedia.org/wiki?curid=2149716", "title": "Haldane principle", "text": "Haldane principle\n\nIn British research policy, the Haldane principle is the idea that decisions about what to spend research funds on should be made by researchers rather than politicians. It is named after Richard Burdon Haldane, who in 1904 and from 1909 to 1918 chaired committees and commissions which recommended this policy.\n\nThe 1904 committee recommended the creation of the University Grants Committee which has evolved via the Universities Funding Council into the current higher education funding councils: Research Councils UK, Higher Education Funding Council for England, Scottish Funding Council and Higher Education Funding Council for Wales.\n\nIn 1918 Haldane's committee produced the \"Haldane Report\". The report suggested that research required by government departments could be separated into that required by specific departments and that which was more general. It recommended that departments should oversee the specific research but the general research should be under the control of autonomous Research Councils, which would be free from political and administrative pressures that might discourage research in certain areas. The principle of the autonomy of the research councils is now referred to as the Haldane Principle. The first research council to be created as a result of the Haldane Report was the Medical Research Council.\n\nThe principle has remained enshrined in British Government policy, but has been criticised and altered over the years. In 1939 J.D. Bernal argued that social good was more important than researchers' freedom in deciding the direction of research. Solly Zuckerman criticised it in 1971 for its artificial separation of basic and applied science, and the consequent elevation of the status of the former.\n\nA major revision to the application of the Haldane Principle in British research funding came in the early 1970s with the Rothschild Report of 1971, and its implementation which transferred about 25% of the then Research Council funds, and the decisions on the research to be funded with them, back to government departments, a move later undone by Margaret Thatcher's government.\n\nThere is currently a debate about the extent to which the principle is still applied in practice.\n\nThe Higher Education and Research Act 2017, which merged the research councils and the research part of the Higher Education Funding Council for England into UK Research and Innovation, enacted the Haldane principle as section 103(3): \"The “Haldane principle” is the principle that decisions on individual research proposals are best taken following an evaluation of the quality and likely impact of the proposals (such as a peer review process).\"\n\n\n\n"}
{"id": "40450007", "url": "https://en.wikipedia.org/wiki?curid=40450007", "title": "Hydroxymatairesinol", "text": "Hydroxymatairesinol\n\nHydroxymatairesinol (HMR) is a lignan found in Norway spruce (\"Picea abies\"). It is an enterolactone precursor with anticancer activities. In rats, HMR decreased the volume of induced tumours and stabilised established tumours, as well as preventing the development of new tumours. It has also shown anti-oxidant properties \"in vitro\".\n\nHMR's chemical structure is similar to matairesinol. At high concentrations, HMR has estrogenic properties, which are considerably weaker than those of estradiol.\n"}
{"id": "20140244", "url": "https://en.wikipedia.org/wiki?curid=20140244", "title": "Jadad scale", "text": "Jadad scale\n\nThe Jadad scale, sometimes known as Jadad scoring or the Oxford quality scoring system, is a procedure to independently assess the methodological quality of a clinical trial. It is named after Colombian physician Alex Jadad who in 1996 described a system for allocating the trial a score of between zero (very poor) and five (rigorous). It is the most widely used such assessment in the world, and as of 2017, its seminal paper has been cited in over 13000 scientific works.\n\nThe Jadad scale independently assesses the methodological quality of a clinical trial judging the effectiveness of blinding. Alejandro \"Alex\" Jadad Bechara, a Colombian physician who worked as a Research Fellow at the Oxford Pain Relief Unit, Nuffield Department of Anaesthetics, at the University of Oxford described the allocating trials a score of between zero (very poor) and five (rigorous) scale in an appendix to a 1996 paper.\nIn a 2007 book Jadad described the randomised controlled trial as \"one of the simplest, most powerful and revolutionary forms of research\".\n\nClinical trials are conducted for the purpose of collecting data on the efficacy of medical treatments. The treatment might be, for example, a new drug, a medical device, a surgical procedure, or a preventative regime. Clinical trial protocols vary considerably depending on the nature of the treatment under investigation, but typically in a controlled trial researchers gather a group of volunteers and subject some to the test treatment, while giving the others either no treatment (known as a placebo), or an established treatment for comparison. After a defined time period, the patients in the test group are assessed for health improvements in comparison with the control group.\n\nHowever, trials can vary greatly in quality. Methodological errors such as poor blinding or poor randomisation allow factors such as the placebo effect or selection bias to adversely affect the results of a trial.\n\nRandomisation is a process to remove potential distortion of statistical results arising from the manner in which the\ntrial is conducted, in particular in the selection of subjects. Studies have indicated, for example, that nonrandomised trials are more likely to show a positive result for a new treatment than for an established conventional one.\n\nThe importance of scientific controls to limit factors under test is well established. However, it is also important that none of those involved in a clinical trial, whether the researcher, the subject patient or any other involved parties, should allow their own prior expectations to affect reporting of results. The placebo effect is known to be a confounding factor in trials; affecting the ability of both patients and doctors to report accurately on the clinical outcome. Experimental blinding is a process to prevent bias, both conscious and subconscious, skewing results.\n\nBlinding frequently takes the form of a placebo, an inactive dummy that is indistinguishable from the real treatment. Blinding can however be difficult to achieve in some trials, for example, surgery or physical therapy. Poor blinding can exaggerate the perceived effects of treatment, particularly if any such effects are small. Blinding should be appropriate to the study, and is ideally double blind, wherein neither the patient nor doctor is aware of whether they are in the control or test group, eliminating any such psychological effects from the study.\n\nWithdrawals and dropouts are those patients who fail to complete a course of treatment, or fail to report back on its outcome to the researchers. The reasons for doing so might be varied: the individuals may have moved away, abandoned the course of treatment, or died. Whatever the reason, the attrition rate can skew results of a study, particularly for those subjects who ceased treatment due to perceived inefficacy. In smoking cessation studies, for example, it is routine to consider all dropouts as failures.\n\nA three-point questionnaire forms the basis for a Jadad score. Each question was to be answered with either a \"yes\" or a \"no\". Each \"yes\" would score a single point, each \"no\" zero points; there were to be no fractional points. The Jadad team stated that they expected it should take no longer than ten minutes to score any individual paper. The questions were as follows: \"Was the study described as randomized?\", \"Was the study described as double blind?\" and \"Was there a description of withdrawals and dropouts?\"\n\nTo receive the corresponding point, an article should describe the number of withdrawals and dropouts, in each of the study groups, and the underlying reasons. Additional points were given if: \"The method of randomisation was described in the paper, and that method was appropriate.\" or \"The method of blinding was described, and it was appropriate.\"\n\nPoints would be \"deducted\" if: \"The method of randomisation was described, but was inappropriate.\" or \"The method of blinding was described, but was inappropriate.\"\n\nA clinical trial could therefore receive a Jadad score of between zero and five. The Jadad scale is sometimes described as a five-point scale, though there are only three questions.\n\nThe Jadad score may be used in a number of ways:\n\n\n, the Jadad score was the most widely used such assessment in the world, and its seminal paper has been cited in over 3000 scientific works.\n\nCritics have charged that the Jadad scale is flawed, being over-simplistic and placing too much emphasis on blinding, and can show low consistency between different raters. Furthermore, it does not take into account allocation concealment, viewed by The Cochrane Collaboration as paramount to avoid bias.\n\n"}
{"id": "18010608", "url": "https://en.wikipedia.org/wiki?curid=18010608", "title": "John Wilbanks", "text": "John Wilbanks\n\nJohn Wilbanks is the chief commons officer at Sage Bionetworks and a senior fellow at the Ewing Marion Kauffman Foundation and at FasterCures. He runs the Consent to Research Project.\n\nWilbanks grew up in Knoxville, Tennessee, US. He attended Tulane University and received a Bachelor of Arts in philosophy in 1994. He also studied modern letters at the Sorbonne in Paris.\n\nFrom 1994 to 1997, he worked in Washington, DC as a legislative aide to Congressman Fortney \"Pete\" Stark. During this time Wilbanks was also a grassroots coordinator and fundraiser for the American Physical Therapy Association. Wilbanks was the Berkman Center for Internet & Society's first assistant director from the fall of 1998 to the summer of 2000. There he led efforts in software development and Internet-mediated learning, and was involved in the Berkman Center's work on ICANN.\n\nWhile at the Berkman Center, Wilbanks founded Incellico, Inc., a bioinformatics company that built semantic graph networks for use in pharmaceutical research and development. He served as President and CEO, and led to the company's acquisition in the summer of 2003. He has also served as a Fellow at the World Wide Web Consortium on Semantic Web for Life Sciences, was a Visiting Scientist in the Project on Mathematics and Computation at MIT, and was a member of the National Advisory Committee for PubMed Central. He is a member of the Board of Directors for Sage Bionetworks and on the advisory boards of Genomera, Genomic Arts, and Boundless Learning. He is an original author of the Panton Principles for sharing data. \n\nWilbanks led a We the People petition supporting the free access of taxpayer-funded research data, which gained over 65,000 signatures. In February 2013, the White House responded, detailing a plan to freely publicize taxpayer-funded research data.\n\nConsent to Research (CtR) is a project that provides a platform for people to donate their health data for the purposes of scientific research and the advancement of medicine. Since health data is restricted and expensive, this project provides people the opportunity to freely donate information that can only positively benefit medicine and patients at large. Consent to Research is connected to the Access2Research project, which aims to free access over the Internet to scientific journal articles that are already taxpayer-funded. Wilbanks founded the project in 2011 and gave a TED Global talk about the project in 2012.\n\nWilbanks worked at Science Commons and Creative Commons from October 2004 to September 2011. As vice president of science he ran the Science Commons project for its five-year lifetime and continued to work on science after he joined the core Creative Commons organization. He has been interviewed by Popular Science magazine, KRUU Radio, and BioMed Central to discuss Science Commons.\n\n\"Scientific American\" featured Wilbanks in \"The Machine That Would Predict The Future\" in 2011. Seed magazine named Wilbanks among their Revolutionary Minds of 2008, as a \"Game Changer\" and the Utne Reader named him in 2009 as one of \"50 visionaries who are changing your world\". He frequently campaigns for wider adoption of open access publishing in science and the increased sharing of data by scientists.\n\n"}
{"id": "40319830", "url": "https://en.wikipedia.org/wiki?curid=40319830", "title": "Journal of Natural Philosophy, Chemistry, and the Arts", "text": "Journal of Natural Philosophy, Chemistry, and the Arts\n\nThe A Journal of Natural Philosophy, Chemistry, and the Arts, generally known as \"Nicholson's Journal\", was the first monthly scientific journal in Great Britain. William Nicholson began it in 1797 and was the editor until it merged with another journal at the end of 1813.\n\nNicholson's journal would accept short papers, written by new or anonymous authors, and decide whether to publish them relatively quickly. These attributes distinguished the new journal from the established scientific journal \"Philosophical Transactions of the Royal Society\". By one account this less-formal model was so appealing that the next year a similar startup launched, Alexander Tilloch's \"Philosophical Magazine\".\n\n\nBy one account, William Nicholson started the journal and made all editorial decisions in a \"pioneering and uncertain attempt\" to make a living from publishing it. Revenues came only from subscriptions. Tilloch's \"Philosophical Magazine\" was more successful as a popular science journal business than Nicholson's journal, according to one source, and another such journal appeared in 1813 (\"Annals of Philosophy\"). Possibly partly because of this competition, William Nicholson ended the journal. By some accounts Nicholson's journal simply ceased, and by others it merged in 1814 with the \"Philosophical Magazine\" to form \"The Philosophical Magazine and Journal\".\n\nThe \"Advertisement\", dated 31 December 1813, at the start of Volume 42 of \"The Philosophical Magazine\" states:\n\n\"Nearly seventeen years have elapsed since \"The Philosophical Journal\" was commenced by Mr. Nicholson, and sixteen since the appearance of the first number of \"The Philosophical Magazine\". [...] [T]he result of [...] deliberations [between the publishers of \"Nicholson's Philosophical Journal\" and \"The Philosophical Magazine\" in order to respond to readers' complaints regarding duplication of material in the two publications] has been that it would certainly be best that we should unite, and that the joint product of our exertions and our correspondence should be consolidated in one periodical work. [...] The Philosophical Journal will henceforth be discontinued; and The Philosophical Magazine will be conducted by William Nicholson and Alexander Tilloch, in the same manner as it has always been carried on.\"\n\nFor the duration of Volume 43 (January to June 1814) the joint publishers of the new merged journal provided duplicate title-pages for each number, ostensibly so that subscribers to \"Nicholson's Philosophical Magazine\" might be enabled to \"preserve their Series without a chasm.\" However, despite their intention to continue this scheme of two-fold numeration, they abandoned it at the end of this trial period in June 1814, because of the perceived \"confusion and risque of many errors\" when referring to future volumes; from July 1814 a single numeration was used, following the numbering of \"The Philosophical Magazine.\"\n\nComplete journal issues have been scanned and are available online at the Biodiversity Heritage Library and at archive.org thanks to the Natural History Museum Library, London, the New York Public Library and google books.\n\n"}
{"id": "70111", "url": "https://en.wikipedia.org/wiki?curid=70111", "title": "Land lab", "text": "Land lab\n\nA land lab is an area of land that has been set aside for use in biological studies. Thus, it is literally an outdoor laboratory based on an area of land.\n\nStudies may be elementary or advanced. For instance, students may simply be given the task of identifying all the tree species in a land lab, or an advanced student may be doing an intensive survey of the microbial life forms found in a soil sample.\n\nLand labs are often marked out in plots or transects for studies. A plot may be any size, usually marked out in square meters. This allows for more intensive, delimited studies of changes and inventories of biota. Transects are straight lines at which, at intervals, measurements are taken for a profile of the ecological community.\n"}
{"id": "719120", "url": "https://en.wikipedia.org/wiki?curid=719120", "title": "Life and Energy", "text": "Life and Energy\n\nLife and Energy is a 1962 book by Isaac Asimov. It is about the biological and physical world, and their contrasts and comparisons. Thus the book is divided into two sections, which is separated by further sub-sections (i.e. chapters): 1) energy; 2) body. In order to accomplish its goal, the book starts with \"layman\" discussions about energy and how these can be used to single out human from other living systems, or even living systems from non-living matter, what differentiates a rock from an oyster, and finishes with advanced concepts, how living systems are able to \"produce\" energy.\n\nThe first chapters covers the common questions of the distinctions between living and inanimate objects.\n\nAsimov then explains in a step by step manner about the physical world first through slow, but interesting chapters. He writes about the effect and major role of the evolution and advance of man by fire and heat, he tells about thermodynamics (and its laws), he recollects the thoughts of previous scientists, and their painstaking works, and finally, the quantum theory and radiation, which has revolutionised physics and technology. An explanation of electricity and basic chemistry laws and features are also included.\nThe physical section ends here, and continues into biology. He now continues on with special chemistry, and leaves behind physics. From this, the book leads into the functions of enzymes, amino acids, cells, the body as a whole, and the process of the cells and organs to work together to become one.\n\n\n"}
{"id": "38563830", "url": "https://en.wikipedia.org/wiki?curid=38563830", "title": "List of citizen science projects", "text": "List of citizen science projects\n\nCitizen science projects are activities sponsored by a wide variety of organizations so non-scientists can meaningfully contribute to scientific research. Activities vary widely from transcribing old ship logbooks to digitize the data as part of the Old Weather project to observing and counting birds at home or in the field for eBird. Participation can be as simple as playing a computer game for a project called Eyewire that may help scientists learn more about retinal neurons. It can also be more in depth, such as when citizens collect water quality data over time to assess the health of local waters, or help discover and name new species of insect. An emerging branch of Citizen Science are Community Mapping projects that utilize smartphone and tablet technology. For example, TurtleSAT is a community mapping project that is mapping freshwater turtle deaths throughout Australia.\n\nLists of citizen science projects may change. For example, the Old Weather project website indicates that January 10, 2015, 51% of the logs were completed. When that project reaches 100 percent, it will move to the completed list.\n\nThere are more than 1,100 active and searchable global citizen science projects listed on the SciStarter website.\n\n"}
{"id": "49538727", "url": "https://en.wikipedia.org/wiki?curid=49538727", "title": "List of inventions and discoveries by women", "text": "List of inventions and discoveries by women\n\nThis page aims to list inventions and discoveries in which women played a major role. \n\n\n\n\n\n\n\n\n\nThe first disposable diaper was invented in 1946 by Marion Donovan, a professional-turned-housewife who wanted to ensure her children's cloth diapers remained dry while they slept. Donovan patented her design (called \n'Boaters') in 1951. She also invented the first paper diapers, but executives did not invest in this idea and it was consequently scrapped for over ten years, until Procter & Gamble used Donovan's design ideas to create Pampers.\n\nAnother diaper design was created by Valerie Hunter Gordon (née de Ferranti), who patented it in 1948 . \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIngrid Daubechies introduced the Daubechies wavelet and contributed to the development of the CDF wavelet, important tools in image compression. \n\n\nIn 1966 Mark Kac asked whether the shape of a drum could be determined by the sound it makes (whether a Riemannian manifold\nis determined by the spectrum of its Laplace-Beltrami operator). John Milnor observed that a theorem due to Witt implied the existence of a pair of 16-dimensional tori that have the same spectrum but different shapes. However, the problem in two dimensions remained open until 1992, when Carolyn S. Gordon with coauthors Webb and Wolpert, constructed a pair of regions in the Euclidean plane that have different shapes but identical eigenvalues (see figure on right). \n\n\nIn mathematics, the Cauchy–Kowalevski theorem (also written as the Cauchy–Kovalevskaya theorem) is the main local existence and uniqueness theorem for analytic partial differential equations associated with Cauchy initial value problems. A special case was proven by Augustin Cauchy (1842), and the full result by Sophia Kovalevskaya (1875). \n\n\nIn classical mechanics, the precession of a rigid body such as a top under the influence of gravity is not, in general, an integrable problem. There are however three (or four) famous cases that are integrable, the Euler, the Lagrange, and the Kovalevskaya top. The Kovalevskaya top is a special symmetric top with a unique ratio of the moments of inertia which satisfy the relation\n\nThat is, two moments of inertia are equal, the third is half as large, and the center of gravity is located in the plane perpendicular to the symmetry axis (parallel to the plane of the two equal points).\n\n\nIn numerical linear algebra, the QR algorithm is an eigenvalue algorithm: that is, a procedure to calculate the eigenvalues and eigenvectors of a matrix. The QR algorithm was developed in the late 1950s by John G. F. Francis and by Vera N. Kublanovskaya, working independently. The basic idea is to perform a QR decomposition, writing the matrix as a product of an orthogonal matrix and an upper triangular matrix, multiply the factors in the reverse order, and iterate.\n\n\nOlga Ladyzhenskaya provided the first rigorous proofs of the convergence of a finite difference method for the Navier–Stokes equations. Ladyzhenskaya was on the shortlist for potential recipients for the 1958 Fields Medal, ultimately awarded to Klaus Roth and René Thom. \n\n\nRuth Lawrence's 1990 paper, \"Homological representations of the Hecke algebra\", in \"Communications in Mathematical Physics\", introduced, among other things, certain novel linear representations of the braid group — known as Lawrence–Krammer representation. In papers published in 2000 and 2001, Daan Krammer and Stephen Bigelow established the faithfulness of Lawrence's representation. This result goes by the phrase \"braid groups are linear.\"\n\n\nRózsa Péter was one of the founders of recursion theory, a branch of mathematical logic, of computer science, and of the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory. \n\n\nHilbert's tenth problem is the tenth on the list of mathematical problems that the German mathematician David Hilbert posed in 1900. It is the challenge to provide a general algorithm which, for any given Diophantine equation (a polynomial equation with integer coefficients and a finite number of unknowns) can decide whether the equation has a solution with all unknowns taking integer values. \n\nFor example, the Diophantine equation formula_2 has an integer solution: formula_3. By contrast, the Diophantine equation formula_4 has no such solution.\n\nHilbert's tenth problem has been solved, and it has a negative answer: such a general algorithm does not exist. This is the result of combined work of Martin Davis, Yuri Matiyasevich, Hilary Putnam and Julia Robinson which spans 21 years, with Yuri Matiyasevich completing the theorem in 1970. The theorem is now known as Matiyasevich's theorem or the MRDP theorem.\n\nIn the design of experiments, optimal designs (or optimum designs) are a class of experimental designs that are optimal with respect to some statistical criterion. The creation of this field of statistics has been credited to Danish statistician Kirstine Smith.\n\nThe three-gap theorem states that if one places \"n\" points on a circle, at angles of \"θ\", 2\"θ\", 3\"θ\" ... from the starting point, then there will be at most three distinct distances between pairs of points in adjacent positions around the circle. When there are three distances, the larger of the three always equals the sum of the other two. Unless \"θ\" is a rational multiple of , there will also be at least two distinct distances.\n\nThis result was conjectured by Hugo Steinhaus, and proved in the 1950s by Vera T. Sós, , and Stanisław Świerczkowski. Its applications include the study of plant growth and musical tuning systems, and the theory of Sturmian words. \n\n\nThe Noether normalization lemma is a result of commutative algebra, introduced by Emmy Noether in 1926. It states that for any field \"k\", and any finitely generated commutative \"k\"-algebra \"A\", there exists a nonnegative integer \"d\" and algebraically independent elements \"y\", \"y\", ..., \"y\" in \"A\" \nsuch that \"A\" is a finitely generated module over the polynomial ring \"S\":=\"k\"[\"y\", \"y\", ..., \"y\"].\n\nThe theorem has a geometric interpretation. Suppose \"A\" is integral. Let \"S\" be the coordinate ring of the \"d\"-dimensional affine space formula_5, and \"A\" as the coordinate ring of some other \"d\"-dimensional affine variety \"X\". Then the inclusion map \"S\" → \"A\" induces a surjective finite morphism of affine varieties formula_6. The conclusion is that any affine variety is a branched covering of affine space.\n\nThe Noether normalization lemma is an important step to proving Hilbert's Nullstellensatz.\n\n\nNoether's (first) theorem states that every differentiable symmetry of the action of a physical system has a corresponding conservation law. The theorem was proven by mathematician Emmy Noether in 1915 and published in 1918, although a special case was proven by E. Cosserat & F. Cosserat in 1909. The action of a physical system is the integral over time of a Lagrangian function (which may or may not be an integral over space of a Lagrangian density function), from which the system's behavior can be determined by the principle of least action.\n\nNoether's theorem is used in theoretical physics and the calculus of variations. A generalization of the formulations on constants of motion in Lagrangian and Hamiltonian mechanics (developed in 1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian alone (e.g. systems with a Rayleigh dissipation function). In particular, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n\nNoether's theorem can be stated informally\n\n\nIn mathematics and theoretical physics, Noether's second theorem relates symmetries of an action functional with a system of differential equations. The action \"S\" of a physical system is an integral of a so-called Lagrangian function \"L\", from which the system's behavior can be determined by the principle of least action.\n\n\nIn mathematics, specifically abstract algebra, the isomorphism theorems are three theorems that describe the relationship between quotients, homomorphisms, and subobjects. Versions of the theorems exist for groups, rings, vector spaces, modules, Lie algebras, and various other algebraic structures. In universal algebra, the isomorphism theorems can be generalized to the context of algebras and congruences.\n\nThe isomorphism theorems were formulated in some generality for homomorphisms of modules by Emmy Noether in her paper \"Abstrakter Aufbau der Idealtheorie in algebraischen Zahl- und Funktionenkörpern\" which was published in 1927 in Mathematische Annalen. Less general versions of these theorems can be found in work of Richard Dedekind and previous papers by Noether.\n\nThree years later, B.L. van der Waerden published his influential \"Algebra,\" the first abstract algebra textbook that took the groups-rings-fields approach to the subject. Van der Waerden credited lectures by Noether on group theory and Emil Artin on algebra, as well as a seminar conducted by Artin, Wilhelm Blaschke, Otto Schreier, and van der Waerden himself on ideals as the main references. The three isomorphism theorems, called \"homomorphism theorem\", and \"two laws of isomorphism\" when applied to groups, appear explicitly.\n\n\nIn mathematics, the Lasker–Noether theorem states that every Noetherian ring is a Lasker ring, which means that every ideal can be decomposed as an intersection, called primary decomposition, of finitely many \"primary ideals\" (which are related to, but not quite the same as, powers of prime ideals). The theorem was first proven by for the special case of polynomial rings and convergent power series rings, and was proven in its full generality by .\n\nThe Lasker–Noether theorem is an extension of the fundamental theorem of arithmetic, and more generally the fundamental theorem of finitely generated abelian groups to all Noetherian rings. The Lasker–Noether theorem plays an important role in algebraic geometry, by asserting that every algebraic set may be uniquely decomposed into a finite union of irreducible components.\n\n\nIn algebraic number theory, the Albert–Brauer–Hasse–Noether theorem states that a central simple algebra over an algebraic number field \"K\" which splits over every completion \"K\" is a matrix algebra over \"K\". The theorem is an example of a local-global principle in algebraic number theory and \nleads to a complete description of finite-dimensional division algebras over algebraic number fields in terms of their local invariants. It was proved independently by Richard Brauer, Helmut Hasse, and Emmy Noether and by Abraham Adrian Albert.\n\n\nFields medalist Maryam Mirzakhani proved the long-standing conjecture that William Thurston's earthquake flow on Teichmüller space is ergodic.\n\n\n\n\n\nGerty Cori, together with Carl Ferdinand Cori, discovered the Cori cycle, the metabolic pathway in which lactate produced by anaerobic glycolysis in the muscles moves to the liver and is converted to glucose, which then returns to the muscles and is metabolized back to lactate.\n\nRosalyn Sussman Yalow developed the radioimmunoassay, an immunoassay that uses radiolabeled molecules in a stepwise formation of [immune complexes at the Veterans Administration Hospital in the Bronx, New York. This technique is used to accurately measure levels of substances such as hormones which are found in small concentrations in the body. \n\nBarbara McClintock discovered transposable elements (also known as transposons and jumping genes), DNA sequences which change their position within the genome. Transposons make up a large fraction of the DNA in eukaryotic cells (44% if the human genome and 90% of the maize genome ) and play an important role in genome function and evolution. . In \"Oxytricha\", which has a unique genetic system, these elements play a critical role in development.\n\nRita Levi-Montalcini and colleague Stanley Cohen discovered nerve growth factor, a neurotrophic factor and neuropeptide primarily involved in the regulation of growth, maintenance, proliferation, and survival of certain target neurons. This discovery was recognized with the Nobel Prize in Physiology or Medicine in 1986. \n\nChristiane Nüsslein-Volhard and colleague Eric Wieschaus were the first to describe gap genes, genes involved in the development of segmentation in Drosophila embryogenesis. This work was foundational to our understanding of the genetic control of embryonic development.\n\n\nElizabeth Blackburn, Carol W. Greider, and Jack W. Szostak co-discovered the enzyme telomerase, which replenishes the telomere, a structure found at the ends of chromosomes which protects the DNA in the rest of the chromosome from damage.\n\nMay-Britt Moser, together with Edvard Moser and their students Torkel Hafting, Marianne Fyhn and Sturla Molden, discovered grid cells, cells which contribute to the brain's positioning and navigation system. The grid cells of a freely moving animal fire when the animal is near the vertices of a hexagonal grid in the environment.\n\n"}
{"id": "5721896", "url": "https://en.wikipedia.org/wiki?curid=5721896", "title": "List of research parks", "text": "List of research parks\n\nThe following is a list of science park, technology parks and biomedical parks of the world, organized by continent.\n\nASEAN Economic Community\nReport listing all the Economic Zones in the ASEAN Economic Community from UNIDO Viet Nam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are approximately 170 university research parks in North America today.\n\nAlberta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSilicon Mallee Adelaide, South Australia\n\n\n"}
{"id": "319875", "url": "https://en.wikipedia.org/wiki?curid=319875", "title": "List of tests", "text": "List of tests\n\nThe following is an alphabetized and categorized list of notable tests.\n\n\n\n\n\n"}
{"id": "21468426", "url": "https://en.wikipedia.org/wiki?curid=21468426", "title": "Mapping controversies", "text": "Mapping controversies\n\nMapping controversies (MC) is an academic course taught in science studies, stemming from the writings of the French sociologist and philosopher Bruno Latour. MC focuses exclusively on the controversies surrounding scientific knowledge rather than the established scientific facts or outcomes. Thus, it helps sociologists, anthropologists and other social scientists get insights not into scientific knowledge \"per se\", but rather into \"the process of gaining knowledge\". Thus, MC sheds light on those intermediate stages corresponding to the actual research process and pinpoints the connections between scientific work and other types of activities.\n\nThe term \"mapping controversies\" was first suggested in relation to analysis of scientific and technological controversies, and then lately re-affirmed as a widely applicable methodological approach going beyond the boundaries of Science Studies. It is usually used for the methodology that identifies and tracks down the polemics or debate surrounding a scientific fact, and utilises various visualisation tools to present the problem in its complexity.\n\nRecently Latour initiated the project \"Mapping Controversies on Science for Politics (MACOSPOL)\". The showcase website is mappingcontroversies.net \n\nIn 2008-2009 several universities in Europe and USA started teaching \"Mapping Controversies\" courses for students in political sciences, engineering, and architecture.\n\nAn earlier attempt to stage controversies in museum settings took place at the Gallery of Research in Vienna in 2005.\n"}
{"id": "6198688", "url": "https://en.wikipedia.org/wiki?curid=6198688", "title": "Medical terminology", "text": "Medical terminology\n\nMedical terminology is language used to precisely describe the human body including its components, processes, conditions affecting it, and procedures performed upon it. Medical terminology is used in the field of medicine.\n\nMedical terminology has quite regular morphology, the same prefixes and suffixes are used to add meanings to different roots. The root of a term often refers to an organ, tissue, or condition. For example, in the disorder hypertension, the prefix \"hyper-\" means \"high\" or \"over\", and the root word \"tension\" refers to pressure, so the word \"hypertension\" refers to abnormally high blood pressure. The roots, prefixes and suffixes are often derived from Greek or Latin, and often quite dissimilar from their English-language variants. This regular morphology means that once a reasonable number of morphemes are learnt it becomes easy to understand very precise terms assembled from these morphemes. A lot of medical language is anatomical terminology, concerning itself with the names of various parts of the body.\n\nIn forming or understanding a word root, one needs a basic comprehension of the terms and the source language. The study of the origin of words is called etymology. For example, if a word was to be formed to indicate a condition of kidneys, there are two primary roots – one from Greek (νεφρός \"nephr(os)\") and one from Latin (\"ren(es)\"). Renal failure would be a condition of kidneys, and nephritis is also a condition, or inflammation, of the kidneys. The suffix \"-itis\" means inflammation, and the entire word conveys the meaning inflammation of the kidney. To continue using these terms, other combinations will be presented for the purpose of examples: The term \"supra-renal\" is a combination of the prefix \"supra-\" (meaning \"above\"), and the word root for kidney, and the entire word means \"situated above the kidneys\". The word \"nephrologist\" combines the root word for kidney to the suffix \"-ologist\" with the resultant meaning of \"one who studies the kidneys\".\n\nThe formation of plurals should usually be done using the rules of forming the proper plural form in the source language. Greek and Latin each have differing rules to be applied when forming the plural form of the word root. Often such details can be found using a medical dictionary.\n\nMedical terminology often uses words created using prefixes and suffixes in Latin and Ancient Greek. In medicine, their meanings, and their etymology, are informed by the language of origin. Prefixes and suffixes, primarily in Greek—but also in Latin, have a dropable \"-o-\". Medical roots generally go together according to language: Greek prefixes go with Greek suffixes and Latin prefixes with Latin suffixes. Although it is technically considered acceptable to create hybrid words, it is strongly preferred not to mix different lingual roots. Examples of well-accepted medical words that do mix lingual roots are neonatology and quadriplegia.\n\n\"Prefixes\" do not normally require further modification to be added to a word root because the prefix normally ends in a vowel or vowel sound, although in some cases they may assimilate slightly and an \"in-\" may change to \"im-\" or \"syn-\" to \"sym-\".\n\n\"Suffixes\" are attached to the end of a word root to add meaning such as condition, disease process, or procedure.\n\nIn the process of creating medical terminology, certain rules of language apply. These rules are part of language mechanics called linguistics. The word root is developed to include a vowel sound following the term to add a smoothing action to the sound of the word when applying a suffix. The result is the formation of a new term with a vowel attached (word root + vowel) called a combining form. In English, the most common vowel used in the formation of the combining form is the letter \"-o-\", added to the word root. For example if there is an inflammation of the stomach and intestines, this would be written as \"gastro-\" and \"enter-\" plus \"-itis\", gastroenteritis.\n\nSuffixes are categorized as either (1) needing the combining form, or (2) not needing the combining form since they start with a vowel.\n"}
{"id": "20552524", "url": "https://en.wikipedia.org/wiki?curid=20552524", "title": "Micro-pulling-down", "text": "Micro-pulling-down\n\nThe micro-pulling-down (µ-PD) method is a crystal growth technique based on continuous transport of the melted substance through micro-channel(s) made in a crucible bottom. Continuous solidification of the melt is progressed on a liquid/solid interface positioned under the crucible. In a steady state, both the melt and the crystal are pulled-down with a constant (but generally different) velocity.\n\nMany different types of crystal are grown by this technique, including YAlO, Si, Si-Ge, LiNbO, \nα-AlO, YO, ScO, \nLiF, CaF, BaF, etc.\n\nStandard routine procedure used in the growth of most of µ-PD crystals is well developed. The general stages of the growths include:\n\n\n\n"}
{"id": "34976715", "url": "https://en.wikipedia.org/wiki?curid=34976715", "title": "Mien Shiang", "text": "Mien Shiang\n\nMien Shiang is a 3,000-year-old Taoist practice that means literally face (mien) reading (shiang). In just moments, one can supposedly determine anyone’s \"Wu Xing\" — Five Element personality type — their character, behavior, and health potential — by analyzing their face. The Taoist Five Elements, Wood, Fire, Earth, Metal and Water, are metaphors devised by the ancient Taoist philosophers to explain the relationship, interaction, and ongoing change of everything in the Universe. In recent times the art of Face Reading is becoming more and more popular. Schools that teach Mien Shiang are becoming more wide spread.\n"}
{"id": "47317405", "url": "https://en.wikipedia.org/wiki?curid=47317405", "title": "Nature of Man Series", "text": "Nature of Man Series\n\nThe Nature of Man Series is a four-volume series of works in paleoanthropology by the prolific playwright, screenwriter, and science writer Robert Ardrey. The books in the series were published between 1961 and 1976.\n\nThe series majorly undermined standing assumptions in social sciences, leading to an abandonment of the \"blank slate\" hypothesis; incited a renaissance in the science of ethology; and led to widespread popular interest in human evolution and human origins.\n\nThe first work, \"African Genesis\" (1961), particularly helped revive interest in ethology, and was a direct precursor to the Konrad Lorenz's \"On Aggression\" (1966), Desmond Morris's \"The Naked Ape\" (1967), Lionel Tiger's \"Men in Groups\" (1969), and Tiger and Robin Fox's \"The Imperial Animal\" (1971). The director of the Smithsonian Institution's Human Origins Program Rick Potts, cited Ardrey's work as inspiring him to go anthropology.\n\nThe works were wildly popular and influenced the public imagination. Stanley Kubrick cited them as major influences in developing his films (1968) and \"A Clockwork Orange\" (1971).\n\nRobert Ardrey was a prolific playwright, screenwriter, and science writer. By the time he returned to the sciences in the 1950s, he had already had a decorated Hollywood and Broadway career, including the award of a Guggenheim Fellowship and an Academy Award nomination for best screenplay.\n\nIn 1955 Ardrey travelled to Africa, where he wrote a series of articles for \"The Reporter\". At the same time he renewed an acquaintance with prominent geologist Richard Foster Flint and investigated claims made by Raymond Dart about a specimen of \"Australopithecus africanus\". This trip would initiate the decades of work Ardrey completed in the field of human evolution.\n\nThe central thesis of \"African Genesis: A Personal Investigation into the Animal Origins and Nature of Man\" was that early man evolved from carnivorous African predecessors, and not, as was then the scientific consensus, from Asian herbivores. It drew particularly on the scientific work of Raymond Dart and Konrad Lorenz. This thesis has been proven and is now scientific doctrine.\n\n\"African Genesis\" also challenged a key methodological assumption of the social sciences, namely that human behavior was distinct from animal behavior. Ardrey instead asserted that evolutionarily inherited traits were a major factor in determining human behavior. This was a hugely controversial hypothesis, though it has gained widespread acceptance today. It was a major theme that would extend throughout the \"Nature of Man\" books and continue to surround them with controversy.\n\n\"African Genesis\" was a major popular success. It was an international bestseller translated into dozens of languages. In 1962 it was a finalist for the National Book Award in nonfiction. In 1969 \"Time\" magazine named \"African Genesis\" the most notable nonfiction book of the 1960s.\n\n\"The Territorial Imperative: A Personal Inquiry Into the Animal Origins of Property and Nations\" extends Ardrey's work in examining the effects of inherited evolutionary traits on human social behavior with an emphasis on the hold that territory has on man. In particular it demonstrates the influence of the drive to possess territory on such phenomena as property ownership and nation-building.\n\n\"The Territorial Imperative\" further developed the nascent science of ethology and increased public interest in human origins.\n\nLike \"African Genesis\" it was also an international bestseller and saw translation into dozens of languages. It influenced several notable figures. Stanley Kubrick cited Ardrey as an inspiration for his films \"\" and \"A Clockwork Orange\". The strategic analyst Andrew Marshall and U.S. Secretary of Defense James Schlesinger are known to have discussed \"The Territorial Imperative\" in connection to military-strategic thinking.\n\n\"The Social Contract: A Personal Inquiry into the Evolutionary Sources of Order and Disorder\" is the most controversial book of the \"Nature of Man\" series. It sought to apply evolutionary thinking to the creation of social order. In particular it examined inherited characteristics' effects in determining hierarchy and inequality. Ardrey argued that, while inequality was not necessarily a social evil, it could only be justly expressed under conditions of absolute equality of opportunity. He also argued that the presence of inequality does not justify the domination of the weak by the strong. \"Ardrey showed that in all societies at any level of the animal world, structures exist to protect the vulnerable, and that this is an evolutionary advantage as it protects diversity, diversity being essential for creativity.\"\n\n\"The Social Contract\" continued Ardrey's refutation of cultural determinists through interwoven analyses of animal and human behavior. It also emphasized the importance of a reasoned respect for nature, foreshadowing the environmental concerns of \"The Hunting Hypothesis.\"\n\n\"The Hunting Hypothesis: A Personal Conclusion Concerning the Evolutionary Nature of Man\" continued Ardrey's examination of the importance of inherited evolutionary traits. In particular it demonstrated the determinant force of traits that co-evolved in early man with hunting behavior.\n\nAt the time of publication, it was not even commonly accepted that early man were hunters, much less that hunting behavior influenced their evolution. Following publication of Ardrey's work this thesis gained support and eventually widespread acceptance.\"For decades researchers have been locked in debate over how and when hunting began and how big a role it played in human evolution. Recent analyses of human anatomy, stone tools and animal bones are helping to fill in the details of this game-changing shift in subsistence strategy. This evidence indicates that hunting evolved far earlier than some scholars had envisioned – and profoundly impacted subsequent human evolution.\"\n\n\"The Hunting Hypothesis\" was also one of the first books to warn about climate change as a possible existential threat to mankind.\n\n\"The Hunting Hypothesis,\" with some exceptions, was remarkably well reviewed. The famed biologist and naturalist E. O. Wilson, the noted anthropologist Colin Turnbull, the acclaimed journalist Max Lerner, and the noteworthy social scientist Roger Masters, among others, all wrote effusive reviews. Antony Jay wrote that \"Robert Ardrey's books are the most important to be written since the war and arguable in the 20th century.\"\n\n"}
{"id": "52660479", "url": "https://en.wikipedia.org/wiki?curid=52660479", "title": "Open energy system databases", "text": "Open energy system databases\n\nOpen energy system database projects employ open data methods to collect, clean, and republish energy-related datasets for open use. The resulting information is then available, given a suitable open license, for statistical analysis and for building numerical energy system models, including open energy system models. Permissive licenses like Creative Commons CC0 and are preferred, but some projects will house data made public under market transparency regulations and carrying unqualified copyright.\n\nThe databases themselves may furnish information on national power plant fleets, renewable generation assets, transmission networks, time series for electricity loads, dispatch, spot prices, and cross-border trades, weather information, and similar. They may also offer other energy statistics including fossil fuel imports and exports, gas, oil, and coal prices, emissions certificate prices, and information on energy efficiency costs and benefits.\n\nMuch of the data is sourced from official or semi-official agencies, including national statistics offices, transmission system operators, and electricity market operators. Data is also crowdsourced using public wikis and public upload facilities. Projects usually also maintain a strict record of the provenance and version histories of the datasets they hold. Some projects, as part of their mandate, also try to persuade primary data providers to release their data under more liberal licensing conditions.\n\nTwo drivers favor the establishment of such databases. The first is a wish to reduce the duplication of effort that accompanies each new analytical project as it assembles and processes the data that it needs from primary sources. And the second is an increasing desire to make public policy energy models more transparent to improve their acceptance by policymakers and the public. Better transparency dictates the use of open information, able to be accessed and scrutinized by third-parties, in addition to releasing the source code for the models in question.\n\nIn the mid-1990s, energy models used structured text files for data interchange but efforts were being made to migrate to relational database management systems for data processing. These early efforts however remained local to a project and did not involve online publishing or open data principles.\n\nThe first energy information portal to go live was OpenEI in late 2009, followed by reegle in 2011.\n\nA 2012 paper marks the first scientific publication to advocate the crowdsourcing of energy data. The 2012 PhD thesis by Chris Davis also discusses the crowdsourcing of energy data in some depth. A 2016 thesis surveyed the spatial (GIS) information requirements for energy planning and finds that most types of data, with the exception of energy expenditure data, are available but nonetheless remain scattered and poorly coordinated.\n\nIn terms of open data, a 2017 paper concludes that energy research has lagged behind other fields, most notably physics, biotechnology, and medicine. The paper also lists the benefits of open data and open models and discusses the reasons that many projects nonetheless remain closed. A one-page opinion piece from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for peer review.\n\nData models are central to the design and organization of databases. Open energy database projects generally try to develop and adhere to well resolved data models, using defacto and published standards where applicable. Some projects attempt to coordinate their data models in order to harmonize their data and improve its utility. Defining and maintaining suitable metadata is also a key issue. The life-cycle management of data includes, but is not limited to, the use of version control to track the provenance of incoming and cleansed data. Some sites allow users to comment on and rate individual datasets.\n\nIssues surrounding copyright remain at the forefront with regard to open energy data. As noted, most energy datasets are collated and published by official or semi-official sources. But many of the publicly available energy datasets carry no license, limiting their reuse in numerical and statistical models, open or otherwise. Copyright protected material cannot lawfully be circulated, nor can it be modified and republished.\n\nMeasures to enforce market transparency have not helped much because the associated information is again not licensed to enable modification and republication. Transparency measures include the 2013 European energy market transparency regulation 543/2013. Indeed, 543/2013 \"is only an obligation to publish, not an obligation to license\". Notwithstanding, 543/2013 does enable downloaded data to be computer processed with legal certainty.\n\nEnergy databases with hardware located with the European Union are protected under a general database law, irrespective of the legal status of the information they hold.\nDatabase rights not waived by public sector providers significantly restrict the amount of data a user can lawfully access.\n\nA December 2017 submission by energy researchers in Germany and elsewhere highlighted a number of concerns over the re-use of public sector information within the Europe Union.\nThe submission drew heavily on a recent legal opinion covering electricity data.\n\nNational and international energy statistics are published regularly by governments and international agencies, such as the IEA. In 2016 the United Nations issued guidelines for energy statistics. While the definitions and sectoral breakdowns are useful when defining models, the information provided is rarely in sufficient detail to enable its use in high-resolution energy system models.\n\nThere are few published standards covering the collection and structuring of high-resolution energy system data. The IEC Common Information Model (CIM) defines data exchange protocols for low and high voltage electricity networks.\n\nEnergy system models are data intensive and normally require detailed information from a number of sources. Dedicated projects to collect, collate, document, and republish energy system datasets have arisen to service this need. Most database projects prefer open data, issued under free licenses, but some will accept datasets with proprietary licenses in the absence of other options.\n\nThe OpenStreetMap project, which uses the Open Database License (ODbL), contains geographic information about energy system components, including transmission lines. Wikipedia itself has a growing set of information related to national energy systems, including descriptions of individual power stations.\n\nThe following table summarizes projects that specifically publish open energy system data. Some are general repositories while others (for instance, oedb) are designed to interact with open energy system models in real-time.\n\nThe Energy Research Data Portal for South Africa is being developed by the Energy Research Centre, University of Cape Town, Cape Town, South Africa. Coverage includes South Africa and certain other African countries where the Centre undertakes projects. The website uses the CKAN open source data portal software. A number of data formats are supported, including CSV and XLSX. The site also offers an API for automated downloads. , the portal contained 65datasets.\n\nThe energydata.info project from the World Bank Group, Washington, DC, USA is an energy database portal designed to support national development by improving public access to energy information. As well as sharing data, the platform also offers tools to visualize and analyze energy data. Although the World Bank Group has made available a number of dataset and apps, external users and organizations are encouraged to contribute. The concepts of open data and open source development are central to the project. energydata.info uses its own fork of the CKAN open source data portal as its web-based platform. The Creative Commons CC BY 4.0 license is preferred for data but other open licenses can be deployed. Users are also bound by the terms of use for the site.\n\n, the database held 131datasets, the great majority related to developing countries. The datasets are tagged and can be easily filtered. A number of download formats, including GIS files, are supported: CSV, XLS, XLSX, ArcGIS, Esri, GeoJSON, KML, and SHP. Some datasets are also offered as HTML. Again, , four apps are available. Some are web-based and run from a browser.\n\nThe semantic wiki-site and database Enipedia lists energy systems data worldwide. Enipedia is maintained by the Energy and Industry Group, Faculty of Technology, Policy and Management, Delft University of Technology, Delft, the Netherlands. A key tenet of Enipedia is that data displayed on the wiki is not trapped within the wiki, but can be extracted via SPARQL queries and used to populate new tools. Any programming environment that can download content from a URL can be used to obtain data. Enipedia went live in March 2011, judging by traffic figures quoted by Davis.\n\nA 2010 study describes how community driven data collection, processing, curation, and sharing is revolutionizing the data needs of industrial ecology and energy system analysis. A 2012 chapter introduces a system of systems engineering (SoSE) perspective and outlines how agent-based models and crowdsourced data can contribute to the solving of global issues.\n\nThe OpenEnergy Platform (OEP) is a collaborative versioned dataset repository for storing open energy system model datasets. A dataset is presumed to be in the form of a database table, together with metadata. Registered users can upload and download datasets manually using a web-interface or programmatically via an API using HTTP POST calls. Uploaded datasets are screened for integrity using deterministic rules and then subject to confirmation by a moderator. The use of versioning means that any prior state of the database can be accessed (as recommended in this 2012 paper). Hence, the repository is specifically designed to interoperate with energy system models. The backend is a PostgreSQL object-relational database under subversion version control. Open source licenses are specific to each dataset. Unlike other database projects, users can download the current version (the public tables) of the entire PostgreSQL database or any previous version. Initial development is being lead by the Reiner Lemoine Institute, Berlin, Germany.\n\nThe Open Data Energy Networks ( or ) portal is run by eight partners, led by the French national transmission system operator (TSO) Réseau de Transport d'Électricité (RTE). The portal was previously known as Open Data RTE. The site offers electricity system datasets under a Creative Commons compatible license, with metadata, an RSS feed for notifying updates, and an interface for submitting questions. of information obtained from the site can also register third-party URLs (be they publications or webpages) against specific datasets.\n\nThe portal uses the French Government Licence Ouverte license and this is explicitly compatible with the United Kingdom Open Government Licence (OGL), the Creative Commons license (and thereby later versions), and the Open Data Commons license.\n\nThe site hosts electricity, gas, and weather information related to France.\n\nThe Open Power System Data (OPSD) project seeks to characterize the German and western European power plant fleets, their associated transmission network, and related information and to make that data available to energy modelers and analysts. The platform was originally implemented by the University of Flensburg, DIW Berlin, the Technical University of Berlin, and the energy economics consultancy Neon Neue Energieökonomik, all from Germany. The first phase of the project, from August 2015 to July 2017, was funded by the Federal Ministry for Economic Affairs and Energy (BMWi) for . The project later received funding for a second phase, from January 2018 to December 2020, with ETH Zurich replacing Flensburg University as a partner.\n\nDevelopers collate and harmonize data from a range of government, regulatory, and industry sources throughout Europe. The website and the metadata utilize English, whereas the original material can be in any one of 24languages. Datasets follow the emerging frictionless data package standard being developed by Open Knowledge International (OKI). The website was launched on 28October 2016. , the project offers the following primary packages, for Germany and other European countries:\n\n\nIn addition, the project hosts selected contributed packages:\n\n\nTo facilitate analysis, the data is aggregated into large structured files (in CSV format) and loaded into data packages with standardized machine-readable metadata (in JSON format). The same data is usually also provided as XLSX (Excel) and SQLite files. The datasets can be accessed in real-time using stable URLs. The Python scripts deployed for data processing are available on GitHub and carry an MIT license. The licensing conditions for the data itself depends on the source and varies in terms of openness. Previous versions of the datasets and scripts can be recovered in order to track changes or replicate earlier studies. The project also engages with energy data providers, such as transmission system operators (TSO) and ENTSO-E, to encourage them to make their data available under open licenses (for instance, Creative Commons and ODbL licenses).\n\nA number of published electricity market modeling analyses are based on OPSD data.\n\nIn 2017, the Open Power System Data project won the Schleswig-Holstein Open Science Award and the Germany Land of Ideas award.\n\nOpen Energy Information (OpenEI) is a collaborative website, run by the US government, providing open energy data to software developers, analysts, users, consumers, and policymakers. The platform is sponsored by the United States Department of Energy (DOE) and is being developed by the National Renewable Energy Laboratory (NREL). OpenEI launched on 9December 2009. While much of its data is from US government sources, the platform is intended to be open and global in scope.\n\nOpenEI provides two mechanisms for contributing structured information: a semantic wiki (using MediaWiki and the Semantic MediaWiki extension) for collaboratively-managed resources and a dataset upload facility for contributor-controlled resources. US government data is distributed under a CC0 public domain dedication, whereas other contributors are free to select an open data license of their choice. Users can rate data using a five-star system, based on accessibility, adaptability, usefulness, and general quality. Individual datasets can be manually downloaded in an appropriate format, often as CSV files. Scripts for processing data can also be shared through the site. In order to build a community around the platform, a number of forums are offered covering energy system data and related topics.\n\nMost of the data on OpenEI is exposed as linked open data (LOD) (described elsewhere on this page). OpenEI also uses LOD methods to populate its definitions throughout the wiki with real-time connections to DBPedia, reegle, and Wikipedia.\n\nOpenEI has been used to classify geothermal resources in the United States. And to publicize municipal utility rates, again within the US.\n\nOpenGridMap employs crowdsourcing techniques to gather detailed data on electricity network components and then infer a realistic network structure using methods from statistics and graph theory. The scope of the project is worldwide and both distribution and transmission networks can be reverse engineered. The project is managed by the Chair of Business Information Systems, TUM Department of Informatics, Technical University of Munich, Munich, Germany. The project maintains a website and a Facebook page and provides an Android mobile app to help the public document electrical devices, such as transformers and substations. The bulk of the data is being made available under a Creative Commons license. The processing software is written primarily in Python and MATLAB and is hosted on GitHub.\n\nOpenGridMap provides a tailored GIS web application, layered on OpenStreetMap, which contributors can use to upload and edit information directly. The same database automatically stores field recordings submitted by the mobile app. Subsequent classification by experts allows normal citizens to document and photograph electrical components and have them correctly identified. The project is experimenting with the use of hobby drones to obtain better information on associated facilities, such as photovoltaic installations. Transmission line data is also sourced from and shared with OpenStreetMap. Each component record is verified by a moderator.\n\nOnce sufficient data is available, the transnet software is run to produce a likely network, using statistical correlation, Voronoi partitioning, and minimum spanning tree (MST) algorithms. The resulting network can be exported in CSV (separate files for nodes and lines), XML, and CIM formats. CIM models are well suited for translation into software-specific data formats for further analysis, including power grid simulation. Transnet also displays descriptive statistics about the resulting network for visual confirmation.\n\nThe project is motivated by the need to provide datasets for high-resolution energy system models, so that energy system transitions (like the German \"Energiewende\") can be better managed, both technically and policy-wise. The rapid expansion of renewable generation and the anticipated uptake of electric vehicles means that electricity system models must increasingly represent distribution and transmission networks in some detail.\n\n, OpenGridMap techniques have been used to estimate the low voltage network in the German city of Garching and to estimate the high voltage grids in several other countries.\n\nreegle is a clean energy information portal covering renewable energy, energy efficiency, and climate compatible development topics. reegle was launched in 2006 by REEEP and REN21 with funding from the Dutch (VROM), German (BMU), and UK (Defra) environment ministries. Originally released as a specialized internet search engine, reegle was relaunched in 2011 as an information portal.\n\nreegle offers and utilizes linked open data (LOD) (described elsewhere on this page). Sources of data include UN and World Bank databases, as well as dedicated partners around the world. reegle maintains a comprehensive structured glossary (driven by an LOD-compliant thesaurus) of energy and climate compatible development terms to assist with the tagging of datasets. The glossary also facilitates intelligent web searches.\n\nreegle offers country profiles which collate and display energy data on a per-country basis for most of the world. These profiles are kept current automatically using LOD techniques.\n\nRenewables.ninja is a website that can calculate the hourly power output from solar photovoltaic installations and wind farms located anywhere in the world. The website is a joint project between the Department of Environmental Systems Science, ETH Zurich, Zürich, Switzerland and the Centre for Environmental Policy, Imperial College London, London, United Kingdom. The website went live during September 2016. The resulting time series are provided under a Creative Commons license and the underlying power plant models are published using a BSD-new license. , only the solar model, written in Python, has been released.\n\nThe project relies on weather data derived from meteorological reanalysis models and weather satellite images. More specifically, it uses the 2016 MERRA-2 reanalysis dataset from NASA and satellite images from CM-SAF SARAH. For locations in Europe, this weather data is further \"corrected\" by country so that it better fits with the output from known PV installations and windfarms. Two 2016 papers describe the methods used in detail in relation to Europe. The first covers the calculation of PV power. And the second covers the calculation of wind power.\n\nThe website displays an interactive world map to aid the selection of a site. Users can then choose a plant type and enter some technical characteristics. , only year 2014 data can be served, due to technical restrictions. The results are automatically plotted and are available for download in hourly CSV format with or without the associated weather information. The site offers an API for programmatic dataset recovery using token-based authorization. Examples deploying cURL and Python are provided.\n\nA number of studies have been undertaking using the power production datasets underpinning the website (these studies predate the launch of the website), with the bulk focusing on energy options for Great Britain.\n\nThe SMARD site (pronounced \"smart\") serves electricity market data from Germany, Austria, and Luxembourg and also provides visual information. The electricity market plots and their underlying time series are released under a permissive CC BY 4.0 license. The site itself was launched on 3July 2017 in German and an English translation followed shortly. The data portal is mandated under the German Energy Industry Act (\"\" or \"EnWG\") section §111d, introduced as an amendment on 13October 2016. Four table formats are offered: CSV, XLS, XML, and PDF. The maximum sampling resolution is . Market data visuals or plots can be downloaded in PDF, SVG, PNG, and JPG formats. Representative output is shown in the thumbnail (on the left), in this case mid-winter dispatch over two days for the whole of Germany. The horizontal ordering by generation type is first split into renewable and conventional generation and then based on merit.\n\n\n\n"}
{"id": "43529327", "url": "https://en.wikipedia.org/wiki?curid=43529327", "title": "Postnormal times", "text": "Postnormal times\n\nPostnormal times (PNT) is a concept developed by Ziauddin Sardar as a development of Post-normal Science. Sardar describes the present as \"postnormal times\", \"in an in-between period where old orthodoxies are dying, new ones have yet to be born, and very few things seem to make sense.\"\n\nIn support of engaging communities of various scope and scale on how to best navigate PNT and imagine preferred pathways toward the future(s), Sardar and Sweeney published an article in the journal \"Futures\" outlining The Three Tomorrows method, which fills a gap in the field as \"many methods of futures and foresight seldom incorporate pluralism and diversity intrinsically in their frameworks, and few, if any, emphasize the dynamic and merging nature of futures possibilities, or highlight the ignorance and uncertainties we constantly confront.\"\n\nRakesh Kapoor criticized PNT in 2011 as a Western concept, that does not apply to India and other emerging markets. Sam Cole criticised the three C’s of PNT (chaos, complexity and contradictions) as \"Alliterative Logic, theorizing through alliterative word-triads that is not based on empirical evidence\". Jay Gray has suggested that PNT is embryonic, needs a more robust framework, and should be extended to include C S Holling's adaptive cycle. Scientists working on complex evolving systems have pointed out that PNT recalls the ‘Long Waves’ of Kondratiev and Joseph Schumpeter’s view of waves of \"creative destruction.\"\n\nPNT is one of the core areas of research for the Center for Postnormal Policy and Futures Studies at East-West University in Chicago, Illinois, US. A number of articles and editorials on PNT have been published in the journal \"East-West Affairs\".\n\n"}
{"id": "55779967", "url": "https://en.wikipedia.org/wiki?curid=55779967", "title": "Priority certificate", "text": "Priority certificate\n\nA priority certificate is a document attesting that the entities named in such a certificate are the first to discover a phenomenon from nature, the first proponent of a theory, abstract idea, solution to a problem, proof of a theorem etc. A person who makes a new and useful discovery is entitled to receive such a priority certificate for that specific discovery. \n\nPrivate or public companies and organizations, such as universities, R&D institutions, trade shows and exhibitions are known to grant priority certificates to confer formal recognition upon the claimant(s).\n\n"}
{"id": "23047", "url": "https://en.wikipedia.org/wiki?curid=23047", "title": "Pseudoscience", "text": "Pseudoscience\n\nPseudoscience consists of statements, beliefs, or practices that are claimed to be both scientific and factual, but are incompatible with the scientific method. Pseudoscience is often characterized by contradictory, exaggerated or unfalsifiable claims; reliance on confirmation bias rather than rigorous attempts at refutation; lack of openness to evaluation by other experts; and absence of systematic practices when developing theories, and continued adherence long after they have been experimentally discredited. The term \"pseudoscience\" is considered pejorative because it suggests something is being presented as science inaccurately or even deceptively. Those described as practicing or advocating pseudoscience often dispute the characterization.\n\nThe demarcation between science and pseudoscience has philosophical and scientific implications. Differentiating science from pseudoscience has practical implications in the case of health care, expert testimony, environmental policies, and science education. Distinguishing scientific facts and theories from pseudoscientific beliefs, such as those found in astrology, alchemy, alternative medicine, occult beliefs, and creation science, is part of science education and scientific literacy.\n\nPseudoscience can cause negative consequences in the real world. Antivaccine activists present pseudoscientific studies that falsely call into question the safety of vaccines. Homeopathic remedies with no active ingredients have been promoted as treatment for deadly diseases.\n\nThe word \"pseudoscience\" is derived from the Greek root \"pseudo\" meaning false and the English word \"science\", from the Latin word \"scientia\", meaning \"knowledge\". Although the term has been in use since at least the late 18th century (e.g. in 1796 by James Pettit Andrews in reference to alchemy) the concept of pseudoscience as distinct from real or proper science seems to have become more widespread during the mid-19th century. Among the earliest uses of \"pseudo-science\" was in an 1844 article in the \"Northern Journal of Medicine\", issue 387:\nAn earlier use of the term was in 1843 by the French physiologist François Magendie. During the 20th century, the word was used pejoratively to describe explanations of phenomena which were claimed to be scientific, but which were not in fact supported by reliable experimental evidence. From time-to-time, though, the usage of the word occurred in a more formal, technical manner in response to a perceived threat to individual and institutional security in a social and cultural setting.\n\nPhilosophers classify types of knowledge. In English, the word \"science\" is used to indicate specifically the natural sciences and related fields, which are called the social sciences.\n\nDifferent philosophers of science may disagree on the exact limits – for example, is mathematics a formal science that is closer to the empirical ones, or is pure mathematics closer to the philosophical study of logic and therefore not a science? – but all agree that all of the ideas that are not scientific are non-scientific. The large category of non-science includes all matters outside the natural and social sciences, such as the study of history, metaphysics, religion, art, and the humanities.\n\nDividing the category again, unscientific claims are a subset of the large category of non-scientific claims. This category specifically includes all matters that are directly opposed to good science. Un-science includes both bad science (such as an error made in a good-faith attempt at learning something about the natural world) and pseudoscience. Thus pseudoscience is a subset of un-science, and un-science, in turn, is subset of non-science.\n\nPseudoscience is differentiated from science because – although it claims to be science – pseudoscience does not adhere to accepted scientific standards, such as the scientific method, falsifiability of claims, and Mertonian norms.\n\nA number of basic principles are accepted by scientists as standards for determining whether a body of knowledge, method, or practice is scientific. Experimental results should be reproducible and verified by other researchers. These principles are intended to ensure experiments can be reproduced measurably given the same conditions, allowing further investigation to determine whether a hypothesis or theory related to given phenomena is valid and reliable. Standards require the scientific method to be applied throughout, and bias to be controlled for or eliminated through randomization, fair sampling procedures, blinding of studies, and other methods. All gathered data, including the experimental or environmental conditions, are expected to be documented for scrutiny and made available for peer review, allowing further experiments or studies to be conducted to confirm or falsify results. Statistical quantification of significance, confidence, and error are also important tools for the scientific method.\n\nDuring the mid-20th century, the philosopher Karl Popper emphasized the criterion of falsifiability to distinguish science from nonscience. Statements, hypotheses, or theories have falsifiability or refutability if there is the inherent possibility that they can be proven false. That is, if it is possible to conceive of an observation or an argument which negates them. Popper used astrology and psychoanalysis as examples of pseudoscience and Einstein's theory of relativity as an example of science. He subdivided nonscience into philosophical, mathematical, mythological, religious and metaphysical formulations on one hand, and pseudoscientific formulations on the other, though he did not provide clear criteria for the differences.\n\nAnother example which shows the distinct need for a claim to be falsifiable was stated in Carl Sagan's publication \"The Demon-Haunted World\" when he discusses an invisible dragon that he has in his garage. The point is made that there is no physical test to refute the claim of the presence of this dragon. No matter what test you think you can devise, there is then a reason why this does not apply to the invisible dragon, so one can never prove that the initial claim is wrong. Sagan concludes; \"Now, what's the difference between an invisible, incorporeal, floating dragon who spits heatless fire and no dragon at all?\". He states that \"your inability to invalidate my hypothesis is not at all the same thing as proving it true\", once again explaining that even if such a claim were true, it would be outside the realm of scientific inquiry.\n\nDuring 1942, Robert K. Merton identified a set of five \"norms\" which he characterized as what makes a real science. If any of the norms were violated, Merton considered the enterprise to be nonscience. These are not broadly accepted by the scientific community. His norms were:\n\nDuring 1978, Paul Thagard proposed that pseudoscience is primarily distinguishable from science when it is less progressive than alternative theories over a long period of time, and its proponents fail to acknowledge or address problems with the theory. During 1983, Mario Bunge has suggested the categories of \"belief fields\" and \"research fields\" to help distinguish between pseudoscience and science, where the former is primarily personal and subjective and the latter involves a certain systematic method.\n\nPhilosophers of science such as Paul Feyerabend argued that a distinction between science and nonscience is neither possible nor desirable. Among the issues which can make the distinction difficult is variable rates of evolution among the theories and methods of science in response to new data.\n\nLarry Laudan has suggested pseudoscience has no scientific meaning and is mostly used to describe our emotions: \"If we would stand up and be counted on the side of reason, we ought to drop terms like 'pseudo-science' and 'unscientific' from our vocabulary; they are just hollow phrases which do only emotive work for us\". Likewise, Richard McNally states, \"The term 'pseudoscience' has become little more than an inflammatory buzzword for quickly dismissing one's opponents in media sound-bites\" and \"When therapeutic entrepreneurs make claims on behalf of their interventions, we should not waste our time trying to determine whether their interventions qualify as pseudoscientific. Rather, we should ask them: How do you know that your intervention works? What is your evidence?\"\n\nFor philosophers Silvio Funtowicz and Jerome R. Ravetz \"pseudo-science may be defined as one where the uncertainty of its inputs must be suppressed, lest they render its outputs totally indeterminate\". The definition, in the book \"Uncertainty and quality in science for policy\" (p. 54), alludes to the loss of craft skills in handling quantitative information, and to the bad practice of achieving precision in prediction (inference) only at the expenses of ignoring uncertainty in the input which was used to formulate the prediction. This use of the term is common among practitioners of post-normal science. Understood in this way, pseudoscience can be fought using good practices to assesses uncertainty in quantitative information, such as NUSAP and – in the case of mathematical modelling – sensitivity auditing.\n\nThe history of pseudoscience is the study of pseudoscientific theories over time. A pseudoscience is a set of ideas that presents itself as science, while it does not meet the criteria to be properly called such.\n\nDistinguishing between proper science and pseudoscience is sometimes difficult. One proposal for demarcation between the two is the falsification criterion, attributed most notably to the philosopher Karl Popper. In the history of science and the history of pseudoscience it can be especially difficult to separate the two, because some sciences developed from pseudosciences. An example of this transformation is the science chemistry, which traces its origins to pseudoscientific or pre-scientific study of alchemy.\n\nThe vast diversity in pseudosciences further complicates the history of science. Some modern pseudosciences, such as astrology and acupuncture, originated before the scientific era. Others developed as part of an ideology, such as Lysenkoism, or as a response to perceived threats to an ideology. Examples of this ideological process are creation science and intelligent design, which were developed in response to the scientific theory of evolution.\n\nA topic, practice, or body of knowledge might reasonably be termed pseudoscientific when it is presented as consistent with the norms of scientific research, but it demonstrably fails to meet these norms.\n\nKarl Popper stated it is insufficient to distinguish science from pseudoscience, or from metaphysics (such as the philosophical question of what existence means), by the criterion of rigorous adherence to the empirical method, which is essentially inductive, based on observation or experimentation. He proposed a method to distinguish between genuine empirical, nonempirical or even pseudoempirical methods. The latter case was exemplified by astrology, which appeals to observation and experimentation. While it had astonishing empirical evidence based on observation, on horoscopes and biographies, it crucially failed to use acceptable scientific standards. Popper proposed falsifiability as an important criterion in distinguishing science from pseudoscience.\n\nTo demonstrate this point, Popper gave two cases of human behavior and typical explanations from Sigmund Freud and Alfred Adler's theories: \"that of a man who pushes a child into the water with the intention of drowning it; and that of a man who sacrifices his life in an attempt to save the child.\" From Freud's perspective, the first man would have suffered from psychological repression, probably originating from an Oedipus complex, whereas the second man had attained sublimation. From Adler's perspective, the first and second man suffered from feelings of inferiority and had to prove himself, which drove him to commit the crime or, in the second case, drove him to rescue the child. Popper was not able to find any counterexamples of human behavior in which the behavior could not be explained in the terms of Adler's or Freud's theory. Popper argued it was that the observation always fitted or confirmed the theory which, rather than being its strength, was actually its weakness.\n\nIn contrast, Popper gave the example of Einstein's gravitational theory, which predicted \"light must be attracted by heavy bodies (such as the Sun), precisely as material bodies were attracted.\" Following from this, stars closer to the Sun would appear to have moved a small distance away from the Sun, and away from each other. This prediction was particularly striking to Popper because it involved considerable risk. The brightness of the Sun prevented this effect from being observed under normal circumstances, so photographs had to be taken during an eclipse and compared to photographs taken at night. Popper states, \"If observation shows that the predicted effect is definitely absent, then the theory is simply refuted.\" Popper summed up his criterion for the scientific status of a theory as depending on its falsifiability, refutability, or testability.\n\nPaul R. Thagard used astrology as a case study to distinguish science from pseudoscience and proposed principles and criteria to delineate them. First, astrology has not progressed in that it has not been updated nor added any explanatory power since Ptolemy. Second, it has ignored outstanding problems such as the precession of equinoxes in astronomy. Third, alternative theories of personality and behavior have grown progressively to encompass explanations of phenomena which astrology statically attributes to heavenly forces. Fourth, astrologers have remained uninterested in furthering the theory to deal with outstanding problems or in critically evaluating the theory in relation to other theories. Thagard intended this criterion to be extended to areas other than astrology. He believed it would delineate as pseudoscientific such practices as witchcraft and pyramidology, while leaving physics, chemistry and biology in the realm of science. Biorhythms, which like astrology relied uncritically on birth dates, did not meet the criterion of pseudoscience at the time because there were no alternative explanations for the same observations. The use of this criterion has the consequence that a theory can be scientific at one time and pseudoscientific at a later time.\n\nScience is also distinguishable from revelation, theology, or spirituality in that it offers insight into the physical world obtained by empirical research and testing. The most notable disputes concern the evolution of living organisms, the idea of common descent, the geologic history of the Earth, the formation of the solar system, and the origin of the universe. Systems of belief that derive from divine or inspired knowledge are not considered pseudoscience if they do not claim either to be scientific or to overturn well-established science. Moreover, some specific religious claims, such as the power of intercessory prayer to heal the sick, although they may be based on untestable beliefs, can be tested by the scientific method.\n\nSome statements and common beliefs of popular science may not meet the criteria of science. \"Pop\" science may blur the divide between science and pseudoscience among the general public, and may also involve science fiction. Indeed, pop science is disseminated to, and can also easily emanate from, persons not accountable to scientific methodology and expert peer review.\n\nIf the claims of a given field can be tested experimentally and standards are upheld, it is not pseudoscience, however odd, astonishing, or counterintuitive the claims are. If claims made are inconsistent with existing experimental results or established theory, but the method is sound, caution should be used, since science consists of testing hypotheses which may turn out to be false. In such a case, the work may be better described as ideas that are \"not yet generally accepted\". \"Protoscience\" is a term sometimes used to describe a hypothesis that has not yet been tested adequately by the scientific method, but which is otherwise consistent with existing science or which, where inconsistent, offers reasonable account of the inconsistency. It may also describe the transition from a body of practical knowledge into a scientific field.\n\n\n\n\n\n\n\nA large percentage of the United States population lacks scientific literacy, not adequately understanding scientific principles and method. In the \"Journal of College Science Teaching\", Art Hobson writes, \"Pseudoscientific beliefs are surprisingly widespread in our culture even among public school science teachers and newspaper editors, and are closely related to scientific illiteracy.\" However, a 10,000-student study in the same journal concluded there was no strong correlation between science knowledge and belief in pseudoscience.\n\nIn his book \"The Demon-Haunted World\" Carl Sagan discusses the government of China and the Chinese Communist Party's concern about Western pseudoscience developments and certain ancient Chinese practices in China. He sees pseudoscience occurring in the United States as part of a worldwide trend and suggests its causes, dangers, diagnosis and treatment may be universal.\n\nDuring 2006, the U.S. National Science Foundation (NSF) issued an executive summary of a paper on science and engineering which briefly discussed the prevalence of pseudoscience in modern times. It said, \"belief in pseudoscience is widespread\" and, referencing a Gallup Poll, stated that belief in the 10 commonly believed examples of paranormal phenomena listed in the poll were \"pseudoscientific beliefs\".\n\nThe items were \"extrasensory perception (ESP), that houses can be haunted, ghosts, telepathy, clairvoyance, astrology, that people can communicate mentally with someone who has died, witches, reincarnation, and channelling\". Such beliefs in pseudoscience represent a lack of knowledge of how science works. The scientific community may attempt to communicate information about science out of concern for the public's susceptibility to unproven claims.\n\nThe National Science Foundation stated that pseudoscientific beliefs in the U.S. became more widespread during the 1990s, peaked about 2001, and then decreased slightly since with pseudoscientific beliefs remaining common. According to the NSF report, there is a lack of knowledge of pseudoscientific issues in society and pseudoscientific practices are commonly followed. Surveys indicate about a third of all adult Americans consider astrology to be scientific.\n\nIn a report Singer and Benassi (1981) wrote that pseudoscientific beliefs have their origin from at least four sources.\n\nAnother American study (Eve and Dunn, 1990) supported the findings of Singer and Benassi and found pseudoscientific belief being promoted by high school life science and biology teachers.\n\nThe psychology of pseudoscience attempts to explore and analyze pseudoscientific thinking by means of thorough clarification on making the distinction of what is considered scientific vs. pseudoscientific. The human proclivity for seeking confirmation rather than refutation (confirmation bias), the tendency to hold comforting beliefs, and the tendency to overgeneralize have been proposed as reasons for pseudoscientific thinking. According to Beyerstein (1991), humans are prone to associations based on resemblances only, and often prone to misattribution in cause-effect thinking.\n\nMichael Shermer's theory of belief-dependent realism is driven by the belief that the brain is essentially a \"belief engine,\" which scans data perceived by the senses and looks for patterns and meaning. There is also the tendency for the brain to create cognitive biases, as a result of inferences and assumptions made without logic and based on instinct — usually resulting in patterns in cognition. These tendencies of patternicity and agenticity are also driven \"by a meta-bias called the bias blind spot, or the tendency to recognize the power of cognitive biases in other people but to be blind to their influence on our own beliefs.\"\nLindeman states that social motives (i.e., \"to comprehend self and the world, to have a sense of control over outcomes, to belong, to find the world benevolent and to maintain one's self-esteem\") are often \"more easily\" fulfilled by pseudoscience than by scientific information. Furthermore, pseudoscientific explanations are generally not analyzed rationally, but instead experientially. Operating within a different set of rules compared to rational thinking, experiential thinking regards an explanation as valid if the explanation is \"personally functional, satisfying and sufficient\", offering a description of the world that may be more personal than can be provided by science and reducing the amount of potential work involved in understanding complex events and outcomes.\n\nThere is a trend to believe in pseudoscience more than scientific evidence. Some people believe the prevalence of pseudoscientific beliefs is due to widespread \"scientific illiteracy\". Individuals lacking scientific literacy are more susceptible to wishful thinking, since they are likely to turn to immediate gratification powered by System 1, our default operating system which requires little to no effort. This system encourages one to accept the conclusions they believe, and reject the ones they do not. Further analysis of complex pseudoscientific phenomena require System 2, which follows rules, compares objects along multiple dimensions and weighs options. These two systems have several other differences which are further discussed in the dual-process theory. The scientific and secular systems of morality and meaning are generally unsatisfying to most people. Humans are, by nature, a forward-minded species pursuing greater avenues of happiness and satisfaction, but we are all too frequently willing to grasp at unrealistic promises of a better life.\n\nPsychology has much to discuss about pseudoscience thinking, as it is the illusory perceptions of causality and effectiveness of numerous individuals that needs to be illuminated. Research suggests that illusionary thinking happens in most people when exposed to certain circumstances such as reading a book, an advertisement or the testimony of others are the basis of pseudoscience beliefs. It is assumed that illusions are not unusual, and given the right conditions, illusions are able to occur systematically even in normal emotional situations. One of the things pseudoscience believers quibble most about is that academic science usually treats them as fools. Minimizing these illusions in the real world is not simple. To this aim, designing evidence-based educational programs can be effective to help people identify and reduce their own illusions.\n\nIn the philosophy and history of science, Imre Lakatos stresses the social and political importance of the demarcation problem, the normative methodological problem of distinguishing between science and pseudoscience. His distinctive historical analysis of scientific methodology based on research programmes suggests: \"scientists regard the successful theoretical prediction of stunning novel facts – such as the return of Halley's comet or the gravitational bending of light rays – as what demarcates good scientific theories from pseudo-scientific and degenerate theories, and in spite of all scientific theories being forever confronted by 'an ocean of counterexamples'\". Lakatos offers a \"novel fallibilist analysis of the development of Newton's celestial dynamics, [his] favourite historical example of his methodology\" and argues in light of this historical turn, that his account answers for certain inadequacies in those of Karl Popper and Thomas Kuhn. \"Nonetheless, Lakatos did recognize the force of Kuhn's historical criticism of Popper – all important theories have been surrounded by an 'ocean of anomalies', which on a falsificationist view would require the rejection of the theory outright... Lakatos sought to reconcile the rationalism of Popperian falsificationism with what seemed to be its own refutation by history\".\n\nThe boundary between science and pseudoscience is disputed and difficult to determine analytically, even after more than a century of study by philosophers of science and scientists, and despite some basic agreements on the fundamentals of the scientific method. The concept of pseudoscience rests on an understanding that the scientific method has been misrepresented or misapplied with respect to a given theory, but many philosophers of science maintain that different kinds of methods are held as appropriate across different fields and different eras of human history. According to Lakatos, the typical descriptive unit of great scientific achievements is not an isolated hypothesis but \"a powerful problem-solving machinery, which, with the help of sophisticated mathematical techniques, digests anomalies and even turns them into positive evidence.\"\n\nThe demarcation problem between science and pseudoscience brings up debate in the realms of science, philosophy and politics. Imre Lakatos, for instance, points out that the Communist Party of the Soviet Union at one point declared that Mendelian genetics was pseudoscientific and had its advocates, including well-established scientists such as Nikolai Vavilov, sent to a Gulag and that the \"liberal Establishment of the West\" denies freedom of speech to topics it regards as pseudoscience, particularly where they run up against social mores.\n\nIt becomes pseudoscientific when science cannot be separated from ideology, scientists misrepresent scientific findings to promote or draw attention for publicity, when politicians, journalists and a nation's intellectual elite distort the facts of science for short-term political gain, or when powerful individuals of the public conflate causation and cofactors by clever wordplay. These ideas reduce the authority, value, integrity and independence of science in society.\n\nDistinguishing science from pseudoscience has practical implications in the case of health care, expert testimony, environmental policies, and science education. Treatments with a patina of scientific authority which have not actually been subjected to actual scientific testing may be ineffective, expensive and dangerous to patients and confuse health providers, insurers, government decision makers and the public as to what treatments are appropriate. Claims advanced by pseudoscience may result in government officials and educators making bad decisions in selecting curricula.\n\nThe extent to which students acquire a range of social and cognitive thinking skills related to the proper usage of science and technology determines whether they are scientifically literate. Education in the sciences encounters new dimensions with the changing landscape of science and technology, a fast-changing culture and a knowledge-driven era. A reinvention of the school science curriculum is one that shapes students to contend with its changing influence on human welfare. Scientific literacy, which allows a person to distinguish science from pseudosciences such as astrology, is among the attributes that enable students to adapt to the changing world. Its characteristics are embedded in a curriculum where students are engaged in resolving problems, conducting investigations, or developing projects.\n\nFriedman mentions why most scientists avoid educating about pseudoscience, including that paying undue attention to pseudoscience could dignify it. On the other hand, Park emphasizes how pseudoscience can be a threat to society and considers that scientists have a responsibility to teach how to distinguish science from pseudoscience.\n\nPseudosciences such as homeopathy, even if generally benign, are used by charlatans. This poses a serious issue because it enables incompetent practitioners to administer health care. True-believing zealots may pose a more serious threat than typical con men because of their affection to homeopathy's ideology. Irrational health care is not harmless and it is careless to create patient confidence in pseudomedicine.\n\nOn December 8, 2016, Michael V. LeVine, writing in \"Business Insider\", pointed out the dangers posed by the \"Natural News\" website: \"Snake-oil salesmen have pushed false cures since the dawn of medicine, and now websites like \"Natural News\" flood social media with dangerous anti-pharmaceutical, anti-vaccination and anti-GMO pseudoscience that puts millions at risk of contracting preventable illnesses.\"\n\n\n\n\n"}
{"id": "231442", "url": "https://en.wikipedia.org/wiki?curid=231442", "title": "Reference range", "text": "Reference range\n\nIn health-related fields, a reference range or reference interval is the range of values for a physiologic measurement in healthy persons (for example, the amount of creatinine in the blood, or the partial pressure of oxygen). It is a basis for comparison (a frame of reference) for a physician or other health professional to interpret a set of test results for a particular patient. Some important reference ranges in medicine are reference ranges for blood tests and reference ranges for urine tests.\n\nThe standard definition of a reference range (usually referred to if not otherwise specified) originates in what is most prevalent in a reference group taken from the general (i.e. total) population. This is the general reference range. However, there are also \"optimal health ranges\" (ranges that appear to have the optimal health impact) and ranges for particular conditions or statuses (such as pregnancy reference ranges for hormone levels).\n\nValues within the reference range (WRR) are those within the normal distribution and are thus often described as within normal limits (WNL). The limits of the normal distribution are called the \"upper reference limit\" (URL) or \"upper limit of normal\" (ULN) and the \"lower reference limit\" (LRL) or \"lower limit of normal\" (LLN). In health care–related publishing, style sheets sometimes prefer the word \"reference\" over the word \"normal\" to prevent the nontechnical senses of \"normal\" from being conflated with the statistical sense. Values outside a reference range are not necessarily pathologic, and they are not necessarily abnormal in any sense other than statistically. Nonetheless, they are indicators of probable pathosis. Sometimes the underlying cause is obvious; in other cases, challenging differential diagnosis is required to determine what is wrong and thus how to treat it.\n\nA cutoff or threshold is a limit used for binary classification, mainly between normal versus pathological (or probably pathological). Establishment methods for cutoffs include using an upper or a lower limit of a reference range.\n\nThe standard definition of a reference range for a particular measurement is defined as the interval between which 95% of values of a reference population fall into, in such a way that 2.5% of the time a value will be less than the lower limit of this interval, and 2.5% of the time it will be larger than the upper limit of this interval, whatever the distribution of these values.\n\nReference ranges that are given by this definition are sometimes referred as \"standard ranges\".\n\nRegarding the target population, if not otherwise specified, a standard reference range generally denotes the one in healthy individuals, or without any known condition that directly affects the ranges being established. These are likewise established using reference groups from the healthy population, and are sometimes termed \"normal ranges\" or \"normal values\" (and sometimes \"usual\" ranges/values). However, using the term \"normal\" may not be appropriate as not everyone outside the interval is abnormal, and people who have a particular condition may still fall within this interval.\n\nHowever, reference ranges may also be established by taking samples from the whole population, with or without diseases and conditions. In some cases, diseased individuals are taken as the population, establishing reference ranges among those having a disease or condition. Preferably, there should be specific reference ranges for each subgroup of the population that has any factor that affects the measurement, such as, for example, specific ranges for each \"sex\", \"age group\", \"race\" or any other general determinant.\n\nMethods for establishing reference ranges are mainly based on assuming a normal distribution or a log-normal distribution, or directly from percentages of interest, as detailed respectively in following sections.\n\nThe 95% interval, is often estimated by assuming a normal distribution of the measured parameter, in which case it can be defined as the interval limited by 1.96 (often rounded up to 2) population standard deviations from either side of the population mean (also called the expected value).\nHowever, in the real world, neither the population mean nor the population standard deviation are known. They both need to be estimated from a sample, whose size can be designated \"n\". The population standard deviation is estimated by the sample standard deviation and the population mean is estimated by the sample mean (also called mean or arithmetic mean). To account for these estimations, the 95% prediction interval (95% PI) is calculated as:\n\nwhere formula_1 is the 97.5% quantile of a Student's t-distribution with \"n\"−1 degrees of freedom.\n\nWhen the sample size is large (\"n\"≥30) formula_2\n\nThis method is often acceptably accurate if the standard deviation, as compared to the mean, is not very large. A more accurate method is to perform the calculations on logarithmized values, as described in separate section later.\n\nThe following example of this (\"not\" logarithmized) method is based on values of fasting plasma glucose taken from a reference group of 12 subjects:\n\nAs can be given from, for example, a table of selected values of Student's t-distribution, the 97.5% percentile with (12-1) degrees of freedom corresponds to \nformula_3\nSubsequently, the lower and upper limits of the standard reference range are calculated as:\n\nThus, the standard reference range for this example is estimated to be 4.4 to 6.3 mmol/L.\n\nThe 90% \"confidence interval of a standard reference range limit\" as estimated assuming a normal distribution can be calculated by:\n\nwhere SD is the standard deviation, and n is the number of samples.\n\nTaking the example from the previous section, the number of samples is 12 and the standard deviation is 0.42 mmol/L, resulting in:\n\nThus, the lower limit of the reference range can be written as 4.4 (90% CI 4.1-4.7) mmol/L.\n\nLikewise, with similar calculations, the upper limit of the reference range can be written as 6.3 (90% CI 6.0-6.6) mmol/L.\n\nThese confidence intervals reflect random error, but do not compensate for systematic error, which in this case can arise from, for example, the reference group not having fasted long enough before blood sampling.\n\nAs a comparison, actual reference ranges used clinically for fasting plasma glucose are estimated to have a lower limit of approximately 3.8 to 4.0, and an upper limit of approximately 6.0 to 6.1.\n\nIn reality, biological parameters tend to have a log-normal distribution, rather than the arithmetical normal distribution (which is generally referred to as normal distribution without any further specification).\n\nAn explanation for this log-normal distribution for biological parameters is: The event where a sample has half the value of the mean or median tends to have almost equal probability to occur as the event where a sample has twice the value of the mean or median. Also, only a log-normal distribution can compensate for the inability of almost all biological parameters to be of negative numbers (at least when measured on absolute scales), with the consequence that there is no definite limit to the size of outliers (extreme values) on the high side, but, on the other hand, they can never be less than zero, resulting in a positive skewness.\n\nAs shown in diagram at right, this phenomenon has relatively small effect if the standard deviation (as compared to the mean) is relatively small, as it makes the log-normal distribution appear similar to an arithmetical normal distribution. Thus, the arithmetical normal distribution may be more appropriate to use with small standard deviations for convenience, and the log-normal distribution with large standard deviations.\n\nIn a log-normal distribution, the geometric standard deviations and geometric mean more accurately estimate the 95% prediction interval than their arithmetic counterparts.\n\nThe necessity to establish a reference range by log-normal distribution rather than arithmetic normal distribution can be regarded as depending on how much difference it would make to \"not\" do so, which can be described as the ratio:\n\nwhere:\n\nThis difference can be put solely in relation to the coefficient of variation, as in the diagram at right, where:\n\nwhere:\n\nIn practice, it can be regarded as necessary to use the establishment methods of a log-normal distribution if the difference ratio becomes more than 0.1, meaning that a (lower or upper) limit estimated from an assumed arithmetically normal distribution would be more than 10% different from the corresponding limit as estimated from a (more accurate) log-normal distribution. As seen in the diagram, a difference ratio of 0.1 is reached for the lower limit at a coefficient of variation of 0.213 (or 21.3%), and for the upper limit at a coefficient of variation at 0.413 (41.3%). The lower limit is more affected by increasing coefficient of variation, and its \"critical\" coefficient of variation of 0.213 corresponds to a ratio of (upper limit)/(lower limit) of 2.43, so as a rule of thumb, if the upper limit is more than 2.4 times the lower limit when estimated by assuming arithmetically normal distribution, then it should be considered to do the calculations again by log-normal distribution.\n\nTaking the example from previous section, the arithmetic standard deviation (s.d.) is estimated at 0.42 and the arithmetic mean (m) is estimated at 5.33. Thus the coefficient of variation is 0.079. This is less than both 0.213 and 0.413, and thus both the lower and upper limit of fasting blood glucose can most likely be estimated by assuming arithmetically normal distribution. More specifically, the coefficient of variation of 0.079 corresponds to a difference ratio of 0.01 (1%) for the lower limit and 0.007 (0.7%) for the upper limit.\n\nA method to estimate the reference range for a parameter with log-normal distribution is to logarithmize all the measurements with an arbitrary base (for example \"e\"), derive the mean and standard deviation of these logarithms, determine the logarithms located (for a 95% prediction interval) 1.96 standard deviations below and above that mean, and subsequently exponentiate using those two logarithms as exponents and using the same base as was used in logarithmizing, with the two resultant values being the lower and upper limit of the 95% prediction interval.\n\nThe following example of this method is based on the same values of fasting plasma glucose as used in the previous section, using \"e\" as a base:\n\nSubsequently, the still logarithmized lower limit of the reference range is calculated as:\n\nand the upper limit of the reference range as:\n\nConversion back to non-logarithmized values are subsequently performed as:\n\nThus, the standard reference range for this example is estimated to be 4.4 to 6.4.\n\nAn alternative method of establishing a reference range with the assumption of log-normal distribution is to use the arithmetic mean and arithmetic value of standard deviation. This is somewhat more tedious to perform, but may be useful for example in cases where a study that establishes a reference range presents only the arithmetic mean and standard deviation, leaving out the source data. If the original assumption of arithmetically normal distribution is shown to be less appropriate than the log-normal one, then, using the arithmetic mean and standard deviation may be the only available parameters to correct the reference range.\n\nBy assuming that the expected value can represent the arithmetic mean in this case, the parameters \"μ\" and \"σ\" can be estimated from the arithmetic mean (\"m\") and standard deviation (\"s.d.\") as:\n\nFollowing the exampled reference group from the previous section:\n\nSubsequently, the logarithmized, and later non-logarithmized, lower and upper limit are calculated just as by logarithmized sample values.\n\nReference ranges can also be established directly from the 2.5th and 97.5th percentile of the measurements in the reference group. For example, if the reference group consists of 200 people, and counting from the measurement with lowest value to highest, the lower limit of the reference range would correspond to the 5th measurement and the upper limit would correspond to the 195th measurement.\n\nThis method can be used even when measurement values do not appear to conform conveniently to any form of normal distribution or other function.\n\nHowever, the reference range limits as estimated in this way have higher variance, and therefore less reliability, than those estimated by an arithmetic or log-normal distribution (when such is applicable), because the latter ones acquire statistical power from the measurements of the whole reference group rather than just the measurements at the 2.5th and 97.5th percentiles. Still, this variance decreases with increasing size of the reference group, and therefore, this method may be optimal where a large reference group easily can be gathered, and the distribution mode of the measurements is uncertain.\n\nIn case of a bimodal distribution (seen at right), it is useful to find out why this is the case. Two reference ranges can be established for the two different groups of people, making it possible to assume a normal distribution for each group. This bimodal pattern is commonly seen in tests that differ between men and women, such as prostate specific antigen.\n\nIn case of medical tests whose results are of continuous values, reference ranges can be used in the interpretation of an individual test result. This is primarily used for diagnostic tests and screening tests, while monitoring tests may optimally be interpreted from previous tests of the same individual instead.\n\nReference ranges aid in the evaluation of whether a test result's deviation from the mean is a result of random variability or a result of an underlying disease or condition. If the reference group used to establish the reference range can be assumed to be representative of the individual person in a healthy state, then a test result from that individual that turns out to be lower or higher than the reference range can be interpreted as that there is less than 2.5% probability that this would have occurred by random variability in the absence of disease or other condition, which, in turn, is strongly indicative for considering an underlying disease or condition as a cause.\n\nSuch further consideration can be performed, for example, by an epidemiology-based differential diagnostic procedure, where potential candidate conditions are listed that may explain the finding, followed by calculations of how probable they are to have occurred in the first place, in turn followed by a comparison with the probability that the result would have occurred by random variability.\n\nIf the establishment of the reference range could have been made assuming a normal distribution, then the probability that the result would be an effect of random variability can be further specified as follows:\n\nThe standard deviation, if not given already, can be inversely calculated by the fact that the absolute value of the difference between the mean and either the upper or lower limit of the reference range is approximately 2 standard deviations (more accurately 1.96), and thus:\n\nThe standard score for the individual's test can subsequently be calculated as:\n\nThe probability that a value is of a certain distance from the mean can subsequently be calculated from the relation between standard score and prediction intervals. For example, a standard score of 2.58 corresponds to a prediction interval of 99%, corresponding to a probability of 0.5% that a result is at least such far from the mean in the absence of disease.\n\nLet's say, for example, that an individual takes a test that measures the ionized calcium in the blood, resulting in a value of 1.30 mmol/L, and a reference group that appropriately represents the individual has established a reference range of 1.05 to 1.25 mmol/L. The individual's value is higher than the upper limit of the reference range, and therefore has less than 2.5% probability of being a result of random variability, constituting a strong indication to make a differential diagnosis of possible causative conditions.\n\nIn this case, an epidemiology-based differential diagnostic procedure is used, and its first step is to find candidate conditions that can explain the finding.\n\nHypercalcemia (usually defined as a calcium level above the reference range) is mostly caused by either primary hyperparathyroidism or malignancy, and therefore, it is reasonable to include these in the differential diagnosis.\n\nUsing for example epidemiology and the individual's risk factors, let's say that the probability that the hypercalcemia would have been caused by primary hyperparathyroidism in the first place is estimated to be 0.00125 (or 0.125%), the equivalent probability for cancer is 0.0002, and 0.0005 for other conditions. With a probability given as less than 0.025 of no disease, this corresponds to a probability that the hypercalcemia would have occurred in the first place of up to 0.02695. However, the hypercalcemia \"has occurred\" with a probability of 100%, resulting adjusted probabilities of at least 4.6% that primary hyperparathyroidism has caused the hypercalcemia, at least 0.7% for cancer, at least 1.9% for other conditions and up to 92.8% for that there is no disease and the hypercalcemia is caused by random variability.\n\nIn this case, further processing benefits from specification of the probability of random variability:\n\nThe value is assumed to conform acceptably to a normal distribution, so the mean can be assumed to be 1.15 in the reference group. The standard deviation, if not given already, can be inversely calculated by knowing that the absolute value of the difference between the mean and, for example, the upper limit of the reference range, is approximately 2 standard deviations (more accurately 1.96), and thus:\n\nThe standard score for the individual's test is subsequently calculated as:\n\nThe probability that a value is of so much larger value than the mean as having a standard score of 3 corresponds to a probability of approximately 0.14% (given by , with 99.7% here being given from the 68-95-99.7 rule).\n\nUsing the same probabilities that the hypercalcemia would have occurred in the first place by the other candidate conditions, the probability that hypercalcemia would have occurred in the first place is 0.00335, and given the fact that hypercalcemia \"has occurred\" gives adjusted probabilities of 37.3%, 6.0%, 14.9% and 41.8%, respectively, for primary hyperparathyroidism, cancer, other conditions and no disease.\n\n\"Optimal (health) range\" or \"therapeutic target\" (not to be confused with biological target) is a reference range or limit that is based on concentrations or levels that are associated with optimal health or minimal risk of related complications and diseases, rather than the standard range based on normal distribution in the population.\n\nIt may be more appropriate to use for e.g. folate, since approximately 90 percent of North Americans may actually suffer more or less from folate deficiency, but only the 2.5 percent that have the lowest levels will fall below the standard reference range. In this case, the actual folate ranges for optimal health are substantially higher than the standard reference ranges. Vitamin D has a similar tendency. In contrast, for e.g. uric acid, having a level not exceeding the standard reference range still does not exclude the risk of getting gout or kidney stones. Furthermore, for most toxins, the standard reference range is generally lower than the level of toxic effect.\n\nA problem with optimal health range is a lack of a standard method of estimating the ranges. The limits may be defined as those where the health risks exceed a certain threshold, but with various risk profiles between different measurements (such as folate and vitamin D), and even different risk aspects for one and the same measurement (such as both deficiency and toxicity of vitamin A) it is difficult to standardize. Subsequently, optimal health ranges, when given by various sources, have an additional variability caused by various definitions of the parameter. Also, as with standard reference ranges, there should be specific ranges for different determinants that affects the values, such as sex, age etc. Ideally, there should rather be an estimation of what is the optimal value for every individual, when taking all significant factors of that individual into account - a task that may be hard to achieve by studies, but long clinical experience by a physician may make this method more preferable than using reference ranges.\nIn many cases, only one side of the range is usually of interest, such as with markers of pathology including cancer antigen 19-9, where it is generally without any clinical significance to have a value below what is usual in the population. Therefore, such targets are often given with only one limit of the reference range given, and, strictly, such values are rather \"cut-off values\" or \"threshold values\".\n\nThey may represent both standard ranges and optimal health ranges. Also, they may represent an appropriate value to distinguish healthy person from a specific disease, although this gives additional variability by different diseases being distinguished. For example, for NT-proBNP, a lower cut-off value is used in distinguishing healthy babies from those with acyanotic heart disease, compared to the cut-off value used in distinguishing healthy babies from those with congenital nonspherocytic anemia.\n\nFor standard as well as optimal health ranges, and cut-offs, sources of inaccuracy and imprecision include:\n\n\nAlso, reference ranges tend to give the impression of definite thresholds that clearly separate \"good\" or \"bad\" values, while in reality there are generally continuously increasing risks with increased distance from usual or optimal values.\n\nWith this and uncompensated factors in mind, the ideal interpretation method of a test result would rather consist of a comparison of what would be expected or optimal in the individual when taking all factors and conditions of that individual into account, rather than strictly classifying the values as \"good\" or \"bad\" by using reference ranges from other people.\n\nIn a recent paper, Rappoport et al. described a novel way to redefine reference range from an Electronic health record system. In such a system, a higher population resolution can be achieved (e.g., age, sex, race and ethnicity-specific).\n\n\n\n"}
{"id": "58910447", "url": "https://en.wikipedia.org/wiki?curid=58910447", "title": "Research site", "text": "Research site\n\nA research site is a place where people conduct research. Common research sites include universities, hospitals, research institutes, and field research locations.\n\nIn clinical research a research site conducts all or part of a clinical trial. For clinical trials which recruit research participants in multiple locations, often the research will have a headquarters then multiple regional research sites to conduct the research in that region. In a network of research sites where all are recruiting study participants, sites with low recruitment benefit from coaching from sites with high recruitment.\n\nCharacteristics of good clinical research sites include setting good timelines, early participant recruitment, and having a management plan for efficiency.\n\nResearchers in nursing have reported challenges accessing the facilities designated for conventional medical research.\n\nThe design of a research site should have a means of detecting fraud.\n\nResearchers who do not have a cultural tie to a research population may have difficulty doing ethnographic research with that community.\n\n"}
{"id": "28130806", "url": "https://en.wikipedia.org/wiki?curid=28130806", "title": "Roshd Biological Education", "text": "Roshd Biological Education\n\nRoshd Biological Education is a quarterly science educational magazine covering recent developments in biology and biology education for a biology teacher Persian -speaking audience. Founded in 1985, it is published by The Teaching Aids Publication Bureau, Organization for Educational Planning and Research, Ministry of Education, Iran. Roshd Biological Education has an editorial board composed of Iranian biologists, experts in biology education, science journalists and biology teachers.\nIt is read by both biology teachers and students, as a way of launching innovations and new trends in biology education, and helping biology teachers to teach biology in better and more effective ways.\n\nAs of Autumn 2012, the magazine is laid out as follows:\n\nDr. Mohammad Karamudini is the current editor in chief. He studied biology and social sciences, and worked as biology teacher, biology curriculum developer, science journalist and science writer for a time as long as 37 years. He has approximately 30 books and numerous articles and research publications. Majority of the publications are in Persian, while only 2 of them are in English.\n\nDr. Alireza Sari is an editor. He studied zoology in UK, and is an assistant professor of zoology in Tehran University.\n\nDr. Shahryar Gharibzadeh is an editor. He studied medicine and human physiology in Tehran University. He is a science journalist and an assistant professor in Amirkabir University, Tehran.\n\nDr. Abbas Akhavan-Sepahi is an editor. He studied microbiology and is a science journalist and an assistant professor of microbiology in Azad University, Tehran.\n\nAli Ale-Mohammad is an editor. He studied human genetics and writer and/or translator of multiple science and/or science educational publications.\n\nElaheh Alavi is the magazine’s executive director and also an editor. He studied plant biology and is translator of multiple publications on biology education.\n\nDr. Hossein Lari-Yazdi is an editor, assistant professor of Plant Biology in Azad Islamic University.\n\nNezam Jalilian is an editor, a Master in Biochemistry, and Biology teacher in Khoozestan Province.\n\nRoshd Biological Education started in 1985 together with many other magazines in other science and art. The first editor was Dr. Nouri-Dalooi, the second editor was Mr. Hossein Daneshfar and the third and present editor is Dr. Mohammad KARAMUDINI. The 87th issue is published in summer 2012.\n\n\n"}
{"id": "19206529", "url": "https://en.wikipedia.org/wiki?curid=19206529", "title": "SCALE-UP", "text": "SCALE-UP\n\nSCALE-UP is a learning environment specifically created to facilitate active, collaborative learning in a studio-like setting. Some people think the rooms look more like restaurants than classrooms. The spaces are carefully designed to facilitate interactions between teams of students who work on short, interesting tasks. A decade of research indicates significant improvements in learning. The approach taken during the development and testing of the learning environment is an application of scientific teaching and has been discussed in several books. Although originated at North Carolina State University, more than five hundred colleges across the US and around the world are known to have directly adopted the SCALE-UP model and adapted it to their particular needs. Information about more than 400 of these implementation sites is available on the SCALE-UP website.\n\nThe SCALE-UP name originally stood for “Student-Centered Activities for Large Enrollment Undergraduate Physics” but since its conception many different institutions have begun teaching a variety of courses of various sizes. The acronym was changed to “Student-Centered Active Learning Environment for Undergraduate Programs.” Now, because of the increasing number of pre-college installations, plus to draw attention to the instruction as well as the space, the name has become \"Student-Centered Active Learning Environment with Upside-down Pedagogies.\" The basic idea is that students are given something interesting to investigate. While they work in teams on these \"tangibles\" (hands-on measurements or observations) and \"ponderables\" (interesting, complex problems), the instructor is free to roam around the classroom–--asking questions, sending one team to help another, or asking why someone else got a different answer. There is no separate lab class and most of the \"lectures\" are actually class-wide discussions. The groups are carefully structured and give students many opportunities to interact. Three teams (labelled a, b, and c) sit at each round table and have white boards nearby. Each team has a laptop in case they need web access. The original design called for 11 round tables of nine students, but many schools have smaller classes while a few have even larger ones. Smaller classes, particularly those in high schools, have also been using D-shaped tables that seat six students.\n"}
{"id": "32671963", "url": "https://en.wikipedia.org/wiki?curid=32671963", "title": "Science, Order, and Creativity", "text": "Science, Order, and Creativity\n\nScience, Order, and Creativity is a book by theoretical physicist David Bohm and physicist and writer F. David Peat. It was originally published 1987 by Bantam Books, USA, then 1989 in Great Britain by Routledge. The second edition, published in 2000 after Bohm`s death, comprises a new foreword by Peat as well as an additional introductory chapter, in which a fictitious dialogue between Bohm and Peat serves to introduce the reader to the context and topics of the book.\n\nIn \"Science, Order and Creativity\", the authors emphasize the role of creativity and communication for science and, also beyond science, for humanity as a whole.\n\n\n\n\n\n\n\n\nThe book has been cited in the fields of education and science education, and knowledge management, among many others. Referencing this book, in the framework of his concept of a Total human ecosystem, Zev Naveh has also referred to implicate orders as \"very important\" for multifunctional landscapes in landscape ecology.\n\n"}
{"id": "19086436", "url": "https://en.wikipedia.org/wiki?curid=19086436", "title": "Science.ie", "text": "Science.ie\n\nThe Science.ie portal provides all sorts of information about careers in science, technology, engineering and mathematics (STEM).\n\nScience.ie is an initiative of the Irish Government’s Discover Science & Engineering (DSE) awareness programme in Ireland. DSE is managed by Forfás on behalf of the Office of Science and Technology at the Department of Jobs, Enterprise and Innovation.\n\nThe careers-related information on Science.ie has been moved to a new DSE website - My Science Career - which was launched in early October 2009. On MyScienceCareer.ie is:\n\n\nA redeveloped Science.ie was also launched in October 2009. The site has been redesigned and includes social media bookmarking and RSS feeds.\n\nScience.ie provides more general information on science in Ireland. This includes listings of science links, news and events. Its \"Resources\" section gives information on activities and visitor centres where you can learn about science.\n\nThe site also provides a free newsletter relating to Irish science, technology and innovation news, events, research and facts which is issued monthly by email.\n\nIn November 2009 Science.ie launched a Twitter channel. Follow Science.ie on Twitter.\n\nDSE runs numerous other initiatives, including My Science Career, Project Blogger, Science Week Ireland and Discover Primary Science.\n\n"}
{"id": "55547169", "url": "https://en.wikipedia.org/wiki?curid=55547169", "title": "Science information on Wikipedia", "text": "Science information on Wikipedia\n\nScience information on Wikipedia includes the information which Wikipedia presents about science. It critiques and discusses the impact and quality of that information, and the culture of Wikipedia editors, scientists, and layman engagement with this information.\n\nA 2017 study found evidence that Wikipedia's popularity as the most popular general information source has influenced how everyone talks and writes about science.\n\nA 2016 study found evidence that Wikipedia increases the distribution and impact of open access science publications.\n\nUNESCO reports that Wikipedia is a popular source of science information because Wikipedia has high ranking in search engines.\n\nA 2018 study examined the way that Wikipedia integrates new scientific information.\n\nIn 2016 the Wiki Education Foundation and the Simons Foundation presented an outreach program called the \"Year of Science\". In this program, Wikipedia educators visited academic conferences and invited scientists to contribute information from their field of expertise to Wikipedia.\n\nSome universities have programs to encourage students to edit Wikipedia's science articles as part of the learning experience.\n\nThe Wikipedia community invites academics to edit Wikipedia articles.\n\nVarious academic societies have encouraged their membership to edit Wikipedia.\n\nA 2005 study published in the journal \"Nature\" compared 40 Wikipedia articles on science topics to their \"Encyclopædia Britannica\" counterpart. Subject experts found four \"serious errors\" in each encyclopedia. They also found 162 less serious problems in Wikipedia, and 123 in \"Britannica\".\n\nA popular science writer for \"Vice\" complained in 2017 that Wikipedia's science articles were too technical.\n\nVarious scientists and media organizations have questioned and critiqued the extent to which Wikipedia articles on science influence political decisions relating to science. \n\n"}
{"id": "3447151", "url": "https://en.wikipedia.org/wiki?curid=3447151", "title": "Scientific demonstration", "text": "Scientific demonstration\n\nA scientific demonstration is a scientific experiment carried out for the purposes of demonstrating scientific principles, rather than for hypothesis testing or knowledge gathering (although they may originally have been carried out for these purposes).\n\nMost scientific demonstrations are simple laboratory demonstrations intended to demonstrate physical principles, often in a surprising or entertaining way. They are carried out in schools and universities, and sometimes in public demonstrations in popular science lectures and TV programs aimed at the general public. Many scientific demonstrations are chosen for their combination of educational merit and entertainment value, which is often provided by dramatic phenomena such as explosions. \n\nPublic scientific demonstrations were a common occurrence in the Age of Enlightenment, and have long been a feature of the British Royal Institution Christmas Lectures, which date back to 1825. In the television era, scientific demonstrations have featured in science-related entertainment shows such as \"MythBusters\" and \"\".\n\nSome famous scientific demonstrations include:\n\n\nNote: many scientific demonstrations are potentially dangerous, and should not be attempted without considerable laboratory experience and appropriate safety precautions. Many older well-known scientific demonstrations, once mainstays of science education, are now effectively impossible to demonstrate to an audience without breaking health and safety laws. Some older demonstrations, such as allowing the audience to play with liquid mercury, are sufficiently dangerous that they should not be attempted by anyone under any circumstances.\n\n"}
{"id": "18093238", "url": "https://en.wikipedia.org/wiki?curid=18093238", "title": "Scientific teaching", "text": "Scientific teaching\n\nScientific teaching is a pedagogical approach used in undergraduate science classrooms whereby teaching and learning is approached with the same rigor as science itself. \n\nAccording to a 2004 Policy Forum in \"Science\" magazine, \"scientific teaching involves active learning strategies to engage students in the process of science and teaching methods that have been systematically tested and shown to reach diverse students.\"\n\nThe 2007 volume \"Scientific Teaching\" lists three major tenets of scientific teaching:\n\nThese elements should underlie educational and pedagogical decisions in the classroom. The \"SCALE-UP\" learning environment is an example of applying the scientific teaching approach. In practice, scientific teaching employs a \"backward design\" approach. The instructor first decides what the students should know and be able to do (learning goals), then determines what would be evidence of student achievement of the learning goals, then designs assessments to measure this achievement. Finally, the instructor plans the learning activities, which should facilitate student learning through scientific discovery.\n\n"}
{"id": "26810", "url": "https://en.wikipedia.org/wiki?curid=26810", "title": "Skepticism", "text": "Skepticism\n\nSkepticism (American English) or scepticism (British English, Australian English, and Canadian English) is generally any questioning attitude or doubt towards one or more items of putative knowledge or belief. It is often directed at domains, such as the supernatural, morality (moral skepticism), religion (skepticism about the existence of God), or knowledge (skepticism about the possibility of knowledge, or of certainty). Formally, skepticism as a topic occurs in the context of philosophy, particularly epistemology, although it can be applied to any topic such as politics, religion, and pseudoscience.\n\nPhilosophical skepticism comes in various forms. Radical forms of skepticism deny that knowledge or rational belief is possible and urge us to suspend judgment on many or all controversial matters. More moderate forms of skepticism claim only that nothing can be known with certainty, or that we can know little or nothing about the \"big questions\" in life, such as whether God exists or whether there is an afterlife. Religious skepticism is \"doubt concerning basic religious principles (such as immortality, providence, and revelation)\". Scientific skepticism concerns testing beliefs for reliability, by subjecting them to systematic investigation using the scientific method, to discover empirical evidence for them.\n\nIn ordinary usage, skepticism (US) or scepticism (UK) (Greek: 'σκέπτομαι' \"skeptomai\", to search, to think about or look for; see also spelling differences) can refer to:\n\n\nIn philosophy, skepticism can refer to:\n\nAs a philosophical school or movement, skepticism originated in ancient Greece. A number of Greek Sophists held skeptical views. Gorgias, for example, reputedly argued that nothing exists, that even if there were something we couldn’t know it, and that even if we could know it we could not communicate it. Another Sophist, Cratylus, refused to discuss anything and would merely wriggle his finger, claiming that communication is impossible since meanings are constantly changing. The Sophists’ leading critic, Socrates, also had skeptical tendencies, claiming that he knew nothing, or at least nothing worthwhile.\n\nThere were two major schools of skepticism in the ancient Greek and Roman world. One was Pyrrhonian skepticism, which was founded by Pyrrho of Elis (c. 360–270 BCE). The other was Academic skepticism, so-called because its two leading defenders, Arcesilaus (c. 315–240 BCE) and Carneades (c. 217–128 BCE) were Heads of Plato’s Academy. Both schools of skepticism denied that knowledge is possible and urged suspension of judgment (\"epoche\") for the sake of mental tranquility (\"ataraxia\"). The major difference between the schools seems to have been that Academic skeptics claimed that some beliefs are more reasonable or probable than others, whereas Pyrrhonian skeptics argued that equally compelling arguments can be given for or against any disputed view. Nearly all the writings of the ancient skeptics are now lost. Most of what we know about ancient skepticism is due to Sextus Empiricus, a Pyrrhonian skeptic who lived in the second or third century A.D. His major work, \"Outlines of Pyrrhonism\", contains a lucid summary of stock skeptical arguments.\n\nAncient skepticism faded out during the late Roman Empire, particularly after Augustine (354–430 CE) attacked the skeptics in his work \"Against the Academics\" (386 CE). There was little knowledge of, or interest in, ancient skepticism in Christian Europe during the Middle Ages. Interest revived during the Renaissance and Reformation, particularly after the complete writings of Sextus Empiricus were translated into Latin in 1569. A number of Catholic writers, including Francisco Sanches (c. 1550–1623), Michel de Montaigne (1533–1592), Pierre Gassendi (1592–1655), and Marin Mersenne (1588–1648) deployed ancient skeptical arguments to defend moderate forms of skepticism and to argue that faith, rather than reason, must be the primary guide to truth. Similar arguments were offered later (perhaps ironically) by the Protestant thinker Pierre Bayle in his influential Historical and Critical Dictionary (1697–1702).\n\nThe growing popularity of skeptical views created an intellectual crisis in seventeenth-century Europe. One major response was offered by the French philosopher and mathematician René Descartes (1596–1650). In his classic work, \"Meditations of First Philosophy\" (1641), Descartes sought to refute skepticism, but only after he had formulated the case for skepticism as powerfully as possible. Descartes argued that no matter what radical skeptical possibilities we imagine there are certain truths (e.g., that thinking is occurring, or that I exist) that are absolutely certain. Thus, the ancient skeptics were wrong to claim that knowledge is impossible. Descartes also attempted to refute skeptical doubts about the reliability of our senses, our memory, and other cognitive faculties. To do this, Descartes tried to prove that God exists and that God would not allow us to be systematically deceived about the nature of reality. Many contemporary philosophers question whether this second stage of Descartes’ critique of skepticism is successful.\n\nIn the eighteenth century a powerful new case for skepticism was offered by the Scottish philosopher David Hume (1711–1776). Hume was an empiricist, claiming that all genuine ideas can be traced back to original impressions of sensation or introspective consciousness. Hume argued forcefully that on empiricist grounds there are no sound reasons for belief in God, an enduring self or soul, an external world, causal necessity, objective morality, or inductive reasoning. In fact, he argued that “Philosophy would render us entirely Pyrrhonian, were not Nature too strong for it.” As Hume saw it, the real basis of human belief is not reason, but custom or habit. We are hard-wired by nature to trust, say, our memories or inductive reasoning, and no skeptical arguments, however powerful, can dislodge those beliefs. In this way, Hume embraced what he called a “mitigated” skepticism, while rejecting an “excessive” Pyrrhonian skepticism that he saw as both impractical and psychologically impossible.\n\nHume’s skepticism provoked a number of important responses. Hume’s Scottish contemporary, Thomas Reid (1710–1796), challenged Hume’s strict empiricism and argued that it is rational to accept “common-sense” beliefs such as the basic reliability of our senses, our reason, our memories, and inductive reasoning, even though none of these things can be proved. In Reid’s view, such common-sense beliefs are foundational and require no proof in order to be rationally justified. Not long after Hume’s death, the great German philosopher Immanuel Kant (1724–1804) argued that human moral awareness makes no sense unless we reject Hume’s skeptical conclusions about the existence of God, the soul, free will, and an afterlife. According to Kant, while Hume was right to claim that we cannot strictly \"know\" any of these things, our moral experience entitles us to believe in them.\n\nToday, skepticism continues to be a topic of lively debate among philosophers.\n\nReligious skepticism generally refers to doubting given religious beliefs or claims. Historically, religious skepticism can be traced back to Socrates, who doubted many religious claims of the time. Modern religious skepticism typically emphasizes scientific and historical methods or evidence, with Michael Shermer writing that skepticism is a process for discovering the truth rather than general non-acceptance. For example, a religious skeptic might believe that Jesus existed while questioning claims that he was the messiah or performed miracles (see historicity of Jesus). Religious skepticism is not the same as atheism or agnosticism, though these often do involve skeptical attitudes toward religion and philosophical theology (for example, towards divine omnipotence). Religious people are generally skeptical about claims of other religions, at least when the two denominations conflict concerning some stated belief. Additionally, they may also be skeptical of the claims made by atheists. The historian Will Durant writes that Plato was \"as skeptical of atheism as of any other dogma.\"\n\nA scientific or empirical skeptic is one who questions beliefs on the basis of scientific understanding.\n\nScientific skepticism may discard beliefs pertaining to \"purported phenomena\" not subject to reliable observation and thus not systematic or testable empirically. Most scientists, being scientific skeptics, test the reliability of certain kinds of claims by subjecting them to a systematic investigation using some type of the scientific method. As a result, a number of claims are considered as \"pseudoscience\", if they are found to improperly apply or ignore the fundamental aspects of the scientific method.\n\nProfessional skepticism is an important concept in auditing. It requires an auditor to have a \"questioning mind,\" to make a critical assessment of evidence, and to consider the sufficiency of the evidence.\n\n\n\n"}
{"id": "47317009", "url": "https://en.wikipedia.org/wiki?curid=47317009", "title": "The Social Contract (1970 book)", "text": "The Social Contract (1970 book)\n\nThe Social Contract: A Personal Inquiry into the Evolutionary Sources of Order and Disorder is a 1970 book by Robert Ardrey. It is the third in his four-book Nature of Man Series.\n\nThe book extended Ardrey's refutation of the prevailing conviction within social sciences that all social behavior is purely learned and not governed by innate patterns. Through interwoven analyses of animals and human social structures Ardrey argued that inherited evolutionary traits are an important determining factor in social behavior.\n\nArdrey dedicated \"The Social Contract\" to Jean-Jacques Rousseau, after whose 1762 work the book was titled.\n\n\"The Social Contract\" was published in 1970. It was the third book in Ardrey's \"Nature of Man\" series, following \"African Genesis\" (1961) and \"The Territorial Imperative\" (1966) and preceding \"The Hunting Hypothesis\" (1976).\n\n\"The Social Contract\" continues Ardrey's work on understanding how evolutionarily inherited traits are manifest by contemporary man. In particular \"The Social Contract\" examines society and hierarchy in terms of genetic diversity. The edition cites many of the scientists who were important influences on Ardrey, including Raymond Dart and Konrad Lorenz. It was illustrated, like the first two books, by Ardrey's wife, the South African actress and illustrator Berdine Ardrey (née Grunewald).\n\n\"The Social Contract\" is a more ideologically motivated book than the other works in his \"Nature of Man\" series. It made assertions about how the social contract \"should\" be organized based on the evolutionary nature of man. In his last chapter, Ardrey writes:\nThe evolutionary nature of man represents, as I see it, a subject for the new philosophy, the new theologian. A set of common assumptions, common dedications, common assurances, of rules and regulations, even considering the limitations of homo sapiens, remains someday possible. As all of our parochial dedications have been eroded by the wash of the science, still a religion unassailable by the sciences exists as a goal worthy of contemporary ambition.\n\n\"The Social Contract\" also called for a reasoned respect of nature (in his next book, \"The Hunting Hypothesis\", Ardrey would be one of the first to warn of climate change as an existential threat to humanity). In \"The Social Contract\" he writes:\nThe philosophy of the impossible has been the dominant motive in human affairs for the past two centuries. We have pursued the mastery of nature as if we ourselves were not a portion of that nature. We have boasted of our command over our physical environment while we ourselves have done our urgent best to destroy it.\n\nCompared to the other works in the \"Nature of Man\" series, \"The Social Contract\" inspired more controversy and received more negative reviews. Furthermore, the central theses of the other three books have come to be commonly accepted in scientific communities: \"African Genesis\" (1961) posited that humans evolved from African meat-eaters instead of Asian carnivores; \"The Territorial Imperative\" (1966) demonstrated the influence of inherited territorial instincts on social formations; and \"The Hunting Hypothesis\" (1976) showed the importance of hunting behavior on the evolutionary course of early man. Because the core of the book is a social proposal and not a scientific hypothesis this is not the case with \"The Social Contract.\"\n\nPart of the controversy surrounding \"The Social Contract\" had to do with its theses on inequality. According to Ardrey, because each individual is born with a unique combination of genetically-determined traits, these traits can be evaluated by the environment, and therefore the diversity of phenotypes becomes inequality. For Ardrey the only way to have a realistic optimism about society was to recognize this inequality. In the words of the reviewer for \"News/Check,\" \"A society of equals, Ardrey argues, is a natural impossibility — things just don't work that way, and those who argue otherwise (conservatives as well as liberals) are denying possibilities of change; they also deny, and this is important to Ardrey, optimism.\"\n\nArdrey argued, therefore, that inequality was not necessarily a social evil, but he emphasized that it could only be justly expressed given absolute equality of opportunity. He also applied evolutionary theory on the level of groups, a move that continues to be scientifically controversial.\n\nIn addition to insisting on the necessity for absolute equality of opportunity, Ardrey argued that the presence of inequality does not justify the domination of the weak by the strong. Rather \"Ardrey showed that in all societies at any level of the animal world, structures exist to protect the vulnerable, and that this is an evolutionary advantage as it protects diversity, diversity being essential for creativity.\"\n\nThe central thesis of Ardrey's series, namely that evolutionary characteristics are manifest in human social relations, has become widely accepted. This shift signaled a major change in the social sciences, particularly in social and cultural anthropology and sociology. Robert Wokler, on the importance of Ardrey's approach, wrote:\nWhat ought to be studied, according to Ardrey, are the relations between individuals that stem from the innate and universal attributes of animal life, whereas cultural anthropologists who detect a fundamental discontinuity between mankind and other zoological species are just impervious to the revolutionary ideas of Darwinism which have reverberated throughout all the life sciences apart from their own.\n\nIn 1972, defending his movie \"A Clockwork Orange\" from Fred M. Hechinger, Stanley Kubrick cited Ardrey. In particular, he quoted \"The Social Contract\" (along with \"African Genesis\"). Kubrick was a notable fan of Ardrey's work, and also cited him as an inspiration for his 1968 film, \"\".\n\n"}
{"id": "158680", "url": "https://en.wikipedia.org/wiki?curid=158680", "title": "The Transparent Society", "text": "The Transparent Society\n\nThe Transparent Society (1998) is a non-fiction book by the science-fiction author David Brin in which he forecasts social transparency and some degree of erosion of privacy, as it is overtaken by low-cost surveillance, communication and database technology, and proposes new institutions and practices that he believes would provide benefits that would more than compensate for lost privacy. The work first appeared as a magazine article by Brin in \"Wired\" in late 1996. In 2008, security expert Bruce Schneier called the transparent society concept a \"myth\" (a characterization Brin later rejected), claiming it ignores wide differences in the relative power of those who access information.\n\nBrin argues that a core level of privacy—protecting our most intimate interactions—may be preserved, despite the rapid proliferation of cameras that become ever-smaller, cheaper and more numerous faster than Moore's law. He feels that this core privacy can be saved simply because that is what humans deeply need and want. Hence, Brin explains that \"...the key question is whether citizens will be potent, sovereign and knowing enough to enforce this deeply human want.\"\n\nThis means they must not only have rights, but also the power to use them and the ability to detect when they are being abused. That will only happen in a world that is mostly open, in which most citizens know most of what is going on, most of the time. It is the only condition under which citizens may have some chance of catching the violators of their freedom and privacy. Privacy is only possible if freedom (including the freedom to know) is protected first.\n\nBrin thus maintains that privacy is a \"contingent right,\" one that grows out of the more primary rights, e.g. to know and to speak. He admits that such a mostly-open world will seem more irksome and demanding; people will be expected to keep negotiating the tradeoffs between knowing and privacy. It will be tempting to pass laws that restrict the power of surveillance to authorities, entrusting them to protect our privacy—or a comforting illusion of privacy. By contrast, a transparent society destroys that illusion by offering everyone access to the vast majority of information out there.\n\nBrin argues that it will be good for society if the powers of surveillance are shared with the citizenry, allowing \"sousveillance\" or \"viewing from below,\" enabling the public to watch the watchers. According to Brin, this only continues the same trend promoted by Adam Smith, John Locke, the US Constitutionalists and the western enlightenment, who held that any elite (whether commercial, governmental, or aristocratic) should experience constraints upon its power. And there is no power-equalizer greater than knowledge. \n\nTransparency is sometimes confused with equiveillance (the balance between surveillance and sousveillance). This balance (equilibrium) allows the individual to construct their own case from evidence they gather themselves, rather than merely having access to surveillance data that could possibly incriminate them. Sousveillance therefore, in addition to transparency, assures contextual integrity of surveillance data (i.e. a lifelong capture of personal experience can provide \"best evidence\" over surveillance data to prevent the surveillance-only data from being taken out of context).\n\nSomewhat more nuanced than simply being \"against privacy,\" Brin spends an entire chapter exploring how important some degree of privacy is for most human beings, allowing them moments of intimacy, to exchange confidences, and to prepare - in some security - for the competitive world. Nevertheless, he suggests that we currently have more privacy than our ancestors, in part, because \"the last two hundred years have opened information flows, rather than shutting them down. Citizens are more able to catch violators of their rights - and hold them accountable - than commonfolk were in the old villages, that were dominated by local gentry, gossips and bullies.\"\n\nThis might seem counter-intuitive at first. But he uses the song \"Harper Valley PTA\" as a metaphor for how people can protect their eccentricities, and even some privacy, by assertively \"looking back.\" Brin also points to restaurants, in which social disapproval keeps people from staring and eavesdropping, even though they can. Enforcement of this social rule is possible because everybody can see.\n\nFrom this perspective, a coming era of \"most of the people, knowing most of what's going on, most of the time,\" would only be an extension of what already gave us the Enlightenment, freedom and privacy. By comparison, he asks what the alternative would be: \"To pass privacy laws that will be enforced by elites, and trust them to refrain from looking at us?\"\n\nBrin participated in the opening keynote panel discussion at the 2005 Association for Computing Machinery (ACM) Computers, Freedom, and Privacy conference, where 500 sousveillance devices were also created to contextualize and explore this debate further. (Each attendee was given a wearable camera-dome bag which created, in effect, an inverse panopticon.)\n\nBrin has introduced versions of the concept into his fiction.\n\nIn \"Earth,\" the setting's future history includes a war pitting most of the Earth against Switzerland, fueled by outrage over the Swiss allowing generations of kleptocrats to hide their stolen wealth in the country's secretive banks. The war results in the end of secret banking and the destruction of Switzerland as a nation. In the setting's present, surveillance by elderly retirees wearing recognizable networked camera-glasses is common.\n\nHis novel \"Kiln People\" is set in a future where cameras are everywhere and anyone can access the public ones and, for a fee, the private ones.\n\n\n\n"}
{"id": "4156877", "url": "https://en.wikipedia.org/wiki?curid=4156877", "title": "This Week in Science", "text": "This Week in Science\n\nThis Week in Science (\"TWIS\") is a science Talk radio broadcast from KDVS (90.3 FM) on the UC Davis campus. Each week, \"TWIS\" founder/host Kiki Sanford and co-host Justin Jackson review current research in technology. Listened to in 60 countries worldwide, \"TWIS\" reaches an international audience and regularly fields science questions on the air from listeners around the world.\n\nThe show is available live on FM radio in Northern California and via live internet broadcasts from the KDVS website. As of 2003, it was the most popular show on the station, which was the most popular college radio station west of the Mississippi river. Archived versions of the show as well as a podcast are available from the show's website.\n\nThe podcasts are linked to by over 300 websites, and is ranked in the top 1.6 million websites in the world.\n\nKirsten Sanford (founder/host) holds a B.S. in Conservation Biology and a Ph.D in Molecular, Cellular and Integrative Physiology from the University of California, Davis and is a frequent lecturer on the Davis campus. Sanford was awarded the 2005 American Association for the Advancement of Science (AAAS) Mass Media Fellowship Award in recognition for her work with \"This Week in Science\". Through this fellowship she worked as a television news producer at WNBC News in New York City working with noted health and science reporter Max Gomez.\n\nJustin Jackson has been the show's co-host since 2005.\n\nBlair Bazdarich joined as the show's third co-host in 2013 after serving as an intern for over a year.\n\n\"This Week in Science\" regularly interviews notable scientists, technologists, and luminaries. Past interviewees include:\n\n"}
{"id": "7217838", "url": "https://en.wikipedia.org/wiki?curid=7217838", "title": "Vancouver system", "text": "Vancouver system\n\nThe Vancouver system, also known as Vancouver reference style or the author–number system, is a citation style that uses numbers within the text that refer to numbered entries in the reference list. It is popular in the physical sciences and is one of two referencing systems normally used in medicine, the other being the author–date, or \"Harvard\", system. Vancouver style is used by MEDLINE and PubMed.\n\nHundreds of scientific journals use author-number systems. They all follow the same essential logic (that is, numbered citations pointing to numbered list entries), although the trivial details of the output mask, such as punctuation, casing of titles, and italic, vary widely among them. They have existed for over a century; the names \"Vancouver system\" or \"Vancouver style\" have existed since 1978. The latest version of the latter is \"Citing Medicine\", per the \"References > Style and Format\" section of the ICMJE Recommendations.\n\nIn the broad sense, the Vancouver system refers to any author-number system regardless of the formatting details. A narrower definition of the Vancouver system refers to a specific author-number format specified by the ICMJE Recommendations (Uniform Requirements for Manuscripts, URM). For example, the AMA reference style is Vancouver style in the broad sense because it is an author-number system that conforms to the URM, but not in the narrow sense because its formatting differs in some minor details from the NLM/PubMed style (such as what is italicized and whether the citation numbers are bracketed).\n\nAuthor–number systems have existed for over a century and throughout that time have been one of the main types of citation style in scientific journals (the other being author–date). In 1978, a committee of editors from various medical journals, the International Committee of Medical Journal Editors (ICMJE), met in Vancouver, BC, Canada to agree to a unified set of requirements for the articles of such journals. This meeting led to the establishment of the Uniform Requirements for Manuscripts Submitted to Biomedical Journals (URMs). Part of the URMs is the reference style, for which the ICMJE selected the long-established author–number principle.\n\nThe URMs were developed 15 years before the World Wide Web debuted. During those years, they were published as articles or supplements in various ICMJE member journals. These included the 1991 BMJ publication, the 1995 \"CMAJ\" publication and the 1997 \"Annals of Internal Medicine\" publication. In the late 1990s and early 2000s, journals were asked to cite the 1997 \"JAMA\" version when reprinting the \"Uniform requirements\".\n\nIn the early 2000s, with the Web having become a major force in academic life, the idea gradually took hold that the logical home for the latest edition of the URMs would be the ICMJE website itself (as opposed to whichever journal article or supplement had most recently published an update). For example, as of 2004, the editors of \"Haematologica\" decided simply to invite their authors to visit www.icmje.org for the 2003 revision of the \"Uniform requirements\".\n\nSince the early to mid-2000s, the United States National Library of Medicine (which runs MEDLINE and PubMed) has hosted the ICMJE's \"Sample References\" pages. Around 2007, the NLM created \"Citing Medicine\", its style guide for citation style, as a new home for the style's details. The ICMJE Recommendations now point to \"Citing Medicine\" as the home for the formatting details of Vancouver style. For example, in the December 2013 edition of the ICMJE Recommendations, the relevant paragraph is IV.A.3.g.ii. (\"References > Style and Format\").\n\nReferences are numbered consecutively in order of appearance in the text – they are identified by Arabic numerals in parentheses (1), square brackets [1], superscript, or a combination. The number usually appears at the end of the material it supports, and an entry in the reference list would give full bibliographical information for the source:\n\nAnd the entry in the reference list would be: 1. \n\nSeveral descriptions of the Vancouver system say that the number can be placed \"outside\" the text punctuation to avoid disruption to the flow of the text, \"or\" be placed \"inside\" the text punctuation, and that there are different cultures in different traditions. The first method is recommended by some universities and colleges, while the latter method is required by scientific publications such as the MLA and IEEE except for in the end of a block quotation. (IEEE are using Vancouver style labels within brackets, for example [1] to cite the first reference in the list, but otherwise refer to Chicago Style Manual.) The original Vancouver system documents (the ICMJE recommendations and Uniform Requirements for Manuscripts Submitted to Biomedical Journals) do not discuss placement of the citation mark.\n\nDifferent formats exist for different types of sources, e.g. books, journal articles etc. Author names are abbreviated to at most two initials. Although \"Citing Medicine\" does not explicitly mandate merging initials (e.g. \"R. K.\" would be merged into \"RK\"), the examples used throughout the book do.\n\n\nAs an option, if a journal carries continuous pagination throughout a volume (as many medical journals do), the month and issue number may be omitted.\n\n\nThe NLM lists all authors for all articles, because it is appropriate for capturing all authors and all of their publications in the MEDLINE database to be found by searches. However, in the reference lists of articles, most journals truncate the list after 3 or 6 names, followed by \"et al.\" (which most medical journals do not italicize):\n\n\nOptionally, a unique identifier (such as the article's DOI or PMID) may be added to the citation:\n\n\nNLM elides ending page numbers and uses a hyphen as the range indicating character (184-5). Some journals do likewise, whereas others expand the ending page numbers in full (184-185), use an en dash instead of a hyphen (184–5), or both (184–185).\n\nVirtually all medical journal articles are published online. Many are published online only, and many others are published online ahead of print. For the date of online publication, at the end of the citation NLM puts \"[Epub Year Mon Day]\" (for online-only publication) or \"[Epub ahead of print]\" for online ahead of print (with the month and day following the year in its normal position). In contrast, AMA style puts \"[published online Month Day, Year]\" at the end of the article title. It no longer uses the term \"Epub\" and no longer includes the words \"ahead of print\". It omits the year from its normal location after the journal title abbreviation if there is no print data to give (online-only publication).\n\nThe titles of journals are abbreviated. There are no periods in the abbreviation. A period comes after the abbreviation, delimiting it from the next field. The abbreviations are standardized. The standardization was formerly incomplete and internal to organizations such as NLM. It is now formalized at the supraorganizational level by documents including \"Citing Medicine\" at Appendix A: Abbreviations for Commonly Used English Words in Journal Titles, ANSI Z39.5, ISO 4: Information and documentation -- Rules for the abbreviation of title words and titles of publications, and the ISSN.org List of Title Word Abbreviations (LTWA).\n\nAs per journal articles in English:\n\n\nThe NLM adds an English translation of the title enclosed in square brackets right after the title. The language is specified in full after the location (pagination), followed by a period.\n\n\n\n\n\nMany medical institutions maintain their own style guides, with information on how to cite sources:\n"}
