{"id": "1349840", "url": "https://en.wikipedia.org/wiki?curid=1349840", "title": "A Letter to a Friend", "text": "A Letter to a Friend\n\nA Letter to a Friend (written 1656; published posthumously in 1690), by Sir Thomas Browne, the 17th century philosopher and physician, is a medical treatise of case-histories and witty speculations upon the human condition.\n\nIt is believed to be the source of a term Mary Leitao found in 2001 to describe her son's skin condition. She chose the name \"Morgellons disease\" from a skin condition described by Browne in \"Letter to a Friend\", thus:\n\nThere is, however, no suggestion that the symptoms described by Browne are linked to the alleged modern cases of Morgellons.\n\n"}
{"id": "51238945", "url": "https://en.wikipedia.org/wiki?curid=51238945", "title": "Advancing Secondary Science Education thru Tetrahymena", "text": "Advancing Secondary Science Education thru Tetrahymena\n\nAdvancing Secondary Science Education thru Tetrahymena (ASSET) is an organization at Cornell University that is dedicated to expanding the use of the protist \"Tetrahymena\" in K-12 classrooms. They are funded by the National Institutes of Health through the SEPA (Science Education Partnership Award) Program. Although their name includes the word \"secondary,\" they have worked in recent years to develop materials for students in elementary, middle and high schools. The group develops modules, which are stand-alone labs or lessons that can be inserted into the curriculum of a class at the discretion of the teacher.\n\nModules are designed to be stand-alone lessons that fit into and compliment a life science curriculum. The ASSET program ships all the equipment that is needed to complete the modules to the teacher in a reusable plastic container, at ASSET's expense. The teacher who requested the materials can use them for up to two weeks. At the end of the two weeks, the teacher uses a pre-paid return label to send the materials back in the same container. Some materials, such as live cells, may be sent separately to provide for a chance for the culture to be established in the teacher's classroom.\n\nASSET has created fifteen science modules, each of which addresses a particular topic relevant to life science education in United States school curricula.\n\nThis module utilizes two species of \"Tetrahymena\": \"Tetrahymena thermophilia\" and \"Tetrahymena vorax\". In the lab, an extract, called stomatin, is made from the thermophilia, then placed into the vorax culture. There, it induces a transformation from the microstome form to the macrostome form in T. vorax. This transformation is most notable by a marked increase in the size of the cell (doubling or sometimes more), the resorption of the microstomal oral apparatus and the construction of a much larger macrostomal oral apparatus. This transformation allows the macrostomal T. vorax cells to prey on T. thermophilia, but also to cannibalize the microstomes of their own species.\n\nASSET has also created five science and society modules, which are designed to integrate social studies and science education into one unit.\n\n\n"}
{"id": "46952181", "url": "https://en.wikipedia.org/wiki?curid=46952181", "title": "African Genesis", "text": "African Genesis\n\nAfrican Genesis: A Personal Investigation into the Animal Origins and Nature of Man, usually referred to as \"African Genesis,\" is a 1961 nonfiction work by Robert Ardrey. It posited the hypothesis that man evolved on the African continent from carnivorous, predatory ancestors who distinguished themselves from apes by the use of weapons. The work bears on questions of human origins, human nature, and human uniqueness. It has been widely read and continues to inspire significant controversy.\n\n\"African Genesis\" is the first in Robert Ardrey's \"Nature of Man Series\". It is followed by \"The Territorial Imperative\" (1966), \"The Social Contract\" (1970), and \"The Hunting Hypothesis\" (1976). It was illustrated by Ardrey's wife, the South African actress and illustrator Berdine Ardrey (née Grunewald).\n\nRobert Ardrey, at the time a working playwright and screenwriter, travelled in 1955 to Africa, partly at the behest of Richard Foster Flint, to investigate claims made by Raymond Dart about a specimen of Australopithecus africanus.\n\nHe met Dart in March 1955. Dart, in his laboratory at Witwatersrand University Medical School, had assembled evidence for a controversial thesis. Among the collection were fossil baboon skulls from the caves of Taung, Terkfontein and Makapan that he believed showed fractures caused by \"Australopithecus\" wielding bone clubs; the jaw of a juvenile ape-man from Makapansgat which had been fractured and lost its incisors; and 7,000 fossil bones from the Makapansgat cave. Among the fossils, skulls and lower leg bones were disproportionately represented, leading Dart to theorize that man's ancestors were hunters who used bones as weapons. His overall thesis was that \"it was the ape-man's instinct for violence, and his successful development of lethal weapons, that gave him his dominance in the animal world from the very beginning. Those instincts are with us today.\" Ardrey was initially much taken by the theory. As a correspondent he wrote an article about it for \"The Reporter\". After receiving significant attention the article was reprinted in \"Science Digest,\" which marked the beginning of the spread of popular notions about \"Australopithecus.\" The article in \"Science Digest\" also led to The Smithsonian Institution contacting Dart and eventually providing him funding to continue his research.\n\nFollowing a visit by Dr. Kenneth P. Oakley Ardrey agreed to write a book on the subject. Oakley secured an office for Ardrey in the National History Museum in London, as well as access to its private libraries. Ardrey spent six years traveling between Northern universities and African archeological sites. During this time he worked with many notable scientists, including Louis Leakey (then affiliated with the Coryndon Museum in Kenya) and Tony Sutcliffe (then affiliated with the Royal Archaeological Institute).\n\nArdrey eventually came to be a vocal proponent of this thesis, introducing it, in modified form, to a broad audience with \"African Genesis\". He added to it his own ideas about the role of territory in human behavior, about hierarchy in social animals, and about the instinctual status of the urge to dominate ones fellows.\n\n\"African Genesis\" met with massive popular success and widespread recognition. It became an international bestseller and was translated into dozens of languages. In 1962 it was a finalist for the National Book Award in nonfiction. In 1969 \"Time\" magazine named \"African Genesis\" the most notable nonfiction book of the '60s. The book has continued to bear on the popular imagination of human nature.\n\nArdrey's theories flew in the face of prevailing theories of human origins. At the time of the publication of \"African Genesis\" it was generally agreed that human beings evolved from Asian ancestors. Furthermore, it was taken for granted that these ancestors were herbivorous. The idea of an African Genesis of humanity was met with fervent resistance in the scientific community.\n\nOn a grander scale, Ardrey challenged the reigning methodological assumption of the social sciences, that human behavior was fundamentally distinct from animal behavior. As he put it in his next book, \"The Territorial Imperative\", \"The dog barking at you from behind his master’s fence acts for a motive indistinguishable from that of his master when the fence was built.\"\n\nFollowing the publication of \"African Genesis\" Ardrey's theories became mired in controversy because of his notions about innate human violence and inherited instinctual aggression. (For more details, see \"The Territorial Imperative\".) Later commentators, however, have come to emphasize the broader implications of Ardrey's theories; it is now commonly accepted that the controversy obscured the core of his thinking. William Wright, for example, writing in 2013, writes \"Not only was Ardrey, with his three-million-year-old unsolved murders, claiming that evolution has saddled us with a battery of behavioral traits, but he was also reckless enough to emphasize the most repugnant, the killer impulse. This inflammatory claim certainly won Ardrey attention, but the angry controversy it provoked almost obscured the main point: that human behavior is as much a product of evolution as the human body.\"\n\nWhile Ardrey's theses on aggression were controversial, he was also challenged on his conviction that the study of animal behavior is necessarily relevant to the study of human behavior. This precept has gained widespread acceptance and, due in large part to Ardrey's work, passed into the scientific commonsense. Following the 1961 publication of \"African Genesis\" the science of ethology, which is based on the methodological assumption of the cross-relevance of anthropology and zoology, underwent a massive flourishing. 1966 saw Lorenz's \"On Aggression\" published, followed by Desmond Morris's \"The Naked Ape\" in 1967, Lionel Tiger's \"Men in Groups\" in 1969, and Tiger and Fox's \"The Imperial Animal\" in 1971. Along with ethology's ascendence came a renaissance of its central premise—then much derided in scientific communities by blank-state theorists—that the study of animal behavior could tell us much about human behavior.\n\n\"African Genesis\" led Ardrey into a long career of work in anthropology and ethology. Regarding his later-in-life return to science, Ardrey wrote \"while peasant and poet may apprehend a truth, it is the obligation of science to define it, to prove it, to assimilate its substance into the body of scientific thought, and to make its conclusions both available and understandable to the society of which science is a part.\"\n\nHis writings on paleoanthropology, ethnology, and anthropology, along with the massive popular success of \"African Genesis,\" are widely credited with initiating public interest in these fields and sparking widespread popular debate about human nature as it is connected to human evolution. C.K. Brain, for example, writes: \n\"African Genesis\" has, in all probability, been read by more people throughout the world than any other book on human evolution and the nature of man. Its influence has been very great indeed as it fermented an intense debate about these topics, and catalysed a new set of concepts in paleoanthropology.\n\nSeveral scientists credit Ardrey's work, and \"African Genesis\" in particular, with launching them into their studies. Famed paleoanthropologist Rick Potts, who has been the director of the Smithsonian Institution Museum of Natural History's Human Origins Program since 1985 points to \"African Genesis\" as one of the two most formative books of his early years. In the 2015 PBS film documentary \"Dawn of Humanity,\" Potts recites the beginning of the book from memory.\n\nIn 1972, defending his film \"A Clockwork Orange\" from Fred M. Hechinger, Stanley Kubrick cited Ardrey. In particular, he quoted \"African Genesis\" (along with \"The Social Contract\"). Kubrick was a notable fan of Ardrey's work, and also cited him as an inspiration for his 1968 film, \"\". Nonetheless, the behavior of the apes in the \"Dawn of Man\" sequence of \"2001\" has since been \"proven false\", since violent apes such as these have now been shown to be \"vegetarians\" instead—according to archeologist K. Kris Hirst in reviewing the 2015 PBS documentary film \"Dawn of Humanity\", which describes, directly in the context of \"2001\", the 2015 studies of fossils of \"Homo naledi\".\n\nA.J. Jacobs, who wrote the 2004 book \"The Know-It-All\", about reading the entire \"Encyclopædia Britannica\", states in an interview that a quote from African Genesis was the most profound thing he read while reading the Encyclopædia.\n\n"}
{"id": "52218622", "url": "https://en.wikipedia.org/wiki?curid=52218622", "title": "Arthur Good", "text": "Arthur Good\n\nArthur Good (16 or 26 August 1853 – 30 March 1928) was a French engineer, science educator, author and caricaturist who used the pen name Tom Tit. He wrote a series of weekly articles, \"La Science Amusante\", or \"Amusing Science\", that were collected in book form and have been translated and republished in more than 130 editions in several languages. The illustrations for his do-it-yourself scientific apparatuses have been described as surrealist collages, and were an inspiration for surrealist artists such as Max Ernst and Joseph Cornell.\n\nArthur Good was born in Montivilliers, Seine-Maritime, France on 16 or 26 August 1853. He was the son of Protestant pastor Gustave Frédéric Good (1823–1896) and Louise Stéphanie Monod (1827–1909).\n\nGood graduated from the École centrale des arts et manufacture in Paris, where he studied engineering.\n\nHe married Jeanne Valon (1857–1910) in Paris on 6 April 1881. They had four children.\n\nUnder the pen name Tom Tit, Arthur Good wrote a series of weekly articles, \"La Science Amusante\", or \"Amusing Science\", for the French magazine \"L’Illustration\". Good presented a range of physical experiments, from \"simple games meant to amuse the family\" to experiments \"of a truly scientific character\". They introduce a range of physical and scientific principles including magnetism and surface tension. Good's articles include geometrical demonstrations, craft projects, and physics experiments which can be carried out with everyday household materials. In books such as \"La Récréation En Famille\" he emphasized that scientific education could be a common activity and amusement for the entire family. He dedicated \"La Science Amusante\" to one of his children, saying \"In dedicating this volume today, I would like it to be a souvenir for you of the happy moments we have spent together trying the experiments and constructing the apparatuses\".\n\nGood created improvised scientific apparatuses like his \"Soap-bubble Chandelier\" using common items such as bottles, eggs, corks, candles, and soap. His constructions have an imaginative charm that has led to them being compared to surrealistic collages. Nonetheless, his drawings were seriously and carefully rendered by scientific engraver Louis Poyet (1846–1913) and his assistants.\n\nThe original columns from \"La Science Amusante\" were collected and published in a three-volume series in France. Each volume contained 100 amusements. Beginning in 1889, they have been reprinted in over 130 editions. Collections of amusements were translated and published in English, Italian, and Spanish. In the United States they appeared as \"Magical Experiments, or Science in Play\", and in England as \"Scientific Amusements\". A selection has also been republished as \"100 Amazing Magic Tricks\". Good's books are considered to have laid the foundations for modern approaches to science education in their introduction of \"kitchen science\" and hands-on experiments for children.\n\nGood also published instructions for DIY entertainments in \"Pour Amuser Les Petits ou les joujoux qu’on peut faire soi-même\" (To Amuse the Little Ones, or Do-It-Yourself Small Toys), \"La Récréation En Famille\" (Family Recreations), \"Les Bons Jeudis\" (Fun Thursdays – in Good's day in France there were no classes on Thursdays), and \"Joujoux en Papier\" (Paper Toys). From 1885 to 1888, he was the editor of a periodical, \"Le Chercheur\", that featured new inventions. He also wrote for \"La Nature\".\n\nIn addition to his science education publications, Good published a set of \"Caricatures\" of famous Britons in London in 1913. He received a medal of honor from the National Society for the Development of Good (\"Société nationale d'encouragement au bien\").\n\nDuring the 1920s and 1930s, surrealist artists such as Max Ernst and\nJoseph Cornell were intrigued by the Tom Tit illustrations, and incorporated them into their own works.\n\n"}
{"id": "664", "url": "https://en.wikipedia.org/wiki?curid=664", "title": "Astronaut", "text": "Astronaut\n\nAn astronaut or cosmonaut is a person trained by a human spaceflight program to command, pilot, or serve as a crew member of a spacecraft. Although generally reserved for professional space travelers, the terms are sometimes applied to anyone who travels into space, including scientists, politicians, journalists, and tourists.\n\nUntil 2002, astronauts were sponsored and trained exclusively by governments, either by the military or by civilian space agencies. With the suborbital flight of the privately funded SpaceShipOne in 2004, a new category of astronaut was created: the commercial astronaut.\n\nThe criteria for what constitutes human spaceflight vary. The Fédération Aéronautique Internationale (FAI) Sporting Code for astronautics recognizes only flights that exceed an altitude of. In the United States, professional, military, and commercial astronauts who travel above an altitude of are awarded astronaut wings.\n\n, a total of 552 people from 36 countries have reached or more in altitude, of which 549 reached low Earth orbit or beyond.\nOf these, 24 people have traveled beyond low Earth orbit, either to lunar orbit, the lunar surface, or, in one case, a loop around the Moon. Three of the 24–Jim Lovell, John Young and Eugene Cernan–did so twice. The three current astronauts who have flown without reaching low Earth orbit are spaceplane pilots Joe Walker, Mike Melvill, and Brian Binnie, who participated in suborbital missions.\n\n, under the U.S. definition, 558 people qualify as having reached space, above altitude. Of eight X-15 pilots who exceeded in altitude, only one exceeded 100 kilometers (about 62 miles). Space travelers have spent over 41,790 man-days (114.5 man-years) in space, including over 100 astronaut-days of spacewalks. , the man with the longest cumulative time in space is Gennady Padalka, who has spent 879 days in space. Peggy A. Whitson holds the record for the most time in space by a woman, 377 days.\n\nIn 1959, when both the United States and Soviet Union were planning, but had yet to launch humans into space, NASA Administrator T. Keith Glennan and his Deputy Administrator, Dr. Hugh Dryden, discussed whether spacecraft crew members should be called \"astronauts\" or \"cosmonauts\". Dryden preferred \"cosmonaut\", on the grounds that flights would occur in the \"cosmos\" (near space), while the \"astro\" prefix suggested flight to the stars. Most NASA Space Task Group members preferred \"astronaut\", which survived by common usage as the preferred American term. When the Soviet Union launched the first man into space, Yuri Gagarin in 1961, they chose a term which anglicizes to \"cosmonaut\".\n\nIn English-speaking nations, a professional space traveler is called an \"astronaut\". The term derives from the Greek words \"ástron\" (ἄστρον), meaning \"star\", and \"nautes\" (ναύτης), meaning \"sailor\". The first known use of the term \"astronaut\" in the modern sense was by Neil R. Jones in his 1930 short story \"The Death's Head Meteor\". The word itself had been known earlier; for example, in Percy Greg's 1880 book \"Across the Zodiac\", \"astronaut\" referred to a spacecraft. In \"Les Navigateurs de l'Infini\" (1925) by J.-H. Rosny aîné, the word \"astronautique\" (astronautic) was used. The word may have been inspired by \"aeronaut\", an older term for an air traveler first applied in 1784 to balloonists. An early use of \"astronaut\" in a non-fiction publication is Eric Frank Russell's poem \"The Astronaut\", appearing in the November 1934 \"Bulletin of the British Interplanetary Society\".\n\nThe first known formal use of the term astronautics in the scientific community was the establishment of the annual International Astronautical Congress in 1950, and the subsequent founding of the International Astronautical Federation the following year.\n\nNASA applies the term astronaut to any crew member aboard NASA spacecraft bound for Earth orbit or beyond. NASA also uses the term as a title for those selected to join its Astronaut Corps. The European Space Agency similarly uses the term astronaut for members of its Astronaut Corps.\n\nBy convention, an astronaut employed by the Russian Federal Space Agency (or its Soviet predecessor) is called a \"cosmonaut\" in English texts. The word is an anglicisation of the Russian word \"kosmonavt\" (, ), one who works in space outside the Earth's atmosphere, a space traveler, which derives from the Greek words \"kosmos\" (κόσμος), meaning \"universe\", and \"nautes\" (ναύτης), meaning \"sailor\". Other countries of the former Eastern Bloc use variations of the Russian word \"kosmonavt\", such as the Polish \"kosmonauta\".\n\nCoinage of the term \"kosmonavt\" has been credited to Soviet aeronautics pioneer Mikhail Tikhonravov (1900–1974). The first cosmonaut was Soviet Air Force pilot Yuri Gagarin, also the first person in space. Valentina Tereshkova, a Russian factory worker, was the first woman in space, as well as the first civilian among the Soviet cosmonaut or NASA astronaut corps to make a spaceflight. On March 14, 1995, Norman Thagard became the first American to ride to space on board a Russian launch vehicle, and thus became the first \"American cosmonaut\".\n\n\"Yǔ háng yuán\" (, \"Space-universe navigating personnel\") is used for astronauts and cosmonauts in general, while \"Hángtiān yuán\" (, \"navigating outer space personnel\") is used for Chinese astronauts. Here, \"Hángtiān\" () is strictly defined as the navigation of outer space within the local star system, i.e. Solar system. The phrase \"tài kōng rén\" (, \"spaceman\") is often used in Hong Kong and Taiwan.\n\nThe term \"taikonaut\" is used by some English-language news media organizations for professional space travelers from China. The word has featured in the Longman and Oxford English dictionaries, the latter of which describes it as \"a hybrid of the Chinese term \"taikong\" (space) and the Greek \"naut\" (sailor)\"; the term became more common in 2003 when China sent its first astronaut Yang Liwei into space aboard the \"Shenzhou 5\" spacecraft. This is the term used by Xinhua News Agency in the English version of the Chinese \"People's Daily\" since the advent of the Chinese space program. The origin of the term is unclear; as early as May 1998, Chiew Lee Yih () from Malaysia, used it in newsgroups.\n\nWith the rise of space tourism, NASA and the Russian Federal Space Agency agreed to use the term \"spaceflight participant\" to distinguish those space travelers from professional astronauts on missions coordinated by those two agencies.\n\nWhile no nation other than the Russian Federation (and previously the former Soviet Union), the United States, and China have launched a manned spacecraft, several other nations have sent people into space in cooperation with one of these countries. Inspired partly by these missions, other synonyms for astronaut have entered occasional English usage. For example, the term \"spationaut\" (French spelling: \"spationaute\") is sometimes used to describe French space travelers, from the Latin word \"spatium\" for \"space\", the Malay term \"angkasawan\" was used to describe participants in the Angkasawan program, and the Indian Space Research Organisation hope to launch a spacecraft in 2022 that would carry \"vyomanauts\", coined from the Sanskrit word for space. In Finland, the NASA astronaut Timothy Kopra, a Finnish American, has sometimes been referred to as \"sisunautti\", from the Finnish word \"sisu\".\n\nThe first human in space was Soviet Yuri Gagarin, who was launched on April 12, 1961, aboard Vostok 1 and orbited around the Earth for 108 minutes. The first woman in space was Soviet Valentina Tereshkova, who launched on June 16, 1963, aboard Vostok 6 and orbited Earth for almost three days.\n\nAlan Shepard became the first American and second person in space on May 5, 1961, on a 15-minute sub-orbital flight. The first American to orbit the Earth was John Glenn, aboard Friendship 7 on February 20, 1962. The first American woman in space was Sally Ride, during Space Shuttle Challenger's mission STS-7, on June 18, 1983. In 1992 Mae Jemison became the first African American woman to travel in space aboard STS-47.\n\nCosmonaut Alexei Leonov was the first person to conduct an extravehicular activity (EVA), (commonly called a \"spacewalk\"), on March 18, 1965, on the Soviet Union's Voskhod 2 mission. This was followed two and a half months later by astronaut Ed White who made the first American EVA on NASA's Gemini 4 mission.\n\nThe first manned mission to orbit the Moon, \"Apollo 8\", included American William Anders who was born in Hong Kong, making him the first Asian-born astronaut in 1968.\n\nThe Soviet Union, through its Intercosmos program, allowed people from other \"socialist\" (i.e. Warsaw Pact and other Soviet-allied) countries to fly on its missions, with the notable exception of France participating in Soyuz TM-7. An example is Czechoslovak Vladimír Remek, the first cosmonaut from a country other than the Soviet Union or the United States, who flew to space in 1978 on a Soyuz-U rocket.\n\nOn July 23, 1980, Pham Tuan of Vietnam became the first Asian in space when he flew aboard Soyuz 37. Also in 1980, Cuban Arnaldo Tamayo Méndez became the first person of Hispanic and black African descent to fly in space, and in 1983, Guion Bluford became the first African American to fly into space. In April 1985, Taylor Wang became the first ethnic Chinese person in space. The first person born in Africa to fly in space was Patrick Baudry (France), in 1985. In 1985, Saudi Arabian Prince Sultan Bin Salman Bin AbdulAziz Al-Saud became the first Arab Muslim astronaut in space. In 1988, Abdul Ahad Mohmand became the first Afghan to reach space, spending nine days aboard the Mir space station.\n\nWith the increase of seats on the Space Shuttle, the U.S. began taking international astronauts. In 1983, Ulf Merbold of West Germany became the first non-US citizen to fly in a US spacecraft. In 1984, Marc Garneau became the first of 8 Canadian astronauts to fly in space (through 2010).\nIn 1985, Rodolfo Neri Vela became the first Mexican-born person in space. In 1991, Helen Sharman became the first Briton to fly in space.\nIn 2002, Mark Shuttleworth became the first citizen of an African country to fly in space, as a paying spaceflight participant. In 2003, Ilan Ramon became the first Israeli to fly in space, although he died during a re-entry accident.\n\nOn October 15, 2003, Yang Liwei became China's first astronaut on the Shenzhou 5 spacecraft.\n\nThe youngest person to fly in space is Gherman Titov, who was 25 years old when he flew Vostok 2. (Titov was also the first person to suffer space sickness).\nThe oldest person who has flown in space is John Glenn, who was 77 when he flew on STS-95.\n\n438 days is the longest time spent in space, by Russian Valeri Polyakov.\nAs of 2006, the most spaceflights by an individual astronaut is seven, a record held by both Jerry L. Ross and Franklin Chang-Diaz. The farthest distance from Earth an astronaut has traveled was , when Jim Lovell, Jack Swigert, and Fred Haise went around the Moon during the Apollo 13 emergency.\n\nThe first civilian in space was Valentina Tereshkova aboard Vostok 6 (she also became the first woman in space on that mission).\nTereshkova was only honorarily inducted into the USSR's Air Force, which did not accept female pilots at that time. A month later, Joseph Albert Walker became the first American civilian in space when his X-15 Flight 90 crossed the line, qualifying him by the international definition of spaceflight. Walker had joined the US Army Air Force but was not a member during his flight. \nThe first people in space who had never been a member of any country's armed forces were both Konstantin Feoktistov and Boris Yegorov aboard Voskhod 1.\n\nThe first non-governmental space traveler was Byron K. Lichtenberg, a researcher from the Massachusetts Institute of Technology who flew on STS-9 in 1983. In December 1990, Toyohiro Akiyama became the first paying space traveler as a reporter for Tokyo Broadcasting System, a visit to Mir as part of an estimated $12 million (USD) deal with a Japanese TV station, although at the time, the term used to refer to Akiyama was \"Research Cosmonaut\". Akiyama suffered severe space sickness during his mission, which affected his productivity.\n\nThe first self-funded space tourist was Dennis Tito on board the Russian spacecraft Soyuz TM-3 on April 28, 2001.\n\nThe first person to fly on an entirely privately funded mission was Mike Melvill, piloting SpaceShipOne flight 15P on a suborbital journey, although he was a test pilot employed by Scaled Composites and not an actual paying space tourist. Seven others have paid the Russian Space Agency to fly into space:\n\n\nThe first NASA astronauts were selected for training in 1959. Early in the space program, military jet test piloting and engineering training were often cited as prerequisites for selection as an astronaut at NASA, although neither John Glenn nor Scott Carpenter (of the Mercury Seven) had any university degree, in engineering or any other discipline at the time of their selection. Selection was initially limited to military pilots. The earliest astronauts for both America and the USSR tended to be jet fighter pilots, and were often test pilots.\n\nOnce selected, NASA astronauts go through twenty months of training in a variety of areas, including training for extravehicular activity in a facility such as NASA's Neutral Buoyancy Laboratory. Astronauts-in-training (astronaut candidates) may also experience short periods of weightlessness (microgravity) in an aircraft called the \"Vomit Comet,\" the nickname given to a pair of modified KC-135s (retired in 2000 and 2004, respectively, and replaced in 2005 with a C-9) which perform parabolic flights. Astronauts are also required to accumulate a number of flight hours in high-performance jet aircraft. This is mostly done in T-38 jet aircraft out of Ellington Field, due to its proximity to the Johnson Space Center. Ellington Field is also where the Shuttle Training Aircraft is maintained and developed, although most flights of the aircraft are conducted from Edwards Air Force Base.\n\nAstronauts is training must learn how to control and fly the Space Shuttle and, it is vital that they are familiar with the International Space Station so they know what they must do when they get there.\n\n\n\n\nMission Specialist Educators, or \"Educator Astronauts\", were first selected in 2004, and as of 2007, there are three NASA Educator astronauts: Joseph M. Acaba, Richard R. Arnold, and Dorothy Metcalf-Lindenburger.\nBarbara Morgan, selected as back-up teacher to Christa McAuliffe in 1985, is considered to be the first Educator astronaut by the media, but she trained as a mission specialist.\nThe Educator Astronaut program is a successor to the Teacher in Space program from the 1980s.\n\nAstronauts are susceptible to a variety of health risks including decompression sickness, barotrauma, immunodeficiencies, loss of bone and muscle, loss of eyesight, orthostatic intolerance, sleep disturbances, and radiation injury. A variety of large scale medical studies are being conducted in space via the National Space and Biomedical Research Institute (NSBRI) to address these issues. Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity Study in which astronauts (including former ISS commanders Leroy Chiao and Gennady Padalka) perform ultrasound scans under the guidance of remote experts to diagnose and potentially treat hundreds of medical conditions in space. This study's techniques are now being applied to cover professional and Olympic sports injuries as well as ultrasound performed by non-expert operators in medical and high school students. It is anticipated that remote guided ultrasound will have application on Earth in emergency and rural care situations, where access to a trained physician is often rare.\n\nA 2006 Space Shuttle experiment found that \"Salmonella typhimurium\", a bacterium that can cause food poisoning, became more virulent when cultivated in space. More recently, in 2017, bacteria were found to be more resistant to antibiotics and to thrive in the near-weightlessness of space. Microorganisms have been observed to survive the vacuum of outer space.\n\nOn December 31, 2012, a NASA-supported study reported that manned spaceflight may harm the brain and accelerate the onset of Alzheimer's disease.\n\nIn October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.\n\nOver the last decade, flight surgeons and scientists at NASA have seen a pattern of vision problems in astronauts on long-duration space missions. The syndrome, known as visual impairment intracranial pressure (VIIP), has been reported in nearly two-thirds of space explorers after long periods spent aboard the International Space Station (ISS).\n\nOn November 2, 2017, scientists reported that significant changes in the position and structure of the brain have been found in astronauts who have taken trips in space, based on MRI studies. Astronauts who took longer space trips were associated with greater brain changes.\n\nBeing in space can be physiologically deconditioning on the body. It can affect the otolith organs and adaptive capabilities of the central nervous system. Zero gravity and cosmic rays can cause many implications for astronauts.\n\nIn October 2018, NASA-funded researchers found that lengthy journeys into outer space, including travel to the planet Mars, may substantially damage the gastrointestinal tissues of astronauts. The studies support earlier work that found such journeys could significantly damage the brains of astronauts, and age them prematurely.\n\nResearchers in 2018 reported, after detecting the presence on the International Space Station (ISS) of five \"Enterobacter bugandensis\" bacterial strains, none pathogenic to humans, that microorganisms on ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts.\n\nAn astronaut on the International Space Station requires about 0.83 kilograms (1.83 pounds) weight of food inclusive of food packaging per meal each day. (The packaging for each meal weighs around 0.12 kilograms - 0.27 pounds) Longer-duration missions require more food.\n\nShuttle astronauts worked with nutritionists to select menus that appeal to their individual tastes. Five months before flight, menus are selected and analyzed for nutritional content by the shuttle dietician. Foods are tested to see how they will react in a reduced gravity environment. Caloric requirements are determined using a basal energy expenditure (BEE) formula.\nOn Earth, the average American uses about 35 gallons (132 liters) of water every day. On board the ISS astronauts limit water use to only about three gallons (11 liters) per day.\n\nIn Russia, cosmonauts are awarded Pilot-Cosmonaut of the Russian Federation upon completion of their missions, often accompanied with the award of Hero of the Russian Federation. This follows the practice established in the USSR where cosmonauts were usually awarded the title Hero of the Soviet Union.\n\nAt NASA, those who complete astronaut candidate training receive a silver lapel pin. Once they have flown in space, they receive a gold pin. U.S. astronauts who also have active-duty military status receive a special qualification badge, known as the Astronaut Badge, after participation on a spaceflight. The United States Air Force also presents an Astronaut Badge to its pilots who exceed in altitude.\n\nEighteen astronauts (fourteen men and four women) have lost their lives during four space flights. By nationality, thirteen were American (including one born in India), four were Russian (Soviet Union), and one was Israeli.\n\nEleven people (all men) have lost their lives training for spaceflight: eight Americans and three Russians. Six of these were in crashes of training jet aircraft, one drowned during water recovery training, and four were due to fires in pure oxygen environments.\n\nThe Space Mirror Memorial, which stands on the grounds of the John F. Kennedy Space Center Visitor Complex, commemorates the lives of the men and women who have died during spaceflight and during training in the space programs of the United States. In addition to twenty NASA career astronauts, the memorial includes the names of a U.S. Air Force X-15 test pilot, a U.S. Air Force officer who died while training for a then-classified military space program, and a civilian spaceflight participant.\n\n"}
{"id": "1251862", "url": "https://en.wikipedia.org/wiki?curid=1251862", "title": "Biological Physics", "text": "Biological Physics\n\nBiological Physics: Energy, Information, Life: With new art by David Goodsell is a book by Philip Nelson, illustrated by David Goodsell. The fifth printing was published by W. H. Freeman in late 2013. It is a work on biology with an emphasis on the application of physical principles.\n\n"}
{"id": "53466077", "url": "https://en.wikipedia.org/wiki?curid=53466077", "title": "Brain Electrical Oscillation Signature Profiling", "text": "Brain Electrical Oscillation Signature Profiling\n\nBrain Electrical Oscillation Signature Profiling (BEOSP or BEOS) is a technique by which a suspect's participation in a crime is detected by eliciting electrophysiological impulses.\n\nIt is a non-invasive, scientific technique with a great degree of sensitivity and a neuro-psychological method of interrogation which is sometimes also referred to as ‘brain fingerprinting’.\n\nThe methodology was developed by Champadi Raman Mukundan (C. R. Mukundan), a Neuroscientist, former Professor & Head of Clinical Psychology at the National Institute of Mental Health and Neurosciences (Bangalore, India), while he worked as a Research Consultant to TIFAC-DFS Project on ‘Normative Data for Brain Electrical Activation Profiling’.\n\nHis works are based on research that was also formerly pursued by other scientists at American universities including J. Peter Rosenfeld, Lawrence Farwell & Emanuel Donchin.\n\nThe human brain receives millions of arrays of signals in different modalities, all through the waking periods. These signals are classified and stored in terms of their relationship perceived as function of experience and available knowledge base of an individual, as well as new relationship produced through sequential processing. The process of encoding is primarily when the individual is directly participating in an activity or experiencing it.\n\nIt is considered secondary, when the information is obtained from a secondary source viz. books, conversations, hearsay etc. in which there is no primary experiential component and the brain deals mainly with conceptual aspects.\n\nPrimary encoding is deep seated and has specific source memory in terms of time and space of occurrence of experience, as individual himself/herself has shared or participated in the experience/act/event at certain time in his/her life at a certain place.\n\nIt is found that when the brain of an individual is activated by a piece of information of an event in which he/she has taken part, the brain of the individual will respond differently from that of a person who has received the same information from secondary sources (non-experiential).\n\nBEOSP is based on this principle, thereby intending to demonstrate that the suspect who have primary encoded information of those who have participated in the suspected events will show responses indicating firsthand (personally acquired) knowledge of the event.\n\n\nIdeally, no questions are to be asked while conducting the test; rather, the subject is simply provided with the probable events/scenarios in the aftermath of which, the results are analyzed to verify if the brain produces any experiential knowledge, which is essentially the recognition of events disclosed. This way, all fundamental rights are protected, as neither there are no questions that are being asked or any answers reciprocated.\n\nUniversity of Pennsylvania conducted a research along with the Brigham & Women's Hospital (Boston, Massachusetts), Children's Hospital Boston & the University Hospital of Freiburg, Germany which determined that Gamma Oscillations in the brain could help distinguish false memories from the real ones. Their analysis concluded that in the retrieval of truthful memories, as compared to false, human brain creates an extremely distinct pattern of gamma oscillations, indicating a recognition of context based information associated with a prior experience.\n\n\n"}
{"id": "57830594", "url": "https://en.wikipedia.org/wiki?curid=57830594", "title": "Candida hypersensitivity", "text": "Candida hypersensitivity\n\nCandida hypersensitivity is a pseudoscientific disease promoted by William G. Crook, M.D. It is spuriously claimed that chronic yeast infections are responsible for many common disorders and non-specific symptoms including fatigue, weight gain, constipation, dizziness, muscle and joint pain, asthma, and others.\n\n\"Candida albicans\" is a fungus that colonizes a large majority of the population (meaning it is present in the body but not causing an infection or any problems). Under certain conditions, however, it can cause an infection. The most common manifestations are thrush (a superficial \"Candida\" infection in the mouth) and vaginitis, also commonly referred to as a yeast infection. \"Candida\" can also cause serious systemic infection, but this is almost always restricted to those with compromised immune systems, such as patients undergoing chemotherapy or with advanced AIDS.\n\nAfter reading publications by C. Orian Truss, M.D., Crook proposed the idea that a condition he termed systemic candidiasis, or Candida hypersensitivity, was responsible for a long list of common conditions and non-specific symptoms including fatigue, asthma, psoriasis, sexual dysfunction, and many others. The list of symptoms is similar to that of multiple chemical sensitivity. Many patients presenting with symptoms of environmental sensitivity claim to suffer from multiple \"fashionable\" syndromes.\n\nThe American Academy of Allergy, Asthma, and Immunology strongly criticized the concept of \"candidiasis hypersensitivity syndrome\" and the diagnostic and treatment approaches its proponents use. AAAAI's position statement concludes: (1) the concept of candidiasis hypersensitivity is speculative and unproven; (2) its basic elements would apply to almost all sick patients at some time because its supposed symptoms are essentially universal; (3) overuse of oral antifungal agents could lead to the development of resistant germs that could menace others; (4) adverse effects of oral antifungal agents are rare, but some inevitably will occur; and (5) neither patients nor doctors can determine effectiveness (as opposed to coincidence) without controlled trials. Because allergic symptoms can be influenced by many factors, including emotions, experiments must be designed to separate the effects of the procedure being tested from the effects of other factors.\n\nBy 2005, scientists were taking note of \"a large pseudoscientific cult\" that had developed around the topic of yeast infections, with claims that up to one in three people were affected by yeast-related illnesses including Candida hypersensitivity. \n\nSome practitioners of alternative medicine have promoted dietary supplements as supposed cures for this non-existent illness, rendering themselves liable to prosecution. In 1990, alternative health vendor Nature's Way signed a FTC consent agreement not to misrepresent in advertising any self-diagnostic test concerning yeast conditions or to make any unsubstantiated representation concerning any food or supplement's ability to control yeast conditions, with a fine of US$30,000 payable to the National Institutes of Health for research in genuine candidiasis.\n\n"}
{"id": "12474403", "url": "https://en.wikipedia.org/wiki?curid=12474403", "title": "Climate change denial", "text": "Climate change denial\n\nClimate change denial, or global warming denial, is part of the global warming controversy. It involves denial, dismissal, or unwarranted doubt that contradicts the scientific opinion on climate change, including the extent to which it is caused by humans, its impacts on nature and human society, or the potential of adaptation to global warming by human actions. Some deniers endorse the term, while others prefer the term climate change skepticism. Several scientists have noted that \"skepticism\" is an inaccurate description for those who deny anthropogenic global warming. In effect, the two terms form a continuous, overlapping range of views, and generally have the same characteristics: both reject, to a greater or lesser extent, the scientific consensus on climate change. Climate change denial can also be implicit, when individuals or social groups accept the science but fail to come to terms with it or to translate their acceptance into action. Several social science studies have analyzed these positions as forms of denialism and pseudoscience.\n\nThe campaign to undermine public trust in climate science has been described as a \"denial machine\" organized by industrial, political and ideological interests, and supported by conservative media and skeptical bloggers to manufacture uncertainty about global warming. In the public debate, phrases such as \"climate skepticism\" have frequently been used with the same meaning as \"climate denialism\". The labels are contested: those actively challenging climate science commonly describe themselves as \"skeptics\", but many do not comply with common standards of scientific skepticism and, regardless of evidence, persistently deny the validity of human caused global warming.\n\nAlthough scientific opinion on climate change is that human activity is extremely likely to be the primary driver of climate change, the politics of global warming have been affected by climate change denial, hindering efforts to prevent climate change and adapt to the warming climate. Those promoting denial commonly use rhetorical tactics to give the appearance of a scientific controversy where there is none.\n\nOf the world's countries, the climate change denial industry is most powerful in the United States. From 2015 to 2017 (after having already served from 2003 to 2007), the United States Senate Committee on Environment and Public Works was chaired by oil lobbyist and climate change denier Jim Inhofe, who had previously called climate change \"the greatest hoax ever perpetrated against the American people\" and claimed to have debunked the alleged hoax in February 2015 when he brought a snowball with him in the Senate chamber and tossed it across the floor. He was succeeded in 2017 by John Barrasso, who similarly said: \"The climate is constantly changing. The role human activity plays is not known.\" Organised campaigning to undermine public trust in climate science is associated with conservative economic policies and backed by industrial interests opposed to the regulation of emissions. Climate change denial has been associated with the fossil fuels lobby, the Koch brothers, industry advocates and conservative think tanks, often in the United States. More than 90% of papers sceptical on climate change originate from right-wing think tanks.\nThe total annual income of these climate change counter-movement-organizations is roughly $900 million. Between 2002 and 2010, nearly $120 million (£77 million) was anonymously donated via the Donors Trust and Donors Capital Fund to more than 100 organisations seeking to undermine the public perception of the science on climate change. In 2013 the Center for Media and Democracy reported that the State Policy Network (SPN), an umbrella group of 64 U.S. think tanks, had been lobbying on behalf of major corporations and conservative donors to oppose climate change regulation.\n\nSince the late 1970s, oil companies have published research broadly in line with the standard views on global warming. Despite this, oil companies organized a climate change denial campaign to disseminate public disinformation for several decades, a strategy that has been compared to the organized denial of the hazards of tobacco smoking by the tobacco industry.\n\n\"Climate change skepticism\" and \"climate change denial\" refer to denial, dismissal or unwarranted doubt of the scientific consensus on the rate and extent of global warming, its significance, or its connection to human behavior, in whole or in part. Though there is a distinction between skepticism which indicates doubting the truth of an assertion and outright denial of the truth of an assertion, in the public debate phrases such as \"climate scepticism\" have frequently been used with the same meaning as climate denialism or contrarianism.\n\nThe terminology emerged in the 1990s. Even though all scientists adhere to scientific skepticism as an inherent part of the process, by mid November 1995 the word \"skeptic\" was being used specifically for the minority who publicised views contrary to the scientific consensus. This small group of scientists presented their views in public statements and the media, rather than to the scientific community. This usage continued. In his December 1995 article \"The Heat is On: The warming of the world's climate sparks a blaze of denial \", Ross Gelbspan said industry had engaged \"a small band of skeptics\" to confuse public opinion in a \"persistent and well-funded campaign of denial\". His 1997 book \"The Heat is On\" may have been the first to concentrate specifically on the topic. In it, Gelbspan discussed a \"pervasive denial of global warming\" in a \"persistent campaign of denial and suppression\" involving \"undisclosed funding of these 'greenhouse skeptics' \" with \"the climate skeptics\" confusing the public and influencing decision makers.\n\nA November 2006 CBC Television documentary on the campaign was titled \"The Denial Machine\". In 2007 journalist Sharon Begley reported on the \"denial machine\", a phrase subsequently used by academics.\n\nIn addition to \"explicit denial\", social groups have shown \"implicit denial\" by accepting the scientific consensus, but failing to come to terms with its implications or take action to reduce the problem. This was exemplified in Kari Norgaard's study of a village in Norway affected by climate change, where residents diverted their attention to other issues.\n\nThe terminology is debated: most of those actively rejecting the scientific consensus use the terms \"skeptic\" and \"climate change skepticism\", and only a few have expressed preference for being described as deniers, but the word \"skepticism\" is incorrectly used, as scientific skepticism is an intrinsic part of scientific methodology. The term \"contrarian\" is more specific, but used less frequently. In academic literature and journalism, the terms \"climate change denial\" and \"climate change deniers\" have well established usage as descriptive terms without any pejorative intent. Both the National Center for Science Education and historian Spencer R. Weart recognise that either option is problematic, but have decided to use \"climate change denial\" rather than \"skepticism\".\n\nTerms related to \"denialism\" have been criticised for introducing a moralistic tone, and potentially implying a link with Holocaust denial. There have been claims that this link is intentional, which academics have strongly disputed. The usage of \"denial\" long predates the Holocaust, and is commonly applied in other areas such as HIV/AIDS denialism: the claim is described by John Timmer of \"Ars Technica\" as itself being a form of denial.\n\nIn December 2014, an open letter from the Committee for Skeptical Inquiry called on the media to stop using the term \"skepticism\" when referring to climate change denial. They contrasted scientific skepticism—which is \"foundational to the scientific method\"—with denial—\"the a priori rejection of ideas without objective consideration\"—and the behavior of those involved in political attempts to undermine climate science. They said \"Not all individuals who call themselves climate change skeptics are deniers. But virtually all deniers have falsely branded themselves as skeptics. By perpetrating this misnomer, journalists have granted undeserved credibility to those who reject science and scientific inquiry.\" In June 2015 Media Matters for America were told by \"The New York Times\" Public Editor that the newspaper was increasingly tending to use \"denier\" when \"someone is challenging established science\", but assessing this on an individual basis with no fixed policy, and would not use the term when someone was \"kind of wishy-washy on the subject or in the middle.\" The executive director of the Society of Environmental Journalists said that while there was reasonable skepticism about specific issues, she felt that denier was \"the most accurate term when someone claims there is no such thing as global warming, or agrees that it exists but denies that it has any cause we could understand or any impact that could be measured.\"\n\nResearch on the effect of on the climate began in 1824, when Joseph Fourier inferred the existence the atmospheric \"greenhouse effect\". In 1860, John Tyndall quantified the effects of greenhouse gases on absorption of infrared radiation. Svante Arrhenius in 1896 showed that coal burning could cause global warming, and in 1938 Guy Stewart Callendar found it already happening to some extent. Research advanced rapidly after 1940; from 1957, Roger Revelle alerted the public to risks that fossil fuel burning was \"a grandiose scientific experiment\" on climate. NASA and NOAA took on research, the 1979 Charney Report concluded that substantial warming was already on the way, and \"A wait-and-see policy may mean waiting until it is too late.\" \n\nIn 1959 a scientist working for Shell suggested in a New Scientist article, that carbon cycles are too vast to upset Nature's balance.\n\nIn response to increasing public awareness of the greenhouse effect in the 1970s, conservative reaction built up, denying environmental concerns which could lead to government regulation. With the 1981 Presidency of Ronald Reagan, global warming became a political issue, with immediate plans to cut spending on environmental research, particularly climate related, and stop funding for monitoring. Reagan appointed as Secretary of Energy James B. Edwards, who said that there was no real global warming problem. Congressman Al Gore had studied under Revelle and was aware of the developing science: he joined others in arranging congressional hearings from 1981 onwards, with testimony by scientists including Revelle, Stephen Schneider and Wallace Smith Broecker. The hearings gained enough public attention to reduce the cuts in atmospheric research. A polarized party-political debate developed. In 1982 Sherwood B. Idso published his book \"Carbon Dioxide: Friend or Foe?\" which said increases in would not warm the planet, but would fertilize crops and were \"something to be encouraged and not suppressed\", while complaining that his theories had been rejected by the \"scientific establishment\". An Environmental Protection Agency (EPA) report in 1983 said global warming was \"not a theoretical problem but a threat whose effects will be felt within a few years\", with potentially \"catastrophic\" consequences. The Reagan administration reacted by calling the report \"alarmist\", and the dispute got wide news coverage. Public attention turned to other issues, then the 1985 finding of a polar ozone hole brought a swift international response. To the public, this was related to climate change and the possibility of effective action, but news interest faded.\n\nPublic attention was renewed amidst summer droughts and heat waves when James Hansen testified to a Congressional hearing on 23 June 1988, stating with high confidence that long term warming was under way with severe warming likely within the next 50 years, and warning of likely storms and floods. There was increasing media attention: the scientific community had reached a broad consensus that the climate was warming, human activity was very likely the primary cause, and there would be significant consequences if the warming trend was not curbed. These facts encouraged discussion about new laws concerning environmental regulation, which was opposed by the fossil fuel industry.\n\nFrom 1989 onwards industry funded organisations including the Global Climate Coalition and the George C. Marshall Institute sought to spread doubt among the public, in a strategy already developed by the tobacco industry. A small group of scientists opposed to the consensus on global warming became politically involved, and with support from conservative political interests, began publishing in books and the press rather than in scientific journals. This small group of scientists included some of the same people that were part of the strategy already tried by the tobacco industry. Spencer Weart identifies this period as the point where legitimate skepticism about basic aspects of climate science was no longer justified, and those spreading mistrust about these issues became deniers. As their arguments were increasingly refuted by the scientific community and new data, deniers turned to political arguments, making personal attacks on the reputation of scientists, and promoting ideas of a global warming conspiracy.\n\nWith the 1989 fall of communism and the environmental movement's international reach at the 1992 Rio Earth Summit, the attention of U.S. conservative think tanks, which had been organised in the 1970s as an intellectual counter-movement to socialism, turned from the \"red scare\" to the \"green scare\" which they saw as a threat to their aims of private property, free trade market economies and global capitalism. As a counter-movement, they used environmental skepticism to promote denial of the reality of problems such as loss of biodiversity and climate change.\n\nIn 1992, an EPA report linked second-hand smoke with lung cancer. The tobacco industry engaged the APCO Worldwide public relations company, which set out a strategy of astroturfing campaigns to cast doubt on the science by linking smoking anxieties with other issues, including global warming, in order to turn public opinion against calls for government intervention. The campaign depicted public concerns as \"unfounded fears\" supposedly based only on \"junk science\" in contrast to their \"sound science\", and operated through front groups, primarily the Advancement of Sound Science Center (TASSC) and its Junk Science website, run by Steven Milloy. A tobacco company memo commented \"Doubt is our product since it is the best means of competing with the 'body of fact' that exists in the mind of the general public. It is also the means of establishing a controversy.\" During the 1990s, the tobacco campaign died away, and TASSC began taking funding from oil companies including Exxon. Its website became central in distributing \"almost every kind of climate-change denial that has found its way into the popular press.\"\n\nIn the 1990s, the Marshall Institute began campaigning against increased regulations on environmental issues such as acid rain, ozone depletion, second-hand smoke, and the dangers of DDT. In each case their argument was that the science was too uncertain to justify any government intervention, a strategy it borrowed from earlier efforts to downplay the health effects of tobacco in the 1980s. This campaign would continue for the next two decades.\n\nThese efforts succeeded in influencing public perception of climate science. Between 1988 and the 1990s, public discourse shifted from the science and data of climate change to discussion of politics and surrounding controversy.\n\nThe campaign to spread doubt continued into the 1990s, including an advertising campaign funded by coal industry advocates intended to \"reposition global warming as theory rather than fact,\" and a 1998 proposal written by the American Petroleum Institute intending to recruit scientists to convince politicians, the media and the public that climate science was too uncertain to warrant environmental regulation. The proposal included a US$ 5,000,000 multi-point strategy to \"maximize the impact of scientific views consistent with ours on Congress, the media and other key audiences\", with a goal of \"raising questions about and undercutting the 'prevailing scientific wisdom'\".\n\nIn 1998, Gelbspan noted that his fellow journalists accepted that global warming was occurring, but said they were in \"'stage-two' denial of the climate crisis\", unable to accept the feasibility of answers to the problem. A subsequent book by Milburn and Conrad on \"The Politics of Denial\" described \"economic and psychological forces\" producing denial of the consensus on global warming issues.\n\nThese efforts by climate change denial groups were recognized as an organized campaign beginning in the 2000s. The sociologists Riley Dunlap and Aaron McCright played a significant role in this shift when they published an article in 2000 exploring the connection between conservative think tanks and climate change denial. Later work would continue the argument specific groups were marshalling skepticism against climate change - A study in 2008 from the University of Central Florida analyzed the sources of \"environmentally skeptical\" literature published in the United States. The analysis demonstrated that 92% of the literature was partly or wholly affiliated with a self-proclaimed conservative think tanks. A later piece of research from 2015 identified 4,556 individuals with overlapping network ties to 164 organizations which are responsible for the most efforts to downplay the threat of climate change in the U.S.\n\nGelbspan's \"Boiling Point\", published in 2004, detailed the fossil-fuel industry's campaign to deny climate change and undermine public confidence in climate science. In \"Newsweek\"'s August 2007 cover story \"The Truth About Denial\", Sharon Begley reported that \"the denial machine is running at full throttle\", and said that this \"well-coordinated, well-funded campaign\" by contrarian scientists, free-market think tanks, and industry had \"created a paralyzing fog of doubt around climate change.\"\n\nReferencing work of sociologists Robert Antonio and Robert Brulle, Wayne A. White has written that climate change denial has become the top priority in a broader agenda against environmental regulation being pursued by neoliberals. Today, climate change skepticism is most prominently seen in the United States, where the media disproportionately features views of the climate change denial community. In addition to the media, the contrarian movement has also been sustained by the growth of the internet, having gained some of its support from internet bloggers, talk radio hosts and newspaper columnists.\n\n\"The New York Times\" and others reported in 2015 that oil companies knew that burning oil and gas could cause climate change and global warming since the 1970s but nonetheless funded deniers for years. Dana Nuccitelli wrote in \"The Guardian\" that a small fringe group of climate deniers were no longer taken seriously at the 2015 United Nations Climate Change Conference, in an agreement that \"we need to stop delaying and start getting serious about preventing a climate crisis.\" However, \"The New York Times\" says any implementation is voluntary and will depend on any future world leaders—and every Republican candidate in 2016 has questioned or denied the science of climate change.\n\nA poll conducted by the New York Times Magazine in 2006 found that only 56% of Americans believe that average global temperatures have risen even though scientists think that they have. A majority of Americans also believe that scientists are still divided on the issue.\n\nA study was conducted to determine whether or not conservative white Americans were more likely to deny climate change. Researchers from Lyman Briggs College took samples of conservative white males of different understandings of global warming and categorized them separately. Categorized into three groups: conservative white males who claim to understand global warming, conservative white males who claim not to understand global warming, and other individuals. It was concluded that many of these conservative white males who self-report understanding global warming believe that the mass media has over exaggerated the effects of global warming and climate change and that the effects of it have never happened. 48.4% of conservative white males who report understanding global warming very well in the study said that the effects of global warming will never happen compared to 19% who do not understand it very well and 7.4% of all other adults. Many people appear to be confused by climate science however, the study showed that the people who self reported understanding global warming were most prone to denying.\n\nAnother study by Utah State University discusses geographic variation on climate change opinions at state and local scales in the United States. It was noted that prior research was found that public climate change policy support and behavior are significantly influenced by public beliefs, attitudes and risk perceptions. In addition, to complement previous stated factors, they're also influenced by knowledge, emotion, ideology, demographics, and personal experience. Where people live can account for some significance, since some people live in weather extreme areas, they could be desensitized to the overall change in climates as well. Surveys were conducted and showed that 63% of Americans believe that global warming is happening, 47% believe that it is human caused and 42% believe that most scientists think it is happening. In the study, a model was constructed to determine the public opinion of climate change across the nation. Representative telephone based surveys were used to investigate opinions in four states: California, Texas, Ohio and Colorado in addition to the Columbus and San Francisco metropolitan areas.\n\nWhile it had been the case as of 2010 that more than two-thirds of television weathercasters had denied that warming was real and largely caused by human activity, as of March 2018 the rate of acceptance among TV forecasters that the climate is changing has increased to ninety-five percent. The number of local television stories about global warming has also increased, by fifteen-fold. Climate Central has received some of the credit for this because they provide classes for meteorologists and graphics for television stations.\n\nThe Clexit Coalition claims to be: \"A new international organisation (which) aims to prevent ratification of the costly and dangerous Paris global warming treaty\". It has members in 26 countries. According to The Guardian newspaper: \"Clexit leaders are heavily involved in tobacco and fossil fuel-funded organizations\".\n\nSome climate change denial groups say that because is only a trace gas in the atmosphere (roughly 400ppm, or 0.04%) it can only have a minor effect on the climate. Scientists have known for over a century that even this small proportion has a significant warming effect, and doubling the proportion leads to a large temperature increase. The scientific consensus, as summarized by the IPCC's fourth assessment report, the U.S. Geological Survey, and other reports, is that human activity is the leading cause of climate change. The burning of fossil fuels accounts for around 30 billion tons of each year, which is 130 times the amount produced by volcanoes. Some groups allege that water vapor is a more significant greenhouse gas, and is left out of many climate models. While water vapor is a greenhouse gas, the very short atmospheric lifetime of water vapor (about 10 days) compared that of (hundreds of years) means that is the primary driver of increasing temperatures; water vapour acts as a feedback, not a forcing. Water vapor has been incorporated into climate models since their inception in the late 1800s. \n\nClimate denial groups may also argue that global warming stopped recently, a global warming hiatus, or that global temperatures are actually decreasing, leading to global cooling. These arguments are based on short term fluctuations, and ignore the long term pattern of warming.\n\nThese groups often point to natural variability, such as sunspots and cosmic rays, to explain the warming trend. According to these groups, there is natural variability that will abate over time, and human influences have little to do with it. These factors are already taken into account when developing climate models, and the scientific consensus is that they cannot explain the observed warming trend.\n\nAt a May 2018 meeting of the United States House Committee on Science, Space, and Technology, Alabama's Representative Mo Brooks claimed that sea level rise is caused not by melting glaciers but rather by coastal erosion and silt that flows from rivers into the ocean.\n\nClimate change denial literature often features the suggestion that we should wait for better technologies before addressing climate change, when they will be more affordable and effective.\n\nGlobal warming conspiracy theories have been posited which allege that the scientific consensus is illusory, or that climatologists are acting on their own financial interests by causing undue alarm about a changing climate. Despite leaked emails during the Climatic Research Unit email controversy, as well as multinational, independent research on the topic, no evidence of such a conspiracy has been presented, and strong consensus exists among scientists from a multitude of political, social, organizational and national backgrounds about the extent and cause of climate change. Several researchers have concluded that around 97% of climate scientists agree with this consensus. As well, much of the data used in climate science is publicly available to be viewed and interpreted by competing researchers as well as the public.\n\nIn 2012, research by Stephan Lewandowsky (then of the University of Western Australia) concluded that belief in other conspiracy theories, such as that the FBI was responsible for the assassination of Martin Luther King, Jr., was associated with being more likely to endorse climate change denial.\n\nIn 2004 Stefan Rahmstorf described how the media give the misleading impression that climate change was still disputed within the scientific community, attributing this impression to PR efforts of climate change skeptics. He identified different positions argued by climate skeptics, which he used as a taxonomy of climate change skepticism: Later the model was also applied on denial.\nThis taxonomy has been used in social science for analysis of publications, and to categorize climate change skepticism and climate change denial. Sometimes, a fourth category called \"consensus denial\" is added, which describes people who question the scientific consensus on anthropogenic global warming.\n\nThe National Center for Science Education describes climate change denial as disputing differing points in the scientific consensus, a sequential range of arguments from denying the occurrence of climate change, accepting that but denying any significant human contribution, accepting these but denying scientific findings on how this would affect nature and human society, to accepting all these but denying that humans can mitigate or reduce the problems. James L. Powell provides a more extended list, as does climatologist Michael E. Mann in \"six stages of denial\", a ladder in which deniers have over time conceded acceptance of points, while retreating to a position which still rejects the mainstream consensus:\n\nJournalists and newspaper columnists including George Monbiot and Ellen Goodman, among others, have described climate change denial as a form of denialism.\n\nDenialism in this context has been defined by Chris and Mark Hoofnagle as the use of rhetorical devices \"to give the appearance of legitimate debate where there is none, an approach that has the ultimate goal of rejecting a proposition on which a scientific consensus exists.\" This process characteristically uses one or more of the following tactics:\n\nIn 2015, environmentalist Bill McKibben accused President Obama (widely regarded as strongly in favour of action on climate change) of \"Catastrophic Climate-Change Denial\", for his approval of oil-drilling permits in offshore Alaska. According to McKibben, the President has also \"opened huge swaths of the Powder River basin to new coal mining.\" McKibben calls this \"climate denial of the status quo sort\", where the President denies \"the meaning of the science, which is that we must keep carbon in the ground.\" \n\nVarious groups, including the National Center for Science Education, have described climate change denial as a form of pseudoscience. Climate change skepticism, while in some cases professing to do research on climate change, has focused instead on influencing the opinion of the public, legislators and the media, in contrast to legitimate science.\n\nIn a review of the book \"The Pseudoscience Wars: Immanuel Velikovsky and the Birth of the Modern Fringe\" by Michael D. Gordin, David Morrison wrote:\n\nPublic opinion on climate change is significantly impacted by media coverage of climate change, and the effects of climate change denial campaigns. Campaigns to undermine public confidence in climate science have decreased public belief in climate change, which in turn have impacted legislative efforts to curb emissions.\n\nThe popular media in the U.S. gives greater attention to climate change skeptics than the scientific community as a whole, and the level of agreement within the scientific community has not been accurately communicated. In some cases, news outlets have allowed climate change skeptics to explain the science of climate change instead of experts in climatology. US and UK media coverage differ from that presented in other countries, where reporting is more consistent with the scientific literature. Some journalists attribute the difference to climate change denial being propagated, mainly in the US, by business-centered organizations employing tactics worked out previously by the US tobacco lobby. In France, the US and the UK, the opinions of climate change skeptics appear much more frequently in conservative news outlets than other news, and in many cases those opinions are left uncontested.\n\nThe efforts of Al Gore and other environmental campaigns have focused on the effects of global warming and have managed to increase awareness and concern, but despite these efforts, the number of Americans believing humans are the cause of global warming was holding steady at 61% in 2007, and those believing the popular media was understating the issue remained about 35%. A recent poll from 2015 suggests that while Americans are growing more aware of the dangers and implications of climate change for future generations, the majority are not worried about it. From a survey conducted in 2004, it was found that more than 30% of news presented in the previous decade showed equal attention to both human and non human contributions to global warming. \n\nA study assessed the public perception and actions to climate change, on grounds of belief systems, and identified seven psychological barriers affecting the behavior that otherwise would facilitate mitigation, adaptation, and environmental stewardship. The author found the following barriers: cognition, ideological world views, comparisons to key people, costs and momentum, discredence toward experts and authorities, perceived risks of change, and inadequate behavioral changes.\n\nIt has been suggested that climate change can conflict with a nationalistic view because it is \"unsolvable\" at the national level and requires collective action between nations or between local communities, and that therefore populist nationalism tends to reject the science of climate change.\n\nIn a TED talk Yuval Noah Harari notes: \n\nOn the other hand, it has been argued that effective climate action is polycentric rather than international, and national interest in multilateral groups can be furthered by overcoming climate change denial. Climate change contrarians may believe in a \"caricature\" of internationalist state intervention that is perceived as threatening national sovereignty, and may re-attribute risks such as flooding to international institutions. UK Independence Party policy on climate change has been influenced by noted contrarian Christopher Monckton and then by its energy spokesman Roger Helmer MEP who stated in a speech \"It is not clear that the rise in atmospheric CO is anthropogenic\".\n\nEfforts to lobby against environmental regulation have included campaigns to manufacture doubt about the science behind climate change, and to obscure the scientific consensus and data. These efforts have undermined public confidence in climate science, and impacted climate change lobbying.\n\nThe political advocacy organizations FreedomWorks and Americans for Prosperity, funded by brothers David and Charles Koch of Koch Industries, were important in supporting the Tea Party movement and in encouraging the movement to focus on climate change. Other conservative organizations such as the Heritage Foundation, Marshall Institute, Cato Institute and the American Enterprise Institute were significant participants in these lobbying attempts, seeking to halt or eliminate environmental regulations.\n\nThis approach to downplay the significance of climate change was copied from tobacco lobbyists; in the face of scientific evidence linking tobacco to lung cancer, to prevent or delay the introduction of regulation. Lobbyists attempted to discredit the scientific research by creating doubt and manipulating debate. They worked to discredit the scientists involved, to dispute their findings, and to create and maintain an apparent controversy by promoting claims that contradicted scientific research. \"\"Doubt is our product,\" boasted a now infamous 1969 industry memo. Doubt would shield the tobacco industry from litigation and regulation for decades to come.\" In 2006, George Monbiot wrote in \"The Guardian\" about similarities between the methods of groups funded by Exxon, and those of the tobacco giant Philip Morris, including direct attacks on peer-reviewed science, and attempts to create public controversy and doubt.\n\nFormer National Academy of Sciences president Frederick Seitz, who, according to an article by Mark Hertsgaard in \"Vanity Fair\", earned about US$585,000 in the 1970s and 1980s as a consultant to R.J. Reynolds Tobacco Company, went on to chair groups such as the Science and Environmental Policy Project and the George C. Marshall Institute alleged to have made efforts to \"downplay\" global warming. Seitz stated in the 1980s that \"Global warming is far more a matter of politics than of climate.\" Seitz authored the Oregon Petition, a document published jointly by the Marshall Institute and Oregon Institute of Science and Medicine in opposition to the Kyoto protocol. The petition and accompanying \"Research Review of Global Warming Evidence\" claimed:\nThe proposed limits on greenhouse gases would harm the environment, hinder the advance of science and technology, and damage the health and welfare of mankind. There is no convincing scientific evidence that human release of carbon dioxide, methane, or other greenhouse gases is causing or will, in the foreseeable future, cause catastrophic heating of the Earth's atmosphere and disruption of the Earth's climate. … We are living in an increasingly lush environment of plants and animals as a result of the carbon dioxide increase. Our children will enjoy an Earth with far more plant and animal life than that with which we now are blessed. This is a wonderful and unexpected gift from the Industrial Revolution.\nGeorge Monbiot wrote in \"The Guardian\" that this petition, which he criticizes as misleading and tied to industry funding, \"has been cited by almost every journalist who claims that climate change is a myth.\" Efforts by climate change denial groups played a significant role in the eventual rejection of the Kyoto protocol in the US.\n\nMonbiot has written about another group founded by the tobacco lobby, The Advancement of Sound Science Coalition (TASSC), that now campaigns against measures to combat global warming. In again trying to manufacture the appearance of a grass-roots movement against \"unfounded fear\" and \"over-regulation,\" Monbiot states that TASSC \"has done more damage to the campaign to halt [climate change] than any other body.\"\n\nDrexel University environmental sociologist Robert Brulle analysed the funding of 91 organizations opposed to restrictions on carbon emissions, which he termed the \"climate change counter-movement.\" Between 2003 and 2013, the donor-advised funds Donors Trust and Donors Capital Fund, combined, were the largest funders, accounting for about one quarter of the total funds, and the American Enterprise Institute was the largest recipient, 16% of the total funds. The study also found that the amount of money donated to these organizations by means of foundations whose funding sources cannot be traced had risen.\n\nSeveral large corporations within the fossil fuel industry provide significant funding for attempts to mislead the public about the trustworthiness of climate science. ExxonMobil and the Koch family foundations have been identified as especially influential funders of climate change contrarianism.\n\nAfter the IPCC released its February 2007 report, the American Enterprise Institute offered British, American and other scientists $10,000 plus travel expenses to publish articles critical of the assessment. The institute had received more than US$1.6 million from Exxon, and its vice-chairman of trustees was former head of Exxon Lee Raymond. Raymond sent letters that alleged the IPCC report was not \"supported by the analytical work.\" More than 20 AEI employees worked as consultants to the George W. Bush administration. Despite her initial conviction that climate change denial would abate with time, Senator Barbara Boxer said that when she learned of the AEI's offer, she \"realized there was a movement behind this that just wasn't giving up.\"\n\nThe Royal Society conducted a survey that found ExxonMobil had given US$2.9 million to American groups that \"misinformed the public about climate change,\" 39 of which \"misrepresented the science of climate change by outright denial of the evidence\". In 2006, the Royal Society issued a demand that ExxonMobil withdraw funding for climate change denial. The letter drew criticism, notably from Timothy Ball who argued the society attempted to \"politicize the private funding of science and to censor scientific debate.\"\n\nExxonMobil denied that it has been trying to mislead the public about global warming. A spokesman, Gantt Walton, said that ExxonMobil's funding of research does not mean that it acts to influence the research, and that ExxonMobil supports taking action to curb the output of greenhouse gasses. Research conducted at an Exxon archival collection at the University of Texas and interviews with former employees by journalists indicate the scientific opinion within the company and their public posture towards climate change was contradictory.\n\nBetween 1989 and 2002 the Global Climate Coalition, a group of mainly United States businesses, used aggressive lobbying and public relations tactics to oppose action to reduce greenhouse gas emissions and fight the Kyoto Protocol. The coalition was financed by large corporations and trade groups from the oil, coal and auto industries. \"The New York Times\" reported that \"even as the coalition worked to sway opinion [towards skepticism], its own scientific and technical experts were advising that the science backing the role of greenhouse gases in global warming could not be refuted.\" In 2000, Ford Motor Company was the first company to leave the coalition as a result of pressure from environmentalists, followed by Daimler-Chrysler, Texaco, the Southern Company and General Motors subsequently left to GCC. The organization closed in 2002.\n\nFrom January 2009 through June 2010, the oil, coal and utility industries spent $500 million in lobby expenditures in opposition to legislation to address climate change.\n\nIn early 2015, several media reports emerged saying that Willie Soon, a popular scientist among climate change deniers, had failed to disclose conflicts of interest in at least 11 scientific papers published since 2008. They reported that he received a total of $1.25m from ExxonMobil, Southern Company, the American Petroleum Institute and a foundation run by the Koch brothers. Charles R. Alcock, director of the Harvard–Smithsonian Center for Astrophysics, where Soon was based, said that allowing funders of Soon's work to prohibit disclosure of funding sources was a mistake, which will not be permitted in future grant agreements.\n\nThe Republican Party in the United States is unique in denying anthropogenic climate change among conservative political parties across the Western world. In 1994, according to a leaked memo, the Republican strategist Frank Luntz advised members of the Republican Party, with regard to climate change, that \"you need to continue to make the lack of scientific certainty a primary issue\" and \"challenge the science\" by \"recruiting experts who are sympathetic to your view.\" (In 2006, Luntz said he still believes \"back [in] '97, '98, the science was uncertain\", but he now agrees with the scientific consensus.) From 2008 to 2017, the Republican Party went from \"debating how to combat human-caused climate change to arguing that it does not exist,\" according to \"The New York Times\". In 2011 \"more than half of the Republicans in the House and three-quarters of Republican senators\" said \"that the threat of global warming, as a man-made and highly threatening phenomenon, is at best an exaggeration and at worst an utter \"hoax\"\" according to Judith Warner writing in \"The New York Times Magazine\".\nIn 2014, more than 55% of congressional Republicans were climate change deniers, according to NBC News.\nAccording to PolitiFact in May 2014, Jerry Brown's statement that 'virtually no Republican' in Washington accepts climate change science, was \"mostly true\"; PolitiFact counted \"eight out of 278, or about 3 percent\" of Republican members of Congress who \"accept the prevailing scientific conclusion that global warming is both real and man-made.\"\n\nIn 2005, \"The New York Times\" reported that Philip Cooney, former fossil fuel lobbyist and \"climate team leader\" at the American Petroleum Institute and President George W. Bush's chief of staff of the Council on Environmental Quality, had \"repeatedly edited government climate reports in ways that play down links between such emissions and global warming, according to internal documents.\" Sharon Begley reported in \"Newsweek\" that Cooney \"edited a 2002 report on climate science by sprinkling it with phrases such as 'lack of understanding' and 'considerable uncertainty.'\" Cooney reportedly removed an entire section on climate in one report, whereupon another lobbyist sent him a fax saying \"You are doing a great job.\" Cooney announced his resignation two days after the story of his tampering with scientific reports broke, but a few days later it was announced that Cooney would take up a position with ExxonMobil.\n\nUnited States Secretary of Energy Rick Perry, in a 19 June 2017 interview with CNBC, acknowledged the existence of climate change and impact from humans, but said that he did not agree with the idea that carbon dioxide was the primary driver of global warming pointing instead to \"the ocean waters and this environment that we live in\". The American Meteorological Society responded in a letter to Perry saying that it is \"critically important that you understand that emissions of carbon dioxide and other greenhouse gases are the primary cause\", pointing to conclusions of scientists worldwide.\n\nRepublican Jim Bridenstine, the first elected politician to serve as NASA administrator, had previously stated that global temperatures were not rising. A month after the Senate confirmed his NASA position in April 2018, he acknowledged that human emissions of greenhouse gases are raising global temperatures.\n\nAccording to documents leaked in February 2012, The Heartland Institute is developing a curriculum for use in schools which frames climate change as a scientific controversy. In 2017, Glenn Branch, Deputy Director of the National Center for Science Education (NCSE), wrote that \"the Heartland Institute is continuing to inflict its climate change denial literature on science teachers across the country\". He also described how some science teachers were reacting to Heartland's mailings: \"Fortunately, the Heartland mailing continues to be greeted with skepticism and dismissed with scorn.\"<ref name=\"NCSE_Branch_6/5/2017\"></ref> The NCSE has prepared Classroom Resources in response to Heartland and other anti-science threats.\n\nBranch also referred to an article by ClimateFeedback.org which reviewed an unsolicited Heartland booklet, entitled \"Why Scientists Disagree about Global Warming\", which was sent to science teachers in the United States. Their intention was to send it to \"more than 200,000 K-12 teachers\". Each significant claim was rated for accuracy by scientists who were experts on that topic. Overall, they scored the accuracy of the booklet with an \"F\": \"it could hardly score lower\", and \"the \"Key Findings\" section are incorrect, misleading, based on flawed logic, or simply factually inaccurate.\"\n\nManufactured uncertainty over climate change, the fundamental strategy of climate change denial, has been very effective, particularly in the US. It has contributed to low levels of public concern and to government inaction worldwide. An Angus Reid poll released in 2010 indicates that global warming skepticism in the United States, Canada, and the United Kingdom has been rising. There may be multiple causes of this trend, including a focus on economic rather than environmental issues, and a negative perception of the United Nations and its role in discussing climate change.\n\nAnother cause may be weariness from overexposure to the topic: secondary polls suggest that the public may have been discouraged by extremism when discussing the topic, while other polls show 54% of U.S. voters believe that \"the news media make global warming appear worse than it really is.\" A poll in 2009 regarding the issue of whether \"some scientists have falsified research data to support their own theories and beliefs about global warming\" showed that 59% of Americans believed it \"at least somewhat likely\", with 35% believing it was \"very likely\".\n\nAccording to Tim Wirth, \"They patterned what they did after the tobacco industry. […] Both figured, sow enough doubt, call the science uncertain and in dispute. That's had a huge impact on both the public and Congress.\" This approach has been propagated by the US media, presenting a false balance between climate science and climate skeptics. \"Newsweek\" reports that the majority of Europe and Japan accept the consensus on scientific climate change, but only one third of Americans considered human activity to play a major role in climate change in 2006; 64% believed that scientists disagreed about it \"a lot.\" A 2007 \"Newsweek\" poll found these numbers were declining, although majorities of Americans still believed that scientists were uncertain about climate change and its causes. Rush Holt wrote a piece for \"Science\", which appeared in \"Newsweek\":\n\nDeliberate attempts by the Western Fuels Association \"to confuse the public\" have succeeded in their objectives. This has been \"exacerbated by media treatment of the climate issue\". According to a Pew poll in 2012, 57% of the US public are unaware of, or outright reject, the scientific consensus on climate change. Some organizations promoting climate change denial have asserted that scientists are increasingly rejecting climate change, but this notion is contradicted by research showing that 97% of published papers endorse the scientific consensus, and that percentage is increasing with time.\n\nSocial psychologist Craig Foster compares climate change denialists to flat-earth believers and the reaction to the latter by the scientific community. Foster states, \"the potential and kinetic energy devoted to counter the flat-earth movement is wasteful and misguided... I don't understand why anybody would worry about the flat-earth gnat while facing the climate change mammoth... Climate change denial does not require belief. It only requires neglect.\"\n\nIn 2016, Aaron McCright argued that anti-environmentalism—and climate change denial specifically—has expanded to a point in the US where it has now become \"a central tenet of the current conservative and Republican identity.\"\n\nOn the other hand, global oil companies have begun to acknowledge the existence of climate change and its risks.\n\nManufactured climate change denial is also influencing how scientific knowledge is communicated to the public. According to climate scientist Michael E. Mann, \"...universities and scientific societies and organizations, publishers, etc.—are too often risk averse when it comes to defending and communicating science that is perceived as threatening by powerful interests...\"\n\n\n\n"}
{"id": "24206741", "url": "https://en.wikipedia.org/wiki?curid=24206741", "title": "Codes for electromagnetic scattering by spheres", "text": "Codes for electromagnetic scattering by spheres\n\nCodes for electromagnetic scattering by spheres - this article list codes for electromagnetic scattering by a homogeneous sphere, layered sphere, and cluster of spheres. \n\nMajority of existing codes for calculation of electromagnetic scattering by a single sphere is based on Mie theory which is an analytical solution of Maxwell's equations in terms of infinite series. Other approximations to scattering by a single sphere include: Debye series, ray tracing (geometrical optics), ray tracing including the effects of interference between rays, Airy theory, Rayleigh scattering, diffraction approximation. There are many phenomena related to light scattering by spherical particles such as resonances, surface waves, plasmons, near-field scattering. Even though Mie theory offers convenient and fast way of solving light scattering problem by homogeneous spherical particles, there are other techniques, such as discrete dipole approximation, FDTD, T-matrix, which can also be used for such tasks.\nThe compilation contains information about the electromagnetic scattering by spherical particles, relevant links, and applications.\n\nAlgorithmic literature includes several contributions\n\n\n\n"}
{"id": "6956", "url": "https://en.wikipedia.org/wiki?curid=6956", "title": "Conservation law", "text": "Conservation law\n\nIn physics, a conservation law states that a particular measurable property of an isolated physical system does not change as the system evolves over time. Exact conservation laws include conservation of energy, conservation of linear momentum, conservation of angular momentum, and conservation of electric charge. There are also many approximate conservation laws, which apply to such quantities as mass, parity, lepton number, baryon number, strangeness, hypercharge, etc. These quantities are conserved in certain classes of physics processes, but not in all.\n\nA local conservation law is usually expressed mathematically as a continuity equation, a partial differential equation which gives a relation between the amount of the quantity and the \"transport\" of that quantity. It states that the amount of the conserved quantity at a point or within a volume can only change by the amount of the quantity which flows in or out of the volume.\n\nFrom Noether's theorem, each conservation law is associated with a symmetry in the underlying physics.\n\nConservation laws are fundamental to our understanding of the physical world, in that they describe which processes can or cannot occur in nature. For example, the conservation law of energy states that the total quantity of energy in an isolated system does not change, though it may change form. In general, the total quantity of the property governed by that law remains unchanged during physical processes. With respect to classical physics, conservation laws include conservation of energy, mass (or matter), linear momentum, angular momentum, and electric charge. With respect to particle physics, particles cannot be created or destroyed except in pairs, where one is ordinary and the other is an antiparticle. With respect to symmetries and invariance principles, three special conservation laws have been described, associated with inversion or reversal of space, time, and charge.\n\nConservation laws are considered to be fundamental laws of nature, with broad application in physics, as well as in other fields such as chemistry, biology, geology, and engineering.\n\nMost conservation laws are exact, or absolute, in the sense that they apply to all possible processes. Some conservation laws are partial, in that they hold for some processes but not for others.\n\nOne particularly important result concerning conservation laws is Noether's theorem, which states that there is a one-to-one correspondence between each one of them and a differentiable symmetry of nature. For example, the conservation of energy follows from the time-invariance of physical systems, and the conservation of angular momentum arises from the fact that physical systems behave the same regardless of how they are oriented in space.\n\nA partial listing of physical conservation equations due to symmetry that are said to be exact laws, or more precisely \"have never been proven to be violated:\"\n\nThere are also approximate conservation laws. These are approximately true in particular situations, such as low speeds, short time scales, or certain interactions.\n\n\nThe total amount of some conserved quantity in the universe could remain unchanged if an equal amount were to appear at one point \"A\" and simultaneously disappear from another separate point \"B\". For example, an amount of energy could appear on Earth without changing the total amount in the Universe if the same amount of energy were to disappear from a remote region of the Universe. This weak form of \"global\" conservation is really not a conservation law because it is not Lorentz invariant, so phenomena like the above do not occur in nature. Due to Special Relativity, if the appearance of the energy at \"A\" and disappearance of the energy at \"B\" are simultaneous in one inertial reference frame, they will not be simultaneous in other inertial reference frames moving with respect to the first. In a moving frame one will occur before the other; either the energy at \"A\" will appear \"before\" or \"after\" the energy at \"B\" disappears. In both cases, during the interval energy will not be conserved. \n\nA stronger form of conservation law requires that, for the amount of a conserved quantity at a point to change, there must be a flow, or \"flux\" of the quantity into or out of the point. For example, the amount of electric charge in a volume is never found to change without an electric current into or out of the volume that carries the difference in charge. Since it only involves continuous \"local\" changes, this stronger type of conservation law is Lorentz invariant; a quantity conserved in one reference frame is conserved in all moving reference frames. This is called a \"local conservation\" law. Local conservation also implies global conservation; that the total amount of the conserved quantity in the Universe remains constant. All of the conservation laws listed above are local conservation laws. A local conservation law is expressed mathematically by a \"continuity equation\", which states that the change in the quantity in a volume is equal to the total net \"flux\" of the quantity through the surface of the volume. The following sections discuss continuity equations in general.\n\nIn continuum mechanics, the most general form of an exact conservation law is given by a continuity equation. For example, conservation of electric charge \"q\" is\n\nwhere ∇⋅ is the divergence operator, \"ρ\" is the density of \"q\" (amount per unit volume), j is the flux of \"q\" (amount crossing a unit area in unit time), and \"t\" is time.\n\nIf we assume that the motion u of the charge is a continuous function of position and time, then\n\nIn one space dimension this can be put into the form of a homogeneous first-order quasilinear hyperbolic equation:\n\nwhere the dependent variable \"y\" is called the \"density\" of a \"conserved quantity\", and \"A(y)\" is called the \"current jacobian\", and the subscript notation for partial derivatives has been employed. The more general inhomogeneous case:\n\nis not a conservation equation but the general kind of balance equation describing a dissipative system. The dependent variable \"y\" is called a \"nonconserved quantity\", and the inhomogeneous term \"s(y,x,t)\" is the-\"source\", or dissipation. For example, balance equations of this kind are the momentum and energy Navier-Stokes equations, or the entropy balance for a general isolated system.\n\nIn the one-dimensional space a conservation equation is a first-order quasilinear hyperbolic equation that can be put into the \"advection\" form:\n\nwhere the dependent variable \"y(x,t)\" is called the density of the \"conserved\" (scalar) quantity (c.q.(d.) = conserved quantity (density)), and \"a(y)\" is called the current coefficient, usually corresponding to the partial derivative in the conserved quantity of a current density (c.d.) of the conserved quantity \"j(y)\":\n\nIn this case since the chain rule applies:\n\nthe conservation equation can be put into the current density form:\n\nIn a space with more than one dimension the former definition can be extended to an equation that can be put into the form:\n\nwhere the \"conserved quantity\" is \"y(r,t)\", \"formula_11\" denotes the scalar product, \"∇\" is the nabla operator, here indicating a gradient, and \"a(y)\" is a vector of current coefficients, analogously corresponding to the divergence of a vector c.d. associated to the c.q. j(y):\n\nThis is the case for the continuity equation:\n\nHere the conserved quantity is the mass, with density \"ρ\"(r,t) and current density \"ρ\"u, identical to the momentum density, while u(r,t) is the flow velocity.\n\nIn the general case a conservation equation can be also a system of this kind of equations (a vector equation) in the form:\n\nwhere y is called the \"conserved\" (vector) quantity, ∇ y is its gradient, 0 is the zero vector, and A(y) is called the Jacobian of the current density. In fact as in the former scalar case, also in the vector case A(y) usually corresponding to the Jacobian of a current density matrix J(y):\n\nand the conservation equation can be put into the form:\n\nFor example, this the case for Euler equations (fluid dynamics). In the simple incompressible case they are:\n\nwhere:\n\nIt can be shown that the conserved (vector) quantity and the c.d. matrix for these equations are respectively:\n\nwhere \"formula_19\" denotes the outer product.\n\nConservation equations can be also expressed in integral form: the advantage of the latter is substantially that it requires less smoothness of the solution, which paves the way to weak form, extending the class of admissible solutions to include discontinuous solutions. By integrating in any space-time domain the current density form in 1-D space:\n\nand by using Green's theorem, the integral form is:\n\nIn a similar fashion, for the scalar multidimensional space, the integral form is:\n\nwhere the line integration is performed along the boundary of the domain, in an anticlock-wise manner.\n\nMoreover, by defining a test function \"φ\"(r,\"t\") continuously differentiable both in time and space with compact support, the weak form can be obtained pivoting on the initial condition. In 1-D space it is:\n\nNote that in the weak form all the partial derivatives of the density and current density have been passed on to the test function, which with the former hypothesis is sufficiently smooth to admit these derivatives.\n\n\n\n\n"}
{"id": "38607104", "url": "https://en.wikipedia.org/wiki?curid=38607104", "title": "Constructivism in science education", "text": "Constructivism in science education\n\nConstructivism has been considered as a dominant paradigm, or research programme, in the field of science education. The term constructivism is widely used in many fields, and not always with quite the same intention. This entry offers an account of how constructivism is most commonly understood in science education.\n\nScience Education is now an established field within Education, and worldwide has its own journals, conferences, university departments and so forth. Although a diverse field, a major influence on its development was research considered to be undertaken from a constructivist perspective on learning, and supporting approaches to teaching that themselves became labelled constructivist. Thus, this constructivism was largely of a psychological flavour, often drawing on the work of Jean Piaget, David Ausubel, Robert M. Gagné and Jerome Bruner. One influential group of science education researchers were also heavily influenced by George Kelly (psychologist)'s Personal Construct Theory. The work of Lev Vygotsky (since being championed in the West by Jerome Bruner) has also been increasingly influential.\n\nThese workers from psychology informed the first generation of science education researchers. Active research groups developed in centres like the University of Waikato (Aotearoa/New Zealand), University of Leeds (UK) and University of Surrey (UK), with a strong interest in students' ideas in science (formed before, or during instruction) as these were recognised as being highly influential on future learning, and so whether canonical scientific would be learnt. This work, sometimes labelled the 'alternative conceptions movement' was motivated by a series of influential publications on children's ideas in science and their implications for learning (and so for how teaching should be planned to take them into account). Whilst a range of influential papers could be cited it has been suggested that a number of seminar contributions in effect set out the commitments, or 'hard core' of a constructivist research programme into the learning and teaching of science. The perspective was also the focus of a number of books aimed at the science education community - researchers and teachers.\n\nThese papers presented learning as process of personal sense making, and an iterative matter such that what is learnt was channelled by existing knowledge and understanding (whether canonical or alternative), and teaching as needing to take learners' existing ideas into account in teaching. The research programme soon amounted to thousands of studies on aspects of students' (of different ages and educational levels, from different countries) thinking and learning in science topics.\n\nThere have been a wide range of criticisms of constructivist work in science, including strong criticism from philosophical perspectives. Such criticisms have done little to stem the influence of the perspective, perhaps because they tend not to refer to the core tenets of constructivism as an approach based on learning theory and research from cognitive science.\n\nLearners' ideas in science have been variously labelled as alternative conceptions, alternative conceptual frameworks, preconceptions, scientific misconceptions, naive theories etc. Although some scholars have attempted to distinguish between these terms, there is no consensual usage and often these terms are in effect synonymous. It has been found that some alternative conceptions are very common, although others appear quite idiosyncratic. Some seem to be readily overcome in teaching, but others have proved to be tenacious and to offer a challenge to effective instruction. Sometimes it is considered important to distinguish fully developed conceptions (i.e., explicit ways of understanding aspects of the natural work that are readily verbalised) from more 'primitive' features of cognition acting at a tacit level, such as the so-called phenomenology primitives. The 'knowledge-in-pieces' perspective suggests the latter act as resources for new learning which have potential to support the development of either alternative or canonical knowledge according to how teachers proceed, whereas alternative conceptions (or misconceptions) tend to be seen as learning impediments to be overcome. What research has shown is the prevalence among learners at all levels of alternative ways to thinking about just about all science topics, and a key feature of guidance to teachers is to elicit students' ideas as part of the teaching process. The success of constructivism is that this is now largely taken-for-granted in science teaching and has become part of standard teaching guidance in many contexts. Previously there was a strong focus on the abstract nature of concepts to be learnt, but little awareness that often the teacher was not seeking to replace ignorance with knowledge, but rather to modify and develop learners existing thinking which was often at odds with the target knowledge set out in the curriculum.\n\nConstructivism is seen as an educational theory, and a key perspective to inform pedagogy. There are many books informing teachers and others about constructivist research findings and ideas, and giving guidance on how to teach science form a constructivist perspective.\n\n"}
{"id": "8085847", "url": "https://en.wikipedia.org/wiki?curid=8085847", "title": "Current research information system", "text": "Current research information system\n\nA current research information system (CRIS) is a database or other information system to store and manage data about research conducted at an institution. \n\nThere is an increasing awareness of the need for quality research management (information) systems:\n\n\nA standard for current research information system is the CERIF (Common European Research Information Format) standard, proposed by the EU and developed and maintained by euroCRIS.\n\nOpen source and commercial CRIS solutions (including handling of contracts, projects, publications, study plans and patents) are available.\n\n"}
{"id": "3961885", "url": "https://en.wikipedia.org/wiki?curid=3961885", "title": "Daffy's Elixir", "text": "Daffy's Elixir\n\nDaffy's Elixir (also sometimes known as Daffey's Elixir or Daffye's Elixir) is a name that has been used by several patent medicines over the years. It was originally designed for diseases of the stomach, but was later marketed as a universal cure. It remained a popular remedy in Britain and later the United States of America throughout the eighteenth and nineteenth centuries.\n\nDaffy's Elixir was one of the most popular and frequently advertised patent medicines in Britain during the 18th century. It is reputed to have been invented by clergyman Thomas Daffy rector of Redmile, Leicestershire, in 1647. He named it elixir salutis (lit. \"elixir of health\") and promoted as a generic cure-all.\n\nAn early recipe for \"True Daffy\" from 1700 lists the following ingredients: aniseed, brandy, cochineal, elecampane, fennel seed, jalap, manna, parsley seed, raisin, rhubarb, saffron, senna and spanish liquorice. Chemical analysis has shown this to be a laxative made mostly from alcohol. Other recipes include Guiuacum wood chips, caraway, Salt of Tartar, and scammony.\n\nAccording to an early nineteenth century advertisement it was used for the following ailments: The Stone in Babies and Children; Convulsion fits; Consumption and Bad Digestives; Agues; Piles; Surfeits; Fits of the Mother and Vapours from the Spleen; Green Sickness; Children's Distempers, whether the Worms, Rickets, Stones, Convulsions, Gripes, King's Evil, Joint Evil or any other disorder proceeding from Wind or Crudities; Gout and Rheumatism; Stone or Gravel in the Kidnies; Cholic and Griping of the Bowels; the Phthisic (both as cure and preventative provided always that the patient be moderate in drinking, have a care to prevent taking cold and keep a good diet; Dropsy and Scurvy. The frequent use of the medicine to treat Colic, gripes or fret in horses was deplored in early veterinary manuals.\n\nAfter Daffy's death in 1680 the recipe was left to his daughter Catherine, and his kinsmen Anthony and Daniel who were apothecaries in Nottingham. Anthony Daffy moved to London in the 1690s and began to exploit the product issuing pamphlets such as \"Directions for taking elixir salutis or, the famous purging cordial, known by the name of Daffy's elixir salutis\" [London], [1690?]. His widow Elleanor Daffy continued from about 1693 and (their daughter?) Katharine from about 1707. During the early 18th century the product was advertised widely in the emerging national and local newspapers. The success attracted several counterfeit copies, using inferior alcohol rather than brandy.\n\nThe medicine was later produced by William and Cluer Dicey & Co. of Bow Church yard c.1775 who claimed the sole rights of manufacture of the True Daffy's Elixir, although the recipe was not subject to any patent. Proprietorship was also then claimed by Peter Swinton of Salisbury Court and his son Anthony Daffy Swinton who may have been descended from the inventor. Dicey and Co. and their successors marketed it in the United States of America.\n\nIt then passed to Dicey and Sutton, and later to Messrs W. Sutton & Co. of Enfield Middlesex who continuing to market it throughout the nineteenth century. The use of Daffy's elixir is referred to in Anthony Trollope's novel Barchester Towers, 1857.\n\nDaffy's elixir is also mentioned on several occasions in Thomas Pynchon's novel Mason & Dixon, particularly by Jeremiah Dixon, who attempts to procure large quantities before beginning his surveying trip with Charles Mason. Dixon is warned by Benjamin Franklin, however, that imported Daffy's Elixir is extremely expensive, and he would be better off ordering a customized version from the apothecary. During the same visit, Dixon also orders laudanum, a well-known constipating agent.\n\nDaffy's elixir is also mentioned in the Charles Dickens book, Oliver Twist, Ch. II, where it is referred to as Daffy, in the sentence: 'Why, it's what I'm obliged to keep a little of in the house, to put into the blessed infants' Daffy, when they ain't well, Mr. Bumble,(the Parish Beadle)' replied Mrs. Mann as she opened a corner cupboard, and took down a bottle and glass. 'It's gin. I'll not deceive you, Mr. B. It's gin.'\n\nDaffy’s Elixir is also mentioned in the Works of William Makepeace Thackeray book, Vanity Fair, Chapter XXXVIII A Family In a Small Way, where it is referenced in the sentence ‘..and there found Mrs. Sedley in the act of surreptitiously administering Daffy’s Elixir to the infant.’\n\n\"Daffy’s original elixir salutis, vindicated against all counterfeits, &c. or, An advertisement by mee, Anthony Daffy, of London, citizen and student in physick, By way of vindication of my famous and generally approved cordial drink, (called elixir salutis) from the notoriously false suggestions of one Tho. Witherden of Bear-steed in the county of Kent, Gent. (as pretended;) Jane White, Robert Brooke, apothecary, and Edward Willet; all new upstatrt counterfitors of my elixir, and Ape-like imitators of my long since printed Books and Directions, (some of them, nigh verbatim, or word for word) and that to the jeopardy of many good, (but mis-in-formed) Peoples Healths, and Lives too; as also, from the false pretentions of other more sneaking Cub-Quacks, not yet lickt into form, but remaining Moon-blind brats, (still in swadling-clouts) I mean the numerous crew of libellous pamphleteeirs, which are (if possible) more dangerous counterfeiters of my Elixer\" . . . Advertisement by mee, Anthony Daffy s.n., 1690?].\n\n\"Daffy’s original and famous elixir salutis: the choice drink of health: or, health-bringing drink. Being a famous cordial drink, found out by the providence of the Almighty, and (for above twenty years) experienced by himself, and divers persons (whose names are at most of their desires here inserted) a most excellent preservative of man-kind. A secret far beyond any medicament yet known, and is found so agreeable to nature, that it effects all its operations, as nature would have it, and as a virtual expedient proposed by her, for reducing all her extreams unto an equal temper; the same being fitted unto all ages, sexes, complexions, and constitutions, and highly fortifying nature against any noxious humour, invading or offending the noble parts. Never published by any but by Anthony Daffy, student in physick, and since continued by his widow Elleanor Daffy\", London : printed with allowance, for the author, by Tho. Milbourn dwelling in Jewen-Street, 1693.\n\n\n"}
{"id": "22168509", "url": "https://en.wikipedia.org/wiki?curid=22168509", "title": "Does God Play Dice?", "text": "Does God Play Dice?\n\nDoes God Play Dice: The New Mathematics of Chaos is a non-fiction book about chaos theory written by British mathematician Ian Stewart. The book was initially published by Blackwell Publishing in 1989.\n\nIn this book, Stewart explains chaos theory to an audience presumably unfamiliar with it. As the book progresses the writing changes from simple explanations of chaos theory to in-depth, rigorous mathematical study. Stewart covers mathematical concepts such as differential equations, resonance, nonlinear dynamics, and probability. The book is illustrated with diagrams and graphs of mathematical concepts and equations when applicable.\n\nThe back of the book, and a summary of its content, reads, \"The science of chaos is forcing scientists to rethink Einstein's fundamental assumptions regarding the way the universe behaves. Chaos theory has already shown that simple systems, obeying precise laws, can nevertheless act in a random manner. Perhaps God plays dice within a cosmic game of complete law and order. Does God Play Dice? reveals a strange universe in which nothing may be as it seems. Familiar geometric shapes such as circles and ellipses give way to infinitely complex structures known as fractals, the fluttering of a butterfly's wings can change the weather, and the gravitational attraction of a creature in a distant galaxy can change the fate of the solar system.\"\n\nThe title of the book is a reference to a famous quote by Albert Einstein.\n\n\n"}
{"id": "51452660", "url": "https://en.wikipedia.org/wiki?curid=51452660", "title": "Doktor Koster's Antigaspills", "text": "Doktor Koster's Antigaspills\n\nDoktor Koster's Antigaspills were an early 20th century alternative medication intended to treat stomach upset and excessive flatulence. They are best known for being administered to Adolf Hitler by his physician, Theodor Morell, to treat Hitler's stomach ailments. Morrell, regarded as a quack by Hitler's associates, administered a wide variety of unorthodox concoctions and medications to Hitler beginning in 1936.\n\nThe pills active ingredients consisted primarily of atropine (an extract of \"Atropa belladonna\") and strychnine.\n"}
{"id": "3830563", "url": "https://en.wikipedia.org/wiki?curid=3830563", "title": "Ecology and Evolutionary Biology", "text": "Ecology and Evolutionary Biology\n\nSome North American universities are home to degree programs titled Ecology and Evolutionary Biology, offering integrated studies in the disciplines of ecology and evolutionary biology. The wording is intended as representing the alternative approach from the frequently used pairing of Cell and Molecular Biology, while being more inclusive than the terminology of Botany or Zoology. Recently, due to advances in the fields of genetics and molecular biology, research and education in ecology and evolutionary biology has integrated many molecular techniques.\n\nA program that focuses on the relationships and interactions that range across levels of biological organization based on a scientific study is Ecology and Evolutionary Biology. The origins and history of ecosystems, species, genes and genomes, and organisms, and how these have changed over time is all part of the studies of how biodiversity has evolved and how it takes place. Ecology and Evolutionary biology in North America is based on research impact determined by the top 10% of ecology programs. The interactive web of organisms and environment are all part of what the field of Ecology explores. There have been studies in evolution that have worked to prove that \"modern organisms have developed from ancestral ones.\" The reason that evolutionary biology is so interesting to learn about is because of the evolutionary processes that is the reason we have such a diversity of life on Earth.There are many processes that make up evolutionary biology that give great insight to how we came to be, some of which include natural selection, speciation, and common descent. \nAmong the best-known Ph.D.-granting departments that use this name are\n"}
{"id": "21155519", "url": "https://en.wikipedia.org/wiki?curid=21155519", "title": "Einstein–Cartan–Evans theory", "text": "Einstein–Cartan–Evans theory\n\nEinstein–Cartan–Evans theory or ECE theory was an attempted unified theory of physics proposed by the Welsh chemist and physicist Myron Wyn Evans (born May 26, 1950), which claimed to unify general relativity, quantum mechanics and electromagnetism. The hypothesis was largely published in the journal \"Foundations of Physics Letters\" between 2003 and 2005. Several of Evans' central claims were later shown to be mathematically incorrect and, in 2008, the new editor of \"Foundations of Physics\", Nobel laureate Gerard 't Hooft, published an editorial note effectively retracting the journal's support for the hypothesis.\n\nEarlier versions of the theory were called \"O(3) electrodynamics\". Evans claims that he is able to derive a generally covariant field equation for electromagnetism and gravity, similar to that derived by Mendel Sachs.\n\nEvans argues that Einstein's theory of general relativity does not take into account torsion, which is included in the Einstein–Cartan theory.\n\nIn 1998 Evans founded the Alpha Institute for Advanced Studies (AIAS) to keep developing his theory. Its website collects papers on the theory and recent developments.\n\nThe theory has been used to justify the motionless electromagnetic generator, a perpetual motion machine. In July 2017, Evans claimed (on his blog): \"There is immediate international interest in [papers] UFT382 and UFT383, describing the new energy from spacetime (ES) circuits. There is also great interest in UFT364, the paper that describes the circuit [...] These circuits should be [...] developed into power stations.\" In November 2017, Evans expanded on this point, as follows (again on his blog): \"There is no reasonable doubt that the vacuum (or aether or spacetime) contains a source of inexhaustible, safe and clean energy. This source can be used in patented and replicated circuits such as those of [Evans' self-published papers] UFT311, UFT364, UFT382, and UFT383.\"\n\nEvans' claims are not accepted by the mainstream physics community. In an editorial note in \"Foundations of Physics\" the Nobel laureate Gerard 't Hooft discussed the \"revolutionary paradigm switch in theoretical physics\" promised by ECE theory. He concluded that activities in the subject \"have remained limited to personal web pages and are absent from the standard electronic archives, while no reference to ECE theory can be spotted in any of the peer reviewed scientific journals\".\n\nSeveral of the published contributions in this theory have been shown to be mathematically incorrect. In response to these demonstrations, 't Hooft's editorial note concludes, \"Taking into account the findings of Bruhn, Hehl and Obukhhov, the discussion of ECE theory in the journal \"Foundations of Physics\" will be concluded herewith unless very good arguments are presented to resume the matter.\"\n\n\n\n\n"}
{"id": "46758811", "url": "https://en.wikipedia.org/wiki?curid=46758811", "title": "Ethical, Legal and Social Aspects research", "text": "Ethical, Legal and Social Aspects research\n\nThe acronyms ELSI (in the United States) and ELSA (in Europe) refer to research activities that anticipate and address ethical, legal and social implications (ELSI) or aspects (ELSA) of emerging life sciences, notably genomics and nanotechnology. ELSI was conceived in 1988 when James Watson, at the press conference announcing his appointment as director of the Human Genome Project (HGP), suddenly and somewhat unexpectedly declared that the ethical and social implications of genomics warranted a special effort and should be directly funded by the National Institutes of Health.\n\nVarious ELSI or ELSA programs have been developed, in Canada, Europe and the Far East. Overview:\n\n\nAt least four features seem typical for an ELSA approach, namely: \n\nThe ELSA approach has been widely endorsed by academics studying the societal impact of science and technology, but also criticized. Michael Yesley, responsible for the US Department of Energy (DOE) part of the ELSI programme, claims that the ELSI Program was in fact a discourse of justification, selecting topics of ethics research that will facilitate rather than challenge the advance of genetic technology. In other words, ELSA genomics as the handmaiden of genomics research. In Europe, in the context of the Horizon 2020 program, ELSA-style research is now usually framed as Responsible Research and Innovation. \nExamples of academic journals open to publishing ELSA research results are \"New Genetics and Society\" (Taylor and Francis) and \"Life Sciences, Society and Policy\" (SpringerOpen).\n"}
{"id": "51489076", "url": "https://en.wikipedia.org/wiki?curid=51489076", "title": "Greek cancer cure", "text": "Greek cancer cure\n\nThe Greek cancer cure was a putative cancer cure invented and promoted by microbiologist Hariton-Tzannis Alivizatos (died 1991). It consisted of intravenous injections of a fluid for which Aliviatos would not reveal the formula.\n\nIn 1983, Dr. Alivizatos announced that he had developed a serum that had a 60 percent success rate in arresting most types of cancer, with the exception of extremely advanced cases. He claimed that the serum could attack a protein-like substance that surrounds cancer cells and weaken the body's ability to keep the disease from spreading. Greek health officials ridiculed Dr. Alivizatos's assertions, while the American Cancer Society warned his current patients that there was no evidence that the diagnostic procedures and treatment for cancer proposed had resulted in any benefits for the treatment of cancer in human beings. They concluded that, \"there is no evidence that any aspect of the diagnostic test nor the treatment... are effective in the treatment of cancer.\" In addition they state \"Nor is there any evidence that.. the intravenous injections are safe.\"\n\n"}
{"id": "19088862", "url": "https://en.wikipedia.org/wiki?curid=19088862", "title": "Greenwave", "text": "Greenwave\n\nGreenwave is a mass science experiment involving primary schools across Ireland. It examines and records how spring arrives in Ireland. This is an educational and science initiative of the Irish Government’s Discover Science & Engineering (DSE) awareness programme.\n\nIrish students observe and record when certain plants and animals react to the longer days and warmer temperatures, in order to find out whether the \"green wave\" of spring moves from south to north across Ireland or inland from the coast to the centre of the country.\n\nTaking part in Greenwave is a practical way to support the teaching of Social Environmental and Scientific Education (SESE) part of the Irish Primary School Curriculum. The students develop their skills in:\n\nSchools registered in the Discover Primary Science programme, another initiative of DSE, can also earn credit towards their Award of Science Excellence by taking part in Greenwave.\n\nDSE runs numerous other initiatives, including Science Week Ireland, Discover Primary Science and Science.ie.\n\n"}
{"id": "53999565", "url": "https://en.wikipedia.org/wiki?curid=53999565", "title": "Holotomography", "text": "Holotomography\n\nHolotomography (HT) is a laser technique to measure three-dimensional refractive index (RI) tomogram of a microscopic sample such as biological cells and tissues. Because the RI can serve as an intrinsic imaging contrast for transparent or phase objects, measurements of RI tomograms can provide label-free quantitative imaging of microscopic phase objects. In order to measure 3-D RI tomogram of samples, HT employs the principle of holographic imaging and inverse scattering. Typically, multiple 2D holographic images of a sample are measured at various illumination angles, employing the principle of interferometric imaging. Then, a 3D RI tomogram of the sample is reconstructed from these multiple 2D holographic images by inversely solving light scattering in the sample.\n\nThe principle of HT is very similar to X-ray computed tomography (CT) or CT scan. CT scan measures multiple 2-D X-ray images of a human body at various illumination angles, and a 3-D tomogram (X-ray absorptivity) is then retrieved via the inverse scattering theory. Both the X-ray CT and laser HT shares the same governing equation – Helmholtz equation, the wave equation for a monochromatic wavelength. HT is also known as optical diffraction tomography.\n\nThe applications of HT includes \nHT provides 3D dynamic images of live cells and thin tissues without using exogenous labeling agents such as fluorescence proteins or dyes. HT enables quantitative live cell imaging, and also provides quantitative information such as cell volume, surface area, protein concentration. HT provides following advantages over conventional 3D microscopic techniques. \nHowever, 3D RI tomography does not provide molecular specificity. Generally, the measured RI information cannot be directly related to information about molecules or proteins, except for notable cases such as gold nanoparties or lipid droplets that exhibit distinctly high RI values compared to cell cytoplams.\n\nHT provide various quantitative imaging capability, providing morphological, biochemical, and mechanical properties of individuals cells. 3D RI tomography directly provides morphological properties including volume, surface area, and sphericity (roundness) of a cell. Local RI value can be translated into biochemical information or cytoplasmic protein concentration, because the RI of a solution is linearly proportional to its concentration. In particular, for the case of red blood cells, RI value can be converted into hemoglobin concentration. Measurements of dynamic cell membrane fluctuatino, which can also be obtained with a HT instrument, provides information about cellular deformability. Furthermore, these various quantitative parameters can be obtained at the single cell level, allowing correlative analysis between various cellular parameters. HT has been utilized for the study of red blood cells, white blood cells, malaria infection, bebasia infection, blood storage, and diabetes.\n\nThe first theoretical proposal was presented by Emil Wolf, and the first experimental demonstration was shown by Fercher et al. From 2000s, HT techniques had been extensively studied and applied to the field of biology and medicine, by several research groups including MIT spectroscopy laboratory (PI: the late Michael S. Feld). Both the technical developments and applications of HT have been significantly advanced. During the mid 2010s, first commercial HT companies Nanolive and Tomocube were founded.\n\n"}
{"id": "1018292", "url": "https://en.wikipedia.org/wiki?curid=1018292", "title": "Hylozoism", "text": "Hylozoism\n\nHylozoism is the philosophical point of view that matter is in some sense alive. The concept dates back at least as far as the Milesian school of pre-Socratic philosophers. The term was introduced to English by Ralph Cudworth in 1678.\n\nAlthough there is a distinction between possessing a mind (hylopsychism) and possessing life (hylozoism); in practice this division is difficult to maintain, because the ancient hylozoists not only regarded the spirits of the material universe and plant world as alive, but also as more or less conscious. Whereas animism tends to view life as taking the form of discrete spirits, and panpsychism tends to refer to strictly philosophical views like that of Gottfried Wilhelm Leibniz, hylozoism refers largely to views such as those of the earliest Greek philosophers (6th and 5th centuries BC), who treated the magnet as alive because of its attractive powers (Thales), or air as divine (Anaximenes), perhaps because of its apparently spontaneous power of movement, or because of its essentiality for life in animals. Later this primitive hylozoism reappeared in modified forms. Some scholars have since claimed that the term \"hylozoism\" should properly be used only where body and soul are explicitly distinguished, the distinction then being rejected as invalid. Nevertheless, hylozoism remains logically distinct both from early forms of animism, which personify nature, and from panpsychism, which attributes some form of consciousness or sensation to all matter.\n\nSome of the ancient Greek philosophers taught a version of hylozoism, as they, however vaguely, conceived the elemental matter as being in some sense animate if not actually conscious and (a directed effort, a striving or tendency; a \"nisus\"). Thales, Anaximenes, and Heraclitus all taught that there is a form of life in all material objects, and the Stoics believed that a \"world soul\" was the vital force of the universe. Note that these philosophies did not necessarily hold that material objects had \"separate life\" or \"identity\", only that they had life, either as part of an overriding entity or as living but insensible entities.\n\nIn the Renaissance, Bernardino Telesio, Paracelsus, Cardanus, and Giordano Bruno revived the doctrine of hylozoism. The latter, for example, held a form of Christian pantheism, wherein God is the source, cause, medium, and end of all things, and therefore all things are participatory in the ongoing Godhead. Bruno's ideas were so radical that he was entirely rejected by the Roman Catholic Church as well as excommunicated from a few Protestant groups, and he was eventually burned at the stake for various heresies. Telesio, on the other hand, began from an Aristotelian basis and, through radical empiricism, came to believe that a living force was what informed all matter. Instead of the intellectual universals of Aristotle, he believed that life generated form.\n\nIn England, some of the Cambridge Platonists approached hylozoism as well. Both Henry More and Ralph Cudworth (\"the Younger\", 1617–1688), through their reconciliation of Platonic idealism with Christian doctrines of deific generation, came to see the divine lifeforce as the informing principle in the world. Thus, like Bruno, but not nearly to the extreme, they saw God's generative impulse as giving life to all things that exist. Accordingly, Cudworth, the most systematic metaphysician of the Cambridge Platonist tradition, fought hylozoism. His work is primarily a critique of what he took to be the two principal forms of atheism—materialism and hylozoism.\n\nCudworth singled out Hobbes not only as a defender of the hylozoic atheism \"which attributes life to matter\", but also as one going beyond it and defending \"hylopathian atheism, which attributes all to matter.\" Cudworth attempted to show that Hobbes had revived the doctrines of Protagoras and was therefore subject to the criticisms which Plato had deployed against Protagoras in the \"Theaetetus\". On the side of hylozoism, Strato of Lampsacus was the official target. However, Cudworth's Dutch friends had reported to him the views which Spinoza was circulating in manuscript. Cudworth remarks in his \"Preface\" that he would have ignored hylozoism had he not been aware that a new version of it would shortly be published.\n\nSpinoza's idealism also tends toward hylozoism. In order to hold a balance even between matter and mind, Spinoza combined materialistic with pantheistic hylozoism, by demoting both to mere attributes of the one infinite substance. Although specifically rejecting identity in inorganic matter, he, like the Cambridge Platonists, sees a life force within, as well as beyond, all matter.\n\nImmanuel Kant presented arguments against hylozoism in the third chapter of his \"Metaphysische Anfangsgründe der Naturwissenschaften\" (\"First Metaphysical Principles of Natural Science,\" 1786) and also in his famous \"Kritik der reinen Vernunft\" (\"Critique of Pure Reason,\" 1783). Yet, in our times, scientific hylozoism – whether modified, or keeping the trend to make all beings conform to some uniform pattern, to which the concept was adhered in modernity by Herbert Spencer, Hermann Lotze, and Ernst Haeckel – was often called upon as a protest against a mechanistic worldview.\n\nIn the 19th century, Haeckel developed a materialist form of hylozoism, specially against Rudolf Virchow's and Hermann von Helmholtz's mechanical views of humans and nature. In his \"Die Welträtsel\" of 1899 (\"The Riddle of the Universe\" 1901), Haeckel upheld a unity of organic and inorganic nature and derived all actions of both types of matter from natural causes and laws. Thus, his form of hylozoism reverses the usual course by maintaining that living and nonliving things are essentially the same, and by erasing the distinction between the two and stipulating that they behave by a single set of laws.\n\nIn contrast, the Argentine-German neurobiological tradition terms \"hylozoic hiatus\" all of the parts of nature which can only behave lawfully or nomically and, upon such a feature, are described as lying outside of minds and amid them – i.e. extramentally. Thereby the hylozoic hiatus becomes contraposed to minds deemed able of behaving semoviently, i.e. able of inaugurating new causal series (semovience). Hylozoism in this contemporary neurobiological tradition is thus restricted to the portions of nature behaving nomically inside the minds, namely the minds' sensory reactions (Christfried Jakob's \"sensory intonations\") whereby minds react to the stimuli coming from the hylozoic hiatus or extramental realm.\n\nMartin Buber too takes an approach that is quasi-hylozoic. By maintaining that the essence of things is identifiable and separate, although not pre-existing, he can see a soul within each thing.\n\nThe French Pythagorean and Rosicrucian alchemist, Francois Jollivet-Castelot (1874-1937), established a hylozoic esoteric school which combined the insight of spagyrics, chemistry, physics, transmutations and metaphysics. He published many books, one of which was called \"L’Hylozoïsme, l’alchimie, les chimistes unitaires\" (1896). In his view there was no difference between spirit and matter except for the degree of frequency and other vibrational conditions.\n\nThe Mormon theologian Orson Pratt taught a form of hylozoism.\n\nAlice A. Bailey wrote a book called \"The Consciousness of the Atom\".\n\nInfluenced by Alice A. Bailey, Charles Webster Leadbeater, and their predecessor Madame Blavatsky, Henry T. Laurency produced voluminous writings describing a hylozoic philosophy.\n\nInfluenced by George Ivanovich Gurdjieff, the English philosopher and mathematician John Godolphin Bennett, in his four-volume work \"The Dramatic Universe\" and his book \"Energies\", developed a six-dimensional framework in which matter-energy takes on 12 levels of hylozoic quality.\n\nKen Wilber embraces hylozoism to explain subjective experience and provides terms describing the ladder of subjective experience experienced by entities from atoms up to Human beings in the \"upper left quadrant\" of his Integral philosophy chart.\n\nPhysicist Thomas Brophy, in The Mechanism Demands a Mysticism, embraces hylozoism as the basis of a framework for re-integrating modern physical science with perennial spiritual philosophy. Brophy coins two additional words to stand with hylozoism as the three possible ontological stances consistent with modern physics. Thus: hylostatism (universe is deterministic, thus “static” in a four-dimensional sense); hylostochastism (universe contains a fundamentally random or stochastic component); hylozoism (universe contains a fundamentally alive aspect).\n\nArchitect Christopher Alexander has put forth a theory of the living universe, where life is viewed as a pervasive patterning that extends to what is normally considered non-living things, notably buildings. He wrote a four-volume work called \"The Nature of Order\" which explicates this theory in detail.\n\nPhilosopher and ecologist David Abram articulates and elaborates a form of hylozoism grounded in the phenomenology of sensory experience. In his books \"Becoming Animal\" and \"The Spell of the Sensuous,\" Abram suggests that matter is never entirely passive in our direct experience, holding rather that material things actively \"solicit our attention\" or \"call our focus,\" coaxing the perceiving body into an ongoing participation with those things. In the absence of intervening technologies, sensory experience is inherently animistic, disclosing a material field that is animate and self-organizing from the get-go. Drawing upon contemporary cognitive and natural science as well as the perspectival worldviews of diverse indigenous, oral cultures, Abram proposes a richly pluralist and story-based cosmology, in which matter is alive through and through. Such an ontology is in close accord, he suggests, with our spontaneous perceptual experience; it calls us back to our senses and to the primacy of the sensuous terrain, enjoining a more respectful and ethical relation to the more-than-human community of animals, plants, soils, mountains, waters and weather-patterns that materially sustains us.\n\nBruno Latour's actor-network theory, in the sociology of science, treats non-living things as active agents and thus bears some metaphorical resemblance to hylozoism.\n\nArt\nLiterature\nMMORPGs \nMusic\n"}
{"id": "12220917", "url": "https://en.wikipedia.org/wiki?curid=12220917", "title": "Irlen filters", "text": "Irlen filters\n\nIrlen Spectral Filters or Irlen Lenses, are coloured overlay filters or tinted lenses crafted specifically for the wearer and worn as glasses or contact lenses. They are intended to help people with the supposed perceptual processing difficulty known as Irlen Syndrome, also known as Scotopic Sensitivity Syndrome or visual stress. For individuals who suffer from Irlen Syndrome, the brain is sensitive to specific wavelengths of light, resulting in difficulties with print clarity and stability and discomfort when performing visually intensive activities such as reading. Irlen Syndrome affects approximately 50 percent of individuals with reading difficulties and dyslexia, 33 percent of those with attention difficulties such as attention-deficit hyperactivity disorder, 33 percent with autism, up to 50 percent of those who have suffered a traumatic brain injury, whiplash or concussion, and approximately 12-14 percent of the general population. Standardised diagnostic procedures have been developed to individualise the colour selection. Experts call the syndrome and the treatment controversial, because it is based on insufficient research. However, current research on the topic includes placebo controls, longitudinal studies, and cutting-edge brain mapping technology, all of which support the use of colour to alleviate the symptoms associated with Irlen Syndrome.\n\nScotopic sensitivity syndrome, also known as Irlen Syndrome, is said to be a visual-perceptual defect related to difficulties with light source, glare, luminance, wavelength and black/white contrast. According to Irlen, these difficulties lead to reading problems, eye-strain, headaches, migraines, and other physical difficulties that can be alleviated by the use of person specific tinted lenses, known as Irlen Spectral Filters, worn as glasses or contact lenses.\n\nThe syndrome has six characteristics:\n\n\nScotopic sensitivity syndrome is diagnosed by interviewing the client and by observing responses to certain visual tasks such as interpreting geometric figures and reading.\n\nThe idea of page based distortion was initially suggested in 1980 by Olive Meares, to improve the reading ability of people with a learning disability, specifically a certain type of dyslexia. Later this was taken further by psychologist Helen Irlen. The proposition by Irlen was made at the Annual Meeting of the American Psychological Association in 1983. At that time, there was little research made on the effect of tinted lenses. Irlen gained notable publicity demonstrating the efficacy of her method on television. Tinted lenses became a commercial success, and testing and prescribing centers were opened throughout USA.\n\nDiagnosis of Irlen Syndrome and treatment with Irlen Spectral Filters has been reviewed by the USA Medical Board, and has been determined as not the practice of medicine; it has also been reviewed by various USA Boards of Optometry and has been found not to be the practice of optometry. Binocular and accommodative anomalies may occur in conjunction with the syndrome, but are not considered to be the underlying physiological basis of the condition.\n\nAccording to Irlen, the lenses can be used to treat a wide variety of problems that are associated with light sensitivity, discomfort and distortions, including head injury, concussion, whiplash, perceptual problems, neurologic impairment, memory loss, language deficits, headaches and migraine, autoimmune disease, fibromyalgia, macular degeneration, cataracts, retinitis pigmentosa, complications from an eye operation, depression, chronic anxiety and others. She has also claimed that a treatment for scotopic sensitivity syndrome could help a number of incarcerated individuals and delinquent children.\n\nThe Irlen method has been criticised for being put to the market prior to serious research. According to Helveston, the scotopic sensitivity syndrome and its treatment has, as a phenomenon, resulted in classic group behaviour and has the characteristics of a fad with a charismatic personality as a leader and the supporting evidence being mostly anecdotal. A 2004 study by Professor Arnold Wilkins at Essex University, though, shows that this may not be the case. It is suspicious that the treatment quickly came to be applied to a wide variety of maladies, suggesting that this treatment is an example of classic group behavior. The syndrome is, however, associated with a growing array of possibly diverse conditions, as well as worldwide franchise according to critics.\n\n\n"}
{"id": "20140244", "url": "https://en.wikipedia.org/wiki?curid=20140244", "title": "Jadad scale", "text": "Jadad scale\n\nThe Jadad scale, sometimes known as Jadad scoring or the Oxford quality scoring system, is a procedure to independently assess the methodological quality of a clinical trial. It is named after Colombian physician Alex Jadad who in 1996 described a system for allocating the trial a score of between zero (very poor) and five (rigorous). It is the most widely used such assessment in the world, and as of 2017, its seminal paper has been cited in over 13000 scientific works.\n\nThe Jadad scale independently assesses the methodological quality of a clinical trial judging the effectiveness of blinding. Alejandro \"Alex\" Jadad Bechara, a Colombian physician who worked as a Research Fellow at the Oxford Pain Relief Unit, Nuffield Department of Anaesthetics, at the University of Oxford described the allocating trials a score of between zero (very poor) and five (rigorous) scale in an appendix to a 1996 paper.\nIn a 2007 book Jadad described the randomised controlled trial as \"one of the simplest, most powerful and revolutionary forms of research\".\n\nClinical trials are conducted for the purpose of collecting data on the efficacy of medical treatments. The treatment might be, for example, a new drug, a medical device, a surgical procedure, or a preventative regime. Clinical trial protocols vary considerably depending on the nature of the treatment under investigation, but typically in a controlled trial researchers gather a group of volunteers and subject some to the test treatment, while giving the others either no treatment (known as a placebo), or an established treatment for comparison. After a defined time period, the patients in the test group are assessed for health improvements in comparison with the control group.\n\nHowever, trials can vary greatly in quality. Methodological errors such as poor blinding or poor randomisation allow factors such as the placebo effect or selection bias to adversely affect the results of a trial.\n\nRandomisation is a process to remove potential distortion of statistical results arising from the manner in which the\ntrial is conducted, in particular in the selection of subjects. Studies have indicated, for example, that nonrandomised trials are more likely to show a positive result for a new treatment than for an established conventional one.\n\nThe importance of scientific controls to limit factors under test is well established. However, it is also important that none of those involved in a clinical trial, whether the researcher, the subject patient or any other involved parties, should allow their own prior expectations to affect reporting of results. The placebo effect is known to be a confounding factor in trials; affecting the ability of both patients and doctors to report accurately on the clinical outcome. Experimental blinding is a process to prevent bias, both conscious and subconscious, skewing results.\n\nBlinding frequently takes the form of a placebo, an inactive dummy that is indistinguishable from the real treatment. Blinding can however be difficult to achieve in some trials, for example, surgery or physical therapy. Poor blinding can exaggerate the perceived effects of treatment, particularly if any such effects are small. Blinding should be appropriate to the study, and is ideally double blind, wherein neither the patient nor doctor is aware of whether they are in the control or test group, eliminating any such psychological effects from the study.\n\nWithdrawals and dropouts are those patients who fail to complete a course of treatment, or fail to report back on its outcome to the researchers. The reasons for doing so might be varied: the individuals may have moved away, abandoned the course of treatment, or died. Whatever the reason, the attrition rate can skew results of a study, particularly for those subjects who ceased treatment due to perceived inefficacy. In smoking cessation studies, for example, it is routine to consider all dropouts as failures.\n\nA three-point questionnaire forms the basis for a Jadad score. Each question was to be answered with either a \"yes\" or a \"no\". Each \"yes\" would score a single point, each \"no\" zero points; there were to be no fractional points. The Jadad team stated that they expected it should take no longer than ten minutes to score any individual paper. The questions were as follows: \"Was the study described as randomized?\", \"Was the study described as double blind?\" and \"Was there a description of withdrawals and dropouts?\"\n\nTo receive the corresponding point, an article should describe the number of withdrawals and dropouts, in each of the study groups, and the underlying reasons. Additional points were given if: \"The method of randomisation was described in the paper, and that method was appropriate.\" or \"The method of blinding was described, and it was appropriate.\"\n\nPoints would be \"deducted\" if: \"The method of randomisation was described, but was inappropriate.\" or \"The method of blinding was described, but was inappropriate.\"\n\nA clinical trial could therefore receive a Jadad score of between zero and five. The Jadad scale is sometimes described as a five-point scale, though there are only three questions.\n\nThe Jadad score may be used in a number of ways:\n\n\n, the Jadad score was the most widely used such assessment in the world, and its seminal paper has been cited in over 3000 scientific works.\n\nCritics have charged that the Jadad scale is flawed, being over-simplistic and placing too much emphasis on blinding, and can show low consistency between different raters. Furthermore, it does not take into account allocation concealment, viewed by The Cochrane Collaboration as paramount to avoid bias.\n\n"}
{"id": "44389561", "url": "https://en.wikipedia.org/wiki?curid=44389561", "title": "LEAP Science and Maths Schools", "text": "LEAP Science and Maths Schools\n\nLanga Education Assistance Program (LEAP), also known as LEAP Science and Maths Schools, is a collection of six non-fee payment secondary education schools located in three provinces in South Africa. The first LEAP school opened in 2004 in rented premises in Observatory, Cape Town and served mainly, learners from the township of Langa. As an independent school, LEAP is mainly founded by South African Corporates with limited subsidies from the Department of Basic Education.\n\nJohn Gilmour was a teacher at Pinelands High School in Cape Town in 1987 when he decided to respond to a call from individuals within the South African business community, to contribute to the redress of the devastation of the Bantu Education Act, a segregation law imposed in the education sector by the Apartheid system in 1953.\n\n“Africa Week” was then introduced by a team led by John Gilmour to bring black learners under the “Bantu Education” system to spend a week at Pinelands High School, which was then a white’s only school. The program became the precursor for the Langa Education Assistance Program (or LEAP) which aimed at providing one hundred black students from the Langa township, with support tuition from Pinelands High teachers in English, Maths and Science, three afternoons a week.\n\nThe prohibitive transport cost of bringing learners from the township schools to Pinelands High School forced the model to be revised. In 1996, it was then decided that instead of learners being bussed in to Pinelands High, teachers will be transported to meet learners in township schools.\n\nCommunity members in Langa perceived the new model of LEAP as an attempt of white teachers to “save” black children. Teachers from the community felt that they were as able as other teachers to provide extra lessons that would address the inadequacies in the students' educational foundations. The uneasiness within the community obliged the leadership of LEAP to change the model and include community teachers in the program.\n\nWhen John Gilmour realised that despite best efforts of LEAP and similar programs in the country there was no increase in the proportion of black learners entering university, especially science-based disciplines, he sought for an alternative model. LEAP Science and Maths school was the alternative model.\n\nLEAP Science and Maths school was a deliberate decision of John Gilmour seeking to increase the number of black learners who take science and maths-based modules at high school in order to increase the chance of being accepted at University, particular in disciplines where these modules are a prerequisite.\n\nJohn resigned as Headmaster of Abbot’s College in 2004, where he has been since he left Pinelands in 1997, to focus on LEAP Science and Maths School. In January 2004, the first LEAP Science and Maths School opened its doors in the suburb of Observatory, Cape Town, with seventy-two learners, seven teachers and one administrative staff member.\n\nThe LEAP Science and Maths school, which started in Observatory later moved to Pinelands and become known as LEAP 1. It serves learners from community of Langa Township. It is the first and oldest of the LEAP Science and Maths schools. The school is headed by its Operation Leader Oscar Dlodlo.\n\nLEAP 2 opened in 2007 also in Pinelands. It serves learners from the townships of Gugulethu and Crossroads. In 2013, the school opened satellite classes to serve grade 9 and 10 learners from the township of Delft. Learners of the school are taught in English and isiXhosa. The head of the school is Mr. Babele Emedi, who also assumes the role of Operations Leader.\n\nThe school opened its doors in 2008 to serve learners from the township of Alexandra in Johannesburg. Learners are taught in English, isiZulu, Sepedi and Sesotho under the leadership of the Operations leader, Mr. Mark Fletcher.\n\nThis school is situated in the township of Diepsloot just outside Johannesburg. The school opened in 2011 through a partnership with the South Africa corporate Aveng Group. LEAP 4 teaches learners in isiZulu and Sepedi as home languages, along with English. Paul Mumba, is the Operations Leader and head of LEAP 4.\n\nIn 2012, LEAP 5 opened its doors in Jane Furse, Sekhukhune in Limpopo province. The school is under the leadership of its Operation Leader, Raphael Mukachi.\n\nLEAP 6 is situated in Ga-Rankuwa near Pretoria. The school opened in 2012 to respond to the request of the Anglican Diocese of Pretoria, concerned with the community’s poor educational results and high unemployment rate, particularly among young people. The school is led by its Operation Leader, Lawrence Manyesa.\n\n“We strive for the positive transformation of communities through the meaningful education of children from those communities”\n\n“LEAP schools provide student-centered, math’s and science-focused education to economically disadvantaged students from grades 8 to 12. We focus on enabling the self-awareness necessary for each student’s growth to healthy adulthood, and to ensuring optimal academic results which will allow for choices for lifelong learning and a fulfilling future.”\n\nAlthough LEAP schools focus on Maths and Sciences; it puts a lot of emphasis on the emotional development of learners. John Gilmour acknowledged the importance of the emotional development in a country where “eight million children come from single-parent households and a further 4.3 million reside with neither biological parent”. In his article “A tale of two systems”, he speaks of LEAP’s response to this problem “has been to recognise that social transformation starts with personal transformation: one person at a time, one room at a time, one school at a time and one community at a time.”\n\nThe school cherishes the following values:\n\nIn a report that analyses the performance of the school between 2005 and 2013, LEAP reported that:\n\n\nThe school claims to outperform national statistics when it compares itself with national averages of learners who write maths and science in 2013. All LEAP learners wrote these modules compared to only 43% who took maths and 33% who took physical science nationally.\n\nBonisanani Mtshekisa is LEAP’s first university graduate. He graduated with a bachelor in Bachelor of Commerce in Finance from the University of the Western Cape. In its 2012 annual report, the school reported to have over 30 university graduates ranging from physiotherapist, engineer to accountants.\n\nIn 2014, LEAP launched a series of celebratory adverts to showcase its work and celebrate the achievements of its past students. Qondisa Nxganga, a Master’s graduate from the prestigious African Institute for Mathematical Sciences (AIMS), is featured in these adverts alongside a number of other LEAP alumni.\n\nLEAP’s model is based on strong engagement with the broader community and building beneficial partnerships and collaboration with other organizations, particular schools. It is a three-way collaboration approach where each LEAP school partners with a more privileged school as well as a less privileged school (a township school) in the community where it operates to promote a culture of shared resources and cultural exchange\n\nLEAP has also been successful in building strong partnerships with both commercial and non-profit organizations. Most of its funding comes from South African corporates, trusts and Foundations.\n\nLEAP, is Teach with Africa’s partner on the continent and together they have established the Teacher Institute which seeks to develop teachers and supports individuals wanting to enter the teaching profession\n\n"}
{"id": "49227605", "url": "https://en.wikipedia.org/wiki?curid=49227605", "title": "Laboratory for the Conservation of Endangered Species", "text": "Laboratory for the Conservation of Endangered Species\n\nLaCONES or Laboratory for the Conservation of Endangered Species, is a Council of Scientific and Industrial Research lab located in Hyderabad. It was conceptualised by Lalji Singh. It is India's only research facility engaged in conservation and preservation of wildlife and its resources. It was established in 1998 with the help of Central Zoo Authority of India, CSIR and the government of Andhra Pradesh. It was dedicated to the nation in 2007 by then President of India APJ Abdul Kalam.It is a part of ccmb (centre for cellular and molecular biology).India's 1st genetic bank for wilflife conservation nwgrb (national wildlife genetic resource bank) stablished by government in lacones\n"}
{"id": "46813512", "url": "https://en.wikipedia.org/wiki?curid=46813512", "title": "List of diagnoses characterized as pseudoscience", "text": "List of diagnoses characterized as pseudoscience\n\nThere are many proposed diseases and diagnoses that are rejected by orthodox medicine consensus and are associated with pseudoscience.\n\nPseudoscience rejects empirical methodology. Pseudoscientific diseases are not defined using objective criteria. They cannot achieve, and perhaps do not seek, medical recognition.\n\nOther conditions may be rejected or contested by orthodox medicine, but are not necessarily associated with pseudoscience. Diagnostic criteria for some of these conditions may be vague, over-inclusive, or otherwise ill-defined. Although the evidence for the disease may be contested or lacking, however, the justification for these diagnoses is nevertheless empirical and therefore amenable to scientific investigation, at least in theory.\n\nExamples of conditions that are not necessarily pseudoscientific include:\n\n\n"}
{"id": "12494249", "url": "https://en.wikipedia.org/wiki?curid=12494249", "title": "List of vineyard soil types", "text": "List of vineyard soil types\n\nThe soil composition of vineyards is one of the most important viticultural considerations when planting grape vines. The soil supports the root structure of the vine and influences the drainage levels and amount of minerals and nutrients that the vine is exposed to. The ideal soil condition for a vine is a layer of thin topsoil and subsoil that sufficiently retains water but also has good drainage so that the roots do not become overly saturated. The ability of the soil to retain heat and/or reflect it back up to the vine is also an important consideration that affects the ripening of the grapes.\n\nThere are several minerals that are vital to the health of vines that all good vineyard soils have. These include calcium which helps to neutralize the Soil pH levels, iron which is essential for photosynthesis, magnesium which is an important component of chlorophyll, nitrogen which is assimilated in the form of nitrates, phosphates which encourages root development, and potassium which improves the vine metabolisms and increases its health for next year's crop.\n\n\"Unless otherwise noted the primary reference for this list is Sotheby's Wine Encyclopedia 2005\"\n\n\n\n\n"}
{"id": "10917819", "url": "https://en.wikipedia.org/wiki?curid=10917819", "title": "Lists of unsolved problems", "text": "Lists of unsolved problems\n\nA list of unsolved problems may refer to several conjectures or open problems in various academic fields:\n\n\n"}
{"id": "2470204", "url": "https://en.wikipedia.org/wiki?curid=2470204", "title": "MadSci Network", "text": "MadSci Network\n\nThe Madsci Network is a website known primarily for its \"Ask-A-Scientist\" forum where users can ask questions to a panel of volunteer scientists. Each question, submitted via a Web interface, is reviewed by a volunteer moderator. If the question is intelligible, not a homework assignment, and has not been answered previously, it may be answered directly by the moderator, or forwarded to one of hundreds of volunteer scientists and professionals. The moderators match each question to a volunteer's area of expertise. After answering the question, the volunteer sends it back to the moderators who then review the answer prior to posting it on the web site. The moderator may ask the scientist to edit the answer or provide references for information. Thereafter, the majority of questions and answers are made publicly available in the extensive archives, which date back to 1996. \n\nThe Madsci Network hosts the Edible and Inedible Experiments Archive, a unique collection of easy science demos, and a guided tour of data from the Visible Human Project.\n\nThe Madsci Network gets approximately 600,000 unique visitors and roughly 3 million page views per month. It is a non-profit ask-a-scientist site with over 700 scientists distributed globally and has been cited in academic publications, web awards, sites/portals like yahoo.com, etc. The principals are: Founder and Executive Director, Dr. Lynn Bry, and Director of Research, Dr. Ricky J Sethi. Notable scientists who have answered questions on the website include Dr. Samuel Conway.\n\n"}
{"id": "13103839", "url": "https://en.wikipedia.org/wiki?curid=13103839", "title": "Magnetic water treatment", "text": "Magnetic water treatment\n\nMagnetic water treatment (also known as anti-scale magnetic treatment or AMT) is a method of supposedly reducing the effects of hard water by passing it through a magnetic field as a non-chemical alternative to water softening. Magnetic water treatment is regarded as unproven and unscientific.\n\nThere is a lack of peer-reviewed laboratory data, mechanistic explanations, and documented field studies to support its effectiveness. Erroneous conclusions about their efficacy are based on applications with uncontrolled variables. There are, however, some studies which have claimed significant effects and proposed possible mechanisms for the observed decrease in water scale.\n\nVendors of magnetic water treatment devices frequently use pictures and testimonials to support their claims, but omit quantitative detail and well-controlled studies. Advertisements and promotions generally omit system variables, such as corrosion or system mass balance analyticals, as well as measurements of post-treatment water such as concentration of hardness ions or the distribution, structure, and morphology of suspended particles.\n\nDuration of exposure and field strength, gradient, rate of change, and orientation along or perpendicular to flow are variously cited as important to the results. Magnetic water treatment proponent Klaus Kronenberg proposed that the shapes of solute lime molecules are modified by strong magnetic fields, leading them to precipitate as spherical or round crystals rather than deposit as sheets or platelets of hard crystals. Simon Parsons of the School of Water Sciences at Cranfield University proposed that the magnetic field reduces the surface charge on small particles, increasing the tendency to coagulate as large particles that stay with the flow rather than depositing as scale. However, an internal study in 1996 at Lawrence Livermore National Laboratory found no difference in preferred crystal structure of scale deposited in magnetic water treatment systems. \nLiu \"et al.\" and Coey and Cass published research in 2010 and 2000 reporting that magnetic treatment causes water containing minerals to favor formation of a more soluble form of calcium carbonate (aragonite rather than calcite).\n\nThe effect of magnetic treatment depends on properties of the pipe. The magnititude of the effect depends on pipe conductivity and surface roughness.\n\nThere are related non-chemical devices based on a variety of physical phenomena which have been marketed for over 50 years with similar claims of scale inhibition. Whilst some are effective, such as electrolytic devices, most do not work.\nOther uses of magnetic devices:\n\n"}
{"id": "47317405", "url": "https://en.wikipedia.org/wiki?curid=47317405", "title": "Nature of Man Series", "text": "Nature of Man Series\n\nThe Nature of Man Series is a four-volume series of works in paleoanthropology by the prolific playwright, screenwriter, and science writer Robert Ardrey. The books in the series were published between 1961 and 1976.\n\nThe series majorly undermined standing assumptions in social sciences, leading to an abandonment of the \"blank slate\" hypothesis; incited a renaissance in the science of ethology; and led to widespread popular interest in human evolution and human origins.\n\nThe first work, \"African Genesis\" (1961), particularly helped revive interest in ethology, and was a direct precursor to the Konrad Lorenz's \"On Aggression\" (1966), Desmond Morris's \"The Naked Ape\" (1967), Lionel Tiger's \"Men in Groups\" (1969), and Tiger and Robin Fox's \"The Imperial Animal\" (1971). The director of the Smithsonian Institution's Human Origins Program Rick Potts, cited Ardrey's work as inspiring him to go anthropology.\n\nThe works were wildly popular and influenced the public imagination. Stanley Kubrick cited them as major influences in developing his films (1968) and \"A Clockwork Orange\" (1971).\n\nRobert Ardrey was a prolific playwright, screenwriter, and science writer. By the time he returned to the sciences in the 1950s, he had already had a decorated Hollywood and Broadway career, including the award of a Guggenheim Fellowship and an Academy Award nomination for best screenplay.\n\nIn 1955 Ardrey travelled to Africa, where he wrote a series of articles for \"The Reporter\". At the same time he renewed an acquaintance with prominent geologist Richard Foster Flint and investigated claims made by Raymond Dart about a specimen of \"Australopithecus africanus\". This trip would initiate the decades of work Ardrey completed in the field of human evolution.\n\nThe central thesis of \"African Genesis: A Personal Investigation into the Animal Origins and Nature of Man\" was that early man evolved from carnivorous African predecessors, and not, as was then the scientific consensus, from Asian herbivores. It drew particularly on the scientific work of Raymond Dart and Konrad Lorenz. This thesis has been proven and is now scientific doctrine.\n\n\"African Genesis\" also challenged a key methodological assumption of the social sciences, namely that human behavior was distinct from animal behavior. Ardrey instead asserted that evolutionarily inherited traits were a major factor in determining human behavior. This was a hugely controversial hypothesis, though it has gained widespread acceptance today. It was a major theme that would extend throughout the \"Nature of Man\" books and continue to surround them with controversy.\n\n\"African Genesis\" was a major popular success. It was an international bestseller translated into dozens of languages. In 1962 it was a finalist for the National Book Award in nonfiction. In 1969 \"Time\" magazine named \"African Genesis\" the most notable nonfiction book of the 1960s.\n\n\"The Territorial Imperative: A Personal Inquiry Into the Animal Origins of Property and Nations\" extends Ardrey's work in examining the effects of inherited evolutionary traits on human social behavior with an emphasis on the hold that territory has on man. In particular it demonstrates the influence of the drive to possess territory on such phenomena as property ownership and nation-building.\n\n\"The Territorial Imperative\" further developed the nascent science of ethology and increased public interest in human origins.\n\nLike \"African Genesis\" it was also an international bestseller and saw translation into dozens of languages. It influenced several notable figures. Stanley Kubrick cited Ardrey as an inspiration for his films \"\" and \"A Clockwork Orange\". The strategic analyst Andrew Marshall and U.S. Secretary of Defense James Schlesinger are known to have discussed \"The Territorial Imperative\" in connection to military-strategic thinking.\n\n\"The Social Contract: A Personal Inquiry into the Evolutionary Sources of Order and Disorder\" is the most controversial book of the \"Nature of Man\" series. It sought to apply evolutionary thinking to the creation of social order. In particular it examined inherited characteristics' effects in determining hierarchy and inequality. Ardrey argued that, while inequality was not necessarily a social evil, it could only be justly expressed under conditions of absolute equality of opportunity. He also argued that the presence of inequality does not justify the domination of the weak by the strong. \"Ardrey showed that in all societies at any level of the animal world, structures exist to protect the vulnerable, and that this is an evolutionary advantage as it protects diversity, diversity being essential for creativity.\"\n\n\"The Social Contract\" continued Ardrey's refutation of cultural determinists through interwoven analyses of animal and human behavior. It also emphasized the importance of a reasoned respect for nature, foreshadowing the environmental concerns of \"The Hunting Hypothesis.\"\n\n\"The Hunting Hypothesis: A Personal Conclusion Concerning the Evolutionary Nature of Man\" continued Ardrey's examination of the importance of inherited evolutionary traits. In particular it demonstrated the determinant force of traits that co-evolved in early man with hunting behavior.\n\nAt the time of publication, it was not even commonly accepted that early man were hunters, much less that hunting behavior influenced their evolution. Following publication of Ardrey's work this thesis gained support and eventually widespread acceptance.\"For decades researchers have been locked in debate over how and when hunting began and how big a role it played in human evolution. Recent analyses of human anatomy, stone tools and animal bones are helping to fill in the details of this game-changing shift in subsistence strategy. This evidence indicates that hunting evolved far earlier than some scholars had envisioned – and profoundly impacted subsequent human evolution.\"\n\n\"The Hunting Hypothesis\" was also one of the first books to warn about climate change as a possible existential threat to mankind.\n\n\"The Hunting Hypothesis,\" with some exceptions, was remarkably well reviewed. The famed biologist and naturalist E. O. Wilson, the noted anthropologist Colin Turnbull, the acclaimed journalist Max Lerner, and the noteworthy social scientist Roger Masters, among others, all wrote effusive reviews. Antony Jay wrote that \"Robert Ardrey's books are the most important to be written since the war and arguable in the 20th century.\"\n\n"}
{"id": "3844654", "url": "https://en.wikipedia.org/wiki?curid=3844654", "title": "New investigator", "text": "New investigator\n\nCertain scholarly funding agencies make a distinction between investigators and new investigators. New investigators would be evaluated in a different way when competing for funding with more seasoned researchers, or they would be able to access funding resources specific to them. The rationale behind is to avoid the current trend that for certain grants the average age of the researchers receiving it for the first time keeps increasing over the years (See graph).\n\nThe formal definition of a new investigator varies with the funding agency. For example, for the NIH, a new investigator is one that does not have a story of previous funding. Other organizations, such as the Alzheimer's Association, consider a new investigator someone that is less than 10 years past their PhD degree. The Canadian Institutes of Health Research require a maximum of 60 months holding a full-time research appointment, and the Leukemia Research Foundation consider them to be within seven years of their first independent position. The Medical Research Council (UK) simply considers postdoctoral researchers in their first academic post.\n\nThe equivalent term Early Researcher (or Early Career Researcher) is also used in different ways, where for example, for the Australian Research Council it means someone that received a PhD or equivalent research doctorate within five years, or for the Ontario Ministry of Research and Innovation must be an independent researcher within the first five years of the start of their independent academic research career.\n\nMany journals have recognized the essential contributions and achievements of early researchers and highlited their profiles in different fomats. For instance the Nature research group journal Communications Biology has lanuched a series for early career scientists, reporting some of the rising stars in all fields of biology throughout the year .\n\n"}
{"id": "7608175", "url": "https://en.wikipedia.org/wiki?curid=7608175", "title": "Ontogeny and Phylogeny (book)", "text": "Ontogeny and Phylogeny (book)\n\nOntogeny and Phylogeny is a 1977 book on evolution by Stephen Jay Gould, in which the author explores the relationship between embryonic development (ontogeny) and biological evolution (phylogeny). Unlike his many popular books of essays, it was a technical book, and over the following decades it was influential in stimulating research into heterochrony, changes in the timing of embryonic development, which had been neglected since Ernst Haeckel's theory that ontogeny recapitulates phylogeny had been largely discredited.\n\n\"Ontogeny and Phylogeny\" is Stephen Jay Gould's first technical book. He wrote that Ernst Mayr had suggested in passing that he write a book on development. Gould stated he \"only began it as a practice run to learn the style of lengthy exposition before embarking on my magnum opus about macroevolution.\" This later work was published in 2002 as \"The Structure of Evolutionary Theory\".\n\nThe first half of the book explores Ernst Haeckel's biogenetic law (recapitulation)—the largely discredited idea that embryonic developmental stages replay the evolutionary transitions of adult forms of an organism's past descendants—and how this idea influenced thinking in biology, theology, and psychology.\n\nThe second half of the book details how modern concepts such as heterochrony (changes in developmental timing) and neoteny (the retardation of developmental expression or growth rates) influence macroevolution (major evolutionary transitions).\n\nThe herpetologist David B. Wake, in \"Paleobiology\", wrote that the topic was \"at once so obviously important and so intrinsically difficult\" that few people would tackle it. The parallelism that Haeckel noted between ontogeny and phylogeny was, Wake observed, a strong argument for evolution, but hardly anyone dared to discuss it. He called the book very good, and predicted that it would set the stage for \"endless research\", but found it also in a way unsatisfying, using \"undigested theory from ecology to \"explain\" what is, as yet, unexplainable. Summing up, Wake calls the book \"erudite, important, provocative, and controversial\", but noted that it could have been much shorter.\n\nThe embryologist Søren Løvtrup, in \"Systematic Zoology\", noted that the book had two objectives, unexceptionably to gain practice, and \"more dubious[ly]\", to show that \"in spite of the collapse of Haeckel's biogenetic law, the subject of parallels between ontogenesis and phylogenesis is still of importance to biology\". In Løvtrup's view, this was because Haeckel's law had been refuted except where evolution had by chance happened to add to the end of development. Gould had little new to report, as people knew half a century earlier that development could be modified at other stages; the book was \"a great disappointment.\" Haeckel could \"of course be of historical interest\" but Gould had chosen not to research Haeckel's influence. Work on \"wrong theories\" represented, Løvtrup wrote, \"a terrible waste of effort and time, and block[ed] further progress.\"\n\nThe anthropologist C. Loring Brace, in \"American Anthropologist\", noted that 2 years earlier, E. O. Wilson's \"Sociobiology\" had with \"woeful ignorance\" strayed into anthropology, and Wilson's \"bright young colleague\" Gould had now done the same thing, possibly making trouble for years to come. Gould was \"a wonderful writer, literate, erudite, gracefully witty, and gifted with the ability to present difficult material in a straightforward and easily readable fashion.\" \nThe bulk of the book was fine, though of no interest to anthropologists. But the tenth chapter, \"Retardation and Neoteny in Human Evolution\", would \"mislead a great many people\" who would be unable to make an informed judgement about its conclusions. Gould \" tums out to be just as much of a teleologist and progressivist as the scholars of previous generations whom he appraises so effectively. He notes that we associate “cute” features with mammals of higher intelligence, features that show 'the common traits of babyhood: relatively large eyes, short face, smooth features, bulbous cranium. The presence of this complex in ad- vanced adult mammals argues for neoteny' (Gould p. 350).\" In Brace's view, \"Gould's main thesis founders between the Scylla of mosaic evolution and the Charybdis of Darwinian theory.\" Brace concluded that Gould had provided \"nothing more useful than the vision that human form can be understood by regarding 'man' as an overgrown retarded child.\"\n\nJames Gorman, in \"The New York Times\", wrote that the book was rich but not easy to read; it was primarily for biologists, with long and precise arguments in technical language; a simpler account of the same topic was to be found in Gould's essay \"Ever Since Darwin\". Gorman called the book scholarly, entertaining and informative, expressed \"with clarity and wit\".\n\nThe zoologist A. J. Cain, in \"Nature\", called it \"a superb analysis of the use of ontogenetic analogy, the controversies over ontogeny and phylogeny, and the classification of the different processes observable in comparing different ontogenies.\" It was a \"massive book\", excellently illustrated with often surprising examples, covering both the history and a functional interpretation of heterochrony. Cain found it refreshing to find someone who had a good word for Ernst Haeckel, and who did not \"treat Charles Bonnet as a stupid monomaniac\" but who brought out the relationship \"between acquired characters and recapitulation in the work of the American neo-Lamarckians\".\n\nThe evolutionary biologists Kenneth McNamara and Michael McKinney stated in 2005 that of all the books that Gould wrote in his career, \"the one with the most impact is probably \"Ontogeny and Phylogeny\" ... to say that this work is a hallmark in this area of evolutionary theory would be an understatement. It proved to be the catalyst for much of the future work in the field, and to a large degree was the inspiration for the modern field of evolutionary developmental biology. Gould's hope was to show that the relationship between ontogeny and phylogeny is fundamental to evolution, and at its heart is a simple premise—that variations in the timing and rate of development provide the raw material upon which natural selection can operate.\"\n\nM. Elizabeth Barnes, in \"The Embryo Project Encyclopedia\", looking back at the book in 2014, writes that it became widely cited in evolutionary and developmental biology, encouraging research on acceleration and retardation of development (forms of heterochrony), and investigation of paedomorphosis in human evolution. Barnes notes that \"along with other work by Gould, such as 'The Spandrels of San Marco and the Panglossian Paradigm' [the book] is often credited for influencing the rise of a biological approach called evolutionary developmental biology or evo-devo, which worked to integrate evolutionary and developmental biology.\"\n\n"}
{"id": "19607864", "url": "https://en.wikipedia.org/wiki?curid=19607864", "title": "Open-notebook science", "text": "Open-notebook science\n\nOpen-notebook science is the practice of making the entire primary record of a research project publicly available online as it is recorded. This involves placing the personal, or laboratory, notebook of the researcher online along with all raw and processed data, and any associated material, as this material is generated. The approach may be summed up by the slogan 'no insider information'. It is the logical extreme of transparent approaches to research and explicitly includes the making available of failed, less significant, and otherwise unpublished experiments; so called 'dark data'. The practice of open notebook science, although not the norm in the academic community, has gained significant recent attention in the research and general media as part of a general trend towards more open approaches in research practice and publishing. Open notebook science can therefore be described as part of a wider open science movement that includes the advocacy and adoption of open access publication, open data, crowdsourcing data, and citizen science. It is inspired in part by the success of open-source software and draws on many of its ideas.\n\nThe term \"open-notebook science\" was first used in 2006 in a blog post by Jean-Claude Bradley, an Associate Professor of Chemistry at Drexel University at the time. Bradley described open-notebook science as follows:\n\n\"A team of groundbreaking scientists at SGC are now sharing their lab notebooks online\".\n\n\n\n\n\nThese are initiatives more open than traditional laboratory notebooks but lacking a key component for full Open Notebook Science. Usually either the notebook is only partially shared or shared with significant delay.\n\nA public laboratory notebook makes it convenient to cite the exact instances of experiments used to support arguments in articles. For example, in a paper on the optimization of a Ugi reaction, three different batches of product are used in the characterization and each spectrum references the specific experiment where each batch was used: EXP099, EXP203 and EXP206. This work was subsequently published in the Journal of Visualized Experiments, demonstrating that the integrity data provenance can be maintained from lab notebook to final publication in a peer-reviewed journal.\n\nWithout further qualifications, Open Notebook Science implies that the research is being reported on an ongoing basis without unreasonable delay or filter. This enables others to understand exactly how research actually happens within a field or a specific research group. Such information could be of value to collaborators, prospective students or future employers. Providing access to selective notebook pages or inserting an embargo period would be inconsistent with the meaning of the term \"Open\" in this context. Unless error corrections, failed experiments and ambiguous results are reported, it will not be possible for an outside observer to understand exactly how science is being done. Terms such as Pseudo or Partial have been used as qualifiers for the sharing of laboratory notebook information in a selective way or with a significant delay.\n\nThe arguments against adopting Open Notebook Science fall mainly into three categories which have differing importance in different fields of science. The primary concern, expressed particularly by biological and medical scientists is that of 'data theft' or 'being scooped'. While the degree to which research groups steal or adapt the results of others remains a subject of debate it is certainly the case that the fear of not being first to publish drives much behaviour, particularly in some fields. This is related to the focus in these fields on the published peer reviewed paper as being the main metric of career success.\n\nThe second argument advanced against Open Notebook Science is that it constitutes prior publication, thus making it impossible to patent and difficult to publish the results in the traditional peer reviewed literature. With respect to patents, publication on the web is clearly classified as disclosure. Therefore, while there may be arguments over the value of patents, and approaches that get around this problem, it is clear that Open Notebook Science is not appropriate for research for which patent protection is an expected and desired outcome. With respect to publication in the peer reviewed literature the case is less clear cut. Most publishers of scientific journals accept material that has previously been presented at a conference or in the form of a preprint. Those publishers that accept material that has been previously published in these forms have generally indicated informally that web publication of data, including Open Notebook Science, falls into this category. Open notebook projects have been successfully published in high impact factor peer reviewed journals but this has not been tested with a wide range of publishers. It is to be expected that those publishers that explicitly exclude these forms of pre-publication will not accept material previously disclosed in an open notebook.\n\nThe final argument relates to the problem of the 'data deluge'. If the current volume of the peer reviewed literature is too large for any one person to manage, then how can anyone be expected to cope with the huge quantity of non peer reviewed material that could potentially be available, especially when some, perhaps most, would be of poor quality? A related argument is that 'my notebook is too specific' for it to be of interest to anyone else. The question of how to discover high quality and relevant material is a related issue. The issue of curation and validating data and methodological quality is a serious issue and one that arguably has relevance beyond Open Notebook Science but is a particular challenge here.\n\nThe Open Notebook Science Challenge, now directed towards reporting solubility measurements in non-aqueous solvent, has received sponsorship from Submeta, Nature and Sigma-Aldrich. The first of ten winners of the contest for December 2008 was Jenny Hale.\n\nLogos can be used on notebooks to indicate the conditions of sharing. Fully open notebooks are marked as \"All Content\" and \"Immediate\" access. Partially open notebooks can be marked as either \"Selected Content\" and/or \"Delayed\".\n\n"}
{"id": "36843243", "url": "https://en.wikipedia.org/wiki?curid=36843243", "title": "Preference test", "text": "Preference test\n\nA preference test is an experiment in which animals are allowed free access to multiple environments which differ in one or more ways. Various aspects of the animal's behaviour can be measured with respect to the alternative environments, such as latency and frequency of entry, duration of time spent, range of activities observed, or relative consumption of a goal object in the environment. These measures can be recorded either by the experimenter or by motion detecting software. Strength of preference can be inferred by the magnitude of the difference in the response, but see \"Advantages and disadvantages\" below. Statistical testing is used to determine whether observed differences in such measures support the conclusion that preference or aversion has occurred. Prior to testing, the animals are usually given the opportunity to explore the environments to habituate and reduce the effects of novelty.\n\nPreference tests can be used to test for preferences of only one characteristic of an environment, e.g. cage colour, or multiple characteristics e.g. a choice between hamster wheel, Habitrail tunnels or additional empty space for extended locomotion.\n\nThe simplest of preference tests offers a choice between two alternatives. This can be done by putting different goal boxes at the ends of the arms of a 'T' shaped maze, or having a chamber divided in into differing halves. A famous example of this simple method is an investigation of the preferences of chickens for different types of wire floor in battery cages. Two types of metal mesh flooring were being used in the 1950s; one type was a large, open mesh using thick wire, the other was a smaller mesh size but the wire was considerably thinner. A prestigious committee, the Brambell Committee, conducting an investigation into farm animal welfare concluded the thicker mesh should be used as this was likely to be more comfortable for the chickens. However, preference tests showed that chickens preferred the thinner wire. Photographs taken from under the cages showed that the thinner mesh offered more points of contact for the feet than the thick mesh, thereby spreading the load on the hens' feet and presumably feeling more comfortable to the birds.\n\nThe number of choices that can be offered is theoretically limitless for some preference tests, e.g., light intensity, cage size, food types; however, the number is often limited by experimental practicalities, current practice (e.g., animal caging systems) or costs. Furthermore, animals usually investigate all areas of the apparatus in a behaviour called \"information gathering\", even those with minor preference, so the more choices that are available may dilute the data on the dominant preference(s).\n\nMost preference tests involve no 'cost' for making a choice, so they do not indicate the strength of an animals motivation or need to obtain the outcome of the choice. For example, if a laboratory mouse is offered three sizes of cage space it may prefer one of them, but this choice does not indicate whether the mouse 'needs' that particular space, or whether it has a relatively slight preference for it. To measure an animals motivation toward a choice one may perform a \"consumer demand test.\" In this sort of test, the choice involves some \"cost\" to the animal, such as physical effort (e.g., lever pressing, weighted door).\n\nPreference tests have been used widely in the study of animal behaviour and motivation, e.g.:\n\n\n\n\n\n\n\n\n\n"}
{"id": "46649163", "url": "https://en.wikipedia.org/wiki?curid=46649163", "title": "Privileged positions of business and science", "text": "Privileged positions of business and science\n\nThe privileged positions of business and science refer to the unique authority that persons in these areas hold in economic, political, and technosocial affairs. Businesses have strong decision-making abilities in the function of society, essentially choosing what technological innovations to develop. Scientists and technologists have valuable knowledge and the ability to pursue the technological innovations they want. They proceed largely without public scrutiny and as if they had the consent of those potentially affected by their discoveries and creations.\n\nBusinesses have considerable power in decision-making processes in business-oriented societies. High level executives make discrete decisions about technological innovation without regard to consequences. Political scientist Edward Woodhouse states that businesses make key economic decisions that include creating jobs, choosing industrial plant and equipment, and deciding which new products to develop and market. Additionally, businesses hold a privileged position related to politics in the sense that they are capable of considerable influence over key public choices. The leadership role that business has in the economy gives executives of large corporations an unusual kind of degree of influence over governmental policy making.\n\nGovernment officials have incentives to carry out businesses' demands. They know that failure of businesses to maintain high employment will upset voters more quickly than anything else. Our economy functions as a chain effect. Businesses that get what they want provide plentiful jobs and stimulate cash flow in the economy. As a result, citizens are happy, and this happiness translates to more trust and faith in the government party. Government officials thus have a higher chance of reelection.\n\nIt is clear that the businesses have the most influence in this chain effect. If a business fails, jobs are lost, citizens are unhappy, and the government party loses trust and their chance of reelection. One example of this was President Bush’s plummeting popularity and resulting defeat by Bill Clinton. Therefore, it would make sense for government officials to reach out to these executives and provide valuable resources to benefit both parties. Power is shifted from government officials to business executives, because these executives have the ability to influence government officials to meet their demands. As a result, a large category of decisions is turned over to businessmen, and taken off the agenda of government. In a way, business managers become the public officials they represent.\n\nThis privilege of power is unique to corporations only. If workers want reform on unfair working conditions or pay, they must form a worker’s union. However, a worker’s union has considerably less influence than a corporation. As opposed to workers’ unions, businesses exercise more control on governmental policy making because workers are expendable – workers need money more urgently than society needs their services, so a business can operate for a lot longer than workers can afford to stay off the job.\n\nWithout the knowledge of scientists and technologists, there would be no possibility of producing new technology and progress in a positive manner. Those who have these types of knowledge and abilities are essentially the \"seed\" of technological innovation. They have the following three privileges:\n\n\nBased on previous scientific knowledge, one or several are able to use it and/or build on it to innovate. Kleinman and Vallas, point out that scientists control the modification, mediation and contest of emerging knowledge. Those with this knowledge can also teach future generations the same to keep this process moving forward. Woodhouse that states that engineers are among the top professionals that promote the new innovations without consequences. This is the foundation of the privileged position of science, that only a select have scientific and/or technological knowledge and capable of substantial feats, good or bad. Engineers and scientists have the powerful position of being able to choose the path of innovation for emerging technology. Technologists and scientists hold a privileged position additionally in that their innovations often have political implications. Woodhouse makes the comparison of innovation to legislative acts, stating that they both establish a framework that will follow for many generations.\n\nBusiness and science are part of an important societal circle specifically relating to the production and implementation of technology. In simple terms, scientists and technologists design and create, while businesses provide the means to create, produce and implement. As Woodhouse claims, it would be a mistake to overlook business executives' and elected officials' dependence on the technologists because innovation is so highly regarded.\n\nBusinesses that rely on scientists and technologists cannot function without them, but have authority over them. Woodhouse claims that the one reason green technology has not emerged faster and stronger is because the brown chemistry formula fulfilled many of the needs of scientists and businessmen. In other words, since brown chemistry was innovated over less toxic and recyclable green technologies, it is shown that business has more authority over that of technologists and scientists. Woodhouse points out that there has been considerable delay on a promising area of green chemistry: supercritical fluids. The opinions range from targeting the culprit as the complexity of the equipment, to maintenance difficulties, to the equipment costs. This is an example of the interconnection of business and science, and how technology can be slowed by technology and/or business (and vice versa). Woodhouse also states that twentieth-century chemists, chemical engineers, and chemical industry executives made central decisions which considerably modeled society's experiences with chemicals. The chemical industry, which includes scientists, technologists, and businessmen, uniquely controls what happens or what can happen to large number of people who are exposed to chemicals.\n\nBusiness and science are considered of utmost importance in current developed countries. However, one working without the other has the potential to cause major issues. Woodhouse points out what the vice president of Shaw Carpets explained at a congressional hearing: that the V.P. is the person whose job it is to tell the chemists and chemical engineers that it is possible to produce carpets using \"green\" processes, because the students had not learned this from their respective university curricula. There can be many positive outcomes of the unison operation of business and science, and it is vital that this does occur in order to prevent major problems and unintended consequences.\n\n\n"}
{"id": "57418849", "url": "https://en.wikipedia.org/wiki?curid=57418849", "title": "Probe tip", "text": "Probe tip\n\nA probe tip in scanning microscopy literally is a very sharp piece of metal or non-metal, like a sewing needle with a point at one end with nano or sub-nanometer order of dimension. It can interact with up to one molecule or atom of a given surface of a sample that can reveal authentic properties of the surface such as morphology, topography, mapping and electrical properties of a single atom or molecule on the surface of the sample.\n\nThe proliferation of the thrust in probe-based tools began after the invention of Scanning tunneling microscope (STM) and Atomic force microscopy (AFM) (collectively called Scanning probe microscopy-SPM) by Gerd Binnig and Heinrich Rohrer at the IBM Zurich research laboratory in 1982. It opened a new era for probing nanoscale world of individual atoms and molecules as well as surface science due to their unprecedented capability of characterizing a wide range of unique properties such as mechanical, chemical, magnetic and optical functionalities of various samples at nanometer-scale resolution in vacuum, ambient and fluid environment. The utilization of sharp probe tips enabled us to see the inside the microscopic world from within the macroscopic world. The increasing demand for sub-nanometer probe tips attributes to their robustness and versatile applicability because of their direct application to the numerous fields of science that includes nanolithography, nanoelectronics, biosensor, Electrochemistry, Semiconductor, Micromachining and biological cells studies. The significant number of applications for the topographic surface characterization of the materials and biological specimen in various fields of science made researchers and scientists that it is imperative for reproducible mass production of probe tip with sharp apex.\n\nProbe tip size and shape in microscopy are important parameters providing direct connection between resolution and imaging quality. Resolution and imaging mechanism may depend on geometry (length, width, shape, aspect ratio, and tip apex radius) and composition (material properties) of tip and surface being probed. Tip size, shape and reproducibility are extremely important to monitor and detect the interaction between the surfaces.\n\nHere, we describe fabrication, characterization and application of sharp tips. A wide range of tip fabrication techniques including cutting, grinding, pulling, beam deposition, ion milling, controlled crashing, field emission, field evaporation, fracture and electrochemical etching/polishing are discussed. Both limitations and advantages are also provided for various tip fabrication method. We also describe history and development, working principles, characterization and applications with recent advancement of sharp tips.\n\nThe discovery of a sharp probe tip has always been of significant interest among the researchers considering its importance in the material, life and biological sciences for mapping the surface structure and material properties at molecular or atomic dimension. The history of tip can be tracked long back in nineteenth century during the invention of Phonautograph in 1859. Phonautograph is the predecessor of modern gramophone. It was invented by Scott and Koenig. It consisted of a parchment diaphragm with an attached stylus (sort of a pen-holder), along with a Hog’s hair, which was used to trace a wavy line on a lamp-blacked surface [Musical mirror by Bryceson, Richard, Jan. 1929-Dec.1930]. In the later development gramophone came out where along with other replacements, the Hog’s hair was replaced by a needle to reproduce sound [Musical mirror by Bryceson, Richard, Jan. 1929-Dec.1930]. In 1940, a pantograph was built utilizing a shielded probe and adjustable tip. A stylus was free to slide vertically to be in contact with the paper. In 1948, a tip was employed in probe circuit to measure peak voltage. The fabrication of electrochemically etched sharp tungsten, copper, nickel and molybdenum tip was reported by Muller in 1937. The revolution for sharp tips occurred in producing a good and various kind of tip with different shape, size, aspect ratio composed of tungsten wire, silicon, diamond, Carbon nanotube etc. occurred with Si-based circuit technologies. This allowed tips for numerous applications in the broad spectrum of nanotechnological fields. Following STM invention came Atomic force microscopy (AFM) by Gerd Binnig, Calvin F. Quate, and Christoph Gerber in 1986. In their instrument they used diamond broken piece as tip sticking it to a hand-cut gold foil cantilever. Focused ion and electron beam technique for the fabrication of strong, stable, reproducible SiN pyramidal tip with 1.0 μm length and 0.1 μm diameter was reported by Russell in 1992. Ground breaking advancement came through the introduction of microfabrication methods for the fabrication of precised conical or pyramidal silicon and silicon nitride tip. Later, numerous research experiments were explored to fabricate comparatively less expensive and robust tungsten tip production and characterization technique to attain less than 50 nm radius of curvature.\n\nThe new horizon in the field of fabrication of probe tip was revealed to the experts when carbon nanotube which is basically about 1 nm cylindrical shells of graphene was introduced. The use of single wall Carbon nanotubes is less vulnerable to breaking or crushing during imaging due to their flexibility. Probe tip made up of Carbon nanotubes can be efficiently used to get high-resolution images of both soft and weakly absorbed Biomolecules like DNA on surface at molecular resolution.\n\nMultifunctional hydrogel nano-probe technique uncovered new scope initiating a completely new concept to the fabrication of tip and their extended ease of applicability to inorganic and biological samples in both air and liquid. The biggest advantage of this mechanical method is that the tip can be made in different shape e.g. hemispherical, embedded spherical, pyramidal, and distorted pyramidal etc. with diameter ranging from 10 nm – 1000 nm for applications including Topography or functional imaging, force spectroscopy on soft matter, biological, chemical and physical Sensors. Table 1. Summarizes various fabrication, material and application of tips.\n\nThe tip itself does not have any working principle for imaging, but depending on the instrumentation, mode of application, and the nature of the sample under investigation, the probe tip may follow different principle to image the surface of the sample. For example, when a tip is integrated with STM, it measures the tunneling current that arises from the interaction between the sample and the tip. In AFM, short-ranged force deflection during the raster scan by tip across the surface is measured. A conductive tip is essential for the STM instrumentation whereas AFM can use conductive and non-conductive prob tip. Although probe tip is used in various techniques with different principle, for STM and AFM coupled with probe tip is discussed in detail.\n\nSomewhat, the name implies STM utilizes tunneling charge transfer principle from tip to surface or vice versa thereby recording the current response. This concept originates from particle in a box concept, that is, if potential energy for a particle is small, electron may be found outside of potential well which is a classically forbidden region. This phenomenon is called tunneling.\n\nExpression derived from Schrodinger equation for transmission charge transfer probability is as follow:\n\nformula_1\n\nwhere\n\nNon-conductive nano-scale tips are widely used for AFM measurements. For non-conducting tip, surface forces acting on the tip/cantilever, are responsible for deflection or attraction of tip. These attractive or repulsive forces are used for surface topology, chemical specifications, magnetic and electronic properties. The distance dependent forces between substrate surface and tip are responsible for imaging in AFM. These interactions include are van der Waals forces, capillary forces, electrostatic forces, Casimir forces and solvation forces. One unique repulsion force is Pauli Exclusion repulsive force is responsible for single atom imaging as in references and Figures 10 & 11 (contact region in Fig. 1).\nTip fabrication techniques fall generally in any two classifications: mechanical and physicochemical. In the early stage of development of probe tips, mechanical procedures were popular because of ease of fabrication. One of mechanical method for the fabrication of hydrogel tip will be discussed in detail.\n\nA few reported mechanical methods to fabricate tips include cutting grinding and pulling. For example, cutting a wire at certain angles with razor blade or wire cutter or scissor. Another mechanical method for tip preparation is fragmentation of bulk pieces into small pointy pieces. Grinding a metal wire/rod into a sharp tip was also a method used. These mechanical procedures usually leave rugged surface with many tiny asperities protruding from the apex which lead to atomic resolution on flat surface. However, irregular shape and large macroscopic radius of curvature result in poor reproducibility and decreased stability especially for probing rough surfaces. Another main disadvantage of making probe by this method is it yields many mini tips which lead to many different signals yielding error in imaging. Cutting, grinding and pulling procedures can only be adapted for metallic tips like W, Ag, Pt, Ir, Pt-Ir and gold. Non-metallic tips cannot be fabricated by these methods.\n\nOn other hand a sophisticated mechanical method for tip fabrication is based on hydro-gel method. This method is based on bottom-up strategy to make probe tips by molecular self-assembly process. First, cantilever is formed in mold by curing pre-polymer solution then it is brought into contact with mold of tip which contains pre-polymer solution. The polymer is cured with ultraviolet light which helps firmly attachment of cantilever with probe. This fabrication method is shown in Fig. 2.\n\nPhysiochemical procedures are fabrication methods of choice these days which yield extremely sharp and symmetric tips with more reproducibility compared to mechanical fabrication-based tips. Among physicochemical method, the electrochemical etching method is one of the most popular method. Etching is two or more steps procedure. The “zone electropolishing” is the second step which further sharpens the tip in a very controlled manner. Other physicochemical methods include chemical vapor deposition, electron beam deposition onto pre-existing tips. Other tip fabrication methods include field ion microscopy and ion milling. In field ion microscopy techniques, consecutive field evaporation of single atoms yields specific atomic configuration at probe tip which yields very high resolution.\n\nElectrochemical etching is one of the easiest, inexpensive, most practical, reliable and most widely accepted metallic probe tip fabrication method with desired quality and reproducibility. Three commonly used electrochemical etching for tungsten tip fabrication are: single lamella drop-off methods, double lamella drop-off method and submerged method. Various cone shape tips can be fabricated by this method by minor changes in the experimental setup. An DC potential between tip and metallic electrode (usually W wire) immersed in solution (Figure 3 a-c). Electrochemical reactions at cathode and anode in basic solutions (2M KOH or 2M NaOH) are usually used. The overall etching process involved is written here:\n\nAnode;\n\n<chem>W (s) + 8OH- -> WO4 + 4H2O + 6e- (E= 1.05V)</chem>\n\nCathode:\n\n<chem>6H2O + 6e- -> 3H2 + 6OH- (E=-2.48V)</chem>\n\nOverall:\n\n<chem>W (s) + 2OH- -> WO4^2- + 2H2O (l) + 6e- + 3H2 (g) (E= -1.43V)</chem>\n\nHere, all the potentials are reported vs. SHE.\n\nSchematics of fabrication method of probe tip through electrochemical etching method is shown in Fig. 3.\n\nExperimental setup for Electrochemical etching is shown in Fig. 3a.\n\nDifferent tips fabricated from etching method with different tip radius and angle is illustrated in Fig. 3(b-e).\n\nIn electrochemical etching process, W is etched at liquid, solid and air interface (due to surface tension), as shown in Fig. 3. Etching is called static if W wire is kept stationary. Once the tip is etched, lower part falls due to the lower tensile strength than the weight of lower part of wire. The irregular shape is produced by the shifting of meniscus. However, slow etching rate can produce regular tips which controlling current slowly through electrochemical cell. The dynamic etching involves slowly pulling up the wire from solution or sometimes it is moved up and down (oscillating wire) producing smooth tips.\n\nIn this method a metal wire is vertically etched reducing the diameter from 0.25 mm ~ 20 nm. Schematic diagram for probe tip fabrication with submerged electrochemical etching method is illustrated in Fig 4. These tips can be used for high quality STM images.\nIn double lamella method the lower part of metal is etched away, and upper part of tip was not etched further. Further etching of the upper part of wire is prevented by covering it with a polymer coating. This method is usually limited to laboratory fabrication. Double lamella method schematic is shown in Fig. 5.\nTransitions metals like Cu, Au and Ag adsorbs single molecules linearly on their surface due to weak van der Waals forces. This linear projection of single molecule allows interaction of terminal atom of tip with atom of substrate resulting in Pauli repulsion for single molecule or atom mapping studies. Gases deposition on tip is carried out in an ultrahigh vacuum (5 x 10 mbar) chamber at a low temperature (10K). Deposition of Xe, Kr, NO, CH or CO on tip have been successfully prepared and used for imaging studies. However, these tips preparation rely on attachment of single atom or molecule on tip and resulting atomic structure of tip is not known exactly. Probability of attachment of simple molecule on metals surface is very tedious and required great skills. Therefore, this method is not ubiquitous and not many laws are able to perform these experiments.\n\nSharp tips used in SPM are fragile and prone to damage and wear and tear easily under high working load. Diamond is considered the best option to address this issue. Diamond tips for SPM application are fabricated by fracture of bulk diamond, grinding and polishing diamond. But, these methods result in considerable loss of diamond. Other strategy to prevent this loss is coating of Silicone tips with thin diamond film. These thin films are usually deposited by CVD. In CVD, diamond is deposited directly on silicon or W cantilever. A schematic diagram for chemical vapor deposition set up is shown in Fig. 6. In this method, flow of methane and hydrogen gas is maintained in such a way that pressure inside chamber is maintained at 40Torr. CH and H are dissociated at elevated temperature of 2100 °C with the help of Ta filament. Nucleation sites are created on the tip of cantilever. Once CVD is complete, CH flow is stopped and chamber is cooled under flow of H. Schematics of CVD set up for diamond tip fabrication for AFM application is in Fig. 6.\n\nIn RIE method, first a grove or structure is made on a substrate followed by deposition of a desired material in that template. Once tip is formed, templating structure is etched off leaving tip and cantilever. A schematic for diamond tip fabrication on silicon wafers through this method has been described in Fig. 7\n\nFocused ion beam milling milling is a sharpening method for probe tips in SPM. In this method, first a blunt tip is fabricated by other methods, for example, pyramid mold can be used to fabricate pyramidal tip, CVD method or any other etching method. Then, this tip is sharpened by FIB milling as shown in Fig. 8. The focused ion beam diameter is controlled through a programmable aperture which directly correlates with tip diameter. \nThis method is used to attach carbon nanotubes on cantilever or blunt tip. A strong adhesive (such as soft acrylic glue) is used to bind CNT with silicon cantilever. CNT are robust, stiff and increase durability of probe tip and can be used for both contact and tapping mode.\n\nElectrochemically etched tips are usually covered with contaminant on surface which cannot be removed simply by rinsing in water, acetone or ethanol. Some oxides layers on metallic tips, especially on tungsten, need to be removed by post fabrication treatment.\n\nTo clean W sharp tips, it is highly desirable to remove contaminant and oxide layer. In this method a tip is heated in UHV chamber at elevated temperature which desorb contaminated layer. Reaction detail is shown below.\n\n2WO + W → 3WO ↑\n\nWO → W (sublimation at formula_51075K)\n\nAs at elevated temperature, trioxides of W are converted to WO which sublimates around 1075K and cleaned metallic W surface left behind. Additional advantage provided by annealing is healing of crystallographic defects produced by fabrication and it also smoothens the tip surface.\n\nIn HF cleaning method, freshly prepared tip is dipped in 15% concentrated hydrofluoric acid which dissolves oxides of W for 10~30 s.\n\nIn this method, argon ions are directed to the tip surface to remove contaminant layer by sputtering. Tip is either rotated in a flux of argon ions at certain angle in a way that this beam hits on the apex. The bombardment of ions on the tip depletes the contaminants and also results in reduction of radius of tip. Bombardment time needs to be finely tuned with respect to shape of tip. Sometimes, a short annealing is required after ion milling.\n\nThis method is very similar to ion milling but, in this procedure, UHV chamber is filled with neon at a pressure of 10 mbar. When negative voltage is applied on the tip, strong electric field (produced by tip under negative potential) will ionize neon gas and these positively charged ions are accelerated back to tip and causing sputtering at the tip. The sputtering removes contaminants and some atoms from tip which, like ion milling reduces apex radius. By, changing the field strength, one can tune radius of tip to 20 nm.\n\nThe surface of the Silicon based tips cannot be easily controlled because they usually carry silanol group. Si surface is hydrophilic and can be contaminated easily by environmental. Another disadvantage of Si tips is wear and tear of tip. It is important to coat Si tip to prevent tip deteriorations. The tip coating may also enhance image quality. Following types of coatings are employed for Si tips. First an adhesive layer is pasted (usually chromium layer on 5 nm thick titanium) and then gold is deposited by vapor deposition (40-100 nm or less). Sometimes, the coating layering reduces tunneling current detection capability of probe tips.\n\nThe most important aspect of a probe tip is imaging the surfaces efficiently at nanometer. Some concerns involving credibility of the imaging or measurement of sample arises when the shape of tip is not determined accurately. For example, when an unknown tip is used to measure linewidth pattern or other high aspect ratio feature of a surface. There may remain some confusion for the determination of the contribution of tip and the sample in the acquired image. Consequently, it is important to fully and accurately characterize the tips. Probe tips can be characterized for their shape, size, sharpness, bluntness, aspect ratio, radius of curvature, geometry and composition using many advanced instrumental techniques. For example, electron field emission measurement, scanning electron microscopy (SEM), transmission electron microscopy (TEM), scanning tunneling spectroscopy as well as more easily accessible optical microscope. In some cases, optical microscopy cannot provide exact measurements for small tips in nanoscale due to resolution limitation of the optical microscopy.\n\nIn electron field emission current measurement method, a high voltage is applied between tip and another electrode followed by measuring field emission current employing Fowler-Nordheim curves formula_6. Large fields-emission current measurements may indicate that the tip is sharp and low field-emission current indicates that the tip is blunt, molten or mechanically damaged. A minimum voltage is essential to facilitate the release of electrons from the surface of tip which in turn indirectly is used to obtain the tip curvature. The downside of this method to the several advantages is that the high electric field required for producing strong electric force that can melt the apex of the tip or might change crystallographic tip nature.\n\nThe size and shape of the tip can be obtained by scanning electron microscopy and transmission electron microscopy measurements. In addition, TEM images are helpful to detect any layer of insulating materials on the surface of the tip as well as to estimate the size of the layer. These oxides are formed gradually on the surface of tip the right after fabrication due to the oxidation of metallic tip by reacting with the O present in the surrounding atmosphere. SEM has a resolution limitation of below 4 nm, TEM may be needed to observe even a single atom theoretically and practically. Tip grain down to 1-3 nm or thin polycrystalline oxides or carbon or graphite layers at the tip apex are routinely measured using TEM. The orientation of tip crystal i.e. the angle between the tip plane in the single-crystal and the tip normal can be estimated.\n\nIn the past, optical microscope has been only used to investigate if the tip is bent microscale imaging at many microscales. This is because the resolution limitation of an optical microscope is about 200 nm. Imaging software including ImageJ allows determination of the curvature, and aspect ratio of the tip. One drawback of this method is that it renders an image of tip which is a object due to the uncertainty in the nanoscale dimension. This problem can be resolved taking images of tip multiple times followed by putting them together into image by confocal microscope with some fluorescent material coating on the tip. Also, it is a time-consuming process considering the necessity of monitoring the wear or damage or degradation of tip by the collision with the surface during scanning the surface after each scan.\n\nThe scanning tunneling spectroscopy (STS) is spectroscopic form of STM in which spectroscopic data based on curve is obtained to analyze the existence of any oxides or impurities on the tip by monitoring the linearity of the curve which represents metallic tunnel junction. Generally, cure is non-linear and hence, the tip has a gap like shape around zero bias voltage for oxidized or impure tip whereas the opposite is observed for sharp pure un-oxidized tip.\n\nIn Auger electron spectroscopy (AES), any oxides present on the tip surface is sputtered out during in-depth analysis with argon ion beam generated by differentially pumped ion pump followed by comparing the sputtering rate of the oxide with experimental sputtering yields. These Auger measurements may estimate the nature of oxides because of the surface contamination and/or composition can be revealed and in some cases thickness of the oxide layer down to 1-3 nm can be estimated. X-ray photoelectron spectroscopy also performs similar characterization for the chemical and surface composition by providing information on the binding energy of the surface elements.\n\nOverall, the aforementioned characterization methods of tips can be categorized in three major classes. They are:\n\n\nProbes tips have a wide variety of applications in different fields of science and technology. One of the major areas where probe tips are used is for application in SPM i.e., STM and AFM. For example, carbon nanotube tips in conjunction with AFM provides an excellent tool for surface characterization in the nanometer realm. CNT tips are also used in tapping-mode Scanning Force Microscopy (SFM), which is a technique where a tip taps a surface by a cantilever driven near resonant frequency of the cantilever. The CNT probe tips fabricated using CVD technique can be used for imaging of biological macromolecules, semiconductor and chemical structure. For example, it is possible to obtain intermittent AFM contact image of IgM macromolecules with excellent resolution using single CNT tip. Individual CNT tips can be used for high resolution imaging of protein molecules.\n\nIn another work, multiwall carbon nanotube (MWCNT) and Single wall carbon nanotube (SWCNT) tips were used to image amyloid β (1-40) derived protofibrils and fibrils by tapping mode AFM. Functionalized probes can be used in Chemical Force Microscopy (CFM) to measure intermolecular forces and map chemical functionality. Functionalized SWCNT probes can be used for chemically sensitive imaging with high lateral resolution and to study binding energy in chemical and biological system. Probe tips that have been functionalized with either hydrophobic or hydrophilic molecules can be used to measure the adhesive interaction between hydrophobic-hydrophobic, hydrophobic-hydrophilic, and hydrophilic-hydrophilic molecules. From these adhesive interactions the friction image of patterned sample surface can be found. Probe tips used in force microscopy can provide imaging of structure and dynamics of adsorbate at the nanometer scale. Self-Assembled Functionalized Organic Thiols onto the surface of Au coated SiN probe tips has been used to study the interaction between molecular groups. Again, carbon nanotube probe tips in conjunction with AFM can be used for probing crevices that occur in microelectronic circuits with improved lateral resolution. Functionality modified probe tips has been to measure the binding force between single protein-ligand pairs. Probe tips has been in used in tapping mode technique to provide information about the elastic properties of materials. Probe tips are also used in mass spectrometer. Enzymatically active probe tips have been used for the enzymatic degradation of analyte. They have also been used as devices to introduce sample into the mass spectrophotometer. For example, trypsin-activated gold (Au/trypsin) probe tips can be used for the peptide mapping of the hen egg lysozyme.\n\nAtomically sharp probe tips can be used for imaging a single atom in molecule. An example of visualizing single atoms in water cluster can be seen in Fig. 10. By visualizing single atoms in molecules present on a surface, the scientists can determine bond length, bond order and discrepancies, if any, in conjugation which was supposed to be impossible by experimental work. Fig. 9 shows experimentally determined bond order in poly aromatic compound which was thought to be very hard in past.\n"}
{"id": "23047", "url": "https://en.wikipedia.org/wiki?curid=23047", "title": "Pseudoscience", "text": "Pseudoscience\n\nPseudoscience consists of statements, beliefs, or practices that are claimed to be both scientific and factual, but are incompatible with the scientific method. Pseudoscience is often characterized by contradictory, exaggerated or unfalsifiable claims; reliance on confirmation bias rather than rigorous attempts at refutation; lack of openness to evaluation by other experts; and absence of systematic practices when developing theories, and continued adherence long after they have been experimentally discredited. The term \"pseudoscience\" is considered pejorative because it suggests something is being presented as science inaccurately or even deceptively. Those described as practicing or advocating pseudoscience often dispute the characterization.\n\nThe demarcation between science and pseudoscience has philosophical and scientific implications. Differentiating science from pseudoscience has practical implications in the case of health care, expert testimony, environmental policies, and science education. Distinguishing scientific facts and theories from pseudoscientific beliefs, such as those found in astrology, alchemy, alternative medicine, occult beliefs, and creation science, is part of science education and scientific literacy.\n\nPseudoscience can cause negative consequences in the real world. Antivaccine activists present pseudoscientific studies that falsely call into question the safety of vaccines. Homeopathic remedies with no active ingredients have been promoted as treatment for deadly diseases.\n\nThe word \"pseudoscience\" is derived from the Greek root \"pseudo\" meaning false and the English word \"science\", from the Latin word \"scientia\", meaning \"knowledge\". Although the term has been in use since at least the late 18th century (e.g. in 1796 by James Pettit Andrews in reference to alchemy) the concept of pseudoscience as distinct from real or proper science seems to have become more widespread during the mid-19th century. Among the earliest uses of \"pseudo-science\" was in an 1844 article in the \"Northern Journal of Medicine\", issue 387:\nAn earlier use of the term was in 1843 by the French physiologist François Magendie. During the 20th century, the word was used pejoratively to describe explanations of phenomena which were claimed to be scientific, but which were not in fact supported by reliable experimental evidence. From time-to-time, though, the usage of the word occurred in a more formal, technical manner in response to a perceived threat to individual and institutional security in a social and cultural setting.\n\nPhilosophers classify types of knowledge. In English, the word \"science\" is used to indicate specifically the natural sciences and related fields, which are called the social sciences.\n\nDifferent philosophers of science may disagree on the exact limits – for example, is mathematics a formal science that is closer to the empirical ones, or is pure mathematics closer to the philosophical study of logic and therefore not a science? – but all agree that all of the ideas that are not scientific are non-scientific. The large category of non-science includes all matters outside the natural and social sciences, such as the study of history, metaphysics, religion, art, and the humanities.\n\nDividing the category again, unscientific claims are a subset of the large category of non-scientific claims. This category specifically includes all matters that are directly opposed to good science. Un-science includes both bad science (such as an error made in a good-faith attempt at learning something about the natural world) and pseudoscience. Thus pseudoscience is a subset of un-science, and un-science, in turn, is subset of non-science.\n\nPseudoscience is differentiated from science because – although it claims to be science – pseudoscience does not adhere to accepted scientific standards, such as the scientific method, falsifiability of claims, and Mertonian norms.\n\nA number of basic principles are accepted by scientists as standards for determining whether a body of knowledge, method, or practice is scientific. Experimental results should be reproducible and verified by other researchers. These principles are intended to ensure experiments can be reproduced measurably given the same conditions, allowing further investigation to determine whether a hypothesis or theory related to given phenomena is valid and reliable. Standards require the scientific method to be applied throughout, and bias to be controlled for or eliminated through randomization, fair sampling procedures, blinding of studies, and other methods. All gathered data, including the experimental or environmental conditions, are expected to be documented for scrutiny and made available for peer review, allowing further experiments or studies to be conducted to confirm or falsify results. Statistical quantification of significance, confidence, and error are also important tools for the scientific method.\n\nDuring the mid-20th century, the philosopher Karl Popper emphasized the criterion of falsifiability to distinguish science from nonscience. Statements, hypotheses, or theories have falsifiability or refutability if there is the inherent possibility that they can be proven false. That is, if it is possible to conceive of an observation or an argument which negates them. Popper used astrology and psychoanalysis as examples of pseudoscience and Einstein's theory of relativity as an example of science. He subdivided nonscience into philosophical, mathematical, mythological, religious and metaphysical formulations on one hand, and pseudoscientific formulations on the other, though he did not provide clear criteria for the differences.\n\nAnother example which shows the distinct need for a claim to be falsifiable was stated in Carl Sagan's publication \"The Demon-Haunted World\" when he discusses an invisible dragon that he has in his garage. The point is made that there is no physical test to refute the claim of the presence of this dragon. No matter what test you think you can devise, there is then a reason why this does not apply to the invisible dragon, so one can never prove that the initial claim is wrong. Sagan concludes; \"Now, what's the difference between an invisible, incorporeal, floating dragon who spits heatless fire and no dragon at all?\". He states that \"your inability to invalidate my hypothesis is not at all the same thing as proving it true\", once again explaining that even if such a claim were true, it would be outside the realm of scientific inquiry.\n\nDuring 1942, Robert K. Merton identified a set of five \"norms\" which he characterized as what makes a real science. If any of the norms were violated, Merton considered the enterprise to be nonscience. These are not broadly accepted by the scientific community. His norms were:\n\nDuring 1978, Paul Thagard proposed that pseudoscience is primarily distinguishable from science when it is less progressive than alternative theories over a long period of time, and its proponents fail to acknowledge or address problems with the theory. During 1983, Mario Bunge has suggested the categories of \"belief fields\" and \"research fields\" to help distinguish between pseudoscience and science, where the former is primarily personal and subjective and the latter involves a certain systematic method.\n\nPhilosophers of science such as Paul Feyerabend argued that a distinction between science and nonscience is neither possible nor desirable. Among the issues which can make the distinction difficult is variable rates of evolution among the theories and methods of science in response to new data.\n\nLarry Laudan has suggested pseudoscience has no scientific meaning and is mostly used to describe our emotions: \"If we would stand up and be counted on the side of reason, we ought to drop terms like 'pseudo-science' and 'unscientific' from our vocabulary; they are just hollow phrases which do only emotive work for us\". Likewise, Richard McNally states, \"The term 'pseudoscience' has become little more than an inflammatory buzzword for quickly dismissing one's opponents in media sound-bites\" and \"When therapeutic entrepreneurs make claims on behalf of their interventions, we should not waste our time trying to determine whether their interventions qualify as pseudoscientific. Rather, we should ask them: How do you know that your intervention works? What is your evidence?\"\n\nFor philosophers Silvio Funtowicz and Jerome R. Ravetz \"pseudo-science may be defined as one where the uncertainty of its inputs must be suppressed, lest they render its outputs totally indeterminate\". The definition, in the book \"Uncertainty and quality in science for policy\" (p. 54), alludes to the loss of craft skills in handling quantitative information, and to the bad practice of achieving precision in prediction (inference) only at the expenses of ignoring uncertainty in the input which was used to formulate the prediction. This use of the term is common among practitioners of post-normal science. Understood in this way, pseudoscience can be fought using good practices to assesses uncertainty in quantitative information, such as NUSAP and – in the case of mathematical modelling – sensitivity auditing.\n\nThe history of pseudoscience is the study of pseudoscientific theories over time. A pseudoscience is a set of ideas that presents itself as science, while it does not meet the criteria to be properly called such.\n\nDistinguishing between proper science and pseudoscience is sometimes difficult. One proposal for demarcation between the two is the falsification criterion, attributed most notably to the philosopher Karl Popper. In the history of science and the history of pseudoscience it can be especially difficult to separate the two, because some sciences developed from pseudosciences. An example of this transformation is the science chemistry, which traces its origins to pseudoscientific or pre-scientific study of alchemy.\n\nThe vast diversity in pseudosciences further complicates the history of science. Some modern pseudosciences, such as astrology and acupuncture, originated before the scientific era. Others developed as part of an ideology, such as Lysenkoism, or as a response to perceived threats to an ideology. Examples of this ideological process are creation science and intelligent design, which were developed in response to the scientific theory of evolution.\n\nA topic, practice, or body of knowledge might reasonably be termed pseudoscientific when it is presented as consistent with the norms of scientific research, but it demonstrably fails to meet these norms.\n\nKarl Popper stated it is insufficient to distinguish science from pseudoscience, or from metaphysics (such as the philosophical question of what existence means), by the criterion of rigorous adherence to the empirical method, which is essentially inductive, based on observation or experimentation. He proposed a method to distinguish between genuine empirical, nonempirical or even pseudoempirical methods. The latter case was exemplified by astrology, which appeals to observation and experimentation. While it had astonishing empirical evidence based on observation, on horoscopes and biographies, it crucially failed to use acceptable scientific standards. Popper proposed falsifiability as an important criterion in distinguishing science from pseudoscience.\n\nTo demonstrate this point, Popper gave two cases of human behavior and typical explanations from Sigmund Freud and Alfred Adler's theories: \"that of a man who pushes a child into the water with the intention of drowning it; and that of a man who sacrifices his life in an attempt to save the child.\" From Freud's perspective, the first man would have suffered from psychological repression, probably originating from an Oedipus complex, whereas the second man had attained sublimation. From Adler's perspective, the first and second man suffered from feelings of inferiority and had to prove himself, which drove him to commit the crime or, in the second case, drove him to rescue the child. Popper was not able to find any counterexamples of human behavior in which the behavior could not be explained in the terms of Adler's or Freud's theory. Popper argued it was that the observation always fitted or confirmed the theory which, rather than being its strength, was actually its weakness.\n\nIn contrast, Popper gave the example of Einstein's gravitational theory, which predicted \"light must be attracted by heavy bodies (such as the Sun), precisely as material bodies were attracted.\" Following from this, stars closer to the Sun would appear to have moved a small distance away from the Sun, and away from each other. This prediction was particularly striking to Popper because it involved considerable risk. The brightness of the Sun prevented this effect from being observed under normal circumstances, so photographs had to be taken during an eclipse and compared to photographs taken at night. Popper states, \"If observation shows that the predicted effect is definitely absent, then the theory is simply refuted.\" Popper summed up his criterion for the scientific status of a theory as depending on its falsifiability, refutability, or testability.\n\nPaul R. Thagard used astrology as a case study to distinguish science from pseudoscience and proposed principles and criteria to delineate them. First, astrology has not progressed in that it has not been updated nor added any explanatory power since Ptolemy. Second, it has ignored outstanding problems such as the precession of equinoxes in astronomy. Third, alternative theories of personality and behavior have grown progressively to encompass explanations of phenomena which astrology statically attributes to heavenly forces. Fourth, astrologers have remained uninterested in furthering the theory to deal with outstanding problems or in critically evaluating the theory in relation to other theories. Thagard intended this criterion to be extended to areas other than astrology. He believed it would delineate as pseudoscientific such practices as witchcraft and pyramidology, while leaving physics, chemistry and biology in the realm of science. Biorhythms, which like astrology relied uncritically on birth dates, did not meet the criterion of pseudoscience at the time because there were no alternative explanations for the same observations. The use of this criterion has the consequence that a theory can be scientific at one time and pseudoscientific at a later time.\n\nScience is also distinguishable from revelation, theology, or spirituality in that it offers insight into the physical world obtained by empirical research and testing. The most notable disputes concern the evolution of living organisms, the idea of common descent, the geologic history of the Earth, the formation of the solar system, and the origin of the universe. Systems of belief that derive from divine or inspired knowledge are not considered pseudoscience if they do not claim either to be scientific or to overturn well-established science. Moreover, some specific religious claims, such as the power of intercessory prayer to heal the sick, although they may be based on untestable beliefs, can be tested by the scientific method.\n\nSome statements and common beliefs of popular science may not meet the criteria of science. \"Pop\" science may blur the divide between science and pseudoscience among the general public, and may also involve science fiction. Indeed, pop science is disseminated to, and can also easily emanate from, persons not accountable to scientific methodology and expert peer review.\n\nIf the claims of a given field can be tested experimentally and standards are upheld, it is not pseudoscience, however odd, astonishing, or counterintuitive the claims are. If claims made are inconsistent with existing experimental results or established theory, but the method is sound, caution should be used, since science consists of testing hypotheses which may turn out to be false. In such a case, the work may be better described as ideas that are \"not yet generally accepted\". \"Protoscience\" is a term sometimes used to describe a hypothesis that has not yet been tested adequately by the scientific method, but which is otherwise consistent with existing science or which, where inconsistent, offers reasonable account of the inconsistency. It may also describe the transition from a body of practical knowledge into a scientific field.\n\n\n\n\n\n\n\nA large percentage of the United States population lacks scientific literacy, not adequately understanding scientific principles and method. In the \"Journal of College Science Teaching\", Art Hobson writes, \"Pseudoscientific beliefs are surprisingly widespread in our culture even among public school science teachers and newspaper editors, and are closely related to scientific illiteracy.\" However, a 10,000-student study in the same journal concluded there was no strong correlation between science knowledge and belief in pseudoscience.\n\nIn his book \"The Demon-Haunted World\" Carl Sagan discusses the government of China and the Chinese Communist Party's concern about Western pseudoscience developments and certain ancient Chinese practices in China. He sees pseudoscience occurring in the United States as part of a worldwide trend and suggests its causes, dangers, diagnosis and treatment may be universal.\n\nDuring 2006, the U.S. National Science Foundation (NSF) issued an executive summary of a paper on science and engineering which briefly discussed the prevalence of pseudoscience in modern times. It said, \"belief in pseudoscience is widespread\" and, referencing a Gallup Poll, stated that belief in the 10 commonly believed examples of paranormal phenomena listed in the poll were \"pseudoscientific beliefs\".\n\nThe items were \"extrasensory perception (ESP), that houses can be haunted, ghosts, telepathy, clairvoyance, astrology, that people can communicate mentally with someone who has died, witches, reincarnation, and channelling\". Such beliefs in pseudoscience represent a lack of knowledge of how science works. The scientific community may attempt to communicate information about science out of concern for the public's susceptibility to unproven claims.\n\nThe National Science Foundation stated that pseudoscientific beliefs in the U.S. became more widespread during the 1990s, peaked about 2001, and then decreased slightly since with pseudoscientific beliefs remaining common. According to the NSF report, there is a lack of knowledge of pseudoscientific issues in society and pseudoscientific practices are commonly followed. Surveys indicate about a third of all adult Americans consider astrology to be scientific.\n\nIn a report Singer and Benassi (1981) wrote that pseudoscientific beliefs have their origin from at least four sources.\n\nAnother American study (Eve and Dunn, 1990) supported the findings of Singer and Benassi and found pseudoscientific belief being promoted by high school life science and biology teachers.\n\nThe psychology of pseudoscience attempts to explore and analyze pseudoscientific thinking by means of thorough clarification on making the distinction of what is considered scientific vs. pseudoscientific. The human proclivity for seeking confirmation rather than refutation (confirmation bias), the tendency to hold comforting beliefs, and the tendency to overgeneralize have been proposed as reasons for pseudoscientific thinking. According to Beyerstein (1991), humans are prone to associations based on resemblances only, and often prone to misattribution in cause-effect thinking.\n\nMichael Shermer's theory of belief-dependent realism is driven by the belief that the brain is essentially a \"belief engine,\" which scans data perceived by the senses and looks for patterns and meaning. There is also the tendency for the brain to create cognitive biases, as a result of inferences and assumptions made without logic and based on instinct — usually resulting in patterns in cognition. These tendencies of patternicity and agenticity are also driven \"by a meta-bias called the bias blind spot, or the tendency to recognize the power of cognitive biases in other people but to be blind to their influence on our own beliefs.\"\nLindeman states that social motives (i.e., \"to comprehend self and the world, to have a sense of control over outcomes, to belong, to find the world benevolent and to maintain one's self-esteem\") are often \"more easily\" fulfilled by pseudoscience than by scientific information. Furthermore, pseudoscientific explanations are generally not analyzed rationally, but instead experientially. Operating within a different set of rules compared to rational thinking, experiential thinking regards an explanation as valid if the explanation is \"personally functional, satisfying and sufficient\", offering a description of the world that may be more personal than can be provided by science and reducing the amount of potential work involved in understanding complex events and outcomes.\n\nThere is a trend to believe in pseudoscience more than scientific evidence. Some people believe the prevalence of pseudoscientific beliefs is due to widespread \"scientific illiteracy\". Individuals lacking scientific literacy are more susceptible to wishful thinking, since they are likely to turn to immediate gratification powered by System 1, our default operating system which requires little to no effort. This system encourages one to accept the conclusions they believe, and reject the ones they do not. Further analysis of complex pseudoscientific phenomena require System 2, which follows rules, compares objects along multiple dimensions and weighs options. These two systems have several other differences which are further discussed in the dual-process theory. The scientific and secular systems of morality and meaning are generally unsatisfying to most people. Humans are, by nature, a forward-minded species pursuing greater avenues of happiness and satisfaction, but we are all too frequently willing to grasp at unrealistic promises of a better life.\n\nPsychology has much to discuss about pseudoscience thinking, as it is the illusory perceptions of causality and effectiveness of numerous individuals that needs to be illuminated. Research suggests that illusionary thinking happens in most people when exposed to certain circumstances such as reading a book, an advertisement or the testimony of others are the basis of pseudoscience beliefs. It is assumed that illusions are not unusual, and given the right conditions, illusions are able to occur systematically even in normal emotional situations. One of the things pseudoscience believers quibble most about is that academic science usually treats them as fools. Minimizing these illusions in the real world is not simple. To this aim, designing evidence-based educational programs can be effective to help people identify and reduce their own illusions.\n\nIn the philosophy and history of science, Imre Lakatos stresses the social and political importance of the demarcation problem, the normative methodological problem of distinguishing between science and pseudoscience. His distinctive historical analysis of scientific methodology based on research programmes suggests: \"scientists regard the successful theoretical prediction of stunning novel facts – such as the return of Halley's comet or the gravitational bending of light rays – as what demarcates good scientific theories from pseudo-scientific and degenerate theories, and in spite of all scientific theories being forever confronted by 'an ocean of counterexamples'\". Lakatos offers a \"novel fallibilist analysis of the development of Newton's celestial dynamics, [his] favourite historical example of his methodology\" and argues in light of this historical turn, that his account answers for certain inadequacies in those of Karl Popper and Thomas Kuhn. \"Nonetheless, Lakatos did recognize the force of Kuhn's historical criticism of Popper – all important theories have been surrounded by an 'ocean of anomalies', which on a falsificationist view would require the rejection of the theory outright... Lakatos sought to reconcile the rationalism of Popperian falsificationism with what seemed to be its own refutation by history\".\n\nThe boundary between science and pseudoscience is disputed and difficult to determine analytically, even after more than a century of study by philosophers of science and scientists, and despite some basic agreements on the fundamentals of the scientific method. The concept of pseudoscience rests on an understanding that the scientific method has been misrepresented or misapplied with respect to a given theory, but many philosophers of science maintain that different kinds of methods are held as appropriate across different fields and different eras of human history. According to Lakatos, the typical descriptive unit of great scientific achievements is not an isolated hypothesis but \"a powerful problem-solving machinery, which, with the help of sophisticated mathematical techniques, digests anomalies and even turns them into positive evidence.\"\n\nThe demarcation problem between science and pseudoscience brings up debate in the realms of science, philosophy and politics. Imre Lakatos, for instance, points out that the Communist Party of the Soviet Union at one point declared that Mendelian genetics was pseudoscientific and had its advocates, including well-established scientists such as Nikolai Vavilov, sent to a Gulag and that the \"liberal Establishment of the West\" denies freedom of speech to topics it regards as pseudoscience, particularly where they run up against social mores.\n\nIt becomes pseudoscientific when science cannot be separated from ideology, scientists misrepresent scientific findings to promote or draw attention for publicity, when politicians, journalists and a nation's intellectual elite distort the facts of science for short-term political gain, or when powerful individuals of the public conflate causation and cofactors by clever wordplay. These ideas reduce the authority, value, integrity and independence of science in society.\n\nDistinguishing science from pseudoscience has practical implications in the case of health care, expert testimony, environmental policies, and science education. Treatments with a patina of scientific authority which have not actually been subjected to actual scientific testing may be ineffective, expensive and dangerous to patients and confuse health providers, insurers, government decision makers and the public as to what treatments are appropriate. Claims advanced by pseudoscience may result in government officials and educators making bad decisions in selecting curricula.\n\nThe extent to which students acquire a range of social and cognitive thinking skills related to the proper usage of science and technology determines whether they are scientifically literate. Education in the sciences encounters new dimensions with the changing landscape of science and technology, a fast-changing culture and a knowledge-driven era. A reinvention of the school science curriculum is one that shapes students to contend with its changing influence on human welfare. Scientific literacy, which allows a person to distinguish science from pseudosciences such as astrology, is among the attributes that enable students to adapt to the changing world. Its characteristics are embedded in a curriculum where students are engaged in resolving problems, conducting investigations, or developing projects.\n\nFriedman mentions why most scientists avoid educating about pseudoscience, including that paying undue attention to pseudoscience could dignify it. On the other hand, Park emphasizes how pseudoscience can be a threat to society and considers that scientists have a responsibility to teach how to distinguish science from pseudoscience.\n\nPseudosciences such as homeopathy, even if generally benign, are used by charlatans. This poses a serious issue because it enables incompetent practitioners to administer health care. True-believing zealots may pose a more serious threat than typical con men because of their affection to homeopathy's ideology. Irrational health care is not harmless and it is careless to create patient confidence in pseudomedicine.\n\nOn December 8, 2016, Michael V. LeVine, writing in \"Business Insider\", pointed out the dangers posed by the \"Natural News\" website: \"Snake-oil salesmen have pushed false cures since the dawn of medicine, and now websites like \"Natural News\" flood social media with dangerous anti-pharmaceutical, anti-vaccination and anti-GMO pseudoscience that puts millions at risk of contracting preventable illnesses.\"\n\n\n\n\n"}
{"id": "9919947", "url": "https://en.wikipedia.org/wiki?curid=9919947", "title": "Quietism (philosophy)", "text": "Quietism (philosophy)\n\nQuietism in philosophy is an approach to the subject that sees the role of philosophy as broadly therapeutic or remedial. Quietist philosophers believe that philosophy has no positive thesis to contribute, but rather that its value is in defusing confusions in the linguistic and conceptual frameworks of other subjects, including non-quietist philosophy. By re-formulating supposed problems in a way that makes the misguided reasoning from which they arise apparent, the quietist hopes to put an end to humanity's confusion, and help return to a state of intellectual quietude.\n\nBy its very nature, quietism is not a philosophical school as understood in the traditional sense of a body of doctrines, but still it can be identified both by its methodology, which focuses on language and the use of words, and by its objective, which is to show that most philosophical problems are only pseudo-problems.\n\nPyrrhonism represents perhaps the earliest example of an identifiably quietist position in the West. Sextus Empiricus regarded Pyrrhonism not as a nihilistic attack but rather as a form of philosophical therapy:\n\nContemporary discussion of quietism can be traced back to Ludwig Wittgenstein, whose work greatly influenced the Ordinary Language philosophers. One of the early 'ordinary language' works, Gilbert Ryle's \"The Concept of Mind\", attempted to demonstrate that dualism arises from a failure to appreciate that mental vocabulary and physical vocabulary are simply different ways of describing one and the same thing, namely human behaviour. J. L. Austin's \"Sense and Sensibilia\" took a similar approach to the problems of skepticism and the reliability of sense perception, arguing that they arise only by misconstruing ordinary language, not because there is anything genuinely wrong with empirical evidence. Norman Malcolm, a friend of Wittgenstein's, took a quietist approach to skeptical problems in the philosophy of mind. More recently, the philosophers John McDowell and Richard Rorty have taken explicitly quietist positions.\n\n\n"}
{"id": "23183722", "url": "https://en.wikipedia.org/wiki?curid=23183722", "title": "ReAction! Chemistry in the Movies", "text": "ReAction! Chemistry in the Movies\n\nReAction! Chemistry in the Movies (2009, ) is a nonfiction book about movies, chemistry, and chemistry in the movies by Chemistry Professor Mark Griep and Artist Marjorie Mikasen published by Oxford University Press USA. The authors were awarded an Alfred P. Sloan Foundation grant in the area of Public Understanding of Science to research and write the book.\n\nThis book is about the chemistry when it is part of the narrative. Most of the examples are contemporary popular feature films while some are documentaries, shorts, silents, and international films. The book uses the dualities personified by the benevolent Dr. Jekyll on one hand and the evil Mr. Hyde on the other to describe how chemists and chemistry are portrayed in the movies.\n\nThere are 10 chapters, the first five of which have dark chemical themes and the second five of which have bright chemical themes. The chapter titles are:\n\nAccording to several reviews, the book's strength is when it explores what might be the real chemicals that inspired the fictional compounds found in certain movies.\n\n"}
{"id": "10681803", "url": "https://en.wikipedia.org/wiki?curid=10681803", "title": "Royal Society Wolfson Research Merit Award", "text": "Royal Society Wolfson Research Merit Award\n\nThe Royal Society Wolfson Research Merit Grant Award was a grant award originally announced in 2000. which has now been superseded by the Royal Society Wolfson Fellowships. \n\nIt was administered by the Royal Society and jointly funded by the Wolfson Foundation and the UK Office of Science and Technology, to give universities additional financial support to attract key researchers to this country or to retain those who might seek to gain higher salaries elsewhere.\" to tackle the brain drain. They were given in four annual rounds, with up to seven awards per round.\n\nWinners of this grant (see ) award included:\n"}
{"id": "26798613", "url": "https://en.wikipedia.org/wiki?curid=26798613", "title": "Science Theatre", "text": "Science Theatre\n\nScience Theatre is an undergraduate student-run science outreach organization at Michigan State University's East Lansing campus. Science Theatre visits schools and events throughout Michigan performing interactive science demonstrations for K-12 students on-stage or up-close. Science Theatre performers are undergraduate and graduate student volunteers and all performances are made free of charge.\n\nThe group's performances consist of arrangements from its catalog of more than seventy demonstrations in biology, chemistry, and physics. Additionally, Science Theatre performs comprehensive shows in Astronomy, Environmental science, Pressure, the Periodic Table, Quantum Mechanics, and FRIB-related science.\n\nScience Theatre was founded in April 1991 under a grant from the National Science Foundation and received the 1993 AAAS Award for Public Understanding of Science and Technology. Science Theatre is a four-time winner of the Outreach Award from the Michigan State University Department of Physics and Astronomy.\n\n"}
{"id": "12082329", "url": "https://en.wikipedia.org/wiki?curid=12082329", "title": "Science in newly industrialized countries", "text": "Science in newly industrialized countries\n\nScientific research is concentrated in the developed world, with only a marginal contribution from the rest of the world. Most Nobel Laureates are either from United States, Europe, or Japan. Many newly industrialized countries have been trying to establish scientific institutions, but with limited success. There is an insufficient dedicated, inspired and motivated labor pool for science and insufficient investment in science education.\n\nThe reason that there have been so few scientists, who have made their mark globally, from most NIC's (Newly Industrialized Countries) is partly historical and partly social A true scientist is nurtured from the school up wards to scientific establishments. Only, if there are inspired and dedicated school science teachers in abundance, there will be sufficient number of inspired students who would like to take science as a career option and who may one day become a successful scientist.\n\nA common thread can indeed be discerned in the state of science in many NICs. Thus although, most of the science establishments in the major NICs can be said to be doing fairly well, none of them have been as successful as the developed countries.\n\nAfter the Second World War, a small technical elite arose in developing countries such as India, Pakistan, Brazil, and Iraq who had been educated as scientists in the industrialized world. They spearheaded the development of science in these countries, presuming that by pushing for Manhattan project-type enterprises in nuclear power, electronics, pharmaceuticals, or space exploration they could leapfrog the dismally low level of development of science establishments in their countries. India, for example, started a nuclear energy program that mobilized thousands of technicians and cost hundreds of millions of dollars but had limited success. Though China, North Korea, India and Pakistan have been successful in deploying nuclear weapons and some of them e.g. China and India have launched fairly successful space programs, (for example, Chandrayaan I (\"Sanskrit\" चंद्रयान-1), which literally means \"Moon Craft,\" is an unmanned lunar mission by the Indian Space Research Organisation and it hopes to land a motorised rover on the moon in 2010 or 2011 as a part of its second Chandrayaan mission; Chang'e I, China's moon probing project is proceeding in full swing in a well-organized way), the fact remains that most of the scientists responsible for these deeds had received their terminal education from some institution or university in US or Europe. In addition there have been hardly any Nobel laureates in science who have conducted the path-breaking research in a native science establishment.\n\nBrazilian science effectively began in the 19th century, until then, Brazil was a poor colony, without universities, printing presses, libraries, museums, etc. This was perhaps a deliberate policy of the Portuguese colonial power, because they feared that the appearance of educated Brazilian classes would boost nationalism and aspirations toward political independence.\n\nThe first attempts of having a Brazilian science establishment were made around 1783, with the expedition of Portuguese naturalist Alexandre Rodrigues, who was sent by Portugal's prime minister, the Marquis of Pombal, to explore and identify Brazilian fauna, flora and geology. His collections, however, were lost to the French, when Napoleon invaded, and were transported to Paris by Étienne Geoffroy Saint-Hilaire. In 1772, the first learned society, the Sociedade Scientifica, was founded in Rio de Janeiro, but lasted only until 1794. Also, in 1797, the first botanic institute was founded in Salvador, Bahia. In the second and third decades of the twentieth century, the main universities in Brazil were organised from a set of existing medical, engineering and law schools. The University of Brazil dates from 1927, the University of São Paulo - today the largest in the Country - dates from 1934.\n\nToday, Brazil has a well-developed organization of science and technology. Basic research in science is largely carried out in public universities and research centers and institutes, and some in private institutions, particularly in non-profit non-governmental organizations. More than 90% of funding for basic research comes from governmental sources.\n\nApplied research, technology and engineering is also largely carried out in the university and research centers system, contrary-wise to more developed countries such as the United States, South Korea, Germany, Japan, etc. A significant trend is emerging lately. Companies such as Motorola, Samsung, Nokia and IBM have established large R&D&I centers in Brazil. One of the incentive factors for this, besides the relatively lower cost and high sophistication and skills of Brazilian technical manpower, has been the so-called Informatics Law, which exempts from certain taxes up to 5% of the gross revenue of high technology manufacturing companies in the fields of telecommunications, computers, digital electronics, etc. The Law has attracted annually more than 1,5 billion dollars of investment in Brazilian R&D&I. Multinational companies have also discovered that some products and technologies designed and developed by Brazilians are significantly competitive and are appreciated by other countries, such as automobiles, aircraft, software, fiber optics, electric appliances, and so on.\n\nThe challenges Brazilian science faces today are: to expand the system with quality, supporting the installed competence; transfer knowledge from the research sector to industry; embark on government action in strategic areas; enhance the assessment of existing programmes and commence innovative projects in areas of relevance for the Country. Furthermore, scientific dissemination plays a fundamental role in transforming the perception of the public at large of the importance of science in modern life. The government has undertaken to meet these challenges using institutional base and the operation of existing qualified scientists.\n\nA question that has been intriguing many historians studying China is the fact that China did not develop a scientific revolution and Chinese technology fell behind that of Europe. Many hypotheses have been proposed ranging from the cultural to the political and economic. has argued that China indeed had a scientific revolution in the 17th century and that we are still far from understanding the scientific revolutions of the West and China in all their political, economic and social ramifications. Some like John K. Fairbank are of the opinion that the Chinese political system was hostile to scientific progress.\n\nNeedham argued, and most scholars agreed, that cultural factors prevented these Chinese achievements from developing into what could be called \"science\". It was the religious and philosophical framework of the Chinese intellectuals which made them unable to believe in the ideas of laws of nature. More recent historians have questioned political and cultural explanations and have focused more on economic causes. Mark Elvin's high level equilibrium trap is one well-known example of this line of thought, as well as Kenneth Pomeranz' argument that resources from the New World made the crucial difference between European and Chinese development.\n\nThus, it was not that there was no order in nature for the Chinese, but rather that it was not an order ordained by a rational personal being, and hence there was no conviction that rational personal beings would be able to spell out in their lesser earthly languages the divine code of laws which he had decreed aforetime. The Taoists, indeed, would have scorned such an idea as being too naive for the subtlety and complexity of the universe as they intuited it. Similar grounds have been found for questioning much of the philosophy behind traditional Chinese medicine, which, derived mainly from Taoist philosophy, reflects the classical Chinese belief that individual human experiences express causative principles effective in the environment at all scales. Because its theory predates use of the scientific method, it has received various criticisms based on scientific thinking. Even though there are physically verifiable anatomical or histological bases for the existence of acupuncture points or meridians, for instance skin conductance measurements show increases at the predicted points.\n\nToday, science and technology establishment in the People's Republic of China is growing rapidly. Even as many Chinese scientists debate what institutional arrangements will be best for Chinese science, reforms of the Chinese Academy of Sciences continue. The average age of researchers at the Chinese Academy of Sciences has dropped by nearly ten years between 1991 and 2003. However, many of them are educated in the United States and other foreign countries.\n\nChinese university undergraduate and graduate enrollments more than doubled from 1995 to 2005. The universities now have more cited PRC papers than CAS in the Science Citation Index. Some Chinese scientists say CAS is still ahead on overall quality of scientific work but that lead will only last five to ten years.\n\nSeveral Chinese immigrants to the United States have also been awarded the Nobel Prize, including:, Samuel C. C. Ting, Chen Ning Yang, Tsung-Dao Lee, Yuan T. Lee, Daniel C. Tsui, and Gao Xingjian. Other overseas ethnic Chinese that have achieved success in sciences include Fields Medal recipient Shing-Tung Yau and Terence Tao, and Turing Award recipient Andrew Yao. Tsien Hsue-shen was a prominent scientist at NASA's Jet Propulsion Laboratory, while Chien-Shiung Wu contributed to the Manhattan Project (some argue she never received the Nobel Prize unlike her colleagues Tsung-Dao Lee and Chen Ning Yang due to sexism by the selection committee). Others include Charles K. Kao, a pioneer in fiber optics technology, and Dr. David Ho, one of the first scientists to propose that AIDS was caused by a virus, thus subsequently developing combination antiretroviral therapy to combat it. Dr. Ho was named TIME magazine's 1996 Man of the Year. In 2015, Tu Youyou, a pharmaceutical chemist, became the first native Chinese scientist, born and educated and carried out research exclusively in the People's Republic of China, to receive the Nobel Prize in natural sciences.\n\nThe earliest applications of science in India took place in the context of medicine, metallurgy, construction technology (such as ship building, manufacture of cement and paints) and in textile production and dyeing. But in the process of understanding chemical processes, led to some theories about physical processes and the forces of nature that are today studied as specific topics within the fields of chemistry and physics.\n\nMany mathematical concepts today were contributed by Indian mathematicians like Aryabhata.\n\nThere was really no place for scientists in the Indian caste system. Thus while there were/are castes for the learned brahmins, the warriors kshatriyas, the traders vaishyas and the menial workers shudras, maybe even the bureaucrats (the kayasths) there was/is hardly any formal place in the social hierarchy for a people who discover new knowledge or invent new devices based on the recently discovered knowledge, even though scientific temper has always been in India, in the form of logic, reasoning and method of acquiring knowledge. Its therefore no wonder that some Indians quickly learned to value science, especially those belonging to the privileged Brahmin caste during the British colonial rule that lasted over two centuries. Some Indians did succeed to achieve notable success and fame, examples include Satyendra Nath Bose, Meghnad Saha, Jagdish Chandra Bose and C. V. Raman even though they belonged to different castes. The science communication had begun with publication of a scientific journal, Asiatick Researches in 1788. Thereafter, the science communication in India has evolved in many facets. Following this, there has been a continuing development in the formation of scientific institutions and publication of scientific literature. Subsequently, scientific publications also started appearing in Indian languages by the end of eighteenth century. The publication of ancient scientific literature and textbooks at mass scale started in the beginning of nineteenth century. The scientific and technical terms, however, had been a great difficulty for a long time for popular science writing.\n\n\n"}
{"id": "33185307", "url": "https://en.wikipedia.org/wiki?curid=33185307", "title": "Science slam", "text": "Science slam\n\nA science slam is a scientific talk where scientists present their own scientific research work in a given time frame - usually 10 minutes - in front of a non-expert audience. The focus lies on teaching current science to a diverse audience in an entertaining way. The presentation is judged by the audience. A science slam is a form of science communication.\n\nScience slams are open to all fields of science. However, events specializing on particular topics exist as well. Examples include: technical science slams,\nhealth science slams,\nsociological science slams, \njunior science slams, kid's science slams, and\nbinational science slams.\n\n"}
{"id": "167742", "url": "https://en.wikipedia.org/wiki?curid=167742", "title": "Scientism", "text": "Scientism\n\nScientism is an ideology that promotes science as the purportedly objective means by which society should determine normative and epistemological values. The term \"scientism\" is generally used critically, pointing to the cosmetic application of science in unwarranted situations not amenable to application of the scientific method or similar scientific standards. \n\nIn philosophy of science, the term \"scientism\" frequently implies a critique of the more extreme expressions of logical positivism and has been used by social scientists such as Friedrich Hayek, philosophers of science such as Karl Popper, and philosophers such as Hilary Putnam and Tzvetan Todorov to describe (for example) the dogmatic endorsement of scientific methodology and the reduction of all knowledge to only that which is measured or confirmatory.\n\nMore generally, scientism is often interpreted as science applied \"in excess\". The term \"scientism\" has two senses:\n\n\nIt is also sometimes used to describe universal applicability of the scientific method and approach, and the view that empirical science constitutes the most authoritative worldview or the most valuable part of human learning—to the complete exclusion of other viewpoints, such as historical, philosophical, economic or cultural worldviews. It has been defined as \"the view that the characteristic inductive methods of the natural sciences are the only source of genuine factual knowledge and, in particular, that they alone can yield true knowledge about man and society\". The term \"scientism\" is also used by historians, philosophers, and cultural critics to highlight the possible dangers of lapses towards excessive reductionism in all fields of human knowledge.\n\nFor social theorists in the tradition of Max Weber, such as Jürgen Habermas and Max Horkheimer, the concept of scientism relates significantly to the philosophy of positivism, but also to the cultural rationalization for modern Western civilization. British novelist Sara Maitland has called scientism a \"myth as pernicious as any sort of fundamentalism.\"\n\nReviewing the references to scientism in the works of contemporary scholars, Gregory R. Peterson detects two main broad themes:\n\nThe term \"scientism\" was popularized by the Nobel Prize winner F.A. Hayek, who defined it as the \"slavish imitation of the method and language of Science\". Karl Popper defines scientism as \"the aping of what is widely mistaken for the method of science\".\n\nMikael Stenmark proposes the expression \"scientific expansionism\" as a synonym of scientism. In the \"Encyclopedia of science and religion\", he writes that, while the doctrines that are described as scientism have many possible forms and varying degrees of ambition, they share the idea that the boundaries of science (that is, typically the natural sciences) could and should be expanded so that something that has not been previously considered as a subject pertinent to science can now be understood as part of science (usually with science becoming the sole or the main arbiter regarding this area or dimension).\n\nAccording to Stenmark, the strongest form of scientism states that science has no boundaries and that all human problems and all aspects of human endeavor, with due time, will be dealt with and solved by science alone. This idea has also been called the Myth of Progress.\n\nE. F. Schumacher, in his \"A Guide for the Perplexed\", criticized scientism as an impoverished world view confined solely to what can be counted, measured and weighed. \"The architects of the modern worldview, notably Galileo and Descartes, assumed that those things that could be weighed, measured, and counted were more true than those that could not be quantified. If it couldn't be counted, in other words, it didn't count.\"\n\nIntellectual historian T.J. Jackson Lears argues there has been a recent reemergence of \"nineteenth-century positivist faith that a reified 'science' has discovered (or is about to discover) all the important truths about human life. Precise measurement and rigorous calculation, in this view, are the basis for finally settling enduring metaphysical and moral controversies.\" Lears specifically identifies Harvard psychologist Steven Pinker's work as falling in this category. Philosophers John N. Gray and Thomas Nagel have leveled similar criticisms against popular works by moral psychologist Jonathan Haidt, neuroscientist Sam Harris, and writer Malcolm Gladwell.\n\nGenetic biologist Austin L. Hughes wrote in conservative journal \"The New Atlantis\" that scientism has much in common with superstition: \"the stubborn insistence that something...has powers which no evidence supports.\"\n\nSeveral scholars use the term to describe the work of vocal critics of religion-as-such. Individuals associated with New Atheism have garnered this label from both religious and non-religious scholars. Theologian John Haught argues that Daniel Dennett and other new atheists subscribe to a belief system of scientific naturalism, which holds the central dogma that \"only nature, including humans and our creations, is real: that God does not exist; and that science alone can give us complete and reliable knowledge of reality.\" Haught argues that this belief system is self-refuting since it requires its adherents to assent to beliefs that violate its own stated requirements for knowledge. Christian Philosopher Peter Williams argues that it is only by conflating science with scientism that new atheists feel qualified to \"pontificate on metaphysical issues.\" Philosopher Daniel Dennett responded to religious criticism of his book \"\" by saying that accusations of scientism \"[are] an all-purpose, wild-card smear... When someone puts forward a scientific theory that [religious critics] really don't like, they just try to discredit it as 'scientism'. But when it comes to facts, and explanations of facts, science is the only game in town\".\n\nNon-religious scholars have also linked New Atheist thought with scientism. Atheist philosopher Thomas Nagel argues that neuroscientist Sam Harris conflates all empirical knowledge with that of scientific knowledge. Marxist literary critic Terry Eagleton argues that Christopher Hitchens possesses an \"old-fashioned scientistic notion of what counts as evidence\" that reduces knowledge to what can and cannot be proven by scientific procedure. Agnostic philosopher Anthony Kenny has also criticized New Atheist philosopher Alexander Rosenberg's \"The Atheist's Guide to Reality\" for resurrecting a self-refuting epistemology of logical positivism and reducing all knowledge of the universe to the discipline of physics.\n\nMichael Shermer, founder of The Skeptics Society, draws a parallel between scientism and traditional religious movements, pointing to the cult of personality that develops around some scientists in the public eye. He defines scientism as a worldview that encompasses natural explanations, eschews supernatural and paranormal speculations, and embraces empiricism and reason.\n\nThe Iranian scholar Seyyed Hossein Nasr has stated that in the Western world, many will accept the ideology of modern science, not as \"simple ordinary science\", but as a replacement for religion.\n\nGregory R. Peterson writes that \"for many theologians and philosophers, scientism is among the greatest of intellectual sins\".\n\nIn his essay \"Against Method\", Paul Feyerabend characterizes science as \"an essentially anarchic enterprise\" and argues emphatically that science merits no exclusive monopoly over \"dealing in knowledge\" and that scientists have never operated within a distinct and narrowly self-defined tradition. He depicts the process of contemporary scientific education as a mild form of indoctrination, aimed at \"making the history of science duller, simpler, more uniform, more 'objective' and more easily accessible to treatment by strict and unchanging rules.\"\n\nThomas M. Lessl argues that religious themes persist in what he calls scientism, the public rhetoric of science. There are two methodologies that illustrate this idea of scientism. One is the epistemological approach, the assumption that the scientific method trumps other ways of knowing and the ontological approach, that the rational mind reflects the world and both operate in knowable ways. According to Lessl, the ontological approach is an attempt to \"resolve the conflict between rationalism and skepticism\". Lessl also argues that without scientism, there would not be a scientific culture.\n\nPhilosopher of religion Keith Ward has said scientism is philosophically inconsistent or even self-refuting, as the truth of the statements \"no statements are true unless they can be proven scientifically (or logically)\" or \"no statements are true unless they can be shown empirically to be true\" cannot themselves be proven scientifically, logically, or empirically.\n\nIn the introduction to his collected oeuvre on the sociology of religion, Max Weber asks why \"the scientific, the artistic, the political, or the economic development [elsewhere]… did not enter upon that path of rationalization which is peculiar to the Occident?\" According to the distinguished German social theorist, Jürgen Habermas, \"For Weber, the intrinsic (that is, not merely contingent) relationship between modernity and what he called 'Occidental rationalism' was still self-evident.\" Weber described a process of rationalisation, disenchantment and the \"disintegration of religious world views\" that resulted in modern secular societies and capitalism.\nHabermas is critical of pure instrumental rationality, arguing that the \"Social Life–World\" is better suited to literary expression, the former being \"intersubjectively accessible experiences\" that can be generalized in a formal language, while the latter \"must generate an intersubjectivity of mutual understanding in each concrete case\":\n\n\n"}
{"id": "5996838", "url": "https://en.wikipedia.org/wiki?curid=5996838", "title": "The Story of Science in America", "text": "The Story of Science in America\n\nThe Story of Science in America is a 1967 science book by L. Sprague de Camp and Catherine Crook de Camp, illustrated by Leonard Everett Fisher, published by Charles Scribner's Sons. It has been translated into Spanish, Portuguese, Burmese and French.\n\nThe book traces the work of inventors and naturalists in the United States from the Colonial era through the mid-19th century, and relates scientific developments in the century following.\n\n\nCritical response to the book was positive. Jane E. Brody, writing for \"The New York Times\", called it \"a fast-moving, informative and thoroughly enjoyable chronicle, with amusing anecdotes, legends and interesting sidelights that reflect the personalities, lives and times of the men who shaped our nation scientifically.\" She noted that \"the authors have kept their writing free of chauvinism,\" and that \"[m]ost of the scientific concepts are well enough explained so that even the newcomer to science should be able to grasp at least the essence of them.\" In the same issue the book was included among seventy-five recommended titles selected by the Children's Editor of the newspaper's Book Review, described as an \"[i]nformative, thoroughly enjoyable chronicle of the development of science in our country.\"\n\n\"Publishers' Weekly\" stated that \"[t]o read the index ... is to read the names of the men and of their discoveries in science in America, from the earliest days ... to the space age. To read the book is to become familiar with the men and their contributions to science.\"\n\nGeorge Basalia, writing for \"Library Journal\", called the book \"a first-rate history of American science and technology for high-school students ... cover[ing] major American technical discoveries as well as our contributions to the purely theoretical aspects of science.\" He found \"much to be praised ... the book is intelligently conceived, carefully organized, clearly written, and handsomely designed. Unfortunately, the illustrations do not do justice to [the] excellent text.\"\n\nH. D. Allen in the \"Montreal Gazette\" wrote that the book's story \"makes fascinating reading,\" and that \"[w]hile the treatment of any one discipline may at first seem superficial and chatty, the total impact is most impressive, for the reader is left with an acquaintance with the leading figures of the age of science and some appreciation of how the contribution of each influenced a way of life.\" He concluded \"The breadth of scientific knowledge which this book represents is remarkable, as is the skill with which it has been set down and the effortlessness with which it reads.\"\n\n\"The Booklist\" called it \"[a] wide-ranging survey [that] reflects the authors' humanistic interests as well as their familiarity with several branches of science and their extensive background reading.\"\n\nHarry C. Stubbs in \"The Horn Book Magazine\" included it among \"half a dozen books dealing ... with the history of science [that] I can recommend [both] to nonscientists as guides toward the Light [and] to scientists and science teachers as reminders that what we know was long, slow, and hard in coming.\" He noted that it \"give[s] us a series of fascinating biographical and anecdotal items strung loosely on the thread of developing scientific knowledge.\"\n\nPhilip and Phylis Morrison in \"Scientific American\" felt it \"manages to convey a sense of coherence, even though it deals at staccato length with so many men, trends and ideas ... The reason is partly in the expert writing--smooth, unusually candid, cheerful and sometimes a bit condescending (as in the two or three pages about Veblen).\" They add that \"[n]ot all the dicta of the authors seem reasonable, but to find any personal judgment at work is so rare in this kind of pedagogy that one is pleased by the De Camps even when one disagrees with them.\"\n"}
