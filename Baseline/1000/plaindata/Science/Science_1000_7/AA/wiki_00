{"id": "8168926", "url": "https://en.wikipedia.org/wiki?curid=8168926", "title": "Amsterdam Science Park", "text": "Amsterdam Science Park\n\nAmsterdam Science Park is a science park in the Oost city district of Amsterdam, The Netherlands with foci on physics, mathematics, information technology and the life sciences. The 70 hectare (175 acre) park provides accommodations for science, business and housing. Resident groups include institutes of the natural science faculties of the University of Amsterdam, several research institutes, and related companies. Three of the colocations of the Amsterdam Internet Exchange are at the institutes SURFsara, NIKHEF, and Equinix-AM3 at the science park. \nIn 2009, the Amsterdam Science Park railway station was opened by mayor Job Cohen.\n\n\nAt the science park, 314 residences and 721 student units have been completed. An additional 423 residences and 617 student units are planned. \n\n\n"}
{"id": "25611728", "url": "https://en.wikipedia.org/wiki?curid=25611728", "title": "Aromatherapy", "text": "Aromatherapy\n\nAromatherapy uses plant materials and aromatic plant oils, including essential oils, and other aroma compounds for improving psychological or physical well-being.\n\nIt can be offered as a complementary therapy or as a form of alternative medicine. Complementary therapy can be offered alongside standard treatment, with alternative medicine offered instead of conventional, evidence-based treatments.\n\nAromatherapists, who specialize in the practice of aromatherapy, utilize blends of therapeutic essential oils that can be issued through topical application, massage, inhalation or water immersion to stimulate a desired response.\n\nThere is no good medical evidence that aromatherapy can either prevent or cure any disease. There is some evidence that it is more effective than placebo in combating postoperative nausea and vomiting, but also that it is less effective than standard anti-emetic drugs. \n\nThe use of essential oils for therapeutic, spiritual, hygienic and ritualistic purposes goes back to a number of ancient civilizations including the Chinese, Indians, Egyptians, Greeks, and Romans who used them in cosmetics, perfumes and drugs.\nOils were used for aesthetic pleasure and in the beauty industry. It was a luxury item and a means of payment. It was believed the essential oils increased the shelf life of wine and improved the taste of food.\n\nOils are described by Dioscorides, along with beliefs of the time regarding their healing properties, in his \"De Materia Medica\", written in the first century. Distilled essential oils have been employed as medicines since the eleventh century, when Avicenna isolated essential oils using steam distillation.\n\nThe concept of aromatherapy was first mooted by a small number of European scientists and doctors, in about 1907. In 1937, the word first appeared in print in a French book on the subject: \"Aromathérapie: Les Huiles Essentielles, Hormones Végétales\" by , a chemist. An English version was published in 1993. In 1910, Gattefossé burned a hand very badly and later claimed he treated it effectively with lavender oil.\n\nA French surgeon, , pioneered the medicinal uses of essential oils, which he used as antiseptics in the treatment of wounded soldiers during World War II.\n\nThe modes of application of aromatherapy include:\n\nSome of the materials employed include:\n\nAromatherapy is the treatment or prevention of disease by use of essential oils. Other stated uses include pain and anxiety reduction, enhancement of energy and short-term memory, relaxation, hair loss prevention, and reduction of eczema-induced itching.\n\nTwo basic mechanisms are offered to explain the purported effects. One is the influence of aroma on the brain, especially the limbic system through the olfactory system. The other is the direct pharmacological effects of the essential oils.\n\nIn the English-speaking world, practitioners tend to emphasize the use of oils in massage. Aromatherapy tends to be regarded as a pseudoscientific fraud at worst.\n\nOils with standardized content of components (marked FCC, for Food Chemicals Codex) are required to contain a specified amount of certain aroma chemicals that normally occur in the oil. There is no law that the chemicals cannot be added in synthetic form to meet the criteria established by the FCC for that oil. For instance, lemongrass essential oil must contain 75% aldehyde to meet the FCC profile for that oil, but that aldehyde can come from a chemical refinery instead of from lemongrass. To say that FCC oils are \"food grade\" makes them seem natural when they are not necessarily so.\n\nUndiluted essential oils suitable for aromatherapy are termed 'therapeutic grade', but there are no established and agreed standards for this category.\n\nAnalysis using gas liquid chromatography (GLC) and mass spectrometry (MS) establishes the quality of essential oils. These techniques are able to measure the levels of components to a few parts per billion. This does not make it possible to determine whether each component is natural or whether a poor oil has been 'improved' by the addition of synthetic aromachemicals, but the latter is often signaled by the minor impurities present. For example, linalool made in plants will be accompanied by a small amount of hydro-linalool, whilst synthetic linalool has traces of dihydro-linalool.\n\nThere is no good medical evidence that aromatherapy can prevent or cure any disease. In 2015, the Australian Government's Department of Health published the results of a review of alternative therapies that sought to determine if any were suitable for being covered by health insurance; aromatherapy was one of 17 therapies evaluated for which no clear evidence of effectiveness was found. Evidence for the efficacy of aromatherapy in treating medical conditions is poor, with a particular lack of studies employing rigorous methodology. A number of systematic reviews have studied the clinical effectiveness of aromatherapy in respect to pain management in labor, the treatment of post-operative nausea and vomiting, managing behaviors that challenge in dementia, and symptom relief in cancer. All of these reviews report a lack of evidence on the effectiveness of aromatherapy. Studies were found to be of low quality, meaning more well-designed, large scale randomized controlled trials are needed before clear conclusions can be drawn as to the effectiveness of aromatherapy.\n\nAromatherapy carries a risk of a number of adverse effects and this consideration, combined with the lack of evidence of its therapeutic benefit, makes the practice of questionable worth.\n\nBecause essential oils are highly concentrated they can irritate the skin when used in undiluted form. Therefore, they are normally diluted with a carrier oil for topical application, such as jojoba oil, olive oil, or coconut oil. Phototoxic reactions may occur with citrus peel oils such as lemon or lime. Also, many essential oils have chemical components that are sensitisers (meaning that they will, after a number of uses, cause reactions on the skin, and more so in the rest of the body). Some of the chemical allergies could even be caused by pesticides, if the original plants are cultivated. Some oils can be toxic to some domestic animals, with cats being particularly prone.\n\nA child hormone specialist at the University of Cambridge claimed \"... these oils can mimic estrogens\" and \"people should be a little bit careful about using these products.\" The Aromatherapy Trade Council of the UK has issued a rebuttal.\nThe Australian Tea Tree Association, a group that promotes the interests of Australian tea tree oil producers, exporters and manufacturers issued a letter that questioned the study and called on the \"New England Journal of Medicine\" for a retraction.\nThe \"New England Journal of Medicine\" has so far not replied and has not retracted the study.\n\nAs with any bioactive substance, an essential oil that may be safe for the general public could still pose hazards for pregnant and lactating women.\n\nWhile some advocate the ingestion of essential oils for therapeutic purposes, licensed aromatherapy professionals do not recommend self-prescription due to the highly toxic nature of some essential oils. Some very common oils like eucalyptus are extremely toxic when taken internally. Doses as low as one teaspoon have been reported to cause clinically significant symptoms and severe poisoning can occur after ingestion of 4 to 5 ml.\nA few reported cases of toxic reactions like liver damage and seizures have occurred after ingestion of sage, hyssop, thuja, and cedar. Accidental ingestion may happen when oils are not kept out of reach of children.\n\nOils both ingested and applied to the skin can potentially have negative interactions with conventional medicine. For example, the topical use of methyl salicylate-heavy oils like sweet birch and wintergreen may cause bleeding in users taking the anticoagulant warfarin.\n\nAdulterated oils may also pose problems depending on the type of substance used.\n\n\n"}
{"id": "40319313", "url": "https://en.wikipedia.org/wiki?curid=40319313", "title": "Biomesotherapy", "text": "Biomesotherapy\n\nBiomesotherapy is an alternative therapy practice that combines homotoxicology, mesotherapy, and acupuncture. Saline solution and homeopathic formulations are injected subcutaneously at specific acupuncture or trigger points, and homeopathic formulations are administered orally during treatment sessions. Biomesotherapy is used for pain management and general well-being.\n\n\"This article incorporates public domain text from the CDC as cited.\"\n"}
{"id": "64221", "url": "https://en.wikipedia.org/wiki?curid=64221", "title": "Biorhythm", "text": "Biorhythm\n\nA biorhythm (from Greek βίος - \"bios\", \"life\" and ῥυθμός - \"rhuthmos\", \"any regular recurring motion, rhythm\") is an attempt to predict various aspects of a person's life through simple mathematical cycles. The theory was developed by Wilhelm Fliess in the late 19th century, and was popularized in the United States in late 1970s. Most scientists believe that the idea has no more predictive power than chance. \"The theory of biorhythms is a theory that claims our daily lives are significantly affected by rhythmic cycles.\"\n\nAccording to the theory of biorhythms, a person's life is influenced by rhythmic biological cycles that affect his or her ability in various domains, such as mental, physical and emotional activity. These cycles begin at birth and oscillate in a steady (sine wave) fashion throughout life, and by modeling them mathematically, it is suggested that a person's level of ability in each of these domains can be predicted from day to day. The theory is built on the idea that the biofeedback chemical and hormonal secretion functions within the body could show a sinusoidal behavior over time.\n\nMost biorhythm models use three cycles: a 23-day physical cycle, a 28-day emotional cycle, and a 33-day intellectual cycle. Although the 28-day cycle is the same length as the average woman's menstrual cycle and was originally described as a \"female\" cycle (see below), the two are not necessarily in synchronization. Each of these cycles varies between high and low extremes sinusoidally, with days where the cycle crosses the zero line described as \"critical days\" of greater risk or uncertainty.\n\nThe numbers from +100% (maximum) to -100% (minimum) indicate where on each cycle the rhythms are on a particular day. In general, a rhythm at 0% is crossing the midpoint and is thought to have no real impact on your life, whereas a rhythm at +100% (at the peak of that cycle) would give you an edge in that area, and a rhythm at -100% (at the bottom of that cycle) would make life more difficult in that area. There is no particular meaning to a day on which your rhythms are all high or all low, except the obvious benefits or hindrances that these rare extremes are thought to have on your life.\n\nIn addition to the three popular cycles, various other cycles have been proposed, based on linear combination of the three, or on longer or shorter rhythms.\n\nTheories published state the equations for the cycles as:\nwhere formula_4 indicates the number of days since birth. Basic arithmetic shows that the combination of the simpler 23- and 28-day cycles repeats every 644 days (or 1-3/4 years), while the triple combination of 23-, 28-, and 33-day cycles repeats every 21,252 days (or 58.18+ years).\n\nThe notion of periodic cycles in human fortunes is ancient; for instance, it is found in natal astrology and in folk beliefs about \"lucky days\". The 23- and 28-day rhythms used by biorhythmists, however, were first devised in the late 19th century by Wilhelm Fliess, a Berlin physician and patient of Sigmund Freud. Fliess believed that he observed regularities at 23- and 28-day intervals in a number of phenomena, including births and deaths. He labeled the 23-day rhythm \"male\" and the 28-day rhythm \"female\", matching the menstrual cycle.\n\nIn 1904, Viennese psychology professor Hermann Swoboda came to similar conclusions. Alfred Teltscher, professor of engineering at the University of Innsbruck, developed Swoboda's work and suggested that his students' good and bad days followed a rhythmic pattern; he believed that the brain's ability to absorb, mental ability, and alertness ran in 33-day cycles. One of the first academic researchers of biorhythms was Estonian-born Nikolai Pärna, who published a book in German called \"Rhythm, Life and Creation\" in 1923.\n\nThe practice of consulting biorhythms was popularized in the 1970s by a series of books by Bernard Gittelson, including \"Biorhythm — A Personal Science\", \"Biorhythm Charts of the Famous and Infamous\", and \"Biorhythm Sports Forecasting\". Gittelson's company, Biorhythm Computers, Inc., made a business selling personal biorhythm charts and calculators, but his ability to predict sporting events was not substantiated.\n\nCharting biorhythms for personal use was popular in the United States during the 1970s; many places (especially video arcades and amusement areas) had a biorhythm machine that provided charts upon entry of date of birth. Biorhythm programs were a common application on personal computers; and in the late 1970s, there were also handheld biorhythm calculators on the market, the \"Kosmos 1\" and the Casio \"Biolator\". Biorhythm charts appeared in the \"Chicago Tribune\" from 1977 to 1979, and Gittelson wrote daily biorhythm charts for the \"Toronto Star\" from 1981 to 1985.\n\nAlthough biorhythms have declined in popularity (pop culture magazine \"Vice\" considered them \"dead\" by the mid 2010s), there are free and proprietary apps and computer programs which have charting and analysis capabilities, as well as numerous websites that offer free biorhythm readings.\n\nThere have been some three dozen studies supporting biorhythm theory, but according to a study by Terence Hines, all of those had methodological and statistical errors. Hines rejected 134 biorhythm studies and concluded that the theory is not valid.\n\nSupporters continued to defend the theory after Hines' review, causing other scientists to consider the field as pseudoscience:\nThe physiologist Gordon Stein in the book \"Encyclopedia of Hoaxes\" (1993) has written: \"Both the theoretical underpinning and the practical scientific verification of biorhythm theory are lacking. Without those, biorhythms became just another pseudoscientific claim that people are willing to accept without required evidence. Those pushing biorhythm calculators and books on a gullible public are guilty of making fraudulent claims. They are hoaxers of the public if they know what they are saying has no factual justification.\"\n\nA 1978 study of the incidence of industrial accidents found neither empirical nor theoretical support for the biorhythm model.\n\n\n\n"}
{"id": "7737", "url": "https://en.wikipedia.org/wiki?curid=7737", "title": "Clairvoyance", "text": "Clairvoyance\n\nClairvoyance (; from French \"clair\" meaning \"clear\" and \"voyance\" meaning \"vision\") is the alleged ability to gain information about an object, person, location, or physical event through extrasensory perception. Any person who is claimed to have such ability is said accordingly to be a clairvoyant () (\"one who sees clearly\").\n\nClaims for the existence of paranormal and psychic abilities such as clairvoyance have not been supported by scientific evidence published in high impact factor peer reviewed journals. Parapsychology explores this possibility, but the existence of the paranormal is not accepted by the scientific community. Parapsychology, including the study of clairvoyance, is an example of pseudoscience.\nPertaining to the ability of clear-sightedness, clairvoyance refers to the paranormal ability to see persons and events that are distant in time or space. It can be divided into roughly three classes: precognition, the ability to perceive or predict future events, retrocognition, the ability to see past events, and remote viewing, the perception of contemporary events happening outside of the range of normal perception.\n\nThroughout history, there have been numerous places and times in which people have claimed themselves or others to be clairvoyant.\n\nA number of Christian saints were said to be able to see or know things that were far removed from their immediate sensory perception as a kind of gift from God, including Columba of Iona, Padre Pio and Anne Catherine Emmerich. Jesus Christ in the Gospels is also recorded as being able to know things that were far removed from his immediate human perception.\n\nIn other religions, similar stories of certain individuals being able to see things far removed from their immediate sensory perception are commonplace, especially within pagan religions where oracles were used. Prophecy often involved some degree of clairvoyance, especially when future events were predicted.\n\nIn most of these cases, however, the ability to see things was attributed to a higher power and not thought of as an ability that lay within the person himself.\n\nIn Jainism, clairvoyance is regarded as one of the five kinds of knowledge. The beings of hell and heaven (devas) are said to possess clairvoyance by birth. According to Jain text Sarvārthasiddhi, \"this kind of knowledge has been called \"avadhi\" as it ascertains matter in downward range or knows objects within limits\".\n\nThe earliest record of somnambulistic clairvoyance is credited to the Marquis de Puységur, a follower of Franz Mesmer, who in 1784 was treating a local dull-witted peasant named Victor Race. During treatment, Race reportedly would go into trance and undergo a personality change, becoming fluent and articulate, and giving diagnosis and prescription for his own disease as well as those of others. Clairvoyance was a reported ability of some mediums during the spiritualist period of the late 19th and early 20th centuries, and psychics of many descriptions have claimed clairvoyant ability up to the present day.\n\nEarly researchers of clairvoyance included William Gregory, Gustav Pagenstecher, and Rudolf Tischner. Clairvoyance experiments were reported in 1884 by Charles Richet. Playing cards were enclosed in envelopes and a subject put under hypnosis attempted to identify them. The subject was reported to have been successful in a series of 133 trials but the results dropped to chance level when performed before a group of scientists in Cambridge. J. M. Peirce and E. C. Pickering reported a similar experiment in which they tested 36 subjects over 23,384 trials which did not obtain above chance scores.\n\nIvor Lloyd Tuckett (1911) and Joseph McCabe (1920) analyzed early cases of clairvoyance and came to the conclusion they were best explained by coincidence or fraud. In 1919, the magician P. T. Selbit staged a séance at his own flat in Bloomsbury. The spiritualist Arthur Conan Doyle attended the séance and declared the clairvoyance manifestations to be genuine.\n\nA significant development in clairvoyance research came when J. B. Rhine, a parapsychologist at Duke University, introduced a standard methodology, with a standard statistical approach to analyzing data, as part of his research into extrasensory perception. A number of psychological departments attempted to repeat Rhine's experiments with failure. W. S. Cox (1936) from Princeton University with 132 subjects produced 25,064 trials in a playing card ESP experiment. Cox concluded \"There is no evidence of extrasensory perception either in the 'average man' or of the group investigated or in any particular individual of that group. The discrepancy between these results and those obtained by Rhine is due either to uncontrollable factors in experimental procedure or to the difference in the subjects.\" Four other psychological departments failed to replicate Rhine's results. It was revealed that Rhine's experiments contained methodological flaws and procedural errors.\n\nEileen Garrett was tested by Rhine at Duke University in 1933 with Zener cards. Certain symbols that were placed on the cards and sealed in an envelope, and she was asked to guess their contents. She performed poorly and later criticized the tests by claiming the cards lacked a psychic energy called \"energy stimulus\" and that she could not perform clairvoyance to order. The parapsychologist Samuel Soal and his colleagues tested Garrett in May, 1937. Most of the experiments were carried out in the Psychological Laboratory at the University College London. A total of over 12,000 guesses were recorded but Garrett failed to produce above chance level. In his report Soal wrote \"In the case of Mrs. Eileen Garrett we fail to find the slightest confirmation of Dr. J. B. Rhine's remarkable claims relating to her alleged powers of extra-sensory perception. Not only did she fail when I took charge of the experiments, but she failed equally when four other carefully trained experimenters took my place.\"\n\nRemote viewing also known as remote sensing, remote perception, telesthesia and travelling clairvoyance is the alleged paranormal ability to perceive a remote or hidden target without support of the senses.\n\nA well known study of remote viewing in recent times has been the US government-funded project at the Stanford Research Institute during the 1970s through the mid-1990s. In 1972, Harold Puthoff and Russell Targ initiated a series of human subject studies to determine whether participants (the \"viewers\" or \"percipients\") could reliably identify and accurately describe salient features of remote locations or \"targets\". In the early studies, a human \"sender\" was typically present at the remote location, as part of the experiment protocol. A three-step process was used, the first step being to randomly select the target conditions to be experienced by the senders. Secondly, in the viewing step, participants were asked to verbally express or sketch their impressions of the remote scene. Thirdly, in the judging step, these descriptions were matched by separate judges, as closely as possible, with the intended targets. The term remote viewing was coined to describe this overall process. The first paper by Puthoff and Targ on remote viewing was published in \"Nature\" in March 1974; in it, the team reported some degree of remote viewing success. After the publication of these findings, other attempts to replicate the experiments were carried out with remotely linked groups using computer conferencing.\n\nThe psychologists David Marks and Richard Kammann attempted to replicate Targ and Puthoff's remote viewing experiments that were carried out in the 1970s at the Stanford Research Institute. In a series of 35 studies, they were unable to replicate the results so investigated the procedure of the original experiments. Marks and Kammann discovered that the notes given to the judges in Targ and Puthoff's experiments contained clues as to which order they were carried out, such as referring to yesterday's two targets, or they had the date of the session written at the top of the page. They concluded that these clues were the reason for the experiment's high hit rates. Marks was able to achieve 100 per cent accuracy without visiting any of the sites himself but by using cues. James Randi has written controlled tests by several other researchers, eliminating several sources of cuing and extraneous evidence present in the original tests, produced negative results. Students were also able to solve Puthoff and Targ's locations from the clues that had inadvertently been included in the transcripts.\n\nIn 1980, Charles Tart claimed that a rejudging of the transcripts from one of Targ and Puthoff's experiments revealed an above-chance result. Targ and Puthoff again refused to provide copies of the transcripts and it was not until July 1985 that they were made available for study when it was discovered they still contained sensory cues. Marks and Christopher Scott (1986) wrote \"considering the importance for the remote viewing hypothesis of adequate cue removal, Tart's failure to perform this basic task seems beyond comprehension. As previously concluded, remote viewing has not been demonstrated in the experiments conducted by Puthoff and Targ, only the repeated failure of the investigators to remove sensory cues.\"\n\nIn 1982 Robert Jahn, then Dean of the School of Engineering at Princeton University wrote a comprehensive review of psychic phenomena from an engineering perspective. His paper included numerous references to remote viewing studies at the time. Statistical flaws in his work have been proposed by others in the parapsychological community and within the general scientific community.\n\nAccording to scientific research, clairvoyance is generally explained as the result of confirmation bias, expectancy bias, fraud, hallucination, self-delusion, sensory leakage, subjective validation, wishful thinking or failures to appreciate the base rate of chance occurrences and not as a paranormal power. Parapsychology is regarded by the scientific community as a pseudoscience. In 1988, the US National Research Council concluded \"The committee finds no scientific justification from research conducted over a period of 130 years, for the existence of parapsychological phenomena.\"\n\nSkeptics say that if clairvoyance were a reality it would have become abundantly clear. They also contend that those who believe in paranormal phenomena do so for merely psychological reasons. According to David G. Myers (\"Psychology,\" 8th ed.):\n\nThe search for a valid and reliable test of clairvoyance has resulted in thousands of experiments. One controlled procedure has invited 'senders' to telepathically transmit one of four visual images to 'receivers' deprived of sensation in a nearby chamber (Bem & Honorton, 1994). The result? A reported 32 percent accurate response rate, surpassing the chance rate of 25 percent. But follow-up studies have (depending on who was summarizing the results) failed to replicate the phenomenon or produced mixed results (Bem & others, 2001; Milton & Wiseman, 2002; Storm, 2000, 2003).One skeptic, magician James Randi, has a longstanding offer—now U.S. $1 million—\"to anyone who proves a genuine psychic power under proper observing conditions\" (Randi, 1999). French, Australian, and Indian groups have parallel offers of up to 200,000 euros to anyone with demonstrable paranormal abilities (CFI, 2003). Large as these sums are, the scientific seal of approval would be worth far more to anyone whose claims could be authenticated. To refute those who say there is no ESP, one need only produce a single person who can demonstrate a single, reproducible ESP phenomenon. So far, no such person has emerged. Randi's offer has been publicized for three decades and dozens of people have been tested, sometimes under the scrutiny of an independent panel of judges. Still, nothing. \"People's desire to believe in the paranormal is stronger than all the evidence that it does not exist.\" Susan Blackmore, \"Blackmore's first law\", 2004.\n\nClairvoyance is considered hallucination by mainstream psychiatry.\n\n\n"}
{"id": "50910908", "url": "https://en.wikipedia.org/wiki?curid=50910908", "title": "Clubes de Ciencia", "text": "Clubes de Ciencia\n\nClubes de Ciencia is a non-profit organization founded in 2014 that organizes hands-on week-long workshops in STEM to kids in developing countries at no cost. The instructors are PhD volunteers from top universities, such as Harvard, Princeton, MIT who organizes the workshops and mentor the students aiming at inspiring students to pursue their passions. By combining hands-on experimental learning, on-line exercises and mentorship, Clubes de Ciencia takes a unique approach to educating the millennials in developing countries. In two years, Clubes de Ciencia grew past its Mexico program, to also work in Colombia and Bolivia. Currently, it also operates in Brazil, Paraguay, Peru and Spain. \n\nThe first edition of Science Clubs was organized in Guanajuato, Mexico, in January 2014,with the support of Universidad de Guanajuato.\n\nIn 2017, Science Clubs expanded to three new countries: Paraguay, Peru and Brazil \n\nThe first edition of the Brazilian Chapter (Clubes de Ciência Brasil) was held at Universidade Federal de Minas Gerais, in Belo Horizonte, in July 2017. Four clubs in the areas of epidemiology, stem cells and gene editing, immunology and entrepreneurship were organized by researchers from top Universities from the US and Brazil, including Harvard, Northeastern and UFMG for 80 students from six different states of the country. \n\nOn April 15, 2015, the Latin American Science Education Network (now Science Clubs International) was among the winners of the MIT \"IDEAS\" Global Challenge Awards.\n\nAlso in 2015, Mohammed Mostajo-Radji, as Executive Director of Clubes de Ciencia Bolivia was awarded the \"Person of the Year\" award by the Bolivian newspaper El Deber and the Franz Tamayo Medal by the Senate of Bolivia.\n\nIn 2016, Hugo Arellano-Santoyo and Clubes de Ciencia were awarded with the Dean's Community Service Award from the Harvard Medical School. The prize is awarded \"to recognize individuals whose dedication and commitment to community service have made a positive impact on the local, national, or international community\".\n\nIn 2016, Maier Avendano, Executive Director of Clubes de Ciencia Colombia was named among the \"Latino 30 under 30\" by the El Mundo Boston. Mohammed Mostajo-Radji and the team of Clubes de Ciencia Bolivia received this award in 2017. \n\nAdditionally in 2016, Clubes de Ciencia Bolivia was awarded the Youth Peace Prize by the Government of Santa Cruz, Bolivia. \n\nIn 2018, Clubes de Ciencia Bolivia received the Melchor Pinto Parada award from the Government of Santa Cruz . This is the maximum award granted by this institution. Also in 2018, Omar Gandarilla, as Operations Director of Clubes de Ciencia Bolivia received the \"Diversity in STEM\" award from MiniPCR.\n\nThe David Rockefeller Center for Latin American Studies, the Department of Molecular and Cellular Biology at Harvard University, the Harvard Stem Cell Institute, COMEXUS and the Fundación México en Harvard University are among the key sponsors of the project.\n\nClubes de Ciencia has been endorsed by a number of academics and celebrities, including 2004 Nobel Laureate Frank Wilczek; Margot Gill, Dean of International Affairs at Harvard University and Beakman.\n"}
{"id": "8989793", "url": "https://en.wikipedia.org/wiki?curid=8989793", "title": "Coding (social sciences)", "text": "Coding (social sciences)\n\nIn the social sciences, coding is an analytical process in which data, in both quantitative form (such as questionnaires results) or qualitative form (such as interview transcripts) are categorized to facilitate analysis.\n\nOne purpose of coding is to transform the data into a form suitable for computer-aided analysis. This categorization of information is an important step, for example, in preparing data for computer processing with statistical software.\n\nSome studies will employ multiple coders working independently on the same data. This minimizes the chance of errors from coding and is believed to increase the reliability of data.\n\nOne code should apply to only one category and categories should be comprehensive. There should be clear guidelines for \"coders\" (individual who do the coding) so that code is consistent.\n\nFor quantitative analysis, data is coded usually into measured and recorded as nominal or ordinal variables.\n\nQuestionnaire data can be \"pre-coded\" (process of assigning codes to expected answers on designed questionnaire), \"field-coded\" (process of assigning codes as soon as data is available, usually during fieldwork), \"post-coded\" (coding of open questions on completed questionnaires) or \"office-coded\" (done after fieldwork). Note that some of the above are not mutually exclusive.\n\nIn social sciences, spreadsheets such as Excel and more advanced software packages such as R, Matlab, PSPP/SPSS, DAP/SAS, MiniTab and Stata are often used.\n\nFor disciplines in which a qualitative format is preferential, including ethnography, humanistic geography or phenomenological psychology a varied approach to coding can be applied. Iain Hay (2005) outlines a two-step process beginning with basic coding in order to distinguish overall themes, followed by a more in depth, interpretive code in which more specific trends and patterns can be interpreted.\n\nMuch of qualitative coding can be attributed to either grounded or \"a priori\" coding. Grounded coding refers to allowing notable themes and patterns emerge from the document themselves, where as \"a priori\" coding requires the researcher to apply pre-existing theoretical frameworks to analyze the documents. As coding methods are applied across various texts, the researcher is able to apply axial coding, which is the process of selecting core thematic categories present in several documents to discover common patterns and relations.\n\nPrior to constructing categories, a researcher must apply a first cycle coding method. There are a multitude of methods available, and a researcher will want to pick one that is suited for the format and nature of their documents. Not all methods can be applied to every type of document. Some examples of first cycle coding methods include:\n\n\nThe process can be done manually, which can be as simple as highlighting different concepts with different colours, or fed into a software package. Some examples of qualitative software packages include Atlas.ti, MAXQDA, NVivo, and QDA Miner.\n\nAfter assembling codes it is time to organize them into broader themes and categories. The process generally involves identifying themes from the existing codes, reducing the themes to a manageable number, creating hierarchies within the themes and then linking themes together through theoretical modeling.\n\nCreating memos during the coding process is integral to both grounded and a priori coding approaches. Qualitative research is inherently reflexive; as the researcher delves deeper into their subject, it is important to chronicle their own thought processes through reflective or methodological memos, as doing so may highlight their own subjective interpretations of data It is crucial to begin memoing at the onset of research. Regardless of the type of memo produced, what is important is that the process initiates critical thinking and productivity in the research. Doing so will facilitate easier and more coherent analyses as the project draws on \nMemos can be used to map research activities, uncover meaning from data, maintaining research momentum and engagement and opening communication.\n\n\nHay, I. (2005). \"Qualitative research methods in human geography\" (2nd ed.). Oxford: Oxford University Press.\n\nGrbich, Carol. (2013). \"Qualitative Data Analysis\" (2nd ed.). The Flinders University of South Australia: SAGE Publications Ltd.\n\nSaldaña, Johnny. (2015). \"The Coding Manual for Qualitative Researchers\" (3rd ed.). SAGE Publications Ltd.\n"}
{"id": "33615960", "url": "https://en.wikipedia.org/wiki?curid=33615960", "title": "Comparison of chemistry and physics", "text": "Comparison of chemistry and physics\n\nChemistry and physics are branches of science that both study matter. The difference between the two lies in their scope and approach. Chemists and physicists are trained differently, and they have different professional roles, even when working in a team. The division between chemistry and physics becomes diffuse at the interface of the two branches, notably in fields such as physical chemistry, chemical physics, quantum mechanics, nuclear physics/chemistry, materials science, spectroscopy, solid state physics, solid-state chemistry, crystallography, and nanotechnology.\n\nPhysics and chemistry may overlap when the system under study involves matter composed of electrons and nuclei made of protons and neutrons. On the other hand, chemistry is not concerned with other forms of matter such as quarks, mu and tau leptons and dark matter.\n\nAlthough fundamental laws that govern the behavior of matter apply both in chemistry and physics, the disciplines of physics and chemistry are distinct. Physics is concerned with nature from a very large scale (the entire universe) down to a very small scale (subatomic particles). All physical phenomena that are measurable follow some behavior that is in accordance with the most basic principles studied in physics.\n\nPhysics is involved with the fundamental principles of physical phenomena and the basic forces of nature, and also gives insight into the aspects of space and time. Physics also deals with the basic principles that explain matter and energy, and may study aspects of atomic matter by following concepts derived from the most fundamental principles.\n\nChemistry focuses on how substances interact with each other and with energy (for example heat and light). The study of change of matter (chemical reactions) and synthesis lies at the heart of chemistry, and gives rise to concepts such as organic functional groups and rate laws for chemical reactions. Chemistry also studies the properties of matter at a larger scale (for example, astrochemistry) and the reactions of matter at a larger scale (for example, technical chemistry), but typically, explanations and predictions are related back to the underlying atomic structure, giving more emphasis on the methods for the identification of molecules and their mechanisms of transformation than any other science.\n\nAlthough both physics and chemistry are concerned with matter and its interaction with energy, the two disciplines differ in approach. In physics, it is typical to abstract from the specific type of matter, and to focus on the common properties of many different materials. In optics, for example, materials are characterized by their index of refraction, and materials with the same index of refraction will have identical properties. Chemistry, on the other hand, focuses on what compounds are present in a sample, and explores how changing the structure of molecules will change their reactivity and their physical properties.\n\nThe two sciences differ in the role that theory plays within the discipline. Physics can be divided into experimental and theoretical physics. Historically, theoretical physics has correctly predicted phenomena that were out of experimental reach at the time, and could be verified only after experimental techniques caught up. In chemistry, the role of theory historically has been a retrospective one, summarizing experimental data and predicting the outcome of similar experiments. However, with the increasing power of computational methods in chemistry, it has become possible to predict whether a hypothetical compound is stable or not before experimental data is available.\n\nIn a typical undergraduate program for physics majors, required courses are in the sub-disciplines of physics, with additional required courses in mathematics. Because much of the insight of physics is described by differential equations relating matter, space, and time (for example Newton's law of motion and the Maxwell equations of electromagnetism), students have to be familiar with differential equations. In a typical undergraduate program for chemistry majors, emphasis is placed on laboratory classes and understanding and applying models describing chemical bonds and molecular structure. Emphasis is also placed in the methods for analysis and the formulas and equations used when considering the chemical transformation. Students take courses in math, physics, chemistry, and often biochemistry. Between the two programs of study, there is a large area of overlap (calculus, introductory physics, quantum mechanics, thermodynamics). However, physics places a larger emphasis on fundamental theory (with its deep mathematical treatment) while chemistry places more emphasis in combining the most important mathematical definitions of the theory with the approach of the molecular models. Laboratory skills may differ in both programs, as students may be involved in different technologies, depending on the program and the institution of higher education (for example, a chemistry student may spend more laboratory time dealing with glassware for distillation and purification or on a form of chromatography-spectroscopy instrument, while a physics student may spend much more time dealing with a laser and non-linear optics technology or some complex electrical circuit).\n\nAccording to Bureau of Labor Statistics (United States Department of Labor), there are 80,000 chemists and 17,000 physicists working in the United States as of May 2010. In addition, 21,000 chemists and 13,500 physicists teach in high school. Chemistry is the only science that has an entire industry, the chemical industry, named after it, and many chemists work in this industry, in research and development, production, training, or management. Other industries employing chemists include the petroleum, pharmaceutical, and food industry. While there is no industry named after physics, many industries have grown out of physics research, most notably the semiconductor and electronics industry. Physicists are also employed outside of science, for example in finance, because of their training in modeling complex systems.\n\nChemistry and physics are not strictly separated sciences, and chemists and physicists work in interdisciplinary teams to explore the following topics.\n"}
{"id": "30694430", "url": "https://en.wikipedia.org/wiki?curid=30694430", "title": "Criticism of the theory of relativity", "text": "Criticism of the theory of relativity\n\nCriticism of the theory of relativity of Albert Einstein was mainly expressed in the early years after its publication in the early twentieth century, on scientific, pseudoscientific, philosophical, or ideological bases. Though some of these criticisms had the support of reputable scientists, Einstein's theory of relativity is now accepted by the scientific community.\n\nReasons for criticism of the theory of relativity have included alternative theories, rejection of the abstract-mathematical method, and alleged errors of the theory. According to some authors, antisemitic objections to Einstein's Jewish heritage also occasionally played a role in these objections. There are still some critics of relativity today, but their opinions are not shared by the majority in the scientific community.\n\nAround the end of the 19th century, the view was widespread that all forces in nature are of electromagnetic origin (the \"electromagnetic worldview\"), especially in the works of Joseph Larmor (1897) and Wilhelm Wien (1900). This was apparently confirmed by the experiments of Walter Kaufmann (1901–1903), who measured an increase of the mass of a body with velocity which was consistent with the hypothesis that the mass was generated by its electromagnetic field. Max Abraham (1902) subsequently sketched a theoretical explanation of Kaufmann's result in which the electron was considered as rigid and spherical. However, it was found that this model was incompatible with the results of many experiments (including the Michelson–Morley experiment, the Experiments of Rayleigh and Brace, and the Trouton–Noble experiment), according to which no motion of an observer with respect to the luminiferous aether (\"aether drift\") had been observed despite numerous attempts to do so. Henri Poincaré (1902) conjectured that this failure arose from a general law of nature, which he called \"the principle of relativity\". Hendrik Antoon Lorentz (1904) created a detailed theory of electrodynamics (Lorentz ether theory) that was premised on the existence of an immobile aether and employed a set of space and time coordinate transformations that Poincaré called the Lorentz transformations, including the effects of length contraction and local time. However, Lorentz's theory only partially satisfied the relativity principle, because his transformation formulas for velocity and charge density were incorrect. This was corrected by Poincaré (1905) who obtained full Lorentz covariance of the electrodynamic equations.\n\nCriticizing Lorentz's 1904 theory, Abraham (1904) held that the Lorentz contraction of electrons requires a non-electromagnetic force to ensure the electron's stability. This was unacceptable to him as a proponent of the electromagnetic worldview. He continued that as long as a consistent explanation is missing as to how those forces and potentials act together on the electron, Lorentz's system of hypotheses is incomplete and doesn't satisfy the relativity principle. Poincaré (1905) removed this objection by showing that the non-electromagnetic potential (\"Poincaré stress\") holding the electron together can be formulated in a Lorentz covariant way, and showed that in principle it is possible to create a Lorentz covariant model for gravitation which he considered non-electromagnetic in nature as well. Thus the consistency of Lorentz's theory was proven, but the electromagnetic worldview had to be given up. Eventually, Albert Einstein published in September 1905 what is now called special relativity, which was based on a radical new application of the relativity principle in connection with the constancy of the speed of light. In special relativity, the space and time coordinates depend on the inertial observer's frame of reference, and the luminiferous aether plays no role in the physics. Although this theory was founded on a very different kinematical model, it was experimentally indistinguishable from the aether theory of Lorentz and Poincaré, since both theories satisfy the relativity principle of Poincaré and Einstein, and both employ the Lorentz transformations. After Minkowski's introduction in 1908 of the geometric spacetime model for Einstein's version of relativity, most physicists eventually decided in favor of the Einstein-Minkowski version of relativity with its radical new views of space and time, in which there was no useful role for the aether.\n\nKaufmann–Bucherer–Neumann experiments: To conclusively decide between the theories of Abraham and Lorentz, Kaufmann repeated his experiments in 1905 with improved accuracy. However, in the meantime the theoretical situation had changed. Alfred Bucherer and Paul Langevin (1904) developed another model, in which the electron is contracted in the line of motion, and dilated in the transverse direction, so that the volume remains constant. While Kaufmann was still evaluating his experiments, Einstein published his theory of special relativity. Eventually, Kaufmann published his results in December 1905 and argued that they are in agreement with Abraham's theory and require rejection of the \"basic assumption of Lorentz and Einstein\" (the relativity principle). Lorentz reacted with the phrase \"I am at the end of my Latin\", while Einstein did not mention those experiments before 1908. Yet, others started to criticize the experiments. Max Planck (1906) alluded to inconsistencies in the theoretical interpretation of the data, and Adolf Bestelmeyer (1906) introduced new techniques, which (especially in the area of low velocities) gave different results and which cast doubts on Kaufmann's methods. Therefore, Bucherer (1908) conducted new experiments and arrived at the conclusion that they confirm the mass formula of relativity and thus the \"relativity principle of Lorentz and Einstein\". Yet Bucherer's experiments were criticized by Bestelmeyer leading to a sharp dispute between the two experimentalists. On the other hand, additional experiments of Hupka (1910), Neumann (1914) and others seemed to confirm Bucherer's result. The doubts lasted until 1940, when in similar experiments Abraham's theory was conclusively disproved. (It must be remarked that besides those experiments, the relativistic mass formula had already been confirmed by 1917 in the course of investigations on the theory of spectra. In modern particle accelerators, the relativistic mass formula is routinely confirmed.)\n\nIn 1902–1906, Dayton Miller repeated the Michelson–Morley experiment together with Edward W. Morley. They confirmed the null result of the initial experiment. However, in 1921–1926, Miller conducted new experiments which apparently gave positive results. Those experiments initially attracted some attention in the media and in the scientific community but have been considered refuted for the following reasons: Einstein, Max Born, and Robert S. Shankland pointed out that Miller hadn't appropriately considered the influence of temperature. A modern analysis by Roberts shows that Miller's experiment gives a null result, when the technical shortcomings of the apparatus and the error bars are properly considered. Additionally, Miller's result is in disagreement with all other experiments, which were conducted before and after. For example, Georg Joos (1930) used an apparatus of similar dimensions to Miller's, but he obtained null results. In recent experiments of Michelson–Morley type where the coherence length is increased considerably by using lasers and masers the results are still negative.\n\nIn the 2011 Faster-than-light neutrino anomaly, the OPERA collaboration published results which appeared to show that the speed of neutrinos is slightly faster than the speed of light. However, sources of errors were found and confirmed in 2012 by the OPERA collaboration, which fully explained the initial results. In their final publication, a neutrino speed consistent with the speed of light was stated. Also subsequent experiments found agreement with the speed of light, see measurements of neutrino speed.\n\nIt was also claimed that special relativity cannot handle acceleration, which would lead to contradictions in some situations. However, this assessment is not correct, since acceleration actually can be described in the framework of special relativity (see Acceleration (special relativity), Proper reference frame (flat spacetime), Hyperbolic motion, Rindler coordinates, Born coordinates). Paradoxes relying on insufficient understanding of these facts were discovered in the early years of relativity. For example, Max Born (1909) tried to combine the concept of rigid bodies with special relativity. That this model was insufficient was shown by Paul Ehrenfest (1909), who demonstrated that a rotating rigid body would, according to Born's definition, undergo a contraction of the circumference without contraction of the radius, which is impossible (Ehrenfest paradox). Max von Laue (1911) showed that rigid bodies cannot exist in special relativity, since the propagation of signals cannot exceed the speed of light, so an accelerating and rotating body will undergo deformations.\n\nPaul Langevin and von Laue showed that the twin paradox can be completely resolved by consideration of acceleration in special relativity. If two twins move away from each other, and one of them is accelerating and coming back to the other, then the accelerated twin is younger than the other one, since he was located in at least two inertial frames of reference, and therefore his assessment of which events are simultaneous changed during the acceleration. For the other twin nothing changes since he remained in a single frame.\n\nAnother example is the Sagnac effect. Two signals were sent in opposite directions around a rotating platform. After their arrival a displacement of the interference fringes occurs. Sagnac himself believed that he had proved the existence of the aether. However, special relativity can easily explain this effect. When viewed from an inertial frame of reference, it is a simple consequence of the independence of the speed of light from the speed of the source, since the receiver runs away from one beam, while it approaches the other beam. When viewed from a rotating frame, the assessment of simultaneity changes during the rotation, and consequently the speed of light is not constant in accelerated frames.\n\nAs was shown by Einstein, the only form of accelerated motion that cannot be described is the one due to gravitation, since special relativity is not compatible with the Equivalence principle. Einstein was also unsatisfied with the fact that inertial frames are preferred over accelerated frames. Thus over the course of several years (1908–1915), Einstein developed general relativity. This theory includes the replacement of Euclidean geometry by non-Euclidean geometry, and the resultant curvature of the path of light led Einstein (1912) to the conclusion that (like in accelerated frames) the speed of light is not constant in extended gravitational fields. Therefore, Abraham (1912) argued that Einstein had given special relativity a coup de grâce. Einstein responded that within its area of application (in areas where gravitational influences can be neglected) special relativity is still applicable with high precision, so one cannot speak of a coup de grâce at all.\n\nIn special relativity, the transfer of signals at superluminal speeds is impossible, since this would violate the Poincaré-Einstein synchronization, and the causality principle. Following an old argument by Pierre-Simon Laplace, Poincaré (1904) alluded to the fact that Newton's law of universal gravitation is founded on an infinitely great speed of gravity. So the clock-synchronization by light signals could in principle be replaced by a clock-synchronization by instantaneous gravitational signals. In 1905, Poincaré himself solved this problem by showing that in a relativistic theory of gravity the speed of gravity is equal to the speed of light. Although much more complicated, this is also the case in Einstein's theory of general relativity.\n\nAnother apparent contradiction lies in the fact that the group velocity in anomalously dispersive media is higher than the speed of light. This was investigated by Arnold Sommerfeld (1907, 1914) and Léon Brillouin (1914). They came to the conclusion that in such cases the signal velocity is not equal to the group velocity, but to the front velocity which is never faster than the speed of light. Similarly, it is also argued that the apparent superluminal effects discovered by Günter Nimtz can be explained by a thorough consideration of the velocities involved.\n\nAlso quantum entanglement (denoted by Einstein as \"spooky action at a distance\"), according to which the quantum state of one entangled particle cannot be fully described without describing the other particle, does not imply superluminal transmission of information (see quantum teleportation), and it is therefore in conformity with special relativity.\n\nInsufficient knowledge of the basics of special relativity, especially the application of the Lorentz transformation in connection with length contraction and time dilation, led and still leads to the construction of various apparent paradoxes. Both the twin paradox and the Ehrenfest paradox and their explanation were already mentioned above. Besides the twin paradox, also the reciprocity of time dilation (\"i.e.\" every inertially moving observer considers the clock of the other one as being dilated) was heavily criticized by Herbert Dingle and others. For example, Dingle wrote a series of letters to Nature at the end of the 1950s. However, the self-consistency of the reciprocity of time dilation had already been demonstrated long before in an illustrative way by Lorentz (in his lectures from 1910, published 1931) and many others—they alluded to the fact that it is only necessary to carefully consider the relevant measurement rules and the relativity of simultaneity. Other known paradoxes are the Ladder paradox and Bell's spaceship paradox, which also can simply be solved by consideration of the relativity of simultaneity.\n\nMany physicists (like Hendrik Lorentz, Oliver Lodge, Albert Abraham Michelson, Edmund Taylor Whittaker, Harry Bateman, Ebenezer Cunningham, Charles Émile Picard, Paul Painlevé) were uncomfortable with the rejection of the aether, and preferred to interpret the Lorentz transformation based on the existence of a preferred frame of reference, as in the aether-based theories of Lorentz, Larmor, and Poincaré. However, the idea of an aether hidden from any observation was not supported by the mainstream scientific community, therefore the aether theory of Lorentz and Poincaré was superseded by Einstein's special relativity which was subsequently formulated in the framework of four-dimensional spacetime by Minkowski.\n\nOthers such as Herbert E. Ives argued that it might be possible to experimentally determine the motion of such an aether, but it was never found despite numerous experimental tests of Lorentz invariance (see tests of special relativity).\n\nAlso attempts to introduce some sort of relativistic aether (consistent with relativity) into modern physics such as by Einstein on the basis of general relativity (1920), or by Paul Dirac in relation to quantum mechanics (1951), were not supported by the scientific community (see Luminiferous aether#End of aether?).\n\nIn his Nobel lecture, George F. Smoot (2006) described his own experiments on the Cosmic microwave background radiation anisotropy as \"New Aether drift experiments\". Smoot explained that \"one problem to overcome was the strong prejudice of good scientists who learned the lesson of the Michelson and Morley experiment and Special Relativity that there were no preferred frames of reference.\" He continued that \"there was an education job to convince them that this did not violate Special Relativity but did find a frame in which the expansion of the universe looked particularly simple.\"\n\nThe theory of complete aether drag, as proposed by George Gabriel Stokes (1844), was used by some critics as Ludwig Silberstein (1920) or Philipp Lenard (1920) as a counter-model of relativity. In this theory, the aether was completely dragged within and in the vicinity of matter, and it was believed that various phenomena, such as the absence of aether drift, could be explained in an \"illustrative\" way by this model. However, such theories are subject to great difficulties. Especially the aberration of light contradicted the theory, and all auxiliary hypotheses, which were invented to rescue it, are self-contradictory, extremely implausible, or in contradiction to other experiments like the Michelson–Gale–Pearson experiment. In summary, a sound mathematical and physical model of complete aether drag was never invented, consequently this theory was no serious alternative to relativity.\n\nAnother alternative was the so-called emission theory of light. As in special relativity the aether concept is discarded, yet the main difference from relativity lies in the fact that the velocity of the light source is added to that of light in accordance with the Galilean transformation. As the hypothesis of complete aether drag, it can explain the negative outcome of all aether drift experiments. Yet, there are various experiments that contradict this theory. For example, the Sagnac effect is based on the independence of light speed from the source velocity, and the image of Double stars should be scrambled according to this model—which was not observed. Also in modern experiments in particle accelerators no such velocity dependence could be observed. These results are further confirmed by the De Sitter double star experiment (1913), conclusively repeated in the X-ray spectrum by K. Brecher in 1977;\nand the terrestrial experiment by Alväger, \"et al\". (1963);, which all show that the speed of light is independent of the motion of the source within the limits of experimental accuracy.\n\nSome consider the \"principle of the constancy of the velocity of light\" insufficiently substantiated. However, as already shown by Robert Daniel Carmichael (1910) and others, the constancy of the speed of light can be interpreted as a natural consequence of \"two\" experimentally demonstrated facts:\n\n\nNote that measurements regarding the speed of light are actually measurements of the two-way speed of light, since the one-way speed of light depends on which convention is chosen to synchronize the clocks.\n\nEinstein emphasized the importance of general covariance for the development of general relativity, and took the position that the general covariance of his 1915 theory of gravity ensured implementation of a generalized relativity principle. This view was challenged by Erich Kretschmann (1917), who argued that every theory of space and time (even including Newtonian dynamics) can be formulated in a covariant way, if additional parameters are included, and thus general covariance of a theory would in itself be insufficient to implement a generalized relativity principle. Although Einstein (1918) agreed with that argument, he also countered that Newtonian mechanics in general covariant form would be too complicated for practical uses. Although it is now understood that Einstein's response to Kretschmann was mistaken (subsequent papers showed that such a theory would still be usable), another argument can be made in favor of general covariance: it is a natural way to express the equivalence principle, \"i.e.\", the equivalence in the description of a free-falling observer and an observer at rest, and thus it is more convenient to use general covariance together with general relativity, rather than with Newtonian mechanics. Connected with this, also the question of absolute motion was dealt with. Einstein argued that the general covariance of his theory of gravity supports Mach's principle, which would eliminate any \"absolute motion\" within general relativity. However, as pointed out by Willem de Sitter in 1916, Mach's principle is not completely fulfilled in general relativity because there exist matter-free solutions of the field equations. This means that the \"inertio-gravitational field\", which describes both gravity and inertia, can exist in the absence of gravitating matter. However, as pointed out by Einstein, there is one fundamental difference between this concept and absolute space of Newton: the inertio-gravitational field of general relativity is determined by matter, thus it is not absolute.\n\nIn the \"Bad Nauheim Debate\" (1920) between Einstein and (among others) Philipp Lenard, the latter stated the following objections: He criticized the lack of \"illustrativeness\" of Einstein's version of relativity, a condition that he suggested could only be met by an aether theory. Einstein responded that for physicists the content of \"illustrativeness\" or \"common sense\" had changed in time, so it could no longer be used as a criterion for the validity of a physical theory. Lenard also argued that with his relativistic theory of gravity Einstein had tacitly reintroduced the aether under the name \"space\". While this charge was rejected (among others) by Hermann Weyl, in an inaugural address given at the University of Leiden in 1920, shortly after the Bad Nauheim debates, Einstein himself acknowledged that according to his general theory of relativity, so-called \"empty space\" possesses physical properties that influence matter and \"vice versa\". Lenard also argued that Einstein's general theory of relativity admits the existence of superluminal velocities, in contradiction to the principles of special relativity; for example, in a rotating coordinate system in which the Earth is at rest, the distant points of the whole universe are rotating around Earth with superluminal velocities. However, as been pointed out by Weyl, it's not possible to handle a rotating extended system as a rigid body (neither in special nor in general relativity)—so the signal velocity of an object never exceeds the speed of light. Another criticism that was raised by both Lenard and Gustav Mie concerned the existence of \"fictitious\" gravitational fields in accelerating frames, which according to Einstein's Equivalence Principle are no less physically real than those produced by material sources. Lenard and Mie argued that physical forces can only be produced by real material sources, while the gravitational field that Einstein supposed to exist in an accelerating frame of reference has no concrete physical meaning. Einstein responded that, based on Mach's principle, one can think of these gravitational fields as induced by the distant masses. In this respect the criticism of Lenard and Mie has been vindicated, since according to the modern consensus, in agreement with Einstein's own mature views, Mach's principle as originally conceived by Einstein is not actually supported by general relativity, as already mentioned above.\n\nLudwik Silberstein, who initially was a supporter of the special theory, objected at different occasions against general relativity. In 1920 he argued that the deflection of light by the sun, as observed by Arthur Eddington et al. (1919), is not necessarily a confirmation of general relativity, but may also be explained by the Stokes-Planck theory of complete aether drag. However, such models are in contradiction with the aberration of light and other experiments (see \"Alternative theories\"). In 1935, Silberstein claimed to have found a contradiction in the Two-body problem in general relativity. However, also this claim was refuted by Einstein and Rosen (1935).\n\nThe consequences of relativity, such as the change of ordinary concepts of space and time, as well as the introduction of non-Euclidean geometry in general relativity, were criticized by some philosophers of different philosophical schools. It was characteristic for many philosophical critics that they had insufficient knowledge of the mathematical and formal basis of relativity, which led to the criticisms often missing the heart of the matter. For example, relativity was misinterpreted as some form of relativism. However, this is misleading as it was emphasized by Einstein or Planck. On one hand it's true that space and time became relative, and the inertial frames of reference are handled on equal footing. On the other hand, the theory makes natural laws invariant—examples are the constancy of the speed of light, or the covariance of Maxwell's equations. Consequently, Felix Klein (1910) called it the \"invariant theory of the Lorentz group\" instead of relativity theory, and Einstein (who reportedly used expressions like \"absolute theory\") sympathized with this expression as well.\n\nCritical responses to relativity were also expressed by proponents of Neo-Kantianism (Paul Natorp, Bruno Bauch, etc.), and Phenomenology (Oskar Becker, Moritz Geiger etc.). While some of them only rejected the philosophical consequences, others rejected also the physical consequences of the theory. Einstein was criticized for violating Immanuel Kant's categoric scheme, \"i.e.\", it was claimed that space-time curvature caused by matter and energy is impossible, since matter and energy already require the concepts of space and time. Also the three-dimensionality of space, Euclidean geometry, and the existence of absolute simultaneity were claimed to be necessary for the understanding of the world; none of them can possibly be altered by empirical findings. By moving all those concepts into a metaphysical area, any form of criticism of Kantianism would be prevented. Other pseudo-Kantians like Ernst Cassirer or Hans Reichenbach (1920), tried to modify Kant's philosophy. Subsequently, Reichenbach rejected Kantianism at all and became a proponent of logical positivism.\n\nBased on Henri Poincaré's conventionalism, philosophers such as Pierre Duhem (1914) or Hugo Dingler (1920) argued that the classical concepts of space, time, and geometry were, and will always be, the most convenient expressions in natural science, therefore the concepts of relativity cannot be correct. This was criticized by proponents of logical positivism such as Moritz Schlick, Rudolf Carnap, or Reichenbach. They argued that Poincaré's conventionalism could be modified, as to bring it into accord with relativity. Although it is true that the basic assumptions of Newtonian mechanics are simpler, it can only be brought into accord with modern experiments by inventing auxiliary hypotheses. On the other hand, relativity doesn't need such hypotheses, thus from a conceptual viewpoint, relativity is in fact simpler than Newtonian mechanics.\n\nSome proponents of Philosophy of Life, Vitalism, Critical realism (in German speaking countries) argued that there is a fundamental difference between physical, biological and psychological phenomena. For example, Henri Bergson (1921), who otherwise was a proponent of special relativity, argued that time dilation cannot be applied to biological organisms, therefore he denied the relativistic solution of the twin paradox. However, those claims were rejected by Paul Langevin, André Metz and others. Biological organisms consist of physical processes, so there is no reason to assume that they are not subject to relativistic effects like time dilation.\n\nBased on the philosophy of Fictionalism, the philosopher Oskar Kraus (1921) and others claimed that the foundations of relativity were only fictitious and even self-contradictory. Examples were the constancy of the speed of light, time dilation, length contraction. These effects appear to be mathematically consistent as a whole, but in reality they allegedly are not true. Yet, this view was immediately rejected. The foundations of relativity (such as the equivalence principle or the relativity principle) are not fictitious, but based on experimental results. Also, effects like constancy of the speed of light and relativity of simultaneity are not contradictory, but complementary to one another.\n\nIn the Soviet Union (mostly in the 1920s), philosophical criticism was expressed on the basis of dialectic materialism. The theory of relativity was rejected as anti-materialistic and speculative, and a mechanistic worldview based on \"common sense\" was required as an alternative. Similar criticisms also occurred in the People's Republic of China during the Cultural Revolution. (On the other hand, other philosophers considered relativity as being compatible with Marxism.)\n\nAlthough Planck already in 1909 compared the changes brought about by relativity with the Copernican Revolution, and although special relativity was accepted by most of the theoretical physicists and mathematicians by 1911, it was not before publication of the experimental results of the eclipse expeditions (1919) by a group around Arthur Stanley Eddington that relativity was noticed by the public. Following Eddington's publication of the eclipse results, Einstein was glowingly praised in the mass media, and was compared to Nikolaus Copernicus, Johannes Kepler and Isaac Newton, which caused a popular \"relativity hype\" (\"Relativitätsrummel\", as it was called by Sommerfeld, Einstein, and others). This triggered a counter-reaction of some scientists and scientific laymen who could not accept the concepts of modern physics, including relativity theory and quantum mechanics. The ensuing public controversy regarding the scientific status of Einstein's theory of gravity, which was unprecedented, was partly carried out in the press. Some of the criticism was not only directed to relativity, but personally at Einstein as well, who some of his critics accused of being behind the promotional campaign in the German press. \n\nSome academic scientists, especially experimental physicists such as the Nobel laureates Philipp Lenard and Johannes Stark, as well as Ernst Gehrcke, Stjepan Mohorovičić, Rudolf Tomaschek and others criticized the increasing abstraction and mathematization of modern physics, especially in the form of relativity theory, and later quantum mechanics. It was seen as a tendency to abstract theory building, connected with the loss of intuitive \"common sense\". In fact, relativity was the first theory, in which the inadequacy of the \"illustrative\" classical physics was thought to have been demonstrated. Some of Einstein's critics ignored these developments and tried to revitalize older theories, such as aether drag models or emission theories (see \"Alternative Theories\"). However, those qualitative models were never sufficiently advanced to compete with the success of the precise experimental predictions and explanatory powers of the modern theories. Additionally, there was also a great rivalry between experimental and theoretical physicists, as regards the professorial activities and the occupation of chairs at German universities. The opinions clashed at the \"Bad Nauheim debates\" in 1920 between Einstein and (among others) Lenard, which attracted much attention in the public.\n\nIn addition, there were many critics (with or without physical training) whose ideas were far outside the scientific mainstream. These critics were mostly people who had developed their ideas long before the publication of Einstein's version of relativity, and they tried to resolve in a straightforward manner some or all of the enigmas of the world. Therefore, Wazeck (who studied some German examples) gave to these \"free researchers\" the name \"world riddle solver\" (\"Welträtsellöser\", such as Arvid Reuterdahl, Hermann Fricke or Johann Heinrich Ziegler). Their views had their quite different roots in monism, Lebensreform, or occultism. Their views were typically characterized by the fact that they practically rejected the entire terminology and the (primarily mathematical) methods of modern science. Their works were published by private publishers, or in popular and non-specialist journals. It was significant for many \"free researchers\" (especially the monists) to explain all phenomena by intuitive and illustrative mechanical (or electrical) models, which also found its expression in their defense of the aether. For this reason they objected to the abstractness and inscrutability of the relativity theory, which was considered a pure calculation method that cannot reveal the true reasons underlying the phenomena. The \"free researchers\" often used Mechanical explanations of gravitation, in which gravity is caused by some sort of \"aether pressure\" or \"mass pressure from a distance\". Such models were regarded as an illustrative alternative to the abstract mathematical theories of gravitation of both Newton and Einstein. Additionally, also the enormous self-confidence of the \"free researchers\" is noteworthy, since they not only believed to have solved all the riddles of the world, but also had the expectation that they would rapidly convince the scientific community.\n\nSince Einstein rarely defended himself against these attacks, this task was undertaken by other relativity theoreticians, who (according to Hentschel) formed some sort of \"defensive belt\" around Einstein. Some representatives were Max von Laue, Max Born, etc. and on popular-scientific and philosophical level Hans Reichenbach, André Metz etc., who led many discussions with critics in semi-popular journals and newspapers. However, most of these discussions failed from the start. Physicists like Gehrcke, some philosophers, and the \"free researchers\" were so obsessed with their own ideas and prejudices that they were unable to grasp the basics of relativity; consequently, the participants of the discussions were talking past each other. In fact, the theory that was criticized by them was not relativity at all, but rather a caricature of it. The \"free researchers\" were mostly ignored by the scientific community, but also, in time, respected physicists such as Lenard and Gehrcke found themselves in a position outside the scientific community. However, the critics didn't believe that this was due to their incorrect theories, but rather due to a conspiracy of the relativistic physicists (and in the 1920s & 1930s of the Jews as well), which allegedly tried to put down the critics, and to preserve and improve their own positions within the academic world. For example, Gehrcke (1920/24) held that the propagation of relativity is a product of some sort of mass suggestion. Therefore, he instructed a media monitoring service to collect over 5000 newspaper clippings which were related to relativity, and published his findings in a book. However, Gehrcke's claims were rejected, because the simple existence of the \"relativity hype\" says nothing about the validity of the theory, and thus it cannot used for or against relativity.\n\nAfterward, some critics tried to improve their positions by the formation of alliances. One of them was the \"Academy of Nations\", which was founded in 1921 in the USA by Robert T. Browne and Arvid Reuterdahl. Other members were Thomas Jefferson Jackson See and as well as Gehrcke and Mohorovičić in Germany. It is unknown whether other American critics such as Charles Lane Poor, Charles Francis Brush, Dayton Miller were also members. The alliance disappeared as early as the mid-1920s in Germany and by 1930 in the USA.\n\nShortly before and during World War I, there appeared some nationalistically motivated criticisms of relativity and modern physics. For example, Pierre Duhem regarded relativity as the product of the \"too formal and abstract\" German spirit, which was in conflict with the \"common sense\". Similarly, popular criticism in the Soviet Union and China, which partly was politically organized, rejected the theory not because of factual objections, but as ideologically motivated as the product of western decadence.\n\nSo in those countries, the Germans or the Western civilization were the enemies. However, in Germany the Jewish ancestry of some leading relativity proponents such as Einstein and Minkowski made them targets of racially minded critics, although many of Einstein's German critics did not show evidence of such motives. The engineer Paul Weyland, a known nationalistic agitator, arranged the first public meeting against relativity in Berlin in 1919. While Lenard and Stark were also known for their nationalistic opinions, they declined to participate in Weyland's rallies, and Weyland's campaign eventually fizzled out due to a lack of prominent speakers. Lenard and others instead responded to Einstein's challenge to his professional critics to debate his theories at the scientific conference held annually at Bad Nauheim. While Einstein's critics, assuming without any real justification that Einstein was behind the activities of the German press in promoting the triumph of relativity, generally avoided antisemitic attacks in their earlier publications, it later became clear to many observers that antisemitism did play a significant role in some of the attacks.\n\nReacting to this underlying mood, Einstein himself openly speculated in a newspaper article that in addition to insufficient knowledge of theoretical physics, antisemitism at least partly motivated their criticisms. Some critics, including Weyland, reacted angrily and claimed that such accusations of antisemitism were only made to force the critics into silence. However, subsequently Weyland, Lenard, Stark and others clearly showed their antisemitic biases by beginning to combine their criticisms with racism. For example, Theodor Fritsch emphasized the alleged negative consequences of the \"Jewish spirit\" within relativity physics, and the far right-press continued this propaganda unhindered. After the murder of Walther Rathenau (1922) and murder threats against Einstein, he left Berlin for some time. Gehrcke's book on \"The mass suggestion of relativity theory\" (1924) was not antisemitic itself, but it was praised by the far-right press as describing an alleged typical Jewish behavior, which was also imputed to Einstein personally. Philipp Lenard in 1922 spoke about the \"foreign spirit\" as the foundation of relativity, and afterward he joined the Nazi party in 1924; Johannes Stark did the same in 1930. Both were proponents of the so-called German Physics, which only accepted scientific knowledge based on experiments, and only if accessible to the senses. According to Lenard (1936), this is the \"Aryan physics or physics by man of Nordic kind\" as opposed to the alleged formal-dogmatic \"Jewish physics\". Additional antisemitic critics can be found in the writings of Wilhelm Müller, Bruno Thüring and others. For example, Müller erroneously claimed that relativity was a purely \"Jewish affair\" and it would correspond to the \"Jewish essence\" etc., while Thüring made comparisons between the Talmud and relativity.\n\nSome of Einstein's critics, like Lenard, Gehrcke and Reuterdahl, accused him of plagiarism, and questioned his priority claims to the authorship of relativity theory. The thrust of such allegations was to promote more traditional alternatives to Einstein's abstract hypothetico-deductive approach to physics, while Einstein himself was to be personally discredited. It was argued by Einstein's supporters that such personal accusations were unwarranted, since the physical content and the applicability of former theories were quite different from Einstein's theory of relativity. However, others argued that between them Poincaré and Lorentz had earlier published several of the core elements of Einstein's 1905 relativity paper, including a generalized relativity principle that was intended by Poincaré to apply to all physics. Some examples:\n\nSome contemporary historians of science have revived the question as to whether Einstein was possibly influenced by the ideas of Poincaré, who first stated the relativity principle and applied it to electrodynamics, developing interpretations and modifications of Lorentz's electron theory that appear to have anticipated what is now called special relativity. Another discussion concerns a possible mutual influence between Einstein and David Hilbert as regards completing the field equations of general relativity (see Relativity priority dispute).\n\nA collection of various criticisms can be found in the book \"Hundert Autoren gegen Einstein\" (\"A Hundred Authors Against Einstein\"), published in 1931. It contains very short texts from 28 authors, and excerpts from the publications of another 19 authors. The rest consists of a list that also includes people who only for some time were opposed to relativity. Besides philosophic objections (mostly based on Kantianism), also some alleged elementary failures of the theory were included; however, as some commented, those failures were due to the authors' misunderstanding of relativity. For example, Hans Reichenbach described the book as an \"accumulation of naive errors\", and as \"unintentionally funny\". Albert von Brunn interpreted the book as a backward step to the 16th and 17th century, and Einstein said, in response to the book, that if he were wrong, then one author would have been enough.\n\nAccording to Goenner, the contributions to the book are a mixture of mathematical–physical incompetence, hubris, and the feelings of the critics of being suppressed by contemporary physicists advocating for the new theory. The compilation of the authors show, Goenner continues, that this was not a reaction within the physics community—only one physicist (Karl Strehl) and three mathematicians (Jean-Marie Le Roux, Emanuel Lasker and Hjalmar Mellin) were present—but a reaction of an inadequately educated academic citizenship, which didn't know what to do with relativity. As regards the average age of the authors: 57% were substantially older than Einstein, one third was around the same age, and only two persons were substantially younger. Two authors (Reuterdahl, von Mitis) were antisemitic and four others were possibly connected to the Nazi movement. On the other hand, no antisemitic expression can be found in the book, and it also included contributions of some authors of Jewish ancestry (Salomo Friedländer, Ludwig Goldschmidt, Hans Israel, Emanuel Lasker, Oskar Kraus, Menyhért Palágyi).\n\nThe theory of relativity is considered to be self-consistent, is consistent with many experimental results, and serves as the basis of many successful theories like quantum electrodynamics. Therefore, fundamental criticism (like that of Herbert Dingle, Louis Essen, Petr Beckmann, Maurice Allais and Tom van Flandern) has not been taken seriously by the scientific community, and due to the lack of quality of many critical publications (found in the process of peer review) they were rarely accepted for publication in reputable scientific journals. Just as in the 1920s, most critical works are published in small publications houses, alternative journals (like \"Apeiron\" or \"Galilean Electrodynamics\"), or private websites. Consequently, where criticism of relativity has been dealt with by the scientific community, it has mostly been in historical studies.\n\nHowever, this does not mean that there is no further development in modern physics. The progress of technology over time has led to extremely precise ways of testing the predictions of relativity, and so far it has successfully passed all tests (such as in particle accelerators to test special relativity, and by astronomical observations to test general relativity). In addition, in the theoretical field there is continuing research intended to unite general relativity and quantum theory. The most promising models are string theory and loop quantum gravity. Some variations of those models also predict violations of Lorentz invariance on a very small scale.\n\n\n\n\n\n\n"}
{"id": "569193", "url": "https://en.wikipedia.org/wiki?curid=569193", "title": "Debunker", "text": "Debunker\n\nA debunker is a person or organization who attempts to expose or discredit claims believed to be false, exaggerated, or pretentious. The term is often associated with skeptical investigation of controversial topics such as UFOs, claimed paranormal phenomena, cryptids, conspiracy theories, alternative medicine, religion, or exploratory or fringe areas of scientific or pseudoscientific research.\n\nAccording to the Merriam-Webster online dictionary, to \"debunk\" is defined as: \"to expose the sham or falseness of.\"\n\nIf debunkers are not careful, their communications may backfire – increasing an audience's long-term belief in myths. Backfire effects can occur if a message spends too much time on the negative case, if it is too complex, or if the message is threatening.\n\nThe American Heritage Dictionary traces the passage of the words bunk (noun), debunk (verb) and debunker (noun) into American English in 1923 as a belated outgrowth of \"bunkum\", of which the first recorded use was in 1828, apparently related to a poorly received \"speech for Buncombe County, North Carolina\" given by North Carolina representative Felix Walker during the 16th United States Congress (1819–1821).\n\nThe term \"debunk\" originated in a 1923 novel \"Bunk\", by American journalist and popular historian William Woodward (1874–1950), who used it to mean to \"take the bunk out of things.\"\n\nThe term \"debunkery\" is not limited to arguments about scientific validity; it is also used in a more general sense at attempts to discredit any opposing point of view, such as that of a political opponent.\n\n\n\nAustralian Professorial Fellow Stephan Lewandowsky and John Cook, Climate Communication Fellow for the Global Change Institute at the University of Queensland (and author at SkepticalScience.com) co-wrote \"Debunking Handbook\", in which they warn that debunking efforts may backfire. Backfire effects occur when science communicators accidentally reinforce false beliefs by trying to correct them, a phenomenon known as belief perseverance.\n\nCook and Lewandowsky offer possible solutions to the backfire effects as described in different psychological studies. They recommend spending little or no time describing misconceptions because people cannot help but remember ideas that they have heard before. They write \"Your goal is to increase people's familiarity with the facts.\" They recommend providing fewer and clearer arguments, considering that more people recall a message when it is simpler and easier to read. \"Less is more\" is especially important because scientific truths can get overwhelmingly detailed; pictures, graphs, and memorable tag lines all help keep things simple.\n\nThe authors write that debunkers should try to build up people's egos in some way before confronting false beliefs because it is difficult to consider ideas that threaten one's worldviews (i.e., threatening ideas cause cognitive dissonance). It is also advisable to avoid words with negative connotations. The authors describe studies which have shown that people abhor incomplete explanations – they write \"In the absence of a better explanation, [people] opt for the wrong explanation\". It is important to fill in conceptual gaps, and to explain the cause of the misconception in the first place. The authors believe these techniques can reduce the odds of a \"backfire\" – that an attempt to debunk bad science will increase the audience's belief in misconceptions.\n\n"}
{"id": "23167508", "url": "https://en.wikipedia.org/wiki?curid=23167508", "title": "Dustbin category", "text": "Dustbin category\n\nThe term \"dustbin category\" is sometimes used to describe a category that includes people or things that might be heterogeneous, only loosely related or poorly understood. It has been used in discussion of law, linguistics, medicine, sociology and other disciplines. For example:\nSome patients' symptoms do not fit well with any recognised category and there is a danger these may be forced into a 'dustbin' category such as 'depression, not otherwise specified.'\n"}
{"id": "3818992", "url": "https://en.wikipedia.org/wiki?curid=3818992", "title": "E-social science", "text": "E-social science\n\nE-social science is a more recent development in conjunction with the wider developments in e-science. It is social science using grid computing and other information technologies to collect, process, integrate, share, and disseminate social and behavioural data.\n\n"}
{"id": "29250092", "url": "https://en.wikipedia.org/wiki?curid=29250092", "title": "Erice statement", "text": "Erice statement\n\nThe Erice statement is a statement written by Paul Dirac, Piotr Kapitza, and Antonino Zichichi asking for freedom of expression for scientists as well as for nuclear disarmament. It has been signed by over 90,000 scientists as well as numerous world leaders including Mikhail Gorbachev, Ronald Reagan and Deng Xiaoping.\n"}
{"id": "48854038", "url": "https://en.wikipedia.org/wiki?curid=48854038", "title": "FONDECYT", "text": "FONDECYT\n\nThe National Fund for Scientific and Technological Development (Spanish: \"Fondo Nacional de Desarrollo Científico y Tecnológico\"), abbreviated FONDECYT, is the main public fund of the Government of Chile to promote basic scientific and technological research in all areas of knowledge. It is managed by the National Commission for Scientific and Technological Research (CONICYT).\n\nThe fund was established in 1981 and began operations in 1982. Since then it has funded more than 16,000 research projects. In 1991 CONICYT opened a second fund, FONDEF (The Science & Technology Development Fund), with the aim of promoting public–private collaboration in research and development.\n\nThe program provides financial support for projects selected under three headings:\n\n"}
{"id": "543667", "url": "https://en.wikipedia.org/wiki?curid=543667", "title": "Flood geology", "text": "Flood geology\n\nFlood geology (also creation geology or diluvial geology) is the attempt to interpret and reconcile geological features of the Earth in accordance with a literal belief in the global flood described in Genesis . In the early 19th century, diluvial geologists hypothesized that specific surface features were evidence of a worldwide flood which had followed earlier geological eras; after further investigation they agreed that these features resulted from local floods or glaciers. In the 20th century, young Earth creationists revived flood geology as an overarching concept in their opposition to evolution, assuming a recent six-day Creation and cataclysmic geological changes during the Biblical Deluge, and incorporating creationist explanations of the sequence of rock strata.\n\nIn the early stages of development of the science of geology, fossils were interpreted as evidence of past flooding. The \"theories of the Earth\" of the 17th century proposed mechanisms based on natural laws, within a timescale set by the biblical chronology. As modern geology developed, geologists found evidence of an ancient Earth, and evidence inconsistent with the notion that the Earth had developed in a series of cataclysms, the most recent of which could be attributed to the Genesis flood. In early 19th-century Britain, \"Diluvialism\" attributed landforms and surface features such as beds of gravel and erratic boulders to the destructive effects of this supposed global Deluge, but by 1830 geologists increasingly found that the evidence only showed relatively local floods. Attempts were made by so-called scriptural geologists to give primacy to literal Biblical explanations, but they lacked background in geology and were marginalised by the scientific community, as well as having little influence on the church.\n\nFlood geology was revived as a field of study within creation science, which is a part of young Earth creationism.\nProponents hold to a literal reading of and view its passages to be historically accurate, using the Bible's internal chronology to place the Flood and the story of Noah's Ark within the last five thousand years.\n\nThe key tenets of flood geology are refuted by scientific analysis. Flood geology contradicts the scientific consensus in geology, stratigraphy, geophysics, physics, paleontology, biology, anthropology, and archeology. Modern geology, its sub-disciplines and other scientific disciplines utilize the scientific method. In contrast, flood geology does not adhere to the scientific method, and it is, therefore, a pseudoscience.\n\nIn pre-Christian times, fossils found on land were thought by Greek philosophers, including Xenophanes, Xanthus and Aristotle, to be evidence that the sea had in past ages covered the land. Their concept of vast time periods in an eternal cosmos was rejected by early Christian writers as incompatible with their belief in Creation by God. Among the church fathers, Tertullian spoke of fossils demonstrating that mountains had been overrun by water without explicitly saying when. Chrysostom and Augustine believed that fossils were the remains of animals that were killed and buried during the brief duration of the Biblical Genesis Flood, and later Martin Luther viewed fossils as having resulted from the Flood.\n\nOther scholars, including Avicenna, thought fossils were produced in the rock by \"petrifying virtue\" acting on \"seeds\" of plants and animals. In 1580 Bernard Palissy speculated that fossils had formed in lakes, and natural historians subsequently disputed the alternatives. Robert Hooke made empirical investigations, and doubted that the numbers of fossil shells or depth of shell beds could have formed in the one year of Noah's Flood. In 1616 Nicolas Steno showed how chemical processes changed organic remains into stone fossils. His fundamental principles of stratigraphy published in 1669 established that rock strata formed horizontally and were later broken and tilted, though he assumed these processes would occur within 6,000 years including a worldwide Flood.\n\nIn his influential \"Principles of Philosophy\" of 1644, René Descartes applied his mechanical physical laws to envisage swirling particles forming the Earth as a layered sphere. This natural philosophy was recast in Biblical terms by the theologian Thomas Burnet, whose \"Sacred Theory of the Earth\" published in the 1680s proposed complex explanations based on natural laws, and explicitly rejected the simpler approach of invoking miracles as incompatible with the methodology of natural philosophy (the precursor to science). Burnet maintained that less than 6,000 years ago the Earth had emerged from chaos as a perfect sphere, with paradise on land over a watery abyss. This crust had dried out and cracked, and its collapse caused the Biblical Deluge, forming mountains as well as underground caverns where the water retreated. He made no mention of fossils, but inspired other diluvial theories that did.\n\nIn 1695, John Woodward's \"An Essay Toward a Natural History of the Earth\" viewed the Genesis Flood as dissolving rocks and earth into a thick slurry which caught up all living things, and when the waters settled formed strata according to the specific gravity of these materials, including fossils of the organisms. When it was pointed out that lower layers were often less dense and forces that shattered rock would destroy organic remains, he resorted to the explanation that a divine miracle had temporarily suspended gravity. William Whiston's \"New Theory of the Earth\" of 1696 combined scripture with Newtonian physics to propose that the original chaos was the atmosphere of a comet with the days of Creation each taking a year, and the Genesis Flood had resulted from a second comet. His explanation of how the Flood caused mountains and the fossil sequence was similar to Woodward's. Johann Jakob Scheuchzer wrote in support of Woodward's ideas in 1708, describing some fossil vertebrae as bones of sinners who had perished in the Flood. A skeleton found in a quarry was described by him in 1726 as \"Homo diluvii testis\", a giant human testifying to the Flood. This was accepted for some time, but in 1812 it was shown to be a prehistoric salamander.\n\nThe modern science of geology developed in the 18th century, the term \"geology\" itself was popularised by the \"Encyclopédie\" of 1751. Steno's categorisation of strata was expanded by several geologists, including Johann Gottlob Lehmann who believed that the oldest mountains had formed early in the Creation, and categorised as \"Flötz-Gebürge\" stratified mountains with few ore deposits but with thin layers containing fossils, overlain by a third category of superficial deposits. In his 1756 publication he identified 30 different layers in this category which he attributed to the action of the Genesis Deluge, possibly including debris from the older mountains. Others including Giovanni Arduino attributed secondary strata to natural causes: Georg Christian Füchsel said that geologists had to take as standard the processes in which nature currently produces solids, \"we know no other way\", and only the most recent deposits could be attributed to a great Flood.\n\nLehman's classification was developed by Abraham Gottlob Werner who thought that rock strata had been deposited from a primeval global ocean rather than by Noah's Flood, a doctrine called Neptunism. The idea of a young Earth was further undermined in 1774 by Nicolas Desmarest, whose studies of a succession of extinct volcanoes in Europe showed layers which would have taken long ages to build up. The fact that these layers were still intact indicated that any later Flood had been local rather than universal. Against Neptunism, James Hutton proposed an indefinitely old cycle of eroded rocks being deposited in the sea, consolidated and heaved up by volcanic forces into mountains which in turn eroded, all in natural processes which continue to operate.\n\nThe first professional geological society, the Geological Society of London, was founded in 1807. By this time, geologists were convinced that an immense time had been needed to build up the huge thickness of rock strata visible in quarries and cliffs, implying extensive pre-human periods. Most accepted a basic time scale classifying rocks as primitive, transition, secondary, or tertiary. Several researchers independently found that strata could be identified by characteristic fossils: secondary strata in southern England were mapped by William Smith from 1799 to 1815.\n\nGeorges Cuvier, working with Alexandre Brongniart, examined tertiary strata in the region around Paris. Cuvier found that fossils identified rock formations as alternating between marine and terrestrial deposits, indicating \"repeated irruptions and retreats of the sea\" which he identified with a long series of sudden catastrophes which had caused extinctions. In his 1812 \"Discours préliminaire\" to his \"Recherches sur les ossemens fossiles de quadrupeds\" put forward a synthesis of this research into the long prehistoric period, and a historical approach to the most recent catastrophe. His historical approach tested empirical claims in the Biblical text of Genesis against other ancient writings to pick out the \"real facts\" from \"interested fictions\". In his assessment, Moses had written the account around 3,300 years ago, long after the events described. Cuvier only discussed the Genesis Flood in general terms, as the most recent example of \"an event of an universal catastrophe, occasioned by an irruption of the waters\" not set \"much further back than five or six thousand years ago\". The historical texts could be loosely related to evidence such as overturned strata and \"heaps of \"debris\" and rounded pebbles\". An English translation was published in 1813 with a preface and notes by Robert Jameson, Regius Professor of Natural History at the University of Edinburgh. He began the preface with a sentence which ignored Cuvier's historical approach and instead deferred to revelation: \n\nThis sentence was removed after the second edition, and Jameson's position changed as shown by his notes in successive editions, but it influenced British views of Cuvier's concept. In 1819 George Bellas Greenough, first president of The Geological Society, issued \"A Critical Examination of the First Principles of Geology\" stating that unless erratic boulders deposited hundreds of miles from their original sources had been moved by seas, rivers, or collapsing lakes, \"the only remaining cause, to which these effects can be ascribed, is a Debacle or Deluge.\"\n\nConservative geologists in Britain welcomed Cuvier's theory to replace Werner's Neptunism, and the Church of England clergyman William Buckland became the foremost proponent of Flood geology as he sought to get the new science of geology accepted on the curriculum of the University of Oxford. In 1818 he was visited by Cuvier, and in his inaugural speech in 1819 as the first professor of geology at the university he defended the subject against allegations that it undermined religion. His speech, published as \"Vindiciae Geologicae; or, The Connexion of Geology with Religion Explained\", equated the last of a long series of catastrophes with the Genesis Flood, and said that \"the grand fact of an universal deluge at no very remote period is proved on grounds so decisive and incontrovertible, that, had we never heard of such an event from Scripture, or any other, authority, Geology of itself must have called in the assistance of some such catastrophe, to explain the phenomena of diluvian action which are universally presented to us, and which are unintelligible without recourse to a deluge exerting its ravages at a period not more ancient than that announced in the Book of Genesis.\" The evidence he proposed included erratic boulders, extensive areas of gravel, and landforms which appeared to have been scoured by water.\n\nThis inaugural address influenced the geologists William Conybeare and William Phillips. In their 1822 book on \"Outlines of the Geology of England and Wales\" Conybeare referred to the same features in an introduction about the relationship between geology and religion, describing how a deluge causing \"the last great geological change to which the surface of our planet appears to have been exposed\" left behind the debris (which he named in Latin \"Diluvium\") as evidence for \"that great and universal catastrophe to which it seems most properly assignable\". In 1823 Buckland published his detailed account of \"Relics of the Flood\", \"Reliquiae Diluvianae;\" or, \"Observations on the Organic Remains Contained in Caves, Fissures, and Diluvial Gravel and on Other Geological Phenomena Attesting the Action of an Universal Deluge\", incorporating his research suggesting that animal fossils had been dragged into the Kirkdale Cave by hyenas then covered by a layer of red mud washed in by the Deluge.\n\nBuckland's views were supported by other Church of England clergymen naturalists: his Oxford colleague Charles Daubeny proposed in 1820 that the volcanoes of the Auvergne showed a sequence of lava flows from before and after the Flood had cut valleys through the region. In an 1823 article \"On the deluge\", John Stevens Henslow, professor of mineralogy at the University of Cambridge, affirmed the concept and proposed that the Flood had originated from a comet, but this was his only comment on the topic. Adam Sedgwick, Woodwardian Professor of Geology at Cambridge, presented two supportive papers in 1825, \"On the origin of alluvial and diluvial deposits\", and \"On diluvial formations\". At this time, most of what Sedgwick called \"The English school of geologists\" distinguished superficial deposits which were \"diluvial\", showing \"great irregular masses of sand, loam, and coarse gravel, containing through its mass rounded blocks sometimes of enormous magnitude\" and supposedly caused by \"some great irregular inundation\", from \"alluvial\" deposits of \"comminuted gravel, silt, loam, and other materials\" attributed to lesser events, the \"propelling force\" of rivers, or \"successive partial inundations\".\n\nIn America, Benjamin Silliman at Yale College spread the concept, and in an 1833 essay dismissed the earlier idea that most stratified rocks had been formed in the Flood, while arguing that surface features showed \"wreck and ruin\" attributable to \"mighty floods and rushing torrents of water\". He said that \"we must charge to moving waters the undulating appearance of stratified sand and gravel, often observed in many places, and very conspicuously in the plain of New Haven, and in other regions of Connecticut and New England\", while both \"bowlder stones\" and sandy deserts across the world could be attributed to \"diluvial agency\".\n\nOther naturalists were critical of Diluvialism: the Church of Scotland pastor John Fleming published opposing arguments in a series of articles from 1823 onwards. He was critical of the assumption that fossils resembling modern tropical species had been swept north \"by some violent means\", which he regarded as absurd considering the \"unbroken state\" of fossil remains. For example, fossil mammoths demonstrated adaptation to the same northern climates now prevalent where they were found. He criticized Buckland's identification of red mud in the Kirkdale cave as diluvial, when near identical mud in other caves had been described as fluvial. While Cuvier had reconciled geology with a loose reading of the Biblical text, Fleming argued that such a union was \"indiscreet\" and turned to a more literal view of Genesis:\nWhen Sedgwick visited Paris at the end of 1826 he found hostility to Diluvialism: Alexander von Humboldt ridiculed it \"beyond measure\", and Louis-Constant Prévost \"lectured against it\". In the summer of 1827 Sedgwick and Roderick Murchison travelled to investigate the geology of the Scottish Highlands, where they found \"so many indications of \"local diluvial\" operations\" that Sedgwick began to change his mind about it being worldwide. When George Poulett Scrope published his investigations into the Auvergne in 1827, he did not use the term \"diluvium\". He was followed by Murchison and Charles Lyell whose account appeared in 1829. All three agreed that the valleys could well have been formed by rivers acting over a long time, and a deluge was not needed. Lyell, formerly a pupil of Buckland, put strong arguments against diluvialism in the first volume of his \"Principles of Geology\" published in 1830, though suggesting the possibility of a deluge affecting a region such as the low-lying area around the Caspian Sea. Sedgwick responded to this book in his presidential address to the Geological Society in February 1830, agreeing that diluvial deposits had formed at differing times. At the society a year later, when retiring from the presidency, Sedgwick described his former belief that \"vast masses of diluvial gravel\" had been scattered worldwide in \"one violent and transitory period\" as \"a most unwarranted conclusion\", and therefore thought \"it right, as one of my last acts before I quit this Chair, thus publicly to read my recantation.\" However, he remained convinced that a flood as described in Genesis was not excluded by geology.\n\nOne student had seen the gradual abandonment of diluvialism: Charles Darwin had attended Jameson's geology lectures in 1826, and at Cambridge became a close friend of Henslow before learning geology from Sedgwick in 1831. At the outset of the \"Beagle\" voyage Darwin was given a copy of Lyell's \"Principles of Geology\", and at the first landfall began his career as a geologist with investigations which supported Lyell's concept of slow uplift while also describing loose rocks and gravel as \"part of the long disputed Diluvium\". Debates continued over the part played by repeated exceptional catastrophes in geology, and in 1832 William Whewell dubbed this view catastrophism, while naming Lyell's insistence on explanations based on current processes uniformitarianism.\n\nBuckland, too, gradually modified his views on the Deluge. In 1832 a student noted Buckland's view on cause of diluvial gravel, \"whether is Mosaic inundation or not, will not say\". In a footnote to his \"Bridgewater Treatise\" of 1836, Buckland backed down from his former claim that the \"violent inundation\" identified in his \"Reliquiae Diluvianae\" was the Genesis flood: \nFor a while, Buckland had continued to insist that \"some\" geological layers were related to the Great Flood, but grew to accept the idea that they represented multiple inundations which occurred well before humans existed. In 1840 he made a field trip to Scotland with the Swiss geologist Louis Agassiz, and became convinced that the \"diluvial\" features which he had attributed to the Deluge had, in fact, been produced by ancient ice ages. Buckland became one of the foremost champions of Agassiz's theory of glaciations, and diluvialism went out of use in geology. Active geologists no longer posited sudden ancient catastrophes with unknown causes, and instead increasingly explained phenomena by observable processes causing slow changes over great periods.\n\nScriptural geologists were a heterogeneous group of writers in the early nineteenth century, who claimed \"the primacy of literalistic biblical exegesis\" and a short Young Earth time-scale. Their views were marginalised and ignored by the scientific community of their time. They generally lacked any background in geology, and had little influence even in church circles.\n\nMany quoted out of date geological writings. Among the most prominent, Granville Penn argued in 1822 that \"mineral geology\" rejected revelation, while true \"Mosaical geology\" showed that God had created primitive rock formations directly, in correspondence with the laws which God then made to produce subsequent effects. A first revolution on the third day of creation deepened the oceans so water rushed in, and in the Deluge 1,656 years afterwards a second revolution sank land areas and raised the sea bed to cause a swirling flood which moved soil and fossil remains into stratified layers, after which God created new vegetation. As Genesis appeared to show that the rivers of Eden had survived this catastrophe, he argued that the verses concerned were an added \"parenthesis\" which should be disregarded. In 1837 George Fairholme expressed disappointment about disappearing belief in the deluge, and about Sedgwick and Buckland recanting diluvialism, while putting forward his own \"New and Conclusive Physical Demonstrations\" which ignored geological findings to claim that strata had been deposited in a quick continuous process while still moist.\n\nGeology was popularized by several authors. John Pye Smith's lectures published in 1840 reconciled an extended time frame with Genesis by the increasingly common gap theology or day-age theology, and said it was likely that the gravel and boulder formations were not \"diluvium\", but had taken long ages predating the creation of humans. He reaffirmed that the Flood was historical as a local event, something which the 17th century theologians Edward Stillingfleet and Matthew Poole had already suggested on a purely Biblical basis. Smith also denounced the \"fanciful\" writings of the scriptural geologists. Edward Hitchcock sought to ensure that geological findings could be corroborated by scripture, and dismissed the scriptural geology of Penn and Fairholme as misrepresenting both scripture and the facts of geology. He noted the difficulty of equating a violent deluge with the more tranquil Genesis account. Hugh Miller supported similar points with considerable detail.\n\nLittle attention was paid to Flood geology over the rest of the 19th century, its few supporters included the author Eleazar Lord in the 1850s and the Lutheran scholar Carl Friedrich Keil in 1860 and 1878. The visions of Ellen G. White published in 1864 formed Seventh-day Adventist Church views, and influenced 20th century creationism.\n\nThe Seventh-day Adventist Church, led by Ellen G. White, took a six-day creation literally, and put her prolific \"inspired\" writings on a level with the Bible. Her visions of the flood and its aftermath, published in 1864, described a catastrophic deluge which reshaped the entire surface of the Earth, followed by a powerful wind which piled up new high mountains, burying the bodies of men and beasts. Buried forests became coal and oil, and where God later caused these to burn, they reacted with limestone and water to cause \"earthquakes, volcanoes and fiery issues\".\n\nEllen G. White's visions prompted several books by one of her followers, George McCready Price, leading to the 20th-century revival of flood geology. After years selling White's books door-to-door, Price took a one-year teacher-training course and taught in several schools. When shown books on evolution and the fossil sequence which contradicted his beliefs, he found the answer in White's \"revealing word pictures\" which suggested how the fossils had been buried. He studied textbooks on geology and \"almost tons of geological documents\", finding \"how the actual facts of the rocks and fossils, \"stripped of mere theories\", splendidly refute this evolutionary theory of the invariable order of the fossils, \"which is the very backbone of the evolution doctrine\"\". In 1902 he produced a manuscript for a book proposing geology based on Genesis, in which the sequence of fossils resulted from the different responses of animals to the encroaching flood. He agreed with White on the origins of coal and oil, and conjectured that mountain ranges (including the Alps and Himalaya) formed from layers deposited by the flood which had then been \"folded and elevated to their present height by the great lateral pressure that accompanied its subsidence\". He then found a report describing paraconformities and a paper on thrust faults. He concluded from these \"providential discoveries\" that it was impossible to prove the age or overall sequence of fossils, and included these points in his self-published paperback of 1906, \"Illogical Geology: The Weakest Point in the Evolution Theory\". His arguments continued this focus on disproving the sequence of strata, and he ultimately sold more than 15,000 copies of his 1923 college textbook \"The New Geology\".\n\nPrice increasingly gained attention outside Adventist groups, and in the creation–evolution controversy other leading Christian fundamentalists praised his opposition to evolution - though none of them followed his young Earth arguments, retaining their belief in the gap or in the day-age interpretation of Genesis. Price corresponded with William Jennings Bryan and was invited to be a witness in the Scopes Trial of 1925, but declined as he was teaching in England and opposed to teaching Genesis in public schools as \"it would be an infringement on the cardinal American principle of separation of church and state\". Price returned from England in 1929 to rising popularity among fundamentalists as a scientific author. In the same year his former student Harold W. Clark self-published the short book \"Back to Creationism\", which recommended Price's flood geology as the new \"science of creationism\", introducing the label \"creationism\" as a replacement for \"anti-evolution\" of \"Christian Fundamentals\".\n\nIn 1935 Price and Dudley Joseph Whitney (a rancher who had co-founded the Lindcove Community Bible Church, and now followed Price) founded the \"Religion and Science Association\" (RSA). They aimed to resolve disagreements among fundamentalists with \"a harmonious solution\" which would convert them all to flood geology. Most of the organising group were Adventists, others included conservative Lutherans with similarly literalist beliefs. Bryon C. Nelson of the Norwegian Lutheran Church of America had included Price's geological views in a 1927 book, and in 1931 published \"The Deluge Story in Stone: A History of the Flood Theory of Geology\", which described Price as the \"one very outstanding advocate of the Flood\" of the century. The first public RSA conference in March 1936 invited various fundamentalist views, but opened up differences between the organisers on the antiquity of creation and on life before Adam. The RSA went defunct in 1937, and a dispute continued between Price and Nelson, who now viewed Creation as occurring over 100,000 years previously.\n\nIn 1938 Price, with a group of Adventists in Los Angeles, founded what became the \"Deluge Geology Society\" (DGS), with membership restricted to those believing that the creation week comprised \"six literal days, and that the Deluge should be studied as the cause of the major geological changes since creation\". Not all DGS-adherents were Adventists; early members included the Independent Baptist Henry M. Morris and the Missouri Lutheran Walter E. Lammerts. The DGS undertook field-work: in June 1941 their first \"Bulletin\" hailed the news that the Paluxy River dinosaur trackways in Texas appeared to include human footprints. Though Nelson had advised Price in 1939 that this was \"absurd\" and that the difficulty of human footprints forming during the turmoil of the deluge would \"knock the Flood theory all to pieces\", in 1943 the DGS began raising funds for \"actual excavation\" by a Footprint Research Committee of members including the consulting geologist Clifford L. Burdick. Initially they tried to keep their research secret from \"unfriendly scientists\". Then in 1945, to encourage backing, they announced giant human footprints, allegedly defeating \"at a single stroke\" the theory of evolution. The revelation that locals had carved the footprints, and an unsuccessful field trip that year, failed to dampen their hopes. However, by then doctrinal arguments had riven the DGS. The most extreme dispute began in late 1938 after Harold W. Clark observed deep drilling in oil fields and had discussions with practical geologists which dispelled the belief that the fossil sequence was random, convincing him that the evidence of thrust faults was \"almost incontrovertible\". He wrote to Price, telling his teacher that the \"rocks do lie in a much more definite sequence than we have ever allowed\", and proposing that the fossil sequence was explained by ecological zones before the flood. Price reacted with fury, and despite Clark emphasising their shared belief in literal recent Creation, the dispute continued. In 1946 Clark set out his views in a book, \"The New Diluvialism\", which Price denounced as \"Theories of Satanic Origin\".\n\nIn 1941 F. Alton Everest co-founded the American Scientific Affiliation (ASA) as a less confrontational forum for evangelical scientists. Some deluge geologists, including Lammerts and Price, urged close cooperation with the DGS, but Everest began to see their views as presenting an \"insurmountable problem\" for the ASA. In 1948 he requested J. Laurence Kulp, a geologist in fellowship with the Plymouth Brethren, to explore the issue. At the convention that year, Kulp examined hominid antiquity demonstrated by radiocarbon dating. At the 1949 convention a paper by Kulp was presented, giving a detailed critique of \"Deluge Geology\", which he said had \"grown and infiltrated the greater portion of fundamental Christianity in America primarily due to the absence of trained Christian geologists\". Kulp demonstrated that \"major propositions of the theory are contraindicated by established physical and chemical laws\". He focused on \"four basic errors\" commonly made by flood geologists:\n\n\nKulp accused Price of ignorance and deception, and concluded that \"this unscientific theory of flood geology has done and will do considerable harm to the strong propagation of the gospel among educated people\". Price said nothing during the presentation and discussion. When invited to speak, he \"said something very brief which missed what everyone was waiting for\". Further publications made the ASA's opposition to flood geology clear.\n\nIn 1942, Henry M. Morris was persuaded by Irwin A. Moon's \"Sermons from Science\" of the importance of harmonising science and the Bible, and was introduced to the concepts of a vapor canopy causing the Flood, and its geological effects. About a year later he found George McCready Price's \"New Geology\" a \"life-changing experience\", and joined the \"Deluge Geology Society\". His book \"That You Might Believe\" for college students included Price's flood geology, and was published in 1946.\n\nMorris had joined the American Scientific Affiliation (ASA) in 1949, and in the summer of 1953 he made a presentation on \"The Biblical Evidence for a Recent Creation and Universal Deluge\" at their annual conference, held at the Grace Theological Seminary's campus. He impressed a graduate student there, John C. Whitcomb, Jr. who was teaching Old Testament and Hebrew. To Whitcomb's distress, Morris was \"politely denounced\" by the ASA members at the presentation.\n\nIn 1955 the ASA held a joint meeting with the Evangelical Theological Society (ETS) at the same campus, and there was considerable discussion about \"The Christian View of Science and Scripture\" (1954) by theologian Bernard Ramm. This book dismissed flood geology as typifying the \"ignoble tradition\" of fundamentalism, and said that Price could not be taken seriously, as he lacked the necessary competence, training and integrity. Instead, Ramm proposed what he called progressive creationism in which the Genesis days were pictorial images revealing a process that had taken place over millions of years. Ramm's views were praised by ASA scientists, but the ETS theologians were unwilling to follow Ramm.\n\nThis encouraged Whitcomb to make his doctoral dissertation a response to Ramm and a defence of Price's position. He systematically asked evangelical professors of apologetics, archaeology and the Old Testament about creation and the flood, and in October told Morris that Ramm's book had been sufficient incentive for him to devote his dissertation to the topic. In 1957 Whitcomb's 450 page dissertation, \"The Genesis Flood\", was completed, and he promptly began summarising it for a book. Moody Publishers responded positively and agreed with him that chapters on scientific aspects should be carefully checked or written by someone with a PhD in science, but Whitcomb's attempts to find someone with a doctorate in geology were unsuccessful. Morris gave helpful advice, expressing concern that sections were too closely based on Price and Velikovsky who were \"both considered by scientists generally as crackpots\". Morris produced an outline of his planned three chapters, and in December 1957 agreed to co-author the book.\n\nMorris completed his draft in early 1959. His intended 100 pages grew to almost 350, around twice the length of Whitcomb's eventual contribution. Whitcomb raised concerns that \"For many people, our position would be somewhat discredited\" by prominent references to \"Price and Seventh-Day Adventism\", and these were deleted. By early 1960 they were impatient at delays when Moody Publishers had misgivings about the length and literal views of the book, and they went along with Rousas Rushdoony's recommendation of a small Philadelphia publishers.\n\n\"The Genesis Flood\" by Whitcomb and Morris was published by the Presbyterian and Reformed Publishing Company of Philadelphia in February 1961. Their premise was that the Bible is infallible, \"the basic argument of this volume is that the Scriptures are true.\" For Whitcomb, Genesis described a worldwide Flood which covered all the high mountains, Noah's ark with capacity equivalent to eight freight trains, flood waters from a canopy and the deeps, and subsequent dispersal of animals from Ararat to all the continents via land bridges. He disputed the views published by Arthur Custance and Bernard Ramm. Morris then confronted readers with the dilemma of whether to believe Scripture or accept the interpretations of trained geologists, and instead of the latter proposed \"a new scheme of historical geology\" true both to Scripture and to God's work revealed in nature. This was essentially Price's \"The New Geology\" updated for the 1960s, though with few direct references to Price.\n\nLike Price before him, Morris argued that most fossil bearing strata had been formed during the global Deluge, disputing uniformitarianism, multiple ice ages, and the geologic column. He explained the apparent fossil sequence as the outcome of marine organisms dying in the slurry of sediments in early stages of the Flood, moving currents sorting object by size and shape, and the mobility of vertebrates allowing them to initially escape the floodwaters. He cited Lammerts in support of Price's views about the thrust fault at Chief Mountain disproving the sequence. \nThe book went beyond Price in some areas. Morris extended the six day creation from the Earth to the entire universe, and said that death and decay had only begun with the Fall of Man, which had therefore introduced entropy and the second law of thermodynamics. He proposed that a vapor canopy, before providing water for the Flood, created a mild even climate and shielded the Earth from cosmic rays so radiocarbon dating would not work. He cited Clifford L. Burdick's testimony that some of the Paluxy River dinosaur trackways overlapped human footprints, but Burdick failed to confirm this and the section was removed from the third edition.\n\nIn a 1957 discussion with Whitcomb, Walter E. Lammerts suggested an \"informal association\" to exchange ideas, and possibly research, on flood geology. Morris was unavailable to get things started, then around 1961 Wiliam J. Tinkle got in touch, and they set about recruiting others. They had difficulty in finding supporters with scientific qualifications. The Creation Research Committee of ten they put together on 9 February 1962 had varying views on the age of the Earth, but all opposed evolution. They then succeeded in recruiting others into what became the Creation Research Society (CRS) in June 1963, and grew rapidly. Getting an agreed statement of belief was problematic, they affirmed that the Bible was \"historically and scientifically true in the original autographs\" so that \"the account of origins in Genesis is a factual presentation of simple historical truths\" and \"The great flood described in Genesis, commonly referred to as the Noachian Flood, was an historic event worldwide in its extent and effect\", but to Morris's disappointment they did not make flood geology mandatory. They lacked a qualified geologist, and Morris persuaded the group to appoint Clifford L. Burdick as their only Earth scientist, overcoming initial concerns raised by Lammerts. The CRS grew rapidly, with an increasing proportion of the membership adhering to strict young Earth flood geology.\n\nThe resources of the CRS for its first decade went into publication of the CRS \"Quarterly\", and a project to publish a creationist school book. Since the 1920s most U.S. schools had not taught pupils about evolution, but Sputnik exposed apparent weaknesses of U.S. science education and the Biological Sciences Curriculum Study produced textbooks in 1963 which included the topic. When the Texas Education Agency held a hearing in October 1964 about adopting these textbooks, creationist objectors were unable to name suitable creationist alternatives. Lammerts organised a CRS textbook committee which lined up a group of authors, with John N. Moore as senior editor bringing their contributions together into a suitable textbook.\n\nThe teaching of evolution, reintroduced in 1963 by the Biological Sciences Curriculum Study textbooks, was prohibited by laws in some states. These bans were contested; the \"Epperson v. Arkansas\" case which began late in 1965 was decided in 1968 by the United States Supreme Court ruling that such laws violated the Establishment Clause of the First Amendment to the United States Constitution.\n\nSome creationists thought a legal decision requiring religious neutrality in schools should shield their children from teachings hostile to their religion; Nell. J. Segraves and Jean E. Sumrall (a friend of Lammerts who was also associated with the Creation Research Society and the Bible-Science Association) petitioned the California State Board of Education to require that school biology texts designate evolution a theory. In 1966 Max Rafferty as California State Superintendent of Public Instruction suggested that they demand equal time for creation, as the Civil Rights Act of 1964 allowed teachers to mention religion as long as they did not promote specific doctrines. Their first attempt failed, but in 1969 controversy arose over a proposed \"Science Framework for California Schools\". Anticipating success, they and others in the Bible-Science Association formed \"Creation Science, Inc.\", to produce textbooks. A compromise acceptable to Segraves, Sumrall and the Board was suggested by Vernon L. Grose, and the revised 1970 \"Framework\" included \"While the Bible and other philosophical treatises also mention creation, science has independently postulated the various theories of creation. Therefore, creation in scientific terms is not a religious or philosophical belief.\" The result kept school texts free of creationism, but downgraded evolution to mere speculative theory.\n\nCreationists reacted to the California developments with a new confidence that they could introduce their ideas into schools by minimizing biblical references. Henry M. Morris declared that \"Creationism is on the way back, this time not primarily as a religious belief, but as an alternative scientific explanation of the world in which we live.\" In 1970 \"Creation Science, Inc.\", combined with a planned studies center at Christian Heritage College as the Creation-Science Research Center. Morris moved to San Diego to become director of the center and academic vice-president of the college. In the fall he presented a course at the college on \"Scientific Creationism\", the first time he is known to have used the term in public. (Two years later, the Creation-Science Research Center split with part becoming the Institute for Creation Research (ICR) led by Morris.)\n\nThe Creation Research Society (CRS) had found schoolbook publishers reluctant to take on their textbook, and eventually the Christian publishing company Zondervan brought out \"Biology: A Search for Order in Complexity\" in 1970. The ten thousand copies printed sold out within a year, and they produced 25,000 as the second impression, but hardly any public schools adopted the book. A preface by Morris claimed that there were two philosophies of creation, \"the doctrine of evolution and the doctrine of special creation\", attempting to give both equal validity. The book mostly covered uncontroversial details of biology, but asserted that these were correctly seen as \"God's creation\" or \"divine creation\", and presented biblical creation as the correct scientific view. A chapter on \"Weaknesses of Geologic Evidence\" disputed evolutionary theories while asserting a \"fact that most fossil material was laid down by the flood in Noah's time\". Another chapter disputed evolutionary theory.\n\nIn the \"Creation Research Society Quarterly\" for September 1971 Morris introduced the \"two-model approach\" asserting that evolution and creation were both equally scientific and equally religious, and soon afterwards he said they were \"competing scientific hypotheses\". For the third printing of \"Biology: A Search for Order in Complexity\" in 1974, the editor John N. Moore added a preface setting out this approach as \"the two basic viewpoints of origins\", the \"evolution model\" and the \"creation model\". When an Indiana school decided to use the book as their biology text, the \"Hendren v. Campbell\" district court case banned its use in public schools as infringing the Establishment Clause. Judge Michael T. Dugan, II, described it as \"a text obviously designed to present \"only\" the view of Biblical Creationism in a favorable light\", contravening the constitution by promotion of a specific sectarian religious view.\n\nAs a tactic to gain the same scientific status as evolution, flood geology proponents had effectively relabeled the Bible-based flood geology of George McCready Price as \"creation science\" or \"scientific creationism\" by the mid 1970s. At the CRS board meeting in the Spring of 1972, members were told to start using \"scientific creationism\", a phrase used interchangeably with \"creation science\"; Morris explained that preferences differed, though neither was ideal as \"one simple term\" could not \"identify such a complex and comprehensive subject.\" In the 1974 ICR handbook for high-school teachers titled \"Scientific Creationism\", Morris used the two-model approach to support his argument that creationism could \"be taught without reference to the book of Genesis or to other religious literature or to religious doctrines\", and in public schools only the \"basic scientific creation model\" should be taught, rather than biblical creationism which \"would open the door to wide interpretations of Genesis\" or to non-Christian cosmogonies. He did not deny having been influenced by the Bible. In his preface to the book dated July 1974, Morris as editor outlined how the \"Public School Edition\" of the book evaluated evidence from a \"strictly scientific point of view\" without \"reference to the Bible or other religious literature\", while the \"General Edition\" was \"essentially identical\" except for an additional chapter on \"Creation according to Scripture\" that \"places the scientific evidence in its proper biblical and theological context.\"\n\nThe main ideas in creation science are: the belief in \"creation \"ex nihilo\"\" (Latin: out of nothing); the conviction that the Earth was created within the last 6,000 years; the belief that mankind and other life on Earth were created as distinct fixed \"baraminological\" \"kinds\"; and the idea that fossils found in geological strata were deposited during a cataclysmic flood which completely covered the entire Earth. As a result, creation science also challenges the commonly accepted geologic and astrophysical theories for the age and origins of the Earth and Universe, which creationists acknowledge are irreconcilable to the account in the Book of Genesis.\n\nThe geologic column and the fossil record are used as major pieces of evidence in the modern scientific explanation of the development and evolution of life on Earth as well as a means to establish the age of the Earth. Young Earth Creationists such as Morris and Whitcomb in their 1961 book, \"The Genesis Flood\", say that the age of the fossils depends on the amount of time credited to the geologic column, which they ascribe to be about one year. Some flood geologists dispute geology's assembled global geologic column since index fossils are used to link geographically isolated strata to other strata across the map. Fossils are often dated by their proximity to strata containing index fossils whose age has been determined by its location on the geologic column. Oard and others say that the identification of fossils as index fossils has been too error-prone for index fossils to be used reliably to make those correlations, or to date local strata using the assembled geologic scale.\n\nOther creationists accept the existence of the geological column and believe that it indicates a sequence of events that might have occurred during the global flood. Institute for Creation Research creationists such as Andrew Snelling, Steven A. Austin and Kurt Wise take this approach, as does Creation Ministries International. They cite the Cambrian explosion — the appearance of abundant fossils in the upper Ediacaran (Vendian) Period and lower Cambrian Period — as the pre-Flood/Flood boundary, the presence in such sediments of fossils that do not occur later in the geological record as part of a pre–flood biota that perished and the absence of fossilized organisms that appear later (such as angiosperms and mammals) as due to erosion of sediments deposited by the flood as waters receded off the land. Creationists say that fossilization can only take place when the organism is buried quickly to protect the remains from destruction by scavengers or decomposition. They say that the fossil record provides evidence of a single cataclysmic flood and not of a series of slow changes accumulating over millions of years.\n\nFlood geologists have proposed numerous hypotheses to reconcile the sequence of fossils evident in the fossil column with the literal account of Noah's flood in the Bible. Whitcomb and Morris proposed three possible factors:\n\n\nSome creationists believe that oil and coal deposits formed rapidly in sedimentary layers as volcanoes or flood waters flattened forests and buried the debris. They believe the vegetation decomposed rapidly into oil or coal due to the heat of the subterranean waters as they were unleashed from the Earth during the flood or by the high temperatures created as the remains were compressed by water and sediment.\n\nCreationists continue to search for evidence in the natural world that they consider consistent with the above description, such as evidence of rapid formation. For example, there have been claims of raindrop marks and water ripples at layer boundaries, sometimes associated with the claimed fossilized footprints of men and dinosaurs walking together. Such footprint evidence has been debunked and some have been shown to be fakes.\n\nProponents of Flood Geology state that \"native global flood stories are documented as history or legend in almost every region on earth\". \"These flood tales are frequently linked by common elements that parallel the biblical account including the warning of the coming flood, the construction of a boat in advance, the storage of animals, the inclusion of family, and the release of birds to determine if the water level had subsided.\" They suggest that \"the overwhelming consistency among flood legends found in distant parts of the globe indicates they were derived from the same origin, but oral transcription has changed the details through time\".\n\nAnthropologist Patrick Nunn rejects this view and highlights the fact that much of the human population lives near water sources such as rivers and coasts, where unusually severe floods can be expected to occur occasionally and will be recorded in tribal mythology.\n\nGeorge McCready Price attempted to fit a great deal of earth's geological history into a model based on a few accounts from the Bible. Price's simple model was used by Whitcomb and Morris initially but they did not build on the model in the 60s and 70s. However, a rough sketch of a creationist model could be constructed from creationist publications and debate material. Recent creationist efforts attempt to build complex models that incorporate as much scientific evidence as possible into the Biblical narrative. Some scientific evidence used for these models was formerly rejected by creationists. These models attempt to explain continental movements in a short time frame, the order of the fossil record, and the Pleistocene ice age.\n\nIn the 60s and 70s a simple creationist model proposed that, \"The Flood split the land mass into the present continents.\" Steve Austin and other creationists proposed a preliminary model of catastrophic plate tectonics (CPT) in 1994. Their work built on earlier papers by John Baumgardner and Russell Humphreys in 1986. Baumgardner proposed a model of mantle convection that allows for runaway subduction and Humphrey associated mantle convection with rapid magnetic reversals in earth history. Baumgardner's proposal holds that the rapid plunge of former oceanic plates into the mantle (caused by an unknown trigger-mechanism) increased local mantle pressures to the point that its viscosity dropped several magnitudes according to known properties of mantle silicates. Once initiated, sinking plates caused the spread of low viscosity throughout the mantle resulting in runaway mantle-convection and catastrophic tectonic motion which dragged continents across the surface of the earth. Once the former ocean plates, which are thought to be denser than the mantle, reached the bottom of the mantle an equilibrium resulted. Pressures dropped, viscosity increased, runaway mantle-convection stopped, leaving the surface of the earth rearranged. Proponents point to subducted slabs in the mantle which are still relatively cool, which they regard as evidence that they have not been there for millions of years which would result in temperature equilibration.\n\nGiven that conventional plate tectonics accounts for much of the geomorphic features of continents and oceans, it is natural that creationists would seek to develop a high speed version of the same process. CPT explains many geological features, provides mechanisms for the Biblical flood, and minimizes appeals to miracles.\n\nSome prominent creationists (Froede, Oard, Read) oppose CPT for various technical reasons. One main objection is that the model assumes the super continent Pangaea was intact at the initiation of the year-long flood. The CPT process then tore Pangaea apart creating the current configuration of the continents. But the breakup of Pangaea started early in the Mesozoic, meaning that CPT only accounts for part of the entire Phanerozoic geological record. CPT in this form only explains part of the geological column that flood geology normally explains. Modifying the CPT model to account for the entire Phanerozoic including multiple Wilson Cycles would complicate the model considerably.\n\nOther objections of CPT include the amount of heat produced for the rapid plate movements, and the fact that the cooling of hot oceanic plates and the raising of continental plates would take a great deal of time and require multiple small scale catastrophes after the flood ended. The original CPT proposal of Austin and others in 1994 was admittedly preliminary but the major issues have not been solved.\n\nThe vast majority of geologists regard the hypothesis of catastrophic plate tectonics as pseudoscience; they reject it in favor of the conventional geological theory of plate tectonics. It has been argued that the tremendous release of energy necessitated by such an event would boil off the Earth's oceans, making a global flood impossible. Not only does catastrophic plate tectonics lack any plausible geophysical mechanism by which its changes might occur, it also is contradicted by considerable geological evidence (which is in turn consistent with conventional plate tectonics), including:\n\n\nConventional plate tectonics accounts for the geological evidence already, including innumerable details that catastrophic plate tectonics cannot, such as why there is gold in California, silver in Nevada, salt flats in Utah, and coal in Pennsylvania, without requiring any extraordinary mechanisms to do so.\n\nIsaac Vail (1840–1912), a Quaker schoolteacher, in his 1912 work \"The Earth's Annular System\", extrapolated from the nebular hypothesis what he called the annular system of earth history, with the earth being originally surrounded by rings resembling those of Saturn, or \"canopies\" of water vapor. Vail hypothesised that, one by one, these canopies collapsed on the Earth, resulting in fossils being buried in a \"succession of stupendous cataclysms, separated by unknown periods of time\". The Genesis flood was thought to have been caused by \"the last remnant\" of this vapor. Although this final flood was geologically significant, it was not held to account for as much of the fossil record as George McCready Price had asserted. \n\nVail's ideas about geology appeared in Charles Taze Russell's \"The Photo-Drama of Creation\" and subsequently in Joseph Franklin Rutherford's \"Creation\" of 1927 and later publications. The Seventh-day Adventist physicist Robert W. Woods also proposed a vapor canopy, before \"The Genesis Flood\" gave it prominent and repeated mention in 1961.\n\nThough the vapor-canopy theory has fallen into disfavour among most creationists, Dillow in 1981 and Vardiman in 2003 attempted to defend the idea. Among its more vocal adherents, controversial Young Earth Creationist Kent Hovind uses it as the basis for his eponymous \"Hovind Theory\". Jehovah's Witnesses propose as the water source of the deluge a \"heavenly ocean\" that was over the earth from the second creative day until the Flood.\n\nModern geology, its sub-disciplines and other scientific disciplines utilize the scientific method to analyze the geology of the earth. The key tenets of flood geology are refuted by scientific analysis and do not have any standing in the scientific community. Modern geology relies on a number of established principles, one of the most important of which is Charles Lyell's principle of uniformitarianism. In relation to geological forces it states that the shaping of the Earth has occurred by means of mostly slow-acting forces that can be seen in operation today. By applying these principles, geologists have determined that the Earth is approximately 4.54 billion years old. They study the lithosphere of the Earth to gain information on the history of the planet. Geologists divide Earth's history into eons, eras, periods, epochs, and faunal stages characterized by well-defined breaks in the fossil record (see Geologic time scale). In general, there is a lack of any evidence for any of the above effects proposed by flood geologists and their claims of fossil layering are not taken seriously by scientists.\n\nThe global flood cannot explain geological formations such as angular unconformities, where sedimentary rocks have been tilted and eroded then more sedimentary layers deposited on top, needing long periods of time for these processes. There is also the time needed for the erosion of valleys in sedimentary rock mountains. In another example, the flood, had it occurred, should also have produced large-scale effects spread throughout the entire world. Erosion should be evenly distributed, yet the levels of erosion in, for example, the Appalachians and the Rocky Mountains differ significantly.\n\nGeochronology is the science of determining the absolute age of rocks, fossils, and sediments by a variety of techniques. These methods indicate that the Earth as a whole is about 4.54 billion years old, and that the strata that, according to flood geology, were laid down during the Flood some 6,000 years ago, were actually deposited gradually over many millions of years.\n\nIf the flood were responsible for fossilization, then all the animals now fossilized must have been living together on the Earth just before the flood. Based on estimates of the number of remains buried in the Karoo fossil formation in Africa, this would correspond to an abnormally high density of vertebrates worldwide, close to 2100 per acre.\nCreationists argue that evidence for the geological column is fragmentary, and all the complex layers of chalk occurred in the approach to the 150th day of Noah's flood. However, the entire geologic column is found in several places, and shows multiple features, including evidence of erosion and burrowing through older layers, which are inexplicable on a short timescale. Carbonate hardgrounds and the fossils associated with them show that the so-called flood sediments include evidence of long hiatuses in deposition that are not consistent with flood dynamics or timing.\n\nProponents of Flood Geology are also unable to account for the alternation between calcite seas and aragonite seas through the Phanerozoic. The cyclical pattern of carbonate hardgrounds, calcitic and aragonitic ooids, and calcite-shelled fauna has apparently been controlled by seafloor spreading rates and the flushing of seawater through hydrothermal vents which changes its Mg/Ca ratio.\n\nPhil Senter's 2011 article, \"The Defeat of Flood Geology by Flood Geology\", in the journal \"Reports of the National Center for Science Education\", discusses \"sedimentologic and other geologic features that Flood geologists have identified as evidence that particular strata cannot have been deposited during a time when the entire planet was under water ... and distribution of strata that predate the existence of the Ararat mountain chain.\" These include continental basalts, terrestrial tracks of animals, and marine communities preserving multiple in-situ generations included in the rocks of most or all Phanerozoic periods, and the basalt even in the younger Precambrian rocks. Others, occurring in rocks of several geologic periods, include lake deposits and eolian (wind) deposits. Using their own words, Flood geologists find evidence in every Paleozoic and Mesozoic period, and in every epoch of the Cenozoic period, indicating that a global flood could not have occurred during that interval.\n\n\n\n\n\n\n\n"}
{"id": "1818327", "url": "https://en.wikipedia.org/wiki?curid=1818327", "title": "Freshet", "text": "Freshet\n\nThe term freshet is most commonly used to describe a spring thaw resulting from snow and ice melt in rivers located in the northern latitudes of North America. A spring freshet can sometimes last several weeks on large river systems, resulting in significant inundation of flood plains as the snowpack melts in the river's catchment area. Freshets occur with generally diminishing strength and duration depending upon the snowpacks having large accumulations and then the local average rates of warming temperatures; late spring melts allowing faster flooding from the relatively longer days and higher solar angle against more southerly latitudes and elevations reaching average melting temperatures sooner where earlier and generally lesser seasonal snow piles melt more gradually spread over a longer melt period. Serious flooding from southern freshets are more often related to rain storms of large tropical weather systems rolling in from the South Atlantic or Gulf of Mexico, to add their powerful heating capacity to lesser snow packs. Tropically induced rainfall influenced quick melts can also affect snow cover to latitudes as far north as southern Canada, so long as the generally colder air mass is not blocking northward movement of low pressure systems.\nIn the eastern part of the continent, annual freshets occur from the Canadian Taiga ranging along both sides of the Great Lakes then down through the heavily forested Appalachian mountain chain and St. Lawrence valley from Northern Maine into barrier ranges in North Carolina and Tennessee.\n\nIn the western part of the continent, freshets occur throughout the generally much higher elevations of the various west coast mountain ranges that extend southward down from Alaska even into the northern parts of Arizona and New Mexico.\n\n\n"}
{"id": "30633444", "url": "https://en.wikipedia.org/wiki?curid=30633444", "title": "Full-Option Science System", "text": "Full-Option Science System\n\nFOSS (Full-Option Science System) is a research-based science curriculum for grades K—8 developed at the Lawrence Hall of Science, University of California, Berkeley. FOSS is also an ongoing research project dedicated to improving the learning and teaching of science.\n\nThe FOSS K–8 program was developed at the Lawrence Hall of Science, University of California at Berkeley, under three separate National Science Foundation grants (1988 1991, 1996.) The program was originally developed and trial tested in urban and suburban San Francisco Bay Area school districts and field-tested and implemented nationally in ten sites. Twenty-six modules were developed for K–6, and nine courses for middle school. The FOSS K–6 program went through a major revision in 2000 and was then adopted in Texas. The program went through a minor revision in 2005. A new, California specific, edition was developed for the California science adoption (K–5) in 2007. FOSS is currently published by Delta Education, a division of School Specialty Inc.\n\nThe FOSS program uses several instructional pedagogies: inquiry-based learning (each investigation is guided by questions), hands-on learning and active investigation (students work with materials and conduct investigations to attempt to answer questions), student-to-student interaction, writing (students keep careful notes in science notebooks), and research/reading (readings are included to enhance or underscore active investigation—students work with materials prior to doing any reading).\n\nEach K–6 module consists of a kit of student materials, a teacher guide, and a student reading book. Each middle school course includes a kit of student materials, a teacher guide, a Science Resources book, a lab notebook, and a course-specific multimedia component. The FOSS website, FOSSweb, contains additional online activities for students and resources for parents and teachers. FOSSweb has two sister sites, FOSSweb CA, for the California 2007 edition, and FOSSweb NYC, for New York City teachers using FOSS.\n\nFOSS is in use in every state in the country with over 100,000 teachers and 2 million students and is in approximately 16% of the nation's school districts. FOSS was the first non-t\nex\ntbook curriculum to make the California adoption list (1992) and was adopted in California for the 2007 science adoption (FOSS California K–5). The program is on many state adoption lists and is used in 50 of the 100 largest U.S. school districts. FOSS is cited as an exemplary program in publications by nationally recognized organizations in the science reform movement, including National Science Resources Center, and the National Science Teachers Association.\n\nFOSS has been approved as a recommended science program in each of the adoption states in which it has been submitted.\n\nScruggs, T., & Mastropieri, M. (1994). The construction\nof scientific knowledge by students with mild disabilities.\nThe Journal of Special Education, 28, 307-321.\n\n"}
{"id": "74847", "url": "https://en.wikipedia.org/wiki?curid=74847", "title": "Geographer", "text": "Geographer\n\nA geographer is a scientist whose area of study is geography, the study of Earth's natural environment and human society. The Greek prefix, \"geo,\" means \"earth\" and the Greek suffix, \"graphy,\" meaning \"description,\" so a geographer is someone who studies the earth. The word \"geography\" is a Middle French word that is believed to have been first used in 1540.\n\nAlthough geographers are historically known as people who make maps, map making is actually the field of study of cartography, a subset of geography. Geographers do not study only the details of the natural environment or human society, but they also study the reciprocal relationship between these two. For example, they study how the natural environment contributes to the human society and how the human society affects the natural environment.\n\nIn particular, physical geographers study the natural environment while human geographers study human society. Modern geographers are the primary practitioners of the GIS (geographic information system), who are often employed by local, state, and federal government agencies as well as in the private sector by environmental and engineering firms.\n\nThe paintings by Johannes Vermeer titled \"The Geographer\" and \"The Astronomer\" are both thought to represent the growing influence and rise in prominence of scientific enquiry in Europe at the time of their painting in 1668–69.\n\nThe ancient Greeks discovered much of the knowledge that formed the basis for later scientific and geographical discoveries. Eratosthenes of Cyrene (born circa 276 BCE in Cyrene, Libya; died circa 194 BCE in Alexandria, Egypt) was a Greek writer, poet, astronomer, and geographer who created the first known measurements of the Earth's circumference. Eratosthenes used the angles at which the sun's rays fall on the ground to calculate the curvature of the Earth's surface, which he then used to determine the full horizontal length around the Earth. According to the works of Aristotle, other scientists had estimated the circumference of the Earth prior to Eratosthenes's discovery, but the records of the specific methods and calculations that they used did not survive.\n\nHipparchus (born in Nicaea, Bithynia [now Iznik, Turkey]; believed to have died after 127 BC in Rhodes) is best known for using astronomical studies to predict the movements and positions of celestial bodies such as the sun, moon, and stars. He discovered the equinoxes (autumnal equinox and spring equinox) and calculated the length of the solar year. Hipparchus was also the first to correctly determine the distance between the earth and the moon (which he believed was equal to 63 times the earth's radius, although it is actually equal to 60 times the earth's radius). Building upon this knowledge, he developed the parallax theory to explain the apparent displacement of celestial bodies relative to the observer's vantage point.\n\nAlexander von Humboldt (born Sept. 14, 1769 in Berlin; died May 6, 1859) was a German geographer, naturalist and explorer who made major contributions to the field of biogeography by producing break-through research on the connection between the climate of a region and the plant life growing there. The records he kept of his travels provided highly valuable data on the geography of Central Asia, which was not very well understood by Europeans at the time. Furthermore, with the help of Sir Edward Sabine, he was able to prove that the origin of storms is extraterrestrial by showing the relationship between magnetic storms on earth with the changing activity of sun spots. In his latter years, Humboldt published a multi-volume series of books called \"Kosmos\", wherein he described the structure of the universe as it was then understood. Initially, he published four volumes, which were received with great success and were subsequently translated into almost every European language. Humboldt began work on the fifth volume, but died before it was completed (he was ninety years old). The Humboldt Current (also known as the Peru Current), which was named after him, is an ocean current that holds warm air off a 600-mile coastal region between Peru and Chile in South America, thereby keeping the climate cool. Humboldt had explored this region in 1802, and it is now known for containing the world's richest marine ecosystem.\n\nAlfred Russel Wallace, also known as A.R. Wallace, (born on Jan. 8, 1823, in Usk, Monmouthshire, Wales; died on Nov. 7, 1913, in Broadstone, Dorset, England), was a British naturalist, geographer, and social critic who is most known for formulating the theory of evolution by natural selection before Charles Darwin published his famous work on the same subject, \"The Origin of Species\". An avid traveler and explorer, Wallace became both rich and famous from the work he did identifying new animal species in Indonesia and the Amazon region. Importantly, Wallace discovered and defined the concept of \"speciation,\" which he discussed in a paper he published around 1865 describing his research on butterflies. Speciation refers to defining animal species based on their interbreeding capabilities; animals within the same species can breed with each other, but not with animals from other species. In this way, individual species can be identified for scientific classification purposes.\n\nSir Richard Francis Burton (born on March 19, 1821, in Torquay, Devonshire, England; died on October 20, 1890, in Trieste, Austria-Hungary [now in Italy]) is known for being the first European to discover Lake Tanganyika in east Africa and for gaining access to Muslim cities, which Europeans had previously been forbidden to access. Lake Tanganyika is estimated to be the second deepest and the second largest freshwater lake in the world and is believed to be one of the oldest lakes in the world (approximately twenty million years old by evolutionary timelines).\n\nStrabo (born circa 64 BC in Amaseia, Pontus; died some time after 21 AD) was a Greek geographer and historian who left a remarkably comprehensive written record of all the people and countries known to the Greeks and Romans around 27 BC-14 AD. This record, a book called \"Geography\", also provides information on the technology geographers used to study the world at that time as well as the history of the many of the countries mentioned in his records. Although Strabo's account is far from perfect (it assumes, for example, that the earth, then called oikoumene, is one giant landmass consisting of Europe, Asia, Africa, and their associated islands surrounded by a large ocean), his writing is highly valued for being a rich compilation of not only his own observations but also the research and ideas of other respected scientific figures and thinkers including Eratosthenes, Hipparchus, Polybius, Artemidorus, Apollodorus, Aristotle, and Strato.\n\nMuhammad al-Idrisi (born in 1100 in Ceutah, Morocco; died around 1165-66 in Ceutah) is known for compiling an enormous world geography for the Norman King of Sicily, Roger II. The book, subsequently named the \"Book of Roger\", was completed in January 1154 and was written as the commentary for a large map of the world created by al-Idrisi (also at the bequest of the King). An avid traveler himself, al-Idrisi relied on contemporary accounts of the world, the works of Ptolemy, as well as geographical literature from Greek, Latin, and Arabic sources to create his work. The \"Book of Roger\" contained fairly accurate information on the countries of Europe and was released in several editions although, as of the mid-1900s, the full text had never been edited.\n\nClaudius Ptolemy, known simply as Ptolemy, (born around 100 AD; died c. 170 AD) was an astronomer, mathematician, and geographer who lived in Alexandria and is known for (among other things) drawing detailed maps of what was known of the world at the time. Ptolemy's geographical works include tables of latitude and longitude and a discussion of how such lines could be mathematically calculated. In Ptolemy's most famous work, \"Almagest\", he comprehensively describes the layout of the stars and other astronomical objects as understood by ancient Greek and Babylonian scientists. He also put forth the idea of an earth-centered universe, an idea widely believed until the 1500s when Copernicus began to suggest that the earth and other planets in the solar system actually revolve around the sun (called the \"heliocentric\" (Sun-centered) theory). \n\nGerardus Mercator, originally name Gerard De Cremer or simply Kremer(?) (born on March 5, 1512 in Rupelmonde, Flanders [now in Belgium]; died on December 2, 1594 in Duisburg, Duchy of Cleve [now in Germany]) was a cartographer famous for creating a heart-shaped representation of the earth (also called the \"Mercator projection\" of the earth) in response to a demand for flat maps that could accurately depict the earth as a spherical object. Mercator was also the first to begin using the term \"atlas\" to describe a collection of maps. Because of his work, travelers and explorers could maintain a course over long distances using straight lines, thus eliminating the need to constantly adjust their compass readings in order to keep track of their movements.\n\nCharles Lyell (1797–1875) was a British geologist whose theory of Uniformitarianism significantly impacted the way modern scientists understand the earth's structure and how this structure can change over time. Uniformitarianism overturned the previously accepted theory of Catastrophism, which held that earth's features were formed by sudden, violent forces rather than by gradual changes over time. Lyell contended that major geological forces in effect today, including volcanic eruptions, earthquakes, and erosion due to wind, water, and gravity, have happened throughout earth's history and are responsible changing its surface features. The theory of evolution is believed to have gained acceptance in part because of Lyell's efforts to promote gradual biological change rather than change due to sudden, extreme events. Building upon the work of James Hutton, Charles Lyell is also responsible for spreading the idea that older rocks are buried underneath newer ones, which led scientists to begin studying fossils and rock layers in order to better understand the history of the earth's geography.\n\nPytheas, a Greek geographer, astronomer, and explorer who lived around 300 bc near Massalia, Gaul (a region comprising mostly modern-day France) is known for producing the first known records of the history of Britain. Pytheas's most important work, \"On the Ocean\", has since been lost, but most of what is known about him was recorded by the Greek historian, Polybius (approx. 200-118 BC). Through travel and detailed observation, Pytheas noticed that the length of the days increased as he traveled northward and that the moon affects the tides in the water. His travels took him to the northernmost parts of Europe including Thule (the northernmost inhabited island, roughly six days sail beyond Britain) and at least to the Arctic Circle (possible Norway or Iceland).\n\nSir Douglas Mawson (born in Yorkshire, England in 1882; died in Brighton, England in 1958) is known for his exploration of Antarctica, although he also studied uranium and other radioactive materials as a preeminent professor of geology at the University of Adelaide in Australia. Along with Edgeworth David and Dr. A.F. Mackay, Mawson was the first man to reach the summit of Mount Erebus, an ice-covered volcanic mountain approximately 11,400 feet high. His expeditions led Britain to surrender its claims over the Antarctic territory to Australia in 1936, and the methods he developed to survive in, and travel through, freezing climates paved the way for subsequent explorations of sub-zero regions.\n\nThere are three major fields of study, which are further subdivided:\n\nThe National Geographic Society identifies five broad key themes for geographers:\n\n\n"}
{"id": "177793", "url": "https://en.wikipedia.org/wiki?curid=177793", "title": "Great chain of being", "text": "Great chain of being\n\nThe Great Chain of Being is a strict hierarchical structure of all matter and life, thought in medieval Christianity to have been decreed by God. The chain starts with God and progresses downward to angels, demons (fallen/renegade angels), stars, moon, kings, princes, nobles, commoners, wild animals, domesticated animals, trees, other plants, precious stones, precious metals and other minerals.\n\nThe Great Chain of Being (, \"Ladder of Being\") is a concept derived from Plato, Aristotle (in his \"Historia Animalium\"), Plotinus and Proclus. Further developed during the Middle Ages, it reached full expression in early modern Neoplatonism.\n\nThe Chain of Being is composed of a great number of hierarchical links, from the most basic and foundational elements up through the very highest perfection: God.\n\nGod sits at the top of the chain, and beneath him sit the angels, both existing wholly in \"spirit\" form. Earthly flesh is fallible and ever-changing, mutable. Spirit, however, is unchanging and permanent. This sense of permanence is crucial to understanding this conception of reality. It is generally impossible to change the position of an object in the hierarchy. (One exception might be in the realm of alchemy, where alchemists attempted to transmute base elements, such as lead, into higher elements, either silver or, more often, gold—the highest \"element\".)\n\nIn the natural order, earth (rock) is at the bottom of the chain; this element possesses only the attribute of existence. Each link succeeding upward contains the positive attributes of the previous link and adds at least one other. Rocks possess only existence; the next link up is plants which possess life \"and\" existence. Animals add motion and appetite as well.\n\nMan is both mortal flesh, as those below him, and also spirit, as those above. In this dichotomy, the struggle between flesh and spirit becomes a moral one. The way of the spirit is higher, more noble; it brings one closer to God. The desires of the flesh move one away from God. The Christian fall of Lucifer is thought of as especially terrible, as angels are wholly spirit, yet Lucifer defied God (who is the ultimate perfection).\n\nEach link in the chain might be divided further into its component parts. In medieval secular society, for example, the king is at the top, succeeded by the aristocratic lords and the clergy, and then the peasants below them. Solidifying the king's position at the top of humanity's social order is the doctrine of the Divine Right of Kings. The implied permanent state of inequality became a source of popular grievance, and led eventually to political change as in the French Revolution. In the family, the father is head of the household; below him, his wife; below her, their children.\n\nMilton's \"Paradise Lost\" ranked the angels (c.f. Pseudo-Dionysius the Areopagite's ranking of angels), and Christian culture conceives of angels in orders of archangels, seraphim, and cherubim, among others.\n\nSubdivisions are equally apparent among animals. At the top of the animals are wild beasts (such as lions), which were seen as superior as they defied training and domestication. Below them are domestic animals, further sub-divided so that useful animals (such as dogs and horses) are higher than docile creatures (such as sheep). Birds are also sub-divided, with eagles above pigeons, for example. Fish come below birds and are subdivided between actual fish and other sea creatures. Below them come insects, with useful insects such as spiders and bees and attractive creatures such as ladybirds and dragonflies at the top, and unpleasant insects such as flies and beetles at the bottom. At the very bottom of the animal sector are snakes, which are relegated to this position as punishment for the serpent's actions in the Garden of Eden.\n\nBelow animals comes the division for plants, which is further subdivided. Trees are at the top, with useful trees such as oaks at the top, and the traditionally demonic yew tree at the bottom. Food-producing plants such as cereals and vegetables are further subdivided.\n\nAt the very bottom of the chain are minerals. At the top of this section are metals (further sub-divided, with gold at the top and lead at the bottom), followed by rocks (with granite and marble at the top), soil (subdivided between nutrient-rich soil and low-quality types), sand, grit, dust, and dirt at the very bottom of the entire great chain.\n\nThe central concept of the Chain of Being is that everything imaginable fits in somewhere, giving order and meaning to the universe.\n\nGod is at the top of the chain and is also external to creation. God is believed to exist outside the physical limitations of time and space. He possessed the spiritual attributes of reason, love, and imagination, like all spiritual beings, but he alone possessed the divine attributes of omnipotence, omniscience, and omnipresence. God serves as the model of authority for the strongest, most virtuous, most excellent type of being within any category.\n\nAngels were beings of pure spirit who had no physical bodies of their own. In order to affect the physical world, angels were thought to build temporary bodies for themselves out of particles of earthly elements. Medieval and Renaissance theologians believed angels to possess reason, love, imagination, and, like God, to stand outside the physical limitations of time. They possessed sensory awareness unbound by physical organs, and they possessed language. They lacked, however, the divine attributes of omnipotence, omniscience, and omnipresence of God, and they simultaneously lacked the physical passions experienced by humans and animals. Depending upon the author, the class of angels was further subdivided into three, seven, nine, or ten ranks, variously known as triads, orders, or choirs. Each rank had greater power and responsibility than the entities below them. The most common classification is that of St. Thomas Aquinas. \n\n\nFor Medieval and Renaissance thinkers, humans occupied a unique position on the Chain of Being, straddling the world of spiritual beings and the world of physical creation. Humans were thought to possess divine powers such as reason, love, and imagination. Like angels, humans were spiritual beings, but unlike angels, human souls were \"knotted\" to a physical body. As such, they were subject to passions and physical sensations—pain, hunger, thirst, sexual desire—just like other animals lower on the Chain of Being. They also possessed the powers of reproduction unlike the minerals and rocks lowest on the Chain of Being. Humans had a particularly difficult position, balancing the divine and the animalistic parts of their nature. For instance, an angel is only capable of intellectual sin such as pride (as evidenced by Lucifer's fall from heaven in Christian belief). Humans, however, were capable of both intellectual sin and physical sins such as lust and gluttony if they let their animal appetites overrule their divine reason. Humans also possessed sensory attributes: sight, touch, taste, hearing, and smell. Unlike angels, however, their sensory attributes were limited by physical organs (they could only know things discerned through the five senses). The highest-ranking human being was the king.\n\nAnimals, like humans higher on the chain, were animated (capable of independent motion). They possessed physical appetites and sensory attributes, the number depending upon their position within the Chain of Being. They had limited intelligence and awareness of their surroundings. Unlike humans, they were thought to lack spiritual and mental attributes such as immortal souls and the ability to use logic and language. The primate of all animals (the \"king of beasts\") was variously thought to be either the lion or the elephant. However, each subgroup of animals also had its own primate, an avatar superior in qualities of its type.\n\n\nNote that avian creatures, linked to the element of air, were considered superior to aquatic creatures linked to the element of water. Air naturally tended to rise and soar above the surface of water, and analogously, aerial creatures were placed higher in the chain.\n\n\nThe chart would continue to descend through various reptiles, amphibians, and insects. The higher up the chart one went, the more noble, mobile, strong, and intelligent the creature in Renaissance belief. At the very bottom of the animal section, we find sessile creatures like the oysters, clams, and barnacles. Like the plants below them, these creatures lacked mobility, and were thought to lack various sensory organs such as sight and hearing. However, they were still considered superior to plants because they had tactile and gustatory senses (touch and taste).\n\nPlants, like other living creatures, possessed the ability to grow in size and reproduce. However, they lacked mental attributes and possessed no sensory organs. Instead, their gifts included the ability to eat soil, air, and \"heat.\" Plants did have greater tolerances for heat and cold, and immunity to the pain that afflicts most animals. At the very bottom of the botanical hierarchy, fungi and mosses, lacking leaf and blossom, were so limited in form that Renaissance thinkers thought them scarcely above the level of minerals. However, each plant was also thought to be gifted with various edible or medicinal virtues unique to its own type.\n\n\nCreations of the earth, the lowest of elements, all minerals lacked the plant's basic ability to grow and reproduce. They also lacked mental attributes and sensory organs found in beings higher on the chain. Their unique gifts, however, were typically their unusual solidity and strength. Many minerals, in fact, were thought to possess magical powers, particularly gems. The mineral primate is the diamond.\n\n\nThe basic idea of a ranking of the world's organisms goes back to Aristotle's biology. In his \"History of Animals\", where he ranked animals over plants based on their ability to move and sense, and graded the animals by their reproductive mode and possession of blood (he ranked all invertebrates as \"bloodless\").\n\nAristotle's non-religious concept of higher and lower organisms was taken up by natural philosophers during the Scholastic period to form the basis of the \"Scala Naturae\". The \"scala\" allowed for an ordering of beings, thus forming a basis for classification where each kind of mineral, plant and animal could be slotted into place. In medieval times, the great chain was seen as a God-given ordering: God at the top, dirt at the bottom, every grade of creature in its place. Just as rock never turns to flowers and worms never turn to lions, humans never turn to angels. This was not our lot in life. In the Northern Renaissance, the scientific focus shifted to biology. The threefold division of the chain below humans formed the basis for Linnaeus's \"Systema Naturæ\" from 1737, where he divided the physical components of the world into the three familiar kingdoms of minerals, plants and animals.\n\nThe set nature of species, and thus the absoluteness of creatures' places in the great chain, came into question during the 18th century. The dual nature of the chain, divided yet united, had always allowed for seeing creation as essentially one continuous whole, with the potential for overlap between the links. Radical thinkers like Jean-Baptiste Lamarck saw a progression of life forms from the simplest creatures striving towards complexity and perfection, a schema accepted by zoologists like Henri de Blainville. The very idea of an ordering of organisms, even if supposedly fixed, laid the basis for the idea of transmutation of species, for example Charles Darwin's theory of evolution.\n\nThe Chain of Being continued to be part of metaphysics in 19th century education, and the concept was well known. The geologist Charles Lyell used it as a metaphor in his 1851 \"Elements of Geology\" description of the geological column, where he used the term \"missing links\" in relation to missing parts of the continuum. The term \"missing link\" later came to signify transitional fossils, particularly those bridging the gulf between man and beasts.\n\nThe idea of the great chain as well as the derived \"missing link\" was abandoned in early 20th century science, as the notion of modern animals representing ancestors of other modern animals was abandoned in biology. The idea of a certain sequence from \"lower\" to \"higher\" however lingers on, as does the idea of progress in biology.\n\nAllenby and Garreau propose the Catholic Church's narrative of the Great Chain of Being kept the peace for centuries in Europe. The very concept of rebellion simply lay outside the reality within which most people lived for to defy the King was to defy God. King James I himself wrote, \"The state of monarchy is the most supreme thing upon earth: for kings are not only God's Lieutenants upon earth, and sit upon God's throne, but even by God himself they are called Gods.\"\n\nThe Enlightenment broke this supposed divine plan and fought the last vestiges of feudal hierarchy by creating secular governmental structures that vested power into the hands of ordinary citizens rather than divinely ordained monarchs.\n\nHowever, scholars such as Brian Tierney and Michael Novak have noted the medieval contribution to democracy and human rights.\n\nThe American spiritual writer and philosopher Ken Wilber uses a concept called the \"Great Nest of Being\" which is similar to the Great Chain of Being, and which he claims to belong to a culture-independent \"perennial philosophy\" traceable across 3000 years of mystical and esoteric writings. Wilber's system corresponds with other concepts of transpersonal psychology.\n\nIn the 1977 book \"A Guide for the Perplexed\", British philosopher and economist E. F. Schumacher wrote that fundamental gaps exist between the existence of minerals, plants, animals and humans, where each of the four classes of existence is marked by a level of existence not shared by that below. Clearly influenced by the great chain of being, but lacking the angels and God, he called his hierarchy the \"levels of being\". In the book, he claims that science has generally avoided seriously discussing these discontinuities, because they present such difficulties for strictly materialistic science, and they largely remain mysteries.\n\n"}
{"id": "386398", "url": "https://en.wikipedia.org/wiki?curid=386398", "title": "Hypothetico-deductive model", "text": "Hypothetico-deductive model\n\nThe hypothetico-deductive model or method is a proposed description of scientific method. According to it, scientific inquiry proceeds by formulating a hypothesis in a form that can be falsifiable, using a test on observable data where the outcome is not yet known. A test outcome that could have and does run contrary to predictions of the hypothesis is taken as a falsification of the hypothesis. A test outcome that could have, but does not run contrary to the hypothesis corroborates the theory. It is then proposed to compare the explanatory value of competing hypotheses by testing how stringently they are corroborated by their predictions.\n\nOne example of an algorithmic statement of the hypothetico-deductive method is as follows:\n\nOne possible sequence in this model would be 1, 2, 3, 4. If the outcome of 4 holds, and 3 is not yet disproven, you may continue with 3, 4, 1, and so forth; but if the outcome of 4 shows 3 to be false, you will have to go back to 2 and try to invent a new 2, deduce a new 3, look for 4, and so forth.\n\nNote that this method can never absolutely verify (prove the truth of) 2. It can only falsify 2. (This is what Einstein meant when he said, \"No amount of experimentation can ever prove me right; a single experiment can prove me wrong.\")\n\nAdditionally, as pointed out by Carl Hempel (1905–1997), this simple view of the scientific method is incomplete; a conjecture can also incorporate probabilities, e.g., the drug is effective about 70% of the time. Tests, in this case, must be repeated to substantiate the conjecture (in particular, the probabilities). In this and other cases, we can quantify a probability for our confidence in the conjecture itself and then apply a Bayesian analysis, with each experimental result shifting the probability either up or down. Bayes' theorem shows that the probability will never reach exactly 0 or 100% (no absolute certainty in either direction), but it can still get very close to either extreme. See also confirmation holism.\n\nQualification of corroborating evidence is sometimes raised as philosophically problematic. The raven paradox is a famous example. The hypothesis that 'all ravens are black' would appear to be corroborated by observations of only black ravens. However, 'all ravens are black' is logically equivalent to 'all non-black things are non-ravens' (this is the contraposition form of the original implication). 'This is a green tree' is an observation of a non-black thing that is a non-raven and therefore corroborates 'all non-black things are non-ravens'. It appears to follow that the observation 'this is a green tree' is corroborating evidence for the hypothesis 'all ravens are black'. Attempted resolutions may distinguish:\n\nEvidence contrary to a hypothesis is itself philosophically problematic. Such evidence is called a falsification of the hypothesis. However, under the theory of confirmation holism it is always possible to save a given hypothesis from falsification. This is so because any falsifying observation is embedded in a theoretical background, which can be modified in order to save the hypothesis. Popper acknowledged this but maintained that a critical approach respecting methodological rules that avoided such \"immunizing stratagems\" is conducive to the progress of science.\n\nPhysicist Sean Carroll claims the model ignores underdetermination.\nThe hypothetico-deductive model (or approach) versus other research models\n\nThe hypothetico-deductive approach contrasts with other research models such as the inductive approach or grounded theory. In the data percolation methodology, \nthe hypothetico-deductive approach is included in a paradigm of pragmatism by which four types of relations between the variables can exist: descriptive, of influence, longitudinal or causal. The variables are classified in two groups, structural and functional, a classification that drives the formulation of hypotheses and the statistical tests to be performed on the data so as to increase the efficiency of the research. \n\n"}
{"id": "99965", "url": "https://en.wikipedia.org/wiki?curid=99965", "title": "Index of computing articles", "text": "Index of computing articles\n\nOriginally, the word computing was synonymous with counting and calculating, and the science and technology of mathematical calculations. Today, \"computing\" means using computers and other computing machines. It includes their operation and usage, the electrical processes carried out within the computing hardware itself, and the theoretical concepts governing them (computer science).\n\n\"See also:\" List of programmers, List of computing people, List of computer scientists, List of basic computer science topics, List of terms relating to algorithms and data structures.\n\nTopics on computing include:\n1.TR.6 –\n100BaseFX –\n100BaseTX –\n100BaseT –\n100BaseVG –\n100VG-AnyLAN –\n10base2 –\n10base5 –\n10baseT –\n120 reset –\n16-bit –\n16-bit application –\n16550 UART –\n1NF –\n1TBS –\n\n2.PAK –\n20-GATE –\n20-GATE –\n28-bit –\n2B1D –\n2B1Q –\n2D –\n2NF –\n\n3-tier (computing) –\n32-bit application –\n32-bit –\n320xx microprocessor –\n320xx –\n386BSD –\n386SPART.PAR –\n3Com Corporation –\n3DO –\n3D computer graphics –\n3GL –\n3NF –\n3Station –\n\n4.2BSD –\n404 error –\n431A –\n473L system –\n486SX –\n4GL –\n4NF –\n\n51-FORTH –\n56 kbit/s line –\n5ESS switch –\n5NF –\n5th Glove –\n\n6.001 –\n64-bit –\n680x0 –\n6x86 –\n\n8-bit clean –\n8.3 filename –\n80x86 –\n82430FX –\n82430HX –\n82430MX –\n82430VX –\n8514 (display standard) –\n8514-A –\n88open –\n8N1 –\n8x86 –\n\n90-90 Rule –\n9PAC\n\nABC ALGOL –\nABLE –\nABSET –\nABSYS –\nAccent –\nAcceptance, Test Or Launch Language –\nAccessible Computing –\nAda –\nAddressing mode –\nAIM alliance –\nAirPort –\nAIX –\nAlan –\nALGOL –\nAlgorithm –\nAltiVec –\nAmdahl's law –\nAmerica Online –\nAmiga –\nAmigaE –\nAnalysis of algorithms –\nAOL –\nAPL –\nApple Computer –\nApple II –\nApple Macintosh –\nAppleScript –\nArray programming –\nArithmetic and logical unit –\nASCII –\nActive Server Pages –\nASP.NET –\nAssembly language –\nAtari –\nAtlas Autocode –\nAutoLISP –\nAutomaton –\nAWK –\n\nB –\nBackus–Naur form –\nBasic Rate Interface (2B+D)--\nBASIC –\nBatch job –\nBCPL –\nBefunge –\nBeOS –\nBerkeley Software Distribution –\nBETA –\nBig Mac –\nBig O notation –\nBinary symmetric channel –\nBinary Synchronous Transmission –\nBinary numeral system –\nBit –\nBLISS –\nBlue –\nBlu-ray Disc –\nBlue screen of death –\nBourne shell (sh)\nBourne-Again shell (bash)\nBrainfuck –\nBtrieve –\nBurrows-Abadi-Needham logic –\nBusiness Computing –\n\nC++ –\nC# –\nC –\nCache –\nCanonical LR parser –\nCat (Unix) –\nCD-ROM –\nCentral processing unit –\nChimera –\nChomsky normal form –\nCIH virus –\nClassic Mac OS –\nCOBOL –\nCocoa (software) –\nCode and fix –\nCode Red worm –\nColdFusion –\nColouring algorithm –\nCOMAL –\nComm (Unix) –\nCommand line interface –\nCommand line interpreter –\nCOMMAND.COM –\nCommercial at (computing) –\nCommodore 1541 –\nCommodore 1581 –\nCommodore 64 –\nCommodore Amiga –\nCommon logarithm –\nCommon Unix Printing System –\nCompact disc –\nCompiler –\nComputability theory –\nComputational complexity theory –\nComputation –\nComputer-aided design –\nComputer-aided manufacturing –\nComputer architecture –\nComputer cluster –\nComputer hardware –\nComputer network –\nComputer numbering formats –\nComputer programming –\nComputer science –\nComputer security –\nComputer software –\nComputer system –\nComputer –\nComputing –\nContext-free grammar –\nContext-sensitive grammar –\nContext-sensitive language –\nControl flow –\nControl store –\nControl unit –\nCORAL66 –\nCP/M operating system –\nCPL –\nCracking (software) –\nCracking (passwords) –\nCryptanalysis –\nCryptography –\nCybersquatting –\nCYK algorithm –\nCyrix 6x86 –\n\nD –\nData compression –\nDatabase normalization –\nDecidable set –\nDeep Blue –\nDesktop environment –\nDesktop publishing –\nDeterministic finite automaton –\nDialer -\nDIBOL –\nDiff –\nDigital camera –\nDEC (Digital Equipment Corporation) –\nDigital signal processing –\nDigital visual interface –\nDirect manipulation interface –\nDisk storage –\nDistance transform –\nDistance map –\nDistance field –\nDVD –\nDVI (TeX) –\nDvorak Simplified Keyboard –\nDylan –\n\nEarth Simulator –\nEBCDIC –\nECMAScript (a.k.a. JavaScript) –\nElectronic data processing (EDP) –\nEnhanced Versatile Disc (EVD) –\nENIAC –\nEnterprise Java Beans (EJB) –\nEntscheidungsproblem –\nEquality (relational operator) –\nErlang –\nEnterprise resource planning (ERP) –\nES EVM –\nEthernet –\nEuclidean algorithm –\nEuphoria –\nExploit (computer science) –\n\nFederated Naming Service –\nField specification -\nFinal Cut Pro –\nFinite state automaton –\nFireWire –\nFirst-generation language –\nFloating point unit –\nFloppy disk –\nFormal language –\nForth –\nFortran –\nFourth-generation language –\nFragmentation –\nFree On-line Dictionary of Computing –\nFree Software Foundation –\nFree software movement –\nFree software –\nFreescale 68HC11 –\nFreeware –\nFunction-level programming –\nFunctional programming –\n\nG4 –\nG5 –\nGEM –\nGeneral Algebraic Modeling System –\nGenie –\nGNU -\nGNU bison –\nGnutella –\nGodiva –\nGraphical user interface –\nGraphics Device Interface –\nGreibach normal form –\nG.hn –\n\nhack (technology slang) –\nHacker (computer security) -\nHacker (hobbyist) -\nHacker (programmer subculture) –\nHacker (term) -\nHalting problem –\nHard Drive –\nHaskell –\nHD DVD –\nHistory of computing –\nHistory of computing hardware –\nHistory of Microsoft Windows –\nHistory of operating systems –\nHistory of the graphical user interface –\nHitachi 6309 –\nHome computer –\nHugo –\nHuman-computer interaction –\n\nIA-32 –\nIA-64 –\nIBM PC –\nInteractive computation -\nIBM –\niBook –\niCab –\niCal –\nIcon –\niDVD –\nIEEE 802.2 –\nIEEE 802.3 –\nIEEE floating-point standard –\niMac –\nimage processing –\niMovie –\nInform –\nInstruction register –\nIntel 8008 –\nIntel 80186 –\nIntel 80188 –\nIntel 80386 –\nIntel 80486SX –\nIntel 80486 –\nIntel 8048 –\nIntel 8051 –\nIntel 8080 –\nIntel 8086 –\nIntel 80x86 –\nIntel –\nINTERCAL –\nInternational Electrotechnical Commission –\nInternet Explorer –\nInternet –\niPhoto –\niPod –\niResQ -\nIrreversible circuit -\niSync –\niTunes –\n\nJ –\nJava Platform, Enterprise Edition –\nJava Platform, Micro Edition –\nJava Platform, Standard Edition –\nJava API –\nJava –\nJava Virtual Machine –\nJavaScript –\nJPEG –\n\nK&R –\nKDE –\nKid –\nKilobyte –\nKleene star –\nKlez –\nKRYPTON –\n\nLALR parser –\nLambda calculus –\nLasso –\nLaTeX –\nLeet –\nLegal aspects of computing –\nLex –\nLimbo –\nLinked list –\nLinux –\nLisp –\nList of coding terms –\nList of computing terms that end in \"ware\" –\nList of IBM products –\nList of information technology management topics –\nList of Intel microprocessors –\nList of programming languages –\nList of operating systems –\nList of Soviet computer systems –\nLL parser –\nLogical programming –\nLogo –\nLotus 1-2-3 –\nLR parser –\nLua –\nLynx language –\nLynx browser\n\nm4 –\nmacOS Server –\nmacOS –\nMAD –\nMainframe computer –\nMalware -\nMary –\nMealy machine –\nMegabyte –\nMelissa worm -\nMercury –\nMesa –\nMicrocode –\nMicroprocessor –\nMicroprogram –\nMicrosequencer –\nMicrosoft Windows –\nMicrosoft –\nMiranda –\nML –\nMMC –\nMMU –\nMMX –\nMobile Trin –\nModula –\nMOO –\nMoore's Law –\nMoore machine –\nMorris worm –\nMOS Technologies 6501 –\nMOS Technologies 6502 –\nMOS Technologies 650x –\nMOS Technologies 6510 –\nMotorola 68000 –\nMotorola 6800 –\nMotorola 68020 –\nMotorola 68030 –\nMotorola 68040 –\nMotorola 68060 –\nMotorola 6809 –\nMotorola 680x0 –\nMotorola 68LC040 –\nMotorola 88000 –\nMozilla –\nMPEG –\nMS-DOS –\nMultics –\nMultiprocessing –\nMUMPS\n\n.NET –\nNetBSD –\nNetlib –\nNetscape Navigator –\nNeXT –\nNial –\nNybble –\nNinety-Ninety Rule –\nNon-Uniform Memory Access –\nNondeterministic finite automaton\n\nOberon –\nObjective-C –\nobject –\nOCaml –\noccam –\nOmniWeb –\nOne True Brace Style –\nOpenBSD –\nOpenOffice.org –\nOpen source –\nOpen Source Initiative –\nOpera (web browser) –\nOperating system advocacy –\nOperating system –\n\nPA-RISC –\nPage description language –\nPancake sorting –\nParallax Propeller –\nParallel computing –\nParser (language) –\nParsing (technique) –\nPartial function –\nPascal –\nPDP –\nPeer-to-peer network –\nPerl –\nPersonal computer –\nPHP –\nPILOT –\nPL/I –\nPointer –\nPoplog –\nPortable Document Format (PDF) –\nPoser –\nPostScript –\nPowerBook –\nPowerPC –\nPrefix grammar –\nPreprocessor –\nPrimitive recursive function –\nProgramming language –\nProlog –\nPSPACE-complete –\nPulse code modulation (PCM) –\nPushdown automaton –\nPython –\n\nQuarkXPress –\nQuickTime –\nQWERTY –\n\nRam disk –\nRAM (random access memory) –\nRandom access –\nRascal –\nRatfor –\nRCA 1802 –\nRead only memory (ROM) –\nREBOL –\nRecovery-oriented computing –\nRecursive descent parser –\nRecursion (computer science) –\nRecursive set –\nRecursively enumerable language –\nRecursively enumerable set –\nReference (computer science) –\nReferential transparency –\nRegister –\nRegular expression –\nRegular grammar –\nRegular language –\nRPG –\nRetrocomputing –\nREXX –\nRFC –\nRISC –\nRS/6000 –\nRuby –\n\nSafari (web browser) –\nSAIL –\nScript kiddie –\nScripting language –\nSCSI –\nSecond-generation programming language –\nSecure Sockets Layer –\nsed –\nSelf (or SELF) –\nSemaphore (programming) –\nSequential access –\nSETL –\nShareware –\nShell script –\nShellcode –\nSIMD –\nSimula –\nSircam –\nSlide rule –\nSLIP –\nSLR parser –\nSmalltalk –\nServer Message Block –\nSMBus –\nSMIL (computer) –\nSmiley –\nSNOBOL –\nSoftware engineering –\nSONET –\nSpace-cadet keyboard –\nSPARC International –\nSpecialist computer –\nSPITBOL –\nSQL –\nSQL slammer worm –\nSR –\nSSL –\nService-oriented architecture -\nS/SL –\nStale pointer bug –\nStandard ML (or SML) –\nStateless server –\nStructured programming –\nSubject-oriented programming –\nsubnetwork –\nSupercomputer –\nSwap space –\nSymbolic mathematics –\nSymlink –\nSymmetric multiprocessing –\nSyntactic sugar –\nSyQuest Technology –\nSYSKEY –\nSystemboard –\nSystem programming language –\nIBM System R –\n\nTADS –\nTcl –\nteco –\nText editor –\nTeX –\nThird generation language –\nTimeline of computing 1950–1979 –\nTimeline of computing 1980–1989 –\nTimeline of computing 1990–1999 –\nTimeline of computing hardware 2400 BC–1949 –\nTimeline of computing –\nTk –\nTPU –\nTrac –\nTransparency (computing) –\nTrin II –\nTrin VX –\nTuring machine –\nTuring –\n2B1Q –\n\nUAT –\nUnicode –\nUnicon –\nUnix –\nUnix shell –\nUNIX System V –\nUnLambda –\nUSB –\nUnreachable memory –\n\nVar'aq –\nVAX –\nVBScript –\nVector processor –\nVentura Publisher –\nVery-large-scale integration –\nVideo editing –\nVirtual Memory System –\nVirtual memory –\nVisual Basic –\nVisual FoxPro –\nVon Neumann architecture –\n\nWeb browser –\nWestern Design Center –\nWestern Design Center 65C02 –\nWestern Design Center 65816 –\nWhitespace –\nwiki –\nwindow manager –\nWindows 1.0 –\nWindows 2000 –\nWindows 95 –\nWindows Me –\nWindows NT –\nWindows XP –\nWord processor –\nWorld wide web –\nWYSIWYG –\n\nX Window System –\nX86 –\nX-Mouse –\n\nYacc –\nYaST –\nYet Another –\nYorick –\n\nZ notation –\nZ shell –\nZilog Z80 –\nZooming User Interface –\nZX80 –\nZX81 –\nZX Spectrum\n"}
{"id": "13467271", "url": "https://en.wikipedia.org/wiki?curid=13467271", "title": "Inherence", "text": "Inherence\n\nInherence refers to Empedocles' idea that the qualities of matter come from the relative proportions of each of the four elements entering into a thing. The idea was further developed by Plato and Aristotle.\n\nThat Plato accepted (or at least did not reject) Empedocles' claim can be seen in the \"Timaeus\". However, Plato also applied it to cover the presence of form in matter. The form is an active principle. Matter, on the other hand is passive, being a mere possibility that the forms bring to life.\n\nAristotle clearly accepted Empedocles' claim, but he rejected Plato's idea of the forms. According to Aristotle, the accidents of a substance are incorporeal beings which are present in it.\n\nA closely related term is participation. If an attribute \"inheres\" in a subject, then the subject is said to \"participate\" in the attribute. For example, if the attribute \"in Athens\" inheres in Socrates, then Socrates is said to participate in the attribute, \"in Athens.\"\n\n"}
{"id": "4919310", "url": "https://en.wikipedia.org/wiki?curid=4919310", "title": "Interpretation of Schizophrenia", "text": "Interpretation of Schizophrenia\n\nInterpretation of Schizophrenia (first edition, 1955) is a book by Italy-born American psychiatrist Silvano Arieti in which the author sets forth demonstrative evidence of a psychological etiology for schizophrenia.\n\nArieti expanded the book vastly in 1974 () and that edition won the U.S. National Book Award in the Science category.\n\n\"Interpretation of schizophrenia\" is a 756-page book divided in 45 chapters. Arieti begins his book stating that it is difficult to define schizophrenia. He asks if schizophrenia is an illness and answers in the negative, since the disorder is not understood in classic Virchowian criterion of cellular pathology. Though those searching for a biological basis of schizophrenia far outnumber those undertaking psychological approaches, Arieti supports the minority view. He believes schizophrenia is an unrealistic way to represent both the self and the world and praises psychiatrist Adolf Meyer for stressing the importance of psychological factors in the etiology of schizophrenia. Arieti also mentions that Freud felt that in schizophrenia the patient's relationship with people is handicapped (an observation that resembles what presently is called \"autism\").\n\nArieti then describes the psychogenic factors that lead to the disorder. The family environment and psychodynamics in the etiology of psychosis comes under scrutiny. Arieti describes the building of neurotic and psychotic defense mechanisms; the emerging schizoid personality, and fully developed schizophrenia understood as an injury to the inner self. Arieti believes that a state of extreme anxiety originating in early childhood produces vulnerability for the whole life of the individual.\n\nA characteristic of \"Homo sapiens\" is a prolonged childhood with a consequently extended dependency on adults. This, according to Arieti, \"is the basis of the psychodynamics of schizophrenia\", a claim that also appears in later writers on child abuse such as Alice Miller and Colin Ross. Arieti reviews the paper by Frieda Fromm-Reichmann about the \"schizophrenogenic\" mother and reaches the tentative conclusion that only 25 percent of the mothers of people with schizophrenia in his clinical experience fit that image. However, he adds that only in a minority of schizophrenia cases \"the child is able to retain the good maternal image\". Arieti also mentions the work of Theodore Lidz, another trauma model author of schizophrenia. Like Lidz, Arieti emphasizes the weakness of the father of the patient with schizophrenia in the paternal role. In Arieti's own words:\n\nThe roles can be reversed when the domineering spouse is the father. Arieti is convinced that each schizophrenia case is representative of those human situations in which something went extremely wrong. \"If we ignore it, we become deaf to a profound message that the patient may try to convey\". For example, Arieti states about one of his patients that \"his adolescence was a crescendo of frustration, anxiety and injury to self-esteem\". Arieti also mentions a catatonic patient who, after introjecting the mother's engulfing behavior, believed that by moving he could produce havoc. The patient's feelings, according to Arieti, became reminiscent of cosmic powers that may cause the destruction of the universe, so the patient chose immobility. For Arieti, the selectivity of certain motor actions is proof that catatonia is not a biological disease or illness, but rather a disorder of the will.\n\nIn Part three of \"Interpretation of schizophrenia\" Arieti describes how in spite of its efforts to stay in reality, the patient's defenses finally succumb. When the patient \"cannot change the unbearable situation of himself any longer, he has to change reality\". Arieti examines the inner world of the person with schizophrenia.\n\nWhen a patient states he is Jesus he is compensating a feeling of extreme humiliation at home. The paranoid schizophrenic, Arieti explains, resorts to \"teleologic causality\" or animism to understand the world. He writes that whatever occurs to the patient is interpreted as willed by the parental alters of the patient. In deterministic or teleologic causality, if Nature's happenings were not willed they simply would not occur. In paranoid projection the schizophrenic takes out from him/herself a disagreeable part of the self onto the world. In \"Interpretation of schizophrenia\" Arieti illustrates all of the above theoretical constructions with concrete cases of his clinical experience as a psychiatrist.\n\nArieti maintains that in every case of schizophrenia that he studied serious family disturbance was found. When the patient idealizes the parent the idealized image of the parent is maintained in the patient's mind at the expense of an unbearable self-image. He speculates that psychosis starts only when the malevolent image of the parent is transformed \"into a distressing other\". The parent or parents alters enter the mind accusing the patient of \"bad child\" or other equivalent accusations in voices that the adult patient hears.\n\nSince the 1980s, and into the beginnings of 21st century, biological psychiatric models of schizophrenia almost completely took over the psychiatric profession. Current research into the disorder focuses on neurobiology. Psychological approaches to schizophrenia like Arieti's are a rarity in the profession, although this structurally created circumstance neglects the obvious connection between psychological phenomena and neurotransmitter levels, which can be changed through certain practices, like Yoga, meditation, hyperventilation, sensory deprivation, sleep deprivation, among others.\n\n\n"}
{"id": "38596702", "url": "https://en.wikipedia.org/wiki?curid=38596702", "title": "List of discoveries influenced by chance circumstances", "text": "List of discoveries influenced by chance circumstances\n\nBelow are discoveries in science that involve chance circumstances in a particularly salient way. This page should not list all chance involved in all discoveries (i.e. it should focus on discoveries reported for their notable circumstances).\n\nRoyston Roberts says that various discoveries required a degree of genius, but also some lucky element for that genius to act on. Richard Gaughan writes that accidental discoveries result from the convergence of preparation, opportunity, and desire.\n\nMajor everyday discoveries that were helped by luck in some way include products like vulcanized rubber, teflon, nylon, penicillin, cyanoacrylate (Super Glue), the implantable pacemaker, the microwave oven, Scotchgard, Saran wrap, Silly Putty, Slinky, safety glass, propeller, snowmaking, stainless steel, Perkin's mauve, and popsicles. Most artificial sweeteners have been discovered when accidentally tasted, including aspartame and saccharin.\n\nIdeas include the theory of the big bang, tissue culture, radio astronomy, and the discovery of DNA.\n\nSuch archeological discoveries as the Rosetta stone, the Dead Sea Scrolls and the ruins of Pompeii also emerged partly out of serendipity.\n\nMany relevant and well known scientific theories were developed by chance at some degree along history. According to a legend, Archimedes realized his principle on hydrostatics when he entered in a bath full of water, which overflows (he then shouted out his famous \"Eureka!\"). And the unexpected, negative results of the Michelson-Morley experiment in their search of the luminiferous aether ultimately led to the Theory of Special Relativity by Albert Einstein.\n\nThe optical illusion called the \"flashed face distortion effect\" suggests a new area of research in the neurology of face perception.\n\nIn his book, Roberts recounts Sir Isaac Newton's discovery of gravity (using Newton's own descriptions and notes). Newton was sitting in his yard when he noticed an apple fall from a tree. The apple fell straight down, perpendicular to the ground, and Newton found himself wondering why the apple never falls upward or off to a side. Newton soon realized that it was a property of all matter to have an attractive force, including the apple, and even the moon – which moves as one would expect if it was passing the earth but nevertheless being attracted. It was another 20 years before Newton published his detailed theory of gravity, but he later visited the tree that helped him provoke the idea. Gaughan elaborates that Newton only had the opportunity to reflect on his orchard because of other chance circumstances: Newton was home because his university was shut down due to an outbreak of plague.\n\nAccording to Roberts, the common story that Alfred Nobel's discovery of dynamite was an accident may not be true. On the other hand, Roberts says, Nobel did make a discovery with the help of luck soon after that. Nobel cut his finger on a piece of glass one day at work and subsequently applied collodion in order to form a protective layer over the wound (similar in principle to liquid bandage). Nobel was kept up at night by the pain in his finger, so he started to think about a problem he was having back at work: Nobel was trying to create a powerful explosive using nitrocellulose and nitroglycerine, but the two would not combine. Roberts reports that Nobel then realized that collodion (which he was using to dress his wound) could allow the two substances to combine, which led to the invention of blasting gelatin (as powerful as dynamite but much safer to handle).\n\nThe French scientist Louis Pasteur is responsible for various discoveries, some of which involved serendipity in some way. This seems to be the case with both his discovery that chemically identical molecules can have chirality (the way a right handed baseball glove will not work with the left hand), as well as his discovery of the chicken cholera vaccine.\n\nRoberts writes \"Pasteur was puzzled: the salts of tartaric acid and racemic acid were said to be identical in chemical composition and crystalline shape, but they had different effects on polarized light.\" Pasteur later prepared a solution of only racemic acid and found that it itself contained salt crystals with chirality and which affected light differently. This was somewhat lucky because the type of salt crystals that Pasteur was studying (sodium ammonium salt of racemic acid) is one of few salts that would be visibly different in Pasteur's time. Moreover, the salts only differentiate if the solution reaches a temperature below ; Pasteur did not know about this temperature requirement, but he did happen to store the solution on a window sill over night and the cold Paris air activated it.\n\nPasteur and his assistants had succeeded in isolating a microbe from chickens sick or dead from cholera. Chickens injected with the isolated microbe invariably died — a key element in Pasteur's reasoning that the microbe was responsible for the disease, rather than a result of the disease, as many thought. Pasteur was searching for a method of preventing the disease, but no matter what he did to the \"broth\" of microbes or to the chickens, all injected chickens died. Gaughan writes \"Finally Pasteur had had enough, he needed a vacation. He told [his assistant] to take care of injecting more chickens with the next batch of bacteria.\" His assistant neglected the task, electing to go on vacation as well. When the men returned and injected chickens with the batch of bacteria that had sat around for a few weeks, none died, indicating to Pasteur that the batch of bacteria had been ruined. But when those same chickens were injected with a new batch of bacteria, none of them died, while chickens that hadn't previously been injected with the \"spoiled\" batch all died. Pasteur reasoned that the \"attenuated\" microbes in the spoiled batch \"'used up' something within the body; something that wasn't there for the fully functional bacteria to eat.\" His explanation was wrong, but his chance creation of attenuated bacteria resulted in the first intentionally created vaccine.\n"}
{"id": "640228", "url": "https://en.wikipedia.org/wiki?curid=640228", "title": "List of mnemonics", "text": "List of mnemonics\n\nThis article contains lists of mnemonics used to remember various objects, lists etc.\n\n\n\n\n\n\n\n\n\nThe articulation of the quadratic equation can be sung to the tune of various songs as a mnemonic device.\n\nFor helping students in remembering the rules in adding and multiplying two signed numbers, Balbuena and Buayan (2015) made the letter strategies LAUS (like signs, add; unlike signs, subtract) and LPUN (like signs, positive; unlike signs, negative), respectively.\n<br>\nOrder of Operations\n<br>\nPEMDAS\n<br>\nPlease - Parenthesis\n<br>\nExcuse - Exponents\n<br>\nMy - Multiplication\n<br>\nDear - Division\n<br>\nAunt - Addition\n<br>\nSally - Subtraction\n<br>\n\nPEMDAS\n<br>\nPineapples - Parenthesis\n<br>\nEat - Exponents\n<br>\nMangoes - Multiplication\n<br>\nDuring - Division\n<br>\nAutumn - Addition\n<br>\nSeason - Subtraction\n\n'A OF A OF A'\n\nThenar (lateral to medial-palmar surface):\nHypothenar (lateral to medial-palmar surface):\n\n\n\n\nThus we get the names of the strings from 6th string to the 1st string in that order.\n\nConversely, a mnemonic listing the strings in the reverse order is:\n\nAs for guitar tuning, there is also a mnemonic for ukuleles. \n\n\n\n\n\n\n\n\n\n\n\nIn most words like friend, field, piece, pierce, mischief, thief, tier, it is \"\"i\" which comes before \"e\". But on some words with c just before the pair of e and i, like receive, perceive, \"e\" comes before \"i\"\". This can be remembered by the following mnemonic,\n\nBut this is not always obeyed as in case of weird and weigh, weight, height, neighbor etc. and can be remembered by extending that mnemonic as given below\n\nAnother variant, which avoids confusion when the two letters represent different sounds instead of a single sound, as in atheist or being, runs\n\nMost frequently \"u\" follows \"q\". e.g.: Que, queen, question, quack, quark, quartz, quarry, quit, Pique, torque, macaque, exchequer. Hence the mnemonic:\n\n\nAdvice, Practice, Licence etc. (those with c) are nouns and Advise, Practise, License etc. are verbs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "31149053", "url": "https://en.wikipedia.org/wiki?curid=31149053", "title": "Literature-based discovery", "text": "Literature-based discovery\n\nLiterature-based discovery refers to the use of papers and other academic publications (the \"literature\") to find new relationships between existing knowledge (the \"discovery\"). The technique was pioneered by Don R. Swanson in the 1980s and has since seen widespread use. \n\nLiterature-based discovery does not generate new knowledge through laboratory experiments, as is customary for empirical sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and \"neglected\". It is marked by empiricism and rationalism in concert or consilience.\n\n\"Swanson linking\" is a term proposed in 2003 that refers to connecting two pieces of knowledge previously thought to be unrelated. For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called \"disjoint data\"), the relationship between illness A and drug C may be unknown. \"Swanson linking\" aims to find these relationships and report them.\n\n\n\n\n"}
{"id": "21468426", "url": "https://en.wikipedia.org/wiki?curid=21468426", "title": "Mapping controversies", "text": "Mapping controversies\n\nMapping controversies (MC) is an academic course taught in science studies, stemming from the writings of the French sociologist and philosopher Bruno Latour. MC focuses exclusively on the controversies surrounding scientific knowledge rather than the established scientific facts or outcomes. Thus, it helps sociologists, anthropologists and other social scientists get insights not into scientific knowledge \"per se\", but rather into \"the process of gaining knowledge\". Thus, MC sheds light on those intermediate stages corresponding to the actual research process and pinpoints the connections between scientific work and other types of activities.\n\nThe term \"mapping controversies\" was first suggested in relation to analysis of scientific and technological controversies, and then lately re-affirmed as a widely applicable methodological approach going beyond the boundaries of Science Studies. It is usually used for the methodology that identifies and tracks down the polemics or debate surrounding a scientific fact, and utilises various visualisation tools to present the problem in its complexity.\n\nRecently Latour initiated the project \"Mapping Controversies on Science for Politics (MACOSPOL)\". The showcase website is mappingcontroversies.net \n\nIn 2008-2009 several universities in Europe and USA started teaching \"Mapping Controversies\" courses for students in political sciences, engineering, and architecture.\n\nAn earlier attempt to stage controversies in museum settings took place at the Gallery of Research in Vienna in 2005.\n"}
{"id": "12108220", "url": "https://en.wikipedia.org/wiki?curid=12108220", "title": "On Nature (Anaximander)", "text": "On Nature (Anaximander)\n\nOn Nature was a philosophical poem which details Anaximander's theories about the evolution of the Earth, plants, animals and humankind. Anaximander described his theory that humans and other animals descended from fish once the world's oceans began to dry up. Also he described a theory of abiogenesis in his book in the way that he believed that the first life forms formed from mist. We know little about his book because it has been lost or destroyed, however it still remains important today because it describes one of the world's earliest theories of evolution.\n\n\n"}
{"id": "55752601", "url": "https://en.wikipedia.org/wiki?curid=55752601", "title": "PEROSH", "text": "PEROSH\n\nThe consortium was established in Rome on 7 November 2003 to foster research on important fields in OSH.\n\nIn Dublin on 7 November 2008 the articles of association were revised and broadened to facilitate the collaboration within the institutes. The most important regulations imply that each partner institute bears the costs of its activities and that the partnership employs its own Manager International Affairs. He has been representing PEROSH since 2009.\n\nThe partnership was renewed and extended in Paris in May 2013.\n\n\nThe PEROSH partners aim to improve the quality and effectiveness of European research and the EU-wide dissemination of results in the field of occupational safety and health. Therefore, PEROSH partners are committed to maintain a proactive dialogue with the EU and other national and international OSH-partners.\n\nJoint interdisciplinary research projects within PEROSH aim to create a close cooperation within the network. This saves resources, creates synergies and fosters the mutual exchange of scientific personnel.\n\nAt the same time, the members of PEROSH intend to accomplish EU-research projects, also in smaller consortiums, and to pool their interest in OSH research towards European institutions.\n\nUsually there are up to ten joint research projects with more than 80 researchers involved. These research projects address prevailing OSH topics, e. g.:\n\n\nThe list of ongoing and completed projects can be found on the website of PEROSH.\n\nPEROSH identifies upcoming trends and risks in the working world to precociously adjust its research activities.\n\nExchanges of experts between the member institutes and technical seminars promote the transfer of knowledge and harmonisation.\n\nPEROSH also carries on research in the field of standardisation, especially the determination of workplace-related safety factors for protective equipment.\n\nIn June 2018 the general directors of the member institutes met in Bonn (Germany) on the occasion of the 15th anniversary of PEROSH. They signed the new PEROSH agreement for the period 2018-2022 and elected two new chairpersons as of January 2019. \n\nTwo boards structure the work of PEROSH: the Steering Committee (SC) and the Scientific Steering Group (SSG).\n\nThe Steering Committee consists of the general directors of the member institutes and is responsible for the strategic management of PEROSH. The SC discusses the inclusion of new members, the appointment of the chairperson and the vice-chairperson of the SC, the establishment and mandate of the SSG and working groups. They agree on the annual budget and approve the annual account of revenue and expenditures.\n\nThe Scientific Steering Group consists of the research/scientific directors of the member institutes of PEROSH. The SSG meets twice a year to coordinate ongoing research projects and to determine new research topics and collaboration options.\n\nThe Manager International Affairs represents PEROSH towards the bodies of the European Union (EU). He promotes European activities in OSH, prepares joint statements to EU queries, reacts to EU requests for tender, and organises workshops and conferences. He is responsible for the homepage and publishes a newsletter on relevant OSH aspects three times a year.\n\n\n"}
{"id": "33938545", "url": "https://en.wikipedia.org/wiki?curid=33938545", "title": "Policy advocacy", "text": "Policy advocacy\n\nPolicy advocacy is defined as active, covert, or inadvertent support of a particular policy or class of policies. Whether it is proper for scientists and other technical experts to act as advocates for their personal policy preferences is contentious. In the scientific community, much of the controversy around policy advocacy involves precisely defining the proper role of science and scientists in the political process. Some scientists choose to act as policy advocates, while others regard such a dichotomous role as inappropriate.\n\nProviding technical and scientific information to inform policy deliberations in an objective and relevant way is recognized as a difficult problem in many scientific and technical professions. The challenge and conflicts have been studied for those working as stock analysts in brokerage firms, for medical experts testifying in malpractice trials, for funding officers at international development agencies, and for intelligence analysts within governmental national security agencies. The job of providing accurate, relevant, and policy neutral information is especially challenging if highly controversial policy issues (such as climate change) that have a significant scientific component. The use of normative science by scientists is a common method used to subtly advocate for preferred policy choices.\n\n\n"}
{"id": "58910447", "url": "https://en.wikipedia.org/wiki?curid=58910447", "title": "Research site", "text": "Research site\n\nA research site is a place where people conduct research. Common research sites include universities, hospitals, research institutes, and field research locations.\n\nIn clinical research a research site conducts all or part of a clinical trial. For clinical trials which recruit research participants in multiple locations, often the research will have a headquarters then multiple regional research sites to conduct the research in that region. In a network of research sites where all are recruiting study participants, sites with low recruitment benefit from coaching from sites with high recruitment.\n\nCharacteristics of good clinical research sites include setting good timelines, early participant recruitment, and having a management plan for efficiency.\n\nResearchers in nursing have reported challenges accessing the facilities designated for conventional medical research.\n\nThe design of a research site should have a means of detecting fraud.\n\nResearchers who do not have a cultural tie to a research population may have difficulty doing ethnographic research with that community.\n\n"}
{"id": "18490995", "url": "https://en.wikipedia.org/wiki?curid=18490995", "title": "STEPS Centre", "text": "STEPS Centre\n\nThe STEPS Centre (Social, Technological and Environmental Pathways to Sustainability) is an interdisciplinary global research and policy engagement hub at the University of Sussex. It is funded by the Economic and Social Research Council. The Centre brings together development studies with science and technology studies and was launched at Portcullis House in London on 25 June 2007 \n\nBy acknowledging the interactions between social, technological and environmental factors in diverse local settings the STEPS Centre seeks to help create more sustainable and socially just conditions for poorer people. Based at the Institute of Development Studies (IDS) and the Science Policy Research Unit (SPRU) at the University of Sussex in the UK, the Centre works with partners in Africa, Asia and Latin America.\n\nProfessor Ian Scoones is the director of the STEPS Centre and Professor Andy Stirling its co-director. Professor Melissa Leach stepped down as STEPS Director in 2014 to become Director of the Institute of Development Studies.\n\nThe STEPS Centre Advisory Board is chaired by Mike Hulme (King's College London) with members including Brian Wynne (CESAGEN, University of Lancaster), Carl Folke (Stockholm Resilience Centre), Dipak Gyawali (Nepal Academy of Science and Technology), Fred Pearce (science writer), Sue Hartley (Director, York Environmental Sustainability Institute), Suman Sahai (Convenor, Gene Campaign), and Thomas Lingard (Global External Affairs Director, Unilever).\n\nThe STEPS Centre brings together social scientists and natural scientists to work together to try and achieve a breakthrough in thinking and action for development. The Centre's unique ‘pathways’ approach interweaves social, technological and environmental conditions with dynamic change across three domains - food and agriculture; health and disease; water and sanitation - and three themes - dynamics; governance; designs.\n\nThrough linking across domains and themes in its projects, the STEPS Centre connects new theory with practical approaches in a bid to help provide sustainable opportunities for poor and marginalised people.\n\nThe STEPS Centre's pathways approach aims to understand the complex, non-linear interactions between social, technological and environmental systems. Some pathways may threaten poor peoples’ livelihoods and health while others create opportunities for sustainability.\n\nThe Centre aims to link social scientists, natural scientists and users to develop new tools and methods linking theory with practical solutions. It also offers teaching, in conjunction with IDS and SPRU, to help train a new generation of researchers on MA and PhD courses.\n\nA paper published in 2007 entitled Pathways to Sustainability: an Overview of the STEPS Centre Approach outlined the STEPS Centre approach to understanding dynamic systems and their governance. The paper laid out the ingredients of the STEPS Centre's work, including linking diverse social and natural science perspectives, connecting theory, policy and practice and an engaged, interactive approach to communications. Promoting pathways to Sustainability that meet the perspectives and priorities of poor and marginalised groups is at the heart of the pathways approach.\n\nAmong the STEPS Centre's projects are:\n\n•\"Innovation, Sustainability, Development: A New Manifesto\" (40 years on from the Sussex Manifesto)\n\n• \"Crop, disease and innovation in Kenya - Maize and farming system dynamics in areas affected by climate change\"\n\n• \"Urbanisation in Asia - urbanisation and sustainability on the expanding peri-urban fringe of Delhi, India\"\n\n• \"Rethinking regulation - assumptions and realities of drug and seed regulation in China and Argentina\"\n\n• \"Risk, uncertainty and technology - framing and responses to risks and uncertainties in areas of rapid scientific and technological advance\"\n\n• \"Epidemics, livelihoods and politics\" - HIV-AIDS, SARS, ‘avian flu, BSE - procedures for addressing epidemics that support rather than compromise poor people\n\nBy Leach. M, Scoones, I. and Stirling, A. (2010) \n\nBy Leach. M., Scoones, I. and Stirling, A. (2007) \n\nBy Scoones, I., Leach, M., Smith, A., Stagl, S., Stirling, A. and Thompson, J. (2007) \"\n\nBy Leach, M., Bloom, G., Ely, A., Nightingale, P., Scoones, I., Shah, E. and Smith, A. (2007) – \n\nBy Stirling, A., Leach, M., Mehta, L., Scoones, I., Smith, A., Stagl, S. and Thompson, J. (2007) \n\nBy Thompson, J., Millstone, E., Scoones, I., Ely, A., Marshall, F., Shah, E.and Stagl, S. (2007) \n\nBy Bloom, G., Edström, J., Leach, M., Lucas, H., MacGregor, H., Standing, H. and Waldman, L. (2007) \n\nBy Mehta, L., Marshall, F., Movik, S., Stirling, A., Shah, E., Smith, A. and Thompson, J. (2007) \nBy Krätli, S. (2008) The ISBN printed in the document (978 1 85864 699 5) is invalid, causing a checksum error.\n\nBy Melissa Leach, Ian Scoones (2006) Demos pamphlet \n\n"}
{"id": "1501245", "url": "https://en.wikipedia.org/wiki?curid=1501245", "title": "Science Commons", "text": "Science Commons\n\nScience Commons (SC) was a Creative Commons project for designing strategies and tools for faster, more efficient web-enabled scientific research. The organization's goals were to identify unnecessary barriers to research, craft policy guidelines and legal agreements to lower those barriers, and develop technology to make research data and materials easier to find and use. Its overarching goal was to speed the translation of data into discovery and thereby the value of research.\n\nScience Commons was located at the MIT Computer Science and Artificial Intelligence Laboratory in the Ray and Maria Stata Center at the Massachusetts Institute of Technology in Cambridge, Massachusetts.\n\nCreative Commons launched the Science Commons project in early 2005. The project sought to achieve for science what Creative Commons had achieved for the world of culture, art and educational material: to ease unnecessary legal and technical barriers to sharing, to promote innovation, and to provide easy, high quality tools that let individuals and organizations specify the terms under which they wished to share their material.\n\nIn 2009, Creative Commons terminated the Science Commons project.\n\nThe Biological Materials Transfer Project, a Material transfer agreement (MTA), developed and deployed standard, modular contracts to lower the costs of transferring biological materials such as DNA, cell lines, model animals and more. The MTA project covered transfer between non-profit institutions, as well as offering transaction solutions to transfers between non-profit entities and for-profit institutions. It integrated existing standard agreements and new Science Commons contracts into a Web-deployed suite, with the goal of developing a transaction system along the lines of Amazon or eBay by using the licensing as a discovery mechanism for materials. \n\nThis metadata driven approach is based on the success of the Creative Commons licensing integration into search engines, further allowing for and facilitating the integration of materials licensing into the research literature itself and databases. The hope being that scientists would eventually be only one click away from accessing and/or ordering the materials referenced in the scholarly literature as they perform their research. Unfortunately, the MTA project's tools were not adopted by more than a very small percentage of the scientific community while Science Commons was active and, for all practical purposes, died out when the Science Commons project folded.\n\nScience Commons’ Neurocommons project set out to create an Open Source knowledge management platform for biological research. The platform combined open access materials (making up the knowledgebase) and open source software (in the form of an analytic platform). The software was still under development when the project ended.\n\nThe Scholar’s Copyright was developed with Scholarly Publishing and Academic Resources Coalition designed to lower the barriers to Open Access (OA) by reducing transaction costs and eliminating contract proliferation by offering tools and resources catering to both methods of achieving Open Access. The Scholar's Copyright Addendum is still in use by SPARC\n\nThe Science Commons Open Access Data Protocol was a method for ensuring that scientific databases can be legally integrated with one another. The protocol was not a license or legal tool, but instead a methodology and best practices document for creating such legal tools in the future, and marking data in the public domain for machine-assisted discovery.\n\n"}
{"id": "1640288", "url": "https://en.wikipedia.org/wiki?curid=1640288", "title": "Scientific control", "text": "Scientific control\n\nA scientific control is an experiment or observation designed to minimize the effects of variables other than the independent variable. This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method.\n\nIn controlled experiments, the same experiment is done in at least two parallel experiments that differ in only one way, with one experiment being the \"control arm\" and the other being the \"experimental arm\". \n\nControls eliminate alternate explanations of experimental results, especially experimental errors and experimenter bias. Many controls are specific to the type of experiment being performed, as in the molecular markers used in SDS-PAGE experiments, and may simply have the purpose of ensuring that the equipment is working properly. The selection and use of proper controls to ensure that experimental results are valid (for example, absence of confounding variables) can be very difficult. Control measurements may also be used for other purposes: for example, a measurement of a microphone's background noise in the absence of a signal allows the noise to be subtracted from later measurements of the signal, thus producing a processed signal of higher quality.\n\nFor example, if a researcher feeds an experimental artificial sweetener to sixty laboratory rats and observes that ten of them subsequently become sick, the underlying cause could be the sweetener itself or something unrelated. Other variables, which may not be readily obvious, may interfere with the experimental design. For instance, the artificial sweetener might be mixed with a dilutant and it might be the dilutant which causes the effect. To control for the effect of the dilutant, the same test is run twice; once with the artificial sweetener in the dilutant, and another done exactly the same way, but using the dilutant alone. Now the experiment is controlled for the dilutant and the experimenter can distinguish between sweetener, dilutant and non-treatment. Controls are most often necessary where a confounding factor cannot easily be separated from the primary treatments. For example, it may be necessary to use a tractor to spread fertilizer where there is no other practicable way to spread fertilizer. The simplest solution is to have a treatment where a tractor is driven over plots without spreading fertilizer and in that way the effects of tractor traffic are controlled.\n\nThe simplest types of control are negative and positive controls, and both are found in many different types of experiments. These two controls, when both are successful, are usually sufficient to eliminate most potential confounding variables: it means that the experiment produces a negative result when a negative result is expected, and a positive result when a positive result is expected.\n\nWhere there are only two possible outcomes, e.g. positive or negative, if the treatment group and the negative control both produce a negative result, it can be inferred that the treatment had no effect. If the treatment group and the negative control both produce a positive result, it can be inferred that a confounding variable is involved in the phenomenon under study, and the positive results are not solely due to the treatment.\n\nIn other examples, outcomes might be measured as lengths, times, percentages, and so forth. In the drug testing example, we could measure the percentage of patients cured. In this case, the treatment is inferred to have no effect when the treatment group and the negative control produce the same results. Some improvement is expected in the placebo group due to the placebo effect, and this result sets the baseline which the treatment must improve upon. Even if the treatment group shows improvement, it needs to be compared to the placebo group. If the groups show the same effect, then the treatment was not responsible for the improvement (because the same number of patients were cured in the absence of the treatment). The treatment is only effective if the treatment group shows more improvement than the placebo group.\n\nPositive controls are often used to assess test validity. For example, to assess a new test's ability to detect a disease (its sensitivity), then we can compare it against a different test that is already known to work. The well-established test is the positive control, since we already know that the answer to the question (whether the test works) is yes.\n\nSimilarly, in an enzyme assay to measure the amount of an enzyme in a set of extracts, a positive control would be an assay containing a known quantity of the purified enzyme (while a negative control would contain no enzyme). The positive control should give a large amount of enzyme activity, while the negative control should give very low to no activity.\n\nIf the positive control does not produce the expected result, there may be something wrong with the experimental procedure, and the experiment is repeated. For difficult or complicated experiments, the result from the positive control can also help in comparison to previous experimental results. For example, if the well-established disease test was determined to have the same effectiveness as found by previous experimenters, this indicates that the experiment is being performed in the same way that the previous experimenters did.\n\nWhen possible, multiple positive controls may be used — if there is more than one disease test that is known to be effective, more than one might be tested. Multiple positive controls also allow finer comparisons of the results (calibration, or standardization) if the expected results from the positive controls have different sizes. For example, in the enzyme assay discussed above, a standard curve may be produced by making many different samples with different quantities of the enzyme.\n\nIn randomization, the groups that receive different experimental treatments are determined randomly. While this does not ensure that there are no differences between the groups, it ensures that the differences are distributed equally, thus correcting for systematic errors.\n\nFor example, in experiments where crop yield is affected (e.g. soil fertility), the experiment can be controlled by assigning the treatments to randomly selected plots of land. This mitigates the effect of variations in soil composition on the yield.\n\nIn blind experiments, at least some information is withheld from participants in the experiments (but not the experimenter). For example, to evaluate the success of a medical treatment, an outside expert might be asked to examine blood samples from each of the patients without knowing which patients received the treatment and which did not. If the expert's conclusions as to which samples represent the best outcome correlates with the patients who received the treatment, this allows the experimenter to have much higher confidence that the treatment is effective.\n\nThe blinding eliminates effects such as confirmation bias and wishful thinking that might occur if the samples were evaluated by someone who knew which samples were in which group.\n\nIn double-blind experiments, at least some participants and some experimenters do not possess full information while the experiment is being carried out. Double-blind experiments are most often used in clinical trials of medical treatments, to verify that the supposed effects of the treatment are produced only by the treatment itself. Trials are typically randomized and double-blinded, with two (statistically) identical groups of patients being compared. The treatment group receives the treatment, and the control group receives a placebo. The placebo is the \"first\" blind, and controls for the patient expectations that come with taking a pill, which can have an effect on patient outcomes. The \"second\" blind, of the experimenters, controls for the effects on patient expectations due to unintentional differences in the experimenters' behavior. Since the experimenters do not know which patients are in which group, they cannot unconsciously influence the patients. After the experiment is over, they then \"unblind\" themselves and analyse the results.\n\nIn clinical trials involving a surgical procedure, a sham operated group is used to ensure that the data reflect the effects of the experiment itself, and are not a consequence of the surgery. In this case, double blinding is achieved by ensuring that the patient does not know whether their surgery was real or sham, and that the experimenters who evaluate patient outcomes are different from the surgeons and do not know which patients are in which group.\n\n"}
{"id": "10466072", "url": "https://en.wikipedia.org/wiki?curid=10466072", "title": "Scientific literacy", "text": "Scientific literacy\n\nScientific literacy or Science literacy encompasses written, numerical, and digital literacy as they pertain to understanding science, its methodology, observations, and theories.\n\nThe OECD PISA Framework (2015) defines scientific literacy as \"the ability to engage with science-related issues, and with the ideas of science, as a reflective citizen.\" A scientifically literate person, therefore, is willing to engage in reasoned discourse about science \nand technology which requires the competencies to:\n\nAccording to the United States National Center for Education Statistics, \"scientific literacy is the knowledge and understanding of scientific concepts and processes required for personal decision making, participation in civic and cultural affairs, and economic productivity\". A scientifically literate person is defined as one who has the capacity to:\n\nScientific literacy may also be defined in language similar to the definitions of ocean literacy, Earth science literacy and Climate Literacy. Thus a scientifically literate person can:\n\nFinally, scientific literacy may involve particular attitudes toward learning and using science. A scientifically-literate citizen is capable of researching matters of fact for him or herself.\n\nReforms in science education in the United States have often been driven by strategic challenges such as the launch of the Sputnik satellite in 1957 and the Japanese economic boom in the 1980s. By contrast, scientific literacy is now taken to mean that everyone should have a working knowledge of science and its role in society. Science literacy is seen as a right of every person and a requirement for responsible members of society, one that helps average people to make better decisions and enrich their lives. The shift occurred in the late 1980s and early 1990s, with the publication of \"Science for All Americans\" and \"Benchmarks for Science Literacy\".\n\nInitial definitions of science literacy included elaborations of the actual content that people should understand, and this content often followed somewhat traditional lines (biology, chemistry, physics). Earth science was somewhat narrowly defined as expanded geological processes. In the decade after those initial documents, ocean scientists and educators revised the notion of science literacy to include more contemporary, systems-oriented views of the natural world, leading to scientific literacy programs for the ocean, climate, earth science, and so on. This shift has ensured that educators' views of science literacy stay in sync with the directions and advances of real science in the real world.\n\nThe interdependence of humans and our natural environment is at the heart of scientific literacy in the Earth systems. As defined by nationwide consensus among scientists and educators, this literacy has two key parts. First, a literate person is defined, in language that echoes the above definition of scientific literacy. Second, a set of concepts are listed, organized into six to nine big ideas or essential principles. This defining process was undertaken first for ocean literacy, then for the Great Lakes, estuaries, the atmosphere, and climate.\nEarth science literacy is one of the types of literacy defined for Earth systems; the qualities of an Earth science literate person are representative of the qualities for all the Earth system literacy definitions.\n\nAccording to the Earth Science Literacy Initiative, an Earth-science-literate person:\n\nAll types of literacy in Earth systems have a definition like the above. Ocean literacy is further defined as \"understanding our impact on the ocean and the ocean's impact on us\".\nSimilarly, the climate literacy website includes a guiding principle for decision making; \"humans can take action to reduce climate change and its impacts\". Each type of Earth systems literacy then defines the concepts students should understand upon graduation from high school. Current educational efforts in Earth systems literacy tend to focus more on the scientific concepts than on the decision-making aspect of literacy, but environmental action remains as a stated goal.\n\nThe theme of science in a socially-relevant context appears in many discussions of scientific literacy. Ideas that turn up in the life sciences include an allusion to ecological literacy, the \"well-being of earth\". Robin Wright, a writer for \"Cell Biology Education\", laments \"will [undergraduates'] misunderstandings or lack of knowledge about science imperil our democratic way of life and national security?\" A discussion of physics literacy includes energy conservation, ozone depletion and global warming.\nThe mission statement of the Chemistry Literacy Project includes environmental and social justice.\nTechnological literacy is defined in a three-dimensional coordinate space; on the knowledge axis, it is noted that technology can be risky, and that it \"reflects the values and culture of society\".\nEnergy Literacy boasts several websites, including one associated with climate literacy\nand two advocacy organizations.\n\nAttitudes about science can have a significant effect on scientific literacy. In education theory, understanding of content lies in the cognitive domain, while attitudes lie in the affective domain. Thus, negative attitudes, such as fear of science, can act as an affective filter and an impediment to comprehension and future learning goals. Studies of college students' attitudes about learning physics suggest that these attitudes may be divided into categories of real world connections, personal connections, conceptual connections, student effort and problem-solving.\n\nThe decision-making aspect of science literacy suggests further attitudes about the state of the world, one's responsibility for its well-being and one's sense of empowerment to make a difference. These attitudes may be important measures of science literacy, as described in the case of ocean literacy.\n\nProponents of scientific literacy tend to focus on what is learned by the time a student graduates from high school. Science literacy has always been an important element of the standards movement in education. All science literacy documents have been drafted with the explicit intent of influencing educational standards, as a means to drive curriculum, teaching, assessment, and ultimately, learning nationwide.\n\nPrograms to promote scientific literacy among students abound, including several programs sponsored by technology companies, as well as quiz bowls and science fairs. A partial list of such programs includes the Global Challenge Award, the National Ocean Sciences Bowl and Action Bioscience.\n\nSome organizations have attempted to compare the scientific literacy of adults in different countries. The Organisation for Economic Co-operation and Development found that scientific literacy in the United States is not measurably different from the OECD average. Science News reports \"The new U.S. rate, based on questionnaires administered in 2008, is seven percentage points behind Sweden, the only European nation to exceed the Americans. The U.S. figure is slightly higher than that for Denmark, Finland, Norway and the Netherlands. And it’s double the 2005 rate in the United Kingdom (and the collective rate for the European Union).\" \n\nUniversity educators are attempting to develop reliable instruments to measure scientific literacy, and the use of concept inventories is increasing in the fields of physics, astronomy, chemistry, biology\nand earth science.\n\n\n\n"}
{"id": "1805271", "url": "https://en.wikipedia.org/wiki?curid=1805271", "title": "Social construction of technology", "text": "Social construction of technology\n\nSocial construction of technology (also referred to as SCOT) is a theory within the field of Science and Technology Studies. Advocates of SCOT—that is, social constructivists—argue that technology does not determine human action, but that rather, human action shapes technology. They also argue that the ways a technology is used cannot be understood without understanding how that technology is embedded in its social context. SCOT is a response to technological determinism and is sometimes known as technological constructivism.\n\nSCOT draws on work done in the constructivist school of the sociology of scientific knowledge, and its subtopics include actor-network theory (a branch of the sociology of science and technology) and historical analysis of sociotechnical systems, such as the work of historian Thomas P. Hughes. Its empirical methods are an adaptation of the Empirical Programme of Relativism (EPOR), which outlines a method of analysis to demonstrate the ways in which scientific findings are socially constructed (see strong program). Leading adherents of SCOT include Wiebe Bijker and Trevor Pinch.\n\nSCOT holds that those who seek to understand the reasons for acceptance or rejection of a technology should look to the social world. It is not enough, according to SCOT, to explain a technology's success by saying that it is \"the best\"—researchers must look at how the criteria of being \"the best\" is defined and what groups and stakeholders participate in defining it. In particular, they must ask who defines the technical criteria success is measured by, why technical criteria are defined this way, and who is included or excluded. Pinch and Bijker argue that technological determinism is a myth that results when one looks backwards and believes that the path taken to the present was the only possible path.\n\nSCOT is not only a theory, but also a methodology: it formalizes the steps and principles to follow when one wants to analyze the causes of technological failures or successes.\n\nAt the point of its conception, the SCOT approach was partly motivated by the ideas of the strong programme in the sociology of science (Bloor 1973). In their seminal article, Pinch and Bijker refer to the \"Principle of Symmetry\" as the most influential tenet of the Sociology of Science, which should be applied in historical and sociological investigations of technology as well. It is strongly connected to Bloor's theory of social causation.\n\nThe \"Principle of Symmetry\" holds that in explaining the origins of scientific beliefs, that is, assessing the success and failure of models, theories, or experiments, the historian/sociologist should deploy the same \"kind\" of explanation in the cases of success as in cases of failure. When investigating beliefs, researchers should be impartial to the (\"a posteriori\" attributed) truth or falsehood of those beliefs, and the explanations should be unbiased. The strong programme adopts a position of relativism or neutralism regarding the arguments that social actors put forward for the acceptance/rejection of any technology. All arguments (social, cultural, political, economic, as well as technical) are to be treated equally.\n\nThe symmetry principle addresses the problem that the historian is tempted to explain the success of successful theories by referring to their \"objective truth\", or inherent \"technical superiority\", whereas s/he is more likely to put forward sociological explanations (citing political influence or economic reasons) only in the case of failures. For example, having experienced the obvious success of the chain-driven bicycle for decades, it is tempting to attribute its success to its \"advanced technology\" compared to the \"primitiveness\" of the Penny Farthing, but if we look closely and symmetrically at their history (as Pinch and Bijker do), we can see that at the beginning bicycles were valued according to quite different standards than nowadays. The early adopters (predominantly young, well-to-do gentlemen) valued the speed, the thrill, and the spectacularity of the Penny Farthing – in contrast to the security and stability of the chain-driven Safety Bicycle. Many other social factors (e.g., the contemporary state of urbanism and transport, women's clothing habits and feminism) have influenced and changed the relative valuations of bicycle models.\n\nA weak reading of the \"Principle of Symmetry\" points out that there often are many competing theories or technologies, which all have the potential to provide slightly different solutions to similar problems. In these cases, sociological factors tip the balance between them: that's why we should pay equal attention to them.\n\nA strong, social constructivist reading would add that even the emergence of the questions or problems to be solved are governed by social determinations, so the Principle of Symmetry is applicable even to the apparently purely technical issues.\n\nThe following concepts and 'stages' of SCOT are adapted from the Empirical Programme of Relativism (EPOR).\n\n\"Interpretative flexibility\" means that each technological artifact has different meanings and interpretations for various groups. Bijker and Pinch show that the air tire of the bicycle meant a more convenient mode of transportation for some people, whereas it meant technical nuisances, traction problems and ugly aesthetics to others. In racing air tires lent to greater speed.\n\nThese alternative interpretations generate different \"problems\" to be solved. How should aesthetics, convenience, and speed be prioritized? What is the \"best\" tradeoff between traction and speed?\n\nThe most basic relevant groups are the \"users\" and the \"producers\" of the technological artifact, but most often many subgroups can be delineated – users with different socioeconomic status, competing producers, etc. Sometimes there are relevant groups who are neither users, nor producers of the technology, for example, journalists, politicians, and civil organizations. Trevor Pinch has argued that the salespeople of technology should also be included in the study of technology. The groups can be distinguished based on their shared or diverging interpretations of the technology in question.\n\nJust as technologies have different meanings in different social groups, there are always multiple ways of constructing technologies. A particular design is only a single point in the large field of technical possibilities, reflecting the interpretations of certain relevant groups.\n\nThe different interpretations often give rise to conflicts between criteria that are hard to resolve technologically (e.g., in the case of the bicycle, one such problem was how a woman could ride the bicycle in a skirt while still adhering to standards of decency), or conflicts between the relevant groups (the \"Anti-cyclists\" lobbied for the banning of the bicycles). Different groups in different societies construct different problems, leading to different designs.\n\nThe first stage of the SCOT research methodology is to reconstruct the alternative interpretations of the technology, analyze the problems and conflicts these interpretations give rise to, and connect them to the design features of the technological artifacts. The relations between groups, problems, and designs can be visualized in diagrams.\n\nOver time, as technologies are developed, the interpretative and design flexibility collapse through closure mechanisms. Two examples of closure mechanisms:\n\n\nClosure is not permanent. New social groups may form and reintroduce interpretative flexibility, causing a new round of debate or conflict about a technology. (For instance, in the 1890s automobiles were seen as the \"green\" alternative, a cleaner environmentally-friendly technology, to horse-powered vehicles; by the 1960s, new social groups had introduced new interpretations about the environmental effects of the automobile, eliciting the opposite conclusion.)\n\nThe second stage of the SCOT methodology is to show how closure is achieved.\n\nThis is the third stage of the SCOT methodology, but the seminal article of Pinch and Bijker does not proceed to this stage. Many other historians and sociologists of technology nevertheless do. For example, Paul N. Edwards shows in his book \"The Closed World: Computers and the Politics of Discourse in Cold War America\" the strong relations between the political discourse of the Cold War and the computer designs of this era.\n\nIn 1993, Langdon Winner published an influential critique of SCOT entitled \"Upon Opening the Black Box and Finding it Empty: Social Constructivism and the Philosophy of Technology.\" In it, he argues that social constructivism is an overly narrow research program. He identifies the following specific limitations in social constructivism:\nOther critics include Stewart Russell with his letter in the journal \"Social Studies of Science\" titled \"The Social Construction of Artifacts: A Response to Pinch and Bijker\".\n\n\n\n"}
{"id": "10896010", "url": "https://en.wikipedia.org/wiki?curid=10896010", "title": "Superfund Research Program", "text": "Superfund Research Program\n\nThe Superfund Research Program (SRP) was created within the National Institute of Environmental Health Sciences in 1986 under the Superfund Amendments and Reauthorization Act (SARA). The SRP is a university-based research program that supports the national Superfund program by addressing a wide variety of scientific concerns. \n\nThe SRP has a broad mandate including:\n\n\nThe SRP currently funds multi-project grants at sixteen institutions (Boston University, Brown University, Columbia University, Dartmouth College, Harvard School of Public Health, Michigan State University, Northeastern University, Oregon State University, University of Arizona, University of California, Berkeley, University of California, Davis, University of California, San Diego, University of Iowa, University of Kentucky, University of North Carolina at Chapel Hill, and the University of Washington). The SRP also funds individual research projects (R01s), small business innovation research, and small business technology transfer research. \n\nThe 2008 budget request amount for the SRP was $50.198 million.\n\n\n"}
{"id": "515662", "url": "https://en.wikipedia.org/wiki?curid=515662", "title": "The Demon-Haunted World", "text": "The Demon-Haunted World\n\nThe Demon-Haunted World: Science as a Candle in the Dark is a 1995 book by astrophysicist Carl Sagan, in which the author aims to explain the scientific method to laypeople, and to encourage people to learn critical and skeptical thinking. He explains methods to help distinguish between ideas that are considered valid science and those that can be considered pseudoscience. Sagan states that when new ideas are offered for consideration, they should be tested by means of skeptical thinking and should stand up to rigorous questioning.\n\nScience to Sagan is not just a body of knowledge but a way of thinking. Sagan claims that the scientific way of thinking is both imaginative and disciplined, bringing humans to an understanding of how the universe is, rather than how they wish to perceive it. He says that science works much better than any other system because it has a \"built-in error-correcting machine\". Superstition and pseudoscience get in the way of many laypersons' ability to appreciate the beauty and benefits of science. Skeptical thinking allows people to construct, understand, reason, and recognize valid and invalid arguments. Wherever possible, there must be independent validation of the concepts whose truth should be proved. He states that reason and logic would succeed once the truth were known. Conclusions emerge from premises, and the acceptability of the premises should not be discounted or accepted because of bias.\n\nAs an example of skeptical thinking, Sagan offers a story concerning a fire-breathing dragon that lives in his garage. When he persuades a rational, open-minded visitor to meet the dragon, the visitor remarks that they are unable to see the creature. Sagan replies that he \"neglected to mention that she's an invisible dragon\". The visitor suggests spreading flour on the floor so that the creature's footprints might be seen, which Sagan says is a good idea, \"but this dragon floats in the air\". When the visitor considers using an infra-red camera to view the creature's invisible fire, Sagan explains that the fire is heatless. He continues to counter every proposed physical test with a reason why the test will not work.\n\nSagan concludes by asking: \"Now what's the difference between an invisible, incorporeal, floating dragon who spits heatless fire and no dragon at all? If there's no way to disprove my contention, no conceivable experiment that would count against it, what does it mean to say that my dragon exists? Your inability to invalidate my hypothesis is not at all the same thing as proving it true.\"\n\nSagan presents a set of tools for skeptical thinking which he calls the \"baloney detection kit\". Skeptical thinking consists both of constructing a reasoned argument and recognizing a fallacious or fraudulent one. In order to identify a fallacious argument, Sagan suggests employing such tools as independent confirmation of facts, debate, development of different hypotheses, quantification, the use of Occam's razor, and the possibility of falsification. Sagan's \"baloney detection kit\" also provides tools for detecting \"the most common fallacies of logic and rhetoric\", such as argument from authority and statistics of small numbers. Through these tools, Sagan argues the benefits of a critical mind and the self-correcting nature of science can take place.\n\nSagan provides a skeptical analysis of several examples of what he refers to as superstition, fraud, and pseudoscience such as witches, UFOs, ESP, and faith healing. He is critical of organized religion.\n\nSagan indicates that science can be misused. Thus, he is highly critical of Edward Teller, the \"father of the hydrogen bomb\", and Teller's influence on politics, and contrasts his stance to that of Linus Pauling and other scientists who took moral positions.\n\nThe book was a \"New York Times\" bestseller and is considered to be a very important book by the contemporary skeptical movement.\n\n\n"}
{"id": "58855040", "url": "https://en.wikipedia.org/wiki?curid=58855040", "title": "The Eppendorf &amp; Science Prize for Neurobiology", "text": "The Eppendorf &amp; Science Prize for Neurobiology\n\nThe Eppendorf & \"Science\" Prize for Neurobiology is a neurobiology prize that is awarded annually by \"Science\" magazine (published by American Association for the Advancement of Science) and underwritten by Eppendorf AG, laboratory device and supply company. Entrees are reviewed by editors from \"Science\" magazine and the top 10% are forwarded to the judging panel. The judging panel is chaired by the Neuroscience Editor of \"Science\" and the remaining judges are nominated from the Society for Neuroscience.\n\nThis Eppendorf & \"Science\" Prize for Neurobiology prize was created in 2002 to promote the work of promising new neurobiologists with cash grants to support their careers. Each applicant must submit a 1000 word essay explaining the focus and motivation for their last three years of work. The winner is awarded $25,000 and the scientist’s winning essay is then published in \"Science\". The winning essay and the essays of the other finalists are all published on \"Science Online\".\n"}
{"id": "6060316", "url": "https://en.wikipedia.org/wiki?curid=6060316", "title": "The Fringe of the Unknown", "text": "The Fringe of the Unknown\n\nThe Fringe of the Unknown is a science book by L. Sprague de Camp, first published in hardcover by Prometheus Books in 1983.\n\nThe book is a collection of articles that constitute a \"study ... of controversial and often little-known happenings in science and technology, with an emphasis on the wayward activities of those who dabble in fringe science.\" The material is organized in three sections, \"Our Ingenious Forebears,\" \"Beasts of Now and Then,\" and \"Scientists, Mad and Otherwise.\" The first debunks extravagant occult and pseudoscientific claims regarding ancient civilizations while highlighting these cultures' actual accomplishments. The second performs much the same function in regard to biology, focusing on elephants, claims regarding the survival of dinosaurs into the present day, and past extinction events. The third explores the distinction between science and pseudoscience as illustrated in the lives of a number of scientists holding extreme views.\n\n1. \"The Wisdom of the Ancients\" (from \"Science Fiction Quarterly\", Nov. 1951)<br>\n2. \"Apollonios Enlists\" (from \"Astounding Science Fiction\", Jun. 1961)<br>\n3. \"Appius Claudius Crassus\" (original title: \"Appius Claudius Crassus: Roman Builder\") (from \"Science Digest\", Jun. 1962)<br>\n4. \"The First Missile Launchers\" (from \"Science Digest\", Oct. 1960)<br>\n5. \"The Iron Pillar of Delhi\" (from \"Analog Science Fiction/Science Fact\", Sep. 1972)<br>\n6. \"The Mechanical Wizards of Alexandria\" (from \"Science Digest\", Aug. 1962)<br>\n7. \"The Landlocked Indian Ocean\" (from \"The Magazine of Fantasy & Science Fiction\", Jun. 1969)\n\n8. \"Dinosaurs Today\" (original title: \"Dinosaurs in Today's World\") (from \"The Magazine of Fantasy & Science Fiction\", Mar. 1968)<br>\n9. \"Mammoths and Mastodons\" (from \"The Magazine of Fantasy & Science Fiction\", May 1965)<br>\n10. \"Death Comes to the Megafauna\" (from \"If Worlds of Science Fiction\", Sep. 1971)<br>\n11. \"Xerxes' Okapi\" (original title: \"Xerxes' Okapi and Greek Geography\") (from \"Isis\", Mar. 1963)<br>\n12. \"The Temperamental Tank\" (original title: \"War Elephants\") (from \"Elephant\", 1964)<br>\n13. \"How to Plan a Fauna\" (from \"The Magazine of Fantasy & Science Fiction\", Oct. 1963)\n\n14. \"The Care and Feeding of Scientists\" (original title: \"The Care and Feeding of Mad Scientists\") (from \"Astounding Science Fiction\", Jul. 1951)<br>\n15. \"The Great Whale Robbery\" (from \"The Day of the Dinosaur\", 1968)<br>\n16. \"Mad Men of Science\" (originally published in two parts, as \"Mad Men of Science\" and \"More Mad Men of Science\") (from \"Future Science Fiction\", Jan. and Mar. 1957)<br>\n17. \"Orthodoxy in Science\" (from \"Astounding Science Fiction\", May 1954)<br>\n18. \"Hoaxes in Science\" (original title: \"Why Do They Do It?\") (from \"Astounding Science Fiction\", Sep. 1950)<br>\n19. \"Little Green Men from Afar\" (from \"The Humanist\", Jul./Aug. 1976)<br>\n20. \"The Need to Know\" (original title: \"Pure Science\") (from \"The Book of Knowledge Annual\", 1959)<br>\n\"Acknowledgments\"\n"}
{"id": "654682", "url": "https://en.wikipedia.org/wiki?curid=654682", "title": "The Millennial Project", "text": "The Millennial Project\n\nThe Millennial Project: Colonizing the Galaxy in Eight Easy Steps by Marshall T. Savage is a book (published in 1992 and reprinted in 1994 with an introduction by Arthur C. Clarke) in the field of Exploratory engineering that gives a series of concrete stages the author believes will lead to interstellar colonization. Many specific scientific and engineering details are presented, as are numerous issues involved in Space colonization.\n\nSavage takes a Malthusian view of the exponential growth of human population and life in general, and also recommends the exponential growth of blue-green algae for sustenance. He states that it is humanity's manifest destiny to colonize every star in the galaxy. He draws heavily on the Fermi paradox (briefly stated as, \"If there is intelligent life in space, why haven't we found it yet?\") to support his position that it is humanity's burden alone to ignite the universe with the \"spark of Life.\"\nIn \"The Millennial Project\", he calls for the creation of an international foundation to realize these goals. Originally known as the \"First Millennial Foundation\" (founded by Savage in 1987), the organization changed its name to the Living Universe Foundation.\n\nThe \"Eight Easy Steps\" proposed by Savage are as follows:\nIn the early stages of the Project, Savage recommends Spirulina algae as a primary foodstuff, supplemented by seafood mariculture from the cities of the Aquarius phase.\n\nThe Living Universe Foundation, previously known as the First Millennial Foundation, is an organization that supports ocean and space colonization more or less based upon the book \"The Millennial Project\". Space Environments Ecovillage in Bastrop, Texas, is one of the few LUF projects that have materialized so far.\n\nThe book has drawn some criticism in that while it is replete with details concerning OTEC construction and space colonization, it touches very little on the subject of how governments and societies will need to change to enact the Project. Defenders and the author himself maintain that one man writing one book cannot be expected to write out the entire course of human development over the next millennium, and that others more suited to the field of social psychology will be needed for the Project's completion.\n\n\n"}
{"id": "26314185", "url": "https://en.wikipedia.org/wiki?curid=26314185", "title": "The Poisoner's Handbook", "text": "The Poisoner's Handbook\n\nThe Poisoner's Handbook: Murder and the Birth of Forensic Medicine in Jazz Age New York is a \"New York Times\" best-selling non-fiction book by Pulitzer Prize-winning science writer Deborah Blum that was released by Penguin Press in 2010.\n\nIn 1918, New York City appointed Charles Norris, Bellevue Hospital's chief pathologist, as its first scientifically trained medical examiner. The book, about Norris and Alexander Gettler, the city's first toxicologist, describes the Jazz Age's poisoning cases. Before the two began working in the medical examiner's office, Blum pointed out in her book, poisoners could get away with murder. The book covers the years from 1915 to 1936, which Blum described as a \"coming-of-age\" for forensic toxicology. \"Under (Norris's) direction, the New York City medical examiner's office would become a department that set forensic standards for the rest of the country,\" Blum wrote.\n\nWhile a guest on National Public Radio’s \"Talk of the Nation/Science Friday\" to discuss the book, Blum told host Ira Flatow that she wrote the book because, \"I've always been interested in poison. I wanted to write about the mystery of how (poisons) kill us.”\n\n\"Reader's Digest\" named \"The Poisoner's Handbook\" one of its Top 10 best crime books, saying, \"This is science writing at its finest that reads like a mystery novel.\"\n\n\"The New York Times\" placed the book on its Top-rated List on March 5, 2010. In its Sunday book review, the \"Times\" said \"The Poisoner's Handbook\" was \"structured like a collection of linked short stories. Each chapter centers on a mysterious death by poison that Norris and Gettler investigate.\"\n\nThe book was listed as a \"New York Times bestseller\" in paperback nonfiction in February 2011. Also, Amazon named \"The Poisoner's Handbook\" in its Top 100 Best of 2010.\n\n\"Not only is \"The Poisoner's Handbook\" as thrilling as any 'CSI' episode,\" wrote reviewer Art Taylor with \"The Washington Post\", \"but it also offers something even better: an education in how forensics really works.\"\n\n\"Kirkus Reviews\" described the book as, \"The rollicking story of the creation of modern forensic science by New York researchers during the Prohibition era.\"\n\nBarnes and Noble's editor's review said this: \"The book is an unexpected yet appropriate open-sesame into a world that was planting seeds for the world -- with lethal toxins and cutting-edge tools -- that would later, darkly bloom.\"\n\nGlen Weldon from NPR Books said: \"Rigorously researched and thoroughly engaging, \"The Poisoner's Handbook\" is a compelling, comprehensive portrait of the time and place that transformed criminal investigation, and made it much more difficult for that most insidious of murderers to escape the law.\"\n\nPBS optioned the book for TV and produced it as an episode of \"American Experience\". It premiered on January 7, 2014.\n\n\n\"Angel Killer: A True Story of Cannibalism, Crime Fighting, and Insanity in New York City\" (The Atavist, 2012)\n\n"}
{"id": "5452205", "url": "https://en.wikipedia.org/wiki?curid=5452205", "title": "Trading zones", "text": "Trading zones\n\nThe metaphor of a trading zone is being applied to collaborations in science and technology. The basis of the metaphor is anthropological studies of how different cultures are able to exchange goods, despite differences in language and culture. \n\nPeter Galison produced the \"trading zone\" metaphor in order to explain how physicists from different paradigms went about collaborating with each other and with engineers to develop particle detectors and radar.\n\nAccording to Galison, \"Two groups can agree on rules of exchange even if they ascribe utterly different significance to the objects being exchanged; they may even disagree on the meaning of the exchange process itself. Nonetheless, the trading partners can hammer out a local coordination, despite vast global differences. In an even more sophisticated way, cultures in interaction frequently establish contact languages, systems of discourse that can vary from the most function-specific jargons, through semispecific pidgins, to full-fledged creoles rich enough to support activities as complex as poetry and metalinguistic reflection\" (Galison 1997, p. 783)\n\nIn the case of radar, for example, the physicists and engineers had to gradually develop what was effectively a pidgin or creole language involving shared concepts like ‘equivalent circuits’ that the physicists represented symbolically in terms of field theory and the engineers saw as extensions of their radio toolkit. \n\nExchanges across disciplinary boundaries can also be carried out with the help of an agent: a person who is familiar enough with the language of two or more cultures to facilitate trade.\n\nAt one point in the development of MRI, surgeons saw a lesion where an engineer familiar with the device would have recognized an artifact produced by the way the device was being used. It took someone with expertise in both physics and surgery to see how each of the different disciplines viewed the device, and develop procedures for correcting the problem (Baird & Cohen, 1999). The ability to converse expertly in more than one discipline is called interactional expertise (Collins & Evans, 2002).\n\n\nA workshop at Arizona State University on Trading Zones, Interactional Expertise and Interdisciplinary Collaboration raised the possibility of applying these concepts to other applications like global health and service science, and also identified avenues for future research (https://archive.is/20121215123346/http://bart.tcc.virginia.edu/Tradzoneworkshop/index.htm).\n\n\n\n"}
{"id": "5833630", "url": "https://en.wikipedia.org/wiki?curid=5833630", "title": "Transport Phenomena (book)", "text": "Transport Phenomena (book)\n\nTransport Phenomena is the first textbook about transport phenomena. It is specifically designed for chemical engineering students. The first edition was published in 1960, two years after having been preliminarily published under the title \"Notes on Transport Phenomena\" based on mimeographed notes prepared for a chemical engineering course taught at the University of Wisconsin–Madison during the academic year 1957-1958. The second edition was published in August 2001. A \"revised\" second edition was published in 2007. This text is often known simply as \"BSL\" after its authors' initials.\n\nAs the chemical engineering profession developed in the first half of the 20th century, the concept of \"unit operations\" arose as being needed in the education of undergraduate chemical engineers. The theories of mass, momentum and energy transfer were being taught at that time only to the extent necessary for a narrow range of applications. As chemical engineers began moving into a number of new areas, problem definitions and solutions required a deeper knowledge of the fundamentals of transport phenomena than those provided in the textbooks then available on unit operations.\n\nIn the 1950s, R. Byron Bird, Warren E. Stewart and Edwin N. Lightfoot stepped forward to develop an undergraduate course at the University of Wisconsin–Madison to integrate the teaching of fluid flow, heat transfer, and diffusion. From this beginning, they prepared their landmark textbook \"Transport Phenomena\".\n\nThe book is divided into three basic sections, named Momentum Transport, Energy Transport and Mass Transport:\n\n\n\"Transport Phenomena\" contains many instances of hidden messages and other word play.\nFor example, the first letters of each sentence of the Preface spell out \"This book is dedicated to O. A. Hougen.\" while in the revised second edition, the first letters of each paragraph spell out \"Welcome\". The first letters of each paragraph in the Postface spell out \"On Wisconsin\". In the first printing, in Fig. 9.L (p. 305) \"Bird\" is typeset safely outside the furnace wall.\n\nAccording to many chemical engineering professors, the first edition is much better than the second edition. There are many reasons in this regard; The second edition has been revised many times despite the fact that there are still many defects and typographical errors in many parts of the book. On account of revision to defects of the revised second edition book, the authors published \"Notes for the 2nd revised edition of TRANSPORT PHENOMENA\" on 9 Aug 2011.\n\n\n"}
{"id": "36990855", "url": "https://en.wikipedia.org/wiki?curid=36990855", "title": "Violation paradigm", "text": "Violation paradigm\n\nA violation paradigm is a scientific method where the scientist perturbs an expected factor to look at the subject's following reactions. These reactions are believed to be relevant to the process studied. For example, creating wrong word segmentations in a text will destabilize the reader. This warns the researcher that the respondent's brain considers the characters are united into words, and not just as a succession of given sets of letters. The process was originally developed by Danks, Bohn & Fear (1983), and proved valid ().\n\n"}
{"id": "12431148", "url": "https://en.wikipedia.org/wiki?curid=12431148", "title": "Visiting scholar", "text": "Visiting scholar\n\nIn US academia, a visiting scholar, visiting researcher, visiting fellow, visiting lecturer or visiting professor is a scholar from an institution who visits a host university and is projected to teach, lecture, or perform research on a topic the visitor is valued for. In many cases the position is not salaried because the scholar typically is salaried by their home institution (or partially salaried, as in some cases of sabbatical leave from US universities), while some visiting positions are salaried. \n\nTypically, a position as visiting scholar is for a couple of months or even a year, though it can be extended. It is not unusual that host institutions provide accommodation for the visiting scholar. Typically, a visiting scholar is invited by the host institution. Being invited as a visiting scholar is often regarded as a significant accolade and recognition of the scholar's prominence in the field. Attracting prominent visiting scholars often allows the permanent faculty and graduate students to cooperate with prominent academics from other institutions, especially foreign ones.\n\nIn the UK, a visiting scholar or visiting academic usually has to pay a so-called bench fee to the university, which will give them access to a shared office space and other university facilities and resources (such as the library). Bench fee amounts vary across the UK universities. \n\nThe purpose of a visiting scholars programs is generally to bring to the university or educational institution in question an exceptional senior scholar who can contribute to and enrich the community's intellectual and research endeavors and international projection. Hence, in addition to conducting their own research, visitors are often expected to actively participate in a number of productive institutional activities, such as:\n\n\n"}
{"id": "20940169", "url": "https://en.wikipedia.org/wiki?curid=20940169", "title": "Wild Solutions", "text": "Wild Solutions\n\nWild Solutions: How Biodiversity is Money in the Bank is a 2001 book by biologists Andrew Beattie and Paul R. Ehrlich. The authors explain the value of \"wild solutions\" to technical and medical problems that may reside in the diversity of the Earth's estimated 5 to 10 million species. Beattie and Ehrlich describe the role of natural substances in medicine, pest control, and manufacturing. The book won a National Outdoor Book Award in 2001.\nA second edition came out in 2004.\n"}
{"id": "38106941", "url": "https://en.wikipedia.org/wiki?curid=38106941", "title": "Woozle effect", "text": "Woozle effect\n\nThe Woozle effect, also known as evidence by citation, or a woozle, occurs when frequent citation of previous publications that lack evidence misleads individuals, groups, and the public into thinking or believing there is evidence, and nonfacts become urban myths and factoids.\n\nA Woozle is an imaginary character in the A. A. Milne book \"Winnie-the-Pooh\", published in 1926. In chapter three, \"In which Pooh and Piglet Go Hunting and Nearly Catch a Woozle\", Winnie-the-Pooh and Piglet start following tracks left in snow believing they are the tracks of an imaginary animal called a \"woozle\". The tracks keep multiplying until Christopher Robin explains to them that they have been following their own tracks in circles around a tree.\n\nPrior to the introduction of the specific term \"Woozle effect\", the underlying research phenomenon (and connection to the Woozle) dates back over 60 years. Bevan (1953), writing about scientific methodology and research errors in the field of psychology, uses the term \"scientific woozle hunters\". Wohlwill (1963) refers to a \"hunt for the woozle\" in social science research, and Stevens (1971) cautions readers about woozles in the study of a misquoted letter.\n\nAccording to Richard J. Gelles, the term \"woozle effect\" was coined by Beverly Houghton in 1979. Other researchers have attributed the term to Gelles (1980) and Gelles and Murray A. Straus (1988). Gelles and Straus argue that the woozle effect describes a pattern of bias seen within social sciences and which is identified as leading to multiple errors in individual and public perception, academia, policy making and government. A woozle is also a claim made about research which is not supported by original findings. According to Dutton, a woozle effect, or a woozle, occurs when frequent citation of previous publications that lack evidence misleads individuals, groups and the public into thinking or believing there is evidence, and non-facts become urban myths and factoids. The creation of woozles is often linked to the changing of language from qualified (\"it may\", \"it might\", \"it could\") to absolute form (\"it is\") firming up language and introducing ideas and views not held by an original author or supported by evidence.\n\nDutton sees the woozle effect as an example of confirmation bias and links it to belief perseverance and groupthink. Because in the social sciences empirical evidence may be based on experiential reports rather than objective measurements, there may be a tendency for researchers to align evidence with expectation. According to Dutton it is also possible that the social sciences may be likely to align with contemporary views and ideals of social justice, leading to bias in favor of those ideals. Gambrill (2012) links the woozle effect to the processes that create pseudoscience. Gambrill and Reiman (2011) also link it with more deliberate propaganda techniques; they also identify introductory phrases like \"Every one knows ...\", \"It is clear that ...\", \"It is obvious that ...\", \"It is generally agreed that ...\" as alarm bells that what follows might be a Woozle line of reasoning.\n\nIn 1979, Houghton illustrated the Woozle effect, showing how work by Gelles (1974) based on a small sample and published in \"The Violent Home\" by Straus, who had written the foreword for Gelles's book, was presented as if it applied to a large sample. Both of these were then cited by Langley & Levy in their 1977 book, \"Wife Beating: The Silent Crisis\". In the 1998 book \"Intimate Violence\", Gelles and Straus use the Winnie-the-Pooh woozle to illustrate how poor practice in research and self-referential research causes older research to be taken as fresh evidence causing error and bias.\n\nIn a study conducted by the Vera Institute of Justice, Weiner and Hala (2008) reported some of the research-related difficulties associated with measuring human trafficking. They describe and map the unfolding of the Woozle effect in connection with prevalence estimates of human trafficking. Searching the relevant literature between 1990 and 2006, Weiner and Hala found 114 prevalence estimates in 45 publications. Only one of the publications cited original research, and several prevalence estimates appeared unsourced. The authors concluded that the sources they reviewed lacked citations, adequate operational definition, and discussion of methodology. Stransky and Finkelhor (2008/2012) criticize the general methodology involved in human trafficking research. They cite the Woozle effect (p. 3) and post a prominent warning on the first page of their report cautioning against citing any specific estimates they present, as the close inspection of the figures \"...reveals that none are based on a strong scientific foundation. (p. 1)\"\n\nGambrill and Reiman (2011) analyze scientific papers and mass-market communications about social anxiety and conclude that many of them engage in disease mongering by presenting the disease model of social anxiety as an incontrovertible fact by resorting to unchallenged repetition techniques and by leaving out of the discourse any competing theories. Gambrill and Reiman further note that even after educating their subjects about the tell-tale signs of such techniques, many of them still failed to pick up the signs in a practical test.\nJames J. Kimble gives as an example the 1994–2015 historiography of the 1943 American \"We Can Do It!\" wartime poster. After Michigan resident Geraldine Hoff Doyle said in 1994 that she was the real-life model for the poster, many sources repeated her assertion without checking the two foundational assumptions: that Doyle was the young factory worker pictured in a 1942 wartime photograph, and that the photograph had inspired commercial artist J. Howard Miller to create the poster. Though some media representations described the connection as unconfirmed, many more enthusiastically endorsed it. The weight of these multiple endorsements gave Doyle's story a \"convincing\" authority, despite the lack of authority in establishing the connection. In 2015 Kimble found the original photographic print of the factory worker, its caption identifying the young woman as Naomi Parker, working in California in March 1942, when Doyle was still in high school.\n\n"}
