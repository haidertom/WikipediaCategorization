{"id": "36746396", "url": "https://en.wikipedia.org/wiki?curid=36746396", "title": "Bernard Price Memorial Lecture", "text": "Bernard Price Memorial Lecture\n\nThe Bernard Price Memorial Lecture is the premier annual lecture of the South African Institute of Electrical Engineers. It is of general scientific or engineering interest and is given by an invited guest, often from overseas, at several of the major centres on South Africa. The main lecture and accompanying dinner are usually held at the University of Witwatersrand and it is also presented in the space of one week at other centres, typically Cape Town, Durban, East London and Port Elizabeth.\n\nThe Lecture is named in memory of the eminent electrical engineer Bernard Price. The first Lecture was held in 1951 and it has occurred as an annual event ever since.\n\n1951 Basil Schonland\n\n1952 A M Jacobs\n\n1953 H J Van Eck\n\n1954 J M Meek\n\n1955 Frank Nabarro\n\n1956 A L Hales\n\n1957 P G Game\n\n1958 Colin Cherry\n\n1959 Thomas Allibone\n\n1960 M G Say\n\n1961 Willis Jackson\n\n1963 W R Stevens\n\n1964 William Pickering\n\n1965 G H Rawcliffe\n\n1966 Harold Bishop\n\n1967 Eric Eastwood\n\n1968 F J Lane\n\n1969 A H Reeves\n\n1970 Andrew R Cooper\n\n1971 Herbert Haslegrave\n\n1972 W J Bray\n\n1973 R Noser\n\n1974 D Kind\n\n1975 L Kirchmayer\n\n1976 S Jones\n\n1977 J Johnson\n\n1978 T G E Cockbain\n\n1979 A R Hileman\n\n1980 James Redmond\n\n1981 L M Muntzing\n\n1982 K F Raby\n\n1983 R Isermann\n\n1984 M N John\n\n1985 J W L de Villiers\n\n1986 Derek Roberts\n\n1987 Wolfram Boeck\n\n1988 Karl Gehring\n\n1989 Leonard Sagan\n\n1990 GKF Heyner\n\n1991 P S Blythin\n\n1992 P M Neches\n\n1993 P Radley\n\n1994 P R Rosen\n\n1995 F P Sioshansi\n\n1996 J Taylor\n\n1997 M Chamia\n\n1998 C Gellings\n\n1999 M W Kennedy\n\n2000 John Midwinter\n\n2001 Pragasen Pillay\n\n2002 Polina Bayvel\n\n2003 Case Rijsdijk\n\n2004 Frank Larkins\n\n2005 Igor Aleksander\n\n2006 Kevin Warwick\n\n2007 Skip Hatfield\n\n2008 Sami Solanki\n\n2009 William Gruver\n\n2010 Glenn Ricart\n\n2011 Philippe Paelinck\n\n2012 Nick Frydas\n\n2013 Vint Cerf\n\n2014 Ian Jandrell\n\n2015 Saurabh Sinha\n\n2016 Tshilidzi Marwala \n\n2017 Fulufhelo Nelwamondo \n\n2018 Ian Craig \n"}
{"id": "6738308", "url": "https://en.wikipedia.org/wiki?curid=6738308", "title": "Borderless selling", "text": "Borderless selling\n\nBorderless selling is the process of selling services to clients outside the country of origin of services through modern methods which eliminate the actions specifically designed to hinder international trade. International trade through \"borderless selling\" is a new phenomenon born in the current \"globalization\" era.\n\nBorderless selling is defined as the process of performing sales transaction between two or more parties from different countries (an exporter and an importer) which is free from actions specifically designed to hinder international trade, such as tariff barriers, currency restrictions, and import quotas.\n\nInternational trade which is the exchange of goods and services across international borders has been present throughout much of history of economics, society and politics. \n\nIt is assumed that offshore outsourcing gave birth to \"borderless selling\". The selling of services by offshore outsourcing service providers to foreign clients is free from actions specifically designed to hinder international trade, such as tariff barriers, currency restrictions, and import quotas. This is largely because most of the services are sold or delivered electronically from the offshore service provider to the foreign client. This phenomenon gave birth to borderless selling.\n\nThere is a high correlation between outsourcing and exporting activity. However, borderless selling is different from free international trade or selling. Under the belief in Mercantilism, most nations had high tariffs and many restrictions on international trade for centuries. In the 19th century, a belief in free trade became paramount in west, especially in Britain and this outlook has since then dominated the thinking of western nations. Traditionally international trade was possible between only those countries which regulated international trade through bilateral treaties. Borderless selling is possible between any two countries of the world because services can be exported using modern telecommunication networks without the need to regulate trade.\n\nThe \"borderless selling\" term was originated as part of the research carried out by team led by Paramjeev Singh Sethi.\n\n\n\nMany services can be sold through borderless selling, popularly including:\n\nDifferent means used for borderless selling:\n\n"}
{"id": "210879", "url": "https://en.wikipedia.org/wiki?curid=210879", "title": "Consilience (book)", "text": "Consilience (book)\n\nConsilience: The Unity of Knowledge is a 1998 book by biologist E. O. Wilson, in which the author discusses methods that have been used to unite the sciences and might in the future unite them with the humanities. Wilson uses the term \"consilience\" to describe the synthesis of knowledge from different specialized fields of human endeavor.\n\n\"Literally a 'jumping together' of knowledge by the linking of facts and fact-based theory across disciplines to create a common groundwork of explanation.\" (page 7)\n\n\n\n\n\n\n\n\n\n"}
{"id": "49602444", "url": "https://en.wikipedia.org/wiki?curid=49602444", "title": "DU spectrophotometer", "text": "DU spectrophotometer\n\nThe DU spectrophotometer or Beckman DU, introduced in 1941, was the first commercially viable scientific instrument for measuring the amount of ultraviolet light absorbed by a substance. This model of spectrophotometer enabled scientists to easily examine and identify a given substance based on its absorption spectrum, the pattern of light absorbed at different wavelengths. Arnold O. Beckman's National Technical Laboratories (later Beckman Instruments) developed three in-house prototype models (A, B, C) and one limited distribution model (D) before moving to full commercial production with the DU. Approximately 30,000 DU spectrophotometers were manufactured and sold between 1941 and 1976.\n\nSometimes referred to as a UV–Vis spectrophotometer because it measured both the ultraviolet (UV) and visible spectra, the DU spectrophotometer is credited as being a truly revolutionary technology. It yielded more accurate results than previous methods for determining the chemical composition of a complex substance, and substantially reduced the time needed for an accurate analysis from weeks or hours to minutes. The Beckman DU was essential to several critical secret research projects during World War II, including the development of penicillin and synthetic rubber.\n\nBefore the development of the DU spectrophotometer, analysis of a test sample to determine its components was a long, costly, and often inaccurate process. A classical wet laboratory contained a wide variety of complicated apparatus. Test samples were run through a series of awkward and time-consuming qualitative processes to separate out and identify their components. Determining quantitative concentrations of those components in the sample involved further steps. Processes could involve techniques for chemical reactions, precipitations, filtrations and dissolutions. Determination of the concentrations of known impurities in a known inorganic substance such as molten iron could be done in under thirty minutes. The determination of complex organic structures such as chlorophyll using wet and dry methods could take decades. \n\nSpectroscopic methods for observing the absorption of electromagnetic radiation in the visible spectrum were known as early as the 1860s.\nScientists had observed that light traveling through a medium would be absorbed at different wavelengths, depending on the matter-composition of the medium involved. A white light source would emit light at multiple wavelengths over a range of frequencies. A prism could be used to separate a light source into specific wavelengths. Shining the light through a sample of a material would cause some wavelengths of light to be absorbed, while others would be unaffected and continue to be transmitted. Wavelengths in the resulting absorption spectrum would differ depending upon the atomic and molecular composition if the material involved.\n\nSpectroscopic methods were predominantly used by physicists and astrophysicists. Spectroscopic techniques were rarely taught in chemistry classes and were unfamiliar to most practicing chemists. Beginning around 1904, Frank Twyman of the London instrument making firm Adam Hilger, Ltd. tried to develop spectroscopic instruments for chemists, but his customer base was consistently made up of physicists rather than chemists.\n\nBy the 1940s, both academic and industrial chemists were becoming increasingly interested in problems involving the composition and detection of biological molecules. Biological molecules, including proteins and nucleic acids, absorb light energy in both the ultraviolet and visible range. The spectrum of visible light was not broad enough to enable scientists to examine substances such as vitamin A. Accurate characterization of complex samples, particularly of biological materials, would require the accurate reading of absorption frequencies in the ultraviolet and infrared (IR) sections of the spectrum in addition to visible light. Existing instruments such as the Cenco \"Spectrophotelometer\" and the Coleman Model DM Spectrophotometer could not be effectively used to examine wavelengths in the ultraviolet range.\n\nThe array of equipment needed to measure light energy reaching beyond the visible spectrum towards the ultraviolet could cost a laboratory as much as $3,000, a huge amount in 1940. Repeated readings of a sample were taken to produce photographic plates showing the absorption spectrum of a material at different wavelengths. An experienced human could compare these to the known images to identify a match. Then information from the plates had to be combined to create a graph showing the spectrum as a whole. Ultimately, the accuracy of such approaches was dependent on accurate, consistent development of the photographic plates, and on human visual acuity and practice in reading the wavelengths.\n\nThe DU was developed at National Technical Laboratories (later Beckman Instruments) under the direction of Arnold Orville Beckman, an American chemist and inventor. Beginning in 1940, National Technical Laboratories developed three in-house prototype models (A, B, C) and one limited distribution model (D) before moving to full commercial production with the DU in 1941. Beckman's research team was led by Howard Cary, who went on to co-found Applied Physics Corporation (later Cary Instruments) which became one of Beckman Instruments' strongest competitors. Other scientists included Roland Hawes and Kenyon George.\n\nColeman Instruments had recently coupled a pH meter with an optical phototube unit to examine the visual spectrum (the Coleman Model DM). Beckman had already developed a successful pH meter for measuring acidity of solutions, his company's breakthrough product. Seeing the potential to build upon their existing expertise, Beckman made it a goal to create an easy-to-use integrated instrument which would both register and report specific wavelengths extending into the ultraviolet range. Rather than depending on development of photographic plates, or a human observer's visual ability to detect wavelengths in the absorption spectrum, phototubes would be used to register and report the specific wavelengths that were detected. This had the potential to increase the instrument's accuracy and reliability as well as its speed and ease of use.\n\nThe first prototype Beckman spectrophotometer, the Model A, was created at National Technologies Laboratories in 1940. It used a tungsten light source with a glass Fery prism as a monochromator. Tungsten was used for incandescent light filaments because it was strong, withstood heat, and emitted a steady light. Types of light sources differed in the range of wavelengths of light that they emitted. Tungsten lamps were useful in the visible light range but gave poor coverage in the ultraviolet range. However, they had the advantage of being readily available because they were used as automobile headlamps. An external amplifier from the Beckman pH meter and a vacuum tube photocell were used to detect wavelengths.\n\nIt was quickly realized that a glass dispersive prism was not suitable for use in the ultraviolet spectrum. Glass absorbed electromagnetic radiation below 400 millimicrons rather than dispersing it. In the Model B, a quartz prism was substituted for the earlier glass.\n\nA tangent bar mechanism was used to adjust the monochromator. The mechanism was highly sensitive and required a skilled operator. Only two Model B prototypes were made. One was sold: in February 1941, to the University of California Chemistry department in Los Angeles.\n\nThe Model B prototype should be distinguished from a later production model of spectrophotometer that was also referred to as the Model \"B\". The production Model \"B\" was introduced in 1949 as a less-expensive, simple-to-use alternative to the Beckman DU. It used a glass Fery prism as a chromator and operated in a narrower range, roughly from 320 millimicrons to 950 millimicrons, and 5 to 20 Å.\n\nThree Model C instruments were then built, improving the instrument's wavelength resolution. The Model B's rotary cell compartment was replaced with a linear sample chamber. The tangent bar mechanism was replaced by a scroll drive mechanism, which could be more precisely controlled to reset the quartz prism and select the desired wavelength. With this new mechanism, results could be more easily and reliably obtained, without requiring a highly skilled operator. This set the pattern for all of Beckman's later quartz prism instruments. Although only three Model B prototypes were built, all were sold, one to Caltech and the other two to companies in the food industry.\n\nThe A, B, and C prototype models all coupled an external Beckman pH meter to the optical component to obtain readouts. In developing the Model D, Beckman took the direct-coupled amplifier circuit from the pH meter and combined the optical and electronic components in a single housing, making it more economical.\n\nMoving from a prototype to production of the Model D involved challenges. \nBeckman originally approached Bausch and Lomb about making quartz prisms for the spectrophotometer. When they turned down the opportunity, National Technical Laboratories designed its own optical system, including both a control mechanism and a quartz prism. Large, high optical quality quartz suitable for creating prisms was difficult to obtain. It came from Brazil, and was in demand for wartime radio oscillators. Beckman had to obtain a wartime priority listing for the spectrophotometer to get access to suitable quartz supplies.\n\nBeckman had previously attempted to find a source of reliable hydrogen lamps, seeking better sensitivity to wavelengths in the ultraviolet range than was possible with tungsten. As described in July 1941, the Beckman spectrophotomter could use a \"hot cathode hydrogen discharge tube\" or a tungsten light source interchangeably. However, Beckman was still unsatisfied with the available hydrogen lamps. National Technical Laboratories designed its own hydrogen lamp, an anode enclosed in a thin blown-glass window. By December 1941, the in-house design was being used in production of the Model D.\n\nThe instrument's design also required a more sensitive phototube than was commercially available at that time. Beckman was able to obtain small batches of an experimental phototube from RCA for the first Model D instruments.\n\nThe Model D spectrophotometer, using the experimental RCA phototube, was shown at MIT's Summer Conference on Spectroscopy in July 1941. The paper that Cary and Beckman presented there was published in the \"Journal of the Optical Society of America\". In it, Cary and Beckman compared designs for a modified self-collimating quartz Fery prism, a mirror-collimated quartz Littrow prism, and various gratings. The Littrow prism was a half-prism, which had a mirrored face. Use of a tungsten light source with the quartz Littrow prism as a monochromator was reported to minimize light scattering within the instrument.\n\nThe Model D was the first model to enter actual production. A small number of Model D instruments were sold, beginning in July 1941, before it was superseded by the DU.\n\nWhen RCA could not meet Beckman's demand for experimental phototubes, National Technical Laboratories again had to design its own components in-house. They developed a pair of phototubes, sensitive to the red and blue areas of the spectrum, capable of amplifying the signals they received. With the incorporation of Beckman's UV-sensitive phototubes, the Model D became the Model DU UV–Vis spectrophotometer. Its designation as a \"UV–Vis\" spectrophotometer indicates its ability to measure light in both the visible and ultraviolet spectra.\n\nThe DU was the first commercially viable scientific instrument for measuring the amount of ultraviolet light absorbed by a substance. As he had done with the pH meter, Beckman had replaced an array of complicated equipment with a single, easy-to-use instrument. One of the first fully integrated instruments or \"black boxes\" used in modern chemical laboratories, it sold for $723 in 1941.\n\nIt is generally assumed that the \"DU\" in the name was a combination of \"D\" for the Model D on which it was based, and \"U\" for the ultraviolet spectrum. However, it has been suggested that \"DU\" may also reference Beckman's fraternity at the University of Illinois, Delta Upsilon, whose members were called \"DU\"s.\n\nA publication in the scholarly literature compared the optical quality of the DU to the Cary 14 Spectrophotometer, another leading UV–Vis spectrophotometer of the time.\n\n<BR>\nFrom 1941 until 1976, when it was discontinued, the Model DU spectrophotometer was built upon what was essentially the same design. It was a single beam instrument.\nThe DU spectrophotometers used a quartz prism to separate light from a lamp into its absorption spectrum and a phototube to electrically measure the light energy across the spectrum. This allowed the user to plot the light absorption spectrum of a substance to obtain a standardized \"fingerprint\" characteristic of a compound. All modern UV–Vis spectrophotometer are built on the same basic principles as the DU spectrophotometer.\n\n<BR>\n\nAlthough the default light source for the instrument was tungsten, a hydrogen or mercury lamp could be substituted depending on the optimal range of measurement for which the instrument was to be used. The tungsten lamp was suitable for transmittance of wavelengths between 320 and 1000 millimicrons; the hydrogen lamp for 220 to 320 millimicrons, and the mercury lamp for checking the calibration of the spectrophotometer.\n\n<BR>\nAs advertised in the 1941 News Edition of the American Chemical Society, the Beckman Spectrophotometer used an autocollimating quartz crystal prism for a monochromator, capable of covering a range from the ultraviolet (200 millimicrons) to the infrared (2000 millimicrons), with a nominal bandwidth of 2 millimicrons or less for most of its spectral range. The slit mechanism was continuously adjustable from .01 to 2.0 mm and claimed to have less than 1/10% of stray light over most of the spectral range. It featured an easy-to-read wavelength scale, simultaneously reporting % Transmission and Density information.\n\nThe sample holder held up to 4 cells. Cells could be moved into the light path via an external control, allowing the user to take multiple readings without opening the cell compartment. As described in the DU's manual, absorbance measurements of a sample were made in comparison to a blank, or standard, \"a solution identical in composition with the sample except that the absorbing material being measured is absent.\" The standard could be a cell filled with a solvent such as distilled water or a prepared solvent of a known concentration. At each wavelength two measurements are made: with the sample and with the standard in the light beam. This enables the ratio, transmittance, to be obtained. For quantitative measurements transmittance is converted to absorbance which is proportional to the solute concentration according to Beer's law. This makes possible the quantitative determination of the amount of a substance in solution.\n\nThe user could also switch between phototubes without removing the sample holder. A 1941 advertisement indicates that three types of phototubes were available, with maximum sensitivity to red, blue and ultraviolet light ranges.\n\nThe 1954 DU spectrophotometer differs in that it claims to be useful from 200 to 1000 millimicrons, and does not mention the ultraviolet phototube. The wavelength selector, however, still ranged from 200 to 2000 millimicrons. and an \"Ultraviolet accessory set\" was available. This shift away from using the DU for infrared measurement is understandable, since by 1954 Beckman Instruments was marketing a separate infrared spectrophotometer. Beckman developed the IR-1 infrared spectrophotometer during World War II, and redesigned it as the IR-4 between 1953 and 1956.\n\nThe Beckman spectrophotometer was the first easy-to-use single instrument containing both the optical and electronic components needed for ultraviolet-absorption spectrophotometry within a single housing. The user could insert a cell tray with standard and sample cells, dial up the desired wavelength of light, confirm that the instrument was properly set by measuring the standard, and then measure the amount of absorption of the sample, reading the frequency from a simple meter. A series of readings at different wavelengths could be taken without disturbing the sample. The DU spectrophotometer's manual scanning method was extremely fast, reducing analysis times from weeks or hours to minutes.\n\nIt was accurate in both the visible and ultraviolet ranges. \nWorking in both the ultraviolet and the visible regions of the spectrum, the model DU produced accurate absorption spectra which could be obtained with relative ease and accurately replicated. The National Bureau of Standards ran tests to certify that the DU's results were accurate and repeatable and recommended its use.\n\nOther advantages included its high resolution and the minimization of stray light in the ultraviolet region. Although it was not cheap, its initial price of $723 made it available to the average laboratory. In comparison, in 1943, the GE Hardy Spectrophotometer cost $6,400. Practical and reliable, the DU rapidly established itself as a standard for laboratory equipment.\n\nCredited with having \"brought about a breakthrough in optical spectroscopy\", the Beckman DU has been identified as \"an indispensable tool for chemistry\" and \"the Model T of laboratory instruments\". Approximately 30,000 DU spectrophotometers were manufactured and sold between 1941 and 1976.\n\nThe DU enabled researchers to perform easier analysis of substances by quickly taking measurements at more than one wavelength to produce an absorption spectrum describing the complete substance. For example, the standard method of analysis of the vitamin A content of shark liver oil, before the introduction of the DU spectrophotometer, involved feeding the oil to rats for 21 days, then cutting off the rats' tails and examining their bone structure. With the DU's UV technology, vitamin A content of shark liver oil could be determined directly in a matter of minutes.\n\nThe Scripps Research Institute and the Massachusetts Institute of Technology credit the DU with improving both accuracy and speed of chemical analysis. MIT states: \"This device forever simplified and streamlined chemical analysis, by allowing researchers to perform a 99.9% accurate quantitative measurement of a substance within minutes, as opposed to the weeks required previously for results of only 25% accuracy.\"\n\nOrganic chemist and philosopher of science Theodore L. Brown states that it \"revolutionized the measurement of light signals from samples\". Nobel laureate Bruce Merrifield is quoted as calling the DU spectrophotometer \"probably the most important instrument ever developed towards the advancement of bioscience.\" Historian of science Peter J. T. Morris identifies the introduction of the DU and other scientific instruments in the 1940s as the beginning of a Kuhnian revolution. \n\nFor the Beckman company, the DU was one of three foundational inventions – the pH meter, the DU spectrophotometer, and the helipot potentiometer – that established the company on a secure financial basis and enabled it to expand.\n\nDevelopment of the spectrophotometer had direct relevance to World War II and the American war effort. The role of vitamins in health was of significant concern, as scientists wanted to identify Vitamin A-rich foods to keep soldiers healthy. Previous methods of assessing Vitamin A levels involved feeding rats a food for several weeks and then performing a biopsy to estimate ingested Vitamin A levels. In contrast, examining a food sample with a DU spectrophotometer yielded better results in a matter of minutes. The DU spectrophotometer could be used to study both vitamin A and its precursor carotenoids, and rapidly became the preferred method of spectrophotometric analysis.\n\nThe DU spectrophotometer was also an important tool for scientists studying and producing the new wonder drug penicillin.\nThe development of penicillin was a secret national mission, involving 17 drug companies, with the goal of providing penicillin to all U.S. Forces engaged in World War II. It was known that penicillin was more effective than sulfa drugs, and that its use reduced mortality, severity of long-term wound trauma, and recovery time. However, its structure was not understood, isolation procedures used to create pure cultures were primitive, and production using known surface culture techniques was slow.\n\nAt Northern Regional Research Laboratory in Peoria, Illinois, researchers collected and examined more than 2,000 specimens of molds (as well as other microorganisms). An extensive research team included Robert Coghill, Norman Heatley, Andrew Moyer, Mary Hunt, Frank H. Stodola and Morris E. Friedkin. Friedkin recalls that an early model of the Beckman DU spectrophotometer was used by the penicillin researchers in Peoria. The Peoria lab was successful in isolating and commercially producing superior strains of the mold, which were 200 times more effective than the original forms discovered by Alexander Fleming. By the end of the war, American pharmaceutical companies were producing 650 billion units of penicillin each month. Much of the work done in this area during World War II was kept secret until after the war.\n\nThe DU spectrophotometer was also used for critical analysis of hydrocarbons. A number of hydrocarbons were of interest to the war effort. Toluene, a hydrocarbon in crude oil, was used in production of TNT for military use. Benzene and butadienes were used in the production of synthetic rubber. Rubber, used in tires for jeeps, airplanes and tanks, was in critically short supply because the United States was cut off from foreign supplies of natural rubber. The Office of Rubber Reserve organized researchers at universities and in industry to secretly work on the problem. The demand for synthetic rubber caused Beckman Instruments to develop infrared spectrophotometers. Infrared spectrophotometers were better suited than UV–Vis spectrophotometers to the analysis of C hydrocarbons, particularly for applications in petroleum refining and gasoline production.\n\nGerty Cori and her husband Carl Ferdinand Cori won the Nobel Prize in Physiology or Medicine in 1947 in recognition of their work on enzymes. They made several discoveries critical to understanding carbohydrate metabolism, including the isolation and discovery of the Cori ester, glucose 1-phosphate, and the understanding of the Cori cycle. They determined that the enzyme phosphorylase catalyzes formation of glucose 1-phosphate, which is the beginning and ending step in the conversions of glycogen into glucose and blood glucose to glycogen. Gerty Cori was also the first to show that a defect in an enzyme can be the cause of a human genetic disease. The Beckman DU spectrophotometer was used in the Cori laboratory to calculate enzyme concentrations, including phosphorylase.\n\nAnother researcher who spent six months in 1947 at the Cori laboratory, \"the most vibrant place in biochemistry\" at that time, was Arthur Kornberg. Kornberg was already familiar with the DU spectrophotometer, which he had used at Severo Ochoa's laboratory at New York University. The \"new and scarce\" Beckman DU, loaned to Ochoa by the American Philosophical Society, was highly prized and in constant use. Kornberg used it to purify aconitase, an enzyme in the citric acid cycle.\n\nKornberg and Bernard L. Horecker used the Beckman DU spectrophotometer for enzyme assays measuring NADH and NADPH. They determined their extinction coefficients, establishing a basis for quantitative measurements in reactions involving nucleotides. This work became one of the most cited papers in biochemistry. Kornberg went on to study nucleotides in DNA synthesis, isolating the first DNA polymerizing enzyme (DNA polymerase I) in 1956 and receiving the Nobel Prize in Physiology or Medicine with Severo Ochoa in 1959.\n\nThe bases of DNA absorbed ultraviolet light near 260 nm. Inspired by the work of Oswald Avery on DNA, Erwin Chargaff used a DU spectrophotometer in the 1940s in measuring the relative concentrations of bases in DNA. Based on this research, he formulated Chargaff's rules. In the first complete quantitative analysis of DNA, he reported the near-equal correspondence of pairs of bases in DNA, with the number of guanine units equaling the number of cytosine units, and the number of adenine units equaling the number of thymine units. He further demonstrated that the relative amounts of guanine, cytosine, adenine and thymine varied between species. In 1952, Chargaff met Francis Crick and James D. Watson, discussing his findings with them. Watson and Crick built upon his ideas in their determination of the structure of DNA.\n\nUltraviolet spectroscopy has wide applicability in molecular biology, particularly the study of photosynthesis. It has been used to study a wide variety of flowering plants and ferns by researchers in departments of biology, plant physiology and agriculture science as well as molecular genetics.\n\nParticularly useful in detecting conjugated double bonds, the new technology made it possible for researchers like Ralph Holman and George O. Burr to study dietary fats, work that had significant implications for human diet. The DU spectrophotometer was also used in the study of steroids by researchers like Alejandro Zaffaroni, who helped to develop the birth control pill, the nicotine patch, and corticosteroids.\n\nThe Beckman team eventually developed additional models, as well as a number of accessories or attachments which could be used to modify the DU for different types of work. One of the first accessories was a flame attachment with a more powerful photo multiplier to enable the user to examine flames such as potassium, sodium and cesium (1947).\n\nIn the 1950s, Beckman Instruments developed the DR and the DK, both of which were double-beam ultraviolet spectrophotometers. The DK was named for Wilbur I. Kaye, who developed it by modifying the DU to expand its range into the near-infrared. He did the initial work while at Tennessee Eastman Kodak, and later was hired by Beckman Instruments. The DKs introduced an automatic recording feature. The DK-1 used a non-linear scroll, and the DK-2 used a linear scroll to automatically record the spectra.\n\nThe DR incorporated a \"robot operator\" which would reset the knobs on the DU to complete a sequence of measurements at different wavelengths, just like a human operator would to generate results for a full spectrum. It used a linear shuttle with four positions, and a superstructure to change the knobs. It had a moving chart recorder to plot results, with red, green and black dots. The price of recording spectrophotometers was substantially higher than non-recording machines.\n\nThe DK was ten times faster than the DR, but not quite as accurate. It used a photomultiplier, which had introduced a source of error. The DK's speed made it preferred to the DR. Kaye eventually developed the DKU, combining infrared and ultraviolet features in one instrument, but it was more expensive than other models.\n\nThe last DU spectrophotometer was produced on July 6, 1976. By the 1980s, computers were being incorporated into scientific instruments such as Bausch & Lomb's Spectronic 2000 UV–Vis spectrophotometer, to improve data acquisition and provide instrument control. Specialized spectrophotometers designed for specific tasks now tend to be used rather than general \"all-purpose machines\" like the DU.\n"}
{"id": "10806718", "url": "https://en.wikipedia.org/wiki?curid=10806718", "title": "Data sharing", "text": "Data sharing\n\nData sharing is the practice of making data used for scholarly research available to other investigators. Replication has a long history in science. The motto of The Royal Society is 'Nullius in verba', translated \"Take no man's word for it.\" Many funding agencies, institutions, and publication venues have policies regarding data sharing because transparency and openness are considered by many to be part of the scientific method.\n\nA number of funding agencies and science journals require authors of peer-reviewed papers to share any supplemental information (raw data, statistical methods or source code) necessary to understand, develop or reproduce published research. A great deal of scientific research is not subject to data sharing requirements, and many of these policies have liberal exceptions. In the absence of any binding requirement, data sharing is at the discretion of the scientists themselves. In addition, in certain situations governments and institutions prohibit or severely limit data sharing to protect proprietary interests, national security, and subject/patient/victim confidentiality. Data sharing may also be restricted to protect institutions and scientists from use of data for political purposes.\n\nData and methods may be requested from an author years after publication. In order to encourage data sharing and prevent the loss or corruption of data, a number of funding agencies and journals established policies on data archiving. Access to publicly archived data is a recent development in the history of science made possible by technological advances in communications and information technology. To take full advantage of modern rapid communication may require consensual agreement on the criteria underlying mutual recognition of respective contributions. Models recognized for improving the timely sharing of data for more effective response to emergent infectious disease threats include the data sharing mechanism introduced by the GISAID Initiative.\n\nDespite policies on data sharing and archiving, data withholding still happens. Authors may fail to archive data or they only archive a portion of the data. Failure to archive data alone is not data withholding. When a researcher requests additional information, an author sometimes refuses to provide it. When authors withhold data like this, they run the risk of losing the trust of the science community.\n\nData sharing may also indicate the sharing of personal information on a social media platform.\n\nOn August 9, 2007, President Bush signed the America COMPETES Act (or the \"America Creating Opportunities to Meaningfully Promote Excellence in Technology, Education, and Science Act\") requiring civilian federal agencies to provide guidelines, policy and procedures, to facilitate and optimize the open exchange of data and research between agencies, the public and policymakers. See Section 1009.\n\nThe NIH Final Statement of Sharing of Research Data says:\nAllegations of misconduct in medical research carry severe consequences. The United States Department of Health and Human Services established an office to oversee investigations of allegations of misconduct, including data withholding. The website defines the mission:\n\nSome research organizations feel particularly strongly about data sharing. Stanford University's WaveLab has a philosophy about reproducible research and disclosing all algorithms and source code necessary to reproduce the research. In a paper titled \"WaveLab and Reproducible Research,\" the authors describe some of the problems they encountered in trying to reproduce their own research after a period of time. In many cases, it was so difficult they gave up the effort. These experiences are what convinced them of the importance of disclosing source code. The philosophy is described:\n\nThe Data Observation Network for Earth (DataONE) and Data Conservancy are projects supported by the National Science Foundation to encourage and facilitate data sharing among research scientists and better support meta-analysis. In environmental sciences, the research community is recognizing that major scientific advances involving integration of knowledge in and across fields will require that researchers overcome not only the technological barriers to data sharing but also the historically entrenched institutional and sociological barriers. Dr. Richard J. Hodes, director of the National Institute on Aging has stated, \"the old model in which researchers jealously guarded their data is no longer applicable\".\n\nThe Alliance for Taxpayer Access is a group of organizations that support open access to government sponsored research. The group has expressed a \"Statement of Principles\" explaining why they believe open access is important. They also list a number of international public access policies. This is no more so than in timely communication of essential information to effectively respond to health emergencies. While public domain archives have been embraced for depositing data, mainly post formal publication, they have failed to encourage rapid data sharing during health emergencies, among them the Ebola and Zika, outbreaks. More clearly defined principles are required to recognize the interests of those generating the data while permitting free, unencumbered access to and use of the data (pre-publication) for research and practical application, such as those adopted by the GISAID Initiative to counter emergent threats from influenza.\n\n\nWithholding of data has become so commonplace in academic genetics that researchers at Massachusetts General Hospital published a journal article on the subject. The study found that \"Because they were denied access to data, 28% of geneticists reported that they had been unable to confirm published research.\"\n\nIn a 2006 study, it was observed that, of 141 authors of a publication from the American Psychology Association (APA) empirical articles, 103 (73%) did not respond with their data over a 6-month period. In a follow up study published in 2015, it was found that 246 out of 394 contacted authors of papers in APA journals did not share their data upon request (62%).\n\nA 2018 study reported on study of a random sample of 48 articles published during February–May 2017 in the \"Journal of Archaeological Science\" which found openly available raw data for 18 papers (53%), with compositional and dating data being the most frequently shared types. The same study also emailed authors of articles on experiments with stone artifacts that were published during 2009 and 2015 to request data relating to the publications. They contacted the authors of 23 articles and received 15 replies, resulting in a 70% response rate. They received five responses that included data files, giving an overall sharing rate of 20%.\n\nA study of scientists in training indicated many had already experienced data withholding. This study has given rise to the fear the future generation of scientists will not abide by the established practices.\n\nRequirements for data sharing are more commonly imposed by institutions, funding agencies, and publication venues in the medical and biological sciences than in the physical sciences. Requirements vary widely regarding whether data must be shared at all, with whom the data must be shared, and who must bear the expense of data sharing.\n\nFunding agencies such as the NIH and NSF tend to require greater sharing of data, but even these requirements tend to acknowledge the concerns of patient confidentiality, costs incurred in sharing data, and the legitimacy of the request. Private interests and public agencies with national security interests (defense and law enforcement) often discourage sharing of data and methods through non-disclosure agreements.\n\nData sharing poses specific challenges in participatory monitoring initiatives, for example where forest communities collect data on local social and environmental conditions. In this case, a rights-based approach to the development of data-sharing protocols can be based on principles of free, prior and informed consent, and prioritise the protection of the rights of those who generated the data, and/or those potentially affected by data-sharing.\n\n\n — discusses the international exchange of data in the natural sciences.\n\n"}
{"id": "19089387", "url": "https://en.wikipedia.org/wiki?curid=19089387", "title": "Discover Sensors", "text": "Discover Sensors\n\nThe Discover Sensors project is an initiative of the Irish Government’s Discover Science & Engineering (DSE) to support the use of sensor technology in hands-on scientific investigations by Junior Certificate Science students. It is designed to provide students with an experience of \"real science\" in the classroom, supporting the investigative approach promoted by the revised Junior Certificate Science syllabus. DSE is managed by Forfás on behalf of the Office of Science & Technology at the Department of Jobs, Enterprise and Innovation.\n\nThe project also has a website to support teachers and students, and encourages teacher collaboration via broadband technology using the Moodle online learning support system.\n\nDiscover Sensors now has 190 participating schools throughout Ireland. If you are a Junior Certificate Science teacher, Discover Sensors can assist you by bringing new learning opportunities to your students using sensor technology.\n\nDSE runs numerous other initiatives, including Science Week Ireland, Discover Primary Science, Greenwave and Science.ie.\n"}
{"id": "159184", "url": "https://en.wikipedia.org/wiki?curid=159184", "title": "Edgar Cayce", "text": "Edgar Cayce\n\nEdgar Cayce (; March 18, 1877 – January 3, 1945) was an American clairvoyant who answered questions on subjects as varied as healing, reincarnation, wars, Atlantis, and future events while claiming to be in a trance. A biographer gave him the nickname, The Sleeping Prophet. A nonprofit organization, the Association for Research and Enlightenment,<ref name='ARE About/Mission'></ref> was founded to facilitate the study of Cayce's work.\n\nSome consider him the true founder and a principal source of the most characteristic beliefs of the New Age movement.\n\nCayce is also notable for his contributions to the notions of diet and health, particularly the issues of food combining, acid/alkaline diet, and the therapeutic use of food.\n\nEdgar Cayce was born on March 18, 1877, near Beverly, south of Hopkinsville, Kentucky. He was one of six children of farmers Carrie Elizabeth (née Major) and Leslie Burr Cayce. As a child, he played with the 'little folk' and was alleged to have seen his deceased grandfather. He regarded them all as incorporeal because he could see through them if he looked hard enough. However, he found it very difficult to keep his mind on his lessons at school.\n\nHe was taken to church when he was 10, and from then he read the Bible, becoming engrossed, and completing a dozen readings by the time he was 12. In May 1889, while reading the Bible in his hut in the woods, he 'saw' a woman with wings who told him that his prayers were answered, and asked him what he wanted most of all. He was frightened, but he said that most of all he wanted to help others, especially sick children. He decided he would like to be a missionary.\n\nThe next night, after a complaint from the school teacher, his father ruthlessly tested him for spelling, eventually knocking him out of his chair with exasperation. At that point, Cayce 'heard' the voice of the lady who had appeared the day before. She told him that if he could sleep a little 'they' could help him. He begged for a rest and put his head on the spelling book. When his father came back into the room and woke him up, he knew all the answers. In fact, he could repeat anything in the book. His father thought he had been fooling before and knocked him out of the chair again. Eventually, Cayce used all his school books that way.\n\nBy 1892, the teacher regarded Cayce as his best student. On being questioned, Cayce told the teacher that he saw pictures of the pages in the books. His father became proud of this accomplishment and spread it around, resulting in Cayce becoming \"different\" from his peers.\n\nShortly after this, Cayce exhibited an ability to diagnose in his sleep. He was struck on the base of the spine by a ball in a school game, after which he began to act very strangely, and eventually was put to bed. He went to sleep and diagnosed the cure, which his family prepared and which cured him as he slept. His father boasted that his son was, \"the greatest fellow in the world when he's asleep.\" However, this ability was not demonstrated again for several years.\n\nCayce's uncommon personality is also shown in an unusual incident in which he rode a certain mule back to the farmhouse at the end of a work day. This stunned everyone there, as the mule could not be ridden. The owner, thinking it may be time to break the animal in again, attempted to mount it but was immediately thrown off. Cayce left for his family in the city that evening.\n\nIn December 1893, the Cayce family moved to Hopkinsville, Kentucky, and occupied 705 West Seventh on the southeast corner of Seventh and Young Streets. During this time, Cayce received an eighth-grade education, is said by the Association for Research and Enlightenment to have developed psychic abilities, and left the family farm to pursue various forms of employment.\n\nCayce's education stopped in the ninth grade because his family could not afford the costs involved. A ninth-grade education was often considered more than sufficient for working-class children. Much of the remainder of Cayce's younger years would be characterized by a search for both employment and money. On March 14, 1897, Cayce became engaged to Gertrude Evans.\n\nThroughout his life, Cayce was drawn to church as a member of the Disciples of Christ. He read the Bible once a year every year, taught at Sunday school, and recruited missionaries. He said he could see auras around people, spoke to angels, and heard voices of departed relatives. In his early years, he agonized over whether these psychic abilities were spiritually delivered from the highest source.\n\nIn 1900, Cayce formed a business partnership with his father to sell Woodmen of the World Insurance; however, in March he was struck by severe laryngitis that resulted in a complete loss of speech. Unable to work, he lived at home with his parents for almost a year. He then decided to take up the trade of photography, an occupation that would exert less strain on his voice. He began an apprenticeship at the photography studio of W. R. Bowles in Hopkinsville, and eventually became quite talented in his trade.\n\nIn 1901, a traveling stage hypnotist and entertainer named Hart, who referred to himself as \"The Laugh Man\", was performing at the Hopkinsville Opera House. Hart heard about Cayce's condition and offered to attempt a cure. Cayce accepted his offer, and the experiment was conducted in the office of Manning Brown, the local throat specialist. Cayce's voice allegedly returned while in a hypnotic trance but disappeared on awakening. Hart tried a posthypnotic suggestion that the voice would continue to function after the trance, but this proved unsuccessful.\n\nSince Hart had appointments at other cities, he could not continue his hypnotic treatments of Cayce, but admitted he had failed because Cayce would not go into the third stage of hypnosis to take a suggestion. A New York hypnotist, Dr Quackenboss, found the same impediment but, after returning to New York, suggested that Cayce should be prompted to take over his own case while in the second stage of hypnosis. The only local hypnotist, Al Layne, offered to help Cayce restore his voice. Layne suggested that Cayce describe the nature of his condition and cure while in a hypnotic trance. Cayce described his own ailment from a first-person plural point of view: \"we\" instead of the singular \"I\". In subsequent sessions, when Cayce wanted to indicate that the connection was made to the \"entity\" of the person that was requesting the reading, he would generally start off with, \"We have the body.\" According to the reading for the \"entity\" of Cayce, his voice loss was due to psychological paralysis, and could be corrected by increasing the blood flow to the voice box. Layne suggested that the blood flow be increased and Cayce's face supposedly became flushed with blood, and both his chest and throat turned bright red. After 20 minutes, Cayce, still in a trance, declared the treatment over. On awakening, his voice was alleged to have remained normal. Apparently, relapses occurred, but were said to have been corrected by Layne in the same way, and eventually the cure was said to be permanent.\n\nLayne had read of similar hypnotic cures by the Marquis de Puységur, a follower of Franz Mesmer, and was keen to explore the limits of the healing knowledge involved with the trance voice. He asked Cayce to describe Layne's own ailments and suggest cures, and reportedly found the results both accurate and effective. Layne regarded the ability as clairvoyance. Layne suggested that Cayce offer his trance healing to the public. Cayce was reluctant as he had no idea what he was prescribing while asleep, and whether the remedies were safe. He also told Layne he himself did not want to know anything about the patient as it was not relevant. He finally agreed, on the condition that readings would be free. He began, with Layne's help, to offer free treatments to the townspeople. Layne described Cayce's method as, \"...a self-imposed hypnotic trance which induces clairvoyance.\" Reports of Cayce's work appeared in the newspapers, which inspired many postal inquiries. Cayce stated he could work just as effectively using a letter from the individual as with the person being present in the room. Given only the person's name and location, Cayce said he could diagnose the physical and mental conditions of what he termed \"the entity,\" and then provide a remedy. Cayce was still reticent and worried, as \"one dead patient was all he needed to become a murderer\". His fiancée, Gertrude Evans, agreed with him. Few people knew what he was up to. There was a common belief at the time that subjects of hypnosis eventually went insane, or at least that their health suffered. Cayce soon became famous, and people from around the world sought his advice through correspondence.\n\nIn May 1902 he got a bookshop job in the town of Bowling Green where he boarded with some young professionals, two of whom were doctors. He lost his voice while there and Layne came to help effect the normal cure, finally visiting every week. Cayce, still worried, kept the meetings secret, and continued to refuse money for his readings. He invented a card game called \"Pit\" or \"Board of Trade\", simulating wheat market trading, that became popular, but when he sent the idea to a game company they copyrighted it and he got no returns. He still refused to give readings for money.\n\nCayce and Gertrude Evans married on June 17, 1903, and she moved to Bowling Green with him. They had three children: Hugh Lynn Cayce (March 16, 1907 – July 4, 1982), Milton Porter Cayce (March 28, 1911 – May 17, 1911), and Edgar Evans Cayce (February 9, 1918 – February 15, 2013). She still disapproved of the readings, and Cayce still agonized over the morality of them. A few days later Layne revealed the activity to the professionals at the boarding house, one of whom was a magistrate and journalist, after which state medical authorities forced Layne to close his practice. He left to acquire osteopathic qualifications in Franklin. Cayce and Gertrude accepted the resulting publicity as best they could, greatly aided by the diplomacy of the young doctors.\n\nCayce and a relative opened a photographic studio in Bowling Green, while the doctors formed a committee with some colleagues to investigate the phenomenon, with Cayce’s co-operation. All the experiments confirmed the accuracy of the readings. However, Cayce refused a lucrative offer to go into business. After a violent examination by doctors while in a trance, Cayce refused any more investigations, declaring that he would only do readings for those who needed help and believed in the readings.\n\nIn 1906 and 1907 fires burned down his two photographic studios, leading to bankruptcy. Between the two fires, his first son was born on March 16, 1907. He became debt free by 1909, although completely broke, and ready to start again. In 1907, outstanding diagnostic successes in the family helped his confidence. He again refused an offer to go into business, this time with homeopath Wesley H. Ketchum from Hopkinsville, who was introduced by his father. He found a job at the H. P. Tresslar photography firm.\n\nHowever, Ketchum was persistent, spread information in various medical circles, and in October 1910 got written up in the press. When a reporter contacted Cayce, he explained to the reporter that he somehow had the ability to easily go into the intuitive sleep when he wanted to, and this was different from how he went to sleep normally like everyone else. When asked the mechanism of the readings via the sleep method, they were told that it happened via the capabilities of the subconscious mind.\n\nKetchum again urged Cayce to join a business company. After soul searching the whole night, Cayce finally accepted the offer under certain conditions, including that he did not take money for the readings. Instead the company was to furnish him with a photographic studio for his livelihood, and build a separate office for the readings. The contract was modified to give 50% of the earnings to Cayce and his father. Cayce read the back readings, but they contained so many technical terms that he gained no more understanding of what he was doing. He preferred to put the readings on a more scientific basis, but only the doctors in Hopkinsville would cooperate, whereas most of the patients were not in that locality. Also, doctors from all specialties were needed as the treatments prescribed varied widely.\n\nEdgar Cayce, and especially Gertrude, still did not give therapeutic priority to the readings and supposedly lost their second child due to this reticence. When Gertrude became fatally ill with tuberculosis, they used the readings after the doctor had given up. Miraculously, the treatment cured her. Shortly after this, in 1912, Cayce, whose everyday conscious mind was not aware during the readings, discovered that Ketchum had not been honest about them, and had also used them to gamble for finance. He argued in defense that the medical profession were not backing them. Cayce quit the company immediately and went back to the Tresslar photography firm in Selma, Alabama.\n\nCayce's work grew in volume as his fame grew. He asked for voluntary donations to support himself and his family so that he could practice full-time. To help raise money he invented \"Pit\", a card game based on the commodities trading at the Chicago Board of Trade, and the game is still sold today. He continued to work in an apparent trance state with a hypnotist all his life. His wife and eldest son later replaced Layne in this role. A secretary, Gladys Davis, recorded his readings in shorthand.\n\nThe growing fame of Cayce along with the popularity he received from newspapers attracted several eager commercially minded men who wanted to seek a fortune by using his clairvoyant abilities. Even though Cayce was reluctant to help them, he was persuaded to give his readings, which left him dissatisfied with himself and unsuccessful. A cotton merchant offered him a hundred dollars a day for his readings about the daily outcomes in the cotton market; however, despite his poor finances, Cayce refused the merchant's offer. Some wanted to know where to hunt for treasures while others wanted to know the outcome of horse races. Several times he was persuaded to give such readings as an experiment. However, when he used his ability for such purposes, he did no better than chance alone would dictate. These experiments allegedly left him depleted of energy, distraught, and unsatisfied with himself. Finally, he decided to use his gift only to help the distressed and sick.\n\nIn 1923, Arthur Lammers, a wealthy printer and student of metaphysics, persuaded Cayce to give readings on philosophical subjects. Cayce was told by Lammers that, while in his trance state, he spoke of Lammers' past lives and of reincarnation, something Lammers believed in. Reincarnation was a popular subject of the day but is not an accepted part of Christian doctrine. Because of this, Cayce questioned his stenographer about what he said in his trance state and remained unconvinced. He challenged Lammers' charge that he had validated astrology and reincarnation in the following dialogue:\n\nCayce's stenographer recorded the following:\n\nCayce was quite unconvinced that he had been referring to the doctrine of reincarnation, and the best Lammers could offer was that the reading \"opens up the door\" and to go on to share his beliefs and knowledge with Cayce. Lammers had come to him with quite a bit of information of his own to share with Cayce and seemed intent upon convincing Cayce now that he felt the reading had confirmed his strongly-held beliefs. It should be noted, however, that 12 years earlier Cayce had briefly alluded to reincarnation. In reading 4841-1, given April 22, 1911, Cayce referred to the soul being \"transmigrated.\" Because Cayce's readings were not systematically recorded until 1923, it is possible that he may have mentioned reincarnation in other earlier readings.\n\nLammers asked Cayce to come to Dayton to pursue metaphysical truth via the readings. Cayce eventually agreed and went to Dayton. Gertrude Cayce was dubious but interested. There, Cayce produced much metaphysical information, which Cayce tried to reconcile with Christianity. Lammers declared that the fifth chapter of Matthew was the constitution of Christianity and the Sermon on the Mount was its Declaration of Independence. It appeared that Cayce's subconscious mind was as much at home with the language of metaphysics as it was with the language of anatomy and medicine.\n\nCayce reported that his conscience bothered him severely over this conflict. His readings of reincarnations were going against his biblical teachings and at one point he wanted to cease his channeling sessions. Once again Cayce lost his voice and in a reading for himself he was informed if he was no longer going to be a channel, his mission in this life was complete. Ultimately his trance voice, the \"we\" of the readings, dialogued with Cayce and finally persuaded him to continue with these kinds of readings.\n\nLammers wanted to ask the readings the purpose of Cayce's clairvoyance, and to put up money for an organisation supporting Cayce's healing methods. Cayce decided to accept the work and asked his family to join him in Dayton as soon as they could. But by the time the Cayces had arrived there, near the end of 1923, Lammers found himself in financial difficulties and could be of no use. Cayce used his knowledge of the Bible to convince his family that it agreed with reincarnation and other metaphysical teachings.\n\nIt was at this time Cayce directed his activities to provide readings centred around health. The remedies that were channeled often involved the use of unusual electrotherapy, ultraviolet light, diet, massage, gemstones, less mental work and more relaxation in sand on the beach. His remedies were coming under the scrutiny of the American Medical Association and Cayce felt that it was time to legitimize the operations with the aid of licensed medical practitioners. In 1925 Cayce reported while in a trance, \"the voice\" had instructed him to move to Virginia Beach, Virginia across the street from the beach. He was informed that the sand's crystals would have curative properties to promote rapid healing.\n\nCayce's mature period, in which he created the several institutions that survived him, can be considered to have started in 1925. By this time he was a professional psychic with a small number of employees and volunteers. The readings increasingly came to involve occult or esoteric themes.\n\nMoney was extremely scarce, but help came from interested persons. The idea of an association and a hospital was mooted again, but the readings insisted on Virginia Beach, not suiting most of the people. Gertrude Cayce began to conduct all the readings. Morton Blumenthal, a young man who worked in the stock exchange in New York with his trader brother, became very interested in the readings, shared Cayce's outlook, and offered to finance the vision in the right spirit. He bought them a house at Virginia Beach.\n\nOn May 6, 1927, the Association of National Investigations was incorporated in the state of Virginia. This would manage building the hospital and a scientific study of the readings. Morton was president and his brother and several others were vice presidents. Cayce was secretary and treasurer, and Gladys was assistant secretary. To protect against legal prosecution, the rules required any person requesting a reading to become a member of the Association and agree they were participating in an experiment in psychic research. Early in 1928, Dr Moseley Brown, head of the psychology dept at Washington and Lee University, became convinced of the readings and joined the Association.\n\nOn October 11, 1928, the dedication ceremonies for the hospital complex were held. It contained a lecture hall, library, vault for storage of the readings, and offices for research workers. There was also a large living room, a 12-car garage, servants quarters, and a tennis court. It contained \"the largest lawn, in fact the only lawn, between the Cavalier and Cape Henry.\" The first patient was admitted the next day.\n\nThis facility would enable consistent checking and rechecking of the remedies, which was Cayce's goal. There were consistent remedies for many of the illnesses regardless of the patient, and Cayce hoped to produce a compendium that could be used by the medical profession. A distinguished chemist, Dr Sunker A. Bisley, DPhil (Oxon), who also used psychic knowledge to produce medicines, collaborated with Cayce to produce Atomidine, an absorbable form of iodine, which was perfected and sold.\n\nThe basic raison d'etre for all the cures was the \"assimilation of needed properties through the digestive system, from food taken into the body… [All treatments, including all schools and types of treatment, were given in order to establish] the proper equilibrium of the assimilating system.\" Therapies as divergent as salt packs, poultices, hot compresses, color healing, magnetism, vibrator treatment, massage, osteopathic manipulation, dental therapy, colonics, enemas, antiseptics, inhalants, homeopathics, essential oils, mud baths were prescribed. Substances used included oils, salts, herbs, iodine, witch hazel, magnesia, bismuth, alcohol, castoria, lactated pepsin, turpentine, charcoal, animated ash, soda, cream of tartar, aconite, laudanum, camphor, and gold solution. These were prescribed to overcome conditions that prevented proper digestion and assimilation of needed nutrients from the prescribed diet. The aim of the readings was to produce a healthy body, removing the cause of the specific ailment. Readings would indicate if the patient's recovery was problematic.\n\nThere was a waiting list of months ahead. \nBlumenthal and Brown went ahead with ambitious plans for a university as a supplement to the hospital and a \"parallel service for the mind and spirit\". In fact, it was to dwarf the hospital and rival other universities in respectability before psychic studies would begin. It was to open on September 22, 1930. On September 16 Blumenthal called a meeting of the Association resulting in his ownership of the hospital to curb expenses. After the first semester he ceased his support of the university, and on February 26, 1931, closed down the Association. Cayce removed the files of the readings from the hospital and took them home.\n\nThe Depression years saw Cayce turn his attention to spiritual teachings. In 1931, Edgar Cayce's friends and family asked him how they could become psychic like him. Out of this seemingly simple question came an eleven-year discourse that led to the creation of \"Study Groups\". From his altered state, Cayce relayed to this group that the purpose of life is not to become psychic, but to become a more spiritually aware and loving person. Study Group #1 was told that they could \"bring light to a waiting world\" and that these lessons would still be studied a hundred years into the future. The readings were now about dreams, coincidence (synchronicity), developing intuition, karma, the akashic records, astrology, past-life relationships, soul mates and other esoteric subjects. Hundreds of books have been published about these readings.\n\nJune 6, 1931, 61 people attended a meeting to carry on the work and form a new organization called the Association for Research and Enlightenment. In July the new association was incorporated, and Cayce legally returned the house to Blumenthal and bought another place.\n\nHugh Lynn proposed that they develop a stock in trade rather than something grandiose, and that they build a library of research into the phenomena and hold study groups, and that Cayce would do two readings a day. The association accepted this, and Hugh Lynn also started a monthly bulletin for association members. The bulletin contained readings on general interest subjects, interesting cases, book reviews on psychic subjects, health hints from readings, and news of psychic phenomena in other fields.\n\nHugh Lynn narrowed the mailing list to some 300 members who were genuinely enthusiastic, and as a result the first annual congress of the association was held in June 1932. He procured speakers on various metaphysical and psychic subjects and included public readings by Cayce. Members left the conference eager to start study groups in their own localities. Records were kept of everything that went on in the readings including the attitudes and routines of Cayce. Everything was then checked with the subjects of the readings, most of whom were not present during the reading, and the data was published in a study entitled \"100 cases of clairvoyance.\" However, the response from scientists in general was that none of the experiments were performed under test conditions. Hugh Lynn continued to build files of case histories, parallel studies in psychic phenomena, and research readings for the study groups.\n\nAssociation activities remained simple and un-publicized. Members raised a building fund for an office, library, and vault, which they erected in 1940-1 as a single unit added on to the Cayce residence. No sign guided visitors to the centre. Association membership averaged 500 to 600. The turnover from year to year was approximately half this total. The other half remained a solid basis for the research work, an audience for case studies, pamphlets, bulletins—and the Congress bulletin, which was a yearbook and record of congress events. A mailing list of several thousand served people who remained interested in Cayce's activities.\nMembers were drawn from all of the Protestant churches: from the Roman, Greek, Syrian and Armenian Catholic churches; from Theosophy, Christian Science and Spiritualism; and from many Oriental religions. Cayce's philosophy was, if it makes you a better member of your church then it's good; if it takes you away from your church, it's bad. The philosophy of the readings was that truth is one, each organization is part of this one, therefore the A.R.E. was not to function as a schism or in opposition to any religious organization. The goal of the work was not something new but something ancient and universal.\nBoth sons entered the forces during the war. They both married, Hugh Lynn in 1941 and Edgar Evans in 1942.\nIn March 1943 the first edition of the only biography written during Cayce's lifetime, by Thomas Sugrue, was published. As a consequence, public demand increased. Office staff had to be increased, and the mailman could no longer carry all the mail so Gertrude got it from the post office by car. Hugh Lynn was away in the forces, and Cayce coped with the letters and increased his readings to four to six per day.\n\nCayce gained national prominence in 1943 after the publication of a high-profile article in the magazine \"Coronet\" titled \"Miracle Man of Virginia Beach\". World War II was taking its toll on American soldiers and he felt he could not refuse the families who requested help for their loved ones who were missing in action. He increased the frequency of his readings to eight per day to try to make an impression on the ever-growing pile of requests. He said this took a toll on his health as it was emotionally draining and often fatigued him. The readings themselves scolded him for attempting too much and that he should limit his workload to just two life readings a day or else these good efforts would eventually kill him.\n\nFrom June 1943 to June 1944, 1,385 readings were taken. By August 1944 Cayce collapsed from strain. When he gave a reading on this situation, the instructions were to rest until he was well or dead. He and Gertrude went away to the mountains of Virginia, but in September Edgar Cayce suffered a stroke at the age of 67, in September 1944, and died on January 3, 1945. He is buried in Riverside Cemetery in Hopkinsville, Kentucky. Gertrude died 3 months later.\n\nAfter the death of Cayce the Association continued the work of classifying and cross referring the over 14,000 files of readings that had been taken throughout Cayce's lifetime from March 31, 1901, to September 17, 1944. The results of these have been disseminated through the Associations publications with the members as the recipients of this material.\n\nUntil September 1923, his readings were not systematically recorded or preserved. However, an article published in the \"Birmingham Post-Herald\" on October 10, 1922, quotes Cayce as saying that he had given 8,056 readings as of that date and it is known that he gave approximately 13,000–14,000 readings after that date. A total of 14,306 are available at the A.R.E. Cayce headquarters in Virginia Beach and on an online member-only section along with background information, correspondence, and follow-up documentation.\n\nOther abilities that have been attributed to Cayce include astral projection, prophesying, mediumship, viewing the Akashic records or \"Book of Life\", and seeing auras. Cayce said he became interested in learning more about these subjects after he was informed about the content of his readings, which he reported that he never actually heard himself.\n\nCayce's clients included a number of famous people such as Woodrow Wilson, Thomas Edison, Irving Berlin, and George Gershwin.\n\nGina Cerminara published books such as \"Many Mansions\" and \"The World Within\". Brian Weiss published a bestseller regarding clinical recollection of past lives, \"Many Lives, Many Masters\". These books provide broad support for spiritualism and reincarnation. \"Many Mansions\" elaborates on Cayce's work and supports his stated abilities with real life examples.\n\nIn 1971 Edgar Cayce's sons Edgar Evans Cayce and Hugh Lynn Cayce published a book titled \"The Outer Limits of Edgar Cayce's Power\", claiming Cayce's readings had an approximate 85% success rate. The majority of the book investigated cases where Cayce's readings were demonstrably incorrect.\n\nKetchum was a physician who worked with Cayce in the early 1900s. Ketchum himself was born in Lisbon, Ohio on November 11, 1878, to Saunders C. Ketchum and Bertha Bennett, and was the oldest of 7 children. He graduated from the Cleveland College of Homeopathic Medicine in 1904, and took up the practice of medicine in Hopkinsville Kentucky. He practiced medicine in Hopkinsville until 1912. In 1913 he traveled across country to San Francisco, and took passage to Honolulu, Hawaii, where he opened a new practice. He returned to California in 1918, and established an office in Palo Alto California, practicing medicine there until the 1950s. He retired to Southern California around 1963, settling in San Marino, just outside Pasadena. He died on November 28, 1968, in Canoga Park, California.\n\nHe wrote \"The Discovery of Edgar Cayce\", published by the A.R.E. Press in 1964.\n\nCayce advocated some controversial ideas in his trance readings. In many trance sessions, he interpreted the history of life on Earth. One of Cayce's controversial claims was that of polygenism. According to Cayce, five human races (white, black, red, brown, and yellow) had been created separately but simultaneously on different parts of the Earth. Cayce also accepted the existence of aliens and Atlantis, and claimed that \"the red race developed in Atlantis and its development was rapid.\" Another claim by Cayce was that \"soul-entities\" on Earth intermingled with animals to produce \"things\": giants that were as much as twelve feet tall.\n\nHistorian Olav Hammer wrote that many of Cayce's readings discussed race and skin color and that the explanation for this is that Cayce was not a racist but was influenced by the occult ideas of Madame Blavatsky. Cayce declared that the Piltdown man was genuine, claiming he was an Atlantean colonizer who had travelled to Britain. However, the Piltdown man was exposed as a hoax in 1953.\n\nPhilosopher and skeptic Robert Todd Carroll, in his book \"The Skeptic's Dictionary\", wrote, \"Cayce is one of the main people responsible for some of the sillier notions about Atlantis.\" Carroll mentioned some of Cayce's ideas, including his belief in a giant solar crystal, activated by the sun, and used to harness energy and provide power on Atlantis, and his prediction that in 1958, the United States would rediscover a death ray that had been used on Atlantis.\n\nIn 1930s, Cayce also incorrectly predicted that North America would experience chaos: \"Los Angeles, San Francisco... will be among those that will be destroyed before New York\". These events were to have happened \"in the period of '58 to '98\".\n\nSkeptics challenge Cayce's alleged psychic abilities. Medical health experts are critical of Cayce's unorthodox treatments, which they regard as quackery.\n\nScience writers and skeptics have suggested that the evidence for Cayce's alleged psychic powers comes from contemporaneous newspaper articles, affidavits, anecdotes, testimonials, and books. Martin Gardner, for example, wrote that while Cayce's trances did happen, most of the information from his trances was derived from books that Cayce had been reading by authors such as Carl Jung, P. D. Ouspensky, and Helena Blavatsky. Gardner's hypothesis was that the trance readings of Cayce contain, \"little bits of information gleaned from here and there in the occult literature, spiced with occasional novelties from Cayce's unconscious.\"\n\nMichael Shermer writes in \"Why People Believe Weird Things\", \"Uneducated beyond the ninth grade, Cayce acquired his broad knowledge through voracious reading and from this he wove elaborate tales.\" Shermer wrote that, \"Cayce was fantasy-prone from his youth, often talking with angels and receiving visions of his dead grandfather.\" Magician James Randi has said that \"Cayce was fond of expressions like 'I feel that' and 'perhaps'—qualifying words used to avoid positive declarations.\" Examination of the readings do not show qualifying terms.\n\nInvestigator Joe Nickell has noted:\n\nAlthough Cayce was never subjected to proper testing, ESP pioneer Joseph B. Rhine of Duke University — who should have been sympathetic to Cayce's claims — was unimpressed. A reading that Cayce gave for Rhine's daughter was notably inaccurate. Frequently, Cayce was even wider off the mark, as when he provided diagnoses of subjects \"who had died\" since the letters requesting the readings were sent.\n\nScience writer Karen Stollznow has written:\n\nTraditional Christians are critical of Cayce's views on issues such as reincarnation, oneness, and the Akashic records.\n\n\n\n"}
{"id": "12891588", "url": "https://en.wikipedia.org/wiki?curid=12891588", "title": "Experimental design diagram", "text": "Experimental design diagram\n\nExperimental Design Diagram (EDD) is a diagram used in science classrooms to design an experiment. This diagram helps to identify the essential components of an experiment. It includes a title, the research hypothesis and null hypothesis, the independent variable, the levels of the independent variable, the number of trials, the dependent variable, the operational definition of the dependent variable and the constants.\n\n[science way of controlling the data]\n"}
{"id": "533106", "url": "https://en.wikipedia.org/wiki?curid=533106", "title": "Experimentum crucis", "text": "Experimentum crucis\n\nIn the sciences, an experimentum crucis (English: crucial experiment or critical experiment) is an experiment capable of decisively determining whether or not a particular hypothesis or theory is superior to all other hypotheses or theories whose acceptance is currently widespread in the scientific community. In particular, such an experiment must typically be able to produce a result that rules out all other hypotheses or theories if true, thereby demonstrating that under the conditions of the experiment (\"i.e.\", under the same external circumstances and for the same \"input variables\" within the experiment), those hypotheses and theories are \"proven false\" but the experimenter's hypothesis \"is not ruled out\".\n\nFor an opposite view putting into question the decisive value of the experimentum crucis in choosing one hypothesis or theory over its rival see Pierre Duhem.\n\nFrancis Bacon in his \"Novum Organum\" first described the concept of a situation in which one theory but not others would hold true, using the name instantia crucis; the phrase \"experimentum crucis\", denoting the deliberate creation of such a situation for the purpose of testing the rival theories, was later coined by Robert Hooke and then famously used by Isaac Newton.\n\nThe production of such an experiment is considered necessary for a particular hypothesis or theory to be considered an established part of the body of scientific knowledge. It is not unusual in the history of science for theories to be developed fully before producing a critical experiment. A given theory which is in accordance with known experiment but which has not yet produced a critical experiment is typically considered worthy of exploration in order to discover such an experimental test.\n\nIn his \"Philosophiæ Naturalis Principia Mathematica\", Isaac Newton (1687) presents a disproof of Descartes' vortex theory of the motion of the planets. In his \"Opticks\", Newton describes an optical \"experimentum crucis\" in the \"First Book, Part I, Proposition II, Theorem II, Experiment 6\", to prove that sunlight consists of rays that differ in their index of refraction.\n\nA 19th-century example was the prediction by Poisson, based on Fresnel's mathematical analysis, that the wave theory of light predicted a bright spot in the center of the shadow of a perfectly circular object, a result that could not be explained by the (then current) particle theory of light. An experiment by François Arago showed the existence of this effect, now called the Arago spot, or \"Poisson's bright spot\", which led to the acceptance of the wave theory.\n\nA famous example in the 20th century of an \"experimentum crucis\" was the expedition led by Arthur Eddington to Principe Island in Africa in 1919 to record the positions of stars around the Sun during a solar eclipse. The observation of star positions confirmed predictions of gravitational lensing made by Albert Einstein in the general theory of relativity published in 1915. Eddington's observations were considered to be the first solid evidence in favor of Einstein's theory.\n\nIn some cases, a proposed theory can account for existing anomalous experimental results for which no other existing theory can furnish an explanation. An example would be the ability of the quantum hypothesis, proposed by Max Planck in 1900, to account for the observed black-body spectrum, an experimental result that the existing classical Rayleigh–Jeans law could not predict. Such cases are not considered strong enough to fully establish a new theory, however, and in the case of quantum mechanics, it took the confirmation of the theory through \"new\" predictions for the theory to gain full acceptance.\n\n"}
{"id": "48854038", "url": "https://en.wikipedia.org/wiki?curid=48854038", "title": "FONDECYT", "text": "FONDECYT\n\nThe National Fund for Scientific and Technological Development (Spanish: \"Fondo Nacional de Desarrollo Científico y Tecnológico\"), abbreviated FONDECYT, is the main public fund of the Government of Chile to promote basic scientific and technological research in all areas of knowledge. It is managed by the National Commission for Scientific and Technological Research (CONICYT).\n\nThe fund was established in 1981 and began operations in 1982. Since then it has funded more than 16,000 research projects. In 1991 CONICYT opened a second fund, FONDEF (The Science & Technology Development Fund), with the aim of promoting public–private collaboration in research and development.\n\nThe program provides financial support for projects selected under three headings:\n\n"}
{"id": "1110558", "url": "https://en.wikipedia.org/wiki?curid=1110558", "title": "Five Billion Years of Change", "text": "Five Billion Years of Change\n\nFive Billion Years of Change: A History of the Land (2003, ) is a book by Denis Wood that attempts a holistic view of reality that ranges from the Big Bang to the World Wide Web. Specifically, this books deals with the formation of various structures:\n\nA key theme is repeated through this book: humans have a tendency to divide our understandings into \"history\" and \"prehistory\". People are shocked when some event from prehistory intrudes upon their current lives; Wood likens the shock of this intrusion to an expulsion from the Garden of Eden. This division is a metaphor for various artificial divisions; for example:\nInstead of thinking in terms of artificial divisions of \"now\" and \"back then\", readers should develop an intuitive mindset of graduated changes in which the \"fossils\" of the ancient past are intermingled with contemporary objects; for instance, the \"oxygen holocaust\" of the paleoproterozoic eon exists in today's oxygen-rich atmosphere.\n\nAlso, the heroic saga induces another faulty thinking style that obscures the true nature of the world. To understand the real story of humanity, Wood argues that people must focus on the mass actions of people or of large impersonal forces rather than a few heroes or kings. Hollywood movies dealing with ecological threats are especially misleading; rather than imparting an accurate image of ecological issues, movies present a villain such as a mad scientist or a greedy, evil business person. Instead, such entertainment and much news reporting distracts us from our individual actions that are at the heart of ecological problems.\n\nInaccurate ways of thinking induce a passive helplessness. Instead, by presenting a sweeping story of successive, interlinked, long term trends, the author hopes to give readers a flexible, authentic model of the world. With that model, readers will be capable of understanding (and possibly dealing with) current global challenges.\n"}
{"id": "19667544", "url": "https://en.wikipedia.org/wiki?curid=19667544", "title": "Functional medicine", "text": "Functional medicine\n\nFunctional medicine is a form of alternative medicine which proponents say focuses on interactions between the environment and the gastrointestinal, endocrine, and immune systems, but opponents have described it as \"pseudoscientific silliness\" and quackery. Practitioners develop individual treatment plans for people they treat. Functional medicine encompasses a number of unproven and disproven methods and treatments.\n\nThe discipline of functional medicine is vaguely defined by its proponents. Oncologist David Gorski has written that the vagueness is a deliberate tactic which facilitates the discipline's promotion, but that in general it centers on unnecessary and expensive testing procedures performed in the name of \"holistic\" health care.\n\nProponents of functional medicine oppose established medical knowledge and reject its models, instead adopting a model of disease based on the notion of \"antecedents\", \"triggers\", and \"mediators\". These are meant to correspond to the underlying causes, the immediate causes, and the particular characteristics of a person's illness respectively. A functional medicine practitioner will devise a \"matrix\" from these things which acts as a basis for treatment.\n\nTreatments, practices, and concepts will generally be those not supported by medical evidence. These include:\n\nFunctional medicine was invented by nutritionist Jeffrey Bland. He and Susan Bland founded the \"Institute for Functional Medicine\" in 1991 as a division of HealthComm. That year, the U.S. Federal Trade Commission said that Jeffrey Bland's corporations HealthComm and Nu-Day Enterprises had falsely advanced claims that their products could alter metabolism and induce weight loss. The FTC found that Bland and his companies violated that consent order in 1995 by making more exaggerated claims. The UltraClear dietary program was said to provide relief from gastrointestinal problems, inflammatory and immunologic problems, fatigue, food allergies, mercury exposure, kidney disorders, and rheumatoid arthritis. The companies were forced to pay a $45,000 civil penalty.\n\nThe opening of centers for functional medicine at the Cleveland Clinic Foundation and at George Washington University has been described by Gorski as an \"unfortunate\" example of pseudoscientific quackery infiltrating medical academia.\n"}
{"id": "11415165", "url": "https://en.wikipedia.org/wiki?curid=11415165", "title": "Goodman fatigue equation", "text": "Goodman fatigue equation\n\nThe Goodman fatigue equation is used by engineers for fatigue analysis. The equation is used to make correlations of experimental fatigue data of metals and other materials.\n\nThe equation is also used to determine the failure mechanisms of sucker rods in pumping oil wells worldwide, and to help design new sucker rod strings for rod-pumped oil wells.\n\nFrom similar triangles the Goodman fatigue equation is :\n\n( 4a) τm /Sus + τa /Ses = 1/n in which the stress components are given by \n\n"}
{"id": "204048", "url": "https://en.wikipedia.org/wiki?curid=204048", "title": "Huff and puff apparatus", "text": "Huff and puff apparatus\n\nThe huff and puff apparatus is used in school biology labs to demonstrate that carbon dioxide is a product of respiration. A pupil breathes in and out of the middle tube. The glass tubing is arranged in such a way that one flask bubbles as the pupils breathes in, the other as the pupil breathes out. A suitable carbon dioxide indicator, such as limewater or bicarbonate indicator shows the increased presence of carbon dioxide in the outgoing breath.This turns the bicarbonate into milky white substance.\n\n"}
{"id": "386398", "url": "https://en.wikipedia.org/wiki?curid=386398", "title": "Hypothetico-deductive model", "text": "Hypothetico-deductive model\n\nThe hypothetico-deductive model or method is a proposed description of scientific method. According to it, scientific inquiry proceeds by formulating a hypothesis in a form that can be falsifiable, using a test on observable data where the outcome is not yet known. A test outcome that could have and does run contrary to predictions of the hypothesis is taken as a falsification of the hypothesis. A test outcome that could have, but does not run contrary to the hypothesis corroborates the theory. It is then proposed to compare the explanatory value of competing hypotheses by testing how stringently they are corroborated by their predictions.\n\nOne example of an algorithmic statement of the hypothetico-deductive method is as follows:\n\nOne possible sequence in this model would be 1, 2, 3, 4. If the outcome of 4 holds, and 3 is not yet disproven, you may continue with 3, 4, 1, and so forth; but if the outcome of 4 shows 3 to be false, you will have to go back to 2 and try to invent a new 2, deduce a new 3, look for 4, and so forth.\n\nNote that this method can never absolutely verify (prove the truth of) 2. It can only falsify 2. (This is what Einstein meant when he said, \"No amount of experimentation can ever prove me right; a single experiment can prove me wrong.\")\n\nAdditionally, as pointed out by Carl Hempel (1905–1997), this simple view of the scientific method is incomplete; a conjecture can also incorporate probabilities, e.g., the drug is effective about 70% of the time. Tests, in this case, must be repeated to substantiate the conjecture (in particular, the probabilities). In this and other cases, we can quantify a probability for our confidence in the conjecture itself and then apply a Bayesian analysis, with each experimental result shifting the probability either up or down. Bayes' theorem shows that the probability will never reach exactly 0 or 100% (no absolute certainty in either direction), but it can still get very close to either extreme. See also confirmation holism.\n\nQualification of corroborating evidence is sometimes raised as philosophically problematic. The raven paradox is a famous example. The hypothesis that 'all ravens are black' would appear to be corroborated by observations of only black ravens. However, 'all ravens are black' is logically equivalent to 'all non-black things are non-ravens' (this is the contraposition form of the original implication). 'This is a green tree' is an observation of a non-black thing that is a non-raven and therefore corroborates 'all non-black things are non-ravens'. It appears to follow that the observation 'this is a green tree' is corroborating evidence for the hypothesis 'all ravens are black'. Attempted resolutions may distinguish:\n\nEvidence contrary to a hypothesis is itself philosophically problematic. Such evidence is called a falsification of the hypothesis. However, under the theory of confirmation holism it is always possible to save a given hypothesis from falsification. This is so because any falsifying observation is embedded in a theoretical background, which can be modified in order to save the hypothesis. Popper acknowledged this but maintained that a critical approach respecting methodological rules that avoided such \"immunizing stratagems\" is conducive to the progress of science.\n\nPhysicist Sean Carroll claims the model ignores underdetermination.\nThe hypothetico-deductive model (or approach) versus other research models\n\nThe hypothetico-deductive approach contrasts with other research models such as the inductive approach or grounded theory. In the data percolation methodology, \nthe hypothetico-deductive approach is included in a paradigm of pragmatism by which four types of relations between the variables can exist: descriptive, of influence, longitudinal or causal. The variables are classified in two groups, structural and functional, a classification that drives the formulation of hypotheses and the statistical tests to be performed on the data so as to increase the efficiency of the research. \n\n"}
{"id": "47280", "url": "https://en.wikipedia.org/wiki?curid=47280", "title": "Infrastructure bias", "text": "Infrastructure bias\n\nIn economics and social policy, infrastructure bias is the influence of the location and availability of pre-existing infrastructure, such as roads and telecommunications facilities, on social and economic development.\n\nIn science, infrastructure bias is the influence of existing social or scientific infrastructure on scientific observations.\n\nIn astronomy and particle physics, where the availability of particular kinds of telescopes or particle accelerators acts as a constraint on the types of experiments that can be done, the data that can be retrieved is biased towards that which can be obtained by the equipment.\n\nProcedural bias, related to infrastructure bias, is shown by a case of irregular genetic sampling of Bolivian wild potatoes. A 2000 report of previous studies' sampling found that 60% of samples had been taken near towns or roads, where 22% would be the average, had the samples been taken at random (or from equidistant points, or at specifically varying distances from towns, representative of the average terrain density).\n"}
{"id": "33298861", "url": "https://en.wikipedia.org/wiki?curid=33298861", "title": "Iron Science Teacher", "text": "Iron Science Teacher\n\nThe Iron Science Teacher is a national competition that celebrates innovation and creativity in science teaching. The competition originated at the Exploratorium in San Francisco. Parodying the cult Japanese TV program, “Iron Chef,” this competition showcases science teachers as they devise classroom activities using a particular ingredient — an everyday item such as a plastic bag, milk carton, or nail. Contestants are currently or formally part of the Exploratorium's Teacher Institute and compete before a live audience for the title of \"Iron Science Teacher.\" Shows are also archived on the Exploratorium's site.\n\nAstrophysicist Dr. Linda Shore, Director of the Exploratorium Teacher Institute and host of the competition, says one goal of the Iron Science Teacher is to \"provide teachers with ideas about how to teach multimillion dollar state and national science teaching standards using, trash, recyclables, and inexpensive materials\" as well as \"to allow teachers to receive applause for great teaching.\"\n\nBack in 1997, the Exploratorium's Phyllis C. Wattis Webcast Studio was looking for new shows. During a staff brainstorming session, a fan of the popular Food Network television show, The Iron Chef, suggested naming a secret ingredient for science teachers to use in an experiment to present to the audience. \"It was honestly and truly a joke,\" Shore says. \"We thought we'd do one show.\"\n\nNow 10 to 12 shows are produced annually for the Exploratorium's website. \"Secret\" ingredients, which are revealed in advance to participants so they can practice, have included everything from ordinary baking soda and food coloring to Marshmallow Peeps and pantyhose.\n\nThe Canadian Iron Science Teacher also parodies the popular TV series Iron Chef and is hosted by Jay Ingram of Daily Planet on Discovery Channel. Unlike the Exploratorium version, where championship comes with no tangible prize, in the Canadian version, five \"finalist\" teachers, with their support teams, are selected to compete in the Iron Science Teacher finals at the University of Calgary in order to win a variety of cash prizes. \n\nColorado Springs, CO initiated their own CoOL Iron Science Teacher Competition as part of their What If: A Festival of Creativity & Innovation on September 11, 2010.\n\n\"(plus links to the webcasts)\"\n"}
{"id": "28548481", "url": "https://en.wikipedia.org/wiki?curid=28548481", "title": "List of esoteric healing articles", "text": "List of esoteric healing articles\n\nEsoteric healing refers to numerous types of alternative therapy which aim to heal disease and disability, using esoteric means, either through faith and human will, or by using pseudoscientific processes. It was first published in the 1950s and was initially inspired by Djwhal Khul and Alice Bailey.\n\n\n"}
{"id": "68140", "url": "https://en.wikipedia.org/wiki?curid=68140", "title": "List of life sciences", "text": "List of life sciences\n\nThe life sciences or biological sciences comprise the branches of science that involve the scientific study of life and organisms – such as microorganisms, plants, and animals including human beings. \n\nLife science is one of the two major branches of natural science, the other being physical science, which is concerned with non-living matter. \n\nBy definition, biology is the natural science that studies life and living organisms, with the other life sciences being its sub-disciplines. \n\nSome life sciences focus on a specific type of organism. For example, zoology is the study of animals, while botany is the study of plants. Other life sciences focus on aspects common to all or many life forms, such as anatomy and genetics. Some focus on the micro scale (e.g. molecular biology, biochemistry) other on larger scales (e.g. cytology, immunology, ethology, ecology). Another major, branch of life sciences involves understanding the mindneuroscience.\n\nLife sciences discoveries are helpful in improving the quality and standard of life, and have applications in health, agriculture, medicine, and the pharmaceutical and food science industries.\n\nBiology – burst and eclectic field, composed of many branches and sub-disciplines. However, despite the complexity and the broad scope of the science, there are certain general and unifying concepts within it that govern all study and research, consolidating it into a single, coherent field. Here are some of biology's major branches:\n\n\n\n"}
{"id": "4647220", "url": "https://en.wikipedia.org/wiki?curid=4647220", "title": "List of multiple discoveries", "text": "List of multiple discoveries\n\nHistorians and sociologists have remarked the occurrence, in science, of \"multiple independent discovery\". Robert K. Merton defined such \"multiples\" as instances in which similar discoveries are made by scientists working independently of each other. \"Sometimes,\" writes Merton, \"the discoveries are simultaneous or almost so; sometimes a scientist will make a new discovery which, unknown to him, somebody else has made years before.\"\n\nCommonly cited examples of multiple independent discovery are the 17th-century independent formulation of calculus by Isaac Newton, Gottfried Wilhelm Leibniz and others, described by A. Rupert Hall; the 18th-century discovery of oxygen by Carl Wilhelm Scheele, Joseph Priestley, Antoine Lavoisier and others; and the theory of the evolution of species, independently advanced in the 19th century by Charles Darwin and Alfred Russel Wallace.\n\nMultiple independent discovery, however, is not limited to only a few historic instances involving giants of scientific research. Merton believed that it is multiple discoveries, rather than unique ones, that represent the \"common\" pattern in science.\n\nMerton contrasted a \"multiple\" with a \"singleton\"—a discovery that has been made uniquely by a single scientist or group of scientists working together.\n\nA distinction is drawn between a discovery and an invention, as discussed for example by Bolesław Prus. However, since the same phenomenon of multiplicity occurs in relation to both discoveries and inventions, this article lists both multiple discoveries and multiple \"inventions\".\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "42335532", "url": "https://en.wikipedia.org/wiki?curid=42335532", "title": "List of regimes", "text": "List of regimes\n\nThis list of regimes lists the results of regime-classification schemes from political science literature, including Polity data series and the Democracy-Dictatorship Index.\n\n"}
{"id": "56023027", "url": "https://en.wikipedia.org/wiki?curid=56023027", "title": "List of scientific misconduct incidents", "text": "List of scientific misconduct incidents\n\nScientific misconduct is the violation of the standard codes of scholarly conduct and ethical behavior in the publication of professional scientific research. A \"Lancet\" review on \"Handling of Scientific Misconduct in Scandinavian countries\" gave examples of policy definitions. In Denmark, scientific misconduct is defined as \"intention[al] or gross negligence leading to fabrication of the scientific message or a false credit or emphasis given to a scientist\", and in Sweden as \"intention[al] distortion of the research process by fabrication of data, text, hypothesis, or methods from another researcher's manuscript form or publication; or distortion of the research process in other ways.\"\n\nA 2009 systematic review and meta-analysis of survey data found that about 2% of scientists admitted to falsifying, fabricating, or modifying data at least once.\n\n\n\n\n\n\n"}
{"id": "31149053", "url": "https://en.wikipedia.org/wiki?curid=31149053", "title": "Literature-based discovery", "text": "Literature-based discovery\n\nLiterature-based discovery refers to the use of papers and other academic publications (the \"literature\") to find new relationships between existing knowledge (the \"discovery\"). The technique was pioneered by Don R. Swanson in the 1980s and has since seen widespread use. \n\nLiterature-based discovery does not generate new knowledge through laboratory experiments, as is customary for empirical sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and \"neglected\". It is marked by empiricism and rationalism in concert or consilience.\n\n\"Swanson linking\" is a term proposed in 2003 that refers to connecting two pieces of knowledge previously thought to be unrelated. For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called \"disjoint data\"), the relationship between illness A and drug C may be unknown. \"Swanson linking\" aims to find these relationships and report them.\n\n\n\n\n"}
{"id": "9440754", "url": "https://en.wikipedia.org/wiki?curid=9440754", "title": "Lithoprobe", "text": "Lithoprobe\n\nLithoprobe is a Canadian national geoscience research project funded by the Natural Sciences and Engineering Research Council. Its aim is to research and map the lithosphere structure and composition. Lithoprobe derives from \"probing the lithosphere\".\n\n\n"}
{"id": "15522138", "url": "https://en.wikipedia.org/wiki?curid=15522138", "title": "Marine counterparts of land creatures", "text": "Marine counterparts of land creatures\n\nThe idea that there are specific marine counterparts to land creatures, inherited from the writers on natural history in Antiquity, was firmly believed in Islam and in Medieval Europe, and is exemplified by the creatures represented in the medieval animal encyclopedias called bestiaries and in the parallels drawn in the moralising attributes attached to each. \"The creation was a mathematical diagram drawn in parallel lines,\" T. H. White said a propos the bestiary he translated. \"Things did not only have a moral they often had physical counterparts in other strata. There was a horse in the land and a sea-horse in the sea. For that matter there was probably a Pegasus in heaven\". The idea of perfect analogies in the fauna of land and sea was considered part of the perfect symmetry of the Creator's plan, offered as the \"book of nature\" to mankind, for which a text could be found in \"Job\":\nBut ask the animals, and they will teach you, or the birds of the air, and they will tell you; or speak to the earth, and it will teach you, or let the fish of the sea inform you. Which of all these does not know that the hand of the Lord has done this? In his hand is the life of every creature and the breath of all mankind.\n\nThe idea appears in the Jewish Tannaic sources as well, as brought down in Babylonian Talmud, Chulin 127a. Rashi (Psalms 49:2) traces this to a biblical source – the land is referred to as \"Chaled\", from the weasel (chulda), because the weasel is the only animal on dry land that does not have its counterpart in the sea.\nAll of Creation was considered to reflect the Creator, and Man could learn about the Creator through studying the Creation, an assumption that underlies the \"watchmaker analogy\" offered as a proof of God's existence.\n\nThe correspondence between the realms of earth and sea, extending to its denizens, offers examples of the taste for allegory engendered by Christian and Islamic methods of exegesis, which also encouraged the doctrine of signatures, a \"key\" to the meaning and use of herbs.\n\nThe source text that was most influential in compiling the bestiaries of the 12th and 13th centuries was the \"Physiologus\", one of the most widely read and copied secular texts of the Middle Ages. Written in Greek in Alexandria the 2nd century CE and accumulating further \"exemplary\" beasts in the next three centuries and more, \"Physiologus\" was transmitted in the West in Latin, and eventually translated into many vernacular languages: many manuscripts in various languages survive.\nAelian, \"On the Characteristics of Animals\" (A. F. Scholfield, in Loeb Classical Library, 1958).\n\nChristian writers, trained in anagogical thinking and expecting to find spiritual instruction inherent in the processes of Nature, disregarded the caveat in Pliny's Natural History, where the idea is presented as a \"vulgar opinion\": \n\nHence it is that the vulgar notion may very possibly be true, that whatever is produced in any other department of Nature, is to be found in the sea as well; while, at the same time, many other productions are there to be found which nowhere else exist. That there are to be found in the sea the forms, not only of terrestrial animals, but of inanimate objects even, is easily to be understood by all who will take the trouble to examine the grape-fish, the sword-fish, the sawfish, and the cucumber-fish, which last so strongly resembles the real cucumber both in colour and in smell.\nPliny points out that many more things are found in the sea than on the land, and also mentions the correspondences that may be discovered between many non-living objects of the land and living creatures in the sea. \n\nSaint Augustine of Hippo reasons based on analogy, that since there is a serpent in the grass, there must be an eel in the sea; because there is a Leviathan in the sea, there must be a Behemoth on the land. (\"City of God\"? xi.15?)\nThe reaction to such anagogical thinking set in with the unfolding of critical scientific thought in the 17th century. Sir Thomas Browne devoted a chapter of his \"Pseudodoxia Epidemica\" to dispelling such a belief: Chapter XXIV: \"That all Animals in the land are in their kinde in the Sea.\" During the Enlightenment the ancient conception was given an innovative and rationalized cast by Benoît de Maillet in describing the transformations and metamorphoses undergone by creatures of the sea to render them fit for life on land, a proto-evolutionist concept, though it was based on superficial morphological similarities:\nThere are in the Sea, Fish of almost all the Figures of Land-Animals, and even of Birds. She includes Plants, Flowers, and some Fruits; the Nettle, the Rose, the Pink, the Melon and the Grape, are to be found there.<br>\n<br>\nAs for the Quadrupeds, we not only find in the Sea, Species of the same Figure and Inclinations, and in the Waves living on the same Aliments by which they are nourished on Land, we have also Examples of those Species living equally in the Air and in the Water. Have not the Sea-Apes precisely the same figure with those of the Land?\n\nThough in \"Moby-Dick\" Ishmael, with a nod to Sir Thomas Browne's wording, denies the claim that land animals find their counterparts in the sea,For though some old naturalists have maintained that all creatures of the land are of their kind in the sea; and though taking a broad general view of the thing, this may very well be; yet coming to specialties, where, for example, does the ocean furnish any fish that in disposition answers to the sagacious kindness of the dog? The accursed shark alone can in any generic respect be said to bear comparative analogy to him.\nin discussing dolphins trained to aid scuba divers, a 1967 \"Popular Mechanics\" article could still casually state: \"It's hoped that the marine counterparts of some land animals can be trained to become useful members of the Man-in-the-Sea program.\"\n"}
{"id": "7916214", "url": "https://en.wikipedia.org/wiki?curid=7916214", "title": "Methodical culturalism", "text": "Methodical culturalism\n\nMethodical culturalism is a philosophical approach developed by Peter Janich and his pupils. The core statement of this approach is that science is not developed from purely theoretical considerations, but as a development of everyday, proto-scientific human behavior. That is to say, science is a stylized form of everyday knowledge-forming practice.\n\nThus, from the viewpoint of methodical culturalism, science is understood as a continuation of the practical processes of the everyday world and must be analyzed from this aspect systematically and methodically.\n\nMethodical culturalism is a development of the methodical constructivism of the Erlanger school (known as \"Erlanger constructivism\"). \n\n\n"}
{"id": "539355", "url": "https://en.wikipedia.org/wiki?curid=539355", "title": "Microbiologist", "text": "Microbiologist\n\nA microbiologist (from Greek ) is a scientist who studies microscopic life forms and processes. This includes study of the growth, interactions and characteristics of microscopic organisms such as bacteria, algae, fungi, and some types of parasites and their vectors. Most microbiologists work in offices and/or research facilities, both in private biotechnology companies as well as in academia. Most microbiologists specialize in a given topic within microbiology such as bacteriology, parasitology, virology, or immunology.\n\nMicrobiologists generally work in some way to increase scientific knowledge, or to utilize that knowledge in a way that improves outcomes in medicine or some industry. For many microbiologists, this work includes planning and conducting experimental research projects in some kind of laboratory setting. Others may have a more administrative role, supervising scientists and evaluating their results. Microbiologists working in the medical field, such as clinical microbiologists, may see patients or patient samples and do various tests to detect disease-causing organisms.\n\nFor microbiologists working in academia, duties include performing research in an academic laboratory, writing grant proposals to fund research, as well as some amount of teaching and designing courses. Microbiologists in industry roles may have similar duties except research is performed in industrial labs in order to develop or improve commercial products and processes. Industry jobs may also include some degree of sales and marketing work, as well as regulatory compliance duties. Microbiologists working in government may have a variety of duties, including laboratory research, writing and advising, developing and reviewing regulatory processes, and overseeing grants offered to outside institutions. Some microbiologists work in the field of patent law, either with national patent offices or private law practices. Here duties include research and navigation of intellectual property regulations. Clinical microbiologists tend to work in government or hospital laboratories where their duties include analyzing clinical specimens to detect microorganisms responsible for disease. Some microbiologists instead work in the field of science outreach, where they develop programs and material to educate students and non-scientists and stimulate interest in the field of microbiology.\n\nEntry-level microbiology jobs generally require at least a bachelor's degree in microbiology or a related field. These degree programs frequently include courses in chemistry, physics, statistics, biochemistry, and genetics, followed by more specialized courses in sub-fields of interest. Many of these courses have laboratory components to teach trainees basic and specialized laboratory skills.\n\nHigher-level and independent jobs generally require a Ph.D. as well as several years experience as a microbiologist. This often includes time spent as a postdoctoral researcher wherein one leads research projects and prepares to transition to an independent career. Postdoctoral researchers are often evaluated largely based on their record of published academic papers, as well as recommendations from their supervisors and colleagues.\n\nIn certain sub-fields of microbiology, licenses or certifications are available or required in order to qualify for certain positions. This is true for clinical microbiologists, as well as those involved in food safety and some aspects of pharmaceutical/medical device development.\n\nMicrobiologists will continue to be needed to advance basic science knowledge and to contribute to development of pharmaceuticals and biotechnology products. However, job prospects vary widely by job and location.\n\nIn the United States, the Bureau of Labor Statistics predicts that employment of microbiologists will grow 4 percent from 2014 (22,400 employed) to 2024 (23,200 employed). This represents slower growth than the average occupation, as well as slower growth than life scientists as a whole (6 percent projected).\n\n"}
{"id": "34454514", "url": "https://en.wikipedia.org/wiki?curid=34454514", "title": "Mississippi State Axion Search", "text": "Mississippi State Axion Search\n\nMississippi State Axion Search is the first of its kind light shining through the wall experiment designed to operate using a continuous radio wave emitter as the source of photons. The experiment contains a radio source and a set of detectors separated by a wall. The aim of the experiment is to limit the mass and coupling constants of an axion like particle or a para photon by looking at the photons on the dark side of the tuned cavity. The experiment is projected to be completed by 2016.\n\nThe collaboration currently includes members from the following institutions;\n"}
{"id": "25695833", "url": "https://en.wikipedia.org/wiki?curid=25695833", "title": "Nature Exchange", "text": "Nature Exchange\n\nThe Nature Exchange is a specialized learning environment that encourages people to explore nature and actively observe, collect, study and share the world around them. It is a turn-key exhibit, now used in nature-based institutions around North America. Developed by Science North and AldrichPears Associates, the Nature Exchange is an interactive forum where visitors trade ethically collected natural objects and information about them to learn and engage with the natural world. Visitors earn points for each trade, based on criteria such as quality, rarity and their knowledge of the item. Science centers, nature centers and zoos use Nature Exchanges to raise awareness of key issues in the natural world, and, through personal interaction, changes attitudes and behavior.\n\n\n"}
{"id": "18400577", "url": "https://en.wikipedia.org/wiki?curid=18400577", "title": "Naturhistorieselskabet", "text": "Naturhistorieselskabet\n\nNaturhistorieselskabet - the Society for Natural History - was a private society that was the only institution to offer education in natural history in Denmark in the late 18th century. The spirit of the Age of Enlightenment and an escalating agricultural crisis, led the king and the Danish elite to call foreign experts on economy, including botany and silviculture, to the country. The autonomous University of Copenhagen, on the other hand, was reluctant to employ foreign experts in little-established disciplines. Naturhistorieselskabet was formed in 1788 in order to ensure education in botany, zoology and mineralogy based on private funds. For example, Martin Vahl lectured in botany. After the appointment in 1795 of a professor in geology and in 1797 one in botany, the society gradually lost its importance. It was soon abolished and its collections donated to the state (much later united with the university collections).\n\nWagner, P.H. 2001. Institutionaliseringen af botanik og geologi i Danmark-Norge i det 18. århundrede (colloquium). Institut for Videnskabshistorie.\n"}
{"id": "38314163", "url": "https://en.wikipedia.org/wiki?curid=38314163", "title": "Next Generation Science Standards", "text": "Next Generation Science Standards\n\nThe Next Generation Science Standards is a multi-state effort in the United States to create new education standards that are \"rich in content and practice, arranged in a coherent manner across disciplines and grades to provide all students an internationally benchmarked science education.\" The standards were developed by a consortium of 26 states and by the National Science Teachers Association, the American Association for the Advancement of Science, the National Research Council, and Achieve, a nonprofit organization that was also involved in developing math and English standards. The public was also invited to review the standards, and organizations such as the California Science Teachers Association encouraged this feedback. The final draft of the standards was released in April 2013.\n\nThe purposes of the standards include combatting ignorance of science, creating common standards for teaching in the U.S., and developing greater interest in science among students so that more of them choose to major in science and technology in college. Overall, the guidelines are intended to help students deeply understand core scientific concepts, to understand the scientific process of developing and testing ideas, and to have a greater ability to evaluate scientific evidence. Curricula based on the standards may cover fewer topics, but will go more deeply into specific topics, possibly using a case-study method and emphasizing critical thinking and primary investigation. Possible approaches to implementing the standards may even include replacing traditionally isolated high school courses such as biology and chemistry with a case-study approach that uses a more holistic method of teaching science to consider both (or more) topics within a single classroom structure. Many education supply companies have already started offering NGSS-aligned products and resources to help teachers implement these new principles.\n\nThe Next Generation Science Standards (NGSS) are based on the \"Framework K–12 Science Education\" that was created by the National Research Council. They have three dimensions that are integrated in instruction at all levels. The first is core ideas, which consists of specific content and subject areas. The second is science and engineering practices. Students are expected not just to learn content but to understand the methods of scientists and engineers. The third is cross-cutting concepts: key underlying ideas that are common to a number of topics. The NGSS give equal emphasis to engineering design and to scientific inquiry. In addition, they are aligned with the Common Core State Standards by grade and level of difficulty. The standards describe \"performance expectations\" for students in the area of science and engineering. They define what students must be able to do in order to show competency.\n\nAn important facet of the standards is that teaching of content is integrated with teaching the practices of scientists and engineers. This is a change from traditional teaching, which typically either dealt with these topics separately or didn't attempt to teach practices. According to NGSS, it is through the integration of content and practice \"that science begins to make sense and allows students to apply the material.\"\n\nOver 40 states have shown interest in the standards, and as of November 2017, 19 states, along with the District of Columbia (D.C.), have adopted the standards: Arkansas, California, Connecticut, Delaware, Hawaii, \nIllinois, Iowa, Kansas, Kentucky, Maryland, Michigan, Nevada, New Hampshire, New Mexico, New Jersey, Oregon, Rhode Island, Vermont, and Washington. These represent over 35% of the students in U.S. \n\nUnlike the earlier roll-out of the Common Core (CC) math and English standards, states have no financial incentives from federal grants to adopt the Next Generation Science Standards. Previously, adoption of the CC standards was incentivized through states accepting federal grants during the 2009 TARP bailouts. Once states accepted the grant, they accepted the responsibility to adopt \"college and career readiness\" standards, which didn't have to be CC, but most states chose CC anyway.\n\nThe 26 states involved in developing the NGSS, called Lead State Partners, were Arizona, Arkansas, California, Delaware, Georgia, Illinois, Iowa, Kansas, Kentucky, Maine, Maryland, Massachusetts, Michigan, Minnesota, Montana, New Jersey, New York, North Carolina, Ohio, Oregon, Rhode Island, South Dakota, Tennessee, Vermont, Washington, and West Virginia.\n\nWhen the standards were released in April 2013, many states were expected to adopt them within one-to-two years. However, according to the \"New York Times\", it would take several more years to actually develop curricula based on the new guidelines, to train teachers in implementing them, and to revise standardized tests. In addition, the pace of adoption is expected to be slower than was seen with the Common Core State Standards because, unlike Common Core, in which the states had financial incentives to adopt, there are no similar incentives for the NGSS. Many education supply companies have started offering NGSS-aligned products and resources to help teachers adopt NGSS.\n\nNews reports have suggested there will likely be resistance towards the Next Generations Science Standards from conservatives due to the inclusion of anthropogenic climate change and evolution. For example, the New Mexico Public Education Department initially attempted to make changes and deletions in the standards prior to adopting them. According to Skeptical Inquirer, the \"proposed changes would have deleted key terms and concepts such as evolution and the 4.6-billion-year age of the Earth. Specifically, 'evolution' would be called 'biological diversity,' the specific age of the Earth would be changed to 'geologic history,' and a 'rise in global temperatures' would be changed to 'temperature fluctuations.'\" Following significant protests by the New Mexico Academy of Science, New Mexicans for Science and Reason, the Coalition for Excellence in Science and Engineering as well as scientists, educators, and faith leaders, the department announced in October 2017 that it would adopt the standards in their entirety. \n\n\n"}
{"id": "47926105", "url": "https://en.wikipedia.org/wiki?curid=47926105", "title": "Open Energy Modelling Initiative", "text": "Open Energy Modelling Initiative\n\nThe Open Energy Modelling Initiative (openmod) is a grass roots community of energy system modellers from universities and research institutes across Europe and elsewhere. The initiative promotes the use of open-source software and open data in energy system modelling for research and policy advice. The Open Energy Modelling Initiative documents a variety of open-source energy models and addresses practical and conceptual issues regarding their development and application. The initiative runs an email list, an internet forum, and a wiki and hosts occasional academic workshops. A statement of aims is available.\n\nThe application of open-source development to energy modelling dates back to around 2010. This section provides some background for the growing interest in open methods.\n\nJust two active open energy modelling projects were cited in a 2011 paper: OSeMOSYS and TEMOA. Balmorel was also open at that time, having been made public in 2001.\n, the openmod wiki lists 24 such undertakings.\n\nAn innovative 2012 paper presents the case for using \"open, publicly accessible software and data as well as crowdsourcing techniques to develop robust energy analysis tools\". The paper claims that these techniques can produce high-quality results and are particularly relevant for developing countries.\n\nThere is an increasing call for the energy models and datasets used for energy policy analysis and advice to be made public in the interests of transparency and quality. A 2010 paper concerning energy efficiency modeling argues that \"an open peer review process can greatly support model verification and validation, which are essential for model development\". One 2012 study argues that the source code and datasets used in such models should be placed under publicly accessible version control to enable third-parties to run and check specific models. Another 2014 study argues that the public trust needed to underpin a rapid transition in energy systems can only be built through the use of transparent open-source energy models. The UK TIMES project (UKTM) is open source, according to a 2014 presentation, because \"energy modelling must be replicable and verifiable to be considered part of the scientific process\" and because this fits with the \"drive towards clarity and quality assurance in the provision of policy insights\". In 2016, the Deep Decarbonization Pathways Project (DDPP) is seeking to improve its modelling methodologies, a key motivation being \"the intertwined goals of transparency, communicability and policy credibility.\" A 2016 paper argues that model-based energy scenario studies, wishing to influence decision-makers in government and industry, must become more comprehensible and more transparent. To these ends, the paper provides a checklist of transparency criteria that should be completed by modelers. The authors note however that they \"consider open source approaches to be an extreme case of transparency that does not automatically facilitate the comprehensibility of studies for policy advice.\" An editorial from 2016 opines that closed energy models providing public policy support \"are inconsistent with the open access movement [and] funded research\". A 2017 paper lists the benefits of open data and models and the reasons that many projects nonetheless remain closed. The paper makes a number of recommendations for projects wishing to transition to a more open approach. The authors also conclude that, in terms of openness, energy research has lagged behind other fields, most notably physics, biotechnology, and medicine. Moreover:\n\nA one-page opinion piece in \"Nature News\" from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for scrutiny, currently only \"Energy Economics\" makes this practice mandatory within the energy domain.\n\nIssues surrounding copyright remain at the forefront with regard to open energy data. Most energy datasets are collated and published by official or semi-official sources, for example, national statistics offices, transmission system operators, and electricity market operators. The doctrine of open data requires that these datasets be available under free licenses (such as ) or be in the public domain. But most published energy datasets carry proprietary licenses, limiting their reuse in numerical and statistical models, open or otherwise. Measures to enforce market transparency have not helped because the associated information is normally licensed to preclude downstream usage. Recent transparency measures include the 2013 European energy market transparency regulation 543/2013 and a 2016 amendment to the German Energy Industry Act to establish a nation energy information platform, slated to launch on 1July 2017. Energy databases are protected under general database law, irrespective of the copyright status of the information they hold.\n\nIn December 2017, participants from the Open Energy Modelling Initiative and allied research communities made a written submission to the European Commission on the of public sector information. The document provides a comprehensive account of the data issues faced by researchers engaged in open energy system modeling and energy market analysis and quoted extensively from a German legal opinion.\n\nIn May 2016 the European Union announced that \"all scientific articles in Europe must be freely accessible as of 2020\". This is a step in the right direction, but the new policy makes no mention of open software and its importance to the scientific process. In August 2016, the United States government announced a new federal source code policy which mandates that at least 20% of custom source code developed by or for any agency of the federal government be released as open-source software (OSS). The US Department of Energy (DOE) is participating in the program. The project is hosted on a dedicated website and subject to a three-year pilot. Open-source campaigners are using the initiative to advocate that European governments adopt similar practices. In 2017 the Free Software Foundation Europe (FSFE) issued a position paper calling for free software and open standards to be central to European science funding, including the flagship EU program Horizon2020. The position paper focuses on open data and open data processing and the question of open modeling is not traversed perse.\n\nThe Open Energy Modelling Initiative participants take turns to host regular academic workshops.\n\n\nRelated to openmod\n\n\nOpen energy data\n\n\nSimilar initiatives\n\n\nOther\n\n"}
{"id": "309968", "url": "https://en.wikipedia.org/wiki?curid=309968", "title": "PiHKAL", "text": "PiHKAL\n\nPiHKAL: A Chemical Love Story is a book by Dr. Alexander Shulgin and Ann Shulgin, published in 1991. The subject of the work is psychoactive phenethylamine chemical derivatives, notably those that act as psychedelics and/or empathogen-entactogens. The main title, PiHKAL, is an acronym that stands for \"Phenethylamines I Have Known And Loved\".\n\nThe book is arranged into two parts, the first part being a fictionalized autobiography of the couple and the second part describing 179\ndifferent psychedelic compounds (most of which Shulgin discovered himself), including detailed synthesis instructions, bioassays, dosages, and other commentary.\n\nThe second part was made freely available by Shulgin on Erowid while the first part is available only in the printed text. While the reactions described are beyond the ability of people with a basic chemistry education, some tend to emphasize techniques that do not require difficult-to-obtain chemicals. Notable among these are the use of mercury-aluminum amalgam (an unusual but easy to obtain reagent) as a reducing agent and detailed suggestions on legal plant sources of important drug precursors such as safrole.\n\nThrough \"PIHKAL\" (and later \"TIHKAL\"), Shulgin sought to ensure that his discoveries would escape the limits of professional research labs and find their way to the public, a goal consistent with his stated beliefs that psychedelic drugs can be valuable tools for self-exploration. The MDMA (\"ecstasy\") synthesis published in \"PIHKAL\" remains one of the most common clandestine methods of its manufacture to this day. Many countries have banned the major substances for which this book gives directions for synthesis, such as 2C-B, 2C-T-2, and 2C-T-7. In the United Kingdom, all but phenethylamine are illegal.\n\nIn 1994, two years after \"PIHKAL\" was published, the Drug Enforcement Administration (DEA) raided Shulgin's lab and requested that he turn over his DEA license. Richard Meyer, spokesman for DEA's San Francisco Field Division, has stated in reference to \"PIHKAL\" \"It is our opinion that those books are pretty much cookbooks on how to make illegal drugs. Agents tell me that in clandestine labs that they have raided, they have found copies of those books,\" suggesting that the publication of \"PIHKAL\" and the termination of Shulgin's license may have been related.\n\nThe \"Essential Amphetamines\" are what Shulgin describes as ten amphetamines that differ from natural products such as safrole or myristicin by an amine group (\"PIHKAL\" Entry #157 TMA). The list consists of:\n\nNot all of these chemicals are bioassayed in \"PIHKAL\", some are merely mentioned.\n\nThe so-called \"magical half-dozen\" refers to Shulgin's self-rated most important phenethylamine compounds, all of which except mescaline he developed and synthesized himself. They are found within the first book of \"PIHKAL\", and are as follows:\n\nAll six are now Schedule I controlled substances in the United States.\n\n\n"}
{"id": "1967161", "url": "https://en.wikipedia.org/wiki?curid=1967161", "title": "Post-normal science", "text": "Post-normal science\n\nPost-normal science (PNS) represents a novel approach for the use of science on issues where \"facts [are] uncertain, values in dispute, stakes high and decisions urgent\". PNS was developed in the 1990s by Silvio Funtowicz and Jerome R. Ravetz. It can be considered as a reaction to the styles of analysis based on risk and cost-benefit analysis prevailing at that time, and as an embodiment of concepts of a new \"critical science\" developed in previous works by the same authors. In a more recent work PNS is described as \"the stage where we are today, where all the comfortable assumptions about science, its production and its use, are in question\".\n\nIn 1962, Thomas Kuhn's \"The Structure of Scientific Revolutions\" introduced the concept of normal science as part of his theory that scientific knowledge progresses through socially constructed paradigm shifts, where normal science is what most scientists do all the time and what all scientists do most of the time. The process of a paradigm shift is essentially as follows:\n\nAn illustration of the theory in practice is the Copernican revolution, where Copernicus’ idea of a (sun-centered) solar system was largely ignored (not in the rules) when first introduced; then Galileo was deemed a heretic for supporting the idea (rules called into question); and finally, after a revolution in cosmology, the solar system became an obvious and foundational part of scientific knowledge (new rules).\n\nAnother example is the question of whether light is a particle or a wave. For a long time there was debate on this point. Advocates on both sides had many valid arguments based on scientific evidence but were lacking a theory that would resolve the conflict. After a revolution in thinking, it was realized that both perspectives could be true.\n\nPhysicist and policy adviser James J. Kay described post-normal science as a process that recognizes the potential for gaps in knowledge and understanding that cannot be resolved in ways other than revolutionary science. He argued that (between revolutions) one should not necessarily attempt to resolve or dismiss contradictory perspectives of the world, whether they are based on science or not, but instead incorporate multiple viewpoints into the same problem-solving process. From the ecological perspective post-normal science can be situated in the context of 'crisis disciplines' – a term coined by the conservation biologist Michael E. Soulé to indicate approaches addressing fears, emerging in the seventies, that the world was on the verge of ecological collapse. In this respect Michael Egan defines PNS as a 'survival science'.\n\nMoving from PNS Ziauddin Sardar developed the concept of Postnormal Times (PNT). Sardar was the editor of FUTURES when it published the article ‘Science for the post-normal age’ presently the most cited paper of the journal . A recent review of academic literature conducted on the Web of Science and encompassing the topics of Futures studies, Foresight, Forecasting and Anticipation Practice identifies the same paper as \"the all-time publication that received the highest number of citations\". \n\n\"At birth Post-normal science was conceived as an inclusive set of robust insights more than as an exclusive fully structured theory or field of practice\". Some of the ideas underpinning PNS can already be found in a work published in 1983 and entitled \"Three types of risk assessment: a methodological analysis\" This and subsequent works show that PNS concentrates on few aspects of the complex relation between science and policy: the communication of uncertainty, the assessment of quality, and the justification and practice of the extended peer communities.\n\nThe horizontal axis represents ‘Systems Uncertainties’ and the vertical one ‘Decision Stakes’. The three quadrants identify Applied Science, Professional Consultancy, and Post-Normal Science. Different standards of quality and styles of analysis are appropriate to different regions in the diagram, i.e. Post-normal science does not claim relevance and cogency on all of science's application but only on those defined by the PNS's mantram with a fourfold challenge: : ‘facts uncertain, values in dispute, stakes high and decisions urgent’. For applied research science’s own peer quality control system will suffice (or so was assumed at the moment PNS was formulated in the early nineties), while professional consultancy was considered appropriate for these settings which cannot be ‘peer-reviewed’, and where the skills and the tacit knowledge of a practitioner are needed at the forefront, e.g. in a surgery room, or in a house on fire. Here a surgeon or a fireman takes a difficult technical decision based on her or his training and appreciation of the situation (the Greek concept of ‘Metis (mythology)’).\n\nThere are important linkages between PNS and complexity science, e.g. system ecology (C. S. Holling) and hierarchy theory (Arthur Koestler). In PNS, complexity is respected through its recognition of a multiplicity of legitimate perspectives on any issue; and reflexivity is realised through the extension of accepted ‘facts’ beyond the supposedly objective productions of traditional research. Also, the new participants in the process are not treated as passive learners at the feet of the experts, being coercively convinced through scientific demonstration. Rather, they will form an ‘extended peer community’, sharing the work of quality assurance of the scientific inputs to the process, and arriving at a resolution of issues through debate and dialogue.\n\nPNS concept of extended peer community moves from and transcends the familiar concept of scientific peer community relative to a well-defined field of scientific research.\nThe peer community is extended in two respects: first, more than one discipline is assumed to have a potential bearing on the issue being debated, thereby providing different lenses to consider the problem. Second the community is extended to lay actors, taken to be all those with stakes, or an interest, in the given issue. Perhaps the best justification of the concept is offered by Paul Feyerabend in Against Method . For Feyerabend the participation of experts together with non-experts would allow the citizens to mature, inter alia by realizing that the experts are themselves lay-people outside their restricted field of competence. For Giandomenico Majone \"In any area of public policy the choice of instruments, far from being a technical exercise that can be safely delegated to the experts, reflects as in a microcosm all the political, moral, and cultural dimensions of policy-making.\" The same author notes: \"Dialectical confrontation between generalists and experts often succeeds in bringing out unstated assumptions, conflicting interpretations of the facts, and the risks posed by the projects\". These considerations justifies the need for an extended peer community, as the arena where the policy instruments and options can be discussed with - but without deference to - the experts and the authorities. \n\nThe lay members of the community thus constituted may also take upon themselves active 'research' tasks; this has happened e.g. in the so-called 'popular epidemiology' , when the official authorities have shown reticence to perform investigations deemed necessary by the communities affected - for example - by a case of air or water pollution , and more recently ‘citizen science’ . The extended community can usefully investigate the quality of the scientific assessments provided by the experts, the definition of the problem, as well as research priorities and research questions .\n\nThus, the extension of the peer community is not only ethically fair or politically correct, but also enhances the quality of the relevant science. An example is provided by Brian Wynne, who discusses the Cumbrian sheep farmers' interaction with scientist and authorities in the relation to the Chernobyl radioactive fallout .\n\nBeside its dominating influence in the literature on 'futures', PNS is considered to have influenced the ecological ‘conservation versus preservation debate’, especially via its reading by American pragmatist Bryan G. Norton. According to Jozef Keulartz the PNS concept of \"extended peer community\" influenced how Norton's developed his 'convergence hypothesis'. The hypothesis posits that ecologists of different orientation will converge once they start thinking 'as a mountain', or as a planet. For Norton this will be achieved via deliberative democracy, which will pragmatically overcome the black and white divide between conservationists and preservationists.\n\nOther authors attribute to PNS the role of having stimulated the take up of transdisciplinary methodological frameworks, reliant on the social constructivist perspective embedded in PNS.\n\nToday Post-normal science is intended as applicable to most instances where the use of evidence is contested due to different norms and values.\n\nAs summarized in a recent work \"the ideas and concepts of post normal science bring about the emergence of new problem solving strategies in which the role of science is appreciated in its full context of the complexity and the uncertainty of natural systems and the relevance of human commitments and values.\n\nFor Peter Gluckman (2014), chief science advisor to the Prime Minister of New Zealand, post normal science approaches are today appropriate for a host of problems including \"eradication of exogenous pests […], offshore oil prospecting, legalization of recreational psychotropic drugs, water quality, family violence, obesity, teenage morbidity and suicide, the ageing population, the prioritization of early-childhood education, reduction of agricultural greenhouse gases, and balancing economic growth and environmental sustainability\".\nFor Carrozza PNS can be \"framed in terms of a call for the ‘democratization of expertise’\", and as a \"reaction against long-term trends of ‘scientization’ of politics—the tendency towards assigning to experts a critical role in policymaking while marginalizing laypeople\". For Mike Hulme (2007), writing on \"The Guardian\" Climate change seems falls into the category of issues which are best dealt with in the context of PNS and notes that \"Disputes in post-normal science focus as often on the process of science - who gets funded, who evaluates quality, who has the ear of policy - as on the facts of science\". Recent reviews of the history and evolution of PNS, its definitions, conceptualizations,\nand uses can be found in Turnpenny et al., 2010, and in The Routledge Handbook of Ecological Economics (Nature and Society). There has been recently an increased reference to post-normal science, e.g. in Nature (journal).\n\nA criticism of post-normal science is offered by Weingart (1997) for whom Post-normal science does not introduce a new epistemology but retraces earlier debates linked to the so-called \"finalization thesis\".\n\nThe journal FUTURES devoted several specials issues to PNS.\n\n\n\nAnother special issue on Post Normal Science was published on the journal Science, Technology & Human Values in May 2011.\n\nMore titles and links relative to PNS special issues are available at the NUSAP net.\n\nA group of scholars of PNS orientation has published in 2016 a volume on the quality control crisis of science. The volume discusses \"inter alia\" what this community perceive as the root causes of the present crisis.\n\nAmong the quantitative styles of analysis which make reference to post-normal science one can mention NUSAP for numerical information, sensitivity auditing for indicators and mathematical modelling and MUSIASEM in the field of social metabolism.\n\nIn relation to mathematical modelling PNS suggests a participatory approach, whereby ‘models to predict and control the future’ are replaced by ‘models to map our ignorance about the future’, in the process exploring and revealing the metaphors embedded in the model. PNS is also known for the its definition of GIGO: in modelling GIGO occurs when the uncertainties in the inputs must be suppressed, lest the outputs become completely indeterminate. \n\n\n\n"}
{"id": "37998886", "url": "https://en.wikipedia.org/wiki?curid=37998886", "title": "Practitioner research", "text": "Practitioner research\n\nPractitioner Research refers to research and/or workplace research such as evaluation performed by individuals who also work in a professional field as opposed to being full-time academic researchers. Practitioner research developed as a recognized type of research in the last quarter of the 20th century. In this context, 'practitioner' means someone who delivers public services, such as a nurse, teacher, advice worker, probation officer, counselor or social worker. Practitioner research developed in disciplinary silos, but by the turn of the century it had been recognized that all disciplines could approach practitioner research in broadly the same way.\n\nFor a practitioner, doing research alongside practice can assist with one or more of the following:\nIt is also held to improve the quality of the practitioner-researcher's practice.\n\nPractitioner research has two categories: \n\n"}
{"id": "41872647", "url": "https://en.wikipedia.org/wiki?curid=41872647", "title": "Registry of Research Data Repositories", "text": "Registry of Research Data Repositories\n\nThe Registry of Research Data Repositories (re3data.org) is an Open Science tool that offers researchers, funding organizations, libraries and publishers an overview of existing international repositories for research data.\n\nre3data.org is a global registry of research data repositories from all academic disciplines. It provides an overview of existing research data repositories in order to help researchers to identify a suitable repository for their data and thus comply with requirements set out in data policies.\nThe registry was officially launched in May 2013.\n\nIn March 2014 the registry lists 634 research data repositories from around the world covering all academic disciplines. 586 of these are described in detail using the re3data.org schema.\nThe project makes all metadata in the registry available for open use under the Creative Commons deed CC0.\n\nThe majority of the listed research data repositories are described in detail by a comprehensive schema, namely the re3data.org Schema for the Description of Research Data Repositories.\nInformation icons support researchers to identify an adequate repository for the storage and reuse of their data.\n\nA repository is indexed when the minimum requirements for inclusion in re3data.org are met: the repository has to be run by a legal entity, such as a sustainable institution (e.g. library, university) and clearly state access conditions to the data and repository as well as the terms of use. Additionally, an English graphical user interface (GUI) plus a focus on research data is needed.\n\nre3data.org is a joint project of the Berlin School of Library and Information Science, the GFZ German Research Centre for Geosciences and the Library of the Karlsruhe Institute of Technology (KIT). The project is funded by the German Research Foundation (DFG).\nThe project cooperates with other Open Science initiatives like Databib, BioSharing, DataCite and OpenAIRE. Several publishers, research institutions and funders refer to re3data.org in their Editorial Policies and guidelines as a tool for the identification of suitable data repositories, e.g. Nature, Springer and the European Commission.\n\n\n"}
{"id": "34033422", "url": "https://en.wikipedia.org/wiki?curid=34033422", "title": "Scholar Indices and Impact", "text": "Scholar Indices and Impact\n\nScholar indices are used to measure the contributions of scholars to their fields of research. Since the 2005 paper of Jorge E. Hirsch, the use of scholar indices has increased.\n\nSometimes called bibliometrics, scholar indices are mathematical and statistical tools that measure the significance of the contributions made by an academic to their field of research. Scholar indices may incorporate other assessments such as citation tracking and journal ranking.\n\nAny aggregator of citations and references, could, given time, money and inclination, generate their own set of scholar indices. Publishers who are prominent in this field include Elsevier and Thomson Reuters.\n\nCommercial software which use parsers and web search engines to generate sets of scholar indices or individual results are now available. Examples are: \"Publish or Perish\"; 'ScholarIndex'; 'Scopus' and 'Google Scholar'.\n\nEach software vendor primarily uses its own data as well as journals, publications, authority files, indexes, and subject categories to produce sets of scholar indices.\n\nWhile some companies provide the data and the evaluated metrics as free downloads, others require subscriptions to cover costs of manufacture and upkeep of an efficient parser, search engine and document database.\n\nScholar indices allow choice of journal collections, application of research funds, ranking of journals and the determination of significant contributors in a subject area.\nAdvocates of scholar indices recommend their use in areas such as liaison services, references, instruction and collection management.\n\nCritics of the use of scholar indices cite their limitations due to issues of accuracy, validity and applicability and debate their application to hiring, tenure, funding, award giving and membership decisions.\n\nAlthough scholar indices may not completely describe the impact of an individual researcher's work, some academics will determine their own scholar indices to include in promotional material and curriculum vita for example. Others may study their scholar indices simply for their own sake.\n\nThose interested in the field of scholar indices may find the results, for example data visualisation projects, exciting.\n\nTo date, a number of scholar indices have been developed. One is the 'h-index' introduced by Jorge E. Hirsch in August 2005. Hirsch described the h-index as unbiased as it involved the relationship of an academic's volume of published papers and the number of citations for those papers creating less bias than either measure alone.\n\nAnother scholar index is the 'g-index' which measures citation counts of all well cited works over a period of time. The 'm-quotient' was developed to introduce a time limit to the h-index which was otherwise, an ever-increasing quantity.\n\nOther variants of the h-index such as hI-index, e-index and others are being developed and reviewed.\n\nThe Erdős number was developed to measure the publication chain started by Paul Erdős.\n\nAll such scholar indices quantify the contribution of a researcher based on citations of their work only. Ideally, an assessment of a researcher's contribution to their field would include both scholar indices and an analysis of the quality of the work itself.\n\nThe h-index index was suggested by Jorge E. Hirsch, a physicist at UCSD, in 2005.\n\nHenry Schaefer, of the University of Georgia, US, together with colleague Amy Peterson, created rankings according to the h-index, from the ISI Web of Science. Though web-based applications can calculate h-indices, Peterson had to check for misspelt or duplicated names.\n\nThe h-index is defined as follows:\n\nTo calculate the h-index, the papers written by an academic are arranged in decreasing order of number of citations. The h index is where the number of papers equals the number of citations (beginning with the paper with the highest number of citations).\n\nAlthough widely used, the h index does not take into account the quality of the papers; other papers which cite the academics publications; the number of co-authors and the position of the author in the author list. Also, all fields are given equal value.\n\nAnother limitation is that the h index does not vary over time. For example, Évariste Galois has an h index of 2 while Claude Shannon has an h index of 7 \n\nWhile the h-index is independent of the date of an academic's career, the m-quotient aims at weighing the period of academic endeavour so that even junior scientists attain the importance that they deserve.\n\nThus, if \"n\"=number of years since the first published paper of the scientist, the m-quotient=h-index/\"n\".\n\nHowever, the m-quotient may not stabilise until later in the scientist's career. for researchers in the early part of their career, who have low h indices, small changes in the h-index can lead to large changes in the m-quotient. Hirsch suggests the researcher's first published paper may not always be the appropriate starting point, especially if it was a minor contribution that was published well before the academic's period of sustained productivity.\n\nAlthough the m-quotient adds time as a weighting factor, it does not cater to the major disadvantages of the h-index including quality of publication and quality of citation.\n\ng-index is a variant of the h-index, which takes into account the citation evolution of the most cited papers over time.\n\nIn other words, the g-index g is the largest rank (where papers are arranged in decreasing order of the number of citations they received) such that the first g papers have (together) at least g^2 citations. \"\n\nIt can be proved that for any set of papers g-index always exists and is unique.\n\nformula_1\n\nwhere the Lotkaian exponent and where T denotes the total number of sources.\n\nSince formula_2, formula_3\n\nFor example, if 2 scientists have h-index 4, it may happen that one of them has published 4 papers which have 4 or more citations, while another scientist may have published 10 papers out of which 3 have more than 100 citations and the 4th paper has 4 citations, and the remaining have less than 4 citations.\n\nIn an attempt to offer a higher weighting to the second scientist who in aggregation has received greater than 304 citations for 10 papers, the g-index was proposed. Thus in our example, the first scientist has g-index=4, while the second scientist has g-index significantly higher.\n\nAn Erdős number measures the collaborative distance between a person and mathematician Paul Erdős, measured by authorship of mathematical papers.\n\nConsidering Paul Erdős to have an index=0, people who co-authored with him have an index=1, co authors of those co-authors have index=2, and so on. Thus, to calculate one’s Erdős number, add 1 to the Erdős number of any co-author with the lowest Erdős, number. The Erdős-Number project at Oakland University maintains a website tracking the Erdős numbers of scientists world-wide.\n\nOne caveat is that most Erdős numbers recorded so far range up to 13, but the average is less than 5, and almost everyone with a finite Erdős number has a number less than 8.\n\nEvaluation of the complete contribution of a scholar to their field of research can be assessed in two ways. One is by accounting for the number of citations received by the scholar. The other is by accounting for the quality of the references referred to by the scholar.\n\nWhile being strongly cited makes a scholar a strong authority in his field, having strong references makes a scholar a strong hub, who knows all the significant work in that field. Calculation of the hub and authority indices requires the knowledge of the relationships between scholars being cited or referred to.\n\nThe Hubs and Authorities algorithm can be used for computing these indices. The algorithm performs a link analysis on a given network and assigns two scores to each node: a hub and an authority.;\n\nA valuable and informative node in a network is usually pointed to by a large number of links, that is, it has a large indegree (see Fig. 1). Such a node is called an authority.\n\nA node that points to many authority nodes is itself a useful resource and is called a hub. A hub usually has a large outdegree. In the context of literature citation, a hub is a review paper which cites many original papers, while an authority is an original seminal paper which is cited by many papers.\n\nA network can be constructed of nodes representing authors and links indicating references to published papers. Outgoing links indicate who the author cited and incoming links indicate who cited the author.\n\nA researcher's hub score is the sum of authors' scores whose work is cited. A researcher's authority score is the sum of authors' hub scores who referenced the researcher's work.\n\nThe hub score increases if the author cites papers published by authors with high authority scores. The authority score increases when published papers are cited by authors with high hubs score.\n\nformula_4\n\nformula_5\n\nThe equations can be rewritten in a matrix-vector form. Let A be an adjacency matrix of the network and vectors h and a to contain all hubs and authorities scores, respectively. Then the scores can be calculated by the following formulas.\n\nformula_6\n\nformula_7\n\nHubs and authorities indices require the knowledge of the connectivity between scholars who refer and cite each other’s work. Since it is not always possible to accurately obtain these connectivity patterns, the adjacency matrix (A) regarding the scholar’s connections can be estimated.\n\nFor example, a scientist has an estimated local connectivity matrix. It is a combination of the work by which he is cited and works which he cites. Once the adjacency network is estimated, the hub and authority indices are determined by eigen-decomposition of \"(A.A’)\" and \"(A’.A)\" respectively. The steps followed for this specific implementation are as follows:\nWhere blocks of X and blocks of Y replace the ‘1’s. The connectivity follows the Fig 2.\n\n\n1 formula_8\n\n2 formula_9\n\n3 formula_10\n\n4 formula_11\n\nThe table utilizes Scopus as a search engine only and the adjacency matrix is an estimation, thus the results tabulated above are extremely aggregated versions, and they must not be confused with absolute indices. A better estimate of adjacency matrix may produce variations in the indices. Also, Scopus keeps track of articles after 1995 only, so that is an additional constraint. (All the indices have been evaluated as of December 12, 2011.)\n\nIn this table, it is evident that different search engines yield different h-indices. It is possible that a scientist with high h-index may be a strong authority but not necessarily a strong hub. The validity of web-search engines is assessed as documents prior to 1995 are inaccessible. The number of publications of a particular author in a particular data base is responsible for affecting the hub-authority indices. Interdisciplinary work may be well assessed by the hub-authority index as opposed to other indices.\n\nThis is an example to understand the interplay of the various scholar indices.\n\nA new scientist who began her academic career in 2009, has published 3 papers. Two papers have 2 citations each while the third paper has no citations. She has referenced 60 papers with 17 strong references among the 60. One of her co-authors has the lowest Erdős number 3. Her scholar indices as per December 2011 are:\n\nIn another year, she publishes another paper with 20 new references such that now she has a cumulative of 31 strong references, 4 papers with 2 citations for 2 papers and 0 citations for other 2 papers. Her hub-authority indices change:\n\nHUB index=12.668\n\nAUTHORITY index= 0.1061\n\nIn another year, her citations for the 3 papers increases to 10 and she continues to have 60 references with 17 strong references:\n\nHUB index=11.568\n\nAUTHORITY index=0.3241\n\nIn another year, her citations increase to 10 and she writes another paper such that the number of references goes up to 80 (with 31 strong ones):\n\nHUB index=12.694\n\nAUTHORITY index=0.3284\n\nThus to summarize, the following hub-authority indices are observed for this toy example.\n\nThe matlab code for the Example 1 and Toy example instances in Example 2 is attached as File 1.\nThese examples demonstrate the importance of the hub-authority indices in quantitative evaluation of the contributions of a scholar in a complete academic career.\n\nScholar indices have limitations including lack of accuracy, validity and applicability. While the accuracy of scholar indices is questionable owing to the difference in spellings, difference in parser, search engines and document data bases maintained by various online sources, it might be possible to solve the accuracy issues if each author could be assigned to a unique ID instead of relying on the names for searches. Also each time these indices are reported, the method and search engines used must be mentioned to avoid ambiguity as much as possible.\n\nThe validity of scholar indices is a limitation because they do not weigh fields of varying importance. For example, John Pople, a theoretical chemist who has received a Nobel Prize, fares poorly in sets of h-indices.\n\nApplicability of scholar indices has limitations when scholars emphasise practical advancement in an area of endeavour rather than the publishing of papers. It is also difficult to document works of an earlier decade as online documents, thereby decreasing their online impact factor. For example, the Scopus is an online database that calculates scholar indices for documents found after 1995 only. Any earlier works are not documented or evaluated.\n\nJorge E. Hirsch suggested the h-index should inform hiring, promotion, funding, tenure, award and societal committees to make judicious decisions. However, because of their limitations they are best viewed in a balanced way.\n"}
{"id": "32449181", "url": "https://en.wikipedia.org/wiki?curid=32449181", "title": "School of Naturalists", "text": "School of Naturalists\n\nThe School of Naturalists or the School of Yin-yang (陰陽家/阴阳家; \"Yīnyángjiā\"; \"Yin-yang-chia\"; \"School of Yin-Yang\") was a Warring States era philosophy that synthesized the concepts of yin-yang and the Five Elements.\n\nZou Yan is considered the founder of this school. His theory attempted to explain the universe in terms of basic forces in nature: the complementary agents of yin (dark, cold, female, negative) and yang (light, hot, male, positive) and the Five Elements or Five Phases (water, fire, wood, metal, and earth). In its early days, this theory was most strongly associated with the states of Yan and Qi. In later periods, these epistemological theories came to hold significance in both philosophy and popular belief. This school was absorbed into the alchemic and magical dimensions of Taoism as well as into the Chinese medical framework. The earliest surviving recordings of this are in the Ma Wang Dui texts and Huang Di Nei Jing.\n\nZou Yan (; 305240 BC) was an ancient Chinese philosopher best known as the representative thinker of the Yin and Yang School (or School of Naturalists) during the Hundred Schools of Thought era in Chinese philosophy. Zou Yan was a noted scholar of the Jixia Academy in the state of Qi. Joseph Needham, a British sinologist, describes Zou as \"The real founder of all Chinese scientific thought.\" His teachings combined and systematized two current theories during the Warring States period: Yin-Yang and the Five Elements/Phases (wood, fire, earth, metal, and water).\n\nDuring the Han dynasty, the concepts of the school were integrated into Confucian ideology, Zhang Cang (253-152 BCE) and Dong Zhongshu (179-104 BCE) being the chief instrumental figures behind this process.\n"}
{"id": "19086436", "url": "https://en.wikipedia.org/wiki?curid=19086436", "title": "Science.ie", "text": "Science.ie\n\nThe Science.ie portal provides all sorts of information about careers in science, technology, engineering and mathematics (STEM).\n\nScience.ie is an initiative of the Irish Government’s Discover Science & Engineering (DSE) awareness programme in Ireland. DSE is managed by Forfás on behalf of the Office of Science and Technology at the Department of Jobs, Enterprise and Innovation.\n\nThe careers-related information on Science.ie has been moved to a new DSE website - My Science Career - which was launched in early October 2009. On MyScienceCareer.ie is:\n\n\nA redeveloped Science.ie was also launched in October 2009. The site has been redesigned and includes social media bookmarking and RSS feeds.\n\nScience.ie provides more general information on science in Ireland. This includes listings of science links, news and events. Its \"Resources\" section gives information on activities and visitor centres where you can learn about science.\n\nThe site also provides a free newsletter relating to Irish science, technology and innovation news, events, research and facts which is issued monthly by email.\n\nIn November 2009 Science.ie launched a Twitter channel. Follow Science.ie on Twitter.\n\nDSE runs numerous other initiatives, including My Science Career, Project Blogger, Science Week Ireland and Discover Primary Science.\n\n"}
{"id": "3447151", "url": "https://en.wikipedia.org/wiki?curid=3447151", "title": "Scientific demonstration", "text": "Scientific demonstration\n\nA scientific demonstration is a scientific experiment carried out for the purposes of demonstrating scientific principles, rather than for hypothesis testing or knowledge gathering (although they may originally have been carried out for these purposes).\n\nMost scientific demonstrations are simple laboratory demonstrations intended to demonstrate physical principles, often in a surprising or entertaining way. They are carried out in schools and universities, and sometimes in public demonstrations in popular science lectures and TV programs aimed at the general public. Many scientific demonstrations are chosen for their combination of educational merit and entertainment value, which is often provided by dramatic phenomena such as explosions. \n\nPublic scientific demonstrations were a common occurrence in the Age of Enlightenment, and have long been a feature of the British Royal Institution Christmas Lectures, which date back to 1825. In the television era, scientific demonstrations have featured in science-related entertainment shows such as \"MythBusters\" and \"\".\n\nSome famous scientific demonstrations include:\n\n\nNote: many scientific demonstrations are potentially dangerous, and should not be attempted without considerable laboratory experience and appropriate safety precautions. Many older well-known scientific demonstrations, once mainstays of science education, are now effectively impossible to demonstrate to an audience without breaking health and safety laws. Some older demonstrations, such as allowing the audience to play with liquid mercury, are sufficiently dangerous that they should not be attempted by anyone under any circumstances.\n\n"}
{"id": "244629", "url": "https://en.wikipedia.org/wiki?curid=244629", "title": "Scientific law", "text": "Scientific law\n\nThe laws of science, also called scientific laws or scientific principles, are statements that describe or predict a range of natural phenomena. Each scientific law is a statement based on repeated experimental observations that describes some aspect of the Universe. The term \"law\" has diverse usage in many cases (approximate, accurate, broad, or narrow theories) across all fields of natural science (physics, chemistry, biology, geology, astronomy, etc.). Scientific laws summarize and explain a large collection of facts determined by experiment, and are tested based on their ability to predict the results of future experiments. They are developed either from facts or through mathematics, and are strongly supported by empirical evidence. It is generally understood that they reflect causal relationships fundamental to reality, and are discovered rather than invented.\n\nLaws reflect scientific knowledge that experiments have repeatedly verified (and never falsified). Their accuracy does not change when new theories are worked out, but rather the scope of application, since the equation (if any) representing the law does not change. As with other scientific knowledge, they do not have absolute certainty (as mathematical theorems or identities do), and it is always possible for a law to be overturned by future observations. A law can usually be formulated as one or several statements or equations, so that it can be used to predict the outcome of an experiment, given the circumstances of the processes taking place.\n\nLaws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws since they have not been verified to the same degree and may not be sufficiently general, although they may lead to the formulation of laws. A law is a more solidified and formal statement, distilled from repeated experiment. Laws are narrower in scope than scientific theories, which may contain one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. Although the nature of a scientific law is a question in philosophy and although scientific laws describe nature mathematically, scientific laws are practical conclusions reached by the scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.\n\nAccording to the unity of science thesis, \"all\" scientific laws follow fundamentally from physics. Laws which occur in other sciences ultimately follow from physical laws. Often, from mathematically fundamental viewpoints, universal constants emerge from a scientific law.\n\nA scientific law always applies under the same conditions, and implies that there is a causal relationship involving its elements. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction.\n\nLaws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, a law is limited in applicability to circumstances resembling those already observed, and may be found false when extrapolated. Ohm's law only applies to linear networks, Newton's law of universal gravitation only applies in weak gravitational fields, the early laws of aerodynamics such as Bernoulli's principle do not apply in case of compressible flow such as occurs in transonic and supersonic flight, Hooke's law only applies to strain below the elastic limit, etc. These laws remain useful, but only under the conditions where they apply.\n\nMany laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as formula_1, where E is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as formula_2.\n\nThe term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws. An example of a scientific law in social sciences is Zipf's law.\n\nLike theories and hypotheses, laws make predictions (specifically, they predict that new observations will conform to the law), and can be falsified if they are found in contradiction with new data.\n\nMost significant laws in science are conservation laws. These fundamental laws follow from homogeneity of space, time and phase, in other words \"symmetry\".\n\n\nConservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as:\n\nwhere ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇•) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point, hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see main article for details). In the table below, the fluxes, flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison.\n\nMore general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation.\n\nAll of classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from this very simple principle:\n\nwhere formula_5 is the action; the integral of the Lagrangian\n\nof the physical system between two times \"t\" and \"t\". The kinetic energy of the system is \"T\" (a function of the rate of change of the configuration of the system), and potential energy is \"V\" (a function of the configuration and its rate of change). The configuration of a system which has \"N\" degrees of freedom is defined by generalized coordinates q = (\"q\", \"q\", ... \"q\").\n\nThere are generalized momenta conjugate to these coordinates, p = (\"p\", \"p\", ..., \"p\"), where:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nThe action is a \"functional\" rather than a \"function\", since it depends on the Lagrangian, and the Lagrangian depends on the path q(\"t\"), so the action depends on the \"entire\" \"shape\" of the path for all times (in the time interval from \"t\" to \"t\"). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the \"entire continuum\" of Lagrangian values corresponding to some path, \"not just one value\" of the Lagrangian, is required (in other words it is \"not\" as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied to the entire \"shape\" of the function, see calculus of variations for more details on this procedure).\n\nNotice \"L\" is \"not\" the total energy \"E\" of the system due to the difference, rather than the sum:\n\nThe following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations, Newton's is very commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications.\n\nFrom the above, any equation of motion in classical mechanics can be derived.\n\n\n\n\nEquations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow.\n\n\n\nPostulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of \"relative motion\".\n\nOften two are stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant\". However the second is redundant, since the speed of light is predicted by Maxwell's equations. Essentially there is only one.\n\nThe said posulate leads to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector\n\nthis replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light \"c\".\n\nThe magnitudes of 4-vectors are invariants - \"not\" \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if \"A\" is the four-momentum, the magnitude can derive the famous invariant equation for mass-energy and momentum conservation (see invariant mass):\n\nin which the (more famous) mass-energy equivalence \"E\" = \"mc\" is a special case.\n\n\nGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass-energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n\n\nIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous \"gravitomagnetic field\". They are well established by the theory, and experimental tests form ongoing research.\n\nThese equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations.\n\n\nThese laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's Equations. Coulomb's Law can be found from Gauss' Law (electrostatic form) and the Biot–Savart Law can be deduced from Ampere's Law (magnetostatic form). Lenz' Law and Faraday's Law can be incorporated into the Maxwell-Faraday equation. Nonetheless they are still very effective for simple calculations.\n\n\n\n\nClassically, optics is based on a variational principle: light travels from one point in space to another in the shortest time.\n\n\nIn geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation).\n\n\nIn physical optics, laws are based on physical properties of materials.\n\n\nIn actuality, optical properties of matter are significantly more complex and require quantum mechanics.\n\nQuantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them.\n\nOne postulate that a particle (or a system of many particles) is described by a wavefunction, and this satisfies a quantum wave equation: namely the Schrödinger equation (which can be written as a non-relativistic wave equation, or a relativistic wave equation). Solving this wave equation predicts the time-evolution of the system's behaviour, analogous to solving Newton's laws in classical mechanics.\n\nOther postulates change the idea of physical observables; using quantum operators; some measurements can't be made at the same instant of time (Uncertainty principles), particles are fundamentally indistinguishable. Another postulate; the wavefunction collapse postulate, counters the usual idea of a measurement in science.\n\nApplying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows.\n\n\nChemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics.\n\n\nThe most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.\n\nAdditional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.\n\nDalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers (i.e. 1:2 for Oxygen:Hydrogen ratio in water); although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction.\n\nMore modern laws of chemistry define the relationship between energy and its transformations.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24763683", "url": "https://en.wikipedia.org/wiki?curid=24763683", "title": "Scientific myth", "text": "Scientific myth\n\nA scientific myth is a myth about science. For example, scientific discoveries are often presented in a mythological way with a theory being presented as a dramatic flash of insight by a heroic individual rather than as the result of sustained experiment and reasoning. For example, Newton's law of universal gravitation is commonly presented as the result of an apple falling upon his head. Newton's observation of an apple falling did indeed play a part in starting him thinking about the problem but it took him about twenty years to fully develop the theory and so the story of the apple has been described as a myth.\n\nThe extent to which this occurs and is problematic is debatable. Scientific historian Douglas Allchin suggests that mythical accounts are misleading because they present the results as handed down by authority figures and understate the importance of error and its resolution by the scientific method. In responding to this, Westerlund and Fairbanks agreed that romantic accounts of science tend to distort its nature but, in the case of Mendel's discovery of the rules of inheritance, they argue that Allchin's criticism of Mendel's role and reasoning is over-stated.\n\n\n"}
{"id": "18093238", "url": "https://en.wikipedia.org/wiki?curid=18093238", "title": "Scientific teaching", "text": "Scientific teaching\n\nScientific teaching is a pedagogical approach used in undergraduate science classrooms whereby teaching and learning is approached with the same rigor as science itself. \n\nAccording to a 2004 Policy Forum in \"Science\" magazine, \"scientific teaching involves active learning strategies to engage students in the process of science and teaching methods that have been systematically tested and shown to reach diverse students.\"\n\nThe 2007 volume \"Scientific Teaching\" lists three major tenets of scientific teaching:\n\nThese elements should underlie educational and pedagogical decisions in the classroom. The \"SCALE-UP\" learning environment is an example of applying the scientific teaching approach. In practice, scientific teaching employs a \"backward design\" approach. The instructor first decides what the students should know and be able to do (learning goals), then determines what would be evidence of student achievement of the learning goals, then designs assessments to measure this achievement. Finally, the instructor plans the learning activities, which should facilitate student learning through scientific discovery.\n\n"}
{"id": "5639364", "url": "https://en.wikipedia.org/wiki?curid=5639364", "title": "Screen media practice research", "text": "Screen media practice research\n\nScreen media practice research is an emerging academic area situated primarily within university Media Studies, Communications, Cultural Studies, Art and Design, and Performing Arts departments. Conducted in the practical production of film, video, internet, visual arts, and similar media, it is a subsection of a wider body of practice research within the arts and humanities.\n\nThe British Arts and Humanities Research Council has stated that \"projects that can be defined as practice-led or applied\" are those \"where creative practice is integral to the project or it is undertaken with the specific goal of producing a defined research output — for example, new or improved systems, designs, artefacts, exhibitions, performances, events, products, processes, materials, devices, services, films, compositions, broadcasts, policy guidance – that will be utilised beyond the research base.\"\n\nIn the UK, in Drama, Dance and the Performing Arts these issues have been the subject of an Arts and Humanities Research Board (now Council) -funded project at Bristol University.\n\nAccording to the Bristol University PARIP site, \"practice as research (PAR) and practice-based research (PBR) — and 'research through practice', 'research by practice', 'performance as research' — are contested terms that resist close definition. Practice as research and practice-based research are frequently used interchangeably to suggest a relationship of research between theory and practice.\" \n\nScreen media practice research is disseminated in a variety of ways: at academic conferences (for example, the Joint Annual Conference of Media, Communications and Cultural Studies Association (MeCCSA), now incorporating the Association of Media Practice Educators (AMPE)); through academic publication; through relationships with cultural and creative industries (such as film festivals, broadcast, online communities, and creative partnerships).\n\nIn the UK there was a debate on whether submissions of practice research for peer review needed to be supported by a written statement evidencing the research, or whether the artefact could stand alone as research. According to the AHRC Review of Research Assessment (September 2003), there was need for a clearer articulation of the research process — including research methods, context and significance — in practice-led research that was submitted to the RAE in 2003.\n\nThe Arts and Humanities Research Council suggested that \"practice-led research\" should incorporate a scholarly apparatus that enabled other researchers to assess the value and significance of the results and that completed work should maintain a record or \"route map\" of the research process. Similarly the Communication, Cultural and Media Studies Panel Report on the UK's Research Assessment Exercise for 2003 valued practice that could give \"a reflexive account of itself as research\", but found that many practitioners did not explain the ways in which the work constituted original investigation.\n\n"}
{"id": "334639", "url": "https://en.wikipedia.org/wiki?curid=334639", "title": "Sluggish schizophrenia", "text": "Sluggish schizophrenia\n\nSluggish schizophrenia or slow progressive schizophrenia (, \"vyalotekushchaya shizofreniya\") was a diagnostic category used in Soviet Union to describe what they claimed was a form of schizophrenia characterized by a slowly progressive course; it was diagnosed even in a patient who showed no symptoms of schizophrenia or other psychotic disorders, on the assumption that these symptoms would appear later. It was developed in the 1960s by Soviet psychiatrist Andrei Snezhnevsky and his colleagues, and was used exclusively in the USSR and several Eastern Bloc countries, until the fall of Communism starting in 1989. The diagnosis has long been discredited because of its scientific inadequacy and its use as a means of confining dissenters. It has never been used or recognized outside of Soviet Union, or by international organizations such as the World Health Organization. It is considered a prime example of the political abuse of psychiatry in the Soviet Union.\n\nSluggish schizophrenia was the most infamous of diagnoses used by Soviet psychiatrists, due to its usage against political dissidents. After being discharged from a hospital, persons diagnosed with sluggish schizophrenia were deprived of their civic rights, credibility and employability. The usage of this diagnosis has been internationally condemned.\n\nIn the Russian version of the 10th revision of the International Statistical Classification of Diseases and Related Health Problems (ICD-10), which has long been used throughout present-day Russia, sluggish schizophrenia is no longer listed as a form of schizophrenia, but it is still included as a schizotypal disorder in section F21 of chapter V.\n\nAccording to Sergei Jargin, the same Russian term \"vyalotekushchaya\" for sluggish schizophrenia continues to be used and is now translated in English summaries of articles not as \"sluggish\" but as \"slow progressive\".\n\nIn the 1960s, professor Andrei Snezhnevsky, the most prominent theorist of Soviet psychiatry and director of the Institute of Psychiatry of the USSR Academy of Medical Sciences, developed a novel classification of mental disorders postulating an original set of diagnostic criteria. Snezhnevsky and his colleagues who developed the concept were supported by Soviet psychiatrists Fedor Kondratev, Sergei Semyonov, and Yakov Frumkin. All were members of the \"Moscow school\" of psychiatry.\n\nA majority of experts believe that the concept was developed under instructions from the Soviet secret service KGB and the Communist Party.\n\nPsychiatric diagnoses such as sluggish schizophrenia were used in the USSR for political purposes; the diagnosis of sluggish schizophrenia was most frequently used for Soviet dissidents. Sluggish schizophrenia as a diagnostic category was created to facilitate the stifling of dissidents and was a root of self-deception among psychiatrists to placate their consciences when the doctors acted as a tool of oppression in the name of a political system. American psychiatrist Peter Breggin points out that the term “sluggish schizophrenia” was created to justify involuntary treatment of political dissidents with drugs normally used for psychiatric patients.\n\nCritics implied that Snezhnevsky designed the Soviet model of schizophrenia (and this diagnosis) to make political dissent a mental illness.\n\nSt. Petersburg academic psychiatrist professor Yuri Nuller notes that the concept of Snezhnevsky's school allowed psychiatrists to consider, for example, schizoid psychopathy and even schizoid character traits as early, delayed in their development, stages of the inevitable progredient process, rather than as personality traits inherent to the individual, the dynamics of which might depend on various external factors. The same also applied to a number of other personality disorders. It entailed the extremely broadened diagnostics of sluggish (neurosis-like, psychopathy-like) schizophrenia. Despite a number of its controversial premises, but in line with the traditions of then Soviet science, Snezhnevsky's hypothesis immediately acquired the status of dogma, which was later overcome in other disciplines but firmly stuck in psychiatry. Snezhnevsky's concept, with its dogmatism, proved to be psychologically comfortable for many psychiatrists, relieving them from doubt when making a diagnosis.\n\nOn the covert orders of the KGB, thousands of social and political reformers—Soviet dissidents—were incarcerated in mental hospitals after being labelled with diagnoses of sluggish schizophrenia. Snezhnevsky himself diagnosed, or was otherwise involved in, a series of famous dissident cases, and in dozens of cases he personally signed a commission decision on the legal insanity of dissidents who were in fact mentally healthy, including Vladimir Bukovsky, Natalya Gorbanevskaya, Leonid Plyushch, , and Pyotr Grigorenko. Revaz Korinteli, a professor of the Grigol Robakidze University, says that Snezhnevsky broadened the borders of schizophrenia, and in this connection there was legal and theoretical justification for employing compulsory, involuntary treatment of dissenters in mental hospitals.\n\nAccording to the Global Initiative on Psychiatry chief executive Robert van Voren, the political abuse of psychiatry in the USSR arose from the concept that people who opposed the Soviet regime were mentally ill (since there was no logical reason to oppose the sociopolitical system considered the best in the world). The diagnosis of sluggish schizophrenia furnished a framework for explaining this behavior. This seemed to many Soviet psychiatrists a logical explanation for why someone would be willing to abandon his happiness, family, and career for a conviction so different from what most individuals seemed to believe.\n\nBecause of diagnoses of sluggish schizophrenia, Russia in 1974 had 5–7 cases of schizophrenia per 1,000 population, compared to 3–4 per 1,000 in the United Kingdom. In the 1980s, Russia had three times as many schizophrenic patients per capita as the USA, twice as many schizophrenic patients as West Germany, Austria and Japan, and more schizophrenic patients than any Western country. The city with the highest diagnosed prevalence of schizophrenia in the world was Moscow.\n\nAlong with paranoia, sluggish schizophrenia was the diagnosis most frequently used for the psychiatric incarceration of dissenters. Darrel Regier of the National Institute of Mental Health, one of the U.S. experts who visited Soviet psychiatric hospitals in 1989, testified that a \"substantial number\" of political dissenters had been recognized as mentally sick on the basis of such symptoms as \"anti-Soviet thoughts\" or \"delusions of reformism\".\n\nAccording to Moscow psychiatrist Alexander Danilin, the nosological approach in the Moscow psychiatric school established by Andrei Snezhnevsky (whom Danilin considered a state criminal) boiled down to the ability to diagnose schizophrenia.\n\nThe Soviet model of schizophrenia is based on the hypothesis that a fundamental characteristic (by which schizophrenia spectrum disorders are distinguished clinically) is its longitudinal course. The hypothesis implies three main types of schizophrenia: \n\nThe classification of schizophrenia types attributed to Snezhnevsky is still used in Russia, and considers sluggish schizophrenia an example of the continuous type. The prevalence of Snezhnevsky's theories has particularly led to a broadening of the boundaries of disease such that even the mildest behavioral change is interpreted as indication of mental disorder.\n\nA carefully crafted description of sluggish schizophrenia established that psychotic symptoms were non-essential for the diagnosis, but symptoms of psychopathy, hypochondria, depersonalization or anxiety were central to it. Symptoms considered part of the \"negative axis\" included pessimism, poor social adaptation and conflict with authorities, and were themselves sufficient for a formal diagnosis of \"sluggish schizophrenia with few symptoms\". \nAccording to Snezhnevsky, patients with sluggish schizophrenia could present as seemingly sane but manifest minimal (and clinically relevant) personality changes which could remain unnoticed by the untrained eye. Patients with non-psychotic mental disorders (or who were not mentally ill) could be diagnosed with sluggish schizophrenia.\nHarold Merskey and Bronislava Shafran write that many conditions which would probably be diagnosed elsewhere as hypochondriacal or personality disorders, anxiety disorders or depressive disorders appear liable to come under the banner of slowly progressive schizophrenia in Snezhnevsky's system.\n\nThe incidence of sluggish schizophrenia increased because, according to Snezhnevsky and his colleagues, patients with this diagnosis were capable of socially functioning almost normally. Their symptoms could resemble those of a neurosis or paranoia. Patients with paranoid symptoms retained insight into their condition, but overestimated their significance and had grandiose ideas of reforming society. Sluggish schizophrenia could have such symptoms as \"reform delusions\", \"perseverance\" and \"struggle for the truth\". As Viktor Styazhkin reported, Snezhnevsky diagnosed a reform delusion in every case where a patient \"develops a new principle of human knowledge, drafts an ideal of human happiness or other projects for the benefit of mankind\".\nDuring the 1960s and 1970s, theories which contained ideas about reforming society, struggling for the truth, and religious convictions were not considered delusional paranoid disorders in nearly any foreign classifications; however, Soviet psychiatry (for ideological reasons) considered critiques of the political system and proposals to reform it as delusional behavior. The diagnoses of sluggish schizophrenia and paranoid states with delusions of reform were used only in the Soviet Union and several Eastern European countries.\n\nAn audience member at a lecture by Georgi Morozov on forensic psychiatry in the Serbsky Institute asked, “Tell us, Georgi Vasilevich, what is actually the diagnosis of sluggish schizophrenia?” Since the question was asked ironically Morozov replied ironically: “You know, dear colleagues, this is a very peculiar disease. There are not delusional disorders, there are not hallucinations, but there is schizophrenia!”\n\nThe two Soviet psychiatrists Marat Vartanyan and Andrei Mukhin in their interview to the Soviet newspaper \"Komsomolskaya Pravda\" issued on 15 July 1987 explained how it was possible that a person might be mentally ill, while people surrounding him did not notice it, for example, in the case of \"sluggish schizophrenia\". What was meant by saying that a person is mentally ill? Marat Vartanyan said, \"… When a person is obsessively occupied with something. If you discuss another subject with him, he is a normal person who is healthy, and who may be your superior in intelligence, knowledge and eloquence. But as soon as you mention his favourite subject, his pathological obsessions flare up wildly.\" Vartanyan confirmed that hundreds of people with this diagnosis were hospitalized in the Soviet Union. According to Mukhin, it took place because \"they disseminate their pathological reformist ideas among the masses.\" A few months later the same newspaper listed \"an exceptional interest in philosophical systems, religion and art\" among symptoms of sluggish schizophrenia from a \"Manual on Psychiatry\" of Snezhnevsky's Moscow school.\n\nOnly specially instructed psychiatrists could recognize sluggish schizophrenia to indefinitely treat dissenters in a \"Special Psychiatric Hospital\" with heavy doses of antipsychotic medication. Convinced of the immortality of the totalitarian USSR, Soviet psychiatrists, especially in Moscow, did not hesitate to form \"scientific\" articles and defend dissertations by using the cases of dissidents. For example, Snezhnevsky diagnosed dissident Vladimir Bukovsky as schizophrenic on 5 July 1962 and on 12 November 1971 wrote to writer Viktor Nekrasov that the characteristics of Bukovsky's mental disease were included in the dissertation by Snezhnevsky's colleague. All the paper products were available in medical libraries. As Semyon Gluzman recollects, when he returned to Kiev in 1982 after his absence of ten years, he was amazed to see all this \"scientific\" literature in open storage at the Kiev medical library and was even more amazed to read all the \"ridiculous stuff\" hardly put into scientific psychiatric terminology. In their papers and dissertations on treatment for litigiousness and reformism, Kosachyov and other Soviet psychiatrists recommended compulsory treatment for persons with litigiousness and reformism, in the same psychiatric hospitals used for murderers: \nWesterners first became aware of sluggish schizophrenia and its political uses in the mid-1970s, as a result of the high reported incidence of schizophrenia in the Russian population. Snezhnevsky was personally attacked in the West as an example of psychiatric abuse in the USSR. He was charged with cynically developing a system of diagnosis that could be bent for political purposes. American psychiatrist Alan A. Stone stated that Western criticism of Soviet psychiatry focused on Snezhnevsky personally because he was responsible for the diagnosis of sluggish schizophrenia for \"reformism\" and other such symptoms.\n\nIn 2010, Yuri Savenko, the president of the Independent Psychiatric Association of Russia, warned that Professor Anatoly Smulevich, author of the monographs \"Problema Paranoyi\" (\"The Problem of Paranoia\") (1972) and \"Maloprogredientnaya Shizofreniya\" (\"Continuous Sluggish Schizophrenia\") (1987), which had contributed to the hyperdiagnosis of sluggish schizophrenia, had again begun to play the same role. Under his influence, therapists have begun to widely use antidepressants and antipsychotics but often in inadequate cases and in inappropriate doses, without consulting psychiatrists. This situation has opened up a huge new market for pharmaceutical firms, and the flow of the mentally ill to internists.\n\nIn their joint book \"Sociodinamicheskaya Psikhiatriya\" (\"Sociodynamic Psychiatry\"), Doctor of Medical Sciences professor of psychiatry Caesar Korolenko and Doctor of Psychological Sciences Nina Dmitrieva note that Smulevich's clinical description of sluggish schizophrenia is extremely elusive and includes almost all possible changes in mental status and conditions that occur in a person without psychopathology: euphoria, hyperactivity, unfounded optimism, irritability, explosiveness, sensitivity, inadequacy and emotional deficit, hysterical reactions with conversive and dissociative symptoms, infantilism, obsessive-phobic states and stubbornness. At present, the hyperdiagnosis of schizophrenia becomes especially negative due to a large number of schizophreniform psychoses caused by the increasing popularity of various esoteric sects. They practice meditation, sensory deprivation, special exercises with rhythmic movements which directly stimulate the deep subconscious and, by doing so, lead to the development of psychoses with mainly reversible course. Smulevich bases the diagnosis of continuous sluggish schizophrenia, in particular, on appearance and lifestyle and stresses that the forefront in the picture of negative changes is given to the contrast between retaining mental activity (and sometimes quite high capacity for work) and mannerism, unusualness of one's appearance and entire lifestyle. In his 2014 interview, Anatoly Smulevich says, \"Now everything has slightly turned in a different way, sluggish schizophrenia has been transformed into schizotypal disorder, etc. I think it is not the end of his [Snezhnevsky's] teaching, because after a while, everything will get back into a rut, but it will not be a simple repetition but will get some new direction.\"\n\nIn 2009, Tatyana Dmitrieva, the then director of the Serbsky Center, said to the BBC Russian Service, \"A diagnosis is now made only according to the international classification, so called ICD-10. In this classification, there is no sluggish schizophrenia, and therefore, even this diagnosis has not just been made for a long time.\" However, according to the 2012 interview by the president of the Ukrainian Psychiatric Association Semyon Gluzman to Radio Liberty, though the diagnosis of sluggish schizophrenia no longer exists in Ukraine, in Russia, as far as he knows, this diagnosis still exists, and was given to Mikhail Kosenko, one of the accused in the Bolotnaya Square case. The prosecution's case for his forced hospitalization rested on confirmation of the diagnosis of sluggish schizophrenia that he has been treated for over the last 12 years, until 2013 when the diagnosis was changed to that of paranoid schizophrenia by the Serbsky Center experts who examined Kosenko and convinced the court to send him for compulsory treatment to a psychiatric hospital. Zurab Kekelidze (), who heads the Serbsky Center and is the chief psychiatrist of the Ministry of Health and Social Development of the Russian Federation, confirmed that Kosenko was diagnosed with sluggish schizophrenia.\n\nAccording to the commentary by the Independent Psychiatric Association of Russia on the 2007 text by Vladimir Rotstein, a doctrinist of Snezhnevsky's school, there are sufficient patients with delusion of reformism in psychiatric inpatient facilities for involuntary treatment. In 2012, delusion of reformism was mentioned as a symptom of mental disorder in \"Psychiatry: National Manual\". In the same year, Vladimir Pashkovsky in his paper reported that he diagnosed 4.7 percent of 300 patients with delusion of reform. As Russian sociologist Alexander Tarasov wrote, \"you will be treated in a hospital so that you and all your acquaintances get to learn forever that only such people as Anatoly Chubais or German Gref can be occupied with reforming in our country.\" According to Raimonds Krumgolds, a former member of the political party The Other Russia, he was examined because of his \"delusion of reformism\", which gave rise to an assumption of slow progressive schizophrenia. In 2012, Tyuvina and Balabanova in their joint paper reported that they used sulpiride to treat slow progressive schizophrenia.\n\n\n"}
{"id": "540497", "url": "https://en.wikipedia.org/wiki?curid=540497", "title": "Timaeus (dialogue)", "text": "Timaeus (dialogue)\n\nTimaeus (; , ) is one of Plato's dialogues, mostly in the form of a long monologue given by the title character Timaeus of Locri, written c. 360 BC. The work puts forward speculation on the nature of the physical world and human beings and is followed by the dialogue \"Critias\".\n\nParticipants in the dialogue include Socrates, Timaeus, Hermocrates, and Critias. Some scholars believe that it is not the Critias of the Thirty Tyrants who is appearing in this dialogue, but his grandfather, who is also named Critias. It has been suggested that \"Timaeus\" was influenced by a book about Pythagoras, written by Philolaus.\n\nThe dialogue takes place the day after Socrates described his ideal state. In Plato's works such a discussion occurs in the Republic. Socrates feels that his description of the ideal state wasn't sufficient for the purposes of entertainment and that \"I would be glad to hear some account of it engaging in transactions with other states\" (19b).\n\nHermocrates wishes to oblige Socrates and mentions that Critias knows just the account (20b) to do so. Critias proceeds to tell the story of Solon's journey to Egypt where he hears the story of Atlantis, and how Athens used to be an ideal state that subsequently waged war against Atlantis (25a). Critias believes that he is getting ahead of himself, and mentions that Timaeus will tell part of the account from the origin of the universe to man.\n\nCritias also cites the Egyptian priest in Sais about long term factors on the fate of mankind:\"\"There have been, and will be again, many destructions of mankind arising out of many causes; the greatest have been brought about by the agencies of fire and water, and other lesser ones by innumerable other causes. There is a story that even you [Greeks] have preserved, that once upon a time, Phaethon, the son of Helios, having yoked the steeds in his father's chariot, because he was not able to drive them in the path of his father, burnt up all that was upon the earth, and was himself destroyed by a thunderbolt. Now this has the form of a myth, but really signifies a declination of the bodies moving in the heavens around the earth, and a great conflagration of things upon the earth, which recurs after long intervals.\"\"\n\nThe history of Atlantis is postponed to \"Critias\". The main content of the dialogue, the exposition by Timaeus, follows.\n\nTimaeus begins with a distinction between the physical world, and the eternal world. The physical one is the world which changes and perishes: therefore it is the object of opinion and unreasoned sensation. The eternal one never changes: therefore it is apprehended by reason (28a).\n\nThe speeches about the two worlds are conditioned by the different nature of their objects. Indeed, \"a description of what is changeless, fixed and clearly intelligible will be changeless and fixed,\" (29b), while a description of what changes and is likely, will also change and be just likely. \"As being is to becoming, so is truth to belief\" (29c). Therefore, in a description of the physical world, one \"should not look for anything more than a likely story\" (29d).\n\nTimaeus suggests that since nothing \"becomes or changes\" without cause, then the cause of the universe must be a demiurge or a god, a figure Timaeus refers to as the father and maker of the universe. And since the universe is fair, the demiurge must have looked to the eternal model to make it, and not to the perishable one (29a). Hence, using the eternal and perfect world of \"forms\" or ideals as a template, he set about creating our world, which formerly only existed in a state of disorder.\n\nTimaeus continues with an explanation of the creation of the universe, which he ascribes to the handiwork of a divine craftsman. The demiurge, being good, wanted there to be as much good as was the world. The demiurge is said to bring order out of substance by imitating an unchanging and eternal model (paradigm). The \"ananke\", often translated as 'necessity', was the only other co-existent element or presence in Plato's cosmogony. Later Platonists clarified that the eternal model existed in the mind of the Demiurge.\n\nTimaeus describes the substance as a lack of homogeneity or balance, in which the four elements (earth, air, fire and water) were shapeless, mixed and in constant motion. Considering that order is favourable over disorder, the essential act of the creator was to bring order and clarity to this substance. Therefore, all the properties of the world are to be explained by the demiurge's choice of what is fair and good; or, the idea of a dichotomy between good and evil.\n\nFirst of all, the world is a \"living creature\". Since the unintelligent creatures are in their appearance less fair than intelligent creatures, and since intelligence needs to be settled in a soul, the demiurge \"put intelligence in soul, and soul in body\" in order to make a living and intelligent whole. \"Wherefore, using the language of probability, we may say that the world became a living creature truly endowed with soul and intelligence by the providence of God\" (30a-b).\n\nThen, since the part is imperfect compared to the whole, the world had to be one and only. Therefore, the demiurge did not create several worlds, but a single unique world (31b). Additionally, because the demiurge wanted his creation to be a perfect imitation of the Eternal \"One\" (the source of all other emanations), there was no need to create more than one world.\n\nThe creator decided also to make the perceptible body of the universe by four elements, in order to render it \"proportioned\". Indeed, in addition to fire and earth, which make bodies visible and solid, a third element was required as a mean: \"two things cannot be rightly put together without a third; there must be some bond of union between them\". Moreover, since the world is not a surface but a solid, a fourth mean was needed to reach harmony: therefore, the creator placed water and air between fire and earth. \"And for these reasons, and out of such elements which are in number four, the body of the world was created, and it was harmonised by proportion\" (31-33).\n\nAs for the figure, the demiurge created the world in the geometric form of a \"globe\". Indeed, the round figure is the most perfect one, because it comprehends or averages all the other figures and it is the most omnimorphic of all figures: \"he [the demiurge] considered that the like is infinitely fairer than the unlike\" (33b).\n\nThe creator assigned then to the world a rotatory or \"circular movement\", which is the \"most appropriate to mind and intelligence\" on account of its being the most uniform (34a).\n\nFinally, he created the soul of the world, placed that soul in the center of the world's body and diffused it in every direction. Having thus been created as a perfect, self-sufficient and intelligent being, the world is a \"god\" (34b).\n\nTimaeus then explains how the soul of the world was created (Plato's following discussion is obscure, and almost certainly intended to be read in light of the \"Sophist\"). The demiurge combined three elements: two varieties of \"Sameness\" (one indivisible and another divisible), two varieties of \"Difference\" (again, one indivisible and another divisible), and two types of \"Being\" (or \"Existence,\" once more, one indivisible and another divisible). From this emerged three compound substances, intermediate (or mixed) Being, intermediate Sameness, and intermediate Difference. From this compound one final substance resulted, the World Soul. He then divided following precise mathematical proportions, cutting the compound lengthways, fixed the resulting two bands in their middle, like in the letter Χ (chi), and connected them at their ends, to have two crossing circles. The demiurge imparted on them a circular movement on their axis: the outer circle was assigned Sameness and turned horizontally to the right, while the inner circle was assigned to Difference and turned diagonally and to the left (34c-36c).\n\nThe demiurge gave the primacy to the motion of Sameness and left it undivided; but he divided the motion of Difference in six parts, to have seven unequal circles. He prescribed these circles to move in opposite directions, three of them with equal speeds, the others with unequal speeds, but always in proportion. These circles are the orbits of the heavenly bodies: the three moving at equal speeds are the Sun, Venus and Mercury, while the four moving at unequal speeds are the Moon, Mars, Jupiter and Saturn (36c-d). The complicated pattern of these movements is bound to be repeated again after a period called a 'complete' or 'perfect' year (39d).\n\nThen, the demiurge connected the body and the soul of the universe: he diffused the soul from the center of the body to its extremities in every direction, allowing the invisible soul to envelop the visible body. The soul began to rotate and this was the beginning of its eternal and rational life (36e).\n\nTherefore, having been composed by Sameness, Difference and Existence (their mean), and formed in right proportions, the soul declares the sameness or difference of every object it meets: when it is a sensible object, the inner circle of the Diverse transmit its movement to the soul, where opinions arise, but when it is an intellectual object, the circle of the Same turns perfectly round and true knowledge arises (37a-c).\n\nTimaeus claims that the minute particle of each element had a special geometric shape: tetrahedron (fire), octahedron (air), icosahedron (water), and cube (earth).\n\nThe \"Timaeus\" makes conjectures on the composition of the four elements which some ancient Greeks thought constituted the physical universe: earth, water, air, and fire. Timaeus links each of these elements to a certain Platonic solid: the element of earth would be a cube, of air an octahedron, of water an icosahedron, and of fire a tetrahedron. Each of these perfect polyhedra would be in turn composed of triangular faces the 30-60-90 and the 45-45-90 triangles. The faces of each element could be broken down into its component right-angled triangles, either isosceles or scalene, which could then be put together to form all of physical matter. Particular characteristics of matter, such as water's capacity to extinguish fire, was then related to shape and size of the constituent triangles. The fifth element (i.e. Platonic solid) was the dodecahedron, whose faces are not triangular, and which was taken to represent the shape of the Universe as a whole, possibly because of all the elements it most approximates a sphere, which Timaeus has already noted was the shape into which God had formed the Universe.\n\nThe extensive final part of the dialogue addresses the creation of humans, including the soul, anatomy, perception, and transmigration of the soul.\n\nThe \"Timaeus\" was translated into Latin by Cicero, and the first part (to 53c) was again translated by Calcidius c. 321 AD. Calcidius' partial translation of the \"Timaeus\" was the only Platonic dialogue, and one of the few works of classical natural philosophy, available to Latin readers in the early Middle Ages. Thus it had a strong influence on medieval Neoplatonic cosmology and was commented on particularly by 12th century Christian philosophers of the Chartres School, such as Thierry of Chartres and William of Conches, who, interpreting it in the light of the Christian faith, understood the dialogue to refer to a creatio ex nihilo.\n\n\n\n"}
{"id": "544460", "url": "https://en.wikipedia.org/wiki?curid=544460", "title": "Unweaving the Rainbow", "text": "Unweaving the Rainbow\n\nUnweaving the Rainbow: Science, Delusion and the Appetite for Wonder is a 1998 book by Richard Dawkins, in which the author discusses the relationship between science and the arts from the perspective of a scientist.\n\nDawkins addresses the misperception that science and art are at odds. Driven by the responses to his books \"The Selfish Gene\" and \"The Blind Watchmaker\" wherein readers resented his naturalistic world view, seeing it as depriving life of meaning, Dawkins felt the need to explain that, as a scientist, he saw the world as full of wonders and a source of pleasure. This pleasure was not in spite of, but rather because he does not assume as cause the inexplicable actions of a deity but rather the understandable laws of nature.\n\nHis starting point is John Keats' well-known, light-hearted accusation that Isaac Newton destroyed the poetry of the rainbow by 'reducing it to the prismatic colours.' Dawkins's agenda is to show the reader that science does not destroy, but rather discovers poetry in the patterns of nature.\n\nIt is of little concern whether or not science can prove that the ultimate fate of the cosmos lacks purpose: we live our lives regardless at a \"human\" level, according to ambitions and perceptions which come more naturally. Therefore, science should not be feared as a sort of cosmological wet blanket. In fact, those in search of beauty or poetry in their cosmology need not turn to the paranormal or even necessarily restrict themselves to the mysterious: science itself, the business of unravelling mysteries, is beautiful and poetic. (The rest of the preface sketches an outline of the book, makes acknowledgements, etc.)\n\n\"We are going to die, and that makes us the lucky ones. Most people are never going to die because they are never going to be born. The potential people who could have been here in my place but who will in fact never see the light of day outnumber the sand grains of Arabia. Certainly those unborn ghosts include greater poets than Keats, scientists greater than Newton. We know this because the set of possible people allowed by our DNA so massively outnumbers the set of actual people. In the teeth of these stupefying odds it is you and I, in our ordinariness, that are here. We privileged few, who won the lottery of birth against all odds, how dare we whine at our inevitable return to that prior state from which the vast majority have never stirred?\"\n\nThe first chapter describes several ways in which the universe appears beautiful and poetic when viewed scientifically. However, it first introduces an additional reason to embrace science. Time and space are vast, so the probability that the reader came to be alive here and now, as opposed to another time or place, was slim. More important, the probability that the reader came to be alive at all were even slimmer: the correct structure of atoms had to align in the universe.\nGiven how special these circumstances are, the \"noble\" thing to do is employ the allotted several decades of human life towards understanding that universe. Rather than simply feeling connected with nature, one should rise above this \"anaesthetic of familiarity\" and \"observe\" the universe scientifically.\n\nThis chapter describes a third reason to embrace science (the first two being beauty and duty): improving one's performance in the arts. Science is often presented publicly in a translated format, \"dumbed down\" to fit the language and existing ideas of non-scientists. This offers a disservice to the public, who are capable of appreciating the beauty of the universe as deeply as a scientist can. The successful communication of unadulterated science enhances, not confuses, the arts; after all, poets (Dawkins' synonym for artists—see page 24) and scientists are motivated by a similar spirit of wonder. We should therefore battle the stereotype that science is difficult, uncool, and not useful for the common person.\n\nStudying a phenomenon, such as a flower, cannot detract from its beauty.\nFirst, some scientists, such as Feynman, are able to appreciate the aesthetics of the flower while engaged in their study. Second, the mysteries which science unfolds lead to new and more exciting mysteries; for example, botany's findings might lead us to wonder about the workings of a fly's consciousness.\nThis effect of multiplying mysteries should satisfy even those who think that scientific understanding is at odds with aesthetics, e.g. people who agree with Einstein that \"the most beautiful thing we can experience is the mysterious\".\n\nThis chapter offers more evidence that science is fun and poetic, by exploring sound waves, birdsong, and low-frequency phenomena such as pendula and periodic mass extinctions.\n\nA fourth reason to embrace science is that it can help deliver justice in a court of law, via DNA fingerprinting or even via simple statistical reasoning.\nEveryone should learn the scientist's art of probability assessment, to make better decisions.\n\nThis chapter explores what Dawkins considers to be fallacies in astrology, religion, magic, and extraterrestrial visitations. Credulity and Hume's criterion are also discussed.\n\nAmazing coincidences are much more common than we may think, and sometimes, when over-interpreted, they lead to faulty conclusions. Statistical significance tests can help determine which patterns are meaningful.\n\nUnlike \"magisterial poetry\" (where metaphors and pretty language are used to describe the familiar), \"pupillary poetry\" uses poetic imagery to assist a scientist's thinking about the exotic (e.g. consider \"being\" an electron temporarily). Although it is useful, some authors take pupillary poetry too far, and, \"drunk on metaphor\", they produce \"bad science\"; i.e. postulate faulty theories. This is powered by humanity's natural tendency to look for representations.\n\nGenes compete with each other, but this occurs within the context of collaboration, as is shown with examples involving mitochondria, bacteria, and termites. Two types of collaboration are co-adaptation (tailoring simultaneously the different parts of an organism, such as flower colour and flower markings), and co-evolution (two species changing together; e.g. predator and prey running speeds may increase together in a sort of arms race).\n\nThe body of any organism provides clues about its habitat. The genes allow one to reconstruct a picture of the range of ways of life that the species has experienced; in this sense DNA would act as a palimpsestic \"digital archive\" if only its language of encoding history could be fully understood. Finally, the curious genetics of cuckoos is discussed.\n\nThe brain is akin to a powerful computer, which creates a sort of virtual reality to model economically the environment. Neural circuitry is discussed, and a comparison is made between brains and genes: albeit over different time scales, both record the environment's past to help the organism make the optimal actions in the (predicted) future.\n\nThe simultaneous explosions in hardware and software of the 20th century are together an example of what Dawkins calls \"self-feeding co-evolution\".\nA similar event occurred over a longer time scale (millions of years) when the minds and brains of our ancestors simultaneously improved very rapidly. Five possible triggers of this improvement were: language, map reading, ballistics, memes, and metaphors/analogies.\n\nThe final two paragraphs of \"The balloon of the mind\" conclude by saying that human beings are the only animal with a sense of purpose in life, and that that purpose should be to construct a comprehensive model of how the universe works.\n\nThe book coins an acronymical term, \"Petwhac\" \"(Population of Events That Would Have Appeared Coincidental)\". This is defined as all those events that may be considered to be a 'coincidence' if studied casually, but are both possible and statistically probable.\n\nA way to get an idea of how to use the petwhac is as follows. Say you see a friend from school you have not seen for years when you are on holiday (an unlikely event); before saying it is fate or coincidence, think what is in the petwhac (meeting any friend from the same time period at least, friends of your brothers, sisters or parents, old flames, neighbours, teachers, someone who worked in the local chip-shop... the list is probably endless, and all would seem coincidental). In short: the bigger the petwhac, the stronger case you have to avoid ascribing something to fate or coincidence.\n\nDawkins offers several examples of petwhacs in the book, two of which are the bedside clock of a woman (Richard Feynman's wife) stopping exactly when she died, and a psychic who stops the watches of his television audience.\n\nThe first is explained by the fact that the clock had a mechanical defect which made it stop when tilted off the horizontal, which is what a nurse did to read the time of death in poor lighting conditions. The matter of the watches, in Dawkins' own words, is explained thus —\n\n\"If somebody's watch stopped three weeks after the spell was cast, even the most credulous would prefer to put it down to chance. We need to decide how large a delay would have been judged by the audience as sufficiently simultaneous with the psychic's announcement to impress. About five minutes is certainly safe, especially since he can keep talking to each caller for a few minutes before the next call ceases to seem roughly simultaneous. There are about 100,000 five-minute periods in a year. The probability that any given watch, say mine, will stop in a designated five-minute period is about 1 in 100,000. Low odds, but there are 10 million people watching the show. If only half of them are wearing watches, we could expect about 25 of those watches to stop in any given minute. If only a quarter of these ring into the studio, that is 6 calls, more than enough to dumbfound a naïve audience. Especially when you add in the calls from people whose watches stopped the day before, people whose watches didn't stop but whose grandfather clocks did, people who died of heart attacks and their bereaved relatives phoned in to say that their 'ticker' gave out, and so on.\"\nDawkins defends his choice of the word \"population\" by writing \"Population may seem an odd word, but it is the correct statistical term.\", adding \"I won't keep using capital letters because they stand so unattractively on the page.\"\n\n"}
{"id": "36990855", "url": "https://en.wikipedia.org/wiki?curid=36990855", "title": "Violation paradigm", "text": "Violation paradigm\n\nA violation paradigm is a scientific method where the scientist perturbs an expected factor to look at the subject's following reactions. These reactions are believed to be relevant to the process studied. For example, creating wrong word segmentations in a text will destabilize the reader. This warns the researcher that the respondent's brain considers the characters are united into words, and not just as a succession of given sets of letters. The process was originally developed by Danks, Bohn & Fear (1983), and proved valid ().\n\n"}
{"id": "57917109", "url": "https://en.wikipedia.org/wiki?curid=57917109", "title": "Wolfstein, the Murderer", "text": "Wolfstein, the Murderer\n\nWolfstein, The Murderer; or, The Secrets of a Robber’s Cave is an 1850 chapbook based on Percy Bysshe Shelley’s 1811 Gothic horror novel \"St. Irvyne; or, The Rosicrucian\".\n\nThe 1811 novel \"St. Irvyne, or, The Rosicrucian\" was republished by John Joseph Stockdale in 1822 following Shelley’s death. Two chapbooks were also published based on the novel. No publication date appeared on the title page.\nThe first chapbook version was entitled \"Wolfstein; or, The Mysterious Bandit\" and was published and printed by John Bailey at 116, Chancery Lane in London in 1822. The chapbook was a condensed version of the novel in 20 pages. The total length was 28 pages including the second story. Chapbooks were meant for popular consumption, serving the same function as a paperback would. The chapbook sold for sixpence.\n\nAnother more condensed twelve-page chapbook was published in 1850 by Thomas Redriffe in London entitled \"Wolfstein, the Murderer; or, The Secrets of a Robber's Cave: A Terrific Romance. To which is Added, The Two Serpents, an Oriental Apologue.\" The Ossian epigraph appeared on the title page: \"A tale of horror, of murder, and of deeds done in darkness.\" Printed for Thomas Redriffe, Piccadilly. The price was \"Two Pence\". Printed by William Bethell, 10, Marshall-street, Liverpool. No date of publication appeared on the title page. The story was six pages long, pages three through eight. The second story was four pages long, pages nine through twelve.\n\nThe frontispiece consisted of a drawing of Wolfstein confronted by a skeleton struck by lightning. He stands over the corpse of Serena. The drawing is a more condensed version of the 1822 frontispiece. The caption reads: \"Deeper grew the gloom of the cavern, and darkness seemed to press around him. Suddenly a flash of lightning burst through the cavern, followed by thunder that appeared to convulse the universal fabric of nature; and borne on the sulphurous blast, the Prince of Terrors stood before him.\"\n\nThe chapbook follows the plot of the first section of the novel \"St. Irvyne\" on the bandits but omits the second part featuring Frederic Nempere in Geneva. The account is radically condensed.\nThe name of Cavigni, leader of the bandits, is changed to Stiletto. The name of Megalena is changed to Serena. The character of Ginotti, the Rosicrucian alchemist, does not appear.\n\nThe opening scene is of a raging thunderstorm. Wolfstein is a wanderer in the Swiss Alps who seeks cover from the storm. He is a distraught outcast who plans to commit suicide. A group of monks carrying a body for burial in a torch-light procession runs into him and saves his life.\n\nBandits then attack them. Wolfstein is accepted as a member of the bandits. He becomes used to a life of crime. But a rivalry develops between him and Stilleto over Serena. He decides to poison the chief by secretly placing a white powder in his wine. Stiletto drinks the poison and dies.\n\nWolfstein is subsequently elected Captain by the other bandits after the murder of Stiletto. He then is racked by dreams of the murdered Cavigni which fill him with dread and foreboding.\nDays after the murder, Wolfstein seeks to seduce Serena. He observes her in prayer. She refuses his advances and pulls out a dagger. In response, Wolfstein unsheaths his sword and stabs her to death.\n\nThunder was heard in the cell. The Prince of Terror then appeared before him. A “strange and unnatural stench” infused the room as “the yawning gulf of hell” swallowed Wolfstein as blue flames swirled around his body.\n\nThe final paragraph concludes with a moral of the story. Plunging into despair and indulging in crime are not the ways to confront misfortune. The tenets of morality and truth should be diligently followed. Only “misery, disgrace and ruin” result when these principles are ignored.\n\n \n"}
