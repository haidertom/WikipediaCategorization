{"id": "11783757", "url": "https://en.wikipedia.org/wiki?curid=11783757", "title": "Bateson's cube", "text": "Bateson's cube\n\nBateson's cube is a model of the cost–benefit analysis for animal research developed by Professor Patrick Bateson, president of the Zoological Society of London. \n\nBateson's cube evaluates proposed research through three criteria:\n\n\nBateson suggested that research that does not meet these requirements should not be approved or performed, in accordance with the Animals (Scientific Procedures) Act 1986. It is not intended as a formal model for optimal trade-offs, but rather a tool for making judicial decisions, since the three axes are not in a common currency. The third criterion also does not necessarily have to be medical benefit, but could be a wider form of utility. \n"}
{"id": "3402144", "url": "https://en.wikipedia.org/wiki?curid=3402144", "title": "Biomedical scientist", "text": "Biomedical scientist\n\nA biomedical scientist is a scientist trained in biology, particularly in the context of medicine. These scientists work to gain knowledge on the main principles of how the human body works and to find new ways to cure or treat disease by developing advanced diagnostic tools or new therapeutic strategies. The research of biomedical scientists is referred to as biomedical research.\n\nThe specific activities of the biomedical scientist can differ in various parts of the world and vary with the level of education. Generally speaking, biomedical scientists conduct research in a laboratory setting, using living organisms as models to conduct experiments. These can include cultured human or animal cells grown outside of the whole organism, small animals such as flies, worms, fish, mice, and rats, or, rarely, larger animals and primates. Biomedical scientists may also work directly with human tissue specimens to perform experiments as well as participate in clinical research.\n\nBiomedical scientists employ a variety of techniques in order to carry out laboratory experiments. These include:\n\nBiomedical scientists typically obtain a bachelor of science degree, and usually take postgraduate studies leading to a diploma, master or doctorate. This degree is necessary for faculty positions at academic institutions, as well as senior scientist positions at most companies. Some biomedical scientists also possess a medical degree (MD, DO, PharmD, Doctor of Medical Laboratory Sciences[MLSD], MBBS, etc.) in addition to an academic degree.\n\nThis category includes tenured faculty positions at universities, colleges, non-profit research institutes, and sometimes hospitals. These positions usually afford more intellectual freedom and give the researcher more latitude in the direction and content of the research. Scientists in academic settings, in addition to conducting experiments, will also attend scientific conferences, compete for research grant funding, publish scientific papers, and teach classes.\n\nIndustry jobs refer to private sector jobs at for-profit corporations. In the case of biomedical scientists, employment is usually at large pharmaceutical companies or biotechnology companies. Positions in industry tend to pay higher salaries than those at academic institutions, but job security compared to tenured academic faculty is significantly less. Researchers in industry tend to have less intellectual freedom in their research than those in the academic sector, owing to the ultimate goal of producing marketable products that benefit the company.\n\nIn recent years, more biomedical scientists have pursued careers where advanced education and experience in biomedical research is needed outside of traditional laboratory research. These areas include patent law, consulting, public policy, and science journalism. The primary reason for growth in these areas is that in recent years fewer positions are available in traditional academic research relative to the number of seekers; approximately 15-20% of PhD life scientists will obtain a tenure-track position or lab-head position in industry.\n\n\"Biomedical scientist\" is the protected title used by professionals qualified to work unsupervised within the pathology department of a hospital. The biomedical sciences are made up of the following disciplines; biochemistry, haematology, immunology, microbiology, histology, cytology, and transfusion services. These professions are regulated within the United Kingdom by the Health and Care Professions Council. Anyone who falsely claims to be a biomedical scientist commits an offence and could be fined up to £5000.\n\nEach department specialises in aiding the diagnosis and treatment of disease. Entry to the profession requires an Institute of Biomedical Science (IBMS) accredited BSc honours degree followed by a minimum of 12 months laboratory training in one of the pathology disciplines, however the actual time spent training can be considerably longer. Trainees are also required to complete a certificate of competence training portfolio, this requires gathering extensive amounts of evidence to demonstrate professional competence. At the end of this period the trainees portfolio and overall competence are assessed; if successful, a certificate of competence is awarded, which can be used to apply for registration with the HCPC. State registration indicates that the applicant has reached a required standard of education and will follow the guidelines and codes of practice created by the Health and Care Professions Council. The NHS, the largest employer of Biomedical Scientist, now run the 'Practitioners Training Program' in conjunction with several Universities which includes a years experienced as a part of a 3-year degree. This is known as BSc Healthcare Science (Life Science) \nBiomedical Scientists are the second largest profession registered by the Health and Care Professions Council and make up a vital component of the health care team. Many of the decisions doctors make are based on the test results generated by Biomedical Scientists. Despite this, much of the general public are unaware of Biomedical Scientists and the important role they play.\n\nBiomedical Scientists are not confined to NHS laboratories. Biomedical Scientists along with scientists in other inter-related medical disciplines seek out to understand human anatomy, genetics, immunology, physiology and behaviour at all levels. This is sometimes achieved through the use of model systems that are homologous to various aspects of human biology. The research that is carried out either in Universities or Pharmaceutical companies by Biomedical Scientists has led to the development of new treatments for a wide range of degenerative and genetic disorders. Stem cell biology, cloning, genetic screening/therapies and other areas of biomedical science have all been generated by the work of Biomedical Scientists from around the world.\n\nBiomedical science graduate programs are maintained at academic institutions and medical schools around the world, and some biomedical graduate programs are administered jointly by an academic institution and a business, hospital, or independent research institute. While graduate students historically committed to a particular research specialty, such as molecular biology, biochemistry, genetics, or developmental biology, the recent trend (particularly in the United States) is to offer interdisciplinary programs that do not specialize and instead aim to incorporate a broad education in multiple biological disciplines.\n\nInitially, graduate students usually rotate through the laboratories of several faculty researchers, after which the student commits to joining a particular laboratory for the remainder of his or her education. The remaining time is spent conducting original research under the direction of the principal investigator to complete and publish a dissertation. Unlike undergraduate and professional schools, there is no set time period for graduate education. Students graduate once a thesis project of significant scope to justify the writing of their dissertation has been completed, a point that is determined by the student's principal investigator as well as his or her faculty advisory committee. The average time to graduation can vary between institutions, but most programs average around 5–6 years.\n\nBiomedical scientists typically study in undergraduate majors that are focused on biological sciences, such as biochemistry, microbiology, zoology, biophysics, etc.\n\nEducation programmes have traditionally encompassed an initial bachelor's degree, which is presupposed for two years of further studies eventually earning the students a \"medicine master's examina\". Many students choose to study on (for as much as) another 4 years to earn a PhD degree, at this time the students specialize in a certain areas such as nephrology, neurology, oncology or virology.\n\nIn the UK specifically, prospective undergraduate students wishing to undertake a BSc in biomedical sciences are required to apply via the UCAS application system (usually during the final year of college or sixth form secondary school). A PhD in Biomedicine is however required for most higher research and teaching positions, which most colleges and universities offer. These graduate degree programs may include classroom and fieldwork, research at a laboratory, and a dissertation. Although a degree in a medicine or biology (biochemistry, microbiology, zoology, biophysics) is common, recent research projects also need graduates in statistics, bioinformatics, physics and chemistry. Abilities preferred for entry in this field include: technical, scientific, numerical, written, and oral skills.\n\nUniversity departments offering degree programmes and/or research in biomedical sciences are represented by the Heads of University Centres of Biomedical Sciences (HUCBMS). HUCBMS has an international membership.\n\nBiomedical scientists can focus on several areas of specialty, including:\n\nHowever, recent trends in biomedical graduate education (particularly in the United States) are for biomedical scientists to remain interdisciplinary and to not specialize. This approach emphasizes focus on a particular body or disease process as a whole and drawing upon the techniques of multiple specialties. (\"See also: Systems biology\")\n\nIn the United Kingdom, the salaries for biomedical scientists range from £21,692 to £67,805 plus high cost area supplements and out of hours payments, depending on experience, education, and position. Job growth for the profession has been forecasted as follows:\nAccording to the US Bureau of Labor Statistics (BLS), the 2010-2011 occupational outlook report suggests that biomedical scientist employment is expected \"to increase 40 percent over the 2008-18 decade, much faster than the average for all occupations.\"\n\nAccording to the 2010 BLS report, the median salaries for biomedical scientists in the United States in particular employment areas are:\n\nThese figures include the salaries of post-doctoral fellows, which are paid significantly less than employees in more permanent positions.\n\n\n"}
{"id": "21524318", "url": "https://en.wikipedia.org/wiki?curid=21524318", "title": "Blue skies research", "text": "Blue skies research\n\nBlue skies research (also called blue sky science) is scientific research in domains where \"real-world\" applications are not immediately apparent. It has been defined as \"research without a clear goal\" and \"curiosity-driven science\". It is sometimes used interchangeably with the term \"basic research\". Proponents of this mode of science argue that unanticipated scientific breakthroughs are sometimes more valuable than the outcomes of agenda-driven research, heralding advances in genetics and stem cell biology as examples of unforeseen benefits of research that was originally seen as purely theoretical in scope. Because of the inherently uncertain return on investment, blue-sky projects are politically and commercially unpopular and tend to lose funding to more reliably profitable or practical research.\n\nSupport for blue skies research has varied over time, ultimately becoming subject to the political process, in countries such as the United States, the United Kingdom, and India. Vannevar Bush's 1945 report, \"Science: The Endless Frontier\", made the argument for the value of basic research in the postwar era, and was the basis for many appeals to the federal funding of basic research. The 1957 launch of Sputnik prompted the United States Air Force Office of Scientific Research to sponsor blue skies research into the 1960s. By the 1970s, financial strains brought pressure on public expenditure on the sciences, first in the UK and the Netherlands, and by the 1990s in Germany and the United States.\n\nIn 1980, British Petroleum (known as BP after 2000) established a blue skies research initiative called the Venture Research Unit, headed by particle physicist Donald Braben. Braben controversially challenged peer review as the mechanism for establishing funding, emphasizing the selection of researchers whose proposals \"could radically change the way we think about something important.\"\n\nIn 2005, Mark Walport, director of the Wellcome Trust and former Professor of Medicine at Imperial College, London warned that excessive emphasis on agenda-driven research could jeopardise serendipitous advances in science:\n\nThe Government is right to recognise the importance of science and technology, but I think it is a mistake to try to ring-fence funds. There is a serious danger that we will spend money on projects that are less good. It is absolutely key that funding is used to support the best scientists with the best ideas.\n\nWhen UK research councils introduced a requirement that grant application include a 2-page statement on the economic impact of the proposed work, 20 scientists, including 1996 Nobel laureate Harold Kroto, wrote a public letter to \"Times Higher Education\" condemning the requirement and calling for peer reviewers to ignore the additional documentation.\n\nThe Royal Society grants up to 1 million pounds sterling annually in \"Theo Murphy Blue Skies awards\" to fund \"research which is considered to be original and exciting but lacks a sufficient evidence base in the literature to be supported by traditional grant schemes.\"\n\nThe UK-based Engineering and Physical Sciences Research Council (EPSRC) operates a program called the IDEAS Factory, intended to promote \"blue sky, curiosity-led research.\"\n\nThe international Organisation for Economic Co-operation and Development (OECD) holds a \"Blue Sky Forum\" every decade.\n\nOwing to its radical nature, blue sky science may challenge accepted scientific paradigms and introduce entirely new fields of study. It has been the inspiration for numerous works of science fiction. It has sometimes been concerned with topics such as unexplained phenomena and the impact of future technologies upon society, asking questions such as \"How would a spacecraft traverse a black hole and where would it arrive upon leaving it?\" and \"Will the universe end, or will it just expand ever outwards for eternity?\"\n\n\n\n"}
{"id": "37545351", "url": "https://en.wikipedia.org/wiki?curid=37545351", "title": "Book of Vermilion Fish", "text": "Book of Vermilion Fish\n\nBook of Vermilion Fish () is the first monograph on goldfish in the world, written by Chinese writer Zhang Qiande () (1577－1643) in 1596 during the Ming dynasty.\n\n\n"}
{"id": "12180808", "url": "https://en.wikipedia.org/wiki?curid=12180808", "title": "British Society for Social Responsibility in Science", "text": "British Society for Social Responsibility in Science\n\nThe British Society for Social Responsibility in Science (BSSRS) was a radical science movement most active in the 1970s. It was formed in 1968 in opposition to university research on chemical and biological weapons, and supported by 83 distinguished scientists, including William Bragg, Francis Crick, Julian Huxley and Bertrand Russell. Nobel laureate Maurice Wilkins was the founding President.\nThe main aims of the BSSRS was to raise awareness of the social responsibilities of scientists, the political aspect of science and technology, and to create an informed public.\nAmong groups that were particularly active in BSSRS were members of\n\nBSSRS's inaugural meeting, sponsored by 64 Fellows, was held at the Royal Society, and attended by more\nthan 300, mostly UK, scientists and engineers. Professor Maurice Wilkins\nwas the founding President (1969–91).\n\nOne of the groups first targets was the British Science Association. At a meeting of the BSA in Durham in 1970, they raised political issues under a banner of \"Science is not neutral\". They continued their stance against the BSA, claiming it served a \"propagandist function\".\n\nBSSRS published a newssheet (1969–72), continued by Science for People, (1972/3), and also had local societies and organized public meetings, as well as publishing longer research monographs. \n\nBurhop (1971); Dickson (1971). See also Hilary Rose and Steven Rose (1976); Pirani (1970); Werskey (1971); Fuller (ed.) (1971) and Rose (2003).\n\nThere is an archive group of BSSRS collecting materials, with a website www.bssrs.org. \nBSSRS (archive)\n\n"}
{"id": "4472724", "url": "https://en.wikipedia.org/wiki?curid=4472724", "title": "Broca's Brain", "text": "Broca's Brain\n\nBroca's Brain: Reflections on the Romance of Science is a 1979 book by astrophysicist Carl Sagan. Its chapters were originally articles published between 1974 and 1979 in various magazines, including \"The Atlantic Monthly\", \"The New Republic\", \"Physics Today\", \"Playboy\" and \"Scientific American\". In the introduction, Sagan wrote:\n\nThe title essay is named in honor of the French physician, anatomist and anthropologist, Paul Broca (1824–1880). He is best known for his discovery that different functions are assigned to different parts of the brain. He believed that by studying the brains of cadavers and correlating the known experiences of the former owners of the organs, human behavior could eventually be discovered and understood. To that end, he saved hundreds of human brains in jars of formalin; among the collection is his own neural organ. When Sagan finds it in Musée de l'Homme, he poses questions that challenge some core ideas of human existence such as \"How much of that man known as Paul Broca can still be found in this jar?\"—a question that evokes both religious and scientific argument.\n\nA major part of the book is devoted to debunking \"paradoxers\" who either live at the edge of science or are outright charlatans. An example of this is the controversy surrounding Immanuel Velikovsky's ideas presented in the book \"Worlds in Collision\". Another large part of the book discusses naming conventions for the members of our solar system and their physical features. Sagan also discusses Science fiction at some length. Here, he mentions Robert A. Heinlein as being one of his favorite science fiction authors in his childhood. Near death experiences and their cultural ambiguity is another topic of the essays. Sagan also criticizes ideas developed in Robert K. G. Temple's book \"The Sirius Mystery\", published three years earlier in 1975.\n\nIn the final section of the book, \"Ultimate Questions.\" Sagan writes:\n\n"}
{"id": "3980048", "url": "https://en.wikipedia.org/wiki?curid=3980048", "title": "Computational scientist", "text": "Computational scientist\n\nA computational scientist is a person skilled in scientific computing. This person is usually a scientist, an engineer, or an applied mathematician who applies high-performance computers in different ways to advance the state-of-the-art in their respective applied disciplines; physics, chemistry, social sciences and so forth. Thus scientific computing has increasingly influenced many areas such as economics, biology, law and medicine to name a few.\n\n\nwho is the computer scientist"}
{"id": "555650", "url": "https://en.wikipedia.org/wiki?curid=555650", "title": "Conceptual framework", "text": "Conceptual framework\n\nA conceptual framework is an analytical tool with several variations and contexts. It can be applied in different categories of work where an overall picture is needed. It is used to make conceptual distinctions and organize ideas. Strong conceptual frameworks capture something real and do this in a way that is easy to remember and apply.\n\nIsaiah Berlin used the metaphor of a \"fox\" and a \"hedgehog\" to make conceptual distinctions in how important philosophers and authors view the world. Berlin describes hedgehogs as those who use a single idea or organizing principle to view the world (such as Dante Alighieri, Blaise Pascal, Fyodor Dostoyevsky, Plato, Henrik Ibsen and Georg Wilhelm Friedrich Hegel). Foxes, on the other hand, incorporate a type of pluralism and view the world through multiple, sometimes conflicting, lenses (examples include Johann Wolfgang von Goethe, James Joyce, William Shakespeare, Aristotle, Herodotus, Molière, and Honoré de Balzac).\n\nEconomists use the conceptual framework of \"supply\" and \"demand\" to distinguish between the behavior and incentive systems of firms and consumers. Like many conceptual frameworks, supply and demand can be presented through visual or graphical representations (see demand curve). Both political Science and economics use principal agent theory as a conceptual framework. The politics-administration dichotomy is a long standing conceptual framework used in public administration. All three of these cases are examples of a macro level conceptual framework.\n\nThe use of the term \"conceptual framework\" crosses both scale (large and small theories) and contexts (social science, marketing, applied science, art etc.). Its explicit definition and application can therefore vary.\n\nConceptual frameworks are particularly useful as organizing devices in empirical research. One set of scholars has applied the notion of conceptual framework to deductive, empirical research at the micro- or individual study level. They employ American football plays as a useful metaphor to clarify the meaning of \"conceptual framework\" (used in the context of a deductive empirical study).\n\nLikewise, conceptual frameworks are abstract representations, connected to the research project's goal that direct the collection and analysis of data (on the plane of observation – the ground). Critically, a football play is a \"plan of action\" tied to a particular, timely, purpose, usually summarized as long or short yardage. Shields and Rangarajan (2013) argue that it is this tie to \"purpose\" that make American football plays such a good metaphor. They define a conceptual framework as \"the way ideas are organized to achieve a research project's purpose\". Like football plays, conceptual frameworks are connected to a research purpose or aim. Explanation is the most common type of research purpose employed in empirical research. The formal hypothesis of a scientific investigation is the framework associated with explanation.\n\nExplanatory research usually focuses on \"why\" or \"what caused\" a phenomenon to occur. Formal hypotheses posit possible explanations (answers to the why question) that are tested by collecting data and assessing the evidence (usually quantitative using statistical tests). For example, Kai Huang wanted to determine what factors contributed to residential fires in U.S. cities. Three factors were posited to influence residential fires. These factors (environment, population and building characteristics) became the hypotheses or conceptual framework he used to achieve his purpose – explain factors that influenced home fires in U.S. cities.\n\nSeveral types of conceptual frameworks have been identified, and line up with a research purpose in the following ways:\n\nNote that Shields and Rangarajan (2013) do not claim that the above are the only framework-purpose pairing. Nor do they claim the system is applicable to inductive forms of empirical research. Rather, the conceptual framework-research purpose pairings they propose are useful and provide new scholars a point of departure to develop their own research design.\n\nFrameworks have also been used to explain conflict theory and the balance necessary to reach what amounts to resolution. Within these conflict frameworks, visible and invisible variables function under concepts of relevance. Boundaries form and within these boundaries, tensions regarding laws and chaos (or freedom) are mitigated. These frameworks often function like cells, with sub-frameworks, stasis, evolution and revolution. Anomalies may exist without adequate \"lenses\" or \"filters\" to see them and may become visible only when the tools exist to define them.\n\n\n"}
{"id": "12284804", "url": "https://en.wikipedia.org/wiki?curid=12284804", "title": "Cosmic Jackpot", "text": "Cosmic Jackpot\n\nCosmic Jackpot, also published under the title The Goldilocks Enigma: Why is the Universe Just Right for Life?, is a 2007 non-fiction book by physicist and cosmologist Paul Davies, describing the idea of a fine-tuned Universe.\n\nIn \"Cosmic Jackpot\", Davies argues that certain universal fundamental physical constants are precisely adjusted to make life in the Universe possible: that we have, in a sense, won a \"cosmic jackpot,\" and that conditions are \"just right\" for life, as in The Story of the Three Bears. As Davies writes elsewhere, \"There is now broad agreement among physicists and cosmologists that the universe is in several respects 'fine-tuned' for life.\"\n\nAfter explaining this enigma, Davies discusses possible solutions, such as the anthropic principle, the idea of a multiverse which contains many different universes (including our \"just right\" one), and the idea of intelligent design.\n\nDavies also discusses a number of other ideas connected with the \"multiverse.\" Much like a pencil falling to the ground from its tip in a trade off of symmetry for stability, Davies writes that the Big Bang could have established a complex but stable universe (or multiverse) from symmetry breaking as the heat radiation in \"space\" lowered abruptly past the Curie Point.\n\n\n"}
{"id": "56452747", "url": "https://en.wikipedia.org/wiki?curid=56452747", "title": "Cryogenic electron microscopy", "text": "Cryogenic electron microscopy\n\nCryo-Electron Microscopy (Cryo-EM) is an electron microscopy (EM) technique applied on samples cooled to cryogenic temperatures and embedded in an environment of vitreous water. An aqueous sample solution is applied to a grid-mesh and plunge-frozen in liquid ethane. While development of the technique began in the 1970s, recent advances in detector technology and software algorithms have allowed for the determination of biomolecular structures at near-atomic resolution. This has attracted wide attention to the approach as an alternative to X-ray crystallography or NMR spectroscopy for macromolecular structure determination without the need for crystallization.\n\nIn 2017, the Nobel Prize in Chemistry was awarded to Jacques Dubochet, Joachim Frank, and Richard Henderson \"for developing cryo-electron microscopy for the high-resolution structure determination of biomolecules in solution.\"\n\nTransmission electron cryomicroscopy (CryoTEM) is a transmission electron microscopy technique that is used in structural biology.\n\nIn the 1960s, scientist were faced with the issue of structure determination methods using electron microscopy damaging the specimen due to high energy electron beams, so cryogenic electron microscopy was considered to overcome this issue as it was expected that low temperatures would reduce beam damage. In 1980, Erwin Knapek and Jacques Dubochet published commenting on beam damage at cryogenic temperatures sharing observations that:Thin crystals mounted on carbon film were found to be from 30 to 300 times more beam-resistant at 4 K than at room temperature... Most of our results can be explained by assuming that cryoprotection in the region of 4 K is strongly dependent on the temperature.However, these results were not reproducible and amendments were published in the Nature international journal of science just 2 years later informing that the beam resistance was less significant than initially anticipated. The protection gained at 4 K was closer to “tenfold for standard samples of L-valine,” than what was previously stated.\n\nIn 2017, three scientists, Jacques Dubochet, Joachim Frank and Richard Henderson were awarded the Nobel Prize in Chemistry for developing a technique that would image biomolecules.\n\nIn 2018, Chemists realized that electron diffraction can be used to readily determine the structures of small molecules that form needle-like crystals, structures that would otherwise need to be determined from X-ray crystallography, by growing larger crystals of the compound.\n\nScanning electron cryomicroscopy (CryoSEM), is scanning electron microscopy technique with a scanning electron microscope's cold stage in a cryogenic chamber.\n\n"}
{"id": "7636424", "url": "https://en.wikipedia.org/wiki?curid=7636424", "title": "Current Protocols", "text": "Current Protocols\n\nCurrent Protocols is a series of laboratory manuals for life scientists. The first title, \"Current Protocols in Molecular Biology\", was established in 1987 by the founding editors Frederick M. Ausubel, Roger Brent, Robert Kingston, David D. Moore, Jon Seidman, Kevin Struhl, and John A. Smith of the Massachusetts General Hospital Department of Molecular Biology and the Harvard Medical School Departments of Genetics and Biological Chemistry, along with Sarah Greene of Greene Publishing Associates The Current Protocols series entered into a partnership with by Wiley-Interscience, John Wiley and Sons, and was then acquired by Wiley in 1995, and has continued to introduce additional titles. Scientists contribute methods that are peer-reviewed by one of 18 editorial boards. The core content of each title is updated, and new material is added, on a quarterly basis. In 2009, the Current Protocols website was launched. The site features online versions of all of the texts, research tools, video protocols, and a blog. As of March, 2018, several Current Protocols titles are indexed in MEDLINE and searchable by PubMed: CP Molecular Biology, CP Immunology, CP Cell Biology, CP Protein Science, CP Microbiology. \n\nAs of March, 2018, titles in the series included:\n"}
{"id": "15631864", "url": "https://en.wikipedia.org/wiki?curid=15631864", "title": "Dark current spectroscopy", "text": "Dark current spectroscopy\n\nDark Current Spectroscopy is a technique that is used to determine contaminants in silicon.\n"}
{"id": "47341258", "url": "https://en.wikipedia.org/wiki?curid=47341258", "title": "Data janitor", "text": "Data janitor\n\nA data janitor is a person who works to take big data and condense it into useful amounts of information. Also known as a \"data wrangler,\" a data janitor sifts through data for companies in the information technology industry. A multitude of start-ups rely on large amounts of data, so a data janitor works to help these businesses with this basic, but difficult process of interpreting data.\n\nWhile it is a commonly held belief that data janitor work is fully automated, many data scientists are employed primarily as data janitors. The Information technology industry has been increasingly turning towards new sources of data gathered on consumers, so data janitors have become more commonplace in recent years.\n\nData janitors work in a process that largely consists of four steps: selection and defining relationships, extraction and organization, loading, and interpretation. Data janitors identify sources of data before selecting which data is relevant and find the relationships between the data that will be useful to the company's projects. Next, they structure the data in an effort to extract the information and put it into a format that can be stored in a secure place for the business. Last, the data janitors work with other employees to create visual aids to present to managers and executives who will eventually benefit from the conclusions that can be made from them. In this way, the work of data janitors is integral to the functioning of businesses that rely on large amounts data to function.\n"}
{"id": "10458499", "url": "https://en.wikipedia.org/wiki?curid=10458499", "title": "Democratic rationalization", "text": "Democratic rationalization\n\nDemocratic rationalization is term used by Andrew Feenberg in his article \"Subversive Rationalization: Technology, Power and Democracy with technology.\" Feenberg argues against the idea of \"technological determinism\" citing flaws in its two fundamental theses.\n\nThe first is the \"thesis of unilinear progress\". This is the belief that technological progress follows a direct and predictable path from lower to higher levels of complexity and that each stage along this path is necessary for progress to occur (Feenberg 211).\n\nThe second is the \"thesis of determination by the base\". This is the concept that in a society where a technology had been introduced, that society must organize itself or adapt to the technology (Feenberg 211).\n\nIn his argument against the former thesis Feenberg says that constructivist studies of technology will lead us to realize that there is not a set path by which development of technologies occur but rather an emerging of similar technologies at the same time leading to a multiplicity of choices. These choices are made based upon certain social factors and upon examining them we will see that they are not deterministic in nature (Feenberg 212).\n\nArguing against the latter thesis, Feenberg calls to our attention social reforms that have been mandated by governments mainly in regards to the protection of its citizens and laborers. Most of the time these mandates are widely accepted after being passed through the governing body. At which point technology and industry will reform and re-evolve to meet the new standards in a way that has greater efficiency than it did so previously (Feenberg 214)\n\n\n"}
{"id": "54194309", "url": "https://en.wikipedia.org/wiki?curid=54194309", "title": "Density ratio", "text": "Density ratio\n\nThe density ratio of a column of seawater is a measure of the relative contributions of temperature and salinity in determining the density gradient. At a density ratio of 1, temperature and salinity are said to be \"compensated\": their density signatures cancel, leaving a density gradient of zero. The formula for the density ratio, R, is:\n\nR = αθ/βS, where\n\n\nWhen a water column is \"doubly stable\"--both temperature and salinity contribute to the stable density gradient--the density ratio is negative (a doubly unstable water column would also have a negative density ratio, but does not commonly occur). A statically stable water column with a density ratio between 0 and 1 (cool fresh overlying warm salty) can support diffusive convection, and a statically stable water column with a density ratio larger than 1 can support salt fingering.\n\nDensity ratio may also be used to describe thermohaline variability over a non-vertical spatial interval, such as across a front in the mixed layer.\n\nIf the signs of both the numerator and denominator are reversed, the density ratio remains unchanged. A related quantity which avoids this ambiguity as well as the infinite values possible when the denominator vanishes is the Turner angle, Tu.\n\n"}
{"id": "2957537", "url": "https://en.wikipedia.org/wiki?curid=2957537", "title": "Dream telepathy", "text": "Dream telepathy\n\nDream telepathy is the purported ability to communicate telepathically with another person while one is dreaming. The first person in modern times to document telepathic dreaming was Sigmund Freud. In the 1940s it was the subject of the Eisenbud-Pederson-Krag-Fodor-Ellis controversy, named after the preeminent psychoanalysts of the time who were involved Jule Eisenbud, Geraldine Pederson-Krag, Nandor Fodor, and Albert Ellis. There is no scientific evidence that dream telepathy is a real phenomenon. Parapsychological experiments into dream telepathy have not produced replicable results.\n\nThe notion and speculation of communication via dreaming was first mooted in psychoanalysis by Sigmund Freud in 1921. He produced a model to express his ideas about telepathic dreaming. His 1922 paper \"Dreams and Telepathy\" is reproduced in the book \"Psychoanalysis and the Occult\" (1953) and was intended to be a lecture to the Vienna Psycho-Analytical Society, although he never delivered it. Freud considered that a connection between telepathy and dreams could be neither proven nor disproven. He was distinctly suspicious of the whole idea, noting that he himself had never had a telepathic dream. (His two dreams that were potentially telepathic, where he dreamed of the deaths of a son and of a sister-in-law, which did not occur, he labeled as \"purely subjective anticipations\".) His ideas were not widely accepted at the time, but he continued to publicly express his interest and findings about telepathic dreaming. He also observed that he had not encountered any evidence of dream telepathy in his patients. Freud claims neutrality about the phenomenon itself, states that the sleep milieu has special likely properties for it if it does exist, and discounts all of the cases presented to him on standard psychoanalytic grounds (e.g. neurosis, transference, etc.).\n\nIn the 1940s Jule Eisenbud, Geraldine Pederson-Krag and Nandor Fodor described alleged cases of dream telepathy. Albert Ellis regarded their conclusions to have been based upon flimsy evidence, and that they could be better explained by bias, coincidence and unconscious cues than by dream telepathy. He also accused them of an emotional involvement in the notion, resulting in their observations and judgement being clouded. Psychologist L. Börje Löfgren also criticised dream telepathy experiments of Eisenbud. He stated that coincidence was a more likely explanation and the \"assumption of paranormal forces to explain them is unnecessary.\"\n\nThere have been many experiments done to test the validity of dream telepathy and its effectiveness, but with significant issues of blinding. Many test subjects find ways to communicate with others to make it look like telepathic communication. Attempts to cut off communication between the agent, sender, and receiver of information failed because subjects found ways to get around blindfolds no matter how intricate and covering they were. In studies at the Maimonides Medical Center in Brooklyn, New York led by Stanley Krippner and Montague Ullman, patients were monitored and awakened after a period of REM then separated to study the claimed ability to communicate telepathically. They concluded the results from some of their experiments supported dream telepathy.\n\nThe picture target experiments that were conducted by Krippner and Ullman were criticized by C. E. M. Hansel. According to Hansel there were weaknesses in the design of the experiments in the way in which the agent became aware of their target picture. Only the agent should have known the target and no other person until the judging of targets had been completed; however, an experimenter was with the agent when the target envelope was opened. Hansel also wrote there had been poor controls in the experiment as the main experimenter could communicate with the subject.\n\nAn attempt to replicate the experiments that used picture targets was carried out by Edward Belvedere and David Foulkes. The finding was that neither the subject nor the judges matched the targets with dreams above chance level. Results from other experiments by Belvedere and Foulkes were also negative.\n\nIn 2003, Simon Sherwood and Chris Roe wrote a review that claimed support for dream telepathy at Maimonides. However, James Alcock noted that their review was based on \"extreme messiness\" of data. Alcock concluded the dream telepathy experiments at Maimonides have failed to provide evidence for telepathy and \"lack of replication is rampant.\"\n\nThe psychologist and noted skeptic Richard Wiseman took part in a dream telepathy experiment. It was conducted by Caroline Watt at a sleep laboratory in an attempt to replicate the results of Krippner and Ullman. The experiment was a complete failure. According to Wiseman \"after monitoring about twenty volunteers for several nights on end, the study didn't discover any evidence in support of the supernatural.\"\n\n\n"}
{"id": "2914954", "url": "https://en.wikipedia.org/wiki?curid=2914954", "title": "Effective theory", "text": "Effective theory\n\nIn science, an effective theory is a scientific theory which proposes to describe a certain set of observations, but explicitly without the claim or implication that the mechanism employed in the theory has a direct counterpart in the actual causes of the observed phenomena to which the theory is fitted.\nI.e. the theory proposes to model a certain \"effect\", without proposing to adequately model any of the \"causes\" which contribute to the effect.\n\nThus, an effective field theory is a theory which describes phenomena in solid-state physics, notably the BCS theory of superconduction, which treats vibrations of the solid-state lattice as a \"field\" (i.e. without claiming that there is \"really\" a field), with its own field quanta, called phonons. Such \"effective particles\" derived from effective fields are also known as quasiparticles.\n\nIn a certain sense, quantum field theory, and any other currently known physical theory, could be described as \"effective\", as in being the \"low energy limit\" of an as-yet unknown \"Theory of Everything\".\n\n"}
{"id": "45700602", "url": "https://en.wikipedia.org/wiki?curid=45700602", "title": "Energy and environmental engineering", "text": "Energy and environmental engineering\n\nEnergy and environmental engineering is a branch of engineering which seeks to efficiently use energy and to maintain the environment. Energy engineers require knowledge across many disciplines. Careers include work in the built environment, renewable and traditional energy industries.\n\nIn this area, solar radiation is important and must be understood. Solar radiation affects the Earth's weather and daylight available. This affects not only the Earth's environment but also the smaller internal environments which we create.\n\nEnergy engineering requires at least an understanding of mechanics, thermodynamics, mathematics, materials, stoichiometry, electrical machines, manufacturing processes and energy systems.\n\nEnvironmental engineering can be branched into two main areas: internal environments and outdoor environments.\n\nInternal environments may consist of housing or offices or other commercial properties. In this area, the environmental engineering sometimes stands for the designing of building services to condition the internal environment to a comfortable state or the removal of excess pollutants such as carbon dioxide or other harmful substances.\n\nExternal environments may be water courses, air, land or seas, and may require new strategies for harnessing energy or the creation of treatment facilities for polluting technologies.\n\nThis broad degree area covers many areas but is mainly mechanically and electrically biased. It seeks to explore cleaner, more efficient ways of using fossil fuels, while investigating and developing systems using renewable and sustainable resources, such as solar, wind and wave energy.\n"}
{"id": "50744122", "url": "https://en.wikipedia.org/wiki?curid=50744122", "title": "Fashion, Faith, and Fantasy in the New Physics of the Universe", "text": "Fashion, Faith, and Fantasy in the New Physics of the Universe\n\nFashion, Faith, and Fantasy in the New Physics of the Universe is a book by mathematical physicist Roger Penrose, released in September 2016. The book is based on his lectures that he gave at Princeton University in 2003.\n\n"}
{"id": "1348347", "url": "https://en.wikipedia.org/wiki?curid=1348347", "title": "Funding of science", "text": "Funding of science\n\nResearch funding is a term generally covering any funding for scientific research, in the areas of both \"hard\" science and technology and social science. The term often connotes funding obtained through a competitive process, in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations or foundations, allocate scarce funds.\n\nMost research funding comes from two major sources, corporations (through research and development departments) and government (primarily carried out through universities and specialized government agencies; often known as \"research councils\"). Some small amounts of scientific research are carried out (or funded) by charitable foundations, especially in relation to developing cures for diseases such as cancer, malaria and AIDS.\n\nAccording to OECD, more than 60% of research and development in scientific and technical fields is carried out by industries, and 20% and 10% respectively by universities and government.\n\nComparatively, in countries with less GDP, such as Portugal and Mexico the industry contribution is significantly lower. The US government spends more than other countries on military R&D, although the proportion has fallen from around 30% in the 1980s to under 20. Government funding for medical research amounts to approximately 36% in the U.S. The government funding proportion in certain industries is higher, and it dominates research in social science and humanities. Similarly, with some exceptions (e.g. biotechnology) government provides the bulk of the funds for \"basic\" scientific research. In commercial research and development, all but the most research-oriented corporations focus more heavily on near-term commercialization possibilities rather than \"blue-sky\" ideas or technologies (such as nuclear fusion).\n\nIn the eighteenth and nineteenth centuries, as the pace of technological progress increased before and during the industrial revolution, most scientific and technological research was carried out by individual inventors using their own funds. A system of patents was developed to allow inventors a period of time (often twenty years) to commercialise their inventions and recoup a profit, although in practice many found this difficult. The talents of an inventor are not those of a businessman, and there are many examples of inventors (e.g. Charles Goodyear) making rather little money from their work whilst others were able to market it.\n\nIn the twentieth century, scientific and technological research became increasingly systematised, as corporations developed, and discovered that continuous investment in research and development could be a key element of success in a competitive strategy. It remained the case, however, that imitation by competitors - circumventing or simply flouting patents, especially those registered abroad - was often just as successful a strategy for companies focused on innovation in matters of organisation and production technique, or even in marketing. A classic example is that of Wilkinson Sword and Gillette in the disposable razor market, where the former has typically had the technological edge, and the latter the commercial one.\n\nDifferent countries spend vastly different amounts on research, in both absolute and relative terms. For instance, South Korea and Israel spend more than 4% of their GDP on research while many Arabic countries spend less than 1% (e.g. Saudi Arabia 0.25%).\n\nThe US spent $456.1 billion for research and development (R&D) in 2013, the most recent year for which such figures are available, according to the National Science Foundation. The private sector accounted for $322.5 billion, or 71%, of total national expenditures, with universities and colleges spending $64.7 billion, or 14%, in second place.\n\nSwitzerland spent CHF 22 billion for R&D in 2015 with an increase of 10.5% compared with 2012 when the last survey was conducted. In relative terms, this represents 3.4% of the country's GDP. R&D activities are carried out by nearly 125,000 individuals, mostly in the private sector (71%) and higher education institutions (27%).\n\nOften scientists apply for research funding which a granting agency may (or may not) approve to financially support. These \"grants\" require a lengthy process as the granting agency can inquire about the researcher(s)'s background, the facilities used, the equipment needed, the time involved, and the overall potential of the scientific outcome. The process of grant writing and grant proposing is a somewhat delicate process for both the grantor and the grantee: the grantors want to choose the research that best fits their scientific principles, and the individual grantees want to apply for research in which they have the best chances but also in which they can build a body of work towards future scientific endeavors.\n\nThe Engineering and Physical Sciences Research Council in the United Kingdom has devised an alternative method of fund-distribution: the sandpit.\n\nMost universities have research administration offices to facilitate the interaction between the researcher and the granting agency.\n\"Research administration is all about service—service to our faculty, to our academic units, to the institution, and to our sponsors. To be of service, we first have to know what our customers want and then determine whether or not we are meeting those needs and expectations.\"\n\nIn the United States of America, the National Council of University Research Administrators (NCURA) serves its members and advances the field of research administration through education and professional development programs, the sharing of knowledge and experience, and by fostering a professional, collegial, and respected community.\n\nGovernment-funded research can either be carried out by the government itself, or through grants to researchers outside the government. The bodies providing public funding are often referred to as \"research councils\".\n\nCritics of basic research are concerned that research funding for the sake of knowledge itself does not contribute to a great return. However, scientific innovations often foreshadow or inspire further ideas unintentionally. For example, NASA's quest to put a man on the moon inspired them to develop better sound recording and reading technologies. NASA's research was furthered by the music industry, who used it to develop audio cassettes. Audio cassettes, being smaller and able to store more music, quickly dominated the music industry and increased the availability of music.\n\nAn additional distinction of government-sponsored research is that the government does not make a claim to the intellectual property, whereas private research-funding bodies sometimes claim ownership of the intellectual property that they are paying to have developed. Consequently, government-sponsored research more often allows the individual discoverer to file intellectual property claims over their own work.\n\nResearch councils are (usually public) bodies that provide research funding in the form of research grants or scholarships. These include arts councils and research councils for the funding of science.\n\nAn incomplete list of national and international pan-disciplinary public research councils:\nPrivate funding for research comes from philanthropists, crowd-funding, private companies, non-profit foundations, and professional organizations. Philanthropists and foundations have been known to pour millions of dollars into a wide variety of scientific investigations, including basic research discovery, disease cures, particle physics, astronomy, marine science, and the environment. Many large technology companies spend billions of dollars on research and development each year to gain an innovative advantage over their competitors, though only about 42% of this funding goes towards projects that are considered substantially new, or capable of yielding radical breakthroughs. New scientific start-up companies initially seek funding from crowd-funding organizations, venture capitalists, and angel investors, gathering preliminary results using rented facilities, but aim to eventually become self-sufficient.\n\nExamples of companies that fund basic research include IBM (high temperature superconductivity was discovered by IBM sponsored basic experimental research in 1986), L'Oreal (which created the L'Oreal-Unesco prize for women scientists and finances internships), AXA (which launched a Research Fund in 2008 and finances Academic Institutions such as advanced fundamental mathematics French Foundation IHES).\n\nA company may share resources with a materials science society to gain proprietary knowledge or trained workers.\n\nIn academic contexts, hard money may refer to funding received from a government or other entity at regular intervals, thus providing a steady inflow of financial resources to the beneficiary. The antonym, soft money, refers to funding provided only through competitive research grants and the writing of grant proposals.\n\nHard money is usually issued by the government for the advancement of certain projects or for the benefit of specific agencies. Community healthcare, for instance, may be supported by the government by providing hard money. Since funds are disbursed regularly and continuously, the offices in charge of such projects are able to achieve their objectives more effectively than if they had been issued one-time grants.\n\nIndividual jobs at a research institute may be classified as \"hard-money positions\" or \"soft-money positions\"; the former are expected to provide job security because their funding is secure in the long term, whereas individual \"soft-money\" positions may come and go with fluctuations in the number of grants awarded to the institution.\n\nThe source of funding may introduce conscious or unconscious biases into a researcher's work. Disclosure of potential conflicts of interest (COIs) is used by biomedical journals to guarantee credibility and transparency of the scientific process. Conflict of interest disclosure, however, is not systematically nor consistently dealt with by journals which publish scientific research results. When research is funded by the same agency that can be expected to gain from a favorable outcome there is a potential for biased results and research shows that results are indeed more favorable than would be expected from a more objective view of the evidence. A 2003 systematic review studied the scope and impact of industry sponsorship in biomedical research. The researchers found financial relationships among industry, scientific investigators, and academic institutions widespread. Results showed a statistically significant association between industry sponsorship and pro-industry conclusions and concluded that \"Conflicts of interest arising from these ties can influence biomedical research in important ways\". A British study found that a majority of the members on national and food policy committees receive funding from food companies.\n\nIn an effort to cut costs, the pharmaceutical industry has turned to the use of private, nonacademic research groups (i.e., contract research organizations [CROs]) which can do the work for less money than academic investigators. In 2001 CROs came under criticism when the editors of 12 major scientific journals issued a joint editorial, published in each journal, on the control over clinical trials exerted by sponsors, particularly targeting the use of contracts which allow sponsors to review the studies prior to publication and withhold publication of any studies in which their product did poorly. They further criticized the trial methodology stating that researchers are frequently restricted from contributing to the trial design, accessing the raw data, and interpreting the results.\n\nThe Cochrane Collaboration, a worldwide group that aims to provide compiled scientific evidence to aid well informed health care decisions, conducts systematic reviews of randomized controlled trials of health care interventions and tries to disseminate the results and conclusions derived from them. A few more recent reviews have also studied the results of non-randomized, observational studies. The systematic reviews are published in the Cochrane Library. A 2011 study done to disclose possible conflicts of interests [COI] in underlying research studies used for medical meta-analyses reviewed 29 meta-analyses and found that COIs in the studies underlying the meta-analyses were rarely disclosed. The 29 meta-analyses reviewed an aggregate of 509 randomized controlled trials (RCTs). Of these, 318 RCTs reported funding sources with 219 (69%) industry funded. 132 of the 509 RCTs reported author COI disclosures, with 91 studies (69%) disclosing industry financial ties with one or more authors. The information was, however, seldom reflected in the meta-analyses. Only two (7%) reported RCT funding sources and none reported RCT author-industry ties. The authors concluded \"without acknowledgement of COI due to industry funding or author industry financial ties from RCTs included in meta-analyses, readers' understanding and appraisal of the evidence from the meta-analysis may be compromised.\"\n\nIn 2003 researchers looked at the association between authors' published positions on the safety and efficacy in assisting with weight loss of olestra, a fat substitute manufactured by the Procter & Gamble (P&G), and their financial relationships with the food and beverage industry. They found that supportive authors were significantly more likely than critical or neutral authors to have financial relationships with P&G and all authors disclosing an affiliation with P&G were supportive. The authors of the study concluded: \"Because authors' published opinions were associated with their financial relationships, obtaining noncommercial funding may be more essential to maintaining objectivity than disclosing personal financial interests.\"\n\nA 2005 study in the journal Nature surveyed 3247 US researchers who were all publicly funded (by the National Institutes of Health). Out of the scientists questioned, 15.5% admitted to altering design, methodology or results of their studies due to pressure of an external funding source.\n\nA theoretical model has been established whose simulations imply that peer review and over-competitive research funding foster mainstream opinion to monopoly.\n\nMost funding agencies mandate efficient use of their funds, that is, they want to maximize outcome for their money spent. Outcome can be measured by publication output, citation impact, number of patents, number of PhDs awarded etc. Another question is how to allocate funds to different disciplines, institutions, or researchers. A recent study by Wayne Walsh found that “prestigious institutions had on average 65% higher grant application success rates and 50% larger award sizes, whereas less-prestigious institutions produced 65% more publications and had a 35% higher citation impact per dollar of funding.”\n\n\n\n"}
{"id": "41091548", "url": "https://en.wikipedia.org/wiki?curid=41091548", "title": "Hybrid plasmid", "text": "Hybrid plasmid\n\nHybrid plasmid is a plasmid that contains an inserted piece of foreign DNA.\n"}
{"id": "185493", "url": "https://en.wikipedia.org/wiki?curid=185493", "title": "Informal mathematics", "text": "Informal mathematics\n\nInformal mathematics, also called naïve mathematics, has historically been the predominant form of mathematics at most times and in most cultures, and is the subject of modern ethno-cultural studies of mathematics. The philosopher Imre Lakatos in his \"Proofs and Refutations\" aimed to sharpen the formulation of informal mathematics, by reconstructing its role in nineteenth century mathematical debates and concept formation, opposing the predominant assumptions of mathematical formalism. Informality may not discern between statements given by \"inductive reasoning\" (as in approximations which are deemed \"correct\" merely because they are useful), and statements derived by \"deductive reasoning\".\n\n\"Informal mathematics\" means any informal mathematical practices, as used in everyday life, or by aboriginal or ancient peoples, without historical or geographical limitation. Modern mathematics, exceptionally from that point of view, emphasizes formal and strict proofs of all statements from given axioms. This can usefully be called therefore \"formal mathematics\". Informal practices are usually understood intuitively and justified with examples—there are no axioms. This is of direct interest in anthropology and psychology: it casts light on the perceptions and agreements of other cultures. It is also of interest in developmental psychology as it reflects a naïve understanding of the relationships between numbers and things. Another term used for informal mathematics is folk mathematics, which is ambiguous; the mathematical folklore article is dedicated to the usage of that term among professional mathematicians.\n\nThe field of naïve physics is concerned with similar understandings of physics. People use mathematics and physics in everyday life, without really understanding (or caring) how mathematical and physical ideas were historically derived and justified.\n\nThere has long been a standard account of the development of geometry in ancient Egypt, followed by Greek mathematics and the emergence of deductive logic. The modern sense of the term \"mathematics\", as meaning only those systems justified with reference to axioms, is however an anachronism if read back into history. Several ancient societies built impressive mathematical systems and carried out complex calculations based on proofless heuristics and practical approaches. Mathematical facts were accepted on a pragmatic basis. Empirical methods, as in science, provided the justification for a given technique. Commerce, engineering, calendar creation and the prediction of eclipses and stellar progression were practiced by ancient cultures on at least three continents. N.C. Ghosh included Informal Mathematics in the list of Folk Mathematics.\n\n"}
{"id": "42566408", "url": "https://en.wikipedia.org/wiki?curid=42566408", "title": "Infrared Science Archive", "text": "Infrared Science Archive\n\nThe Infrared Science Archive (IRSA) curates the science products of NASA's infrared and submillimeter projects, such as the Spitzer Space Telescope, the Wide-field Infrared Survey Explorer (WISE), the Infrared Astronomical Satellite (IRAS), and the Two Micron All-Sky Survey (2MASS). IRSA also serves data from infrared and submillimeter European Space Agency missions with NASA participation, including the Infrared Space Observatory (ISO), Planck, and the Herschel Space Observatory. , IRSA provides access to more than 20 billion astronomical measurements, including all-sky coverage in 20 bands, spanning wavelengths from 1 micron to 10 millimeters. Approximately 10% of all refereed astronomical journal articles cite data sets curated by IRSA.\n\nIRSA is part of the Infrared Processing and Analysis Center (IPAC) and is located on the campus of the California Institute of Technology. It is one of NASA's Astrophysics Data Centers, along with the High Energy Astrophysics Science Archive Research Center (HEASARC) and the Mikulski Archive for Space Telescopes (MAST).\n\n"}
{"id": "56079882", "url": "https://en.wikipedia.org/wiki?curid=56079882", "title": "Interrogating Ethnography", "text": "Interrogating Ethnography\n\n\"Interrogating Ethnography: Why Evidence Matters\" is a 2017 book by Steven Lubet of Northwestern University Law School, critiquing methods used it the discipline of ethnography. Writing in \"Contexts\", Syid Ali of Long Island University called it \"an essential critique of the most public-facing product sociology has to offer.\"\n\n\"Interrogating Ethnography\", which criticizes ethnographers for the practice of changing the name of the area they study, is part of the Replication crisis. But it is largely a critique of ethnography methodology when it relies on the narratives of interviewees with no attempt to verify assertions of fact.\n\nLubet began the project of writing this book after reading and publishing a notable critique of the use of evidence Alice Goffman's controversial 2014 book \"On the Run: Fugitive Life in an American City,\" That critique led him to read and attempt to verify \"more than 50 ethnographic monographs and an equivalent number of articles. Focusing on sociologists’ studies of American cities... (and checking) facts that could be documented — or not... by consulting experts and pulling public records.\"\n\nAccording to the Chronicle of Higher Education, the book \"has touched off a debate about what ethnographers might learn from legal scholars, and vice versa.\n\nThe books reception by ethnographers has been mixed.\n"}
{"id": "3730328", "url": "https://en.wikipedia.org/wiki?curid=3730328", "title": "Laundry ball", "text": "Laundry ball\n\nA laundry ball or washing ball is a product that is promoted as a substitute for laundry detergent. Producers of laundry balls often make pseudoscientific claims about how these balls work and exaggerate the extent of their benefits.\n\nIn the US the product was often sold on home shopping channels or by participants in multilevel marketing, although this is not the case in all countries.\n\nWhile many individuals report that these balls work, most test results show them to be similar to or less effective than washing in water without any detergent. Most of the effect can be attributed to the mechanical effect of the ball or to using hot water instead of cold water.\n\nThe US Federal Trade Commission has taken action against some of the manufacturers of these products because of their misleading claims. Consumer organizations from several countries have recommended against buying this type of product.\n\nThere are several shapes of laundry balls: laundry disks, globes, spheres or doughnuts. Some of the balls carry components inside, like ceramic pieces, magnetic material or coloured liquid that is claimed to be \"activated water\". Manufacturers claim that these components have certain effects on washing efficacy, although studies don't show any difference between the different types of balls. Some balls can be refilled with pellets of special detergent, or other ingredients.\n\nLaundry balls are marketed as cheaper, environmentally friendly alternatives to ordinary washing powders or liquids. The manufacturers claim the following benefits, which are technically real:\n\n\nBut they also claim many benefits that laundry balls do not have, according to many studies:\n\n\nHowever, the real effects are comparable to those of washing without any detergent and are sometimes worse.\n\nThe laundry ball could break open during washing, and the ceramic pieces inside it could damage the machinery of the washing machine.\n\nManufacturers rarely agree on why their laundry balls work, hinting that these claims are just made up by each individual manufacturer. Some claims are not backed by science, while others are an exaggeration of benefits. Balls that contain detergents may offer more cleaning power than water alone because their ingredients are comparable to normal washing powder, but in smaller quantities. It is claimed that conventional washing powder manufacturers recommend using more powder than is necessary, and that these powders contain unnecessary fillers or fragrances.\n\nThe effect of the laundry balls may be explained by simple mechanical action and by the usage of hotter water. Some manufacturers claim that their products reduce energy consumption, but their pamphlets recommend using hot water. Hot water will clean some types of spots better than cold water, leading some people to conclude that the balls worked. The mechanical action of the laundry balls can help clean some types of spots, but a golf ball will achieve the same effect for much less money.\n\nSome manufacturers claim that the components inside their balls emit far infrared rays, which are claimed to reduce the surface tension of water and facilitate washing. The claim of emitting infrared is not false, as almost all materials emit \"far infrared waves\" at room temperature, in other words, heat radiation. It is also true that heating reduces the surface tension of water, but the effect of the radiation emitted by the balls is negligible compared to the radiation emitted by the internal walls of the washing machine or by the water, especially if the water is hot.\n\nMagnetic water softeners claim that their magnetic fields can help remove scale from the washing machine and pipes, and prevent new limescale from adhering. Some companies claim to remove hardness ions from hard water, or to precipitate the molecules in the water so they won't \"stick\" to the pipes, or to reduce the surface tension of water. The claims are dubious, the scientific basis is unclear, the working mechanism is vaguely defined and understudied, and high-quality studies report negative results. The reputation of these products is further damaged by the pseudoscientific explanations that promoters keep putting forward.\n\nSome magnetic products claim that they \"change the molecular structure of water\", a pseudoscientific claim with no real scientific basis. There is no such thing as \"magnetized water\". Water is not paramagnetic, so its water molecules do not align in the presence of a magnetic field. Water is weakly diamagnetic (so it is repelled by magnets), but only to an extent so small that it is undetectable to most instruments.\n\nSome balls are refillable with small pellets of detergent which are sold only by the manufacturer of the ball. Critics question whether the amount and type of detergent released by these balls is sufficient to generate significant cleaning effects.\n\nIn 1997, Amway offered a ceramic washing disk on its catalogue, but removed it after concluding that it had \"no measurable impact on overall cleaning.\"\n\nIn 1997 Trade-Net, sold a laundry ball product (the Blue Laundry Ball) in various US states. Trade-Net claimed that the blue liquid inside their balls was structured water \"that emits a negative charge through the walls of the container into your laundry water.\" \"This causes the water molecule cluster to disassociate, allowing much smaller individual water molecules to penetrate into the innermost part of the fabric.\" Dennis Barnum, a professor of inorganic chemistry at Portland State University, said that the liquid was just water with a blue dye and couldn't possibly have the effect claimed by the manufacturer. Barnum also said that the claims were \"gibberish\" and used scientific terms in ways that sounded educated to the layman but didn't make any real sense. The Oregonian tested the balls, and found they washed marginally better than hot water with no detergent, and worse than using detergent.\n\nAfter complaints, Trade-Net's claims were investigated by consumer protection departments in Utah, Oregon and Florida, amongst others, and the company was prohibited from making certain claims, including that \"such product cleans as well as conventional laundry detergent\". Trade-Net offered a 'new' laundry ball product after this, but were forced to pay fines, including $190,000 to Oregon's Department of Justice, $10,000 to Utah and then in April 1999, $155,000 to the states of New York, Arizona, Arkansas, Hawaii, Idaho, Illinois, Michigan, Missouri, Nebraska, Nevada, Oklahoma and the FTC. The company disappeared shortly thereafter. The Federal Trade Commission has levied fines against other companies for similar fraudulent claims. However, other companies kept selling similar products over the Internet.\n\nThe judge ruling against Trade-Net, issued in April 1999, said the manufacturers failed to substantiate their claims and hadn't informed consumers about reports showing that the claims were incorrect.\n\nThe Australian Consumers' Association published a report in the April 1998 issue of its magazine \"Choice\". It concluded that laundry balls were no better than cold water.\n\nThe US Federal Trade Commission published in 1999 about laundry balls, rings and discs: \"Tests show that these gadgets do little more than clean out your wallet. At best, they’re marginally better than washing clothes in hot water alone, and not as effective as washing them with laundry detergent. At worst, the products are completely useless.\"\n\nIn 2000 the magazine \"Good Housekeeping\" tested several laundry balls sold in the US and concluded that \"these gizmos do little more than clean out your wallet.\"\n\nIn April 2009 the Italian consumer association Altroconsumo carried a small test and concluded that laundry balls didn't wash better than plain water.\n\nIn 2009 the Spanish consumer organization OCU made a study of \"ecobolas\" (a type of laundry ball marketed in Spain). It compared the efficacy of the laundry ball, normal detergent, and no detergent at all. It concluded that laundry balls were no better than using just water, and it recommended that consumers simply use a minimum amount of detergent.\n\nIn November 2011, the Hong Kong Consumer Council published a report on the effect of using washing liquid, washing powder and washing balls. The former two were shown to be effective in removing stains, while the washing balls were not more effective than plain water.\n\nSome organizations recommending against their use are Consumers Union, International Fabricare Institute (now called Drycleaning and Laundry Institute), Maytag, Soap and Detergent Association and Spanish OCU.\n\nIn February 2011 the Spanish National Institute of Consume (Instituto Nacional del Consumo INC) ordered 14 manufacturers to cease their deceiving advertisement after testing the wash balls and concluding that they are as effective, or even \"less\" effective, than washing with water alone.\n\nIn August 2012 the Portuguese Consumer Rights Council requested a ban on the washing balls because none of the advantages advertised were proven in tests.\n\nThe Australian consumer advocacy group (Choice Australia) gave a 'Shonky Award' to Nanosmart Laundry Balls in October 2015, stating that they \"don't work\" and that they should be renamed \"Nano-not-so-smart\" after testing the balls against plain water and finding they had no effect and that their scientific claims were simply untrue. Choice Australia states that they will refer the product to the Australian Competition and Consumer Commission for investigation over Nanosmart's misleading claims.\n\nBy making very vague claims, marketers can continue to sell laundry balls without running afoul of consumer protection laws that require veracity in advertisement.\n\nThe product is often sold by participants in multilevel marketing. They can also be found in ecological retail stores.\n\nDuring the initial marketing boom, balls were manufactured for other market niches, like washing cars.\n\n\n"}
{"id": "42335532", "url": "https://en.wikipedia.org/wiki?curid=42335532", "title": "List of regimes", "text": "List of regimes\n\nThis list of regimes lists the results of regime-classification schemes from political science literature, including Polity data series and the Democracy-Dictatorship Index.\n\n"}
{"id": "21396853", "url": "https://en.wikipedia.org/wiki?curid=21396853", "title": "List of science and engineering blunders", "text": "List of science and engineering blunders\n\nThis is a list of engineering blunders, i.e., gross errors or mistakes resulting from grave lack of proper consideration, such as stupidity, confusion, carelessness, or culpable ignorance, which resulted in notable incidents.\n\n\n"}
{"id": "1876520", "url": "https://en.wikipedia.org/wiki?curid=1876520", "title": "Mass formula", "text": "Mass formula\n\nA mass formula is an equation or set of equations in physics which attempts to predict the mass or mass ratios of the subatomic particles. \nAn important step in high energy physics was the discovery of the Gell-Mann–Okubo mass formula predicting relationships between masses of the members of SU(3) multiplets.\n\nThe development of an accurate mass formula is one of several fundamental aspects to developing a working theory of everything, which is expected to overcome the incompatibilities between current classical and quantum physics theories.\n\nThere are currently no universal mass formulae which are generally accepted as correct by the mainstream physics community, however several versions of potential mass formulae have been presented and are currently being explored by some (largely amateur) physics theorists.\n\n"}
{"id": "11817317", "url": "https://en.wikipedia.org/wiki?curid=11817317", "title": "Mechanical explanations of gravitation", "text": "Mechanical explanations of gravitation\n\nMechanical explanations of gravitation (or kinetic theories of gravitation) are attempts to explain the action of gravity by aid of basic mechanical processes, such as pressure forces caused by pushes, without the use of any action at a distance. These theories were developed from the 16th until the 19th century in connection with the aether. However, such models are no longer regarded as viable theories within the mainstream scientific community and general relativity is now the standard model to describe gravitation without the use of actions at a distance. Modern \"quantum gravity\" hypotheses also attempt to describe gravity by more fundamental processes such as particle fields, but they are not based on classical mechanics.\n\nThis theory is probably the best-known mechanical explanation, and was developed for the first time by Nicolas Fatio de Duillier in 1690, and re-invented, among others, by Georges-Louis Le Sage (1748), Lord Kelvin (1872), and Hendrik Lorentz (1900), and criticized by James Clerk Maxwell (1875), and Henri Poincaré (1908).\n\nThe theory posits that the force of gravity is the result of tiny particles or waves moving at high speed in all directions, throughout the universe. The intensity of the flux of particles is assumed to be the same in all directions, so an isolated object A is struck equally from all sides, resulting in only an inward-directed pressure but no net directional force. With a second object B present, however, a fraction of the particles that would otherwise have struck A from the direction of B is intercepted, so B works as a shield, so-to-speak—that is, from the direction of B, A will be struck by fewer particles than from the opposite direction. Likewise, B will be struck by fewer particles from the direction of A than from the opposite direction. One can say that A and B are \"shadowing\" each other, and the two bodies are pushed toward each other by the resulting imbalance of forces.\nThis shadow obeys the inverse square law, because the imbalance of momentum flow over an entire spherical surface enclosing the object is independent of the size of the enclosing sphere, whereas the surface area of the sphere increases in proportion to the square of the radius. To satisfy the need for mass proportionality, the theory posits that a) the basic elements of matter are very small so that gross matter consists mostly of empty space, and b) that the particles are so small, that only a small fraction of them would be intercepted by gross matter. The result is, that the \"shadow\" of each body is proportional to the surface of every single element of matter.\n\n\"Criticism\": This theory was declined primarily for thermodynamic reasons because a shadow only appears in this model if the particles or waves are at least partly absorbed, which should lead to an enormous heating of the bodies. Also drag, \"i.e.\" the resistance of the particle streams in the direction of motion, is a great problem too. This problem can be solved by assuming superluminal speeds, but this solution largely increases the thermal problems and contradicts special relativity.\n\nBecause of his philosophical beliefs, René Descartes proposed in 1644 that no empty space can exist and that space must consequently be filled with matter. The parts of this matter tend to move in straight paths, but because they lie close together, they can not move freely, which according to Descartes implies that every motion is circular, so the aether is filled with vortices. Descartes also distinguishes between different forms and sizes of matter in which rough matter resists the circular movement more strongly than fine matter. Due to centrifugal force, matter tends towards the outer edges of the vortex, which causes a condensation of this matter there. The rough matter cannot follow this movement due to its greater inertia—so due to the pressure of the condensed outer matter those parts will be pushed into the center of the vortex. According to Descartes, this inward pressure is nothing else than gravity. He compared this mechanism with the fact that if a rotating, liquid filled vessel is stopped, the liquid goes on to rotate. Now, if one drops small pieces of light matter (e.g. wood) into the vessel, the pieces move to the middle of the vessel.\n\nFollowing the basic premises of Descartes, Christiaan Huygens between 1669 and 1690 designed a much more exact vortex model. This model was the first theory of gravitation which was worked out mathematically. He assumed that the aether particles are moving in every direction, but were thrown back at the outer borders of the vortex and this causes (as in the case of Descartes) a greater concentration of fine matter at the outer borders. So also in his model the fine matter presses the rough matter into the center of the vortex. Huygens also found out that the centrifugal force is equal to the force, which acts in the direction of the center of the vortex (centripetal force). He also posited that bodies must consist mostly of empty space so that the aether can penetrate the bodies easily, which is necessary for mass proportionality. He further concluded that the aether moves much faster than the falling bodies. At this time, Newton developed his theory of gravitation which is based on attraction, and although Huygens agreed with the mathematical formalism, he said the model was insufficient due to the lack of a mechanical explanation of the force law. Newton's discovery that gravity obeys the inverse square law surprised Huygens and he tried to take this into account by assuming that the speed of the aether is smaller in greater distance.\n\n\"Criticism\": Newton objected to the theory because drag must lead to noticeable deviations of the orbits which were not observed. Another problem was that moons often move in different directions, against the direction of the vortex motion. Also, Huygens' explanation of the inverse square law is circular, because this means that the aether obeys Kepler's third law. But a theory of gravitation has to explain those laws and must not presuppose them.\n\nIn a 1675 letter to Henry Oldenburg, and later to Robert Boyle, Newton wrote the following: [Gravity is the result of] “a condensation causing a flow of ether with a corresponding thinning of the ether density associated with the increased velocity of flow.” He also asserted that such a process was consistent with all his other work and Kepler's Laws of Motion. Newtons' idea of a pressure drop associated with increased velocity of flow was mathematically formalised as Bernoulli's principle published in Daniel Bernoulli's book \"Hydrodynamica\" in 1738.\n\nHowever, although he later proposed a second explanation (see section below), Newton's comments to that question remained ambiguous. In the third letter to Bentley in 1692 he wrote:\n\nIt is inconceivable that inanimate brute matter should, without the mediation of something else which is not material, operate upon and affect other matter, without mutual contact, as it must do if gravitation in the sense of Epicurus be essential and inherent in it. And this is one reason why I desired you would not ascribe 'innate gravity' to me. That gravity should be innate, inherent, and essential to matter, so that one body may act upon another at a distance, through a vacuum, without the mediation of anything else, by and through which their action and force may be conveyed from one to another, is to me so great an absurdity, that I believe no man who has in philosophical matters a competent faculty of thinking can ever fall into it. Gravity must be caused by an agent acting constantly according to certain laws; but whether this agent be material or immaterial, I have left to the consideration of my readers.\n\nOn the other hand, Newton is also well known for the phrase Hypotheses non fingo, written in 1713:\n\nI have not as yet been able to discover the reason for these properties of gravity from phenomena, and I do not feign hypotheses. For whatever is not deduced from the phenomena must be called a hypothesis; and hypotheses, whether metaphysical or physical, or based on occult qualities, or mechanical, have no place in experimental philosophy. In this philosophy particular propositions are inferred from the phenomena, and afterwards rendered general by induction.\n\nAnd according to the testimony of some of his friends, such as Nicolas Fatio de Duillier or David Gregory, Newton thought that gravitation is based directly on divine influence.\n\nSimilar to Newton, but mathematically in greater detail, Bernhard Riemann assumed in 1853 that the gravitational aether is an incompressible fluid and normal matter represents sinks in this aether. So if the aether is destroyed or absorbed proportionally to the masses within the bodies, a stream arises and carries all surrounding bodies into the direction of the central mass. Riemann speculated that the absorbed aether is transferred into another world or dimension.\n\nAnother attempt to solve the energy problem was made by Ivan Osipovich Yarkovsky in 1888. Based on his aether stream model, which was similar to that of Riemann, he argued that the absorbed aether might be converted into new matter, leading to a mass increase of the celestial bodies.\n\n\"Criticism\": As in the case of Le Sage's theory, the disappearance of energy without explanation violates the energy conservation law. Also some drag must arise, and no process which leads to a creation of matter is known.\n\nNewton updated the second edition of \"Optics\" (1717) with another mechanical-ether theory of gravity. Unlike his first explanation (1675 - see Streams), he proposed a stationary aether which gets thinner and thinner nearby the celestial bodies. On the analogy of the lift, a force arises, which pushes all bodies to the central mass. He minimized drag by stating an extremely low density of the gravitational aether.\n\nLike Newton, Leonhard Euler presupposed in 1760 that the gravitational aether loses density in accordance with the inverse square law. Similarly to others, Euler also assumed that to maintain mass proportionality, matter consists mostly of empty space.\n\n\"Criticism\": Both Newton and Euler gave no reason why the density of that static aether should change. Furthermore, James Clerk Maxwell pointed out that in this \"hydrostatic\" model \"the state of stress... which we must suppose to exist in the invisible medium, is 3000 times greater than that which the strongest steel could support\".\n\nRobert Hooke speculated in 1671 that gravitation is the result of all bodies emitting waves in all directions through the aether. Other bodies, which interact with these waves, move in the direction of the source of the waves. Hooke saw an analogy to the fact that small objects on a disturbed surface of water move to the center of the disturbance.\n\nA similar theory was worked out mathematically by James Challis from 1859 to 1876. He calculated that the case of attraction occurs if the wavelength is large in comparison with the distance between the gravitating bodies. If the wavelength is small, the bodies repel each other. By a combination of these effects, he also tried to explain all other forces.\n\n\"Criticism\": Maxwell objected that this theory requires a steady production of waves, which must be accompanied by an infinite consumption of energy.\nChallis himself admitted, that he hadn't reached a definite result due to the complexity of the processes.\n\nLord Kelvin (1871) and Carl Anton Bjerknes (1871) assumed that all bodies pulsate in the aether. This was in analogy to the fact that, if the pulsation of two spheres in a fluid is in phase, they will attract each other; and if the pulsation of two spheres is \"not\" in phase, they will repel each other. This mechanism was also used for explaining the nature of electric charges. Among others, this hypothesis has also been examined by George Gabriel Stokes and Woldemar Voigt.\n\n\"Criticism\" : To explain universal gravitation, one is forced to assume that all pulsations in the universe are in phase—which appears very implausible. In addition, the aether should be incompressible to ensure that attraction also arises at greater distances. And Maxwell argued that this process must be accompanied by a permanent new production and destruction of aether.\n\nIn 1690, Pierre Varignon assumed that all bodies are exposed to pushes by aether particles from all directions, and that there is some sort of limitation at a certain distance from the Earth's surface which cannot be passed by the particles. He assumed that if a body is closer to the Earth than to the limitation boundary, then the body would experience a greater push from above than from below, causing it to fall toward the Earth.\n\nIn 1748, Mikhail Lomonosov assumed that the effect of the aether is proportional to the complete surface of the elementary components of which matter consists (similar to Huygens and Fatio before him). He also assumed an enormous penetrability of the bodies. However, no clear description was given by him as to how exactly the aether interacts with matter so that the law of gravitation arises.\n\nIn 1821, John Herapath tried to apply his co-developed model of the kinetic theory of gases on gravitation. He assumed that the aether is heated by the bodies and loses density so that other bodies are pushed to these regions of lower density.\nHowever, it was shown by Taylor that the decreased density due to thermal expansion is compensated for by the increased speed of the heated particles; therefore, no attraction arises.\n\nThese mechanical explanations for gravity never gained widespread acceptance, although such ideas continued to be studied occasionally by physicists until the beginning of the twentieth century, by which time it was generally considered to be conclusively discredited. However, some researchers outside the scientific mainstream still try to work out some consequences of those theories.\n\nLe Sage's theory was studied by Radzievskii and Kagalnikova (1960), Shneiderov (1961), Buonomano and Engels (1976), Adamut (1982), Jaakkola (1996), Tom Van Flandern (1999), and Edwards (2007). A variety of Le Sage models and related topics are discussed in Edwards, et al.\n\nGravity due to static pressure was recently studied by Arminjon.\n\n"}
{"id": "40418235", "url": "https://en.wikipedia.org/wiki?curid=40418235", "title": "Micromatabilin", "text": "Micromatabilin\n\nMicromatabilin, the green pigment of the spider species \"Micrommata virescens\", is characterized as a mixture of biliverdin conjugates. The two isolated fractions have identical absorption bands (free base: 620–630 μm, hydrochloride: 690 μm, zinc complex: 685–690 μm). Chromic acid degradation yields imides I, II, IIIa, and IIIb. Differences in the non-hydrolytic degradation and in polarity lead to the conclusion that fraction 1 is a monoconjugate and fraction 2a diconjugate of biliverdin.\n"}
{"id": "6321284", "url": "https://en.wikipedia.org/wiki?curid=6321284", "title": "Molecularium Project", "text": "Molecularium Project\n\nThe Molecularium Project is an informal science education project of Rensselaer Polytechnic Institute. The Molecularium Project introduces young audiences to the world of atoms and molecules using character driven stories, immersive animation, interactive games and activities, and state of the art molecular visualizations. Rensselaer's three principal Scientist / Educators behind the project are Dr. Linda S. Schadler, Dr. Richard W. Siegel, and Dr. Shekhar Garde. The Molecularium Project began as an outreach project of Rensselaer's Nanoscale Science and Engineering Center. To realize the productions, the scientists employed the creative team Nanotoon Entertainment, led by writer/director V. Owen Bush, and writer/producer Kurt Przybilla. The Molecularium Project is funded by Rensselaer, the National Science Foundation, and New York State.\n\nIn 2002, Dr. Schadler and Dr. Garde produced a seven-minute pilot show for the local planetarium called “Molecularium” for the Digistar II Planetarium system. It introduces children to the concepts of atoms and molecules from small molecules like H2O to larger molecules like polymers.\n\nIn early 2004, Schadler, Garde, and Siegel were awarded a U.S. National Science Foundation grant to make a new Molecularium show exclusively for the fulldome medum. They recruited the filmmaker and experience designer V. Owen Bush to bring the idea to life. Bush founded the production company Nanotoon Entertainment with writer/producer Kurt Przybilla to realize the new project. Bush and Przybilla proposed an adventure story of personified atoms flying a ship called the Molecularium through nanoscale materials including a snowflake, a penny, a stick of gum and the human body.\n\nIn February 2005, the team debuted \"Molecularium - Riding Snowflakes\" a 23-minute digital planetarium show at the Children's Museum of Science and Technology. In 2005, \"Molecularium - Riding Snowflakes\" won the Domie at Domefest in Albuquerque New Mexico. \"Molecularium - Riding Snowflakes\" has shown at Chabot Space and Science Center in Oakland Ca., the Newark Museum Planetarium in Newark, NJ, Dubai Children’s City, UAE, and Thinktank, Birmingham, UK, among many other digital planetariums. It has been translated and versioned in Arabic, Korean and Turkish. It is Distributed by E&S, Spitz, Sky-Skan, and e-Planetarium.\n\nIn 2010, the American Library Association (ALA) selected the Molecularium Kid's Site for inclusion to its Great Websites for Kids.\n\nMolecules to the MAX! is a 41-minute fully animated 3D IMAX film for the Giant Screen. The film re-imagines the characters and story developed for \"Molecularium- Riding Snowflakes\" for an older audience and a different medium. The film's simulations and rendering were partially computed at the Computational Center for Nanotechnology Innovations. The film was produced by Nanotoon Entertainment and Developed at Rensselaer, with a gift from Curtis R. Priem, co-founder of Nvidia corporation.\n\nThe digital version of the film premiered at EMPAC, in Troy, NY on Feb. 27, 2009. The IMAX version premiered at the Giant Screen Cinema Association International Conference and Trade Show in Indianapolis, Indiana, on September 22, 2009. The IMAX 3D Premiere was at the GSCA Film Expo in Los Angeles on Feb. 24, 2010. It is available in 2D & 3D for 15/70 and 8/70 large format film and in digital 3D. The film has been composed with Omnimax / IMAX Domes in mind. Molecules to the MAX! was nominated for Best Film Produced for the Giant Screen, Best Film for Lifelong Learning and Best Sound Design at the 2010 GSCA’s Achievement Awards. Molecules to the MAX! has shown at the National Museum of Natural Science (Taichung, Taiwan) Maloka Interactive Museum (Bogata, Columbia), The Scientific Center (Salmiya, Kuwait), McWane Science Center (Birmingham, Alabama) Proctor's Theatre (Schenectady, New York) among others. It has been translated and versioned in Spanish, Chinese, Japanese and Arabic. It is distributed to Giant Screen theaters by SK Films.\n\nIn the spring of 2012, the Molecularium Project launched NanoSpace, an online molecular theme park. Visitors to NanoSpace learn scientific concepts with games, activities and movies. Areas within Nanospace include the Hall of Atoms and Molecules, H2O park (the water cycle), Sizes in the Universe (scale and scientific notation), Material Boulevard (Materials Science), and DNA Land (Molecular Biology).\n"}
{"id": "22208238", "url": "https://en.wikipedia.org/wiki?curid=22208238", "title": "Moscow State University of Fine Chemical Technologies", "text": "Moscow State University of Fine Chemical Technologies\n\nMoscow State University of Fine Chemical Technologies named after M.V. Lomonosov (traditional abbreviation \"MITHT\") is one of the oldest universities in the country that offer training in a wide range of specialties in the field of chemical technology.\n\nCurrently, there are more than 4,500 students in nine areas of undergraduate, 28 master's programs and 23 scientific specialties for training of candidates and doctors of science. In MITHT there are 8 dissertation councils for doctoral and PhD theses.\n\nResearch and teaching activities are performed by more than 400 professors and 158 scientists, including more than 120 doctors of science and professors. Located in Moscow at Vernadsky Avenue, Building 86 (new building complex) and Malaya Pirogovskaya, Building 1 (historic building).\n\nHistory of the University and its continuing operations as a higher education institution begins 1 July 1900 and covers several stages.\n\n1 July (14 July, New Style) 1900 was organized the Moscow Higher Women Courses (MHWC). Their structure originally consisted of two departments: History and Philosophy and Physics and Mathematics. On the last one were soon opened two offices: mathematical and natural, and after a few years two more – medical and chemical-pharmaceutical. The initiators and the first lecturers were outstanding scientists, academics subsequently S. A. Chaplygin, V. I. Vernadsky, N. D. Zelinsky (the inventor of the gas mask (1916)), Professors V. F. Davidov, B. K. Mlodzeevskii, A. N. Reformatsky, A. A. Eichenwald, S. G. Krapivin. The first director of MHWC was Professor V. I. Guerrier.\n\nIn 1905 as a director was elected S. A. Chaplygin, the leading scientist in the field of hydro- and aerodynamics, the organizer of the construction of school buildings on the Malaya Pirogovskaya street (formerly Devichie Pole). He remained in that post until 1918.\n\nBy the beginning of World War I MHWC turned into the one of the largest higher education institutions in the country. The number of trainees reached 710, and during the existence of courses released 5760 professionals. In turning into a first-class university MHWC paramount importance had an exceptional organizational skill of S. A. Chaplygin, later shown to them with equal brilliance in creating TsAGI.\n\n16 October 1918 MHWC were converted into second Moscow State University. The first rector of the 2nd Moscow State University was appointed academician S. S. Nametkin who worked since 1913 as a head of the Department of Organic Chemistry of MHWC. As rector, he remained until 1924. As part of the 2nd Moscow State University became the chemical-pharmaceutical department, which in 1919 was transformed into the chemical and pharmaceutical department. At this time, on the faculty worked well-known Professors A. M. Berkengeim, B. K. Mlodzeevskii, S. S. Nametkin, M. I. Prozin, A. N. Reformatsky, O. N. Tsuberbiller. In 1929, the faculty became a chemical faculty of the university type with specialties such as:\n\nFaculty graduates go to work in the factories, involve in the implementation of research projects that receive a wide scope. During 1922 – 1928 years it has been published about 300 papers and 11 monographs. The greatest successes are achieved in the fields of organic and pharmaceutical chemistry under the direction of heads of departments, academics S. S. Nametkin, B. M. Rodionov, Professor A. M. Berkengeim. Production of new drugs being introduced in the pharmaceutical factory belonging to faculty.\n\n18 April 1930 by order of the People's Commissariat second MSU was reorganized into three independent institutions: Medical (now RSMU them. Pirogov) Pedagogical (now MPSU), and Chemical Technology (now MITHT). Last transferred to the jurisdiction of the Vsehimprom VSNKh USSR. In addressing this issue directly involved Sergo Ordzhonikidze.\n\n10 May 1931 Chemical and Pharmaceutical Faculty became an independent and received a new name – the Moscow Institute of Fine Chemical Technology (MITHT). Historically, the name of the Institute is due to the nature of objects that are studied by students: they were small capacity chemical and pharmaceutical technology, technology of platinum group metals and rare-earth elements. From this moment begins a new stage of development of the institution, which is quickly becoming one of the leading universities in the chemical industry. Front of it set the goal of training for high-tech industries of chemical technology. In MITHT the first time in the country began to train engineers on the technology of thin inorganic products, synthetic rubber, thin organic produce synthetic liquid fuels, organometallic compounds and a number of other specialties.\n\nIn the process of restructuring of education at the institute have been preserved and developed the best traditions of MHWC and 2nd Moscow State University: a high level of theoretical training and a combination of academic and scientific work, helped by the fact that the teaching work at the institute and chairing of departments were performed by outstanding scientists and educators which created a school and research areas.\n\nSpecial departments were prepared engineers for industries which were still being created in the first five years. At the time, were of great importance establishment of a domestic pharmaceutical industry and the country's liberation on imports of medicines. Were developed and implemented methods of production of such drugs as atophan, benzocaine, procaine, bromural, thiokol, ichtyol, validol, antipyrine, caffeine, alkaloids and others. In 1938, in MITHT under the leadership of academic A. N. Nesmeyanov (later President of the RAS) began work in the field of organometallic compounds. Also, the Institute prepared professionals for the companies producing such important national defense materials, such as tungsten, molybdenum, vanadium, and rare-earth elements. So, Professor I. Ya. Bashilov created the production technology of uranium and radium. And under the guidance of Professor G. A. Meyerson were carried out important work on carbothermy and getting super-hard alloys.\n\n7 May 1940 for academic achievement and great progress in the preparation of chemists institute is named after the outstanding Russian scientist Mikhail Vasilievich Lomonosov.\n\nDuring the war, groups of departments in collaboration with industry and research institutes conduct intensive research on defense-related development and implementation of the executed work. Thus, under the supervision of Professor N. I. Gelperin was created the most powerful bomb in World War II – FAB-5000NG that terrified Hitlerites. Efforts of the Institute as a whole and individual professors and teachers were appreciated by the country. Professors B. A. Dogadkin, N. I. Krasnopevtsev, V. V. Lebedinskij, S. S. Medvedev, S. I. Sklyarenko and Ya. K. Sirkin were awarded the Stalin Prize laureates, and associate professor K. A. Bolshakov won that title twice.\n\nThe post-war period was characterized by intensive work of the Institute staff in the aftermath of the war, the creation of the necessary conditions for teaching and research. Among the most important achievements of the postwar period MITHT need to include: \n\nThe high scientific – technical research conducted at the institute, evidenced by the fact that MITHT became one of the first places among the universities and research institutes on the number of implemented inventions. Employees of the institute published 450–500 scientific papers and received 50–60 invention certificates a year. The Institute conducts research on economic agreements and contracts on cooperation with industry and research institutes, whose number exceeded 150.\n\nMITHT, in fact, has turned into a complex of the university and research institutes. On 2500–3000 students, in addition to 400 professors and lecturers, employed more than 900 scientists. Two to one – that is the ratio of students, academic and teaching staff, which was reached in MITHT. Good specialist, engineer cannot be prepared without bringing him to participate in real, serious scientific research in the learning process.\n\n11 February 1971 for his services to the training of specialists for the national economy and the development of scientific research institute was awarded the Order of the Red Banner of Labour.\n\nEducation in MITHT always featured a deep fundamental training, which included, along with a full range of natural sciences study the big cycle engineering and technological disciplines. Special training was carried out by the so-called \"thin\" chemical technologies that are, as a rule, small capacity technology, implemented on the basis of the latest achievements of chemical science and technology. That is, in fact, the students received a university degree in combination with engineering training.\n\nIn 1992, the Moscow Institute of Fine Chemical Technology named after M. V. Lomonosov has received a new, higher educational status – status of the Academy. With the name change has changed the status and range of activities of the institute along with old technology, new specialty Humanitarian – Management Profile: \"Economics and Management (chemical industry)\", \"Environmental Protection\", \"Standardization and Certification\". These specialties are subject to major technologies and solve their narrow-profile tasks.\n\nBefore the transition to a tiered structure the education of students was carried out in one direction, \"Chemical Technology and Biotechnology\", which consists of seven specialties. After the transition academy led training in seven areas of undergraduate, graduate five areas (including 26 master's programs) and 13 majors (including 25 majors) in full-time and part-correspondence courses, and conducted post-graduate education in 24 specialties and additional education in primary educational programs MITHT.\n\nTo implement in MITHT a tiered system of higher education were opened new training units. Along with main faculties teaching of students was performed at the Faculty of Natural Science, Faculty of Humanities, Faculty of Management, Economics and Environment, Faculty of Engineering, Faculty of further education at the Institute of Distance Education.\n\nAccording to the Federal Agency for Supervision in Education and Science in 2008 in MITHT worked one of the most highly qualified scientific and technical teaching staff of universities and academies of Russia: doctors and candidates of sciences accounted for about 80% of the teachers. At the academy a total enrollment of 4,500 undergraduate and graduate students, taught 119 professors, doctors, and 218 associate professors, candidates of sciences.\n\nIn 2011, the Academy received a University status.\n\nThe successes of the University in education, research and innovation, the recognition of the scientific and pedagogical schools, its international status and worldwide fame are undoubtedly merit in the first professors, assistant professors, lecturers, and they have created in the walls of the university unique creative scientific and educational environment. Friendly and supportive atmosphere of the business and human interaction with the students and teachers together form the necessary conditions for the development of the individual, focused, motivated professional growth.\n\n\n\n\nChemistry:\n\nChemical technology:\n\nBiotechnology:\n\nMaterials science and technology of materials:\n\nTechnosphere Safety:\n\nStandardization and Certification:\n\n\n\n\n\n\n\n\n"}
{"id": "4368966", "url": "https://en.wikipedia.org/wiki?curid=4368966", "title": "Nature writing", "text": "Nature writing\n\nNature writing is nonfiction or fiction prose or poetry about the natural environment. Nature writing encompasses a wide variety of works, ranging from those that place primary emphasis on natural history facts (such as field guides) to those in which philosophical interpretation predominate. It includes natural history essays, poetry, essays of solitude or escape, as well as travel and adventure writing.\n\nNature writing often draws heavily on scientific information and facts about the natural world; at the same time, it is frequently written in the first person and incorporates personal observations of and philosophical reflections upon nature.\n\nModern nature writing traces its roots to the works of natural history that were popular in the second half of the 18th century and throughout the 19th. An important early figures was the \"parson-naturalist\" Gilbert White (1720 – 1793), a pioneering English naturalist and ornithologist. He is best known for his \"Natural History and Antiquities of Selborne\" (1789).\n\nWilliam Bartram (1739 – 1823) is a significant early American pioneer naturalist who first work was published in 1791.\n\nGilbert White is regarded by many as England's first ecologist, and one of those who shaped the modern attitude of respect for nature. He said of the earthworm: \"Earthworms, though in appearance a small and despicable link in the chain of nature, yet, if lost, would make a lamentable chasm. [...] worms seem to be the great promoters of vegetation, which would proceed but lamely without them\" White and William Markwick collected records of the dates of emergence of more than 400 plant and animal species in Hampshire and Sussex between 1768 and 1793, which was summarised in \"The Natural History and Antiquities of Selborne\", as the earliest and latest dates for each event over the 25-year period, are among the earliest examples of modern phenology.\n\nThe tradition of clerical naturalists predates White and can be traced back to some monastic writings of the Middle Ages, although some argue that their writings about animals and plants cannot be correctly classified as natural history. Notable early parson-naturalists were William Turner (1508–1568), John Ray (1627–1705), William Derham (1657–1735).\n\nWilliam Bertram, in 1773, embarked on a four year journey through eight southern American colonies. Bartram made many drawings and took notes on the native flora and fauna, and the native American Indians. In 1774, he explored the St. Johns River. William Bartram wrote of his experiences exploring the Southeast in his book known today as \"Bartram's Travels\", published in 1791. Ephraim George Squier and Edwin Hamilton Davis, in their book, \"Ancient Monuments of the Mississippi Valley\", name Bartram as \"the first naturalist who penetrated the dense tropical forests of Florida.\"\n\nAfter Gilbert White and William Bertram, other significant writers include American ornithologist John James Audubon (1785 – 1851), Charles Darwin( (1809 – 1882), Richard Jefferies (1848 – 1887), Susan Fenimore Cooper (1813 – 1894), mother of American nature writting, and Henry David Thoreau (1817 – 1862), who is often considered the father of modern American nature writing, Ralph Waldo Emerson (1803 – 1882) John Burroughs, John Muir, Aldo Leopold, Rachel Carson, M. Krishnan, and Edward Abbey (although he rejected the term for himself).\n\nAnother important early work is \"A History of British Birds\" by Thomas Bewick, published in two volumes. Volume 1, \"Land Birds\", appeared in 1797. Volume 2, \"Water Birds\", appeared in 1804. The book was effectively the first \"field guide\" for non-specialists. Bewick provides an accurate illustration of each species, from life if possible, or from skins. The common and scientific name(s) are listed, citing the naming authorities. The bird is described, with its distribution and behaviour, often with extensive quotations from printed sources or correspondents. Critics note Bewick's skill as a naturalist as well as an engraver.\n\nSome important contemporary figures in Britain include Richard Mabey, Roger Deakin, Mark Cocker, and Oliver Rackham. Rackham's books included \"Ancient Woodland\" (1980) and \"The History of the Countryside\" (1986). Richard Maybey has been involved with radio and television programmes on nature, and his book \"Nature Cure\", describes his experiences and recovery from depression in the context of man’s relationship with landscape and nature. He has also edited and introduced editions of Richard Jefferies, Gilbert White, Flora Thompson and Peter Matthiessen. Mark Crocker has written extensively for British newspapers and magazines and his books include Birds Britannica (with Richard Mabey) (2005). and \"Crow Country\" (2007). He frequently writes about modern responses to the wild, whether found in landscape, human societies or in other species. Roger Deakin was an English writer, documentary-maker and environmentalist. In 1999, Deakin's acclaimed book \"Waterlog\" was published. Inspired in part by the short story \"The Swimmer\" by John Cheever, it describes his experiences of 'wild swimming' in Britain's rivers and lakes and advocates open access to the countryside and waterways. Deakin's book \"Wildwood\" appeared posthumously in 2007. It describes a series of journeys across the globe that Deakin made to meet people whose lives are intimately connected to trees and wood.\n\nIn 2017 the German book publishing company Matthes & Seitz Berlin started to grant the German Award for Nature Writing, an annual literary award for writers in German language that excellently fulfil the criteria of the literary genre. It comes with a price money of 10.000 Euro and additionallly an artist in residency grant of six weeks at the International Academy for Nature Conservation of Germany on the German island Vilm. The British Council in 2018 is offering an education bursary and workshops to six young German authors dedicated to Nature writing.\n\n\n\n"}
{"id": "55071594", "url": "https://en.wikipedia.org/wiki?curid=55071594", "title": "Neutron embrittlement", "text": "Neutron embrittlement\n\nNeutron embrittlement, sometimes more broadly radiation embrittlement, is the embrittlement of various materials due to the action of neutrons. This is primarily seen in nuclear reactors, where the release of high-energy neutrons causes the long-term degradation of the reactor materials. The embrittlement is caused by the microscopic movement of atoms that are hit by the neutrons, this same action also gives rise to neutron-induced swelling causes materials to grow in size, and the Wigner effect causes energy buildup in certain materials that can lead to sudden releases of energy.\n\nNeutron embrittlement mechanisms include:\n\nNeutron irradiation embrittlement limits the service life of reactor-pressure vessels (RPV) in nuclear power plants due to the degradation of reactor materials. In order to perform at high efficiency and safely contain coolant water at temperatures around 290ºC and pressures of ~7 MPa (for boiling water reactors) to 14 MPa (for pressurized water reactors), the RPV must be heavy-section steel. Due to regulations, RPV failure probabilities must be very low. To achieve sufficient safety, the design of the reactor assumes large cracks and extreme loading conditions. Under such conditions, a probable failure mode is rapid, catastrophic fracture if the vessel steel is brittle. Tough RPV base metals that are typically used are A302B, A533B plates, or A508 forgings; these are quenched and tempered, low-alloy steels with primarily tempered bainitic microstructures. Over the past few decades, RPV embrittlement has been addressed by the use of tougher steels with lower trace impurity contents, the decrease of neutron flux that the vessel is subject to, and the elimination of beltline welds. However, embrittlement remains an issue for older reactors.\n\nPressurized water reactors are more susceptible to embrittlement than boiling water reactors. This is due to PWRs sustaining more neutron impacts. To counteract this, many PWRs have a specific core design that reduces the number of neutrons hitting the vessel wall. Moreover, PWR designs must be especially mindful of embrittlement because of pressurized thermal shock, an accident scenario that occurs when cold water enters a pressurized reactor vessel, introducing large thermal stress. This thermal stress may cause fracture if the reactor vessel is sufficiently brittle.\n\n\n"}
{"id": "7608175", "url": "https://en.wikipedia.org/wiki?curid=7608175", "title": "Ontogeny and Phylogeny (book)", "text": "Ontogeny and Phylogeny (book)\n\nOntogeny and Phylogeny is a 1977 book on evolution by Stephen Jay Gould, in which the author explores the relationship between embryonic development (ontogeny) and biological evolution (phylogeny). Unlike his many popular books of essays, it was a technical book, and over the following decades it was influential in stimulating research into heterochrony, changes in the timing of embryonic development, which had been neglected since Ernst Haeckel's theory that ontogeny recapitulates phylogeny had been largely discredited.\n\n\"Ontogeny and Phylogeny\" is Stephen Jay Gould's first technical book. He wrote that Ernst Mayr had suggested in passing that he write a book on development. Gould stated he \"only began it as a practice run to learn the style of lengthy exposition before embarking on my magnum opus about macroevolution.\" This later work was published in 2002 as \"The Structure of Evolutionary Theory\".\n\nThe first half of the book explores Ernst Haeckel's biogenetic law (recapitulation)—the largely discredited idea that embryonic developmental stages replay the evolutionary transitions of adult forms of an organism's past descendants—and how this idea influenced thinking in biology, theology, and psychology.\n\nThe second half of the book details how modern concepts such as heterochrony (changes in developmental timing) and neoteny (the retardation of developmental expression or growth rates) influence macroevolution (major evolutionary transitions).\n\nThe herpetologist David B. Wake, in \"Paleobiology\", wrote that the topic was \"at once so obviously important and so intrinsically difficult\" that few people would tackle it. The parallelism that Haeckel noted between ontogeny and phylogeny was, Wake observed, a strong argument for evolution, but hardly anyone dared to discuss it. He called the book very good, and predicted that it would set the stage for \"endless research\", but found it also in a way unsatisfying, using \"undigested theory from ecology to \"explain\" what is, as yet, unexplainable. Summing up, Wake calls the book \"erudite, important, provocative, and controversial\", but noted that it could have been much shorter.\n\nThe embryologist Søren Løvtrup, in \"Systematic Zoology\", noted that the book had two objectives, unexceptionably to gain practice, and \"more dubious[ly]\", to show that \"in spite of the collapse of Haeckel's biogenetic law, the subject of parallels between ontogenesis and phylogenesis is still of importance to biology\". In Løvtrup's view, this was because Haeckel's law had been refuted except where evolution had by chance happened to add to the end of development. Gould had little new to report, as people knew half a century earlier that development could be modified at other stages; the book was \"a great disappointment.\" Haeckel could \"of course be of historical interest\" but Gould had chosen not to research Haeckel's influence. Work on \"wrong theories\" represented, Løvtrup wrote, \"a terrible waste of effort and time, and block[ed] further progress.\"\n\nThe anthropologist C. Loring Brace, in \"American Anthropologist\", noted that 2 years earlier, E. O. Wilson's \"Sociobiology\" had with \"woeful ignorance\" strayed into anthropology, and Wilson's \"bright young colleague\" Gould had now done the same thing, possibly making trouble for years to come. Gould was \"a wonderful writer, literate, erudite, gracefully witty, and gifted with the ability to present difficult material in a straightforward and easily readable fashion.\" \nThe bulk of the book was fine, though of no interest to anthropologists. But the tenth chapter, \"Retardation and Neoteny in Human Evolution\", would \"mislead a great many people\" who would be unable to make an informed judgement about its conclusions. Gould \" tums out to be just as much of a teleologist and progressivist as the scholars of previous generations whom he appraises so effectively. He notes that we associate “cute” features with mammals of higher intelligence, features that show 'the common traits of babyhood: relatively large eyes, short face, smooth features, bulbous cranium. The presence of this complex in ad- vanced adult mammals argues for neoteny' (Gould p. 350).\" In Brace's view, \"Gould's main thesis founders between the Scylla of mosaic evolution and the Charybdis of Darwinian theory.\" Brace concluded that Gould had provided \"nothing more useful than the vision that human form can be understood by regarding 'man' as an overgrown retarded child.\"\n\nJames Gorman, in \"The New York Times\", wrote that the book was rich but not easy to read; it was primarily for biologists, with long and precise arguments in technical language; a simpler account of the same topic was to be found in Gould's essay \"Ever Since Darwin\". Gorman called the book scholarly, entertaining and informative, expressed \"with clarity and wit\".\n\nThe zoologist A. J. Cain, in \"Nature\", called it \"a superb analysis of the use of ontogenetic analogy, the controversies over ontogeny and phylogeny, and the classification of the different processes observable in comparing different ontogenies.\" It was a \"massive book\", excellently illustrated with often surprising examples, covering both the history and a functional interpretation of heterochrony. Cain found it refreshing to find someone who had a good word for Ernst Haeckel, and who did not \"treat Charles Bonnet as a stupid monomaniac\" but who brought out the relationship \"between acquired characters and recapitulation in the work of the American neo-Lamarckians\".\n\nThe evolutionary biologists Kenneth McNamara and Michael McKinney stated in 2005 that of all the books that Gould wrote in his career, \"the one with the most impact is probably \"Ontogeny and Phylogeny\" ... to say that this work is a hallmark in this area of evolutionary theory would be an understatement. It proved to be the catalyst for much of the future work in the field, and to a large degree was the inspiration for the modern field of evolutionary developmental biology. Gould's hope was to show that the relationship between ontogeny and phylogeny is fundamental to evolution, and at its heart is a simple premise—that variations in the timing and rate of development provide the raw material upon which natural selection can operate.\"\n\nM. Elizabeth Barnes, in \"The Embryo Project Encyclopedia\", looking back at the book in 2014, writes that it became widely cited in evolutionary and developmental biology, encouraging research on acceleration and retardation of development (forms of heterochrony), and investigation of paedomorphosis in human evolution. Barnes notes that \"along with other work by Gould, such as 'The Spandrels of San Marco and the Panglossian Paradigm' [the book] is often credited for influencing the rise of a biological approach called evolutionary developmental biology or evo-devo, which worked to integrate evolutionary and developmental biology.\"\n\n"}
{"id": "48781", "url": "https://en.wikipedia.org/wiki?curid=48781", "title": "Philosophiæ Naturalis Principia Mathematica", "text": "Philosophiæ Naturalis Principia Mathematica\n\nPhilosophiæ Naturalis Principia Mathematica (Latin for \"Mathematical Principles of Natural Philosophy\"), often referred to as simply the Principia , is a work in three books by Isaac Newton, in Latin, first published 5 July 1687. After annotating and correcting his personal copy of the first edition, Newton published two further editions, in 1713 and 1726. The \"Principia\" states Newton's laws of motion, forming the foundation of classical mechanics; Newton's law of universal gravitation; and a derivation of Kepler's laws of planetary motion (which Kepler first obtained empirically).\n\nThe \"Principia\" is considered one of the most important works in the history of science.\n\nThe French mathematical physicist Alexis Clairaut assessed it in 1747: \"The famous book of \"Mathematical Principles of Natural Philosophy\" marked the epoch of a great revolution in physics. The method followed by its illustrious author Sir Newton ... spread the light of mathematics on a science which up to then had remained in the darkness of conjectures and hypotheses.\"\n\nA more recent assessment has been that while acceptance of Newton's theories was not immediate, by the end of a century after publication in 1687, \"no one could deny that\" (out of the \"Principia\") \"a science had emerged that, at least in certain respects, so far exceeded anything that had ever gone before that it stood alone as the ultimate exemplar of science generally.\"\n\nIn formulating his physical theories, Newton developed and used mathematical methods now included in the field of calculus. But the language of calculus as we know it was largely absent from the \"Principia\"; Newton gave many of his proofs in a geometric form of infinitesimal calculus, based on limits of ratios of vanishing small geometric quantities. In a revised conclusion to the \"Principia\" (see \"General Scholium\"), Newton used his expression that became famous, \"Hypotheses non fingo\" (\"I formulate no hypotheses\").\n\nIn the preface of the \"Principia\", Newton wrote:\n\nThe \"Principia\" deals primarily with massive bodies in motion, initially under a variety of conditions and hypothetical laws of force in both non-resisting and resisting media, thus offering criteria to decide, by observations, which laws of force are operating in phenomena that may be observed. It attempts to cover hypothetical or possible motions both of celestial bodies and of terrestrial projectiles. It explores difficult problems of motions perturbed by multiple attractive forces. Its third and final book deals with the interpretation of observations about the movements of planets and their satellites.\n\nIt shows:\n\nThe opening sections of the \"Principia\" contain, in revised and extended form, nearly all of the content of Newton's 1684 tract \"De motu corporum in gyrum\".\n\nThe \"Principia\" begin with \"Definitions\" and \"Axioms or Laws of Motion\", and continues in three books:\n\nBook 1, subtitled \"De motu corporum\" (\"On the motion of bodies\") concerns motion in the absence of any resisting medium. It opens with a mathematical exposition of \"the method of first and last ratios\", a geometrical form of infinitesimal calculus.\nThe second section establishes relationships between centripetal forces and the law of areas now known as Kepler's second law (Propositions 1–3), and relates circular velocity and radius of path-curvature to radial force (Proposition 4), and relationships between centripetal forces varying as the inverse-square of the distance to the center and orbits of conic-section form (Propositions 5–10).\n\nPropositions 11–31 establish properties of motion in paths of eccentric conic-section form including ellipses, and their relation with inverse-square central forces directed to a focus, and include Newton's theorem about ovals (lemma 28).\n\nPropositions 43–45 are demonstration that in an eccentric orbit under centripetal force where the apse may move, a steady non-moving orientation of the line of apses is an indicator of an inverse-square law of force.\n\nBook 1 contains some proofs with little connection to real-world dynamics. But there are also sections with far-reaching application to the solar system and universe:\n\nPropositions 57–69 deal with the \"motion of bodies drawn to one another by centripetal forces\". This section is of primary interest for its application to the solar system, and includes Proposition 66 along with its 22 corollaries: here Newton took the first steps in the definition and study of the problem of the movements of three massive bodies subject to their mutually perturbing gravitational attractions, a problem which later gained name and fame (among other reasons, for its great difficulty) as the three-body problem.\n\nPropositions 70–84 deal with the attractive forces of spherical bodies. The section contains Newton's proof that a massive spherically symmetrical body attracts other bodies outside itself as if all its mass were concentrated at its centre. This fundamental result, called the Shell theorem, enables the inverse square law of gravitation to be applied to the real solar system to a very close degree of approximation.\n\nPart of the contents originally planned for the first book was divided out into a second book, which largely concerns motion through resisting mediums. Just as Newton examined consequences of different conceivable laws of attraction in Book 1, here he examines different conceivable laws of resistance; thus discusses resistance in direct proportion to velocity, and goes on to examine the implications of resistance in proportion to the square of velocity. Book 2 also discusses (in ) hydrostatics and the properties of compressible fluids. The effects of air resistance on pendulums are studied in , along with Newton's account of experiments that he carried out, to try to find out some characteristics of air resistance in reality by observing the motions of pendulums under different conditions. Newton compares the resistance offered by a medium against motions of globes with different properties (material, weight, size). In Section 8, he derives rules to determine the speed of waves in fluids and relates them to the density and condensation(Proposition 48; this would become very important in acoustics). He assumes that these rules apply equally to light and sound and estimates that the speed of sound is around 1088 feet per second and can increase depending on the amount of water in air.\n\nLess of Book 2 has stood the test of time than of Books 1 and 3, and it has been said that Book 2 was largely written on purpose to refute a theory of Descartes which had some wide acceptance before Newton's work (and for some time after). According to this Cartesian theory of vortices, planetary motions were produced by the whirling of fluid vortices that filled interplanetary space and carried the planets along with them. Newton wrote at the end of Book 2 his conclusion that the hypothesis of vortices was completely at odds with the astronomical phenomena, and served not so much to explain as to confuse them.\n\nBook 3, subtitled \"De mundi systemate\" (\"On the system of the world\"), is an exposition of many consequences of universal gravitation, especially its consequences for astronomy. It builds upon the propositions of the previous books, and applies them with further specificity than in Book 1 to the motions observed in the solar system. Here (introduced by Proposition 22, and continuing in Propositions 25–35) are developed several of the features and irregularities of the orbital motion of the Moon, especially the variation. Newton lists the astronomical observations on which he relies, and establishes in a stepwise manner that the inverse square law of mutual gravitation applies to solar system bodies, starting with the satellites of Jupiter and going on by stages to show that the law is of universal application. He also gives starting at Lemma 4 and Proposition 40 the theory of the motions of comets, for which much data came from John Flamsteed and Edmond Halley, and accounts for the tides, attempting quantitative estimates of the contributions of the Sun and Moon to the tidal motions; and offers the first theory of the precession of the equinoxes. Book 3 also considers the harmonic oscillator in three dimensions, and motion in arbitrary force laws.\n\nIn Book 3 Newton also made clear his heliocentric view of the solar system, modified in a somewhat modern way, since already in the mid-1680s he recognised the \"deviation of the Sun\" from the centre of gravity of the solar system. For Newton, \"the common centre of gravity of the Earth, the Sun and all the Planets is to be esteem'd the Centre of the World\", and that this centre \"either is at rest, or moves uniformly forward in a right line\". Newton rejected the second alternative after adopting the position that \"the centre of the system of the world is immoveable\", which \"is acknowledg'd by all, while some contend that the Earth, others, that the Sun is fix'd in that centre\". Newton estimated the mass ratios Sun:Jupiter and Sun:Saturn, and pointed out that these put the centre of the Sun usually a little way off the common center of gravity, but only a little, the distance at most \"would scarcely amount to one diameter of the Sun\".\n\nThe sequence of definitions used in setting up dynamics in the \"Principia\" is recognisable in many textbooks today. Newton first set out the definition of mass\n\nThis was then used to define the \"quantity of motion\" (today called momentum), and the principle of inertia in which mass replaces the previous Cartesian notion of \"intrinsic force\". This then set the stage for the introduction of forces through the change in momentum of a body. Curiously, for today's readers, the exposition looks dimensionally incorrect, since Newton does not introduce the dimension of time in rates of changes of quantities.\n\nHe defined space and time \"not as they are well known to all\". Instead, he defined \"true\" time and space as \"absolute\" and explained:\n\nTo some modern readers it can appear that some dynamical quantities recognised today were used in the \"Principia\" but not named. The mathematical aspects of the first two books were so clearly consistent that they were easily accepted; for example, Locke asked Huygens whether he could trust the mathematical proofs, and was assured about their correctness.\n\nHowever, the concept of an attractive force acting at a distance received a cooler response. In his notes, Newton wrote that the inverse square law arose naturally due to the structure of matter. However, he retracted this sentence in the published version, where he stated that the motion of planets is consistent with an inverse square law, but refused to speculate on the origin of the law. Huygens and Leibniz noted that the law was incompatible with the notion of the aether. From a Cartesian point of view, therefore, this was a faulty theory. Newton's defence has been adopted since by many famous physicists—he pointed out that the mathematical form of the theory had to be correct since it explained the data, and he refused to speculate further on the basic nature of gravity. The sheer number of phenomena that could be organised by the theory was so impressive that younger \"philosophers\" soon adopted the methods and language of the \"Principia\".\n\nPerhaps to reduce the risk of public misunderstanding, Newton included at the beginning of Book 3 (in the second (1713) and third (1726) editions) a section entitled \"Rules of Reasoning in Philosophy.\" In the four rules, as they came finally to stand in the 1726 edition, Newton effectively offers a methodology for handling unknown phenomena in nature and reaching towards explanations for them. The four Rules of the 1726 edition run as follows (omitting some explanatory comments that follow each):\n\nRule 1: \"We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances.\"\n\nRule 2: \"Therefore to the same natural effects we must, as far as possible, assign the same causes.\"\n\nRule 3: \"The qualities of bodies, which admit neither intensification nor remission of degrees, and which are found to belong to all bodies within the reach of our experiments, are to be esteemed the universal qualities of all bodies whatsoever.\"\n\nRule 4: \"In experimental philosophy we are to look upon propositions inferred by general induction from phenomena as accurately or very nearly true, not withstanding any contrary hypothesis that may be imagined, till such time as other phenomena occur, by which they may either be made more accurate, or liable to exceptions.\"\n\nThis section of Rules for philosophy is followed by a listing of 'Phenomena', in which are listed a number of mainly astronomical observations, that Newton used as the basis for inferences later on, as if adopting a consensus set of facts from the astronomers of his time.\n\nBoth the 'Rules' and the 'Phenomena' evolved from one edition of the \"Principia\" to the next. Rule 4 made its appearance in the third (1726) edition; Rules 1–3 were present as 'Rules' in the second (1713) edition, and predecessors of them were also present in the first edition of 1687, but there they had a different heading: they were not given as 'Rules', but rather in the first (1687) edition the predecessors of the three later 'Rules', and of most of the later 'Phenomena', were all lumped together under a single heading 'Hypotheses' (in which the third item was the predecessor of a heavy revision that gave the later Rule 3).\n\nFrom this textual evolution, it appears that Newton wanted by the later headings 'Rules' and 'Phenomena' to clarify for his readers his view of the roles to be played by these various statements.\n\nIn the third (1726) edition of the \"Principia\", Newton explains each rule in an alternative way and/or gives an example to back up what the rule is claiming. The first rule is explained as a philosophers' principle of economy. The second rule states that if one cause is assigned to a natural effect, then the same cause so far as possible must be assigned to natural effects of the same kind: for example respiration in humans and in animals, fires in the home and in the Sun, or the reflection of light whether it occurs terrestrially or from the planets. An extensive explanation is given of the third rule, concerning the qualities of bodies, and Newton discusses here the generalisation of observational results, with a caution against making up fancies contrary to experiments, and use of the rules to illustrate the observation of gravity and space.\n\nIsaac Newton’s statement of the four rules revolutionised the investigation of phenomena. With these rules, Newton could in principle begin to address all of the world’s present unsolved mysteries. He was able to use his new analytical method to replace that of Aristotle, and he was able to use his method to tweak and update Galileo’s experimental method. The re-creation of Galileo's method has never been significantly changed and in its substance, scientists use it today.\n\nThe \"General Scholium\" is a concluding essay added to the second edition, 1713 (and amended in the third edition, 1726). It is not to be confused with the \"General Scholium\" at the end of Book 2, Section 6, which discusses his pendulum experiments and resistance due to air, water, and other fluids.\n\nHere Newton used what became his famous expression Hypotheses non fingo, \"I formulate no hypotheses\", in response to criticisms of the first edition of the \"Principia\". (\"<nowiki>'Fingo'</nowiki>\" is sometimes nowadays translated 'feign' rather than the traditional 'frame'.) Newton's gravitational attraction, an invisible force able to act over vast distances, had led to criticism that he had introduced \"occult agencies\" into science. Newton firmly rejected such criticisms and wrote that it was enough that the phenomena implied gravitational attraction, as they did; but the phenomena did not so far indicate the cause of this gravity, and it was both unnecessary and improper to frame hypotheses of things not implied by the phenomena: such hypotheses \"have no place in experimental philosophy\", in contrast to the proper way in which \"particular propositions are inferr'd from the phenomena and afterwards rendered general by induction\".\n\nNewton also underlined his criticism of the vortex theory of planetary motions, of Descartes, pointing to its incompatibility with the highly eccentric orbits of comets, which carry them \"through all parts of the heavens indifferently\".\n\nNewton also gave theological argument. From the system of the world, he inferred the existence of a Lord God, along lines similar to what is sometimes called the argument from intelligent or purposive design. It has been suggested that Newton gave \"an oblique argument for a unitarian conception of God and an implicit attack on the doctrine of the Trinity\", but the General Scholium appears to say nothing specifically about these matters.\n\nIn January 1684, Edmond Halley, Christopher Wren and Robert Hooke had a conversation in which Hooke claimed to not only have derived the inverse-square law, but also all the laws of planetary motion. Wren was unconvinced, Hooke did not produce the claimed derivation although the others gave him time to do it, and Halley, who could derive the inverse-square law for the restricted circular case (by substituting Kepler's relation into Huygens' formula for the centrifugal force) but failed to derive the relation generally, resolved to ask Newton.\n\nHalley's visits to Newton in 1684 thus resulted from Halley's debates about planetary motion with Wren and Hooke, and they seem to have provided Newton with the incentive and spur to develop and write what became \"Philosophiae Naturalis Principia Mathematica\". Halley was at that time a Fellow and Council member of the Royal Society in London (positions that in 1686 he resigned to become the Society's paid Clerk). Halley's visit to Newton in Cambridge in 1684 probably occurred in August. When Halley asked Newton's opinion on the problem of planetary motions discussed earlier that year between Halley, Hooke and Wren, Newton surprised Halley by saying that he had already made the derivations some time ago; but that he could not find the papers. (Matching accounts of this meeting come from Halley and Abraham De Moivre to whom Newton confided.) Halley then had to wait for Newton to 'find' the results, but in November 1684 Newton sent Halley an amplified version of whatever previous work Newton had done on the subject. This took the form of a 9-page manuscript, \"De motu corporum in gyrum\" (\"Of the motion of bodies in an orbit\"): the title is shown on some surviving copies, although the (lost) original may have been without title.\n\nNewton's tract \"De motu corporum in gyrum\", which he sent to Halley in late 1684, derived what are now known as the three laws of Kepler, assuming an inverse square law of force, and generalised the result to conic sections. It also extended the methodology by adding the solution of a problem on the motion of a body through a resisting medium. The contents of \"De motu\" so excited Halley by their mathematical and physical originality and far-reaching implications for astronomical theory, that he immediately went to visit Newton again, in November 1684, to ask Newton to let the Royal Society have more of such work. The results of their meetings clearly helped to stimulate Newton with the enthusiasm needed to take his investigations of mathematical problems much further in this area of physical science, and he did so in a period of highly concentrated work that lasted at least until mid-1686.\n\nNewton's single-minded attention to his work generally, and to his project during this time, is shown by later reminiscences from his secretary and copyist of the period, Humphrey Newton. His account tells of Isaac Newton's absorption in his studies, how he sometimes forgot his food, or his sleep, or the state of his clothes, and how when he took a walk in his garden he would sometimes rush back to his room with some new thought, not even waiting to sit before beginning to write it down. Other evidence also shows Newton's absorption in the \"Principia\": Newton for years kept up a regular programme of chemical or alchemical experiments, and he normally kept dated notes of them, but for a period from May 1684 to April 1686, Newton's chemical notebooks have no entries at all. So it seems that Newton abandoned pursuits to which he was normally dedicated, and did very little else for well over a year and a half, but concentrated on developing and writing what became his great work.\n\nThe first of the three constituent books was sent to Halley for the printer in spring 1686, and the other two books somewhat later. The complete work, published by Halley at his own financial risk, appeared in July 1687. Newton had also communicated \"De motu\" to Flamsteed, and during the period of composition he exchanged a few letters with Flamsteed about observational data on the planets, eventually acknowledging Flamsteed's contributions in the published version of the \"Principia\" of 1687.\n\nThe process of writing that first edition of the \"Principia\" went through several stages and drafts: some parts of the preliminary materials still survive, while others are lost except for fragments and cross-references in other documents.\n\nSurviving materials show that Newton (up to some time in 1685) conceived his book as a two-volume work. The first volume was to be titled \"De motu corporum, Liber primus\", with contents that later appeared in extended form as Book 1 of the \"Principia\".\n\nA fair-copy draft of Newton's planned second volume \"De motu corporum, Liber secundus\" survives, its completion dated to about the summer of 1685. It covers the application of the results of \"Liber primus\" to the Earth, the Moon, the tides, the solar system, and the universe; in this respect it has much the same purpose as the final Book 3 of the \"Principia\", but it is written much less formally and is more easily read.\nIt is not known just why Newton changed his mind so radically about the final form of what had been a readable narrative in \"De motu corporum, Liber secundus\" of 1685, but he largely started afresh in a new, tighter, and less accessible mathematical style, eventually to produce Book 3 of the \"Principia\" as we know it. Newton frankly admitted that this change of style was deliberate when he wrote that he had (first) composed this book \"in a popular method, that it might be read by many\", but to \"prevent the disputes\" by readers who could not \"lay aside the[ir] prejudices\", he had \"reduced\" it \"into the form of propositions (in the mathematical way) which should be read by those only, who had first made themselves masters of the principles established in the preceding books\". The final Book 3 also contained in addition some further important quantitative results arrived at by Newton in the meantime, especially about the theory of the motions of comets, and some of the perturbations of the motions of the Moon.\n\nThe result was numbered Book 3 of the \"Principia\" rather than Book 2, because in the meantime, drafts of \"Liber primus\" had expanded and Newton had divided it into two books. The new and final Book 2 was concerned largely with the motions of bodies through resisting mediums.\n\nBut the \"Liber secundus\" of 1685 can still be read today. Even after it was superseded by Book 3 of the \"Principia\", it survived complete, in more than one manuscript. After Newton's death in 1727, the relatively accessible character of its writing encouraged the publication of an English translation in 1728 (by persons still unknown, not authorised by Newton's heirs). It appeared under the English title \"A Treatise of the System of the World\". This had some amendments relative to Newton's manuscript of 1685, mostly to remove cross-references that used obsolete numbering to cite the propositions of an early draft of Book 1 of the \"Principia\". Newton's heirs shortly afterwards published the Latin version in their possession, also in 1728, under the (new) title \"De Mundi Systemate\", amended to update cross-references, citations and diagrams to those of the later editions of the \"Principia\", making it look superficially as if it had been written by Newton after the \"Principia\", rather than before. The \"System of the World\" was sufficiently popular to stimulate two revisions (with similar changes as in the Latin printing), a second edition (1731), and a 'corrected' reprint of the second edition (1740).\n\nThe text of the first of the three books of the \"Principia\" was presented to the Royal Society at the close of April 1686. Hooke made some priority claims (but failed to substantiate them), causing some delay. When Hooke's claim was made known to Newton, who hated disputes, Newton threatened to withdraw and suppress Book 3 altogether, but Halley, showing considerable diplomatic skills, tactfully persuaded Newton to withdraw his threat and let it go forward to publication. Samuel Pepys, as President, gave his imprimatur on 30 June 1686, licensing the book for publication. The Society had just spent its book budget on a \"History of Fishes\", and the cost of publication was borne by Edmund Halley (who was also then acting as publisher of the \"Philosophical Transactions of the Royal Society\"): the book appeared in summer 1687.\n\nNicolaus Copernicus had moved the Earth away from the center of the universe with the heliocentric theory for which he presented evidence in his book \"De revolutionibus orbium coelestium\" (\"On the revolutions of the heavenly spheres\") published in 1543. The structure was completed when Johannes Kepler wrote the book \"Astronomia nova\" (\"A new astronomy\") in 1609, setting out the evidence that planets move in elliptical orbits with the sun at one focus, and that planets do not move with constant speed along this orbit. Rather, their speed varies so that the line joining the centres of the sun and a planet sweeps out equal areas in equal times. To these two laws he added a third a decade later, in his book \"Harmonices Mundi\" (\"Harmonies of the world\"). This law sets out a proportionality between the third power of the characteristic distance of a planet from the sun and the square of the length of its year.\n\nThe foundation of modern dynamics was set out in Galileo's book \"Dialogo sopra i due massimi sistemi del mondo\" (\"Dialogue on the two main world systems\") where the notion of inertia was implicit and used. In addition, Galileo's experiments with inclined planes had yielded precise mathematical relations between elapsed time and acceleration, velocity or distance for uniform and uniformly accelerated motion of bodies.\n\nDescartes' book of 1644 \"Principia philosophiae\" (\"Principles of philosophy\") stated that bodies can act on each other only through contact: a principle that induced people, among them himself, to hypothesize a universal medium as the carrier of interactions such as light and gravity—the aether. Newton was criticized for apparently introducing forces that acted at distance without any medium. Not until the development of particle theory was Descartes' notion vindicated when it was possible to describe all interactions, like the strong, weak, and electromagnetic fundamental interactions, using mediating gauge bosons and gravity through hypothesized gravitons. Although he was mistaken in his treatment of circular motion, this effort was more fruitful in the short term when it led others to identify circular motion as a problem raised by the principle of inertia. Christiaan Huygens solved this problem in the 1650s and published it much later in 1673 in his book \"Horologium oscillatorium sive de motu pendulorum\".\n\nNewton had studied these books, or, in some cases, secondary sources based on them, and taken notes entitled \"Quaestiones quaedam philosophicae\" (\"Questions about philosophy\") during his days as an undergraduate. During this period (1664–1666) he created the basis of calculus, and performed the first experiments in the optics of colour. At this time, his proof that white light was a combination of primary colours (found via prismatics) replaced the prevailing theory of colours and received an overwhelmingly favourable response, and occasioned bitter disputes with Robert Hooke and others, which forced him to sharpen his ideas to the point where he already composed sections of his later book \"Opticks\" by the 1670s in response. Work on calculus is shown in various papers and letters, including two to Leibniz. He became a fellow of the Royal Society and the second Lucasian Professor of Mathematics (succeeding Isaac Barrow) at Trinity College, Cambridge.\n\nIn the 1660s Newton studied the motion of colliding bodies, and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called 'endeavour to recede' (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.\n\nHooke published his ideas about gravitation in the 1660s and again in 1674. He argued for an attracting principle of gravitation in \"Micrographia\" of 1665, in a 1666 Royal Society lecture \"On gravity\", and again in 1674, when he published his ideas about the \"System of the World\" in somewhat developed form, as an addition to \"An Attempt to Prove the Motion of the Earth from Observations\". Hooke clearly postulated mutual attractions between the Sun and planets, in a way that increased with nearness to the attracting body, along with a principle of linear inertia. Hooke's statements up to 1674 made no mention, however, that an inverse square law applies or might apply to these attractions. Hooke's gravitation was also not yet universal, though it approached universality more closely than previous hypotheses. Hooke also did not provide accompanying evidence or mathematical demonstration. On these two aspects, Hooke stated in 1674: \"Now what these several degrees [of gravitational attraction] are I have not yet experimentally verified\" (indicating that he did not yet know what law the gravitation might follow); and as to his whole proposal: \"This I only hint at present\", \"having my self many other things in hand which I would first compleat, and therefore cannot so well attend it\" (i.e., \"prosecuting this Inquiry\").\n\nIn November 1679, Hooke began an exchange of letters with Newton, of which the full text is now published. Hooke told Newton that Hooke had been appointed to manage the Royal Society's correspondence, and wished to hear from members about their researches, or their views about the researches of others; and as if to whet Newton's interest, he asked what Newton thought about various matters, giving a whole list, mentioning \"compounding the celestial motions of the planets of a direct motion by the tangent and an attractive motion towards the central body\", and \"my hypothesis of the lawes or causes of springinesse\", and then a new hypothesis from Paris about planetary motions (which Hooke described at length), and then efforts to carry out or improve national surveys, the difference of latitude between London and Cambridge, and other items. Newton's reply offered \"a fansy of my own\" about a terrestrial experiment (not a proposal about celestial motions) which might detect the Earth's motion, by the use of a body first suspended in air and then dropped to let it fall. The main point was to indicate how Newton thought the falling body could experimentally reveal the Earth's motion by its direction of deviation from the vertical, but he went on hypothetically to consider how its motion could continue if the solid Earth had not been in the way (on a spiral path to the centre). Hooke disagreed with Newton's idea of how the body would continue to move. A short further correspondence developed, and towards the end of it Hooke, writing on 6 January 1680 to Newton, communicated his \"supposition ... that the Attraction always is in a duplicate proportion to the Distance from the Center Reciprocall, and Consequently that the Velocity will be in a subduplicate proportion to the Attraction and Consequently as Kepler Supposes Reciprocall to the Distance.\" (Hooke's inference about the velocity was actually incorrect.)\n\nIn 1686, when the first book of Newton's \"Principia\" was presented to the Royal Society, Hooke claimed that Newton had obtained from him the \"notion\" of \"the rule of the decrease of Gravity, being reciprocally as the squares of the distances from the Center\". At the same time (according to Edmond Halley's contemporary report) Hooke agreed that \"the Demonstration of the Curves generated therby\" was wholly Newton's.\n\nA recent assessment about the early history of the inverse square law is that \"by the late 1660s,\" the assumption of an \"inverse proportion between gravity and the square of distance was rather common and had been advanced by a number of different people for different reasons\". Newton himself had shown in the 1660s that for planetary motion under a circular assumption, force in the radial direction had an inverse-square relation with distance from the center. Newton, faced in May 1686 with Hooke's claim on the inverse square law, denied that Hooke was to be credited as author of the idea, giving reasons including the citation of prior work by others before Hooke. Newton also firmly claimed that even if it had happened that he had first heard of the inverse square proportion from Hooke, which it had not, he would still have some rights to it in view of his mathematical developments and demonstrations, which enabled observations to be relied on as evidence of its accuracy, while Hooke, without mathematical demonstrations and evidence in favour of the supposition, could only guess (according to Newton) that it was approximately valid \"at great distances from the center\".\n\nThe background described above shows there was basis for Newton to deny deriving the inverse square law from Hooke. On the other hand, Newton did accept and acknowledge, in all editions of the \"Principia\", that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke and Halley in this connection in the Scholium to Proposition 4 in Book 1. Newton also acknowledged to Halley that his correspondence with Hooke in 1679–80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: \"yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ...\".) Newton's reawakening interest in astronomy received further stimulus by the appearance of a comet in the winter of 1680/1681, on which he corresponded with John Flamsteed.\n\nIn 1759, decades after the deaths of both Newton and Hooke, Alexis Clairaut, mathematical astronomer eminent in his own right in the field of gravitational studies, made his assessment after reviewing what Hooke had published on gravitation. \"One must not think that this idea ... of Hooke diminishes Newton's glory\", Clairaut wrote; \"The example of Hooke\" serves \"to show what a distance there is between a truth that is glimpsed and a truth that is demonstrated\".\n\nSince only between 250 and 400 copies were printed by the Royal Society, the first edition is very rare. Several rare-book collections contain first edition and other early copies of Newton's \"Principia Mathematica\", including:\n\n\nIn 2016, a first edition sold for $3.7 million.\nA facsimile edition (based on the 3rd edition of 1726 but with variant readings from earlier editions and important annotations) was published in 1972 by Alexandre Koyré and I. Bernard Cohen.\nTwo later editions were published by Newton:\n\nNewton had been urged to make a new edition of the \"Principia\" since the early 1690s, partly because copies of the first edition had already become very rare and expensive within a few years after 1687. Newton referred to his plans for a second edition in correspondence with Flamsteed in November 1694: Newton also maintained annotated copies of the first edition specially bound up with interleaves on which he could note his revisions; two of these copies still survive: but he had not completed the revisions by 1708, and of two would-be editors, Newton had almost severed connections with one, Nicolas Fatio de Duillier, and the other, David Gregory seems not to have met with Newton's approval and was also terminally ill, dying later in 1708. Nevertheless, reasons were accumulating not to put off the new edition any longer. Richard Bentley, master of Trinity College, persuaded Newton to allow him to undertake a second edition, and in June 1708 Bentley wrote to Newton with a specimen print of the first sheet, at the same time expressing the (unfulfilled) hope that Newton had made progress towards finishing the revisions. It seems that Bentley then realised that the editorship was technically too difficult for him, and with Newton's consent he appointed Roger Cotes, Plumian professor of astronomy at Trinity, to undertake the editorship for him as a kind of deputy (but Bentley still made the publishing arrangements and had the financial responsibility and profit). The correspondence of 1709–1713 shows Cotes reporting to two masters, Bentley and Newton, and managing (and often correcting) a large and important set of revisions to which Newton sometimes could not give his full attention. Under the weight of Cotes' efforts, but impeded by priority disputes between Newton and Leibniz, and by troubles at the Mint, Cotes was able to announce publication to Newton on 30 June 1713. Bentley sent Newton only six presentation copies; Cotes was unpaid; Newton omitted any acknowledgement to Cotes.\n\nAmong those who gave Newton corrections for the Second Edition were: Firmin Abauzit, Roger Cotes and David Gregory. However, Newton omitted acknowledgements to some because of the priority disputes. John Flamsteed, the Astronomer Royal, suffered this especially.\n\nThe Second Edition was the basis of the first edition to be printed abroad, which appeared in Amsterdam in 1714.\n\nThe third edition was published 25 March 1726, under the stewardship of \"Henry Pemberton, M.D., a man of the greatest skill in these matters...\"; Pemberton later said that this recognition was worth more to him than the two hundred guinea award from Newton.\n\nIn 1739–42, two French priests, Pères Thomas LeSeur and François Jacquier (of the Minim order, but sometimes erroneously identified as Jesuits), produced with the assistance of J.-L. Calandrini an extensively annotated version of the \"Principia\" in the 3rd edition of 1726. Sometimes this is referred to as the \"Jesuit edition\": it was much used, and reprinted more than once in Scotland during the 19th century.\n\nÉmilie du Châtelet also made a translation of Newton's Principia into French. Unlike LeSeur and Jacquier's edition, hers was a complete translation of Newton's three books and their prefaces. She also included a Commentary section where she fused the three books into a much clearer and easier to understand summary. She included an analytical section where she applied the new mathematics of calculus to Newton's most controversial theories. Previously, geometry was the standard mathematics used to analyse theories. Du Châtelet's translation is the only complete one to have been done in French and hers remains the standard French translation to this day.\n\nTwo full English translations of Newton's \"Principia\" have appeared, both based on Newton's 3rd edition of 1726.\n\nThe first, from 1729, by Andrew Motte, was described by Newton scholar I. Bernard Cohen (in 1968) as \"still of enormous value in conveying to us the sense of Newton's words in their own time, and it is generally faithful to the original: clear, and well written\". The 1729 version was the basis for several republications, often incorporating revisions, among them a widely used modernised English version of 1934, which appeared under the editorial name of Florian Cajori (though completed and published only some years after his death). Cohen pointed out ways in which the 18th-century terminology and punctuation of the 1729 translation might be confusing to modern readers, but he also made severe criticisms of the 1934 modernised English version, and showed that the revisions had been made without regard to the original, also demonstrating gross errors \"that provided the final impetus to our decision to produce a wholly new translation\".\n\nThe second full English translation, into modern English, is the work that resulted from this decision by collaborating translators I. Bernard Cohen, Anne Whitman, and Julia Budenz; it was published in 1999 with a guide by way of introduction.\n\nWilliam H. Donahue has published a translation of the work's central argument, published in 1996, along with expansion of included proofs and ample commentary. The book was developed as a textbook for classes at St. John's College and the aim of this translation is to be faithful to the Latin text.\n\nIn 2014, British astronaut Tim Peake named his upcoming mission to the International Space Station \"Principia\" after the book, in \"honour of Britain's greatest scientist\". Tim Peake's \"Principia\" launched on December 15, 2015 aboard Soyuz TMA-19M.\n\n\n\n\n\n\n\n\n"}
{"id": "2384021", "url": "https://en.wikipedia.org/wiki?curid=2384021", "title": "Potentiality and actuality", "text": "Potentiality and actuality\n\nIn philosophy, potentiality and actuality are a pair of closely connected principles which Aristotle used to analyze motion, causality, ethics, and physiology in his \"Physics\", \"Metaphysics\", \"Nicomachean Ethics\" and \"De Anima\", which is about the human psyche.\n\nThe concept of potentiality, in this context, generally refers to any \"possibility\" that a thing can be said to have. Aristotle did not consider all possibilities the same, and emphasized the importance of those that become real of their own accord when conditions are right and nothing stops them.\nActuality, in contrast to potentiality, is the motion, change or activity that represents an exercise or fulfillment of a possibility, when a possibility becomes real in the fullest sense.\n\nThese concepts, in modified forms, remained very important into the middle ages, influencing the development of medieval theology in several ways. Going further into modern times, while the understanding of nature, and according to some interpretations deity, implied by the dichotomy lost importance, the terminology has found new uses, developing indirectly from the old. This is most obvious in words like \"energy\" and \"dynamic\"--words first used in modern physics by the German scientist and philosopher, Gottfried Wilhelm Leibniz. Another example is the highly controversial biological concept of an \"entelechy\".\n\nPotentiality and potency are translations of the Ancient Greek word \"dunamis\" (δύναμις) as it is used by Aristotle as a concept contrasting with actuality. Its Latin translation is \"potentia\", root of the English word potential, and used by some scholars instead of the Greek or English variants.\n\n\"Dunamis\" is an ordinary Greek word for possibility or capability. Depending on context, it could be translated \"potency\", \"potential\", \"capacity\", \"ability\", \"power\", \"capability\", \"strength\", \"possibility\", \"force\" and is the root of modern English words \"dynamic\", \"dynamite\", and \"dynamo\". In early modern philosophy, English authors like Hobbes and Locke used the English word \"power\" as their translation of Latin \"potentia\".\n\nIn his philosophy, Aristotle distinguished two meanings of the word \"dunamis\". According to his understanding of nature there was both a weak sense of potential, meaning simply that something \"might chance to happen or not to happen\", and a stronger sense, to indicate how something could be done \"well\". For example, \"sometimes we say that those who can merely take a walk, or speak, without doing it as well as they intended, cannot speak or walk\". This stronger sense is mainly said of the potentials of living things, although it is also sometimes used for things like musical instruments.\n\nThroughout his works, Aristotle clearly distinguishes things that are stable or persistent, with their own strong natural tendency to a specific type of change, from things that appear to occur by chance. He treats these as having a different and more real existence. \"Natures which persist\" are said by him to be one of the causes of all things, while natures that do not persist, \"might often be slandered as not being at all by one who fixes his thinking sternly upon it as upon a criminal\". The potencies which persist in a particular material are one way of describing \"the nature itself\" of that material, an innate source of motion and rest within that material. In terms of Aristotle's theory of four causes, a material's non-accidental potential, is the material cause of the things that can come to be from that material, and one part of how we can understand the substance (\"ousia\", sometimes translated as \"thinghood\") of any separate thing. (As emphasized by Aristotle, this requires his distinction between accidental causes and natural causes.) According to Aristotle, when we refer to the nature of a thing, we are referring to the form, shape or look of a thing, which was already present as a potential, an innate tendency to change, in that material before it achieved that form, but things show what they are more fully, as a real thing, when they are \"fully at work\".\n\nActuality is often used to translate both \"energeia\" (ενέργεια) and \"entelecheia\" (ἐντελέχεια) (sometimes rendered in English as \"entelechy\"). \"Actuality\" comes from Latin \"\" and is a traditional translation, but its normal meaning in Latin is \"anything which is currently happening\".\n\nThe two words \"energeia\" and \"entelecheia\" were coined by Aristotle, and he stated that their meanings were intended to converge. In practice, most commentators and translators consider the two words to be interchangeable. They both refer to something being in its own type of action or at work, as all things are when they are real in the fullest sense, and not just potentially real. For example, \"to be a rock is to strain to be at the center of the universe, and thus to be in motion unless constrained otherwise\".\n\n\"Energeia\" is a word based upon ἔργον (\"ergon\"), meaning \"work\". It is the source of the modern word \"energy\" but the term has evolved so much over the course of the history of science that reference to the modern term is not very helpful in understanding the original as used by Aristotle. It is difficult to translate his use of \"energeia\" into English with consistency. Joe Sachs renders it with the phrase \"being–at–work\" and says that \"we might construct the word is-at-work-ness from Anglo-Saxon roots to translate \"energeia\" into English\". Aristotle says the word can be made clear by looking at examples rather than trying to find a definition.\n\nTwo examples of \"energeiai\" in Aristotle's works are pleasure and happiness (eudaimonia). Pleasure is an \"energeia\" of the human body and mind whereas happiness is more simply the \"energeia\" of a human being a human.\n\n\"Kinesis\", translated as movement, motion, or in some contexts change, is also explained by Aristotle as a particular type of \"energeia\". See below.\n\nEntelechy, in Greek \"entelécheia\", was coined by Aristotle and transliterated in Latin as \"\". According to :\n\nAristotle invents the word by combining \"entelēs\" (ἐντελής, \"complete, full-grown\") with \"echein\" (<nowiki>=</nowiki> \"hexis\", to be a certain way by the continuing effort of holding on in that condition), while at the same time punning on \"endelecheia\" (ἐνδελέχεια, \"persistence\") by inserting \"telos\" (τέλος, \"completion\"). This is a three-ring circus of a word, at the heart of everything in Aristotle's thinking, including the definition of motion.\n\nSachs therefore proposed a complex neologism of his own, \"being-at-work-staying-the-same\". Another translation in recent years is \"being-at-an-end\" (which Sachs has also used).\n\n\"Entelecheia\", as can be seen by its derivation, is a kind of completeness, whereas \"the end and completion of any genuine being is its being-at-work\" (\"energeia\"). The \"entelecheia\" is a continuous being-at-work (\"energeia\") when something is doing its complete \"work\". For this reason, the meanings of the two words converge, and they both depend upon the idea that every thing's \"thinghood\" is a kind of work, or in other words a specific way of being in motion. All things that exist now, and not just potentially, are beings-at-work, and all of them have a tendency towards being-at-work in a particular way that would be their proper and \"complete\" way.\n\nSachs explains the convergence of \"energeia\" and \"entelecheia\" as follows, and uses the word actuality to describe the overlap between them:\n\nJust as \"energeia\" extends to \"entelecheia\" because it is the activity which makes a thing what it is, \"entelecheia\" extends to \"energeia\" because it is the end or perfection which has being only in, through, and during activity.\n\nAristotle discusses motion (\"kinēsis\") in his \"Physics\" quite differently from modern science. Aristotle's definition of motion is closely connected to his actuality-potentiality distinction. Taken literally, Aristotle defines motion as the actuality (\"entelecheia\") of a \"potentiality as such\". What Aristotle meant however is the subject of several different interpretations. A major difficulty comes from the fact that the terms actuality and potentiality, linked in this definition, are normally understood within Aristotle as opposed to each other. On the other hand, the \"as such\" is important and is explained at length by Aristotle, giving examples of \"potentiality as such\". For example, the motion of building is the \"energeia\" of the \"dunamis\" of the building materials \"as building materials\" as opposed to anything else they might become, and this potential in the unbuilt materials is referred to by Aristotle as \"the buildable\". So the motion of building is the actualization of \"the buildable\" and not the actualization of a house as such, nor the actualization of any other possibility which the building materials might have had.\n\nIn an influential 1969 paper Aryeh Kosman divided up previous attempts to explain Aristotle's definition into two types, criticised them, and then gave his own third interpretation. While this has not become a consensus, it has been described as having become \"orthodox\". This and similar more recent publications are the basis of the following summary.\n\n and associate this approach with W.D. Ross. points out that it was also the interpretation of Averroes and Maimonides.\n\nThis interpretation is, to use the words of Ross that \"it is the passage to actuality that is \"kinesis\"” as opposed to any potentiality being an actuality.\n\nThe argument of Ross for this interpretation requires him to assert that Aristotle actually used his own word \"entelecheia\" wrongly, or inconsistently, only within his definition, making it mean \"actualization\", which is in conflict with Aristotle's normal use of words. According to this explanation also can not account for the \"as such\" in Aristotle's definition.\n\n associates this interpretation with St Thomas of Aquinas and explains that by this explanation \"the apparent contradiction between potentiality and actuality in Aristotle’s definition of motion\" is resolved \"by arguing that in every motion actuality and potentiality are mixed or blended\". Motion is therefore \"the actuality of any potentiality insofar as it is still a potentiality\". Or in other words:\n\nThe Thomistic blend of actuality and potentiality has the characteristic that, to the extent that it is actual it is not potential and to the extent that it is potential it is not actual; the hotter the water is, the less is it potentially hot, and the cooler it is, the less is it actually, the more potentially, hot.\nAs with the first interpretation however, objects that:\n\nOne implication of this interpretation is that whatever happens to be the case right now is an \"entelechia\", as though something that is intrinsically unstable as the instantaneous position of an arrow in flight deserved to be described by the word that everywhere else Aristotle reserves for complex organized states that persist, that hold out against internal and external causes that try to destroy them.\nIn a more recent paper on this subject, Kosman associates the view of Aquinas with those of his own critics, David Charles, Jonathan Beere, and Robert Heineman.\n\n, amongst other authors (such as Aryeh Kosman and Ursula Coope), proposes that the solution to problems interpreting Aristotle's definition must be found in the distinction Aristotle makes between two different types of potentiality, with only one of those corresponding to the \"potentiality as such\" appearing in the definition of motion. He writes:\nThe man with sight, but with his eyes closed, differs from the blind man, although neither is seeing. The first man has the capacity to see, which the second man lacks. There are then potentialities as well as actualities in the world. But when the first man opens his eyes, has he lost the capacity to see? Obviously not; while he is seeing, his capacity to see is no longer merely a potentiality, but is a potentiality which has been put to work. The potentiality to see exists sometimes as active or at-work, and sometimes as inactive or latent.\n\nComing to motion, Sachs gives the example of a man walking across the room and says that...\n\n, in his commentary of Aristotle's \"Physics\" book III gives the following results from his understanding of Aristotle's definition of motion:\nThe genus of which motion is a species is being-at-work-staying-itself (\"entelecheia\"), of which the only other species is thinghood. The being-at-work-staying-itself of a potency (\"dunamis\"), as material, is thinghood. The being-at-work-staying-the-same of a potency as a potency is motion.\n\nThe actuality-potentiality distinction in Aristotle is a key element linked to everything in his physics and metaphysics.\n\nAristotle describes potentiality and actuality, or potency and action, as one of several distinctions between things that exist or do not exist. In a sense, a thing that exists potentially does not exist, but the potential does exist. And this type of distinction is expressed for several different types of being within Aristotle's categories of being. For example, from Aristotle's \"Metaphysics\", 1017a:\n\nWithin the works of Aristotle the terms \"energeia\" and \"entelecheia\", often translated as actuality, differ from what is merely actual because they specifically presuppose that all things have a proper kind of activity or work which, if achieved, would be their proper end. Greek for end in this sense is telos, a component word in \"entelecheia\" (a work that is the proper end of a thing) and also teleology. This is an aspect of Aristotle's theory of four causes and specifically of formal cause (\"eidos\", which Aristotle says is \"energeia\") and final cause (\"telos\").\n\nIn essence this means that Aristotle did not see things as matter in motion only, but also proposed that all things have their own aims or ends. In other words, for Aristotle (unlike modern science) there is a distinction between things with a natural cause in the strongest sense, and things that truly happen by accident. He also distinguishes non-rational from rational potentialities (e.g. the capacity to heat and the capacity to play the flute, respectively), pointing out that the latter require desire or deliberate choice for their actualization. Because of this style of reasoning, Aristotle is often referred to as having a teleology, and sometimes as having a theory of forms.\n\nWhile actuality is linked by Aristotle to his concept of a formal cause, potentiality (or potency) on the other hand, is linked by Aristotle to his concepts of hylomorphic matter and material cause. Aristotle wrote for example that \"matter exists potentially, because it may attain to the form; but when it exists actually, it is then in the form\".\n\nThe active intellect was a concept Aristotle described that requires an understanding of the actuality-potentiality dichotomy. Aristotle described this in his \"De Anima\" (book 3, ch. 5, 430a10-25) and covered similar ground in his \"Metaphysics\" (book 12, ch.7-10). The following is from the \"De Anima\", translated by Joe Sachs, with some parenthetic notes about the Greek. The passage tries to explain \"how the human intellect passes from its original state, in which it does not think, to a subsequent state, in which it does.\" He inferred that the energeia/dunamis distinction must also exist in the soul itself:-\n...since in nature one thing is the material [\"hulē\"] for each kind [\"genos\"] (this is what is in potency all the particular things of that kind) but it is something else that is the causal and productive thing by which all of them are formed, as is the case with an art in relation to its material, it is necessary in the soul [\"psuchē\"] too that these distinct aspects be present;\nthe one sort is intellect [\"nous\"] by becoming all things, the other sort by forming all things, in the way an active condition [\"hexis\"] like light too makes the colors that are in potency be at work as colors [\"to phōs poiei ta dunamei onta chrōmata energeiai chrōmata\"].\nThis sort of intellect is separate, as well as being without attributes and unmixed, since it is by its thinghood a being-at-work, for what acts is always distinguished in stature above what is acted upon, as a governing source is above the material it works on.\nKnowledge [\"epistēmē\"], in its being-at-work, is the same as the thing it knows, and while knowledge in potency comes first in time in any one knower, in the whole of things it does not take precedence even in time.\nThis does not mean that at one time it thinks but at another time it does not think, but when separated it is just exactly what it is, and this alone is deathless and everlasting (though we have no memory, because this sort of intellect is not acted upon, while the sort that is acted upon is destructible), and without this nothing thinks.\n\nThis has been referred to as one of \"the most intensely studied sentences in the history of philosophy\". In the \"Metaphysics\", Aristotle wrote at more length on a similar subject and is often understood to have equated the active intellect with being the \"unmoved mover\" and God. Nevertheless, as Davidson remarks:\nJust what Aristotle meant by potential intellect and active intellect - terms not even explicit in the \"De anima\" and at best implied - and just how he understood the interaction between them remains moot to this day. Students of the history of philosophy continue to debate Aristotle's intent, particularly the question whether he considered the active intellect to be an aspect of the human soul or an entity existing independently of man.\n\nAlready in Aristotle's own works, the concept of a distinction between \"energeia\" and \"dunamis\" was used in many ways, for example to describe the way striking metaphors work, or human happiness. Polybius about 150 BC, in his work the \"Histories\" uses Aristotle's word \"energeia\" in both an Aristotelian way and also to describe the \"clarity and vividness\" of things. Diodorus Siculus in 60-30 BC used the term in a very similar way to Polybius. However Diodorus uses the term to denote qualities unique to individuals. Using the term in ways that could translated as \"vigor\" or \"energy\" (in a more modern sense); for society, \"practice\" or \"custom\"; for a thing, \"operation\" or \"working\"; like vigor in action.\n\nPlotinus was a late classical pagan philosopher and theologian whose monotheistic re-workings of Plato and Aristotle were influential amongst early Christian theologians. In his \"Enneads\" he sought to reconcile ideas of Aristotle and Plato together with a form of monotheism, that used three fundamental metaphysical principles, which were conceived of in terms consistent with Aristotle's energeia/dunamis dichotomy, and one interpretation of his concept of the Active Intellect (discussed above):-\nThis was based largely upon Plotinus' reading of Plato, but also incorporated many Aristotelian concepts, including the Unmoved Mover as \"energeia\".\n\nIn Eastern Orthodox Christianity, St Gregory Palamas wrote about the \"energies\" (actualities; singular \"energeia\" in Greek, or \"actus\" in Latin) of God in contrast to God's \"essence\" (\"ousia\"). These are two distinct types of existence, with God's energy being the type of existence which people can perceive, while the essence of God is outside of normal existence or non-existence or human understanding, in that it is not caused or created by anything else.\n\nPalamas gave this explanation as part of his defense of the Eastern Orthodox ascetic practice of hesychasm. Palamism became a standard part of Orthodox dogma after 1351.\n\nIn contrast, the position of Western Medieval (or Catholic) Christianity, can be found for example in the philosophy of Thomas Aquinas, who relied on Aristotle's concept of entelechy, when he defined God as \"actus purus\", pure act, actuality unmixed with potentiality. The existence of a truly distinct essence of God which is not actuality, is not generally accepted in Catholic Theology.\n\nThe notion of possibility was greatly analyzed by medieval and modern philosophers. Aristotle's logical work in this area is considered by some to be an anticipation of modal logic and its treatment of potentiality and time. Indeed, many philosophical interpretations of possibility are related to a famous passage on Aristotle's \"On Interpretation\", concerning the truth of the statement: \"There will be a sea battle tomorrow\".\n\nContemporary philosophy regards possibility, as studied by modal metaphysics, to be an aspect of modal logic. Modal logic as a named subject owes much to the writings of the Scholastics, in particular William of Ockham and John Duns Scotus, who reasoned informally in a modal manner, mainly to analyze statements about essence and accident.\n\nAristotle's metaphysics, his account of nature and causality, was for the most part rejected by the early modern philosophers. Francis Bacon in his \"Novum Organon\" in one explanation of the case for rejecting the concept of a formal cause or \"nature\" for each type of thing, argued for example that philosophers must still look for formal causes but only in the sense of \"simple natures\" such as colour, and weight, which exist in many gradations and modes in very different types of individual bodies. In the works of Thomas Hobbes then, the traditional Aristotelian terms, \"potentia et actus\", are discussed, but he equates them simply to \"cause and effect\".\n\nThere was an adaptation of at least one aspect of Aristotle's potentiality and actuality distinction, which has become part of modern physics, although as per Bacon's approach it is a generalized form of energy, not one connected to specific forms for specific things. The definition of energy in modern physics as the product of mass and the square of velocity, was derived by Leibniz, as a correction of Descartes, based upon Galileo's investigation of falling bodies. He preferred to refer to it as an \"entelecheia\" or \"living force\" (Latin \"vis viva\"), but what he defined is today called \"kinetic energy\", and was seen by Leibniz as a modification of Aristotle's \"energeia\", and his concept of the potential for movement which is in things. Instead of each type of physical thing having its own specific tendency to a way of moving or changing, as in Aristotle, Leibniz said that instead, force, power, or motion itself could be transferred between things of different types, in such a way that there is a general conservation of this energy. In other words, Leibniz's modern version of entelechy or energy obeys its own laws of nature, whereas different types of things do not have their own separate laws of nature. Leibniz wrote: ...the entelechy of Aristotle, which has made so much noise, is nothing else but force or activity ; that is, a state from which action naturally flows if nothing hinders it. But matter, primary and pure, taken without the souls or lives which are united to it, is purely passive ; properly speaking also it is not a substance, but something incomplete.\n\nLeibniz's study of the \"entelechy\" now known as energy was a part of what he called his new science of \"dynamics\", based on the Greek word \"dunamis\" and his understanding that he was making a modern version of Aristotle's old dichotomy. He also referred to it as the \"new science of power and action\", (Latin \"\"potentia et effectu\" and \"potentia et actione\"\"). And it is from him that the modern distinction between statics and dynamics in physics stems. The emphasis on \"dunamis\" in the name of this new science comes from the importance of his discovery of potential energy which is not active, but which conserves energy nevertheless. \"As 'a science of power and action', dynamics arises when Leibniz proposes an adequate architectonic of laws for constrained, as well as unconstrained, motions.\"\n\nFor Leibniz, like Aristotle, this law of nature concerning entelechies was also understood as a metaphysical law, important not only for physics, but also for understanding life and the soul. A soul, or spirit, according to Leibniz, can be understood as a type of entelechy (or living monad) which has distinct perceptions and memory.\n\nAs discussed above, terms derived from \"dunamis\" and \"energeia\" have become parts of modern scientific vocabulary with a very different meaning from Aristotle's. The original meanings are not used by modern philosophers unless they are commenting on classical or medieval philosophy. In contrast, \"entelecheia\", in the form of \"entelechy\" is a word used much less in technical senses in recent times.\n\nAs mentioned above, the concept had occupied a central position in the metaphysics of Leibniz, and is closely related to his monad in the sense that each sentient entity contains its own entire universe within it. But Leibniz' use of this concept influenced more than just the development of the vocabulary of modern physics. Leibniz was also one of the main inspirations for the important movement in philosophy known as German Idealism, and within this movement and schools influenced by it entelechy may denote a force propelling one to self-fulfillment.\n\nIn the biological vitalism of Hans Driesch, living things develop by \"entelechy\", a common purposive and organising field. Leading vitalists like Driesch argued that many of the basic problems of biology cannot be solved by a philosophy in which the organism is simply considered a machine. Vitalism and its concepts like entelechy have since been discarded as without value for scientific practice by the overwhelming majority of professional biologists. \n\nHowever, in philosophy aspects and applications of the concept of entelechy have been explored by scientifically interested philosophers and philosophically inclined scientists alike. One example was the American critic and philosopher Kenneth Burke (1897–1993) whose concept of the \"terministic screens\" illustrates his thought on the subject. Most prominent was perhaps the German quantum physicist Werner Heisenberg. He looked to the notions of potentiality and actuality in order to better understand the relationship of quantum theory to the world.\n\n\n\n"}
{"id": "57418849", "url": "https://en.wikipedia.org/wiki?curid=57418849", "title": "Probe tip", "text": "Probe tip\n\nA probe tip in scanning microscopy literally is a very sharp piece of metal or non-metal, like a sewing needle with a point at one end with nano or sub-nanometer order of dimension. It can interact with up to one molecule or atom of a given surface of a sample that can reveal authentic properties of the surface such as morphology, topography, mapping and electrical properties of a single atom or molecule on the surface of the sample.\n\nThe proliferation of the thrust in probe-based tools began after the invention of Scanning tunneling microscope (STM) and Atomic force microscopy (AFM) (collectively called Scanning probe microscopy-SPM) by Gerd Binnig and Heinrich Rohrer at the IBM Zurich research laboratory in 1982. It opened a new era for probing nanoscale world of individual atoms and molecules as well as surface science due to their unprecedented capability of characterizing a wide range of unique properties such as mechanical, chemical, magnetic and optical functionalities of various samples at nanometer-scale resolution in vacuum, ambient and fluid environment. The utilization of sharp probe tips enabled us to see the inside the microscopic world from within the macroscopic world. The increasing demand for sub-nanometer probe tips attributes to their robustness and versatile applicability because of their direct application to the numerous fields of science that includes nanolithography, nanoelectronics, biosensor, Electrochemistry, Semiconductor, Micromachining and biological cells studies. The significant number of applications for the topographic surface characterization of the materials and biological specimen in various fields of science made researchers and scientists that it is imperative for reproducible mass production of probe tip with sharp apex.\n\nProbe tip size and shape in microscopy are important parameters providing direct connection between resolution and imaging quality. Resolution and imaging mechanism may depend on geometry (length, width, shape, aspect ratio, and tip apex radius) and composition (material properties) of tip and surface being probed. Tip size, shape and reproducibility are extremely important to monitor and detect the interaction between the surfaces.\n\nHere, we describe fabrication, characterization and application of sharp tips. A wide range of tip fabrication techniques including cutting, grinding, pulling, beam deposition, ion milling, controlled crashing, field emission, field evaporation, fracture and electrochemical etching/polishing are discussed. Both limitations and advantages are also provided for various tip fabrication method. We also describe history and development, working principles, characterization and applications with recent advancement of sharp tips.\n\nThe discovery of a sharp probe tip has always been of significant interest among the researchers considering its importance in the material, life and biological sciences for mapping the surface structure and material properties at molecular or atomic dimension. The history of tip can be tracked long back in nineteenth century during the invention of Phonautograph in 1859. Phonautograph is the predecessor of modern gramophone. It was invented by Scott and Koenig. It consisted of a parchment diaphragm with an attached stylus (sort of a pen-holder), along with a Hog’s hair, which was used to trace a wavy line on a lamp-blacked surface [Musical mirror by Bryceson, Richard, Jan. 1929-Dec.1930]. In the later development gramophone came out where along with other replacements, the Hog’s hair was replaced by a needle to reproduce sound [Musical mirror by Bryceson, Richard, Jan. 1929-Dec.1930]. In 1940, a pantograph was built utilizing a shielded probe and adjustable tip. A stylus was free to slide vertically to be in contact with the paper. In 1948, a tip was employed in probe circuit to measure peak voltage. The fabrication of electrochemically etched sharp tungsten, copper, nickel and molybdenum tip was reported by Muller in 1937. The revolution for sharp tips occurred in producing a good and various kind of tip with different shape, size, aspect ratio composed of tungsten wire, silicon, diamond, Carbon nanotube etc. occurred with Si-based circuit technologies. This allowed tips for numerous applications in the broad spectrum of nanotechnological fields. Following STM invention came Atomic force microscopy (AFM) by Gerd Binnig, Calvin F. Quate, and Christoph Gerber in 1986. In their instrument they used diamond broken piece as tip sticking it to a hand-cut gold foil cantilever. Focused ion and electron beam technique for the fabrication of strong, stable, reproducible SiN pyramidal tip with 1.0 μm length and 0.1 μm diameter was reported by Russell in 1992. Ground breaking advancement came through the introduction of microfabrication methods for the fabrication of precised conical or pyramidal silicon and silicon nitride tip. Later, numerous research experiments were explored to fabricate comparatively less expensive and robust tungsten tip production and characterization technique to attain less than 50 nm radius of curvature.\n\nThe new horizon in the field of fabrication of probe tip was revealed to the experts when carbon nanotube which is basically about 1 nm cylindrical shells of graphene was introduced. The use of single wall Carbon nanotubes is less vulnerable to breaking or crushing during imaging due to their flexibility. Probe tip made up of Carbon nanotubes can be efficiently used to get high-resolution images of both soft and weakly absorbed Biomolecules like DNA on surface at molecular resolution.\n\nMultifunctional hydrogel nano-probe technique uncovered new scope initiating a completely new concept to the fabrication of tip and their extended ease of applicability to inorganic and biological samples in both air and liquid. The biggest advantage of this mechanical method is that the tip can be made in different shape e.g. hemispherical, embedded spherical, pyramidal, and distorted pyramidal etc. with diameter ranging from 10 nm – 1000 nm for applications including Topography or functional imaging, force spectroscopy on soft matter, biological, chemical and physical Sensors. Table 1. Summarizes various fabrication, material and application of tips.\n\nThe tip itself does not have any working principle for imaging, but depending on the instrumentation, mode of application, and the nature of the sample under investigation, the probe tip may follow different principle to image the surface of the sample. For example, when a tip is integrated with STM, it measures the tunneling current that arises from the interaction between the sample and the tip. In AFM, short-ranged force deflection during the raster scan by tip across the surface is measured. A conductive tip is essential for the STM instrumentation whereas AFM can use conductive and non-conductive prob tip. Although probe tip is used in various techniques with different principle, for STM and AFM coupled with probe tip is discussed in detail.\n\nSomewhat, the name implies STM utilizes tunneling charge transfer principle from tip to surface or vice versa thereby recording the current response. This concept originates from particle in a box concept, that is, if potential energy for a particle is small, electron may be found outside of potential well which is a classically forbidden region. This phenomenon is called tunneling.\n\nExpression derived from Schrodinger equation for transmission charge transfer probability is as follow:\n\nformula_1\n\nwhere\n\nNon-conductive nano-scale tips are widely used for AFM measurements. For non-conducting tip, surface forces acting on the tip/cantilever, are responsible for deflection or attraction of tip. These attractive or repulsive forces are used for surface topology, chemical specifications, magnetic and electronic properties. The distance dependent forces between substrate surface and tip are responsible for imaging in AFM. These interactions include are van der Waals forces, capillary forces, electrostatic forces, Casimir forces and solvation forces. One unique repulsion force is Pauli Exclusion repulsive force is responsible for single atom imaging as in references and Figures 10 & 11 (contact region in Fig. 1).\nTip fabrication techniques fall generally in any two classifications: mechanical and physicochemical. In the early stage of development of probe tips, mechanical procedures were popular because of ease of fabrication. One of mechanical method for the fabrication of hydrogel tip will be discussed in detail.\n\nA few reported mechanical methods to fabricate tips include cutting grinding and pulling. For example, cutting a wire at certain angles with razor blade or wire cutter or scissor. Another mechanical method for tip preparation is fragmentation of bulk pieces into small pointy pieces. Grinding a metal wire/rod into a sharp tip was also a method used. These mechanical procedures usually leave rugged surface with many tiny asperities protruding from the apex which lead to atomic resolution on flat surface. However, irregular shape and large macroscopic radius of curvature result in poor reproducibility and decreased stability especially for probing rough surfaces. Another main disadvantage of making probe by this method is it yields many mini tips which lead to many different signals yielding error in imaging. Cutting, grinding and pulling procedures can only be adapted for metallic tips like W, Ag, Pt, Ir, Pt-Ir and gold. Non-metallic tips cannot be fabricated by these methods.\n\nOn other hand a sophisticated mechanical method for tip fabrication is based on hydro-gel method. This method is based on bottom-up strategy to make probe tips by molecular self-assembly process. First, cantilever is formed in mold by curing pre-polymer solution then it is brought into contact with mold of tip which contains pre-polymer solution. The polymer is cured with ultraviolet light which helps firmly attachment of cantilever with probe. This fabrication method is shown in Fig. 2.\n\nPhysiochemical procedures are fabrication methods of choice these days which yield extremely sharp and symmetric tips with more reproducibility compared to mechanical fabrication-based tips. Among physicochemical method, the electrochemical etching method is one of the most popular method. Etching is two or more steps procedure. The “zone electropolishing” is the second step which further sharpens the tip in a very controlled manner. Other physicochemical methods include chemical vapor deposition, electron beam deposition onto pre-existing tips. Other tip fabrication methods include field ion microscopy and ion milling. In field ion microscopy techniques, consecutive field evaporation of single atoms yields specific atomic configuration at probe tip which yields very high resolution.\n\nElectrochemical etching is one of the easiest, inexpensive, most practical, reliable and most widely accepted metallic probe tip fabrication method with desired quality and reproducibility. Three commonly used electrochemical etching for tungsten tip fabrication are: single lamella drop-off methods, double lamella drop-off method and submerged method. Various cone shape tips can be fabricated by this method by minor changes in the experimental setup. An DC potential between tip and metallic electrode (usually W wire) immersed in solution (Figure 3 a-c). Electrochemical reactions at cathode and anode in basic solutions (2M KOH or 2M NaOH) are usually used. The overall etching process involved is written here:\n\nAnode;\n\n<chem>W (s) + 8OH- -> WO4 + 4H2O + 6e- (E= 1.05V)</chem>\n\nCathode:\n\n<chem>6H2O + 6e- -> 3H2 + 6OH- (E=-2.48V)</chem>\n\nOverall:\n\n<chem>W (s) + 2OH- -> WO4^2- + 2H2O (l) + 6e- + 3H2 (g) (E= -1.43V)</chem>\n\nHere, all the potentials are reported vs. SHE.\n\nSchematics of fabrication method of probe tip through electrochemical etching method is shown in Fig. 3.\n\nExperimental setup for Electrochemical etching is shown in Fig. 3a.\n\nDifferent tips fabricated from etching method with different tip radius and angle is illustrated in Fig. 3(b-e).\n\nIn electrochemical etching process, W is etched at liquid, solid and air interface (due to surface tension), as shown in Fig. 3. Etching is called static if W wire is kept stationary. Once the tip is etched, lower part falls due to the lower tensile strength than the weight of lower part of wire. The irregular shape is produced by the shifting of meniscus. However, slow etching rate can produce regular tips which controlling current slowly through electrochemical cell. The dynamic etching involves slowly pulling up the wire from solution or sometimes it is moved up and down (oscillating wire) producing smooth tips.\n\nIn this method a metal wire is vertically etched reducing the diameter from 0.25 mm ~ 20 nm. Schematic diagram for probe tip fabrication with submerged electrochemical etching method is illustrated in Fig 4. These tips can be used for high quality STM images.\nIn double lamella method the lower part of metal is etched away, and upper part of tip was not etched further. Further etching of the upper part of wire is prevented by covering it with a polymer coating. This method is usually limited to laboratory fabrication. Double lamella method schematic is shown in Fig. 5.\nTransitions metals like Cu, Au and Ag adsorbs single molecules linearly on their surface due to weak van der Waals forces. This linear projection of single molecule allows interaction of terminal atom of tip with atom of substrate resulting in Pauli repulsion for single molecule or atom mapping studies. Gases deposition on tip is carried out in an ultrahigh vacuum (5 x 10 mbar) chamber at a low temperature (10K). Deposition of Xe, Kr, NO, CH or CO on tip have been successfully prepared and used for imaging studies. However, these tips preparation rely on attachment of single atom or molecule on tip and resulting atomic structure of tip is not known exactly. Probability of attachment of simple molecule on metals surface is very tedious and required great skills. Therefore, this method is not ubiquitous and not many laws are able to perform these experiments.\n\nSharp tips used in SPM are fragile and prone to damage and wear and tear easily under high working load. Diamond is considered the best option to address this issue. Diamond tips for SPM application are fabricated by fracture of bulk diamond, grinding and polishing diamond. But, these methods result in considerable loss of diamond. Other strategy to prevent this loss is coating of Silicone tips with thin diamond film. These thin films are usually deposited by CVD. In CVD, diamond is deposited directly on silicon or W cantilever. A schematic diagram for chemical vapor deposition set up is shown in Fig. 6. In this method, flow of methane and hydrogen gas is maintained in such a way that pressure inside chamber is maintained at 40Torr. CH and H are dissociated at elevated temperature of 2100 °C with the help of Ta filament. Nucleation sites are created on the tip of cantilever. Once CVD is complete, CH flow is stopped and chamber is cooled under flow of H. Schematics of CVD set up for diamond tip fabrication for AFM application is in Fig. 6.\n\nIn RIE method, first a grove or structure is made on a substrate followed by deposition of a desired material in that template. Once tip is formed, templating structure is etched off leaving tip and cantilever. A schematic for diamond tip fabrication on silicon wafers through this method has been described in Fig. 7\n\nFocused ion beam milling milling is a sharpening method for probe tips in SPM. In this method, first a blunt tip is fabricated by other methods, for example, pyramid mold can be used to fabricate pyramidal tip, CVD method or any other etching method. Then, this tip is sharpened by FIB milling as shown in Fig. 8. The focused ion beam diameter is controlled through a programmable aperture which directly correlates with tip diameter. \nThis method is used to attach carbon nanotubes on cantilever or blunt tip. A strong adhesive (such as soft acrylic glue) is used to bind CNT with silicon cantilever. CNT are robust, stiff and increase durability of probe tip and can be used for both contact and tapping mode.\n\nElectrochemically etched tips are usually covered with contaminant on surface which cannot be removed simply by rinsing in water, acetone or ethanol. Some oxides layers on metallic tips, especially on tungsten, need to be removed by post fabrication treatment.\n\nTo clean W sharp tips, it is highly desirable to remove contaminant and oxide layer. In this method a tip is heated in UHV chamber at elevated temperature which desorb contaminated layer. Reaction detail is shown below.\n\n2WO + W → 3WO ↑\n\nWO → W (sublimation at formula_51075K)\n\nAs at elevated temperature, trioxides of W are converted to WO which sublimates around 1075K and cleaned metallic W surface left behind. Additional advantage provided by annealing is healing of crystallographic defects produced by fabrication and it also smoothens the tip surface.\n\nIn HF cleaning method, freshly prepared tip is dipped in 15% concentrated hydrofluoric acid which dissolves oxides of W for 10~30 s.\n\nIn this method, argon ions are directed to the tip surface to remove contaminant layer by sputtering. Tip is either rotated in a flux of argon ions at certain angle in a way that this beam hits on the apex. The bombardment of ions on the tip depletes the contaminants and also results in reduction of radius of tip. Bombardment time needs to be finely tuned with respect to shape of tip. Sometimes, a short annealing is required after ion milling.\n\nThis method is very similar to ion milling but, in this procedure, UHV chamber is filled with neon at a pressure of 10 mbar. When negative voltage is applied on the tip, strong electric field (produced by tip under negative potential) will ionize neon gas and these positively charged ions are accelerated back to tip and causing sputtering at the tip. The sputtering removes contaminants and some atoms from tip which, like ion milling reduces apex radius. By, changing the field strength, one can tune radius of tip to 20 nm.\n\nThe surface of the Silicon based tips cannot be easily controlled because they usually carry silanol group. Si surface is hydrophilic and can be contaminated easily by environmental. Another disadvantage of Si tips is wear and tear of tip. It is important to coat Si tip to prevent tip deteriorations. The tip coating may also enhance image quality. Following types of coatings are employed for Si tips. First an adhesive layer is pasted (usually chromium layer on 5 nm thick titanium) and then gold is deposited by vapor deposition (40-100 nm or less). Sometimes, the coating layering reduces tunneling current detection capability of probe tips.\n\nThe most important aspect of a probe tip is imaging the surfaces efficiently at nanometer. Some concerns involving credibility of the imaging or measurement of sample arises when the shape of tip is not determined accurately. For example, when an unknown tip is used to measure linewidth pattern or other high aspect ratio feature of a surface. There may remain some confusion for the determination of the contribution of tip and the sample in the acquired image. Consequently, it is important to fully and accurately characterize the tips. Probe tips can be characterized for their shape, size, sharpness, bluntness, aspect ratio, radius of curvature, geometry and composition using many advanced instrumental techniques. For example, electron field emission measurement, scanning electron microscopy (SEM), transmission electron microscopy (TEM), scanning tunneling spectroscopy as well as more easily accessible optical microscope. In some cases, optical microscopy cannot provide exact measurements for small tips in nanoscale due to resolution limitation of the optical microscopy.\n\nIn electron field emission current measurement method, a high voltage is applied between tip and another electrode followed by measuring field emission current employing Fowler-Nordheim curves formula_6. Large fields-emission current measurements may indicate that the tip is sharp and low field-emission current indicates that the tip is blunt, molten or mechanically damaged. A minimum voltage is essential to facilitate the release of electrons from the surface of tip which in turn indirectly is used to obtain the tip curvature. The downside of this method to the several advantages is that the high electric field required for producing strong electric force that can melt the apex of the tip or might change crystallographic tip nature.\n\nThe size and shape of the tip can be obtained by scanning electron microscopy and transmission electron microscopy measurements. In addition, TEM images are helpful to detect any layer of insulating materials on the surface of the tip as well as to estimate the size of the layer. These oxides are formed gradually on the surface of tip the right after fabrication due to the oxidation of metallic tip by reacting with the O present in the surrounding atmosphere. SEM has a resolution limitation of below 4 nm, TEM may be needed to observe even a single atom theoretically and practically. Tip grain down to 1-3 nm or thin polycrystalline oxides or carbon or graphite layers at the tip apex are routinely measured using TEM. The orientation of tip crystal i.e. the angle between the tip plane in the single-crystal and the tip normal can be estimated.\n\nIn the past, optical microscope has been only used to investigate if the tip is bent microscale imaging at many microscales. This is because the resolution limitation of an optical microscope is about 200 nm. Imaging software including ImageJ allows determination of the curvature, and aspect ratio of the tip. One drawback of this method is that it renders an image of tip which is a object due to the uncertainty in the nanoscale dimension. This problem can be resolved taking images of tip multiple times followed by putting them together into image by confocal microscope with some fluorescent material coating on the tip. Also, it is a time-consuming process considering the necessity of monitoring the wear or damage or degradation of tip by the collision with the surface during scanning the surface after each scan.\n\nThe scanning tunneling spectroscopy (STS) is spectroscopic form of STM in which spectroscopic data based on curve is obtained to analyze the existence of any oxides or impurities on the tip by monitoring the linearity of the curve which represents metallic tunnel junction. Generally, cure is non-linear and hence, the tip has a gap like shape around zero bias voltage for oxidized or impure tip whereas the opposite is observed for sharp pure un-oxidized tip.\n\nIn Auger electron spectroscopy (AES), any oxides present on the tip surface is sputtered out during in-depth analysis with argon ion beam generated by differentially pumped ion pump followed by comparing the sputtering rate of the oxide with experimental sputtering yields. These Auger measurements may estimate the nature of oxides because of the surface contamination and/or composition can be revealed and in some cases thickness of the oxide layer down to 1-3 nm can be estimated. X-ray photoelectron spectroscopy also performs similar characterization for the chemical and surface composition by providing information on the binding energy of the surface elements.\n\nOverall, the aforementioned characterization methods of tips can be categorized in three major classes. They are:\n\n\nProbes tips have a wide variety of applications in different fields of science and technology. One of the major areas where probe tips are used is for application in SPM i.e., STM and AFM. For example, carbon nanotube tips in conjunction with AFM provides an excellent tool for surface characterization in the nanometer realm. CNT tips are also used in tapping-mode Scanning Force Microscopy (SFM), which is a technique where a tip taps a surface by a cantilever driven near resonant frequency of the cantilever. The CNT probe tips fabricated using CVD technique can be used for imaging of biological macromolecules, semiconductor and chemical structure. For example, it is possible to obtain intermittent AFM contact image of IgM macromolecules with excellent resolution using single CNT tip. Individual CNT tips can be used for high resolution imaging of protein molecules.\n\nIn another work, multiwall carbon nanotube (MWCNT) and Single wall carbon nanotube (SWCNT) tips were used to image amyloid β (1-40) derived protofibrils and fibrils by tapping mode AFM. Functionalized probes can be used in Chemical Force Microscopy (CFM) to measure intermolecular forces and map chemical functionality. Functionalized SWCNT probes can be used for chemically sensitive imaging with high lateral resolution and to study binding energy in chemical and biological system. Probe tips that have been functionalized with either hydrophobic or hydrophilic molecules can be used to measure the adhesive interaction between hydrophobic-hydrophobic, hydrophobic-hydrophilic, and hydrophilic-hydrophilic molecules. From these adhesive interactions the friction image of patterned sample surface can be found. Probe tips used in force microscopy can provide imaging of structure and dynamics of adsorbate at the nanometer scale. Self-Assembled Functionalized Organic Thiols onto the surface of Au coated SiN probe tips has been used to study the interaction between molecular groups. Again, carbon nanotube probe tips in conjunction with AFM can be used for probing crevices that occur in microelectronic circuits with improved lateral resolution. Functionality modified probe tips has been to measure the binding force between single protein-ligand pairs. Probe tips has been in used in tapping mode technique to provide information about the elastic properties of materials. Probe tips are also used in mass spectrometer. Enzymatically active probe tips have been used for the enzymatic degradation of analyte. They have also been used as devices to introduce sample into the mass spectrophotometer. For example, trypsin-activated gold (Au/trypsin) probe tips can be used for the peptide mapping of the hen egg lysozyme.\n\nAtomically sharp probe tips can be used for imaging a single atom in molecule. An example of visualizing single atoms in water cluster can be seen in Fig. 10. By visualizing single atoms in molecules present on a surface, the scientists can determine bond length, bond order and discrepancies, if any, in conjugation which was supposed to be impossible by experimental work. Fig. 9 shows experimentally determined bond order in poly aromatic compound which was thought to be very hard in past.\n"}
{"id": "6895400", "url": "https://en.wikipedia.org/wiki?curid=6895400", "title": "Raw data", "text": "Raw data\n\nRaw data, also known as primary data, is data (e.g., numbers, instrument readings, figures, etc.) collected from a source. If a scientist sets up a computerized thermometer which records the temperature of a chemical mixture in a test tube every minute, the list of temperature readings for every minute, as printed out on a spreadsheet or viewed on a computer screen is \"raw data\". Raw data has not been subjected to processing, \"cleaning\" by researchers to remove outliers, obvious instrument reading errors or data entry errors, or any analysis (e.g., determining central tendency aspects such as the average or median result). As well, raw data has not been subject to any other manipulation by a software program or a human researcher, analyst or technician. It is also referred to as \"primary\" data. Raw data is a relative term (see data), because even once raw data has been \"cleaned\" and processed by one team of researchers, another team may consider this processed data to be \"raw data\" for another stage of research. Raw data can be inputted to a computer program or used in manual procedures such as analyzing statistics from a survey. The term \"raw data\" can refer to the binary data on electronic storage devices, such as hard disk drives (also referred to as \"low-level data\").\n\nData has two ways of being created or generated. The first is what is called 'captured data', and is found through purposeful investigation or analysis. The second is called 'exhaust data', and is gathered usually by machines or terminals as a secondary function. For example, cash registers, smartphones, and speedometers serve a main function but may collect data as a secondary task. Exhaustive data is usually too large or of little use to process and becomes 'transient' or thrown away.\n\nIn computing, raw data may have the following attributes: it may possibly contain human, machine, or instrument errors, it may not be validated; it might be in different areen (colloquial) formats; uncoded or unformatted; or some entries might be \"suspect\" (e.g., outliers), requiring confirmation or citation. For example, a data input sheet might contain dates as raw data in many forms: \"31st January 1999\", \"31/01/1999\", \"31/1/99\", \"31 Jan\", or \"today\". Once captured, this raw data may be processed stored as a normalized format, perhaps a Julian date, to make it easier for computers and humans to interpret during later processing. Raw data (sometimes colloquially called \"sourcey\" data or \"eggy\" data, the latter a reference to the data being \"uncooked\", that is, \"unprocessed\", like a raw egg) are the data input to processing. A distinction is made between \"data\" and \"information\", to the effect that information is the \"end\" product of \"data\" processing. Raw data that has undergone processing are sometimes referred to as \"cooked\" data in a colloquial sense. Although raw data has the potential to be transformed into \"information,\" extraction, organization, analysis and formatting for presentation are required before raw data can be transformed into usable information.\n\nFor example, a point-of-sale terminal (POS terminal, a computerized cash register) in a busy supermarket collects huge volumes of raw data each day about customers' purchases. However, this list of grocery items and their prices and the time and date of purchase does not yield much information until it is processed. Once processed and analyzed by a software program or even by a researcher using a pen and paper and a calculator, this raw data may indicate the particular items that each customer buys, when they buy them, and at what price; as well, an analyst or manager could calculate the average total sales per customer or the average expenditure per day of the week by hour. This processed and analyzed data provides information for the manager, that the manager could then use to help her determine, for example, how many cashiers to hire and at what times. Such \"information\" could then become \"data\" for further processing, for example as part of a predictive marketing campaign. As a result of processing, raw data sometimes ends up being put in a database, which enables the raw data to become accessible for further processing and analysis in any number of different ways.\n\nTim Berners-Lee (inventor of the World Wide Web) argues that sharing raw data is important for society. Inspired by a post by Rufus Pollock of the Open Knowledge Foundation his call to action is \"Raw Data Now\", meaning that everyone should demand that governments and businesses share the data they collect as raw data. He points out that \"data drives a huge amount of what happens in our lives… because somebody takes the data and does something with it.\" To Berners-Lee, it is essentially from this sharing of raw data, that advances in science will emerge. Advocates of open data argue that once citizens and civil society organizations have access to data from businesses and governments, it will enable citizens and NGOs to do their \"own\" analysis of the data, which can empower people and civil society. For example, a government may claim that its policies are reducing the unemployment rate, but a poverty advocacy group may be able to have its staff econometricians do their own analysis of the raw data, which may lead this group to draw different conclusions about the data set.\n\n"}
{"id": "34056766", "url": "https://en.wikipedia.org/wiki?curid=34056766", "title": "Researchsome", "text": "Researchsome\n\nResearchsome belongs to the -omics set of words, which comprises genomics, proteomics, metabolomics and related words dealing with scientific research, mostly in the biological sciences and related disciplines, like bioinformatics.\n\nDevised partly as a joke, the term was coined by Ivan Erill, from UMBC, and first used in an oral communication at the 15th Evolutionary Biology Meeting at Marseilles (France) in 2011. As opposed to other -omic words, which originate as portmanteau from incorrect application of either the -some suffix (derived from the Greek σῶμα (soma, body) as in chromosome or the -nome suffix (from the Greek νόμος (nomos, \"custom\" or \"law\") as in economics), researchsome is among the few newly minted words in the biosciences that derives properly from soma and should be read as \"body of research\". Obviously, the word has a double entendre interpretation as \"some research\".\n\nAs an entity, the researchsome is meant to be a graphical representation of the body of research conducted by a particular individual or organization. It depicts research fields and their interconnections, much in the same way that graphical renditions of interactomes outline molecules and their interactions. In its original application, the researchsome was designed to convey a rapid overview of a researcher's areas of expertise onto which the relevant fields for a given lecture or talk could be highlighted.\n\n\n"}
{"id": "6663803", "url": "https://en.wikipedia.org/wiki?curid=6663803", "title": "Scientific priority", "text": "Scientific priority\n\nIn science, priority is the credit given to the individual or group of individuals who first made the discovery or propose the theory. Fame and honours usually go to the first person or group to publish a new finding, even if several researchers arrived at the same conclusion independently and at the same time. Thus between two or more independent discoverers, the first to make formal publication is the legitimate winner. Hence, the tradition is often referred to as the priority rule, the procedure of which is nicely summed up in a phrase \"publish or perish\", because there are no second prizes. In a way, the race to be first inspires risk-taking that can lead to scientific breakthroughs which is beneficial to the society (such as discovery of malaria transmission, DNA, HIV, etc.); on the other hand, it can create an unhealthy competition, thus, becoming detrimental to scientific progress.\n\nPriority becomes a difficult issue usually in the context of priority disputes, where the priority for a given theory, understanding, or discovery comes into question. In most cases historians of science disdain retrospective priority disputes as enterprises which generally lack understanding about the nature of scientific change and usually involve gross misreadings of the past to support the idea of a long-lost priority claim. Historian and biologist Stephen Jay Gould once remarked that \"debates about the priority of ideas are usually among the most misdirected in the history of science.\"\n\nRichard Feynman told Freeman Dyson that he avoided priority disputes by \"Always giv[ing] the bastards more credit than they deserve.\" Dyson remarked that he also follows this rule, and that this practice is \"remarkably effective for avoiding quarrels and making friends.\" \n\nThe priority rule came into existence before or as soon as modern scientific methods were established. For example, the earliest documented controversy was a bitter claim between Isaac Newton and Gottfried Wilhelm Leibniz in the 17th century about priority in the invention of calculus. This particular incidence clearly shows human biases and prejudice. It has become unanimously accepted that both the mathematicians independently developed calculus. Since then priority has caused a number of historical maladies in the history of science.\nIn the cases of scientists who have since achieved incredible levels of popularity, such as Charles Darwin and Albert Einstein, priority questions are often rooted in taking too seriously the myth of the \"lone genius\" which is often cultivated around such quasi-mythic figures (see Great Man theory and Whig history). In an attempt to laud such scientists as visionaries, the context in which they worked is often neglected by popularizers, making it appear as if they worked without assistance or without reference to other work, something which is rarely the case.\n\n\n"}
{"id": "5645049", "url": "https://en.wikipedia.org/wiki?curid=5645049", "title": "Self-experimentation in medicine", "text": "Self-experimentation in medicine\n\nSelf-experimentation refers to scientific experimentation in which the experimenter conducts the experiment on her- or himself. Often this means that the designer, operator, subject, analyst, and user or reporter of the experiment are all the same. Self-experimentation has a long and well-documented history in medicine which continues to the present. Some of these experiments have been very valuable and shed new and often unexpected insights into different areas of medicine.\n\nThere are many motivations for self-experiment. These include the wish to get results quickly and avoid the need for a formal organisational structure, to take the ethical stance of taking the same risk as volunteers, or just a desire to do good for humanity. Other ethical issues include whether a researcher should self-experiment because another volunteer would not get the same benefit as the researcher will get, and the question of whether informed consent of a volunteer can truly be given by those outside a research program.\n\nA number of distinguished scientists have indulged in self-experimentation, including at least five Nobel laureates; in several cases, the prize was awarded for findings the self-experimentation made possible. Many experiments were dangerous; various people exposed themselves to pathogenic, toxic or radioactive materials. Some self-experimenters, like Jesse Lazear and Daniel Alcides Carrión, died in the course of their research. Notable examples of self-researchers occur in many fields; infectious disease (Jesse Lazear: yellow fever, Max von Pettenkofer: cholera), vaccines (Daniel Zagury: AIDS), cancer (Nicholas Senn, Jean-Louis-Marc Alibert), blood (Karl Landsteiner, William J. Harrington), and drugs (Albert Hofmann, and too many others to list). Research has not been limited to disease and drugs. John Stapp tested the limits of human deceleration, Humphry Davy breathed nitrous oxide, and Nicholas Senn pumped hydrogen into his gastrointestinal tract to test the utility of the method for diagnosing perforations.\nThere is no formal definition of what constitutes self-experimentation. A strict definition might limit it to cases where there is a single-subject experiment and the experimenter performs the procedure on himself. A looser definition might include cases where the experimenters put themselves amongst the volunteers for the experiment. According to S. C. Gandevia of the University of New South Wales, who was looking at the question from the perspective of ethics, it is only self-experiment if the would-be self-experimenter would be named as an author on any subsequent published paper. That is, the person who would receive the academic credit for the experiment must also be the subject of it.\n\nThere are many reasons experimenters decide to self-test, but amongst the most fundamental is the ethical principle that the experimenter should not subject the participants in the experiment to any procedure they would not be willing to undertake themselves. This idea was first codified in the Nuremberg Code in 1947, which was a result of the trials of Nazi doctors at the Nuremberg trials accused of murdering and torturing victims in valueless experiments. Several of these doctors were hanged. Point five of the Nuremberg Code requires that no experiment should be conducted that is dangerous to the subjects unless the experimenters themselves also take part. The Nuremberg Code has influenced medical experiment codes of practice around the world, as has the exposure of experiments that have since failed to follow it such as the notorious Tuskegee syphilis experiment.\n\nCritics of self-experimenters point to other less savoury motivations such as simple self-aggrandisement. Some scientists have resorted to self-experiment to avoid the \"red tape\" of seeking permission from the relevant ethics committee of their institution. Werner Forssmann was so determined to proceed with his self-experiment that he continued with it even after permission had been denied. He was twice dismissed for this activity, but the importance of his work was eventually recognised in a Nobel Prize. Some researchers, apparently, even believe that self-experimentation is not permitted. However, this is not true, at least in the United States where the same rules apply regardless of who the subject of the experiment is.\n\nSelf-experimentation is also criticised for the risk of over-enthusiastic researchers, eager to prove a point, not accurately noting the results. Against this it is argued by those supporting self-experiment that medically trained persons are in a better position to understand and record symptoms, and self-experiment is usually at the very early stage of a program before volunteers have been recruited.\n\nA wish to commit suicide is sometimes offered as a reason for self-experimentation. However, Lawrence K. Altman, author of \"Who Goes First?: The Story of Self-experimentation in Medicine\", while acknowledging that this may sometimes occur, after extensive research could find only one verified case of attempted suicide by self-experimentation. This was Nobel Prize winner Élie Metchnikoff, who, in 1881, suffering from depression, injected himself with relapsing fever. This was his second suicide attempt, but according to his wife, Olga, he chose this method of death so that it would be of benefit to medicine. However, Metchnikoff survived and in 1892 also self-experimented with cholera, but this is not thought to have been a suicide attempt.\n\nPerhaps the noblest motivation is the simple altruistic desire to do something of benefit to humanity regardless of the risks. There most certainly are risks, as Jesse Lazear found to his cost when he died of yellow fever after deliberately infecting himself. Max von Pettenkofer, after ingesting cholera bacteria said:\n\nAccording to Ian Kerridge, professor of bioethics at the University of Sydney, the most common reason for undertaking self-experimentation is not so much anything noble, but rather \"an insatiable scientific curiosity and a need to participate closely in their own research\".\n\nAs already mentioned, it is an ethical principle that the researcher should not inflict on volunteers anything that the researcher would not be willing to do to him- or herself, but the researcher is not always a suitable, or even possible, subject for the experiment. For instance, the researcher may be the wrong gender if the research is into hormone treatment for women, or may be too old, or too young. The ethical question for the researchers is would they agree to the experiment if they were in the same position as the volunteers?\n\nAnother issue that can lead researchers not to take part is whether the researcher would stand to gain any benefit from taking part in the experiment. It is an ethical principle that volunteers must stand to gain some benefit from the research, even if that is only a remote future possibility of treatment being found for a disease that they only have a small chance of contracting. Tests on experimental drugs are sometimes conducted on sufferers of an untreatable condition. If the researcher does not have that condition then there can be no possible benefit to them personally. For instance, Ronald C. Desrosiers in responding to why he did not test an AIDS vaccine he was developing on himself said that he was not at risk of AIDS so could not possibly benefit. Against that, the early stages of testing a new drug are usually focused merely on the safety of the substance, rather than any benefits it may have. Healthy individuals are required for this stage, not volunteers suffering from the target condition, so if the researcher is healthy, he or she is a potential candidate for testing. An issue peculiar to AIDS vaccine research is that the test will leave HIV antibodies in the volunteers blood, causing the person to show HIV positive when tested even if they have never been in contact with an HIV carrier. This could cause a number of social problems for the volunteers (including any self-testers) such as issues with life insurance.\n\nThe ethics of informed consent is relevant to self-experimentation. Informed consent is the principle that the volunteers in the experiment should fully understand the procedure that is going to take place, be aware of all the risks involved, and give their consent to taking part in the experiment beforehand. The principle of informed consent was first enacted in the U.S. Army's research into Yellow fever in Cuba in 1901. However, there was no general or official guidance at this time. That remained the case until the yellow fever program was referenced in the drafting of the Nuremberg Code. This was further developed in the Declaration of Helsinki in 1964 by the World Medical Association which has since become the foundation for ethics committees' guidelines.\n\nSome researchers believe that experimental research is too complex for the general public ever to be able to give proper informed consent. One such researcher is Eugene G. Laforet, who believes that the researchers taking part in the experiment themselves is more valuable to the volunteers than a legal consent form. Another is 1977 Nobel Prize winner Rosalyn S. Yalow who said \"In our laboratory we always used ourselves because we are the only ones who can give truly informed consent.\" On the other side of the coin, there is the possibility that members of a research team may be coerced into participating by peer pressure.\n\nThe question of who should be first to try the procedure in a new experiment is an ethical one. However, according to Altman it is not a question that can successfully be legislated. A law requiring self-test would force researchers to take risks that may sometimes be inappropriate. A code forbidding it might inhibit valuable discoveries.\n\nSelf-experimentation has a role in medical education. Although no longer encouraged, in former times it was perfectly standard to expect medical students to try for themselves the drugs they were going to be prescribing. Charles-Édouard Brown-Séquard, whose own self-experiments led him to the concept of what are now called hormones, was a nineteenth century proponent of the practice:\n\nSelf-experimentation has value in rapidly obtaining the first results. In some cases, such as with Forssmann's experiments done in defiance of official permission, results may be obtained that would never otherwise have come to light. However, self-experiment lacks the statistical validity of a larger experiment. It is not possible to generalise from an experiment on a single person. For instance, a single successful blood transfusion does not indicate, as we now know from the work of Karl Landsteiner, that all such transfusions between any two random people will also be successful. Likewise, a single failure does not absolutely prove that a procedure is worthless. Psychological issues such as confirmation bias and the placebo effect are unavoidable in a single-person self-experiment where it is not possible to put scientific controls in place.\n\nSuch concerns do not apply so much if the self-experimenter is just one of many volunteers (as long as the self-experimenter is not also responsible for recording the results) but his or her presence still has value. As noted above, this can reassure the other participants. It also acts as a check on the experimenter when considering whether the experiment is ethical or dangerous.\n\nLidocaine, the first amino amide–type local anaesthetic, was first synthesized under the name \"xylocaine\" by Swedish chemist Nils Löfgren in 1943. His colleague Bengt Lundqvist performed the first injection anaesthesia experiments on himself.\n\nRoger Altounyan developed the use of sodium cromoglycate as a remedy for asthma, based on khella, a traditional Middle Eastern remedy, with experiments on himself.\n\nDr. Karl Landsteiner's discovery of the ABO blood group system in 1900 was based on an analysis of blood samples from six members of his laboratory staff, including himself.\n\nIn the Harrington–Hollingsworth experiment in 1950, William J. Harrington performed an exchange blood transfusion between himself and a thrombocytopenic patient, discovering the immune basis of idiopathic thrombocytopenic purpura and providing evidence for the existence of autoimmunity.\n\nIn 1901, Nicholas Senn investigated whether cancer was contagious. He surgically inserted under his skin a piece of cancerous lymph node from a patient with cancer of the lip. After two weeks, the transplant started to fade and Senn concluded that cancer is not contagious.\n\nMuch earlier, in 1808, Jean-Louis-Marc Alibert injected himself with a discharge from breast cancer. The site of injection became inflamed, but did not develop cancer.\n\nGerhard Domagk, in 1949, injected himself with sterilised extract of human cancer in an attempt to prove that immunisation against cancer was possible.\n\nDaniel Zagury, in 1986, was the first to test his proposed AIDS vaccine.\n\nDaniel Alcides Carrión, in 1885, infected himself from the pus in the purple wart (verruga peruana) of a female patient. Carrión developed an acute form of bartonellosis now known as Carrion's disease or Oroya fever. This is a rare disease found only in Peru and certain other parts of South America. He kept detailed notes of his condition and succeeded in showing through this self-experiment that the chronic and acute forms were the same disease. He died from the disease after several weeks. A student who had assisted Carrion in carrying out this work was arrested and charged with murder, but later released.\n\nMax von Pettenkofer, in October 1892, drank bouillon deliberately infected with a large dose of cholera bacteria. Pettenkofer was attempting to disprove the theory of Robert Koch that the disease was caused by the bacteria \"Vibrio cholerae\" alone. Pettenkofer also took bicarbonate of soda to counter a claim by Koch that stomach acid killed the bacteria. Pettenkofer escaped with mild symptoms and claimed success, but the modern view is that he did indeed have cholera, luckily just a mild case, and possibly had some immunity from a previous episode.\n\nS.O. Levinson with H.J. Shaugnessy – and others between 1942 and 1947 – injected themselves with a vaccine against dysentry. The vaccine had previously been tested on mice, which had all died within minutes, and the effect on humans was completely unknown. The experimenters survived but suffered strong side effects.\n\nIn 1984 a Western Australian scientist, Dr Barry Marshall, discovered the link between \"Helicobacter pylori\" (at that time known as \"Campylobacter pylori\") and gastritis. This was based on a series of self-experiments that involved gastroscopy and biopsy, ingestion of \"H. pylori\", regastroscopy and biopsy and subsequent treatment with tinidazole. His only option was self-experimentation: ethical measures forbade him from administering \"H. pylori\" to any other person. In 2005, Marshall and his long-time collaborator Robin Warren were awarded Nobel Prize in Physiology or Medicine, \"for their discovery of the bacterium \"Helicobacter pylori\" and its role in gastritis and peptic ulcer disease\".\n\nMarshall's experiment debunked the long-held belief of the medical profession that stress was the cause of gastritis. This cleared the way for the development of antibiotic treatments for gastritis and peptic ulcers and a new line of research into the likely role of \"H. pylori\" in stomach cancer.\n\nMarshall's investigation was preceded by David A. Robinson who, in 1980, ingested \"Campylobacter jejuni,\" a bacterium found in cow's milk, to investigate whether gastritis could be caused by drinking milk infected with \"C. jejuni.\" Robinson became sick as a result. Robinson needed to do a human experiment because the alternative, testing on cows, was not viable as infected cows frequently do not become ill.\n\nGail Monroe Dack (1901–1976), a former president of the American Society for Microbiology, gave himself food poisoning by eating cake tainted with \"Staphylococcus\".\n\nConstantin Levaditi (1874–1953) injected himself with spirochaete from rabbits suffering from syphilis but did not contract the disease himself.\n\nIn Cuba, U.S. Army doctors from Walter Reed's research team infected themselves with yellow fever including James Carroll, Aristides Agramonte, and, most notably, Jesse Lazear, who died from yellow fever complications in 1900. These efforts ultimately resulted in proof of the mosquito-borne nature of yellow fever transmission and saved countless lives. Stubbins Ffirth had investigated the contagious nature of the disease at the end of the 18th century.\n\nThere was an unsuccessful campaign to award a Nobel Prize to Reed's team. Lazear, in any event, could not be awarded the prize because it is never given posthumously. However, a Nobel Prize was awarded to a later yellow fever researcher and self-experimenter, Max Theiler who, in 1951, developed the first yellow fever vaccine and was the first to try it.\n\nAnatolii Al'bertovich Shatkin, in 1961, injected trachoma virus into the conjunctival sac of his eye and rapidly developed trachoma. He did not begin treatment of the condition for 26 days.\n\nIn July 1944, physician researcher Claude Barlow ingested over 200 schistosome worms to carry back to the United States from Egypt to study whether domestic snails could become infected and introduce the disease into the United States. Attempts to send infected snails, the intermediate host, by mail had been unsuccessful. He refused treatment, despite being desperately ill by December, so as not to lose the eggs for further study. He finally passed 4,630 eggs in his semen and 200 eggs in his urine. The U.S. government decided not to use the eggs, so his self-sacrifice was to no avail. It was November 1945 before he finally cleared all the parasites, after treatment with tartar emetic.\n\nWilliam Bosworth Castle, in 1926, ate minced raw beef every morning, regurgitated it an hour later, and then fed it to his patients suffering from pernicious anaemia. Castle was testing his theory that there was an intrinsic factor produced in a normal stomach that hugely increased the uptake of the extrinsic factor (now identified as vitamin B), lack of which leads to pernicious anaemia. Beef is a good source of B, but patients did not respond with beef alone. Castle reasoned they lacked production of intrinsic factor and he could provide it from his own stomach. While Castle was not the recipient of this treatment, his story is included in \"Who Goes First?: The Story of Self-experimentation in Medicine\" and is considered a self-experimenter by the author.\n\nElliott Cutler (1888–1947) took sufficient thyroid extract to give himself hyperthyroidism and enable him to study the effect of the condition on kidney function.\n\nIn 1936, Edwin Katskee took a very large dose of cocaine. He attempted to write notes on his office wall, but these became increasingly illegible as the experiment proceeded. Katskee was found dead the next morning.\n\nIn 1945, during the German occupation of Denmark, Erik Jacobsen and Jens Hald at the Danish drug company Medicinalco (which had a group of enthusiastic self-experimenters that called itself the \"Death Batallion\") were exploring the possible use of disulfiram to treat intestinal parasites, and in the course of testing it on themselves, accidentally discovered its effects when alcohol is ingested, which led several years later to the drug called Antabuse.\n\nChauncey D. Leake, in 1930, took furan as a possible substitute for aspirin but it just gave him a splitting headache and painful urination that lasted three days.\n\nDavid G. Bailey, in 1989, was researching the effects of drinking alcohol while taking the then experimental drug felodipine. It was usual in this kind of research to mix the alcohol with orange juice but Bailey did not like the taste of this drink so used grapefruit juice instead. Bailey found that there was three times more felodipine in his, and fellow researchers', blood than had been reported by other scientists using orange juice. It was later found that grapefruit juice suppresses an enzyme responsible for breaking down a large number of different drugs.\n\nAs part of the team who developed ibuprofen in the 1960s, Stewart Adams initially tested it on a hangover.\n\nFriedrich Sertürner isolated morphine from opium in 1804. Morphine was the first-ever alkaloid isolated from any plant. Sertürner wanted to prove his findings to his colleague with a public experiment on himself and three other friends.\n\nJacques-Joseph Moreau published his study \"Du Hachisch et de l'aliénation mentale\" in 1845. He self-experimented with hashish and observed its varying effects on other people. Moreau insisted that researchers should self-experiment to gain understanding of the altered states of consciousness produced by psychoactive substances.\n\nPsychopharmacologist Arthur Heffter isolated mescaline from the peyote cactus in 1897 and conducted experiments on its effects by comparing the effects of peyote and mescaline on himself.\n\nAlbert Hofmann discovered the psychedelic properties of LSD in 1943 by accidentally absorbing it and later intentionally ingesting it to verify that the effects were caused by LSD. He was also the first to isolate psilocybin from psilocybin mushrooms and self-experimented with it to prove it to be the active principle of psilocybin mushroom's psychoactive effects.\n\nPsychopharmacologist Alexander Shulgin synthesized and experimented with a wide array of new phenethylamine and tryptamine drugs, discovering a range of previously unknown psychoactive drug effects.\n\nTimothy Leary took LSD and was a well-known proponent of the social use of the drug in the 1960s.\n\nAround 1886, Nicholas Senn pumped nearly six litres of hydrogen through his anus. Senn used a rubber balloon holding four US gallons connected to a rubber tube inserted in the anus. An assistant sealed the tube by squeezing the anus against it. The hydrogen was inserted by squeezing the balloon while monitoring the pressure on a manometer. Senn had previously carried out this experiment on dogs to the point of rupturing the intestine. Senn was a pioneer of using this technique to determine if the bullet in gunshot wounds had penetrated the intestinal tract. In experiments on gunshot wounds to dogs, Senn verified that the gas escaping from the wound was hydrogen by setting light to it.\n\nReports that Senn used helium in this experiment are almost certainly erroneous. Helium was first detected on Earth in 1882, but not isolated until 1895, and extractable reserves not found until 1903.\n\nHumphry Davy self-experimented with breathing of several different gases, most notably nitrous oxide.\n\nSelf-experimentation with gene therapies have been reported. Every gene therapy has a unique risk of harm, including the risk associated with the gene delivery method (i.e., the particular viral vector or form of transfection) that is used and the risk associated with a specific genetic modification. Examples of potential risks for some gene therapies include tissue damage and an immune response to foreign DNA, among many others.\n\nThomas Lewis and Jonas Kellgren studied pain in the 1930s. To do this, they injected hypertonic saline into various parts of their own bodies.\n\nIn the early 1900s Nicolae Minovici, a professor of forensic science in Bucharest, undertook a series of experiments into hanging. At first he put the noose around his neck while lying down and had an assistant put tension on the rope. He then moved on to full suspension by the neck. Finally, he attempted suspension with a slipping hangman's knot, but the pain was too great for him to continue. He could not swallow for a month. Minovici was determined to surpass a record set by Dr. Fleichmann of Erlangen, who in 1832, self-asphyxiated for two minutes. However, Minovici could not get close to this and disbelieved Fleichmann.\n\nMinovici and Fleichmann are not the only ones to self-experiment with strangulation. Graeme Hammond, a doctor in New York, tried it in 1882. Francis Bacon described an even earlier occasion in 1623 when the self-experimenter stepped off a stool with a rope around his neck, but was unable to regain his footing on the stool without assistance.\n\nJohn Paul Stapp, in 1954, sat in a rocket sled fired along rails in a series of steadily more violent tests. Speeds reached 631 mph, almost the speed of sound. This is a speed record for a manned rail vehicle that still stands today. At the end of the track the sled hit a trough of water that brought it to a rapid stop in around 1.4 seconds. In the most severe test, Stapp underwent an acceleration of 20 g as the rocket engine accelerated the vehicle up to speed and 46 g of deceleration (also a record) as the vehicle was brought to a stop. Stapp suffered numerous injuries in these tests (previous animal tests had shown that limbs could be broken merely by being pulled into the air stream), and several concussions. In the last test his eyes were bloodied as blood vessels burst in his eyes.\n\nThese tests were carried out for the US Air Force to determine the forces that pilots could be subjected to and to enable better restraining straps to be designed.\n\nSantorio Santorio spent a large portion of 30 years living on a platform meticulously measuring his daily weight combined with that of his intake and excretion in an effort to test Galen's theory that respiration occurs through the skin as \"perspiratio insensibilis\" (insensible perspiration). The result was the 1614 publication \"De Statica Medicina\" (\"On Medical Measurements\").\n\nAllan Blair of the University of Alabama, in 1933, deliberately caused a black widow spider to bite him. At the time there was some doubt that the reported symptoms of some victims were the result of a spider bite or some other cause. Blair's experiment was intended to settle the matter. Blair became seriously ill and was hospitalised for several days in great pain, but survived.\n\nJoseph Barcroft, in 1917, tested hydrogen cyanide on himself as part of research into poison gas in World War I. He was shut in a chamber with a dog and exposed to the gas. Barcroft continued with the experiment even after the dog went into tetanic convulsions and appeared to die. The experiment was continued for less than two minutes. The next morning the dog was found to be alive and apparently fully recovered. It is not known why dogs are more susceptible to the gas than humans.\n\n\nTim Friede created his own vaccine against snakebite using pure venom injections from all four species of mambas, and four cobra species to achieve high immunity. He also survived IgE shock six times with mamba injections. Others have also injected venom to create immunity to snake venom: Bill Haast, Harold Mierkey, Ray Hunter, Joel La Rocque, Herschel Flowers, Martin Crimmins, and Charles Tanner.\n\nIn 1921, Maurice Crowther Hall ingested carbon tetrachloride to test its safety with a view to its possible use as a treatment for hookworm. Hall reported mild side effects. Carbon tetrachloride has since been found to cause acute liver failure. In 1925, Hall ingested tetrachloroethylene (once the most common dry cleaning fluid) for the same purpose.\n\nGary Earl Leinbach, in 1972, swallowed radioactive iodine and a knife in a tube for a biopsy. Leinbach was investigating a new way of diagnosing steatorrhea.\n\nKenneth Gordon Scott, in 1949, inhaled aerosols of plutonium and uranium.\n\nKlaus Hansen drank heavy water.\n\nClinical application of cardiac catheterization began with Werner Forssmann in the 1930s, who inserted a catheter into the brachial vein of his own forearm, guided it fluoroscopically into his right atrium, and took an X-ray picture of it. Forssmann did this procedure without permission. He obtained the assistance of a nurse by deceiving her that she was to be the subject of the experiment. He tied down her arms while inserting the catheter into his own arm, only releasing her at the point it was too late to change, and he needed her assistance. Forssmann was twice fired for carrying out these self-experiments, but shared the Nobel Prize in Physiology or Medicine in 1956 for this achievement. Cardiac catheterization is now a routine procedure in heart surgery.\n\nThere have been several cases of surgeons operating on themselves, but most often it has been in the nature of an emergency rather than experiment. Such a case was Leonid Rogozov who was obliged to remove his own appendix in 1961 while stranded in Antarctica in winter. However, the first surgeon to carry out this self-operation, Evan O'Neill Kane in 1921, did so with an element of experiment. Although Kane's operation was necessary, it was not necessary to do it himself, so that in itself was experimental. More than that, Kane wished to experience the operation under local anaesthetic before trying the procedure on his patients. Kane advocated a reduction in the use of general anaesthetic by surgeons.\n\nJohn C. Lilly developed the first sensory deprivation tanks and self-experimented them with the intention to study the origin of consciousness and its relation to the brain by creating an environment which isolates an individual from external stimulation.\n\nJoseph Barcroft, in 1920, spent six days in a sealed glass chamber to investigate respiration at altitude. The partial pressure of oxygen was initially 163 mmHg falling to 84 mmHg (equivalent to an altitude of 18,000 ft) as the experiment progressed. Barcroft was attempting to disprove a theory of John Scott Haldane that the lungs actively secrete oxygen into the blood (rather than just through the process of passive diffusion) under conditions of low oxygen partial pressure. Barcroft suffered from severe hypoxia. At the end of experiment, part of Barcroft's left radial artery was removed for investigation.\n\nIn 1931, Barcroft subjected himself to freezing temperatures while naked. Towards the end of the experiment he showed signs of the final stages of hypothermia. He was thought to be close to death and had to be rescued by colleagues.\n\nKevin Warwick had an array of 100 electrodes fired into the median nerve fibres of his left arm. With this in place, over a 3-month period, he conducted a number of experiments linking his nervous system with the internet.\n\n\n"}
{"id": "24718287", "url": "https://en.wikipedia.org/wiki?curid=24718287", "title": "Structuralism (philosophy of science)", "text": "Structuralism (philosophy of science)\n\nStructuralism (also known as scientific structuralism or as the structuralistic theory-concept) is an active research program in the philosophy of science, which was first developed in the late 1960s and throughout the 1970s by several analytic philosophers.\n\nStructuralism asserts that all aspects of reality are best understood in terms of empirical scientific constructs of entities and their relations, rather than in terms of concrete entities in themselves. For instance, the concept of matter should be interpreted not as an absolute property of nature in itself, but instead of how scientifically-grounded mathematical relations describe how the concept of matter interacts with other properties, whether that be in a broad sense such as the gravitational fields that mass produces or more empirically as how matter interacts with sense systems of the body to produce sensations such as weight. Its aim is to comprise all important aspects of an empirical theory in one formal framework. The proponents of this meta-theoretic theory are Bas van Fraassen, Frederick Suppe, Patrick Suppes, Ronald Giere, Joseph D. Sneed, Wolfgang Stegmüller, Carlos Ulises Moulines, Wolfgang Balzer, John Worrall, Elie Georges Zahar, Pablo Lorenzano, Otávio Bueno, Anjan Chakravartty, Tian Yu Cao, Steven French, and Michael Redhead.\n\nThe term \"structural realism\" for the variation of scientific realism motivated by structuralist arguments, was coined by American philosopher in 1968. In 1998, the British structural realist philosopher James Ladyman distinguished epistemic and ontic forms of structural realism.\n\nThe philosophical concept of (scientific) structuralism is related to that of epistemic structural realism (ESR). ESR, a position originally and independently held by Henri Poincaré (1902), Bertrand Russell (1927), and Rudolf Carnap (1928), was resurrected by John Worrall (1989), who proposes that there is retention of structure across theory change. Worrall, for example, argued that Fresnel's equations imply that light has a structure and that Maxwell's equations, which replaced Fresnel's, do also; both characterize light as vibrations. Fresnel postulated that the vibrations were in a mechanical medium called \"ether\"; Maxwell postulated that the vibrations were of electric and magnetic fields. The structure in both cases is the vibrations and it was retained when Maxwell's theories replaced Fresnel's. Because structure is retained, structural realism both (a) avoids pessimistic meta-induction and (b) does not make the success of science seem miraculous, i.e., it puts forward a no-miracles argument.\n\nThe so-called Newman problem (also Newman's problem, Newman objection, Newman's objection) refers to the critical notice of Russell's \"The Analysis of Matter\" (1927) published by Max Newman in 1928. Newman argued that the ESR claim that one can know only the abstract structure of the external world trivializes scientific knowledge. The basis of his argument is the realization that \"[a]ny collection of things can be organized so as to have structure \"W\", provided there are the right\nnumber of them\", where \"W\" is an arbitrary structure.\n\nJohn Worrall (2000) advocates a version of ESR augmented by the Ramsey sentence reconstruction of physical theories (a Ramsey sentence aims at rendering propositions containing non-observable theoretical terms clear by substituting them with observable terms). John Worrall and Elie Georges Zahar (2001) claim that Newman's objection applies only if a distinction between observational and theoretical terms is not made. \n\nRamsey-style epistemic structural realism is distinct from and incompatible with the original Russellian epistemic structural realism (the difference between the two being that Ramsey-style ESR makes an epistemic commitment to Ramsey sentences, while Russellian ESR makes an epistemic commitment to abstract structures, that is, to (second-order) isomorphism classes of the observational structure of the world and not the (first-order) physical structure itself). Ioannis Votsis (2004) claims that Russellian ESR is \"also\" impervious to the Newman objection: Newman falsely attributed the trivial claim \"there exists \"a\" relation with a particular abstract structure\" to ESR, while ESR makes the non-trivial claim that there is a \"unique\" physical relation that is causally linked with a \"unique\" observational relation and the two are isomorphic.\n\nThe traditional scientific realist and notable critic of structural realism Stathis Psillos (1999) remarks that \"structural realism is best understood as issuing an epistemic constraint on what can be known and on what scientific theories can reveal.\" He thinks that ESR faces a number of insurmountable objections. These include among others that ESR's only epistemic commitment is uninterpreted equations which are not by themselves enough to produce predictions and that the \"structure versus nature\" distinction that ESR appeals to cannot be sustained.\n\nVotsis (2004) replies that the structural realist \"does subscribe to interpreted equations, but attempts to distinguish between interpretations that link the terms to observations from those that do not\" and he can appeal to the Russellian view that \"nature\" just means the non-isomorphically specifiable part of entities. \n\nPsillos also defends David Lewis's descriptive-causal theory of reference (according to which the abandoned theoretical terms after a theory change are regarded as successfully referring \"after all\") and claims that it can adequately deal with referential continuity in conceptual transitions, during which theoretical terms are abandoned, thus rendering ESR redundant.\n\nVotsis (2004) replies that a scientific realist needs not tie the approximate truth of a theory to referential success. Notably, structural realism initially did not dictate any particular theory of reference; however Votsis (2012) proposed a structuralist theory of reference according to which \"scientific terms are able to refer to individual objects, i.e. in a term-by-term fashion, but that to fix this reference requires taking into account the relations these objects instantiate.\"\n\nWhile ESR claims that only the structure of reality is knowable, ontic structural realism (OSR) goes further to claim that \"structure is all there is\". In this view, reality has no \"nature\" underlying its observed structure. Rather, reality is fundamentally structural, though variants of OSR disagree on precisely which aspects of structure are primitive. OSR is strongly motivated by modern physics, particularly quantum field theory, which undermines intuitive notions of identifiable objects with intrinsic properties. Some early quantum physicists held this view, including Hermann Weyl (1931), Ernst Cassirer (1936), and Arthur Eddington (1939). Recently, OSR has been called \"the most fashionable ontological framework for modern physics\".\n\nMax Tegmark takes this concept even further with the mathematical universe hypothesis, which proposes that, if our universe is only a particular structure, then it is no more real than any other structure.\n\nIn mathematical logic, a mathematical structure is a standard concept. A mathematical structure is a set of abstract entities with relations between them. The natural numbers under arithmetic constitute a structure, with relations such as \"is evenly divisible by\" and \"is greater than\". Here the relation \"is greater than\" includes the element (3, 4), but not the element (4, 3). Points in space and the real numbers under Euclidean geometry are another structure, with relations such as \"the distance between point P1 and point P2 is real number R1\"; equivalently, the \"distance\" relation includes the element (P1, P2, R1). Other structures include the Riemann space of general relativity and the Hilbert space of quantum mechanics. The entities in a mathematical structure do not have any independent existence outside their participation in relations. Two descriptions of a structure are considered equivalent, and to be describing the same underlying structure, if there is a correspondence between the descriptions that preserves all relations.\n\nMany proponents of structural realism formally or informally ascribe \"properties\" to the abstract objects; some argue that such properties, while they can perhaps be \"shoehorned\" into the formalism of relations, should instead be considered distinct from relations.\n\nIn quantum field theory (QFT), traditional proposals for \"the most basic known structures\" divide into \"particle interpretations\" such as ascribing reality to the Fock space of particles, and \"field interpretations\" such as considering the quantum wavefunction to be identical to the underlying reality. Varying interpretations of quantum mechanics provide one complication; another, perhaps minor, complication is that neither fields nor particles are completely localized in standard QFT. A third, less obvious, complication is that \"unitarily inequivalent representations\" are endemic in QFT; for example, the same patch of spacetime can be represented by a vacuum by an inertial observer, but as a thermal heat bath by an accelerating observer that perceives Unruh radiation, raising the difficult question of whether the vacuum structure or heat bath structure is the real structure, or whether both of these inequivalent structures are separately real. Another example, which does not require the complications of curved spacetime, is that in ferromagnetism, symmetry-breaking analysis results in inequivalent Hilbert spaces. More broadly, QFT's infinite degrees of freedom lead to inequivalent representations in the general case.\n\nIn general relativity, scholars often grant a \"basic structure\" status to the spacetime structure, sometimes via its metric.\n\n\n\n\n"}
{"id": "23421828", "url": "https://en.wikipedia.org/wiki?curid=23421828", "title": "Technological revolution", "text": "Technological revolution\n\nA technological revolution is a period in which one or more technologies is replaced by another technology in a short amount of time. It is an era of accelerated technological progress characterized by new innovations whose rapid application and diffusion cause an abrupt change in society.\n\nA technological revolution is made of interconnected technological changes. \n\nA technological revolution increases productivity and efficiency. It may involve material or ideological changes caused by the introduction of a device or system. Some examples of its potential impact are business management, education, social interactions, finance and research methodology; it is not limited strictly to technical aspects. Technological revolution rewrites the material conditions of human existence and can reshape culture. It can play a role of a trigger of a chain of various and unpredictable changes:\n\nWhat distinguishes a technological revolution from a random collection of technology systems and justifies conceptualizing it as a revolution are two basic features:\n\n1. The strong interconnectedness and interdependence of the participating systems in their technologies and markets.\n\n2. The capacity to transform profoundly the rest of the economy (and eventually society).\n\nThe consequences of a technological revolution are not necessarily positive. For example, innovations, such as the use of coal as an energy source, can have negative environmental impact and cause technological unemployment. The concept of technological revolution is based on the idea that technological progress is not linear but undulatory. Technological revolution can be:\n\n\nThe concept of universal technological revolutions is a key factor in the Neo-Schumpeterian theory of long economic waves/cycles (Carlota Perez, Tessaleno Devezas, Daniel Šmihula and others).\n\nThe most known example of technological revolution was the Industrial Revolution in the 19th century, the scientific-technical revolution about 1950–1960, the Neolithic revolution, the Digital revolution and so on. The notion of \"technological revolution\" is frequently overused, therefore it is not easy to define which technological revolutions having occurred during world history were really crucial and influenced not only one segment of human activity, but had a universal impact. One universal technological revolution should be composed from several sectoral technological revolutions (in science, industry, transport and the like).\n\nWe can identify several universal technological revolutions which occurred during the modern era in Western culture:\n\nAttempts to find comparable periods of well defined technological revolutions in the pre-modern era are highly speculative. Probably one of the most systematic attempts to suggest a timeline of technological revolutions in pre-modern Europe was done by Daniel Šmihula: \n\nAfter 2000 there became popular the idea that a sequence of technological revolutions is not over and in the forthcoming future we will witness the dawn of a new universal technological revolution. The main innovations should develop in the fields of nanotechnologies, alternative fuel and energy systems, biotechnologies, genetic engineering, new materials technologies and so on .\n\nSometimes the notion of \"technological revolution\" is used for the Second Industrial Revolution in the period about 1900, but in this case the designation \"technical revolution\" would be more proper. When the notion of technical revolution is used in more general meaning it is almost identical with technological revolution, but technological revolution requires material changes in used tools, machines, energy sources, production processes. Technical revolution can be restricted to changes in management, organisation and so called non-material technologies (e.g. a progress in mathematics or accounting).\n\n\n"}
{"id": "27288108", "url": "https://en.wikipedia.org/wiki?curid=27288108", "title": "The Department of Mad Scientists", "text": "The Department of Mad Scientists\n\nThe Department of Mad Scientists: How DARPA is Remaking Our World, from the Internet to Artificial Limbs is a book by Michael Belfiore about the history and origins of DARPA. Belfiore describes DARPA's creation as the agency ARPA in Department of Defense and some of its notable contributions to artificial limbs, the Internet, space exploration and robotic automobiles.\n\nBelfiore also highlights some of the unique aspects of DARPA's internal structure such as term limits for project managers, willingness to fund ambitious research, and subcontracting of all research.\n\n"}
{"id": "4149673", "url": "https://en.wikipedia.org/wiki?curid=4149673", "title": "The Naked Ape", "text": "The Naked Ape\n\nThe Naked Ape: A Zoologist's Study of the Human Animal (Hardback: ; Reprint: ) is a 1967 book by zoologist and ethologist Desmond Morris that looks at humans as a species and compares them to other animals. \"The Human Zoo\", a follow-up book by Morris that examined the behaviour of people in cities, was published in 1969.\n\n\"The Naked Ape\", which was serialised in the \"Daily Mirror\" newspaper and has been translated into 23 languages, depicts human behaviour as largely evolved to meet the challenges of prehistoric life as a hunter (see nature versus nurture). The book was so named because out of 193 species of monkeys and apes, only humans (\"Homo sapiens sapiens\") are not covered in hair. Desmond Morris, the author, who had been the curator of mammals at London Zoo, said his book was intended to popularise and demystify science.\n\nMorris said that \"Homo sapiens\" not only have the largest brains of all higher primates, but that sexual selection in human evolution has caused humans to have the highest ratio of penis size to body mass. Morris conjectured that human ear-lobes developed as an additional erogenous zone to facilitate the extended sexuality necessary in the evolution of human monogamous pair bonding. Morris further stated that the more rounded shape of human female breasts means they are mainly a sexual signalling device rather than simply for providing milk for infants. \n\nMorris framed many features of human behaviour in the context of evolution at a time when cultural explanations were more orthodox. For example, Morris wrote that the intense human pair bond evolved so that men who were out hunting could trust that their mates back home were not having sex with other men, and suggested the possibility that sparse body hair evolved because the \"nakedness\" helped intensify pair bonding by increasing tactile pleasure. Like many other writers in the late 1960s and 1970s, Morris warned against the \"population explosion\" and overcrowding that might cause terminal damage to heretofore relatively stable social structures that humans had evolved in the long course of their development.\n\nA 1973 film directed by Donald Driver, very loosely based on the book, was made starring Johnny Crawford and Victoria Principal. In 2006, an independent film was made, based loosely on the book, written and directed by Daniel Mellitz, starring Josh Wise, Chelse Swain, Sean Shanks, Amanda MacDonald, Tony LaThanh, Corbin Bernsen. Beyond their scripts being loosely based on his book, Morris was not involved in either movie in any way.\n\n\nCritical response\nIn February 1976, the book was removed from high school library shelves by the board of education of the Island Trees Union Free School District in New York. This case became the subject of a U.S. Supreme Court case in 1982.\n\nThe naked ape is mentioned in the Italian entry for the 2017 Eurovision Song Contest \"Occidentali's Karma\" by Francesco Gabbani, in which most of the lyrics contain philosophical references. A lyricist came up with using it in the song lyrics after reading \"The Naked Ape\" himself. Morris, \"fascinated by the culture, beauty and richness\" of the references to his theories, sent Gabbani a signed copy of the Italian translation of the book as a sign of gratitude and support for the latter.Terence McKenna referenced Morris and \"The Naked Ape\" in his lecture \"Culture is Your Operating System\".\n\n\nNotes\nReferences\n"}
{"id": "26314185", "url": "https://en.wikipedia.org/wiki?curid=26314185", "title": "The Poisoner's Handbook", "text": "The Poisoner's Handbook\n\nThe Poisoner's Handbook: Murder and the Birth of Forensic Medicine in Jazz Age New York is a \"New York Times\" best-selling non-fiction book by Pulitzer Prize-winning science writer Deborah Blum that was released by Penguin Press in 2010.\n\nIn 1918, New York City appointed Charles Norris, Bellevue Hospital's chief pathologist, as its first scientifically trained medical examiner. The book, about Norris and Alexander Gettler, the city's first toxicologist, describes the Jazz Age's poisoning cases. Before the two began working in the medical examiner's office, Blum pointed out in her book, poisoners could get away with murder. The book covers the years from 1915 to 1936, which Blum described as a \"coming-of-age\" for forensic toxicology. \"Under (Norris's) direction, the New York City medical examiner's office would become a department that set forensic standards for the rest of the country,\" Blum wrote.\n\nWhile a guest on National Public Radio’s \"Talk of the Nation/Science Friday\" to discuss the book, Blum told host Ira Flatow that she wrote the book because, \"I've always been interested in poison. I wanted to write about the mystery of how (poisons) kill us.”\n\n\"Reader's Digest\" named \"The Poisoner's Handbook\" one of its Top 10 best crime books, saying, \"This is science writing at its finest that reads like a mystery novel.\"\n\n\"The New York Times\" placed the book on its Top-rated List on March 5, 2010. In its Sunday book review, the \"Times\" said \"The Poisoner's Handbook\" was \"structured like a collection of linked short stories. Each chapter centers on a mysterious death by poison that Norris and Gettler investigate.\"\n\nThe book was listed as a \"New York Times bestseller\" in paperback nonfiction in February 2011. Also, Amazon named \"The Poisoner's Handbook\" in its Top 100 Best of 2010.\n\n\"Not only is \"The Poisoner's Handbook\" as thrilling as any 'CSI' episode,\" wrote reviewer Art Taylor with \"The Washington Post\", \"but it also offers something even better: an education in how forensics really works.\"\n\n\"Kirkus Reviews\" described the book as, \"The rollicking story of the creation of modern forensic science by New York researchers during the Prohibition era.\"\n\nBarnes and Noble's editor's review said this: \"The book is an unexpected yet appropriate open-sesame into a world that was planting seeds for the world -- with lethal toxins and cutting-edge tools -- that would later, darkly bloom.\"\n\nGlen Weldon from NPR Books said: \"Rigorously researched and thoroughly engaging, \"The Poisoner's Handbook\" is a compelling, comprehensive portrait of the time and place that transformed criminal investigation, and made it much more difficult for that most insidious of murderers to escape the law.\"\n\nPBS optioned the book for TV and produced it as an episode of \"American Experience\". It premiered on January 7, 2014.\n\n\n\"Angel Killer: A True Story of Cannibalism, Crime Fighting, and Insanity in New York City\" (The Atavist, 2012)\n\n"}
{"id": "12431148", "url": "https://en.wikipedia.org/wiki?curid=12431148", "title": "Visiting scholar", "text": "Visiting scholar\n\nIn US academia, a visiting scholar, visiting researcher, visiting fellow, visiting lecturer or visiting professor is a scholar from an institution who visits a host university and is projected to teach, lecture, or perform research on a topic the visitor is valued for. In many cases the position is not salaried because the scholar typically is salaried by their home institution (or partially salaried, as in some cases of sabbatical leave from US universities), while some visiting positions are salaried. \n\nTypically, a position as visiting scholar is for a couple of months or even a year, though it can be extended. It is not unusual that host institutions provide accommodation for the visiting scholar. Typically, a visiting scholar is invited by the host institution. Being invited as a visiting scholar is often regarded as a significant accolade and recognition of the scholar's prominence in the field. Attracting prominent visiting scholars often allows the permanent faculty and graduate students to cooperate with prominent academics from other institutions, especially foreign ones.\n\nIn the UK, a visiting scholar or visiting academic usually has to pay a so-called bench fee to the university, which will give them access to a shared office space and other university facilities and resources (such as the library). Bench fee amounts vary across the UK universities. \n\nThe purpose of a visiting scholars programs is generally to bring to the university or educational institution in question an exceptional senior scholar who can contribute to and enrich the community's intellectual and research endeavors and international projection. Hence, in addition to conducting their own research, visitors are often expected to actively participate in a number of productive institutional activities, such as:\n\n\n"}
