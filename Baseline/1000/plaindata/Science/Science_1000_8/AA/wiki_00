{"id": "42001", "url": "https://en.wikipedia.org/wiki?curid=42001", "title": "Antonie van Leeuwenhoek", "text": "Antonie van Leeuwenhoek\n\nAntonie Philips van Leeuwenhoek FRS ( , ; 24 October 1632 – 26 August 1723) was a Dutch businessman and scientist in the Golden Age of Dutch science and technology. A largely self-taught man in science, he is commonly known as \"the Father of Microbiology\", and one of the first microscopists and microbiologists. Van Leeuwenhoek is best known for his pioneering work in microscopy and for his contributions toward the establishment of microbiology as a scientific discipline.\n\nRaised in Delft, Dutch Republic, van Leeuwenhoek worked as a draper in his youth and founded his own shop in 1654. He became well recognized in municipal politics and developed an interest in lensmaking. In the 1670s, he started to explore microbial life with his microscope. This was one of the notable achievements of the Golden Age of Dutch exploration and discovery (c. 1590s–1720s).\n\nUsing single-lensed microscopes of his own design, van Leeuwenhoek was the first to experiment with microbes, which he originally referred to as \"animalcules\" (from Latin \"animalculum\" = \"tiny animal\"). Through his experiments, he was the first to relatively determine their size. Most of the \"animalcules\" are now referred to as unicellular organisms, although he observed multicellular organisms in pond water. He was also the first to document microscopic observations of muscle fibers, bacteria, spermatozoa, red blood cells, crystals in gouty tophi, and blood flow in capillaries. Although van Leeuwenhoek did not write any books, his discoveries came to light through correspondence with the Royal Society, which published his letters.\n\nAntonie van Leeuwenhoek was born in Delft, Dutch Republic, on 24 October 1632. On 4 November, he was baptized as \"Thonis\". His father, Philips Antonisz van Leeuwenhoek, was a basket maker who died when Antonie was only five years old. His mother, Margaretha (Bel van den Berch), came from a well-to-do brewer's family. She remarried Jacob Jansz Molijn, a painter. Antonie had four older sisters: Margriet, Geertruyt, Neeltje, and Catharina. When he was around ten years old his step-father died. He attended school in Warmond for a short time before being sent to live in Benthuizen with his uncle, an attorney. At the age of 16 he became a bookkeeper's apprentice at a linen-draper's shop in Amsterdam, which was owned by the Scot William Davidson. Van Leeuwenhoek left there after six years.\n\nVan Leeuwenhoek married Barbara de Mey in July 1654, with whom he fathered one surviving daughter, Maria (four other children died in infancy). That same year he returned to Delft, where he would live and study for the rest of his life. He opened a draper's shop, which he ran throughout the 1650s. His wife died in 1666, and in 1671, Van Leeuwenhoek remarried to Cornelia Swalmius with whom he had no children. His status in Delft had grown throughout the years. In 1660 he received a lucrative job as chamberlain for the assembly chamber of the Delft sheriffs in the city hall, a position which he would hold for almost 40 years. In 1669 he was appointed as a land surveyor by the court of Holland; at some time he combined it with another municipal job, being the official \"wine-gauger\" of Delft and in charge of the city wine imports and taxation.\n\nVan Leeuwenhoek was a contemporary of another famous Delft citizen, the painter Johannes Vermeer, who was baptized just four days earlier. It has been suggested that he is the man portrayed in two Vermeer paintings of the late 1660s, \"The Astronomer\" and \"The Geographer\", but others argue that there appears to be little physical similarity. Because they were both relatively important men in a city with only 24,000 inhabitants, it is likely that they were at least acquaintances; Van Leeuwenhoek acted as the executor of Vermeer's will after the painter died in 1675.\n\nWhile running his draper shop, van Leeuwenhoek wanted to see the quality of the thread better than what was possible using the magnifying lenses of the time. He developed an interest in lensmaking, although few records exist of his early activity. Van Leeuwenhoek's interest in microscopes and a familiarity with glass processing led to one of the most significant, and simultaneously well-hidden, technical insights in the history of science.\n\nBy placing the middle of a small rod of soda lime glass in a hot flame, van Leeuwenhoek could pull the hot section apart to create two long whiskers of glass. Then, by reinserting the end of one whisker into the flame, he could create a very small, high-quality glass sphere. These spheres became the lenses of his microscopes, with the smallest spheres providing the highest magnifications.\n\nAfter developing his method for creating powerful lenses and applying them to the study of the microscopic world, van Leeuwenhoek introduced his work to his friend, the prominent Dutch physician Reinier de Graaf. When the Royal Society in London published the groundbreaking work of an Italian lensmaker in their journal \"Philosophical Transactions of the Royal Society\", de Graaf wrote to the editor of the journal, Henry Oldenburg, with a ringing endorsement of van Leeuwenhoek's microscopes which, he claimed, \"far surpass those which we have hitherto seen\". In response, in 1673 the society published a letter from van Leeuwenhoek that included his microscopic observations on mold, bees, and lice.\nVan Leeuwenhoek's work fully captured the attention of the Royal Society, and he began corresponding regularly with the society regarding his observations. At first he had been reluctant to publicize his findings, regarding himself as a businessman with little scientific, artistic, or writing background, but de Graaf urged him to be more confident in his work. By the time van Leeuwenhoek died in 1723, he had written some 190 letters to the Royal Society, detailing his findings in a wide variety of fields, centered on his work in microscopy. He only wrote letters in his own colloquial Dutch; he never published a proper scientific paper in Latin. He strongly preferred to work alone, distrusting the sincerity of those who offered their assistance. The letters were translated into Latin or English by Henry Oldenburg, who had learned Dutch for this very purpose. Despite the initial success of van Leeuwenhoek's relationship with the Royal Society, soon relations became severely strained. In 1676, his credibility was questioned when he sent the Royal Society a copy of his first observations of microscopic single-celled organisms. Previously, the existence of single-celled organisms was entirely unknown. Thus, even with his established reputation with the Royal Society as a reliable observer, his observations of microscopic life were initially met with some skepticism.\n\nEventually, in the face of van Leeuwenhoek's insistence, the Royal Society arranged for Alexander Petrie, minister to the English Reformed Church in Delft; Benedict Haan, at that time Lutheran minister at Delft; and Henrik Cordes, then Lutheran minister at the Hague, accompanied by Sir Robert Gordon and four others, to determine whether it was in fact van Leeuwenhoek's ability to observe and reason clearly, or perhaps, the Royal Society's theories of life that might require reform. Finally in 1677, van Leeuwenhoek's observations were fully acknowledged by the Royal Society.\n\nAntonie van Leeuwenhoek was elected to the Royal Society in February 1680 on the nomination of William Croone, a then-prominent physician. Van Leeuwenhoek was \"taken aback\" by the nomination, which he considered a high honor, although he did not attend the induction ceremony in London, nor did he ever attend a Royal Society meeting.\n\nBy the end of the seventeenth century, van Leeuwenhoek had a virtual monopoly on microscopic study and discovery. His contemporary Robert Hooke, an early microscope pioneer, bemoaned that the field had come to rest entirely on one man's shoulders. He was visited over the years by many notable individuals, such as the Russian Tsar Peter the Great. To the disappointment of his guests, van Leeuwenhoek refused to reveal the cutting-edge microscopes he relied on for his discoveries, instead showing visitors a collection of average-quality lenses.\n\nAn experienced businessman, van Leeuwenhoek believed that if his simple method for creating the critically important lens was revealed, the scientific community of his time would likely disregard or even forget his role in microscopy. He therefore allowed others to believe that he was laboriously spending most of his nights and free time grinding increasingly tiny lenses to use in microscopes, even though this belief conflicted both with his construction of hundreds of microscopes and his habit of building a new microscope whenever he chanced upon an interesting specimen that he wanted to preserve. He made about 200 microscopes with a different magnification.\n\nVan Leeuwenhoek was visited by Leibniz, William III of Orange and his wife, Mary II of England, and the burgemeester (mayor) Johan Huydecoper of Amsterdam, the latter being very interested in collecting and growing plants for the Hortus Botanicus Amsterdam, and all gazed at the \"tiny creatures\". In 1698, van Leeuwenhoek was invited to visit the Tsar Peter the Great on his boat. On this occasion van Leeuwenhoek presented the Tsar an \"eel-viewer\", so Peter could study blood circulation whenever he wanted.\n\nAntonie van Leeuwenhoek made more than 500 optical lenses. He also created at least 25 single-lens microscopes, of differing types, of which only nine have survived. These microscopes were made of silver or copper frames, holding hand-made lenses. Those that have survived are capable of magnification up to 275 times. It is suspected that van Leeuwenhoek possessed some microscopes that could magnify up to 500 times. Although he has been widely regarded as a dilettante or amateur, his scientific research was of remarkably high quality.\n\nThe single-lens microscopes of van Leeuwenhoek were relatively small devices, the largest being about 5 cm long. They are used by placing the lens very close in front of the eye, while looking in the direction of the sun. The other side of the microscope had a pin, where the sample was attached in order to stay close to the lens. There were also three screws to move the pin and the sample along three axes: one axis to change the focus, and the two other axes to navigate through the sample.\n\nVan Leeuwenhoek maintained throughout his life that there are aspects of microscope construction \"which I only keep for myself\", in particular his most critical secret of how he made the lenses. For many years no-one was able to reconstruct van Leeuwenhoek's design techniques, however, in 1957, C.L. Stong used thin glass thread fusing instead of polishing, and successfully created some working samples of a van Leeuwenhoek design microscope. Such a method was also discovered independently by A. Mosolov and A. Belkin at the Russian Novosibirsk State Medical Institute.\nVan Leeuwenhoek used samples and measurements to estimate numbers of microorganisms in units of water. He also made good use of the huge advantage provided by his method. He studied a broad range of microscopic phenomena, and shared the resulting observations freely with groups such as the British Royal Society. Such work firmly established his place in history as one of the first and most important explorers of the microscopic world. Van Leeuwenhoek was one of the first people to observe cells, much like Robert Hooke.\n\nVan Leeuwenhoek's main discoveries are:\n\nIn 1687, van Leeuwenhoek reported his research on the coffee bean. He roasted the bean, cut it into slices and saw a spongy interior. The bean was pressed, and an oil appeared. He boiled the coffee with rain water twice and set it aside.\n\nLike Robert Boyle and Nicolaas Hartsoeker, van Leeuwenhoek was interested in dried cochineal, trying to find out if the dye came from a berry or an insect.\n\nVan Leeuwenhoek's religion was \"Dutch Reformed\" Calvinist. He often referred with reverence to the wonders God designed in making creatures great and small, and believed that his discoveries were merely further proof of the wonder of creation.\n\nBy the end of his life, van Leeuwenhoek had written approximately 560 letters to the Royal Society and other scientific institutions concerning his observations and discoveries. Even during the last weeks of his life, van Leeuwenhoek continued to send letters full of observations to London. The last few contained a precise description of his own illness. He suffered from a rare disease, an uncontrolled movement of the midriff, which now is named \"van Leeuwenhoek's disease\". He died at the age of 90, on 26 August 1723, and was buried four days later in the Oude Kerk in Delft.\n\nIn 1981, the British microscopist Brian J. Ford found that van Leeuwenhoek's original specimens had survived in the collections of the Royal Society of London. They were found to be of high quality, and all were well preserved. Ford carried out observations with a range of single-lens microscopes, adding to our knowledge of van Leeuwenhoek's work. In Ford's opinion, Leeuwenhoek remained imperfectly understood, the popular view that his work was crude and undisciplined at odds with the evidence of conscientious and painstaking observation. He constructed rational and repeatable experimental procedures and was willing to oppose received opinion, such as spontaneous generation, and he changed his mind in the light of evidence.\"\n\nOn his importance in the history of microbiology and science in general, the British biochemist Nick Lane wrote that he was \"the first even to think of looking—certainly, the first with the power to see.\" His experiments were ingenious and he was \"a scientist of the highest calibre\", attacked by people who envied him or \"scorned his unschooled origins\", not helped by his secrecy about his methods.\n\nThe Antoni van Leeuwenhoek Hospital in Amsterdam, named after van Leeuwenhoek, is specialized in oncology. In 2004, a public poll in the Netherlands to determine the greatest Dutchman (\"De Grootste Nederlander\") named van Leeuwenhoek the 4th-greatest Dutchman of all time.\n\nOn 24 October 2016, Google commemorated the 384th anniversary of van Leeuwenhoek's birth with a Doodle that depicted his discovery of \"little animals\" or animalcules, now known as bacteria.\n\nThe Leeuwenhoek Medal, Leeuwenhoek Lecture, Leeuwenhoek (crater), \"Leeuwenhoeckia\", \"Levenhookia\" (a genus in the family Stylidiaceae), and \"Leeuwenhoekiella\" (an aerobic bacterial genus) are named after him.\n\n\n\n"}
{"id": "38751", "url": "https://en.wikipedia.org/wiki?curid=38751", "title": "ArXiv", "text": "ArXiv\n\narXiv (pronounced \"archive\") is a repository of electronic preprints (known as e-prints) approved for publication after moderation, that consists of scientific papers in the fields of mathematics, physics, astronomy, electrical engineering, computer science, quantitative biology, statistics, and quantitative finance, which can be accessed online. In many fields of mathematics and physics, almost all scientific papers are self-archived on the arXiv repository. Begun on August 14, 1991, arXiv.org passed the half-million-article milestone on October 3, 2008, and had hit a million by the end of 2014. By October 2016 the submission rate had grown to more than 10,000 per month.\n\narXiv was made possible by the low-bandwidth TeX file format, which allowed scientific papers to be easily transmitted over the Internet and rendered client-side. Around 1990, Joanne Cohn began emailing physics preprints to colleagues as TeX files, but the number of papers being sent soon filled mailboxes to capacity. Paul Ginsparg recognized the need for central storage, and in August 1991 he created a central repository mailbox stored at the Los Alamos National Laboratory which could be accessed from any computer. Additional modes of access were soon added: FTP in 1991, Gopher in 1992, and the World Wide Web in 1993. The term e-print was quickly adopted to describe the articles.\n\nIt began as a physics archive, called the LANL preprint archive, but soon expanded to include astronomy, mathematics, computer science, quantitative biology and, most recently, statistics. Its original domain name was xxx.lanl.gov. Due to LANL's lack of interest in the rapidly expanding technology, in 2001 Ginsparg changed institutions to Cornell University and changed the name of the repository to arXiv.org. It is now hosted principally by Cornell, with eight mirrors around the world.\n\nIts existence was one of the precipitating factors that led to the current movement in scientific publishing known as open access. Mathematicians and scientists regularly upload their papers to arXiv.org for worldwide access and sometimes for reviews before they are published in peer-reviewed journals. Ginsparg was awarded a MacArthur Fellowship in 2002 for his establishment of arXiv.\n\nThe annual budget for arXiv is approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions. This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Each member institution pledges a five-year funding commitment to support arXiv. Based on institutional usage ranking, the annual fees are set in four tiers from $1,000 to $4,400. Cornell's goal is to raise at least $504,000 per year through membership fees generated by approximately 220 institutions.\n\nIn September 2011, Cornell University Library took overall administrative and financial responsibility for arXiv's operation and development. Ginsparg was quoted in the \"Chronicle of Higher Education\" as saying it \"was supposed to be a three-hour tour, not a life sentence\". However, Ginsparg remains on the arXiv Scientific Advisory Board and on the arXiv Physics Advisory Committee.\n\nAlthough arXiv is not peer reviewed, a collection of moderators for each area review the submissions; they may recategorize any that are deemed off-topic, or reject submissions that are not scientific papers. The lists of moderators for many sections of arXiv are publicly available, but moderators for most of the physics sections remain unlisted.\n\nAdditionally, an \"endorsement\" system was introduced in 2004 as part of an effort to ensure content is relevant and of interest to current research in the specified disciplines. Under the system, for categories that use it, an author must be endorsed by an established arXiv author before being allowed to submit papers to those categories. Endorsers are not asked to review the paper for errors, but to check whether the paper is appropriate for the intended subject area. New authors from recognized academic institutions generally receive automatic endorsement, which in practice means that they do not need to deal with the endorsement system at all. However, the endorsement system has attracted criticism for allegedly restricting scientific inquiry.\n\nA majority of the e-prints are also submitted to journals for publication, but some work, including some very influential papers, remain purely as e-prints and are never published in a peer-reviewed journal. A well-known example of the latter is an outline of a proof of Thurston's geometrization conjecture, including the Poincaré conjecture as a particular case, uploaded by Grigori Perelman in November 2002. Perelman appears content to forgo the traditional peer-reviewed journal process, stating: \"If anybody is interested in my way of solving the problem, it's all there let them go and read about it\". Despite this non-traditional method of publication, other mathematicians recognized this work by offering the Fields Medal and Clay Mathematics Millennium Prizes to Perelman, both of which he refused.\n\nPapers can be submitted in any of several formats, including LaTeX, and PDF printed from a word processor other than TeX or LaTeX. The submission is rejected by the arXiv software if generating the final PDF file fails, if any image file is too large, or if the total size of the submission is too large. arXiv now allows one to store and modify an incomplete submission, and only finalize the submission when ready. The time stamp on the article is set when the submission is finalized.\n\nThe standard access route is through the arXiv.org website or one of several mirrors. Several other interfaces and access routes have also been created by other un-associated organisations. These include the University of California, Davis's \"front\", a web portal that offers additional search functions and a more self-explanatory interface for arXiv.org, and is referred to by some mathematicians as (the) Front. A similar function used to be offered by eprintweb.org, launched in September 2006 by the Institute of Physics, and was switched off on June 30, 2014. Carnegie Mellon provides Table, a search engine for tables extracted from arXiv publications. Google Scholar and Live Search Academic (now defunct) can also be used to search for items in arXiv. A full text and author search engine for arXiv is provided by Scientillion. Finally, researchers can select sub-fields and receive daily e-mailings or RSS feeds of all submissions in them.\n\nFiles on arXiv can have a number of different copyright statuses:\n\nWhile arXiv does contain some dubious e-prints, such as those claiming to refute famous theorems or proving famous conjectures such as Fermat's Last Theorem using only high-school mathematics, they are \"surprisingly rare\". arXiv generally re-classifies these works, e.g. in \"General mathematics\", rather than deleting them; however, some authors have voiced concern over the lack of transparency\nin the arXiv screening process.\n\n\n"}
{"id": "34892999", "url": "https://en.wikipedia.org/wiki?curid=34892999", "title": "Astronaut training", "text": "Astronaut training\n\nAstronaut training describes the complex process of preparing astronauts for their space missions before, during and after the flight, which includes medical tests, physical training, extra-vehicular activity (EVA) training, procedure training, rehabilitation process, as well as training on experiments they will accomplish during their stay in space.\n\nThe training is geared to the special conditions and environments astronauts will be confronted with during launch, in space, and during landing. All phases of the flight must be considered during training to ensure safety to, and functionality of the astronauts, as well as to ensure a successful completion of the mission. The Apollo astronauts that walked on the Moon also received training for geology fieldwork on the Lunar surface.\n\nThe effects of launching and landing has various effects on astronauts, with the most significant effects that occur being space motion sickness, orthostatic intolerance, and cardiovascular events.\n\nSpace motion sickness is an event that can occur within minutes of being in changing gravity environments (i.e. from 1g on Earth prior to launch to more than 1g during launch, and then from microgravity in space to hypergravity during re-entry and again to 1g after landing). The symptoms range from drowsiness and headaches, to nausea and vomiting. There are three general categories of space motion sickness:\n\n\nAbout three-fourths of astronauts experience \nspace motion sickness, with effects rarely exceeding two days. There is a risk for post-flight motion sickness, however this is only significant following long-duration space missions.\n\nPost-flight, following exposure to microgravity, the vestibular system, located in the inner ear is disrupted because of the microgravity-induced unresponsiveness of the otoliths which are small calcareous concretions that sense body postures and are responsible for ensuring proper balance. In most cases, this leads to some postflight postural illusions.\n\nCardiovascular events represent important factors during the three phases of a space mission. They can be divided in:\n\nAstronauts are trained in preparation for the conditions of launch as well as the harsh environment of space. This training aims to prepare the crew for events falling under two broad categories: events relating to operation of the spacecraft (internal events), and events relating to the space environment (external events).\n\nDuring training, astronauts are familiarized with the engineering systems of the spacecraft including spacecraft propulsion, spacecraft thermal control, and life support systems. In addition to this, astronauts receive training in orbital mechanics, scientific experimentation, earth observation, and astronomy. This training is particularly important for missions when an astronaut will encounter multiple systems (for example on the International Space Station (ISS)). Training is performed in order to prepare astronauts for events that may pose a hazard to their health, the health of the crew, or the successful completion of the mission. These types of events may be: failure of a critical life support system, capsule depressurization, fire, and other life-threatening events. In addition to the need to train for hazardous events, astronauts will also need to train to ensure the successful completion of their mission. This could be in the form of training for EVA, scientific experimentation, or spacecraft piloting.\n\nExternal events refers more broadly to the ability to live and work in the extreme environment of space. This includes adaptation to microgravity (or weightlessness), isolation, confinement, and radiation. The difficulty associated with living and working in microgravity include spatial disorientation, motion sickness, and vertigo. During long-duration missions, astronauts will often experience isolation and confinement. This has been known to limit performance of astronaut crews and hence training aims to prepare astronauts for such challenges. The long-term effects of radiation on crews is still largely unknown. However, it is theorized that astronauts on a trip to Mars will likely receive more than 1000x the radiation dosage of a typical person on earth. As such, present and future training must incorporate systems and processes for protecting astronauts against radiation.\n\nScientific experimentation has historically been an important element of human spaceflight, and is the primary focus of the International Space Station. Training on how to successfully carry out these experiments is an important part of astronaut training, as it maximizes the scientific return of the mission. Once on-orbit, communication between astronauts and scientists on the ground can be limited, and time is strictly apportioned between different mission activities. It is vital that astronauts are familiar with their assigned experiments in order to complete them in a timely manner, with as little intervention from the ground as possible.\n\nFor missions to the ISS, each astronaut is required to become proficient at one hundred or more experiments. During training, the scientists responsible for the experiments do not have direct contact with the astronauts who will be carrying them out. Instead, scientists instruct trainers who in turn prepare the astronauts for carrying out the experiment. Much of this training is done at the European Astronaut Center.\n\nFor human experiments, the scientists describe their experiments to the astronauts who then choose whether to participate on board the ISS. For these experiments, the astronauts will be tested before, during, and after the mission to establish a baseline and determine when the astronaut returned to the baseline.\n\nAt NASA, following the selection phase, the so-called \"AsCans\" (Astronaut candidates) have to undergo up to two years of training/indoctrination period to become fully qualified astronauts. \nInitially, all AsCans must go through basic training to learn both technical and soft skills. There are 16 different technical courses in:\n\n\nAsCans initially go through Basic Training, where they are trained on Soyuz, and ISS systems, flight safety and operations, as well as land or water survival. Pilot AsCans will receive training on NASA's T-38 Trainer Jet. Furthermore, because modern space exploration is done by a consortium of different countries and is a very publicly visible area, astronauts received professional and cultural training, as well as language courses (specifically in Russian).\n\nFollowing completion of Basic Training candidates proceed to NASA's Advanced Training. AsCans are trained on life-sized models to get a feel of what they will be doing in space. This was done both through the use of the Shuttle Training Aircraft while it was still operational and is done through simulation mock-ups. The shuttle training aircraft was exclusively used by the commander and pilot astronauts for landing practices until the retirement of the Shuttle, while advanced simulation system facilities are used by all the candidates to learn how to work and successfully fulfill their tasks in the space environment. Simulators and EVA training facilities help candidates to best prepare their different mission operations. In particular, vacuum chambers, parabolic flights, and neutral buoyancy facilities (NBF) allow candidates to get acclimated to the micro gravity environment, particularly for EVA. Virtual reality is also becoming increasingly used as a tool to immerse AsCans into the space environment.\nThe final phase is the Intensive Training. It starts at about three months prior to launch and serves to prepare the candidates specifically for the mission they have been assigned to. Flight-specific integrated simulations are designed to provide a dynamic testing ground for mission rules and flight procedures. The final Intensive Training joint crew/flight controller training is carried out in parallel with \"mission planning.\" This phase is where candidates will undergo mission specific operational training, as well as experience with their assigned experiments. Crew medical officer training is also included to effectively intervene with proactive and reactive actions in case of medical issues.\n\nAstronaut training in Europe is carried out by the European Astronaut Centre (EAC), headquartered in Cologne, Germany. European training has three phases: Basic training, Advanced training, and Increment Specific Training.\n\nFor all ESA selected astronauts, Basic Training begins at the EAC headquarters. This section of the training cycle has four separate training blocks that last 16 months. Astronauts will receive an orientation on the major spacefaring nations, their space agencies, and all major manned and unmanned space programs. Training in this phase also looks into applicable laws and policies of the space sector. Technical (including engineering, astrodynamics, propulsion, orbital mechanics, etc.) and scientific (including human physiology, biology, earth observation, and astronomy) basics are introduced, to ensure that all new astronauts have the required base level of knowledge. Training is done on ISS operations and facilities, including an introduction to all major operating systems on board the ISS that are required for its functionality as a manned space research laboratory. This phase also covers in-depth systems operations for all spacecraft that service the ISS (e.g. Soyuz, Progress, Automatic Transfer Vehicle (ATV), and the H-II Transfer Vehicle (HTV)), as well as ground control and launch facility training. This training phase also focuses on skills such as robotic operations, rendezvous and docking, Russian language courses, human behavior and performance, and finally a PADI open water scuba diving course. This scuba course provides basic EVA training at ESA's NBF before moving onto the larger NASA training facility at the Lyndon B. Johnson Space Center.\n\nAdvanced Training includes a much more in-depth look into the ISS, including learning how to service and operate all systems. Enhanced science training is also implemented at this time to ensure all astronauts can perform science experiments on board the ISS. This phase takes around one year to complete and training is completed across the ISS partner network, no longer only at the EAC. It is only upon completion of this phase that astronauts are assignment to a spaceflight.\n\nIncrement-Specific Training starts only after an astronaut has been assigned to a flight. This phase lasts 18 months and prepares them for their role on their assigned mission. During this phase crew members as well as backup crews will train together. The crew tasks on the ISS are individually tailored, with consideration to the astronaut's particular experience and professional background. There are three different user levels for all on-board equipment (i.e. user level, operator level, and specialist level). A crew member can be a specialist on systems while also only being an operator or user on others, hence why the training program is individually tailored. Increment Specific Training also includes training to deal with off-nominal situations. Astronauts will also learn how to run the experiments that are specifically scheduled for their assigned missions.\n\nTraining for cosmonauts falls into three phases: General Space Training, Group Training, and Crew Training. General Space Training lasts about two years and consists of classes, survival training, and a final exam which determines whether a cosmonaut will be a test or research cosmonaut. The next year is devoted to Group Training where cosmonauts specialize in the Soyuz or ISS as well as professional skills. The final phases, the Crew Training phase, lasts a year and a half and is dedicated to detailed vehicle operations procedures, ISS training, and English language.\n\nTraining primarily takes place at the Yuri Gagarin Cosmonaut Training Center. The center facilities have full size mockups of all major Soviet and Russian spacecraft including the ISS. As with the ISS astronauts, cosmonauts train in the USA, Germany, Japan, and Canada for specific training in the various ISS modules.\n\nThe Japanese human spaceflight program has historically focused on training astronauts for Space Shuttle missions. As such, training previously took place at NASA’s Lyndon B. Johnson Space Center, and followed that of NASA astronauts and other international participants in the Space Shuttle program.\n\nSince the development of domestic training facilities at the Tsukuba Space Center, training has increasingly taken place in Japan. With Japan’s participation in the ISS, the training of Japanese astronauts follows a similar structure to that of other ISS partners. Astronauts carry out 1.5 years of Basic Training mainly at Tsukuba, followed by 1.5–2 years of Advanced Training at Tsukuba and ISS partner sites. Training for any international ISS astronauts involving the Kibo module will also be carried out at Tsukuba Space Center.\n\nAdvanced Training is followed by Increment-Specific Training, which, along with any Kibo training, will be carried out at Tsukuba. EVA training for Kibo takes place in the Weightless Environment Test System (WETS). WETS is a Neutral Buoyancy Facility featuring a full-scale mock-up of the Kibo module on the ISS. The Tsukuba Space Center also includes medical facilities for assessing suitability of candidates, an isolation chamber for simulating some of the mental and emotional stressors of long duration spaceflight, and a hypobaric chamber for training in hull breach or Life Support System failure scenarios resulting in a reduction or loss of air pressure.\n\nAlthough official detail of the selection process for the Shenzhou program is not available, what is known is that candidates are chosen by the Chinese National Space Administration from the Chinese air force and must be between 25 and 30 years of age, with a minimum of 800 hours flying time, and a degree-level education. Candidates must be between 160 cm and 172 cm in height, and between 50 kg and 70 kg in weight.\n\nFor China's Shenzhou astronauts, training begins with a year-long program of education in the basics of spaceflight. During this period, candidates are also introduced to human physiology and psychology. The second phase of training, lasting nearly 3 years involves extensive training in piloting the Shenzhou vehicle in nominal and emergency modes. The third and final stage of training is mission specific training, and lasts approximately 10 months. During this phase of training, astronauts are trained in the high fidelity Shenzhou trainer, as well as the Neutral Buoyancy Facility located at the Astronaut Center of China (ACC), in Beijing. As well as time spent in the Neutral Buoyancy Facility (NBF), training for EVA takes place in a high vacuum, low temperature chamber that simulates the environmental conditions of space. At all stages of training, astronauts undergo physical conditioning, including time in a human centrifuge located at the ACC, and a program of micro gravity flights, carried out in Russia.\n\nThe Indian human space flight program still awaits a formal go ahead. Once cleared the mission is expected to take two Indians in a Soyuz-type orbital vehicle into low earth orbit. The training for these astronauts should be based on the lessons learned from training India’s only Cosmonaut Wing Commander Rakesh Sharma (\"See Salyut-7 1984\") and through India’s international co-operation with NASA and Roscosmos. This would allow India to gain insights from their rich experiences in human spaceflight. There also lies a possibility that India may go proceed through its human spaceflight program individually, necessitating the Indian Space Research Organisation (ISRO) to develop its own training program. For astronaut training, India is deciding a place which is at a distance of 8 to 10 km from Kempegowda international airport. This land is under the ownership of ISRO. Astronaut training and biomedical engineering centers will be built on it. Though India’s first man mission training will take place in USA or in Russia, this place can be used for future training. Moreover, center will have chambers for radiation regulation, thermal cycling and centrifugal for the acceleration training. .\n\nWhile it is likely that the first generation of non-government spaceflight astronauts will perform suborbital trajectories, currently some companies like Virgin Galactic and Xcor Aerospace are developing their own proprietary suborbital astronaut training programs, however the first official Suborbital Astronaut Training program of the 21st century was a joint effort between the two government agencies, the Ecuadorian Air Force and the Gagarin Cosmonaut Training Center<ref name=\"FAE-ESAA ASA/T astronaut training:2005\">Ecuadorian Air Force Official document on ASA/T program. <http://www.exa.ec/bp8/FAE-ESAA.pdf></ref> developed the ASA/T (Advanced Suborbital Astronaut Training) program with a duration up to 16 months, it started in 2005 and completed in 2007 focusing in command and research duties during short duration missions with suborbital trajectories up to 180 kilometers. This program had one Ecuadorian citizen graduate in 2007,<ref name=\"FAE-ESAA ASA/T astronaut training:2007\">Ecuadorian Air Force presented with ASA/T program completion brief. <http://www.exa.ec/bp8/></ref><ref name=\"FAE-ESAA ASA/T astronaut training:2008\">Ecuadorian Air Force presented with space program brief. <http://www.exa.ec/bp9/></ref> recently the Ecuadorian Space Agency has made a call for a new class of ASA/T training candidates, accordingly to the EXA, they will focus on renting commercial suborbital vehicles in order to perform manned space research\n\nLooking ahead, the emergence of commercial space tourism will necessitate new standards for flight participants that currently do not exist. These standards will be to ensure that medical screenings are done properly in order to ensure safe and successful flights. This type of medical screening will differ from space agency astronaut selection and training because the goal is not to fly the highest performing individual, but merely to ensure a safe flight for the passengers under the rigors of space travel. The main considerations for this type of travel will be:\n\n\nMedical regulations for commercial space flight might mitigate commercial space company risk by selecting only those capable of passing standard medical criteria, as opposed to allowing anyone who can purchase a ticket to fly. The first generation of commercial space flight will likely be suborbital trajectories which invoke significant acceleration changes, causing cardiovascular and pulmonary issues. Because of this any future medical criteria for commercial spaceflight participants needs to focus specifically on the detrimental effects of rapidly changing gravitational levels, and which individuals will be capable of tolerating this.\n\nBioastronautics and upper-atmospheric research have been conducted by Project PoSSUM scientist-astronaut candidates since 2015. As of October 2018, the program has attracted members from 37 different countries and published research on mesospheric dynamics and human performance in space suits, microgravity, and post-landing environments. \n\nCurrent research on fitness training and strategies for commercial astronauts conducted by Astrowright Spaceflight Consulting, the first commercial firm to offer dedicated fitness training for space tourists, suggests that conventional fitness training is inadequate to support safe movement in microgravity, and that training utilizing reduced points of stability should be emphasized.\n\nAstronauts for long term missions such as those to the Moon, an asteroid, or even to Mars need to carry out multiple tasks and duties, because on such missions the astronauts will need to function largely autonomously, and will need to be proficient in many different areas. For these types of missions, the training to prepare astronauts will likely include training as doctors, scientists, engineers, technicians, pilots, and geologists. In addition there will be a focus on the psychological aspects of long-duration missions where crew is largely isolated.\n\nCurrently a six-month mission to the ISS requires up to five years of astronaut training. This level of training is to be expected and likely to be expanded upon for future space exploration missions. It may also include in-flight training aspects. It may be possible that the ISS will be used as a long-duration astronaut training facility in the future.\n\nA powerful tool for astronaut training will be the continuing use of analog environments, including NASA Extreme Environment Mission Operations (NOAA NEEMO), NASA's Desert Research and Technology Studies (Desert RATS), Envihab (planned), Flight Analog Research Unit, Haughton-Mars Project (HMP), or even the ISS (in-flight). In fact, at NEEMO a total of 15 mission astronauts (known as aquanauts) have been trained for future missions to asteroids. The use of virtual reality will also continue to be used as a means of training astronauts in a cost effective manner, particularly for operations such as extra-vehicular activity (EVA).\n\nThese missions are not completely independent without the presence of robots. This opens up a new avenue towards Human-Robot Interaction which has to be thoroughly understood and practised to develop a harmonious relationship between astronauts and robots. These robots would aid the astronauts from being their personal assistants to next generation of extreme environment explorers. Currently there is a robot on the ISS aiding the astronauts in their mammoth tasks with a human touch. Intercultural and human robot interaction training is the need of the hour for long duration missions.\n\nTraining also has to be evolved for moon landing to Manned mission to Mars. Factors like crew dynamics, crew size, and crew activities play a crucial role as these missions would last from 1 year to Moon to 3 years on Mars. The training required for such missions has to be versatile and easy to learn, adapt, and improvise.\n\n\n\n"}
{"id": "31695373", "url": "https://en.wikipedia.org/wiki?curid=31695373", "title": "Bathometer", "text": "Bathometer\n\nA bathometer (also bathymeter) is an instrument for measuring water depth. It was previously used mainly in oceanographical studies, but is rarely employed nowadays. The term originates from Greek \"βαθύς\" (\"bathys\"), \"deep\" and \"μέτρον\" (\"métron\"), \"measure\".\n\n"}
{"id": "3428791", "url": "https://en.wikipedia.org/wiki?curid=3428791", "title": "Blue hour", "text": "Blue hour\n\nThe blue hour (from French ) is the period of twilight in the morning or evening, during the civil and nautical stages, when the Sun is at a significant depth below the horizon and residual, indirect sunlight takes on a predominantly blue shade.\n\nWhen the sky is clear, the blue hour can be a colorful spectacle, with the indirect sunlight tinting the sky yellow, orange, red, and blue. This effect is caused by the relative diffusibility of shorter wavelengths (bluer rays) of visible light versus the longer wavelengths (redder rays). During the blue \"hour\", which typically lasts roughly 20 minutes (depending on latitude), red light passes through space while blue light is scattered in the atmosphere, and thus reaches Earth's surface.\n\nMany artists value this period for the quality of the soft light. Although the blue hour does not have an official definition, the blue color spectrum is most prominent when the Sun is between 4° and 8° below the horizon.\n\n\n"}
{"id": "20948", "url": "https://en.wikipedia.org/wiki?curid=20948", "title": "Brainwashing", "text": "Brainwashing\n\nBrainwashing (also known as mind control, menticide, coercive persuasion, thought control, thought reform, and re-education) is the concept that the human mind can be altered or controlled by certain psychological techniques. Brainwashing is said to reduce its subject’s ability to think critically or independently, to allow the introduction of new, unwanted thoughts and ideas into the subject’s mind, as well as to change his or her attitudes, values, and beliefs. \n\nThe concept of brainwashing was originally developed in the 1950s to explain how the Chinese government appeared to make people cooperate with them. Advocates of the concept also looked at Nazi Germany, at some criminal cases in the United States, and at the actions of human traffickers. It was later applied by Margaret Singer, Philip Zimbardo and some others in the anti-cult movement to explain conversions to some new religious movements and other groups. This resulted in scientific and legal debate with Eileen Barker, James Richardson, and other scholars, as well as legal experts, rejecting at least the popular understanding of brainwashing.\n\nOther views have been expressed by scholars including: Dick Anthony, Robert Cialdini, Stanley A. Deetz, Michael J. Freeman, Robert Jay Lifton, Joost Meerloo, Daniel Romanovsky, Kathleen Taylor, Louis Jolyon West, and Benjamin Zablocki. The concept of brainwashing is sometimes involved in legal cases, especially regarding child custody; and is also a major theme in science fiction and in criticism of modern political and corporate culture. Although the term appears in the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5) of the American Psychiatric Association it is not accepted as scientific fact.\n\nThe Chinese term \"xǐnăo\" (洗脑，literally \"wash brain\") was originally used to describe the coercive persuasion used under the Maoist government in China, which aimed to transform \"reactionary\" people into \"right-thinking\" members of the new Chinese social system. The term punned on the Taoist custom of \"cleansing/washing the heart/mind\" (\"xǐxīn\"，洗心) before conducting ceremonies or entering holy places.\n\nThe \"Oxford English Dictionary\" records the earliest known English-language usage of the word \"brainwashing\" in an article by newspaperman Edward Hunter, in \"Miami News\", published on 24 September 1950. Hunter was an outspoken anticommunist and was said to be a CIA agent working undercover as a journalist. Hunter and others used the Chinese term to explain why, during the Korean War (1950-1953), some American prisoners of war (POWs) cooperated with their Chinese captors, even in a few cases defected to their side. British radio operator Robert W. Ford and British army Colonel James Carne also claimed that the Chinese subjected them to brainwashing techniques during their war-era imprisonment.\n\nThe U.S. military and government laid charges of brainwashing in an effort to undermine confessions made by POWs to war crimes, including biological warfare. After Chinese radio broadcasts claimed to quote Frank Schwable, Chief of Staff of the First Marine Air Wing admitting to participating in germ warfare, United Nations commander Gen. Mark W. Clark asserted:\n\nBeginning in 1953, Robert Jay Lifton interviewed American servicemen who had been POWs during the Korean War as well as priests, students, and teachers who had been held in prison in China after 1951. In addition to interviews with 25 Americans and Europeans, Lifton interviewed 15 Chinese citizens who had fled after having been subjected to indoctrination in Chinese universities. (Lifton's 1961 book \"\", was based on this research.) Lifton found that when the POWs returned to the United States their thinking soon returned to normal, contrary to the popular image of \"brainwashing.\"\n\nIn 1956, after reexamining the concept of brainwashing following the Korean War, the U.S. Army published a report entitled \"Communist Interrogation, Indoctrination, and Exploitation of Prisoners of War\", which called brainwashing a \"popular misconception\". The report states \"exhaustive research of several government agencies failed to reveal even one conclusively documented case of 'brainwashing' of an American prisoner of war in Korea.\"\n\nIn George Orwell's 1949 dystopian novel \"Nineteen Eighty-Four\" the main character is subjected to imprisonment, isolation, and torture in order to conform his thoughts and emotions to the wishes of the rulers of Orwell's fictional future totalitarian society. Orwell's vision influenced Hunter and is still reflected in the popular understanding of the concept of brainwashing. Written about the same time, J.R.R. Tolkien’s \"The Lord of the Rings\" also addressed brainwashing, although in a fantasy setting. The science fiction stories of Cordwainer Smith (written from the 1940s until his death in 1966) depict brainwashing to remove memories of traumatic events as a normal and benign part of future medical practice.\n\nIn the 1950s many American films were filmed that featured brainwashing of POWs, including \"The Rack\", \"The Bamboo Prison\", \"Toward the Unknown\", and \"The Fearmakers\". \"Forbidden Area\" told the story of Soviet secret agents who had been brainwashed through classical conditioning by their own government so they wouldn't reveal their identities. In 1962 \"The Manchurian Candidate\" (based on the 1959 novel by Richard Condon) \"put brainwashing front and center\" by featuring a plot by the Soviet government to take over the United States by use of a brainwashed presidential candidate. The concept of brainwashing became popularly associated with the research of Russian psychologist Ivan Pavlov, which mostly involved dogs, not humans, as subjects. In \"The Manchurian Candidate\" the head brainwasher is Dr. Yen Lo, of the Pavlov Institute.\n\nMind control remains an important theme in science fiction. Terry O'Brien comments: \"Mind control is such a powerful image that if hypnotism did not exist, then something similar would have to have been invented: the plot device is too useful for any writer to ignore. The fear of mind control is equally as powerful an image.\" A subgenre is \"corporate mind control\", in which a future society is run by one or more business corporations that dominate society using advertising and mass media to control the population's thoughts and feelings.\n\nFor twenty years starting in the early 1950s, the United States Central Intelligence Agency (CIA) and the United States Department of Defense conducted secret research, including Project MKUltra, in an attempt to develop practical brainwashing techniques; the results are unknown. (See also Sidney Gottlieb.) CIA experiments using various psychedelic drugs such as LSD and Mescaline drew from Nazi human experimentation.\n\nIn 1974, Patty Hearst, a member of the wealthy Hearst family, was kidnapped by a left-wing group calling itself the Symbionese Liberation Army. After several weeks of captivity she agreed to join the group and took part in their activities. In 1975, she was arrested and charged with bank robbery and use of a gun in committing a felony. Her attorney, F. Lee Bailey argued in her trial that she should not be held responsible for her actions since her treatment by her captors was the equivalent of the brainwashing of Korean War POWs. (See: diminished responsibility.) Hearst was found guilty, but her “brainwashing defense” brought the topic to renewed public attention in the United States, as did the 1969 to 1971 case of Charles Manson, who was said to have brainwashed his followers to commit murder and other crimes.\n\nBailey developed his case in conjunction with psychiatrist Louis Jolyon West and psychologist Margaret Singer. They had both studied the experiences of Korean War POWs. In 1996 Singer published her theories in her best-selling book \"Cults in Our Midst\". In 2003, the brainwashing defense was used unsuccessfully in the defense of Lee Boyd Malvo, who was charged with murder for his part in the D.C. sniper attacks. Some legal scholars have argued that the brainwashing defense undermines the law’s fundamental premise of free will.\n\nItaly has had controversy over the concept of \"plagio\", a crime consisting in an absolute psychological—and eventually physical—domination of a person. The effect is said to be the annihilation of the subject's freedom and self-determination and the consequent negation of his or her personality. The crime of plagio has rarely been prosecuted in Italy, and only one person was ever convicted. In 1981, an Italian court found that the concept is imprecise, lacks coherence, and is liable to arbitrary application. By the twenty-first century, the concept of brainwashing was being applied \"with some success\" in child custody and child sexual abuse cases. In some cases \"one parent is accused of brainwashing the child to reject the other parent, and in child sex abuse cases where one parent is accused of brainwashing the child to make sex abuse accusations against the other parent\" (possibly resulting in or causing parental alienation).\n\nIn 2003, forensic psychologist Dick Anthony said that \"no reasonable person would question that there are situations where people can be influenced against their best interests, but those arguments are evaluated on the basis of fact, not bogus expert testimony.\" In 2016, Israeli anthropologist of religion and fellow at the Van Leer Jerusalem Institute Adam Klin-Oron said about then-proposed \"anti-cult\" legislation: \n\nIn the 1970s, the anti-cult movement applied the concept of brainwashing to explain seemingly sudden and dramatic religious conversions to various new religious movements (NRMs) and other groups they considered cults. News media reports tended to support the brainwashing view and social scientists sympathetic to the anti-cult movement, who were usually psychologists, developed revised models of mind control. While some psychologists were receptive to the concept, sociologists were for the most part skeptical of its ability to explain conversion to NRMs.\n\nPhilip Zimbardo defined mind control as, \"the process by which individual or collective freedom of choice and action is compromised by agents or agencies that modify or distort perception, motivation, affect, cognition or behavioral outcomes,\" and he suggested that any human being is susceptible to such manipulation. Another adherent to this view, Jean-Marie Abgrall was heavily criticized by forensic psychologist Dick Anthony for employing a pseudo-scientific approach and lacking any evidence that anyone's worldview was substantially changed by these coercive methods. On the contrary, the concept and the fear surrounding it was used as a tool for the anti-cult movement to rationalize the persecution of minority religious groups.\n\nEileen Barker criticized the concept of mind control because it functioned to justify costly interventions such as deprogramming or exit counseling. She has also criticized some mental health professionals, including Singer, for accepting expert witness jobs in court cases involving NRMs. Her 1984 book, \"\" describes the religious conversion process to the Unification Church (whose members are sometimes informally referred to as \"Moonies\"), which had been one of the best known groups said to practice brainwashing. Barker spent close to seven years studying Unification Church members. She interviewed in depth or gave probing questionnaires to church members, ex-members, \"non-joiners,\" and control groups of uninvolved people from similar backgrounds, as well as parents, spouses, and friends of members. She also attended numerous church workshops and communal facilities. Barker writes that she rejects the \"brainwashing\" theory, because it explains neither the many people who attended a recruitment meeting and did not become members, nor the voluntary disaffiliation of members.\n\nJames Richardson observed that if the new religious movements had access to powerful brainwashing techniques, one would expect that they would have high growth rates, yet in fact most have not had notable success in recruitment. Most adherents participate for only a short time, and the success in retaining members is limited. For this and other reasons, sociologists of religion including David Bromley and Anson Shupe consider the idea that \"cults\" are brainwashing American youth to be \"implausible.\" In addition, Thomas Robbins, Massimo Introvigne, Lorne Dawson, Gordon Melton, Marc Galanter, and Saul Levine, amongst other scholars researching NRMs, have argued and established to the satisfaction of courts, relevant professional associations and scientific communities that there exists no generally accepted scientific theory, based upon methodologically sound research, that supports the concept of brainwashing as advanced by the anti-cult movement.\n\nBenjamin Zablocki responded that brainwashing is not \"a process that is directly observable,\" and that the \"real sociological issue\" is whether \"brainwashing occurs frequently enough to be considered an important social problem\", and that Richardson misunderstands brainwashing, conceiving of it as a recruiting process, instead of a retaining process, and that the number of people who attest to brainwashing in interviews (performed in accordance with guidelines of the National Institute of Mental Health and National Science Foundation) is too large result from anything other than a genuine phenomenon. Zablocki also pointed out that in the two most prestigious journals dedicated to the sociology of religion there have been no articles \"supporting the brainwashing perspective,\" while over one hundred such articles have been published in other journals \"marginal to the field.\" He concludes that the concept of brainwashing has been unfairly blacklisted.\n\nIn 1983, the American Psychological Association (APA) asked Singer to chair a taskforce called the APA Task Force on Deceptive and Indirect Techniques of Persuasion and Control (DIMPAC) to investigate whether brainwashing or coercive persuasion did indeed play a role in recruitment by NRMs.\n\"Cults and large group awareness trainings have generated considerable controversy because of their widespread use of deceptive and indirect techniques of persuasion and control. These techniques can compromise individual freedom, and their use has resulted in serious harm to thousands of individuals and families. This report reviews the literature on this subject, proposes a new way of conceptualizing influence techniques, explores the ethical ramifications of deceptive and indirect techniques of persuasion and control, and makes recommendations addressing the problems described in the report.\"\nOn 11 May 1987, the APA's Board of Social and Ethical Responsibility for Psychology (BSERP) rejected the DIMPAC report because the report \"lacks the scientific rigor and evenhanded critical approach necessary for APA imprimatur\", and concluded that \"after much consideration, BSERP does not believe that we have sufficient information available to guide us in taking a position on this issue.\"\n\nKathleen Barry, co-founder of the United Nations NGO, the Coalition Against Trafficking in Women (CATW), in her 1979 book \"Female Sexual Slavery\" prompted international awareness of human sex trafficking. In his 1986 book \"Woman Abuse: Facts Replacing Myths\" Lewis Okun reported that: “Kathleen Barry shows in Female Sexual Slavery that forced female prostitution involves coercive control practices very similar to thought reform.” In their 1996 book, \"Casting Stones: Prostitution and Liberation in Asia and the United States\", Rita Nakashima Brock and Susan Brooks Thistlethwaite report that the methods commonly used by pimps to control their victims \"closely resemble the brainwashing techniques of terrorists and paranoid cults.\"\n\nSome of the techniques used by traffickers include feigning love and concern for the victims' well-being to gain trust before beginning to track, manipulate and control the entire life of the victim, including environment, relationships, access to information and daily activities, promises of lucrative employment or corrupt marriage proposals, debt bondage, kidnapping, induced drug dependency and fear tactics such as threats about law enforcement, deportation, and harm to friends or family members. Physical captivity, shame, Stockholm Syndrome, traumatic bonding and fear of arrest can contribute to victims’ inability to seek assistance.\n\nRussian historian Daniel Romanovsky, who interviewed survivors and eyewitnesses in the 1970s, reported on what he called \"Nazi brainwashing\" of the people of Belarus by the occupying Germans during the Second World War, which took place through both mass propaganda and intense re-education, especially in schools. Romanovsky noted that very soon most people had adopted the Nazi view that the Jews were an inferior race and were closely tied to the Soviet government, views that had not been at all common before the German occupation.\n\nJoost Meerloo, a Dutch psychiatrist, was an early proponent of the concept of brainwashing. (\"Menticide\" is a neologism coined by him meaning: \"killing of the mind.\") Meerloo's view was influenced by his experiences during the German occupation of his country and his work with the Dutch government and the American military in the interrogation of accused Nazi war criminals. He later emigrated to the United States and taught at Columbia University. His best-selling 1956 book, \"The Rape of the Mind\", concludes by saying: \n\nScholars have said that modern business corporations practice mind control to create a work force that shares common values and culture. Critics have linked \"corporate brainwashing\" with globalization, saying that corporations are attempting to create a worldwide monocultural network of producers, consumers, and managers. Modern educational systems have also been criticized, by both the left and the right, for contributing to corporate brainwashing. In his 1992 book, \"Democracy in an Age of Corporate Colonization\", Stanley A. Deetz says that modern \"self awareness\" and \"self improvement\" programs provide corporations with even more effective tools to control the minds of employees than traditional brainwashing.\n\nIn his 2000 book, \"Destroying the World to Save It: Aum Shinrikyo, Apocalyptic Violence, and the New Global Terrorism\", Robert Lifton applied his original ideas about thought reform to Aum Shinrikyo and the War on Terrorism, concluding that in this context thought reform was possible without violence or physical coercion. He also pointed out that in their efforts against terrorism Western governments were also using some mind control techniques, including thought-terminating clichés.\n\nIn her 2004 popular science book, \"\", neuroscientist and physiologist Kathleen Taylor reviewed the history of mind control theories, as well as notable incidents. She suggests that persons under its influence have more rigid neurological pathways, and that can make it more difficult to rethink situations or be able to later reorganize these pathways. Reviewers praised her book for its clear presentation, while some criticized it for oversimplification.\n\n"}
{"id": "304471", "url": "https://en.wikipedia.org/wiki?curid=304471", "title": "Case study", "text": "Case study\n\nIn the social sciences and life sciences, a case study is a research method involving an up-close, in-depth, and detailed examination of a subject of study (the case), as well as its related contextual conditions.\n\nCase studies can be produced by following a formal research method. These case studies are likely to appear in formal research venues, as journals and professional conferences, rather than popular works. The resulting body of 'case study research' has long had a prominent place in many disciplines and professions, ranging from psychology, anthropology, sociology, and political science to education, clinical science, social work, and administrative science.\n\nIn doing case study research, the \"case\" being studied may be an individual, organization, event, or action, existing in a specific time and place. For instance, clinical science has produced both well-known case studies of individuals and also case studies of clinical practices. However, when \"case\" is used in an abstract sense, as in a claim, a proposition, or an argument, such a case can be the subject of many research methods, not just case study research.\n\nAnother suggestion is that \"case study\" should be defined as a \"research strategy\", an empirical inquiry that investigates a phenomenon within its real-life context. Case study research can mean single and multiple case studies, can include quantitative evidence, relies on multiple sources of evidence, and benefits from the prior development of theoretical propositions. Case studies should not be confused with qualitative research and they can be based on any mix of quantitative and qualitative evidence.Single-subject research provides the statistical framework for making inferences from quantitative case-study data. \n\nCase studies may involve both qualitative and quantitative research methods.\n\nIn business research, four common case study approaches are distinguished. First, there is the \"no theory first\" type of case study design, which is closely connected to Kathleen M. Eisenhardt's methodological work. The second type of research design is about \"gaps and holes\", following Robert K. Yin's guidelines and making positivist assumptions. A third design deals with a \"social construction of reality\", represented by the work of Robert E. Stake. Finally, the reason for case study research can also be to identify \"anomalies\"; a representative scholar of this approach is Michael Burawoy. Each of these four approaches has its areas of application, but it is important to understand their unique ontological and epistomological assumptions. There are substantial methodological differences between these approaches.\n\nAn average, or typical case, is often not the richest in information. In clarifying lines of history and causation it is more useful to select subjects that offer an interesting, unusual or particularly revealing set of circumstances. A case selection that is based on representativeness will seldom be able to produce these kinds of insights. When selecting a case for a case study, researchers will therefore use information-oriented sampling, as opposed to random sampling. Outlier cases (that is, those which are extreme, deviant or atypical) reveal more information than the potentially representative case, as seen in cases selected for more qualitative safety scientific analyses of accidents. A case may be chosen because of the inherent interest of the case or the circumstances surrounding it. Alternatively it may be chosen because of researchers' in-depth local knowledge; where researchers have this local knowledge they are in a position to \"soak and poke\" as Richard Fenno put it, and thereby to offer reasoned lines of explanation based on this rich knowledge of setting and circumstances.\n\nThree types of cases may thus be distinguished for selection:\n\n\nWhatever the frame of reference for the choice of the subject of the case study (key, outlier, local knowledge), there is a distinction to be made between the subject and the object of the case study. The subject is the \"practical, historical unity\" through which the theoretical focus of the study is being viewed. The object is that theoretical focus – the analytical frame. Thus, for example, if a researcher were interested in US resistance to communist expansion as a theoretical focus, then the Korean War might be taken to be the \"subject\", the lens, the case study through which the theoretical focus, the \"object\", could be viewed and explicated.\n\nBeyond decisions about case selection and the subject and object of the study, decisions need to be made about purpose, approach and process in the case study. Gary Thomas thus proposes a typology for the case study wherein purposes are first identified (evaluative or exploratory), then approaches are delineated (theory-testing, theory-building or illustrative), then processes are decided upon, with a principal choice being between whether the study is to be single or multiple, and choices also about whether the study is to be retrospective, snapshot or diachronic, and whether it is nested, parallel or sequential. It is thus possible to take many routes through this typology, with, for example, an exploratory, theory-building, multiple, nested study, or an evaluative, theory-testing, single, retrospective study. The typology thus offers many permutations for case-study structure.\n\nA closely related study in medicine is the case report, which identifies a specific case as treated and/or examined by the authors as presented in a novel form. These are, to a differentiable degree, similar to the case study in that many contain reviews of the relevant literature of the topic discussed in the thorough examination of an array of cases published to fit the criterion of the report being presented. These case reports can be thought of as brief case studies with a principal discussion of the new, presented case at hand that presents a novel interest.\n\nSome issues are usually realised in a situation where marketing is concerned. One must, therefore, ensure that he/she can fully understand these things. In a case where the market of any organisation is in a messy state, the agency will always seek to find out some of the reasons why the scenario is that way. They will have to gather information that may help them in solving such issues. For this to be fully achieved, one must be able to carry out a market research to establish where the problem is. This, therefore, calls for the different methods which can be used in a situation where one wants to conduct a marketing research. Some ways can be used to come up with the purpose of study that is most appropriate. The organisations have to choose one of the available techniques so that they can thoroughly conduct their investigations. Some of the primary methods that would be used included interviews, surveys, focus groups, observations and in some cases use field trials. These methods mainly depended on the amount of cash the organisation is willing to spend in having this market research done and also the kind of data that is required by the group.\n\nIn public-relations research, three types of case studies are used:\n\n\nUnder the more generalized category of case study exist several subdivisions, each of which is custom selected for use depending upon the goals of the investigator. These types of case study include the following:\n\n\nAt Harvard Law School In 1870, Christopher Langdell departed from the traditional lecture-and-notes approach to teaching contract law and began using cases pled before courts as the basis for class discussions. By 1920, this practice had become the dominant pedagogical approach used by law schools in the United States; it also was adopted by Harvard Business School.\n\nResearch in business disciplines is usually based on a positivist epistemology, namely, that reality is something that is objective and can be discovered and understood by a scientific examination of empirical evidence. But organizational behavior cannot always be easily reduced to simple tests that prove something to be true or false. Reality may be an objective thing, but it is understood and interpreted by people who, in turn, act upon it, and so critical realism, which addresses the connection between the natural and social worlds, is a useful basis for analyzing the environment of and events within an organization.\n\nCase studies in management are generally used to interpret strategies or relationships, to develop sets of \"best practices\", or to analyze the external influences or the internal interactions of a firm. With several notable exceptions (e.g., Janis on Groupthink), they are rarely used to propose new theories.\n\nA critical case is defined as having strategic importance in relation to the general problem. A critical case allows the following type of generalization: \"If it is valid for this case, it is valid for all (or many) cases.\" In its negative form, the generalization would run: \"If it is not valid for this case, then it is not valid for any (or valid for only few) cases.\"\n\nThe case study is effective for generalizing using the type of test that Karl Popper called falsification, which forms part of critical reflexivity. Falsification offers one of the most rigorous tests to which a scientific proposition can be subjected: if just one observation does not fit with the proposition it is considered not valid generally and must therefore be either revised or rejected. Popper himself used the now famous example: \"All swans are white\", and proposed that just one observation of a single black swan would falsify this proposition and in this way have general significance and stimulate further investigations and theory-building. The case study is well suited for identifying \"black swans\" because of its in-depth approach: what appears to be \"white\" often turns out on closer examination to be \"black\".\n\nGalileo Galilei built his rejection of Aristotle's law of gravity on a case study selected by information-oriented sampling and not by random sampling. The rejection consisted primarily of a conceptual experiment and later on a practical one. These experiments, with the benefit of hindsight, seem self-evident. Nevertheless, Aristotle's incorrect view of gravity had dominated scientific inquiry for nearly two thousand years before it was falsified. In his experimental thinking, Galileo reasoned as follows: if two objects with the same weight are released from the same height at the same time, they will hit the ground simultaneously, having fallen at the same speed. If the two objects are then stuck together into one, this object will have double the weight and will according to the Aristotelian view therefore fall faster than the two individual objects. This conclusion seemed contradictory to Galileo. The only way to avoid the contradiction was to eliminate weight as a determinant factor for acceleration in free fall. Galileo’s experimentalism did not involve a large random sample of trials of objects falling from a wide range of randomly selected heights under varying wind conditions, and so on. Rather, it was a matter of a single experiment, that is, a case study.\n\nGalileo’s view continued to be subjected to doubt, however, and the Aristotelian view was not finally rejected until half a century later, with the invention of the air pump. The air pump made it possible to conduct the ultimate experiment, known by every pupil, whereby a coin or a piece of lead inside a vacuum tube falls with the same speed as a feather. After this experiment, Aristotle’s view could be maintained no longer. What is especially worth noting, however, is that the matter was settled by an individual case due to the clever choice of the extremes of metal and feather. One might call it a critical case, for if Galileo’s thesis held for these materials, it could be expected to be valid for all or a large range of materials. Random and large samples were at no time part of the picture. However it was Galileo's view that was the subject of doubt as it was not reasonable enough to be the Aristotelian view. By selecting cases strategically in this manner one may arrive at case studies that allow generalization.\n\nIt is generally believed that Frederic Le Play first introduced the case-study method into social science in 1829 as a handmaiden to statistics in his studies of family budgets.\n\nOther roots stem from the early 20th century, when researchers working in the disciplines of sociology, psychology, and anthropology began making case studies. In all these disciplines, case studies were an occasion for postulating new theories, as in the grounded-theory work of sociologists Barney Glaser (1930- ) and Anselm Strauss (1916-1996).\n\nThe popularity of case studies in testing theories or hypotheses has developed only in recent decades. One of the areas in which case studies have been gaining popularity is education and in particular educational evaluation.\n\nEducators have used case studies as a teaching method and as part of professional development, especially in business and legal education. The problem-based learning (PBL) movement offers an example. When used in (non-business) education and professional development, case studies are often referred to as \"critical incidents\".\n\nEthnography exemplifies a type of case study, commonly found in communication case studies. Ethnography is the description, interpretation, and analysis of a culture or social group, through field research in the natural environment of the group being studied. The main method of ethnographic research is thorough observation, where the researcher observes study participants over an extended period of time within the participants' own environment.\n\nComparative case studies have become more popular in social science, policy, and education research. One approach encourages researchers to compare horizontally, vertically, and temporally.\n\nUsing case studies in research differs from their use in teaching, where they are commonly called case methods and casebook methods. Teaching case studies have been a highly popular pedagogical format in many fields ranging from business education to science education. Harvard Business School has been among the most prominent developers and users of teaching case studies. Business school faculty generally develop case studies with particular learning objectives in mind. Additional relevant documentation, such as financial statements, time-lines, and short biographies, often referred to in the case study as exhibits, and multimedia supplements (such as video-recordings of interviews with the case subject) often accompany the case studies. Similarly, teaching case studies have become increasingly popular in science education. The National Center for Case Studies in Teaching Science has made a growing body of case studies available for classroom use, for university as well as secondary school coursework. Nevertheless, the principles involved in doing case study research contrast with those involved in doing case studies for teaching. Teaching case studies need not adhere strictly to the use of evidence, as they can be manipulated to satisfy educational needs. The generalizations from teaching case studies also may relate to pedagogical issues rather than the substance of the case being studied.\n\nCase studies are commonly used in case competitions and in job interviews for consulting firms such as McKinsey & Company, CEB Inc. and the Boston Consulting Group, in which candidates are asked to develop the best solution for a case in an allotted time frame.\n\n\n\n"}
{"id": "2259059", "url": "https://en.wikipedia.org/wiki?curid=2259059", "title": "Chronometry", "text": "Chronometry\n\nChronometry (from Greek χρόνος \"chronos\", \"time\" and μέτρον \"metron\", \"measure\") is the science of the measurement of time, or timekeeping. Chronometry applies to electronic devices, while horology refers to mechanical devices.\n\nIt should not to be confused with chronology, the science of locating events in time, which often relies upon it.\n\n"}
{"id": "33615960", "url": "https://en.wikipedia.org/wiki?curid=33615960", "title": "Comparison of chemistry and physics", "text": "Comparison of chemistry and physics\n\nChemistry and physics are branches of science that both study matter. The difference between the two lies in their scope and approach. Chemists and physicists are trained differently, and they have different professional roles, even when working in a team. The division between chemistry and physics becomes diffuse at the interface of the two branches, notably in fields such as physical chemistry, chemical physics, quantum mechanics, nuclear physics/chemistry, materials science, spectroscopy, solid state physics, solid-state chemistry, crystallography, and nanotechnology.\n\nPhysics and chemistry may overlap when the system under study involves matter composed of electrons and nuclei made of protons and neutrons. On the other hand, chemistry is not concerned with other forms of matter such as quarks, mu and tau leptons and dark matter.\n\nAlthough fundamental laws that govern the behavior of matter apply both in chemistry and physics, the disciplines of physics and chemistry are distinct. Physics is concerned with nature from a very large scale (the entire universe) down to a very small scale (subatomic particles). All physical phenomena that are measurable follow some behavior that is in accordance with the most basic principles studied in physics.\n\nPhysics is involved with the fundamental principles of physical phenomena and the basic forces of nature, and also gives insight into the aspects of space and time. Physics also deals with the basic principles that explain matter and energy, and may study aspects of atomic matter by following concepts derived from the most fundamental principles.\n\nChemistry focuses on how substances interact with each other and with energy (for example heat and light). The study of change of matter (chemical reactions) and synthesis lies at the heart of chemistry, and gives rise to concepts such as organic functional groups and rate laws for chemical reactions. Chemistry also studies the properties of matter at a larger scale (for example, astrochemistry) and the reactions of matter at a larger scale (for example, technical chemistry), but typically, explanations and predictions are related back to the underlying atomic structure, giving more emphasis on the methods for the identification of molecules and their mechanisms of transformation than any other science.\n\nAlthough both physics and chemistry are concerned with matter and its interaction with energy, the two disciplines differ in approach. In physics, it is typical to abstract from the specific type of matter, and to focus on the common properties of many different materials. In optics, for example, materials are characterized by their index of refraction, and materials with the same index of refraction will have identical properties. Chemistry, on the other hand, focuses on what compounds are present in a sample, and explores how changing the structure of molecules will change their reactivity and their physical properties.\n\nThe two sciences differ in the role that theory plays within the discipline. Physics can be divided into experimental and theoretical physics. Historically, theoretical physics has correctly predicted phenomena that were out of experimental reach at the time, and could be verified only after experimental techniques caught up. In chemistry, the role of theory historically has been a retrospective one, summarizing experimental data and predicting the outcome of similar experiments. However, with the increasing power of computational methods in chemistry, it has become possible to predict whether a hypothetical compound is stable or not before experimental data is available.\n\nIn a typical undergraduate program for physics majors, required courses are in the sub-disciplines of physics, with additional required courses in mathematics. Because much of the insight of physics is described by differential equations relating matter, space, and time (for example Newton's law of motion and the Maxwell equations of electromagnetism), students have to be familiar with differential equations. In a typical undergraduate program for chemistry majors, emphasis is placed on laboratory classes and understanding and applying models describing chemical bonds and molecular structure. Emphasis is also placed in the methods for analysis and the formulas and equations used when considering the chemical transformation. Students take courses in math, physics, chemistry, and often biochemistry. Between the two programs of study, there is a large area of overlap (calculus, introductory physics, quantum mechanics, thermodynamics). However, physics places a larger emphasis on fundamental theory (with its deep mathematical treatment) while chemistry places more emphasis in combining the most important mathematical definitions of the theory with the approach of the molecular models. Laboratory skills may differ in both programs, as students may be involved in different technologies, depending on the program and the institution of higher education (for example, a chemistry student may spend more laboratory time dealing with glassware for distillation and purification or on a form of chromatography-spectroscopy instrument, while a physics student may spend much more time dealing with a laser and non-linear optics technology or some complex electrical circuit).\n\nAccording to Bureau of Labor Statistics (United States Department of Labor), there are 80,000 chemists and 17,000 physicists working in the United States as of May 2010. In addition, 21,000 chemists and 13,500 physicists teach in high school. Chemistry is the only science that has an entire industry, the chemical industry, named after it, and many chemists work in this industry, in research and development, production, training, or management. Other industries employing chemists include the petroleum, pharmaceutical, and food industry. While there is no industry named after physics, many industries have grown out of physics research, most notably the semiconductor and electronics industry. Physicists are also employed outside of science, for example in finance, because of their training in modeling complex systems.\n\nChemistry and physics are not strictly separated sciences, and chemists and physicists work in interdisciplinary teams to explore the following topics.\n"}
{"id": "47754883", "url": "https://en.wikipedia.org/wiki?curid=47754883", "title": "Conical refiner", "text": "Conical refiner\n\nThe conical refiner is a machine used in the refining of pulp in the papermaking process. It may also be referred to as a Jordan refiner, after the American inventor Joseph Jordan who patented the device in 1858.\n\nThe conical refiner is a chamber with metal bars mounted around the inside of the container. The material to be refined is pumped into the chamber at high-pressure rate in order to create an abrasive effect as the material is forced through the machine, abraided by the metal bars. At the opposite end of the chamber the resulting pulp is pumped out.\n"}
{"id": "5326", "url": "https://en.wikipedia.org/wiki?curid=5326", "title": "Creationism", "text": "Creationism\n\nCreationism is the religious belief that the universe and life originated \"from specific acts of divine creation\", as opposed to the scientific conclusion that they came about through natural processes. Creationism covers a spectrum of views including \"evolutionary creationism\", a theological variant of theistic evolution which asserts that both evolutionary science and a belief in creation are true, but the term is commonly used for literal creationists who reject various aspects of science, and instead promote pseudoscientific beliefs.\n\nLiteral creationists base their beliefs on a fundamentalist reading of religious texts, including the creation myths found in Genesis and the Quran. For young Earth creationists, these beliefs are based on a literalist interpretation of the Genesis creation narrative and rejection of the scientific theory of evolution. Literalist creationists believe that evolution cannot adequately account for the history, diversity, and complexity of life on Earth. Pseudoscientific branches of creationism include creation science, flood geology, and intelligent design, as well as subsets of pseudoarchaeology, pseudohistory, and pseudolinguistics.\n\nThe first use of the term \"creationist\" to describe a proponent of creationism is found in an 1856 letter of Charles Darwin describing those who objected on religious grounds to the then emerging science of evolution.\n\nThe basis for many creationists' beliefs is a literal or quasi-literal interpretation of the Old Testament, especially from stories from the book of Genesis:\n\n\nA further important element is the interpretation of the Biblical chronology, the elaborate system of life-spans, \"generations,\" and other means by which the Bible measures the passage of events from the creation (Genesis 1:1) to the Book of Daniel, the last biblical book in which it appears. Recent decades have seen attempts to de-link creationism from the Bible and recast it as science; these include creation science and intelligent design. There are also non-Christian forms of creationism, notably Islamic creationism and Hindu creationism.\n\nSeveral attempts have been made to categorize the different types of creationism, and create a \"taxonomy\" of creationists. Creationism (broadly construed) covers a spectrum of beliefs which have been categorized into the general types listed below.\n\nYoung Earth creationists such as Ken Ham and Doug Phillips believe that God created the Earth within the last ten thousand years, literally as described in the Genesis creation narrative, within the approximate time-frame of biblical genealogies (detailed for example in the Ussher chronology). Most young Earth creationists believe that the universe has a similar age as the Earth. A few assign a much older age to the universe than to Earth. Creationist cosmologies give the universe an age consistent with the Ussher chronology and other young Earth time frames. Other young Earth creationists believe that the Earth and the universe were created with the appearance of age, so that the world appears to be much older than it is, and that this appearance is what gives the geological findings and other methods of dating the Earth and the universe their much longer timelines.\n\nThe Christian organizations Institute for Creation Research (ICR) and the Creation Research Society (CRS) both promote young Earth creationism in the US. Another organization with similar views, Answers in Genesis (AiG)—based in both the U.S. and the United Kingdom—has opened the Creation Museum in Petersburg, Kentucky, to promote young Earth creationism. Creation Ministries International promotes young Earth views in Australia, Canada, South Africa, New Zealand, the US, and the UK. Among Roman Catholics, the Kolbe Center for the Study of Creation promotes similar ideas. In 2007, Ken Ham founded the Creation Museum and Ark Encounter in northern Kentucky.\n\nOld Earth creationism holds that the physical universe was created by God, but that the creation event described in the Book of Genesis is to be taken figuratively. This group generally believes that the age of the universe and the age of the Earth are as described by astronomers and geologists, but that details of modern evolutionary theory are questionable.\n\nOld Earth creationism itself comes in at least three types:\n\nGap creationism, also called \"restoration creationism,\" holds that life was recently created on a pre-existing old Earth. This version of creationism relies on a particular interpretation of . It is considered that the words \"formless\" and \"void\" in fact denote waste and ruin, taking into account the original Hebrew and other places these words are used in the Old Testament. Genesis 1:1–2 is consequently translated:\n\nThus, the six days of creation (verse 3 onwards) start sometime after the Earth was \"without form and void.\" This allows an indefinite \"gap\" of time to be inserted after the original creation of the universe, but prior to the creation according to Genesis, (when present biological species and humanity were created). Gap theorists can therefore agree with the scientific consensus regarding the age of the Earth and universe, while maintaining a literal interpretation of the biblical text.\n\nSome gap creationists expand the basic version of creationism by proposing a \"primordial creation\" of biological life within the \"gap\" of time. This is thought to be \"the world that then was\" mentioned in 2 Peter 3:3–7. Discoveries of fossils and archaeological ruins older than 10,000 years are generally ascribed to this \"world that then was,\" which may also be associated with Lucifer's rebellion. These views became popular with publications of Hebrew Lexicons such as \"Strong's Concordance\", and Bible commentaries such as the \"Scofield Reference Bible\" and \"The Companion Bible\".\n\nDay-age creationism states that the \"six days\" of the Book of Genesis are not ordinary 24-hour days, but rather much longer periods (for instance, each \"day\" could be the equivalent of millions, or billions of years of human time). The physicist Gerald Schroeder is one such proponent of this view. This version of creationism often states that the Hebrew word \"yôm,\" in the context of Genesis 1, can be properly interpreted as \"age.\"\n\nStrictly speaking, day-age creationism is not so much a version of creationism as a hermeneutic option which may be combined with other versions of creationism such as progressive creationism.\n\nProgressive creationism holds that species have changed or evolved in a process continuously guided by God, with various ideas as to how the process operated—though it is generally taken that God directly intervened in the natural order at key moments in Earth history. This view accepts most of modern physical science including the age of the Earth, but rejects much of modern evolutionary biology or looks to it for evidence that evolution by natural selection alone is incorrect. Organizations such as Reasons To Believe, founded by Hugh Ross, promote this version of creationism.\n\nProgressive creationism can be held in conjunction with hermeneutic approaches to the Genesis creation narrative such as the day-age creationism or framework/metaphoric/poetic views.\n\nCreation science, or initially scientific creationism, is a pseudoscience that emerged in the 1960s with proponents aiming to have young Earth creationist beliefs taught in school science classes as a counter to teaching of evolution. Common features of creation science argument include: creationist cosmologies which accommodate a universe on the order of thousands of years old, criticism of radiometric dating through a technical argument about radiohalos, explanations for the fossil record as a record of the Genesis flood narrative (see flood geology), and explanations for the present diversity as a result of pre-designed genetic variability and partially due to the rapid degradation of the perfect genomes God placed in \"created kinds\" or \"Baramin\" (see creationist biology) due to mutations.\n\nNeo-creationism is a pseudoscientific movement which aims to restate creationism in terms more likely to be well received by the public, by policy makers, by educators and by the scientific community. It aims to re-frame the debate over the origins of life in non-religious terms and without appeals to scripture. This comes in response to the 1987 ruling by the United States Supreme Court in \"Edwards v. Aguillard\" that creationism is an inherently religious concept and that advocating it as correct or accurate in public-school curricula violates the Establishment Clause of the First Amendment.\n\nOne of the principal claims of neo-creationism propounds that ostensibly objective orthodox science, with a foundation in naturalism, is actually a dogmatically atheistic religion. Its proponents argue that the scientific method excludes certain explanations of phenomena, particularly where they point towards supernatural elements, thus effectively excluding religious insight from contributing to understanding the universe. This leads to an open and often hostile opposition to what neo-creationists term \"Darwinism\", which they generally mean to refer to evolution, but which they may extend to include such concepts as abiogenesis, stellar evolution and the Big Bang theory.\n\nUnlike their philosophical forebears, neo-creationists largely do not believe in many of the traditional cornerstones of creationism such as a young Earth, or in a dogmatically literal interpretation of the Bible.\n\nIntelligent design (ID) is the pseudoscientific view that \"certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection.\" All of its leading proponents are associated with the Discovery Institute, a think tank whose Wedge strategy aims to replace the scientific method with \"a science consonant with Christian and theistic convictions\" which accepts supernatural explanations. It is widely accepted in the scientific and academic communities that intelligent design is a form of creationism, and is sometimes referred to as \"intelligent design creationism.\"\n\nID originated as a re-branding of creation science in an attempt to avoid a series of court decisions ruling out the teaching of creationism in American public schools, and the Discovery Institute has run a series of campaigns to change school curricula. In Australia, where curricula are under the control of state governments rather than local school boards, there was a public outcry when the notion of ID being taught in science classes was raised by the Federal Education Minister Brendan Nelson; the minister quickly conceded that the correct forum for ID, if it were to be taught, is in religious or philosophy classes.\n\nIn the US, teaching of intelligent design in public schools has been decisively ruled by a federal district court to be in violation of the Establishment Clause of the First Amendment to the United States Constitution. In Kitzmiller v. Dover, the court found that intelligent design is not science and \"cannot uncouple itself from its creationist, and thus religious, antecedents,\" and hence cannot be taught as an alternative to evolution in public school science classrooms under the jurisdiction of that court. This sets a persuasive precedent, based on previous US Supreme Court decisions in \"Edwards v. Aguillard\" and \"Epperson v. Arkansas\" (1968), and by the application of the Lemon test, that creates a legal hurdle to teaching intelligent design in public school districts in other federal court jurisdictions.\n\nIn astronomy, the geocentric model (also known as geocentrism, or the Ptolemaic system), is a description of the Cosmos where Earth is at the orbital center of all celestial bodies. This model served as the predominant cosmological system in many ancient civilizations such as ancient Greece. As such, they assumed that the Sun, Moon, stars, and naked eye planets circled Earth, including the noteworthy systems of Aristotle (see Aristotelian physics) and Ptolemy.\n\nArticles arguing that geocentrism was the biblical perspective appeared in some early creation science newsletters associated with the Creation Research Society pointing to some passages in the Bible, which, when taken literally, indicate that the daily apparent motions of the Sun and the Moon are due to their actual motions around the Earth rather than due to the rotation of the Earth about its axis for example, Joshua 10:12 where the Sun and Moon are said to stop in the sky, and Psalms 93:1 where the world is described as immobile. Contemporary advocates for such religious beliefs include Robert Sungenis, co-author of the self-published \"Galileo Was Wrong: The Church Was Right\" (2006). These people subscribe to the view that a plain reading of the Bible contains an accurate account of the manner in which the universe was created and requires a geocentric worldview. Most contemporary creationist organizations reject such perspectives.\n\nThe Omphalos hypothesis argues that in order for the world to be functional, God must have created a mature Earth with mountains and canyons, rock strata, trees with growth rings, and so on; therefore \"no\" evidence that we can see of the presumed age of the Earth and age of the universe can be taken as reliable. The idea has seen some revival in the 20th century by some modern creationists, who have extended the argument to address the \"starlight problem\". The idea has been criticised as Last Thursdayism, and on the grounds that it requires a deliberately deceptive creator.\n\nTheistic evolution, or evolutionary creation, is a belief that \"the personal God of the Bible created the universe and life through evolutionary processes.\" According to the American Scientific Affiliation:\n\nThrough the 19th century the term \"creationism\" most commonly referred to direct creation of individual souls, in contrast to traducianism. Following the publication of \"Vestiges of the Natural History of Creation\", there was interest in ideas of Creation by divine law. In particular, the liberal theologian Baden Powell argued that this illustrated the Creator's power better than the idea of miraculous creation, which he thought ridiculous. When \"On the Origin of Species\" was published, the cleric Charles Kingsley wrote of evolution as \"just as noble a conception of Deity.\" Darwin's view at the time was of God creating life through the laws of nature, and the book makes several references to \"creation,\" though he later regretted using the term rather than calling it an unknown process. In America, Asa Gray argued that evolution is the secondary effect, or \"modus operandi\", of the first cause, design, and published a pamphlet defending the book in theistic terms, \"Natural Selection not inconsistent with Natural Theology\". Theistic evolution, also called, evolutionary creation, became a popular compromise, and St. George Jackson Mivart was among those accepting evolution but attacking Darwin's naturalistic mechanism. Eventually it was realised that supernatural intervention could not be a scientific explanation, and naturalistic mechanisms such as neo-Lamarckism were favoured as being more compatible with purpose than natural selection.\n\nSome theists took the general view that, instead of faith being in opposition to biological evolution, some or all classical religious teachings about Christian God and creation are compatible with some or all of modern scientific theory, including specifically evolution; it is also known as \"evolutionary creation.\" In Evolution versus Creationism, Eugenie Scott and Niles Eldredge state that it is in fact a type of evolution.\n\nIt generally views evolution as a tool used by God, who is both the first cause and immanent sustainer/upholder of the universe; it is therefore well accepted by people of strong theistic (as opposed to deistic) convictions. Theistic evolution can synthesize with the day-age creationist interpretation of the Genesis creation narrative; however most adherents consider that the first chapters of the Book of Genesis should not be interpreted as a \"literal\" description, but rather as a literary framework or allegory.\n\nFrom a theistic viewpoint, the underlying laws of nature were designed by God for a purpose, and are so self-sufficient that the complexity of the entire physical universe evolved from fundamental particles in processes such as stellar evolution, life forms developed in biological evolution, and in the same way the origin of life by natural causes has resulted from these laws.\n\nIn one form or another, theistic evolution is the view of creation taught at the majority of mainline Protestant seminaries. For Roman Catholics, human evolution is not a matter of religious teaching, and must stand or fall on its own scientific merits. Evolution and the Roman Catholic Church are not in conflict. The Catechism of the Catholic Church comments positively on the theory of evolution, which is neither precluded nor required by the sources of faith, stating that scientific studies \"have splendidly enriched our knowledge of the age and dimensions of the cosmos, the development of life-forms and the appearance of man.\" Roman Catholic schools teach evolution without controversy on the basis that scientific knowledge does not extend beyond the physical, and scientific truth and religious truth cannot be in conflict. Theistic evolution can be described as \"creationism\" in holding that divine intervention brought about the origin of life or that divine laws govern formation of species, though many creationists (in the strict sense) would deny that the position is creationism at all. In the creation–evolution controversy, its proponents generally take the \"evolutionist\" side. This sentiment was expressed by Fr. George Coyne, (the Vatican's chief astronomer between 1978 and 2006):...in America, creationism has come to mean some fundamentalistic, literal, scientific interpretation of Genesis. Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in a belief that everything depends upon God, or better, all is a gift from God.\n\nWhile supporting the methodological naturalism inherent in modern science, the proponents of theistic evolution reject the implication taken by some atheists that this gives credence to ontological materialism. In fact, many modern philosophers of science, including atheists, refer to the long-standing convention in the scientific method that observable events in nature should be explained by natural causes, with the distinction that it does not assume the actual existence or non-existence of the supernatural. \n\n, most Christians around the world accepted evolution as the most likely explanation for the origins of species, and did not take a literal view of the Genesis creation myth. The United States is an exception where belief in religious fundamentalism is much more likely to affect attitudes towards evolution than it is for believers elsewhere. Political partisanship affecting religious belief may be a factor because political partisanship in the US is highly correlated with fundamentalist thinking, unlike in Europe.\n\nMost contemporary Christian leaders and scholars from mainstream churches, such as Anglicans and Lutherans, consider that there is no conflict between the spiritual meaning of creation and the science of evolution. According to the former Archbishop of Canterbury, Rowan Williams, \"...for most of the history of Christianity, and I think this is fair enough, most of the history of the Christianity there's been an awareness that a belief that everything depends on the creative act of God, is quite compatible with a degree of uncertainty or latitude about how precisely that unfolds in creative time.\"\n\nLeaders of the Anglican and Roman Catholic churches have made statements in favor of evolutionary theory, as have scholars such as the physicist John Polkinghorne, who argues that evolution is one of the principles through which God created living beings. Earlier supporters of evolutionary theory include Frederick Temple, Asa Gray and Charles Kingsley who were enthusiastic supporters of Darwin's theories upon their publication, and the French Jesuit priest and geologist Pierre Teilhard de Chardin saw evolution as confirmation of his Christian beliefs, despite condemnation from Church authorities for his more speculative theories. Another example is that of Liberal theology, not providing any creation models, but instead focusing on the symbolism in beliefs of the time of authoring Genesis and the cultural environment.\n\nMany Christians and Jews had been considering the idea of the creation history as an allegory (instead of historical) long before the development of Darwin's theory of evolution. For example, Philo, whose works were taken up by early Church writers, wrote that it would be a mistake to think that creation happened in six days, or in any set amount of time. Augustine of the late fourth century who was also a former neoplatonist argued that everything in the universe was created by God at the same moment in time (and not in six days as a literal reading of the Book of Genesis would seem to require); It appears that both Philo and Augustine felt uncomfortable with the idea of a seven-day creation because it detracted from the notion of God's omnipotence. In 1950, Pope Pius XII stated limited support for the idea in his encyclical \"Humani generis\". In 1996, Pope John Paul II stated that \"new knowledge has led to the recognition of the theory of evolution as more than a hypothesis,\" but, referring to previous papal writings, he concluded that \"if the human body takes its origin from pre-existent living matter, the spiritual soul is immediately created by God.\"\n\nIn the US, Evangelical Christians have continued to believe in a literal Genesis. Members of evangelical Protestant (70%), Mormon (76%) and Jehovah's Witnesses (90%) denominations are the most likely to reject the evolutionary interpretation of the origins of life.\n\nJehovah's Witnesses adhere to a combination of gap creationism and day-age creationism, asserting that scientific evidence about the age of the universe is compatible with the Bible, but that the 'days' after Genesis 1:1 were each thousands of years in length.\n\nThe historic Christian literal interpretation of creation requires the harmonization of the two creation stories, Genesis 1:1–2:3 and Genesis 2:4–25, for there to be a consistent interpretation. They sometimes seek to ensure that their belief is taught in science classes, mainly in American schools. Opponents reject the claim that the literalistic biblical view meets the criteria required to be considered scientific. Many religious groups teach that God created the Cosmos. From the days of the early Christian Church Fathers there were allegorical interpretations of the Book of Genesis as well as literal aspects.\n\nChristian Science, a system of thought and practice derived from the writings of Mary Baker Eddy, interprets the Book of Genesis figuratively rather than literally. It holds that the material world is an illusion, and consequently not created by God: the only real creation is the spiritual realm, of which the material world is a distorted version. Christian Scientists regard the story of the creation in the Book of Genesis as having symbolic rather than literal meaning. According to Christian Science, both creationism and evolution are false from an absolute or \"spiritual\" point of view, as they both proceed from a (false) belief in the reality of a material universe. However, Christian Scientists do not oppose the teaching of evolution in schools, nor do they demand that alternative accounts be taught: they believe that both material science and literalist theology are concerned with the illusory, mortal and material, rather than the real, immortal and spiritual. With regard to material theories of creation, Eddy showed a preference for Darwin's theory of evolution over others.\n\nAccording to Hindu creationism, all species on Earth including humans have \"devolved\" or come down from a high state of pure consciousness. Hindu creationists claim that species of plants and animals are material forms adopted by pure consciousness which live an endless cycle of births and rebirths. Ronald Numbers says that: \"Hindu Creationists have insisted on the antiquity of humans, who they believe appeared fully formed as long, perhaps, as trillions of years ago.\" Hindu creationism is a form of old Earth creationism, according to Hindu creationists the universe may even be older than billions of years. These views are based on the Vedas, the creation myths of which depict an extreme antiquity of the universe and history of the Earth.\n\nIslamic creationism is the belief that the universe (including humanity) was directly created by God as explained in the Qur'an. It usually views the Book of Genesis as a corrupted version of God's message. The creation myths in the Qur'an are vaguer and allow for a wider range of interpretations similar to those in other Abrahamic religions.\n\nIslam also has its own school of theistic evolutionism, which holds that mainstream scientific analysis of the origin of the universe is supported by the Qur'an. Some Muslims believe in evolutionary creation, especially among liberal movements within Islam.\n\nWriting for \"The Boston Globe\", Drake Bennett noted: \"Without a Book of Genesis to account for ... Muslim creationists have little interest in proving that the age of the Earth is measured in the thousands rather than the billions of years, nor do they show much interest in the problem of the dinosaurs. And the idea that animals might evolve into other animals also tends to be less controversial, in part because there are passages of the Koran that seem to support it. But the issue of whether human beings are the product of evolution is just as fraught among Muslims.\" However, some Muslims, such as Adnan Oktar (also known as Harun Yahya), do not agree that one species can develop from another.\n\nSince the 1980s, Turkey has been a site of strong advocacy for creationism, supported by American adherents.\n\nThere are several verses in the Qur'an which some modern writers have interpreted as being compatible with the expansion of the universe, Big Bang and Big Crunch theories:\n\nThe Ahmadiyya movement actively promotes evolutionary theory. Ahmadis interpret scripture from the Qur'an to support the concept of macroevolution and give precedence to scientific theories. Furthermore, unlike orthodox Muslims, Ahmadis believe that humans have gradually evolved from different species. Ahmadis regard Adam as being the first Prophet of Godas opposed to him being the first man on Earth. Rather than wholly adopting the theory of natural selection, Ahmadis promote the idea of a \"guided evolution,\" viewing each stage of the evolutionary process as having been selectively woven by God. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus \"Revelation, Rationality, Knowledge & Truth\" (1998) that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community.\n\nFor Orthodox Jews who seek to reconcile discrepancies between science and the creation myths in the Bible, the notion that science and the Bible should even be reconciled through traditional scientific means is questioned. To these groups, science is as true as the Torah and if there seems to be a problem, epistemological limits are to blame for apparently irreconcilable points. They point to discrepancies between what is expected and what actually is to demonstrate that things are not always as they appear. They note that even the root word for \"world\" in the Hebrew language—עולם (Olam)—means hidden—נעלם (Neh-Eh-Lahm). Just as they know from the Torah that God created man and trees and the light on its way from the stars in their observed state, so too can they know that the world was created in its over the six days of Creation that reflects progression to its currently-observed state, with the understanding that physical ways to verify this may eventually be identified. This knowledge has been advanced by Rabbi Dovid Gottlieb, former philosophy professor at Johns Hopkins University. Also, relatively old Kabbalistic sources from well before the scientifically apparent age of the universe was first determined are in close concord with modern scientific estimates of the age of the universe, according to Rabbi Aryeh Kaplan, and based on Sefer Temunah, an early kabbalistic work attributed to the first-century Tanna Nehunya ben HaKanah. Many kabbalists accepted the teachings of the Sefer HaTemunah, including the medieval Jewish scholar Nahmanides, his close student Isaac ben Samuel of Acre, and David ben Solomon ibn Abi Zimra. Other parallels are derived, among other sources, from Nahmanides, who expounds that there was a Neanderthal-like species with which Adam mated (he did this long before Neanderthals had even been discovered scientifically). Reform Judaism does not take the Torah as a literal text, but rather as a symbolic or open-ended work.\n\nSome contemporary writers such as Rabbi Gedalyah Nadel have sought to reconcile the discrepancy between the account in the Torah, and scientific findings by arguing that each day referred to in the Bible was not 24 hours, but billions of years long. Others claim that the Earth was created a few thousand years ago, but was deliberately made to look as if it was five billion years old, e.g. by being created with ready made fossils. The best known exponent of this approach being Rabbi Menachem Mendel Schneerson Others state that although the world was physically created in six 24 hour days, the Torah accounts can be interpreted to mean that there was a period of billions of years before the six days of creation.\n\nIn the creation myth taught by Bahá'u'lláh, the Bahá'í Faith founder, the universe has \"neither beginning nor ending,\" and that the component elements of the material world have always existed and will always exist. With regard to evolution and the origin of human beings, `Abdu'l-Bahá gave extensive comments on the subject when he addressed western audiences in the beginning of the 20th century. Transcripts of these comments can be found in \"Some Answered Questions\", \"Paris Talks\" and \"The Promulgation of Universal Peace\". `Abdu'l-Bahá described the human species as having evolved from a primitive form to modern man, but that the capacity to form human intelligence was always in existence.\n\nMost vocal literalist creationists are from the US, and strict creationist views are much less common in other developed countries. According to a study published in \"Science\", a survey of the US, Turkey, Japan and Europe showed that public acceptance of evolution is most prevalent in Iceland, Denmark and Sweden at 80% of the population. There seems to be no significant correlation between believing in evolution and understanding evolutionary science.\n\nA 2009 Nielsen poll, showed that almost a quarter of Australians believe \"the biblical account of human origins.\" Forty-two percent believe in a \"wholly scientific\" explanation for the origins of life, while 32 percent believe in an evolutionary process \"guided by God.\"\n\nA 2012 survey, by Angus Reid Public Opinion revealed that 61 percent of Canadians believe in evolution. The poll asked \"Where did human beings come fromdid we start as singular cells millions of year ago and evolve into our present form, or did God create us in his image 10,000 years ago?\"\n\nIn Europe, literalist creationism is more widely rejected, though regular opinion polls are not available. Most people accept that evolution is the most widely accepted scientific theory as taught in most schools. In countries with a Roman Catholic majority, papal acceptance of evolutionary creationism as worthy of study has essentially ended debate on the matter for many people.\n\nIn the UK, a 2006 poll on the \"origin and development of life\", asked participants to choose between three different perspectives on the origin of life: 22% chose creationism, 17% opted for intelligent design, 48% selected evolutionary theory, and the rest did not know. A subsequent 2010 YouGov poll on the correct explanation for the origin of humans found that 9% opted for creationism, 12% intelligent design, 65% evolutionary theory and 13% didn't know. The former Archbishop of Canterbury Rowan Williams, head of the worldwide Anglican Communion, views the idea of teaching creationism in schools as a mistake.\n\nIn Italy, Education Minister Letizia Moratti wanted to retire evolution from the secondary school level; after one week of massive protests, she reversed her opinion.\n\nThere continues to be scattered and possibly mounting efforts on the part of religious groups throughout Europe to introduce creationism into public education. In response, the Parliamentary Assembly of the Council of Europe has released a draft report titled \"The dangers of creationism in education\" on June 8, 2007, reinforced by a further proposal of banning it in schools dated October 4, 2007.\n\nSerbia suspended the teaching of evolution for one week in September 2004, under education minister Ljiljana Čolić, only allowing schools to reintroduce evolution into the curriculum if they also taught creationism. \"After a deluge of protest from scientists, teachers and opposition parties\" says the BBC report, Čolić's deputy made the statement, \"I have come here to confirm Charles Darwin is still alive\" and announced that the decision was reversed. Čolić resigned after the government said that she had caused \"problems that had started to reflect on the work of the entire government.\"\n\nPoland saw a major controversy over creationism in 2006, when the Deputy Education Minister, Mirosław Orzechowski, denounced evolution as \"one of many lies\" taught in Polish schools. His superior, Minister of Education Roman Giertych, has stated that the theory of evolution would continue to be taught in Polish schools, \"as long as most scientists in our country say that it is the right theory.\" Giertych's father, Member of the European Parliament Maciej Giertych, has opposed the teaching of evolution and has claimed that dinosaurs and humans co-existed.\n\nA 2017 poll by Pew Research found that 62% of Americans believe humans have evolved over time and 34% of Americans believe humans and other living things have existed in their present form since the beginning of time. Another 2017 Gallup creationism survey found that 38% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which Gallup noted was the lowest level in 35 years.\n\nAccording to a 2014 Gallup poll, about 42% of Americans believe that \"God created human beings pretty much in their present form at one time within the last 10,000 years or so.\" Another 31% believe that \"human beings have developed over millions of years from less advanced forms of life, but God guided this process,\"and 19% believe that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process.\"\n\nBelief in creationism is inversely correlated to education; of those with postgraduate degrees, 74% accept evolution. In 1987, \"Newsweek\" reported: \"By one count there are some 700 scientists with respectable academic credentials (out of a total of 480,000 U.S. earth and life scientists) who give credence to creation-science, the general theory that complex life forms did not evolve but appeared 'abruptly.'\"\n\nA 2000 poll for People for the American Way found 70% of the US public felt that evolution was compatible with a belief in God.\n\nAccording to a study published in \"Science\", between 1985 and 2005 the number of adult North Americans who accept evolution declined from 45% to 40%, the number of adults who reject evolution declined from 48% to 39% and the number of people who were unsure increased from 7% to 21%. Besides the US the study also compared data from 32 European countries, Turkey, and Japan. The only country where acceptance of evolution was lower than in the US was Turkey (25%).\n\nAccording to a 2011 Fox News poll, 45% of Americans believe in Creationism, down from 50% in a similar poll in 1999. 21% believe in 'the theory of evolution as outlined by Darwin and other scientists' (up from 15% in 1999), and 27% answered that both are true (up from 26% in 1999).\n\nIn September 2012, educator and television personality Bill Nye spoke with the Associated Press and aired his fears about acceptance of creationism, believing that teaching children that creationism is the only true answer without letting them understand the way science works will prevent any future innovation in the world of science. In February 2014, Nye defended evolution in the classroom in a debate with creationist Ken Ham on the topic of whether creation is a viable model of origins in today's modern, scientific era.\n\nIn the US, creationism has become centered in the political controversy over creation and evolution in public education, and whether teaching creationism in science classes conflicts with the separation of church and state. Currently, the controversy comes in the form of whether advocates of the intelligent design movement who wish to \"Teach the Controversy\" in science classes have conflated science with religion.\n\nPeople for the American Way polled 1500 North Americans about the teaching of evolution and creationism in November and December 1999. They found that most North Americans were not familiar with Creationism, and most North Americans had heard of evolution, but many did not fully understand the basics of the theory. The main findings were:\nIn such political contexts, creationists argue that their particular religiously based origin belief is superior to those of other belief systems, in particular those made through secular or scientific rationale. Political creationists are opposed by many individuals and organizations who have made detailed critiques and given testimony in various court cases that the alternatives to scientific reasoning offered by creationists are opposed by the consensus of the scientific community.\n\nMost Christians disagree with the teaching of creationism as an alternative to evolution in schools. Several religious organizations, among them the Catholic Church, hold that their faith does not conflict with the scientific consensus regarding evolution. The Clergy Letter Project, which has collected more than 13,000 signatures, is an \"endeavor designed to demonstrate that religion and science can be compatible.\"\n\nIn his 2002 article \"Intelligent Design as a Theological Problem,\" George Murphy argues against the view that life on Earth, in all its forms, is direct evidence of God's act of creation (Murphy quotes Phillip E. Johnson's claim that he is speaking \"of a God who acted openly and left his fingerprints on all the evidence.\"). Murphy argues that this view of God is incompatible with the Christian understanding of God as \"the one revealed in the cross and resurrection of Christ.\" The basis of this theology is Isaiah 45:15, \"Verily thou art a God that hidest thyself, O God of Israel, the Saviour.\"\n\nMurphy observes that the execution of a Jewish carpenter by Roman authorities is in and of itself an ordinary event and did not require divine action. On the contrary, for the crucifixion to occur, God had to limit or \"empty\" himself. It was for this reason that Paul the Apostle wrote, in Philippians 2:5-8:\n\nLet this mind be in you, which was also in Christ Jesus: Who, being in the form of God, thought it not robbery to be equal with God: But made himself of no reputation, and took upon him the form of a servant, and was made in the likeness of men: And being found in fashion as a man, he humbled himself, and became obedient unto death, even the death of the cross.\n\nMurphy concludes that,Just as the Son of God limited himself by taking human form and dying on a cross, God limits divine action in the world to be in accord with rational laws which God has chosen. This enables us to understand the world on its own terms, but it also means that natural processes hide God from scientific observation.For Murphy, a theology of the cross requires that Christians accept a \"methodological\" naturalism, meaning that one cannot invoke God to explain natural phenomena, while recognizing that such acceptance does not require one to accept a \"metaphysical\" naturalism, which proposes that nature is all that there is.\n\nThe Jesuit priest George Coyne has stated that is \"unfortunate that, especially here in America, creationism has come to mean...some literal interpretation of Genesis.\" He argues that \"...Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in belief that everything depends on God, or better, all is a gift from God.\"\n\nOther Christians have expressed qualms about teaching creationism. In March 2006, then Archbishop of Canterbury Rowan Williams, the leader of the world's Anglicans, stated his discomfort about teaching creationism, saying that creationism was \"a kind of category mistake, as if the Bible were a theory like other theories.\" He also said: \"My worry is creationism can end up reducing the doctrine of creation rather than enhancing it.\" The views of the Episcopal Churcha major American-based branch of the Anglican Communionon teaching creationism resemble those of Williams.\n\nThe National Science Teachers Association is opposed to teaching creationism as a science, as is the Association for Science Teacher Education, the National Association of Biology Teachers, the American Anthropological Association, the American Geosciences Institute, the Geological Society of America, the American Geophysical Union, and numerous other professional teaching and scientific societies.\n\nIn April 2010, the American Academy of Religion issued \"Guidelines for Teaching About Religion in K‐12 Public Schools in the United States\", which included guidance that creation science or intelligent design should not be taught in science classes, as \"Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning.\" However, they, as well as other \"worldviews that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others.\"\n\nRandy Moore and Sehoya Cotner, from the biology program at the University of Minnesota, reflect on the relevance of teaching creationism in the article \"The Creationist Down the Hall: Does It Matter When Teachers Teach Creationism?\" They conclude that \"Despite decades of science education reform, numerous legal decisions declaring the teaching of creationism in public-school science classes to be unconstitutional, overwhelming evidence supporting evolution, and the many denunciations of creationism as nonscientific by professional scientific societies, creationism remains popular throughout the United States.\"\n\nScience is a system of knowledge based on observation, empirical evidence, and the development of theories that yield testable explanations and predictions of natural phenomena. By contrast, creationism is often based on literal interpretations of the narratives of particular religious texts. Some creationist beliefs involve purported forces that lie outside of nature, such as supernatural intervention, and often do not allow predictions at all. Therefore, these can neither be confirmed nor disproved by scientists. However, many creationist beliefs can be framed as testable predictions about phenomena such as the age of the Earth, its geological history and the origins, distributions and relationships of living organisms found on it. Early science incorporated elements of these beliefs, but as science developed these beliefs were gradually falsified and were replaced with understandings based on accumulated and reproducible evidence that often allows the accurate prediction of future results.\n\nSome scientists, such as Stephen Jay Gould, consider science and religion to be two compatible and complementary fields, with authorities in distinct areas of human experience, so-called non-overlapping magisteria. This view is also held by many theologians, who believe that ultimate origins and meaning are addressed by religion, but favor verifiable scientific explanations of natural phenomena over those of creationist beliefs. Other scientists, such as Richard Dawkins, reject the non-overlapping magisteria and argue that, in disproving literal interpretations of creationists, the scientific method also undermines religious texts as a source of truth. Irrespective of this diversity in viewpoints, since creationist beliefs are not supported by empirical evidence, the scientific consensus is that any attempt to teach creationism as science should be rejected.\n\n\n\n"}
{"id": "30694430", "url": "https://en.wikipedia.org/wiki?curid=30694430", "title": "Criticism of the theory of relativity", "text": "Criticism of the theory of relativity\n\nCriticism of the theory of relativity of Albert Einstein was mainly expressed in the early years after its publication in the early twentieth century, on scientific, pseudoscientific, philosophical, or ideological bases. Though some of these criticisms had the support of reputable scientists, Einstein's theory of relativity is now accepted by the scientific community.\n\nReasons for criticism of the theory of relativity have included alternative theories, rejection of the abstract-mathematical method, and alleged errors of the theory. According to some authors, antisemitic objections to Einstein's Jewish heritage also occasionally played a role in these objections. There are still some critics of relativity today, but their opinions are not shared by the majority in the scientific community.\n\nAround the end of the 19th century, the view was widespread that all forces in nature are of electromagnetic origin (the \"electromagnetic worldview\"), especially in the works of Joseph Larmor (1897) and Wilhelm Wien (1900). This was apparently confirmed by the experiments of Walter Kaufmann (1901–1903), who measured an increase of the mass of a body with velocity which was consistent with the hypothesis that the mass was generated by its electromagnetic field. Max Abraham (1902) subsequently sketched a theoretical explanation of Kaufmann's result in which the electron was considered as rigid and spherical. However, it was found that this model was incompatible with the results of many experiments (including the Michelson–Morley experiment, the Experiments of Rayleigh and Brace, and the Trouton–Noble experiment), according to which no motion of an observer with respect to the luminiferous aether (\"aether drift\") had been observed despite numerous attempts to do so. Henri Poincaré (1902) conjectured that this failure arose from a general law of nature, which he called \"the principle of relativity\". Hendrik Antoon Lorentz (1904) created a detailed theory of electrodynamics (Lorentz ether theory) that was premised on the existence of an immobile aether and employed a set of space and time coordinate transformations that Poincaré called the Lorentz transformations, including the effects of length contraction and local time. However, Lorentz's theory only partially satisfied the relativity principle, because his transformation formulas for velocity and charge density were incorrect. This was corrected by Poincaré (1905) who obtained full Lorentz covariance of the electrodynamic equations.\n\nCriticizing Lorentz's 1904 theory, Abraham (1904) held that the Lorentz contraction of electrons requires a non-electromagnetic force to ensure the electron's stability. This was unacceptable to him as a proponent of the electromagnetic worldview. He continued that as long as a consistent explanation is missing as to how those forces and potentials act together on the electron, Lorentz's system of hypotheses is incomplete and doesn't satisfy the relativity principle. Poincaré (1905) removed this objection by showing that the non-electromagnetic potential (\"Poincaré stress\") holding the electron together can be formulated in a Lorentz covariant way, and showed that in principle it is possible to create a Lorentz covariant model for gravitation which he considered non-electromagnetic in nature as well. Thus the consistency of Lorentz's theory was proven, but the electromagnetic worldview had to be given up. Eventually, Albert Einstein published in September 1905 what is now called special relativity, which was based on a radical new application of the relativity principle in connection with the constancy of the speed of light. In special relativity, the space and time coordinates depend on the inertial observer's frame of reference, and the luminiferous aether plays no role in the physics. Although this theory was founded on a very different kinematical model, it was experimentally indistinguishable from the aether theory of Lorentz and Poincaré, since both theories satisfy the relativity principle of Poincaré and Einstein, and both employ the Lorentz transformations. After Minkowski's introduction in 1908 of the geometric spacetime model for Einstein's version of relativity, most physicists eventually decided in favor of the Einstein-Minkowski version of relativity with its radical new views of space and time, in which there was no useful role for the aether.\n\nKaufmann–Bucherer–Neumann experiments: To conclusively decide between the theories of Abraham and Lorentz, Kaufmann repeated his experiments in 1905 with improved accuracy. However, in the meantime the theoretical situation had changed. Alfred Bucherer and Paul Langevin (1904) developed another model, in which the electron is contracted in the line of motion, and dilated in the transverse direction, so that the volume remains constant. While Kaufmann was still evaluating his experiments, Einstein published his theory of special relativity. Eventually, Kaufmann published his results in December 1905 and argued that they are in agreement with Abraham's theory and require rejection of the \"basic assumption of Lorentz and Einstein\" (the relativity principle). Lorentz reacted with the phrase \"I am at the end of my Latin\", while Einstein did not mention those experiments before 1908. Yet, others started to criticize the experiments. Max Planck (1906) alluded to inconsistencies in the theoretical interpretation of the data, and Adolf Bestelmeyer (1906) introduced new techniques, which (especially in the area of low velocities) gave different results and which cast doubts on Kaufmann's methods. Therefore, Bucherer (1908) conducted new experiments and arrived at the conclusion that they confirm the mass formula of relativity and thus the \"relativity principle of Lorentz and Einstein\". Yet Bucherer's experiments were criticized by Bestelmeyer leading to a sharp dispute between the two experimentalists. On the other hand, additional experiments of Hupka (1910), Neumann (1914) and others seemed to confirm Bucherer's result. The doubts lasted until 1940, when in similar experiments Abraham's theory was conclusively disproved. (It must be remarked that besides those experiments, the relativistic mass formula had already been confirmed by 1917 in the course of investigations on the theory of spectra. In modern particle accelerators, the relativistic mass formula is routinely confirmed.)\n\nIn 1902–1906, Dayton Miller repeated the Michelson–Morley experiment together with Edward W. Morley. They confirmed the null result of the initial experiment. However, in 1921–1926, Miller conducted new experiments which apparently gave positive results. Those experiments initially attracted some attention in the media and in the scientific community but have been considered refuted for the following reasons: Einstein, Max Born, and Robert S. Shankland pointed out that Miller hadn't appropriately considered the influence of temperature. A modern analysis by Roberts shows that Miller's experiment gives a null result, when the technical shortcomings of the apparatus and the error bars are properly considered. Additionally, Miller's result is in disagreement with all other experiments, which were conducted before and after. For example, Georg Joos (1930) used an apparatus of similar dimensions to Miller's, but he obtained null results. In recent experiments of Michelson–Morley type where the coherence length is increased considerably by using lasers and masers the results are still negative.\n\nIn the 2011 Faster-than-light neutrino anomaly, the OPERA collaboration published results which appeared to show that the speed of neutrinos is slightly faster than the speed of light. However, sources of errors were found and confirmed in 2012 by the OPERA collaboration, which fully explained the initial results. In their final publication, a neutrino speed consistent with the speed of light was stated. Also subsequent experiments found agreement with the speed of light, see measurements of neutrino speed.\n\nIt was also claimed that special relativity cannot handle acceleration, which would lead to contradictions in some situations. However, this assessment is not correct, since acceleration actually can be described in the framework of special relativity (see Acceleration (special relativity), Proper reference frame (flat spacetime), Hyperbolic motion, Rindler coordinates, Born coordinates). Paradoxes relying on insufficient understanding of these facts were discovered in the early years of relativity. For example, Max Born (1909) tried to combine the concept of rigid bodies with special relativity. That this model was insufficient was shown by Paul Ehrenfest (1909), who demonstrated that a rotating rigid body would, according to Born's definition, undergo a contraction of the circumference without contraction of the radius, which is impossible (Ehrenfest paradox). Max von Laue (1911) showed that rigid bodies cannot exist in special relativity, since the propagation of signals cannot exceed the speed of light, so an accelerating and rotating body will undergo deformations.\n\nPaul Langevin and von Laue showed that the twin paradox can be completely resolved by consideration of acceleration in special relativity. If two twins move away from each other, and one of them is accelerating and coming back to the other, then the accelerated twin is younger than the other one, since he was located in at least two inertial frames of reference, and therefore his assessment of which events are simultaneous changed during the acceleration. For the other twin nothing changes since he remained in a single frame.\n\nAnother example is the Sagnac effect. Two signals were sent in opposite directions around a rotating platform. After their arrival a displacement of the interference fringes occurs. Sagnac himself believed that he had proved the existence of the aether. However, special relativity can easily explain this effect. When viewed from an inertial frame of reference, it is a simple consequence of the independence of the speed of light from the speed of the source, since the receiver runs away from one beam, while it approaches the other beam. When viewed from a rotating frame, the assessment of simultaneity changes during the rotation, and consequently the speed of light is not constant in accelerated frames.\n\nAs was shown by Einstein, the only form of accelerated motion that cannot be described is the one due to gravitation, since special relativity is not compatible with the Equivalence principle. Einstein was also unsatisfied with the fact that inertial frames are preferred over accelerated frames. Thus over the course of several years (1908–1915), Einstein developed general relativity. This theory includes the replacement of Euclidean geometry by non-Euclidean geometry, and the resultant curvature of the path of light led Einstein (1912) to the conclusion that (like in accelerated frames) the speed of light is not constant in extended gravitational fields. Therefore, Abraham (1912) argued that Einstein had given special relativity a coup de grâce. Einstein responded that within its area of application (in areas where gravitational influences can be neglected) special relativity is still applicable with high precision, so one cannot speak of a coup de grâce at all.\n\nIn special relativity, the transfer of signals at superluminal speeds is impossible, since this would violate the Poincaré-Einstein synchronization, and the causality principle. Following an old argument by Pierre-Simon Laplace, Poincaré (1904) alluded to the fact that Newton's law of universal gravitation is founded on an infinitely great speed of gravity. So the clock-synchronization by light signals could in principle be replaced by a clock-synchronization by instantaneous gravitational signals. In 1905, Poincaré himself solved this problem by showing that in a relativistic theory of gravity the speed of gravity is equal to the speed of light. Although much more complicated, this is also the case in Einstein's theory of general relativity.\n\nAnother apparent contradiction lies in the fact that the group velocity in anomalously dispersive media is higher than the speed of light. This was investigated by Arnold Sommerfeld (1907, 1914) and Léon Brillouin (1914). They came to the conclusion that in such cases the signal velocity is not equal to the group velocity, but to the front velocity which is never faster than the speed of light. Similarly, it is also argued that the apparent superluminal effects discovered by Günter Nimtz can be explained by a thorough consideration of the velocities involved.\n\nAlso quantum entanglement (denoted by Einstein as \"spooky action at a distance\"), according to which the quantum state of one entangled particle cannot be fully described without describing the other particle, does not imply superluminal transmission of information (see quantum teleportation), and it is therefore in conformity with special relativity.\n\nInsufficient knowledge of the basics of special relativity, especially the application of the Lorentz transformation in connection with length contraction and time dilation, led and still leads to the construction of various apparent paradoxes. Both the twin paradox and the Ehrenfest paradox and their explanation were already mentioned above. Besides the twin paradox, also the reciprocity of time dilation (\"i.e.\" every inertially moving observer considers the clock of the other one as being dilated) was heavily criticized by Herbert Dingle and others. For example, Dingle wrote a series of letters to Nature at the end of the 1950s. However, the self-consistency of the reciprocity of time dilation had already been demonstrated long before in an illustrative way by Lorentz (in his lectures from 1910, published 1931) and many others—they alluded to the fact that it is only necessary to carefully consider the relevant measurement rules and the relativity of simultaneity. Other known paradoxes are the Ladder paradox and Bell's spaceship paradox, which also can simply be solved by consideration of the relativity of simultaneity.\n\nMany physicists (like Hendrik Lorentz, Oliver Lodge, Albert Abraham Michelson, Edmund Taylor Whittaker, Harry Bateman, Ebenezer Cunningham, Charles Émile Picard, Paul Painlevé) were uncomfortable with the rejection of the aether, and preferred to interpret the Lorentz transformation based on the existence of a preferred frame of reference, as in the aether-based theories of Lorentz, Larmor, and Poincaré. However, the idea of an aether hidden from any observation was not supported by the mainstream scientific community, therefore the aether theory of Lorentz and Poincaré was superseded by Einstein's special relativity which was subsequently formulated in the framework of four-dimensional spacetime by Minkowski.\n\nOthers such as Herbert E. Ives argued that it might be possible to experimentally determine the motion of such an aether, but it was never found despite numerous experimental tests of Lorentz invariance (see tests of special relativity).\n\nAlso attempts to introduce some sort of relativistic aether (consistent with relativity) into modern physics such as by Einstein on the basis of general relativity (1920), or by Paul Dirac in relation to quantum mechanics (1951), were not supported by the scientific community (see Luminiferous aether#End of aether?).\n\nIn his Nobel lecture, George F. Smoot (2006) described his own experiments on the Cosmic microwave background radiation anisotropy as \"New Aether drift experiments\". Smoot explained that \"one problem to overcome was the strong prejudice of good scientists who learned the lesson of the Michelson and Morley experiment and Special Relativity that there were no preferred frames of reference.\" He continued that \"there was an education job to convince them that this did not violate Special Relativity but did find a frame in which the expansion of the universe looked particularly simple.\"\n\nThe theory of complete aether drag, as proposed by George Gabriel Stokes (1844), was used by some critics as Ludwig Silberstein (1920) or Philipp Lenard (1920) as a counter-model of relativity. In this theory, the aether was completely dragged within and in the vicinity of matter, and it was believed that various phenomena, such as the absence of aether drift, could be explained in an \"illustrative\" way by this model. However, such theories are subject to great difficulties. Especially the aberration of light contradicted the theory, and all auxiliary hypotheses, which were invented to rescue it, are self-contradictory, extremely implausible, or in contradiction to other experiments like the Michelson–Gale–Pearson experiment. In summary, a sound mathematical and physical model of complete aether drag was never invented, consequently this theory was no serious alternative to relativity.\n\nAnother alternative was the so-called emission theory of light. As in special relativity the aether concept is discarded, yet the main difference from relativity lies in the fact that the velocity of the light source is added to that of light in accordance with the Galilean transformation. As the hypothesis of complete aether drag, it can explain the negative outcome of all aether drift experiments. Yet, there are various experiments that contradict this theory. For example, the Sagnac effect is based on the independence of light speed from the source velocity, and the image of Double stars should be scrambled according to this model—which was not observed. Also in modern experiments in particle accelerators no such velocity dependence could be observed. These results are further confirmed by the De Sitter double star experiment (1913), conclusively repeated in the X-ray spectrum by K. Brecher in 1977;\nand the terrestrial experiment by Alväger, \"et al\". (1963);, which all show that the speed of light is independent of the motion of the source within the limits of experimental accuracy.\n\nSome consider the \"principle of the constancy of the velocity of light\" insufficiently substantiated. However, as already shown by Robert Daniel Carmichael (1910) and others, the constancy of the speed of light can be interpreted as a natural consequence of \"two\" experimentally demonstrated facts:\n\n\nNote that measurements regarding the speed of light are actually measurements of the two-way speed of light, since the one-way speed of light depends on which convention is chosen to synchronize the clocks.\n\nEinstein emphasized the importance of general covariance for the development of general relativity, and took the position that the general covariance of his 1915 theory of gravity ensured implementation of a generalized relativity principle. This view was challenged by Erich Kretschmann (1917), who argued that every theory of space and time (even including Newtonian dynamics) can be formulated in a covariant way, if additional parameters are included, and thus general covariance of a theory would in itself be insufficient to implement a generalized relativity principle. Although Einstein (1918) agreed with that argument, he also countered that Newtonian mechanics in general covariant form would be too complicated for practical uses. Although it is now understood that Einstein's response to Kretschmann was mistaken (subsequent papers showed that such a theory would still be usable), another argument can be made in favor of general covariance: it is a natural way to express the equivalence principle, \"i.e.\", the equivalence in the description of a free-falling observer and an observer at rest, and thus it is more convenient to use general covariance together with general relativity, rather than with Newtonian mechanics. Connected with this, also the question of absolute motion was dealt with. Einstein argued that the general covariance of his theory of gravity supports Mach's principle, which would eliminate any \"absolute motion\" within general relativity. However, as pointed out by Willem de Sitter in 1916, Mach's principle is not completely fulfilled in general relativity because there exist matter-free solutions of the field equations. This means that the \"inertio-gravitational field\", which describes both gravity and inertia, can exist in the absence of gravitating matter. However, as pointed out by Einstein, there is one fundamental difference between this concept and absolute space of Newton: the inertio-gravitational field of general relativity is determined by matter, thus it is not absolute.\n\nIn the \"Bad Nauheim Debate\" (1920) between Einstein and (among others) Philipp Lenard, the latter stated the following objections: He criticized the lack of \"illustrativeness\" of Einstein's version of relativity, a condition that he suggested could only be met by an aether theory. Einstein responded that for physicists the content of \"illustrativeness\" or \"common sense\" had changed in time, so it could no longer be used as a criterion for the validity of a physical theory. Lenard also argued that with his relativistic theory of gravity Einstein had tacitly reintroduced the aether under the name \"space\". While this charge was rejected (among others) by Hermann Weyl, in an inaugural address given at the University of Leiden in 1920, shortly after the Bad Nauheim debates, Einstein himself acknowledged that according to his general theory of relativity, so-called \"empty space\" possesses physical properties that influence matter and \"vice versa\". Lenard also argued that Einstein's general theory of relativity admits the existence of superluminal velocities, in contradiction to the principles of special relativity; for example, in a rotating coordinate system in which the Earth is at rest, the distant points of the whole universe are rotating around Earth with superluminal velocities. However, as been pointed out by Weyl, it's not possible to handle a rotating extended system as a rigid body (neither in special nor in general relativity)—so the signal velocity of an object never exceeds the speed of light. Another criticism that was raised by both Lenard and Gustav Mie concerned the existence of \"fictitious\" gravitational fields in accelerating frames, which according to Einstein's Equivalence Principle are no less physically real than those produced by material sources. Lenard and Mie argued that physical forces can only be produced by real material sources, while the gravitational field that Einstein supposed to exist in an accelerating frame of reference has no concrete physical meaning. Einstein responded that, based on Mach's principle, one can think of these gravitational fields as induced by the distant masses. In this respect the criticism of Lenard and Mie has been vindicated, since according to the modern consensus, in agreement with Einstein's own mature views, Mach's principle as originally conceived by Einstein is not actually supported by general relativity, as already mentioned above.\n\nLudwik Silberstein, who initially was a supporter of the special theory, objected at different occasions against general relativity. In 1920 he argued that the deflection of light by the sun, as observed by Arthur Eddington et al. (1919), is not necessarily a confirmation of general relativity, but may also be explained by the Stokes-Planck theory of complete aether drag. However, such models are in contradiction with the aberration of light and other experiments (see \"Alternative theories\"). In 1935, Silberstein claimed to have found a contradiction in the Two-body problem in general relativity. However, also this claim was refuted by Einstein and Rosen (1935).\n\nThe consequences of relativity, such as the change of ordinary concepts of space and time, as well as the introduction of non-Euclidean geometry in general relativity, were criticized by some philosophers of different philosophical schools. It was characteristic for many philosophical critics that they had insufficient knowledge of the mathematical and formal basis of relativity, which led to the criticisms often missing the heart of the matter. For example, relativity was misinterpreted as some form of relativism. However, this is misleading as it was emphasized by Einstein or Planck. On one hand it's true that space and time became relative, and the inertial frames of reference are handled on equal footing. On the other hand, the theory makes natural laws invariant—examples are the constancy of the speed of light, or the covariance of Maxwell's equations. Consequently, Felix Klein (1910) called it the \"invariant theory of the Lorentz group\" instead of relativity theory, and Einstein (who reportedly used expressions like \"absolute theory\") sympathized with this expression as well.\n\nCritical responses to relativity were also expressed by proponents of Neo-Kantianism (Paul Natorp, Bruno Bauch, etc.), and Phenomenology (Oskar Becker, Moritz Geiger etc.). While some of them only rejected the philosophical consequences, others rejected also the physical consequences of the theory. Einstein was criticized for violating Immanuel Kant's categoric scheme, \"i.e.\", it was claimed that space-time curvature caused by matter and energy is impossible, since matter and energy already require the concepts of space and time. Also the three-dimensionality of space, Euclidean geometry, and the existence of absolute simultaneity were claimed to be necessary for the understanding of the world; none of them can possibly be altered by empirical findings. By moving all those concepts into a metaphysical area, any form of criticism of Kantianism would be prevented. Other pseudo-Kantians like Ernst Cassirer or Hans Reichenbach (1920), tried to modify Kant's philosophy. Subsequently, Reichenbach rejected Kantianism at all and became a proponent of logical positivism.\n\nBased on Henri Poincaré's conventionalism, philosophers such as Pierre Duhem (1914) or Hugo Dingler (1920) argued that the classical concepts of space, time, and geometry were, and will always be, the most convenient expressions in natural science, therefore the concepts of relativity cannot be correct. This was criticized by proponents of logical positivism such as Moritz Schlick, Rudolf Carnap, or Reichenbach. They argued that Poincaré's conventionalism could be modified, as to bring it into accord with relativity. Although it is true that the basic assumptions of Newtonian mechanics are simpler, it can only be brought into accord with modern experiments by inventing auxiliary hypotheses. On the other hand, relativity doesn't need such hypotheses, thus from a conceptual viewpoint, relativity is in fact simpler than Newtonian mechanics.\n\nSome proponents of Philosophy of Life, Vitalism, Critical realism (in German speaking countries) argued that there is a fundamental difference between physical, biological and psychological phenomena. For example, Henri Bergson (1921), who otherwise was a proponent of special relativity, argued that time dilation cannot be applied to biological organisms, therefore he denied the relativistic solution of the twin paradox. However, those claims were rejected by Paul Langevin, André Metz and others. Biological organisms consist of physical processes, so there is no reason to assume that they are not subject to relativistic effects like time dilation.\n\nBased on the philosophy of Fictionalism, the philosopher Oskar Kraus (1921) and others claimed that the foundations of relativity were only fictitious and even self-contradictory. Examples were the constancy of the speed of light, time dilation, length contraction. These effects appear to be mathematically consistent as a whole, but in reality they allegedly are not true. Yet, this view was immediately rejected. The foundations of relativity (such as the equivalence principle or the relativity principle) are not fictitious, but based on experimental results. Also, effects like constancy of the speed of light and relativity of simultaneity are not contradictory, but complementary to one another.\n\nIn the Soviet Union (mostly in the 1920s), philosophical criticism was expressed on the basis of dialectic materialism. The theory of relativity was rejected as anti-materialistic and speculative, and a mechanistic worldview based on \"common sense\" was required as an alternative. Similar criticisms also occurred in the People's Republic of China during the Cultural Revolution. (On the other hand, other philosophers considered relativity as being compatible with Marxism.)\n\nAlthough Planck already in 1909 compared the changes brought about by relativity with the Copernican Revolution, and although special relativity was accepted by most of the theoretical physicists and mathematicians by 1911, it was not before publication of the experimental results of the eclipse expeditions (1919) by a group around Arthur Stanley Eddington that relativity was noticed by the public. Following Eddington's publication of the eclipse results, Einstein was glowingly praised in the mass media, and was compared to Nikolaus Copernicus, Johannes Kepler and Isaac Newton, which caused a popular \"relativity hype\" (\"Relativitätsrummel\", as it was called by Sommerfeld, Einstein, and others). This triggered a counter-reaction of some scientists and scientific laymen who could not accept the concepts of modern physics, including relativity theory and quantum mechanics. The ensuing public controversy regarding the scientific status of Einstein's theory of gravity, which was unprecedented, was partly carried out in the press. Some of the criticism was not only directed to relativity, but personally at Einstein as well, who some of his critics accused of being behind the promotional campaign in the German press. \n\nSome academic scientists, especially experimental physicists such as the Nobel laureates Philipp Lenard and Johannes Stark, as well as Ernst Gehrcke, Stjepan Mohorovičić, Rudolf Tomaschek and others criticized the increasing abstraction and mathematization of modern physics, especially in the form of relativity theory, and later quantum mechanics. It was seen as a tendency to abstract theory building, connected with the loss of intuitive \"common sense\". In fact, relativity was the first theory, in which the inadequacy of the \"illustrative\" classical physics was thought to have been demonstrated. Some of Einstein's critics ignored these developments and tried to revitalize older theories, such as aether drag models or emission theories (see \"Alternative Theories\"). However, those qualitative models were never sufficiently advanced to compete with the success of the precise experimental predictions and explanatory powers of the modern theories. Additionally, there was also a great rivalry between experimental and theoretical physicists, as regards the professorial activities and the occupation of chairs at German universities. The opinions clashed at the \"Bad Nauheim debates\" in 1920 between Einstein and (among others) Lenard, which attracted much attention in the public.\n\nIn addition, there were many critics (with or without physical training) whose ideas were far outside the scientific mainstream. These critics were mostly people who had developed their ideas long before the publication of Einstein's version of relativity, and they tried to resolve in a straightforward manner some or all of the enigmas of the world. Therefore, Wazeck (who studied some German examples) gave to these \"free researchers\" the name \"world riddle solver\" (\"Welträtsellöser\", such as Arvid Reuterdahl, Hermann Fricke or Johann Heinrich Ziegler). Their views had their quite different roots in monism, Lebensreform, or occultism. Their views were typically characterized by the fact that they practically rejected the entire terminology and the (primarily mathematical) methods of modern science. Their works were published by private publishers, or in popular and non-specialist journals. It was significant for many \"free researchers\" (especially the monists) to explain all phenomena by intuitive and illustrative mechanical (or electrical) models, which also found its expression in their defense of the aether. For this reason they objected to the abstractness and inscrutability of the relativity theory, which was considered a pure calculation method that cannot reveal the true reasons underlying the phenomena. The \"free researchers\" often used Mechanical explanations of gravitation, in which gravity is caused by some sort of \"aether pressure\" or \"mass pressure from a distance\". Such models were regarded as an illustrative alternative to the abstract mathematical theories of gravitation of both Newton and Einstein. Additionally, also the enormous self-confidence of the \"free researchers\" is noteworthy, since they not only believed to have solved all the riddles of the world, but also had the expectation that they would rapidly convince the scientific community.\n\nSince Einstein rarely defended himself against these attacks, this task was undertaken by other relativity theoreticians, who (according to Hentschel) formed some sort of \"defensive belt\" around Einstein. Some representatives were Max von Laue, Max Born, etc. and on popular-scientific and philosophical level Hans Reichenbach, André Metz etc., who led many discussions with critics in semi-popular journals and newspapers. However, most of these discussions failed from the start. Physicists like Gehrcke, some philosophers, and the \"free researchers\" were so obsessed with their own ideas and prejudices that they were unable to grasp the basics of relativity; consequently, the participants of the discussions were talking past each other. In fact, the theory that was criticized by them was not relativity at all, but rather a caricature of it. The \"free researchers\" were mostly ignored by the scientific community, but also, in time, respected physicists such as Lenard and Gehrcke found themselves in a position outside the scientific community. However, the critics didn't believe that this was due to their incorrect theories, but rather due to a conspiracy of the relativistic physicists (and in the 1920s & 1930s of the Jews as well), which allegedly tried to put down the critics, and to preserve and improve their own positions within the academic world. For example, Gehrcke (1920/24) held that the propagation of relativity is a product of some sort of mass suggestion. Therefore, he instructed a media monitoring service to collect over 5000 newspaper clippings which were related to relativity, and published his findings in a book. However, Gehrcke's claims were rejected, because the simple existence of the \"relativity hype\" says nothing about the validity of the theory, and thus it cannot used for or against relativity.\n\nAfterward, some critics tried to improve their positions by the formation of alliances. One of them was the \"Academy of Nations\", which was founded in 1921 in the USA by Robert T. Browne and Arvid Reuterdahl. Other members were Thomas Jefferson Jackson See and as well as Gehrcke and Mohorovičić in Germany. It is unknown whether other American critics such as Charles Lane Poor, Charles Francis Brush, Dayton Miller were also members. The alliance disappeared as early as the mid-1920s in Germany and by 1930 in the USA.\n\nShortly before and during World War I, there appeared some nationalistically motivated criticisms of relativity and modern physics. For example, Pierre Duhem regarded relativity as the product of the \"too formal and abstract\" German spirit, which was in conflict with the \"common sense\". Similarly, popular criticism in the Soviet Union and China, which partly was politically organized, rejected the theory not because of factual objections, but as ideologically motivated as the product of western decadence.\n\nSo in those countries, the Germans or the Western civilization were the enemies. However, in Germany the Jewish ancestry of some leading relativity proponents such as Einstein and Minkowski made them targets of racially minded critics, although many of Einstein's German critics did not show evidence of such motives. The engineer Paul Weyland, a known nationalistic agitator, arranged the first public meeting against relativity in Berlin in 1919. While Lenard and Stark were also known for their nationalistic opinions, they declined to participate in Weyland's rallies, and Weyland's campaign eventually fizzled out due to a lack of prominent speakers. Lenard and others instead responded to Einstein's challenge to his professional critics to debate his theories at the scientific conference held annually at Bad Nauheim. While Einstein's critics, assuming without any real justification that Einstein was behind the activities of the German press in promoting the triumph of relativity, generally avoided antisemitic attacks in their earlier publications, it later became clear to many observers that antisemitism did play a significant role in some of the attacks.\n\nReacting to this underlying mood, Einstein himself openly speculated in a newspaper article that in addition to insufficient knowledge of theoretical physics, antisemitism at least partly motivated their criticisms. Some critics, including Weyland, reacted angrily and claimed that such accusations of antisemitism were only made to force the critics into silence. However, subsequently Weyland, Lenard, Stark and others clearly showed their antisemitic biases by beginning to combine their criticisms with racism. For example, Theodor Fritsch emphasized the alleged negative consequences of the \"Jewish spirit\" within relativity physics, and the far right-press continued this propaganda unhindered. After the murder of Walther Rathenau (1922) and murder threats against Einstein, he left Berlin for some time. Gehrcke's book on \"The mass suggestion of relativity theory\" (1924) was not antisemitic itself, but it was praised by the far-right press as describing an alleged typical Jewish behavior, which was also imputed to Einstein personally. Philipp Lenard in 1922 spoke about the \"foreign spirit\" as the foundation of relativity, and afterward he joined the Nazi party in 1924; Johannes Stark did the same in 1930. Both were proponents of the so-called German Physics, which only accepted scientific knowledge based on experiments, and only if accessible to the senses. According to Lenard (1936), this is the \"Aryan physics or physics by man of Nordic kind\" as opposed to the alleged formal-dogmatic \"Jewish physics\". Additional antisemitic critics can be found in the writings of Wilhelm Müller, Bruno Thüring and others. For example, Müller erroneously claimed that relativity was a purely \"Jewish affair\" and it would correspond to the \"Jewish essence\" etc., while Thüring made comparisons between the Talmud and relativity.\n\nSome of Einstein's critics, like Lenard, Gehrcke and Reuterdahl, accused him of plagiarism, and questioned his priority claims to the authorship of relativity theory. The thrust of such allegations was to promote more traditional alternatives to Einstein's abstract hypothetico-deductive approach to physics, while Einstein himself was to be personally discredited. It was argued by Einstein's supporters that such personal accusations were unwarranted, since the physical content and the applicability of former theories were quite different from Einstein's theory of relativity. However, others argued that between them Poincaré and Lorentz had earlier published several of the core elements of Einstein's 1905 relativity paper, including a generalized relativity principle that was intended by Poincaré to apply to all physics. Some examples:\n\nSome contemporary historians of science have revived the question as to whether Einstein was possibly influenced by the ideas of Poincaré, who first stated the relativity principle and applied it to electrodynamics, developing interpretations and modifications of Lorentz's electron theory that appear to have anticipated what is now called special relativity. Another discussion concerns a possible mutual influence between Einstein and David Hilbert as regards completing the field equations of general relativity (see Relativity priority dispute).\n\nA collection of various criticisms can be found in the book \"Hundert Autoren gegen Einstein\" (\"A Hundred Authors Against Einstein\"), published in 1931. It contains very short texts from 28 authors, and excerpts from the publications of another 19 authors. The rest consists of a list that also includes people who only for some time were opposed to relativity. Besides philosophic objections (mostly based on Kantianism), also some alleged elementary failures of the theory were included; however, as some commented, those failures were due to the authors' misunderstanding of relativity. For example, Hans Reichenbach described the book as an \"accumulation of naive errors\", and as \"unintentionally funny\". Albert von Brunn interpreted the book as a backward step to the 16th and 17th century, and Einstein said, in response to the book, that if he were wrong, then one author would have been enough.\n\nAccording to Goenner, the contributions to the book are a mixture of mathematical–physical incompetence, hubris, and the feelings of the critics of being suppressed by contemporary physicists advocating for the new theory. The compilation of the authors show, Goenner continues, that this was not a reaction within the physics community—only one physicist (Karl Strehl) and three mathematicians (Jean-Marie Le Roux, Emanuel Lasker and Hjalmar Mellin) were present—but a reaction of an inadequately educated academic citizenship, which didn't know what to do with relativity. As regards the average age of the authors: 57% were substantially older than Einstein, one third was around the same age, and only two persons were substantially younger. Two authors (Reuterdahl, von Mitis) were antisemitic and four others were possibly connected to the Nazi movement. On the other hand, no antisemitic expression can be found in the book, and it also included contributions of some authors of Jewish ancestry (Salomo Friedländer, Ludwig Goldschmidt, Hans Israel, Emanuel Lasker, Oskar Kraus, Menyhért Palágyi).\n\nThe theory of relativity is considered to be self-consistent, is consistent with many experimental results, and serves as the basis of many successful theories like quantum electrodynamics. Therefore, fundamental criticism (like that of Herbert Dingle, Louis Essen, Petr Beckmann, Maurice Allais and Tom van Flandern) has not been taken seriously by the scientific community, and due to the lack of quality of many critical publications (found in the process of peer review) they were rarely accepted for publication in reputable scientific journals. Just as in the 1920s, most critical works are published in small publications houses, alternative journals (like \"Apeiron\" or \"Galilean Electrodynamics\"), or private websites. Consequently, where criticism of relativity has been dealt with by the scientific community, it has mostly been in historical studies.\n\nHowever, this does not mean that there is no further development in modern physics. The progress of technology over time has led to extremely precise ways of testing the predictions of relativity, and so far it has successfully passed all tests (such as in particle accelerators to test special relativity, and by astronomical observations to test general relativity). In addition, in the theoretical field there is continuing research intended to unite general relativity and quantum theory. The most promising models are string theory and loop quantum gravity. Some variations of those models also predict violations of Lorentz invariance on a very small scale.\n\n\n\n\n\n\n"}
{"id": "54286399", "url": "https://en.wikipedia.org/wiki?curid=54286399", "title": "Curie's Principle", "text": "Curie's Principle\n\nCurie's Principle or Curie's Symmetry Principle, is a maxim about cause and effect formulated by Pierre Curie in 1894: \"the symmetries of the causes are to be found in the\neffects\".\n"}
{"id": "2024212", "url": "https://en.wikipedia.org/wiki?curid=2024212", "title": "Dinosaur in a Haystack", "text": "Dinosaur in a Haystack\n\nDinosaur in a Haystack (1995) is the seventh volume of collected essays by the Harvard paleontologist Stephen Jay Gould. The essays were culled from his monthly column \"The View of Life\" published in \"Natural History\" magazine, which Gould contributed for 27 years. The book deals with themes familiar to Gould's writing: evolution, science biography, probabilities, and strange oddities found in nature. \n\nHis essay \"Poe's Greatest Hit\" analyzes the controversial conchology textbook \"The Conchologist's First Book\" (1839), edited by Edgar Allan Poe. Poe's volume on natural history sold out within two months, and was his only book republished during his lifetime. Essay \"Dinomania\" is a review of Michael Crichton's novel and Steven Spielberg's blockbuster film, \"Jurassic Park\".\n\nGould's book received favorable reviews in \"Publishers Weekly\" and \"The New York Times\".\n\n"}
{"id": "41710616", "url": "https://en.wikipedia.org/wiki?curid=41710616", "title": "Diseases from Space", "text": "Diseases from Space\n\nDiseases from Space is a book published in 1979 that authored by astronomers Fred Hoyle and Chandra Wickramasinghe, where they propose that many of the most common diseases which afflict humanity, such as influenza, the common cold and whooping cough, have their origins in extraterrestrial sources. The two authors argue the case for outer space being the main source for these pathogens- or at least their causative agents. \n\nThe claim connecting terrestrial disease and extraterrestrial pathogens was rejected by the scientific community.\n\nFred Hoyle and Chandra Wickramasinghe spent over 20 years investigating the nature and composition of interstellar dust. Though many hypotheses regarding this dust had been postulated by various astronomers since the middle of the 19th century, all were found to be wanting as and when new data on the gas and dust clouds became available. probably polymers. Chandra Wickramasinghe proposed the existence of polymeric composition based on the molecule formaldehyde (HCO).\n\nIn 1974 Wickramasinghe first proposed the hypothesis that some dust in interstellar space was largely organic (containing carbon and nitrogen), and followed this up with other research confirming the hypothesis. Wickramasinghe also proposed and confirmed the existence of polymeric compounds based on the molecule formaldehyde (HCO). Fred Hoyle and Wickramasinghe later proposed the identification of bicyclic aromatic compounds from an analysis of the ultraviolet extinction absorption at 2175A., thus demonstrating the existence of polycyclic aromatic hydrocarbon molecules in space.\n\nHoyle and Wickramasinghe went further and speculated that the overall spectroscopic data of cosmic dust and gas clouds also matched those for desiccated bacteria. This led them to conclude that diseases such as influenza and the common cold are incident from space and fall upon the Earth in what they term \"pathogenic patches.\" Hoyle and Wickramasinghe viewed the process of evolution in a manner at variance with the standard Darwinian model. They speculated that genetic material in the form of incoming pathogens from the cosmos provided the mechanism for driving the evolutionary engine. Hoyle passed away in 2001, and Wickramasinghe still advocates for these views and beliefs.\n\nThe claim connecting terrestrial disease and extraterrestrial pathogens was rejected and dismissed by the scientific community. On 24 May 2003 \"The Lancet\" journal published a letter from Wickramasinghe, jointly signed by Milton Wainwright and Jayant Narlikar, in which they speculate that the virus that causes severe acute respiratory syndrome (SARS) could be extraterrestrial in origin instead of originating from chickens. \"The Lancet\" subsequently published three responses to this letter, showing that the hypothesis was not evidence-based, and casting doubts on the quality of the experiments referenced by Wickramasinghe in his letter.\n\nFirst published in 1979 by J.M. Dent & Sons Ltd.\nPublished in 1980 by Harper & Row.\nPublished in 1981 by Sphere Books Ltd.\n\n"}
{"id": "2666148", "url": "https://en.wikipedia.org/wiki?curid=2666148", "title": "Domestication theory", "text": "Domestication theory\n\nDomestication theory is an approach in science and technology studies and media studies that describe the processes by which innovations, especially new technology is 'tamed' or appropriated by its users. First, technologies are integrated into everyday life and adapted to daily practices. Secondly, the user and its environment change and adapt accordingly. Thirdly, these adaptations feedback into innovation processes in industry, shaping the next generation of technologies and services.\nThe theory was initially developed to help understand the adoption and use of new media technologies by households (Silverstone et al. 1992), but has since been expanded in the innovation literature as a tool to understand technologies and innovations entering any consuming unit (workplace, country etc. e.g. Lie et al., Habib, Punie, Sørensen) that can be analysed economically, culturally and sociologically. The domestication approach considers both the practical and the symbolic aspects of the adoption and use of technologies, showing how these two elements- the meanings of things, and their materiality, are equally important understanding how technologies become part of everyday life. It is a foremost a social theory as it highlights the negotiations, challenges to power and control, rule-making and breaking that accompany the introduction of technologies into any social setting.\n\nOne variant of domestication theory identified three stages of technology being adapted by users. According to Nancy K. Baym, these three stages are (1) initially marvelous and strange, (2) then become capable of creating greatness and horror and (3) and are then so ordinary as to be invisible (Baym, 2015). This can also be thought of as (1) euphoria, (2) moral panic, and (3) domestication. An example of this is the introduction of video games into society. Initially, there was a euphoric response to video games as it had the potential to improve hand, eye, and brain coordination. Then, moral panic set in and there was a fear of violence, addiction, and obesity. Lastly, there was a domestication of video games with acceptance of the technology as an ordinary part of society. \n\nThe Domestication approach has roots in cultural studies of media use, but is informed by Science and technology studies, gender studies of household technology, sociology of everyday life, consumption studies and innovation studies, and has been most widely used in studying the mass adoption of computers, the internet and mobile phones.\n\nAs a strand of the Social shaping of technology approach to understanding how technology is created, Domestication theory highlights the role of users in innovation - the work done by individuals and communities in order to make a technology from the outside do practical work, and make sense within that community. This strand of work links to the role of end users, lead users etc. in long term innovation processes (Williams et al. 2004).\n\nDomestication studies are generally done using qualitative methods, such as long interviews and ethnography to explore the emerging meanings of technologies, and the changing routines, and conflicts that would not normally be accessible to quantitative methods.\n\nThe Domestication approach uses a number of different concepts to distinguish various aspects of the process. For example: Appropriation is the process of bringing a technology into a household, or another local cultural context; Conversion is the remaking of the meanings, or values and norms associated with the technology and the transfer of these back to the 'outside' world.\n\nThe principal criticism of the domestication approach is its reliance on detailed case studies, and its rather descriptive approach which is difficult to turn into prescriptive lessons of the type required by business and policy makers. However, this rich-descriptive approach is also its strength: it enables processes and the complex interplay of artifacts and cultural values to be explored in much more depth than individualistic, quantitative methods.\n\nThe Domestication approach, describing the integration of technologies into social relationships and structures using evidence obtained using qualitative methods, stands in sharp contrast to individualistic and quantitative approaches (such as Technology acceptance model) of North-American marketing and IS research, that draw on primarily psychological models.\n\n\n"}
{"id": "7236219", "url": "https://en.wikipedia.org/wiki?curid=7236219", "title": "Dry needling", "text": "Dry needling\n\nDry needling, also known as myofascial trigger point dry needling, is an unproven technique in alternative medicine similar to acupuncture. It involves the use of either solid filiform needles or hollow-core hypodermic needles for therapy of muscle pain, including pain related to myofascial pain syndrome. Dry needling is sometimes also known as intramuscular stimulation (IMS).\n\nWhile many studies have been performed to test the efficacy of dry needling as a treatment for muscle pain, there remains no scientific consensus as to whether or not it is effective. Some results suggest that it is an effective treatment for certain kinds of muscle pain, while other studies have shown no benefit compared to a placebo. There are not enough high-quality studies of the technique to draw clear conclusions about its efficacy.\n\nThe origin of the term \"dry needling\" is attributed to Janet G. Travell. In her book, \"Myofascial Pain and Dysfunction: Trigger Point Manual\", Travell uses the term \"dry needling\" to differentiate between two hypodermic needle techniques when performing trigger point therapy. However, Travell did not elaborate on the details on the techniques of dry needling; the current techniques of dry needling were based on the traditional and western medical acupuncture. The two techniques Travell described are the injection of a local anesthetic and the mechanical use of a hypodermic needle without injecting a solution (Travell, Simons, & Simons, 1999, pp. 154–155). Travell preferred a 22-gauge, 1.5-in hypodermic needle for trigger point therapy and used this needle for both injection therapy and dry needling. Travell never used an acupuncture needle. Travell had access to acupuncture needles but reasoned that they were far too thin for trigger point therapy. She preferred hypodermic needles because of their strength and tactile feedback: \"A 22-gauge, 3.8-cm (1.5-in) needle is usually suitable for most superficial muscles. In hyperalgesic patients, a 25-gauge, 3.8-cm (1.5-in) needle may cause less discomfort, but will not provide the clear \"feel\" of the structures being penetrated by the needle and is more likely to be deflected by the dense contraction knots that are the target... A 27-gauge needle, 3.8-cm (1.5-in) needle is even more flexible; the tip is more likely to be deflected by the contraction knots and it provides less tactile feedback for precision injection\" (Travell, Simons, & Simons, 1999, p. 156).\n\nThe use of a hypodermic needle for dry needling was described by Chang-Zern Hong in his research paper on \"Lidocaine Injection Versus Dry Needling to Myofascial Trigger Point\". In his research, he describes the procedure for trigger point or MTrP injection and dry needling by using a 27-gauge hypodermic needle -in long (Hong, 1994). Both Travell and Hong used hypodermic needles for dry needling. Hong, like Travell, did not use an acupuncture needle for dry needling.\n\nAlthough dry needling originally utilized only hypodermic needles due to the concern that solid needles had neither the strength or tactile feedback that hypodermic needles provided and that the needle could be deflected by \"dense contraction knots\", those concerns have proven unfounded and many healthcare practitioners who perform dry needling have found that the acupuncture needles not only provides better tactile feedback but also penetrate the \"dense muscle knots\" better and are easier to manage and caused less discomfort to patients. For that reason, both the use of hypodermic needles and the use of acupuncture needles are now accepted in dry needling practice. Ofttimes practitioners who use hypodermic needles also provide trigger point injection treatment to patients and therefore find the use of hypodermic needles a better choice. As their use became more common, some dry needling practitioners without acupuncture in their scope of practice, started to refer to these needles by their technical design term as \"solid filiform needles\" as opposed to the FDA designation \"acupuncture needle\".\n\nThe \"solid filiform needle\" used in dry needling is regulated by the FDA as a Class II medical device described in the code titled \"Sec. 880.5580 Acupuncture needle\" as \"a device intended to pierce the skin in the practice of acupuncture\". Per the Food and Drug Act of 1906 and the subsequent Amendments to said act, the FDA definition applies to how the needles can be marketed and does not mean that acupuncture is the only medical procedure where these needles can be used. Dry needling using such a needle contrasts with the use of a hollow hypodermic needle to inject substances such as saline solution, botox or corticosteroids to the same point. In a small number of studies, the use of a solid needle has been found to be as effective as injection of substances in such cases as relief of pain in muscles and connective tissue.\n\nThe Founder of Integrative Systemic Dry Needling (ISDN), Yun-Tao Ma, has been spearheading the \"dry needling\" movement in the United States. Ma states, \"Although ISDN originated in traditional Chinese methods, it has developed from the ancient empirical approach to become modern medical art rooted in evidence-based thinking and practice.\" Ma then contradicts himself stating, \"Dry needling technique is a modern Western medical modality that is not related to traditional Chinese acupuncture in any way. Dry needling has its own theoretical concepts, terminology, needling technique and clinical application.\" \nMa realizing both the self-contradictions and the legal ramifications of dry needling being rooted in acupuncture and Chinese medicine has since taken down all information in his bios regarding his education in Chinese Medicine and being a Licensed Acupuncturist in the United States.\n\nThe American Academy of Orthopedic Manual Physical Therapists (AAOMPT) states:\n\nThe statement above is self-explanatory on the functional, physiological and medical aspect of treatment. His book Manual of Dry Needling Techniques Color Edition (2) (Volume 1) is a basic reference text for the therapists who are trained in the method of dry needling procedures in accordance with the norm of the practice of their respective countries. The basic steps given in the book can make a practicing therapist use dry needling technique for the subjects in different clinical conditions. The text focus not only on the steps needed to be performed but also focus on what should not be done by a therapist while performing the procedure. At work, we have taken all the guidelines given by OSHA for blood borne diseases as well as WHO guideline on workplace and hand hygiene.\n\nDry needling for the treatment of myofascial (muscular) trigger points is based on theories similar, but not exclusive, to traditional acupuncture; both acupuncture and dry needling target the trigger points, which is a direct and palpable source of patient pain. However, dry needling theory is only beginning to describe the complex sensation referral patterns that have been documented as \"channels\" or \"meridians\" in Chinese Medicine. Dry needling, and its treatment techniques and desired effects, would be most directly comparable to the use of 'a-shi' points in acupuncture. What further distinguishes dry needling from traditional acupuncture is that it does not use the full range of traditional theories of Chinese Medicine which is used to treat not only pain but other non-musculoskeletal issues which often are the cause of pain. The distinction between trigger points and acupuncture points for the relief of pain is blurred. As reported by Melzack, et al., there is a high degree of correspondence (71% based on their analysis) between published locations of trigger points and classical acupuncture points for the relief of pain. The debated distinction between dry needling and acupuncture has become a controversy because it relates to an issue of scope of practice of various professions.\n\nIn the treatment of trigger points for persons with myofascial pain syndrome, dry needling is an invasive procedure in which a filiform needle is inserted into the skin and muscle directly at a myofascial trigger point. A myofascial trigger point consists of multiple contraction knots, which are related to the production and maintenance of the pain cycle. Deep dry needling for treating trigger points was first introduced by Czech physician Karel Lewit in 1979.\nLewit had noticed that the success of injections into trigger points in relieving pain was apparently unconnected to the analgesic used.\n\nProper dry needling of a myofascial trigger point will elicit a local twitch response (LTR), which is an involuntary spinal cord reflex in which the muscle fibers in the taut band of muscle contract. The LTR indicates the proper placement of the needle in a trigger point. Dry needling that elicits LTRs improves treatment outcomes, and may work by activating endogenous opioids. The activation of the endogenous opioids is for an analgesic effect using the Gate Control Theory of Pain. Inserting the needle can itself cause considerable pain, although when done by well-trained practitioners that is not a common occurrence. No study to date has reported the reliability of trigger point diagnosis and physical diagnosis cannot be recommended as a reliable test for the diagnosis of trigger points.\n\nThere is currently no standardized form of dry needling, no body of evidence that indicates its efficacy, and there is no medical action pathway that provides a theoretical basis for why dry needling should be efficacious. Many of the studies published about dry needling do not have strong evidence; either the studies were not randomized, contained small sample sizes, had high dropout rates, used active interventions in the control group, did not follow the minimally acceptable criteria to diagnose a myofascial trigger point, or did not clearly state that myofascial trigger points were the sole cause for the pain. For example, in a systematic review on needling therapies in the management of myofascial trigger points, only 8 of the 23 trials described the minimally acceptable criteria for diagnosing a trigger point. Locating the trigger point for dry needling is the basis for performing dry needling and should, therefore, be documented in each study performing this technique. In the same review, two studies tested the efficacy beyond placebo of dry needling in the treatment of myofascial trigger point pain, but, in one, the dropout rate was 48% and it was neither blinded nor randomized, and the other study used potentially active interventions in the control group. Another concluded that dry needling can reduce pain, thus improving mood, function, and disability. The study used the dry needling on trigger points to relieve pain in patients with chronic myofascial pain.\n\nAnother systematic review concluded that dry needling for the treatment of myofascial pain syndrome in the lower back appeared to be a useful addition to standard therapies, but stated clear recommendations could not be made because the published studies are small and of low quality. A 2007 meta-analysis examining dry needling of myofascial trigger points concluded that the effect of needling was not significantly different to that of placebo controls, though the trend in the results could be compatible with a treatment effect. One study (Lorenzo et al. 2004) did show a short-term reduction in shoulder pain in stroke patients who received needling with standard rehabilitation compared to those who received standard care alone, but the study was open-label and measurement timings differed, limiting the use of the study. Again the small sample size and poor quality of studies was highlighted.\nA 2013 systematic review and meta-analysis released by JOSPT on \"effectiveness of dry needling for upper-quarter myofascial pain\" recommends the usage of dry needling, compared to sham or placebo, for decreasing pain immediately after treatment and at 4 weeks in patients with upper quarter myofascial pain syndrome. However, the authors caution that \"the limited number of studies performed to date, combined with methodological flaws in many of the studies, prompts caution in interpreting the results of the meta-analysis performed\".\n\nA 2014 review of dry needling found insufficient high-quality evidence for the use of direct dry needling for short and long-term pain and disability reduction in patients with musculoskeletal pain syndromes.\nThe same review found that robust evidence validating the clinical diagnostic criteria for trigger point identification or diagnosis is lacking and that high-quality studies demonstrate that manual examination for the identification and localization of a trigger point is neither valid nor reliable between-examiners. A 2017 systematic review and meta-analysis found very little evidence supporting the use of trigger point dry needling to treat upper shoulder pain and dysfunction.\n\nA 2018 literature review collated 16 eligible studies and found that although it appeared to be a superior technique to acupuncture for lower back pain current evidence is not robust enough to draw a clear conclusion about safety and efficacy. Another found that although dry needling appears to reduce pain for patients in all 11 studies considered, it was not clear what, if any other than placebo, mechanism was acting.\n\nMany physical therapists and chiropractors have asserted that they are not practicing acupuncture when dry needling. They assert that much of the basic physiological and biomechanical knowledge that dry needling utilizes is taught as part of their core physical therapy and chiropractic education and that the specific dry needling skills are supplemental to that knowledge and not exclusive to acupuncture. However, the originators and proponents of dry needling acknowledged that certain aspects of this techniques were inspired by acupuncture although they also acknowledge that the medical basis for it is purely Western Medicine in nature and therefore is not validly a subset of acupuncture and is a separate medical process. Many acupuncturists have argued that dry needling appears to be an acupuncture technique requiring minimal training that has been re-branded under a new name (\"dry needling\"). Whether dry needling is considered to be acupuncture depends on the definition of acupuncture, and it is argued that trigger points do not correspond to acupuncture points or meridians. They correspond by definition to the ad hoc category of 'a-shi' acupoints. It is important to note that this category of points is not necessarily distinct from other formal categories of acupoints. In 1983, Janet Travell et al. described trigger point locations as 92% in correspondence with known acupuncture points. In 2006, Peter T. Dorsher, acupuncturist at the Mayo Clinic, concludes that the two point systems are in over 90% agreement. In 2009, Dorsher and Fleckenstein conclude that the strong (up to 91%) consistency of the distributions of trigger point regions’ referred pain patterns to acupuncture meridians provides evidence that trigger points most likely represent the same physiological phenomenon as acupuncture points in the treatment of pain disorders. An article in Acupuncture Today (May 2011, p. 3, \"Scope and Standards for Acupuncture: Dry Needling?\") further corroborates the 92% correspondence of trigger points to acupuncture points. In 2011, The Council of Colleges of Acupuncture and Oriental Medicine (CCAOM) published a position paper describing dry needling as an acupuncture technique.\nThe North Carolina Acupuncture Licensing Board has published a position statement asserting that dry needling is acupuncture and thus is covered by the North Carolina Acupuncture Licensing law, and is not within the present scope of practice of Physical Therapists, and Physical Therapists are not among the professions exempt from the law. The Attorney General was asked for an opinion by the North Carolina Acupuncture Licensing Board which he gave dated Dec 1st, 2011 saying that \"In our opinion, the Board of Physical Therapy Examiners\nmay determine that dry needling is within the scope of practice of physical therapy if it conducts\nrulemaking under the Administrative Procedure Act and adopts rules that relate dry needling to\nthe statutory definition of practice of physical therapy.\" But that is a matter of opinion and not a matter of law. The North Carolina Rules Review Committee of the legislative branch found that the North Carolina Physical Therapy Board had no statutory authority for the proposed rule. The Physical Therapy board subsequently decided that they had the right to declare dry needling within scope anyway \"the Board believes physical therapists can continue to perform dry needling so long as they possess the requisite education and training required by N.C.G.S. § 90-270.24(4), but there are no regulations to set the specific requirements for engaging in dry needling.\".\n\nIn May 2011 the Oregon Board of Chiropractic Examiners ruled to allow \"dry needling\" into the chiropractic scope of practice with 24 hours of training. In July 2011 the Court of Appeals of the State of Oregon issued an injunction, preventing chiropractors from practicing dry needling until the case is heard in court. The document issued by the court states that \"dry needling\" is \"substantially the same\" as acupuncture and that the \"respondent has not explained how 24 hours of training, with no clinical component, provides sufficient training to chiropractors to adequately protect patients.\" In September 2011, the Oregon Board of Chiropractic Examiners And Oregon Attorney General appealed said order on the grounds that they feel the commissioner who issued the order was mistaken in his assertion. On November 10, 2011, The Court of Appeals of the State of Oregon issued an Order Denying the Motion for Reconsideration. The effect of said ruling is that the entire Appeals Court will now determine if the stay was appropriate. The stay is relevant \"only\" in the State of Oregon.\n\nIn January 2014, The Oregon Court of Appeals ruled that the Oregon Board of Chiropractic Examiners did not have the statutory authority to include dry needling in the scope of practice for chiropractors in that state. The ruling did not address whether chiropractors have the medical expertise to use dry needling or whether the training they were being given was adequate. Pending further discussion of training requirements the Oregon Physical Therapist Licensing Board has advised all Oregon physical therapists against practicing dry needling. They have not changed their ruling that dry needling is within the scope of practice for Oregon Physical Therapists.\n\nThe American Medical Association adopted a policy in 2016 that said physical therapists and other non-physicians practicing dry needling should – at a minimum – have standards that are similar to the ones for training, certification, and continuing education that exist for acupuncture. AMA board member Russell W. H. Kridel, M.D. stated \"Lax regulation and nonexistent standards surround this invasive practice. For patients' safety, practitioners should meet standards required for licensed acupuncturists and physicians.\"\n\n"}
{"id": "9560227", "url": "https://en.wikipedia.org/wiki?curid=9560227", "title": "E-research", "text": "E-research\n\nThe term e-Research (alternately spelled eResearch) refers to the use of information technology to support existing and new forms of research. E-research extends e-Science and cyberinfrastructure to other disciplines, including the humanities and social sciences.\n\nExamples of e-Research problems range across disciplines which include:\n\nE-Research includes research activities that use a spectrum of advanced information and communication technology (ICT) capabilities. It embraces new research methodologies emerging from increasing access to:\n\n\nSpecialist services, centres or programmes instituted to support Australian data and technology intensive research operate under the umbrella term: eResearch. In March 2012, representatives from these eResearch groups came together to discuss the need build a \"collaborative program to strengthen eResearch and address issues facing the sector nationally\". The Australian eResearch Organisation (AeRO) emerged from this forum as \"a collaborative organisation of national and state-based research organisations to advance eResearch implementation and innovation in Australia\". Professionals working in Australian eResearch annually convene a conference known as: eResearch Australasia.\n\n\n"}
{"id": "21954724", "url": "https://en.wikipedia.org/wiki?curid=21954724", "title": "European grid", "text": "European grid\n\nThe European grid is a proposed, multipurpose Pan-European mapping standard. It is based on the ETRS89 Lambert Azimuthal Equal-Area projection coordinate reference system, with the centre of the projection at the point 52° N, 10° E and false easting: x0 = 4321000 m, false northing: y0 = 3210000 m (CRS identifier in Inspire: ETRS89-LAEA).\nThe grid is designated as Grid_ETRS89-LAEA5210. For identification of an individual resolution level, the name is extended by identification of cell size in metres (example: _100K).\n\nThe origin of Grid_ETRS89-LAEA5210 coincides with the false origin of the ETRS89-LAEA coordinate reference system (x=0, y=0). Grid points of grids based on ETRS89-LAEA must coincide with grid points at Grid_ETRS89-LAEA5210.\n\nThe grid is defined as hierarchical one in metric coordinates in power of 10.\nThe resolution of the grid is 1m, 10m, 100m, 1000m, 10,000m, 100,000m.\nThe grid orientation is south-north, west-east.\nThe reference point of the grid cell for grids based on ETRS89-LAEA is the lower left corner of the grid cell.\n\nETRS89\nLambert Azimuthal Equal-Area projection\n\n"}
{"id": "11627", "url": "https://en.wikipedia.org/wiki?curid=11627", "title": "Faith healing", "text": "Faith healing\n\nFaith healing is the practice of prayer and gestures (such as laying on of hands) that are believed by some to elicit divine intervention in spiritual and physical healing, especially the Christian practice. Believers assert that the healing of disease and disability can be brought about by religious faith through prayer and/or other rituals that, according to adherents, can stimulate a divine presence and power. Religious belief in divine intervention does not depend on empirical evidence that faith healing achieves an evidence-based outcome.\n\nClaims \"attributed to a myriad of techniques\" such as prayer, divine intervention, or the ministrations of an individual healer can cure illness have been popular throughout history. There have been claims that faith can cure blindness, deafness, cancer, AIDS, developmental disorders, anemia, arthritis, corns, defective speech, multiple sclerosis, skin rashes, total body paralysis, and various injuries. Recoveries have been attributed to many techniques commonly classified as faith healing. It can involve prayer, a visit to a religious shrine, or simply a strong belief in a supreme being.\n\nMany people interpret the Bible, especially the New Testament, as teaching belief in, and the practice of, faith healing. According to a 2004 \"Newsweek\" poll, 72 percent of Americans said they believe that praying to God can cure someone, even if science says the person has an incurable disease. Unlike faith healing, advocates of spiritual healing make no attempt to seek divine intervention, instead believing in divine energy. The increased interest in alternative medicine at the end of the 20th century has given rise to a parallel interest among sociologists in the relationship of religion to health.\n\nVirtually all scientists and philosophers dismiss faith healing as pseudoscience. Faith healing can be classified as a spiritual, supernatural, or paranormal topic, and, in some cases, belief in faith healing can be classified as magical thinking. The American Cancer Society states \"available scientific evidence does not support claims that faith healing can actually cure physical ailments\". \"Death, disability, and other unwanted outcomes have occurred when faith healing was elected instead of medical care for serious injuries or illnesses.\" When parents have practiced faith healing rather than medical care, many children have died that otherwise would have been expected to live. Similar results are found in adults.\n\nRegarded as a Christian belief that God heals people through the power of the Holy Spirit, faith healing often involves the laying on of hands. It is also called supernatural healing, divine healing, and miracle healing, among other things. Healing in the Bible is often associated with the ministry of specific individuals including Elijah, Jesus and Paul.\n\nChristian physician Reginald B. Cherry views faith healing as a pathway of healing in which God uses both the natural and the supernatural to heal. Being healed has been described as a privilege of accepting Christ's redemption on the cross. Pentecostal writer Wilfred Graves, Jr. views the healing of the body as a physical expression of salvation. , after describing Jesus exorcising at sunset and healing all of the sick who were brought to him, quotes these miracles as a fulfillment of the prophecy in : \"He took up our infirmities and carried our diseases.\"\n\nEven those Christian writers who believe in faith healing do not all believe that one's faith presently brings about the desired healing. \"[Y]our faith does not effect your healing now. When you are healed rests entirely on what the sovereign purposes of the Healer are.\" Larry Keefauver cautions against allowing enthusiasm for faith healing to stir up false hopes. \"Just believing hard enough, long enough or strong enough will not strengthen you or prompt your healing. Doing mental gymnastics to 'hold on to your miracle' will not cause your healing to manifest now.\" Those who actively lay hands on others and pray with them to be healed are usually aware that healing may not always follow immediately. Proponents of faith healing say it may come later, and it may not come in this life. \"The truth is that your healing may manifest in eternity, not in time.\"\n\nParts of the four gospels in the New Testament say that Jesus cured physical ailments well outside the capacity of first-century medicine. One example is the case of \"a woman who had had a discharge of blood for twelve years, and who had suffered much under many physicians, and had spent all that she had, and was not better but rather grew worse.\" After healing her, Jesus tells her, \"Daughter, your faith has made you well. Go in peace! Be cured from your illness.\" At least two other times Jesus credited the sufferer's faith as the means of being healed: and .\n\nJesus endorsed the use of the medical assistance of the time (medicines of oil and wine) when he told the parable of the Good Samaritan (Luke 10:25-37), who \"bound up [an injured man's] wounds, pouring on oil and wine\" (verse 34) as a physician would. Jesus then told the doubting teacher of the law (who had elicited this parable by his self-justifying question, \"And who is my neighbor?\" in verse 29) to \"go, and do likewise\" in loving others with whom he would never ordinarily associate (verse 37).\n\nThe healing in the gospels is referred to as a \"sign\" to prove Jesus' divinity and to foster belief in him as the Christ. However, when asked for other types of miracles, Jesus refused some but granted others in consideration of the motive of the request. Some theologians' understanding is that Jesus healed \"all\" who were present every single time. Sometimes he determines whether they had faith that he would heal them.\n\nJesus told his followers to heal the sick and stated that signs such as healing are evidence of faith. Jesus also told his followers to \"cure sick people, raise up dead persons, make lepers clean, expel demons. You received free, give free\".\n\nJesus sternly ordered many who received healing from him: \"Do not tell anyone!\" Jesus did not approve of anyone asking for a sign just for the spectacle of it, describing such as coming from a \"wicked and adulterous generation.\"\n\nThe apostle Paul believed healing is one of the special gifts of the Holy Spirit, and that the possibility exists that certain persons may possess this gift to an extraordinarily high degree.\n\nIn the New Testament Epistle of James, the faithful are told that to be healed, those who are sick should call upon the elders of the church to pray over [them] and anoint [them] with oil in the name of the Lord.\n\nThe New Testament says that during Jesus' ministry and after his Resurrection, the apostles healed the sick and cast out demons, made lame men walk, raised the dead and performed other miracles.\n\nJesus used miracles to convince people that he was inaugurating the Messianic Age. as in Mt 12.28. Scholars have described Jesus' miracles as establishing the kingdom during his lifetime.\n\nAt the beginning of the 20th century, the new Pentecostal movement drew participants from the Holiness movement and other movements in America that already believed in divine healing. By the 1930s, several faith healers drew large crowds and established worldwide followings.\n\nThe first Pentecostals in the modern sense appeared in Topeka, Kansas, in a Bible school conducted by Charles Fox Parham, a holiness teacher and former Methodist pastor. Pentecostalism achieved worldwide attention in 1906 through the Azusa Street Revival in Los Angeles led by William Joseph Seymour.\n\nSmith Wigglesworth was also a well-known figure in the early part of the 20th century. A former English plumber turned evangelist who lived simply and read nothing but the Bible from the time his wife taught him to read, Wigglesworth traveled around the world preaching about Jesus and performing faith healings. Wigglesworth claimed to raise several people from the dead in Jesus' name in his meetings.\n\nDuring the 1920s and 1930s, Aimee Semple McPherson was a controversial faith healer of growing popularity during the Great Depression. Subsequently, William M. Branham has been credited as the initiater of the post-World War II healing revivals. The healing revival he began led many to emulate his style and spawned a generation of faith healers. Because of this, Branham has been recognized as the \"father of modern faith healers.\" According to writer and researcher Patsy Sims, \"the power of a Branham service and his stage presence remains a legend unparalleled in the history of the Charismatic movement.\" By the late 1940s, Oral Roberts, who was associated with and promoted by Branham's Voice of Healing magazine also became well known, and he continued with faith healing until the 1980s. Roberts discounted faith healing in the late 1950s, stating, \"I never was a faith healer and I was never raised that way. My parents believed very strongly in medical science and we have a doctor who takes care of our children when they get sick. I cannot heal anyone – God does that.\" A friend of Roberts was Kathryn Kuhlman, another popular faith healer, who gained fame in the 1950s and had a television program on CBS. Also in this era, Jack Coe and A. A. Allen were faith healers who traveled with large tents for large open-air crusades.\n\nOral Roberts's successful use of television as a medium to gain a wider audience led others to follow suit. His former pilot, Kenneth Copeland, started a healing ministry. Pat Robertson, Benny Hinn, and Peter Popoff became well-known televangelists who claimed to heal the sick. Richard Rossi is known for advertising his healing clinics through secular television and radio. Kuhlman influenced Benny Hinn, who adopted some of her techniques and wrote a book about her.\n\nThe Roman Catholic Church recognizes two \"not mutually exclusive\" kinds of healing, one justified by science and one justified by faith:\n\nIn 2000, the Congregation for the Doctrine of the Faith issued \"Instruction on prayers for healing\" with specific norms about prayer meetings for obtaining healing, which presents the Catholic Church's doctrines of sickness and healing.\n\nIt accepts \"that there may be means of natural healing that have not yet been understood or recognized by science,\" but it rejects superstitious practices which are neither compatible with Christian teaching nor compatible with scientific evidence.\n\nFaith healing is reported by Catholics as the result of intercessory prayer to a saint or to a person with the gift of healing. According to \"U.S. Catholic\" magazine, \"Even in this skeptical, postmodern, scientific age—miracles really are possible.\" Three-fourths of American Catholics say they pray for miracles.\n\nAccording to John Cavadini, when healing is granted, \"The miracle is not primarily for the person healed, but for all people, as a sign of God's work in the ultimate healing called 'salvation,' or a sign of the kingdom that is coming.\" Some might view their own healing as a sign they are particularly worthy or holy, while others do not deserve it.\n\nThe Catholic Church has a special Congregation dedicated to the careful investigation of the validity of alleged miracles attributed to prospective saints. Pope Francis tightened the rules on money and miracles in the canonization process. Since Catholic Christians believe the lives of canonized saints in the Church will reflect Christ's, many have come to expect healing miracles. While the popular conception of a miracle can be wide-ranging, the Catholic Church has a specific definition for the kind of miracle formally recognized in a canonization process.\n\nAccording to \"Catholic Encyclopedia\", it is often said that cures at shrines and during Christian pilgrimages are mainly due to psychotherapy — partly to confident trust in Divine providence, and partly to the strong expectancy of cure that comes over suggestible persons at these times and places.\n\nAmong the best-known accounts by Catholics of faith healings are those attributed to the miraculous intercession of the apparition of the Blessed Virgin Mary known as Our Lady of Lourdes at the Sanctuary of Our Lady of Lourdes in France and the remissions of life-threatening disease claimed by those who have applied for aid to Saint Jude, who is known as the \"patron saint of lost causes\".\n, Catholic medics have asserted that there have been 67 miracles and 7,000 unexplainable medical cures at Lourdes since 1858. In a 1908 book, it says these cures were subjected to intense medical scrutiny and were only recognized as authentic spiritual cures after a commission of doctors and scientists, called the Lourdes Medical Bureau, had ruled out any physical mechanism for the patient's recovery. Belgian philosopher and skeptic Etienne Vermeersch coined the term Lourdes effect as a criticism of the magical thinking and placebo effect possibilities for the claimed miraculous cures as there are no documented events where a severed arm has been reattached through faith healing at Lourdes. Vermeersch identifies ambiguity and equivocal nature of the miraculous cures as a key feature of miraculous events.\n\nChristian Science claims that healing is possible through an understanding of the underlying spiritual perfection of God's creation. The world as humanly perceived is believed to be a distortion of spiritual reality. Christian Scientists believe that healing through prayer is possible insofar as it succeeds in correcting the distortion. Christian Scientists believe that prayer does not change the spiritual creation but gives a clearer view of it, and the result appears in the human scene as healing: the human picture adjusts to coincide more nearly with the divine reality. Prayer works through love: the recognition of God's creation as spiritual, intact, and inherently lovable.\n\nAn important point in Christian Science is that effectual prayer and the moral regeneration of one's life go hand-in-hand: that \"signs and wonders are wrought in the metaphysical healing of physical disease; but these signs are only to demonstrate its divine origin, to attest the reality of the higher mission of the Christ-power to take away the sins of the world.\" Christian Science teaches that disease is mental, a mortal fear, a mistaken belief or conviction of the necessity and power of ill-health – an ignorance of God's power and goodness. The chapter \"Prayer\" in \"Science and Health with Key to the Scriptures\" gives a full account of healing through prayer, while the testimonies at the end of the book are written by people who believe they have been healed through spiritual understanding gained from reading the book.\n\nThe Church of Jesus Christ of Latter-day Saints (LDS) has had a long history of faith healings. Many members of the LDS Church have told their stories of healing within the LDS publication, the \"Ensign\". The church believes healings come most often as a result of priesthood blessings given by the laying on of hands; however, prayer often accompanied with fasting is also thought to cause healings. Healing is always attributed to be God's power. Latter-day Saints believe that the Priesthood of God, held by prophets (such as Moses) and worthy disciples of the Savior, was restored via heavenly messengers to the first prophet of this dispensation, Joseph Smith.\n\nAccording to LDS doctrine, even though members may have the restored priesthood authority to heal in the name of Jesus Christ, all efforts should be made to seek the appropriate medical help. Brigham Young stated this effectively, while also noting that the ultimate outcome is still dependent on the will of God.\n\nKonkhogin Haokip has claimed some Muslims believe that the Quran was sent not only as a revelation, but as a medicine, and that they believe the Quran heals any physical and spiritual ailments through such practices as\n\nSome critics of Scientology have referred to some of its practices as being similar to faith healing, based on claims made by L. Ron Hubbard in \"\" and other writings.\n\nNearly all scientists dismiss faith healing as pseudoscience. Some opponents of the pseudoscience label assert that faith healing makes no scientific claims and thus should be treated as a matter of faith that is not testable by science. Critics reply that claims of medical cures should be tested scientifically because, although faith in the supernatural is not in itself usually considered to be the purview of science, claims of reproducible effects are nevertheless subject to scientific investigation.\n\nScientists and doctors generally find that faith healing lacks biological plausibility or epistemic warrant, which is one of the criteria to used to judge whether clinical research is ethical and financially justified. A Cochrane review of intercessory prayer found \"although some of the results of individual studies suggest a positive effect of intercessory prayer, the majority do not\". The authors concluded: \"We are not convinced that further trials of this intervention should be undertaken and would prefer to see any resources available for such a trial used to investigate other questions in health care.\" \n\nA review in 1954 investigated spiritual healing, therapeutic touch and faith healing. Of the hundred cases reviewed, none revealed that the healer's intervention alone resulted in any improvement or cure of a measurable organic disability.\n\nIn addition, at least one study has suggested that adult Christian Scientists, who generally use prayer rather than medical care, have a higher death rate than other people of the same age.\n\nThe Global Medical Research Institute (GMRI) was created in 2012 to start collecting medical records of patients who claim to have received a supernatural healing miracle as a result of Christian Spiritual Healing practices. The organization has a panel of medical doctors who review the patient’s records looking at entries prior to the claimed miracles and entries after the miracle was claimed to have taken place. “The overall goal of GMRI is to promote an empirically grounded understanding of the physiological, emotional, and sociological effects of Christian Spiritual Healing practices.” This is accomplished by applying the same rigorous standards used in other forms of medical and scientific research.\n\nSkeptics of faith healing offer primarily two explanations for anecdotes of cures or improvements, relieving any need to appeal to the supernatural. The first is \"post hoc ergo propter hoc\", meaning that a genuine improvement or spontaneous remission may have been experienced coincidental with but independent from anything the faith healer or patient did or said. These patients would have improved just as well even had they done nothing. The second is the placebo effect, through which a person may experience genuine pain relief and other symptomatic alleviation. In this case, the patient genuinely has been helped by the faith healer or faith-based remedy, not through any mysterious or numinous function, but by the power of their own belief that they would be healed. In both cases the patient may experience a real reduction in symptoms, though in neither case has anything miraculous or inexplicable occurred. Both cases, however, are strictly limited to the body's natural abilities.\n\nAccording to the American Cancer Society:\nThe American Medical Association considers that prayer as therapy should not be a medically reimbursable or deductible expense.\n\nReliance on faith healing to the exclusion of other forms of treatment can have a public health impact when it reduces or eliminates access to modern medical techniques. This is evident in both higher mortality rates for children and in reduced life expectancy for adults. Critics have also made note of serious injury that has resulted from falsely labelled \"healings\", where patients erroneously consider themselves cured and cease or withdraw from treatment. For example, at least six people have died after faith healing by their church and being told they had been healed of HIV and could stop taking their medications. It is the stated position of the AMA that \"prayer as therapy should not delay access to traditional medical care\". Choosing faith healing while rejecting modern medicine can and does cause people to die needlessly.\n\nChristian theological criticism of faith healing broadly falls into two distinct levels of disagreement.\n\nThe first is widely termed the \"open-but-cautious\" view of the miraculous in the church today. This term is deliberately used by Robert L. Saucy in the book \"Are Miraculous Gifts for Today?\". Don Carson is another example of a Christian teacher who has put forward what has been described as an \"open-but-cautious\" view. In dealing with the claims of Warfield, particularly \"Warfield's insistence that miracles ceased,\" Carson asserts, \"But this argument stands up only if such miraculous gifts are theologically tied exclusively to a role of attestation; and that is demonstrably not so.\" However, while affirming that he does not expect healing to happen today, Carson is critical of aspects of the faith healing movement, \"Another issue is that of immense abuses in healing practises... The most common form of abuse is the view that since all illness is directly or indirectly attributable to the devil and his works, and since Christ by his cross has defeated the devil, and by his Spirit has given us the power to overcome him, healing is the inheritance right of all true Christians who call upon the Lord with genuine faith.\"\n\nThe second level of theological disagreement with Christian faith healing goes further. Commonly referred to as cessationism, its adherents either claim that faith healing will not happen today at all, or may happen today, but it would be unusual. Richard Gaffin argues for a form of cessationism in an essay alongside Saucy's in the book \"Are Miraculous Gifts for Today\"? In his book \"Perspectives on Pentecost\" Gaffin states of healing and related gifts that \"the conclusion to be drawn is that as listed in 1 Corinthians 12(vv. 9f., 29f.) and encountered throughout the narrative in Acts, these gifts, particularly when exercised regularly by a given individual, are part of the foundational structure of the church... and so have passed out of the life of the church.\" Gaffin qualifies this, however, by saying \"At the same time, however, the sovereign will and power of God today to heal the sick, particularly in response to prayer (see e.g. James 5:14,15), ought to be acknowledged and insisted on.\"\n\nSkeptics of faith healers point to fraudulent practices either in the healings themselves (such as plants in the audience with fake illnesses), or concurrent with the healing work supposedly taking place and claim that faith healing is a quack practice in which the \"healers\" use well known non-supernatural illusions to exploit credulous people in order to obtain their gratitude, confidence and money. James Randi's \"The Faith Healers\" investigates Christian evangelists such as Peter Popoff, who claimed to heal sick people on stage in front of an audience. Popoff pretended to know private details about participants' lives by receiving radio transmissions from his wife who was off-stage and had gathered information from audience members prior to the show. According to this book, many of the leading modern evangelistic healers have engaged in deception and fraud. The book also questioned how faith healers use funds that were sent to them for specific purposes. Physicist Robert L. Park and doctor and consumer advocate Stephen Barrett have called into question the ethics of some exorbitant fees.\n\nThere have also been legal controversies. For example, in 1955 at a Jack Coe revival service in Miami, Florida, Coe told the parents of a three-year-old boy that he healed their son who had polio. Coe then told the parents to remove the boy's leg braces. However, their son was not cured of polio and removing the braces left the boy in constant pain. As a result, through the efforts of Joseph L. Lewis, Coe was arrested and charged on February 6, 1956 with practicing medicine without a license, a felony in the state of Florida. A Florida Justice of the Peace dismissed the case on grounds that Florida exempts divine healing from the law. Later that year Coe was diagnosed with bulbar polio, and died a few weeks later at Dallas' Parkland Hospital on December 17, 1956.\n\nTV personality Derren Brown produced a show on faith healing entitled \"Miracles for sale\" which arguably exposed the art of faith healing as a scam. In this show, Derren trained a scuba diver trainer picked from the general public to be a faith healer and took him to Texas to successfully deliver a faith healing session to a congregation.\n\nThe 1974 Child Abuse Prevention and Treatment Act (CAPTA) required states to grant religious exemptions to child neglect and child abuse laws in order to receive federal money. The CAPTA amendments of 1996 state:\n\nThirty-one states have child-abuse religious exemptions. These are Alabama, Alaska, California, Colorado, Delaware, Florida, Georgia, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Michigan, Minnesota, Mississippi, Missouri, Montana, Nevada, New Hampshire, New Jersey, New Mexico, Ohio, Oklahoma, Oregon, Pennsylvania, Vermont, Virginia, and Wyoming. In six of these states, Arkansas, Idaho, Iowa, Louisiana, Ohio and West Virginia, the exemptions extend to murder and manslaughter. Of these, Idaho is the only state accused of having a large number of deaths due to the legislation in recent times. In February 2015, controversy was sparked in Idaho over a bill believed to further reinforce parental rights to deny their children medical care.\n\nParents have been convicted of child abuse and felony reckless negligent homicide and found responsible for killing their children when they withheld lifesaving medical care and chose only prayers.\n\n"}
{"id": "50744122", "url": "https://en.wikipedia.org/wiki?curid=50744122", "title": "Fashion, Faith, and Fantasy in the New Physics of the Universe", "text": "Fashion, Faith, and Fantasy in the New Physics of the Universe\n\nFashion, Faith, and Fantasy in the New Physics of the Universe is a book by mathematical physicist Roger Penrose, released in September 2016. The book is based on his lectures that he gave at Princeton University in 2003.\n\n"}
{"id": "51489076", "url": "https://en.wikipedia.org/wiki?curid=51489076", "title": "Greek cancer cure", "text": "Greek cancer cure\n\nThe Greek cancer cure was a putative cancer cure invented and promoted by microbiologist Hariton-Tzannis Alivizatos (died 1991). It consisted of intravenous injections of a fluid for which Aliviatos would not reveal the formula.\n\nIn 1983, Dr. Alivizatos announced that he had developed a serum that had a 60 percent success rate in arresting most types of cancer, with the exception of extremely advanced cases. He claimed that the serum could attack a protein-like substance that surrounds cancer cells and weaken the body's ability to keep the disease from spreading. Greek health officials ridiculed Dr. Alivizatos's assertions, while the American Cancer Society warned his current patients that there was no evidence that the diagnostic procedures and treatment for cancer proposed had resulted in any benefits for the treatment of cancer in human beings. They concluded that, \"there is no evidence that any aspect of the diagnostic test nor the treatment... are effective in the treatment of cancer.\" In addition they state \"Nor is there any evidence that.. the intravenous injections are safe.\"\n\n"}
{"id": "31493190", "url": "https://en.wikipedia.org/wiki?curid=31493190", "title": "Group green exercise", "text": "Group green exercise\n\nGroup Green Exercise refers to physical exercise undertaken in natural environments carried out as a group. Physical exercise has positive outcomes for both physical and mental health, there is growing evidence confirming the benefits to be had from contact with nature, while the work of Prof. Jules Pretty at the University of Essex has revealed the synergistic benefits of combining the two in green exercise. New research, by Auckland University of Technology, is now investigating the additional social, physical and mental health benefits of Group Green Exercise.\n\n \n"}
{"id": "21073209", "url": "https://en.wikipedia.org/wiki?curid=21073209", "title": "Hypothesis", "text": "Hypothesis\n\nA hypothesis (plural hypotheses) is a proposed explanation for a phenomenon. For a hypothesis to be a scientific hypothesis, the scientific method requires that one can test it. Scientists generally base scientific hypotheses on previous observations that cannot satisfactorily be explained with the available scientific theories. Even though the words \"hypothesis\" and \"theory\" are often used synonymously, a scientific hypothesis is not the same as a scientific theory. A working hypothesis is a provisionally accepted hypothesis proposed for further research, in a process beginning with an educated guess or thought. \n\nA different meaning of the term \"hypothesis\" is used in formal logic, to denote the antecedent of a proposition; thus in the proposition \"If \"P\", then \"Q\"\", \"P\" denotes the hypothesis (or antecedent); \"Q\" can be called a consequent. \"P\" is the assumption in a (possibly counterfactual) \"What If\" question.\n\nThe adjective \"hypothetical\", meaning \"having the nature of a hypothesis\", or \"being assumed to exist as an immediate consequence of a hypothesis\", can refer to any of these meanings of the term \"hypothesis\".\n\nIn its ancient usage, \"hypothesis\" referred to a summary of the plot of a classical drama. The English word \"hypothesis\" comes from the ancient Greek ὑπόθεσις word \"hupothesis\", meaning \"to put under\" or \"to suppose\".\n\nIn Plato's \"Meno\" (86e–87b), Socrates dissects virtue with a method used by mathematicians, that of \"investigating from a hypothesis.\" In this sense, 'hypothesis' refers to a clever idea or to a convenient mathematical approach that simplifies cumbersome calculations. Cardinal Bellarmine gave a famous example of this usage in the warning issued to Galileo in the early 17th century: that he must not treat the motion of the Earth as a reality, but merely as a hypothesis.\n\nIn common usage in the 21st century, a \"hypothesis\" refers to a provisional idea whose merit requires evaluation. For proper evaluation, the framer of a hypothesis needs to define specifics in operational terms. A hypothesis requires more work by the researcher in order to either confirm or disprove it. In due course, a confirmed hypothesis may become part of a theory or occasionally may grow to become a theory itself. Normally, scientific hypotheses have the form of a mathematical model. Sometimes, but not always, one can also formulate them as existential statements, stating that some particular instance of the phenomenon under examination has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic.\n\nIn entrepreneurial science, a hypothesis is used to formulate provisional ideas within a business setting. The formulated hypothesis is then evaluated where either the hypothesis is proven to be \"true\" or \"false\" through a verifiability- or falsifiability-oriented experiment.\n\nAny useful hypothesis will enable predictions by reasoning (including deductive reasoning). It might predict the outcome of an experiment in a laboratory setting or the observation of a phenomenon in nature. The prediction may also invoke statistics and only talk about probabilities. Karl Popper, following others, has argued that a hypothesis must be falsifiable, and that one cannot regard a proposition or theory as scientific if it does not admit the possibility of being shown false. Other philosophers of science have rejected the criterion of falsifiability or supplemented it with other criteria, such as verifiability (e.g., verificationism) or coherence (e.g., confirmation holism). The scientific method involves experimentation, to test the ability of some hypothesis to adequately answer the question under investigation. In contrast, unfettered observation is not as likely to raise unexplained issues or open questions in science, as would the formulation of a crucial experiment to test the hypothesis. A thought experiment might also be used to test the hypothesis as well.\n\nIn framing a hypothesis, the investigator must not currently know the outcome of a test or that it remains reasonably under continuing investigation. Only in such cases does the experiment, test or study potentially increase the probability of showing the truth of a hypothesis. If the researcher already knows the outcome, it counts as a \"consequence\" — and the researcher should have already considered this while formulating the hypothesis. If one cannot assess the predictions by observation or by experience, the hypothesis needs to be tested by others providing observations. For example, a new technology or theory might make the necessary experiments feasible.\n\nPeople refer to a trial solution to a problem as a hypothesis, often called an \"educated guess\" because it provides a suggested outcome based on the evidence. However, some scientists reject the term \"educated guess\" as incorrect. Experimenters may test and reject several hypotheses before solving the problem.\n\nAccording to Schick and Vaughn, researchers weighing up alternative hypotheses may take into consideration:\n\nA working hypothesis is a hypothesis that is provisionally accepted as a basis for further research in the hope that a tenable theory will be produced, even if the hypothesis ultimately fails.\nLike all hypotheses, a working hypothesis is constructed as a statement of expectations, which can be linked to the exploratory research purpose in empirical investigation. Working hypotheses are often used as a conceptual framework in qualitative research.\n\nThe provisional nature of working hypotheses make them useful as an organizing device in applied research. Here they act like a useful guide to address problems that are still in a formative phase.\n\nIn recent years, philosophers of science have tried to integrate the various approaches to evaluating hypotheses, and the scientific method in general, to form a more complete system that integrates the individual concerns of each approach. Notably, Imre Lakatos and Paul Feyerabend, Karl Popper's colleague and student, respectively, have produced novel attempts at such a synthesis.\n\nConcepts in Hempel's deductive-nomological model play a key role in the development and testing of hypotheses. Most formal hypotheses connect concepts by specifying the expected relationships between propositions. When a set of hypotheses are grouped together they become a type of conceptual framework. When a conceptual framework is complex and incorporates causality or explanation it is generally referred to as a theory. According to noted philosopher of science Carl Gustav Hempel \"An adequate empirical interpretation turns a theoretical system into a testable theory: The hypothesis whose constituent terms have been interpreted become capable of test by reference to observable phenomena. Frequently the interpreted hypothesis will be derivative hypotheses of the theory; but their confirmation or disconfirmation by empirical data will then immediately strengthen or weaken also the primitive hypotheses from which they were derived.\"\n\nHempel provides a useful metaphor that describes the relationship between a conceptual framework and the framework as it is observed and perhaps tested (interpreted framework). \"The whole system floats, as it were, above the plane of observation and is anchored to it by rules of interpretation. These might be viewed as strings which are not part of the network but link certain points of the latter with specific places in the plane of observation. By virtue of those interpretative connections, the network can function as a scientific theory.\" Hypotheses with concepts anchored in the plane of observation are ready to be tested. In \"actual scientific practice the process of framing a theoretical structure and of interpreting it are not always sharply separated, since the intended interpretation usually guides the construction of the theoretician.\" It is, however, \"possible and indeed desirable, for the purposes of logical clarification, to separate the two steps conceptually.\"\n\nWhen a possible correlation or similar relation between phenomena is investigated, such as whether a proposed remedy is effective in treating a disease, the hypothesis that a relation exists cannot be examined the same way one might examine a proposed new law of nature. In such an investigation, if the tested remedy shows no effect in a few cases, these do not necessarily falsify the hypothesis. Instead, statistical tests are used to determine how likely it is that the overall effect would be observed if the hypothesized relation does not exist. If that likelihood is sufficiently small (e.g., less than 1%), the existence of a relation may be assumed. Otherwise, any observed effect may be due to pure chance.\n\nIn statistical hypothesis testing, two hypotheses are compared. These are called the null hypothesis and the alternative hypothesis. The null hypothesis is the hypothesis that states that there is no relation between the phenomena whose relation is under investigation, or at least not of the form given by the alternative hypothesis. The alternative hypothesis, as the name suggests, is the alternative to the null hypothesis: it states that there \"is\" some kind of relation. The alternative hypothesis may take several forms, depending on the nature of the hypothesized relation; in particular, it can be two-sided (for example: there is \"some\" effect, in a yet unknown direction) or one-sided (the direction of the hypothesized relation, positive or negative, is fixed in advance).\n\nConventional significance levels for testing hypotheses (acceptable probabilities of wrongly rejecting a true null hypothesis) are .10, .05, and .01. The significance level for deciding whether the null hypothesis is rejected and the alternative hypothesis is accepted must be determined in advance, before the observations are collected or inspected. If these criteria are determined later, when the data to be tested are already known, the test is invalid.\n\nThe above procedure is actually dependent on the number of the participants (units or sample size) that are included in the study. For instance, to avoid having the sample size be too small to reject a null hypothesis, it is recommended that one specify a sufficient sample size from the beginning. It is advisable to define a small, medium and large effect size for each of a number of important statistical tests which are used to test the hypotheses.\n\n\n"}
{"id": "379978", "url": "https://en.wikipedia.org/wiki?curid=379978", "title": "In situ", "text": "In situ\n\nIn situ (; often not italicized in English) is a Latin phrase that translates literally to \"on site\" or \"in position.\" It can mean \"locally,\" \"on site,\" \"on the premises,\" or \"in place\" to describe an event where it takes place, and is used in many different contexts. For example, in fields such as physics, chemistry, or biology, \"in situ\" may describe the way a measurement is taken, that is, in the same place the phenomenon is occurring without isolating it from other systems or altering the original conditions of the test.\n\nIn the aerospace industry, equipment on-board aircraft must be tested \"in situ\", or in place, to confirm everything functions properly as a system. Individually, each piece may work but interference from nearby equipment may create unanticipated problems. Special test equipment is available for this \"in situ\" testing.\n\nIn archaeology, \"in situ\" refers to an artifact that has not been moved from its original place of deposition. In other words, it is stationary, meaning \"still.\" An artifact being \"in situ\" is critical to the interpretation of that artifact and, consequently, of the culture which formed it. Once an artifact's 'find-site' has been recorded, the artifact can then be moved for conservation, further interpretation and display. An artifact that is not discovered \"in situ\" is considered out of context and as not providing an accurate picture of the associated culture. However, the out of context artifact can provide scientists with an example of types and locations of \"in situ\" artifacts yet to be discovered. When excavating a burial site or surface deposit \"in situ\" refers to cataloging, recording, mapping, photographing human remains in the position they are discovered.\n\nThe label \"in situ\" indicates only that the object has not been \"newly\" moved. Thus, an archaeological \"in situ\" find may be an object that was historically looted from another place, an item of \"booty\" of a past war, a traded item, or otherwise of foreign origin. Consequently, the \"in situ\" find site may still not reveal its provenance, but with further detective work may help uncover links that otherwise would remain unknown. It is also possible for archaeological layers to be reworked on purpose or by accident (by humans, natural forces or animals). For example, in a Tell mound, where layers are not typically uniform or horizontal, or in land cleared or tilled for farming.\n\nThe term \"in situ\" is often used to describe ancient sculpture that was carved in place such as the Sphinx or Petra. This distinguishes it from statues that were carved and moved like the Colossi of Memnon, which was moved in ancient times.\n\nIn art, \"in situ\" refers to a work of art made specifically for a host site, or that a work of art takes into account the site in which it is installed or exhibited. For a more detailed account see: Site-specific art. The term can also refer to a work of art created at the site where it is to be displayed, rather than one created in the artist's studio and then installed elsewhere (\"e.g.,\" a sculpture carved \"in situ\"). In architectural sculpture the term is frequently employed to describe sculpture that is carved on a building, frequently from scaffolds, after the building has been erected.\nAlso commonly used to describe the site specific dance festival “Insitu”. Held in Queens, New York.\n\nA fraction of the globular star clusters in our galaxy, as well as those in other massive galaxies, might have formed \"in situ\". The rest might have been accreted from now defunct dwarf galaxies.\n\nIn astronomy, \"in situ\" also refers to \"in situ\" planet formation, in which planets are hypothesized to have been formed in the orbit that they are currently observed to be in rather than migrating from a different orbit.\n\nIn biology and biomedical engineering, \"in situ\" means to examine the phenomenon exactly in place where it occurs (i.e., without moving it to some special medium).\n\nIn the case of observations or photographs of living animals, it means that the organism was observed (and photographed) in the wild, exactly as it was found and exactly where it was found. This means it was not taken out of the area. The organism had not been moved to another (perhaps more convenient) location such as an aquarium.\n\nThis phrase \"in situ\" when used in laboratory science such as cell science can mean something intermediate between \"in vivo\" and \"in vitro\". For example, examining a cell within a whole organ intact and under perfusion may be \"in situ\" investigation. This would not be \"in vivo\" as the donor is sacrificed by experimentation, but it would not be the same as working with the cell alone (a common scenario for \"in vitro\" experiments).\n\n\"In vitro\" was among the first attempts to qualitatively and quantitatively analyze natural occurrences in the lab. Eventually, the limitation of \"in vitro\" experimentation was that they were not conducted in natural environments. To compensate for this problem, \"in vivo\" experimentation allowed testing to occur in the original organism or environment. To bridge the dichotomy of benefits associated with both methodologies, \"in situ\" experimentation allowed the controlled aspects of \"in vitro\" to become coalesced with the natural environmental compositions of \"in vivo\" experimentation.\n\nIn conservation of genetic resources, \"\"in situ\" conservation\" (also \"on-site conservation\") is the process of protecting an endangered plant or animal species in its natural habitat, as opposed to \"ex situ\" conservation (also \"off-site conservation\").\n\nIn chemistry, \"in situ\" typically means \"in the reaction mixture.\"\n\nThere are numerous situations in which chemical intermediates are synthesized \"in situ\" in various processes. This may be done because the species is unstable, and cannot be isolated, or simply out of convenience. Examples of the former include the Corey-Chaykovsky reagent and adrenochrome.\n\nIn biomedical engineering, protein nanogels made by the in situ polymerization method provide a versatile platform for storage and release of therapeutic proteins. It has tremendous applications for cancer treatment, vaccination, diagnosis, regenerative medicine, and therapies for loss-of-function genetic diseases.\n\nIn chemical engineering, \"in situ\" often refers to industrial plant \"operations or procedures that are performed in place.\" For example, aged catalysts in industrial reactors may be regenerated in place (\"in situ\") without being removed from the reactors.\n\nIn architecture and building, \"in situ\" refers to construction which is carried out at the building site using raw materials. Compare that with \"prefabricated\" construction, in which building components are made in a factory and then transported to the building site for assembly. For example, concrete slabs may be \"in situ\" (also \"cast-in-place\") or \"prefabricated\".\n\nIn situ techniques are often more labour-intensive, and take longer, but the materials are cheaper, and the work is versatile and adaptable. \"Prefabricated\" techniques are usually much quicker, therefore saving money on labour costs, but factory-made parts can be expensive. They are also inflexible, and must often be designed on a grid, with all details fully calculated in advance. Finished units may require special handling due to excessive dimensions.\n\nThe phrase may also refer to those assets which are present at or near a project site. In this case, it is used to designate the state of an unmodified sample taken from a given stockpile.\n\nSite construction usually involves grading the existing soil surface so that material is \"cut\" out of one area and \"filled\" in another area creating a flat pad on an existing slope. The term \"in situ\" distinguishes soil still in its existing condition from soil modified (filled) during construction. The differences in the soil properties for supporting building loads, accepting underground utilities, and infiltrating water persist indefinitely.\n\n. For example, a file backup may be restored over a running system, without needing to take the system down to perform the restore. In the context of a database, a restore would allow the database system to continue to be available to users while a restore happened. An \"in situ\" upgrade would allow an operating system, firmware or application to be upgraded while the system was still running, perhaps without the need to reboot it, depending on the sophistication of the system.\n\nAnother use of the term in-situ that appears in Computer Science focuses primarily on the use of technology and user interfaces to provide continuous access to situationally relevant information in various locations and contexts. Examples include athletes viewing biometric data on smartwatches to improve their performance , a presenter looking at tips on a smart glass to reduce their speaking rate during a speech , or technicians receiving online and stepwise instructions for repairing an engine. \n\n, that is, does not exceed a constant no matter how large the input ---except for space for recursive calls on the \"call stack.\" Typically such an algorithm operates on data objects directly in place rather than making copies of them. \n\nFor example, heapsort is an \"in situ\" sorting algorithm, which sorts the elements of an array in place. Quicksort is an \"in situ\" sorting algorithm, but in the worst case it requires linear space on the call stack (this can be reduced to log space). Merge sort is generally not written as an \"in situ\" algorithm. \n\nIn designing user interfaces, , for example, if a word processor displays an image and allows the image to be edited without launching a separate image editor, this is called \"in situ editing.\"\n\nAJAX partial page data updates is another example of \"in situ\" in a Web UI/UX context. \"Web 2.0\" included AJAX and the concept of asynchronous requests to servers to replace a portion of a web page with new data, without reloading\nthe entire page, as the early HTML model dictated. Arguably, \"all\" asynchronous data transfers or \"any\" background task is \"in situ\" as the normal state is normally unaware of background tasks, usually notified on completion\nby a callback mechanism.\n\nIn Big Data space, in situ data would mean bringing the computation to where data is located, rather than the other way like in traditional RDBMS systems where data is moved to computational space.\n\nIn design and advertising the term typically means the superimposing of theoretical design elements onto photographs of real world locations. This is a pre-visualization tool to aid in illustrating a proof of concept.\n\nIn physical geography and the Earth sciences, \"in situ\" typically describes natural material or processes prior to transport. For example, \"in situ\" is used in relation to the distinction between weathering and erosion, the difference being that erosion requires a transport medium (such as wind, ice, or water), whereas weathering occurs \"in situ\". Geochemical processes are also often described as occurring to material \"in situ\".\n\nIn the atmospheric sciences, \"in situ\" refers to obtained through direct contact with the respective subject, such as a radiosonde measuring a parcel of air or an anemometer measuring wind, as opposed to remote sensing such as weather radar or satellites.\n\nIn economics, \"in situ\" is used when referring to the \"in place\" storage of a product, usually a natural resource. More generally, it refers to any situation where there is no out-of-pocket cost to store the product so that the only storage cost is the opportunity cost of waiting longer to get your money when the product is eventually sold. Examples of \"in situ\" storage would be oil and gas wells, all types of mineral and gem mines, stone quarries, timber that has reached an age where it could be harvested, and agricultural products that do not need a physical storage facility such as hay.\n\nIn electrochemistry, the phrase in situ refers to performing electrochemical experiments under operating conditions of the electrochemical cell, i.e., under potential control. This is opposed to doing ex situ experiments that are performed under the absence of potential control. Potential control preserves the electrochemical environment essential to maintain the double layer structure intact and the electron transfer reactions occurring at that particular potential in the electrode/electrolyte interphasial region.\n\n\"In situ\" can refer to where a clean up or remediation of a polluted site is performed using and simulating the natural processes in the soil, contrary to \"ex situ\" where contaminated soil is excavated and cleaned elsewhere, off site.\n\nIn experimental physics \"in situ\" typically refers to a method of data collection or manipulation of a sample without exposure to an external environment. For example, the Si(111) 7x7 surface reconstruction is visible using a scanning tunneling microscope when it is prepared and analyzed \"in situ\".\n\nIn psychology experiments, \"in situ\" typically refers to those experiments done in a field setting as opposed to a laboratory setting.\n\nIn gastronomy, \"in situ\" refers to the art of cooking with the different resources that are available at the site of the event. Here a person is not going to the restaurant, but the restaurant comes to the person's home.\n\nIn legal contexts, \"in situ\" is often used for its literal meaning. For example, in Hong Kong, \"\"in situ\" land exchange\" involves the government exchanging the original or expired lease of a piece of land with a new grant or re-grant with the same piece of land or a portion of that.\n\nIn the field of recognition of governments under public international law the term \"in situ\" is used to distinguish between an exiled government and a government with effective control over the territory, i.e. the government \"in situ\".\n\nIn linguistics, specifically syntax, an element may be said to be \"in situ\" if it is pronounced in the position where it is interpreted. For example, questions in languages such as Chinese have \"in situ\" wh-elements, with structures comparable to \"John bought what?\" with \"what\" in the same position in the sentence as the grammatical object would be in its affirmative counterpart (for example, \"John bought bread\"). An example of an English wh-element that is not \"in situ\" (see wh-movement): \"What did John buy?\"\n\nIn literature \"in situ\" is used to describe a condition. The Rosetta Stone, for example, was originally erected in a courtyard, for public viewing. Most pictures of the famous stone are not \"in situ\" pictures of it erected, as it would have been originally. The stone was uncovered as part of building material, within a wall. Its in situ condition today is that it is erected, vertically, on public display at the British Museum in London, England.\n\nIn oncology: for a carcinoma, \"in situ\" means that malignant cells are present as a tumor but have not metastasized, or invaded, beyond the basement membrane of where the tumor was discovered. This can happen anywhere in the body, such as the skin, breast tissue, or lung. This type of tumor can often, depending on where it is located, be removed by surgery.\n\nIn anatomy: \"in situ\" refers to viewing structures as they appear in normal healthy bodies. For example, one can open up a cadaver's abdominal cavity and view the liver \"in situ\" or one can look at an isolated liver that has been removed from the cadaver's body.\n\nIn nursing, \"in situ\" describes any devices or appliances on the patient's body that remain in their desired and optimal position.\n\nIn medical simulation, \"in situ\" refers to the practice of clinical professionals using high fidelity patient simulators to train for clinical practice in patient care environments, such as wards, operating rooms, and other settings, rather than in dedicated simulation training facilities.\n\nIn biomedical, protein nanogels made by the in situ polymerization method provide a versatile platform for storage and release of therapeutic proteins. It has tremendous applications for cancer treatment, vaccination, diagnosis, regenerative medicine, and therapies for loss-of-function genetic diseases.\n\n\"In situ leaching\" or \"in situ recovery\" refers to the mining technique of injecting water underground to dissolve ore and bringing the uranium-impregnated water to the surface for extraction.\n\n\"In situ\" refers to recovery techniques which apply heat or solvents to heavy crude oil or bitumen reservoirs beneath the earth's crust. There are several varieties of \"in situ\" techniques, but the ones which work best in the oil sands use heat (steam).\n\nThe most common type of \"in situ\" petroleum production is referred to as SAGD (steam-assisted gravity drainage) this is becoming very popular in the Alberta Oil Sands.\n\nIn radio frequency (RF) transmission systems, \"in situ\" is often used to describe the location of various components while the system is in its standard transmission mode, rather than operation in a test mode. For example, if an \"in situ\" wattmeter is used in a commercial broadcast transmission system, the wattmeter can accurately measure power while the station is \"on air.\"\n\nFuture space exploration or terraforming may rely on obtaining supplies \"in situ\", such as previous plans to power the Orion space vehicle with fuel minable on the moon. The Mars Direct mission concept is based primarily on the \"in situ\" fuel production using Sabatier reaction.\n\nIn the space sciences, \"in situ\" refers to measurements of the particle and field environment that the satellite is embedded in, such as the detection of energetic particles in the solar wind, or magnetic field measurements from a magnetometer.\n\nIn urban planning, in-situ upgrading is an approach to and method of upgrading informal settlements.\n\nIn vacuum technology, \"in situ\" baking refers to heating parts of the vacuum system while they are under vacuum in order to drive off volatile substances that may be absorbed or adsorbed on the walls so they cannot cause outgassing.\n\nThe term \"in situ\", used as \"repair in situ,\" means to repair a vehicle at the place where it has a breakdown.\n\n"}
{"id": "47492430", "url": "https://en.wikipedia.org/wiki?curid=47492430", "title": "Ion reactor", "text": "Ion reactor\n\nIon Reactor, is an invention by a British scientist.\n\nIn 2011, Dr Christopher Strevens (an inventor from London) began posting a website with instructions of how to build his \"fusion reactor\", which he says: \"Creates helium from hydrogen. It also captures the power given off during the reaction as electrical power.\" He also posted several videos to YouTube showing his prototype in operation, and showing the different color of gas from before versus after; as well as showing spectral analysis that indicates that the hydrogen that he puts into the system has transmuted to helium—a nuclear phenomenon.\n\nHe said: \"I found that when I increased the exciter power to 800 Watts, the output rose to 2,000 Watts [2.5-times overunity], and when I isolated the reactor from the exciter, this power remained. The spark gap regulator became active, keeping the power at this level. I only allowed this for a short time before reconnecting the exciter and turning the power down and the reaction ceased.\"\n\nBecause of the experiment being dangerous it would require a special paramagnetic ceramic bottle glazed inside to contain hydrogen ions and use the magnetic field of the coils and induction of ion fields to make a magnetic bottle to confine the ion reactions.\n\nA brief description of the device, being a hydrogen tube wrapped with high voltage coils and a sort of energizing coil being similar to a high voltage tesla coil for the ionizer. The device having been made in a clear glass tube in early experiments uses induction to ionize the gases and the field made by ionized gases to energize ion tube coils.\n\nThe effect is similar to pyroelectric fusion but instead of pyroelectric crystals stripping the electrons from hydrogen atoms it is a high voltage induced electromagnetic field of coils and electrostatic induction of the ionized gas.\n\nhttps://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19660030642.pdf\n\n"}
{"id": "634124", "url": "https://en.wikipedia.org/wiki?curid=634124", "title": "Leuchter report", "text": "Leuchter report\n\nThe Leuchter report is a pseudoscientific document authored by American execution technician Fred A. Leuchter, who was commissioned by Ernst Zündel to defend him at his trial in Canada for distributing Holocaust denial material. Leuchter compiled the report in 1988 with the intention of investigating the feasibility of mass homicidal gassings at Nazi extermination camps, specifically at Auschwitz. He travelled to the camp, collected multiple pieces of brick from the remains of the crematoria and gas chambers (without the camp's permission), brought them back to the United States, and submitted them for chemical analysis. At the trial, Leuchter was called upon to defend the report in the capacity of an expert witness; however during the trial, the court ruled that he had neither the qualifications nor experience to act as such.\n\nLeuchter cited the absence of Prussian blue in the homicidal gas chambers to support his view that they could not have been used to gas people. However, residual iron-based cyanide compounds are not a categorical consequence of cyanide exposure. By not discriminating against that, Leuchter introduced an unreliable factor into his experiment, and his findings were seriously flawed as a result. In contrast, tests conducted by Polish forensic scientists (who discriminated against iron-based compounds) confirmed the presence of cyanide in the locations, in accordance with where and how it was used in the Holocaust. In addition, the report was criticized as Leuchter had overlooked critical evidence, such as documents in the SS architectural office which recorded the mechanical operation of the gas chambers and others which verified the rate at which the Nazis could burn the bodies of those gassed.\n\nIn 1985, Ernst Zündel, a German pamphleteer and publisher living in Canada, was put on trial for publishing Richard Verrall's Holocaust denial pamphlet \"Did Six Million Really Die?\", which was deemed to violate Canadian laws against distributing false news. Zündel was found guilty, but the conviction was overturned on appeal. This led to a second prosecution.\n\nZündel and his lawyers were joined by Robert Faurisson, a French academic of literature and Holocaust denier, who came to Toronto to advise the defence, having previously testified as expert witness at the first trial. He was joined by David Irving, an English writer and Holocaust denier, who was to assist the defence and testify on Zündel's behalf. Faurisson claimed that it was technically and physically impossible for the gas chambers at Auschwitz to have functioned as extermination facilities, based on comparisons with American execution gas chambers; he therefore suggested getting an American prison warden who had participated in executions by gas to testify. Irving and Faurisson therefore invited Bill Armontrout, warden of the Missouri State Penitentiary, who agreed to testify and suggested they contact Fred A. Leuchter, a Bostonian execution equipment designer. Faurisson reported that Leuchter initially accepted the mainstream account of the Holocaust, but after two days of discussion with him, he stated that Leuchter was convinced that homicidal gassings never occurred. After having met Zündel in Toronto and agreeing to serve as an expert witness for his defence, Leuchter travelled with them to spend a week in Poland. He was accompanied by his draftsman, a cinematographer supplied by Zündel, a translator fluent in German and Polish, and his wife. Although Zündel and Faurisson did not accompany them, Leuchter said that they were with them \"every step of the way\" in spirit.\n\nAfter arriving in Poland the group spent three days at the former Auschwitz concentration camp site, and another at the former Majdanek concentration camp. At these, they filmed Leuchter illicitly collecting what he regarded to be forensic quality samples of materialsfrom the wreckage of the former gas extermination facilities, while his wife and the translator acted as lookouts. Drawings of where the samples were taken from, the film footage of their physical collection and Leuchter's notebook detailing the work were surrendered to the trial court as evidence. Leuchter claimed that his conclusions were based on his expert knowledge of gas chamber operation, his visual inspection of what remained of the structures at Auschwitz, and original drawings and blueprints of some of the facilities. He said that the blueprints had been given to him by Auschwitz Museum officials.\n\nThe compiled report was published in Canada as \"The Leuchter Report: An Engineering Report on the Alleged Execution Gas Chambers at Auschwitz, Birkenau, and Majdanek, Poland\", by Zündel's Samisdat Publications, and in England as \"Auschwitz: The End of the Line. The Leuchter Report: The First Forensic Examination of Auschwitz\" by Focal Point Publications, David Irving's publishing house. However, the court accepted the report only as evidentiary display and not as direct evidence; Leuchter was therefore required to explain it and testify to its veracity in the trial.\n\nBefore Leuchter could do this, he was examined by the court. He admitted that he was not a toxicologist and dismissed the need for having a degree in engineering:\n\nLeuchter admitted under oath that he only had a bachelor of arts degree and implicitly suggested that an engineering degree was unavailable to him by saying that his college did not offer an engineering degree during his studies. Boston University actually offered three different kinds of such qualification when he was a student there. When asked by the court if the B.A. he obtained was in a field that entitled him to operate as an engineer, he confirmed that this was so, even though his degree was in history. Similarly, Leuchter claimed that he obtained most of his research material on the camps (including original crematoria blueprints) from the Auschwitz and Majdanek camps' archives, and testified that these documents had a far more important role in shaping his conclusions than the physical samples he collected, yet after the trial the director of the Auschwitz museum denied that Leuchter had received any plans or blueprints from them.\n\nJudge Ronald Thomas began to label Leuchter's methodology as \"ridiculous\" and \"preposterous\", dismissing many of the report's conclusions on the basis that they were based on \"second-hand information\", and refused to allow him to testify on the effect of Zyklon B on humans because he had never worked with the substance, and was neither a toxicologist nor a chemist. Judge Thomas dismissed Leuchter's opinion because it was of \"no greater value than that of an ordinary tourist\", and in regards to Leuchter's opinion said:\n\nWhen questioned on the functioning of the crematoria, the judge also prevented Leuchter from testifying because \"he hasn't any expertise\". Leuchter also claimed that consultation relating to sodium cyanide and hydrogen cyanide with DuPont was \"an on-going thing\". DuPont, the largest American manufacturer of hydrogen cyanide, stated that it had \"never provided any information on cyanides to persons representing themselves as Holocaust deniers, including Fred Leuchter\", and had \"never provided any information regarding the use of cyanide at Auschwitz, Birkenau or Majdanek.\"\n\nThe contents of the report, in particular Leuchter's methodology, are heavily criticised. James Roth, the manager of the lab that carried out the analysis on the samples Leuchter collected, swore under oath to the results at the trial. Roth did not learn what the trial was about until he got off the stand. He later stated that cyanide would have only penetrated to a depth of around 10 micrometres, a tenth of the thickness of a human hair. The samples of brick, mortar and concrete that Leuchter took were of indeterminate thickness: not being aware of this, the lab ground the samples to a fine powder which thus severely diluted the cyanide-containing layer of each sample with an indeterminate amount of brick, varying for each sample. A more accurate analysis would have been obtained by analysing the surface of the samples Leuchter collected. Roth offered the analogy that the investigation was like analyzing paint on a wall by analyzing the timber behind it.\n\nLeuchter's opposition to the possibility of homicidal gassings at Auschwitz relies on residual cyanide remains found in the homicidal gas chambers and delousing chambers at Auschwitz. While both facilities were exposed to the same substance (Zyklon B), many of the delousing chambers are stained with an iron based compound known as Prussian blue, which is not apparent in the homicidal gas chambers. It is not only this disparity that Leuchter cites, but accordingly from his samples (which included measurements of it) that he claims he measured much more cyanide in the delousing chambers than in the gas chambers, which he argues is inconsistent between the amounts necessary to kill human beings and lice. This argument is often cited by Holocaust deniers, and similar claims are also made by Germar Rudolf.\n\nAccording to Richard J. Green:\n\nIn other words, Green states that Leuchter failed to show that Prussian Blue would have been produced in the homicidal gas chambers in the first place—meaning its absence is not in itself proof that no homicidal gassings took place.\nThe problem with Prussian blue is that it is by no means a categorical sign of cyanide exposure. One factor necessary in its formation is a very high concentration of cyanide. In terms of the difference between amounts measured in the delousing chambers and homicidal gas chambers, critics explain that the exact opposite of what deniers claim is true. Insects have a far higher resistance to cyanide than humans, with concentration levels up to 16,000ppm (parts per million) and an exposure time of more than 20 hours (sometimes as long as 72 hours) being necessary for them to succumb. In contrast, a cyanide concentration of only 300ppm is fatal to humans in a matter of minutes. This difference is one of the reasons behind the concentration disparity. Another exceedingly sensitive factor by which very small deviances could determine whether Prussian blue may form is pH. pH could be affected by the presence of human beings. Also, while the delousing chambers were left intact, the ruins of the crematoria at Birkenau had been exposed to the elements for over forty years by the time Leuchter collected his samples. This would have severely affected his results, because unlike Prussian blue and other iron based cyanides, cyanide salts are highly soluble in water.\n\nSince the formation of Prussian blue is not an unconditional outcome of exposure to cyanide, it is not a reliable indicator. Leuchter and Rudolf claim to have measured much more cyanide in the delousing chambers than in the homicidal gas chambers, but since they did not discriminate against an unreliable factor, Green maintains that instant bias is introduced into their experiments. Similarly, Rudolf acknowledges that Prussian blue does not always form upon exposure to cyanide and is thus not a reliable marker, yet continues to include the iron compounds in his analysis. Green describe this as \"disingenuous\". Since a building that contains Prussian blue staining would exhibit much higher levels of detectable cyanides than one without any, Green writes that Leuchter's and Rudolf's measurements reveal nothing more than what is already visible to the naked eye.\n\nIn February 1990, Professor Jan Markiewicz, director of The Institute for Forensic Research (IFRC) in Kraków conducted a fair experiment where iron compounds were excluded. Given that the ruins of the gas chambers at Birkenau have been washed by a column of water at least 35m in height based on climatological records since 1945, Markiewicz and his team were not optimistic at being able to detect cyanides so many years later; nevertheless, having the legal permission to obtain samples, they collected some from areas as sheltered from the elements as possible.\n\nLeuchter's report stated that the small amounts of cyanide he detected in the ruins of the crematoria are merely the result of fumigation. However the IFRC points out that the control samples they took from living areas which may have been fumigated only once as part of the 1942 typhus epidemic tested negative for cyanide, and that the typhus epidemic occurred before the crematoria at Birkenau even existed.\nAccordingly, the IFRC demonstrated that cyanides were present in all of the facilities where it is claimed that they were exposed, i.e. all five crematoria, the cellars of Block 11 and the delousing facilities. Critics state that any attempt to demonstrate that the crematoria could not have functioned as homicidal gas chambers on the basis that they were not exposed to cyanide is unsuccessful, given that its presence in what remains of these facilities is incontrovertible, and write that all of the gas chambers were exposed to cyanide at levels higher than background levels elsewhere in the camp, such as living areas, where no cyanides at all were detected. In addition, tests conducted at Auschwitz in 1945 revealed the presence of cyanides on ventilation grilles found in the ruins of Crematorium II (thus also demonstrating that the Leuchter report was not the first forensic examination of the camp as purported in the title of the London edition). The historian Richard J. Evans argued that due to Leuchter's ignorance of the large disparity between the amounts of cyanide necessary to kill humans and lice, instead of disproving the homicidal use of gas chambers, the small amounts of cyanide which Leuchter detected actually tended to confirm it.\n\nBy order of Heinrich Himmler, the crematoria and gas chambers at Birkenau were destroyed by the SS in order to hide evidence of genocide. Nothing more than the bases of Crematoria IV and V can be seen: the floor plans of both facilities are indicated by bricks laid out across the concrete foundations, and Crematoria II and III are in ruins. Professor Robert Jan van Pelt labels Leuchter's comment that the facilities have not changed at all since 1942 or 1941 as \"nonsense\".\n\nBecause hydrogen cyanide is explosive, Leuchter maintained that the gas chambers could never have been operated due to their proximity to the ovens of the crematoria. It is correct that hydrogen cyanide is explosive, but only at concentrations of 56,000 ppm and above – over 186 times more than the lethal dose of 300 ppm. Critics estimate conservatively that within 5 to 15 minutes, gas chamber victims were exposed to 450 – 1810 ppmv – again considerably lower than the lower explosion limit.\n\nLeuchter incorrectly assumed that the gas chambers were not ventilated. The basement gas chambers of Crematoria II and III were mechanically ventilated via motors in the roof space of the main crematorium structure capable of extracting the remaining gas and renewing the air every three to four minutes.\n\nWhen ventilation was not used such as in Crematoria IV and V (although a ventilation system was later installed in Crematorium V in May 1944), Sonderkommando prisoners wore gas masks when removing the bodies. When presented in court with a document by the chief Auschwitz architect SS-\"Sturmbannführer\" Karl Bischoff, Leuchter misconstrued aeration (\"Belüftung\") and ventilation (\"Entlüftung\") as part of the furnace blower systems, when they were actually in reference to the ventilation channels in the walls that straddle the gas chambers. These are visible on blueprints, and can still partly be seen in the ruined east wall of the Crematorium III gas chamber.\n\nLeuchter was also prepared to act as expert witness regarding crematoria ovens despite admitting during cross examination that he had no expert knowledge. Leuchter presented his own estimate of 156 corpses as the total daily incineration capacity of the installations at Auschwitz. During cross-examination, he was presented with a letter written by the Auschwitz Central Construction Office (\"Auschwitz Zentralbauleitung\") of June 28, 1943, from SS-\"Sturmbannführer\" Jahrling to SS-\"Brigadeführer\" Hans Kammler stating that the five crematoria installations had a collective daily capacity of 4,756 corpses. Leuchter conceded that this was quite different from his own figure, and that he had never seen the document in question before.\n\nA patent application by the makers of the ovens, (both of which were made during the war) and two independent testimonies confirmed the capacity of the crematoria. Because the 4,756 figure is evidence of the Nazis equipping a camp of a maximum of 125,000 prisoners with the facility to cremate 140,000 of them per month, critics of Leuchter explain that this reveals the true exterminationist purpose of Auschwitz: a camp with the capacity to reduce its entire population to ash on a monthly basis was not merely a benign internment camp.\n\nAt various times (such as in the summer of 1944 when the crematoria couldn't keep up with the extermination rate), bodies were burnt in open-air pits. Accordingly, the capacity of the crematoria was never a limiting factor, and the pits yielded practically no limit to the number of corpses that could be burnt.\n\n\n\n"}
{"id": "256043", "url": "https://en.wikipedia.org/wiki?curid=256043", "title": "List of science magazines", "text": "List of science magazines\n\nA science magazine is a periodical publication with news, opinions and reports about science, generally written for a non-expert audience. In contrast, a periodical publication, usually including primary research and/or reviews, that is written by scientific experts is called a \"scientific journal\". Science magazines are read by non-scientists and scientists who want accessible information on fields outside their specialization.\n\nArticles in science magazines are sometimes republished or summarized by the general press.\n\n\n\n\n"}
{"id": "4366909", "url": "https://en.wikipedia.org/wiki?curid=4366909", "title": "Ludwik Fleck Prize", "text": "Ludwik Fleck Prize\n\nThe Ludwik Fleck Prize is an annual award given for a book in the field of science and technology studies. It was created by the 4S Council (Society for the Social Studies of Science) in 1992 and is named after microbiologist Ludwik Fleck.\n"}
{"id": "39140116", "url": "https://en.wikipedia.org/wiki?curid=39140116", "title": "MDRC", "text": "MDRC\n\nMDRC is a nonprofit, nonpartisan education and social policy research organization based in New York City and Oakland, CA. MDRC mounts large-scale demonstrations and uses randomized controlled trials to measure the effects of social and educational policy initiatives. MDRC is led by President Gordon Berlin and Senior Vice Presidents Jesús Amadeo and Robert Ivry.\n\nIn 1974 the Ford Foundation and six government agencies together created the Manpower Demonstration Research Corporation. Its purpose was to implement and document the results of new programs intended to help the poor. In the 1980s and 1990s, it became well known for its evaluations of state welfare-to-work programs. It formally adopted \"MDRC\" as its registered corporate identity in 2003.\n\nMDRC evaluates and pilot-tests programs in six main areas:\n\nMDRC helped pioneer the use of random assignment to test social programs. Its evaluations of welfare work programs influenced the welfare reform of the 1990s. In the 1990s and 2000s, MDRC’s evaluation of the Career Academies high school reform model, which showed impacts on participants’ earnings eight years after graduation, influenced the expansion of the model around the nation. MDRC was the intermediary for the first social impact bond demonstration in the United States, a project to reduce recidivism among 16- to 18-year-olds incarcerated at Rikers Island. Most recently, MDRC's study of the City University of New York's Accelerated Study in Associate Programs (ASAP) has demonstrated that the program has doubled the three-year graduation rate of students who begin college requiring remedial education. \n\n"}
{"id": "58713360", "url": "https://en.wikipedia.org/wiki?curid=58713360", "title": "Midbrain activation", "text": "Midbrain activation\n\nMidbrain activation is a pseudoscientific training method claiming to allow the development of blind vision and to improve memory and concentration. The courses and workshops, generally targeting school going students, are offered by various organisations in the Indian subcontinent, particularly in South India. The origins of midbrain activation are unclear. Parallels have been drawn between Midbrain Activation and Nada yoga. Midbrain itself refers to the Mesencephalon of anatomical human brain.\n\nThe training programs run by the franchise organisations have generated criticism from rationalists, most notably Narendra Nayak, president of the Federation of Indian Rationalist Associations (FIRA).\n"}
{"id": "18400577", "url": "https://en.wikipedia.org/wiki?curid=18400577", "title": "Naturhistorieselskabet", "text": "Naturhistorieselskabet\n\nNaturhistorieselskabet - the Society for Natural History - was a private society that was the only institution to offer education in natural history in Denmark in the late 18th century. The spirit of the Age of Enlightenment and an escalating agricultural crisis, led the king and the Danish elite to call foreign experts on economy, including botany and silviculture, to the country. The autonomous University of Copenhagen, on the other hand, was reluctant to employ foreign experts in little-established disciplines. Naturhistorieselskabet was formed in 1788 in order to ensure education in botany, zoology and mineralogy based on private funds. For example, Martin Vahl lectured in botany. After the appointment in 1795 of a professor in geology and in 1797 one in botany, the society gradually lost its importance. It was soon abolished and its collections donated to the state (much later united with the university collections).\n\nWagner, P.H. 2001. Institutionaliseringen af botanik og geologi i Danmark-Norge i det 18. århundrede (colloquium). Institut for Videnskabshistorie.\n"}
{"id": "30858407", "url": "https://en.wikipedia.org/wiki?curid=30858407", "title": "Not even wrong", "text": "Not even wrong\n\nThe phrase \"not even wrong\" describes an argument or explanation that purports to be scientific but is based on invalid reasoning or speculative premises that can neither be proven correct nor falsified. Hence, it refers to statements that cannot be discussed in a rigorous, scientific sense. For a meaningful discussion on whether a certain statement is true or false, the statement must satisfy the criterion called \"falsifiability\"—the inherent possibility for the statement to be tested and found false. In this sense, the phrase \"not even wrong\" is synonymous to \"nonfalsifiable\".\n\nThe phrase is generally attributed to theoretical physicist Wolfgang Pauli, who was known for his colorful objections to incorrect or careless thinking. Rudolf Peierls documents an instance in which \"a friend showed Pauli the paper of a young physicist which he suspected was not of great value but on which he wanted Pauli's views. Pauli remarked sadly, 'It is not even wrong'.\" This is also often quoted as \"That is not only not right; it is not even wrong\", or in Pauli's native German, \"\". Peierls remarks that quite a few apocryphal stories of this kind have been circulated and mentions that he listed only the ones personally vouched for by him. He also quotes another example when Pauli replied to Lev Landau, \"What you said was so confused that one could not tell whether it was nonsense or not.\"\n\nThe phrase is often used to describe pseudoscience or bad science and is considered derogatory.\n\n"}
{"id": "4624242", "url": "https://en.wikipedia.org/wiki?curid=4624242", "title": "Parascience", "text": "Parascience\n\nParascience is the study of subjects that are outside the scope of the natural and social sciences because they cannot be explained by accepted scientific theory or tested by conventional scientific methods. This study may be concerned with phenomena assumed to be beyond the scope of scientific inquiry or for which no scientific explanation exists. The parasciences include history, philosophy, art, and religion.\n\nParascience can also be defined as a subject, method, etc., purporting to be scientific but regarded as unorthodox or unacceptable by the scientific community; an \"alternative\" science.\n"}
{"id": "48781", "url": "https://en.wikipedia.org/wiki?curid=48781", "title": "Philosophiæ Naturalis Principia Mathematica", "text": "Philosophiæ Naturalis Principia Mathematica\n\nPhilosophiæ Naturalis Principia Mathematica (Latin for \"Mathematical Principles of Natural Philosophy\"), often referred to as simply the Principia , is a work in three books by Isaac Newton, in Latin, first published 5 July 1687. After annotating and correcting his personal copy of the first edition, Newton published two further editions, in 1713 and 1726. The \"Principia\" states Newton's laws of motion, forming the foundation of classical mechanics; Newton's law of universal gravitation; and a derivation of Kepler's laws of planetary motion (which Kepler first obtained empirically).\n\nThe \"Principia\" is considered one of the most important works in the history of science.\n\nThe French mathematical physicist Alexis Clairaut assessed it in 1747: \"The famous book of \"Mathematical Principles of Natural Philosophy\" marked the epoch of a great revolution in physics. The method followed by its illustrious author Sir Newton ... spread the light of mathematics on a science which up to then had remained in the darkness of conjectures and hypotheses.\"\n\nA more recent assessment has been that while acceptance of Newton's theories was not immediate, by the end of a century after publication in 1687, \"no one could deny that\" (out of the \"Principia\") \"a science had emerged that, at least in certain respects, so far exceeded anything that had ever gone before that it stood alone as the ultimate exemplar of science generally.\"\n\nIn formulating his physical theories, Newton developed and used mathematical methods now included in the field of calculus. But the language of calculus as we know it was largely absent from the \"Principia\"; Newton gave many of his proofs in a geometric form of infinitesimal calculus, based on limits of ratios of vanishing small geometric quantities. In a revised conclusion to the \"Principia\" (see \"General Scholium\"), Newton used his expression that became famous, \"Hypotheses non fingo\" (\"I formulate no hypotheses\").\n\nIn the preface of the \"Principia\", Newton wrote:\n\nThe \"Principia\" deals primarily with massive bodies in motion, initially under a variety of conditions and hypothetical laws of force in both non-resisting and resisting media, thus offering criteria to decide, by observations, which laws of force are operating in phenomena that may be observed. It attempts to cover hypothetical or possible motions both of celestial bodies and of terrestrial projectiles. It explores difficult problems of motions perturbed by multiple attractive forces. Its third and final book deals with the interpretation of observations about the movements of planets and their satellites.\n\nIt shows:\n\nThe opening sections of the \"Principia\" contain, in revised and extended form, nearly all of the content of Newton's 1684 tract \"De motu corporum in gyrum\".\n\nThe \"Principia\" begin with \"Definitions\" and \"Axioms or Laws of Motion\", and continues in three books:\n\nBook 1, subtitled \"De motu corporum\" (\"On the motion of bodies\") concerns motion in the absence of any resisting medium. It opens with a mathematical exposition of \"the method of first and last ratios\", a geometrical form of infinitesimal calculus.\nThe second section establishes relationships between centripetal forces and the law of areas now known as Kepler's second law (Propositions 1–3), and relates circular velocity and radius of path-curvature to radial force (Proposition 4), and relationships between centripetal forces varying as the inverse-square of the distance to the center and orbits of conic-section form (Propositions 5–10).\n\nPropositions 11–31 establish properties of motion in paths of eccentric conic-section form including ellipses, and their relation with inverse-square central forces directed to a focus, and include Newton's theorem about ovals (lemma 28).\n\nPropositions 43–45 are demonstration that in an eccentric orbit under centripetal force where the apse may move, a steady non-moving orientation of the line of apses is an indicator of an inverse-square law of force.\n\nBook 1 contains some proofs with little connection to real-world dynamics. But there are also sections with far-reaching application to the solar system and universe:\n\nPropositions 57–69 deal with the \"motion of bodies drawn to one another by centripetal forces\". This section is of primary interest for its application to the solar system, and includes Proposition 66 along with its 22 corollaries: here Newton took the first steps in the definition and study of the problem of the movements of three massive bodies subject to their mutually perturbing gravitational attractions, a problem which later gained name and fame (among other reasons, for its great difficulty) as the three-body problem.\n\nPropositions 70–84 deal with the attractive forces of spherical bodies. The section contains Newton's proof that a massive spherically symmetrical body attracts other bodies outside itself as if all its mass were concentrated at its centre. This fundamental result, called the Shell theorem, enables the inverse square law of gravitation to be applied to the real solar system to a very close degree of approximation.\n\nPart of the contents originally planned for the first book was divided out into a second book, which largely concerns motion through resisting mediums. Just as Newton examined consequences of different conceivable laws of attraction in Book 1, here he examines different conceivable laws of resistance; thus discusses resistance in direct proportion to velocity, and goes on to examine the implications of resistance in proportion to the square of velocity. Book 2 also discusses (in ) hydrostatics and the properties of compressible fluids. The effects of air resistance on pendulums are studied in , along with Newton's account of experiments that he carried out, to try to find out some characteristics of air resistance in reality by observing the motions of pendulums under different conditions. Newton compares the resistance offered by a medium against motions of globes with different properties (material, weight, size). In Section 8, he derives rules to determine the speed of waves in fluids and relates them to the density and condensation(Proposition 48; this would become very important in acoustics). He assumes that these rules apply equally to light and sound and estimates that the speed of sound is around 1088 feet per second and can increase depending on the amount of water in air.\n\nLess of Book 2 has stood the test of time than of Books 1 and 3, and it has been said that Book 2 was largely written on purpose to refute a theory of Descartes which had some wide acceptance before Newton's work (and for some time after). According to this Cartesian theory of vortices, planetary motions were produced by the whirling of fluid vortices that filled interplanetary space and carried the planets along with them. Newton wrote at the end of Book 2 his conclusion that the hypothesis of vortices was completely at odds with the astronomical phenomena, and served not so much to explain as to confuse them.\n\nBook 3, subtitled \"De mundi systemate\" (\"On the system of the world\"), is an exposition of many consequences of universal gravitation, especially its consequences for astronomy. It builds upon the propositions of the previous books, and applies them with further specificity than in Book 1 to the motions observed in the solar system. Here (introduced by Proposition 22, and continuing in Propositions 25–35) are developed several of the features and irregularities of the orbital motion of the Moon, especially the variation. Newton lists the astronomical observations on which he relies, and establishes in a stepwise manner that the inverse square law of mutual gravitation applies to solar system bodies, starting with the satellites of Jupiter and going on by stages to show that the law is of universal application. He also gives starting at Lemma 4 and Proposition 40 the theory of the motions of comets, for which much data came from John Flamsteed and Edmond Halley, and accounts for the tides, attempting quantitative estimates of the contributions of the Sun and Moon to the tidal motions; and offers the first theory of the precession of the equinoxes. Book 3 also considers the harmonic oscillator in three dimensions, and motion in arbitrary force laws.\n\nIn Book 3 Newton also made clear his heliocentric view of the solar system, modified in a somewhat modern way, since already in the mid-1680s he recognised the \"deviation of the Sun\" from the centre of gravity of the solar system. For Newton, \"the common centre of gravity of the Earth, the Sun and all the Planets is to be esteem'd the Centre of the World\", and that this centre \"either is at rest, or moves uniformly forward in a right line\". Newton rejected the second alternative after adopting the position that \"the centre of the system of the world is immoveable\", which \"is acknowledg'd by all, while some contend that the Earth, others, that the Sun is fix'd in that centre\". Newton estimated the mass ratios Sun:Jupiter and Sun:Saturn, and pointed out that these put the centre of the Sun usually a little way off the common center of gravity, but only a little, the distance at most \"would scarcely amount to one diameter of the Sun\".\n\nThe sequence of definitions used in setting up dynamics in the \"Principia\" is recognisable in many textbooks today. Newton first set out the definition of mass\n\nThis was then used to define the \"quantity of motion\" (today called momentum), and the principle of inertia in which mass replaces the previous Cartesian notion of \"intrinsic force\". This then set the stage for the introduction of forces through the change in momentum of a body. Curiously, for today's readers, the exposition looks dimensionally incorrect, since Newton does not introduce the dimension of time in rates of changes of quantities.\n\nHe defined space and time \"not as they are well known to all\". Instead, he defined \"true\" time and space as \"absolute\" and explained:\n\nTo some modern readers it can appear that some dynamical quantities recognised today were used in the \"Principia\" but not named. The mathematical aspects of the first two books were so clearly consistent that they were easily accepted; for example, Locke asked Huygens whether he could trust the mathematical proofs, and was assured about their correctness.\n\nHowever, the concept of an attractive force acting at a distance received a cooler response. In his notes, Newton wrote that the inverse square law arose naturally due to the structure of matter. However, he retracted this sentence in the published version, where he stated that the motion of planets is consistent with an inverse square law, but refused to speculate on the origin of the law. Huygens and Leibniz noted that the law was incompatible with the notion of the aether. From a Cartesian point of view, therefore, this was a faulty theory. Newton's defence has been adopted since by many famous physicists—he pointed out that the mathematical form of the theory had to be correct since it explained the data, and he refused to speculate further on the basic nature of gravity. The sheer number of phenomena that could be organised by the theory was so impressive that younger \"philosophers\" soon adopted the methods and language of the \"Principia\".\n\nPerhaps to reduce the risk of public misunderstanding, Newton included at the beginning of Book 3 (in the second (1713) and third (1726) editions) a section entitled \"Rules of Reasoning in Philosophy.\" In the four rules, as they came finally to stand in the 1726 edition, Newton effectively offers a methodology for handling unknown phenomena in nature and reaching towards explanations for them. The four Rules of the 1726 edition run as follows (omitting some explanatory comments that follow each):\n\nRule 1: \"We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances.\"\n\nRule 2: \"Therefore to the same natural effects we must, as far as possible, assign the same causes.\"\n\nRule 3: \"The qualities of bodies, which admit neither intensification nor remission of degrees, and which are found to belong to all bodies within the reach of our experiments, are to be esteemed the universal qualities of all bodies whatsoever.\"\n\nRule 4: \"In experimental philosophy we are to look upon propositions inferred by general induction from phenomena as accurately or very nearly true, not withstanding any contrary hypothesis that may be imagined, till such time as other phenomena occur, by which they may either be made more accurate, or liable to exceptions.\"\n\nThis section of Rules for philosophy is followed by a listing of 'Phenomena', in which are listed a number of mainly astronomical observations, that Newton used as the basis for inferences later on, as if adopting a consensus set of facts from the astronomers of his time.\n\nBoth the 'Rules' and the 'Phenomena' evolved from one edition of the \"Principia\" to the next. Rule 4 made its appearance in the third (1726) edition; Rules 1–3 were present as 'Rules' in the second (1713) edition, and predecessors of them were also present in the first edition of 1687, but there they had a different heading: they were not given as 'Rules', but rather in the first (1687) edition the predecessors of the three later 'Rules', and of most of the later 'Phenomena', were all lumped together under a single heading 'Hypotheses' (in which the third item was the predecessor of a heavy revision that gave the later Rule 3).\n\nFrom this textual evolution, it appears that Newton wanted by the later headings 'Rules' and 'Phenomena' to clarify for his readers his view of the roles to be played by these various statements.\n\nIn the third (1726) edition of the \"Principia\", Newton explains each rule in an alternative way and/or gives an example to back up what the rule is claiming. The first rule is explained as a philosophers' principle of economy. The second rule states that if one cause is assigned to a natural effect, then the same cause so far as possible must be assigned to natural effects of the same kind: for example respiration in humans and in animals, fires in the home and in the Sun, or the reflection of light whether it occurs terrestrially or from the planets. An extensive explanation is given of the third rule, concerning the qualities of bodies, and Newton discusses here the generalisation of observational results, with a caution against making up fancies contrary to experiments, and use of the rules to illustrate the observation of gravity and space.\n\nIsaac Newton’s statement of the four rules revolutionised the investigation of phenomena. With these rules, Newton could in principle begin to address all of the world’s present unsolved mysteries. He was able to use his new analytical method to replace that of Aristotle, and he was able to use his method to tweak and update Galileo’s experimental method. The re-creation of Galileo's method has never been significantly changed and in its substance, scientists use it today.\n\nThe \"General Scholium\" is a concluding essay added to the second edition, 1713 (and amended in the third edition, 1726). It is not to be confused with the \"General Scholium\" at the end of Book 2, Section 6, which discusses his pendulum experiments and resistance due to air, water, and other fluids.\n\nHere Newton used what became his famous expression Hypotheses non fingo, \"I formulate no hypotheses\", in response to criticisms of the first edition of the \"Principia\". (\"<nowiki>'Fingo'</nowiki>\" is sometimes nowadays translated 'feign' rather than the traditional 'frame'.) Newton's gravitational attraction, an invisible force able to act over vast distances, had led to criticism that he had introduced \"occult agencies\" into science. Newton firmly rejected such criticisms and wrote that it was enough that the phenomena implied gravitational attraction, as they did; but the phenomena did not so far indicate the cause of this gravity, and it was both unnecessary and improper to frame hypotheses of things not implied by the phenomena: such hypotheses \"have no place in experimental philosophy\", in contrast to the proper way in which \"particular propositions are inferr'd from the phenomena and afterwards rendered general by induction\".\n\nNewton also underlined his criticism of the vortex theory of planetary motions, of Descartes, pointing to its incompatibility with the highly eccentric orbits of comets, which carry them \"through all parts of the heavens indifferently\".\n\nNewton also gave theological argument. From the system of the world, he inferred the existence of a Lord God, along lines similar to what is sometimes called the argument from intelligent or purposive design. It has been suggested that Newton gave \"an oblique argument for a unitarian conception of God and an implicit attack on the doctrine of the Trinity\", but the General Scholium appears to say nothing specifically about these matters.\n\nIn January 1684, Edmond Halley, Christopher Wren and Robert Hooke had a conversation in which Hooke claimed to not only have derived the inverse-square law, but also all the laws of planetary motion. Wren was unconvinced, Hooke did not produce the claimed derivation although the others gave him time to do it, and Halley, who could derive the inverse-square law for the restricted circular case (by substituting Kepler's relation into Huygens' formula for the centrifugal force) but failed to derive the relation generally, resolved to ask Newton.\n\nHalley's visits to Newton in 1684 thus resulted from Halley's debates about planetary motion with Wren and Hooke, and they seem to have provided Newton with the incentive and spur to develop and write what became \"Philosophiae Naturalis Principia Mathematica\". Halley was at that time a Fellow and Council member of the Royal Society in London (positions that in 1686 he resigned to become the Society's paid Clerk). Halley's visit to Newton in Cambridge in 1684 probably occurred in August. When Halley asked Newton's opinion on the problem of planetary motions discussed earlier that year between Halley, Hooke and Wren, Newton surprised Halley by saying that he had already made the derivations some time ago; but that he could not find the papers. (Matching accounts of this meeting come from Halley and Abraham De Moivre to whom Newton confided.) Halley then had to wait for Newton to 'find' the results, but in November 1684 Newton sent Halley an amplified version of whatever previous work Newton had done on the subject. This took the form of a 9-page manuscript, \"De motu corporum in gyrum\" (\"Of the motion of bodies in an orbit\"): the title is shown on some surviving copies, although the (lost) original may have been without title.\n\nNewton's tract \"De motu corporum in gyrum\", which he sent to Halley in late 1684, derived what are now known as the three laws of Kepler, assuming an inverse square law of force, and generalised the result to conic sections. It also extended the methodology by adding the solution of a problem on the motion of a body through a resisting medium. The contents of \"De motu\" so excited Halley by their mathematical and physical originality and far-reaching implications for astronomical theory, that he immediately went to visit Newton again, in November 1684, to ask Newton to let the Royal Society have more of such work. The results of their meetings clearly helped to stimulate Newton with the enthusiasm needed to take his investigations of mathematical problems much further in this area of physical science, and he did so in a period of highly concentrated work that lasted at least until mid-1686.\n\nNewton's single-minded attention to his work generally, and to his project during this time, is shown by later reminiscences from his secretary and copyist of the period, Humphrey Newton. His account tells of Isaac Newton's absorption in his studies, how he sometimes forgot his food, or his sleep, or the state of his clothes, and how when he took a walk in his garden he would sometimes rush back to his room with some new thought, not even waiting to sit before beginning to write it down. Other evidence also shows Newton's absorption in the \"Principia\": Newton for years kept up a regular programme of chemical or alchemical experiments, and he normally kept dated notes of them, but for a period from May 1684 to April 1686, Newton's chemical notebooks have no entries at all. So it seems that Newton abandoned pursuits to which he was normally dedicated, and did very little else for well over a year and a half, but concentrated on developing and writing what became his great work.\n\nThe first of the three constituent books was sent to Halley for the printer in spring 1686, and the other two books somewhat later. The complete work, published by Halley at his own financial risk, appeared in July 1687. Newton had also communicated \"De motu\" to Flamsteed, and during the period of composition he exchanged a few letters with Flamsteed about observational data on the planets, eventually acknowledging Flamsteed's contributions in the published version of the \"Principia\" of 1687.\n\nThe process of writing that first edition of the \"Principia\" went through several stages and drafts: some parts of the preliminary materials still survive, while others are lost except for fragments and cross-references in other documents.\n\nSurviving materials show that Newton (up to some time in 1685) conceived his book as a two-volume work. The first volume was to be titled \"De motu corporum, Liber primus\", with contents that later appeared in extended form as Book 1 of the \"Principia\".\n\nA fair-copy draft of Newton's planned second volume \"De motu corporum, Liber secundus\" survives, its completion dated to about the summer of 1685. It covers the application of the results of \"Liber primus\" to the Earth, the Moon, the tides, the solar system, and the universe; in this respect it has much the same purpose as the final Book 3 of the \"Principia\", but it is written much less formally and is more easily read.\nIt is not known just why Newton changed his mind so radically about the final form of what had been a readable narrative in \"De motu corporum, Liber secundus\" of 1685, but he largely started afresh in a new, tighter, and less accessible mathematical style, eventually to produce Book 3 of the \"Principia\" as we know it. Newton frankly admitted that this change of style was deliberate when he wrote that he had (first) composed this book \"in a popular method, that it might be read by many\", but to \"prevent the disputes\" by readers who could not \"lay aside the[ir] prejudices\", he had \"reduced\" it \"into the form of propositions (in the mathematical way) which should be read by those only, who had first made themselves masters of the principles established in the preceding books\". The final Book 3 also contained in addition some further important quantitative results arrived at by Newton in the meantime, especially about the theory of the motions of comets, and some of the perturbations of the motions of the Moon.\n\nThe result was numbered Book 3 of the \"Principia\" rather than Book 2, because in the meantime, drafts of \"Liber primus\" had expanded and Newton had divided it into two books. The new and final Book 2 was concerned largely with the motions of bodies through resisting mediums.\n\nBut the \"Liber secundus\" of 1685 can still be read today. Even after it was superseded by Book 3 of the \"Principia\", it survived complete, in more than one manuscript. After Newton's death in 1727, the relatively accessible character of its writing encouraged the publication of an English translation in 1728 (by persons still unknown, not authorised by Newton's heirs). It appeared under the English title \"A Treatise of the System of the World\". This had some amendments relative to Newton's manuscript of 1685, mostly to remove cross-references that used obsolete numbering to cite the propositions of an early draft of Book 1 of the \"Principia\". Newton's heirs shortly afterwards published the Latin version in their possession, also in 1728, under the (new) title \"De Mundi Systemate\", amended to update cross-references, citations and diagrams to those of the later editions of the \"Principia\", making it look superficially as if it had been written by Newton after the \"Principia\", rather than before. The \"System of the World\" was sufficiently popular to stimulate two revisions (with similar changes as in the Latin printing), a second edition (1731), and a 'corrected' reprint of the second edition (1740).\n\nThe text of the first of the three books of the \"Principia\" was presented to the Royal Society at the close of April 1686. Hooke made some priority claims (but failed to substantiate them), causing some delay. When Hooke's claim was made known to Newton, who hated disputes, Newton threatened to withdraw and suppress Book 3 altogether, but Halley, showing considerable diplomatic skills, tactfully persuaded Newton to withdraw his threat and let it go forward to publication. Samuel Pepys, as President, gave his imprimatur on 30 June 1686, licensing the book for publication. The Society had just spent its book budget on a \"History of Fishes\", and the cost of publication was borne by Edmund Halley (who was also then acting as publisher of the \"Philosophical Transactions of the Royal Society\"): the book appeared in summer 1687.\n\nNicolaus Copernicus had moved the Earth away from the center of the universe with the heliocentric theory for which he presented evidence in his book \"De revolutionibus orbium coelestium\" (\"On the revolutions of the heavenly spheres\") published in 1543. The structure was completed when Johannes Kepler wrote the book \"Astronomia nova\" (\"A new astronomy\") in 1609, setting out the evidence that planets move in elliptical orbits with the sun at one focus, and that planets do not move with constant speed along this orbit. Rather, their speed varies so that the line joining the centres of the sun and a planet sweeps out equal areas in equal times. To these two laws he added a third a decade later, in his book \"Harmonices Mundi\" (\"Harmonies of the world\"). This law sets out a proportionality between the third power of the characteristic distance of a planet from the sun and the square of the length of its year.\n\nThe foundation of modern dynamics was set out in Galileo's book \"Dialogo sopra i due massimi sistemi del mondo\" (\"Dialogue on the two main world systems\") where the notion of inertia was implicit and used. In addition, Galileo's experiments with inclined planes had yielded precise mathematical relations between elapsed time and acceleration, velocity or distance for uniform and uniformly accelerated motion of bodies.\n\nDescartes' book of 1644 \"Principia philosophiae\" (\"Principles of philosophy\") stated that bodies can act on each other only through contact: a principle that induced people, among them himself, to hypothesize a universal medium as the carrier of interactions such as light and gravity—the aether. Newton was criticized for apparently introducing forces that acted at distance without any medium. Not until the development of particle theory was Descartes' notion vindicated when it was possible to describe all interactions, like the strong, weak, and electromagnetic fundamental interactions, using mediating gauge bosons and gravity through hypothesized gravitons. Although he was mistaken in his treatment of circular motion, this effort was more fruitful in the short term when it led others to identify circular motion as a problem raised by the principle of inertia. Christiaan Huygens solved this problem in the 1650s and published it much later in 1673 in his book \"Horologium oscillatorium sive de motu pendulorum\".\n\nNewton had studied these books, or, in some cases, secondary sources based on them, and taken notes entitled \"Quaestiones quaedam philosophicae\" (\"Questions about philosophy\") during his days as an undergraduate. During this period (1664–1666) he created the basis of calculus, and performed the first experiments in the optics of colour. At this time, his proof that white light was a combination of primary colours (found via prismatics) replaced the prevailing theory of colours and received an overwhelmingly favourable response, and occasioned bitter disputes with Robert Hooke and others, which forced him to sharpen his ideas to the point where he already composed sections of his later book \"Opticks\" by the 1670s in response. Work on calculus is shown in various papers and letters, including two to Leibniz. He became a fellow of the Royal Society and the second Lucasian Professor of Mathematics (succeeding Isaac Barrow) at Trinity College, Cambridge.\n\nIn the 1660s Newton studied the motion of colliding bodies, and deduced that the centre of mass of two colliding bodies remains in uniform motion. Surviving manuscripts of the 1660s also show Newton's interest in planetary motion and that by 1669 he had shown, for a circular case of planetary motion, that the force he called 'endeavour to recede' (now called centrifugal force) had an inverse-square relation with distance from the center. After his 1679–1680 correspondence with Hooke, described below, Newton adopted the language of inward or centripetal force. According to Newton scholar J Bruce Brackenridge, although much has been made of the change in language and difference of point of view, as between centrifugal or centripetal forces, the actual computations and proofs remained the same either way. They also involved the combination of tangential and radial displacements, which Newton was making in the 1660s. The difference between the centrifugal and centripetal points of view, though a significant change of perspective, did not change the analysis. Newton also clearly expressed the concept of linear inertia in the 1660s: for this Newton was indebted to Descartes' work published 1644.\n\nHooke published his ideas about gravitation in the 1660s and again in 1674. He argued for an attracting principle of gravitation in \"Micrographia\" of 1665, in a 1666 Royal Society lecture \"On gravity\", and again in 1674, when he published his ideas about the \"System of the World\" in somewhat developed form, as an addition to \"An Attempt to Prove the Motion of the Earth from Observations\". Hooke clearly postulated mutual attractions between the Sun and planets, in a way that increased with nearness to the attracting body, along with a principle of linear inertia. Hooke's statements up to 1674 made no mention, however, that an inverse square law applies or might apply to these attractions. Hooke's gravitation was also not yet universal, though it approached universality more closely than previous hypotheses. Hooke also did not provide accompanying evidence or mathematical demonstration. On these two aspects, Hooke stated in 1674: \"Now what these several degrees [of gravitational attraction] are I have not yet experimentally verified\" (indicating that he did not yet know what law the gravitation might follow); and as to his whole proposal: \"This I only hint at present\", \"having my self many other things in hand which I would first compleat, and therefore cannot so well attend it\" (i.e., \"prosecuting this Inquiry\").\n\nIn November 1679, Hooke began an exchange of letters with Newton, of which the full text is now published. Hooke told Newton that Hooke had been appointed to manage the Royal Society's correspondence, and wished to hear from members about their researches, or their views about the researches of others; and as if to whet Newton's interest, he asked what Newton thought about various matters, giving a whole list, mentioning \"compounding the celestial motions of the planets of a direct motion by the tangent and an attractive motion towards the central body\", and \"my hypothesis of the lawes or causes of springinesse\", and then a new hypothesis from Paris about planetary motions (which Hooke described at length), and then efforts to carry out or improve national surveys, the difference of latitude between London and Cambridge, and other items. Newton's reply offered \"a fansy of my own\" about a terrestrial experiment (not a proposal about celestial motions) which might detect the Earth's motion, by the use of a body first suspended in air and then dropped to let it fall. The main point was to indicate how Newton thought the falling body could experimentally reveal the Earth's motion by its direction of deviation from the vertical, but he went on hypothetically to consider how its motion could continue if the solid Earth had not been in the way (on a spiral path to the centre). Hooke disagreed with Newton's idea of how the body would continue to move. A short further correspondence developed, and towards the end of it Hooke, writing on 6 January 1680 to Newton, communicated his \"supposition ... that the Attraction always is in a duplicate proportion to the Distance from the Center Reciprocall, and Consequently that the Velocity will be in a subduplicate proportion to the Attraction and Consequently as Kepler Supposes Reciprocall to the Distance.\" (Hooke's inference about the velocity was actually incorrect.)\n\nIn 1686, when the first book of Newton's \"Principia\" was presented to the Royal Society, Hooke claimed that Newton had obtained from him the \"notion\" of \"the rule of the decrease of Gravity, being reciprocally as the squares of the distances from the Center\". At the same time (according to Edmond Halley's contemporary report) Hooke agreed that \"the Demonstration of the Curves generated therby\" was wholly Newton's.\n\nA recent assessment about the early history of the inverse square law is that \"by the late 1660s,\" the assumption of an \"inverse proportion between gravity and the square of distance was rather common and had been advanced by a number of different people for different reasons\". Newton himself had shown in the 1660s that for planetary motion under a circular assumption, force in the radial direction had an inverse-square relation with distance from the center. Newton, faced in May 1686 with Hooke's claim on the inverse square law, denied that Hooke was to be credited as author of the idea, giving reasons including the citation of prior work by others before Hooke. Newton also firmly claimed that even if it had happened that he had first heard of the inverse square proportion from Hooke, which it had not, he would still have some rights to it in view of his mathematical developments and demonstrations, which enabled observations to be relied on as evidence of its accuracy, while Hooke, without mathematical demonstrations and evidence in favour of the supposition, could only guess (according to Newton) that it was approximately valid \"at great distances from the center\".\n\nThe background described above shows there was basis for Newton to deny deriving the inverse square law from Hooke. On the other hand, Newton did accept and acknowledge, in all editions of the \"Principia\", that Hooke (but not exclusively Hooke) had separately appreciated the inverse square law in the solar system. Newton acknowledged Wren, Hooke and Halley in this connection in the Scholium to Proposition 4 in Book 1. Newton also acknowledged to Halley that his correspondence with Hooke in 1679–80 had reawakened his dormant interest in astronomical matters, but that did not mean, according to Newton, that Hooke had told Newton anything new or original: \"yet am I not beholden to him for any light into that business but only for the diversion he gave me from my other studies to think on these things & for his dogmaticalness in writing as if he had found the motion in the Ellipsis, which inclined me to try it ...\".) Newton's reawakening interest in astronomy received further stimulus by the appearance of a comet in the winter of 1680/1681, on which he corresponded with John Flamsteed.\n\nIn 1759, decades after the deaths of both Newton and Hooke, Alexis Clairaut, mathematical astronomer eminent in his own right in the field of gravitational studies, made his assessment after reviewing what Hooke had published on gravitation. \"One must not think that this idea ... of Hooke diminishes Newton's glory\", Clairaut wrote; \"The example of Hooke\" serves \"to show what a distance there is between a truth that is glimpsed and a truth that is demonstrated\".\n\nSince only between 250 and 400 copies were printed by the Royal Society, the first edition is very rare. Several rare-book collections contain first edition and other early copies of Newton's \"Principia Mathematica\", including:\n\n\nIn 2016, a first edition sold for $3.7 million.\nA facsimile edition (based on the 3rd edition of 1726 but with variant readings from earlier editions and important annotations) was published in 1972 by Alexandre Koyré and I. Bernard Cohen.\nTwo later editions were published by Newton:\n\nNewton had been urged to make a new edition of the \"Principia\" since the early 1690s, partly because copies of the first edition had already become very rare and expensive within a few years after 1687. Newton referred to his plans for a second edition in correspondence with Flamsteed in November 1694: Newton also maintained annotated copies of the first edition specially bound up with interleaves on which he could note his revisions; two of these copies still survive: but he had not completed the revisions by 1708, and of two would-be editors, Newton had almost severed connections with one, Nicolas Fatio de Duillier, and the other, David Gregory seems not to have met with Newton's approval and was also terminally ill, dying later in 1708. Nevertheless, reasons were accumulating not to put off the new edition any longer. Richard Bentley, master of Trinity College, persuaded Newton to allow him to undertake a second edition, and in June 1708 Bentley wrote to Newton with a specimen print of the first sheet, at the same time expressing the (unfulfilled) hope that Newton had made progress towards finishing the revisions. It seems that Bentley then realised that the editorship was technically too difficult for him, and with Newton's consent he appointed Roger Cotes, Plumian professor of astronomy at Trinity, to undertake the editorship for him as a kind of deputy (but Bentley still made the publishing arrangements and had the financial responsibility and profit). The correspondence of 1709–1713 shows Cotes reporting to two masters, Bentley and Newton, and managing (and often correcting) a large and important set of revisions to which Newton sometimes could not give his full attention. Under the weight of Cotes' efforts, but impeded by priority disputes between Newton and Leibniz, and by troubles at the Mint, Cotes was able to announce publication to Newton on 30 June 1713. Bentley sent Newton only six presentation copies; Cotes was unpaid; Newton omitted any acknowledgement to Cotes.\n\nAmong those who gave Newton corrections for the Second Edition were: Firmin Abauzit, Roger Cotes and David Gregory. However, Newton omitted acknowledgements to some because of the priority disputes. John Flamsteed, the Astronomer Royal, suffered this especially.\n\nThe Second Edition was the basis of the first edition to be printed abroad, which appeared in Amsterdam in 1714.\n\nThe third edition was published 25 March 1726, under the stewardship of \"Henry Pemberton, M.D., a man of the greatest skill in these matters...\"; Pemberton later said that this recognition was worth more to him than the two hundred guinea award from Newton.\n\nIn 1739–42, two French priests, Pères Thomas LeSeur and François Jacquier (of the Minim order, but sometimes erroneously identified as Jesuits), produced with the assistance of J.-L. Calandrini an extensively annotated version of the \"Principia\" in the 3rd edition of 1726. Sometimes this is referred to as the \"Jesuit edition\": it was much used, and reprinted more than once in Scotland during the 19th century.\n\nÉmilie du Châtelet also made a translation of Newton's Principia into French. Unlike LeSeur and Jacquier's edition, hers was a complete translation of Newton's three books and their prefaces. She also included a Commentary section where she fused the three books into a much clearer and easier to understand summary. She included an analytical section where she applied the new mathematics of calculus to Newton's most controversial theories. Previously, geometry was the standard mathematics used to analyse theories. Du Châtelet's translation is the only complete one to have been done in French and hers remains the standard French translation to this day.\n\nTwo full English translations of Newton's \"Principia\" have appeared, both based on Newton's 3rd edition of 1726.\n\nThe first, from 1729, by Andrew Motte, was described by Newton scholar I. Bernard Cohen (in 1968) as \"still of enormous value in conveying to us the sense of Newton's words in their own time, and it is generally faithful to the original: clear, and well written\". The 1729 version was the basis for several republications, often incorporating revisions, among them a widely used modernised English version of 1934, which appeared under the editorial name of Florian Cajori (though completed and published only some years after his death). Cohen pointed out ways in which the 18th-century terminology and punctuation of the 1729 translation might be confusing to modern readers, but he also made severe criticisms of the 1934 modernised English version, and showed that the revisions had been made without regard to the original, also demonstrating gross errors \"that provided the final impetus to our decision to produce a wholly new translation\".\n\nThe second full English translation, into modern English, is the work that resulted from this decision by collaborating translators I. Bernard Cohen, Anne Whitman, and Julia Budenz; it was published in 1999 with a guide by way of introduction.\n\nWilliam H. Donahue has published a translation of the work's central argument, published in 1996, along with expansion of included proofs and ample commentary. The book was developed as a textbook for classes at St. John's College and the aim of this translation is to be faithful to the Latin text.\n\nIn 2014, British astronaut Tim Peake named his upcoming mission to the International Space Station \"Principia\" after the book, in \"honour of Britain's greatest scientist\". Tim Peake's \"Principia\" launched on December 15, 2015 aboard Soyuz TMA-19M.\n\n\n\n\n\n\n\n\n"}
{"id": "41872647", "url": "https://en.wikipedia.org/wiki?curid=41872647", "title": "Registry of Research Data Repositories", "text": "Registry of Research Data Repositories\n\nThe Registry of Research Data Repositories (re3data.org) is an Open Science tool that offers researchers, funding organizations, libraries and publishers an overview of existing international repositories for research data.\n\nre3data.org is a global registry of research data repositories from all academic disciplines. It provides an overview of existing research data repositories in order to help researchers to identify a suitable repository for their data and thus comply with requirements set out in data policies.\nThe registry was officially launched in May 2013.\n\nIn March 2014 the registry lists 634 research data repositories from around the world covering all academic disciplines. 586 of these are described in detail using the re3data.org schema.\nThe project makes all metadata in the registry available for open use under the Creative Commons deed CC0.\n\nThe majority of the listed research data repositories are described in detail by a comprehensive schema, namely the re3data.org Schema for the Description of Research Data Repositories.\nInformation icons support researchers to identify an adequate repository for the storage and reuse of their data.\n\nA repository is indexed when the minimum requirements for inclusion in re3data.org are met: the repository has to be run by a legal entity, such as a sustainable institution (e.g. library, university) and clearly state access conditions to the data and repository as well as the terms of use. Additionally, an English graphical user interface (GUI) plus a focus on research data is needed.\n\nre3data.org is a joint project of the Berlin School of Library and Information Science, the GFZ German Research Centre for Geosciences and the Library of the Karlsruhe Institute of Technology (KIT). The project is funded by the German Research Foundation (DFG).\nThe project cooperates with other Open Science initiatives like Databib, BioSharing, DataCite and OpenAIRE. Several publishers, research institutions and funders refer to re3data.org in their Editorial Policies and guidelines as a tool for the identification of suitable data repositories, e.g. Nature, Springer and the European Commission.\n\n\n"}
{"id": "47647187", "url": "https://en.wikipedia.org/wiki?curid=47647187", "title": "Reproducibility Project", "text": "Reproducibility Project\n\nThe Reproducibility Project: Psychology was a collaboration of 270 contributing authors to repeat 100 published experimental and correlational psychological studies. This project was led by the Center for Open Science and its co-founder, Brian Nosek, who started the project in November 2011. The results of this collaboration were published in August 2015. Reproducibility is the ability to produce a copy or duplicate, in this case it is the ability to replicate the results of the original studies. The project has illustrated the growing problem of failed reproducibility in social science. This project has started a movement that has spread through the science world with the expanded testing of the reproducibility of published works.\n\nBrian Nosek of University of Virginia and colleagues sought out to replicate 100 different studies that all were published in 2008. The project pulled these studies from three different journals\", Psychological Science\", the \"Journal of Personality and Social Psychology\", and the \"\", published in 2008 to see if they could get the same results as the initial findings. In their initial publications 97 of these 100 studies claimed to have significant results. To stay as true as they could the group went through extensive measures to remain true to the original studies, to the extent of consulting the original authors. Even with all the extra steps taken to ensure the same conditions of the original 97 studies only 35 of the studies replicated (36.1%), and if they did replicate their effects were smaller than the initial studies effects. The authors emphasized that the findings reflect a problem that affects all of science not just psychology, and that there is room to improve reproducibility in psychology.\n\nFailure to replicate can be caused by a few different reasons. The first is a type II error, which is when you accept the null hypothesis when it is false. This can be classified as a false negative. A type I error is the rejection of a null hypothesis even if it is true, so this is considered a false positive.\n\nThe Center for Open Science was founded by Brian Nosek and Jeff Spies in 2013 with a $5.25 million grant from the Laura and John Arnold Foundation. They have built a team that today has about 50 individuals on it. The goal of the group is to help increase the openness, integrity and reproducibility of scientific research. The COS which is a rather small number of individuals oversee much larger groups that are helping with the COS's mission. The group is made up of multiple different kinds of scientists which include astronomers, biologists, chemists, computer scientists, education researchers, engineers, neuroscientists, and psychologists. By 2017 the Foundation had provided an additional $10 million in funding.\n\nThere have been multiple implications of the Reproducibility Project. People all over have started to question the legitimacy of scientific studies that have been published in esteemed journals. Journals typically only publish articles with big effect sizes that reject the null hypothesis. Leading into the huge issue of people re-doing studies that have already found to fail, but not knowing because there is no record of the failed studies, which will lead to more false positives to be published. It is unknown if any of the original study authors committed fraud in publishing their projects, but some of the authors of the original studies are part of the 270 contributors of this project.\n\nOne earlier study found that around $28 billion worth of research per year in medical fields is non-reproducible.\n"}
{"id": "25524", "url": "https://en.wikipedia.org/wiki?curid=25524", "title": "Research", "text": "Research\n\nResearch comprises \"creative and systematic work undertaken to increase the stock of knowledge, including knowledge of humans, culture and society, and the use of this stock of knowledge to devise new applications.\" It is used to establish or confirm facts, reaffirm the results of previous work, solve new or existing problems, support theorems, or develop new theories. A research project may also be an expansion on past work in the field. Research projects can be used to develop further knowledge on a topic, or in the example of a school research project, they can be used to further a student's research prowess to prepare them for future jobs or reports. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects or the project as a whole. The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, or the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life, technological, etc.\n\nThe word \"research\" is derived from the Middle French \"recherche\", which means \"to go about seeking\", the term itself being derived from the Old French term \"recerchier\" a compound word from \"re-\" + \"cerchier\", or \"sercher\", meaning 'search'. The earliest recorded use of the term was in 1577.\n\nResearch has been defined in a number of different ways, and while there are similarities, there does not appear to be a single, all-encompassing definition that is embraced by all who engage in it.\n\nOne definition of research is used by the OECD, \"Any creative systematic activity undertaken in order to increase the stock of knowledge, including knowledge of man, culture and society, and the use of this knowledge to devise new applications.\"\n\nAnother definition of research is given by John W. Creswell, who states that \"research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue\". It consists of three steps: pose a question, collect data to answer the question, and present an answer to the question.\n\nThe Merriam-Webster Online Dictionary defines research in more detail as \"studious inquiry or examination; \"especially\" : investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws\"\n\nOriginal research is research that is not exclusively based on a summary, review or synthesis of earlier publications on the subject of research. This material is of a primary source character. The purpose of the original research is to produce new knowledge, rather than to present the existing knowledge in a new form (\"e.g.\", summarized or classified).\n\nOriginal research can take a number of forms, depending on the discipline it pertains to. In experimental work, it typically involves direct or indirect observation of the researched subject(s), e.g., in the laboratory or in the field, documents the methodology, results, and conclusions of an experiment or set of experiments, or offers a novel interpretation of previous results. In analytical work, there are typically some new (for example) mathematical results produced, or a new way of approaching an existing problem. In some subjects which do not typically carry out experimentation or analysis of this kind, the originality is in the particular way existing understanding is changed or re-interpreted based on the outcome of the work of the researcher.\n\nThe degree of originality of the research is among major criteria for articles to be published in academic journals and usually established by means of peer review. Graduate students are commonly required to perform original research as part of a dissertation.\n\nScientific research is a systematic way of gathering data and harnessing curiosity. This research provides scientific information and theories for the explanation of the nature and the properties of the world. It makes practical applications possible. Scientific research is funded by public authorities, by charitable organizations and by private groups, including many companies. Scientific research can be subdivided into different classifications according to their academic and application disciplines. Scientific research is a widely used criterion for judging the standing of an academic institution, but some argue that such is an inaccurate assessment of the institution, because the quality of research does not tell about the quality of teaching (these do not necessarily correlate).\n\nResearch in the humanities involves different methods such as for example hermeneutics and semiotics. Humanities scholars usually do not search for the ultimate correct answer to a question, but instead, explore the issues and details that surround it. Context is always important, and context can be social, historical, political, cultural, or ethnic. An example of research in the humanities is historical research, which is embodied in historical method. Historians use primary sources and other evidence to systematically investigate a topic, and then to write histories in the form of accounts of the past. Other studies aim to merely examine the occurrence of behaviours in societies and communities, without particularly looking for reasons or motivations to explain these. These studies may be qualitative or quantitative, and can use a variety of approaches, such as queer theory or feminist theory.\n\nArtistic research, also seen as 'practice-based research', can take form when creative works are considered both the research and the object of research itself. It is the debatable body of thought which offers an alternative to purely scientific methods in research in its search for knowledge and truth.\n\nGenerally, research is understood to follow a certain structural process. Though step order may vary depending on the subject matter and researcher, the following steps are usually part of most formal research, both basic and applied:\n\nA common misconception is that a hypothesis will be proven (see, rather, null hypothesis). Generally, a hypothesis is used to make predictions that can be tested by observing the outcome of an experiment. If the outcome is inconsistent with the hypothesis, then the hypothesis is rejected (see falsifiability). However, if the outcome is consistent with the hypothesis, the experiment is said to support the hypothesis. This careful language is used because researchers recognize that alternative hypotheses may also be consistent with the observations. In this sense, a hypothesis can never be proven, but rather only supported by surviving rounds of scientific testing and, eventually, becoming widely thought of as true.\n\nA useful hypothesis allows prediction and within the accuracy of observation of the time, the prediction will be verified. As the accuracy of observation improves with time, the hypothesis may no longer provide an accurate prediction. In this case, a new hypothesis will arise to challenge the old, and to the extent that the new hypothesis makes more accurate predictions than the old, the new will supplant it. Researchers can also use a null hypothesis, which states no relationship or difference between the independent or dependent variables.\n\nThe historical method comprises the techniques and guidelines by which historians use historical sources and other evidence to research and then to write history. There are various history guidelines that are commonly used by historians in their work, under the headings of external criticism, internal criticism, and synthesis. This includes lower criticism and sensual criticism. Though items may vary depending on the subject matter and researcher, the following concepts are part of most formal historical research:\n\nThe controversial trend of artistic teaching becoming more academics-oriented is leading to artistic research being accepted as the primary mode of enquiry in art as in the case of other disciplines. One of the characteristics of artistic research is that it must accept subjectivity as opposed to the classical scientific methods. As such, it is similar to the social sciences in using qualitative research and intersubjectivity as tools to apply measurement and critical analysis.\n\nArtistic research has been defined by the University of Dance and Circus (Dans och Cirkushögskolan, DOCH), Stockholm in the following manner - \"Artistic research is to investigate and test with the purpose of gaining knowledge within and for our artistic disciplines. It is based on artistic practices, methods, and criticality. Through presented documentation, the insights gained shall be placed in a context.\" Artistic research aims to enhance knowledge and understanding with presentation of the arts. A more simple understanding by Julian Klein defines Artistic Research as any kind of research employing the artistic mode of perception. For a survey of the central problematics of today's Artistic Research, see Giaco Schiesser.\n\nAccording to artist Hakan Topal, in artistic research, \"perhaps more so than other disciplines, intuition is utilized as a method to identify a wide range of new and unexpected productive modalities\". Most writers, whether of fiction or non-fiction books, also have to do research to support their creative work. This may be factual, historical, or background research. Background research could include, for example, geographical or procedural research.\n\nThe Society for Artistic Research (SAR) publishes the triannual \"Journal for Artistic Research\" (JAR), an international, online, open access, and peer-reviewed journal for the identification, publication, and dissemination of artistic research and its methodologies, from all arts disciplines and it runs the \"Research Catalogue\" (RC), a searchable, documentary database of artistic research, to which anyone can contribute.\n\nPatricia Leavy addresses eight arts-based research (ABR) genres: narrative inquiry, fiction-based research, poetry, music, dance, theatre, film, and visual art.\n\nIn 2016 ELIA (European League of the Institutes of the Arts) launched \"The Florence Principles' on the Doctorate in the Arts\". The Florence Principles relating to the Salzburg Principles and the Salzburg Recommendations of EUA (European University Association) name seven points of attention to specify the Doctorate / PhD in the Arts compared to a scientific doctorate / PhD The Florence Principles have been endorsed and are supported also by AEC, CILECT, CUMULUS and SAR.\n\nResearch is often conducted using the hourglass model structure of research. The hourglass model starts with a broad spectrum for research, focusing in on the required information through the method of the project (like the neck of the hourglass), then expands the research in the form of discussion and results. The major steps in conducting research are:\n\nThe steps generally represent the overall process; however, they should be viewed as an ever-changing iterative process rather than a fixed set of steps. Most research begins with a general statement of the problem, or rather, the purpose for engaging in the study. The literature review identifies flaws or holes in previous research which provides justification for the study. Often, a literature review is conducted in a given subject area before a research question is identified. A gap in the current literature, as identified by a researcher, then engenders a research question. The research question may be parallel to the hypothesis. The hypothesis is the supposition to be tested. The researcher(s) collects data to test the hypothesis. The researcher(s) then analyzes and interprets the data via a variety of statistical methods, engaging in what is known as empirical research. The results of the data analysis in rejecting or failing to reject the null hypothesis are then reported and evaluated. At the end, the researcher may discuss avenues for further research. However, some researchers advocate for the reverse approach: starting with articulating findings and discussion of them, moving \"up\" to identification of a research problem that emerges in the findings and literature review. The reverse approach is justified by the transactional nature of the research endeavor where research inquiry, research questions, research method, relevant research literature, and so on are not fully known until the findings have fully emerged and been interpreted.\n\nRudolph Rummel says, \"... no researcher should accept any one or two tests as definitive. It is only when a range of tests are consistent over many kinds of data, researchers, and methods can one have confidence in the results.\"\n\nPlato in Meno talks about an inherent difficulty, if not a paradox, of doing research that can be paraphrased in the following way, \"If you know what you're searching for, why do you search for it?! [i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\"\n\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\nThere are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\n\n\nSocial media posts are used for qualitative research.\n\n\nThe quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that are easy to summarize, compare, and generalize. Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest.\n\nIf the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment). If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants.\n\nIn either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible.\n\nMixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common. This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.\n\nBig data has brought big impacts on research methods so that now many researchers do not put much effort into data collection; furthermore, methods to analyze easily available huge amounts of data have also been developed.\n\nNon-empirical (theoretical) research is an approach that involves the development of theory as opposed to using observation and experimentation. As such, non-empirical research seeks solutions to problems using existing knowledge as its source. This, however, does not mean that new ideas and innovations cannot be found within the pool of existing and established knowledge. Non-empirical research is not an absolute alternative to empirical research because they may be used together to strengthen a research approach. Neither one is less effective than the other since they have their particular purpose in science. Typically empirical research produces observations that need to be explained; then theoretical research tries to explain them, and in so doing generates empirically testable hypotheses; these hypotheses are then tested empirically, giving more observations that may need further explanation; and so on. See Scientific method.\n\nA simple example of a non-empirical task is the prototyping of a new drug using a differentiated application of existing knowledge; another is the development of a business process in the form of a flow chart and texts where all the ingredients are from established knowledge. Much of cosmological research is theoretical in nature. Mathematics research does not rely on externally available data; rather, it seeks to prove theorems about mathematical objects.\n\nResearch ethics involves the application of fundamental ethical principles to a variety of topics involving research, including scientific research. These principles include deontology, consequentialism, virtue ethics and value (ethics). Ethical issues may arise in the design and implementation of research involving human experimentation or animal experimentation, such as: various aspects of academic scandal, including scientific misconduct (such as fraud, fabrication of data and plagiarism), whistleblowing; regulation of research, etc. Research ethics is most developed as a concept in medical research. The key agreement here is the 1964 Declaration of Helsinki. The Nuremberg Code is a former agreement, but with many still important notes. Research in the social sciences presents a different set of issues than those in medical research and can involve issues of researcher and participant safety, empowerment and access to justice.\n\nWhen research involves human subjects, obtaining informed consent from them is essential.\n\nIn many disciplines, Western methods of conducting research are predominant. Researchers are overwhelmingly taught Western methods of data collection and study. The increasing participation of indigenous peoples as researchers has brought increased attention to the lacuna in culturally-sensitive methods of data collection. Non-Western methods of data collection may not be the most accurate or relevant for research on non-Western societies. For example, \"Hua Oranga\" was created as a criterion for psychological evaluation in Māori populations, and is based on dimensions of mental health important to the Māori people – \"taha wairua (the spiritual dimension), taha hinengaro (the mental dimension), taha tinana (the physical dimension), and taha whanau (the family dimension)\".\n\nPeriphery scholars face the challenges of exclusion and linguicism in research and academic publication. As the great majority of mainstream academic journals are written in English, multilingual periphery scholars often must translate their work to be accepted to elite Western-dominated journals. Multilingual scholars' influences from their native communicative styles can be assumed to be incompetence instead of difference.\n\nPeer review is a form of self-regulation by qualified members of a profession within the relevant field. Peer review methods are employed to maintain standards of quality, improve performance, and provide credibility. In academia, scholarly peer review is often used to determine an academic paper's suitability for publication. Usually, the peer review process involves experts in the same field who are consulted by editors to give a review of the scholarly works produced by a colleague of theirs from an unbiased and impartial point of view, and this is usually done free of charge. The tradition of peer reviews being done for free has however brought many pitfalls which are also indicative of why most peer reviewers decline many invitations to review. It was observed that publications from periphery countries rarely rise to the same elite status as those of North America and Europe, because limitations on the availability of resources including high-quality paper and sophisticated image-rendering software and printing tools render these publications less able to satisfy standards currently carrying formal or informal authority in the publishing industry. These limitations in turn result in the under-representation of scholars from periphery nations among the set of publications holding prestige status relative to the quantity and quality of those scholars' research efforts, and this under-representation in turn results in disproportionately reduced acceptance of the results of their efforts as contributions to the body of knowledge available worldwide.\n\nThe open access movement assumes that all information generally deemed useful should be free and belongs to a \"public domain\", that of \"humanity\". This idea gained prevalence as a result of Western colonial history and ignores alternative conceptions of knowledge circulation. For instance, most indigenous communities consider that access to certain information proper to the group should be determined by relationships.\n\nThere is alleged to be a double standard in the Western knowledge system. On the one hand, \"digital right management\" used to restrict access to personal information on social networking platforms is celebrated as a protection of privacy, while simultaneously when similar functions are used by cultural groups (i.e. indigenous communities) this is denounced as \"access control\" and reprehended as censorship.\n\nEven though Western dominance seems to be prominent in research, some scholars, such as Simon Marginson, argue for \"the need [for] a plural university world\". Marginson argues that the East Asian Confucian model could take over the Western model.\n\nThis could be due to changes in funding for research both in the East and the West. Focussed on emphasizing educational achievement, East Asian cultures, mainly in China and South Korea, have encouraged the increase of funding for research expansion. In contrast, in the Western academic world, notably in the United Kingdom as well as in some state governments in the United States, funding cuts for university research have occurred, which some say may lead to the future decline of Western dominance in research.\n\nIn several national and private academic systems, the professionalisation of research has resulted in formal job titles.\n\nIn present-day Russia, the former Soviet Union and in some post-Soviet states the term \"researcher\" (, \"nauchny sotrudnik\") is both a generic term for a person who carried out scientific research, as well as a job position within the frameworks of the USSR Academy of Sciences, Soviet universities, and in other research-oriented establishments. The term is also sometimes translated as \"research fellow\", \"research associate\", etc.\n\nThe following ranks are known:\n\n\nAcademic publishing is a system that is necessary for academic scholars to peer review the work and make it available for a wider audience. The system varies widely by field and is also always changing, if often slowly. Most academic work is published in journal article or book form. There is also a large body of research that exists in either a thesis or dissertation form. These forms of research can be found in databases explicitly for theses and dissertations. In publishing, STM publishing is an abbreviation for academic publications in science, technology, and medicine.\nresearch paper guides\nMost established academic fields have their own scientific journals and other outlets for publication, though many academic journals are somewhat interdisciplinary, and publish work from several distinct fields or subfields. The kinds of publications that are accepted as contributions of knowledge or research vary greatly between fields, from the print to the electronic format. A study suggests that researchers should not give great consideration to findings that are not replicated frequently. It has also been suggested that all published studies should be subjected to some measure for assessing the validity or reliability of its procedures to prevent the publication of unproven findings. Business models are different in the electronic environment. Since about the early 1990s, licensing of electronic resources, particularly journals, has been very common. Presently, a major trend, particularly with respect to scholarly journals, is open access. There are two main forms of open access: open access publishing, in which the articles or the whole journal is freely available from the time of publication, and self-archiving, where the author makes a copy of their own work freely available on the web.\n\nMost funding for scientific research comes from three major sources: corporate research and development departments; private foundations, for example, the Bill and Melinda Gates Foundation; and government research councils such as the National Institutes of Health in the USA and the Medical Research Council in the UK. These are managed primarily through universities and in some cases through military contractors. Many senior researchers (such as group leaders) spend a significant amount of their time applying for grants for research funds. These grants are necessary not only for researchers to carry out their research but also as a source of merit.\n\nThe Social Psychology Network provides a comprehensive list of U.S. Government and private foundation funding sources.\n\n\n\n"}
{"id": "16624451", "url": "https://en.wikipedia.org/wiki?curid=16624451", "title": "Research fellow", "text": "Research fellow\n\nA research fellow is an academic research position at a university or a similar research institution, usually for academic staff or faculty members. A research fellow may act either as an independent investigator or under the supervision of a principal investigator.\n\nIn contrast to a research assistant, the position of research fellow normally requires a doctoral degree, or equivalent work experience for instance in industry and research centers. Some research fellows undertake postdoctoral research or have some moderate teaching responsibilities. Research fellow positions vary in different countries and academic institutions. In some cases, they are permanent with the possibility of promotion, while in other instances they are temporary.\n\nIn many universities this position is a career grade of a \"Research Career Pathway\", following on from a postdoctoral position such as Research Associate, and may be open-ended, subject to normal probation regulations. Within such a path, the next two higher career grades are usually senior research fellow and professorial fellow. Although similar to the position of a research fellow, these two positions are research only posts, with the rise of the career grade there will normally be a formal requirement of a moderate amount of teaching and/or supervision (often at postgraduate level). These positions are for researchers with a proven track record of generating research income to fund themselves and producing high-quality research output that is internationally recognised.\n\nIn some universities, research career grades roughly correspond to the grades of the \"Teaching and Scholarship Career Pathways\" in the following way: research fellow—lecturer, professorial fellow—professor, whereas senior research fellow is somewhere between a reader and a senior lecturer. However, at some top universities, a senior research fellowship may be a position of comparable academic standing to a full professorship at these universities, without any teaching requirements.\n\nIn the past, the term research fellow often referred to a junior researcher, who worked on a specific project on a temporary basis. Research fellows tended to be paid either from central university funds or by an outside organisation such as a charity or company, or through an external grant-awarding body such as a research council or a royal society, for example in the Royal Society University Research Fellowship. Particularly in Oxbridge style colleges, research fellows appointed as fellows of a college tended to, or still do, partially receive remuneration in form of college housing and subsistence. Colleges may award junior research fellowships as the equivalent of post-doctoral research posts, lasting for three or four years. In contrast, senior research fellows tended to be established academics, often a professor on sabbatical from another institution, conducting temporally research elsewhere.\n\nIn India, the position of research fellowship is provided to scholars from various streams like science, arts, literature, agriculture to reward their excellence . Research fellowship is funded by government academic and research institutes, and private companies as well. Research fellows research under the supervision of experienced faculty, professor, head of department, Dean on two different posts known as junior research fellow(JRF) and senior research fellow(SRF). Research organisations like ICAR, CSIR, UGC, ICMR, SERB recruit research fellows through National Eligibility Test. After the completion of pre-defined tenure, JRF can be considered for senior research fellowship based on research fellow's performance & interview conducted by committee by research institute, research fellow is working with.\n\nIn the Russian Federation, the position and title research fellow is unknown; however, there is a broadly similar position of (, literally \"scientific worker\"). This position normally requires a degree of Candidate of Sciences approximately corresponding to the PhD. More senior positions normally require, in addition to the aforementioned degree, a track record of publications or certified inventions, as well as practical contributions to major research and development projects.\nResearch fellows in South Africa are considered as the best asset to research organisations and universities. There are highly ranked universities like University of the Witwatersrand, University of Stellenbosch Business School, rhode university which offers fellowship to South Africa's nationals in a certain field of research.\n\nIn some countries, the English term research fellow is sometimes used to refer to the \"holder\" of a \"research fellowship\" that funds research.\n\nIn Germany, institutions such as the Alexander von Humboldt Foundation offer research fellowship for postdoctoral research and refer to the holder as research fellows, while the award holder may formally hold a specific title at his or her home institution (e.g., \"Privatdozent\").\n\n"}
{"id": "1593617", "url": "https://en.wikipedia.org/wiki?curid=1593617", "title": "Retrodiction", "text": "Retrodiction\n\nRetrodiction (also known as postdiction—although this should not be confused with the use of the term in criticisms of parapsychological research) is the act of making a \"prediction\" about the past.\n\nThe activity of retrodiction (or postdiction) involves moving backwards in time, step-by-step, in as many stages as are considered necessary, from the present into the speculated past to establish the ultimate cause of a specific event (for instance, in the case of reverse engineering, forensics, etc.).\n\nGiven that retrodiction is a process in which \"past observations, events and data are used as evidence to infer the process(es) that produced them\" and that diagnosis \"involve[s] going from visible effects such as symptoms, signs and the like to their prior causes\" the essential balance between prediction and retrodiction could be characterized as:\nregardless of whether the prognosis is of the course of the disease in the absence of treatment, or of the application of a specific treatment regimen to a specific disorder in a particular patient:\n\nIn scientific method, the terms \"retrodiction\" or \"postdiction\" are used in several senses.\n\nOne use refers to the act of evaluating a scientific theory by predicting known rather than new events. For example, a theory in physics that claims to extend or replace the standard model but that fails to predict the existence of known particles has not met the test of \"postdiction\".\n\nMichael Clive Price has written: \n\nA retrodiction occurs when already gathered data is accounted for by a later theoretical advance in a more convincing fashion. The advantage of a retrodiction over a prediction is that the already gathered data is more likely to be free of experimenter bias. An example of a retrodiction is the perihelion shift of Mercury which Newtonian mechanics plus gravity was unable, totally, to account for whilst Einstein's general relativity made short work of it.\n\nAnother use refers to a process by which one attempts to test a theory whose predictions are too long-term to be tested by waiting for a future event to occur. Instead, one speculates about uncertain events in the more distant past, and applies the theory to consider how it would have predicted a known event in the less distant past. This is useful in, for example, the fields of archaeology, climatology, evolutionary biology, financial analysis, forensic science, and cosmology.\n\nIn the field of neuroscience, the term \"postdiction\" was introduced by David Eagleman to describe a perceptual process in which the brain collects information after an event before it retrospectively decides what happened at the time of the event (Eagleman and Sejnowski, 2000). Some perceptual illusions in which the brain mistakenly perceives the location of moving stimuli may involve postdiction. Such illusions include the flash lag illusion and the cutaneous rabbit illusion.\n\n\n"}
{"id": "20685207", "url": "https://en.wikipedia.org/wiki?curid=20685207", "title": "Science Under Siege", "text": "Science Under Siege\n\nScience Under Siege: The Politicians' War on Nature and Truth is a 1998 book by journalist Todd Wilkinson. Wilkinson describes the careers of a variety of publicly employed scientists who, in the course of their work for government agencies, found habitat degradation, threatened species, or other decline in availability of a natural resource. When they expressed their views that certain activities must be scaled back or areas protected, they met with poor job performance ratings, hostility from their supervisors, transfers out of the region, and in many cases a severely damaged career. Science is \"under siege\" in these cases because many of the researchers were told to modify their scientific reports so that commercial uses or environmentally destructive activities could continue.\n\n"}
{"id": "51586709", "url": "https://en.wikipedia.org/wiki?curid=51586709", "title": "Second-wave positive psychology", "text": "Second-wave positive psychology\n\nSecond wave positive psychology (PP 2.0) is concerned with how to bring out the best in individuals and society in spite of and because of the dark side of human existence through the dialectical principles of yin and yang. There has also been a distinct shift from focusing on individual happiness and success to the double vision of individual well-being and the big picture of humanity. PP 2.0 is more about bringing out the \"better angels of our nature\" than achieving optimal happiness or personal success, because the better angels of empathy, compassion, reason, justice, and self-transcendence will make people better human beings and this world a better place. PP 2.0 pivots around the universal human capacity for meaning seeking and meaning making in achieving optimal human functioning under both desirable and undesirable conditions. This emerging movement is an inevitable and necessary corrective response to the inherent problems of what has been called \"positive psychology as usual\".\n\nPositive psychology \"as usual\" has been presented as the branch of psychology that uses scientific understanding and effective intervention to aid in the achievement of a satisfactory life, rather than treating mental illness. The focus of positive psychology is on personal growth rather than on pathology.It has been argued that this binary, dichotomous view has fuelled both positive psychology's success and decline. The single-minded focus on positivity has resulted in persistent backlash (e.g., Frawley, 2015 for a recent review). The following criticisms have been leveled against positive psychology by researchers both outside of and within the positive psychology community. These include the \"tyranny\" of positivity and the lack of balance between positives and negatives; failing to cover the entire spectrum of human experiences; failing to recognize the importance of contextual variables; and assuming that the Western individualistic culture represents the universal human experience. As a result, various positive psychologists have proposed the need for a balanced perspective.\n\nIn order to correct the limitations of positive psychology, Paul Wong has argued for the need to integrate positive psychology with existential psychology, resulting in \"existential positive psychology\" (EPP). This approach differs significantly from positive psychology \"as usual\" both in terms of epistemology and content.\n\nEPP favors a pluralistic and holistic approach to research. It is open to insights and wisdoms from both the East and the West and research findings from all sources regardless of the paradigm of truth claims. In terms of content, it explores both people's existential anxieties and their ultimate concerns. Thus, EPP contributes to a broader and more comprehensive understanding of human experiences.\n\nBoth existential philosophers and psychologists see life as a series of paradoxes, predicaments, and problems. From this existential perspective, life is also full of striving and sense-making, tragedies and triumphs. The dynamic interplay between good and evil, negatives and positives is one of the hallmarks of EPP. Positives cannot exist apart from negatives, and authentic happiness grows from pain and suffering. This paradoxical view reflects Albert Camus' insight that \"there is no joy of life without despair\" (p. 56) and Rollo May's observation that \"the ultimate paradox is that negation becomes affirmation\" (p. 164).\n\nIt may be argued that positive psychology is intrinsically existential because it is concerned with such fundamental questions about human existence as: What is the good life? What makes life worth living? How can one find happiness? Positive psychology research on these existential issues without taking into account the existential literature inevitably leads to superficiality or mischaracterization.\n\nA comprehensive positive psychology cannot be developed without taking into account the reality of death, the only certainty for all living organisms. However, human beings alone are burdened with the cognitive capacity to be aware of their own mortality and to fear what may follow after one's own demise. However, death awareness may be essential to meaningful living; \"though the physicality of death destroys us, the idea of death saves us\" (p. 7). Thus, awareness of our finality is indeed an important motivational factor for us to do something meaningful and significant with our lives.\n\nTherefore, EPP advocates that the proper context of studying well-being and the meaningful life is the reality of suffering and death. Researchers who share this view include Bretherton and Ørner, Schneider, and Taheny.\n\nRecently, positive psychologists have recognized that positive psychology is rooted in humanistic-existential psychology, but in practice it continues to distance itself from its heritage because of the alleged lack of scientific research in humanistic psychology. A mature positive psychology needs to return to its existential-humanistic roots, because it can both broaden and deepen positive psychology.\n\nPaul Wong extends EPP to second wave positive psychology (PP 2.0) by formally incorporating the dialectical principles of Chinese psychology, the bio-behavioral dual-system model of adaptation, and cross-cultural positive psychology. Thus, PP 2.0 provides a big tent that allows for multiple indigenous positive psychologies and a much broader list of variables that contribute to well-being and flourishing.\n\nPP 2.0 is necessary, because neither positive psychology nor humanistic-existential psychology can adequately understand such complex human phenomena as meaning, virtue, and happiness. Such deep knowledge can only be achieved by an integrative and collaborative endeavor. This calls for a humble science. In other words, PP 2.0 denies that the positivist paradigm is the only way to examine truth claims, especially when we research the profound questions of what makes life worth living.\n\n\n\nTaoist dialectical thinking permeates every aspect of Chinese psychology. Peng and Nisbett showed that Chinese thinking is dialectical rather than binary. Similarly, Sundararajan documents the dialectical co-existence of positive and negative emotions in Chinese people.\n\nPaul Wong's research has also demonstrated that the wisdom of yin and yang operates in many situations. He has argued that Chinese people can hold external and internal loci of control simultaneously. Therefore, their locus of control beliefs can only be a measured by a two-dimensional space with external and internal loci of control as two independent scales. However, dialectic thinking is not unique to Chinese culture. For example, pessimism and optimism can co-exist, resulting in tragic optimism. Death fear and death acceptance can also co-exist, resulting in the multidimensional Death Attitude Profile. Resources and deficits co-exist, as conceptualized in the Resource-Congruence Model. Thus, family can be resources for effective coping, but intra-family conflict can also be a deficit or stressor.\n\nWong's Dual-Systems Model of approach and avoidance spells out the mechanisms whereby the good life can be achieved in the midst of adversities, not by accentuating the positive and avoiding the negative, but by embracing the dynamic and dialectic interaction between positive and negative experiences. This general bio-behavioral model is also based on the dialectical principle. Dialectical thinking represents a simple but powerful conceptual framework, capable of integrating a great deal of the literature relevant to well-being. Yin represents not only the dark side of life, but also the conservative and passive modes of adaptation, such as acceptance, letting go, avoidance, withdrawal, disengagement, doing nothing, and self-transcendence. Yang represents not only the bright side of life, but also the energetic and active modes of adaptation, such as goal setting and goal striving, problem solving and controlling, and expanding and maintaining territories.\n\nThe dynamic balance between positive and negative forces is mediated through dialectical principles. For instance, Lomas and Ivtzan have identified three ways of restoring and maintaining the balance: (a) the principle of appraisal, (b) the principle of co-valence, and (c) the principle of complementarity.\n\nSimilarly, Wong identifies four principles of transforming the dark side: (a) becoming wiser and better through the synthesis of opposites, (b) becoming more balanced and flexible through the co-existence of opposites which complement or moderate each other, (c) becoming more aware and appreciative of the after-effects or contrast effect due to the opponent-process, and (d) becoming stronger and more spiritual through self-transcendence.\n\nThus, the wisdom of achieving the golden mean or the middle way is through the dialectical interactions between yin and yang. These dialectical principles constitute the foundation of PP 2.0 and ensure that the dark side of life serves the adaptive functions of survival and flourishing.\n\nThe dark side refers to more than just challenging experiences, thoughts, emotions, and behaviors that trigger discomfort. It also encompasses existential anxieties and the evitable sufferings in life. Apart from existential concerns, the dark side also refers to our Achilles' heel. From Aristotle to Shakespeare, the literature has always recognized the existence of tragic heroes—powerful and successful individuals who are eventually ruined by their own character flaws. As Aristotle said, \"A man cannot become a hero until he can see the root of his own downfall.\" All one's talents, character strengths, and efforts will eventually come to null, with disastrous consequences to oneself and others, when one pays no attention to one's own Achilles heel.\n\nPaul Wong proposes that the meaning hypothesis is an overarching conceptual framework for PP 2.0 because it is based on the universal human capacity for meaning making and meaning seeking and the vital role meaning plays in human experience and well-being. It hypothesizes that meaning is the best possible end value for the good life and offers the best protection against existential anxieties and adversities.\n\nThe meaning hypothesis places more emphasis on a fundamental change in global beliefs and values than behavioral change. The \"meaning mindset\" affirms that life has unconditional meaning and it can be found in any situation. Figure 1 presents a schematic presentation of the meaning mindset.\n\nIf one chooses the meaning mindset, one can still find meaning and fulfillment even when failing to complete one's life mission. Thus, there is no failure when one pursues a virtuous and noble mission as one's life goal. A perspective shift to the meaning mindset helps eliminate one main source of human misery related to the striving to achieve material success or worldly fame. Cultivating a meaning mindset may yield better payoff than positive psychology exercises of enhancing happiness and character strengths because the perspective shift reorients one's focus away from egotistic pursuits to self-transcendence and altruism, which benefit both the individual and society.\n\nScience is always self-corrective and progressive. PP 2.0 avoids many of the problems inherent in positive psychology \"as usual\" and opens up new avenues of research and applications. The future of psychology can benefit from integrating three distinct movements—humanistic-existential psychology, positive psychology, and indigenous psychology.\n\nThe 21st century belongs to PP 2.0 because of its ability to integrate various sub-disciplines of mainstream psychology and its humble science approach. PP 2.0 is willing to put aside dogmatic epistemological positions in the service of the greater good as some have recommended.\n\nMeaning management of the dialectical principles is sensitive to individual and cultural contexts, but is, at the same time, also cognizant of the common good of humility and self-transcendence. This big picture perspective of PP 2.0 avoids many of the excesses associated with the egotistic pursuits of happiness and success in positive psychology as usual.\n\nPP 2.0 is spearheaded by Paul Wong from the International Network on Personal Meaning and Saybrook University, and Itai Ivtzan and Tim Lomas from the University of East London (UEL). Wong is teaching a course on PP 2.0 at Saybrook. Ivtzan is the program director along with Tim who is a lecturer & program leader of the UEL Master of Applied Positive Psychology, which has a PP 2.0 orientation.\n"}
{"id": "34909429", "url": "https://en.wikipedia.org/wiki?curid=34909429", "title": "Socio-scientific issues", "text": "Socio-scientific issues\n\nSocioscientific Issues (SSI) are controversial social issues which relate to science. They are ill-structured, open-ended problems which have multiple solutions.\n\nSSI are utilized in science education in order to promote scientific literacy, which emphasizes the ability to apply scientific and moral reasoning to real-world situations. Some examples of SSI include issues such as genetic engineering, climate change, animal testing for medical purposes, oil drilling in national parks, and \"fat taxes\" on \"unhealthy\" foods, among many others. Research studies have shown SSI to be effective at increasing students' understanding of science in various contexts, argumentation skills, empathy, and moral reasoning.\n\nSupporters of SSI argue that it can:\nevaluation, interpretation, and self-regulation Science educators often refer to all of these aspects together as,\"functional scientific literacy.\"\n\nScientific literacy has been defined by two competing visions. A Vision I approach to scientific literacy is characterized by content-driven, decontextualized science knowledge. A Vision II approach to scientific literacy is a context-driven, student-centered approach which seeks to prepare students for informed civic engagement. The SSI framework follows a Vision II approach as it is believed to provide an opportunity for contextualized learning of science content as well as an opportunity for moral development.\n\nSSI is conceptually related to Science, Technology, and Society (STS) education. However, while both approaches connect science to societal issues, SSI is distinguished from STS because of its emphasis on the development of character and virtue as well as content knowledge.\n\nResearch suggests that SSI creates cognitive dissonance by compelling students to consider claims that may be at odds with their own beliefs and values. Dissonance of this nature is believed by some to advance moral reasoning by ‘empowering students to consider how science based issues and the decisions made concerning them reflect, in part, the moral principles and qualities of virtue that encompass their own lives, as well as the physical and social world around them.'\n\nSSI education has been empirically investigated and linked to particular outcomes including:\n• Promoting developmental changes in reflective judgment;\n• Moving students to more informed views of the nature of science;\n• Increasing moral sensitivity and empathy;\n• Increasing conceptual understanding of scientific content;• Increase students’ ability to transfer concepts and scaffold ideas;\n• Revealing and reconstructing alternative perceptions of science;\n• Facilitating moral reasoning;\n• Improve argumentation skills;\n• Promote understanding of eco-justice and environmental awareness; and\n• Engage students’ interest in the inquiry of science.\n\nMore recently, SSI research has been focused on cross-cultural comparisons and research has reflected international partnerships. It has been hypothesized by some that more advanced stages of epistemological reasoning allows individuals to apply a kind of socioscientific reasoning (SSR) akin to scientific habits of mind. SSR is a theoretical construct that entails the ability to tap key traits while negotiating SSI. These include skepticism, complexity, multiple perspective and inquiry.\n\nTeachers utilize SSI to foster understanding of science content and consequences involved in everyday scientific issues. For example, in a study of ecology, an elementary class might consider whether pesticides confer more benefit or harm to our ecosystem. This type of analysis would require students to research the interractions between organisms in food webs and food chains, as well as the human impacts of pesticides. Students could make evidence-based decisions and discuss them through various means including whole-class discussions, debates, online discussion boards, etc... Similarly, older grades might consider issues such as whether genetic engineering should be used to treat genetic diseases.\n\nThis type of analysis would require extensive study of genetics and modern genetic engineering techniques, as well as the ethical issues involved in personal freedoms, religious prohibitions on intervention, and so on. Advocates suggest that, through evidence-based discourse, students learn to formulate their own informed decisions and understand those whose views differ from themselves. An essential aspect of the implementation of SSI is that the teacher is not promoting any particular belief; rather, the teacher's role is to promote evidence-based critical thinking and argumentation.\n"}
{"id": "50597374", "url": "https://en.wikipedia.org/wiki?curid=50597374", "title": "Statistical alchemy", "text": "Statistical alchemy\n\nStatistical alchemy was a term originated by John Maynard Keynes to describe econometrics in 1939. The phrase has subsequently been used by Alvan Feinstein to describe meta-analysis. It is generally regarded as a deprecatory term which undermines attempts to present such activities as meeting the rigorous standards of science.\n\nKeynes (1939) wrote a review of Jan Tinbergen's \"Statistical Testing of Business-Cycle Theories\". Although he praised Tinbergen for his objectivity, he however depicted his methodology as \"black magic\" which he regarded as essentially untrustworthy. He was unpersuaded that \"this brand of \"statistical alchemy\" is ripe to become a branch of science\" (emphasis in the original).\n\nOften this metaphor is seen as a way of suggesting that econometricians were following a foolhardy pursuit comparable to the alchemical quest of turning base metal into gold. However G. M. P. Swann points out that Keynes was well aware that such eminent early scientists as Isaac Newton. He rather proposes a more nuanced interpretation of the metaphor as referring to the Alkahest, a universal solvent, which, it was claimed could turn stone into water. He claimed that by restricting econometrics to theory, mathematics and statistics, econometricians had discarded other important applied techniques. Although Ragnar Frisch had made warnings about this, these had been subsequently ignored by other econometricians who had ended up claiming that econometrics constituted a universal solvent.\n\nFeinstein (1995) published \"Meta-analysis: statistical alchemy for the 21st century\" where he claimed that in meta-analysis scientific requirements had been removed or destroyed, eliminating the scientific requirements of reproducibility and precision. This was equivalent to a \"free lunch\", comparable to the alchemical transmutation of base metals to gold. Detourning the adage concerning the combination of apples and oranges, Feinstein suggested that meta-analytic mixtures were so heterogeneous that they might be better described as \"combining rotten fruits\". He argues that meta-analysis violates the Bradford Hill criteria of consistency as inconsistencies are ignored or buried through the process of agglomerating the data.\n\n"}
{"id": "13698761", "url": "https://en.wikipedia.org/wiki?curid=13698761", "title": "Technopolis Innovation Park Delft", "text": "Technopolis Innovation Park Delft\n\nThe Technopolis Innovation Park Delft (or Technopolis) is a science park in Delft, Netherlands. It's a new centre for research and innovation that was started in 2005.\n\nTechnopolis is located next to the campus of Delft University of Technology. The park covers 70 hectares (168 acres) which is available for companies and institutes conducting research and development activities. \n\n\n"}
{"id": "6875598", "url": "https://en.wikipedia.org/wiki?curid=6875598", "title": "The Republican War on Science", "text": "The Republican War on Science\n\nThe Republican War on Science is a 2005 book by Chris C. Mooney, an American journalist who focuses on the politics of science policy. In the book, Mooney discusses the Republican Party leadership's stance on science, and in particular that of the George W. Bush administration, with regard to issues such as global warming, the creation–evolution controversy, bioethics, alternative medicine, pollution, separation of church and state, and the government funding of education, research, and environmental protection. The book argues that the administration regularly distorted and/or suppressed scientific research to further its own political aims.\n\nThe book was reviewed in \"Science\" and \"Nature Medicine\" as well as the popular press. It was featured on the cover of \"The New York Times Book Review\" and selected as an \"Editors' Choice\" by \"The New York Times\" which described it as, \"A frankly polemical survey of scientific findings and procedures in collision with political operations.\"\n\nFilmmaker Morgan Spurlock (of \"Super Size Me\" fame) optioned the rights for the book to create a documentary film, but in 2008 announced that he had released the option.\n\nA review in \"Science\" by Naomi Oreskes states the author recounts the 20-year campaign by \"influential Republicans—initially in Congress and now also in the White House—in concert with determined allies in private industry and fundamentalist Christian organizations\" to systematically deny, disparage and misrepresent scientific information related to public policy. She gave the following list of topics, \"acid rain, global warming, the efficacy of condoms in preventing the spread of sexually transmitted diseases, the health impacts of excess dietary sugar and fat, the alleged link between abortion and breast cancer, the status of endangered species, the efficacy of abstinence-only sex education programs, the therapeutic potential of adult stem cells, and more.\" Oreskes goes on to detail the tactics used in the attempt to mislead both the public and politicians, \"misrepresenting real debates, exaggerating uncertainty, interfering with the activities of expert agencies, trumpeting the views of outlier scientists whose interpretations are rarely to be found in the refereed literature, and attacking the integrity of genuine experts.\" She states that Mooney points out that multiple misinformation campaigns have involved the same individuals and groups. Oreskes concludes, \"Mooney's book makes it clear that when sensible people stand on the sidelines, a great deal of nonsense can be spread.\"\n\nMichael Stebbins wrote in \"Nature Medicine\" that \"This book should serve as a harsh wake-up call to the scientific community and the American public.\" He stated that Mooney \"painstakingly documents the roots of the efforts to undercut the influence of science on national policy and the relentless politicization of US science policy by conservatives working on behalf of the Republican Party.\" He notes that the author clearly documents the \"Bush administrations' attacks on the integrity of science information\" listing examples that include some of those mentioned by Oreskes and \"undercutting the Clean Air Act and the Endangered Species Act, and stacking agencies and advisory committees with unqualified ideologues.\" Stebbins credits the book with providing context by detailing the tactics employed by \"conservative Republicans\" and establishing the roots of these undercutting techniques with examples from the last 40 years. He goes on to state, \"Mooney's documentation of the willful manipulation of science on the part of conservatives to suit an agenda is well supported and nauseating.\" Stebbins addresses two criticisms of the book, the first, that it doesn't explain the science involved, which he explains is not the purpose of the book, and the second, that it doesn't detail the misuse of science by democrats and liberals, which he dismisses as untrue. He finds issue with the last chapter, which proposes solutions, stating, \"His suggestions are sound and well thought out, but seem more of an afterthought than a real goal of the book.\"\n\nDaniel Sarewitz panned the book in a review in \"Issues in Science and Technology\" describing it as a, \"tiresome polemic masquerading as a defense of scientific purity.\"\n\nIn a positive review in \"Scientific American\" Boyce Rensenberger described the book as, \"a well-researched, closely argued and amply referenced indictment of the right wing's assault on science and scientists.\" Lisa Margonelli of \"The New York Times Book Review\" wrote that Mooney, \"juggled extensive research and sharp arguments [...] with precision and a showman’s wink that made his unpromising subject fun.\"\n\nKeay Davidson wrote in the \"Washington Post\" that \"Mooney's political heart is in the right place\" but says \"Mooney is like a judge who interprets a law one way to convict his enemies and another way to acquit his friends.\"\n\nWriting in \"The New York Times\", John Horgan said of the book that the prose was \"often clunky and clichéd\", but explains that Mooney \"addresses a vitally important topic and gets it basically right.\" Horgan defends the book against another reviewer's criticism, saying \"the journalist Keay Davidson faults Mooney for not acknowledging how hard it can be to distinguish good science from bad... But in many of the cases that [Mooney] examines, demarcation is easy, because one side has an a priori commitment to something other than the truth — God or money, to put it bluntly.\"\n\nStuart Derbyshire, a senior lecturer at the University of Birmingham School of Psychology, praises Mooney and notes that he explained how Republicans had manipulated the uncertainty in science \"to ensure that Congress rarely hears any consensus opinion that may damage a Bush policy.\" Derbyshire agrees with Mooney's claim that there is Republican \"flagrant twisting\" of research findings and that it \"violates the integrity of science.\"\n\nMooney was interviewed about the book on \"Science Friday\".\n\n\n"}
{"id": "55269375", "url": "https://en.wikipedia.org/wiki?curid=55269375", "title": "University technology transfer offices", "text": "University technology transfer offices\n\nUniversity technology transfer offices (TTOs), or Technology Licensing Offices (TLOs), are responsible for technology transfer and other aspects of the commercialization of research that takes place in a university. TTOs engage in a variety of commercial activities that are meant to facilitate the process of bringing research developments to market, often acting as a channel between academia and industry. Most major research universities have established TTOs in the past decades in an effort to increase the impact of university research and provide opportunities for financial gain. While TTOs are commonplace, many studies have questioned their financial benefit to the university.\n\nThe history of technology transfer is intimately linked with the history of the science policy of the United States. The foundation for modern American science policy laid way out in Vannevar Bush's letter in response to President Roosevelt's query about whether the US should maintain the high level of research funding it had been pouring into the Office of Scientific Research and Development, which had coordinated large private-public partnership research projects as part of the war effort, including the Manhattan Project. Bush's answer was \"Science - the Endless Frontier\". In that letter, Bush advocated that the US should continue to fund basic research at high levels, arguing that while the US no longer had a geographic frontier, extending the boundaries of science would allow the creation of new technologies, which in turn would spur new industries, create jobs, generate wealth, and maintain US power. As the US worked out its approach to funding science in the 1950s, Congress decided that the federal government should maintain ownership of patents on inventions funded by the federal government.\n\nFederal research funding drove the growth of the research university. Many universities in the early 20th century did not engage in patenting and licensing, since the government owned most inventions, and out of fear of interfering with their missions of supporting the growth of knowledge and objective inquiry. Prior to the postwar period, universities relied mostly on external patent management organizations such as the Research Corporation, while few set up their own research foundations that were independent from but affiliated to the university. Some universities, such as Stanford University and the University of Wisconsin, had active licensing programs of their own. There was a shift in universities' approaches to technology transfer between 1970-1980. During this period, universities began taking commercialization efforts into their own hands and setting up TTOs.\n\nThe Bayh–Dole Act of 1980 led many US universities to set up tech transfer offices. The Act was created to try to spur the stagnant US economy of the 1970s, harking back to Vannevar Bush's vision of the role of federal research funding in the US economy. The Act decentralized ownership of inventions funded with federal grants, allowing universities that received federal grant funding to maintain ownership of such inventions, obligating them to try to patent and license the inventions to US companies, and requiring universities to share license income with inventors.\n\nWhile the broad goal of TTOs is to commercialize university research, they engage in numerous activities that not only bring these developments to market but also encourage and support faculty and students in the entire technology transfer process. Such encouragement may increase the chances of faculty and students creating research developments that can be commercialized. Some of the major functions of TTOs include:\n\nAn important task of many TTOs is to create and maintain industry partnerships that may be crucial for collaboration and bringing technologies to market. Some universities such as MIT and Northwestern have separate offices for industry and corporate relations which typically work in conjunction with the TTO of the institution. In this case, TTOs often exploit the relationships developed by the corporate relations office, focusing more specifically on the technology transfer process itself. TTOs often employ two methods when engaging with industry partners: 1) the \"pull\" method, in which TTOs \"receive\" interest from industry partners in bringing specific technologies at the university to market, and 2) the \"push\" method, in which TTOs actively \"seek\" industry partners for this purpose.\n\nThe Bayh-Dole Act obligated universities to seek patent protection, when appropriate, for inventions to which they elect title; after passage of the Bayh-Dole Act many US universities created intellectual property policies that obligated faculty to assign inventions to the university. Universities typically license the patent to a company that will invest money in developing the invention into a product, which it will then be able to sell at a premium, recouping its investment and making profit before the patent expires.\n\nTTOs at many universities often provide general business and legal counseling to foster entrepreneurship among faculty and students. By providing resources, funding, and connections to university spin-off companies, TTOs attempt to increase the chances of startup success, which may result in financial gain if the university owns the intellectual property of the invention or has an equity stake in the company. Hence, many TTOs establish business incubators and programs for faculty and students in an attempt to enhance the entrepreneurial atmosphere among researchers at the university. Some examples of such incubators and programs include the Blavatnik Biomedical Accelerator as well as the Physical Sciences and Engineering Accelerator at Harvard University, and Fab Lab MSI, affiliated with the University of Chicago. Research has suggested that incubators at TTOs have not had a high incidence of technology transfer, despite this being one of the reasons they were established, and may even negatively impact the success of TTOs and technology transfer at the university.\n\nThe structure and organization of TTOs can affect its overall performance and can vary among universities. Since TTOs deal with both academic research and industry, they consist of a diverse set of individuals, including scientists, lawyers, analysts, licensing experts, and business managers. By having individuals (particularly different scientists, engineers, and analysts) with varying sets of expertise in research, TTOs attempt to more effectively assess, protect, and profit from the research developments taking place in multiple disciplines throughout the university.\n\nTTOs can by classified into three different types: \nAs of 2012 the \"internal\" type was most common in the US.\n\nTTOs attempt to capitalize on the research developments made at the university by employing strategies focused on providing the university with opportunities for financial gain and increased research impact. A common strategy that TTOs engage in is licensing their inventions, either to an industry partner or back to the university inventor if the inventor started a company (i.e. a university spin-off). Through this approach, TTOs can bring university technologies to market without having to engage in production and distribution themselves. TTOs can also take an equity stake in the spin-off company rather than licensing the technology. Some research has suggested that equity in spin-off companies may provide higher returns than licensing, but this strategy seems to be more common with TTOs that are financially independent from the parent university (i.e. external TTO structure). While these strategies vary greatly among TTOs at different universities, a majority of them employ some combination of licensing and equity stakes, with licensing being a more standard practice.\n\nAs many major research universities across the US began to adopt TTOs, institutions outside the US became attracted to the idea of taking control of their commercialization activities as well. Prior to the 2000s, many German-speaking and Scandinavian countries had a policy of \"professor's privilege\", in which faculty retain the right to control the intellectual property of their inventions. In addition, in recent years many OECD and EU nations have created legislation that emulates Bayh-Dole, in an attempt to increase the commercialization activities and impact of their respective research universities. Denmark was among the first to abolish professor's privilege, followed by Germany, Austria, Norway and Finland between 2000-2007. Countries such as France and the UK, which already had policies in place that grant intellectual property rights to universities during this period, began heavily encouraging and enforcing these institutional ownership rights. As of 2011, most European countries grant universities the rights to the intellectual property of inventions developed by faculty researchers, yet a few countries such as Italy and Sweden still employ professor's privilege. Hence, there has been a marked increase in the commercialization activities of universities and creation of TTOs in Europe.\n\nSeveral Asian countries such as Japan, China, and India have also shifted towards a Bayh-Dole type legislation, although some countries such as Malaysia have a shared ownership model. Moreover, there has been a general shift towards increased commercialization and the establishment of TTOs across higher education institutions in Asian countries. \n\nAlthough universities created TTOs with hopes of financial gain, many TTOs have retained losses in their commercialization activities and have not generated significant local economic development. It has been argued that protecting intellectual property and patenting is a costly process, and of all the patents and licenses a university issues, there may be a limited number of inventions that actually yield enough revenue to cover or surpass these costs. Research has shown that larger, more established TTOs are sufficiently profitable, whereas many smaller, more recent TTOs are not, and that an estimated half of TTOs retain losses in their commercialization activities (of those that do not have losses, a majority do no better than to cover their costs). Even the most profitable TTOs only produce revenue that amounts to 1-3% of the total research expenditures at the university. Moreover, less than 1% of licensed technologies actually yield over $1M in revenue. Another criticism of TTOs is its role in the research atmosphere of the university, with many scholars arguing that its presence and purpose of engaging in commercialization activities conflicts with a university's mission of furthering knowledge and objective academic inquiry.\n\nRebecca Eisenberg and Michael Heller have argued that the Bayh-Dole Act spurred university tech transfer offices to become too aggressive in patenting, creating patent thickets and a tragedy of the anticommons especially in the field of biomedical research. As of 2012, evidence for such an anticommons effect in the practice of biomedical science was lacking.\n"}
{"id": "57537760", "url": "https://en.wikipedia.org/wiki?curid=57537760", "title": "William Phelps Ornithological Collection", "text": "William Phelps Ornithological Collection\n\nThe William Phelps Ornithological Collection, also known as the Phelps Ornithological Museum, is a museum of natural sciences dedicated to the study, exhibition and preservation of the birds of Venezuela and the rest of Latin America. The collection is located east of Caracas and in the geographic center of Greater Caracas, in the heart of the Sabana Grande district. The William Phelps ornithological collection is the most important in Latin America and it is also the most important private collection in the world in its research area. \n\nIn this private museum one will find important Phelps family study books, as well as 8000 scientific volumes in the library, more than 83,000 anatomical specimens, more than 80,000 skins, etc. For the year 1990, it was said that the William Phelps Ornithological Collection contained more than 76,300 skins and a small number of anatomical specimens, in the Gran Sabana Building of Sabana Grande. The Phelps library in 1990 already had 6,000 books, 800 journals and 5,500 reprints, mostly from natural sciences.\n\nThe ornithological collection was born in 1938, although it did not have its own headquarters on the Boulevard of Sabana Grande until 1949. At the beginning of 2018, it celebrated its 80th anniversary in Caracas, Venezuela. With the passing of time, the collection has been growing and still has great international scientific relevance. In 2005, an investigation was carried out on \"plumage differences in four subspecies of golden warbler Basileuterus culicivorus in Venezuela\". \n\nThe Phelps Foundation has been recognized worldwide for its scientific research. Since 1937, this foundation has dedicated to the study of the distribution of birds in Venezuela as well as to the dissemination of ornithology in Venezuela. Since 1949, it has expanded globally in its mission to discover, interpret and disseminate information about ornithology through a program of scientific research, education and dissemination in the natural sciences. The Foundation has had an important global trajectory for which it is recognized and is a regional compulsory study resource on tropical birds for experts who want to know more about this area. This museum has historically been connected to the American Museum of Natural History, thanks to the work of Billy Phelps. The ornithological collection has also been expanded thanks to the research carried out with Armando Dugand from Bogotá, Colombia. Most of the funds to carry out these investigations were collected by the Phelps Foundation.\n\nIn March and April 1977, the Phelps Ornithological Collection, with the collaboration of the Venezuelan-Brazilian Border Commission, Demarcador de Limites, carried out a collection of birds at Cerro Urutaní (62 ° 05'W, 3 ° 40'N), which is a low altitude tepui on the Venezuelan-Brazilian border in the Sierra Pacaraima. A total of 511 specimens of birds were collected between 1150 and 1280 meters high s.n.m., representing 78 different species. Gilberto Pérez Chinchilla, Manuel Castro and Dickerman prepared the copies of the collection. A full report on these birds was published in the international press and was published in the Bulletin of the \"American Museum of Natural History\", New York. It was necessary to work in conjunction with the Boundary Directorate of Venezuela.\n\n\n"}
{"id": "21029957", "url": "https://en.wikipedia.org/wiki?curid=21029957", "title": "World Intellectual Property Indicators", "text": "World Intellectual Property Indicators\n\nWIPO's World Intellectual Property Indicators is an annual report providing a wide range of indicators covering the areas of intellectual property. It draws on data from national and regional IP offices, WIPO, the World Bank and UNESCO.\n\nPatent applications for the top 10 offices, 2014\nPatent grants for the top 10 offices, 2014\n\nPatent applications per GDP for the top 10 origins, 2012\nPatent applications per million population for the top 10 origins, 2012\n\nApplication design counts for the top 10 offices, 2012\nApplication design counts for the top 10 origins, 2012\n\nApplication design counts per million population for the top 10 origins, 2012\nIndustrial design registrations in force for the top 10 offices, 2012\n\n\n"}
