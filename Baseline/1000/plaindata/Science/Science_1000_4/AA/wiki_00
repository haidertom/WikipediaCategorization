{"id": "93070", "url": "https://en.wikipedia.org/wiki?curid=93070", "title": "A New Kind of Science", "text": "A New Kind of Science\n\nA New Kind of Science is a best-selling, controversial book by Stephen Wolfram, published by his own company in 2002. It contains an empirical and systematic study of computational systems such as cellular automata. Wolfram calls these systems \"simple programs\" and argues that the scientific philosophy and methods appropriate for the study of simple programs are relevant to other fields of science.\n\nThe thesis of \"A New Kind of Science\" (\"NKS\") is twofold: that the nature of computation must be explored experimentally, and that the results of these experiments have great relevance to understanding the physical world. Since its nascent beginnings in the 1930s, computation has been primarily approached from two traditions: engineering, which seeks to build practical systems using computations; and mathematics, which seeks to prove theorems about computation. However, as recently as the 1970s, computing has been described as being at the crossroads of mathematical, engineering, and empirical traditions.\n\nWolfram introduces a third tradition that seeks to empirically investigate computation for its own sake: He argues that an entirely new method is needed to do so because traditional mathematics fails to meaningfully describe complex systems, and that there is an upper limit to complexity in all systems.\n\nThe basic subject of Wolfram's \"new kind of science\" is the study of simple abstract rules—essentially, elementary computer programs. In almost any class of a computational system, one very quickly finds instances of great complexity among its simplest cases (after a time series of multiple iterative loops, applying the same simple set of rules on itself, similar to a self-reinforcing cycle using a set of rules). This seems to be true regardless of the components of the system and the details of its setup. Systems explored in the book include, amongst others, cellular automata in one, two, and three dimensions; mobile automata; Turing machines in 1 and 2 dimensions; several varieties of substitution and network systems; primitive recursive functions; nested recursive functions; combinators; tag systems; register machines; reversal-addition. For a program to qualify as simple, there are several requirements:\n\nGenerally, simple programs tend to have a very simple abstract framework. Simple cellular automata, Turing machines, and combinators are examples of such frameworks, while more complex cellular automata do not necessarily qualify as simple programs. It is also possible to invent new frameworks, particularly to capture the operation of natural systems. The remarkable feature of simple programs is that a significant percentage of them are capable of producing great complexity. Simply enumerating all possible variations of almost any class of programs quickly leads one to examples that do unexpected and interesting things. This leads to the question: if the program is so simple, where does the complexity come from? In a sense, there is not enough room in the program's definition to directly encode all the things the program can do. Therefore, simple programs can be seen as a minimal example of emergence. A logical deduction from this phenomenon is that if the details of the program's rules have little direct relationship to its behavior, then it is very difficult to directly engineer a simple program to perform a specific behavior. An alternative approach is to try to engineer a simple overall computational framework, and then do a brute-force search through all of the possible components for the best match.\n\nSimple programs are capable of a remarkable range of behavior. Some have been proven to be universal computers. Others exhibit properties familiar from traditional science, such as thermodynamic behavior, continuum behavior, conserved quantities, percolation, sensitive dependence on initial conditions, and others. They have been used as models of traffic, material fracture, crystal growth, biological growth, and various sociological, geological, and ecological phenomena. Another feature of simple programs is that, according to the book, making them more complicated seems to have little effect on their overall complexity. \"A New Kind of Science\" argues that this is evidence that simple programs are enough to capture the essence of almost any complex system.\n\nIn order to study simple rules and their often complex behaviour, Wolfram argues that it is necessary to systematically explore all of these computational systems and document what they do. He further argues that this study should become a new branch of science, like physics or chemistry. The basic goal of this field is to understand and characterize the computational universe using experimental methods.\n\nThe proposed new branch of scientific exploration admits many different forms of scientific production. For instance, qualitative classifications are often the results of initial forays into the computational jungle. On the other hand, explicit proofs that certain systems compute this or that function are also admissible. There are also some forms of production that are in some ways unique to this field of study. For example, the discovery of computational mechanisms that emerge in different systems but in bizarrely different forms.\n\nAnother kind of production involves the creation of programs for the analysis of computational systems. In the \"NKS\" framework, these themselves should be simple programs, and subject to the same goals and methodology. An extension of this idea is that the human mind is itself a computational system, and hence providing it with raw data in as effective a way as possible is crucial to research. Wolfram believes that programs and their analysis should be visualized as directly as possible, and exhaustively examined by the thousands or more. Since this new field concerns abstract rules, it can in principle address issues relevant to other fields of science. However, in general Wolfram's idea is that novel ideas and mechanisms can be discovered in the computational universe, where they can be represented in their simplest forms, and then other fields can choose among these discoveries for those they find relevant.\n\nWolfram has since expressed \"A central lesson of \"A New Kind of Science\" is that there’s a lot of incredible richness out there in the computational universe. And one reason that’s important is that it means that there’s a lot of incredible stuff out there for us to 'mine' and harness for our purposes.\"\n\nWhile Wolfram advocates simple programs as a scientific discipline, he also argues that its methodology will revolutionize other fields of science. The basis of his argument is that the study of simple programs is the minimal possible form of science, grounded equally in both abstraction and empirical experimentation. Every aspect of the methodology advocated in \"NKS\" is optimized to make experimentation as direct, easy, and meaningful as possible while maximizing the chances that the experiment will do something unexpected. Just as this methodology allows computational mechanisms to be studied in their simplest forms, Wolfram argues that the process of doing so engages with the mathematical basis of the physical world, and therefore has much to offer the sciences.\n\nWolfram argues that the computational realities of the universe make science hard for fundamental reasons. But he also argues that by understanding the importance of these realities, we can learn to use them in our favor. For instance, instead of reverse engineering our theories from observation, we can enumerate systems and then try to match them to the behaviors we observe. A major theme of \"NKS\" is investigating the structure of the possibility space. Wolfram argues that science is far too ad hoc, in part because the models used are too complicated and unnecessarily organized around the limited primitives of traditional mathematics. Wolfram advocates using models whose variations are enumerable and whose consequences are straightforward to compute and analyze.\n\nWolfram argues that one of his achievements is in providing a coherent system of ideas that justifies computation as an organizing principle of science. For instance, he argues that the concept of \"computational irreducibility\" (that some complex computations are not amenable to short-cuts and cannot be \"reduced\"), is ultimately the reason why computational models of nature must be considered in addition to traditional mathematical models. Likewise, his idea of intrinsic randomness generation—that natural systems can generate their own randomness, rather than using chaos theory or stochastic perturbations—implies that computational models do not need to include explicit randomness.\n\nBased on his experimental results, Wolfram developed the principle of computational equivalence (PCE): the principle states that systems found in the natural world can perform computations up to a maximal (\"universal\") level of computational power. Most systems can attain this level. Systems, in principle, compute the same things as a computer. Computation is therefore simply a question of translating input and outputs from one system to another. Consequently, most systems are computationally equivalent. Proposed examples of such systems are the workings of the human brain and the evolution of weather systems.\n\nThe principle can be restated as follows: almost all processes that are not obviously simple are of equivalent sophistication. From this principle, Wolfram draws an array of concrete deductions which he argues reinforce his theory. Possibly the most important among these is an explanation as to why we experience randomness and complexity: often, the systems we analyze are just as sophisticated as we are. Thus, complexity is not a special quality of systems, like for instance the concept of \"heat,\" but simply a label for all systems whose computations are sophisticated. Wolfram argues that understanding this makes possible the \"normal science\" of the \"NKS\" paradigm.\n\nAt the deepest level, Wolfram argues that—like many of the most important scientific ideas—the principle of computational equivalence allows science to be more general by pointing out new ways in which humans are not \"special\"; that is, it has been claimed that the complexity of human intelligence makes us special, but the Principle asserts otherwise. In a sense, many of Wolfram's ideas are based on understanding the scientific process—including the human mind—as operating within the same universe it studies, rather than being outside it.\n\nThere are a number of specific results and ideas in the \"NKS\" book, and they can be organized into several themes. One common theme of examples and applications is demonstrating how little complexity it takes to achieve interesting behavior, and how the proper methodology can discover this behavior.\n\nFirst, there are several cases where the \"NKS\" book introduces what was, during the book's composition, the simplest known system in some class that has a particular characteristic. Some examples include the first primitive recursive function that results in complexity, the smallest universal Turing Machine, and the shortest axiom for propositional calculus. In a similar vein, Wolfram also demonstrates many simple programs that exhibit phenomena like phase transitions, conserved quantities, continuum behavior, and thermodynamics that are familiar from traditional science. Simple computational models of natural systems like shell growth, fluid turbulence, and phyllotaxis are a final category of applications that fall in this theme.\n\nAnother common theme is taking facts about the computational universe as a whole and using them to reason about fields in a holistic way. For instance, Wolfram discusses how facts about the computational universe inform evolutionary theory, SETI, free will, computational complexity theory, and philosophical fields like ontology, epistemology, and even postmodernism.\n\nWolfram suggests that the theory of computational irreducibility may provide a resolution to the existence of free will in a nominally deterministic universe. He posits that the computational process in the brain of the being with free will is actually complex enough so that it cannot be captured in a simpler computation, due to the principle of computational irreducibility. Thus, while the process is indeed deterministic, there is no better way to determine the being's will than, in essence, to run the experiment and let the being exercise it.\n\nThe book also contains a vast number of individual results—both experimental and analytic—about what a particular automaton computes, or what its characteristics are, using some methods of analysis.\n\nThe book contains a new technical result in describing the Turing completeness of the Rule 110 cellular automaton. Very small Turing machines can simulate Rule 110, which Wolfram demonstrates using a 2-state 5-symbol universal Turing machine. Wolfram conjectures that a particular 2-state 3-symbol Turing machine is universal. In 2007, as part of commemorating the book's fifth anniversary, Wolfram's company offered a $25,000 prize for proof that this Turing machine is universal. Alex Smith, a computer science student from Birmingham, UK, won the prize later that year by proving Wolfram's conjecture.\n\nEvery year, Wolfram and his group of instructors organize a summer school. From 2003 to 2006, these classes were held at Brown University. In 2007, the summer school began being hosted by the University of Vermont at Burlington, with the exception of 2009 which was held at the Istituto di Scienza e Tecnologie dell'Informazione of the CNR in Pisa, Italy. In 2012, the program was held at Curry College in Milton, Massachusetts. Since 2013, the Wolfram Summer School has been held annually at Bentley University in Waltham, Massachusetts. After 14 consecutive summer schools, more than 550 people have participated, some of whom continued developing their 3-week research projects as their Master's or Ph.D theses. Some of the research done in the summer school has resulted in publications.\n\nPeriodicals gave \"A New Kind of Science\" coverage, including articles in \"The New York Times\", \"Newsweek\", \"Wired\", and \"The Economist\". Some scientists criticized the book as abrasive and arrogant, and perceived a fatal flaw—that simple systems such as cellular automata are not complex enough to describe the degree of complexity present in evolved systems, and observed that Wolfram ignored the research categorizing the complexity of systems. Although critics accept Wolfram's result showing universal computation, they view it as minor and dispute Wolfram's claim of a paradigm shift. Others found that the work contained valuable insights and refreshing ideas. Wolfram addressed his critics in a series of blog posts.\n\nIn an article published on April 3, 2018, \"A New Kind of Science\" was listed among the 190 books recommended by Bill Gates.\n\nA tenet of \"NKS\" is that the simpler the system, the more likely a version of it will recur in a wide variety of more complicated contexts. Therefore, \"NKS\" argues that systematically exploring the space of simple programs will lead to a base of reusable knowledge. However, many scientists believe that of all possible parameters, only some actually occur in the universe. For instance, of all possible permutations of the symbols making up an equation, most will be essentially meaningless. \"NKS\" has also been criticized for asserting that the behavior of simple systems is somehow representative of all systems.\n\nA common criticism of \"NKS\" is that it does not follow established scientific methodology. For instance, \"NKS\" does not establish rigorous mathematical definitions, nor does it attempt to prove theorems; and most formulas and equations are written in Mathematica rather than standard notation. Along these lines, \"NKS\" has also been criticized for being heavily visual, with much information conveyed by pictures that do not have formal meaning. It has also been criticized for not using modern research in the field of complexity, particularly the works that have studied complexity from a rigorous mathematical perspective. And it has been criticized for misrepresenting chaos theory: \"Throughout the book, he equates chaos theory with the phenomenon of sensitive dependence on initial conditions (SDIC).\"\n\n\"NKS\" has been criticized for not providing specific results that would be immediately applicable to ongoing scientific research. There has also been criticism, implicit and explicit, that the study of simple programs has little connection to the physical universe, and hence is of limited value. Steven Weinberg has pointed out that no real world system has been explained using Wolfram's methods in a satisfactory fashion.\n\nThe Principle of computational equivalence has been criticized for being vague, unmathematical, and for not making directly verifiable predictions. It has also been criticized for being contrary to the spirit of research in mathematical logic and computational complexity theory, which seek to make fine-grained distinctions between levels of computational sophistication, and for wrongly conflating different kinds of universality property. Moreover, critics such as Ray Kurzweil have argued that it ignores the distinction between hardware and software; while two computers may be equivalent in power, it does not follow that any two programs they might run are also equivalent. Others suggest it is little more than a rechristening of the Church–Turing thesis.\n\nWolfram's speculations of a direction towards a fundamental theory of physics have been criticized as vague and obsolete. Scott Aaronson, Professor of Computer Science at University of Texas Austin, also claims that Wolfram's methods cannot be compatible with both special relativity and Bell's theorem violations, and hence cannot explain the observed results of Bell test experiments. However, Aaronson's arguments are either right and apply to the entire scientific field of Quantum gravity that seeks to find theories unifying relativity and quantum mechanics or they are fundamentally flawed (e.g. under a non-local hidden variable theory of superdeterminism acknowledged by Bell himself), and even explored by e.g. physics Nobel laureate Gerard 't Hooft, see also replies to criticism of digital physics.\n\nEdward Fredkin and Konrad Zuse pioneered the idea of a computable universe, the former by writing a line in his book on how the world might be like a cellular automaton, and later further developed by Fredkin using a toy model called Salt. It has been claimed that \"NKS\" tries to take these ideas as its own but Wolfram's model of the universe is a rewriting network and not a cellular automaton as Wolfram himself has suggested a cellular automaton cannot account for relativistic features such as no absolute time frame. Jürgen Schmidhuber has also charged that his work on Turing machine-computable physics was stolen without attribution, namely his idea on enumerating possible Turing-computable universes.\n\nIn a 2002 review of \"NKS\", the Nobel laureate and elementary particle physicist Steven Weinberg wrote, \"Wolfram himself is a lapsed elementary particle physicist, and I suppose he can't resist trying to apply his experience with digital computer programs to the laws of nature. This has led him to the view (also considered in a 1981 paper by Richard Feynman) that nature is discrete rather than continuous. He suggests that space consists of a set of isolated points, like cells in a cellular automaton, and that even time flows in discrete steps. Following an idea of Edward Fredkin, he concludes that the universe itself would then be an automaton, like a giant computer. It's possible, but I can't see any motivation for these speculations, except that this is the sort of system that Wolfram and others have become used to in their work on computers. So might a carpenter, looking at the moon, suppose that it is made of wood.\"\n\nNobel laureate Gerard 't Hooft has more recently also suggested a cellular automaton-based unifying theory of quantum gravity as an interpretation of superstring theory where the evolution equations are classical, \"[b]oth the bosonic string theory and superstring theory can be reformulated in terms of a special basis of states, defined on a space-time lattice with lattice length formula_1\"\n\nWolfram's claim that natural selection is not the fundamental cause of complexity in biology has led non-scientist journalist Chris Lavers to state that Wolfram does not understand the theory of evolution.\n\n\"NKS\" has been heavily criticized as not being original or important enough to justify its title and claims.\n\nThe authoritative manner in which \"NKS\" presents a vast number of examples and arguments has been criticized as leading the reader to believe that each of these ideas was original to Wolfram; in particular, one of the most substantial new technical results presented in the book, that the rule 110 cellular automaton is Turing complete, was not proven by Wolfram, but by his research assistant, Matthew Cook. However, the notes section at the end of his book acknowledges many of the discoveries made by these other scientists citing their names together with historical facts, although not in the form of a traditional bibliography section. Additionally, the idea that very simple rules often generate great complexity is already an established idea in science, particularly in chaos theory and complex systems.\n\n\n"}
{"id": "10181793", "url": "https://en.wikipedia.org/wiki?curid=10181793", "title": "Azeotrope tables", "text": "Azeotrope tables\n\nThis page contains tables of azeotrope data for various binary and ternary mixtures of solvents. The data include the composition of a mixture by weight (in binary azeotropes, when only one fraction is given, it is the fraction of the second component), the boiling point (b.p.) of a component, the boiling point of a mixture, and the specific gravity of the mixture. Boiling points are reported at a pressure of 760 mm Hg unless otherwise stated. Where the mixture separates into layers, values are shown for upper (U) and lower (L) layers.\n\nThe data were obtained from Lange's 10th edition and CRC 44th edition unless otherwise noted (see color code table).\n\nA list of 15825 binary and ternary mixtures was collated and published by the ACS. An azeotrope databank is also available online through Edinburgh University\n\nTables of various ternary azeotropes (that is azeotropes consisting of three components). Fraction percentages are given by weight.\n"}
{"id": "11783757", "url": "https://en.wikipedia.org/wiki?curid=11783757", "title": "Bateson's cube", "text": "Bateson's cube\n\nBateson's cube is a model of the cost–benefit analysis for animal research developed by Professor Patrick Bateson, president of the Zoological Society of London. \n\nBateson's cube evaluates proposed research through three criteria:\n\n\nBateson suggested that research that does not meet these requirements should not be approved or performed, in accordance with the Animals (Scientific Procedures) Act 1986. It is not intended as a formal model for optimal trade-offs, but rather a tool for making judicial decisions, since the three axes are not in a common currency. The third criterion also does not necessarily have to be medical benefit, but could be a wider form of utility. \n"}
{"id": "22844503", "url": "https://en.wikipedia.org/wiki?curid=22844503", "title": "Blue Obelisk", "text": "Blue Obelisk\n\nBlue Obelisk is an informal group of chemists who promote open data, open source, and open standards; it was initiated by Peter Murray-Rust and others in 2005. Multiple open source cheminformatics projects associate themselves with the Blue Obelisk, among which, in alphabetical order, Avogadro, Bioclipse, cclib, Chemistry Development Kit, GaussSum, JChemPaint, JOELib, Kalzium, Openbabel, OpenSMILES, and UsefulChem.\n\nThe project has handed out personal awards for achievements in promoting Open Data, Open Source and Open Standards. Among those who received a Blue Obelisk Award are:\n\n\n"}
{"id": "57896735", "url": "https://en.wikipedia.org/wiki?curid=57896735", "title": "CEASE therapy", "text": "CEASE therapy\n\nCEASE (Complete Elimination of Autistic Spectrum Expression) therapy is used by naturopaths (particularly homeopaths) who claim, without evidence, that it can treat or even cure people with autism. It involves a mixture of supplements, high-dose vitamin C, 'orthomolecular support', dietary restrictions and homeopathy and was developed by the late Tinus Smits - a Dutch doctor. Smits claimed to have used it to treat over 300 children with autism. The therapy became more notable in 2017/2018 because of regulatory action taken by professional bodies in The Netherlands, UK and Canada following a series of complaints about unfounded claims. \n\nSmits in the book \"Autism Beyond Despair - CEASE Therapy\" stated that autistic children should never be vaccinated. \n\nIn October 2017 the Dutch Advertising Code Foundation (Stichting Reclame Code) found that the official website for CEASE therapy was in breach of advertising regulations.\n\nIn the United Kingdom, the Professional Standards Authority (PSA) placed some requirements on the Society of Homeopaths (SoH) when they reaccredited their members' register under their Accredited Register scheme, due to concerns about the way in which members marketed CEASE therapy. The PSA asked the SoH to confirm \"what action it will take to ensure children are safe as a condition of its re-accreditation\". In June 2018 the Society of Homeopaths published a position statement advising their members not to imply any cure of autism when marketing CEASE therapy. It has been estimated that more than 120 homeopaths are offering CEASE in the UK though not all are SoH members. In July 2015 the UK's Advertising Standards Authority (ASA) found Teddington Homeopathy's marketing of CEASE therapy in breach of the Advertising Standards Code. The following month the ASA added the company to its list of non-compliant online advertisers for \"making unproven efficacy claims for CEASE therapy\". In July 2018 the ASA upheld an adjudication against Bubbling Life's website, determining that the claims relating to CEASE, vaccination, autism and ASD could discourage customers from seeking appropriate advice or treatment. \n\nIn British Columbia, Canada, the Board of the College of Naturopathic Physicians investigated three CEASE practitioners following complaints from the public and subsequently \"determined that naturopathic doctors in British Columbia must not advertise or offer CEASE therapy\". As well as this prohibition the College's updated position statements also clarify that naturopathic doctors in BC must not offer anti-vaccination materials or advice (including on social media) and must not imply that vaccination causes autism.\n\n"}
{"id": "8793093", "url": "https://en.wikipedia.org/wiki?curid=8793093", "title": "Challenger Center for Space Science Education", "text": "Challenger Center for Space Science Education\n\nChallenger Center for Space Science Education is a United States 501(c)(3) non-profit organization headquartered in Washington, DC. It was founded in 1986 by the families of the astronauts who died in the Space Shuttle \"Challenger\" disaster on January 28, 1986.\n\nThe organization offers dynamic, hands-on exploration and discovery opportunities to students around the world. These programs equip students with the knowledge, confidence, and skills that will help better our national social and economic well-being.\n\nChallenger Learning Centers give students the chance to become astronauts and engineers and solve real-world problems as they share the thrill of discovery on missions through the Solar System. Using space simulation and role-playing strategies, students bring their classroom studies to life and cultivate the skills needed for future success.\n\nChallenger Learning Center at the Ontario Science Center (Toronto, Canada)\nChallenger Learning Center at SongAm Space Center (Gyeonggi-do, South Korea)\nChallenger Learning Center at the National Space Centre (Leicester, United Kingdom)\nNotable members of the advisor council have included:\n\nNotable members of the Board of Directors include:\n\n"}
{"id": "5636", "url": "https://en.wikipedia.org/wiki?curid=5636", "title": "Chemist", "text": "Chemist\n\nA chemist (from Greek chēm (ía) alchemy; replacing chymist from Medieval Latin alchimista) is a scientist trained in the study of chemistry. Chemists study the composition of matter and its properties. Chemists carefully describe the properties they study in terms of quantities, with detail on the level of molecules and their component atoms. Chemists carefully measure substance proportions, reaction rates, and other chemical properties. The word 'chemist' is also used to address Pharmacists in Commonwealth English.\n\nChemists use this knowledge to learn the composition and properties of unfamiliar substances, as well as to reproduce and synthesize large quantities of useful naturally occurring substances and create new artificial substances and useful processes. Chemists may specialize in any number of subdisciplines of chemistry. Materials scientists and metallurgists share much of the same education and skills with chemists. The work of chemists is often related to the work of chemical engineers, who are primarily concerned with the proper design, construction and evaluation of the most cost-effective large-scale chemical plants and work closely with industrial chemists on the development of new processes and methods for the commercial-scale manufacture of chemicals and related products.\n\nThe roots of chemistry can be traced to the phenomenon of burning. Fire was a mystical force that transformed one substance into another and thus was of primary interest to mankind. It was fire that led to the discovery of iron and glasses. After gold was discovered and became a precious metal, many people were interested to find a method that could convert other substances into gold. This led to the protoscience called alchemy. The word \"chemist\" is derived from the New Latin noun \"chimista\", an abbreviation of \"alchimista\" (alchemist). Alchemists discovered many chemical processes that led to the development of modern chemistry. Chemistry as we know it today, was invented by Antoine Lavoisier with his law of conservation of mass in 1783. The discoveries of the chemical elements has a long history culminating in the creation of the periodic table by Dmitri Mendeleev. The Nobel Prize in Chemistry created in 1901 gives an excellent overview of chemical discovery since the start of the 20th century.\n\nJobs for chemists usually require at least a bachelor's degree, but many positions, especially those in research, require a Master of Science or a Doctor of Philosophy (PhD.). Most undergraduate programs emphasize mathematics and physics as well as chemistry, partly because chemistry is also known as \"the central science\", thus chemists ought to have a well-rounded knowledge about science. At the Master's level and higher, students tend to specialize in a particular field. Fields of specialization include biochemistry, nuclear chemistry, organic chemistry, inorganic chemistry, polymer chemistry, analytical chemistry, physical chemistry, theoretical chemistry, quantum chemistry, environmental chemistry, and thermochemistry. Postdoctoral experience may be required for certain positions.\n\nWorkers whose work involves chemistry, but not at a complexity requiring an education with a chemistry degree, are commonly referred to as \"chemical technicians\". Such technicians commonly do such work as simpler, routine analyses for quality control or in clinical laboratories, having an associate degree. A chemical technologist has more education or experience than a chemical technician but less than a chemist, often having a bachelor's degree in a different field of science with also an associate degree in chemistry (or many credits related to chemistry) or having the same education as a chemical technician but more experience. There are also degrees specific to become a chemical technologist, which are somewhat distinct from those required when a student is interested in becoming a professional chemist. A Chemical technologist is more involved in the management and operation of the equipment and instrumentation necessary to perform chemical analyzes than a chemical technician. They are part of the team of a chemical laboratory in which the quality of the raw material, intermediate products and finished products is analyzed. They also perform functions in the areas of environmental quality control and the operational phase of a chemical plant.\n\nIn addition to all the training usually given to chemical technologists in their respective degree (or one given via an associate degree), a chemist is also trained to understand more details related to chemical phenomena so that the chemist can be capable of more planning on the steps to achieve a distinct goal via a chemistry-related endeavor. The higher the competency level achieved in the field of chemistry (as assessed via a combination of education, experience and personal achievements), the higher the responsibility given to that chemist and the more complicated the task might be. Chemistry, as a field, have so many applications that different tasks and objectives can be given to workers or scientists with these different levels of education or experience. The specific title of each job varies from position to position, depending on factors such as the kind of industry, the routine level of the task, the current needs of a particular enterprise, the size of the enterprise or hiring firm, the philosophy and management principles of the hiring firm, the visibility of the competency and individual achievements of the one seeking employment, economic factors such as recession or economic depression, among other factors, so this makes it difficult to categorize the exact roles of these chemistry-related workers as standard for that given level of education. Because of these factors affecting exact job titles with distinct responsibilities, some chemists might begin doing technician tasks while other chemists might begin doing more complicated tasks than those of a technician, such as tasks that also involve formal applied research, management, or supervision included within the responsibilities of that same job title. The level of supervision given to that chemist also varies in a similar manner, with factors similar to those that affect the tasks demanded for a particular chemist.\n\nIt is important that those interested in a Chemistry degree understand the variety of roles available to them (on average), which vary depending on education and job experience. Those Chemists who hold a bachelor's degree are most commonly involved in positions related to either research assistance (working under the guidance of senior chemists in a research-oriented activity), or, alternatively, they may work on distinct (chemistry-related) aspects of a business, organization or enterprise including aspects that involve quality control, quality assurance, manufacturing, production, formulation, inspection, method validation, visitation for troubleshooting of chemistry-related instruments, regulatory affairs, \"on-demand\" technical services, chemical analysis for non-research purposes (e.g., as a legal request, for testing purposes, or for government or non-profit agencies); chemists may also work in environmental evaluation and assessment. Other jobs or roles may include sales and marketing of chemical products and chemistry-related instruments or technical writing. The more experience obtained, the more independence and leadership or management roles these chemists may perform in those organizations. Some chemists with relatively higher experience might change jobs or job position to become a manager of a chemistry-related enterprise, a supervisor, an entrepreneur or a chemistry consultant. Other chemists choose to combine their education and experience as a chemist with a distinct credential to provide different services (e.g., forensic chemists, chemistry-related software development, patent law specialists, environmental law firm staff, scientific news reporting staff, engineering design staff, etc.).\n\nIn comparison, chemists who have obtained a Master of Science (M.S.) in chemistry or in a very related discipline may find chemist roles that allow them to enjoy more independence, leadership and responsibility earlier in their careers with less years of experience than those with a bachelor's degree as highest degree. Sometimes, M.S. chemists receive more complex tasks duties in comparison with the roles and positions found by chemists with a bachelor's degree as their highest academic degree and with the same or close-to-same years of job experience. There are positions that are open only to those that at least have a degree related to chemistry at the master's level. Although good chemists without a Ph. D. degree but with relatively many years of experience may be allowed some applied research positions, the general rule is that Ph. D. chemists are preferred for research positions and are typically the preferred choice for the highest administrative positions on big enterprises involved in chemistry-related duties. Some positions, especially research oriented, will only allow those chemists who are Ph. D. holders. Jobs that involve intensive research and actively seek to lead the discovery of completely new chemical compounds under specifically assigned monetary funds and resources or jobs that seek to develop new scientific theories require a Ph. D. more often than not. Chemists with a Ph. D. as the highest academic degree are found typically on the research-and-development department of an enterprise and can also hold university positions as professors. Professors for research universities or for big universities usually have a Ph. D., and some research-oriented institutions might require post-doctoral training. Some smaller colleges (including some smaller four-year colleges or smaller non-research universities for undergraduates) as well as community colleges usually hire chemists with a M.S. as professors too (and rarely, some big universities who need part-time or temporary instructors, or temporary staff), but when the positions are scarce and the applicants are many, they might prefer Ph. D. holders instead.\n\nThe three major employers of chemists are academic institutions, industry, especially the chemical industry and the pharmaceutical industry, and government laboratories.\n\nChemistry typically is divided into several major sub-disciplines. There are also several main cross-disciplinary and more specialized fields of chemistry. There is a great deal of overlap between different branches of chemistry, as well as with other scientific fields such as biology, medicine, physics, radiology, and several engineering disciplines.\n\n\n\nAll the above major areas of chemistry employ chemists. Other fields where chemical degrees are useful include astrochemistry (and cosmochemistry), atmospheric chemistry, chemical engineering, chemo-informatics, electrochemistry, environmental science, forensic science, geochemistry, green chemistry, history of chemistry, materials science, medical science, molecular biology, molecular genetics, nanotechnology, nuclear chemistry, oenology, organometallic chemistry, petrochemistry, pharmacology, photochemistry, phytochemistry, polymer chemistry, supramolecular chemistry and surface chemistry.\n\nChemists may belong to professional societies specifically for professionals and researchers within the field of Chemistry, such as the Royal Society of Chemistry in the United Kingdom, or the American Chemical Society (ACS) in the United States.\n\nThe highest honor awarded to chemists is the Nobel Prize in Chemistry, awarded since 1901, by the Royal Swedish Academy of Sciences.\n\n\n"}
{"id": "49245321", "url": "https://en.wikipedia.org/wiki?curid=49245321", "title": "Chinese Chemical Society (Beijing)", "text": "Chinese Chemical Society (Beijing)\n\nThe Chinese Chemical Society (CCS; ) is a professional society of chemists headquartered in Beijing. It is part of the China Association for Science and Technology. Current membership is at around 55,000.\n\nThe CCS was founded in Nanjing on August 4, 1932. It merged with the Chinese Chemical Engineering Society in 1959. The organizations were separated again in 1963. CSS has been a member of the International Union of Pure and Applied Chemistry (IUPAC) since 1980 and of the Federation of Asian Chemical Societies (FACS) since 1984.\n\n\nThe CCS publishes many academic journals, including:\n\n\n"}
{"id": "57624819", "url": "https://en.wikipedia.org/wiki?curid=57624819", "title": "Correactology", "text": "Correactology\n\nCorreactology is a system of alternative medicine based on the unsubstantiated claim that \"regulating the density of cells\" in the body improves a wide range of ailments. An offshoot of chiropractics, the treatment was developed in Sudbury (Ontario) in 2002 by brothers Michael and Allan Lapointe. It has been branded a pseudoscience by critics and the sole training program in the subject has been taken to court by former students.\n\nDuring correactology treatments, the practitioners hold their hands over patients, snap their fingers and apply a light touch to the skin of the patient. Allan Lapointe claims they are first looking for \"areas of resistance in the eight segments of cells, indicating areas of pathological or anatomical dysfunction.\" The practitioner then allegedly uses his hands to change the behaviour of proteins present in human cells, to improve wellness and reduce pain: \"We cause a cascading effect to change the behaviour of the proteins. When the proteins change, our behaviour changes because of course proteins control all.\"\n\nMedical professionals have been questioning the practice as it becomes more widespread. Deputy Editor of the \"Canadian Medical Association Journal\" Matthew Stanbrook denounced the claims made by the practitioners: \"That is pseudo-science that uses a scientific word that doesn't mean what density means. That makes absolutely no sense, to talk about the density of cells not being optimal. It makes even less sense to put forward the idea that through the manipulation involving touching, one could set the density of cells to an optimum level.\" McGill University's Office for Science and Society also identifies correactology as a pseudoscience.\n\nWhile they have no formal medical training, the Lapointes say they are working on research showing their techniques are more effective than placebos.\n\nPractitioners are present in a dozen clinics throughout Ontario, as well as one in Gatineau (Quebec). Correactology is a term trademarked by a corporation owned by the Lapointe brothers and run by their parents. The treatment is not recognized by either the Ontario College of Chiropractors or the Ontario College of Physicians. The Quebec College of Physicians is investigating the techniques and claims related to correactology.\n\nIn 2016, the Collège Boréal signed a four-year agreement by which Sudbury's Correactology Health Care Center would offer a training programme for the College's students, at a cost of $50,000 per student. Boréal cancelled the agreement after two years, as three former students took the correactology center to court on charges of false statements, conspiracy, impediment to trade and breach of contract.\n\nThe court proceedings showed the students had to commit to giving 30 percent of their future earnings to the correactology center. An attempt to have the court case dismissed by invoking the confidentiality clause of the training contract was rebuffed by the court in July, 2018. The court proceedings continue.\n"}
{"id": "3961885", "url": "https://en.wikipedia.org/wiki?curid=3961885", "title": "Daffy's Elixir", "text": "Daffy's Elixir\n\nDaffy's Elixir (also sometimes known as Daffey's Elixir or Daffye's Elixir) is a name that has been used by several patent medicines over the years. It was originally designed for diseases of the stomach, but was later marketed as a universal cure. It remained a popular remedy in Britain and later the United States of America throughout the eighteenth and nineteenth centuries.\n\nDaffy's Elixir was one of the most popular and frequently advertised patent medicines in Britain during the 18th century. It is reputed to have been invented by clergyman Thomas Daffy rector of Redmile, Leicestershire, in 1647. He named it elixir salutis (lit. \"elixir of health\") and promoted as a generic cure-all.\n\nAn early recipe for \"True Daffy\" from 1700 lists the following ingredients: aniseed, brandy, cochineal, elecampane, fennel seed, jalap, manna, parsley seed, raisin, rhubarb, saffron, senna and spanish liquorice. Chemical analysis has shown this to be a laxative made mostly from alcohol. Other recipes include Guiuacum wood chips, caraway, Salt of Tartar, and scammony.\n\nAccording to an early nineteenth century advertisement it was used for the following ailments: The Stone in Babies and Children; Convulsion fits; Consumption and Bad Digestives; Agues; Piles; Surfeits; Fits of the Mother and Vapours from the Spleen; Green Sickness; Children's Distempers, whether the Worms, Rickets, Stones, Convulsions, Gripes, King's Evil, Joint Evil or any other disorder proceeding from Wind or Crudities; Gout and Rheumatism; Stone or Gravel in the Kidnies; Cholic and Griping of the Bowels; the Phthisic (both as cure and preventative provided always that the patient be moderate in drinking, have a care to prevent taking cold and keep a good diet; Dropsy and Scurvy. The frequent use of the medicine to treat Colic, gripes or fret in horses was deplored in early veterinary manuals.\n\nAfter Daffy's death in 1680 the recipe was left to his daughter Catherine, and his kinsmen Anthony and Daniel who were apothecaries in Nottingham. Anthony Daffy moved to London in the 1690s and began to exploit the product issuing pamphlets such as \"Directions for taking elixir salutis or, the famous purging cordial, known by the name of Daffy's elixir salutis\" [London], [1690?]. His widow Elleanor Daffy continued from about 1693 and (their daughter?) Katharine from about 1707. During the early 18th century the product was advertised widely in the emerging national and local newspapers. The success attracted several counterfeit copies, using inferior alcohol rather than brandy.\n\nThe medicine was later produced by William and Cluer Dicey & Co. of Bow Church yard c.1775 who claimed the sole rights of manufacture of the True Daffy's Elixir, although the recipe was not subject to any patent. Proprietorship was also then claimed by Peter Swinton of Salisbury Court and his son Anthony Daffy Swinton who may have been descended from the inventor. Dicey and Co. and their successors marketed it in the United States of America.\n\nIt then passed to Dicey and Sutton, and later to Messrs W. Sutton & Co. of Enfield Middlesex who continuing to market it throughout the nineteenth century. The use of Daffy's elixir is referred to in Anthony Trollope's novel Barchester Towers, 1857.\n\nDaffy's elixir is also mentioned on several occasions in Thomas Pynchon's novel Mason & Dixon, particularly by Jeremiah Dixon, who attempts to procure large quantities before beginning his surveying trip with Charles Mason. Dixon is warned by Benjamin Franklin, however, that imported Daffy's Elixir is extremely expensive, and he would be better off ordering a customized version from the apothecary. During the same visit, Dixon also orders laudanum, a well-known constipating agent.\n\nDaffy's elixir is also mentioned in the Charles Dickens book, Oliver Twist, Ch. II, where it is referred to as Daffy, in the sentence: 'Why, it's what I'm obliged to keep a little of in the house, to put into the blessed infants' Daffy, when they ain't well, Mr. Bumble,(the Parish Beadle)' replied Mrs. Mann as she opened a corner cupboard, and took down a bottle and glass. 'It's gin. I'll not deceive you, Mr. B. It's gin.'\n\nDaffy’s Elixir is also mentioned in the Works of William Makepeace Thackeray book, Vanity Fair, Chapter XXXVIII A Family In a Small Way, where it is referenced in the sentence ‘..and there found Mrs. Sedley in the act of surreptitiously administering Daffy’s Elixir to the infant.’\n\n\"Daffy’s original elixir salutis, vindicated against all counterfeits, &c. or, An advertisement by mee, Anthony Daffy, of London, citizen and student in physick, By way of vindication of my famous and generally approved cordial drink, (called elixir salutis) from the notoriously false suggestions of one Tho. Witherden of Bear-steed in the county of Kent, Gent. (as pretended;) Jane White, Robert Brooke, apothecary, and Edward Willet; all new upstatrt counterfitors of my elixir, and Ape-like imitators of my long since printed Books and Directions, (some of them, nigh verbatim, or word for word) and that to the jeopardy of many good, (but mis-in-formed) Peoples Healths, and Lives too; as also, from the false pretentions of other more sneaking Cub-Quacks, not yet lickt into form, but remaining Moon-blind brats, (still in swadling-clouts) I mean the numerous crew of libellous pamphleteeirs, which are (if possible) more dangerous counterfeiters of my Elixer\" . . . Advertisement by mee, Anthony Daffy s.n., 1690?].\n\n\"Daffy’s original and famous elixir salutis: the choice drink of health: or, health-bringing drink. Being a famous cordial drink, found out by the providence of the Almighty, and (for above twenty years) experienced by himself, and divers persons (whose names are at most of their desires here inserted) a most excellent preservative of man-kind. A secret far beyond any medicament yet known, and is found so agreeable to nature, that it effects all its operations, as nature would have it, and as a virtual expedient proposed by her, for reducing all her extreams unto an equal temper; the same being fitted unto all ages, sexes, complexions, and constitutions, and highly fortifying nature against any noxious humour, invading or offending the noble parts. Never published by any but by Anthony Daffy, student in physick, and since continued by his widow Elleanor Daffy\", London : printed with allowance, for the author, by Tho. Milbourn dwelling in Jewen-Street, 1693.\n\n\n"}
{"id": "272167", "url": "https://en.wikipedia.org/wiki?curid=272167", "title": "De rerum natura", "text": "De rerum natura\n\nDe rerum natura (; On the Nature of Things) is a first-century BC didactic poem by the Roman poet and philosopher Lucretius (c. 99 BC – c. 55 BC) with the goal of explaining Epicurean philosophy to a Roman audience. The poem, written in some 7,400 dactylic hexameters, is divided into six untitled books, and explores Epicurean physics through poetic language and metaphors. Namely, Lucretius explores the principles of atomism; the nature of the mind and soul; explanations of sensation and thought; the development of the world and its phenomena; and explains a variety of celestial and terrestrial phenomena. The universe described in the poem operates according to these physical principles, guided by \"fortuna\" (\"chance\"), and not the divine intervention of the traditional Roman deities.\n\nTo the Greek philosopher Epicurus, the unhappiness and degradation of humans arose largely from the dread which they entertained of the power of the deities, from terror of their wrath. This wrath was supposed to be displayed by the misfortunes inflicted in this life and by the everlasting tortures that were the lot of the guilty in a future state (or, where these feelings were not strongly developed, from a vague dread of gloom and misery after death). Epicurus thus made it his mission to remove these fears, and thus to establish tranquility in the minds of his readers. To do this, Epicurus invoked the atomism of Democritus to demonstrate that the material universe was formed not by a Supreme Being, but by the mixing of elemental particles that had existed from all eternity governed by certain simple laws. He argued that the deities (whose existence he did not deny) lived forevermore in the enjoyment of absolute peacestrangers to all the passions, desires, and fears, which affect humans and totally indifferent to the world and its inhabitants, unmoved alike by their virtues and their crimes. This meant that humans had nothing to fear from them.\n\nLucretius's task was to clearly state and fully develop these views in an attractive form; his work was an attempt to show through poetry that everything in nature can be explained by natural laws, without the need for the intervention of divine beings. Lucretius identifies the supernatural with the notion that the deities created our world or interfere with its operations in some way. He argues against fear of such deities by demonstrating, through observations and arguments, that the operations of the world can be accounted for in terms of natural phenomena. These phenomena are the result of regular, but purposeless motions and interactions of tiny atoms in empty space.\n\nThe poem consists of six untitled books, in dactylic hexameter. The first three books provide a fundamental account of being and nothingness, matter and space, the atoms and their movement, the infinity of the universe both as regards time and space, the regularity of reproduction (no prodigies, everything in its proper habitat), the nature of mind (\"animus\", directing thought) and spirit (\"anima\", sentience) as material bodily entities, and their mortality, since, according to Lucretius, they and their functions (consciousness, pain) end with the bodies that contain them and with which they are interwoven. The last three books give an atomic and materialist explanation of phenomena preoccupying human reflection, such as vision and the senses, sex and reproduction, natural forces and agriculture, the heavens, and disease.\n\nLucretius opens his poem by addressing Venus not only as the mother of Rome (\"Aeneadum genetrix\") but also as the veritable mother of nature (\"Alma Venus\"), urging her to pacify her lover Mars and spare Rome from strife. By recalling the opening to poems by Homer, Ennius, and Hesiod (all of which begin with an invocation to the Muses), the proem to \"De rerum natura\" conforms to epic convention. The entire proem is also written in the format of a hymn, recalling other early literary works, texts, and hymns and in particular the Homeric Hymn to Aphrodite. The choice to address Venus may have been due to Empedocles's belief that Aphrodite represents \"the great creative force in the cosmos\". Given that Lucretius goes on to argue that the gods are removed from human life, many have thus seen this opening to be contradictory: how can Lucretius pray to Venus and then deny that the gods listen to or care about human affairs? In response, many scholars argue that the poet uses Venus poetically as a metonym. For instance, Diskin Clay sees Venus as a poetic substitute for sex, and Bonnie Catto sees the invocation of the name as a metonym for the \"creative process of \"natura\"\".\n\nAfter the opening, the poem commences with an enunciation of the proposition on the nature and being of the deities, which leads to an invective against the evils of superstition. Lucretius then dedicates time to exploring the axiom that nothing can be produced from nothing, and that nothing can be reduced to nothing (\"Nil fieri ex nihilo, in nihilum nil posse reverti\"). Following this, the poet argues that the universe comprises an infinite number of Atoms, which are scattered about in an infinite and vast void (\"Inane\"). The shape of these atoms, their properties, their movements, the laws under which they enter into combination and assume forms and qualities appreciable by the senses, with other preliminary matters on their nature and affections, together with a refutation of objections and opposing hypotheses, occupy the first two books.\n\nIn the third book, the general concepts proposed thus far are applied to demonstrate that the vital and intellectual principles, the \"Anima\" and \"Animus\", are as much a part of us as are our limbs and members, but like those limbs and members have no distinct and independent existence, and that hence soul and body live and perish together; the book concludes by arguing that the fear of death is a folly, as death merely extinguishes all feelingboth the good and the bad.\n\nThe fourth book is devoted to the theory of the senses, sight, hearing, taste, smell, of sleep and of dreams, ending with a disquisition upon love and sex.\n\nThe fifth book is described by Ramsay as the most finished and impressive, while Stahl argues that its \"puerile conceptions\" is proof that Lucretius should be judged as a poet, not as a scientist. This book addresses the origin of the world and of all things therein, the movements of the heavenly bodies, the changing of the seasons, day and night, the rise and progress of humankind, society, political institutions, and the invention of the various arts and sciences which embellish and ennoble life.\n\nThe sixth book contains an explanation of some of the most striking natural appearances, especially thunder, lightning, hail, rain, snow, ice, cold, heat, wind, earthquakes, volcanoes, springs and localities noxious to animal life, which leads to a discourse upon diseases. This introduces a detailed description of the great pestilence that devastated Athens during the Peloponnesian War. With this episode, the book closes; this abrupt ending suggests that Lucretius might have died before he was able to finalize and fully edit his poem.\n\nLucretius wrote this epic poem to \"Memmius\", who may be Gaius Memmius, who in 58 BC was a praetor, a judicial official deciding controversies between citizens and the government. There are over a dozen references to \"Memmius\" scattered throughout the long poem in a variety of contexts in translation, such as \"Memmius mine\", \"my Memmius\", and \"illustrious Memmius\". According to Lucretius's frequent statements in his poem, the main purpose of the work was to free Gaius Memmius's mind of the supernatural and the fear of death—and to induct him into a state of \"ataraxia\" by expounding the philosophical system of Epicurus, whom Lucretius glorifies as the hero of his epic poem.\n\nHowever, the purpose of the poem is subject to ongoing scholarly debate. Lucretius refers to Memmius by name four times in the first book, three times in the second, five in the fifth, and not at all in the third, fourth, or sixth books. In relation to this discrepancy in the frequency of Lucretius's reference to the apparent subject of his poem, Kannengiesse advances the theory that Lucretius wrote the first version of \"De rerum natura\" for the reader at large, and subsequently revised in order to write it for Memmius. However, Memmius' name is central to several critical verses in the poem, and this theory has therefore been largely discredited. The German classicists Ivo Bruns and Samuel Brandt set forth an alternative theory that Lucretius did at first write the poem with Memmius in mind, but that his enthusiasm for his patron cooled. Stearns suggests that this is because Memmius reneged on a promise to pay for a new school to be built on the site of the old Epicurean school. Memmius was also a tribune in 66, praetor in 58, governor of Bithynia in 57, and was a candidate for the consulship in 54 but was disqualified for bribery, and Stearns suggests that the warm relationship between patron and client may have cooled (\"sed tua me virtus tamen et sperata voluptas\" / \"suavis amicitiae quemvis efferre laborem\", \"But still your merit, and as I hope, the joy / Of our sweet friendship, urge me to any toil\").\n\nThere is a certain irony to the poem, namely that while Lucretius extols the virtue of the Epicurean school of thought, Epicurus himself had advised his acolytes from penning poetry because he believed it to make that which was simple overly complicated. Near the end of his first book, Lucretius defends his fusion of Epicureanism and poetry with a simile, arguing that the philosophy he espouses is like a medicine: life-saving but often unpleasant. Poetry, on the other hand, is like honey, in that it is a \"a sweetener that sugarcoats the bitter medicine of Epicurean philosophy and entices the audience to swallow it.\" (Of note, Lucretius repeats these 25 lines, almost verbatim, in the introduction to the fourth book.)\n\nThe state of the poem as it currently exists suggests that it was released in an unfinished state. For instance, the poem concludes rather abruptly, there are dual passages throughout, and at 5.155 Lucretius mentions that he will spend a great deal of time discussing the nature of the gods, which never comes to pass. Some have suggested that Lucretius died before being able to edit, finalize, and publish his work.\n\nAfter the poem was rediscovered and made its rounds across Europe and beyond, numerous thinkers began to see Lucretius's Epicureanism as a \"threat synonymous with atheism.\" Some Christian apologists viewed \"De rerum natura\" as an atheist manifesto and a dangerous foil to be thwarted. However, at that time the label was extremely broad and did not necessarily mean a denial of divine entities (for example, some large Christian sects labelled dissenting groups as atheists). What is more, Lucretius does not deny the existence of deities; he simply argues that they did not create the universe, that they do not care about human affairs, and that they do not intervene in the world. Regardless, due to the ideas espoused in the poem, much of Lucretius's work was seen by many as direct a challenge to theistic, Christian belief. Ada Palmer has labelled six ideas in Lucretius's thought (viz. his assertion that the world was created from chaos, and his denials of Providence, divine participation, miracles, the efficacy of prayer, and an afterlife) as \"proto-atheistic\". She qualifies her use of this term, cautioning that it is not to be used to say that Lucretius was himself an atheist in the modern sense of the word, nor that atheism is a teleological necessity, but rather that many of his ideas were taken up by 19th, 20th, and 21st century atheists.\n\n\"De rerum natura\" does not argue that the soul does not exist; rather, the poem claims that the soul, like all things in existence, is made up of atoms, and because these atoms will one day drift apart, the human soul is not immortal. Lucretius thus argues that death is simply annihilation, and that there is no afterlife. He likens the physical body to a vessel that holds both the mind (\"mens\") and spirit (\"anima\"). To prove that neither the mind nor spirit can survive independent of the body, Lucretius uses a simple analogy: when a vessel shatters, its contents spill everywhere; likewise, when the body dies, the mind and spirit dissipate. And as a simple ceasing-to-be, death can be neither good nor bad for this being, since a dead personbeing completely devoid of sensation and thoughtcannot miss being alive. To further alleviate the fear of non-existence, Lucretius makes use of the symmetry argument: he argues that the eternal oblivion awaiting all humans after death is exactly the same as the infinite nothingness that preceded our birth. Since that nothingness (which in fact, was like a deep, peaceful sleep) caused us no pain or discomfort, we should not fear the same nothingness that will follow our own demise:\n\nAccording to the \"Stanford Encyclopedia of Philosophy\", Lucretius sees those who fear death as embracing the fallacious assumption that they will be present in some sense \"to regret and bewail [their] own non-existence.\"\n\nLucretius maintained that he could free humankind from fear of the deities by demonstrating that all things occur by natural causes without any intervention by the deities. Historians of science, however, have been critical of the limitations of his Epicurean approach to science, especially as it pertained to astronomical topics, which he relegated to the class of \"unclear\" objects.\n\nThus, he began his discussion by claiming that he would\nexplain by what forces nature steers the courses of the Sun and the journeyings of the Moon, so that we shall not suppose that they run their yearly races between heaven and earth of their own free will [i.e., are gods themselves] or that they are rolled round in furtherance of some divine plan...\n\nHowever, when he set out to put this plan into practice, he limited himself to showing how one, or several different, naturalistic accounts could explain certain natural phenomena. He was unable to tell his readers how to determine which of these alternatives might be the true one. For instance, when considering the reason for stellar movements, Lucretius provides two possible explanations: that the sky itself rotates, or that the sky as a whole is stationary while constellations move. If the latter is true, Lucretius, notes, this is because: \"either swift currents of ether whirl round and round and roll their fires at large across the nocturnal regions of the sky\"; \"an external current of air from some other quarter may whirl them along in their course\"; or \"they may swim of their own accord, each responsive to the call of its own food, and feed their fiery bodies in the broad pastures of the sky\". Lucretius concludes that \"one of these causes must certainly operate in our world... But to lay down which of them it is lies beyond the range of our stumbling progress.\"\n\nDrawing on these, and other passages, William Stahl considered that \"The anomalous and derivative character of the scientific portions of Lucretius' poem makes it reasonable to conclude that his significance should be judged as a poet, not as a scientist.\"\n\nDeterminism appears to conflict with the concept of free will. Lucretius attempts to allow for free will in his physicalistic universe by postulating an indeterministic tendency for atoms to veer randomly (, literally \"the turning aside of a thing\", but often translated as \"the swerve\"). According to Lucretius, this unpredictable swerve occurs at no fixed place or time:\n\nWhen atoms move straight down through the void by their own weight, they deflect a bit in space at a quite uncertain time and in uncertain places, just enough that you could say that their motion has changed. But if they were not in the habit of swerving, they would all fall straight down through the depths of the void, like drops of rain, and no collision would occur, nor would any blow be produced among the atoms. In that case, nature would never have produced anything.\n\nThis swerving provides the indeterminacy that Lucretius argues allows for the \"free will which living things throughout the world have\" (\"libera per terras\" ... \"haec animantibus exstat\" ... \"voluntas\").\n\nMartin Ferguson Smith notes that Cicero's close friend, Titus Pomponius Atticus, was an Epicurean publisher, and it is possible his slaves made the very first copies of \"De rerum natura\". If this were the case, then it might explain how Cicero came to be familiar with Lucretius's work. In c. AD 380, St. Jerome would contend in his \"Chronicon\" that Cicero amended and edited \"De rerum natura\", although most scholars argue that this is an erroneous claim; the classicist David Butterfield argues that this mistake was likely made by Jerome (or his sources) because the earliest reference to Lucretius is in the aforementioned letter from Cicero. Nevertheless, a small minority of scholars argue that Jerome's assertion may be credible.\n\nThe oldest purported fragments of \"De rerum natura\" were published by K. Kleve in 1989 and consist of sixteen fragments. These remnants were discovered among the Epicurean library in the Villa of the Papyri, Herculaneum. Because, as W. H. D. Rouse notes, \"the fragments are so minute and bear so few certainly identifiable letters\", at this point in time \"some scepticism about their proposed authorship seems pardonable and prudent.\" However, Kleve contends that four of the six books are represented in the fragments, which he argues is reason to assume that the entire poem was at one time kept in the library. If Lucretius's poem were to be definitely placed at the Villa of the Papyri, it would suggest that it was studied by the Neapolitan Epicurean school.\n\nCopies of the poem were preserved in a number of medieval libraries, with the earliest extant manuscripts dating from the ninth-century. The oldestand, according to David Butterfield, most famousof these is the Codex Oblongus, often called O. This copy has been dated to the early ninth century and was produced by a Carolingian scriptorium (likely a monastery connected to the court of Charlemagne). O is currently housed at Leiden University. The second of these ninth-century manuscripts is the Codex Quadratus, often called Q. This manuscript was likely copied after O, sometime in the mid-ninth century. Today, Q is also housed at Leiden University. The third and final ninth-century manuscriptwhich comprises the \"Schedae Gottorpienses\" fragment (commonly called G and located in the Kongelige Bibliotek of Copenhagen) and the \"Schedae Vindobonenses\" fragments (commonly called V and U and located in the Austrian National Library in Vienna)was christened by Butterfield as S and has been dated to the latter part of the ninth century. Scholars consider manuscripts O, Q, and S to all be descendants of the original archetype, which they dub Ω. However, while O is a direct descendant of the archetype, Q and S are believed to have both been derived from a manuscript (Ψ) that in turn had been derived from a damaged and modified version of the archetype (Ω).\n\nWhile there exist a handful of references to Lucretius in Romance and Germanic sources dating between the ninth-and fifteenth centuries (references that, according to Ada Palmer, \"indicate a tenacious, if spotty knowledge of the poet and some knowledge of [his] poem\"), no manuscripts of \"De rerum natura\" currently survive from this span of time. Rather, all the remaining Lucertian manuscripts that are currently extant date from or after the fifteenth century. This is because \"De rerum natura\" was rediscovered in January 1417 by Poggio Bracciolini, who probably found the poem in the Benedictine library at Fulda. The manuscript that Poggio discovered did not survive, but a copy (the \"Codex Laurentianus 35.30\") of it by Poggio's friend, Niccolò de' Niccoli, did, and today it is kept at the Laurentian Library in Florence.\n\nMachiavelli made a copy early in his life. Molière produced a verse translation which does not survive; John Evelyn translated the first book.\n\nThe Italian scholar Guido Billanovich demonstrated that Lucretius's poem was well known in its entirety by Lovato Lovati (1241–1309) and some other Paduan pre-humanists during the thirteenth century. This proves that the work was known in select circles long before the official rediscovery by Poggio. It has been suggested that Dante (1265–1321) might have read Lucretius's poem, as a few verses of his \"Divine Comedy\" exhibit a great affinity with \"De rerum natura\", but there is no conclusive evidence that Dante ever read Lucretius.\n\nThe first printed edition of \"De rerum natura\" was produced in Brescia, Lombardy, in 1473. Other printed editions followed soon after. Additionally, although only published in 1996, Lucy Hutchinson's translation of \"De rerum natura\" was in all likelihood the first in English and was most likely completed some time in the late 1640s or 1650s.\n\nThe earliest recorded critique of Lucretius's work is in a letter written by the Roman statesman Cicero to his brother Quintus, in which the former claims that Lucretius's poetry is \"full of inspired brilliance, but also of great artistry\" (\"Lucreti poemata, ut scribis, ita sunt, multis luminibus ingeni, multae tamen artis\").\n\nIt is also believed that the Roman poet Virgil referenced Lucretius and his work in the second book of his \"Georgics\" when he wrote: \"Happy is he who has discovered the causes of things and has cast beneath his feet all fears, unavoidable fate, and the din of the devouring Underworld\" (\"felix qui potuit rerum cognoscere causas\"/\"atque metus omnis et inexorabile fatum\"/\"subiecit pedibus strepitumque Acherontis avari\"). According to David Sedley of the \"Stanford Encyclopedia of Philosophy\", \"With these admiring words, Virgil neatly encapsulates four dominant themes of the poemuniversal causal explanation, leading to elimination of the threats the world seems to pose, a vindication of free will, and disproof of the soul's survival after death.\"\n\nLucretius was almost certainly read by the imperial poet Marcus Manilius (fl. 1st century AD), whose didactic poem \"Astronomica\" (written c. AD 1020), alludes to \"De rerum natura\" in a number of places. However, Manilius's poem, espouses a Stoic, deterministic understanding of the universe, and by its very nature attacks the very philosophical underpinnings of Lucretius's worldview. This has led scholars like Katharina Volk to argue that \"Manilius is a veritable anti-Lucretius\". What is more, Manilius also seems to suggest throughout this poem that his work is superior to that of Lucretius's. (Coincidentally, \"De rerum natura\" and the \"Astronomica\" were both rediscovered by Poggio Bracciolini in the early 15th century.)\n\nAdditionally, Lucretius's work is discussed by the Augustan poet Ovid, who in his \"Amores\" writes \"the verses of the sublime Lucretius will perish only when a day will bring the end of the world\" (\"Carmina sublimis tunc sunt peritura Lucreti / exitio terras cum dabit una dies\"), and the Silver Age poet Statius, who in his \"Silvae\" praises Lucretius as being highly \"learned\". David Butterfield also writes that \"clear echoes and/or responses\" to \"De rerum natura\" can be detected in the works of the Roman elegiac poets Catullus, Propertius, and Tibullus, as well as the lyric poet Horace.\n\nIn regards to prose writers, a number either quote from Lucretius's poem or express great admiration for \"De rerum natura\", including: Vitruvius (in \"De Architectura\"), Marcus Velleius Paterculus (in the \"Historiae Romanae\"), Quintilian (in the \"Institutio Oratoria\"), Tacitus (in the \"Dialogus de oratoribus\"), Marcus Cornelius Fronto (in \"De eloquentia\"), Cornelius Nepos (in the \"Life Of Atticus\"), Apuleius (in \"De Deo Socratis\"), and Gaius Julius Hyginus (in the \"Fabulae\"). Additionally, Pliny the Elder lists Lucretius (presumably referring to his \"De rerum natura\") as a source at the beginning of his \"Naturalis Historia\", and Seneca the Younger quoted six passages from \"De rerum natura\" across several of his works.\n\nBecause Lucretius was critical of religion and the claim of an immortal soul, his poem was disparaged by most early Church Fathers. The Early Christian apologist Lactantius, in particular, heavily cites and critiques Lucretius in his \"The Divine Institutes\" and its \"Epitome\", as well as his \"De ira Dei\". While he argued that Lucretius's criticism of Roman religion were \"sound attacks on paganism and superstition\", Lactantius claimed that they were futile against the \"True Faith\" of Christianity. Lactantius also disparages the science of \"De rerum natura\" (as well as of Epicureanism in general), calls Lucretius \"the most worthless of the poets\" (\"poeta inanissimus\"), notes that he is unable to read more than a few lines of \"De rerum natura\" without laughing, and sarcastically asks, \"Who would think that [Lucretius] had a brain when he said these things?\"\n\nAfter Lactantius's time, Lucretius was almost exclusively referenced or alluded to in a negative manner by the Church Fathers. The one major exception to this was Isidore of Seville, who at the start of the 7th century produced a work on astronomy and natural history dedicated to the Visigothic king Sisebut that was entitled \"De natura rerum\". In both this work, and as well as his more well-known \"Etymologiae\" (c. AD 600625), Isidore liberally quotes from Lucretius a total of twelve times, drawing verses from all of Lucretius's books except his third. (Of note, about a century later, the British historian and Doctor of the Church Bede produced a work also called \"De natura rerum\", partly based on Isidore's work but apparently ignorant of Lucretius's poem.)\n\nMontaigne owned a Latin edition published in Paris, in 1563, by Denis Lambin which he heavily annotated. His \"Essays\" contain almost a hundred quotes from \"De rerum natura\". Additionally, in his essay \"Of Books\", he lists Lucretius along with Virgil, Horace, and Catullus as his four top poets.\n\nNotable figures who owned copies include Ben Jonson whose copy is held at the Houghton Library, Harvard; and Thomas Jefferson, who owned at least five Latin editions and English, Italian and French translations.\n\nLucretius has also had a marked influence upon modern philosophy, as perhaps the most complete expositor of Epicurean thought. His influence is especially notable in the work of the Spanish-American philosopher George Santayana, who praised Lucretiusalong with Dante and Goethein his book \"Three Philosophical Poets\".\n\nIn 2011, the historian and literary scholar Stephen Greenblatt wrote a popular history book about the poem, entitled \"\". In the work, Greenblatt argues that Poggio Bracciolini's discovery of \"De rerum natura\" reintroduced important ideas that sparked the modern age. The book was well-received, and later earned the 2012 Pulitzer Prize for General Non-Fiction and the 2011 National Book Award for Nonfiction.\n\n\n"}
{"id": "30303286", "url": "https://en.wikipedia.org/wiki?curid=30303286", "title": "Decoding Reality", "text": "Decoding Reality\n\nDecoding Reality: The Universe as Quantum Information is a popular science book by Vlatko Vedral published by Oxford University Press in 2010. Vedral examines information theory and proposes information as the most fundamental building block of reality. He argues what a useful framework this is for viewing all natural and physical phenomena. In building out this framework the books touches upon the origin of information, the idea of entropy, the roots of this thinking in thermodynamics, the replication of DNA, development of social networks, quantum behaviour at the micro and macro level, and the very role of indeterminism in the universe. The book finishes by considering the answer to the ultimate question: where did all of the information in the Universe come from? The ideas address concepts related to the nature of particles, time, determinism, and of reality itself.\n\nVedral believes in the principle that information is physical. \"Creation ex nihilo\" comes from Catholic dogma, the idea being that God created the universe out of nothing. Vedral says that invoking a supernatural being as an explanation for creation does not explain reality because the supernatural being would have to come into existence itself too somehow presumably from nothing (or else from an infinite regression of supernatural beings), thus of course the reality can come from nothing without a supernatural being. Occam's razor principle favours the simplest explanation. Vedral believes information is the fundamental building block of reality as it occurs at the macro level (economics, human behaviour etc.) as well as the subatomic level. Vedral argues that information is the only candidate for such a building block that can explain its own existence as information generates additional information that needs to be compressed thus generating more information. 'Annihilation of everything' is a more fitting term than \"creation ex nihilo\" Vedral states, as compression of possibilities is the process of how new information is created.\n\nVedral uses an Italo Calvino philosophical story about a tarot-like card game as the kernel for his metaphor of conscious life arriving \"in medias res\" to a pre-existing contextual reality. In this game the individual observers/players (Vedral suggests: quantum physics, thermodynamics, biology, sociology, economics, philosophy) lay down cards with ambiguous meanings as an attempt to communicate messages to deduce meaning out of the other players' interactions. The results (information) of previous rounds establish contextual rules for observers/players in subsequent rounds. The point of this game is not established until the last card has been played as later cards can change the meaning of previous events, as in the case of the quantum explanation for the photoelectric effect instantly disproving classical physics. Vedral points out that in our reality there is no last card.\n\nShannon entropy or information content measured as the surprise value of a particular event, is essentially inversely proportional to the logarithm of the event's probability, i = log(1/p). Claude Shannon's information theory arose from research at Bell labs, building upon George Boole's digital logic. As information theory predicts common and easily predicted words tend to become shorter for optimal communication channel efficiency while less common words tend to be longer for redundancy and error correction. Vedral compares the process of life to John von Neumann's self replicating automata. These are enduring information carriers that will survive wear and tear of the individual by producing copies that can in turn go on to produce more copies.\n\nGenetic code as an efficient digital information store, containing built in codon redundancy for error correction in transcription.\n\nExamines the Second law of thermodynamics and the process of information increasing entropy. Maxwell's Demon was thought to be a way around this inevitability, however such a demon would run out of information storage space and have to delete unwanted data thus having to do work to do so, increasing entropy.\n\nBlackjack as controlled risk taking using Shannon's information theory probability formulas. Casino as a ′cool′ financial entropy source and the gambler as a ′hot′ financial source, once again the Second law of thermodynamics means the flow is almost always from hot to cold in the long run. For managed risk spread bets widely and in high risk high reward investments (assuming a known probability), this is the Log optimal portfolio approach.\n\nSix degrees of separation means well connected people tend to be more successful as their social networks expose them to more chances to make choices they want. Schelling precommitment as strategy in social and self-control, for example burning your bridges by buying gym membership to help motivated self win over lazy self. Mutual information resulting in phase transitions in social and political demography as well as physical systems, like water freezing into ice at a particular critical temperature or magnetic fields spontaneously aligning in certain atoms when cooling from a molten state.\n\nVedral examines the basis of quantum information, the qubit, and examines one-time pad quantum cryptography as the most secure form of encryption because of its uncomputability. Quantum entanglement demonstrates the importance of mutual information in defining outcomes in a reality.\n\nQuantum computers offer a search advantage over classical computers by searching many database elements at once as a result of quantum superpositions. A sufficiently advanced quantum computer would break current encryption methods by factorizing large numbers several orders of magnitude faster than any existing classical computer. Any computable problem may be expressed as a general quantum search algorithm although classical computers may have an advantage over quantum search when using more efficient tailored classical algorithms. The issue with quantum computers is that a measurement must be made to determine if the problem is solved which collapses the superposition. Vedral points out that unintentional interaction with the environment can be mitigated with redundancy, and this would be necessary if we were to scale up current quantum computers to achieve greater utility, i.e. to utilize 10 qubits have a 100 atom quantum system so that if one atom decoheres a consensus will still be held by the other 9 for the state of the same qubit.\n\nRandomness is key to generating new sources of surprise in a reality. Compression of these new sources to discard unimportant information is the deterministic element and organising principle.\n\nThe information content of the universe as measured in bits or qubits. Vedral uses the initial effort of Archimedes of Syracuse in calculating the amount of sand that could theoretically fit inside the universe and compares it to a modern-day attempt to calculate the bit content of the universe. Vedral however sees this content as ultimately limitless as possibly maximum entropy is never reached as compression of complexity is an open ended process and random events will continue to occur. As Vedral sees information as the ultimate building block of physical reality, he speculates that information originating at any scale can force outcomes in all other scales to abide where mutual information is shared. For example, a human performed macro-level scientific test in search of a behaviour in a quantum particle could set parameters for that type of particle in the future when subjected to a similar test.\n\nThe information basis for \"creation ex nihilo\". According to John von Neumann, starting trivially from an empty set of numbers an infinite sequence of numbers can bootstrap their way out. An empty set creates the number 1 by observing an empty set within itself which is enough of a basis for distinguishability. It creates the number 2 by observing an empty set within the second empty set and the number 1, and so on. Vedral sees this not as creation but as data compression, as every event of a reality breaks the symmetry of the pre-existing formlessness. Science is the process of describing a large amount of observed phenomena in a compressed programmatic way to predict future outcomes, and in this process of data compression science creates new information by eliminating all contrary possibilities to explain those phenomena.\n\nThe book explains the world as being made up of information. The Universe and its workings are the ebb and flow of information. We are all transient patterns of information, passing on the recipe for our basic forms to future generations using a four-letter digital code called DNA. In this engaging and mind-stretching account, Vlatko Vedral considers some of the deepest questions about the Universe and considers the implications of interpreting it in terms of information. He explains the nature of information, the idea of entropy, and the roots of this thinking in thermodynamics. He describes the bizarre effects of quantum behaviour - effects such as 'entanglement', which Einstein called 'spooky action at a distance' and explores cutting edge work on the harnessing quantum effects in hyperfast quantum computers, and how recent evidence suggests that the weirdness of the quantum world, once thought limited to the tiniest scales, may reach into the macro world. Vedral finishes by considering the answer to the ultimate question: where did all of the information in the Universe come from? The answers he considers are exhilarating, drawing upon the work of distinguished physicist John Wheeler and his concept of “it from bit”. The ideas challenge our concept of the nature of particles, of time, of determinism, and of reality itself.\n\n"}
{"id": "31500839", "url": "https://en.wikipedia.org/wiki?curid=31500839", "title": "Dryad (repository)", "text": "Dryad (repository)\n\nDryad is an international disciplinary repository of data underlying scientific and medical publications. Dryad is a curated general-purpose repository that makes data discoverable, freely reusable, and citable. The scientific, educational, and charitable mission of Dryad is to promote the availability of data underlying findings in the scientific literature for research and educational reuse.\n\nThe vision of Dryad is a scholarly communication system in which learned societies, publishers, institutions of research and education, funding bodies and other stakeholders collaboratively sustain and promote the preservation and reuse of data underlying the scholarly literature.\n\nDryad aims to allow researchers to validate published findings, explore new analysis methodologies, re-purpose data for research questions unanticipated by the original authors, and perform synthetic studies such as formal meta-analyses. For many publications, existing data repositories do not capture the whole data package. As a result, many important datasets are not being preserved and are no longer available, or usable, at the time that they are sought by later investigators.\n\nDryad serves as a repository for tables, spreadsheets, flat files, and all other kinds of published data for which specialized repositories do not already exist. Optimally, authors submit data to Dryad in conjunction with article publication, so that links to the data can be included in the published article. All data files in Dryad are associated with a published article, and are made available for reuse under the terms of a Creative Commons Zero waiver.\n\nDryad is also a non-profit membership organization registered in the US, providing a forum for all stakeholders to set priorities for the repository, participate in planning, and share knowledge and coordinate action around data policies.\n\nDryad is listed in the Registry of Research Data Repositories re3data.org.\n\nDryad enables authors, journals, societies and publishers to facilitate data archiving at the time of publication, when the data are readily available. Data in Dryad receives a permanent, unique Digital object identifier (DOI), which can be included in the published article so that readers are able to access the data. Authors can archive data in Dryad and be assured of its preservation, while satisfying journals' and research funding agencies' mandates to disseminate their research outputs.\n\nAuthors submit data to Dryad either when the associated article is under review or has been accepted for publication. The choice depends on whether the journal includes data within the scope of peer reviewer. Authors may also submit data after an article has been published.\n\nData submission is facilitated by journals sending notices of new manuscripts to Dryad. This saves authors from having to re-enter the bibliographic details when they upload their data files.\n\nDryad curators review submitted data files and perform quality control on metadata descriptions before inclusion of new content in the repository. Dryad’s metadata approach emphasizes simplicity and interoperability, supported by a Dublin Core metadata application profile. The Metadata Research Center now at the College of Computing and Informatics, Drexel University, serves as Dryad’s central curation hub, with curation activity also taking place via the non-profit in North Carolina. \n\nDryad coordinates data submission to specialized repositories where in order to (a) lower user burden by streamlining the submission workflow and (b) allow Dryad and specialized repositories to exchange identifiers and other metadata in order to enable cross-referencing of the different data products associated with a given publication. The first two handshaking partners are TreeBASE and GenBank, which Dryad's partner journals have previously identified as required points of deposition for phylogenetic tree data and DNA sequences, respectively.\n\nDryad is governed by a twelve-member Board of Directors, elected by its Members. Members may be independent journals, societies, publishers, research and educational institutions, libraries, funders, or other organizations that support Dryad's mission. The organization coordinates data sharing policies and promotes the long-term sustainability of the repository.\n\nDryad began charging submission fees in September 2013. Dryad is a nonprofit organization that provides long-term access to its contents at no cost to researchers, educators or students, irrespective of nationality or institutional affiliation. Dryad is able to provide free access to data due to financial support from members and data submitters. Dryad’s submission fees are designed to sustain its core functions by recovering the basic costs of curating and preserving data.\n\nDryad emerged from a National Evolutionary Synthesis Center (NESCent) workshop entitled \"Digital data preservation, sharing, and discovery: Challenges for Small Science Communities in the Digital Era\" in May 2007. Initial funding for Dryad was provided by the National Science Foundation to the National Evolutionary Synthesis Center and other partners in the US.\n\nDryadUK was a Jisc-funded project run from the British Library and the University of Oxford, in partnership with NESCent, the Digital Curation Centre, and Charles Beagrie Ltd. The project led to a UK mirror of the Dryad repository based at the British Library. The project also improved the tools available for the publication and citation of data, expanded the disciplinary range of participating journals, and further developed the business framework for an international organization dedicated to long-term data preservation.\n\nDryad is a member of the Data Observation Network for Earth (DataONE).\n\nDryad is built upon the open source DSpace repository software, developed by the Massachusetts Institute of Technology and Hewlett-Packard. Many customizations have been integrated into the main DSpace codebase; customizations specific to Dryad are maintained in the open source Dryad code repository.\n\n\n"}
{"id": "7962227", "url": "https://en.wikipedia.org/wiki?curid=7962227", "title": "Dunstan Baby Language", "text": "Dunstan Baby Language\n\nDunstan Baby Language is a theory about infantile vocal reflexes as signals, in humans. The theory is that across cultures and linguistic groups there are five sounds, each with a meaning, that are used by infants before the language acquisition period. The hypothesis was developed by Australian former mezzo-soprano, Priscilla Dunstan, and has been featured on \"The Oprah Winfrey Show\". Dunstan's theory has not been scientifically validated.\n\nBetween 0–3 months, infants make what Dunstan calls \"sound reflexes\". According to Dunstan, we all have reflexes, like sneezes, hiccups, and burps, that all have a recognizable pattern when sound is added to the reflex. There are other reflexes that all babies experience, and when sound is added to these, a distinct, preemptive \"cry\" will occur before the infant breaks into what Dunstan calls the hysterical cry. Dunstan states that these preemptive cries can indicate what the infant requires (e.g., \"food\", \"comfort\", \"sleep\", etc.), and they escalate to the hysterical cry if they are not answered. As the infant matures past 3 months in vocalization, the sound reflexes become replaced with more elaborate babbling.\n\nAccording to Dunstan, the five universal words (or \"sound reflexes\") used by infants are:\n\n\nDunstan states that she has a photographic memory for sounds and that this, combined with her years in the opera and her experience as a mother, allowed her to recognize certain sounds in the human voice. A DVD set called \"The Dunstan Baby Language\" was released by Dunstan in November 2006. The two-disc set covered the five universal words of the language, methods of learning how to recognize the vocalizations and sounds, numerous examples of baby cries from around the world to \"tune your ear,\" and live demonstrations of newborn mother groups experimenting with the language.\n\nLinguistics experts point out that Dunstan's hypothesis has not been subjected to rigorous testing or academic scrutiny. The Dunstan company had, at one time, developed a clinical trial plan with Brown University researchers to test its claims, but abandoned it for consumer surveys and small-group observations in order \"to hasten the development of a system that could be used by parents,\" skipping rigorous testing to go straight to market.\n\n\n\n"}
{"id": "5976300", "url": "https://en.wikipedia.org/wiki?curid=5976300", "title": "Engines (children's book)", "text": "Engines (children's book)\n\nEngines: Man's Use of Power, from the Water Wheel to the Atomic Pile is a science book for children by L. Sprague de Camp, illustrated by Jack Coggins, published by Golden Press in 1959. A revised edition issued as part of the publisher's Golden Library of Knowledge Series was published in 1961, and a paperback edition in 1969.\n\nAs stated on the cover, the work is a survey of \"Man's use of power, from the water wheel to the atomic pile.\"\n\n\"The Science News-Letter\", in its July 18, 1959 issue, listed the book among its \"Books of the Week,\" describing the work as a \"[f]actual book for young readers.\"\n"}
{"id": "48854038", "url": "https://en.wikipedia.org/wiki?curid=48854038", "title": "FONDECYT", "text": "FONDECYT\n\nThe National Fund for Scientific and Technological Development (Spanish: \"Fondo Nacional de Desarrollo Científico y Tecnológico\"), abbreviated FONDECYT, is the main public fund of the Government of Chile to promote basic scientific and technological research in all areas of knowledge. It is managed by the National Commission for Scientific and Technological Research (CONICYT).\n\nThe fund was established in 1981 and began operations in 1982. Since then it has funded more than 16,000 research projects. In 1991 CONICYT opened a second fund, FONDEF (The Science & Technology Development Fund), with the aim of promoting public–private collaboration in research and development.\n\nThe program provides financial support for projects selected under three headings:\n\n"}
{"id": "49054606", "url": "https://en.wikipedia.org/wiki?curid=49054606", "title": "Holism and Evolution", "text": "Holism and Evolution\n\nHolism and Evolution is a 1926 book by South African statesman Jan Smuts, in which he coined the word \"holism\", although Smuts' meaning is different from the modern concept of holism. Smuts defined holism as the \"fundamental factor operative towards the creation of wholes in the universe.\"\n\nThe book was part of a broader trend of interest in holism in European and colonial academia during the early twentieth century. Smuts' philosophy of holism was based in the thoughts behind his earlier book \"Walt Whitman: A Study in the Evolution of Personality\", written during his time in Cambridge. He described a \"process-orientated, hierarchical view of nature\" and has been influential among criticisms of reductionism. Smuts' formulation of holism has also been linked with his political-military activity, especially his aspiration to create a league of nations.\n"}
{"id": "2623949", "url": "https://en.wikipedia.org/wiki?curid=2623949", "title": "Iben Browning", "text": "Iben Browning\n\nIben Browning (January 9, 1918 – July 18, 1991) was an American business consultant, author, and \"self-proclaimed climatologist.\" He is most notable for having made various failed predictions of disasters involving climate, volcanoes, earthquakes, and government collapse.\n\nBrowning was born in Edna, Texas, grew up in Jackson County, Texas, and graduated from Southwest Texas State Teachers College in 1937, majoring in both math and physics. During World War II, he served in the U.S. Army Air Corps. Subsequently, he earned an M.A. at the University of Texas at Austin in 1947, and then his doctorate (Ph.D.) the following year at the same school. His doctorate was in zoology, with minors in genetics and bacteriology.\n\nBrowning worked in various scientific fields, including artificial intelligence and bio-engineering, and eventually became interested in long-term weather forecasting and climate changes. He believed that climate fluctuations are caused by changes in the amount of particulate matter in the atmosphere mostly from volcanic activity. He believed that volcanic activity can be triggered by land tidal forces caused by the Moon, Earth's elliptical orbit of the Sun, and the alignment of these three bodies. His climate predictions assumed that the dust thrown into the atmosphere by those eruptions reflects sunlight, which results in climatic cooling. Browning believed that climatic changes, especially cooling, are associated with increased troubles in human society, including famine, revolutions, and war.\n\nAfter founding \"The Browning Newsletter\" in 1974, Browning described his climatic theories and findings in \"Climate and the Affairs of Men\" (1975), which he co-authored with Nels Winkless III. At that time, he believed that Earth had been through a long warm period and was moving into a dangerous cooling phase. He also declared that he had not detected any effect of human activity on the climate.\n\nBrowning received notoriety for his erroneous prediction that a major earthquake would occur on the New Madrid Fault around December 2 and 3, 1990. This prediction had no scientific legitimacy, and was largely ignored by credentialed seismologists, who thought it would give the prediction undeserved attention if they had debunked it in public. In spite of this it was widely reported in the national media, which promoted fear, anxiety, and hysteria among residents of the Mississippi Valley. No earthquake occurred in that area on those dates. A study done by the USGS to understand the causes of the earthquake scare described Browning's methodology as pseudoscience.\n\nBrowning wrote four books, held 90 patents, and served as a climatologist and business consultant to Paine Webber in various scientific and engineering fields. He was married to the former Florence Pinto and had one daughter, Evelyn Browning-Garriss., who succeeded him as editor of 'The Browning Newsletter'. He lived his later years in Albuquerque, New Mexico and died at his home there on July 18, 1991 from a heart attack at the age of 73.\n\nNotes\n"}
{"id": "47402328", "url": "https://en.wikipedia.org/wiki?curid=47402328", "title": "Index of branches of science", "text": "Index of branches of science\n\nScience (from Latin \"scientia\", meaning \"knowledge\") is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.\n\nModern science is typically divided into three major branches that consist of the natural sciences (e.g. biology, chemistry, physics), which study nature in the broadest sense; the social sciences (e.g. psychology, sociology, economics) which study people and societies; and the formal sciences (e.g. mathematics, logic, theoretical computer science), which study abstract concepts. There is disagreement, however, on the formal sciences being a science as they do not rely on empirical evidence. Disciplines that use science, such as engineering and medicine, are described as applied sciences.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "185493", "url": "https://en.wikipedia.org/wiki?curid=185493", "title": "Informal mathematics", "text": "Informal mathematics\n\nInformal mathematics, also called naïve mathematics, has historically been the predominant form of mathematics at most times and in most cultures, and is the subject of modern ethno-cultural studies of mathematics. The philosopher Imre Lakatos in his \"Proofs and Refutations\" aimed to sharpen the formulation of informal mathematics, by reconstructing its role in nineteenth century mathematical debates and concept formation, opposing the predominant assumptions of mathematical formalism. Informality may not discern between statements given by \"inductive reasoning\" (as in approximations which are deemed \"correct\" merely because they are useful), and statements derived by \"deductive reasoning\".\n\n\"Informal mathematics\" means any informal mathematical practices, as used in everyday life, or by aboriginal or ancient peoples, without historical or geographical limitation. Modern mathematics, exceptionally from that point of view, emphasizes formal and strict proofs of all statements from given axioms. This can usefully be called therefore \"formal mathematics\". Informal practices are usually understood intuitively and justified with examples—there are no axioms. This is of direct interest in anthropology and psychology: it casts light on the perceptions and agreements of other cultures. It is also of interest in developmental psychology as it reflects a naïve understanding of the relationships between numbers and things. Another term used for informal mathematics is folk mathematics, which is ambiguous; the mathematical folklore article is dedicated to the usage of that term among professional mathematicians.\n\nThe field of naïve physics is concerned with similar understandings of physics. People use mathematics and physics in everyday life, without really understanding (or caring) how mathematical and physical ideas were historically derived and justified.\n\nThere has long been a standard account of the development of geometry in ancient Egypt, followed by Greek mathematics and the emergence of deductive logic. The modern sense of the term \"mathematics\", as meaning only those systems justified with reference to axioms, is however an anachronism if read back into history. Several ancient societies built impressive mathematical systems and carried out complex calculations based on proofless heuristics and practical approaches. Mathematical facts were accepted on a pragmatic basis. Empirical methods, as in science, provided the justification for a given technique. Commerce, engineering, calendar creation and the prediction of eclipses and stellar progression were practiced by ancient cultures on at least three continents. N.C. Ghosh included Informal Mathematics in the list of Folk Mathematics.\n\n"}
{"id": "43335285", "url": "https://en.wikipedia.org/wiki?curid=43335285", "title": "Information Technology Park, Nepal", "text": "Information Technology Park, Nepal\n\nThe Information Technology Park (commonly known as Info Tech Park or IT Park), Nepal's only Information Technology Park, is situated between Banepa and Panauti of Kavrepalanchowk District, Nepal, although it comes under Panauti Municipality. It lies about northeast of the capital city Kathmandu.\n\nThe Information Technology Park was completed in April 2003 with a total investment of NRs. 270 million (~ million USD). The initiation of the Park was formally done by an American company IBM but it left after nine months. Javra Software Company from the Netherlands had also started work but it too left, stating technical reasons and shifted to Kathmandu. By July 2014, it has not come into operation, as a result of which the Government of Nepal is missing millions of rupees in revenue. The government has formed a department named the Department of Information Technology for the socio-economic transformation to form a developed country.\n\nThe IT Park was established under the Ministry of Industry, for the development and promotion of information technology and services in the country. It has been said to be providing employment opportunities to about 15,000 technicians after the full range of operations of the IT Park. It has aimed to provide government services by maximizing the use of information and technology.\n\nThe Park has been distributed within the area of 235 Ropanis () consisting of a commercial building, an administrative building, four residential buildings and a security building built by investing NRs. 500 million (~ million USD).\n"}
{"id": "34407922", "url": "https://en.wikipedia.org/wiki?curid=34407922", "title": "Institute of Aviation, Warsaw", "text": "Institute of Aviation, Warsaw\n\nThe Institute of Aviation or Warsaw Institute of Aviation (Polish \"Instytut Lotnictwa\") is a research and development center established in 1926, located in Warsaw, Poland.\n\n"}
{"id": "67752", "url": "https://en.wikipedia.org/wiki?curid=67752", "title": "Insulin potentiation therapy", "text": "Insulin potentiation therapy\n\nInsulin potentiation therapy (IPT) is an unproven alternative cancer treatment using insulin as an adjunct to low-dose chemotherapy.\n\nAccording to Quackwatch, \"Insulin Potentiation Therapy (IPT) is one of several unproven, dangerous treatments that is promoted by a small group of practitioners without trustworthy evidence that it works.\"\n\nIt was developed by Donato Perez Garcia, MD in 1930. Originally, Garcia targeted syphilis, and later tried the treatment for chronic degenerative diseases and some types of cancer.\n\nGenerally, a dose of insulin is injected into a vein, followed by a low dose of chemotherapy drugs when the insulin has been absorbed. The chemotherapy dose is usually 10% to 25% of the proven dose. Then sugar water is injected to stop the hypoglycemia (low blood sugar) caused by the insulin injection.\n\nIPT has not been proven to work. Long-term outcomes, such as survival, have never been published. Four individual case studies, one small, uncontrolled clinical trial and one small prospective, randomized controlled trial have shown temporary reductions in the size of tumors for some patients.\n\nThe immediate risk is hypoglycemia.\n\nThe use of lower than normal doses of chemotherapy can cause drug resistance, which could make future treatment at standard, proven doses ineffective. For some cancers, especially breast and colon cancers, insulin may promote tumor growth.\n\nTwo main ideas about how it might work have been proposed. The first idea is that insulin makes cells more permeable, so that the chemotherapy drugs are absorbed faster into cells. The other idea is that insulin might cause the cells to start dividing, which makes them more susceptible to destruction of many cytotoxic chemotherapy drugs.\n\nCosts run up to US $2,000 per treatment session. Multiple sessions are normal. Patients often pay the full cost out of pocket, because it is an unproven therapy that is not covered by health insurance.\n\n"}
{"id": "58839014", "url": "https://en.wikipedia.org/wiki?curid=58839014", "title": "J-phenomenon", "text": "J-phenomenon\n\nThe J-phenomenon was a hypothetical form of X-ray behaviour similar to X-ray fluorescence. It was reported and studied by C. G. Barkla but other scientists were not persuaded that that this was a different mechanism from other known effects such as Compton scattering and so the theory was not successful.\n"}
{"id": "14004644", "url": "https://en.wikipedia.org/wiki?curid=14004644", "title": "Laboratory Life", "text": "Laboratory Life\n\nLaboratory Life: The Construction of Scientific Facts is a 1979 book by sociologists of science Bruno Latour and Steve Woolgar.\n\nThis influential book in the field of science studies presents an anthropological study of Roger Guillemin's scientific laboratory at the Salk Institute. It advances a number of observations regarding how scientific work is conducted, including descriptions of the complex relationship between the routine lab practices performed by scientists, the publication of papers, scientific prestige, research finances and other elements of laboratory life.\n\nThe book is considered to be one of the most influential works in the laboratory studies tradition within Science and Technology Studies. It is inspired but not entirely dependent on the ethnomethodological approach. In turn, it served as the inspiration for Actor-Network Theory (or ANT); many of ANT's core concepts (like transcription, inscription, translation, and the deployment of networks) are present in \"Laboratory Life\".\n\nLatour and Woolgar state that their work \"concerns the way in which the daily activities of working scientists lead to the construction of scientific facts\" (40). \"Laboratory Life\" therefore stands in opposition to the study of scandalous moments in which the so-called \"normal\" operation of science was disrupted by external forces. In contrast, Latour and Woolgar give an account of a how scientific facts are produced in a laboratory \"in situ\", or as it happens.\n\nThe initial methodology of \"Laboratory Life\" involves an \"anthropological strangeness\" (40) in which the laboratory is a tribe foreign to the researcher. The study of the lab begins with a semi-fictionalized account of an ignorant observer who knows nothing of laboratories or scientists. In this account, Latour and Woolgar \"bracket\" (44) their previous knowledge of scientific practice and ironically ask seemingly-nonsensical questions about observed practices in the laboratory, such as \"Are the heated debates in front of the blackboard part of some gambling contest?\" In the asking and answering of these questions, the observer's understanding of laboratory practices is gradually refined, leading to a strong focus on the significance of paper documents.\n\nThe observer soon recognizes that all the scientists and technicians in the lab write in some fashion, and that few activities in the lab are not connected to some sort of transcription or inscription. The foreign observer describes the laboratory as \"strange tribe\" of \"compulsive and manic writers ... who spend the greatest part of their day coding, marking, altering, correcting, reading, and writing\" (48-9). Large and expensive laboratory equipment (such as bioassays or mass spectrometers) are interpreted as \"inscription device[s]\" that have the sole purpose of \"transform[ing] a material substance into a figure or diagram\" (51). In this way, the observer works to organize and systematize the laboratory such that it \"began to take on the appearance of a system of literary inscription\" (52).\n\nHaving concluded that the \"production of papers\" for publication in a scientific journal is the primary focus of a laboratory, the observer next aims to \"consider papers as objects in much the same way as manufactured goods\" (71). This involves asking how papers are produced, what their constituent elements (or raw materials) are, and why these papers are so important. First, the authors recognize that in papers, \"some statements appeared more fact-like than others\" (76). From this observation, a five-element continuum of facticity is constructed, which spans from type 5 statements which are taken for granted to type 1 statements which are unqualified speculations, with various intermediate levels in between. The conclusion reached is that statements in a laboratory routinely travel up and down this continuum, and the main purpose of a laboratory is to take statements of one level of facticity and transform them to another level.\n\nHowever, Latour and Woolgar recognize that this semi-fictionalized account of an ignorant observer aiming to systematize the alien laboratory has several problems. While the observer's rich descriptions of activity in the lab are taken as accurate, the observer has not established that the interpretation of this data in terms of literary inscription is exhaustive or the only way in which laboratory life can be analyzed. In the authors' words, the observer's account is not \"immune from all possibility of future qualification\" (88).\n\nThe next chapter aims at giving a precise account of the way in which this process operates with respect to a single scientific fact: the peptide TRF(H). This historical account, which Latour and Woolgar admit is, like all histories, a \"necessarily literary fiction\" (107), has the ostensible purpose of qualifying the initial account given by the observer. To this end, the chapter focuses on the specific way in which TRF(H) was constructed as a fact, describing how one scientist, Guillemin, \"redefine[d] the TRF subspecialty solely in terms of determining the structure of the substance\" (119). As sequencing TRF(H) required far more sophisticated equipment and techniques than merely determining its physiological effects, Guillemin raised the cost of entry to this field and cut his potential competitors by three-fourths.\n\nThe authors next claim that the fact regarding TRF(H)'s structure progressed by decreases in the number of \"'logically' possible alternatives\" (146). However, Latour and Woolgar critique the explanation that \"logic\" or \"deduction\" is a satisfactory and complete explanation for the specific way in which a scientific fact is constructed. Instead, as their historical account of TRF(H) shows, the \"list of possible alternatives by which we can evaluate the logic of a deduction is sociologically (rather than logically) determined\" (136). Specifically, the material, technical, and human resources of a laboratory affected what kinds of challenges and counter-facts could be constructed and formulated, leading Latour and Woolgar to later conclude that \"the set of statements considered too costly to modify constitute what is referred to as reality\" (243).\n\nIn the previous section, Latour and Woolgar used a semi-fictional observer to describe the laboratory as a literary system in which mere statements are turned into facts and vice versa. The most sound and established facts were those statements which could be divorced from their contingent circumstances. The authors next aim to interrogate how this process operates on a very small and specific scale by looking at how this process operated with respect to the molecule TRF(H), whose molecular structure went through various stages of facticity both in and out of the laboratory Latour studied. In this section, Latour and Woolgar aim to \"specify the precise time and place in the process of fact construction when a statement became transformed into a fact and hence freed from the circumstances of its production\" (105).\n\nInstead of trying to construct a \"precise chronology\" of what \"really happened,\" in the field, they aim to demonstrate how \"a hard fact can be sociologically deconstructed\" (107) by showing how it emerged in what they call a network. A \"network\" is \"a set of positions within which an object such as TRF has meaning\" (107), and they recognize that TRF only has meaning within certain networks. For example, outside of the network of post-1960s endocrinology, TRF is \"an unremarkable white powder\" (108), which leads to the claim that a \"well-established fact loses its meaning when divorced from its context\" (110). Latour and Woolgar stress that \"to say that TRF is constructed is not to deny its solidity as a fact. Rather, it is to emphasize how, where, and why it was created\" (127).\n\nThis chapter turns back from grander historical accounts to the micro details of laboratory life. Through analysis of the conversations and discussions between scientists at the lab, it shows that the grander notion of science as a debate of contrasting ideas influences actual scientists only through social mechanisms. Instead of attempting to do their studies more carefully to be sure they get the right answer, scientists appear to only use as much care as they think will be necessary to defeat the counterarguments of their detractors and get the acclamation they desire for their work.\n\nIt also notes that the stories scientists tell about the history of their field often omit social and institutional factors in favor of \"moment of discovery\" narratives. For example, one scientist tells this story:\n\nThis story is contrasted with another story based on interviews with the participants: The University of California required that graduate students get credits in a field totally unrelated to their own. Sara, one of Slovik's students, fulfilled this requirement by taking selenium studies, since it had a vague relation to her major. Graduate students had a tradition of informal seminars where they discussed these unrelated classes. At one meeting, Sara presented a paper on the effects of Selenium on cancer and noted that someone on campus proposed that the geographical distribution of selenium content in water might correlate with the geographical distribution of cancer rates. Slovik was at the meeting and thought that this might explain the geographical difference in his assay working. He phoned a colleague to tell him the idea and ask him to test the selenium in the water.\n\nOne story says merely that Slovik \"got the idea\"—the other notes that institutions (the University, grad student meetings) and other people (Sara, the colleague) provided key pieces of the inspiration.\n\nThe chapter closes by arguing that scientists do not simply use their inscription devices to discover already-existing entities. Instead, they project new entities out of the analysis of their inscriptions. Statements to the effect that \"it's amazing they were able to discover it\" only make sense when one ignores the arduous process to construct the discovery out of the inscriptions available. Similarly, justifications that the discovery is valid because it works well outside the laboratory are fallacious. Any claims as to whether a new substance like TRF works are only valid in a laboratory context (or its extension) -- the only way one can know that the substance is actually TRF (and thus that TRF is working) is through laboratory analysis. However, the authors stress that they are not relativists—they simply believe that the social causes of statements should be investigated.\n\nScientists frequently explain their choice of field by referring to curves of interest and development, as in \"peptide chemistry [is] tapering off ... but now ... this is the future, molecular biology, and I knew that this lab would move faster to this new area\" (191). Desire for credit appears to only be a secondary phenomenon; instead a kind of \"credibility capital\" seems to be the driving motive. In a case study, they show one scientist sequentially choosing a school, a field, a professor to study under, a specialty to get expertise in, and a research institution to work at, by maximizing and reinvesting this credibility (i.e. ability to do science), despite not having received much in the way of credit (e.g. awards, recognition).\n\nFour examples: (a) X threatens to fire Ray if his assay fails, (b) a number of scientists flood into a field with theories after a successful experiment then leave when new evidence disproves their theories, (c) Y supports the results of \"a big shot in his field\" when others question them in order to receive invitations to meetings from the big shot where Y can meet new people, (d) K dismisses some of L's results on the grounds that \"good people\" won't believe them unless the level of noise is reduced (as opposed to K thinking them unreliable himself).\n\nThe credibility of a scientist and their results is largely seen as identical. \"For a working scientist, the most vital question is not 'Did I repay my debt in the form of recognition because of the good paper he wrote?' but 'Is he reliable enough to be believed? Can I trust him/his claim? Is he going to provide me with hard facts?'\" (202) CVs are the major way this credibility is proven and career trajectories are the story of its use. Technicians and minor leaguers, by contrast, do not accumulate capital but instead are paid a \"salary\" by major leaguers.\n\n\nThe preface to the second edition (1986) reads:\nSo \"social construction\" becomes just \"construction of scientific facts\". This change indicates a shift from social constructivism to Actor-network theory, which leaves more room for the non-social or 'natural' (albeit in a non-naturalistic / non-essentialist sense).\n\n\n"}
{"id": "49681713", "url": "https://en.wikipedia.org/wiki?curid=49681713", "title": "List of rectores magnifici of Leiden University", "text": "List of rectores magnifici of Leiden University\n\nThis is a list of chancellors (\"rectores magnifici\") of Leiden University, as from 1575. Three Nobel laureates are among these chancellors: Hendrik Lorentz, Heike Kamerlingh Onnes and Willem Einthoven.\n\n\n\n"}
{"id": "25691654", "url": "https://en.wikipedia.org/wiki?curid=25691654", "title": "List of scientists whose names are used as non SI units", "text": "List of scientists whose names are used as non SI units\n\nMost of the physical units are named after great scientists. By this convention, their names are immortalised. The scientists whose names are used in SI units are listed in List of scientists whose names are used as SI units. Below is the list of the scientists whose names are used in non-SI units.\n\n"}
{"id": "39498799", "url": "https://en.wikipedia.org/wiki?curid=39498799", "title": "Los Angeles Science Teachers Network", "text": "Los Angeles Science Teachers Network\n\nLos Angeles Science Teachers Network is a professional development network for science education by science teachers for science teachers committed to creating the most engaging hands-on curriculum possible through inquiry-based learning and scientific literacy.\n\nThe first meeting was held in Lisa Ellen Niver's home in September 2009. In October 2013, the professional development network had its fifteenth session. Over seventy teachers and forty schools have been involved to date. In \"Westside Today\", information about teaching, science and LASTN was presented. In the Bill & Melinda Gates Foundation \"Impatient Optimists\" article, \"Teachers Need A Village,\" the importance of teachers needing support from groups like LASTN is explored. \"Why So Many Of America's Teachers Are Leaving The Profession?\" cites Los Angeles Science Teachers Network as an example to teachers everywhere to find support and stay involved.\n\nSeveral teachers formed the core group and served as advisors to Niver over the first three years: Susan Bagdasarian, Sasha Moore, Joseph Rose and Judy Weiskopf. Niver was frequently asked about teaching science, she has been quoted in Parenting.com, PBS.org, \"Green Living Arizona\" (in print), the \"Huffington Post\" and on National television. While Niver is on sabbatical with We Said Go Travel, Susan Bagdasarian has been the director of the network.\n\n\n\n\n2014-2015\n\n2013-2014\n\n2012-2013\n2011-2012\niPad apps and hands-on labs\n2010-2011\n2009-2010\n"}
{"id": "39140116", "url": "https://en.wikipedia.org/wiki?curid=39140116", "title": "MDRC", "text": "MDRC\n\nMDRC is a nonprofit, nonpartisan education and social policy research organization based in New York City and Oakland, CA. MDRC mounts large-scale demonstrations and uses randomized controlled trials to measure the effects of social and educational policy initiatives. MDRC is led by President Gordon Berlin and Senior Vice Presidents Jesús Amadeo and Robert Ivry.\n\nIn 1974 the Ford Foundation and six government agencies together created the Manpower Demonstration Research Corporation. Its purpose was to implement and document the results of new programs intended to help the poor. In the 1980s and 1990s, it became well known for its evaluations of state welfare-to-work programs. It formally adopted \"MDRC\" as its registered corporate identity in 2003.\n\nMDRC evaluates and pilot-tests programs in six main areas:\n\nMDRC helped pioneer the use of random assignment to test social programs. Its evaluations of welfare work programs influenced the welfare reform of the 1990s. In the 1990s and 2000s, MDRC’s evaluation of the Career Academies high school reform model, which showed impacts on participants’ earnings eight years after graduation, influenced the expansion of the model around the nation. MDRC was the intermediary for the first social impact bond demonstration in the United States, a project to reduce recidivism among 16- to 18-year-olds incarcerated at Rikers Island. Most recently, MDRC's study of the City University of New York's Accelerated Study in Associate Programs (ASAP) has demonstrated that the program has doubled the three-year graduation rate of students who begin college requiring remedial education. \n\n"}
{"id": "8956567", "url": "https://en.wikipedia.org/wiki?curid=8956567", "title": "Material World (radio programme)", "text": "Material World (radio programme)\n\nMaterial World was a weekly science magazine programme on BBC Radio 4 broadcast on a Thursday afternoon. The programme's regular presenter was Quentin Cooper, with contributions from scientists researching areas under discussion in each programme.\n\nThe programme began as \"The Material World\" in April 1998. It was presented by Trevor Phillips, a chemistry graduate of Imperial College. In September 2000 Phillips was told that he could no longer work at the BBC due to his close links with the Labour Party, which broke BBC rules of impartiality. He was one of the few regular black broadcasters on Radio 4. The programme was presented by Quentin Cooper from 2000 to its end in 2013.\n\nMaterial World was one of the BBC's main conduits for up-to-date scientific news, along with \"Frontiers\", \"Science in Action\", and \"Bang Goes the Theory\".\n\nFrom 5 April 2010 the programme was repeated on a Monday evening at 21.00, in the former slot of \"Costing the Earth\". For a short time, when programmes on 5 Live began webstreaming with video, \"Material World\" was also webcast.\n\nOn 14 June 2013 it was announced that the show was to be cancelled, to be replaced by a new show, \"Inside Science\". The last programme presented by Quentin Cooper was broadcast on 20 June 2013 with the final episode airing a week later on 27 June 2013, presented by Gareth Mitchell.\n\nA typical episode programme covered three or four topics, giving each 7-10 minutes. For many years the programme was divided into two sections of fifteen minutes on separate topics. It took the form of interviewing a guest scientist or engineer. Cooper often ended the programme with a terrible scientific pun.\n\nMany past programmes are available for online listening via the programme's website. Some sequential sets of programmes were made in collaboration with the Open University.\n\n\n"}
{"id": "50405438", "url": "https://en.wikipedia.org/wiki?curid=50405438", "title": "Mdx mouse", "text": "Mdx mouse\n\nThe mdx mouse is a popular model for studying Duchenne muscular dystrophy (DMD).\n\nThe mdx mouse has a point mutation in its DMD gene, changing the amino acid coding for a glutamine to STOP codon. This causes the muscle cells to produce a small, nonfunctional dystrophin protein. As a result, the mouse has a mild form of DMD where there is increased muscle damage and weakness.\n"}
{"id": "188094", "url": "https://en.wikipedia.org/wiki?curid=188094", "title": "Nikolaas Tinbergen", "text": "Nikolaas Tinbergen\n\nNikolaas \"Niko\" Tinbergen (; ; 15 April 1907 – 21 December 1988) was a Dutch biologist and ornithologist who shared the 1973 Nobel Prize in Physiology or Medicine with Karl von Frisch and Konrad Lorenz for their discoveries concerning organization and elicitation of individual and social behavior patterns in animals. He is regarded as one of the founders of modern ethology, the study of animal behavior.\n\nIn 1951, he published \"The Study of Instinct\", an influential book on animal behaviour.\nIn the 1960s, he collaborated with filmmaker Hugh Falkus on a series of wildlife films, including \"The Riddle of the Rook\" (1972) and \"Signals for Survival\" (1969), which won the Italia prize in that year and the American blue ribbon in 1971.\n\nBorn in The Hague, Netherlands, he was one of five children of Dirk Cornelis Tinbergen and his wife Jeannette van Eek. His brother, Jan Tinbergen, won the first Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel in 1969. They are the only siblings to each win a Nobel Prize. Another brother, Luuk Tinbergen was also a noted biologist.\n\nTinbergen's interest in nature manifested itself when he was young. He studied biology at Leiden University and was a prisoner of war during World War II. Tinbergen's experience as a prisoner of the Nazis led to some friction with longtime intellectual collaborator Konrad Lorenz, and it was several years before the two reconciled.\n\nAfter the war, Tinbergen moved to England, where he taught at the University of Oxford and was a fellow first at Merton College, Oxford and later at Wolfson College, Oxford. Several of his graduate students went on to become prominent biologists including Richard Dawkins, Marian Dawkins, Desmond Morris, Iain Douglas-Hamilton, and Tony Sinclair.\n\nIn 1951 Tinbergen's \"The Study of Instinct\" was published. Behavioural ecologists and evolutionary biologists still recognise the contribution this book offered the field of behavioural science studies. \"The Study of Instinct\" summarises Tinbergen's ideas on innate behavioural reactions in animals and the adaptiveness and evolutionary aspects of these behaviours. By behaviour, he means the total movements made by the intact animal; innate behaviour is that which is not changed by the learning process. The major question of the book is the role of internal and external stimuli in controlling the expression of behaviour.\n\nIn particular, he was interested in explaining 'spontaneous' behaviours: those that occurred in their complete form the first time they were performed and that seemed resistant to the effects of learning. He explains how behaviour can be considered a combination of these spontaneous behaviour patterns and as set series of reactions to particular stimuli. Behaviour is a reaction in that to a certain extent it is reliant on external stimuli, however it is also spontaneous since it is also dependent upon internal causal factors.\n\nHis model for how certain behavioural reactions are provoked was based on work by Konrad Lorenz. Lorenz postulated that for each instinctive act there is a specific energy which builds up in a reservoir in the brain. In this model, Lorenz envisioned a reservoir with a spring valve at its base that an appropriate stimulus could act on, much like a weight on a scale pan pulling against a spring and releasing the reservoir of energy, an action which would lead an animal to express the desired behaviour.\n\nTinbergen added complexity to this model, a model now known as Tinbergen's hierarchical model. He suggested that motivational impulses build up in nervous centres in the brain which are held in check by blocks. The blocks are removed by an innate releasing mechanism that allows the energy to flow to the next centre (each centre containing a block that needs to be removed) in a cascade until the behaviour is expressed. Tinbergen's model shows multiple levels of complexity and that related behaviours are grouped.\n\nAn example is in his experiments with foraging honey bees. He showed that honey bees show curiosity for yellow and blue paper models of flowers, and suggested that these were visual stimuli causing the buildup of energy in one specific centre. However, the bees rarely landed on the model flowers unless the proper odour was also applied. In this case, the chemical stimuli of the odour allowed the next link in the chain to be released, encouraging the bee to land. The final step was for the bee to insert its mouthparts into the flower and initiate suckling. Tinbergen envisioned this as concluding the reaction set for honey bee feeding behaviour.\n\nIn 1973 Tinbergen, along with Konrad Lorenz and Karl von Frisch, were awarded the Nobel Prize in Physiology or Medicine \"for their discoveries concerning organization and elicitation of individual and social behaviour patterns\". The award recognised their studies on genetically programmed behaviour patterns, their origins, maturation and their elicitation by key stimuli. In his Nobel Lecture, Tinbergen addressed the somewhat unconventional decision of the Nobel Foundation to award the prize for Physiology or Medicine to three men who had until recently been regarded as \"mere animal watchers\". Tinbergen stated that their revival of the \"watching and wondering\" approach to studying behaviour could indeed contribute to the relief of human suffering.\n\nThe studies performed by the trio on fish, insects and birds laid the foundation for further studies on the importance of specific experiences during critical periods of normal development, as well as the effects of abnormal psychosocial situations in mammals. At the time, these discoveries were stated to have caused \"a breakthrough in the understanding of the mechanisms behind various symptoms of psychiatric disease, such as anguish, compulsive obsession, stereotypic behaviour and catatonic posture\". Tinbergen’s contribution to these studies included the testing of the hypotheses of Lorenz/von Frisch by means of \"comprehensive, careful, and ingenious experiments\" as well as his work on supernormal stimuli. The work of Tinbergen during this time was also regarded as having possible implications for further research in child development and behaviour.\n\nHe also caused some intrigue by dedicating a large part of his acceptance speech to FM Alexander, originator of the Alexander technique, a method which investigates postural reflexes and responses in human beings.\n\nIn 1950 Tinbergen became member of the Royal Netherlands Academy of Arts and Sciences. He was elected a Fellow of the Royal Society (FRS) in 1962. He was also awarded the Godman-Salvin Medal in 1969 by the British Ornithologists' Union, and in 1973 received the Swammderdam Medal and Wilhelm Bölsche Medal (from the Genootschap ter bervordering van Natuur-, Genees- en Heelkunde of the University of Amsterdam and the Kosmos-Gesellschaft der Naturfreunde respectively).\n\nTinbergen described four questions he believed should be asked of any animal behaviour, which were:\n\n\nIn ethology and sociobiology, causation and ontogeny are summarised as the \"proximate mechanisms\", while adaptation and phylogeny are the \"ultimate mechanisms\". They are still considered as the cornerstone of modern ethology, sociobiology and transdisciplinarity in Human Sciences.\n\nA major body of Tinbergen's research focused on what he termed the supernormal stimulus. This was the concept that one could build an artificial object which was a stronger stimulus or releaser for an instinct than the object for which the instinct originally evolved. He constructed plaster eggs to see which a bird preferred to sit on, finding that they would select those that were larger, had more defined markings, or more saturated colour—and a dayglo-bright one with black polka dots would be selected over the bird's own pale, dappled eggs.\n\nTinbergen found that territorial male three-spined stickleback (a small freshwater fish) would attack a wooden fish model more vigorously than a real male if its underside was redder. He constructed cardboard dummy butterflies with more defined markings that male butterflies would try to mate with in preference to real females. The superstimulus, by its exaggerations, clearly delineated what characteristics were eliciting the instinctual response.\n\nAmong the modern works calling attention to Tinbergen's classic work is Deirdre Barrett's 2010 book, \"Supernormal Stimuli\".\n\nTinbergen applied his observational methods to the problems of autistic children. He recommended a \"holding therapy\" in which parents hold their autistic children for long periods of time while attempting to establish eye contact, even when a child resists the embrace. However, his interpretations of autistic behaviour, and the holding therapy that he recommended, lacked scientific support and the therapy is described as controversial and potentially abusive.\n\nSome of the publications of Tinbergen are:\n\nPublications about Tinbergen and his work:\n\nTinbergen was a member of the advisory committee to the Anti-Concorde Project and was also an atheist.\n\nTinbergen married Elisabeth Rutten and they had five children. Later in life he suffered depression and feared he might, like his brother Luuk, commit suicide. He was treated by his friend, whose ideas he had greatly influenced, John Bowlby. Tinbergen died on 21 December 1988, after suffering a stroke at his home in Oxford, England.\n"}
{"id": "3406503", "url": "https://en.wikipedia.org/wiki?curid=3406503", "title": "Outline of Earth sciences", "text": "Outline of Earth sciences\n\nThe following outline is provided as an overview of and topical guide to Earth science:\n\nEarth science – all-embracing term for the sciences related to the planet Earth. It is also known as geoscience, the geosciences or the Earth sciences, and is arguably a special case in planetary science, the Earth being the only known life-bearing planet.\n\nEarth science is a branch of the physical sciences which is a part of the natural sciences. It in turn has many branches.\n\nEcosphere – there are many subsystems that make up the natural environment (the planetary ecosystem or \"ecosphere\") of the Earth. Many of the subsystems are characterized as \"spheres\", coinciding with the shape of the planet. The four spheres (for which most of the other spheres are a subtype of) are the atmosphere, the biosphere, the hydrosphere and the geosphere. Listed roughly from outermost to innermost the named spheres of the Earth are:\n\n\nAtmospheric sciences – The study of the atmosphere, its processes, and interactions with other systems\n\nEnvironmental science – The integrated, quantitative, and interdisciplinary approach to the study of environmental systems.\n\n\nGeography – The science that studies the terrestrial surface, the societies that inhabit it and the territories, landscapes, places or regions that form it.\n\nGeoinformatics – The application of information science methods in geography, cartography, and geosciences\n\nOceanography – The study of the physical and biological aspects of the ocean\n\nPlanetary science – The study of planets (including Earth), moons, and planetary systems (in particular those of the Solar System) and the processes that form them.\n\nSoil science – The study of soil as a natural resource on the surface of the earth\n\nHistory of Earth science – history of the all-embracing sciences related to the planet Earth. Earth science, and all of its branches, are branches of physical science.\n\n\n\n"}
{"id": "44763458", "url": "https://en.wikipedia.org/wiki?curid=44763458", "title": "Particle mass analyser", "text": "Particle mass analyser\n\nParticle mass analyser is a measurement technique for classifying aerosol particles according to their mass-to-charge ratio.\n\nTechniques exist for classifying (selecting) aerosol particles in the sub 1,000 nm range according to electrical mobility using devices such as differential mobility analysers.\n\nIn electrical mobility measurement, aerosol particles are classified according to their aerodynamic drag-charge ratio. If the particle is non-spherical, the electrical-mobility diameter will not correspond to any measurable physical dimensions of the particle. (For a spherical particle, the electrical-mobility diameter will correspond to physically measurable diameter.)\n\nAn alternative technique classifies particles according to their mass-to-charge ratio, using opposing electrical and centrifugal forces. This allows the classifier to select particles of a specified mass-to-charge ratio independent of particle shape. \n\nFurther work on the technique used centrifugal and electrostatic forces to classify particles according to their mass-to-charge ratio.\n"}
{"id": "3792917", "url": "https://en.wikipedia.org/wiki?curid=3792917", "title": "Philosophy of technology", "text": "Philosophy of technology\n\nThe philosophy of technology is a sub-field of philosophy that studies the nature of technology and its social effects.\n\nPhilosophical discussion of questions relating to technology (or its Greek ancestor \"techne\") dates back to the very dawn of Western philosophy. The phrase \"philosophy of technology\" was first used in the late 19th century by German-born philosopher and geographer Ernst Kapp, who published a book titled \"Grundlinien einer Philosophie der Technik\".\n\nThe western term 'technology' comes from the Greek term \"techne\" (τέχνη) (art, or craft knowledge) and philosophical views on technology can be traced to the very roots of Western philosophy. A common theme in the Greek view of \"techne\" is that it arises as an imitation of nature (for example, weaving developed out of watching spiders). Greek philosophers such as Heraclitus and Democritus endorsed this view. In his Physics, Aristotle agreed that this imitation was often the case, but also argued that \"techne\" can go beyond nature and complete \"what nature cannot bring to a finish.\" Aristotle also argued that nature (\"physis\") and \"techne\" are ontologically distinct because natural things have an inner principle of generation and motion, as well as an inner teleological final cause. While \"techne\" is shaped by an outside cause and an outside \"telos\" (goal or end) which shapes it. Natural things strive for some end and reproduce themselves, while \"techne\" does not. In Plato's Timaeus, the world is depicted as being the work of a divine craftsman (Demiurge) who created the world in accordance with eternal forms as an artisan makes things using blueprints. Moreover, Plato argues in the Laws, that what a craftsman does is imitate this divine craftsman.\n\nDuring the period of the Roman empire and late antiquity authors produced practical works such as Vitruvius' \"De Architectura\" (1st century BC) and Agricola's \"De Re Metallica\" (1556). Medieval Scholastic philosophy generally upheld the traditional view of technology as imitation of nature. During the Renaissance, Francis Bacon became one of the first modern authors to reflect on the impact of technology on society. In his utopian work \"New Atlantis\" (1627), Bacon put forth an optimistic worldview in which a fictional institution (Salomon's House) uses natural philosophy and technology to extend man's power over nature - for the betterment of society, through works which improve living conditions. The goal of this fictional foundation is \"...the knowledge of causes, and secret motions of things; and the enlarging of the bounds of human empire, to the effecting of all things possible\".\n\nThe native German philosopher and geographer Ernst Kapp, who was based in Texas, published the fundamental book \"Grundlinien einer Philosophie der Technik\" in 1877. Kapp was deeply inspired by the philosophy of Hegel and regarded technique as a projection of human organs. In the European context, Kapp is referred to as the founder of the philosophy of technology.\n\nAnother, more materialistic position on technology which became very influential in the 20th-century philosophy of technology was centered on the ideas of Benjamin Franklin and Karl Marx.\n\nFive early prominent 20th-century philosophers to directly address the effects of modern technology on humanity were John Dewey, Martin Heidegger, Herbert Marcuse, Günther Anders and Hannah Arendt. They all saw technology as central to modern life, although Heidegger, Anders, Arendt and Marcuse were more ambivalent and critical than Dewey. The problem for Heidegger was the hidden nature of technology's essence, \"Gestell\" or \"Enframing\" which posed for humans what he called its greatest danger and thus its greatest possibility. Heidegger's major work on technology is found in \"The Question Concerning Technology\".\n\nContemporary philosophers with an interest in technology include Jean Baudrillard, Albert Borgmann, Andrew Feenberg, Langdon Winner, Donna Haraway, Avital Ronell, Brian Holmes, Don Ihde, Bruno Latour, Paul Levinson, Ernesto Mayz Vallenilla, Carl Mitcham, Leo Marx, Gilbert Simondon, Lewis Mumford, Jacques Ellul, Bernard Stiegler, Paul Virilio, Günter Ropohl, Nicole C. Karafyllis, Richard Sennett, and George Grant.\n\nWhile a number of important individual works were published in the second half of the twentieth century, Paul Durbin has identified two books published at the turn of the century as marking the development of the philosophy of technology as an academic subdiscipline with canonical texts. Those were \"Technology and the Good Life\" (2000), edited by Eric Higgs, Andrew Light, and David Strong and \"American Philosophy of Technology\" (2001) by Hans Achterhuis. Several collected volumes with topics in philosophy of technology have come out over the past decade and the journals \"\" (the journal of the Society for Philosophy and Technology, published by the Philosophy Documentation Center) and \"Philosophy & Technology\" (Springer) publish exclusively works in philosophy of technology. Philosophers of technology reflect broadly and work in the area and include interest on diverse topics of geoengineering, internet data and privacy, our understandings of internet cats, technological function and epistemology of technology, computer ethics, biotechnology and its implications, transcendence in space, and technological ethics more broadly.\n\nTechnological determinism is the idea that \"features of technology [determines] its use and the role of a progressive society was to adapt to [and benefit from] technological change.\" The alternative perspective would be social determinism which looks upon society being at fault for the \"development and deployment\" of technologies. Lelia Green used recent gun massacres such as the Port Arthur Massacre and the Dunblane Massacre to selectively show technological determinism and social determinism. According to Green, a technology can be thought of as a neutral entity only when the sociocultural context and issues circulating the specific technology are removed. It will be then visible to us that there lies a relationship of social groups and power provided through the possession of technologies.\n\n\n\n\n"}
{"id": "26180636", "url": "https://en.wikipedia.org/wiki?curid=26180636", "title": "Pioneers of Science", "text": "Pioneers of Science\n\nPioneers of Science () () is a book by Cemal Yıldırım which has run to 22 editions. The book explains the scientific method with anecdotes from significant figures in scientific history such as Albert Einstein, Niels Bohr and Marie Curie.\n"}
{"id": "7981638", "url": "https://en.wikipedia.org/wiki?curid=7981638", "title": "Richard Dawkins Foundation for Reason and Science", "text": "Richard Dawkins Foundation for Reason and Science\n\nThe Richard Dawkins Foundation for Reason and Science (RDFRS or RDF) is a division of Center for Inquiry (CFI) founded by British biologist Richard Dawkins in 2006 to promote scientific literacy and secularism.\n\nOriginally a non-profit based in Washington, D.C., the organization merged with CFI in 2016.\n\nAfter Richard Dawkins’ success with the book \"The God Delusion\", he created the foundation with its headquarters in the United States to work toward a world in which religion no longer interferes with the advance of science and in which people use their critical thinking skills to evaluate theist claims about the nature of reality.\n\nDawkins complained of the difficulty he faced in gaining tax-free status, which he attributes to the secular nature of the organization. In contrast to the presumption by officials that religious organizations benefit humanity without evidence (for instance Our Lady of Perpetual Exemption), he points to a letter he received from the British Charity Commission requesting evidence for the claim that the advancement of science is connected to the public good.\n\nTheist author Marion Ledwig suggests that the foundation may have been set up as an atheist counterpart to the John Templeton Foundation, an organization which Dawkins has publicly criticized, especially in \"The God Delusion\", for corrupting science. In a TED talk prior to writing \"The God Delusion\", Dawkins had called for the need for an \"anti-Templeton\" to step up, saying that if his books sold better, he would take the initiative himself.\n\nDawkins describes his foundation’s purpose this way:“Critical thinking is the real saviour of humankind. My foundation promotes respect for people who hold critical thinking as a cherished personal value and use it in day-to-day life. The logical counter to religious extremism is people who rely on evidence to make decisions. Yet the voice of secular people is maligned in this country. Forty-five percent of Americans think you have to believe in God to be moral. Secular voices are considered immoral. They are not listened to on that basis. We must counter this baseless hostility to allow the contributions of secular people in vital national debates to count. Making secular views and people welcome in politics and policy-making will advance human safety, security, health, achievement, prosperity and most of all, \"science\".\"The organization began accepting members in April 2015.\nAmong its activities, RDFRS finances research into the psychology of belief and religion, finances scientific education programs and materials, and publicizes and supports secular charitable organisations.\n\nThe foundation has been granted charitable status in the United Kingdom and status as a 501(c)(3) nonprofit private operating foundation in the United States. RDFRS financed research on the psychology of belief and religion, financed scientific education programs and materials, and publicised and supported charitable organisations that are secular in nature. The foundation also offers humanist, rationalist, and scientific materials through its website.\n\nDawkins has said the \"trend toward theocratic thinking in the United States is a danger not only for America but for the entire world.\" Connected to this concern, Dawkins invited Sean Faircloth to serve as opening speaker on Dawkins's 2011 US book tour. Faircloth is author of the book \"Attack of the Theocrats, How the Religious Right Harms Us All and What We Can Do About It\". The Richard Dawkins Foundation (United States branch) later hired Faircloth, who has ten years experience as a state legislator, as Director of Strategy and Policy.\n\nRDFRS also invests in creating, producing and influencing the development of entertainment products for general audiences that support secularism and fight scientific illiteracy.\n\nIn March 2009, following proposed anti-evolution resolutions by Oklahoma State Representative Todd Thomsen, including condemning a visit by Dawkins to Oklahoma, he instructed the U.S. branch of the Richard Dawkins Foundation for Reason and Science to donate $5,000 to Oklahomans for Excellence in Science Education.\n\nIn March 2011, the RDFRS along with the Freedom From Religion Foundation began The Clergy Project which is a confidential on-line community that supports members as they move from their faith.\n\nIn 2014 RDFRS joined several similar organizations, including the Stiefel Freethought Foundation, the Secular Student Alliance, and the Secular Coalition for America, to form Openly Secular, an initiative which aims to combat and draw attention to anti-atheist discrimination and to encourage more people to openly self-identify as nonbelievers. Among the strategies is to get celebrities to come forward as openly secular. Videos have been posted by Penn & Teller, Bill Maher, NFL star Arian Foster, Julia Sweeney, John Davidson and others.\n\nIn April 2015, RDFRS launched the Teacher Institute for Evolutionary Science (TIES) to provide middle school teachers with workshops and free online tools to teach evolution and answer its critics. TIES is led by Bertha Vazquez, an award-winning middle school science teacher in Miami, FL.\n\nIn January 2016, RDFRS announced that it was merging with the Center for Inquiry, with Robyn Blumner as the CEO of the combined organizations.\n\nThe merger was completed in December 2016, with RDFRS becoming a division of CFI.\n\n\n"}
{"id": "10869139", "url": "https://en.wikipedia.org/wiki?curid=10869139", "title": "Science Week Ireland", "text": "Science Week Ireland\n\nScience Week Ireland is an annual week-long event in Ireland each November, celebrating science in our everyday lives. Science Week is an initiative of Science Foundation Ireland (SFI) It is the largest science festival in the country, engaging tens of thousands of members of the general public in workshops, science shows, talks, laboratory demonstrations, science walks and other science-related events. Science Week is a collaboration of events involving industry, colleges, schools, libraries, teachers, researchers and students throughout Ireland. \n\nScience Week supports Science Foundation Ireland’s mission to catalyse, inspire and guide the best in science, technology, engineering and maths (STEM) education and public engagement. The ultimate aim of this effort is that Ireland will have the most engaged and scientifically informed public by 2020 as outlined in Science Foundation Ireland’s strategy Agenda 2020. This also aligns to the national science innovation strategy, Innovation 2020.\n\nOver the years, Science Week Ireland has grown from a small pilot initiative to a large promotional and event engine to its current identity as a recognised vehicle for regional activity supported by a national promotional campaign. \nThe first Science Week in Ireland was held in 1996. It was organised by Forfás on behalf of the Office of Science and Technology at the Department of Jobs, Enterprise and Innovation under the name 'Information Technology and Science Week'. The week aimed to raise general awareness of the benefits of science and information technology to people, young and old throughout society. In 1997 it was renamed as Science Week. Professional bodies, voluntary groups, colleges, businesses and the public sector combined to organise 50 events countrywide. Events included conferences, lectures, interactive exhibitions, debates, and competitions for primary school students. \n\nSFI took over Science Week from the Forfás Discover Science and Engineering programme in 2012. Science Week continued to grow and develop over the following years into what it represents today, a week-long celebration of STEM public engagement, enhancing the public’s interest in STEM and enabling them to see the relevance of STEM to their daily lives.\n\nScience Week 2007 took place between 11–18 November and the theme was \"Surrounded by Science\". The programme of events set out to illustrate that behind the everyday objects in our lives is a great inventor, scientist or engineer. This included a series of lectures which featured Craig Johnston, inventor of the Adidas Predator; Joe F. Edwards, Jr., former NASA astronaut; and Dr. Sheila Willis, Director of the Forensic Science Laboratory.\n\n2007 was the eleventh year of Science Week and saw an estimated 95,000 people attend lectures, exhibitions and workshops throughout the country.\n\nThe 2008 Science Week took place between 11–16 November. The theme was 'Science – Shaping Our World' which celebrates the International Year of Planet Earth. \n\nThe guest lecturers include Professor Aubrey Manning, distinguished zoologist and broadcaster; Gerry Johnston, director of Special Effects Ireland; Dr. Cynthia Breazeal, Associate Professor at Massachusetts Institute of Technology; Stephen Attenborough of Virgin Galactic; and Patrick Collison, Irish Young Scientist of the Year winner 2005. These can be viewed on YouTube.\n\nScience Week 2009 took place between 8–15 November. The theme was 'Science – Inspiring Creativity and Innovation’, which links to the European Year of Creativity and Innovation.\n\nIn the summer of 2009, DSE launched a Twitter account for the latest news on Science Week. \n\nA lecture series included speakers from the Tyndall National Institute, Cork and Sustainable Energy Ireland. These can be viewed on YouTube.\n\nScience Week 2010 ran from 7-14 November. Its theme was ‘Our Place in Space’, which explored the latest happenings in astronomy, Ireland’s role in the space industry, and the vital role played by science, technology, engineering and mathematics (STEM) in helping us to make sense of our universe.\n\nScience Week 2011 ran from 13–20 November. The theme was 'The Chemistry of Life', demonstrating the importance of chemistry to our everyday lives – from the atoms that are the building blocks of nature to the chemistry that creates lasting bonds between people.\n\nScience Week 2012 ran from 11–18 November. The theme was ‘Everyday Experimenting’, highlighting how we are all involved in science every day, carrying out scientific processes and experimenting, even when not aware of it.\n\nScience Week 2013 ran from 10-17 November. The theme was ‘Exploring the XTRA-Ordinary’, which called on the public to go ‘behind the scenes’ of everyday life and explore the extraordinary processes taking place in front of our eyes.\n\nAn estimated 250,000 people took part in science festivals, demonstrations, seminars and tours across the country during the 19th annual national Science Week, which took place from 9-16 November 2014. The theme was ‘Power of Science’. Over 800 events took place across Ireland, including science festivals in Sligo, Galway, Mayo, Dublin, Cork, Waterford and the Midlands, aiming to \"entertain, educate and enthral young and old alike with the power of science\". Jamie Heaslip acted as a Science Week ambassador.\n\n2015 marked the 20th anniversary of Science Week, which took place from 8-15 November. The theme was ‘Science Week 2.0 Design Your Future’. It celebrated how science empowers ‘you’ to ‘Design Your Future’. Numerous events were held in every county, and regional festivals took place in Mayo, Sligo, Galway, Waterford, Cork, Limerick and the midlands. \n\nScience Week 2016 took place from 13-20 November. The theme was ‘Science Rising’ which looks at how science is key to our success – it is part of our past, an important part of our present and there is endless potential still to be realised. \nScience Week 2016 reached more people, all over Ireland, across a wider demographic than had been achieved before. Science Week 2016 saw 10 Regional Festivals across the country. \n\nScience Week 2017 ran nationwide from 12-19 November 2017. The theme was ‘Believe in Science’. More than 1180 events took place across Ireland. 12 regional science festivals took place in Cavan/Monaghan, Cork, Carlow, Festival of Farming and Food (Teagasc), Galway, Kerry, Limerick, Mayo, the Midlands, Sligo, Southeast and Tipperary.\n\nA number of Science Foundation Ireland-funded Science Week events took place throughout the week, including events by the Ark Theatre, The British Council of Ireland, Dublinia, Dunsink Observatory, Foodoppi, Learnit Educational Solutions, the Science Gallery, ADAPT, The Rediscovery Centre and Wexford Co. Council. The Scintillating Science event with Dara O'Briain launched the beginning of Science Week in the National Concert Hall, and the Dublin Science Week Family Open Day at the Convention Centre Dublin was held at the end of the week. \n\nDuring Science Week 2017, SFI launched the #StopAndAsk social media campaign which calls on people to ask questions about the world around them. Science Foundation Ireland, its partners and the science community answered a selection of these questions throughout the week.\nA show reel of some highlights of Science Week 2017 can be viewed on YouTube.\n\nScience Week 2018 will run from 11-18 November 2018.\n\nThe SFI website outlines tips on how to get involved with Science Week in Ireland.\n\n"}
{"id": "5876843", "url": "https://en.wikipedia.org/wiki?curid=5876843", "title": "Science attaché", "text": "Science attaché\n\nA science attaché (also known as a scientific attaché or a technical attaché) is a member of a diplomatic mission, usually an embassy. A science attaché traditionally had three primary functions: advise the ambassador on scientific and technical matters, report scientific and technological events, and represent his or her country in scientific and technical matters to foreign scientific and technical academies, industry, and government bodies.\n\nA science attaché also helped forge formal ties between domestic and foreign scientists and researchers and acted as a catalyst for scientific exchange initiatives. The non-advising roles of the science attaché seem somewhat less important in the age of the internet and the truly international scientific community it has helped create.\n\nThe role of science attachés of the United States was first outlined in 1950 in a report entitled \"Science and Foreign Relations\", issued by the United States State Department. It listed the primary duties of science attachés as:\n\nThe modern trend seems to be to emphasize the advisory role of the science attaché over the facilitation of scientific and technical exchanges. As recently as 1998, the National Academy of Sciences called for the appointment of more science-savvy diplomats to the State Department to improve the quality of the scientific advice available to foreign policymakers. The panel also emphasized the need to encourage general foreign service staff to acquire scientific skills.\n\nWhile there has been more emphasis on the advisory role, science attachés could still play a role in facilitating exchanges and collaborations by helping scientists from their home country understand the host nation’s science culture and practices.\n\nFormerly, being appointed science attaché was viewed as the \"kiss of death\" for advancement within the foreign service. However, with the growing importance of scientific issues such as global warming, global infectious diseases, and bioterrorism to foreign policymaking and diplomacy, this perception may be changing.\n\n"}
{"id": "6663803", "url": "https://en.wikipedia.org/wiki?curid=6663803", "title": "Scientific priority", "text": "Scientific priority\n\nIn science, priority is the credit given to the individual or group of individuals who first made the discovery or propose the theory. Fame and honours usually go to the first person or group to publish a new finding, even if several researchers arrived at the same conclusion independently and at the same time. Thus between two or more independent discoverers, the first to make formal publication is the legitimate winner. Hence, the tradition is often referred to as the priority rule, the procedure of which is nicely summed up in a phrase \"publish or perish\", because there are no second prizes. In a way, the race to be first inspires risk-taking that can lead to scientific breakthroughs which is beneficial to the society (such as discovery of malaria transmission, DNA, HIV, etc.); on the other hand, it can create an unhealthy competition, thus, becoming detrimental to scientific progress.\n\nPriority becomes a difficult issue usually in the context of priority disputes, where the priority for a given theory, understanding, or discovery comes into question. In most cases historians of science disdain retrospective priority disputes as enterprises which generally lack understanding about the nature of scientific change and usually involve gross misreadings of the past to support the idea of a long-lost priority claim. Historian and biologist Stephen Jay Gould once remarked that \"debates about the priority of ideas are usually among the most misdirected in the history of science.\"\n\nRichard Feynman told Freeman Dyson that he avoided priority disputes by \"Always giv[ing] the bastards more credit than they deserve.\" Dyson remarked that he also follows this rule, and that this practice is \"remarkably effective for avoiding quarrels and making friends.\" \n\nThe priority rule came into existence before or as soon as modern scientific methods were established. For example, the earliest documented controversy was a bitter claim between Isaac Newton and Gottfried Wilhelm Leibniz in the 17th century about priority in the invention of calculus. This particular incidence clearly shows human biases and prejudice. It has become unanimously accepted that both the mathematicians independently developed calculus. Since then priority has caused a number of historical maladies in the history of science.\nIn the cases of scientists who have since achieved incredible levels of popularity, such as Charles Darwin and Albert Einstein, priority questions are often rooted in taking too seriously the myth of the \"lone genius\" which is often cultivated around such quasi-mythic figures (see Great Man theory and Whig history). In an attempt to laud such scientists as visionaries, the context in which they worked is often neglected by popularizers, making it appear as if they worked without assistance or without reference to other work, something which is rarely the case.\n\n\n"}
{"id": "198507", "url": "https://en.wikipedia.org/wiki?curid=198507", "title": "Scientific theory", "text": "Scientific theory\n\nA scientific theory is an explanation of an aspect of the natural world that can be repeatedly tested and verified in accordance with the scientific method, using accepted protocols of observation, measurement, and evaluation of results. Where possible, theories are tested under controlled conditions in an experiment. In circumstances not amenable to experimental testing, theories are evaluated through principles of abductive reasoning. Established scientific theories have withstood rigorous scrutiny and embody scientific knowledge.\n\nThe meaning of the term \"scientific theory\" (often contracted to \"theory\" for brevity) as used in the disciplines of science is significantly different from the common vernacular usage of \"theory\". In everyday speech, \"theory\" can imply an explanation that represents an unsubstantiated and speculative guess, whereas in science it describes an explanation that has been tested and widely accepted as valid. These different usages are comparable to the opposing usages of \"prediction\" in science versus common speech, where it denotes a mere hope.\n\nThe strength of a scientific theory is related to the diversity of phenomena it can explain and its simplicity. As additional scientific evidence is gathered, a scientific theory may be modified and ultimately rejected if it cannot be made to fit the new findings; in such circumstances, a more accurate theory is then required. That doesn’t mean that all theories can be fundamentally changed (for example, well established foundational scientific theories such as evolution, heliocentric theory, cell theory, theory of plate tectonics etc). In certain cases, the less-accurate unmodified scientific theory can still be treated as a theory if it is useful (due to its sheer simplicity) as an approximation under specific conditions. A case in point is Newton's laws of motion, which can serve as an approximation to special relativity at velocities that are small relative to the speed of light.\n\nScientific theories are testable and make falsifiable predictions. They describe the causes of a particular natural phenomenon and are used to explain and predict aspects of the physical universe or specific areas of inquiry (for example, electricity, chemistry, and astronomy). Scientists use theories to further scientific knowledge, as well as to facilitate advances in technology or medicine.\n\nAs with other forms of scientific knowledge, scientific theories are both deductive and inductive, aiming for predictive and explanatory power.\n\nThe paleontologist Stephen Jay Gould wrote that \"...facts and theories are different things, not rungs in a hierarchy of increasing certainty. Facts are the world's data. Theories are structures of ideas that explain and interpret facts.\"\n\nAlbert Einstein described two types of scientific theories: \"Constructive theories\" and \"principle theories\". Constructive theories are constructive models for phenomena: for example, kinetic energy. Principle theories are empirical generalisations such as Newton's laws of motion.\n\nTypically for any theory to be accepted within most academia there is one simple criterion. The essential criterion is that the theory must be observable and repeatable. The aforementioned criterion is essential to prevent fraud and perpetuate science itself. \n\nThe defining characteristic of all scientific knowledge, including theories, is the ability to make falsifiable or testable predictions. The relevance and specificity of those predictions determine how potentially useful the theory is. A would-be theory that makes no observable predictions is not a scientific theory at all. Predictions not sufficiently specific to be tested are similarly not useful. In both cases, the term \"theory\" is not applicable.\n\nA body of descriptions of knowledge can be called a theory if it fulfills the following criteria:\n\nThese qualities are certainly true of such established theories as special and general relativity, quantum mechanics, plate tectonics, the modern evolutionary synthesis, etc.\n\nIn addition, scientists prefer to work with a theory that meets the following qualities:\n\nThe United States National Academy of Sciences defines scientific theories as follows:\n\nThe formal scientific definition of theory is quite different from the everyday meaning of the word. It refers to a comprehensive explanation of some aspect of nature that is supported by a vast body of evidence. Many scientific theories are so well established that no new evidence is likely to alter them substantially. For example, no new evidence will demonstrate that the Earth does not orbit around the sun (heliocentric theory), or that living things are not made of cells (cell theory), that matter is not composed of atoms, or that the surface of the Earth is not divided into solid plates that have moved over geological timescales (the theory of plate tectonics)...One of the most useful properties of scientific theories is that they can be used to make predictions about natural events or phenomena that have not yet been observed.\n\nFrom the American Association for the Advancement of Science:\n\nA scientific theory is a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment. Such fact-supported theories are not \"guesses\" but reliable accounts of the real world. The theory of biological evolution is more than \"just a theory\". It is as factual an explanation of the universe as the atomic theory of matter or the germ theory of disease. Our understanding of gravity is still a work in progress. But the phenomenon of gravity, like evolution, is an accepted fact.\n\nNote that the term \"theory\" would not be appropriate for describing untested but intricate hypotheses or even scientific models.\n\nThe scientific method involves the proposal and testing of hypotheses, by deriving predictions from the hypotheses about the results of future experiments, then performing those experiments to see whether the predictions are valid. This provides evidence either for or against the hypothesis. When enough experimental results have been gathered in a particular area of inquiry, scientists may propose an explanatory framework that accounts for as many of these as possible. This explanation is also tested, and if it fulfills the necessary criteria (see above), then the explanation becomes a theory. This can take many years, as it can be difficult or complicated to gather sufficient evidence.\n\nOnce all of the criteria have been met, it will be widely accepted by scientists (see scientific consensus) as the best available explanation of at least some phenomena. It will have made predictions of phenomena that previous theories could not explain or could not predict accurately, and it will have resisted attempts at falsification. The strength of the evidence is evaluated by the scientific community, and the most important experiments will have been replicated by multiple independent groups.\n\nTheories do not have to be perfectly accurate to be scientifically useful. For example, the predictions made by classical mechanics are known to be inaccurate in the relatistivic realm, but they are almost exactly correct at the comparatively low velocities of common human experience. In chemistry, there are many acid-base theories providing highly divergent explanations of the underlying nature of acidic and basic compounds, but they are very useful for predicting their chemical behavior. Like all knowledge in science, no theory can ever be completely certain, since it is possible that future experiments might conflict with the theory's predictions. However, theories supported by the scientific consensus have the highest level of certainty of any scientific knowledge; for example, that all objects are subject to gravity or that life on Earth evolved from a common ancestor.\n\nAcceptance of a theory does not require that all of its major predictions be tested, if it is already supported by sufficiently strong evidence. For example, certain tests may be unfeasible or technically difficult. As a result, theories may make predictions that have not yet been confirmed or proven incorrect; in this case, the predicted results may be described informally with the term \"theoretical\". These predictions can be tested at a later time, and if they are incorrect, this may lead to the revision or rejection of the theory.\n\nIf experimental results contrary to a theory's predictions are observed, scientists first evaluate whether the experimental design was sound, and if so they confirm the results by independent replication. A search for potential improvements to the theory then begins. Solutions may require minor or major changes to the theory, or none at all if a satisfactory explanation is found within the theory's existing framework. Over time, as successive modifications build on top of each other, theories consistently improve and greater predictive accuracy is achieved. Since each new version of a theory (or a completely new theory) must have more predictive and explanatory power than the last, scientific knowledge consistently becomes more accurate over time.\n\nIf modifications to the theory or other explanations seem to be insufficient to account for the new results, then a new theory may be required. Since scientific knowledge is usually durable, this occurs much less commonly than modification. Furthermore, until such a theory is proposed and accepted, the previous theory will be retained. This is because it is still the best available explanation for many other phenomena, as verified by its predictive power in other contexts. For example, it has been known since 1859 that the observed perihelion precession of Mercury violates Newtonian mechanics, but the theory remained the best explanation available until relativity was supported by sufficient evidence. Also, while new theories may be proposed by a single person or by many, the cycle of modifications eventually incorporates contributions from many different scientists.\n\nAfter the changes, the accepted theory will explain more phenomena and have greater predictive power (if it did not, the changes would not be adopted); this new explanation will then be open to further replacement or modification. If a theory does not require modification despite repeated tests, this implies that the theory is very accurate. This also means that accepted theories continue to accumulate evidence over time, and the length of time that a theory (or any of its principles) remains accepted often indicates the strength of its supporting evidence.\n\nIn some cases, two or more theories may be replaced by a single theory that explains the previous theories as approximations or special cases, analogous to the way a theory is a unifying explanation for many confirmed hypotheses; this is referred to as \"unification\" of theories. For example, electricity and magnetism are now known to be two aspects of the same phenomenon, referred to as electromagnetism.\n\nWhen the predictions of different theories appear to contradict each other, this is also resolved by either further evidence or unification. For example, physical theories in the 19th century implied that the Sun could not have been burning long enough to allow certain geological changes as well as the evolution of life. This was resolved by the discovery of nuclear fusion, the main energy source of the Sun. Contradictions can also be explained as the result of theories approximating more fundamental (non-contradictory) phenomena. For example, atomic theory is an approximation of quantum mechanics. Current theories describe three separate fundamental phenomena of which all other theories are approximations; the potential unification of these is sometimes called the Theory of Everything.\n\nIn 1905, Albert Einstein published the principle of special relativity, which soon became a theory. Special relativity predicted the alignment of the Newtonian principle of Galilean invariance, also termed \"Galilean relativity\", with the electromagnetic field. By omitting from special relativity the luminiferous aether, Einstein stated that time dilation and length contraction measured in an object in relative motion is inertial—that is, the object exhibits constant velocity, which is speed with direction, when measured by its observer. He thereby duplicated the Lorentz transformation and the Lorentz contraction that had been hypothesized to resolve experimental riddles and inserted into electrodynamic theory as dynamical consequences of the aether's properties. An elegant theory, special relativity yielded its own consequences, such as the equivalence of mass and energy transforming into one another and the resolution of the paradox that an excitation of the electromagnetic field could be viewed in one reference frame as electricity, but in another as magnetism.\n\nEinstein sought to generalize the invariance principle to all reference frames, whether inertial or accelerating. Rejecting Newtonian gravitation—a central force acting instantly at a distance—Einstein presumed a gravitational field. In 1907, Einstein's equivalence principle implied that a free fall within a uniform gravitational field is equivalent to inertial motion. By extending special relativity's effects into three dimensions, general relativity extended length contraction into space contraction, conceiving of 4D space-time as the gravitational field that alters geometrically and sets all local objects' pathways. Even massless energy exerts gravitational motion on local objects by \"curving\" the geometrical \"surface\" of 4D space-time. Yet unless the energy is vast, its relativistic effects of contracting space and slowing time are negligible when merely predicting motion. Although general relativity is embraced as the more explanatory theory via \"scientific realism\", Newton's theory remains successful as merely a predictive theory via \"instrumentalism\". To calculate trajectories, engineers and NASA still uses Newton's equations, which are simpler to operate.\n\nBoth scientific laws and scientific theories are produced from the scientific method through the formation and testing of hypotheses, and can predict the behavior of the natural world. Both are typically well-supported by observations and/or experimental evidence. However, scientific laws are descriptive accounts of how nature will behave under certain conditions. Scientific theories are broader in scope, and give overarching explanations of how nature works and why it exhibits certain characteristics. Theories are supported by evidence from many different sources, and may contain one or several laws.\n\nA common misconception is that scientific theories are rudimentary ideas that will eventually graduate into scientific laws when enough data and evidence have been accumulated. A theory does not change into a scientific law with the accumulation of new or better evidence. A theory will always remain a theory; a law will always remain a law. Both theories and laws could potentially be falsified by countervailing evidence.\n\nTheories and laws are also distinct from hypotheses. Unlike hypotheses, theories and laws may be simply referred to as scientific fact.\nHowever, in science, theories are different from facts even when they are well supported. For example, evolution is both a theory and a fact.\n\nThe logical positivists thought of scientific theories as statements in a formal language. First-order logic is an example of a formal language. The logical positivists envisaged a similar scientific language. In addition to scientific theories, the language also included observation sentences (\"the sun rises in the east\"), definitions, and mathematical statements. The phenomena explained by the theories, if they could not be directly observed by the senses (for example, atoms and radio waves), were treated as theoretical concepts. In this view, theories function as axioms: predicted observations are derived from the theories much like theorems are derived in Euclidean geometry. However, the predictions are then tested against reality to verify the theories, and the \"axioms\" can be revised as a direct result.\n\nThe phrase \"the received view of theories\" is used to describe this approach. Terms commonly associated with it are \"linguistic\" (because theories are components of a language) and \"syntactic\" (because a language has rules about how symbols can be strung together). Problems in defining this kind of language precisely, e.g., are objects seen in microscopes observed or are they theoretical objects, led to the effective demise of logical positivism in the 1970s.\n\nThe semantic view of theories, which identifies scientific theories with models rather than propositions, has replaced the received view as the dominant position in theory formulation in the philosophy of science. A model is a logical framework intended to represent reality (a \"model of reality\"), similar to the way that a map is a graphical model that represents the territory of a city or country.\n\nIn this approach, theories are a specific category of models that fulfill the necessary criteria (see above). One can use language to describe a model; however, the theory is the model (or a collection of similar models), and not the description of the model. A model of the solar system, for example, might consist of abstract objects that represent the sun and the planets. These objects have associated properties, e.g., positions, velocities, and masses. The model parameters, e.g., Newton's Law of Gravitation, determine how the positions and velocities change with time. This model can then be tested to see whether it accurately predicts future observations; astronomers can verify that the positions of the model's objects over time match the actual positions of the planets. For most planets, the Newtonian model's predictions are accurate; for Mercury, it is slightly inaccurate and the model of general relativity must be used instead.\n\nThe word \"semantic\" refers to the way that a model represents the real world. The representation (literally, \"re-presentation\") describes particular aspects of a phenomenon or the manner of interaction among a set of phenomena. For instance, a scale model of a house or of a solar system is clearly not an actual house or an actual solar system; the aspects of an actual house or an actual solar system represented in a scale model are, only in certain limited ways, representative of the actual entity. A scale model of a house is not a house; but to someone who wants to \"learn about\" houses, analogous to a scientist who wants to understand reality, a sufficiently detailed scale model may suffice.\n\nSeveral commentators have stated that the distinguishing characteristic of theories is that they are explanatory as well as descriptive, while models are only descriptive (although still predictive in a more limited sense). Philosopher Stephen Pepper also distinguished between theories and models, and said in 1948 that general models and theories are predicated on a \"root\" metaphor that constrains how scientists theorize and model a phenomenon and thus arrive at testable hypotheses.\n\nEngineering practice makes a distinction between \"mathematical models\" and \"physical models\"; the cost of fabricating a physical model can be minimized by first creating a mathematical model using a computer software package, such as a computer aided design tool. The component parts are each themselves modelled, and the fabrication tolerances are specified. An exploded view drawing is used to lay out the fabrication sequence. Simulation packages for displaying each of the subassemblies allow the parts to be rotated, magnified, in realistic detail. Software packages for creating the bill of materials for construction allows subcontractors to specialize in assembly processes, which spreads the cost of manufacturing machinery among multiple customers. See: Computer-aided engineering, Computer-aided manufacturing, and 3D printing\n\nAn assumption (or axiom) is a statement that is accepted without evidence. For example, assumptions can be used as premises in a logical argument. Isaac Asimov described assumptions as follows:\n\n...it is incorrect to speak of an assumption as either true or false, since there is no way of proving it to be either (If there were, it would no longer be an assumption). It is better to consider assumptions as either useful or useless, depending on whether deductions made from them corresponded to reality...Since we must start somewhere, we must have assumptions, but at least let us have as few assumptions as possible.\n\nCertain assumptions are necessary for all empirical claims (e.g. the assumption that reality exists). However, theories do not generally make assumptions in the conventional sense (statements accepted without evidence). While assumptions are often incorporated during the formation of new theories, these are either supported by evidence (such as from previously existing theories) or the evidence is produced in the course of validating the theory. This may be as simple as observing that the theory makes accurate predictions, which is evidence that any assumptions made at the outset are correct or approximately correct under the conditions tested.\n\nConventional assumptions, without evidence, may be used if the theory is only intended to apply when the assumption is valid (or approximately valid). For example, the special theory of relativity assumes an inertial frame of reference. The theory makes accurate predictions when the assumption is valid, and does not make accurate predictions when the assumption is not valid. Such assumptions are often the point with which older theories are succeeded by new ones (the general theory of relativity works in non-inertial reference frames as well).\n\nThe term \"assumption\" is actually broader than its standard use, etymologically speaking. The Oxford English Dictionary (OED) and online Wiktionary indicate its Latin source as \"assumere\" (\"accept, to take to oneself, adopt, usurp\"), which is a conjunction of \"ad-\" (\"to, towards, at\") and \"sumere\" (to take). The root survives, with shifted meanings, in the Italian \"assumere\" and Spanish \"sumir\". The first sense of \"assume\" in the OED is \"to take unto (oneself), receive, accept, adopt\". The term was originally employed in religious contexts as in \"to receive up into heaven\", especially \"the reception of the Virgin Mary into heaven, with body preserved from corruption\", (1297 CE) but it was also simply used to refer to \"receive into association\" or \"adopt into partnership\". Moreover, other senses of assumere included (i) \"investing oneself with (an attribute)\", (ii) \"to undertake\" (especially in Law), (iii) \"to take to oneself in appearance only, to pretend to possess\", and (iv) \"to suppose a thing to be\" (all senses from OED entry on \"assume\"; the OED entry for \"assumption\" is almost perfectly symmetrical in senses). Thus, \"assumption\" connotes other associations than the contemporary standard sense of \"that which is assumed or taken for granted; a supposition, postulate\" (only the 11th of 12 senses of \"assumption\", and the 10th of 11 senses of \"assume\").\n\nKarl Popper described the characteristics of a scientific theory as follows:\n\nPopper summarized these statements by saying that the central criterion of the scientific status of a theory is its \"falsifiability, or refutability, or testability\". Echoing this, Stephen Hawking states, \"A theory is a good theory if it satisfies two requirements: It must accurately describe a large class of observations on the basis of a model that contains only a few arbitrary elements, and it must make definite predictions about the results of future observations.\" He also discusses the \"unprovable but falsifiable\" nature of theories, which is a necessary consequence of inductive logic, and that \"you can disprove a theory by finding even a single observation that disagrees with the predictions of the theory\".\n\nSeveral philosophers and historians of science have, however, argued that Popper's definition of theory as a set of falsifiable statements is wrong because, as Philip Kitcher has pointed out, if one took a strictly Popperian view of \"theory\", observations of Uranus when first discovered in 1781 would have \"falsified\" Newton's celestial mechanics. Rather, people suggested that another planet influenced Uranus' orbit—and this prediction was indeed eventually confirmed.\n\nKitcher agrees with Popper that \"There is surely something right in the idea that a science can succeed only if it can fail.\" He also says that scientific theories include statements that cannot be falsified, and that good theories must also be creative. He insists we view scientific theories as an \"elaborate collection of statements\", some of which are not falsifiable, while others—those he calls \"auxiliary hypotheses\", are.\n\nAccording to Kitcher, good scientific theories must have three features:\n\nLike other definitions of theories, including Popper's, Kitcher makes it clear that a theory must include statements that have observational consequences. But, like the observation of irregularities in the orbit of Uranus, falsification is only one possible consequence of observation. The production of new hypotheses is another possible and equally important result.\n\nThe concept of a scientific theory has also been described using analogies and metaphors. For instance, the logical empiricist Carl Gustav Hempel likened the structure of a scientific theory to a \"complex spatial network:\"\n\nMichael Polanyi made an analogy between a theory and a map:\n\nA theory is something other than myself. It may be set out on paper as a system of rules, and it is the more truly a theory the more completely it can be put down in such terms. Mathematical theory reaches the highest perfection in this respect. But even a geographical map fully embodies in itself a set of strict rules for finding one's way through a region of otherwise uncharted experience. Indeed, all theory may be regarded as a kind of map extended over space and time.\n\nA scientific theory can also be thought of as a book that captures the fundamental information about the world, a book that must be researched, written, and shared. In 1623, Galileo Galilei wrote:\n\nPhilosophy [i.e. physics] is written in this grand book—I mean the universe—which stands continually open to our gaze, but it cannot be understood unless one first learns to comprehend the language and interpret the characters in which it is written. It is written in the language of mathematics, and its characters are triangles, circles, and other geometrical figures, without which it is humanly impossible to understand a single word of it; without these, one is wandering around in a dark labyrinth.\n\nThe book metaphor could also be applied in the following passage, by the contemporary philosopher of science Ian Hacking:\n\nI myself prefer an Argentine fantasy. God did not write a Book of Nature of the sort that the old Europeans imagined. He wrote a Borgesian library, each book of which is as brief as possible, yet each book of which is inconsistent with every other. No book is redundant. For every book there is some humanly accessible bit of Nature such that that book, and no other, makes possible the comprehension, prediction and influencing of what is going on…Leibniz said that God chose a world which maximized the variety of phenomena while choosing the simplest laws. Exactly so: but the best way to maximize phenomena and have simplest laws is to have the laws inconsistent with each other, each applying to this or that but none applying to all.\n\nIn physics, the term \"theory\" is generally used for a mathematical framework—derived from a small set of basic postulates (usually symmetries—like equality of locations in space or in time, or identity of electrons, etc.)—that is capable of producing experimental predictions for a given category of physical systems. A good example is classical electromagnetism, which encompasses results derived from gauge symmetry (sometimes called gauge invariance) in a form of a few equations called Maxwell's equations. The specific mathematical aspects of classical electromagnetic theory are termed \"laws of electromagnetism,\" reflecting the level of consistent and reproducible evidence that supports them. Within electromagnetic theory generally, there are numerous hypotheses about how electromagnetism applies to specific situations. Many of these hypotheses are already considered to be adequately tested, with new ones always in the making and perhaps untested. An example of the latter might be the radiation reaction force. As of 2009, its effects on the periodic motion of charges are detectable in synchrotrons, but only as \"averaged\" effects over time. Some researchers are now considering experiments that could observe these effects at the instantaneous level (i.e. not averaged over time).\n\nNote that many fields of inquiry do not have specific named theories, e.g. developmental biology. Scientific knowledge outside a named theory can still have a high level of certainty, depending on the amount of evidence supporting it. Also note that since theories draw evidence from many different fields, the categorization is not absolute.\n\n"}
{"id": "1805271", "url": "https://en.wikipedia.org/wiki?curid=1805271", "title": "Social construction of technology", "text": "Social construction of technology\n\nSocial construction of technology (also referred to as SCOT) is a theory within the field of Science and Technology Studies. Advocates of SCOT—that is, social constructivists—argue that technology does not determine human action, but that rather, human action shapes technology. They also argue that the ways a technology is used cannot be understood without understanding how that technology is embedded in its social context. SCOT is a response to technological determinism and is sometimes known as technological constructivism.\n\nSCOT draws on work done in the constructivist school of the sociology of scientific knowledge, and its subtopics include actor-network theory (a branch of the sociology of science and technology) and historical analysis of sociotechnical systems, such as the work of historian Thomas P. Hughes. Its empirical methods are an adaptation of the Empirical Programme of Relativism (EPOR), which outlines a method of analysis to demonstrate the ways in which scientific findings are socially constructed (see strong program). Leading adherents of SCOT include Wiebe Bijker and Trevor Pinch.\n\nSCOT holds that those who seek to understand the reasons for acceptance or rejection of a technology should look to the social world. It is not enough, according to SCOT, to explain a technology's success by saying that it is \"the best\"—researchers must look at how the criteria of being \"the best\" is defined and what groups and stakeholders participate in defining it. In particular, they must ask who defines the technical criteria success is measured by, why technical criteria are defined this way, and who is included or excluded. Pinch and Bijker argue that technological determinism is a myth that results when one looks backwards and believes that the path taken to the present was the only possible path.\n\nSCOT is not only a theory, but also a methodology: it formalizes the steps and principles to follow when one wants to analyze the causes of technological failures or successes.\n\nAt the point of its conception, the SCOT approach was partly motivated by the ideas of the strong programme in the sociology of science (Bloor 1973). In their seminal article, Pinch and Bijker refer to the \"Principle of Symmetry\" as the most influential tenet of the Sociology of Science, which should be applied in historical and sociological investigations of technology as well. It is strongly connected to Bloor's theory of social causation.\n\nThe \"Principle of Symmetry\" holds that in explaining the origins of scientific beliefs, that is, assessing the success and failure of models, theories, or experiments, the historian/sociologist should deploy the same \"kind\" of explanation in the cases of success as in cases of failure. When investigating beliefs, researchers should be impartial to the (\"a posteriori\" attributed) truth or falsehood of those beliefs, and the explanations should be unbiased. The strong programme adopts a position of relativism or neutralism regarding the arguments that social actors put forward for the acceptance/rejection of any technology. All arguments (social, cultural, political, economic, as well as technical) are to be treated equally.\n\nThe symmetry principle addresses the problem that the historian is tempted to explain the success of successful theories by referring to their \"objective truth\", or inherent \"technical superiority\", whereas s/he is more likely to put forward sociological explanations (citing political influence or economic reasons) only in the case of failures. For example, having experienced the obvious success of the chain-driven bicycle for decades, it is tempting to attribute its success to its \"advanced technology\" compared to the \"primitiveness\" of the Penny Farthing, but if we look closely and symmetrically at their history (as Pinch and Bijker do), we can see that at the beginning bicycles were valued according to quite different standards than nowadays. The early adopters (predominantly young, well-to-do gentlemen) valued the speed, the thrill, and the spectacularity of the Penny Farthing – in contrast to the security and stability of the chain-driven Safety Bicycle. Many other social factors (e.g., the contemporary state of urbanism and transport, women's clothing habits and feminism) have influenced and changed the relative valuations of bicycle models.\n\nA weak reading of the \"Principle of Symmetry\" points out that there often are many competing theories or technologies, which all have the potential to provide slightly different solutions to similar problems. In these cases, sociological factors tip the balance between them: that's why we should pay equal attention to them.\n\nA strong, social constructivist reading would add that even the emergence of the questions or problems to be solved are governed by social determinations, so the Principle of Symmetry is applicable even to the apparently purely technical issues.\n\nThe following concepts and 'stages' of SCOT are adapted from the Empirical Programme of Relativism (EPOR).\n\n\"Interpretative flexibility\" means that each technological artifact has different meanings and interpretations for various groups. Bijker and Pinch show that the air tire of the bicycle meant a more convenient mode of transportation for some people, whereas it meant technical nuisances, traction problems and ugly aesthetics to others. In racing air tires lent to greater speed.\n\nThese alternative interpretations generate different \"problems\" to be solved. How should aesthetics, convenience, and speed be prioritized? What is the \"best\" tradeoff between traction and speed?\n\nThe most basic relevant groups are the \"users\" and the \"producers\" of the technological artifact, but most often many subgroups can be delineated – users with different socioeconomic status, competing producers, etc. Sometimes there are relevant groups who are neither users, nor producers of the technology, for example, journalists, politicians, and civil organizations. Trevor Pinch has argued that the salespeople of technology should also be included in the study of technology. The groups can be distinguished based on their shared or diverging interpretations of the technology in question.\n\nJust as technologies have different meanings in different social groups, there are always multiple ways of constructing technologies. A particular design is only a single point in the large field of technical possibilities, reflecting the interpretations of certain relevant groups.\n\nThe different interpretations often give rise to conflicts between criteria that are hard to resolve technologically (e.g., in the case of the bicycle, one such problem was how a woman could ride the bicycle in a skirt while still adhering to standards of decency), or conflicts between the relevant groups (the \"Anti-cyclists\" lobbied for the banning of the bicycles). Different groups in different societies construct different problems, leading to different designs.\n\nThe first stage of the SCOT research methodology is to reconstruct the alternative interpretations of the technology, analyze the problems and conflicts these interpretations give rise to, and connect them to the design features of the technological artifacts. The relations between groups, problems, and designs can be visualized in diagrams.\n\nOver time, as technologies are developed, the interpretative and design flexibility collapse through closure mechanisms. Two examples of closure mechanisms:\n\n\nClosure is not permanent. New social groups may form and reintroduce interpretative flexibility, causing a new round of debate or conflict about a technology. (For instance, in the 1890s automobiles were seen as the \"green\" alternative, a cleaner environmentally-friendly technology, to horse-powered vehicles; by the 1960s, new social groups had introduced new interpretations about the environmental effects of the automobile, eliciting the opposite conclusion.)\n\nThe second stage of the SCOT methodology is to show how closure is achieved.\n\nThis is the third stage of the SCOT methodology, but the seminal article of Pinch and Bijker does not proceed to this stage. Many other historians and sociologists of technology nevertheless do. For example, Paul N. Edwards shows in his book \"The Closed World: Computers and the Politics of Discourse in Cold War America\" the strong relations between the political discourse of the Cold War and the computer designs of this era.\n\nIn 1993, Langdon Winner published an influential critique of SCOT entitled \"Upon Opening the Black Box and Finding it Empty: Social Constructivism and the Philosophy of Technology.\" In it, he argues that social constructivism is an overly narrow research program. He identifies the following specific limitations in social constructivism:\nOther critics include Stewart Russell with his letter in the journal \"Social Studies of Science\" titled \"The Social Construction of Artifacts: A Response to Pinch and Bijker\".\n\n\n\n"}
{"id": "875785", "url": "https://en.wikipedia.org/wiki?curid=875785", "title": "Social effects of evolutionary theory", "text": "Social effects of evolutionary theory\n\nThe social effects of evolutionary thought have been considerable. As the scientific explanation of life's diversity has developed, it has often displaced alternative, sometimes very widely held, explanations. Because the theory of evolution includes an explanation of humanity's origins, it has had a profound impact on human societies. Some have vigorously denied acceptance of the scientific explanation due to its perceived religious implications (e.g. its implied rejection of the special creation of humans presumably described in the Bible). This has led to a vigorous conflict between creation and evolution in public education, primarily in the United States.\n\nThe theory of evolution by natural selection has also been adopted as a foundation for various ethical and social systems, such as social Darwinism, an idea that preceded the publication of The Origin of Species, popular in the 19th century, which holds that \"the survival of the fittest\" (a phrase coined in 1851 by Herbert Spencer, 6 years before Darwin published his theory of evolution) explains and justifies differences in wealth and success among societies and people. A similar interpretation was one created by Darwin's cousin, Francis Galton, known as eugenics, which claimed that human civilization was subverting natural selection by allowing the less bright and less healthy to survive and out-breed the more smart and more healthy.\n\nLater advocates of this theory suggested radical and often coercive social measures in an attempt to \"correct\" this imbalance. Thomas Huxley spent much time demonstrating through a series of thought experiments that it would not only be immoral, but impossible, Stephen Jay Gould and others have argued that social Darwinism is based on misconceptions of evolutionary theory, and many ethicists regard it as a case of the is-ought problem. After the atrocities of the Holocaust became linked with eugenics, it greatly fell out of favor with public and scientific opinion, though it was never universally accepted by either, and at no point in Nazi literature is Charles Darwin or the scientific theory of evolution mentioned.\n\nIn his book \"The End of Faith\", Sam Harris argues that Nazism was largely a continuation of Christian anti-Semitism. Jim Walker compiled a list of 129 quotes from Mein Kampf in which Hitler described himself as a Christian, or mentioned God, Jesus or a biblical passage. Some argue that six million of the people killed during the Holocaust were killed because of their religion (Judaism) not their race, \"strength,\" or any reason with an obvious link to the mechanism of Darwinian evolution. Hitler often used Christian beliefs like, \"Jews killed Jesus,\" to justify his anti-Semitism.\n\nThe notion that humans share ancestors with other animals has also affected how some people view the relationship between humans and other species. Many proponents of animal rights hold that if animals and humans are of the same nature, then rights cannot be distinct to humans.\n\nCharles Darwin, in fact, considered \"sympathy\" to be one of the most important moral virtues — and that it was, indeed, a product of natural selection and a trait beneficial to social animals (including humans). Darwin further argued that the most \"sympathetic\" societies would consequently be the most \"successful.\" He also stated that our sympathy should be extended to \"all sentient beings\":\n\nThomas Huxley, \"Darwin's Bulldog\", spent much of his book Evolution and Ethics debunking Social Darwinism, piece by piece. The following is a summary of his arguments in the \"Prolegomena\", the most detailed and comprehensive of the two sections devoted to it. It should be noted that Huxley is here attempting to disprove the science behind Social Darwinism; as such, the moral arguments only come in later in the essay.\n\nConsider a garden. Without constant upkeep, it would return to the \"state of nature\", even the very walls surrounding it crumbling in sufficient time, but by constant diligence of the gardener, may be maintained in a \"state of art\". This \"state of art\" is not permanent: It is instead the replacement of natural selection by artificial selection through the human energy expended in maintaining it.\n\nThis artificial selection is, however, part of natural selection: It is the action upon a set of species by the human species by way of the human species expending energy through evolved intelligence on its choice of selection. It is thus no less natural than, for example, a predator expending energy through evolved instinct on preferentially hunting a certain prey species. The presence of humans may change the dynamic, but in a perfectly natural way. Hence, it is part of the \"cosmic process\", that is natural laws, even though the \"histological process\" may remove many aspects of the \"struggle for existence\" that is a key part of the natural laws that apply to biology, from its preferred plant species by substituting human work for work done by the species itself.\nNot only is the state of nature hostile to the state of art of the garden; but the principle of the horticultural process, by which the latter is created and maintained, is antithetic to that of the cosmic process. The characteristic feature of the latter is the intense and unceasing competition of the struggle for existence. The characteristic of the former is the elimination of that struggle, by the removal of the conditions which give rise to it. The tendency of the cosmic process is to bring about the adjustment of the forms of plant life to the current conditions; the tendency of the horticultural process is the adjustment of the conditions to the needs of the forms of plant life which the gardener desires to raise.\nNature uses unrestricted breeding to let hundreds compete for the natural resources that would only support one, and uses frost and drought to kill off the weak and unlucky, requiring not just strength, but \"flexibility and good fortune.\" However, a gardener restricts multiplication, gives each plant sufficient space and nourishment, protects from frost and drought—and, in every other way, attempts to modify the conditions to benefit the forms that most nearly approach the result he desires. However, though the gardener's actions may have circumvented natural selection, he can still improve the species, should he find them wanting, through selective breeding. The struggle for existence is not actually required for improvement: only heritability, variation, and some form of selective pressure.\n\nCan we then apply this to humans? Let's see how far we can take the analogy with respect to colonization:\nSuppose a shipload of English colonists sent to form a settlement, in such a country as Tasmania was in the middle of the last century. On landing, they find themselves in the midst of a state of nature, widely different from that left behind them in everything but the most general physical conditions. The common plants, the common birds and quadrupeds, are as totally distinct as the men from anything to be seen on the side of the globe from which they come. The colonists proceed to put an end to this state of things over as large an area as they desire to occupy. They clear away the native vegetation, extirpate or drive out the animal population, so far as may be necessary, and take measures to defend themselves from the re-immigration of either. In their place, they introduce English grain and fruit trees; English dogs, sheep, cattle, horses; and English men; in fact, they set up a new Flora and Fauna and a new variety of mankind, within the old state of nature. Their farms and pastures represent a garden on a great scale, and themselves the gardeners who have to keep it up, in watchful antagonism to the old regime. Considered as a whole, the colony is a composite unit introduced into the old state of nature; and, thenceforward, a competitor in the struggle for existence, to conquer or be vanquished.\n\nUnder the conditions supposed, there is no doubt of the result, if the work of the colonists be carried out energetically and with intelligent combination of all their forces. On the other hand, if they are slothful, stupid, and careless; or if they waste their energies in contests with one another, the chances are that the old state of nature will have the best of it. The native savage will destroy the immigrant civilized man; of the English animals and plants some will be extirpated by their indigenous rivals, others will pass into the feral state and themselves become components of the state of nature. In a few decades, all other traces of the settlement will have vanished.\nHowever, as yet we lack an organized gardener. Let us imagine an idealized one: an administrative authority of intelligence and foresight as much greater than men as men are to their livestock. The unwanted native species - men, animals, or plants - are all weeded out and destroyed. Those to replace them are chosen with a view to his ideal of the colony, just as a gardener tries to create through his selection his ideal garden. And, finally, to ensure that no struggle for existence between the colonists interferes with the struggle against nature, he provides them with sufficient food, housing, and so on. \"With every step of this progress in civilization, the colonists would become more and more independent of the state of nature; more and more, their lives would be conditioned by a state of art. In order to attain his ends, the administrator would have to avail himself of the courage, industry, and co-operative intelligence of the settlers; and it is plain that the interest of the community would be best served by increasing the proportion of persons who possess such qualities, and diminishing that of persons devoid of them. In other words, by selection directed towards an ideal.\"\n\nHowever, though this might create a paradise where every aspect of nature works to support its colonists, problems arise: \"as soon as the colonists began to multiply, the administrator would have to face the tendency to the reintroduction of the cosmic struggle into his artificial fabric, in consequence of the competition, not merely for the commodities, but for the means of existence. When the colony reached the limit of possible expansion, the surplus population must be disposed of somehow; or the fierce struggle for existence must recommence and destroy that peace, which is the fundamental condition of the maintenance of the state of art against the state of nature.\n\nIf the administrator is guided purely by scientific considerations, he would work to restrict the population by removing \"the hopelessly diseased, the infirm aged, the weak or deformed in body or in mind, and the excess of infants born,\" just as a \"gardener pulls up defective and superfluous plants, or the breeder\ndestroys undesirable cattle. Only the strong and the healthy, carefully matched, with a view to the progeny best adapted to the purposes of the administrator, would be permitted to perpetuate their kind.\"\n\nAnd so we have reached Social Darwinism. However, we do not have an idealized administrator:\n\nOf the more thoroughgoing of the multitudinous attempts to apply the principles of cosmic evolution, or what are supposed to be such, to social and political problems, which have appeared of late years, a considerable proportion appear to me to be based upon the notion that human society is competent to furnish, from its own resources, an administrator of the kind I have imagined. The pigeons, in short, are to be their own Sir John Sebright. A despotic government, whether individual or collective, is to be endowed with the preternatural intelligence, and with what, I am afraid, many will consider the preternatural ruthlessness, required for the purpose of carrying out the principle of improvement by selection, with the somewhat drastic thoroughness upon which the success of the method depends. Experience certainly does not justify us in limiting the ruthlessness of individual \"saviors of society\"; and, on the well-known grounds of the aphorism which denies both body and soul to corporations, it seems probable (indeed the belief is not without support in history) that a collective despotism, a mob got to believe in its own divine right by demagogic missionaries, would be capable of more thorough work in this direction than any single tyrant, puffed up with the same illusion, has ever achieved. But intelligence is another affair. The fact that \"saviors of society\" take to that trade is evidence enough that they have none to spare. And such as they possess is generally sold to the capitalists of physical force on whose resources they depend. However, I doubt whether even the keenest judge of character, if he had before him a hundred boys and girls under fourteen, could pick out, with the least chance of success, those who should be kept, as certain to be serviceable members of the polity, and those who should be chloroformed, as equally sure to be stupid, idle, or vicious. The \"points\" of a good or of a bad citizen are really far harder to discern than those of a puppy or a short-horn calf; many do not show themselves before the practical difficulties of life stimulate manhood to full exertion. And by that time the mischief is done. The evil stock, if it be one, has had time to multiply, and selection is nullified.\nHowever, humans are not cattle, nor flowers: the organization of human society is kept together by\n\n...bonds of such a singular character, that the attempt to perfect society after his fashion would run serious risk of loosening them. They do not even correspond to social insects such as bees: With bees, \"The members of the society are each organically predestined to the performance of one particular class of functions only. If they were endowed with desires, each could desire to perform none but those offices for which its organization specially fits it; and which, in view of the good of the whole, it is proper it should do. Among mankind, on the contrary, there is no such predestination to a sharply defined place in the social organism. However much men may differ in the quality of their intellects, the intensity of their passions, and the delicacy of their sensations, it cannot be said that one is fitted by his organization to be an agricultural laborer and nothing else, and another to be a landowner and nothing else. Moreover, with all their enormous differences in natural endowment, men agree in one thing, and that is their innate desire to enjoy the pleasures and to escape the pains of life; and, in short, to do nothing but that which it pleases them to do, without the least reference to the welfare of the society into which they are born, checked only by sympathy, familial and social bonds, and fear of the judgment of ones fellow man. \"Every forward step of social progress brings men into closer relations with their fellows, and increases the importance of the pleasures and pains derived from sympathy.\n\nIn short, he describes a creation of morality.\n\nSince morality is what keeps the desire for selfishness in check, it is necessary to the propagation of society, with one requirement: the punishment of wrongdoers being necessary for the continuation of society, self-restraint must not be taken so far that wrongdoers may act unrestrained: Without the protection of society against them, \"The followers of the \"golden rule\" may indulge in hopes of heaven, but they must reckon with the certainty that other people will be masters of the earth.\"\n\nHuxley sums up this section of his argument against Social Darwinism:\nI have further shown cause for the belief that direct selection, after the fashion of the horticulturist and the breeder, neither has played, nor can play, any important part in the evolution of society; apart from other reasons, because I do not see how such selection could be practiced without a serious weakening, it may be the destruction, of the bonds which hold society together. It strikes me that men who are accustomed to contemplate the active or passive extirpation of the weak, the unfortunate, and the superfluous; who justify that conduct on the ground that it has the sanction of the cosmic process, and is the only way of ensuring the progress of the race; who, if they are consistent, must rank medicine among the black arts and count the physician a mischievous preserver of the unfit; on whose matrimonial undertakings the principles of the stud have the chief influence; whose whole lives, therefore, are an education in the noble art of suppressing natural affection and sympathy, are not likely to have any large stock of these commodities left. But, without them, there is no conscience, nor any restraint on the conduct of men, except the calculation of self-interest, the balancing of certain present gratifications against doubtful future pains; and experience tells us how much that is worth. Every day, we see firm believers in the hell of the theologians commit acts by which, as they believe when cool, they risk eternal punishment; while they hold back from those which are opposed to the sympathies of their associates.\nHuxley finishes with a series of short, further evidences against Social Darwinism, including:\n\n\nBefore Darwin's argument and presentation of the evidence for evolution, Western religions generally discounted or condemned any claims that diversity of life is the result of an evolutionary process, as did most scientists in the English scientific establishment. However, evolution was accepted by some religious groups such as the Unitarian church and the liberal Anglican theologians who went on to publish \"Essays and Reviews\". as well as by many scientists in France and Scotland and some in England, notably Robert Edmund Grant. Literal or authoritative interpretations of Scripture hold that a supreme being directly created humans and other animals as separate \"Created kinds\", which to some means species. This view is commonly referred to as creationism. From the 1920s to the present in the US, there has been a strong religious backlash to the teaching of evolution theory, particularly by conservative evangelicals. They have expressed concerns about the effects of the teaching of evolution on society and their faith (see Creation-evolution controversy).\n\nIn response to the wide scientific acceptance of the theory of evolution, many religions have formally or informally synthesized the scientific and religious viewpoints. Several important 20th century scientists (Fisher, Dobzhansky) whose work confirmed Darwin's theory, were also Christians who saw no incompatibility between their experimental and theoretical confirmations of evolution and their faith. Some religions have adopted a theistic evolution viewpoint, where God provides a divine spark that ignited the process of evolution and (or), where God has guided evolution in one way or another.\n\nThe Roman Catholic Church, beginning in 1950 with Pope Pius XII's encyclical Humani Generis, took up a neutral position with regard to evolution. \"The Church does not forbid that...research and discussions, on the part of men experienced in both fields, take place with regard to the doctrine of evolution, in as far as it inquires into the origin of the human body as coming from pre-existent and living matter.\"\n\nIn an October 22, 1996, address to the Pontifical Academy of Science, Pope John Paul II updated the Church's position, recognizing that Evolution is \"more than a hypothesis\" - \"In his encyclical Humani Generis, my predecessor Pius XII has already affirmed that there is no conflict between evolution and the doctrine of the faith regarding man and his vocation... Today, more than a half-century after the appearance of that encyclical, some new findings lead us toward the recognition of evolution as more than an hypothesis. In fact it is remarkable that this theory has had progressively greater influence on the spirit of researchers, following a series of discoveries in different scholarly disciplines.\"\n\nClassical figures have not discussed the subject as it has only come up in the 19th century. Contemporaries have come up with several distinct stances. One stance is that adaptation, or evolution on a micro scale, is accepted within a species, but cross-species evolution, that is evolution from one species into another species, is not as the human beginning is considered to be miraculous. However, this traditional thought would not conflict with the view that human-like beings could have been created around the same time as human beings, which, in this view, would explain the fossil records that look human but are not. Another stance is that since evolution is the simplest explanation it is the most reasonable to accept under the condition that it is not random but occurs only with the permission of God every step of the way. One particular argument that supports the idea that evolution is possible is the one stating that in that the stages of human development in evolution are akin to the distinct stages of development acknowledged in the Koran. The final stance completely rejects cross-species evolution across all organisms, but approves of adaptation (micro evolution).\n\nMany important political figures on the left have never publicized their views on biology, and so their opinions of evolutionary theory are unknown. To some extent, Marxists are the exception. Karl Marx, Friedrich Engels and Vladimir Lenin supported Darwin's evolutionary theory. Marx even sent Darwin a copy of his book \"Das Kapital\", though Darwin never wrote back to him. Karl Marx's work was based on a material view of the world that showed natural causes and effects for all aspects of human society and economy. He recognized that Darwin's work provided a similar material explanation for all of nature, thus supporting Marx's worldview.\n\nIn 1861 Karl Marx wrote to his friend Ferdinand Lassalle, \"Darwin’s work is most important and suits my purpose in that it provides a basis in natural science for the historical class struggle. ... Despite all shortcomings, it is here that, for the first time, 'teleology' in natural science is not only dealt a mortal blow but its rational meaning is empirically explained.\"\n\nMost later Marxists agreed with this view, but some – particularly those in the early Soviet Union – believed that evolutionary theory conflicted with their economic and social ideals. As a result, they came to support Lamarckism instead – the idea that an organism can pass on characteristics that it acquired during its lifetime to its offspring. This led to the practice of Lysenkoism, which caused agricultural problems.\n\nIn his book, \"\", anarcho-communist Peter Kropotkin argued that co-operation and mutual aid are as important in the evolution of the species as competition and mutual strife, if not more so.\n\nOn the contemporary moderate left, some authors such as Peter Singer (in his book, \"A Darwinian Left\") support Darwinism but reach different political and economic lessons than more conservative observers. Richard Dawkins' book, \"The Selfish Gene\", has a chapter, \"Nice guys finish first,\" that attempts to explain the role of altruism and cooperation in evolution and how social animals not only cannot survive without such traits, but how evolution will create them. Dawkins explains that when an animal sacrifices itself or uses its resources for the survival of other members of the same species, its genes, present on the other animals, survive. For example, if a mother dies to save three of its pups, one and a half copies (on average) of its genes will survive, because there is a 50% chance of a particular gene being present in its offspring. Dawkins also made a documentary of the same name. According to the documentary, Dawkins added that chapter as a way of overcoming modern day misinterpretations of the concept of \"survival of the fittest\".\n\n\"Social Darwinism\" is a derogatory term associated with the 19th century Malthusian theory developed by Whig philosopher Herbert Spencer. It is associated with evolutionary theory but now widely regarded as unwarranted. Social Darwinism was later expanded by others into ideas about \"survival of the fittest\" in commerce and human societies as a whole, and led to claims that social inequality, sexism, racism and imperialism were justified. However, these ideas contradict Darwin's own views, and contemporary scientists and philosophers consider these ideas to be neither mandated by evolutionary theory nor supported by data.\n\nSocial Darwinism is further linked with nationalism and imperialism. During the age of New Imperialism, the concepts of evolution justified the exploitation of \"lesser breeds without the law\" by \"superior races.\" To elitists, strong nations were composed of white people who were successful at expanding their empires, and as such, these strong nations would survive in the struggle for dominance. With this attitude, Europeans, except for Christian missionaries, seldom adopted the customs and languages of local people under their empires. Christian missionaries, on the other hand, were the very first individuals to meet new peoples and develop writing systems for local inhabitants' languages that lacked one. Being critics of Darwinism, they ardently opposed slavery and provided an education and religious instruction to the new peoples they interacted with since they felt that this was their duty as Christians.\n\n"}
{"id": "89493", "url": "https://en.wikipedia.org/wiki?curid=89493", "title": "São Carlos", "text": "São Carlos\n\nSão Carlos (Saint Charles, in English, ; named after Saint Charles Borromeo) is a city of 246,088 inhabitants (IBGE/2017) in the state of São Paulo, Brazil. It is located at , at about 230 km from the city of São Paulo.\n\nThe region started to be settled in the end of the 18th century, with the opening of a road that led to the gold mines in Cuiabá and Goiás. Leaving from Piracicaba, passing through Rio Claro, the hills, fields and by typical vegetations of the Brazilian countryside, settlers established in the region. São Carlos' history started in 1831, when the \"Pinhal\" (Pines) allotment was demarcated.\n\nOn the city's foundation date, November 4, 1857, the population resided in some houses around the chapel and the inhabitants were mostly Arruda Botelho's family heirs, who were the first owners of the \"Pinhal\" alloments. Between 1831 and 1857 the pioneer coffee farms were formed, starting the first economic activity in the city. The coffee crops came to the \"Pinhal\" farm in 1840 and spread throughout the fertile lands around, becoming the main export item. The city foundation is credited to Antônio Carlos de Arruda Botelho, Count of Pinhal, an influent farmer and entrepreneur.\n\nSão Carlos was elevated to village in 1865, when a \"Câmara\", or ruling chamber, was created. In 1874, the village had 6,897 inhabitants, as a humble highlight of its fast growth and regional importance. It became a city in 1880 and in 1886, with a population of 16,104, its urban structure was settled.\n\nThe city arises on the coffee crops expansion context, which is relevant to the last two decades of the 19th century and to the first two of the 20th century. The arrival of the railway in 1884 provided an efficient system to transport the coffee production to the Santos harbor and boosted the economy of the region. The railway also contributed to the political and economic consolidation of the central area of the city.\n\nWhen slavery ended, government created incentives to bring in immigrants. São Carlos had already received German nationals brought by the Count of Pinhal in 1876. Between 1880 and 1904, the city was one of the most important immigration centers in São Paulo state, the majority of them being Italians - specifically, Northern Italians. They worked in coffee plantations and in manufacturing factories, as well as trading activities.\n\nIn the beginning of the 20th century, countless cultural societies developed social activities aiming to promote literacy. Vittorio Emanuele Society in 1900 and Dante Alighieri in 1902 were but a few of them. The Italian presence was so significant that during the first half of the 20th century, the Italian government had a consulate branch in São Carlos.\n\nWith the Wall Street crash of 1929, coffee production went through a crisis, which made many immigrants leave rural areas for factories, wood artifact production, pottery, and construction.\n\nFarmers had already applied the profits obtained with coffee in the constitution of several types of companies in São Carlos: banks, electricity, cable cars, telephones, water pumps, sewers, theaters, hospitals and schools. This established a foundation for industrialization in the city. With the arrival of immigrants from other urban centers from the 1930s - 1940s, their expertise was used to consolidate industrialization as the main economic activity in the city. Its peak years were the 1950s, when São Carlos became a manufacturing center, with relevant industrial expression in São Paulo state.\n\nThe industrial sector also developed through workshops that incorporated the coffee industry. The manufacture of processing machinery, shoes, fertilizers, hardware, furniture, pasta, cigars, as well as activities such as tailory, breweries, foundries, sawmills, weaving, pottery and pencil production expanded the economy of São Carlos in the 1930s. In the 1950s and 1960s, with the expansion of refrigeration, new factories of machinery and tractors arrived. Numerous small- and medium-sized companies which provided products and services were also established.\n\nIn the second half of the 20th century, the city received a boost of technological and higher educational development when in 1953 the Escola de Engenharia de São Carlos, or the Engineering School of the University of São Paulo, was created. In the 1970s, the Federal University of São Carlos was launched.\n\nSão Carlos is located on the geographic center of the São Paulo state, approximately from the city of São Paulo. The city is the center of a microregion with 308,777 inhabitants.\n\nIts altitude is over 856 m, offering a mild altitude climate. Most of the year the city is windy, sunny and with mild temperatures at night. For this reason the city is nicknamed \"\"Cidade do Clima\" (Weather City) and celebrates once a year an event called \"Festa do Clima\"\", or \"Weather Festival\".\n\nThe city has a total area of 1.141 km², which includes two districts to the north (\"Santa Eudóxia\" and \"Água Vermelha\"), one district to the west (\"Bela Vista São-carlense\"), and one district to the east (\"Vila Nery\").\n\nAccording to the Köppen Climate Classification, the city has an altitude tropical climate with dry winter, with minimum average temperatures of 15,3 °C and maximum of 27,0 °C.\n\nThe city has an active industrial profile with important national and international industries and certain agricultural importance, backed by technologies developed by Embrapa, owner of two research complexes in the city. Due to its increasing number of high technology industries, the city has been proclaimed \"The National Capital of Technology\" by Brazilian President Dilma Rousseff in 2011.\n\nThe city hosts several locally-grown technology-based companies, such as Opto Eletrônicos, and factories of multinational corporations such as Faber Castell, Electrolux, Husqvarna, Tecumseh and the Brazilian plant of Volkswagen engines, and national corporations such as TAM MRO - Technology Center, Toalhas São Carlos, Tapetes São Carlos, Papel São Carlos, Prominas Brasil and Latina.\n\nThe economic basis of São Carlos is the tertiary sector. Commerce and services corresponds to 65.9% of the city's GDP. Industry is also relevant. With 32.3% of the economy, the secondary sector has a bigger participation than the state of São Paulo's average. The primary sector corresponds to 1.7% of the GDP.\n\nSão Carlos is home to two Universidade de São Paulo campuses and the Universidade Federal de São Carlos (UFSCar), two of the most important higher learning centers in Brazil. Moreover, another minor and private university, Centro Universitário Central Paulista (UNICEP), is also based in São Carlos, and community colleges like SENAI, SESI, SESC, SENAC and the \"Escola Técnica Estadual Paulino Botelho\". This has turned São Carlos into a university-oriented town, with an abundance of student-focused commercial establishments. It is also known for its student parties.\n\nSão Carlos' cultural life is marked by a young audience that enjoys musical concerts of Brazilian contemporary alternative artists that usually include the city in their tours. Also, São Carlos has 3 theaters and 7 commercial movie-theaters rooms.\n\nThere are two important events celebrated every year in the city, the \"Climate Party\", which happens in April and has a traditional \"Orchid Exposition\" which features a craftwork fair and several food barracks. An Oktobertech fest is held yearly along with the São Carlos High Tech Fair (Fealtec).\n\nThe TAM Airlines Wings of a Dream Museum (\"Museu TAM\") was in São Carlos, from central São Carlos.\n\n\nThe city is served by Mário Pereira Lopes Airport, where one of the maintenance bases of TAM Airlines is located and well as the air and space TAM Museum, owned and maintained by the company.\n\nWere born in São Carlos:\n\n\n\n\n"}
{"id": "34066587", "url": "https://en.wikipedia.org/wiki?curid=34066587", "title": "Technological transitions", "text": "Technological transitions\n\nTechnological innovations have occurred throughout history and rapidly increased over the modern age. New technologies are developed and co-exist with the old before supplanting them. Transport offers several examples; from sailing to steam ships to automobiles replacing horse-based transportation. Technological transitions (TT) describe how these technological innovations occur and are incorporated into society. Alongside the technological developments TT considers wider societal changes such as “user practices, regulation, industrial networks (supply, production, distribution), infrastructure, and symbolic meaning or culture”. For a technology to have use, it must be linked to social structures human agency and organisations to fulfil a specific need. Hughes refers to the ‘seamless web’ where physical artefacts, organisations, scientific communities, and social practices combine. A technological system includes technical and non-technical aspects, and it a major shift in the socio-technical configurations (involving at least one new technology) is when a technological transition occurs.\n\nWork on technological transitions draws on a number of fields including history of science, technology studies, and evolutionary economics. The focus of evolutionary economics is on economic change, but as a driver of this technological change has been considered in the literature. Joseph Schumpeter, in his classic \"Theory of Economic Development\" placed the emphasis on non-economic forces as the driver for growth. The human actor, the entrepreneur is seen as the cause of economic development which occurs as a cyclical process. Schumpeter proposed that radical innovations were the catalyst for Kondratiev cycles.\n\nThe Russian economist Kondratiev proposed that economic growth operated in boom and bust cycles of approximately 50 year periods. These cycles were characterised by periods of expansion, stagnation and recession. The period of expansion is associated with the introduction of a new technology, e.g. steam power or the microprocessor. At the time of publication, Kondratiev had considered that two cycles had occurred in the nineteenth century and third was beginning at the turn of the twentieth. Modern writers, such as Freeman and Perez outlined five cycles in the modern age:\n\n\nFreeman and Perez proposed that each cycle consists of pervasive technologies, their production and economic structures that support them. Termed ‘techno-economic paradigms’, they suggest that the shift from one paradigm to another is the result of emergent new technologies. \n\nFollowing the recent economic crisis, authors such as Moody and Nogrady have suggested that a new cycle is emerging from the old, centred on the use of sustainable technologies in a resource depleted world.\n\nThomas Kuhn described how a paradigm shift is a wholesale shift in the basic understanding of a scientific theory. Examples in science include the change of thought from miasma to germ theory as a cause of disease. Building on this work, Giovanni Dosi developed the concept of ’technical paradigms’ and ‘technological trajectories’. In considering how engineers work, the technical paradigm is an outlook on the technological problem, a definition of what the problems and solutions are. It charts the idea of specific progress. By identifying the problems to be solved the paradigm exerts an influence on technological change. The pattern of problem solving activity and the direction of progress is the technological trajectory. In similar fashion, Nelson and Winter (,)defined the concept of the ‘technological regime’ which directs technological change through the beliefs of engineers of what problems to solve. The work of the actors and organisations is the result of organisational and cognitive routines which determines search behaviour. This places boundaries and also trajectories (direction) to those boundaries.\n\nIn analysing (historic) cases of technological transitions researchers from the systems in transition branch of transitions research have used a multi-level perspective (MLP) as a heuristic model to understand changes in socio-technical systems. () Innovation system approaches traditionally focus on the production side. A socio-technical approach combines the science and technology in devising a production, with the application of the technology in fulfilling a societal function. Linking the two domains are the distribution, infrastructure and markets of the product. This approach considers a transition to be multi-dimensional as technology is only one aspect. \n\nThe MLP proposes three analytical levels: the niche, regime and landscape. \n\nNiche (Micro-level)\nRadical innovations occur at the niche level. These act as ‘safe havens’ for fledgling technologies to develop, largely free from market pressures which occur at the regime level. The US Military has acted as niche for major twentieth century technologies such as the aircraft, radio and the internet. More recently, California’s Silicon Valley has provided an arena for ICT focused technologies to emerge. Some innovations will challenge the existing regime while others fail. \n\nRegime (Meso-level)\nThe socio-technical regime, as defined by Geels, includes a web of inter-linking actors across different social groups and communities following a set of rules. In effect, the established practices of a given system. Seven dimensions have been identified in the socio-technical regime: technology, user practices and application, the symbolic meaning of technology, infrastructure, policy and techno-scientific knowledge. Change does occur at the regime level but it is normally slow and incremental unlike the radical change at the niche level. The actors who constitute the existing regime are set to gain from perpetuating the incumbent technology at the expense of the new. This is known as ‘lock-in’.\n\nLandscape (Macro-level)\nExogenous to the previous levels is the socio-technical landscape. A broad range of factors are contained here, such as economic pressures, cultural values, social trends, wars and environmental issues. Change occurs at an even slower rate than at the regime level. \n\nA transition is said to happen when a regime shift has occurred. This is the result of the interplay between the three levels. Regimes are relatively inert and resistant to change being structured to incremental innovation following established trajectories. As such, transitions are difficult to achieve. The current regime is typically suffering internal issues. Pressure from the landscape level may cause ‘cracks’ or ‘windows of opportunity’ through which innovations at the niche level may initially co-exist with the established technology before achieving ascendency. Once the technology has fully embedded into society the transition is said to be completed.\n\nThe MLP has been used in describing a range of historic transitions in socio-technical regimes for mobility, sanitation, food, lighting and so on. While early research focused on historical transitions, a second strand of research was more focused on transitions to sustainable technologies in key sectors such as transport, energy and housing.\n\nGeels presented three historical transitions on system innovation relating to modes of transportation. The technological transition from sailing ships to steamships in the UK will be summarised and shown in the context of a wider system innovation. \n\nGreat Britain was the world’s leading naval power in the nineteenth century, and led the way in the transition from sail to steam. At first, the introduction of steam technology co-existed with the current regime. Steam tugs assisted sail ships into port and hybrid steam / sail ships appeared. Landscape developments create the necessity for improvements in the technology. A demand for trans-Atlantic emigration was prompted by the Irish potato famine, European political instability and the lure of gold in California. The requirement for such arduous journeys had prompted a wealth of innovations at the niche level in steamship-development. From the late 1880s, as steamship technology improved and costs dropped, the new technology was widely diffused and a new regime established. The changes go beyond a technological transition as it involved new ship management and fleet management practices, new supporting infrastructures and new functionalities.\n\nThe nature of transitions varies and the differing qualities result in multiple pathways occurring. Geels and Schot defined five transition paths:\n\n\nSix characteristics of technological transitions have been identified.,\n\n\"Transitions are co-evolutionary and multi-dimensional\"\nTechnological developments occur intertwined with societal needs, wants and uses. A technology is adopted and diffused based on this interplay between innovation and societal requirements. Co-evolution has different aspects. As well as the co-evolution of technology and society, aspects between science, technology, users and culture have been considered.\n\"Multi-actors are involved\"\nScientific and engineering communities are central to the development of a technology, but a wide range of actors are involved in a transition. This can include organisations, policy-makers, government, NGOs, special interest groups and others.\n\n\"Transitions occur at multiple levels\"\nAs shown in the MLP transitions occur through the interplay of processes at different levels. \n\n\"Transitions are a long-term process\"\nComplete system-change takes time and can be decades in the making. Case studies show them to be between 40 and 90 years.\n\n\"Transitions are radical\"\nFor a true transition to occur the technology has to be a radical innovation. \n\n\"Change is Non-linear\"\nThe rate of change will vary over time. For example, the pace of change may be slow at the gestation period (at the niche level) but much more rapid when a breakthrough is occurring.\n\nDiffusion of an innovation is the concept of how it is picked up by society, at what rate and why. Everett (1962).The diffusion of a technological innovation into society can be considered in distinct phases. Pre-development is the gestation period where the new technology has yet to make an impact. Take-off is when the process of a system shift is beginning. A breakthrough is occurring when fundamental changes are occurring in existing structures through the interplay of economic, social and cultural forces. Once the rate of change has decreased and a new balance is achieved, stabilization is said to have occurred. A full transition involves an overhaul of existing rules and change of beliefs which takes time, typically spanning at least a generation. This process can be speeded-up through seismic, unforeseen events such as war or economic strife. \n\nGeels proposed a similar four phased approach which draws on the multi-level perspective (MLP) developed by Dutch scholars. Phases one sees the emergence of a novelty, born from the existing regime. Development then occurs in the niche level at phase two. As before, breakthrough then occurs at phase three. In the parlance of the MLP the new technology, having been developed at the niche level, is in competition with the established regime. To breakthrough and achieve wide diffusion, external factors – ‘windows of opportunity’ are required.\n\nA number of possible circumstances can act as windows of opportunity for the diffusion of new technologies: \n\n\nAlongside external influences, internal drivers catalyse diffusion. These include economic factors such as the price performance ration. Socio-technical perspectives focus on the links between disparate social and technological elements. Following the breakthrough, the final phases see the new technology supersede the old.\n\nThe study of technological transitions has an impact beyond academic interest. The transitions referred to in the literature may relate to historic processes, such as the transportation transitions studied by Geels, but system changes are required to achieve a safe transition to a low carbon-economy. (). Current structural problems are apparent in a range of sectors. Dependency on oil is problematic in the energy sector due to availability, access and contribution to greenhouse gas (GHG) emissions. Transportation is a major user of energy causing significant emission of GHGs. Food production will need to keep pace with an ever-growing world population while overcoming challenges presented by global warming and transportation issues. Incremental change has provided some improvements but a more radical transition is required to achieve a more sustainable future. \n\nDeveloped from the work on technological transitions is the field of transition management. Within this is an attempt to shape the direction of change complex socio-technical systems to more sustainable patterns. Whereas work on technological transitions is largely based on historic processes, proponents of transition management seek to actively steer transitions in progress.\n\nGenus and Coles outlined a number of criticisms against the analysis of technological transitions, in particular when using the MLP. Empirical research on technological transitions occurring now has been limited, with the focus on historic transitions. Depending on the perspective on transition case studies they could be presented as having occurred on a different transition path to what was shown. For example, the bicycle could be considered an intermediate transport technology between the horse and the car. Judged from shorter different time-frame this could appear a transition in its own right. Determining the nature of a transition is problematic; when it started and ended, or whether one occurred in the sense of a radical innovation displacing an existing socio-technical regime. The perception of time casts doubt on whether a transition has occurred. If viewed over a long enough period even inert regimes may demonstrate radical change in the end. The MLP has also been criticised by scholars studying sustainability transitions using Social Practice Theories.\n\n"}
{"id": "7925880", "url": "https://en.wikipedia.org/wiki?curid=7925880", "title": "Technology Centre Teknia (Kuopio Science Park)", "text": "Technology Centre Teknia (Kuopio Science Park)\n\nKuopio Science Park is a university related research park located in Kuopio, Finland. The name is not as such used in marketing purposes.\n\nTechnology Centre Teknia Ltd. (Teknia), which is the central enterprise service organisation, is used as the marketing brand of the whole area - hence the lack of logo for Kuopio Science Park. Teknia has been the fastest growing Technology Park in Finland for the new millennium.\n\nTeknia was acquired in spring 2008 by Technopolis plc for total cost of 18,1 million euros (67 million euros debts included).\n\n\n\nThe businesses based in Teknia specialize in information and communication technology and wellbeing technology (agrobiotechnology, drug design, and health care technology).\n\n\"Teknia's International Cooperation Partner Parks\": Zhongguancun Software Park (zPark, located in \"the Chinese Silicon Valley\"), Wuxi Life Science & Technology Park, Shanghai Zhangjiang Hi-Tech Park, Hong Kong Science and Technology Parks, IZET Innovationszentrum ltzehoe, TEKNIKBYN Västerås Technology Park, University of Surrey, University Enterprise Laboratories, Inc. (University of Minnesota)\n\n\n\n"}
{"id": "5997068", "url": "https://en.wikipedia.org/wiki?curid=5997068", "title": "The Ragged Edge of Science", "text": "The Ragged Edge of Science\n\nThe Ragged Edge of Science is a science book by L. Sprague de Camp, illustrated by Don Simpson. It was first published by Owlswick Press in 1980.\n\nThe book is a collection of twenty-two articles (two of them book reviews) on various curiosities and wonders exploring the boundaries between science and pseudo-science. \"The[ir] common thread is [their] skeptical takes on subjects that are often muddled by paranormal and pseudoscientific claims.\" De Camp viewed such phenomena from a skeptically rational viewpoint, pointing out the fallacies in supernatural and otherwise fantastic explanations. His debunking efforts were an important and characteristic feature of his nonfiction, and the present collection is a notable instance of it.\n\nThe book's constituent articles were originally published in a variety of science magazines, science fiction magazines, and other publications from 1950-1976.\n\n\nThe essays in the book fall into three general categories, dealing with ancient civilizations and certain unscientific theories regarding them, occult-related subjects, and pseudoscience in general. Anecdotes from history and de Camp's travels to some of the locales he writes about pepper the narrative.\n\nThe first eight chapters fall into the first category. Discussions of Bronze Age Troy and the ancient Sudanese civilization of Kush counter romantic speculations with a resume of what is known of them from historical sources and archaeological investigations. In contrast, the section on King Arthur, of whom little factual information has been established, puts to rest unverified notions regarding him by tracing the development and elaboration of his legend down through the ages. The chapter on the Maya debunks diffusionist theories seeking the origin of their culture in Old World civilizations rather than from indigenous factors. Later sections about Teotihuacan and the Toltecs serve more as general introductions to these cultures. There is also a brief discussion of the \"Tour Magne\", a Roman ruin in Nîmes, France, and a chapter on myths that discounts them as reliable reportage of prehistoric events.\n\nChapters in the second category include discussions of memories of previous lives supposedly recovered via hypnosis, the Kabbalah, lives of famous charlatans claiming to have been magicians, such as Cagliostro and Aleister Crowley, the hoax perpetrated by Léo Taxil and others that purported to expose Freemasonry as devil worship, theosophist C. W. Leadbeater, the development of occultist cultism around Mount Shasta in Northern California (demonstrated to have a literary basis), and the origins of the mystic trance, with rational explanations for the visions experienced. A satirical chapter of advice on how to set one's self up as a prophet rounds out the section.\n\nAn account of the early history of Fundamentalist movement to prohibit the teaching of evolution in schools leads off the third category. There is also a biography of Populist politician Ignatius Donnelly focusing on his speculations regarding Atlantis and like matters, and then a speculative chapter regarding future languages, essentially a didactic piece on language change with application to science fictional treatments of time-travel. It leads into a discussion of nonscientific claims about the \"fourth dimension\" in general. This part of the book also includes reviews of Immanuel Velikovsky's \"Worlds in Collision\" and Erich von Däniken's \"Chariots of the Gods?\", both of which de Camp discounts.\n\nCritical reviews of the book were generally positive. Writing in the wake of its release, Tom Easton observed in \"Analog Science Fiction/Science Fact\" that \"[i]f you know L. Sprague de Camp's work at all, you know what to expect ... He's always readable and entertaining, as he sticks his thumbs into gaping holes of fact and logic ... He's full of the straight dope (though he often doesn't go into things as deeply as I would like).\" He urged readers to \"buy the book.\" Michael Schuyler, writing for \"Library Journal\", took a more neutral stance, judging only that \"[m]ost of these mysteries have been well documented elsewhere, and De Camp [sic] presents no revelations.\" The book was also reviewed by Darrell Schweitzer in \"Science Fiction Review\" v. 10, issue 1 (Spring, 1981), p. 22.\n\nMore recently, an exhaustive review from 2007 sums up the book as \"a very pleasant and readable collection of essays, an excellent and classical example of skeptical writing and debunkery of various kinds of pseudoscientific and paranormal nonsense.\" The reviewer notes de Camp's \"accessible, down-to-earth style,\" humor, and story-telling expertise, as well as \"somewhat conservative opinions ... which occasionally show in his writing.\" Its conclusion is \"[o]verall I highly recommend this book.\"\n"}
{"id": "13240212", "url": "https://en.wikipedia.org/wiki?curid=13240212", "title": "Under the Sea Wind", "text": "Under the Sea Wind\n\nUnder the Sea Wind: A Naturalist's Picture of Ocean Life (1941) is the first book written by the American marine biologist Rachel Carson. It was published by Simon & Schuster in 1941, when it received very good reviews but sold poorly. After the great success of a sequel \"The Sea Around Us\" (Oxford, 1951), it was reissued by Oxford University Press; that edition was an alternate Book-of-the-Month Club selection and became another bestseller. It is recognised today as one of the \"definitive works of American nature writing,\" and is in print as one of the Penguin Nature Classics.\n\n\"Under the Sea Wind\" describes the behaviour of fish and seabirds accurately, but in story form, often using the scientific names of species as character names. Carson's stated goal in doing so was \"to make the sea and its life as vivid a reality for those who may read the book as it has become for me during the past decade.\" The first of her characters is introduced this way:\n\nWith the dusk a strange bird came to the island from its nesting grounds on the outer banks. Its wings were pure black, and from tip to tip their spread was more than the length of a man's arm. It flew steadily and without haste across the sound, its progress as measured and as meaningful as that of the shadows which little by little were dulling the bright water path. The bird was called Rynchops, the black skimmer.\"\nThe middle section of the book follows the life-story of Scomber, the mackerel, while the last part describes pond creatures such as eels and ducks. A glossary at the end of the book provides additional detail.\n\nThe style of Carson's writing makes the book suitable for children as well as adults, and the appeal is enhanced with illustrations, originally by Howard Frech, and replaced in 1991 with illustrations by Robert W. Hines. Carson acknowledged the influence of nature-novelist Henry Williamson on her writing style, but uses her scientific expertise to ground \"Under the Sea Wind\" in scientifically accurate detail on each animal's appearance, diet and behaviour.\n"}
{"id": "57537760", "url": "https://en.wikipedia.org/wiki?curid=57537760", "title": "William Phelps Ornithological Collection", "text": "William Phelps Ornithological Collection\n\nThe William Phelps Ornithological Collection, also known as the Phelps Ornithological Museum, is a museum of natural sciences dedicated to the study, exhibition and preservation of the birds of Venezuela and the rest of Latin America. The collection is located east of Caracas and in the geographic center of Greater Caracas, in the heart of the Sabana Grande district. The William Phelps ornithological collection is the most important in Latin America and it is also the most important private collection in the world in its research area. \n\nIn this private museum one will find important Phelps family study books, as well as 8000 scientific volumes in the library, more than 83,000 anatomical specimens, more than 80,000 skins, etc. For the year 1990, it was said that the William Phelps Ornithological Collection contained more than 76,300 skins and a small number of anatomical specimens, in the Gran Sabana Building of Sabana Grande. The Phelps library in 1990 already had 6,000 books, 800 journals and 5,500 reprints, mostly from natural sciences.\n\nThe ornithological collection was born in 1938, although it did not have its own headquarters on the Boulevard of Sabana Grande until 1949. At the beginning of 2018, it celebrated its 80th anniversary in Caracas, Venezuela. With the passing of time, the collection has been growing and still has great international scientific relevance. In 2005, an investigation was carried out on \"plumage differences in four subspecies of golden warbler Basileuterus culicivorus in Venezuela\". \n\nThe Phelps Foundation has been recognized worldwide for its scientific research. Since 1937, this foundation has dedicated to the study of the distribution of birds in Venezuela as well as to the dissemination of ornithology in Venezuela. Since 1949, it has expanded globally in its mission to discover, interpret and disseminate information about ornithology through a program of scientific research, education and dissemination in the natural sciences. The Foundation has had an important global trajectory for which it is recognized and is a regional compulsory study resource on tropical birds for experts who want to know more about this area. This museum has historically been connected to the American Museum of Natural History, thanks to the work of Billy Phelps. The ornithological collection has also been expanded thanks to the research carried out with Armando Dugand from Bogotá, Colombia. Most of the funds to carry out these investigations were collected by the Phelps Foundation.\n\nIn March and April 1977, the Phelps Ornithological Collection, with the collaboration of the Venezuelan-Brazilian Border Commission, Demarcador de Limites, carried out a collection of birds at Cerro Urutaní (62 ° 05'W, 3 ° 40'N), which is a low altitude tepui on the Venezuelan-Brazilian border in the Sierra Pacaraima. A total of 511 specimens of birds were collected between 1150 and 1280 meters high s.n.m., representing 78 different species. Gilberto Pérez Chinchilla, Manuel Castro and Dickerman prepared the copies of the collection. A full report on these birds was published in the international press and was published in the Bulletin of the \"American Museum of Natural History\", New York. It was necessary to work in conjunction with the Boundary Directorate of Venezuela.\n\n\n"}
{"id": "41151248", "url": "https://en.wikipedia.org/wiki?curid=41151248", "title": "Woman in Science", "text": "Woman in Science\n\nWoman in Science is a book written by H. J. Mozans (a pseudonym for John Augustine Zahm) in 1913. It is an account of women who have contributed to the sciences, up to the time when it was published.\n\nThe comprehensive theme that is depicted throughout \"Woman in Science\" is that of women's biological capacity. It is asserted that women being less prominent than men in science is due to the lack of educational and career opportunities available rather than, the biological aspects of brain size or structure. In addition, the book encompasses the many developments of science throughout history. The main objective of the author/book was for women to become more involved and gain a respected position in the scientific field, in addition to increasing educational and career opportunities for women interested in science. It was one of the first collaborations of women's contributions to the scientific community, and it \"explored the barriers to women's participation in science.\"\n\nPREFACE\n\n\nThe biographies include, but are not limited to, the following women, by chapter:\n\n"}
