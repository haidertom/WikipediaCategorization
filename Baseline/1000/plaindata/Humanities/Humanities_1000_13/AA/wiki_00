{"id": "60731", "url": "https://en.wikipedia.org/wiki?curid=60731", "title": "Anachronism", "text": "Anachronism\n\nAn anachronism (from the Greek , \"against\" and , \"time\") is a chronological inconsistency in some arrangement, especially a juxtaposition of persons, events, objects, or customs from different periods of time. The most common type of anachronism is an object misplaced in time, but it may be a verbal expression, a technology, a philosophical idea, a musical style, a material, a plant or animal, a custom, or anything else associated with a particular period in time that is placed outside its proper temporal domain.\n\nAn anachronism may be either intentional or unintentional. Intentional anachronisms may be introduced into a literary or artistic work to help a contemporary audience engage more readily with a historical period. Anachronism can also be used for purposes of rhetoric, comedy, or shock. Unintentional anachronisms may occur when a writer, artist, or performer is unaware of differences in technology, language, customs, attitudes, or fashions between different historical eras.\n\nA parachronism (from the Greek , \"on the side\", and , \"time\") is anything that appears in a time period in which it is not normally found (though not sufficiently out of place as to be impossible).\n\nThis may be an object, idiomatic expression, technology, philosophical idea, musical style, material, custom, or anything else so closely bound to a particular time period as to seem strange when encountered in a later era. They may be objects or ideas that were once common but are now considered rare or inappropriate. They can take the form of obsolete technology or outdated fashion.\n\nExamples of parachronisms could include a suburban housewife in the United States around 1960 using a washboard for laundry (well after washing machines had become the norm); or a teenager from that time period being an avid fan of ragtime music. Often, a parachronism is identified when a work based on a particular era's state of knowledge is read within the context of a later era—with a different state of knowledge. Many scientific works that rely on theories that have later been discredited have become anachronistic with the removal of those underpinnings, and works of speculative fiction often find their speculations outstripped by real-world technological developments or scientific discoveries.\n\nA prochronism (from the Greek , \"before\", and , \"time\") is an impossible anachronism which occurs when an object or idea has not yet been invented when the situation takes place, and therefore could not have possibly existed at the time (e.g. \"The Last Supper\" by Leonardo da Vinci at the top of the article). A prochronism may be an object not yet developed, a verbal expression that had not yet been coined, a philosophy not yet formulated, a breed of animal not yet evolved (or perhaps engineered), or use of a technology that had not yet been created.\n\nThe intentional use of older, often obsolete cultural artifacts may be regarded as anachronistic. For example, it could be considered anachronistic for a modern-day person to wear a top hat, write with a quill, or carry on a conversation in Latin. Such choices may reflect an eccentricity or an aesthetic preference.\nSome writings and works of art promoting a political, nationalist or revolutionary cause use anachronism to depict an institution or custom as being more ancient than it actually is. For example, the 19th-century Romanian painter Constantin Lecca depicts the peace agreement between Ioan Bogdan Voievod and Radu Voievod—two leaders in Romania's 16th-century history—with the flags of Moldavia (blue-red) and of Wallachia (yellow-blue) seen in the background. These flags date only from the 1830s. Here anachronism promotes legitimacy for the unification of Moldavia and Wallachia into the Kingdom of Romania at the time the painting was made.\n\nAnachronism is used especially in works of imagination that rest on a historical basis. Anachronisms may be introduced in many ways: for example, in the disregard of the different modes of life and thought that characterize different periods, or in ignorance of the progress of the arts and sciences and other facts of history. They vary from glaring inconsistencies to scarcely perceptible misrepresentation. It is only since the close of the 18th century that this kind of deviation from historical reality has jarred on a general audience. Sir Walter Scott justified the use of anachronism in historical literature: \"It is necessary, for exciting interest of any kind, that the subject assumed should be, as it were, translated into the manners as well as the language of the age we live in.\" However, as fashions move on, such attempts to use anachronisms to engage an audience may have quite the reverse effect, as the details in question are increasingly recognized as belonging neither to the historical era being represented, nor to the present, but to the intervening period in which the artwork was created. \"Nothing becomes obsolete like a period vision of an older period\", writes Anthony Grafton; \"Hearing a mother in a historical movie of the 1940s call out 'Ludwig! Ludwig van Beethoven! Come in and practice your piano now!' we are jerked from our suspension of disbelief by what was intended as a means of reinforcing it, and plunged directly into the American bourgeois world of the filmmaker.\"\n\nAnachronism can also be an aesthetic choice. Anachronisms abound in the works of Raphael and Shakespeare, as well as in those of less celebrated painters and playwrights of earlier times. Anachronisms can exist in ancient texts. Carol Meyers says that these anachronisms can be used to better understand the stories by asking what the anachronism represents. Repeated anachronisms and historical errors can become an accepted part of popular culture, such as Roman legionaries that wear leather armor.\nComedy fiction set in the past may use anachronism for humorous effect. Comedic anachronism can be used to make serious points about both historical and modern society, such as drawing parallels to political or social conventions.\n\nEven with careful research, science fiction writers risk anachronism as their works age because they cannot predict all political and technological change.\n\nFor example, many books and films nominally set in the mid-21st century or later refer to the Soviet Union, to Saint Petersburg in Russia as Leningrad, to the continuing struggle between the Eastern and Western Blocs and to divided Germany and divided Berlin. \"Star Trek\" has suffered from future anachronisms; instead of \"retconning\" these errors, the 2009 film retained them for consistency with older franchises. \n\nBuildings or natural features, such as the World Trade Center in New York City, can become out of place once they disappear.\n\nLanguage anachronisms in novels and films are quite common, both intentional and unintentional. Intentional anachronisms inform the audience more readily about a film set in the past. In this regard, language and pronunciation change so fast that most modern people (even many scholars) would find it difficult, or even impossible, to understand a film with dialogue in 15th-century English; thus, we willingly accept characters speaking an updated language, and modern slang and figures of speech are often used in these films.\n\nUnintentional anachronisms may occur even in what are intended as wholly objective and accurate records or representations of historic artifacts and artworks, because the recorder's perspective is conditioned by the assumptions and practices of his or her own times (a form of cultural bias). One example is the attribution of historically inaccurate beards to various medieval tomb effigies and figures in stained glass in records made by English antiquaries of the late 16th and early 17th centuries. Working in an age in which beards were in fashion and widespread, the antiquaries seem to have subconsciously projected the fashion back into an era in which it was rare.\n\nThe extensive science fiction subgenre depicting time travel in effect consists of deliberate, consciously created anachronisms, letting people of one time meet and interact with those of another time. Covers of time-travel books often depict deliberate anachronisms of this kind. For example, the cover of Harry Turtledove's \"The Guns of the South\" (1992) features a portrait of Confederate General Robert E. Lee holding an AK-47 rifle.\n\nIn historical writing, the most common type of anachronism is the adoption of the political, social or cultural concerns and assumptions of one era to interpret or evaluate the events and actions of another. The anachronistic application of present-day perspectives to comment on the historical past is sometimes described as presentism. Empiricist historians, working in the traditions established by Leopold von Ranke in the 19th century, regard this as a great error, and a trap to be avoided. Arthur Marwick has argued that \"a grasp of the fact that past societies are very different from our own, and ... very difficult to get to know\" is an essential and fundamental skill of the professional historian; and that \"anachronism is still one of the most obvious faults when the unqualified (those expert in other disciplines, perhaps) attempt to do history\". Anachronism in academic writing is considered at best embarrassing, as in early-20th-century scholarship's use of Translatio imperii, first formulated in the 12th century, to interpret 10th-century literature.\n\nThe use of anachronism in a rhetorical or hyperbolic sense is more complex. To refer to the Holy Roman Empire as the First Reich, for example, is technically inaccurate but may be a useful comparative exercise; the application of theory to works which predate Marxist, Feminist or Freudian subjectivities is considered an essential part of theoretical practice. In most cases, however, the practitioner will acknowledge or justify the use or context.\n\nThe ability to identify anachronisms may be employed as a critical and forensic tool to demonstrate the fraudulence of a document or artifact purporting to be from an earlier time. Anthony Grafton discusses, for example, the work of the 3rd-century philosopher Porphyry, of Isaac Casaubon (1559–1614), and of Richard Reitzenstein (1861–1931), all of whom succeeded in exposing literary forgeries and plagiarisms, such as those included in the \"Hermetic Corpus\", through – among other techniques – the recognition of anachronisms. The detection of anachronisms is an important element within the scholarly discipline of diplomatics, the critical analysis of the forms of documents, developed by the Maurist scholar Jean Mabillon (1632–1707) and his successors René-Prosper Tassin (1697–1777) and Charles-François Toustain (1700–1754). The philosopher and reformer Jeremy Bentham wrote at the beginning of the 19th century:\nThe falsehood of a writing will often be detected, by its making direct mention of, or allusions more or less indirect to, some fact posterior to the date which it bears. ... \"The mention of posterior facts;\" – first indication of forgery.<br>\n\nIn a living language there are always variations in words, in the meaning of words, in the construction of phrases, in the manner of spelling, which may detect the age of a writing, and lead to legitimate suspicions of forgery. ... \"The use of words not used till after the date of the writing;\" – second indication of forgery.\n\nThe exposure by Lorenzo Valla in 1440 of the so-called Donation of Constantine, a decree purportedly issued by the Emperor Constantine the Great in either 315 or 317 AD, as a later forgery, depended to a considerable degree on the identification of anachronisms, such as references to the city of Constantinople (a name not in fact bestowed until 330 AD). A large number of apparent anachronisms in the Book of Mormon have served to convince critics that the book was written in the 19th century, and not, as its adherents claim, in pre-Columbian America. The use of 19th- and 20th-century anti-semitic terminology demonstrates that the purported \"Franklin Prophecy\" (attributed to Benjamin Franklin, who died in 1790) is a forgery. The \"\"William Lynch speech\", an address, supposedly delivered in 1712, on the control of slaves in Virginia, is now considered to be a 20th-century forgery, partly on account of its use of anachronistic terms such as \"program\" and \"refueling\".\n\n\n"}
{"id": "37229379", "url": "https://en.wikipedia.org/wiki?curid=37229379", "title": "Anserated", "text": "Anserated\n\nAnserated refers to a condition where extremities of a creature end in the head of an eagle, lion etc.\n\n"}
{"id": "1367992", "url": "https://en.wikipedia.org/wiki?curid=1367992", "title": "Anthrobotics", "text": "Anthrobotics\n\nAnthrobotics is the science of developing and studying robots that are either entirely or in some way human-like.\n\nThe term \"anthrobotics\" was originally coined by Mark Rosheim in a paper entitled \"Design of An Omnidirectional Arm\" presented at the IEEE International Conference on Robotics and Automation, May 13–18, 1990, pp. 2162–2167. Rosheim says he derived the term from \"...Anthropomorphic and Robotics to distinguish the new generation of dexterous robots from its simple industrial robot forebears.\" The word gained wider recognition as a result of its use in the title of Rosheim's subsequent book \"Robot Evolution: The Development of Anthrobotics\", which focussed on facsimiles of human physical and psychological skills and attributes.\n\nHowever, a wider definition of the term \"anthrobotics\" has been proposed, in which the meaning is derived from \"anthropology\" rather than \"anthropomorphic\". This usage includes robots that respond to input in a human-like fashion, rather than simply mimicking human actions, thus theoretically being able to respond more flexibly or to adapt to unforeseen circumstances. This expanded definition also encompasses robots that are situated in social environments with the ability to respond to those environments appropriately, such as insect robots, robotic pets, and the like.\n\nAnthrobotics is now taught at some universities, encouraging students not only to design and build robots for environments beyond current industrial applications, but also to speculate on the future of robotics that are embedded in the world at large, as mobile phones and computers are today. In 2016 philosopher Luis de Miranda created the Anthrobotics Cluster at the University of Edinburgh \"a platform of cross-disciplinary research that seeks to investigate some of the biggest questions that will need to be answered\" on the relationship between humans, robots and intelligent systems and \"a think tank on the social spread of robotics, and also how automation is part of the definition of what humans have always been\". to explore the symbiotic relationship between humans and automated protocols.\n\n"}
{"id": "23225955", "url": "https://en.wikipedia.org/wiki?curid=23225955", "title": "Art valuation", "text": "Art valuation\n\nArt valuation, an art-specific subset of financial valuation, is the process of estimating either the market value of works of art. As such, it is more of a financial rather than an aesthetic concern, however, subjective views of cultural value play a part as well. Art valuation involves comparing data from multiple sources such as art auction houses, private and corporate collectors, curators, art dealer activities, gallerists (gallery owners), experienced consultants, and specialized market analysts to arrive at a value. Art valuation is accomplished not only for collection, investment, divestment, and financing purposes, but as part of estate valuations, for charitable contributions, for tax planning, insurance, and loan collateral purposes. This article deals with the valuation of works of fine art, especially contemporary art, at the top end of the international market, but similar principles apply to the valuation of less expensive art and antiques.\n\nThe source of a work's artistic charisma has long been debated between artists who create and patrons who enable, but the charismatic power of artworks on those who would possess them is historically the initial driver of value. In the 1960s that charismatic power started edging over to accommodate commercialized culture and a new industry of art, when aesthetic value fell from prominence to parity with Pop art and Andy Warhol's idea of business art, a recognition that art has become a business and making money in business is an art. One of many artists to follow Warhol is Jeff Koons, a stockbroker turned artist who also borrowed imagery from popular culture and made millions.\n\nFor collectors, the emotional connection felt toward a work or collection creates subjective personal value. The weight assigned by such a collector to that subjective measure as a portion of a work's overall financial value may be greater than that by an art speculator not sharing the collector's emotional investment, however, non-economic value measures such as \"Do I like it?\" or \"Does it speak to me?\" still have economic effect because such measures can be deciding factors in a purchase.\n\nIn contrast, the Art Dealer's Association of America (ADAA) suggests that the key issues are authenticity, quality, rarity, condition, provenance and value. \n\nArt valuation activity concerns itself with estimating market demand, estimating liquidity capability of lots, works, and artists, the condition and provenance of works, and with valuation trends such as average sale price and mean estimates. As with other markets, the art market uses its own industry-specific terms of art or vocabulary, for example, \"bought-in\", describing the disadvantageous situation occurring when a work or lot at auction is returned to its owner having been passed over, withdrawn or otherwise unsold. Valuing art is also necessary when a piece is to be used as collateral. The art-lending market has expanded to an estimated $15 billion to $19 billion of loans outstanding in the USA.\nThe shift in Asia towards investment in western art is a factor that has allowed art lending companies to launch offices in Asia as western works are often easier to use as collateral.\n\nAs in the housing market, \"comparables\" are used to determine what level of demand similar items have in a current market. The freshness of the comparables is important because the art market is fluid and stale comparables will yield estimates that may have little relation to a work's current value. Subject matter and the medium of a work also affect market demand, as does rarity.\n\nLiquidity in the art market means having artworks in very high demand and being able to sell those works without impediment. Art sales slow in downturns resulting in the market becoming more illiquid. There is a greater degree of liquidity risk facing the art investor than with other financial assets because there is a limited pool of potential buyers, and with artworks not reaching their reserve prices and not being sold, this has an effect on the auction prices. In a divorce action between a couple who sought to divide a $102 million collection between them, the couple decided a sale would prove problematic because selling the entire collection and dividing the profit would saturate the market and drive down prices; in reporting on the case, \"The Seattle Times\" described the case as a study on how people measure the value of art, and which counts for more — pragmatism or sentiment. The newspaper reported that one of the two litigants had a more sentimental view of the value of the works, while the other had a more businesslike view, wanting balance and diversification. The newspaper attempted to calculate the value of the many artworks at issue in the case by determining a per-square-inch price based on each piece's value divided by its dimension, to end up with a per-square-inch price to apply to the amount of wall space the businesslike litigant wanted to cover with the available art. \"The Times\" ultimately concluded that using this formula as between the litigants, John Singer Sargent's \"Dans les Oliviers à Capri\" was valued at $26,666.67 per square inch, that the sentimental litigant received $3,082 of appraised value per square inch while the businesslike litigant received $1,942 per square inch, but could cover more wall space.\n\nTrends for values from the world's top auction houses are compared for study of market direction and how that direction affects given artists and works. Valuations for art sold at the market's top houses usually carry more weight than valuations from less established houses, as most of the top houses have hundreds of years of experience.\n\nLong term economic trends can have a great impact on the valuation of certain types of work. In recent decades the values of historic Russian and Chinese art have greatly benefited from increased wealth in those countries creating new and very rich collectors, as the values of Orientalist and Islamic art had earlier been boosted by oil wealth in the Arab world.\n\nOne of many factors in the primary market's price of a living artist's work is a dealer's contract with an artist: many dealers, as stakeholders in their artists' success, agree to buy their own stable of artists' work at auction in order to prevent price drops, to maintain price stability, or to increase perceived value, or all three, thus dealers bidding on their own artists at auction have a direct impact on the selling price for those artists' works and as a result, the valuation of those works.\n\nResearch data available from art auction houses such as Christie's, Sotheby's, Phillips, Bonhams, and Lyon & Turnbull are those tracking market trends such as yearly lot transactions, bought-in statistics, sales volume, price levels, and pre-auction estimates. There are also companies such as ArtTactic utilizing art auction data as providers of art market research analysis. Information available from internet-based art sale history databases generally does not include the condition of a work, a very important factor, therefore the prices quoted in those databases reflect auction hammer price without other, crucial valuation factors. Additionally, the databases of auctioned work do not cover private sales of works, and thus their use for art valuation is but one source among many needed for determining value.\n\nValuation estimates by auction houses are typically given in ranges of prices to offset uncertainty. Generally, estimates are made by looking at what a comparable piece of art sold for recently, with estimates given in a range of prices rather than one fixed figure, and in the case of contemporary art especially, having few comparables or when an artist is not well known and has no auction history, the risks of incorrect valuation are greatest.\n\nOne potential factor in valuation are the seller's reasons for selling a particular work and/or the buyer's reasons for buying, neither of which may have anything to do with the other. For example, a seller may be motivated by financial need, boredom with a particular artwork, or the desire to raise funds for a different purchase. Common motivations include the so-called three D's: death, debt and divorce. Buyers may be motivated by market excitement, may be acting in accord with a collection plan, or buying like buyers of stock: to drive value up or down either for themselves or for another person. \n\nOne method for pricing pieces by new artists of uncertain value is to ignore aesthetics and consider, besides market trends, three semi-commoditized aspects: \"scale\" - size and level of detail, \"intensity\" - effort, and \"medium\" - quality of the materials.\n\nValuing artworks in such a specialized market, therefore, takes into account a wide variety of factors, some indeed in conflict with each other.\n\nIn the United States, art bought and sold by collectors is treated as a capital asset for tax purposes, and disputes relating to the valuation of art or the nature of gain on its sale are usually decided by the federal Tax Court, and sometimes by other courts applying federal tax law to specific cases.\n\n\n"}
{"id": "9110963", "url": "https://en.wikipedia.org/wiki?curid=9110963", "title": "Articulatory gestures", "text": "Articulatory gestures\n\nArticulatory gestures are the actions necessary to enunciate language. Examples of articulatory gestures are the hand movements necessary to enunciate sign language and the mouth movements of speech. In semiotic terms, they are the physical embodiment (signifiers) of speech signs, which are gestural by nature (see below).\n\nThe definition of \"gesture\" varies greatly, but here it will be taken in its widest sense, namely, any meaningful action. An intentional action is meaningful if it is not strictly utilitarian: for example, sending flowers to a friend is a gesture, because this action is performed not only for the purpose of moving flowers from one place to another, but also to express some sentiment or even a conventional message in the language of flowers. Use of the broadest definition of gesture (not restricted to hand movements) allows Hockett’s “rapid fading” design feature of human language to be accommodated as a type of sign in semiotic theory.\n\nBut if an articulatory gesture is to be considered a true gesture in the above sense, it must be meaningful. Therefore, an articulatory gesture must be at least as large as the smallest meaningful unit of language, the morpheme. A morpheme corresponds roughly to a spoken word or a sign language gesture. \n\nThis definition differs from the practice, common among linguists, of referring to phonemes (meaningless mouth movements) as articulatory gestures (see articulatory phonology). In semiotics, meaningless components of spoken gestures (written as individual letters), or meaningless components of sign language gestures (such as location of hand contact) are known as figurae, the constituents of signs. \n\nIt also differs from the tradition of considering speech sounds to be the signifiers of speech signs. But this practice confuses signals with symbols. Sound and light are analogue signals, whereas mouth and hand gestures are discrete symbolic entities. A sound or light signal is subject to random noise, whereas the image of the gesture is subject to regular distortion, as when a signer’s hand is viewed from different angles. In speech, the sound of the contact of the tongue in the letter T can be distorted by surrounding mouth movements, as in the phrase “perfect memory”. When pronounced at conversational speed, the sound of the tongue contact is completely obscured by surrounding consonants even though this T movement is fully carried out.\n\nArticulatory gestures, when seen as the physical embodiment of speech and sign language symbols, provide a link between these two language types, and show how speech resembles sign language more closely than is generally presumed.\n\n\n"}
{"id": "2604638", "url": "https://en.wikipedia.org/wiki?curid=2604638", "title": "Avunculate", "text": "Avunculate\n\nThe avunculate, sometimes called avunculism or avuncularism, is any social institution where a special relationship exists between an uncle and his sisters' children. This relationship can be formal or informal, depending on the society. Early anthropological research focused on the association between the avunculate and matrilineal descent, while later research has expanded to consider the avunculate in general society.\n\nThe term \"avunculate\" comes from the Latin \"avunculus,\" the maternal uncle.\n\nThe 1989 \"Oxford English Dictionary\" defines \"avunculate\" as follows:\n\nAn avunculocal society is one in which a married couple traditionally lives with the man's mother's eldest brother, which most often occurs in matrilineal societies. The anthropological term \"avunculocal residence\" refers to this convention, which has been identified in about 4% of the world's societies.\n\nThis pattern generally occurs when a man obtains his status, his job role, or his privileges from their nearest elder matrilineal male relative. When a woman's son lives near her brother, he is able to more easily learn how he needs to behave in the matrilineal role he has inherited.\n\nAccording to the Kazakh common law, the avunculate nephews could take anything from the relatives of the mother up to three times. In the Kyrgyz past a nephew, at a feast at his maternal uncle or grandfather, could take any horse from their herd or any delicacy.\n\nIn the Southwest United States, the Apache tribe practices a form of this, where the uncle is responsible for teaching the children social values and proper behavior while inheritance and ancestry is reckoned through the mother's family alone. Modern day influences have somewhat but not completely erased this tradition.\n\nThe Chamorros of the Mariana Islands and the Taíno of Turks and Caicos Islands are examples of societies that have practiced avunculocal residence.\n\nResearch on the avunculate in the early 20th century focused on the association between the avunculate and patrilineal/matrilineal societies. Franz Boas categorized various avunculate arrangements based on the location of residence in 1922. Henri Alexandre Juno made the claim that the avunculate in the Tsonga indicated that society had previously been matrilineal. Alfred Radcliffe-Brown identified the Tsonga (BaThonga) of Mozambique, the Tongans of the Pacific, and the Nama of Namibia as avunculate societies as early as 1924. He also expanded the concept to incorporate other family relationships.\n\nLater research moved beyond the issue of matrilinealism. Claude Lévi-Strauss incorporating the avunculate into his \"atom of kinship\". Jan N. Bremmer argued based on a survey of the Indo-European peoples that the avunculate is explained by the principle of education outside the (extended) family, and does not indicate matrilinealism.\n\nIn historical (not anthropological) terminology, an avunculate marriage is the marriage of a man with the daughter of his sister (not explicitly forbidden by the listings in Leviticus 18). In most cultures with avunculate customs in the sense used by anthropologists, such a marriage would violate incest taboos governing relations between members of the same matrilineal lineage.\n\n\n"}
{"id": "2226891", "url": "https://en.wikipedia.org/wiki?curid=2226891", "title": "Ballad opera", "text": "Ballad opera\n\nThe ballad opera is a genre of English stage entertainment that originated in the early 18th century, and continued to develop over the following century and later. Like the earlier \"comédie en vaudeville\" and the later \"Singspiel\", its distinguishing characteristic is the use of tunes in a popular style (either pre-existing or newly composed) with spoken dialogue. These English plays were 'operas' mainly insofar as they satirized the conventions of the imported \"opera seria\". Music critic Peter Gammond describes the ballad opera as \"an important step in the emancipation of both the musical stage and the popular song.\"\n\nBallad opera has been called an \"eighteenth-century protest against the Italian conquest of the London operatic scene.\" It consists of racy and often satirical spoken (English) dialogue, interspersed with songs that are deliberately kept very short (mostly a single short stanza and refrain) to minimize disruptions to the flow of the story, which involves lower class, often criminal, characters, and typically shows a suspension (or inversion) of the high moral values of the Italian opera of the period.\n\nIt is generally accepted that the first ballad opera, and the one that was to prove the most successful, was \"The Beggar's Opera\" of 1728. It had a libretto by John Gay and music arranged by Johann Christoph Pepusch, both of whom probably experienced vaudeville theatre in Paris, and may have been motivated to reproduce it in an English form. They were also probably influenced by the burlesques and musical plays of Thomas D'Urfey (1653–1723) who had a reputation for fitting new words to existing songs; a popular anthology of these settings was published in 1700 and frequently re-issued. A number of the tunes from this anthology were recycled in \"The Beggar's Opera\".\n\nAfter the success of \"The Beggar's Opera\", many similar pieces were staged. The actor Thomas Walker, who played Macheath in the original production, wrote several ballad operas, and Gay produced further works in this style, including a much less successful sequel, \"Polly\". Henry Fielding, Colley Cibber, Charles Coffey, Thomas Arne, Charles Dibdin, Arnold, Shield, Jackson of Exeter, Hook and many others produced ballad operas that enjoyed great popularity. By the middle of the century, however, the genre was already in decline.\n\nAlthough they featured the lower reaches of society, the audiences for these works were typically the London bourgeois. As a reaction to serious opera (at this time almost invariably sung in Italian), the music, for these audiences, was as satirical in its way as the words of the play. The plays themselves contained references to contemporary politics—in \"The Beggar's Opera\" the character Peachum was a lampoon of Sir Robert Walpole. This satirical element meant that many of them risked censorship and banning—as was the case with Gay's successor to \"The Beggar's Opera\", \"Polly\".\n\nThe tunes of the original ballad operas were almost all pre-existing (somewhat in the manner of a modern \"jukebox musical\"): however they were taken from a wide variety of contemporary sources, including folk melodies, popular airs by classical composers (such as Purcell) and even children's nursery rhymes. A significant source from which the music was drawn was the fund of popular airs to which 18th century London broadside ballads are set. It is from this connection that the term \"ballad opera\" is drawn. This ragbag of \"pre-loved\" music is a good test for distinguishing between the original type of ballad opera and its later forms. Many ballad operas used the same tunes, such as \"Lillibullero\", and by about 1750 it had become clear that there was a need for new tunes to be written. In 1762, Thomas Arne's \"Love in a Village\" presented a new form of ballad opera, with mainly new music and much less reliance on traditional tunes. It was followed in similar style by Charles Dibdin's \"Lionel and Clarissa\" in 1768.\n\n\"The Disappointment\" (1762) represents an early American attempt at such a ballad opera.\n\nIn 1736 the Prussian ambassador in England commissioned an arrangement in German of a popular ballad opera, \"The Devil to Pay\", by Charles Coffey. This was successfully performed in Hamburg, Leipzig and elsewhere in Germany in the 1740s. A new version was produced by C. F. Weisse and Johann Adam Hiller in 1766. The success of this version was the first of many by these collaborators, who have been called (according to Grove) \"the fathers of the German Singspiel\". (The storyline of \"The Devil to Pay\" was also adapted for Gluck for his 1759 French opera \"Le diable à quatre\").\n\nA later development, also often referred to as ballad opera, was a more \"pastoral\" form. In subject matter, especially, these \"ballad operas\" were antithetical to the more satirical variety. In place of the rag-bag of pre-existing music found in (for example) \"The Beggar's Opera\", the scores of these works consisted in the main of original music, although they not infrequently quoted folk melodies, or imitated them. Thomas Arne and Isaac Bickerstaffe's \"Love in a Village\", and William Shield's \"Rosina\" (1781) are typical examples. Many of these works were introduced as after-pieces to performances of Italian operas.\n\nLater in the century broader comedies such as Richard Brinsley Sheridan's \"The Duenna\" and the innumerable works of Charles Dibdin moved the balance back towards the original style, but there was little remaining of the impetus of the satirical ballad opera.\n\nEnglish 19th century opera is very heavily drawn from the \"pastoral\" form of the ballad opera, and traces even of the satiric kind can be found in the work of \"serious\" practitioners such as John Barnett. Much of the satiric spirit (albeit in a greatly refined form) of the original ballad opera can be found in Gilbert's contribution to the Savoy operas of Gilbert and Sullivan, and the more pastoral form of ballad opera is satirised in one of Gilbert and Sullivan's early works, \"The Sorcerer\" (1877). Balfe's opera \"The Bohemian Girl\" (1843) is one of the few British Ballad operas of the 20th century to gain international recognition.\n\n\"The Threepenny Opera\" of Kurt Weill and Bertolt Brecht (1928) is a reworking of \"The Beggar's Opera\", setting a similar story with the same characters, and containing much of the same satirical bite. On the other hand, it uses just one tune from the original—all the other music being specially composed, and thus omits one of the most distinctive features of the original ballad opera.\n\nIn a completely different vein, \"Hugh the Drover\", an opera in two acts by Ralph Vaughan Williams first staged in 1924, is also sometimes referred to as a \"ballad opera\". It is plainly much closer to Shield's \"Rosina\" than to \"The Beggar's Opera\".\n\nIn the twentieth century folk singers have produced musical plays with folk or folk-like songs called \"ballad operas\". Alan Lomax, Pete Seeger, Burl Ives, and others recorded \"The Martins and the Coys\" in 1944, and Peter Bellamy and others recorded \"The Transports\" in 1977. The first of these is in some ways connected to the \"pastoral\" form of the ballad opera, and the latter to the satiric \"Beggar's Opera\" type, but in all they represent yet further reinterpretations of the term.\n\nIronically, it is in the musicals of Kander and Ebb—especially \"Chicago\" and \"Cabaret\"—that the kind of satire embodied in \"The Beggar's Opera\" and its immediate successors is probably best preserved, although here, as in Weill's version, the music is specially composed, unlike the first ballad operas of the 18th century.\n\n\n"}
{"id": "9993804", "url": "https://en.wikipedia.org/wiki?curid=9993804", "title": "Big man (anthropology)", "text": "Big man (anthropology)\n\nA big man is a highly influential individual in a tribe, especially in Melanesia and Polynesia. Such a person may not have formal tribal or other authority (through for instance material possessions, or inheritance of rights), but can maintain recognition through skilled persuasion and wisdom. The big man has a large group of followers, both from his clan and from other clans. He provides his followers with protection and economic assistance, in return receiving support which he uses to increase his status.\n\nThe American anthropologist Marshall Sahlins has studied the big man phenomenon. In his much-quoted \n1963 article \"Poor Man, Rich Man, Big Man, Chief: Political Types in Melanesia and Polynesia\", Sahlins uses analytically constructed ideal-types of hierarchy and equality to compare a larger-scale Polynesian-type hierarchical society of chiefs and sub-chiefs with a Melanesian-type big-man system.\n\nThe latter consists of segmented lineage groups, locally held together by faction-leaders who compete for power in the social structure of horizontally arranged and principally equal groupings (factions). Here, leadership is not ascribed, but rather gained through action and competition \"with other ambitious men\".\n\nA big man's position is never secured in an inherited position at the top of a hierarchy, but is always challenged by the different big-men who compete with one another in an ongoing process of reciprocity and re-distribution of material and political resources. As such the big man is subject to a transactional order based on his ability to balance the simultaneously opposing pulls of securing his own renown through distributing resources to other big man groups (thereby spreading the word of his power and abilities) \"and\" redistributing resources to the people of his own faction (thereby keeping them content followers of his able leadership).\n\nThe big man concept is relatively fluid, and formal authority of such figures is very low to nonexistent. His position is not inherently heritable.\n\nIn the Island of Malaita in Solomon Islands the big man system is dying away as westernization is influencing the people, but the big man system can be seen at the political level. Every four years in Solomon Islands' National Elections the system can be clearly seen among the people, especially in the Melanesian Islands.\n\nThe first use of the term may be found in the English-translation of Dreissig Jahre in der Sudsee (1907) by Richard Parkinson. The term may be often found in many historical works dealing with Papua New Guinea. Andrew J. Strathern applies the concept of big-men to a community in Mount Hagen, Papua New Guinea.\n\nTraditionally, among peoples of non-Austronesian-speaking communities, authority was obtained by a man (the so-called \"big man\") recognised as \"performing most capably in social, political, economic and ceremonial activities\". His function was not to command, but to influence his society through his example. He was expected to act as a negotiator with neighbouring groups, and to periodically redistribute food (generally produced by his wives). In this sense, he was seen as ensuring the well-being of his community.\n\nSuch a system is still found in many parts of Papua New Guinea, and other parts of Melanesia.\n\n"}
{"id": "9568471", "url": "https://en.wikipedia.org/wiki?curid=9568471", "title": "Biolinguistics", "text": "Biolinguistics\n\nBiolinguistics is the study of the biology and evolution of language. It is a highly interdisciplinary field, including linguists, biologists, neuroscientists, psychologists, mathematicians, and others. By shifting the focus of investigation in linguistics to a comprehensive scheme that embraces natural sciences, it seeks to yield a framework by which we can understand the fundamentals of the faculty of language.\n\nThe biolinguistic perspective began to take shape in the mid-twentieth century, among the linguists influenced by the developments in biology and mathematics. Eric Lenneberg’s \"Biological Foundations of Language\" remains a basic document of the field. In 1974, the first Biolinguistic conference was organized by Massimo Piattelli-Palmarini, bringing together evolutionary biologists, neuroscientists, linguists, and others interested in the development of language in the individual, its origins, and evolution.\n\nRecent work in theoretical linguistics and cognitive studies at MIT construes human language as a highly non-redundant species-specific system. Noam Chomsky's latest contribution to the study of the mind in general and language in particular is his minimalist approach to syntactic representations. This effort to understand how much of language can be given a principled explanation has resulted in the minimalist program. In syntax, lexical items are merged externally, building argument representations; next, the internal merge induces movement and creates constituent structures where each is part of a larger unit. This mechanism allows people to combine words into infinite strings. If this is true, then the objective of biolinguists is to find out as much as we can about the principles underlying mental recursion.\n\nIt is possible that the core principles of the language faculty be correlated to natural laws (such as for example, the Fibonacci sequence—an array of numbers where each consecutive number is a sum of the two that precede it, see for example the discussion Uriagereka 1997 and Carnie and Medeiros 2005). According to the hypothesis being developed, the essential properties of language arise from nature itself: the efficient growth requirement appears everywhere, from the pattern of petals in flowers, leaf arrangements in trees and the spirals of a seashell to the structure of DNA and proportions of human head and body. If this law applies to existing systems of cognition, both in humans and non-humans, then what allows our mind to create language? Could it be that a single cycle exists, a unique component of which gives rise to our ability to construct sentences, refer to ourselves and other persons, group objects and establish relations between them, and eventually understand each other? The answer to this question will be a landmark breakthrough, not only within linguistics but in our understanding of cognition in general.\n\nDavid Poeppel, a neuroscientist and linguist, has noted that if neuroscience and linguistics are done wrong, there is a risk of \"inter-disciplinary cross-sterilization\", arguing that there is a \"Granularity Mismatch Problem\", as different levels of representations used in linguistics and neural science lead to vague metaphors linking brain structures to linguistic components. Poeppel and Embick also introduce the \"Ontological Incommensurability Problem\", where computational processes described in linguistic theory cannot be restored to neural computational processes. Poeppel suggests that neurolinguistic research should try to have theories of how the brain encodes linguistic information and what could be cognitively realistic computation.\n\nA more positive critique comes from the side of biosemiotics, claiming that meaning-making begins far before the emergence of human language.\n\n\n\n\n"}
{"id": "3183005", "url": "https://en.wikipedia.org/wiki?curid=3183005", "title": "Blytt–Sernander system", "text": "Blytt–Sernander system\n\nThe Blytt-Sernander classification, or sequence, is a series of north European climatic periods or phases based on the study of Danish peat bogs by Axel Blytt (1876) and Rutger Sernander (1908). The classification was incorporated into a sequence of pollen zones later defined by Lennart von Post, one of the founders of palynology.\n\nLayers in peat were first noticed by Heinrich Dau in 1829. A prize was offered by the Royal Danish Academy of Sciences and Letters to anyone who could explain them. Blytt hypothesized that the darker layers were deposited in drier times; the lighter, in moister times, applying his terms \"Atlantic\" (warm, moist) and \"Boreal\" (cool, dry). In 1926 C.A. Weber noticed the sharp boundary horizons, or \"Grenzhorizonte\", in German peat, which matched Blytt’s classification. Sernander defined subboreal and subatlantic periods, as well as the late glacial periods. Other scientists have since added other information.\n\nThe classification was devised before the development of more accurate dating methods, such as C-14 dating and oxygen isotope ratio cycles. Currently geologists working in different regions are studying sea levels, peat bogs and ice core samples by a variety of methods, with a view toward further verifying and refining the Blytt-Sernander sequence. They find a general correspondence across Eurasia and North America.\n\nThe fluctuations of climatic change are more complex than Blytt-Sernander periodizations can identify. For example, recent peat core samples at Roskilde Fjord and also Lake Kornerup in Denmark identified 40 to 62 distinguishable layers of pollen, respectively. However, no universally accepted replacement model has been proposed.\n\nToday the Blytt-Sernander sequence has been substantiated by a wide variety of scientific dating methods, mainly radiocarbon dates obtained from peat. Earlier radiocarbon dates were often left uncalibrated; that is, they were derived by assuming a constant concentration of atmospheric radiocarbon. In fact the atmospheric radiocarbon concentration has varied over time and thus radiocarbon dates need to be calibrated.\n\nThe Blytt-Sernander classification has been used as a temporal framework for the archaeological cultures of Europe and America. Some have gone so far as to identify stages of technology in north Europe with specific periods; however, this approach is an oversimplification not generally accepted. There is no reason, for example, why the north Europeans should stop using bronze and start using iron abruptly at the lower boundary of the Subatlantic at 600 BC. In the warm Atlantic period, Denmark was occupied by Mesolithic cultures, rather than Neolithic, notwithstanding the climatic evidence. Moreover, the technology stages vary widely globally.\n\nThe Pleistocene phases and approximate calibrated dates (see above) are:\n\nThe Holocene phases are:\n\nSome marker plant genera/species studied in peat are\n\nMore sphagnum appears in wet periods. Dry periods feature more tree stumps, of birch and pine.\n\n"}
{"id": "13845737", "url": "https://en.wikipedia.org/wiki?curid=13845737", "title": "Casting television", "text": "Casting television\n\nCasting television is a competition television program where the winner wins a recording contract, or takes part in a competition.\n\nSlovakia's competition is Superstars, and the prize is a car, cash and recording contracts. Stars such as Zdenka Predna came to prominence this way.\n\nSimon Cowell's \"Popstars\" is perhaps the most famous one of the lot, and other shows on a similar focus are \"Stars in Their Eyes\", and Andrew Lloyd Webber's \"How Do You Solve a Problem Like Maria?\".\n\nFollowing directly on the UK template is the \"American Idol\" competition.\n\nIreland's version, called \"You're a Star\", had a unique twist: winners came into the Eurovision Song Contest as the prize. However, after the disaster of the McCall Twins that format has been abandoned. The most famous winner and most successful in Eurovision (though he did not win) was Mickey Joe Harte.\n\n"}
{"id": "8739390", "url": "https://en.wikipedia.org/wiki?curid=8739390", "title": "Cognized environment", "text": "Cognized environment\n\nCognized environment is a concept first introduced by the late anthropologist, Roy Rappaport (1968), in contrast to what he called the \"operational environment\" (see Rappaport 1979:97-144, 1984:337-352). Rappaport was an ecological anthropologist, like Andrew P. Vayda, and wished to contrast the actual reality and adaptations (the operational environment) within a people's ecological niche – say, the existence of tsetse flies and their role in causing sleeping sickness among humans – with how the people’s culture understands nature (the cognized environment) – say, the belief that witches live in those areas that science knows is the habitat of the tsetse. Rappaport’s principal concern was the role of ritual in mediating the cognized and operational environments.\n\nAnother group of anthropologists later took up the use of Rappaport’s concepts and applied them toward developing a school of neuroanthropology called Biogenetic Structuralism (see Laughlin and Brady 1978: 6, d'Aquili, Laughlin and McManus 1979: 12ff, Rubinstein, Laughlin and McManus 1984: 21ff, and Laughlin, McManus and d'Aquili 1990:82-90). According to this group, all properties and qualities of experience are mediated by our body’s neuroendocrine systems. These systems function individually and collectively to model reality. The sum total of these models in the brain is the cognized environment. The operational environment refers to the actual niche in which the human or other animal with a brain dwells and adapts. The operational environment is the real world that is modelled by our cognized environment.\n\n\n"}
{"id": "1645052", "url": "https://en.wikipedia.org/wiki?curid=1645052", "title": "Colon (rhetoric)", "text": "Colon (rhetoric)\n\nA colon (from Greek: , \"pl.\" , \"cola\") is a rhetorical figure consisting of a clause which is grammatically, but not logically, complete. In Latin, it is called a \"membrum\" or \"membrum orationis\".\n\nSentences consisting of two cola are called \"dicola\"; those with three are \"tricola\". The corresponding adjectives are \"dicolic\" and \"tricolic\"; \"colic\" is not used in this sense. In writing, these cola are often separated by colons.\n\nAn isocolon is a sentence composed of cola of equal syllabic length.\n\nThe Septuagint used this system in the poetic books such as the Psalms. \nWhen Jerome translated the books of the Prophets, he arranged the text colometrically.\n\nThe colometric system was used in bilingual codices of New Testament, such as Codex Bezae and Codex Claromontanus. Some Greek and Latin manuscripts also used this system, including Codex Coislinianus and Codex Amiatinus.\n\n\n\n"}
{"id": "48553215", "url": "https://en.wikipedia.org/wiki?curid=48553215", "title": "Deaccessioning (museum)", "text": "Deaccessioning (museum)\n\nDeaccessioning is defined as the process by which a work of art or other object is permanently removed from a museum’s collection. Deaccessioning is a practical and constructive tool of collections care that, if practiced thoughtfully supports the long-term preservation of a collection and can help a museum refine the scope of its collection in order to better serve its mission and community.\n\nThe process undertaken by a museum to deaccession a work involves several steps that are usually laid out in a museum`s Collection Management Policy. The terms under which an object may be considered for removal, as well as the individuals with the authority to approve the process are outlined in the \"Deaccession\" section of this document. Additionally, this section lays out the legal restrictions and ethic considerations associated with removal of the object and the types of disposal that are appropriate based on the reason for the deaccession.\n\nEach museum establishes its own method and workflow for the deaccession process according to its organizational structure. However all object deaccessioning involves the two processing steps of deaccession and disposal.\n\nThe process begins with the curator creating a document called a \"Statement of Justification,\" which outlines their decision criteria and reasoning for presenting the work as a possible deaccession. In order to determine if a work should be deaccessioned from a museum`s collection, a curator or registrar completes and documents a series of justification steps and then present their findings to the museum director and governing board for final approval.\n\nThere are a number of reasons why deaccessioning might be considered. The following is a typical list of criteria for deaccession and disposal:\n\n\nThe typical steps that need to be taken in order to justify the deaccession and disposal of the work include:\n\n\nDisposal is defined as the transfer of ownership by the museum after a work has been deaccessioned. Following approval of deaccession from the Governing Board and/or the CEO/Museum Director, the work is disposed of and the title of ownership is completely transferred away from the museum or terminated. The method chosen is determined by the physical condition of the work, the intrinsic value or cultural value of the work and extrinsic value or monetary value of the work. With all methods of disposal, museums are charged to maintain and retain all records of the object, its deaccession and disposal.\n\nThe process of disposal is completed through the following methods:\n\n\nSeveral professional museum associations have drafted codes of ethics governing the practice of deaccession.nTwo majors areas of ethical concern that are common in these codes of ethics are the prohibition of sale or transfer of collection items to museum trustees, staff, board members, or their relatives and the need to restrict the use of proceeds from any works disposed of via sale or auction.\n\nThe first of these ethical concerns is rather straightforward. The second has become a point of contention in recent years since museums and cities, like Detroit, have been struggling with financial shortfalls.\n\nAccording to the AAMD (The Association of Art Museum Directors): \"Funds received from the disposal of a deaccessioned work shall not be used for operations or capital expenses. Such funds, including any earnings and appreciation thereon, may be used only for the acquisition of works in a manner consistent with the museum’s policy on the use of restricted acquisition funds.\"\n\nAccording to the AAM (the American Association of Museums): \"Proceeds from the sale of nonliving collections are to be used consistent with the established standards of the museum`s discipline, but in no event shall they be used for anything other than acquisition or direct care of collections.\"\n\nAccording to the AASLH (the American Association for State and Local History): \"Collections shall not be deaccessioned or disposed of in order to provide financial support for institutional operations, facilities maintenance, or any reason other than the preservation or acquisition of collections.\"\n\nAccording to ICOM (the International Council of Museums): Proceeds should be applied solely to the purchase of additions to museum collections.\n\nThese associations have each determined to their own degree that all proceeds from sale or auction should be restricted to the future acquisition of collection objects and/ or to the ongoing maintenance of current collection holdings. Their decision and perspective on the practice of deaccession reflects a long-term view of museum collections as items held in public trust and preserved for access, appreciation, education, and enjoyment of not only today`s public but the future public. See Public trust doctrine\n\nDeaccessioning is a controversial topic and activity, with diverging opinions from artists, arts professionals and the general public. Some commentators, such as Donn Zaretsky of The Art Law Blog critique the notion of \"the public trust\" and argue that deaccessioning rules should probably be thrown out altogether. Others, such as Susan Taylor, director of the New Orleans Museum of Art and the AAMD's current president, believes that proceeds from the sale or funds from the deaccession can only be used to buy other works of art.\n\n"}
{"id": "401963", "url": "https://en.wikipedia.org/wiki?curid=401963", "title": "Deathtrap (plot device)", "text": "Deathtrap (plot device)\n\nA deathtrap is a literary and dramatic plot device in which a villain who has captured the hero or another sympathetic character attempts to use an elaborate, improbable, and usually sadistic method of murdering them.\n\nIt is often used as a means to create dramatic tension in the story and to have the villain reveal important information to the hero, confident that the hero will shortly not be able to use it. It may also be a means to show the hero's resourcefulness in escaping, or the writer's ingenuity at devising a last-minute rescue or deus ex machina.\n\nThis plot device is generally believed to have been popularized by movie serials and 19th-century theatrical melodramas. A well-known example is the cliché of the moustache-twirling villain leaving the heroine tied to railroad tracks. Its use in the James Bond film series and superhero stories is well known.\n\nIt is a common criticism that it is unbelievable in story plots to have villains try to kill the heroes in such elaborate ways when they could use simple methods like shooting them. Through the decades, comic book writers have responded to these complaints by devising ways in which the deathtraps have served other purposes.\n\nFor instance, one \"Legion of Super-Heroes\" story by Jim Shooter had a team of Legionnaires put into a variety of deathtraps and the villains \"wanted\" the heroes to successfully escape. This was because the real purpose of the deathtraps was to have the Legionnaires use a great deal of energy doing so, which the villains then harnessed for their own benefit. Other stories have had villains use deathtraps as a means of testing the heroes or to distract them while the villain attends to other matters. On some occasions, the deathtrap is a machine that \"absorbs\" the energy from the hero/heroes.\n\nAnother rationalization for a deathtrap is when a particular villain simply enjoys leaving his victims some small chance of survival, just for the sake of sport. Such \"sporting\" villains include the Riddler, who has an uncontrollable compulsion to create intellectual challenges for his enemies. Also included in this list is the Jigsaw Killer, who places his victims in life-or-death situations to prove that they appreciate life. The Joker and Arcade are other villains who simply enjoy the challenge.\n\nOn occasion, the villain may employ a slow deathtrap because they enjoy their victim's suffering prior to death, either due to sadistic tendencies or a desire for painful vengeance.\n\nIn a similar vein, the villain, often a megalomaniac, may feel that, as a reflection of his own imagined greatness, it would be \"beneath him\" to murder his enemy like any common criminal, and that his enemy's death should be the worthy spectacle that a successful deathtrap would provide. In contrast, he may feel that his enemy, having provided him with a worthy challenge in their earlier encounters, himself \"deserves\" such a grandiose death, or that the enmity between the two is so \"epic\" that it merits no less than such a conclusion.\n\nConversely, the protagonists' act of falling into such a trap may itself be the reason they are written off and left unattended. The villain, disappointed in such a non-threatening opponent, loses interest, and intentionally leaves some chance of escape for the protagonists to \"redeem\" themselves. However, the disillusioned villain tends to assume that the chance is minuscule. Despite secretly hoping that the opponent survives and proves worthy of interest, the now-bored villain is invariably shocked when that actually occurs.\n\nIf fully serious, the villain may simply be too insane to recognize the impracticality of the situation, although this characterization is rarely seen outside of deliberately parodic characters such as Dr. Evil.\n\nA more recent reason is villains do it simply because it is considered 'tradition' or 'rule' of being a supervillain to place a hero in a deathtrap and then leave them to their fate. This even goes as far as heroes, or other villains, insulting a villain for attempting to avoid using a deathtrap or staying to watch. El Sombrero is one villain who exemplifies this reason.\n\nWhen a hero's sidekick or loved one is placed in a deathtrap, its purpose is often to distract the hero, occupying time and attention while the villain pursues their evil plan. Less frequently, the villain intends to instill grief and guilt as a means of defeating a hero that cannot be defeated physically. Multiple secondary characters may be placed in deathtraps to offer the hero an agonizing choice, ostensibly forcing the hero to save one victim and leave the other(s) to die.\n\nA simpler variation on the deathtrap is the villain speech, also known as \"monologuing\". The villain, after having captured the hero or another victim, gives a long speech taunting and sneering at his victim, pontificating on how said victim will soon die, and reminiscing over how he tried for so long to get his kill and is now about to reap the reward. Villains may also give away details of their evil plots, on the rationale that the victim will die immediately and the villain often believes their victim deserves to know. This speech, given when the villain could have just killed the victim in a matter of seconds, is invariably used to give another character time to come in and save the victim, or for the victim to escape. In \"The Incredibles\" (which used the term \"monologuing\"), Mr. Incredible and Frozone attacked villains in the middle of their speeches (Mr. Incredible is seen attacking Syndrome and Frozone is mentioned to have attacked Baron von Ruthless off-camera). In a literary sense, the villain speech is also used as a form of exposition.\n\nEven in relatively realistic stories, villains will often take a moment to say something pithy before finishing off the victim. The antagonist would often leave the victim to die whilst they commit their evil scheme. This is echoed in the film \"2001 - A Space Odyssey\" when Hal the supercomputer, confident that Dave will soon perish outside the ship, tells him that he is about to take control of the expedition and then sees Dave off with the flat remark: \"This conversation can have no meaningful purpose anymore - goodbye!\". Dave manages to make his way inside and kill Hal.\n\nThe concept of the deathtrap/monologue is featured in many satires.\n\n"}
{"id": "1538554", "url": "https://en.wikipedia.org/wiki?curid=1538554", "title": "Dispositio", "text": "Dispositio\n\nDispositio is the system used for the organization of arguments in Western classical rhetoric. The word is Latin, and can be translated as \"organization\" or \"arrangement\".\n\nIt is the second of five canons of classical rhetoric (the first being inventio, and the remaining being elocutio, memoria, and pronuntiatio) that concern the crafting and delivery of speeches and writing.\n\nThe first part of any rhetorical exercise was to discover the proper arguments to use, which was done under the formalized methods of \"inventio\". The next problem facing the orator or writer was to select various arguments and organize them into an effective discourse.\n\nAristotle defined two essential parts of a discourse: the statement of the case and the proof of the case. For example, in a legal argument, a prosecutor must first declare the charges against the defendant and provide the relevant facts; then he must present the evidence that proves guilt. Aristotle allowed that in practice most discourse also requires an introduction and a conclusion.\n\nLater writers on rhetoric, such as Cicero and Quintilian refined this organizational scheme even further, so that there were eventually six parts:\n\nWhile this structure might appear to be highly rigid (and certainly some writers on the subject were overly pedantic), it was in practice a flexible model. Cicero and Quintilian, for example, encouraged writers to rearrange the structure when it strengthened their case: for instance, if the opposing arguments were known to be powerful, it might be better to place the refutation before the proof.\n\nWithin each major part, there were additional tactics that might be employed. For instance, a prosecutor might sum up his case with forceful repetition of his main points using a technique known as \"accumulatio\". The defense attorney in the same case might use a different approach in his summation.\n\nFinally, \"dispositio\" was also seen as an iterative process, particularly in conjunction with \"inventio\". The very process of organizing arguments might lead to the need to discover and research new ones. An orator would refine his arguments and their organization until they were properly arranged. He would then proceed to those areas that we generally associate with rhetoric today—the development of the style and delivery of the arguments.\n\nThe \"exordium\" (; meaning \"beginning\" in Latin; from \"exordiri\", meaning \"to begin\") was the introductory portion of an oration. The term is Latin and the Greek equivalent was called the \"proem\" or \"prooimion\". \n\nIn the \"exordium\", the orator lays out the purpose of the discourse. In doing this, they need to consider several things:\n\n\nIn short, the \"exordium\" was the portion of the discourse in which the orator would prepare the audience to hear his arguments in a favorable frame of mind. \"An exordium can serve different kinds of functions in the differing species of rhetoric, but in all of them some of the major themes of the coming discourse will be announced in advance\".\n\nThe \"peroratio\" (\"peroration\"), as the final part of a speech, had two main purposes in classical rhetoric: to remind the audience of the main points of the speech (\"recapitulatio\") and to influence their emotions (\"affectus\"). The role of the peroration was defined by Greek writers on rhetoric, who called it \"epilogos\"; but it is most often associated with Roman orators, who made frequent use of emotional appeals. A famous example was the speech of Marcus Antonius in defence of Aquillius, during which Antonius tore open the tunic of Aquillius to reveal his battle scars.\n\nIn the first century B.C. it was common for two or more speakers to appear on each side in major court cases. In such cases it was considered a mark of honour to be asked to deliver the peroration.\n\n"}
{"id": "32998335", "url": "https://en.wikipedia.org/wiki?curid=32998335", "title": "Dominican Institute for Oriental Studies", "text": "Dominican Institute for Oriental Studies\n\nThe Dominican Institute for Oriental Studies or IDEO (from French: \"Institut dominicain d'études orientales\") is a basic research centre located in Cairo. Its research field covers the ten first centuries of Islam.\n\nThe Dominican Institute for Oriental Studies was founded in the early 50's, after the Second World War came to an end. \n\n\n"}
{"id": "1957258", "url": "https://en.wikipedia.org/wiki?curid=1957258", "title": "Electronic media", "text": "Electronic media\n\nElectronic media are media that use electronics or electromechanical audience to access the content. This is in contrast to static media (mainly print media), which today are most often created electronically, but do not require electronics to be accessed by the end user in the printed form. The primary electronic media sources familiar to the general public are video recordings, audio recordings, multimedia presentations, slide presentations, CD-ROM and online content. Most new media are in the form of digital media. However, electronic media may be in either analogue electronics data or digital electronic data format.\n\nAlthough the term is usually associated with content recorded on a storage medium, recordings are not required for live broadcasting and online networking.\n\nAny equipment used in the electronic communication process (e.g. television, radio, telephone, desktop computer, game console, handheld device) may also be considered electronic media.\n\nElectronic media are ubiquitous in most of the developed world. Electronic media devices have found their way into all parts of modern life. The term is relevant to media ecology for studying its impact compared to printed media and broadening the scope of understanding media beyond a simplistic aspect of media such as one delivery platform (e.g. the World Wide Web) aside from many other options. The term is also relevant to professional career development regarding related skill set.\n"}
{"id": "33150436", "url": "https://en.wikipedia.org/wiki?curid=33150436", "title": "European Association for Digital Humanities", "text": "European Association for Digital Humanities\n\nThe European Association for Digital Humanities (EADH), formerly known as the Association for Literary and Linguistic Computing (ALLC), is a digital humanities organisation founded in London in 1973. Its purpose is to promote the advancement of education in the digital humanities through the development and use of computational methods in research and teaching in the Humanities and related disciplines, especially literary and linguistic computing. In 2005, the Association joined the Alliance of Digital Humanities Organizations (ADHO).\n\nA precursor for the later following annual conferences of the association was a meeting on literary and linguistic computing organized by Roy Wisbey and Michael Farringdon at the University of Cambridge in March, 1970. The year after the second conference in Edinburgh, Scotland, in 1972, the Association for Literary and Linguistic Computing was founded at a meeting at King's College, London (1973). Together with the Association for Computers and the Humanities (ACH) and the Association for Computational Linguistics (ACL), the Association for Literary and Linguistic Computing sponsored and organized the Text Encoding Initiative (TEI) in 1987.\n\nIn December 2011 the Association's name was changed to the European Association for Digital Humanities, while keeping the allc.org domain name. The change to 'EADH' was made in 2013.\n\nThe first conferences of the association were held annually until 1988, when a protocol was agreed with the Association for Computing in the Humanities for co-sponsorship of joint international conferences. The venue for these joint conferences alternated between Europe and North America. The first one took place in 1989 at the University of Toronto in Canada. After the Alliance of Digital Humanities Organizations had been formed in 2005, the first joint conference with the new name “Digital Humanities” was held at the Sorbonne in Paris, France, in 2006.\n\nInitially, EADH published its own \"Bulletin\" three times a year; its journal twice yearly from 1980 to 1985. Afterwards, bulletin and journal were merged in order to become \"Literary and Linguistic Computing\" (LLC) in 1986. Literary and Linguistic Computing is a peer-reviewed, international journal that publishes texts \"on all aspects of computing and information technology applied to literature and language research and teaching.\"\nMembership of the association is by subscription to LLC. As of December 2010 there were 314 individual subscribers to Literary and Linguistic Computing.\n\n\n"}
{"id": "12634188", "url": "https://en.wikipedia.org/wiki?curid=12634188", "title": "Exocannibalism", "text": "Exocannibalism\n\nExocannibalism (from Greek \"exo-\", \"from outside\" and cannibalism, \"to eat humans\"), as opposed to endocannibalism, is the consumption of flesh outside one's close social group—for example, eating one's enemy. When done ritually, it has been associated with being a means of imbibing valued qualities of the victim or as an act of final violence against the deceased in the case of sociopathy, as well as a symbolic expression of the domination of an enemy in warfare. Such practices have been documented in cultures including the Aztecs from Mexico, the Carib and the Tupinambá from South America.\n\nHistorically, it has also been used as a practical expediency in especially desperate attritional or guerrilla warfare when the extreme hunger and the abundance of humans being killed coincide to create conditions ripe for cannibalism. Some viewed the practice of exocannibalism as an act of predation tying the action to more of a prey versus predator scenario than one of ceremonial meaning. Exocannibalism has also historically been viewed as a way to acquire the strength and ability of a defeated enemy. It serves as a final act to either intake or extinguish the existence of an enemy. Notably, the cultures that view exocannibalism as a form of predation do not view the act as taboo. \n\nCannibalism is something that has been found wherever and whenever humans have formed societies. Traditionally, accounts of cannibalism were found embedded in myths and folklore as a common motive that indicated people were less than fully human. Exocannibalism in the form of eating enemies is usually done to express hostility and domination toward the victim. The perpetrator eats their victim to inflict ultimate indignity and humiliation. It has also been practiced along with headhunting and scalping to display war trophies. John Kantner, an archaeologist who studied alleged cannibalism in the American Southwest, believes that when resources decrease the competition of societies increased and exocannibalism can ensue. Exocannibalism would generally be considered to be the opposite of endocannibalism, but they are both forms of ritual cannibalism. There have been no previous accounts of a culture practicing both forms of ritual cannibalism, aside from a recent study that confirmed the Wari', an Amazonian tribe in Brazil, practiced both forms.\n\n\nThe Wari people of South America are known for their practice of both endocannibalism and exocannibalism. Endocannibalism had the ability to serve as a form of recognition and respect for the dead. Exocannibalism on the other hand was part of warfare. The Wari had very separate motives behind why they performed each of these modes of cannibalism but both forms had the same basic steps of roasting either flesh or bone and then eating it. Wari warriors would kill enemies such as the Brazilians, Bolivians, and members of enemy tribes. The Wari consumed these enemies as a means of transforming them into a form of prey. They viewed warfare cannibalism as a form of predation or hunting. They used exocannibalism as a means by which to label their enemies as subhuman and make their flesh as unimportant as that of any other animal that was typically killed for food. This practice of cannibalism was continued by the Wari people until the 1960s.\n\nThe people of Fiji are also documented as having participated in exocannibalism as a form of ritualistic behavior, though history of this is typically hidden by European modification. From Fijian legend, the development of the island was due to a god who brought with him cannibalism and warfare. When he arrived on the island, he then married into the single indigenous family. That family then populated the island. This legend along with cannibalism continued into the reality of Fijian people. During wartime, chiefs were able to have their pick of the warriors and soldiers who were killed, seeking out the most famous of those slain. The rest of the soldiers killed that chief did not want would be consumed by the rest of the common people. This form of consumption of the dead was not out of need but instead served as a means by which to assert their power over a conquered people. Consumption of human flesh was not viewed as taboo, but instead was viewed as an act of dining with the gods or dining on the food of gods. Along with consuming the flesh to show domination over slain enemies, cannibalism was also part of both political and religious rituals performed by the Fijian people. Cannibalism persisted in the Fijian culture because of the cultural beliefs regarding it.\n\n\n"}
{"id": "24082935", "url": "https://en.wikipedia.org/wiki?curid=24082935", "title": "Gender differences in suicide", "text": "Gender differences in suicide\n\nGender differences in suicide rates have been shown to be significant. There are different rates of completed suicides and suicidal behavior between males and females. While women more often have suicidal thoughts, men die by suicide more frequently. This is also known as the gender paradox in suicide.\n\nGlobally, death by suicide occurred about 1.8 times more often among males than among females in 2008, and 1.7 times in 2015. In the western world, males die by suicide three to four times more often than do females. This greater male frequency is increased in those over the age of 65. Suicide attempts are between two and four times more frequent among females. Researchers have attributed the difference between attempted and completed suicides among the sexes to males using more lethal means to end their lives. The extent of suicidal thoughts is not clear, but research suggests that suicidal thoughts are more common among females than among males, particularly in those under the age of 25.\n\nThe role that gender plays as a risk factor for suicide has been studied extensively. While females show higher rates of non-fatal suicidal behavior and suicide ideation (thoughts), and reportedly attempt suicide more frequently than males do, males have a much higher rate of completed suicides.\n\nAs of recent World Health Organization (WHO) releases, challenges represented by social stigma, the taboo to openly discuss suicide, and low availability of data are still to date obstacles leading to poor data quality for both suicide and suicide attempts: \"given the sensitivity of suicide – and the illegality of suicidal behaviour in some countries – it is likely that under-reporting and misclassification are greater problems for suicide than for most other causes of death.\".\n\nMany researchers have attempted to find explanations for why gender is such a significant indicator for suicide.\nA common explanation relies on the social constructions of hegemonic masculinity and femininity. According to literature on gender and suicide, male suicide rates are explained in terms of traditional gender roles. Male gender roles tend to emphasize greater levels of strength, independence, risk-taking behavior, economic status, individualism. Reinforcement of this gender role often prevents males from seeking help for suicidal feelings and depression.\n\nNumerous other factors have been put forward as the cause of the gender paradox. Part of the gap may be explained by heightened levels of stress that result from traditional gender roles. For example, the death of a spouse and divorce are risk factors for suicide in both genders, but the effect is somewhat mitigated for females. In the Western world, females are more likely to maintain social and familial connections that they can turn to for support after losing their spouse. Another factor closely tied to gender roles is employment status. Males' vulnerability may be heightened during times of unemployment because of societal expectations that males should provide for themselves and their families.\n\nIt has been noted that the gender gap is less stark in developing nations. One theory put forward for the smaller gap is the increased burden of motherhood due to cultural norms. In regions where the identity of females is constructed around the family, having young children may correlate with lower risks for suicide. At the same time, stigma attached to infertility or having children outside of marriage can contribute to higher rates of suicide among women.\n\nIn 2003, a group of sociologists examined the gender and suicide gap by considering how cultural factors impacted suicide rates. The four cultural factors – power-distance, individualism, uncertainty avoidance, and masculinity – were measured for 66 countries using data from the World Health Organization. Cultural beliefs regarding individualism were most closely tied to the gender gap; countries that placed a higher value on individualism showed higher rates of male suicide. Power-distance, defined as the social separation of people based on finances or status, was negatively correlated with suicide. However, countries with high levels of power-distance had higher rates of female suicide. The study ultimately found that stabilizing cultural factors had a stronger effect on suicide rates for women than men.\n\nThe reported difference in suicide rates for males and females is partially a result of the methods used by each gender. Although females attempt suicide at a higher rate, they are more likely to use methods that are less immediately lethal. Males frequently complete suicide via high mortality actions such as hanging, carbon-monoxide poisoning, and firearms. This is in contrast to females, who tend to rely on drug overdosing. While overdosing can be deadly, it is less immediate and therefore more likely to be caught before death occurs. In Europe, where the gender discrepancy is the greatest, a study found that the most frequent method of suicide among both genders was hanging; however, the use of hanging was significantly higher in males (54.3%) than in females (35.6%). The same study found that the second most common methods were firearms (9.7%) for men and poisoning by drugs (24.7%) for women.\n\nIn the United States, both the Department of Health and Human Services and the American Foundation for Suicide Prevention address different methods of reducing suicide, but do not recognize the separate needs of males and females. In 2002, the English Department of Health launched a suicide prevention campaign that was aimed at high-risk groups including young men, prisoners, and those with mental health disorders. The Campaign Against Living Miserably is a charity in the UK that attempts to highlight this issue for public discussion. Some studies have found that because young females are at a higher risk of attempting suicide, policies tailored towards this demographic are most effective at reducing overall rates. Researchers have also recommended more aggressive and long-term treatments and follow up for males that show indications of suicidal thoughts. Shifting cultural attitudes about gender roles and norms, and especially ideas about masculinity, may also contribute to closing the gender gap.\n\nThe incidence of completed suicide is vastly higher among males than females among all age groups in most of the world. As of 2015, almost two-thirds of worldwide suicides (representing about 1.5% of all deaths) are committed by men.\n\nSince the 1950s, typically males have died from suicidal attempts three to five times more often than females. Use of mental health resources may be a significant contributor to the gender difference in suicide rates in the US. Studies have shown that females are 13–21% more likely than males to receive a psychiatric affective diagnosis. 72–89% of females who committed suicide had contact with a mental health professional at some point in their life and 41–58% of males who committed suicide had contact with a mental health professional.\n\nWithin the United States, there are variances in gendered rates of suicide by ethnic group. According to the CDC, as of 2013 the suicide rates of Whites and American Indians are more than twice the rates of African Americans and Hispanics. Explanations for why rates of attempted and completed suicide vary by ethnicity are often based on cultural differences. Among African American suicides, it has been suggested that females usually have better access to communal and familial relations that may mitigate other risk factors for suicide. Among Hispanic populations, the same study showed that cultural values of \"marianismo\", which emphasizes female docility and deference to males, may help explain the higher rate of suicide of Latinas relative to Latinos. The authors of this study did not extrapolate their conclusions on ethnicity to populations outside the United States.\n\nThe gender-suicide gap is generally highest in Western countries. Among the nations of Europe, the gender gap is particularly large in Eastern European countries such as Lithuania, Belarus, and Hungary. Some researchers attribute the higher rates in former Soviet countries to be a remnant of recent political instability. An increased focus on family led to females becoming more highly valued. Rapid economic fluctuations prevented males from providing fully for their families, which prevented them from fulfilling their traditional gender role. Combined, these factors could account for the gender gap. Other research indicates that higher instances of alcoholism among males in these nations may be to blame. In 2014, suicides rates amongst under-45 men in UK reached a 15-year high of 78% of the total 5,140.\n\nA higher male mortality from suicide is also evident from data of non-Western countries: the Caribbean, often considered part of the West is the most prominent example. In 1979–81, out of 74 countries with a non-zero suicide rate, 69 countries had male suicide rates greater than females, two reported equal rates for the sexes (Seychelles and Kenya), while three reported female rates exceeding male rates (Papua New Guinea, Macau, and French Guiana). The contrast is even greater today, with WHO statistics showing China as the only country where the suicide rate of females matches or exceeds that of males. Barraclough found that the female rates of those aged 5–14 equaled or exceeded the male rates only in 14 countries, mainly in South America and Asia.\n\nIn most countries, the majority of committed suicides are made by men but, in China, women are more likely to commit suicide. In 2015 China's ratio was around 8 males for every 10 females.\n\nTraditional gender roles in China hold women responsible for keeping the family happy and intact. Suicide for women in China is shown in literature to be an acceptable way to avoid disgrace that may be brought to themselves or their families. According to a 2002 review, the most common reasons for the difference in rate between genders are: \"the lower status of Chinese women, love, marriage, marital infidelity, and family problems, the methods used to commit suicide, and mental health of Chinese women.\" Another explanation for increased suicide in women in China is that pesticides are easily accessible and tend to be used in many suicide attempts made by women. The rate of nonlethal suicidal behavior is 40 to 60 percent higher in women than it is in men. This is due to the fact that more women are diagnosed as depressed than men, and also that depression is correlated with suicide attempts.\n\n"}
{"id": "52111558", "url": "https://en.wikipedia.org/wiki?curid=52111558", "title": "Gender disparity in computing", "text": "Gender disparity in computing\n\nGlobal concerns about the gender disparity in computing occupations have gained more importance with the emerging information age. These concerns motivated public policy debates addressing gender equality as computer applications exerted increasing influence in society. This dialogue helped to expand information technology innovations and to reduce the consequences of sexism.\n\nIn the early days of computers and computing, women were well-represented in the field. Women often worked as \"human computers,\" making complicated calculations and working in large groups, such as the Harvard Computers. Women also worked on ballistics calculations and cryptography. Human computers who were women could be paid less than their male counterparts. By 1943, the majority of human computers were women. Many early programmers on machines such as ENIAC, were mostly women. The reason that women were involved as programmers and human computers was because \"they expected programming to be a low skill clerical function,\" and that the difficult work was the creation of the hardware, which male engineers largely worked on. The programmers of the ENIAC, six women who designed the public demonstrations and prepared the machine for its public debut were not fully recognized for their contributions by the media.\n\nBy the 1960s, while computer programming was still touted as a good field for women to go into, major shifts were beginning to take place that would help push women out of the field. Men who were programming started to make the field more \"prestigious,\" creating professional associations, education requirements for work and by actively discouraging hiring women in the field. Hiring tools were introduced in which answers were shared among all-male groups and clubs. Another way to push women out of the field was to use personality tests that favored people who were not interested in working with others, which was slanted towards a certain type of male applicant. After these trends were entrenched in the industry, it has continued to perpetuate itself into the modern day. Some computer science programs, such as Princeton, in the mid-1960s wouldn't even admit women into their program.\n\nA survey, conducted by SWIFT (\"Supporting Women in Information Technology\") based in Vancouver, British Columbia, Canada, asked 7,411 participants questions about their career choices. The survey found that females tend to believe that they lack the skill set needed to be successful in the field of computing. This study (as well as others) provides a strong base for a positive correlation between perceived ability and career choice.\n\nA project based in Edinburgh, Scotland, \"Strategies of Inclusion: Gender and the Information Society\" (SIGIS) released its findings based on research conducted in 48 separate case studies all over Europe. The findings focus on recruiting as well as retention techniques for women already studying in the field. These techniques range from the introduction of role models, advertisement campaigns, and the allocation of quotas, in order to make the computing field appear more gender neutral. Educating reforms, which will increase the quality of the educating body and technological facilities, are also suggested.\n\nResearch suggests that Malaysia has a much more equal split that varies around the half-way mark. A job in the computing industry also implies a safe work environment. Strong belief by the previous generation that IT would be a flourishing sector with many job opportunities caused parents to encourage their children to take a computing career, no matter the gender.\n\nIn India, a growing number of women are studying and taking careers in technical fields. The percentage of women engineers graduating from IIT Bombay grew from \n1.8% in 1972 to 8% in 2005. Arab women made up 59% of students enrolled in computer science in 2014 at government universities located in Saudi Arabia. Women in Eastern Europe, especially in Bulgaria and Romania, have high rates of pursuing coding and technology. However, women remain underrepresented in information technology fields.\n\nIn the United States, the proportion of women represented in undergraduate computer science education and the white-collar information technology workforce peaked in the mid-1980s, and has declined ever since. In 1984, 37.1% of Computer Science degrees were awarded to women; the percentage dropped to 29.9% in 1989-1990, and 26.7% in 1997-1998. Figures from the Computing Research Association Taulbee Survey indicate that fewer than 12% of Computer Science bachelor's degrees were awarded to women at U.S. PhD-granting institutions in 2010-11.\n\nAlthough teenage girls are now using computers and the Internet at rates similar to their male peers, they are five times less likely to consider a technology-related career or plan on taking post-secondary technology classes. The National Center for Women & Information Technology (NCWIT) reports that of the SAT takers who intend to major in computer and information sciences, the proportion of girls has steadily decreased relative to the proportion of boys, from 20 percent in 2001 to 12 percent in 2006. While this number has been decreasing, in 2001, the total number of these students (both boys and girls) reached its peak at 73,466.\n\nAccording to a College Board report, in 2006 there were slightly more girls than boys amongst SAT takers that reported to having \"course work or experience\" in computer literacy, word processing, internet activity, and creating spreadsheets/databases. It was also determined that more boys than girls (59% vs 41%) reported course work or experience with computer programming, although this may likely be caused by false reporting. Of the 146,437 students (13%) who reported having \"no\" course work or experience, 61% were girls and 39% were boys.\n\nMore boys than girls take Advanced Placement (AP) Computer Science exams. According to the College Board in 2006, 2,594 girls and 12,068 boys took the AP Computer Science A exam, while 517 girls and 4,422 boys took the more advanced AP Computer Science AB exam. From 1996 to 2004, girls made up 16–17% of those taking the AP Computer Science A exam and around 10% of those taking AP Computer Science AB exam.\n\nWomen's representation in the computing and information technology workforce has been falling from a peak of 38% in the mid-1980s. From 1993 through 1999, NSF's SESTAT reported that the percentage of women working as computer/information scientists (including those who hold a bachelor's degree or higher in an S&E field or have a bachelor's degree or higher and are working in an S&E field) declined slightly from 33.1% to 29.6% percent while the absolute numbers increased from 170,500 to 185,000. Numbers from the Bureau of Labor Statistics and Catalyst in 2006 indicated that women comprise 27-29% of the computing workforce. A National Public Radio report in 2013 stated that about 20% of all U.S. computer programmers are female. In open source fields, only 10% of women are programmers.\n\nA gender-diverse team is more likely to create products that meet people's requirements. When women are underrepresented, many technical decisions are based on men's experiences, opinions, and judgement, resulting in a male-slanted bias. In addition, a review of research on gender-diverse teams reveals that gender-diverse teams are more productive, more creative, and more able to stay on schedule and within budget, compared to homogenous teams, while other research review suggests that the results are mixed, with many studies showing no result, non-linear results or even negative results of gender diversity on team performances. Research conducted by McKinsey & Company showed that companies with women in top management were more financially successful, in contrast analysis of sample major US companies showed no effect of inclusion of women (or minority members) on financial performance, these varied results give no conclusive evidence of the benefits of diversity.\n\nThe book \"Gender and Computers: Understanding the Digital Divide\" states that the lack of participation of females in computing excludes them from the \"new economy\", which calls for sophisticated computer skills in exchange for high salary positions.\n\nDiminished participation by women relative to men in computer science dates from about 1984 following mass marketing of personal computers to boys as toys to play games. Fiddling with computers by boys resulted in increased interest and readiness for computer science classes by young men.\n\nA study of over 7000 high school students in Vancouver, British Columbia, Canada showed that the degree of interest in the field of computer science for young women is comparably lower than that of young men. The same effect is seen in higher education; for instance, only 4% of female college freshmen expressed intention to major in computer science in the US. Research has shown that some aspects about computing may discourage women. One of the biggest turn-offs is the \"geek factor\". High school girls often envisage a career in computing as a lifetime in an isolated cubicle writing code. The \"geek factor\" affects both male and female high school students, but it seems to have more of a negative effect on the female students. In addition, computer programmers depicted in popular media are overwhelmingly male, contributing to an absence of role models for would-be female computer programmers. However, in 2015, computer science has for the first time become the most popular major for female students at Stanford University.\n\nIn part to qualify for federal education funding distributed through the states, most U.S. states and districts now focus on ensuring that all students are at least \"proficient\" in mathematics and reading, making it difficult for teachers to focus on teaching concepts beyond the test. According to a Rand Corporation study, such a concentration on testing can cause administrators to focus resources on tested subjects at the expense of other subjects (e.g., science) or distract their attention from other needs. Thus, computational thinking is unlikely to be taught either standalone or as integrated into other areas of study (e.g., mathematics, biology) anytime in the near future. The National Center for Women & IT distributes free resources for increasing awareness of the need for teaching computer science in schools, including the \"Talking Points\" card, \"Moving Beyond Computer Literacy: Why Schools Should Teach Computer Science\".\n\nAccording to a 1998–2000 ethnographic study by Jane Margolis and Allan Fisher at Carnegie Mellon University, men and women viewed computers very differently. Women interviewees were more likely to state that they saw the computer as a tool for use within a larger societal and/or interdisciplinary context than did the men interviewed. On the other hand, men were more likely to express an interest in the computer as a machine. Moreover, women interviewed in this study perceived that many of their male peers were \"geeks,\" with limited social skills. Females often disliked the idea that computers \"become their life.\" The students observed and interviewed in that study were probably not representative of students in general, since at that time, in order to be admitted to CMU Computer Science a student needed to have some programming experience. More research is needed to understand the ability to generalize Margolis' and Fisher's findings.\n\nA two-year research initiative published in 2000 by AAUW found that \"Girls approach the computer as a \"tool\" useful primarily for what it can do; boys more often view the computer as a \"toy\" and/or an extension of the self. For boys, the computer is inherently interesting. Girls are interested in its instrumental possibilities, which may include its use as an artistic medium. They express scorn toward boys who confuse \"real\" power and power on a screen. \"I see a computer as a tool,\" a high school girl declares. \"You [might] go play Kung Fu Fighting, but in real life you are still a stupid little person living in a suburban way.\" Still, the National Assessment of Educational Progress showed as far back as 2000 that boys and girls use computers at about the same rates, albeit for somewhat different purposes.\n\nNearly 1000 students in University of Akron were surveyed, and it was discovered that females hold a more negative attitude towards computers than males. Another study assessed the computer-related attitude of over 300 students in University of Winnipeg and obtained similar results.\n\nThis is thought to contribute to the gender disparity phenomenon in computing, in particular the females' early lack of interest in the field.\n\nResearch on the barriers that women face in undergraduate computing has highlighted such factors as:\n\n\nJust like in the pre-college situation, solutions are most often implemented outside of the mainstream (e.g., providing role models, mentoring, and women's groups), which can also create the perception among women, their male peers, and their professors that to be successful, women need \"extra help\" to graduate. Most people do not realize that the \"extra help\" is not academic, but instead access to the kind of peer networks more readily available to male students. Many women decline to participate in these extracurricular support groups because they do not want to appear deficient. In short, the conditions under which women (and underrepresented minority students) study computing are not the same as those experienced by men.\n\nWomen in technical roles often feel that the skills and feedback they bring to their jobs are not valued. According to a Catalyst report called \"Women in Technology: Maximizing Talent, Minimizing Barriers\", 65% of females in technical roles felt that those they reported to were receptive and responsive to their suggestions, as compared to 75% of women in non-technical roles. This also speaks directly to the retention of females in the industry as females will commonly leave a company when they feel that what they are offering a company is not valued. The report shows the concerns felt about this by sharing the following quote from an interviewee: \"I would like to be involved with more projects than I am currently involved in; I feel that I am being underutilized. I would prefer my supervisor give me an opportunity to expand my skill sets and my responsibility at work\".\n\nHowever, it is not enough to just acknowledge skills. Women also lack the support and advocacy needed to promote these skills. Women feel alone and at a loss because they lack role models, networks, and mentors. These support systems not only help women develop talent and opportunities for career advancement, but they are also needed to promote women to more senior roles. It can be understood that advocacy is a major player in the advancement of females into senior tech roles.\n\nOther research examines that undergraduates' stereotype of the people in computer science and how changing this stereotype through media can influence women's interest in computer science. Through this study they concluded that the image of computer science majors that is most prevalent in popular culture and in the minds of current undergraduates is someone who is highly intelligent, primarily obsessed with computers, and socially unskilled. This image can be considered to contrast with the more people-oriented, traditionally feminine image. According to this study, students continue to generate and propagate this stereotype when asked to describe people in computer science. Based on the results of their experiment based on this idea, they took a group of women and men undergraduates and had them read a stereotypical article and a non-stereotypical article. They found that women who read the non-stereotypical article were much more interested in computer science than those who read the article with the above-mentioned stereotypical computer science student. Overall, they concluded that the underrepresentation of women in computing not due to women's lack of interest. The study contests the perception that college major decisions are free choices, instead they discuss the implications that the major decisions are more constrained by the prevalent stereotypes. This has a negative consequence such that it prevents women from developing an interest in these technical fields. The finding suggests that the stereotypical image of the computer scientists is unattractive to women who would otherwise be interested if presented with a true representation or role model from the computer science field.\n\nRacial stereotyping is also an issue, as computer scientists can often be thought of as white or Asian males, which can make it difficult for people who fall outside of those ethnicities to get hired. Non-white or Asian women may experience additional difficulty because they fail to match either half of the stereotype. Nonetheless, it has been found that a women’s race is less likely to affect the probability of her choosing computing or a related field.\n\nSome cases that subvert the stereotype of typical people in computing include the person coming from a family that is already involved in computing or a related field. Also, coming from a family of a higher socioeconomic status is correlated to a higher likelihood of women choosing computing or a related field. Yet, many computing companies only search for employees from prestigious schools, which leaves fewer opportunities.\n\nThe disproportionate number of startups in the computing industry, and the disproportionate hiring of primarily young workers, have created an environment in which many firms' technical teams consist largely of workers who are recent college graduates, sometimes giving the businesses fraternity-like cultures, leading to sexism that discourages female participation. The phenomenon of fraternity-like environments among technology teams of startup firms has been termed \"brogrammer culture\".\n\nWomen, on aggregate, prefer people-oriented careers. while their male counterparts show a preference for thing-oriented careers. The difference between male and female interests is larger in gender-egalitarian countries than in non gender-egalitarian countries, which contradicts the theory that these differences are solely due to societal roles.\n\nThe majority of data collected about women in IT has been qualitative analysis such as interviews and case studies. This data has been used to create effective programs addressing the underrepresentation of women in IT. Suggestions for incorporating more women in IT careers include formal mentoring, ongoing training opportunities, employee referral bonuses, multicultural training for all IT employees, as well as educational programs targeting women.\n\nThe number of female college entrants expressing interest in majoring in computer science decreased in the 2000s to pre-1980's levels. A research study was initialized by Allan Fisher, then Associate Dean for Undergraduate Computer Science Education at Carnegie Mellon University, and Jane Margolis, a social scientist and expert in gender equity in education, into the nature of this problem. The main issues discovered in interesting and retaining women in computer science were feelings of an experience gap, confidence doubts, interest in curriculum and pedagogy, and peer culture. Universities across North America are changing their computer science programs to make them more appealing to women. Proactive and positive exposures to early computer experiences, such as The Alice Project, founded by the late Randy Pausch at Carnegie Mellon University, are thought to be effective in terms of retention and creation of enthusiasm for women who may later consider entering the field. Institutions of higher education are also beginning to make changes regarding the process and availability of mentoring to women that are undergraduates in technical fields.\n\nAnother strategy for addressing this issue has been early outreach to elementary and high-school girls. Programs like all-girl computer camps, girls' after-school computer clubs, and support groups for girls have been instilled to create more interest at a younger age. A specific example of this kind of program is the Canadian Information Processing Society outreach program, in which a representative is sent to schools in Canada, speaking specifically to grade nine girls about the benefits of Information Technology careers. The purpose is to inform girls about the benefits and opportunities within the field of information technology. Companies like IBM also encourage young women to become interested in engineering, technology and science. IBM offers EX.I.T.E. (Exploring Interests in Technology and Engineering) camps for young women from the ages of 11 to 13.\n\nAdditionally, attempts are being made to make the efforts of female computer scientists more visible through events such as the Grace Hopper Celebration of Women conference series which allows women in the field to meet, collaborate and present their work. In the U.S., the Association for Women in Computing was founded in Washington, D.C. in 1978. Its purpose is to provide opportunities for the professional growth of women in computing through networking, and through programs on technical and career-oriented topics. In the United Kingdom, the British Computer Society (BCS) and other organizations have groups which promote the cause of women in computing, such as BCSWomen, founded by Sue Black, and the BCS Women's Forum. In Ontario, Canada, the Gr8 Designs for Gr8 Girls program was founded to develop grade 8 girls' interest in computer science.\n\nThe National Center for Women & IT (NCWIT) is currently one of the lead supporters of women's entry and retention in computing. Their goal is to help to create academic and work environments that are welcoming and fair for women. Their research shows that encouragement is one of the key elements to help women enter a primarily male-dominated field. They found women report more often than their male-counterpart that they entered computer science due to the influence of a teacher, family member, or friend's encouragement. Their findings conclude that support can make the difference in a woman's belief that she is competent enough to compete in computing. Thus, the NCWIT developed a program called Aspirations in Computing in order to provide girls with the necessary encouragement, a network of support, and female role models. In a survey done, nearly half of the girls polled said they would feel uncomfortable being the only girl in a group or class, one of the Aspirations main goals is to enable girls to feel less isolated in these predicaments. They have found that creating a sense of belonging or \"fitting in\" is fundamental for interest and current retention. The NCWIT Aspirations Award was created in order to involve women in a national competition, awardees are selected for their computing and IT aptitude, leadership skills, academics, and plans for graduate schooling. Due to their reach and awareness of the program, they saw a 54% increase in the girls applying in the 2013 season compared to the previous year.\n\nIn September 2013, Ada Developers Academy, a tuition-free one year intensive school in software development for women was launched by Technology Alliance in Seattle, and students could even apply to receive a $1000-per-month-stipend. The first half of the course focuses on HTML/CSS, JavaScript, Ruby on Rails and computer science fundamentals.\n\nHaving started in the US, Girl Develop It is a network of city chapters that teach women from all parts of the country learn to develop software with HTML and CSS, Javascript, Ruby on Rails, Python, and Android. The organization was co-founded by Sara Chipps and Vanessa Hurst in 2010. As of 2013, it has 17 city chapters running regular courses and events. The programs offered by Girl Develop It are all taught by volunteers that are employed in the technology field. Structural and content resources used to teach the programs have been developed and are offered for free both on their website and on GitHub.com.\n\nGeek Girl is an organization that was started in March 2006 by Leslie Fishlock. It is an organization that acts as a technology resource for women. The organization strives to empower women of all ages through making technology easy to understand and use. These services are provided entirely by women. Though the target audience tends to be female and the organization was founded on the goal to empower women, men are also encouraged to participate in any of the events or services the organization offers.\n\nGeek Girl hosts localized events, meetups, and conferences. The organization also supports a video channel titled GeekGirl TV that provides workshops about technological tools as well as provides coverage for their events for those who are unable to attend. Additionally, Geek Girl's website hosts a blog that provides technology-related news and information that is accessible to a reader with minimal technology experience.\n\nGrace Hopper Academy, named after Rear Admiral Grace Hopper, is another woman-only immersive programming school, located in New York City. A partner school to Fullstack Academy, Grace Hopper's curriculum focuses on the MEAN stack, and through education and mentorship, aims to help women begin careers in software engineering.\n\nNerd Girls was launched in 2000 by Dr. Karen Panetta, a Professor of Electrical and Computer Engineering at Tufts University. It is an organization that is represented by a group of female engineering students each year and encourages women to take on roles in the engineering and technology profession. The organization celebrates the coincidence of science knowledge and femininity. Participating members solve real-world problems as a group by addressing and fixing technology related issues in the community. Nerd Girls has gained national attention since its launch and has been approached by media producers to create a reality show based off the organization's problem-solving activities. Nerd Girls is sponsored by the Institute of Electrical and Electronics Engineers (IEEE).\n\nFemgineer was started in 2007 by Poornima Vijayashanker. It was originally developed as a blog that focused on engineers, which evolved into an organization that supports women in technology careers. Femgineers is now an education-focused organization that offers workshops, free teaching resources on the topic of technology, supports forums and Meetups, and a team has been developed to continue to expand on the original blog. Poornima Vijayashanker is an avid public speaker and regularly speaks at technology-related conferences and events about the technology industry and about Femgineer itself. In addition to founding Femgineer, she also founded a startup called BizeeBee in 2010 that supports growing fitness businesses, teaches technology workshops for tech-driven organizations around the country, and was named one of the ten women to watch in tech in 2013 by Inc Magazine.\n\nNumerous postsecondary education institutions have student-run organizations that focus on the advancement of women in computer science. In addition to she++ based out of Stanford University, Rochester Institute of Technology (RIT) supports a chapter of the organization called Women In Computing. The campus's chapter of the organization is composed of students, faculty and staff at RIT and they strive to support and further develop the culture of computing to women. This effort is not only focused on their campus, but in the larger community. They host events both on their campus located in Henrietta, New York, and within surrounding Rochester schools. RIT is among a national list of schools that host a chapter of Women in Computing, which is founded in the organization Association of Computing Machinery's committee for women in computing (ACM-W).\n\nHarvard University hosts the organization called Harvard Undergraduate Women in Computer Science (WiSC). The organization aims to promote women in computing across a variety of schools and industries, educate women on the profession of computer science, and provide opportunities for women in technical fields. WiCS supports the annual conference named WECode, a conference that aims to promote women's involvement in computer science.\n\nIn an effort to improve the gender composition in computing, the Women & Information Technology (NCWIT) created a nationwide U.S. program called \"Pacesetters\". Through this program, twenty-four academic and corporate organizations added close to 1,000 \"Net New Women\" to the field of computer science by 2012. These Net New Women are women in the sciences that had not originally intended on pursuing a computer science degree. Pacesetters is the first program of its kind where different organizations come together to identify effective ways to broaden the participation of women in computer science. There are currently more than 300 corporations, academic institutions, government agencies and non-profit organizations devoted to this cause. Together they build internal teams in order to develop and fund the needed programs and share their overall results. Pacesetters organizations include some very prestigious companies such as AT&T, Intel, Microsoft, Google, Georgia Tech, Pfizer, and IBM to name a few. These are a few examples of their results due to the work with Pacesetters:\n\nThere are a number of thinkers who engage with gender theories and issues related to women and technology. Such thinkers include, for example, Donna Haraway, Sadie Plant, Julie Wosk, Sally L. Hacker, Evelyn Fox Keller, Janet Abbate, Thelma Estrin, and Thomas J. Misa, among others. A 2008 book titled \"Gender and Information Technology: Moving Beyond Access to Co-Create Global Partnership\" uses Riane Eisler's cultural transformation theory to offer an interdisciplinary, social systems perspective on issues of access to technology. The book explores how shifting from dominator towards partnership systems—as reflected in four primary social institutions (communication, media, education, and business) - might help society move beyond the simplistic notion of access to co-create a real digital revolution worldwide.\n\nA 2000 book titled \"Athena Unbound\" provides a life-course analysis (based on interviews and surveys) of women in the sciences from an early childhood interest, through university, to graduate school and finally into the academic workplace. The thesis of this book is that \"women face a special series of gender related barriers to entry and success in scientific careers that persist, despite recent advances\".\n\nComputer scientist Karen Petrie, from University of Dundee, has developed an argument to illustrate why an attack on sexism in computing is not an attack on men. Ian Gent, University of St Andrews, has described this idea which is key to the argument as the \"Petrie Multiplier\".\n\nAccording to J. McGrath Cohoon, senior research scientist for the National Center for Women & Information Technology, there are a few possible hypotheses for why women are underrepresented in computer sciences attributed to already established theories about the influence of gender and technology stereotypes. One gender related hypothesis is that women find it more difficult than men to contribute to the intellectual life of the field in the sense that reviewers of their work are unconsciously downgraded due to their status as women or those women have lower confidence in this field that inhibits women's willingness to publicly present their technical findings. Due to this barrier of women as second-class citizens in the computing world, it creates an environment that is not accessible to women. A study by the Psychology of Women Quarterly backs this hypothesis up by concluding that even the enduring effect of single brief exposures to stereotypical role models leaves a strong mark. Their findings reported that the most important factor in recruiting women to the computer science field is that women meet with a potential role model, regardless of gender of that role model, that conveys to the woman a sense of belonging in the field. This finding suggests that support and encouragement are the two most important aspects that can influence women participation in computing. In order for women to be more receptive to the field is if the environment became a more welcoming place by their male counterparts.\n\nCordelia Fine in her book Delusions of Gender argues that apparent differences are due to constant exposure to societal beliefs of gender difference. Fine also argues that \"...while social effects on sex differences are well-established, spurious results, poor methodologies and untested assumptions mean we don't yet know whether, on average, males and females are born differently predisposed to systemizing versus empathising.\"\n\nAnother argument for why women are less prevalent in computer science is the ill-defined nature of computing according to Paul De Palma. In his article Why Women Avoid Computer Science, he postulates that women find careers in computing unattractive. He finds that among the many reasons offered, he believes the nature of computing is what drives them away. He claims that young men who are drawn to computer science and engineering are those that like to tinker, those who like to use tools to create and dismantle objects. He further claims that computing is not a true profession, that traditional career paths such as law, business, and medicine are more certain and profitable on average than computing. He compares it to using a computer, computers nowadays do not come with lengthy manuals on the inner workings of the modern day computer, in fact our tools are always more complicated than their what they are used for, thus the tinkering nature of men, the drive born from gender stereotyping from birth, has made men successful in this field for they are more inclined to spend endless hours of tinkering with software and hardware. His claim revolves around the focus that boys and girls fall into gender stereotypes, girls who usually are given dolls and boys who are given trucks and toy tool boxes. He claims that these gender roles placed on children is one of the primary causes for the gender gap seen in computer science. He postulates that if we were to see more girls playing with trucks and other \"boy-related\" toys that perhaps we would see an increase in this tinkering nature and therefore more participation of women in the computer science field.\n\n\n\n"}
{"id": "1703917", "url": "https://en.wikipedia.org/wiki?curid=1703917", "title": "Gender expression", "text": "Gender expression\n\nA gender expression is a person's behavior, mannerisms, interests, and appearance that are associated with gender in a particular cultural context, specifically with the categories of femininity or masculinity. This also includes gender roles. These categories rely on stereotypes about gender.\n\nGender expression typically reflects a person's gender identity (their internal sense of their own gender), but this is not always the case. Gender expression is separate and independent both from sexual orientation and gender assigned at birth. A type of gender expression that is considered atypical for a person's externally perceived gender may be described as gender non-conforming.\n\nIn men and boys, typical gender expression is often described as \"manly\", while atypical expression is known as effeminate. In girls, atypical expression is called tomboyish. In (especially queer) women, atypical and typical expression are known as butch and femme respectively. A mixture of typical and atypical expression may be described as androgynous. A type of expression that is perceived as neither typically feminine or masculine can be described as gender-neutral or undifferentiated.\n\nThe term \"gender expression\" is used in the Yogyakarta Principles, which concern the application of international human rights law in relation to sexual orientation, gender identity, gender expression and sex characteristics.\n\nThe Bem Sex-Role Inventory was designed to evaluate gender expression objectively (within a White American cultural context).\n\n\n"}
{"id": "42510705", "url": "https://en.wikipedia.org/wiki?curid=42510705", "title": "Gender inequality in the United Kingdom", "text": "Gender inequality in the United Kingdom\n\nIn the United Kingdom, sexism or gender inequality denotes the inconsistencies between individuals due to gender. The topic covers a variety of concerns from education to equal opportunity in terms of employment and wages.\n\nIn response to the concerns, the government has implemented various legislation, especially concerning gender discrimination, both institutional and personal, in the workplace; the Sex Discrimination Act of 1975 protects individuals from being discriminated against in employment, vocational training, education, the provision and sale of goods, facilities and services, premises and the exercise of public functions due to their sex/gender; this was amended by the Sex Discrimination (Election Candidates) Act 2002. The Equal Pay Act of 1970 mandates equal pay for equal work regardless of an individual's sex/gender, and the Sex Discrimination (Gender Reassignment) Regulations 1999 also protect the rights of individuals who intend to undergo, are undergoing, or have undergone sex reassignment. These Regulations pertain to pay and treatment in employment, self-employment and vocational training.\n\nThe Equality Acts of 2006 and 2010 served to codify and combine all equality enactments within Great Britain, and provide comparable protections across all equality strands.\n\nThe Equality and Human Rights Commission (EHRC) is a non-departmental public body which has responsibility for the promotion and enforcement of equality and non-discrimination laws in England, Scotland and Wales, while the Government Equalities Office is a government department charged with promoting and improving gender equality within the UK government itself, responsible for leading the Discrimination Law Review, and providing advice on all other forms of equality to other UK government departments. The GEO was formerly known as the Women and Equality Unit.\n\nIt has increasingly been observed that a pervasive 'lad culture' has developed in the U.K., described as an ironic, self-conscious method for young males to adopt \"an anti-intellectual position, scorning sensitivity and caring in favour of drinking, violence, and a pre-feminist and racist attitude to women as both sex objects and creatures from another species\". In April 2014, the UN special rapporteur on violence against women similarly concluded that Britain has a \"boys' club sexist culture\".\n\nThe culture has attracted wide criticism from feminist circles; Germaine Greer critiques it in her 2000 book \"The Whole Woman\", while Kira Cochrane asserts that \"it's a dark world that \"Loaded\" and the lad culture has bequeathed us\".\n\nCommentators believe that lad culture has affected politics and decreased the ability of women to participate, and studies of industries such as the architecture profession found that lad culture had a negative impact on women completing their professional education.\n\nIn April 2012, British feminist writer Laura Bates founded the Everyday Sexism Project, a website and social media presence whose aim is to document everyday examples of sexism as reported by contributors; the submissions are then collated by a small group of volunteers led by Emer O'Toole, a researcher at Royal Holloway, University of London. By April 2013 the site had collected 25,000 entries from 15 countries.\n\nVarious U.K. media institutions have been labelled problematic with regards to sexism and gender discrimination - most notably being the Page 3 feature found in the British tabloid newspaper \"The Sun\", which consists of a large photograph of a topless female glamour model usually published on the newspaper's third page. In August 2012, the No More Page 3 campaign emerged, sparking widespread discussion and receiving heavy support from Green MP Caroline Lucas and cross-party support from over 140 other MPs.\n\nA YouGov survey in October 2012 found differences in attitude toward Page 3 among readers of different newspapers; 61% of \"Sun\" readers wished to retain the feature, while 24 percent said that the newspaper should stop showing Page 3 women. However, only 4% of \"Guardian\" readers said \"The Sun\" should keep Page 3, while 86% said it should be abolished. The poll also found wide differences by gender, with 48% of men overall saying that Page 3 should be retained, but just 17% of women taking that position.\n\nThe U.K. has a persistent gender pay gap, the most significant factors associated with which are part-time work, education, the size of the firm a person is employed in, and occupational segregation (women are under-represented in managerial and high-paying professional occupations.) When comparing full-time roles, men in the U.K. tend to work slightly longer hours than women in full-time employment. Depending on the age bracket and percentile of hours worked men in full-time employment work between 1.35% and 17.94% more hours than women in full-time employment. Even when taking the differences in hours worked into account, a pay gap still exists in the U.K., and typically increases with age and earnings percentile.\n\nInequality in the context of education has greater implications in terms of future employment. Elements of the school curriculum still advocate certain gender-specific practices.\n\nBoys lag behind girls at important educational milestones. At Key Stage 2 girls outperform boys. The proportion of students achieving level 4 and above in reading, writing and maths, in 2015, in England, was 77% boys compared to 83% girls. The gap is wider for students who receive a free school meal. \n\nGirls outperform boys in headline GCSE results. In state-funded schools, the gap in those achieving 5+ grades a*-C including English and maths is around 10 percentage points.\n\nYoung women are more likely to enrol at university. In 2016 the gender gap in favour of women was the highest on record. \"In England, young women are 36% more likely to apply to university and when both sexes are from disadvantaged backgrounds young women are 58% more likely to apply.\"\n\nCurrently, sex education is not compulsory in the U.K., and in England and Wales, the optional curriculum focuses mainly on biological areas such as the reproductive system, foetal development, and the physical changes of adolescence, while information about contraception and safe sex is discretionary and discussion about relationships and gender roles is often neglected.\n\nIn April 2014, Bates remarked that \"better sex and relationships education in schools is desperately needed\" to teach areas around \"healthy relationships, consent, respect and sexual abuse\" in response to tackling everyday sexism earlier in life.\n"}
{"id": "22163223", "url": "https://en.wikipedia.org/wiki?curid=22163223", "title": "German Council of Science and Humanities", "text": "German Council of Science and Humanities\n\nThe Wissenschaftsrat (\"WR\"; Council of Science and Humanities) is an advisory body to the German Federal Government and the state (\"Länder\") governments. It makes recommendations on the development of science, research, and the universities, as well as on the competitiveness of German science. These recommendations involve both quantitative and financial considerations, as well as their implementation. Funding is provided by the federal and state governments.\n\nThe \"Wissenschaftliche Kommission\" (Scientific Commission) of the \"Wissenschaftsrat\" has 32 members appointed by the Federal President. Twenty-four scientists are jointly proposed by the \"Deutsche Forschungsgemeinschaft\" (DFG, German Research Foundation), the \"Max-Planck-Gesellschaft zur Förderung der Wissenschaften\" (MPG, Max Planck Society for the Advancement of Science), the \"Hochschulrektorenkonferenz\" (HRK, German Rector’s Conference), the Helmholtz Association of German Research Centres, the \"Fraunhofer-Gesellschaft\" (FhG, Fraunhofer Society), and the \"Wissenschaftsgemeinschaft Gottfried Wilhelm Leibniz\" (WGL, Gottfried Wilhelm Leibniz Scientific Community). Another eight persons of high public standing are jointly proposed by the Federal Government and the state (\"Länder\") governments.\n\nPresidents (\"Vorsitzender\") of the organization:\n\n\n"}
{"id": "1676362", "url": "https://en.wikipedia.org/wiki?curid=1676362", "title": "History of anthropology", "text": "History of anthropology\n\nHistory of anthropology'scientific word during the Renaissance, has always meant \"the study (or science) of man\". The topics to be included and the terminology have varied historically. At present they are more elaborate than they were during the development of anthropology. For a presentation of modern social and cultural anthropology as they have developed in Britain, France, and North America since approximately 1900, see the relevant sections under Anthropology.\n\nThe term \"anthropology\" ostensibly is a produced compound of Greek \"anthrōpos\", \"human being\" (understood to mean \"humankind\" or \"humanity\"), and a supposed -λογία \"-logia\", \"study\". The compound, however, is unknown in ancient Greek or Latin, whether classical or mediaeval. It first appears sporadically in the scholarly Latin \"anthropologia\" of Renaissance France, where it spawns the French word \"anthropologie\", transferred into English as anthropology. It does belong to a class of words produced with the -logy suffix, such as archeo-logy, bio-logy, etc., \"the study (or science) of\".\n\nThe mixed character of Greek \"anthropos\" and Latin \"-logia\" marks it as New Latin. There is no independent noun, logia, however, of that meaning in classical Greek. The word λόγος (logos) has that meaning. James Hunt attempted to rescue the etymology in his first address to the Anthropological Society of London as president and founder, 1863. He did find an \"anthropologos\" from Aristotle in the standard ancient Greek Lexicon, which he says defines the word as \"speaking or treating of man\". This view is entirely wishful thinking, as Liddell and Scott go on to explain the meaning: \"i.e. fond of personal conversation\". If Aristotle, the very philosopher of the logos, could produce such a word without serious intent, there probably was at that time no anthropology identifiable under that name.\n\nThe lack of any ancient denotation of anthropology, however, is not an etymological problem. Liddell and Scott list 170 Greek compounds ending in \"–logia\", enough to justify its later use as a productive suffix. The ancient Greeks often used suffixes in forming compounds that had no independent variant. The etymological dictionaries are united in attributing \"–logia\" to \"logos\", from \"legein\", \"to collect\". The thing collected is primarily ideas, especially in speech. The American Heritage Dictionary says: \"(It is one of) derivatives independently built to logos.\" Its morphological type is that of an abstract noun: log-os > log-ia (a \"qualitative abstract\")\n\nThe Renaissance origin of the name of anthropology does not exclude the possibility that ancient authors presented anthropogical material under another name (see below). Such an identification is speculative, depending on the theorist's view of anthropology; nevertheless, speculations have been formulated by credible anthropologists, especially those that consider themselves functionalists and others in history so classified now.\n\nMarvin Harris, a historian of anthropology, begins \"The Rise of Anthropological Theory\" with the statement that anthropology is \"the science of history\". He is not suggesting that history be renamed to anthropology, or that there is no distinction between history and prehistory, or that anthropology excludes current social practices, as the general meaning of history, which it has in \"history of anthropology\", would seem to imply. He is using \"history\" in a special sense, as the founders of cultural anthropology used it: \"the natural history of society\", in the words of Herbert Spencer, or the \"universal history of mankind\", the 18th-century Age of Enlightenment objective. Just as natural history comprises the characteristics of organisms past and present, so cultural or social history comprises the characteristics of society past and present. It includes both documented history and prehistory, but its slant is toward institutional development rather than particular non-repeatable historical events.\n\nAccording to Harris, the 19th-century anthropologists were theorizing under the presumption that the development of society followed some sort of laws. He decries the loss of that view in the 20th century by the denial that any laws are discernable or that current institutions have any bearing on ancient. He coins the term ideographic for them. The 19th-century views, on the other hand, are nomothetic; that is, they provide laws. He intends \"to reassert the methodological priority of the search for the laws of history in the science of man\". He is looking for \"a general theory of history\". His perception of the laws: \"I believe that the analogue of the Darwinian strategy in the realm of sociocultural phenomena is the principle of techno-environmental and techno-economic determinism\", he calls cultural materialism, which he also details in \"Cultural Materialism: The Struggle for a Science of Culture.\"\n\nElsewhere he refers to \"my theories of historical determinism\", defining the latter: \"By a deterministic relationship among cultural phenomena, I mean merely that similar variables under similar conditions tend to give rise to similar consequences.\" The use of \"tends to\" implies some degree of freedom to happen or not happen, but in strict determinism, given certain causes, the result and only that result must occur. Different philosophers, however, use determinism in different senses. The deterministic element that Harris sees is lack of human social engineering: \"free will and moral choice have had virtually no significant effect upon the direction taken thus far by evolving systems of social life.\"\n\nHarris agrees with the 19th-century view that laws are abstractions from empirical evidence: \"...sociocultural entities are constructed from the direct or indirect observation of the behavior and thought of specific individuals ...\" Institutions are not a physical reality; only people are. When they act in society, they do so according to the laws of history, of which they are not aware; hence, there is no historical element of free will. Like the 20th-century anthropologists in general, Harris places a high value on the empiricism, or collection of data. This function must be performed by trained observers.\n\nHe borrows terms from linguistics: just as a phon-etic system is a description of sounds developed without regard to the meaning and structure of the language, while a phon-emic system describes the meaningful sounds actually used within the language, so anthropological data can be emic and etic. Only trained observers can avoid eticism, or description without regard to the meaning in the culture: \"... etics are in part observers' emics incorrectly applied to a foreign system...\" He makes a further distinction between synchronic and diachronic. Synchronic (\"same time\") with reference to anthropological data is contemporaneous and cross-cultural. Diachronic (\"through time\") data shows the development of lines through time. Cultural materialism, being a \"processually holistic and globally comparative scientific research strategy\" must depend for accuracy on all four types of data. Cultural materialism differs from the others by the insertion of culture as the effect. Different material factors produce different cultures.\n\nHarris, like many other anthropologists, in looking for anthropological method and data before the use of the term anthropology, had little difficulty finding them among the ancient authors. The ancients tended to see players on the stage of history as ethnic groups characterized by the same or similar languages and customs: the Persians, the Germans, the Scythians, etc. Thus the term history meant to a large degree the \"story\" of the fortunes of these players through time. The ancient authors never formulated laws. Apart from a rudimentary three-age system, the stages of history, such as are found in Lubbock, Tylor, Morgan, Marx and others, are yet unformulated.\n\nEriksen and Nielsen use the term proto-anthropology to refer to near-anthropological writings, which contain some of the criteria for being anthropology, but not all. They classify proto-anthropology as being \"travel writing or social philosophy\", going on to assert \"It is only when these aspects ... are fused, that is, when data and theory are brought together, that anthropology appears.\" This process began to occur in the 18th century of the Age of Enlightenment.\n\nMany anthropological writers find anthropological-quality theorizing in the works of Classical Greece and Classical Rome; for example, John Myres in \"Herodotus and Anthropology\" (1908); E. E. Sikes in \"The Anthropology of the Greeks\" (1914); Clyde Kluckhohn in \"Anthropology and the Classics\" (1961), and many others. An equally long list may be found in French and German as well as other languages.\n\nHerodotus was a 5th-century BC Greek historian who set about to chronicle and explain the Greco-Persian Wars that transpired early in that century. He did so in a surviving work conventionally termed \"the History\" or \"the Histories\". His first line begins: \"These are the researches of Herodotus of Halicarnassus ...\"\n\nThe Achaemenid Empire, deciding to bring Greece into its domain, conducted a massive invasion across the Bosphorus using multi-cultural troops raised from many different locations. They were decisively defeated by the Greek city-states. Herodotus was far from interested in only the non-repeatable events. He provides ethnic details and histories of the peoples within the empire and to the north of it, in most cases being the first to do so. His methods were reading accounts, interviewing witnesses, and in some cases taking notes for himself.\n\nThese \"researches\" have been considered anthropological since at least as early as the late 19th century. The title, \"Father of History\" (\"pater historiae\"), had been conferred on him probably by Cicero. Pointing out that John Myres in 1908 had believed that Herodotus was an anthropologist on a par with those of his own day, James M. Redfield asserts: \"Herodotus, as we know, was both Father of History and Father of Anthropology.\" Herodotus calls his method of travelling around taking notes \"theorizing\". Redfield translates it as \"tourism\" with a scientific intent. He identifies three terms of Herodotus as overlapping on culture: \"diaitia\", material goods such as houses and consumables; \"ethea\", the mores or customs; and \"nomoi\", the authoritative precedents or laws.\n\nThe Roman historian, Tacitus, wrote many of our only surviving contemporary accounts of several ancient Celtic and Germanic peoples.\n\nAnother candidate for one of the first scholars to carry out comparative ethnographic-type studies in person was the medieval Persian scholar Abū Rayhān Bīrūnī in the eleventh century, who wrote about the peoples, customs, and religions of the Indian subcontinent. According to Akbar S. Ahmed, like modern anthropologists, he engaged in extensive participant observation with a given group of people, learnt their language and studied their primary texts, and presented his findings with objectivity and neutrality using cross-cultural comparisons. Others argue, however, that he hardly can be considered an anthropologist in the conventional sense. He wrote detailed comparative studies on the religions and cultures in the Middle East, Mediterranean, and especially South Asia. Biruni's tradition of comparative cross-cultural study continued in the Muslim world through to Ibn Khaldun's work in the fourteenth century.\n\nMedieval scholars may be considered forerunners of modern anthropology as well, insofar as they conducted or wrote detailed studies of the customs of peoples considered \"different\" from themselves in terms of geography. John of Plano Carpini reported of his stay among the Mongols. His report was unusual in its detailed depiction of a non-European culture.\n\nMarco Polo's systematic observations of nature, anthropology, and geography are another example of studying human variation across space. Polo's travels took him across such a diverse human landscape and his accounts of the peoples he met as he journeyed were so detailed that they earned for Polo the name \"the father of modern anthropology\".\n\nThe first use of the term \"anthropology\" in English to refer to a natural science of humanity was apparently in Richard Harvey's 1593 \"Philadelphus, a defense of the legend of Brutus in British history\", which, includes the passage: \"Genealogy or issue which they had, Artes which they studied, Actes which they did. This part of History is named Anthropology.\"\n\nMany scholars consider modern anthropology as an outgrowth of the Age of Enlightenment (1715–89), a period when Europeans attempted to study human behavior systematically, the known varieties of which had been increasing since the fifteenth century as a result of the first European colonization wave. The traditions of jurisprudence, history, philology, and sociology then evolved into something more closely resembling the modern views of these disciplines and informed the development of the social sciences, of which anthropology was a part.\n\nIt took Immanuel Kant (1724-1804) 25 years to write one of the first major treatises on anthropology, \"Anthropology from a Pragmatic Point of View\" (1798), which treats it as a branch of philosophy. Kant is not generally considered to be a modern anthropologist, as he never left his region of Germany, nor did he study any cultures besides his own. He did, however, begin teaching an annual course in anthropology in 1772.\nDevelopments in the systematic study of ancient civilizations through the disciplines of Classics and Egyptology informed both archaeology and eventually social anthropology, as did the study of East and South Asian languages and cultures. At the same time, the Romantic reaction to the Enlightenment produced thinkers, such as Johann Gottfried Herder and later Wilhelm Dilthey, whose work formed the basis for the \"culture concept\", which is central to the discipline.\n\nInstitutionally, anthropology emerged from the development of natural history (expounded by authors such as Buffon) that occurred during the European colonization of the seventeenth, eighteenth, nineteenth and twentieth centuries. Programs of ethnographic study originated in this era as the study of the \"human primitives\" overseen by colonial administrations.\n\nThere was a tendency in late eighteenth century Enlightenment thought to understand human society as natural phenomena that behaved according to certain principles and that could be observed empirically. In some ways, studying the language, culture, physiology, and artifacts of European colonies was not unlike studying the flora and fauna of those places.\n\nEarly anthropology was divided between proponents of unilinealism, who argued that all societies passed through a single evolutionary process, from the most primitive to the most advanced, and various forms of non-lineal theorists, who tended to subscribe to ideas such as diffusionism. Most nineteenth-century social theorists, including anthropologists, viewed non-European societies as windows onto the pre-industrial human past.\n\nMarxist anthropologist Eric Wolf once characterized anthropology as \"the most scientific of the humanities, and the most humanistic of the social sciences\". Understanding how anthropology developed contributes to understanding how it fits into other academic disciplines.\nScholarly traditions of jurisprudence, history, philology and sociology developed during this time and informed the development of the social sciences of which anthropology was a part. At the same time, the Romantic reaction to the Enlightenment produced thinkers such as Herder and later Wilhelm Dilthey whose work formed the basis for the culture concept which is central to the discipline.\n\nThese intellectual movements in part grappled with one of the greatest paradoxes of modernity: as the world is becoming smaller and more integrated, people's experience of the world is increasingly atomized and dispersed. As Karl Marx and Friedrich Engels observed in the 1840s:\n\nIronically, this universal interdependence, rather than leading to greater human solidarity, has coincided with increasing racial, ethnic, religious, and class divisions, and new—and to some confusing or disturbing—cultural expressions. These are the conditions of life with which people today must contend, but they have their origins in processes that began in the 16th century and accelerated in the 19th century.\n\nInstitutionally anthropology emerged from natural history (expounded by authors such as Buffon). This was the study of human beings—typically people living in European colonies. Thus studying the language, culture, physiology, and artifacts of European colonies was more or less equivalent to studying the flora and fauna of those places. It was for this reason, for instance, that Lewis Henry Morgan could write monographs on both \"The League of the Iroquois\" and \"The American Beaver and His Works\". This is also why the material culture of 'civilized' nations such as China have historically been displayed in fine arts museums alongside European art while artifacts from Africa or Native North American cultures were displayed in natural history museums with dinosaur bones and nature dioramas. Curatorial practice has changed dramatically in recent years, and it would be wrong to see anthropology as merely an extension of colonial rule and European chauvinism, since its relationship to imperialism was and is complex.\n\nDrawing on the methods of the natural sciences as well as developing new techniques involving not only structured interviews but unstructured \"participant-observation\"—and drawing on the new theory of evolution through natural selection, they proposed the scientific study of a new object: \"humankind\", conceived of as a whole. Crucial to this study is the concept \"culture\", which anthropologists defined both as a universal capacity and propensity for social learning, thinking, and acting (which they see as a product of human evolution and something that distinguishes Homo sapiens—and perhaps all species of genus \"Homo\"—from other species), and as a particular adaptation to local conditions that takes the form of highly variable beliefs and practices. Thus, \"culture\" not only transcends the opposition between nature and nurture; it transcends and absorbs the peculiarly European distinction between politics, religion, kinship, and the economy as autonomous domains. Anthropology thus transcends the divisions between the natural sciences, social sciences, and humanities to explore the biological, linguistic, material, and symbolic dimensions of humankind in all forms.\n\nAs academic disciplines began to differentiate over the course of the nineteenth century, anthropology grew increasingly distinct from the biological approach of natural history, on the one hand, and from purely historical or literary fields such as Classics, on the other. A common criticism was that many social sciences (such as economists, sociologists, and psychologists) in Western countries focused disproportionately on Western subjects, while anthropology focuseed disproportionately on the \"other\".\n\nMuseums such as the British Museum weren't the only site of anthropological studies: with the New Imperialism period, starting in the 1870s, zoos became unattended \"laboratories\", especially the so-called \"ethnological exhibitions\" or \"Negro villages\". Thus, \"savages\" from the colonies were displayed, often nudes, in cages, in what has been called \"human zoos\". For example, in 1906, Congolese pygmy Ota Benga was put by anthropologist Madison Grant in a cage in the Bronx Zoo, labelled \"the missing link\" between an orangutan and the \"white race\"—Grant, a renowned eugenicist, was also the author of \"The Passing of the Great Race\" (1916). Such exhibitions were attempts to illustrate and prove in the same movement the validity of scientific racism, which first formulation may be found in Arthur de Gobineau's \"An Essay on the Inequality of Human Races\" (1853–55). In 1931, the Colonial Exhibition in Paris still displayed Kanaks from New Caledonia in the \"indigenous village\"; it received 24 million visitors in six months, thus demonstrating the popularity of such \"human zoos\".\n\nAnthropology grew increasingly distinct from natural history and by the end of the nineteenth century the discipline began to crystallize into its modern form—by 1935, for example, it was possible for T.K. Penniman to write a history of the discipline entitled \"A Hundred Years of Anthropology\". At the time, the field was dominated by 'the comparative method'. It was assumed that all societies passed through a single evolutionary process from the most primitive to most advanced. Non-European societies were thus seen as evolutionary 'living fossils' that could be studied in order to understand the European past. Scholars wrote histories of prehistoric migrations which were sometimes valuable but often also fanciful. It was during this time that Europeans first accurately traced Polynesian migrations across the Pacific Ocean for instance—although some of them believed it originated in Egypt. Finally, the concept of race was actively discussed as a way to classify—and rank—human beings based on difference.\n\nEdward Burnett Tylor (2 October 1832 – 2 January 1917) and James George Frazer (1 January 1854 – 7 May 1941) are generally considered the antecedents to modern social anthropology in Britain. Although Tylor undertook a field trip to Mexico, both he and Frazer derived most of the material for their comparative studies through extensive reading, not fieldwork, mainly the Classics (literature and history of Greece and Rome), the work of the early European folklorists, and reports from missionaries, travelers, and contemporaneous ethnologists.\n\nTylor advocated strongly for unilinealism and a form of \"uniformity of mankind\". Tylor in particular laid the groundwork for theories of cultural diffusionism, stating that there are three ways that different groups can have similar cultural forms or technologies: \"independent invention, inheritance from ancestors in a distant region, transmission from one race to another\".\n\nTylor formulated one of the early and influential anthropological conceptions of culture as \"that complex whole, which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by [humans] as [members] of society\". However, as Stocking notes, Tylor mainly concerned himself with describing and mapping the distribution of particular elements of culture, rather than with the larger function, and he generally seemed to assume a Victorian idea of progress rather than the idea of non-directional, multilineal cultural development proposed by later anthropologists.\n\nTylor also theorized about the origins of religious beliefs in human beings, proposing a theory of animism as the earliest stage, and noting that \"religion\" has many components, of which he believed the most important to be belief in supernatural beings (as opposed to moral systems, cosmology, etc.). Frazer, a Scottish scholar with a broad knowledge of Classics, also concerned himself with religion, myth, and magic. His comparative studies, most influentially in the numerous editions of \"The Golden Bough\", analyzed similarities in religious belief and symbolism globally. Neither Tylor nor Frazer, however, was particularly interested in fieldwork, nor were they interested in examining how the cultural elements and institutions fit together. The Golden Bough was abridged drastically in subsequent editions after his first.\n\nToward the turn of the twentieth century, a number of anthropologists became dissatisfied with this categorization of cultural elements; historical reconstructions also came to seem increasingly speculative to them. Under the influence of several younger scholars, a new approach came to predominate among British anthropologists, concerned with analyzing how societies held together in the present (synchronic analysis, rather than diachronic or historical analysis), and emphasizing long-term (one to several years) immersion fieldwork. Cambridge University financed a multidisciplinary expedition to the Torres Strait Islands in 1898, organized by Alfred Cort Haddon and including a physician-anthropologist, William Rivers, as well as a linguist, a botanist, and other specialists. The findings of the expedition set new standards for ethnographic description.\n\nA decade and a half later, Polish anthropology student Bronisław Malinowski (1884–1942) was beginning what he expected to be a brief period of fieldwork in the old model, collecting lists of cultural items, when the outbreak of the First World War stranded him in New Guinea. As a subject of the Austro-Hungarian Empire resident on a British colonial possession, he was effectively confined to New Guinea for several years.\n\nHe made use of the time by undertaking far more intensive fieldwork than had been done by \"British\" anthropologists, and his classic ethnography, \"Argonauts of the Western Pacific\" (1922) advocated an approach to fieldwork that became standard in the field: getting \"the native's point of view\" through participant observation. Theoretically, he advocated a functionalist interpretation, which examined how social institutions functioned to satisfy individual needs.\n\nBritish social anthropology had an expansive moment in the Interwar period, with key contributions coming from the Polish-British Bronisław Malinowski and Meyer Fortes\n\nA. R. Radcliffe-Brown also published a seminal work in 1922. He had carried out his initial fieldwork in the Andaman Islands in the old style of historical reconstruction. However, after reading the work of French sociologists Émile Durkheim and Marcel Mauss, Radcliffe-Brown published an account of his research (entitled simply \"The Andaman Islanders\") that paid close attention to the meaning and purpose of rituals and myths. Over time, he developed an approach known as structural functionalism, which focused on how institutions in societies worked to balance out or create an equilibrium in the social system to keep it functioning harmoniously. (This contrasted with Malinowski's functionalism, and was quite different from the later French structuralism, which examined the conceptual structures in language and symbolism.)\n\nMalinowski and Radcliffe-Brown's influence stemmed from the fact that they, like Boas, actively trained students and aggressively built up institutions that furthered their programmatic ambitions. This was particularly the case with Radcliffe-Brown, who spread his agenda for \"Social Anthropology\" by teaching at universities across the British Commonwealth. From the late 1930s until the postwar period appeared a string of monographs and edited volumes that cemented the paradigm of British Social Anthropology (BSA). Famous ethnographies include \"The Nuer,\" by Edward Evan Evans-Pritchard, and \"The Dynamics of Clanship Among the Tallensi,\" by Meyer Fortes; well-known edited volumes include \"African Systems of Kinship and Marriage\" and \"African Political Systems.\"\n\nMax Gluckman, together with many of his colleagues at the Rhodes-Livingstone Institute and students at Manchester University, collectively known as the Manchester School, took BSA in new directions through their introduction of explicitly Marxist-informed theory, their emphasis on conflicts and conflict resolution, and their attention to the ways in which individuals negotiate and make use of the social structural possibilities.\n\nIn Britain, anthropology had a great intellectual impact, it \"contributed to the erosion of Christianity, the growth of cultural relativism, an awareness of the survival of the primitive in modern life, and the replacement of diachronic modes of analysis with synchronic, all of which are central to modern culture.\"\n\nLater in the 1960s and 1970s, Edmund Leach and his students Mary Douglas and Nur Yalman, among others, introduced French structuralism in the style of Lévi-Strauss; while British anthropology has continued to emphasize social organization and economics over purely symbolic or literary topics, differences among British, French, and American sociocultural anthropologies have diminished with increasing dialogue and borrowing of both theory and methods. Today, social anthropology in Britain engages internationally with many other social theories and has branched in many directions.\n\nIn countries of the British Commonwealth, social anthropology has often been institutionally separate from physical anthropology and primatology, which may be connected with departments of biology or zoology; and from archaeology, which may be connected with departments of Classics, Egyptology, and the like. In other countries (and in some, particularly smaller, British and North American universities), anthropologists have also found themselves institutionally linked with scholars of folklore, museum studies, human geography, sociology, social relations, ethnic studies, cultural studies, and social work.\n\nAnthropology has been used in Britain to provide an alternative explanation for the Financial crisis of 2007–2010 to the technical explanations rooted in economic and political theory. Dr. Gillian Tett, a Cambridge University trained anthropologist who went on to become a senior editor at the Financial Times is one of the leaders in this use of anthropology.\n\nCanadian anthropology began, as in other parts of the Colonial world, as ethnological data in the records of travellers and missionaries. In Canada, Jesuit missionaries such as Fathers LeClercq, Le Jeune and Sagard, in the 17th century, provide the oldest ethnographic records of native tribes in what was then the Dominion of Canada. The academic discipline has drawn strongly on both the British Social Anthropology and the American Cultural Anthropology traditions, producing a hybrid \"Socio-cultural\" anthropology.\n\nTrue anthropology began with a Government department: the Geological Survey of Canada, and George Mercer Dawson (director in 1895). Dawson's support for anthropology created impetus for the profession in Canada. This was expanded upon by Prime Minister Wilfrid Laurier, who established a Division of Anthropology within the Geological Survey in 1910.\n\nAnthropologists were recruited from England and the USA, setting the foundation for the unique Canadian style of anthropology. Scholars include the linguist and Boasian Edward Sapir.\n\nAnthropology in France has a less clear genealogy than the British and American traditions, in part because many French writers influential in anthropology have been trained or held faculty positions in sociology, philosophy, or other fields rather than in anthropology.\n\nMost commentators consider Marcel Mauss (1872–1950), nephew of the influential sociologist Émile Durkheim, to be the founder of the French anthropological tradition. Mauss belonged to Durkheim's \"Année Sociologique\" group. While Durkheim and others examined the state of modern societies, Mauss and his collaborators (such as Henri Hubert and Robert Hertz) drew on ethnography and philology to analyze societies that were not as 'differentiated' as European nation states.\n\nTwo works by Mauss in particular proved to have enduring relevance: \"Essay on the Gift,\" a seminal analysis of exchange and reciprocity, and his Huxley lecture on the notion of the person, the first comparative study of notions of person and selfhood cross-culturally.\n\nThroughout the interwar years, French interest in anthropology often dovetailed with wider cultural movements such as surrealism and primitivism, which drew on ethnography for inspiration. Marcel Griaule and Michel Leiris are examples of people who combined anthropology with the French avant-garde. During this time most of what is known as \"ethnologie\" was restricted to museums, such as the Musée de l'Homme founded by Paul Rivet, and anthropology had a close relationship with studies of folklore.\n\nAbove all, Claude Lévi-Strauss helped institutionalize anthropology in France. Along with the enormous influence that his theory of structuralism exerted across multiple disciplines, Lévi-Strauss established ties with American and British anthropologists. At the same time, he established centers and laboratories within France to provide an institutional context within anthropology, while training influential students such as Maurice Godelier and Françoise Héritier. They proved influential in the world of French anthropology. Much of the distinct character of France's anthropology today is a result of the fact that most anthropology is carried out in nationally funded research laboratories (CNRS) rather than academic departments in universities\n\nOther influential writers in the 1970s include Pierre Clastres, who explains in his books on the Guayaki tribe in Paraguay that \"primitive societies\" actively oppose the institution of the state. These stateless societies are not less evolved than societies with states, but chose to conjure the institution of authority as a separate function from society. The leader is only a spokesperson for the group when it has to deal with other groups (\"international relations\") but has no inside authority, and may be violently removed if he attempts to abuse this position.\n\nThe most important French social theorist since Foucault and Lévi-Strauss is Pierre Bourdieu, who trained formally in philosophy and sociology and eventually held the Chair of Sociology at the Collège de France. Like Mauss and others before him, he worked on topics both in sociology and anthropology. His fieldwork among the Kabyle of Algeria places him solidly in anthropology, while his analysis of the function and reproduction of fashion and cultural capital in European societies places him as solidly in sociology.\nFrom its beginnings in the early 19th century through the early 20th century, anthropology in the United States was influenced by the presence of Native American societies. \nCultural anthropology in the United States was influenced greatly by the ready availability of Native American societies as ethnographic subjects. The field was pioneered by staff of the Bureau of Indian Affairs and the Smithsonian Institution's Bureau of American Ethnology, men such as John Wesley Powell and Frank Hamilton Cushing.\n\nLate-eighteenth-century ethnology established the scientific foundation for the field, which began to mature in the United States during the presidency of Andrew Jackson (1829–1837). Jackson was responsible for implementing the Indian Removal Act, the coerced and forced removal of an estimated 100,000 American Indians during the 1830s to Indian Territory in present-day Oklahoma; for insuring that the franchise was extended to all white men, irrespective of financial means while denying virtually all black men the right to vote; and, for suppressing abolitionists' efforts to end slavery while vigorously defending that institution. Finally, he was responsible for appointing Chief Justice Roger B. Taney who would decide, in Scott v. Sandford (1857), that Negroes were \"beings of an inferior order, and altogether unfit to associate with the white race ... and so far inferior that they had no rights which the white man was bound to respect\". As a result of this decision, black people, whether free or enslaved, could never become citizens of the United States.\n\nIt was in this context that the so-called American School of Anthropology thrived as the champion of polygenism or the doctrine of multiple origins—sparking a debate between those influenced by the Bible who believed in the unity of humanity and those who argued from a scientific standpoint for the plurality of origins and the antiquity of distinct types. Like the monogenists, these theories were not monolithic and often used words like races, species, hybrid, and mongrel interchangeably. A scientific consensus began to emerge during this period \"that there exists a Genus Homo, embracing many primordial types of 'species'\". Charles Caldwell, Samuel George Morton, Samuel A. Cartwright, George Gliddon, Josiah C. Nott, and Louis Agassiz, and even South Carolina Governor James Henry Hammond were all influential proponents of this school. While some were disinterested scientists, others were passionate advocates who used science to promote slavery in a period of increasing sectional strife. All were complicit in establishing the putative science that justified slavery, informed the Dred Scott decision, underpinned miscegenation laws, and eventually fueled Jim Crow. Samuel G. Morton, for example, claimed to be just a scientist but he did not hesitate to provide evidence of Negro inferiority to John C. Calhoun, the prominent pro-slavery Secretary of State to help him negotiate the annexation of Texas as a slave state.\n\nThe high-water mark of polygenic theories was Josiah Nott and Gliddon's voluminous eight-hundred page tome titled \"Types of Mankind\", published in 1854. Reproducing the work of Louis Agassiz and Samuel Morton, the authors spread the virulent and explicitly racist views to a wider, more popular audience. The first printing sold out quickly and by the end of the century it had undergone nine editions. Although many Southerners felt that all the justification for slavery they needed was found in the Bible, others used the new science to defend slavery and the repression of American Indians. Abolitionists, however, felt they had to take this science on its own terms. And for the first time, African American intellectuals waded into the contentious debate. In the immediate wake of Types of Mankind and during the pitched political battles that led to Civil War, Frederick Douglass (1818–1895), the statesman and persuasive abolitionist, directly attacked the leading theorists of the American School of Anthropology. In an 1854 address, entitled \"The Claims of the Negro Ethnologically Considered\", Douglass argued that \"by making the enslaved a character fit only for slavery, [slaveowners] excuse themselves for refusing to make the slave a freeman... For let it be once granted that the human race are of multitudinous origin, naturally different in their moral, physical, and intellectual capacities ... a chance is left for slavery, as a necessary institution... There is no doubt that Messrs. Nott, Glidden, Morton, Smith and Agassiz were duly consulted by our slavery propagating statesmen\" (p. 287).\n\nLewis Henry Morgan (1818–1881), a lawyer from Rochester, New York, became an advocate for and ethnological scholar of the Iroquois. His comparative analyses of religion, government, material culture, and especially kinship patterns proved to be influential contributions to the field of anthropology. Like other scholars of his day (such as Edward Tylor), Morgan argued that human societies could be classified into categories of cultural evolution on a scale of progression that ranged from \"savagery\", to \"barbarism\", to \"civilization\". He focused on understanding how cultures integrated and systematized, and how the various features of one culture indicate an evolutionary status in comparison with other cultures. Generally, Morgan used technology (such as bowmaking or pottery) as an indicator of position on this scale.\n\nFranz Boas established academic anthropology in the United States in opposition to this sort of evolutionary perspective. His approach was empirical, skeptical of overgeneralizations, and eschewed attempts to establish universal laws. For example, Boas studied immigrant children to demonstrate that biological race was not immutable, and that human conduct and behavior resulted from nurture, rather than nature.\n\nInfluenced by the German tradition, Boas argued that the world was full of distinct \"cultures,\" rather than societies whose evolution could be measured by how much or how little \"civilization\" they had. He believed that each culture has to be studied in its particularity, and argued that cross-cultural generalizations, like those made in the natural sciences, were not possible.\n\nIn doing so, he fought discrimination against immigrants, blacks, and indigenous peoples of the Americas. Many American anthropologists adopted his agenda for social reform, and theories of race continue to be popular subjects for anthropologists today. The so-called \"Four Field Approach\" has its origins in Boasian Anthropology, dividing the discipline in the four crucial and interrelated fields of sociocultural, biological, linguistic, and archaic anthropology (e.g. archaeology). Anthropology in the United States continues to be deeply influenced by the Boasian tradition, especially its emphasis on culture.\nBoas used his positions at Columbia University and the American Museum of Natural History to train and develop multiple generations of students. His first generation of students included Alfred Kroeber, Robert Lowie, Edward Sapir and Ruth Benedict, who each produced richly detailed studies of indigenous North American cultures. They provided a wealth of details used to attack the theory of a single evolutionary process. Kroeber and Sapir's focus on Native American languages helped establish linguistics as a truly general science and free it from its historical focus on Indo-European languages.\n\nThe publication of Alfred Kroeber's textbook, \"Anthropology,\" marked a turning point in American anthropology. After three decades of amassing material, Boasians felt a growing urge to generalize. This was most obvious in the 'Culture and Personality' studies carried out by younger Boasians such as Margaret Mead and Ruth Benedict. Influenced by psychoanalytic psychologists including Sigmund Freud and Carl Jung, these authors sought to understand the way that individual personalities were shaped by the wider cultural and social forces in which they grew up.\n\nThough such works as \"Coming of Age in Samoa\" and \"The Chrysanthemum and the Sword\" remain popular with the American public, Mead and Benedict never had the impact on the discipline of anthropology that some expected. Boas had planned for Ruth Benedict to succeed him as chair of Columbia's anthropology department, but she was sidelined by Ralph Linton, and Mead was limited to her offices at the AMNH.\n\nAnthropology as it emerged amongst the Western colonial powers (mentioned above) has generally taken a different path than that in the countries of southern and central Europe (Italy, Greece, and the successors to the Austro-Hungarian and Ottoman empires). In the former, the encounter with multiple, distinct cultures, often very different in organization and language from those of Europe, has led to a continuing emphasis on cross-cultural comparison and a receptiveness to certain kinds of cultural relativism.\n\nIn the successor states of continental Europe, on the other hand, anthropologists often joined with folklorists and linguists in building cultural perspectives on nationalism. Ethnologists in these countries tended to focus on differentiating among local ethnolinguistic groups, documenting local folk culture, and representing the prehistory of what has become a nation through various forms of public education (e.g., museums of several kinds).\n\nIn this scheme, Russia occupied a middle position. On the one hand, it had a large region (largely east of the Urals) of highly distinct, pre-industrial, often non-literate peoples, similar to the situation in the Americas. On the other hand, Russia also participated to some degree in the nationalist (cultural and political) movements of Central and Eastern Europe. After the Revolution of 1917, views expressed by anthropologists in the USSR, and later the Soviet Bloc countries, were highly shaped by the requirement to conform to Marxist theories of social evolution.\n\nIn Greece, there was since the 19th century a science of the folklore called \"laographia\" (laography), in the form of \"a science of the interior\", although theoretically weak; but the connotation of the field deeply changed after World War II, when a wave of Anglo-American anthropologists introduced a science \"of the outside\". \n\nIn Italy, the development of ethnology and related studies did not receive as much attention as other branches of learning, but nonetheless included important researchers and thinkers like Ernesto De Martino.\n\nGermany and Norway are the countries that showed the most division and conflict between scholars focusing on domestic socio-cultural issues and scholars focusing on \"other\" societies.. Some German and Austrian scholars have increased cultural anthropology as both legal anthropology regarding \"other\" societies and anthropology of Western civilization.\n\nThe development of world anthropologies has followed different trajectories.\n\nIn the mid-20th century, American anthropology began to study its own history more systematically. In 1967 Marvin Harris published his \"The Rise of Anthropological Theory\", presenting argumentative examinations of anthropology's historical developments, and George W. Stocking, Jr., established the historicist school, examining the historical contexts of anthropological movements.\n\n\n"}
{"id": "9715234", "url": "https://en.wikipedia.org/wiki?curid=9715234", "title": "History of media studies", "text": "History of media studies\n\nThis article outlines the history of media studies.\n\nThough not yet named as such, media studies' roots are in the Chicago School and thinkers such as John Dewey, Charles Cooley and George Mead. These authors saw American society on the cusp of positive social change toward pure democracy. Mead argued that for an ideal society to exist, a form of communication must be developed to allow the unique individual to appreciate the attitudes, viewpoints and positions of others unlike himself, and allow him to be understood by others as well. Mead believed that this \"new media\" would allow humans to empathize with others, and therefore moves toward an \"ideal of human society.\" Where Mead sees an ideal society, Dewey names it the \"Great Community,\" and further asserts the assumption that humans are intelligent enough for self-government, and that that knowledge is \"a function of association and communication.\" Similarly, Cooley asserts that political communication makes public opinion possible, which in turn promotes democracy. Each of these authors represent the Chicago School’s attention to electronic communication as a facilitator of democracy, its faith in the informed electorate, and its focus on the individual as opposed to the mass.\n\nThe social impact of mass communication has been studied at The New School University in New York since its founding in 1919. The first college course to investigate the motion picture was offered here in 1926. Marshall McLuhan's colleague, John Culkin, brought his Center for Understanding Media to The New School in 1975 and The New School began offering the Master of Arts degree in Media Studies, one of the first graduate programs of its kind. Today, among other programs, MA in Media Studies is still being offered by School of Media Studies, The New School, which will celebrate 40th anniversary of Media Studies at The New School during the academic year 2015-2016\n\nBetween the First and Second World Wars, the Institute for Propaganda Analysis briefly rose to importance. Their definition of propaganda was \"expression of opinion or action by individuals or groups deliberately designed to influence opinion or actions of other individuals or groups with reference to predetermined ends.\" \n\nHarold Lasswell, who worked in the paradigm of the Chicago School of sociology wrote Propaganda Technique in the World War, which included this definition of propaganda:\n\"Propaganda in the broadest sense is the technique of influencing human action by the manipulation of representations. These representations may take spoken, written, pictorial or musical form.\"\n\nThese definitions of propaganda clearly show that this was a school of thought that focused on media effects, as it highlighted the influence that media could have over its audiences attitudes and actions.\n\nEpitomizing this early school of media effects studies are experiments done by The Experimental Section of the Research Branch of the U.S. War Department's Information and Education Division. In the experiments, the effects of various U.S. wartime propaganda films on soldiers were observed.\n\nCurrent Propaganda studies are applied into many fields besides politics. Herman described a propaganda model as \"a model of media behavior and performance, not of media effects.\" (Herman, 2000, p. 63) He argued: \"They are profit-seeking business, owned by very wealthy people (or other companies); and they are funded largely by advertisers who are also profit-seeking entities, and who want their advertisements to appear in a supportive selling environment.\" He also presented \"five factors: owner ship, advertising, sourcing, flak and anti-communist ideology-work as filters through which information must pass, and that individually and often in cumulative fashion they greatly influence media choices.\" Until now, there is no conclusion of propaganda, debate still continues.\n\nTypified by the philosophical and theoretical orientations of Max Horkheimer, Theodor Adorno, Walter Benjamin, Leo Lowenthal, and Herbert Marcuse, the Frankfurt school contributed greatly to the development and application of critical theory in media studies. Their Marxist critique of market-driven media was critical of its atomizing and leveling effects.\n\nThe Frankfurt school also lamented the effects of the \"culture industry\" on the production and appreciation of art. For example, in \"A Social Critique of Radio Music\", Adorno asserts: \"…music has ceased to be a human force and is consumed like other consumers’ goods. This produces ‘commodity listening’…The listener suspends all intellectual activity.\"\n\nAs the Frankfurt school lamented on the effects of the \"culture industry\" they also began to identify mass culture and high culture as two distinct entities. Scholars like Benjamin (1936) and Adorno (1945) can be credited with what would eventually become known as popular culture and high culture. Their finite distinction of equating original production with ritualistic behavior as compared with mass culture that finds its identifying symbols in reproductions. These reproductions are souless and lacking in definition and originality.\n\nThe less paradigm in media studies since the Second World War has been associated with the ideas, methods and findings of Paul F. Lazarsfeld and his school: media effect studies. Their studies focused on measurable, short-term behavioral ‘effects’ of media and concluded that the media played a limited role in influencing public opinion. The \"Limited-Effects\" Model developed by Lazarsfeld and his colleagues from Columbia was highly influential in the development of media studies. The model claims the mass media has \"limited-effects\" on voting patterns. Voters are influenced, rather, through the ‘two-step flow’ model, the idea that media messages are disseminated through personal interaction with ‘opinion leaders’.\n\nThe model of limited- effects was so influential that the question of media \"effects\" on politics was left largely unaddressed until the late 1960s. Eventually Mass Communication scholars began to study political behavior again and the limited-effects model was called into question.\n\nAs a response to the previous emphasis upon media effects, from the 1970s researchers became interested in how audiences make sense of media texts. The \"uses and gratifications\" model, associated with Jay Blumler and Elihu Katz, reflected this growing interest in the 'active audience'. One such example of this type of research was conducted by Hodge and Tripp, and separately Palmer, about how school-children make sense of the Australian soap opera \"Prisoner\". They found that pupils could identify with the prisoners: they were \"shut in\", separated from their friends and wouldn't be there had they not been made to be, etc. Also, the children could compare the wardens to their teachers: \"the hard-bitten old [one], the soft new one, the one you could take advantage of...\"John Fiske summarises:\n\nThe children inserted meanings of the program into their social experience of school in a way that informed both -- the meanings of school and the meanings of \"Prisoner\" were each influenced by the other, and the fit between them validated the other.\n"}
{"id": "13372", "url": "https://en.wikipedia.org/wiki?curid=13372", "title": "Human geography", "text": "Human geography\n\nHuman geography is the branch of geography that deals with the study of people and their communities,\ncultures, economies, and interactions with the environment by studying their relations with and across space and place. Human geography attends to human patterns of social interaction, as well as spatial level interdependencies, and how they influence or affect the earth's environment. As an intellectual discipline, geography is divided into the sub-fields of physical geography and human geography, the latter concentrating upon the study of human activities, by the application of qualitative and quantitative research methods.\n\nGeography was not recognized as a formal academic discipline until the 18th century, although many scholars had undertaken geographical scholarship for much longer, particularly through cartography.\n\nThe Royal Geographical Society was founded in England in 1830, although the United Kingdom did not get its first full Chair of geography until 1917. The first real geographical intellect to emerge in United Kingdom's geographical minds was Halford John Mackinder, appointed reader at Oxford University in 1887.\n\nThe National Geographic Society was founded in the United States in 1888 and began publication of the \"National Geographic\" magazine which became, and continues to be, a great popularizer of geographic information. The society has long supported geographic research and education on geographical topics.\n\nThe Association of American Geographers was founded in 1904 and was renamed the American Association of Geographers in 2016 to better reflect the increasingly international character of its membership.\n\nOne of the first examples of geographic methods being used for purposes other than to describe and theorize the physical properties of the earth is John Snow's map of the 1854 Broad Street cholera outbreak. Though Snow was primarily a physician and a pioneer of epidemiology rather than a geographer, his map is probably one of the earliest examples of health geography.\n\nThe now fairly distinct differences between the subfields of physical and human geography have developed at a later date. This connection between both physical and human properties of geography is most apparent in the theory of environmental determinism, made popular in the 19th century by Carl Ritter and others, and has close links to the field of evolutionary biology of the time. Environmental determinism is the theory, that people's physical, mental and moral habits are directly due to the influence of their natural environment. However, by the mid-19th century, environmental determinism was under attack for lacking methodological rigor associated with modern science, and later as a means to justify racism and imperialism.\n\nA similar concern with both human and physical aspects is apparent during the later 19th and first half of the 20th centuries focused on regional geography.The goal of regional geography, through something known as regionalisation, was to delineate space into regions and then understand and describe the unique characteristics of each region through both human and physical aspects. With links to (possibilism) (geography) and cultural ecology some of the same notions of causal effect of the environment on society and culture remain with environmental determinism.\n\nBy the 1960s, however, the quantitative revolution led to strong criticism of regional geography. Due to a perceived lack of scientific rigor in an overly descriptive nature of the discipline, and a continued separation of geography from its two subfields of physical and human geography and from geology, geographers in the mid-20th century began to apply statistical and mathematical models in order to solve spatial problems. Much of the development during the quantitative revolution is now apparent in the use of geographic information systems; the use of statistics, spatial modeling, and positivist approaches are still important to many branches of human geography. Well-known geographers from this period are Fred K. Schaefer, Waldo Tobler, William Garrison, Peter Haggett, Richard J. Chorley, William Bunge, and Torsten Hägerstrand.\n\nFrom the 1970s, a number of critiques of the positivism now associated with geography emerged. Known under the term 'critical geography,' these critiques signaled another turning point in the discipline. Behavioral geography emerged for some time as a means to understand how people made perceived spaces and places, and made locational decisions. The more influential 'radical geography' emerged in the 1970s and 1980s. It draws heavily on Marxist's theory and techniques, and is associated with geographers such as David Harvey and Richard Peet. Radical geographers seek to say meaningful things about problems recognized through quantitative methods, provide explanations rather than descriptions, put forward alternatives and solutions, and be politically engaged, rather than using the detachment associated with positivists. (The detachment and objectivity of the quantitative revolution was itself critiqued by radical geographers as being a tool of capital). Radical geography and the links to Marxism and related theories remain an important part of contemporary human geography (See: \"Antipode\"). Critical geography also saw the introduction of 'humanistic geography', associated with the work of Yi-Fu Tuan, which pushed for a much more qualitative approach in methodology.\n\nThe changes under critical geography have led to contemporary approaches in the discipline such as feminist geography, new cultural geography, \"demonic\" geographies, and the engagement with postmodern and post-structural theories and philosophies.\n\nThe primary fields of study in human geography focus around the core fields of:\n\nCultural geography is the study of cultural products and norms - their variation across spaces and places, as well as their relations. It focuses on describing and analyzing the ways language, religion, economy, government, and other cultural phenomena vary or remain constant from one place to another and on explaining how humans function spatially. \n\nDevelopment geography is the study of the Earth's geography with reference to the standard of living and the quality of life of its human inhabitants, study of the location, distribution and spatial organization of economic activities, across the Earth. The subject matter investigated is strongly influenced by the researcher's methodological approach.\n\nEconomic geography examines relationships between human economic systems, states, and other factors, and the biophysical environment.\n\nHealth geography is the application of geographical information, perspectives, and methods to the study of health, disease, and health care. Health geography deals with the spatial relations and patterns between people and the environment. This is a sub-discipline of human geography, researching how and why diseases are spread.\n\nHistorical geography is the study of the human, physical, fictional, theoretical, and \"real\" geographies of the past. Historical geography studies a wide variety of issues and topics. A common theme is the study of the geographies of the past and how a place or region changes through time. Many historical geographers study geographical patterns through time, including how people have interacted with their environment, and created the cultural landscape.\n\nPolitical geography is concerned with the study of both the spatially uneven outcomes of political processes and the ways in which political processes are themselves affected by spatial structures.\n\nPopulation geography is the study of ways in which spatial variations in the distribution, composition, migration, and growth of populations are related to their environment or location.\n\nSettlement geography, including urban geography, is the study of urban and rural areas with specific regards to spatial, relational and theoretical aspects of settlement. That is the study of areas which have a concentration of buildings and infrastructure. These are areas where the majority of economic activities are in the secondary sector and tertiary sectors. In case of urban settlement, they probably have a high population density.\nUrban geography is the study of cities, towns, and other areas of relatively dense settlement. Two main interests are site (how a settlement is positioned relative to the physical environment) and situation (how a settlement is positioned relative to other settlements). Another area of interest is the internal organization of urban areas with regard to different demographic groups and the layout of infrastructure. This subdiscipline also draws on ideas from other branches of Human Geography to see their involvement in the processes and patterns evident in an urban area.\n\nWithin each of the subfields, various philosophical approaches can be used in research; therefore, an urban geographer could be a Feminist or Marxist geographer, etc.\n\nSuch approaches are:\n\n\nAs with all social sciences, human geographers publish research and other written work in a variety of academic journals. Whilst human geography is interdisciplinary, there are a number of journals that focus on human geography.\n\nThese include:\n\n\n\n\n"}
{"id": "221773", "url": "https://en.wikipedia.org/wiki?curid=221773", "title": "Human migration", "text": "Human migration\n\nHuman migration is the movement by people from one place to another with the intentions of settling, permanently or temporarily in a new location. The movement is often over long distances and from one country to another, but internal migration is also possible; indeed, this is the dominant form globally. People may migrate as individuals, in family units or in large groups. A person who moves from their home to another place because of natural disaster or civil disturbance may be described as a refugee or, especially within the same country, a displaced person. A person seeking refuge from political, religious, or other forms of persecution is usually described as an asylum seeker.\n\nNomadic movements are normally not regarded as migrations as there is no intention to settle in the new place and because the movement is generally seasonal. Only a few nomadic people have retained this form of lifestyle in modern times. Also, the temporary movement of people for the purpose of travel, tourism, pilgrimages, or the commute is not regarded as migration, in the absence of an intention to live and settle in the visited places.\nMany estimates of statistics in worldwide migration patterns exist.\n\nThe World Bank has published its \"Migration and Remittances Factbook\" annually since 2008. The International Organisation for Migration (IOM) has published a yearly \"World Migration Report\" since 1999. The United Nations Statistics Division also keeps a database on worldwide migration. Recent advances in research on migration via the Internet promise better understanding of migration patterns and migration motives.\n\nSubstantial internal migration can also take place within a country, either seasonal human migration (mainly related to agriculture and to tourism to urban places), or shifts of population into cities (urbanisation) or out of cities (suburbanisation). Studies of worldwide migration patterns, however, tend to limit their scope to international migration.\n\nThe World Bank's \"Migration and Remittances Factbook\" of 2011 lists the following estimates for the year 2010: total number of immigrants: 215.8 million or 3.2% of world population. In 2013, the percentage of international migrants worldwide increased by 33% with 59% of migrants targeting developed regions. Almost half of these migrants are women, which is one of the most significant migrant-pattern changes in the last half century. Women migrate alone or with their family members and community. Even though female migration is largely viewed as associations rather than independent migration, emerging studies argue complex and manifold reasons for this.\n\nOften a distinction is made between voluntary and involuntary migration, or between refugees fleeing political conflict or natural disaster vs. economic or labor migration, but these distinctions are difficult to make and partially subjective, as the motivators for migration are often correlated. The World Bank's report estimates that, as of 2010, 16.3 million or 7.6% of migrants qualified as refugees. At the end of 2012, approximately 15.4 million people were refugees and persons in refugee-like situations - 87% of them found asylum in developing countries.\n\nStructurally, there is substantial South-South and North-North migration, i.e., most emigrants from high-income O.E.C.D. countries migrate to other high-income countries, and a substantial part (estimated at 43%) of emigrants from developing countries migrate to other developing countries. The United Nations Population Fund says that \"[while the North has experienced a higher absolute increase in the migrant stock since 2000 (32 million) compared to the South (25 million), the South recorded a higher growth rate. Between 2000 and 2013 the average annual rate of change of the migrant population in the developing regions (2.3%) slightly exceeded that of the developed regions (2.1%).\nThe top immigration countries are:\n\nThe top countries of origin are:\n\nThe top migration corridors worldwide are:<br>1. Libya–European Union <br>2. Mexico–United States<br>3. Morocco-European Union<br>4. Russia–Ukraine<br>5. Ukraine–Russia<br>6. Bangladesh–India<br>7. Nepal-India<br>8. Turkey–Germany<br>9. Kazakhstan–Russia<br>10. Russia–Kazakhstan<br>11. Cuba-United States<br>12. China–Northern America<br>13. Algeria-France<br>14. India-Northen America<br>15. Philippines-Northern America<br>16. South Korea-Northern America<br>17. Vietnam-Northern America<br>18. China mainland–Hong Kong<br>19. Vietnam-Australia<br>20. Hong Kong-Canada\n\nRemittances, i.e., funds transferred by migrant workers to their home country, form a substantial part of the economy of some countries. The top ten remittance recipients in 2017.\nThe Global Commission on International Migration (GCIM), launched in 2003, published a report in 2005. International migration challenges at the global level are addressed through the Global Forum on Migration and Development and the Global Migration Group, both established in 2006.\n\nThe United Nations reported that 2014 had the highest level of forced migration on record: 59.5 million individuals, caused by \"persecution, conflict, generalized violence, or human rights violations\", as compared with 51.2 million in 2013 (an increase of 8.3 million) and with 37.5 million a decade prior. one of every 122 humans is a refugee, internally displaced, or seeking asylum. National Geographic has published 5 maps showing human migrations in progress in 2015 based on the UN report.\n\nNumerous causes impel migrants to move to another country. For instance, globalization has increased the demand for workers in order to sustain national economies. Thus one category of economic migrants - generally from impoverished developing countries - migrates to obtain sufficient income for survival.\nSuch migrants often send some of their income home to family members in the form of economic remittances, which have become an economic staple in a number of developing countries. People may also move or are forced to move as a result of conflict, of human-rights violations, of violence, or to escape persecution. In 2013 it was estimated that around 51.2 million people fell into this category. Other reasons people may move include to gain access to opportunities and services or to escape extreme weather. This type of movement, usually from rural to urban areas, may class as internal migration. Socio-cultural and geo-historical factors also play a major role. In North Africa, for example, emigrating Europe counts as a sign of social prestige. Moreover, many countries were former colonies. This means that many have relatives who live legally in the (former) colonial metropole, and who often provide important help for immigrants arriving in that metropole.\nRelatives may help with job research and with accommodation. The geographical proximity of Africa to Europe and the long historical ties between Northern and Southern Mediterranean countries also prompt many to migrate.\n\nA number of theories attempt to explain the international flow of capital and people from one country to another.\n\nThis theory of migration states that the main reason for labor migration is wage difference between two geographic locations. These wage differences are usually linked to geographic labor demand and supply. It can be said that areas with a shortage of labor but an excess of capital have a high relative wage while areas with a high labor supply and a dearth of capital have a low relative wage. Labor tends to flow from low-wage areas to high-wage areas. Often, with this flow of labor comes changes in the sending as well as the receiving country. Neoclassical economic theory is best used to describe transnational migration, because it is not confined by international immigration laws and similar governmental regulations.\n\nDual labor market theory states that migration is mainly caused by pull factors in more developed countries. This theory assumes that the labor markets in these developed countries consist of two segments: the primary market, which requires high-skilled labor, and the secondary market, which is very labor-intensive requiring low-skilled workers. This theory assumes that migration from less developed countries into more developed countries is a result of a pull created by a need for labor in the developed countries in their secondary market. Migrant workers are needed to fill the lowest rung of the labor market because the native laborers do not want to do these jobs as they present a lack of mobility. This creates a need for migrant workers. Furthermore, the initial dearth in available labor pushes wages up, making migration even more enticing.\n\nThis theory states that migration flows and patterns can't be explained solely at the level of individual workers and their economic incentives, but that wider social entities must be considered as well. One such social entity is the household. Migration can be viewed as a result of risk aversion on the part of a household that has insufficient income. The household, in this case, is in need of extra capital that can be achieved through remittances sent back by family members who participate in migrant labor abroad. These remittances can also have a broader effect on the economy of the sending country as a whole as they bring in capital. Recent research has examined a decline in U.S. interstate migration from 1991 to 2011, theorizing that the reduced interstate migration is due to a decline in the geographic specificity of occupations and an increase in workers’ ability to learn about other locations before moving there, through both information technology and inexpensive travel. Other researchers find that the location-specific nature of housing is more important than moving costs in determining labour reallocation.\n\nRelative deprivation theory states that awareness of the income difference between neighbors or other households in the migrant-sending community is an important factor in migration. The incentive to migrate is a lot higher in areas that have a high level of economic inequality. In the short run, remittances may increase inequality, but in the long run, they may actually decrease it. There are two stages of migration for a worker: first, they invest in human capital formation, and then they try to capitalize on their investments. In this way, successful migrants may use their new capital to provide for better schooling for their children and better homes for their families. Successful high-skilled emigrants may serve as an example for neighbors and potential migrants who hope to achieve that level of success.\n\nWorld-systems theory looks at migration from a global perspective. It explains that interaction between different societies can be an important factor in social change within societies. Trade with one country, which causes economic decline in another, may create incentive to migrate to a country with a more vibrant economy. It can be argued that even after decolonization, the economic dependence of former colonies still remains on mother countries. This view of international trade is controversial, however, and some argue that free trade can actually reduce migration between developing and developed countries. It can be argued that the developed countries import labor-intensive goods, which causes an increase in employment of unskilled workers in the less developed countries, decreasing the outflow of migrant workers. The export of capital-intensive goods from rich countries to poor countries also equalizes income and employment conditions, thus also slowing migration. In either direction, this theory can be used to explain migration between countries that are geographically far apart.\n\nOld migration theories are generally embedded in geography, sociology or economics. They explain migration in specific periods and spaces. In fact, Osmosis theory explains the whole phenomenon of human migration. Based on the history of human migration, Djelti (2017a) studies the evolution of its natural determinants. According to him, human migration is divided into two main types: the simple migration and the complicated one. The simple migration is divided, in its turn, into diffusion, stabilisation and concentration periods. During these periods, water availability, adequate climate, security and population density represent the natural determinants of human migration. For the complicated migration, it is characterised by the speedy evolution and the emergence of new sub-determinants notably earning, unemployment, networks and migration policies. Osmosis theory (Djelti, 2017b) explains analogically human migration by the biophysical phenomenon of osmosis. In this respect, the countries are represented by animal cells, the borders by the semipermeable membranes and the humans by ions of water. As to osmosis phenomenon, according to the theory, humans migrate from countries with less migration pressure to countries with high migration pressure. In order to measure the latter, the natural determinants of human migration replace the variables of the second principle of thermodynamics used to measure the osmotic pressure.\n\nA number of social scientists have examined immigration from a sociological perspective, paying particular attention to how immigration affects, and is affected by, matters of race and ethnicity, as well as social structure. They have produced three main sociological perspectives: symbolic interactionism, which aims to understand migration via face-to-face interactions on a micro-level; social conflict theory examines migration through the prism of competition for power and resources; structural functionalism, based on the ideas of Émile Durkheim, examines the role of migration in fulfilling certain functions within each society, such as the decrease of despair and aimlessness and the consolidation of social networks.\n\nMore recently, as attention shifted away from countries of destination, sociologists have attempted to understand how transnationalism allows us to understand the interplay between migrants, their countries of destination, and their countries of origins. In this framework, work on social remittances by Peggy Levitt and others has led to a stronger conceptualisation of how migrants affect socio-political processes in their countries of origin.\n\nPolitical scientists have put forth a number of theoretical frameworks on migration, offering different perspectives on processes of security, citizenship, and international relations. The political importance of diasporas has also become a growing field of interest, as scholars examine questions of diaspora activism, state-diaspora relations, out-of-country voting processes, and states' soft power strategies. In this field, the majority of work has focused on immigration politics, viewing migration from the perspective of the country of destination. With regard to emigration processes, political scientists have expanded on Albert Hirschman's framework on 'voice' vs. 'exit' to discuss how emigration affects the politics within the countries of origin.\n\nCertain laws of social science have been proposed to describe human migration. The following was a standard list after Ravenstein's (1834–1913) proposal in the 1880s. The laws are as follows:\n\n\nLee's laws divide factors causing migrations into two groups of factors: push and pull factors. Push factors are things that are unfavourable about the area that one lives in, and pull factors are things that attract one to another area.\n\nPush factors\nPull factors\n\nSee also article by Gürkan Çelik, in Turkish Review: Turkey Pulls, The Netherlands Pushes? An increasing number of Turks, the Netherlands’ largest ethnic minority, are beginning to return to Turkey, taking with them the education and skills they have acquired abroad, as the Netherlands faces challenges from economic difficulties, social tension and increasingly powerful far-right parties. At the same time Turkey’s political, social and economic conditions have been improving, making returning home all the more appealing for Turks at large. (pp. 94–99)\n\nThe modern field of climate history suggests that the successive waves of Eurasian nomadic movement throughout history have had their origins in climatic cycles, which have expanded or contracted pastureland in Central Asia, especially Mongolia and to its west the Altai. People were displaced from their home ground by other tribes trying to find land that could be grazed by essential flocks, each group pushing the next further to the south and west, into the highlands of Anatolia, the Pannonian Plain, into Mesopotamia, or southwards, into the rich pastures of China. Bogumil Terminski uses the term \"migratory domino effect\" to describe this process in the context of Sea People invasion.\n\n\n\n\n\n\n"}
{"id": "49050655", "url": "https://en.wikipedia.org/wiki?curid=49050655", "title": "Kalahari Debate", "text": "Kalahari Debate\n\nThe Kalahari Debate is a series of back and forth arguments that began in the 1980s amongst anthropologists, archaeologists, and historians about how the San people and hunter-gatherer societies in southern Africa have lived in the past. On one side of the debate were scholars led by Richard Borshay Lee and Irven DeVore, considered traditionalists or \"isolationists.\" On the other side of the debate were scholars led by Edward Wilmsen and James Denbow, considered revisionists or \"integrationists.\"\nLee conducted early and extensive ethnographic research among a San community, the !Kung San. He and other traditionalists consider the San to have been, historically, isolated and independent hunter/gatherers separate from nearby societies. Wilmsen, Denbow and the revisionists oppose these views. They believe that the San have not always been an isolated community, but rather have played important economic roles in surrounding communities. They claim that over time the San have become a dispossessed and marginalized people.\n\nBoth sides use both anthropological and archaeological evidence to fuel their arguments. They interpret cave paintings in Tsodilo Hills, and they also use artifacts such as faunal remains of cattle or sheep found at San sites. They even find Early Stone Age and Early Iron Age technologies at San sites, which both sides use to back their arguments.\n\nThe San are a relatively small group of people whose communities are scattered throughout the Kalahari Desert in southern Africa. They are well known for practicing a hunter/gatherer subsistence strategy (also known as a \"foraging\" mode of production). Traditionalists, including Richard Lee and other anthropologists, view the San as maintaining this old but adaptable way of life, even in the face of changing external circumstances. These anthropologists view the San as isolates who are not, and have never been, part of a greater Kalahari economy. The traditionalists believe that the San have adapted over time but without help from other societies. Emphasis is thereby placed on the cultural continuity and the cultural integrity of the San peoples.\n\nIn Lee’s 1979 book \"The !Kung San: Men, Women, and Work in a Foraging Society,\" his main goal was to be fully immersed in the !Kung San culture so that he could fully understand their way of life. He was puzzled as to how these people seemed to be living such an easy and happy life that relied heavily on hard work and the availability of food. Most of his studies of the San took place in the Dobe area, near the Tsodilo Hills. He was adopted into a kinship and given the name /Tontah which meant “White-Man.” He claims that the San were an isolated hunter-gatherer society that changed to farming and foraging at the end of the 1970s. Most of Lee’s historical data comes from oral stories told by the !Kung San because they did not have anything written down. According to Lee the San were originally afraid of contact with outsiders.\n\nLee reports that the men did the hunting and hard labor while the women did housework. He later found out that the San weren’t just hunter-gatherers, but also herders, foragers, and farmers. In his book he states, “I learned that most of the men had had experience herding cattle at some point in their lives and that many men had owned cattle and goats in the past.” He claims that they have learned all of this on their own. The San wanted wage pay for farming and taking care of cattle, goats, and sheep. This was their new way of life.\n\nEdwin Wilmsen's 1989 book \"Land Filled With Flies\" kicked off the Kalahari Debate. Wilmsen made several remarks attacking anthropologists’ view of the San people. Most of his attacks were at Richard Lee and his work. Wilmsen made claims about the San such as, “Their appearance as foragers is a function of their relegations to an underclass in the playing out of historical processes that began before the current millennium and culminated in the early decades of this century.” This statement upsets the traditionalists because it says that the San are not isolates but have been an underclass in a society throughout history. Wilmsen makes another statement against the traditionalists when he says, “The isolation in which they are said to have been found is a creation of our own view of them, not of their history as they lived it.” He is beginning to say that anthropologists’ judgment is clouded because they already have a predisposed view of the San and hunter-gatherer societies as being isolates. Wilmsen states that the terms “Bushmen,” “Forager,” and “Hunter-Gatherer” contribute to the ideology of them being isolates. He says this is because these terms are commonly associated with isolated groups but his main claim is that for the San this is not the case. Wilmsen also goes on to claim that Lee approaches the San as a people without a history, that they have been doing the same thing forever. He states, “they are permitted antiquity while denied history” Wilmsen continues the argument that anthropologists’ goal is to study hunter-gatherer groups who have lived on their own for centuries, which builds a stereotype for hunter-gatherers. He believes this is why Richard Lee’s views are flawed, and also why he is saying that the San are incorporated in a wider political economy in southern Africa.\n\nThe revisionists believe the !Kung were associated with Bantu-speaking overlords throughout history, and involved with merchant capital. They believe the San in the Kalahari are a classless society because they are actually the lower class of a greater Kalahari society. The revisionists believe the !Kung San were heavily involved in trade. They believe the San were transformed by centuries of contact with Iron Age, Bantu-speaking agro-pastoralists. This argues against the idea that they were a well-adapted hunter-gatherer culture, but instead advanced only through trade and help from nearby economies.\n\nWhen it comes to archaeological evidence, much work still has yet to be done. However, artifacts and ecofacts have been found at southern African sites that could help prove the revisionist view of the San people.\n\nTheir strongest supporting site is in the Tsodilo Hills, where rock art displays San looking over Bantu cattle. In the hills, there are 160 cattle pictures, 10 of which display stick figures near them.\n\nOther evidence revisionists point to includes Early Iron Age products found in Later Stone Age sites. This includes metal and pottery found in the Dobe, Xia, and Botswana regions. Cow bones have also been found in northern Botswana, at Lotshitshi. These products are believed to be payment to the San for labor of caring for or possibly herding Bantu cattle.\n\nThe fuel of this debate is the constant back and forth critiquing by various scholars of each other’s work. Wilmsen would say Lee is blinded by a pre-destined view of the San as isolates. Lee would counter-argue every point that Wilmsen would make, saying either that he made mistakes in research or presents conclusions with little evidence to support them.\n\nOne specific instance is where Lee called out Wilmsen for mistaking the word “oxen” for “onins”, which meant “onions” in an old map of the Kalahari region. This discovery would make the San herders before the arrival of the anthropologists in the 50’s and 60’s and not after the 70’s, as Lee believes. This instance gave rise to Lee’s article \"Oxen or Onions.\" In the article, Lee points out other flaws he believes he has found in Wilmsen’s argument. Critiques of Wilmsen’s work say that the cattle paintings could represent San stealing cattle rather than herding them. Another attack on Wilmsen’s work was that the amounts of pottery and iron found in Dobe and Botswana regions were so small they could fit in one hand. The small numbers of these artifacts make some scholars believe they are insufficient to be able to make such a claim. The same is true of the cattle bones found in Botswana. The small numbers of cattle bone fragments found on San archaeological sites have made scholars question Wilmsen’s argument.\n"}
{"id": "460394", "url": "https://en.wikipedia.org/wiki?curid=460394", "title": "Keyword (rhetoric)", "text": "Keyword (rhetoric)\n\nKeywords are the words that academics use to reveal the internal structure of an author's \nreasoning. While they are used primarily for rhetoric, they are also used in a strictly \ngrammatical sense for structural composition, reasoning, and comprehension. Indeed, they are \nan essential part of any language.\n\nThere are many different types of keyword categories including: Conclusion, Continuation, Contrast,\nEmphasis, Evidence, Illustration and Sequence. Each category serves its own function, as do the keywords\ninside of a given category.\n\nWhen someone uses a search engine, they type in one or more words describing what they are looking for:\n'Norwich florist' or 'cheap holidays Greece', for example. These words or phrases are known as keywords.\n\nIn corpus linguistics, key words are words that appear with statistically unusual frequency in a text\nor a corpus of texts. They are identified by software that compares a word-list of the text with a word-list\nbased on a larger reference corpus. A suitable term for the phenomenon is keyness. The procedure used,\nfor example by WordSmith, to list key words and phrases and plot where they appear in texts. These items\nare very often of interest—particularly those human readers would not likely notice, such as prepositions,\ntime adverbs, and pronouns.\n\nOne method used to teach keywords is called the \"keyword method.\" It involves taking complex keywords that students do not know very well, and makes them into easier words. The easier words must have something to do with the complex keywords so students can correlate between the two.\n\nAmsterdam: Benjamins.\n\n"}
{"id": "17524", "url": "https://en.wikipedia.org/wiki?curid=17524", "title": "Language", "text": "Language\n\nLanguage is a system that consists of the development, acquisition, maintenance and use of complex systems of communication, particularly the human ability to do so; and a language is any specific example of such a system.\n\nThe scientific study of language is called linguistics. Questions concerning the philosophy of language, such as whether words can represent experience, have been debated at least since Gorgias and Plato in ancient Greece. Thinkers such as Rousseau have argued that language originated from emotions while others like Kant have held that it originated from rational and logical thought. 20th-century philosophers such as Wittgenstein argued that philosophy is really the study of language. Major figures in linguistics include Ferdinand de Saussure and Noam Chomsky.\n\nEstimates of the number of human languages in the world vary between 5,000 and 7,000. However, any precise estimate depends on a partly arbitrary distinction between languages and dialects. Natural languages are spoken or signed, but any language can be encoded into secondary media using auditory, visual, or tactile stimuli – for example, in whistling, signed, or braille. This is because human language is modality-independent. Depending on philosophical perspectives regarding the definition of language and meaning, when used as a general concept, \"language\" may refer to the cognitive ability to learn and use systems of complex communication, or to describe the set of rules that makes up these systems, or the set of utterances that can be produced from those rules. All languages rely on the process of semiosis to relate signs to particular meanings. Oral, manual and tactile languages contain a phonological system that governs how symbols are used to form sequences known as words or morphemes, and a syntactic system that governs how words and morphemes are combined to form phrases and utterances.\n\nHuman language has the properties of productivity and displacement, and relies entirely on social convention and learning. Its complex structure affords a much wider range of expressions than any known system of animal communication. Language is thought to have originated when early hominins started gradually changing their primate communication systems, acquiring the ability to form a theory of other minds and a shared intentionality. This development is sometimes thought to have coincided with an increase in brain volume, and many linguists see the structures of language as having evolved to serve specific communicative and social functions. Language is processed in many different locations in the human brain, but especially in Broca's and Wernicke's areas. Humans acquire language through social interaction in early childhood, and children generally speak fluently by approximately three years old. The use of language is deeply entrenched in human culture. Therefore, in addition to its strictly communicative uses, language also has many social and cultural uses, such as signifying group identity, social stratification, as well as social grooming and entertainment.\n\nLanguages evolve and diversify over time, and the history of their evolution can be reconstructed by comparing modern languages to determine which traits their ancestral languages must have had in order for the later developmental stages to occur. A group of languages that descend from a common ancestor is known as a language family. The Indo-European family is the most widely spoken and includes languages as diverse as English, Russian and Hindi; the Sino-Tibetan family includes Mandarin, Bodo and the other Chinese languages, and Tibetan; the Afro-Asiatic family includes Arabic, Somali, and Hebrew; the Bantu languages include Swahili, and Zulu, and hundreds of other languages spoken throughout Africa; and the Malayo-Polynesian languages include Indonesian, Malay, Tagalog, and hundreds of other languages spoken throughout the Pacific. The languages of the Dravidian family, spoken mostly in Southern India, include Tamil Telugu and Kannada. Academic consensus holds that between 50% and 90% of languages spoken at the beginning of the 21st century will probably have become extinct by the year 2100.\n\nThe English word \"language\" derives ultimately from Proto-Indo-European \"\" \"tongue, speech, language\" through Latin \"lingua\", \"language; tongue\", and Old French \"language\". The word is sometimes used to refer to codes, ciphers, and other kinds of artificially constructed communication systems such as formally defined computer languages used for computer programming. Unlike conventional human languages, a formal language in this sense is a system of signs for encoding and decoding information. This article specifically concerns the properties of natural human language as it is studied in the discipline of linguistics.\n\nAs an object of linguistic study, \"language\" has two primary meanings: an abstract concept, and a specific linguistic system, e.g. \"French\". The Swiss linguist Ferdinand de Saussure, who defined the modern discipline of linguistics, first explicitly formulated the distinction using the French word \"langage\" for language as a concept, \"langue\" as a specific instance of a language system, and \"parole\" for the concrete usage of speech in a particular language.\n\nWhen speaking of language as a general concept, definitions can be used which stress different aspects of the phenomenon. These definitions also entail different approaches and understandings of language, and they also inform different and often incompatible schools of linguistic theory. Debates about the nature and origin of language go back to the ancient world. Greek philosophers such as Gorgias and Plato debated the relation between words, concepts and reality. Gorgias argued that language could represent neither the objective experience nor human experience, and that communication and truth were therefore impossible. Plato maintained that communication is possible because language represents ideas and concepts that exist independently of, and prior to, language.\n\nDuring the Enlightenment and its debates about human origins, it became fashionable to speculate about the origin of language. Thinkers such as Rousseau and Herder argued that language had originated in the instinctive expression of emotions, and that it was originally closer to music and poetry than to the logical expression of rational thought. Rationalist philosophers such as Kant and Descartes held the opposite view. Around the turn of the 20th century, thinkers began to wonder about the role of language in shaping our experiences of the world – asking whether language simply reflects the objective structure of the world, or whether it creates concepts that it in turn imposes on our experience of the objective world. This led to the question of whether philosophical problems are really firstly linguistic problems. The resurgence of the view that language plays a significant role in the creation and circulation of concepts, and that the study of philosophy is essentially the study of language, is associated with what has been called the linguistic turn and philosophers such as Wittgenstein in 20th-century philosophy. These debates about language in relation to meaning and reference, cognition and consciousness remain active today.\n\nOne definition sees language primarily as the mental faculty that allows humans to undertake linguistic behaviour: to learn languages and to produce and understand utterances. This definition stresses the universality of language to all humans, and it emphasizes the biological basis for the human capacity for language as a unique development of the human brain. Proponents of the view that the drive to language acquisition is innate in humans argue that this is supported by the fact that all cognitively normal children raised in an environment where language is accessible will acquire language without formal instruction. Languages may even develop spontaneously in environments where people live or grow up together without a common language; for example, creole languages and spontaneously developed sign languages such as Nicaraguan Sign Language. This view, which can be traced back to the philosophers Kant and Descartes, understands language to be largely innate, for example, in Chomsky's theory of Universal Grammar, or American philosopher Jerry Fodor's extreme innatist theory. These kinds of definitions are often applied in studies of language within a cognitive science framework and in neurolinguistics.\n\nAnother definition sees language as a formal system of signs governed by grammatical rules of combination to communicate meaning. This definition stresses that human languages can be described as closed structural systems consisting of rules that relate particular signs to particular meanings. This structuralist view of language was first introduced by Ferdinand de Saussure, and his structuralism remains foundational for many approaches to language.\n\nSome proponents of Saussure's view of language have advocated a formal approach which studies language structure by identifying its basic elements and then by presenting a formal account of the rules according to which the elements combine in order to form words and sentences. The main proponent of such a theory is Noam Chomsky, the originator of the generative theory of grammar, who has defined language as the construction of sentences that can be generated using transformational grammars. Chomsky considers these rules to be an innate feature of the human mind and to constitute the rudiments of what language is. By way of contrast, such transformational grammars are also commonly used to provide formal definitions of language are commonly used in formal logic, in formal theories of grammar, and in applied computational linguistics. In the philosophy of language, the view of linguistic meaning as residing in the logical relations between propositions and reality was developed by philosophers such as Alfred Tarski, Bertrand Russell, and other formal logicians.\n\nYet another definition sees language as a system of communication that enables humans to exchange verbal or symbolic utterances. This definition stresses the social functions of language and the fact that humans use it to express themselves and to manipulate objects in their environment. Functional theories of grammar explain grammatical structures by their communicative functions, and understand the grammatical structures of language to be the result of an adaptive process by which grammar was \"tailored\" to serve the communicative needs of its users.\n\nThis view of language is associated with the study of language in pragmatic, cognitive, and interactive frameworks, as well as in sociolinguistics and linguistic anthropology. Functionalist theories tend to study grammar as dynamic phenomena, as structures that are always in the process of changing as they are employed by their speakers. This view places importance on the study of linguistic typology, or the classification of languages according to structural features, as it can be shown that processes of grammaticalization tend to follow trajectories that are partly dependent on typology. In the philosophy of language, the view of pragmatics as being central to language and meaning is often associated with Wittgenstein's later works and with ordinary language philosophers such as J.L. Austin, Paul Grice, John Searle, and W.O. Quine.\n\nA number of features, many of which were described by Charles Hockett and called design features set human language apart from other known systems of communication, such as those used by non-human animals.\n\nCommunication systems used by other animals such as bees or apes are closed systems that consist of a finite, usually very limited, number of possible ideas that can be expressed. In contrast, human language is open-ended and productive, meaning that it allows humans to produce a vast range of utterances from a finite set of elements, and to create new words and sentences. This is possible because human language is based on a dual code, in which a finite number of elements which are meaningless in themselves (e.g. sounds, letters or gestures) can be combined to form an infinite number of larger units of meaning (words and sentences). However, one study has demonstrated that an Australian bird, the chestnut-crowned babbler, is capable of using the same acoustic elements in different arrangements to create two functionally distinct vocalizations. Additionally, pied babblers have demonstrated the ability to generate two functionally distinct vocalisations composed of the same sound type, which can only be distinguished by the number of repeated elements.\n\nSeveral species of animals have proved to be able to acquire forms of communication through social learning: for instance a bonobo named Kanzi learned to express itself using a set of symbolic lexigrams. Similarly, many species of birds and whales learn their songs by imitating other members of their species. However, while some animals may acquire large numbers of words and symbols, none have been able to learn as many different signs as are generally known by an average 4 year old human, nor have any acquired anything resembling the complex grammar of human language.\n\nHuman languages also differ from animal communication systems in that they employ grammatical and semantic categories, such as noun and verb, present and past, which may be used to express exceedingly complex meanings. Human language is also unique in having the property of recursivity: for example, a noun phrase can contain another noun phrase (as in \"<nowiki>the chimpanzee]'s lips]</nowiki>\") or a clause can contain another clause (as in \"<nowiki>[I see [the dog is running</nowiki>\"). Human language is also the only known natural communication system whose adaptability may be referred to as \"modality independent\". This means that it can be used not only for communication through one channel or medium, but through several. For example, spoken language uses the auditive modality, whereas sign languages and writing use the visual modality, and braille writing uses the tactile modality.\n\nHuman language is also unique in being able to refer to abstract concepts and to imagined or hypothetical events as well as events that took place in the past or may happen in the future. This ability to refer to events that are not at the same time or place as the speech event is called \"displacement\", and while some animal communication systems can use displacement (such as the communication of bees that can communicate the location of sources of nectar that are out of sight), the degree to which it is used in human language is also considered unique.\n\nTheories about the origin of language differ in regard to their basic assumptions about what language is. Some theories are based on the idea that language is so complex that one cannot imagine it simply appearing from nothing in its final form, but that it must have evolved from earlier pre-linguistic systems among our pre-human ancestors. These theories can be called continuity-based theories. The opposite viewpoint is that language is such a unique human trait that it cannot be compared to anything found among non-humans and that it must therefore have appeared suddenly in the transition from pre-hominids to early man. These theories can be defined as discontinuity-based. Similarly, theories based on the generative view of language pioneered by Noam Chomsky see language mostly as an innate faculty that is largely genetically encoded, whereas functionalist theories see it as a system that is largely cultural, learned through social interaction.\n\nChomsky is one prominent proponent of a discontinuity-based theory of human language origins. He suggests that for scholars interested in the nature of language, \"talk about the evolution of the language capacity is beside the point.\" Chomsky proposes that perhaps \"some random mutation took place [...] and it reorganized the brain, implanting a language organ in an otherwise primate brain.\" Though cautioning against taking this story literally, Chomsky insists that \"it may be closer to reality than many other fairy tales that are told about evolutionary processes, including language.\"\n\nContinuity-based theories are held by a majority of scholars, but they vary in how they envision this development. Those who see language as being mostly innate, for example psychologist Steven Pinker, hold the precedents to be animal cognition, whereas those who see language as a socially learned tool of communication, such as psychologist Michael Tomasello, see it as having developed from animal communication in primates: either gestural or vocal communication to assist in cooperation. Other continuity-based models see language as having developed from music, a view already espoused by Rousseau, Herder, Humboldt, and Charles Darwin. A prominent proponent of this view is archaeologist Steven Mithen. Stephen Anderson states that the age of spoken languages is estimated at 60,000 to 100,000 years and that: Researchers on the evolutionary origin of language generally find it plausible to suggest that language was invented only once, and that all modern spoken languages are thus in some way related, even if that relation can no longer be recovered ... because of limitations on the methods available for reconstruction.\n\nBecause language emerged in the early prehistory of man, before the existence of any written records, its early development has left no historical traces, and it is believed that no comparable processes can be observed today. Theories that stress continuity often look at animals to see if, for example, primates display any traits that can be seen as analogous to what pre-human language must have been like. And early human fossils can be inspected for traces of physical adaptation to language use or pre-linguistic forms of symbolic behaviour. Among the signs in human fossils that may suggest linguistic abilities are: the size of the brain relative to body mass, the presence of a larynx capable of advanced sound production and the nature of tools and other manufactured artifacts.\n\nIt was mostly undisputed that pre-human australopithecines did not have communication systems significantly different from those found in great apes in general. However, a 2017 study on Ardipithecus ramidus challenges this belief. Scholarly opinions vary as to the developments since the appearance of the genus \"Homo\" some 2.5 million years ago. Some scholars assume the development of primitive language-like systems (proto-language) as early as \"Homo habilis\" (2.3 million years ago) while others place the development of primitive symbolic communication only with \"Homo erectus\" (1.8 million years ago) or \"Homo heidelbergensis\" (0.6 million years ago), and the development of language proper with Anatomically Modern \"Homo sapiens\" with the Upper Paleolithic revolution less than 100,000 years ago.\n\nThe study of language, linguistics, has been developing into a science since the first grammatical descriptions of particular languages in India more than 2000 years ago, after the development of the Brahmi script. Modern linguistics is a science that concerns itself with all aspects of language, examining it from all of the theoretical viewpoints described above.\n\nThe academic study of language is conducted within many different disciplinary areas and from different theoretical angles, all of which inform modern approaches to linguistics. For example, descriptive linguistics examines the grammar of single languages, theoretical linguistics develops theories on how best to conceptualize and define the nature of language based on data from the various extant human languages, sociolinguistics studies how languages are used for social purposes informing in turn the study of the social functions of language and grammatical description, neurolinguistics studies how language is processed in the human brain and allows the experimental testing of theories, computational linguistics builds on theoretical and descriptive linguistics to construct computational models of language often aimed at processing natural language or at testing linguistic hypotheses, and historical linguistics relies on grammatical and lexical descriptions of languages to trace their individual histories and reconstruct trees of language families by using the comparative method.\n\nThe formal study of language is often considered to have started in India with Pāṇini, the 5th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. However, Sumerian scribes already studied the differences between Sumerian and Akkadian grammar around 1900 BC. Subsequent grammatical traditions developed in all of the ancient cultures that adopted writing.\n\nIn the 17th century AD, the French Port-Royal Grammarians developed the idea that the grammars of all languages were a reflection of the universal basics of thought, and therefore that grammar was universal. In the 18th century, the first use of the comparative method by British philologist and expert on ancient India William Jones sparked the rise of comparative linguistics. The scientific study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt. Early in the 20th century, Ferdinand de Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them.\n\nBy introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still fundamental in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the Langue-parole distinction, distinguishing language as an abstract system (\"langue\"), from language as a concrete manifestation of this system (\"parole\").\n\nIn the 1960s, Noam Chomsky formulated the generative theory of language. According to this theory, the most basic form of language is a set of syntactic rules that is universal for all humans and which underlies the grammars of all human languages. This set of rules is called Universal Grammar; for Chomsky, describing it is the primary objective of the discipline of linguistics. Thus, he considered that the grammars of individual languages are only of importance to linguistics insofar as they allow us to deduce the universal underlying rules from which the observable linguistic variability is generated.\n\nIn opposition to the formal theories of the generative school, functional theories of language propose that since language is fundamentally a tool, its structures are best analyzed and understood by reference to their functions. Formal theories of grammar seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, while functional theories seek to define the functions performed by language and then relate them to the linguistic elements that carry them out. The framework of cognitive linguistics interprets language in terms of the concepts (which are sometimes universal, and sometimes specific to a particular language) which underlie its forms. Cognitive linguistics is primarily concerned with how the mind creates meaning through language.\n\nSpeaking is the default modality for language in all cultures. The production of spoken language depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus, the ability to acoustically decode speech sounds, and the neurological apparatus required for acquiring and producing language. The study of the genetic bases for human language is at an early stage: the only gene that has definitely been implicated in language production is FOXP2, which may cause a kind of congenital language disorder if affected by mutations.\n\n The brain is the coordinating center of all linguistic activity; it controls both the production of linguistic cognition and of meaning and the mechanics of speech production. Nonetheless, our knowledge of the neurological bases for language is quite limited, though it has advanced considerably with the use of modern imaging techniques. The discipline of linguistics dedicated to studying the neurological aspects of language is called neurolinguistics.\n\nEarly work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with fluency, articulation, word-finding, word repetition, and producing and comprehending complex grammatical sentences, both orally and in writing. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.\n\nWith technological advances in the late 20th century, neurolinguists have also incorporated non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electrophysiology to study language processing in individuals without impairments.\n\nSpoken language relies on human physical ability to produce sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract – the throat, the mouth, and the nose. By controlling the different parts of the speech apparatus, the airstream can be manipulated to produce different speech sounds.\n\nThe sound of speech can be analyzed into a combination of segmental and suprasegmental elements. The segmental elements are those that follow each other in sequences, which are usually represented by distinct letters in alphabetic scripts, such as the Roman script. In free flowing speech, there are no clear boundaries between one segment and the next, nor usually are there any audible pauses between words. Segments therefore are distinguished by their distinct sounds which are a result of their different articulations, and they can be either vowels or consonants. Suprasegmental phenomena encompass such elements as stress, phonation type, voice timbre, and prosody or intonation, all of which may have effects across multiple segments.\n\nConsonants and vowel segments combine to form syllables, which in turn combine to form utterances; these can be distinguished phonetically as the space between two inhalations. Acoustically, these different segments are characterized by different formant structures, that are visible in a spectrogram of the recorded sound wave (See illustration of Spectrogram of the formant structures of three English vowels). Formants are the amplitude peaks in the frequency spectrum of a specific sound.\n\nVowels are those sounds that have no audible friction caused by the narrowing or obstruction of some part of the upper vocal tract. They vary in quality according to the degree of lip aperture and the placement of the tongue within the oral cavity. Vowels are called \"close\" when the lips are relatively closed, as in the pronunciation of the vowel (English \"ee\"), or \"open\" when the lips are relatively open, as in the vowel (English \"ah\"). If the tongue is located towards the back of the mouth, the quality changes, creating vowels such as (English \"oo\"). The quality also changes depending on whether the lips are rounded as opposed to unrounded, creating distinctions such as that between (unrounded front vowel such as English \"ee\") and (rounded front vowel such as German \"ü\").\n\nConsonants are those sounds that have audible friction or closure at some point within the upper vocal tract. Consonant sounds vary by place of articulation, i.e. the place in the vocal tract where the airflow is obstructed, commonly at the lips, teeth, alveolar ridge, palate, velum, uvula, or glottis. Each place of articulation produces a different set of consonant sounds, which are further distinguished by manner of articulation, or the kind of friction, whether full closure, in which case the consonant is called \"occlusive\" or \"stop\", or different degrees of aperture creating \"fricatives\" and \"approximants\". Consonants can also be either \"voiced or unvoiced\", depending on whether the vocal cords are set in vibration by airflow during the production of the sound. Voicing is what separates English in \"bus\" (unvoiced sibilant) from in \"buzz\" (voiced sibilant).\n\nSome speech sounds, both vowels and consonants, involve release of air flow through the nasal cavity, and these are called \"nasals\" or \"nasalized\" sounds. Other sounds are defined by the way the tongue moves within the mouth: such as the l-sounds (called \"laterals\", because the air flows along both sides of the tongue), and the r-sounds (called \"rhotics\") that are characterized by how the tongue is positioned relative to the air stream.\n\nBy using these speech organs, humans can produce hundreds of distinct sounds: some appear very often in the world's languages, whereas others are much more common in certain language families, language areas, or even specific to a single language.\n\nWhen described as a system of symbolic communication, language is traditionally seen as consisting of three parts: signs, meanings, and a code connecting signs with their meanings. The study of the process of semiosis, how signs and meanings are combined, used, and interpreted is called semiotics. Signs can be composed of sounds, gestures, letters, or symbols, depending on whether the language is spoken, signed, or written, and they can be combined into complex signs, such as words and phrases. When used in communication, a sign is encoded and transmitted by a sender through a channel to a receiver who decodes it.\nSome of the properties that define human language as opposed to other communication systems are: the arbitrariness of the linguistic sign, meaning that there is no predictable connection between a linguistic sign and its meaning; the duality of the linguistic system, meaning that linguistic structures are built by combining elements into larger structures that can be seen as layered, e.g. how sounds build words and words build phrases; the discreteness of the elements of language, meaning that the elements out of which linguistic signs are constructed are discrete units, e.g. sounds and words, that can be distinguished from each other and rearranged in different patterns; and the productivity of the linguistic system, meaning that the finite number of linguistic elements can be combined into a theoretically infinite number of combinations.\n\nThe rules by which signs can be combined to form words and phrases are called syntax or grammar. The meaning that is connected to individual signs, morphemes, words, phrases, and texts is called semantics. The division of language into separate but connected systems of sign and meaning goes back to the first linguistic studies of de Saussure and is now used in almost all branches of linguistics.\n\nLanguages express meaning by relating a sign form to a meaning, or its content. Sign forms must be something that can be perceived, for example, in sounds, images, or gestures, and then related to a specific meaning by social convention. Because the basic relation of meaning for most linguistic signs is based on social convention, linguistic signs can be considered arbitrary, in the sense that the convention is established socially and historically, rather than by means of a natural relation between a specific sign form and its meaning.\n\nThus, languages must have a vocabulary of signs related to specific meaning. The English sign \"dog\" denotes, for example, a member of the species \"Canis familiaris\". In a language, the array of arbitrary signs connected to specific meanings is called the lexicon, and a single sign connected to a meaning is called a lexeme. Not all meanings in a language are represented by single words. Often, semantic concepts are embedded in the morphology or syntax of the language in the form of grammatical categories.\n\nAll languages contain the semantic structure of predication: a structure that predicates a property, state, or action. Traditionally, semantics has been understood to be the study of how speakers and interpreters assign truth values to statements, so that meaning is understood to be the process by which a predicate can be said to be true or false about an entity, e.g. \"<nowiki>[x [is y]]\" or \"[x [does y]]</nowiki>\". Recently, this model of semantics has been complemented with more dynamic models of meaning that incorporate shared knowledge about the context in which a sign is interpreted into the production of meaning. Such models of meaning are explored in the field of pragmatics.\n\nDepending on modality, language structure can be based on systems of sounds (speech), gestures (sign languages), or graphic or tactile symbols (writing). The ways in which languages use sounds or signs to construct meaning are studied in phonology. The study of how humans produce and perceive vocal sounds is called phonetics. In spoken language, meaning is produced when sounds become part of a system in which some sounds can contribute to expressing meaning and others do not. In any given language, only a limited number of the many distinct sounds that can be created by the human vocal apparatus contribute to constructing meaning.\n\nSounds as part of a linguistic system are called phonemes. Phonemes are abstract units of sound, defined as the smallest units in a language that can serve to distinguish between the meaning of a pair of minimally different words, a so-called minimal pair. In English, for example, the words \"bat\" and \"pat\" form a minimal pair, in which the distinction between and differentiates the two words, which have different meanings. However, each language contrasts sounds in different ways. For example, in a language that does not distinguish between voiced and unvoiced consonants, the sounds and (if they both occur) could be considered a single phoneme, and consequently, the two pronunciations would have the same meaning. Similarly, the English language does not distinguish phonemically between aspirated and non-aspirated pronunciations of consonants, as many other languages like Korean and Hindi do: the unaspirated in \"spin\" and the aspirated in \"pin\" are considered to be merely different ways of pronouncing the same phoneme (such variants of a single phoneme are called allophones), whereas in Mandarin Chinese, the same difference in pronunciation distinguishes between the words 'crouch' and 'eight' (the accent above the á means that the vowel is pronounced with a high tone).\n\nAll spoken languages have phonemes of at least two different categories, vowels and consonants, that can be combined to form syllables. As well as segments such as consonants and vowels, some languages also use sound in other ways to convey meaning. Many languages, for example, use stress, pitch, duration, and tone to distinguish meaning. Because these phenomena operate outside of the level of single segments, they are called suprasegmental. Some languages have only a few phonemes, for example, Rotokas and Pirahã language with 11 and 10 phonemes respectively, whereas languages like Taa may have as many as 141 phonemes. In sign languages, the equivalent to phonemes (formerly called cheremes) are defined by the basic elements of gestures, such as hand shape, orientation, location, and motion, which correspond to manners of articulation in spoken language.\n\nWriting systems represent language using visual symbols, which may or may not correspond to the sounds of spoken language. The Latin alphabet (and those on which it is based or that have been derived from it) was originally based on the representation of single sounds, so that words were constructed from letters that generally denote a single consonant or vowel in the structure of the word. In syllabic scripts, such as the Inuktitut syllabary, each sign represents a whole syllable. In logographic scripts, each sign represents an entire word, and will generally bear no relation to the sound of that word in spoken language.\n\nBecause all languages have a very large number of words, no purely logographic scripts are known to exist. Written language represents the way spoken sounds and words follow one after another by arranging symbols according to a pattern that follows a certain direction. The direction used in a writing system is entirely arbitrary and established by convention. Some writing systems use the horizontal axis (left to right as the Latin script or right to left as the Arabic script), while others such as traditional Chinese writing use the vertical dimension (from top to bottom). A few writing systems use opposite directions for alternating lines, and others, such as the ancient Maya script, can be written in either direction and rely on graphic cues to show the reader the direction of reading.\n\nIn order to represent the sounds of the world's languages in writing, linguists have developed the International Phonetic Alphabet, designed to represent all of the discrete sounds that are known to contribute to meaning in human languages.\n\nGrammar is the study of how meaningful elements called \"morphemes\" within a language can be combined into utterances. Morphemes can either be \"free\" or \"bound\". If they are free to be moved around within an utterance, they are usually called \"words\", and if they are bound to other words or morphemes, they are called affixes. The way in which meaningful elements can be combined within a language is governed by rules. The rules for the internal structure of words are called morphology. The rules of the internal structure of phrases and sentences are called \"syntax\".\n\nGrammar can be described as a system of categories and a set of rules that determine how categories combine to form different aspects of meaning. Languages differ widely in whether they are encoded through the use of categories or lexical units. However, several categories are so common as to be nearly universal. Such universal categories include the encoding of the grammatical relations of participants and predicates by grammatically distinguishing between their relations to a predicate, the encoding of temporal and spatial relations on predicates, and a system of grammatical person governing reference to and distinction between speakers and addressees and those about whom they are speaking.\n\nLanguages organize their parts of speech into classes according to their functions and positions relative to other parts. All languages, for instance, make a basic distinction between a group of words that prototypically denotes things and concepts and a group of words that prototypically denotes actions and events. The first group, which includes English words such as \"dog\" and \"song\", are usually called nouns. The second, which includes \"run\" and \"sing\", are called verbs. Another common category is the adjective: words that describe properties or qualities of nouns, such as \"red\" or \"big\". Word classes can be \"open\" if new words can continuously be added to the class, or relatively \"closed\" if there is a fixed number of words in a class. In English, the class of pronouns is closed, whereas the class of adjectives is open, since an infinite number of adjectives can be constructed from verbs (e.g. \"saddened\") or nouns (e.g. with the -like suffix, as in \"noun-like\"). In other languages such as Korean, the situation is the opposite, and new pronouns can be constructed, whereas the number of adjectives is fixed.\n\nWord classes also carry out differing functions in grammar. Prototypically, verbs are used to construct predicates, while nouns are used as arguments of predicates. In a sentence such as \"Sally runs\", the predicate is \"runs\", because it is the word that predicates a specific state about its argument \"Sally\". Some verbs such as \"curse\" can take two arguments, e.g. \"Sally cursed John\". A predicate that can only take a single argument is called \"intransitive\", while a predicate that can take two arguments is called \"transitive\".\n\nMany other word classes exist in different languages, such as conjunctions like \"and\" that serve to join two sentences, articles that introduce a noun, interjections such as \"wow!\", or ideophones like \"splash\" that mimic the sound of some event. Some languages have positionals that describe the spatial position of an event or entity. Many languages have classifiers that identify countable nouns as belonging to a particular type or having a particular shape. For instance, in Japanese, the general noun classifier for humans is \"nin\" (人), and it is used for counting humans, whatever they are called:\n\nFor trees, it would be:\n\nIn linguistics, the study of the internal structure of complex words and the processes by which words are formed is called morphology. In most languages, it is possible to construct complex words that are built of several morphemes. For instance, the English word \"unexpected\" can be analyzed as being composed of the three morphemes \"un-\", \"expect\" and \"-ed\".\n\nMorphemes can be classified according to whether they are independent morphemes, so-called roots, or whether they can only co-occur attached to other morphemes. These bound morphemes or affixes can be classified according to their position in relation to the root: \"prefixes\" precede the root, suffixes follow the root, and infixes are inserted in the middle of a root. Affixes serve to modify or elaborate the meaning of the root. Some languages change the meaning of words by changing the phonological structure of a word, for example, the English word \"run\", which in the past tense is \"ran\". This process is called \"ablaut\". Furthermore, morphology distinguishes between the process of inflection, which modifies or elaborates on a word, and the process of derivation, which creates a new word from an existing one. In English, the verb \"sing\" has the inflectional forms \"singing\" and \"sung\", which are both verbs, and the derivational form \"singer\", which is a noun derived from the verb with the agentive suffix \"-er\".\n\nLanguages differ widely in how much they rely on morphological processes of word formation. In some languages, for example, Chinese, there are no morphological processes, and all grammatical information is encoded syntactically by forming strings of single words. This type of morpho-syntax is often called isolating, or analytic, because there is almost a full correspondence between a single word and a single aspect of meaning. Most languages have words consisting of several morphemes, but they vary in the degree to which morphemes are discrete units. In many languages, notably in most Indo-European languages, single morphemes may have several distinct meanings that cannot be analyzed into smaller segments. For example, in Latin, the word \"bonus\", or \"good\", consists of the root \"bon-\", meaning \"good\", and the suffix -\"us\", which indicates masculine gender, singular number, and nominative case. These languages are called \"fusional languages\", because several meanings may be fused into a single morpheme. The opposite of fusional languages are agglutinative languages which construct words by stringing morphemes together in chains, but with each morpheme as a discrete semantic unit. An example of such a language is Turkish, where for example, the word \"evlerinizden\", or \"from your houses\", consists of the morphemes, \"ev-ler-iniz-den\" with the meanings \"house-plural-your-from\". The languages that rely on morphology to the greatest extent are traditionally called polysynthetic languages. They may express the equivalent of an entire English sentence in a single word. For example, in Persian the single word \"nafahmidamesh\" means \"I didn't understand it\" consisting of morphemes \"na-fahm-id-am-esh\" with the meanings, \"negation.understand.past.I.it\". As another example with more complexity, in the Yupik word \"tuntussuqatarniksatengqiggtuq\", which means \"He had not yet said again that he was going to hunt reindeer\", the word consists of the morphemes \"tuntu-ssur-qatar-ni-ksaite-ngqiggte-uq\" with the meanings, \"reindeer-hunt-future-say-negation-again-third.person.singular.indicative\", and except for the morpheme \"tuntu\" (\"reindeer\") none of the other morphemes can appear in isolation.\n\nMany languages use morphology to cross-reference words within a sentence. This is sometimes called \"agreement\". For example, in many Indo-European languages, adjectives must cross-reference the noun they modify in terms of number, case, and gender, so that the Latin adjective \"bonus\", or \"good\", is inflected to agree with a noun that is masculine gender, singular number, and nominative case. In many polysynthetic languages, verbs cross-reference their subjects and objects. In these types of languages, a single verb may include information that would require an entire sentence in English. For example, in the Basque phrase \"ikusi nauzu\", or \"you saw me\", the past tense auxiliary verb \"n-au-zu\" (similar to English \"do\") agrees with both the subject (you) expressed by the \"n\"- prefix, and with the object (me) expressed by the – \"zu\" suffix. The sentence could be directly transliterated as \"see you-did-me\"\n\nAnother way in which languages convey meaning is through the order of words within a sentence. The grammatical rules for how to produce new sentences from words that are already known is called syntax. The syntactical rules of a language determine why a sentence in English such as \"I love you\" is meaningful, but \"*love you I\" is not. Syntactical rules determine how word order and sentence structure is constrained, and how those constraints contribute to meaning. For example, in English, the two sentences \"the slaves were cursing the master\" and \"the master was cursing the slaves\" mean different things, because the role of the grammatical subject is encoded by the noun being in front of the verb, and the role of object is encoded by the noun appearing after the verb. Conversely, in Latin, both \"Dominus servos vituperabat\" and \"Servos vituperabat dominus\" mean \"the master was reprimanding the slaves\", because \"servos\", or \"slaves\", is in the accusative case, showing that they are the grammatical object of the sentence, and \"dominus\", or \"master\", is in the nominative case, showing that he is the subject.\n\nLatin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word order in questions, which exists in many languages. This rule explains why when in English, the phrase \"John is talking to Lucy\" is turned into a question, it becomes \"Who is John talking to?\", and not \"John is talking to who?\". The latter example may be used as a way of placing special emphasis on \"who\", thereby slightly altering the meaning of the question. Syntax also includes the rules for how complex sentences are structured by grouping words together in units, called phrases, that can occupy different places in a larger syntactic structure. Sentences can be described as consisting of phrases connected in a tree structure, connecting the phrases to each other at different levels. To the right is a graphic representation of the syntactic analysis of the English sentence \"the cat sat on the mat\". The sentence is analyzed as being constituted by a noun phrase, a verb, and a prepositional phrase; the prepositional phrase is further divided into a preposition and a noun phrase, and the noun phrases consist of an article and a noun.\n\nThe reason sentences can be seen as being composed of phrases is because each phrase would be moved around as a single element if syntactic operations were carried out. For example, \"the cat\" is one phrase, and \"on the mat\" is another, because they would be treated as single units if a decision was made to emphasize the location by moving forward the prepositional phrase: \"[And] on the mat, the cat sat\". There are many different formalist and functionalist frameworks that propose theories for describing syntactic structures, based on different assumptions about what language is and how it should be described. Each of them would analyze a sentence such as this in a different manner.\n\nLanguages can be classified in relation to their grammatical types. Languages that belong to different families nonetheless often have features in common, and these shared features tend to correlate. For example, languages can be classified on the basis of their basic word order, the relative order of the verb, and its constituents in a normal indicative sentence. In English, the basic order is SVO: \"The snake(S) bit(V) the man(O)\", whereas for example, the corresponding sentence in the Australian language Gamilaraay would be \"d̪uyugu n̪ama d̪ayn yiːy\" (snake man bit), SOV. Word order type is relevant as a typological parameter, because basic word order type corresponds with other syntactic parameters, such as the relative order of nouns and adjectives, or of the use of prepositions or postpositions. Such correlations are called implicational universals. For example, most (but not all) languages that are of the SOV type have postpositions rather than prepositions, and have adjectives before nouns.\n\nAll languages structure sentences into Subject, Verb, and Object, but languages differ in the way they classify the relations between actors and actions. English uses the nominative-accusative word typology: in English transitive clauses, the subjects of both intransitive sentences (\"I run\") and transitive sentences (\"I love you\") are treated in the same way, shown here by the nominative pronoun \"I\". Some languages, called ergative, Gamilaraay among them, distinguish instead between Agents and Patients. In ergative languages, the single participant in an intransitive sentence, such as \"I run\", is treated the same as the patient in a transitive sentence, giving the equivalent of \"me run\". Only in transitive sentences would the equivalent of the pronoun \"I\" be used. In this way the semantic roles can map onto the grammatical relations in different ways, grouping an intransitive subject either with Agents (accusative type) or Patients (ergative type) or even making each of the three roles differently, which is called the tripartite type.\n\nThe shared features of languages which belong to the same typological class type may have arisen completely independently. Their co-occurrence might be due to universal laws governing the structure of natural languages, \"language universals\", or they might be the result of languages evolving convergent solutions to the recurring communicative problems that humans use language to solve.\n\nWhile humans have the ability to learn any language, they only do so if they grow up in an environment in which language exists and is used by others. Language is therefore dependent on communities of speakers in which children learn language from their elders and peers and themselves transmit language to their own children. Languages are used by those who speak them to communicate and to solve a plethora of social tasks. Many aspects of language use can be seen to be adapted specifically to these purposes. Due to the way in which language is transmitted between generations and within communities, language perpetually changes, diversifying into new languages or converging due to language contact. The process is similar to the process of evolution, where the process of descent with modification leads to the formation of a phylogenetic tree.\n\nHowever, languages differ from biological organisms in that they readily incorporate elements from other languages through the process of diffusion, as speakers of different languages come into contact. Humans also frequently speak more than one language, acquiring their first language or languages as children, or learning new languages as they grow up. Because of the increased language contact in the globalizing world, many small languages are becoming endangered as their speakers shift to other languages that afford the possibility to participate in larger and more influential speech communities.\n\nThe semantic study of meaning assumes that meaning is in a relation between signs and meanings that are firmly established through social convention. However, semantics does not study the way in which social conventions are made and affect language. Rather, when studying the way in which words and signs are used, it is often the case that words have different meanings, depending on the social context of use. An important example of this is the process called deixis, which describes the way in which certain words refer to entities through their relation between a specific point in time and space when the word is uttered. Such words are, for example, the word, \"I\" (which designates the person speaking), \"now\" (which designates the moment of speaking), and \"here\" (which designates the position of speaking). Signs also change their meanings over time, as the conventions governing their usage gradually change. The study of how the meaning of linguistic expressions changes depending on context is called pragmatics. Deixis is an important part of the way that we use language to point out entities in the world. Pragmatics is concerned with the ways in which language use is patterned and how these patterns contribute to meaning. For example, in all languages, linguistic expressions can be used not just to transmit information, but to perform actions. Certain actions are made only through language, but nonetheless have tangible effects, e.g. the act of \"naming\", which creates a new name for some entity, or the act of \"pronouncing someone man and wife\", which creates a social contract of marriage. These types of acts are called speech acts, although they can also be carried out through writing or hand signing.\n\nThe form of linguistic expression often does not correspond to the meaning that it actually has in a social context. For example, if at a dinner table a person asks, \"Can you reach the salt?\", that is, in fact, not a question about the length of the arms of the one being addressed, but a request to pass the salt across the table. This meaning is implied by the context in which it is spoken; these kinds of effects of meaning are called conversational implicatures. These social rules for which ways of using language are considered appropriate in certain situations and how utterances are to be understood in relation to their context vary between communities, and learning them is a large part of acquiring communicative competence in a language.\n\nAll healthy, normally developing human beings learn to use language. Children acquire the language or languages used around them: whichever languages they receive sufficient exposure to during childhood. The development is essentially the same for children acquiring sign or oral languages. This learning process is referred to as first-language acquisition, since unlike many other kinds of learning, it requires no direct teaching or specialized study. In \"The Descent of Man\", naturalist Charles Darwin called this process \"an instinctive tendency to acquire an art\".\n\nFirst language acquisition proceeds in a fairly regular sequence, though there is a wide degree of variation in the timing of particular stages among normally developing infants. From birth, newborns respond more readily to human speech than to other sounds. Around one month of age, babies appear to be able to distinguish between different speech sounds. Around six months of age, a child will begin babbling, producing the speech sounds or handshapes of the languages used around them. Words appear around the age of 12 to 18 months; the average vocabulary of an eighteen-month-old child is around 50 words. A child's first utterances are holophrases (literally \"whole-sentences\"), utterances that use just one word to communicate some idea. Several months after a child begins producing words, he or she will produce two-word utterances, and within a few more months will begin to produce telegraphic speech, or short sentences that are less grammatically complex than adult speech, but that do show regular syntactic structure. From roughly the age of three to five years, a child's ability to speak or sign is refined to the point that it resembles adult language. Studies published in 2013 have indicated that unborn fetuses are capable of language acquisition to some degree.\n\nAcquisition of second and additional languages can come at any age, through exposure in daily life or courses. Children learning a second language are more likely to achieve native-like fluency than adults, but in general, it is very rare for someone speaking a second language to pass completely for a native speaker. An important difference between first language acquisition and additional language acquisition is that the process of additional language acquisition is influenced by languages that the learner already knows.\n\nLanguages, understood as the particular set of speech norms of a particular community, are also a part of the larger culture of the community that speaks them. Languages differ not only in pronunciation, vocabulary, and grammar, but also through having different \"cultures of speaking.\" Humans use language as a way of signalling identity with one cultural group as well as difference from others. Even among speakers of one language, several different ways of using the language exist, and each is used to signal affiliation with particular subgroups within a larger culture. Linguists and anthropologists, particularly sociolinguists, ethnolinguists, and linguistic anthropologists have specialized in studying how ways of speaking vary between speech communities.\n\nLinguists use the term \"varieties\" to refer to the different ways of speaking a language. This term includes geographically or socioculturally defined dialects as well as the jargons or styles of subcultures. Linguistic anthropologists and sociologists of language define communicative style as the ways that language is used and understood within a particular culture.\n\nBecause norms for language use are shared by members of a specific group, communicative style also becomes a way of displaying and constructing group identity. Linguistic differences may become salient markers of divisions between social groups, for example, speaking a language with a particular accent may imply membership of an ethnic minority or social class, one's area of origin, or status as a second language speaker. These kinds of differences are not part of the linguistic system, but are an important part of how people use language as a social tool for constructing groups.\n\nHowever, many languages also have grammatical conventions that signal the social position of the speaker in relation to others through the use of registers that are related to social hierarchies or divisions. In many languages, there are stylistic or even grammatical differences between the ways men and women speak, between age groups, or between social classes, just as some languages employ different words depending on who is listening. For example, in the Australian language Dyirbal, a married man must use a special set of words to refer to everyday items when speaking in the presence of his mother-in-law. Some cultures, for example, have elaborate systems of \"social deixis\", or systems of signalling social distance through linguistic means. In English, social deixis is shown mostly through distinguishing between addressing some people by first name and others by surname, and in titles such as \"Mrs.\", \"boy\", \"Doctor\", or \"Your Honor\", but in other languages, such systems may be highly complex and codified in the entire grammar and vocabulary of the language. For instance, in languages of east Asia such as Thai, Burmese, and Javanese, different words are used according to whether a speaker is addressing someone of higher or lower rank than oneself in a ranking system with animals and children ranking the lowest and gods and members of royalty as the highest.\n\nThroughout history a number of different ways of representing language in graphic media have been invented. These are called writing systems.\n\nThe use of writing has made language even more useful to humans. It makes it possible to store large amounts of information outside of the human body and retrieve it again, and it allows communication across distances that would otherwise be impossible. Many languages conventionally employ different genres, styles, and registers in written and spoken language, and in some communities, writing traditionally takes place in an entirely different language than the one spoken. There is some evidence that the use of writing also has effects on the cognitive development of humans, perhaps because acquiring literacy generally requires explicit and formal education.\n\nThe invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age in the late 4th millennium BC. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered to be the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 3400–3200 BC with the earliest coherent texts from about 2600 BC. It is generally agreed that Sumerian writing was an independent invention; however, it is debated whether Egyptian writing was developed completely independently of Sumerian, or was a case of cultural diffusion. A similar debate exists for the Chinese script, which developed around 1200 BC. The pre-Columbian Mesoamerican writing systems (including among others Olmec and Maya scripts) are generally believed to have had independent origins.\n\nAll languages change as speakers adopt or invent new ways of speaking and pass them on to other members of their speech community. Language change happens at all levels from the phonological level to the levels of vocabulary, morphology, syntax, and discourse. Even though language change is often initially evaluated negatively by speakers of the language who often consider changes to be \"decay\" or a sign of slipping norms of language usage, it is natural and inevitable.\n\nChanges may affect specific sounds or the entire phonological system. Sound change can consist of the replacement of one speech sound or phonetic feature by another, the complete loss of the affected sound, or even the introduction of a new sound in a place where there had been none. Sound changes can be \"conditioned\" in which case a sound is changed only if it occurs in the vicinity of certain other sounds. Sound change is usually assumed to be \"regular\", which means that it is expected to apply mechanically whenever its structural conditions are met, irrespective of any non-phonological factors. On the other hand, sound changes can sometimes be \"sporadic\", affecting only one particular word or a few words, without any seeming regularity. Sometimes a simple change triggers a chain shift in which the entire phonological system is affected. This happened in the Germanic languages when the sound change known as Grimm's law affected all the stop consonants in the system. The original consonant * became /b/ in the Germanic languages, the previous * in turn became /p/, and the previous * became /f/. The same process applied to all stop consonants and explains why Italic languages such as Latin have \"p\" in words like pater\" and pisces\", whereas Germanic languages, like English, have father\" and fish\".\n\nAnother example is the Great Vowel Shift in English, which is the reason that the spelling of English vowels do not correspond well to their current pronunciation. This is because the vowel shift brought the already established orthography out of synchronization with pronunciation. Another source of sound change is the erosion of words as pronunciation gradually becomes increasingly indistinct and shortens words, leaving out syllables or sounds. This kind of change caused Latin \"mea domina\" to eventually become the French \"madame\" and American English \"ma'am\".\n\nChange also happens in the grammar of languages as discourse patterns such as idioms or particular constructions become grammaticalized. This frequently happens when words or morphemes erode and the grammatical system is unconsciously rearranged to compensate for the lost element. For example, in some varieties of Caribbean Spanish the final /s/ has eroded away. Since Standard Spanish uses final /s/ in the morpheme marking the second person subject \"you\" in verbs, the Caribbean varieties now have to express the second person using the pronoun \"tú\". This means that the sentence \"what's your name\" is \"¿como te llamas?\" in Standard Spanish, but in Caribbean Spanish. The simple sound change has affected both morphology and syntax. Another common cause of grammatical change is the gradual petrification of idioms into new grammatical forms, for example, the way the English \"going to\" construction lost its aspect of movement and in some varieties of English has almost become a full-fledged future tense (e.g. \"I'm gonna\").\n\nLanguage change may be motivated by \"language internal\" factors, such as changes in pronunciation motivated by certain sounds being difficult to distinguish aurally or to produce, or through patterns of change that cause some rare types of constructions to drift towards more common types. Other causes of language change are social, such as when certain pronunciations become emblematic of membership in certain groups, such as social classes, or with ideologies, and therefore are adopted by those who wish to identify with those groups or ideas. In this way, issues of identity and politics can have profound effects on language structure.\n\nOne important source of language change is contact and resulting diffusion of linguistic traits between languages. Language contact occurs when speakers of two or more languages or varieties interact on a regular basis. Multilingualism is likely to have been the norm throughout human history and most people in the modern world are multilingual. Before the rise of the concept of the ethno-national state, monolingualism was characteristic mainly of populations inhabiting small islands. But with the ideology that made one people, one state, and one language the most desirable political arrangement, monolingualism started to spread throughout the world. Nonetheless, there are only 250 countries in the world corresponding to some 6000 languages, which means that most countries are multilingual and most languages therefore exist in close contact with other languages.\n\nWhen speakers of different languages interact closely, it is typical for their languages to influence each other. Through sustained language contact over long periods, linguistic traits diffuse between languages, and languages belonging to different families may converge to become more similar. In areas where many languages are in close contact, this may lead to the formation of language areas in which unrelated languages share a number of linguistic features. A number of such language areas have been documented, among them, the Balkan language area, the Mesoamerican language area, and the Ethiopian language area. Also, larger areas such as South Asia, Europe, and Southeast Asia have sometimes been considered language areas, because of widespread diffusion of specific areal features.\n\nLanguage contact may also lead to a variety of other linguistic phenomena, including language convergence, borrowing, and relexification (replacement of much of the native vocabulary with that of another language). In situations of extreme and sustained language contact, it may lead to the formation of new mixed languages that cannot be considered to belong to a single language family. One type of mixed language called pidgins occurs when adult speakers of two different languages interact on a regular basis, but in a situation where neither group learns to speak the language of the other group fluently. In such a case, they will often construct a communication form that has traits of both languages, but which has a simplified grammatical and phonological structure. The language comes to contain mostly the grammatical and phonological categories that exist in both languages. Pidgin languages are defined by not having any native speakers, but only being spoken by people who have another language as their first language. But if a Pidgin language becomes the main language of a speech community, then eventually children will grow up learning the pidgin as their first language. As the generation of child learners grow up, the pidgin will often be seen to change its structure and acquire a greater degree of complexity. This type of language is generally called a creole language. An example of such mixed languages is Tok Pisin, the official language of Papua New-Guinea, which originally arose as a Pidgin based on English and Austronesian languages; others are Kreyòl ayisyen, the French-based creole language spoken in Haiti, and Michif, a mixed language of Canada, based on the Native American language Cree and French.\n\n\"SIL Ethnologue\" defines a \"living language\" as \"one that has at least one speaker for whom it is their first language\". The exact number of known living languages varies from 6,000 to 7,000, depending on the precision of one's definition of \"language\", and in particular, on how one defines the distinction between languages and dialects. As of 2016, \"Ethnologue\" cataloged 7,097 living human languages. The \"Ethnologue\" establishes linguistic groups based on studies of mutual intelligibility, and therefore often includes more categories than more conservative classifications. For example, the Danish language that most scholars consider a single language with several dialects is classified as two distinct languages (Danish and Jutish) by the \"Ethnologue\".\n\nAccording to the \"Ethnologue\", 389 languages (nearly 6%) have more than a million speakers. These languages together account for 94% of the world's population, whereas 94% of the world's languages account for the remaining 6% of the global population. To the right is a table of the world's 10 most spoken languages with population estimates from the \"Ethnologue\" (2009 figures).\n\nThere is no clear distinction between a language and a dialect, notwithstanding a famous aphorism attributed to linguist Max Weinreich that \"a language is a dialect with an army and navy\". For example, national boundaries frequently override linguistic difference in determining whether two linguistic varieties are languages or dialects. Hakka, Cantonese and Mandarin are, for example, often classified as \"dialects\" of Chinese, even though they are more different from each other than Swedish is from Norwegian. Before the Yugoslav civil war, Serbo-Croatian was considered a single language with two dialects, but now Croatian and Serbian are considered different languages and employ different writing systems. In other words, the distinction may hinge on political considerations as much as on cultural differences, distinctive writing systems, or degree of mutual intelligibility.\n\nThe world's languages can be grouped into language families consisting of languages that can be shown to have common ancestry. Linguists recognize many hundreds of language families, although some of them can possibly be grouped into larger units as more evidence becomes available and in-depth studies are carried out. At present, there are also dozens of language isolates: languages that cannot be shown to be related to any other languages in the world. Among them are Basque, spoken in Europe, Zuni of New Mexico, Purépecha of Mexico, Ainu of Japan, Burushaski of Pakistan, and many others.\n\nThe language family of the world that has the most speakers is the Indo-European languages, spoken by 46% of the world's population. This family includes major world languages like English, Spanish, Russian, and Hindustani (Hindi/Urdu). The Indo-European family achieved prevalence first during the Eurasian Migration Period (c. 400–800 AD), and subsequently through the European colonial expansion, which brought the Indo-European languages to a politically and often numerically dominant position in the Americas and much of Africa. The Sino-Tibetan languages are spoken by 20% of the world's population and include many of the languages of East Asia, including Hakka, Mandarin Chinese, Cantonese, and hundreds of smaller languages.\n\nAfrica is home to a large number of language families, the largest of which is the Niger-Congo language family, which includes such languages as Swahili, Shona, and Yoruba. Speakers of the Niger-Congo languages account for 6.9% of the world's population. A similar number of people speak the Afroasiatic languages, which include the populous Semitic languages such as Arabic, Hebrew language, and the languages of the Sahara region, such as the Berber languages and Hausa.\n\nThe Austronesian languages are spoken by 5.5% of the world's population and stretch from Madagascar to maritime Southeast Asia all the way to Oceania. It includes such languages as Malagasy, Māori, Samoan, and many of the indigenous languages of Indonesia and Taiwan. The Austronesian languages are considered to have originated in Taiwan around 3000 BC and spread through the Oceanic region through island-hopping, based on an advanced nautical technology. Other populous language families are the Dravidian languages of South Asia (among them Kannada Tamil and Telugu), the Turkic languages of Central Asia (such as Turkish), the Austroasiatic (among them Khmer), and Tai–Kadai languages of Southeast Asia (including Thai).\n\nThe areas of the world in which there is the greatest linguistic diversity, such as the Americas, Papua New Guinea, West Africa, and South-Asia, contain hundreds of small language families. These areas together account for the majority of the world's languages, though not the majority of speakers. In the Americas, some of the largest language families include the Quechumaran, Arawak, and Tupi-Guarani families of South America, the Uto-Aztecan, Oto-Manguean, and Mayan of Mesoamerica, and the Na-Dene, Iroquoian, and Algonquian language families of North America. In Australia, most indigenous languages belong to the Pama-Nyungan family, whereas New Guinea is home to a large number of small families and isolates, as well as a number of Austronesian languages.\n\nLanguage endangerment occurs when a language is at risk of falling out of use as its speakers die out or shift to speaking another language. Language loss occurs when the language has no more native speakers, and becomes a \"dead language\". If eventually no one speaks the language at all, it becomes an \"extinct language\". While languages have always gone extinct throughout human history, they have been disappearing at an accelerated rate in the 20th and 21st centuries due to the processes of globalization and neo-colonialism, where the economically powerful languages dominate other languages.\n\nThe more commonly spoken languages dominate the less commonly spoken languages, so the less commonly spoken languages eventually disappear from populations. The total number of languages in the world is not known. Estimates vary depending on many factors. The consensus is that there are between 6,000 and 7,000 languages spoken as of 2010, and that between 50–90% of those will have become extinct by the year 2100. The top 20 languages, those spoken by more than 50 million speakers each, are spoken by 50% of the world's population, whereas many of the other languages are spoken by small communities, most of them with less than 10,000 speakers.\n\nThe United Nations Educational, Scientific and Cultural Organization (UNESCO) operates with five levels of language endangerment: \"safe\", \"vulnerable\" (not spoken by children outside the home), \"definitely endangered\" (not spoken by children), \"severely endangered\" (only spoken by the oldest generations), and \"critically endangered\" (spoken by few members of the oldest generation, often semi-speakers). Notwithstanding claims that the world would be better off if most adopted a single common \"lingua franca\", such as English or Esperanto, there is a consensus that the loss of languages harms the cultural diversity of the world. It is a common belief, going back to the biblical narrative of the tower of Babel in the Old Testament, that linguistic diversity causes political conflict, but this is contradicted by the fact that many of the world's major episodes of violence have taken place in situations with low linguistic diversity, such as the Yugoslav and American Civil War, or the genocide of Rwanda, whereas many of the most stable political units have been highly multilingual.\n\nMany projects aim to prevent or slow this loss by revitalizing endangered languages and promoting education and literacy in minority languages. Across the world, many countries have enacted specific legislation to protect and stabilize the language of indigenous speech communities. A minority of linguists have argued that language loss is a natural process that should not be counteracted, and that documenting endangered languages for posterity is sufficient.\n\n"}
{"id": "14191346", "url": "https://en.wikipedia.org/wiki?curid=14191346", "title": "Language and gender", "text": "Language and gender\n\nResearch into the many possible relationships, intersections and tensions between language and gender is diverse. It crosses disciplinary boundaries, and, as a bare minimum, could be said to encompass work notionally housed within applied linguistics, linguistic anthropology, conversation analysis, cultural studies, feminist media studies, feminist psychology, gender studies, interactional sociolinguistics, linguistics, mediated stylistics, sociolinguistics and media studies. In methodological terms, there is no single approach that could be said to 'hold the field'. Discursive, poststructural, ethnomethodological, ethnographic, phenomenological, positivist and experimental approaches can all be seen in action during the study of language and gender, producing and reproducing what Susan Speer has described as 'different, and often competing, theoretical and political assumptions about the way discourse, ideology and gender identity should be conceived and understood'. As a result, research in this area can perhaps most usefully be divided into three main areas of study: first, there is a broad and sustained interest in the varieties of speech associated with a particular gender; second, there is a related interested in the social norms and conventions that (re)produce gendered language use (a variety of speech (or sociolect) associated with a particular gender is sometimes called a genderlect); and third, there are studies that focus on the contextually specific and locally situated ways in which gender is constructed and operationalized. The study of gender and language in sociolinguistics and gender studies is often said to have begun with Robin Lakoff's 1975 book, \"Language and Woman's Place\", as well as some earlier studies by Lakoff. The study of language and gender has developed greatly since the 1970s. Prominent scholars include Deborah Tannen, Penelope Eckert, Janet Holmes, Mary Bucholtz, Kira Hall, Deborah Cameron, and others. The 1995 edited volume \"Gender Articulated: Language and the Socially Constructed Self\" is often referred to as a central text on language and gender.\n\nIn 1975 Robin Lakoff identified a \"women's register\", which she argued served to maintain women's (inferior) role in society. Lakoff argued that women tend to use linguistic forms that reflect and reinforce a subordinate role. These include tag questions, question intonation, and \"weak\" directives, among others (see also Speech practices associated with gender, below).\n\nStudies such as Lakoff's have been labeled the \"deficit approach\", since they posit that one gender is deficient in terms of the other. Descriptions of women's speech as deficient can actually be dated as far back as Otto Jespersen's \"The Woman\", a chapter in his 1922 book \"Language: Its Nature and Development, and Origin.\" Jespersen's idea that women's speech is deficient relative to a male norm went largely unchallenged until Lakoff's work appeared fifty years later. Nevertheless, despite the political incorrectness of the chapter's language from a modern perspective, Jespersen's contributions remain relevant. These include the prospect of language change based on social and gendered opportunity, lexical and phonological differences, and the idea of genderlects and gender roles influence language.\n\nNot long after the publication of \"Language and Woman's Place\", other scholars began to produce studies that both challenged Lakoff's arguments and expanded the field of language and gender studies. One refinement of the deficit argument is the so-called \"dominance approach\", which posits that gender differences in language reflect power differences in society.\n\nJennifer Coates outlines the historical range of approaches to gendered speech in her book \"Women, Men and Language\". She contrasts the four approaches known as the deficit, dominance, difference, and dynamic approaches.\n\n\"Deficit\" is an approach attributed to Jespersen that defines adult male language as the standard, and women's language as deficient. This approach created a dichotomy between women's language and men's language. This triggered criticism to the approach in that highlighting issues in women's language by using men's as a benchmark. As such, women's language was considered to have something inherently 'wrong' with it.\n\n\"Dominance\" is an approach whereby the female sex is seen as the subordinate group whose difference in style of speech results from male supremacy and also possibly an effect of patriarchy. This results in a primarily male-centered language. Scholars such as Dale Spender and Don Zimmerman and Candace West subscribe to this view.\n\n\"Difference\" is an approach of equality, differentiating men and women as belonging to different 'sub-cultures' as they have been socialised to do so since childhood. This then results in the varying communicative styles of men and women. Deborah Tannen is a major advocate of this position. Tannen compares gender differences in language to cultural differences. Comparing conversational goals, she argues that men tend to use a \"report style\", aiming to communicate factual information, whereas women more often use a \"rapport style\", which is more concerned with building and maintaining relationships.\n\nThe \"dynamic\" or \"social constructionist\" approach is, as Coates describes, the most current approach to language and gender. Instead of speech falling into a natural gendered category, the dynamic nature and multiple factors of an interaction help a socially appropriate gendered construct. As such, West and Zimmerman describe these constructs as \"doing gender\" instead of the speech itself necessarily being classified in a particular category. This is to say that these social constructs, while affiliated with particular genders, can be utilized by speakers as they see fit.\n\nScholars including Tannen and others argue that differences are pervasive across media, including face-to-face conversation, written essays of primary school children, email, and even toilet graffiti.\n\nDeborah Cameron, among other scholars, argues that there are problems with both the dominance and the difference approach. Cameron notes that throughout the history of scholarship on language and gender male-associated forms have been seen as the unmarked norm from which the female deviates. For example, the norm 'manager' becomes the marked form 'manageress' when referring to a female counterpart. On the other hand, Cameron argues that what the difference approach labels as different ways of using or understanding language are actually displays of differential power. Cameron suggests, \"It is comforting to be told that nobody needs to 'feel awful': that there are no real conflicts, only misunderstandings. ... But the research evidence does not support the claims made by Tannen and others about the nature, the causes, and the prevalence of male-female miscommunication.\" She argues that social differences between men's and women's roles are not clearly reflected in language use. One additional example is a study she has done on call center operators in the UK, where these operators are trained to be scripted in what they say and to perform the necessary 'emotional labor' (smiling, expressive intonation, showing rapport/empathy and giving minimal responses) for their customer-callers. This emotional labor is commonly associated with the feminine domain, and the call center service workers are also typically females. However, the male workers in this call center do not orient to the covertly gendered meanings when they are tasked to perform this emotional labor. While this does not mean that the 'woman's language' is revalued, nor does this necessarily call for a feminist celebration, Cameron highlights that it is possible that with time, more men may work in this service industry, and this may lead to a subsequent \"de-gendering\" of this linguistic style.\n\nCommunication styles are always a product of context, and as such, gender differences tend to be most pronounced in single-gender groups. One explanation for this, is that people accommodate their language towards the style of the person they are interacting with. Thus, in a mixed-gender group, gender differences tend to be less pronounced. A similarly important observation is that this accommodation is usually towards the language style, not the gender of the person. That is, a polite and empathic male will tend to be accommodated to on the basis of their being polite and empathic, rather than their being male.\n\nHowever, Ochs argues that gender can be indexed directly and indirectly. Direct indexicality is the primary relationship between linguistics resources (such as lexicon, morphology, syntax, phonology, dialect and language) and gender. For example, the pronouns \"he\" and \"she\" directly indexes \"male\" and \"female\". However, there can be a secondary relationship between linguistic resources and gender where the linguistic resources can index certain acts, activities or stances which then indirectly index gender. In other words, these linguistic resources help constitute gender. Examples include the Japanese particles \"wa\" and \"ze\". The former directly index delicate intensity, which then indirectly indexes the female \"voice\" while the latter directly indexes coarse intensity, which then indirectly indexes the male \"voice\".\n\nWomen are generally believed to speak a better \"language\" than men do. This is a constant misconception, but scholars believe that no gender speaks a better language, but that each gender instead speaks its own unique language. This notion has sparked further research into the study of the differences between the way men and women communicate.\n\nA specific area of study within the field of language and gender is the way in which it affects \"children's television\". Mulac et al.'s \"Male/Female Language Differences and Attributional Consequences in Children's Television\" focuses on identifying differing speech patterns of male versus female characters in popular children's television programs at the time (the 1980s). The data gathered by Mulac et al. comes from a two-week period in 1982 from three Public Broadcasting Service daytime programs and three categories from commercial network programs (action, comedy/adventure, and commercials) that aired on Saturdays. They analyzed randomly selected interactive dialogue taken once from every ten minutes of their tapes. Mulac et al. collected data for 37 language variables, from which they determined the thirteen that showed significant differences between usage by male and female characters. Mulac et al.'s definitions of these thirteen features are as follows:\nThe following tended to be higher in frequency for males: vocalized pauses, action verbs, present tense verbs, justifiers, subordinating conjunctions, and grammatical \"errors\". On the other hand, the following were found to occur more for females: total verbs, uncertainty verbs, adverbials beginning sentences, judgmental adjectives, concrete nouns, and polite forms. In addition, female characters had longer sentences on average.\n\nAnother facet of Mulac et al.'s research was to gather participants' subjective ratings on characters' socio-intellectual status (high/low social status, white/blue collar, literate/illiterate, rich/poor), dynamism (aggressive/unaggressive, strong/weak, loud/soft, active/passive), and aesthetic quality (pleasing/displeasing, sweet/sour, nice/awful, beautiful/ugly), based on the transcripts from the shows' dialogue.\n\nAubrey's 2004 study \"The Gender-Role Content of Children's Favorite Television Programs and Its Links to Their Gender-Related Perceptions\" identifies gender stereotypes in children’s television programs and evaluates the effects of these stereotypes on children’s personal gender-role values and interpersonal attraction. Aubrey chose shows for the study based on children’s responses when asked to name their favorite television program (the top-named show was Rugrats, followed by Doug). Some of the stereotypes found in the study pertain to language/communication, but most are stereotypes or attributes of the characters such as assertiveness, aggression, emotionality, and cattiness. In regards to language, the study found that male characters were more likely to ask questions, assert opinions, and direct others than female characters. Female characters, on the other hand, were more likely to \"receive or make comments about body or beauty\" than their male counterparts.\n\nIn general, Aubrey found less stereotypical content for female characters than for male, which they recognize to be a possible effect of either the higher presence of male characters or the difficulty of measuring passivity.\n\nNot all members of a particular sex may follow the specific gender roles that are prescribed by society. The patterns in gender and communication that follow are only the norms for each gender, and not every member of the corresponding sex may fit into those patterns.\n\nOne of the ways in which the communicative behaviors of men and women differ is in their use of minimal responses, i.e., paralinguistic features such as 'mm' and 'yeah', which is behaviour associated with collaborative language use. Men generally use them less frequently than women, and when they do, it is usually to show agreement, as Don Zimmerman and Candace West's study of turn-taking in conversation indicates.\n\nWhile the above can be true in some contexts and situations, studies that dichotomize the communicative behavior of men and women may run the risk of over-generalization. For example, \"minimal responses appearing \"throughout streams of talk\", such as \"mm\" or \"yeah\", may only function to display active listening and interest and are not always signs of \"support work\", as Fishman claims. They can—as more detailed analysis of minimal responses show—signal understanding, demonstrate agreement, indicate scepticism or a critical attitude, demand clarification or show surprise. In other words, both male and female participants in a conversation can employ these minimal responses for interactive functions, rather than gender-specific functions.\n\nMen and women differ in their use of questions in conversations. For men, a question is usually a genuine request for information whereas with women it can often be a rhetorical means of engaging the other's conversational contribution or of acquiring attention from others conversationally involved, techniques associated with a collaborative approach to language use. Therefore, women use questions more frequently. However, a study carried out by Alice Freed and Alice Greenwood in 1996 showed that there was no significant difference in the use of questions between genders. In writing, however, both genders use rhetorical questions as literary devices. For example, Mark Twain used them in \"The War Prayer\" to provoke the reader to question his actions and beliefs. Tag questions are frequently used to verify or confirm information, though in women's language they may also be used to avoid making strong statements.\n\nAs the work of Victoria DeFrancisco shows, female linguistic behaviour characteristically encompasses a desire to take turns in conversation with others, which is opposed to men's tendency towards centering on their own point or remaining silent when presented with such implicit offers of conversational turn-taking as are provided by hedges such as \"y' know\" and \"isn't it\". This desire for turn-taking gives rise to complex forms of interaction in relation to the more regimented form of turn-taking commonly exhibited by men.\n\nAccording to Bruce Dorval in his study of same-sex friend interaction, males tend to change subject more frequently than females. This difference may well be at the root of the conception that women chatter and talk too much. Goodwin observes that girls and women link their utterances to previous speakers and develop each other's topics, rather than introducing new topics.\n\nHowever, a study of young American couples and their interactions reveal that while women raise twice as many topics as men, it is the men's topics that are usually taken up and subsequently elaborated in the conversation.\n\nFemale tendencies toward self-disclosure, i.e., sharing their problems and experiences with others, often to offer sympathy, contrasts with male tendencies to non-self disclosure and professing advice or offering a solution when confronted with another's problems.\n\nSelf-disclosure is not simply providing information to another person. Instead, scholars define self-disclosure as sharing information with others that they would not normally know or discover. Self-disclosure involves risk and vulnerability on the part of the person sharing the information. When it comes to genderlect, self-disclosure is important because genderlect is defined as the differences in male and female communication. Men and women have completely different views of self-disclosure. Developing a close relationship with another person requires a certain level of intimacy, or self-disclosure. It typically is much easier to get to know a woman than it is to get to know a man. It has been proven that women get to know someone on a more personal level and they are more likely to desire to share their feelings.\n\nIt has also been said that people share more via technology. The phenomenon is known as Computer Mediated Communication, also known as CMC. This form of communication typically involves text only messages that tend to lose their nonverbal cues. Men and women are both more likely to self-disclose on the computer than they would be face to face. People are more confident when using Computer Mediated Communication because communication is faceless, which makes it easier to divulge information.\n\nResearch has been conducted to examine whether self-disclosure in adult friendship differs according to gender and marital status. Sixty-seven women and fifty-three men were asked about intimate and non-intimate self-disclosure to closest same-sex friends. Disclosure to spouse among married respondents was also assessed. The intimate disclosure of married men to friends was lower than that of unmarried men, married women and unmarried women; the intimate disclosure of these last three groups was similar. Married people's non-intimate disclosure to friends was lower than that of unmarried people, regardless of gender. Married people's intimate disclosure to their spouses was high regardless of gender; in comparison, married men's intimate disclosure to their friends was low, while married women's disclosure to their friends was moderate or even as high as disclosure to their spouses. The results suggest that sex roles are not the only determinant of gender differences in disclosure to friends. Marital status appears to have an important influence on disclosure in friendship for men but not for women. It was concluded that research on gender differences in self-disclosure and friendship has neglected an important variable, that of marital status.\" This research goes to show that when a man is married he is less likely to have intimate self-disclosure. This could be because a man may feel he is betraying his wife's confidence by disclosing information that might be considered private. However, the research also showed that the married women didn't change much in either situation, because women tend to self disclose more than men.\n\nMen tend to communicate differently with other men than they do with other women, while women tend to communicate the same with both men and women. \"Male and female American students who differed in masculinity and in femininity self-disclosed to a same-sex stranger in contexts that made either social/expressive motives or instrumental motives salient. The results were consistent with the primary assertion that measures of sex role identity are better predictors of contextual variations in self-disclosure than is sex per se. Sex consistently failed to predict subjects' willingness to self-disclose, both within and across contexts, whereas femininity promoted self-disclosure in the context that was clearly social and expressive in character. Although masculinity failed to exert the expected facilitative impact on self-disclosure within the instrumental context, it nonetheless influenced the results; androgynous subjects, who scored high in both masculinity and femininity, were more self-revealing across contexts than was any other group.\" This research shows that people have the ability to still self disclose very clearly regardless of masculine or feminine communication traits. Displaying strictly feminine or masculine traits will not be to one's advantage in communication, because it is important to be able to recognize and utilize these traits to be an effective communicator.\n\nFrom a social skills perspective, gender, tunic, and cultural differences in relationships may stem, in part, from differences in communication. The influence of biological sex on communication values has received scholarly attention. In general, women value affectively oriented communication skills more than men, and men value instrumentally oriented communication skills more than women, although the effect size for these differences are generally small.\n\nSelf-disclosure is also very important when it comes to a close dating relationship between men and women. Successful communication in relationships is one of the greatest difficulties most couples are forced to overcome. Men in relationships with women may practice self-disclosure more often than their female partner. Self-disclosure is considered to be a key factor in facilitating intimacy. For example, American heterosexual couples were studied using various measures twice a year. By using the average scores of both partners, they found that self-disclosure was higher in those couples who remained together at the second administration of the surveys than in those who broke up between two administrations. Similarly, researchers asked heterosexual couples who had just begun dating to complete a self-disclosure measure and to answer the same questionnaire four months later. They found that couples who were still dating four months later reported greater self-disclosure at the initial contact than did those who later broke up. This test shows self-disclosure can be beneficial to facilitating a positive relationship. Self-disclosure is a process which typically begins rapidly, but then plateaus as the couple gains more information. The initial self-disclosure is extremely important when first meeting someone. The first interactions between a potential couple could be deciding factors in the success or failure of the relationship.\n\nSelf-disclosure is difficult because not all women and men communicate the same.\n\nAggression can be defined by its three intersecting counterparts: indirect, relational and social. Indirect aggression occurs when the victim is attacked through covert and concealed attempts to cause social suffering. Examples are gossiping, exclusion or ignoring of the victim. Relational aggression, while similar to indirect, is more resolute in its attentions. It can be a threat to terminate a friendship or spreading false rumors. The third type of aggression, social aggression, \"is directed toward damaging another's self-esteem, social status, or both, and may take direct forms such as verbal rejection, negative facial expressions or body movements, or more indirect forms such as slanderous rumors or social exclusion.\" This third type has become more common in adolescent, both male and female, behavior.\n\nDr. M.K. Underwood, leading researcher in child clinical psychology and developmental psychology, began using the term social aggression in several of her experiments. In one study, Underwood followed 250 third-graders and their families in order to understand how anger is communicated in relationships, especially in face-to-face and behind-the-back situations. It was found that technology and electronic communication has become a key factor in social aggression. This discovery has been termed cyber-bullying. In another experiment, social aggression was used to see if verbal and nonverbal behaviors contributed to a person's social value. It was found that those who communicated nonverbal signals were seen as angry and annoyed by their peers. In a third study, the experimenters determined that while socially aggressive students were vastly disliked, they were alleged to be the popular kids and had the highest marked social status. Most research has been based on teacher assessments, case studies and surveys.\n\nFor years, all research on aggression focused primarily on males because it was believed females were non-confrontational. Recently however, people have realized that while \"boys tend to be more overtly and physically aggressive, girls are more indirectly, socially, and relationally aggressive.\" In a study done measuring cartoon character's aggressive acts on television, these statistics were found:\n\n\nPhysical and social aggression emerge at different points in life. Physical aggression occurs in a person's second year and continues till preschool. Toddlers use this aggression to obtain something they want that is otherwise denied or another has. In preschool, children become more socially aggressive and this progresses through adolescence and adulthood. Social aggression is not used to acquire materialistic things but to accomplish social goals.\n\nStarting in first grade, research has shown that young females are more disliked when they are socially aggressive than when young males are physically aggressive. However, until the fourth grade there is an overall negative correlation between aggression and popularity. By the end of fifth grade, aggressive children, both male and female, are more popular than their non-aggressive counterparts. This popularity does not insinuate likeability.\n\nIn the seventh grade, social aggression seems to be at its peak. When eight-, eleven- and fifteen-year-olds were compared, there were high reports of social aggression but no apparent statistical differences between the age groups.\n\nSeveral studies have shown that social aggression and high academic performance are incompatible. In classrooms with a high achievement record, researchers were less likely to find social aggression. Vice versa can be found for classrooms with a low achievement record.\n\nIn adolescence, social aggression boosts female's popularity by maintaining and controlling the social hierarchy. Furthermore, males are also ranked higher in popularity if they are physically aggressive. But, if males practice relational or social aggression then they are seen as unpopular among their peers. When it comes to different forms social aggression, males are more prone to use direct measures and females indirect.\n\nIn addition to gender, the conditions in which a child grows up in also affects the likelihood of aggression. Children raised in a divorced, never married or low-income family are more likely to show social aggression. This is speculated because of the higher rates of conflict and fighting already in the household. Parents who use an aversive style of parenting can also contribute to the social aggression in their children. Researchers venture that \"perhaps children who are treated harshly by parents have a higher baseline level of anger…and may lack the opportunities to practice more direct, assertive strategies for conflict resolution so may be prone to maligning others or lashing out when they are angry.\"\n\nThrough the last couple decades, the media has increased its influence over America's youth. In a study done measuring the aggressive acts committed by cartoon characters on television, out of 8927 minutes of programming time 7856 aggressive acts took place. This is roughly .88 aggressive acts per minute. Because television and cartoons are one of the main mediums for entertainment, these statistics can be troubling. If children relate to the characters, then they are more likely to commit similar acts of aggression. For teenagers, popular films and series such as \"Mean Girls\" (2004), \"Easy A\" (2010) and \"Gossip Girl\" (2007) have shown an exaggerated, damaging view of how society works. Already, latest studies have shown an increase of social aggression in girls. Other experiments, such as one done by Albert Bandura, the Bobo doll experiment, have shown similar results of society shaping your behavior because of the impact of a model.\n\nThe development of social aggression can be explained by the social identity theory and evolutionary perspective.\n\nThe social identity theory categorizes people into two groups, in-groups and out-groups. You see yourself as part of the in-group and people who are dissimilar to you as part of the out-group. In middle and high school these groups are known as cliques and can have several names. In the popular 2004 teen drama \"Mean Girls\", \"varsity jocks\", \"desperate wannabes\", \"over-sexed band geeks\", \"girls who eat their feelings\", \"cool Asians\" and \"the Plastics\" were several cliques from the movie. Two common middle and high school cliques seen in everyday life are the popular crowd, in-group, and everyone else, out-group. The out-group has several other divisions but for the most part the in-group will categorize the out-groups all as one.\n\nAround this time, it becomes important for a females social identity to be associated with the in-group. When a girl possess qualities that are valued in the in-group, then her social identity will increase. However, if her characteristics resemble those of the out-group, then she will be attack the out-group in order to keep her social standing within the in-group. This intergroup struggle, also known as social competition, mostly comes the in-group condemning the out-group, not the other way around.\n\nMoreover, social aggression can lead to intragroup competition. Inside the social groups there is also a hierarchal ranking, there are followers and there are leaders. When one's position in the group does not lead to positive self-identity, then the group members will feud with one another to increase status and power within the clique. Studies show that the closer a female is to her attacker, the less likely she is to forgive.\n\nWhere the social identity theory explains direct social aggression, research done in the evolutionary perspective explains indirect social aggression. This aggression stemmed from \"successful competition for scarce resources… and enables optimal growth and development.\" Two tactics used are the coercive and prosocial strategies. The coercive strategies involve controlling and regulating all resources of the out-group through a monopoly. For this scheme, one must rely heavily on threats and aggression. The other strategy, prosocial, involves helping and sharing resources. This method shows complete dominance for the in-group, because in order for others to survive they must subordinate themselves to receive resources. Ability to control resources effectively results in higher-ranking in the in-group, popular crowd.\n\nSocial aggression can be detrimental for both ends of the spectrum, the out-group and in-group members. Longitudinal studies prove that aggression can lead to victims feeling lonely and socially isolated. In addition, targets report feeling depressed and affected by other health risks such as headaches, sleepiness, abdominal pain and bedwetting. The aggressors on the other hand, were suggested to \"encounter future problems in social relationships or emotional difficulties during early childhood.\" In academics, victims were reported to having below average test scores and low achievement.\n\nStudies that measure cross-gender differences show that females find social aggression to be more hurtful than males do. The results of the hurt and pain felt by female victims can be seen in all ages. Pre-school teachers have reported several cases of female students feeling depressed. In high school, the female victims begin to slowly isolate themselves. A year later, this seclusion has led to social phobia. Furthermore, in college, pressure and aggression from Greek life has lowered life satisfaction and increased antisocial behavior in several female students.\n\nWhile social aggression has several downfalls, it has also led to a mature social competence of males and females. Being part of an in-group can increase a person's self-worth and contribute to his or her personal identity. In terms of the evolutionary perspective, being able to control definite and indefinite resources can increase a person's social competence. Some research argues that reports of social aggression and bullying can teach students in school what is considered unacceptable behavior. In a 1998 survey, 60% of students found that bullying \"makes kids tougher.\" However, there is additional need for support on this claim.\n\nSince 1992, there have been nine school intervention and prevention programs, which have met the rigorous criteria of efficacy, to avert social aggression. The programs include Early Childhood Friendship Project (2009), You Can't Say You Can't Play (1992), I Can Problem Solve (2008), Walk Away, Ignore, Talk, Seek Help (2003), Making Choices: Social Problem Skills for Children (2005), Friend to Friend (2009), Second Step (2002), Social Aggression Prevention Program (2006), Sisters of Nia (2004).\n\nWhen designing a prevention program, it is important to remember to keep the program age and gender appropriate. For example, Early Childhood Friendship Project and You Can't Say You Can't Play have visual activities for the preschoolers and integrate puppet shows into the lesson plan. In addition, because males and females approach aggression differently there must be personalized plans to fit both genders.\n\nHowever, intervention programs even with the best intentions can be harmful. For one, the progress of the intervention can be short lived. Studies have measured the effectiveness of intervention programs three separate times during the course of one year and no improvements were shown. Secondly, because social aggression is said to increase social identity and belonging to a group, many students have tried to disrupt the programs. A third implication is that the interventions need to study how adverse behaviors develop. Otherwise the solution might not fit the problem. Lastly, the programs must be designed to fit the needs of girls and boys and not the ones of the researchers. If the intervention program is designed to give insight for research rather than reducing and bettering aggression, then it can be detrimental to society.\n\nAlthough a few forms of behavior may be sex-specific, in general they reflect patterns of power and control between the sexes, which are found in all human groups, regardless of sex composition. These modes of behaviors are perhaps more appropriately labeled 'powerlects' instead of 'genderlects'.\n\nIn a conversation, meaning does not reside in the words spoken, but it filled in by the person listening. Each person decides if they think others are speaking in the spirit of differing status or symmetrical connection. The likelihood that individuals will tend to interpret someone else's words as one or the other depends more on the hearer's own focus, concerns, and habits than on the spirit in which the words were intended.\n\nIt appears that women attach more weight than men to the importance of listening in conversation, with its connotations of power to the listener as confidant of the speaker. This attachment of import by women to listening is inferred by women's normally lower rate of interruption — i.e., disrupting the flow of conversation with a topic unrelated to the previous one — and by their largely increased use of minimal responses in relation to men. Men, however, interrupt far more frequently with non-related topics, especially in the mixed sex setting and, far from rendering a female speaker's responses minimal, are apt to greet her conversational spotlights with silence, as the work of Victoria DeFrancisco demonstrates.\n\nWhen men talk, women listen and agree. However men tend to misinterpret this agreement, which was intended in a spirit of connection, as a reflection of status and power. A man might conclude that a woman is indecisive or insecure as a result of her listening and attempts of acknowledgment. When in all actuality, a woman's reasons for behaving this way have nothing to do with her attitudes toward her knowledge, but are a result of her attitudes toward her relationships. The act of giving information frames the speaker with a higher status, while the act of listening frames the listener as lower. However, when women listen to men, they are not necessarily thinking in terms of status, but in terms of connection and support.\n\nIndividual roles are gendered by social interaction, the media, and language. There are certain stereotypes society places on the way men and women communicate. Women are often believed to talk too much while men talk too little. Men are stereotyped to be more of a public speaker and leader, while women are stereotyped to talk more in private among their family and friends. For women, society views their use of communication as a way to express feelings and emotions. For men, society views their use of communication as a way to express power and negotiate status among other individuals. There are also certain societal stereotypes about how men and women communicate within a heterosexual marriage or relationship. When a man and a women are communicating within their relationship, the traditional language roles are altered. The man becomes more passive and the woman becomes more active. A man's stereotypical silent communication style is often disappointing for women, while a woman's emotionally articulate communication style is often seen as aggravating for a man. This creates the assumption that women and men have opposing communication styles, therefore creating society's cliche that men and women don't understand each other. This leads to many inter-relational quarrels because of one individual blaming the other for communicating inefficiently due to using the communication style that society has constructed according to their biological sex.\n\nThis, in turn, suggests a dichotomy between a male desire for conversational dominance – noted by Helena Leet-Pellegrini with reference to male experts speaking more verbosely than their female counterparts – and a female aspiration to group conversational participation. One corollary of this is, according to Jennifer Coates, that males are afforded more attention in the context of the classroom and that this can lead to their gaining more attention in scientific and technical subjects, which in turn can lead to their achieving better success in those areas, ultimately leading to their having more power in a technocratic society.\n\nConversation is not the only area where power is an important aspect of the male/female dynamic. Power is reflected in every aspect of communication from what the actual topic of the communication, to the ways in which it is communicated. Women are typically less concerned with power more concerned with forming and maintaining relationships, whereas men are more concerned with their status. Girls and women feel it is crucial that they be liked by their peers, a form of involvement that focuses on symmetrical connection. Boys and men feel it is crucial that they be respected by their peers, as form of involvement that focuses on asymmetrical status. These differences in priorities are reflected in the ways in which men and women communicate. A woman's communication will tend to be more focused on building and maintaining relationships. Men on the other hand, will place a higher priority on power, their communication styles will reflect their desire to maintain their status in the relationship.\n\nAccording to Tannen's research, men tend to tell stories as another way to maintain their status. Primarily, men tell jokes, or stories that focus on themselves. Women on the other hand, are less concerned with their own power, and therefore their stories revolve not around themselves, but around others. By putting themselves on the same level as those around them, women attempt to downplay their part in their own stories, which strengthens their connections to those around them.\n\nLakoff identified three forms of politeness: formal, deference, and camaraderie. Women's language is characterized by formal and deference politeness, whereas men's language is exemplified by camaraderie.\n\nPoliteness in speech is described in terms of positive and negative face. \"Positive face\" refers to one's desire to be liked and admired, while \"negative face\" refers to one's wish to remain autonomous and not to suffer imposition. Both forms, according to Penelope Brown's study of the Tzeltal language, are used more frequently by women whether in mixed or single-sex pairs, suggesting for Brown a greater sensitivity in women than have men to face the needs of others. In short, women are to all intents and purposes largely more polite than men. However, negative face politeness can be potentially viewed as weak language because of its associated hedges and tag questions, a view propounded by O'Barr and Atkins (1980) in their work on courtroom interaction.\n\nSome natural languages have intricate systems of gender-specific vocabulary.\n\n\n\n"}
{"id": "31588507", "url": "https://en.wikipedia.org/wiki?curid=31588507", "title": "Media Practice Model", "text": "Media Practice Model\n\nThe Media Practice Model is a media effects model used within the area of mass communication. This model was developed by Jeanne R. Steele and Jane D. Brown in 1995, and it takes a practice perspective which means that it focuses on everyday activities and routines of media consumption. This theoretical framework was developed to better understand what drives teenagers to pick one media source over another, and what factors play a role in this decision. The Media Practice Model emphasizes the constant interaction between consumers and the media, and focuses on the dialectical aspect of this interaction, suggesting that it is the adolescents’ individual characteristics, environment and daily practices that allow the media to have stronger or weaker effects on them (Steele & Brown, 1995).\n\nThe model was developed based on a study that progressed from 1987 through 1993, and used a variety of methods such as daily journals, in-depth interviews, self-administered questionnaires and “room touring” to understand, as the Steele and Brown call it, the adolescents’ “room culture.” This environment was chosen based on the facts that adolescents’ bedrooms are usually cluttered with different media materials and sources, as well as the fact that this is a place where adolescents spend a good part of their day. This is also a place that provides an intimate environment where adolescents can experiment with possible selves (Markus & Nurius, 1986). The Media Practice Model was based on the findings of this first study, and the incorporation of other media effects theories such as Selective Exposure Theory, Uses and Gratifications, Framing, Cultivation theory, and Emotional Conditioning.\n\nThe Media Practice Model proposed by Steele and Brown has the concepts of selection, interaction and application as the three main components, but also takes into account one’s identity and lived experience. Identity formation is the key component of this model and it represents the main task of adolescent formation (Steele & Brown, 1995). It is believed that adolescents’ sense of who they are influences their interactions with the media, and those interactions in turn influence their sense of who they are, in what Steele and Brown call an ongoing process of cultural production and reproduction. The theoretical perspective of “lived through experience” (Vygotsky, 1978), also takes into account the developmental stage, the sociocultural differences based on gender, class, and race, as well as other factors such as religious beliefs, interactions within one’s neighborhood, school, family, friends circle, and so on. This concept of “lived experience” emphasizes the idea that adolescents’ interaction with the media does not happen in a vacuum. It is argued that the continuous dialectal relationship between the other four components of the Media Practice Model, selection, interaction, application, and identity, all occur within the context of “lived experience (Steele & Brown, 1995).” \n\nSelection, within the Media Practice Model, is simply defined as the act of choosing among media related alternatives (Steele & Brown, 1995). Selection is influenced by motivations, which in turn affects attention to the media selected. In this context, the concept of motivation is defined based on previous theories such as selective exposure and uses and gratifications theory. This means that a teenager’s motivation to attend to certain media might be based on affective, behavioral, cognitive and instrumental needs, or it could also simply be a result of habituation (Steele & Brown, 1995). As far as factoring in “lived experience”, in their initial study Steele and Brown found that selection is also influenced by gender and race. Thus, boys were more likely to be more interested in media that dealt with sports, and this was also reflected in how their rooms were decorated. Also, an African-American girl was more interested in magazines and certain types of music that reflected her ethnic heritage. \n\nInteraction is defined as the cognitive, affective, and behavioral engagement with the media that produces cultural meanings, which are affected by adolescents’ evaluation and interpretation of the media content. In terms of practice, interaction was conceptually defined as what is actually happening at the moment adolescent use the media (Steele & Brown, 1995). As noted in the definition, adolescents can interact with the media on different levels such as cognitive (processing information), affective (arousal), or behavioral (dancing, channel surfing, etc.). However, researchers in communication have been more concerned with the cognitive and affective interactions, and adolescents’ evaluation and interpretation of media content are clearly more closely related to those two types of interactions.\n\nThe concept of “lived experience” was shown to have a role in the interaction component of the model as well. As found by Steele and Brown, racial understanding, class and gender also affected how teenagers interpreted and evaluated the messages conveyed by the media.\n\nApplication is defined as the concrete ways in which adolescents use media in their everyday lives (Steele & Brown, 1995). The Media Practice Model looks at two different types of application: appropriation and incorporation.\n\nBased on reviewers’ critiques that the concepts of incorporation and appropriation are one and the same thing, and also based on a new study performed in 1999, Steele slightly revised this component of the Media Practice Model. Based on the study, Steele noticed that within the application component of the model, a new factor stood out: resistance. Resistance was defined as “teens’ practice of using media to open up a space for combating the status quo (Steele, 1999).” This suggests that when adolescents feel marginalized they engage with media that go against the dominant culture. This way they become fans of movies that depict drug usage, are graphically violent or show sexually explicit material, or start listening to less culturally popular genres of music such as hard rock or heavy metal (Steele, 1999). \nBesides contributing to the Media Practice Model through the discussed revision to the application component, Steele’s (1999) study also provided more depth to the “lived experience” component. It demonstrated more explicitly how one’s race, gender, and developmental stage plays an important role in their media practices and choices. Also, while acknowledging the importance of media content, this study puts more emphasis on media practice rather than on media content or media effects.\n\nSince it was first introduced by Steele and Brown in 1995, and then revised in 1999 by Steele, the Media Practice Model has been used by other researchers in different contexts; however, this model has experienced modest popularity. One of the main drawbacks of this model is its general scope and simplified description of media use. Even so, this model allows for the integration of a whole multitude of variables within its main components, and offers scholars the possibility to use it for more in-depth investigative purposes if desired.\n"}
{"id": "19552", "url": "https://en.wikipedia.org/wiki?curid=19552", "title": "Media studies", "text": "Media studies\n\nFor a history of the field, see \"History of media studies\".\n\nMedia is studied as a broad subject in most states in Australia, with the state of Victoria being world leaders in curriculum development . Media studies in Australia was first developed as an area of study in Victorian universities in the early 1960s, and in secondary schools in the mid 1960s.\n\nToday, almost all Australian universities teach media studies. According to the Government of Australia's \"Excellence in Research for Australia\" report, the leading universities in the country for media studies (which were ranked well above World standards by the report's scoring methodology) are Monash University, QUT, RMIT, University of Melbourne, University of Queensland and UTS.\n\nIn secondary schools, an early film studies course first began being taught as part of the Victorian junior secondary curriculum during the mid 1960s. And, by the early 1970s, an expanded media studies course was being taught. The course became part of the senior secondary curriculum (later known as the Victorian Certificate of Education or \"VCE\") in the 1980s. It has since become, and continues to be, a strong component of the VCE. Notable figures in the development of the Victorian secondary school curriculum were the long time Rusden College media teacher Peter Greenaway (not the British film director), Trevor Barr (who authored one of the first media text books \"Reflections of Reality\") and later John Murray (who authored \"The Box in the Corner\", \"In Focus\", and \"10 Lessons in Film Appreciation\").\n\nToday, Australian states and territories that teach media studies at a secondary level are Australian Capital Territory, Northern Territory, Queensland, South Australia, Victoria and Western Australia. Media studies does not appear to be taught in the state of New South Wales at a secondary level.\n\nIn Victoria, the VCE media studies course is structured as: Unit 1 - Representation, Technologies of Representation, and New Media; Unit 2 - Media Production, Australian Media Organisations; Unit 3 - Narrative Texts, Production Planning; and Unit 4 - Media Process, Social Values, and Media Influence. Media studies also form a major part of the primary and junior secondary curriculum, and includes areas such as photography, print media and television.\n\nVictoria also hosts the peak media teaching body known as ATOM which publishes \"Metro\" and \"Screen Education\" magazines.\n\nIn Canada, media studies and communication studies are incorporated in the same departments and cover a wide range of approaches (from critical theory to organizations to research-creation and political economy, for example). Over time, research developed to employ theories and methods from cultural studies, philosophy, political economy, gender, sexuality and race theory, management, rhetoric, film theory, sociology, and anthropology. Harold Innis and Marshall McLuhan are famous Canadian scholars for their contributions to the fields of media ecology and political economy in the 20th century. They were both important members of the Toronto School of Communication at the time. More recently, the School of Montreal and its founder James R. Taylor significantly contributed to the field of organizational communication by focusing on the ontological processes of organizations.\n\nCarleton University and the University of Western Ontario, 1945 and 1946 prospectively, created Journalism specific programs or schools. A Journalism specific program was also created at Ryerson in 1950. The first communication programs in Canada were started at Ryerson and Concordia Universities. The Radio and Television Arts program at Ryerson were started in the 1950s, while the Film, Media Studies/Media Arts, and Photography programs also originated from programs started in the 1950s. The Communication studies department at Concordia was created in the late 1960s. Ryerson's Radio and Television, Film, Media and Photography programs were renowned by the mid 1970s, and its programs were being copied by other colleges and universities nationally and Internationally.\n\nToday, most universities offer undergraduate degrees in Media and Communication Studies, and many Canadian scholars actively contribute to the field, among which: Brian Massumi (philosophy, cultural studies), Kim Sawchuk (cultural studies, feminist, ageing studies), Carrie Rentschler (feminist theory), and François Cooren (organizational communication).\n\nIn his book “Understanding Media, The Extensions of Man”, media theorist Marshall McLuhan suggested that \"the medium is the message\", and that all human artefacts and technologies are media. His book introduced the usage of terms such as “media” into our language along with other precepts, among them “global village” and “Age of Information”. A medium is anything that mediates our interaction with the world or other humans. Given this perspective, media study is not restricted to just media of communications but all forms of technology. Media and their users form an ecosystem and the study of this ecosystem is known as media ecology.\n\nMcLuhan says that the “technique of fragmentation that is the essence of machine technology” shaped the restructuring of human work and association and “the essence of automation technology is the opposite”. He uses an example of the electric light to make this connection and to explain “the medium is the message”. The electric light is pure information and it is a medium without a message unless it is used to spell out some verbal ad or a name. The characteristic of all media means the “content” of any medium is always another medium. For example, the content of writing is speech, the written word is the content of print, and print is the content of the telegraph. The change that the medium or technology introduces into human affairs is the “message”. If the electric light is used for Friday night football or to light up your desk you could argue that the content of the electric light is these activities. The fact that it is the medium that shapes and controls the form of human association and action makes it the message. The electric light is over looked as a communication medium because it doesn’t have any content. It is not until the electric light is used to spell a brand name that it is recognized as medium. Similar to radio and other mass media electric light eliminates time and space factors in human association creating deeper involvement. McLuhan compared the “content” to a juicy piece of meat being carried by a burglar to distract the “watchdog of the mind”. The effect of the medium is made strong because it is given another media “content”. The content of a movie is a book, play or maybe even an opera.\n\nMcLuhan talks about media being “hot” or “cold” and touches on the principle that distinguishes them from one another. A hot medium (i.e., radio or Movie) extends a single sense in “high definition”. High definition means the state of being well filled with data. A cool medium (i.e., Telephone and TV) is considered “low definition” because a small amount of data/information is given and has to be filled in. Hot media are low in participation and cool media are high in participation. Hot media are low in participation because it is giving most of the information and it excludes. Cool media are high in participation because it gives you information but you have to fill in the blanks and it is inclusive. He used lecturing as an example for hot media and seminars as an example for low media. If you use a hot medium in a hot or cool culture makes a difference.\n\nThere are two universities in China that specialize in media studies. Communication University of China, formerly known as the Beijing Broadcasting Institute, that dates back to 1954. CUC has 15,307 full-time students, including 9264 undergraduates, 3512 candidates for doctor and master's degrees and 16780 students in programs of continuing education. The other university known for media studies in China is Zhejiang University of Media and Communications (ZUMC) which has campuses in Hangzhou and Tongxiang. Almost 10,000 full-time students are currently studying in over 50 programs at the 13 Colleges and Schools of ZUMC. Both institutions have produced some of China's brightest broadcasting talents for television as well as leading journalists at magazines and newspapers.\n\nThere is no university specialized on journalism and media studies, but there are seven public universities which have a department of media stuides. Three biggest are based in Prague (Charles University), Brno (Masaryk University) and Olomouc (Palacký University). There are another nine private universities and colleges which has media studies department.\n\nOne prominent French media critic is the sociologist Pierre Bourdieu who wrote among other books \"On Television\" (New Press, 1999). Bourdieu's analysis is that television provides far less autonomy, or freedom, than we think. In his view, the market (which implies the hunt for higher advertising revenue) not only imposes uniformity and banality, but also a form of invisible censorship. When, for example, television producers \"pre-interview\" participants in news and public affairs programs, to ensure that they will speak in simple, attention-grabbing terms, and when the search for viewers leads to an emphasis on the sensational and the spectacular, people with complex or nuanced views are not allowed a hearing.\n\nIn Germany two main branches of media theory or media studies can be identified.\n\nThe first major branch of media theory has its roots in the humanities and cultural studies, such as film studies (\"Filmwissenschaft\"), theater studies (\"Theaterwissenschaft\") and German language and literature studies (\"Germanistik\") as well as Comparative Literature Studies (\"Komparatistik\"). This branch has broadened out substantially since the 1990s. And it is on this initial basis that a culturally-based media studies (often emphasised more recently through the disciplinary title \"Medienkulturwissenschaft\") in Germany has primarily developed and established itself. \n\nThis plurality of perspectives make it difficult to single out one particular site where this branch of Medienwissenschaft originated. While the Frankfurt-based theatre scholar, Hans-Theis Lehmanns term \"post dramatic theater\" points directly to the increased blending of co-presence and mediatized material in the German theater (and elsewhere) since the 1970s, the field of theater studies from the 1990s onwards at the Freie Universität Berlin, led in particular by Erika Fischer-Lichte, showed particular interest in the ways in which theatricality influenced notions of performativity in aesthetic events. Within the field of Film Studies, again, both Frankfurt and Berlin were dominant in the development of new perspectives on moving image media. Heide Schlüpman in Frankfurt and Gertrud Koch, first in Bochum then in Berlin, were key theorists contributing to an aesthetic theory of the cinema (Schlüpmann) as \"dispositif\" and the moving image as medium, particularly in the context of illuion (Koch). Many scholars who became known as media scholars in Germany originally were scholars of German, such as Friedrich Kittler, who taught at the Humboldt Universität zu Berlin, completed both his dissertation and habilitation in the context of \"Germanistik\". One of the early publications in this new direction is a volume edited by Helmut Kreuzer, \"Literature Studies - Media Studies\" (\"Literaturwissenschaft – Medienwissenschaft\"), which summarizes the presentations given at the Düsseldorfer Germanistentag 1976.\n\nThe second branch of media studies in Germany is comparable to Communication Studies. Pioneered by Elisabeth Noelle-Neumann in the 1940s, this branch studies mass media, its institutions and its effects on society and individuals. The German Institute for Media and Communication Policy, founded in 2005 by media scholar Lutz Hachmeister, is one of the few independent research institutions that is dedicated to issues surrounding media and communications policies.\n\nThe term \"Wissenschaft\" cannot be translated straightforwardly as \"studies\", as it calls to mind both scientific methods and the humanities. Accordingly, German media theory combines philosophy, psychoanalysis, history, and scienctific studies with media-specific research.\n\n\"Medienwissenschaften\" is currently one of the most popular courses of study at universities in Germany, with many applicants mistakenly assuming that studying it will automatically lead to a career in TV or other media. This has led to widespread disillusionment, with students blaming the universities for offering highly theoretical course content. The universities maintain that practical journalistic training is not the aim of the academic studies they offer.\n\nMedia Studies is a fast growing academic field in India, with several dedicated departments and research institutes. With a view to making the best use of communication facilities for information, publicity and development, the Government of India in 1962-63 sought the advice of the Ford Foundation/UNESCO team of internationally known mass communication specialists who recommended the setting up of a national institute for training, teaching and research in mass communication. Anna University was the first university to start Master of Science in Electronic Media programmes. It offers a five-year integrated programme and a two-year programme in Electronic Media. The Department of Media Sciences was started in January 2002, branching off from the UGC's Educational Multimedia Research Centre (EMMRC). National Institute of Open Schooling, the world's largest open schooling system, offers Mass Communication as a subject of studies at senior secondary level. All the major universities in the country have mass media and journalism studies departments. Centre for the Study of Developing Societies (CSDS), Delhi has media studies as one of their major emphasis. Centre for Internet and Society, Bangaluru that does interdisciplinary research on internet and digital technologies also is worth mentioning.\nMain scholars who are working on Indian media include Arvind Rajagopal, Ravi Sundaram, Robin Jeffrey, Sevanti Ninan, Shohini Ghosh, and Usha M. Rodrigues and Maya Ranganathan. The work of Nalin Mehta on the expansion of private television channels in India, Amelia Bonea's research on the history of telegraph and journalism, and Shiju Sam Varughese's work on science and mass media open new areas of research in Indian media studies.\n\nIn the Netherlands, media studies are split into several academic courses such as (applied) communication sciences, communication- and information sciences, communication and media, media and culture or theater, film and television sciences. Whereas communication sciences focuses on the way people communicate, be it mediated or unmediated, media studies tends to narrow the communication down to just mediated communication. However, it would be a mistake to consider media studies a specialism of communication sciences, since media make up just a small portion of the overall course. Indeed, both studies tend to borrow elements from one another.\n\nCommunication sciences (or a derivative thereof) can be studied at Erasmus University Rotterdam, Radboud University, Tilburg University, University of Amsterdam, University of Groningen, University of Twente, Roosevelt Academy, University of Utrecht, VU University Amsterdam and Wageningen University and Research Centre.\n\nMedia studies (or something similar) can be studied at the University of Amsterdam, VU University Amsterdam, Erasmus University Rotterdam, University of Groningen and the University of Utrecht.\n\nMedia studies in New Zealand is healthy, especially due to renewed activity in the country's film industry and is taught at both secondary and tertiary education institutes. Media studies in NZ can be regarded as a singular success, with the subject well-established in the tertiary sector (such as Screen and Media Studies at the University of Waikato; Media Studies, Victoria University of Wellington; Film, Television and Media Studies, University of Auckland; Media Studies, Massey University; Communication Studies, University of Otago). Different Media Studies courses can offer students a range of specialisations- such as cultural studies, media theory and analysis, practical film-making, journalism and communications studies. But what makes the case of New Zealand particularly significant in respect of Media Studies is that for more than a decade it has been a nationally mandated and very popular subject in secondary (high) schools, taught across three years in a very structured and developmental fashion, with Scholarship in Media Studies available for academically gifted students. According to the New Zealand Ministry of Education Subject Enrolment figures 229 New Zealand schools offered Media Studies as a subject in 2016, representing more than 14,000 students. \n\nIn Pakistan, media studies programs are widely offered. University of the Punjab Lahore is the oldest department. Later on University of Karachi, Peshawar University, BZU Multaan, Islamia University Bahwalpur also started communication programs. Now, newly established universities are also offering mass communication program in which University of Gujrat emerged as a leading department. Bahria University which is established by Pakistan Navy is also offering BS in media studies.\n\nIn Switzerland, media and communication studies are offered by several higher education institutions including the International University in Geneva, Zurich University of Applied Sciences, University of Lugano, University of Fribourg and others.\n\nIn the United Kingdom, media studies developed in the 1960s from the academic study of English, and from literary criticism more broadly. The key date, according to Andrew Crisell, is 1959:\n\nWhen Joseph Trenaman left the BBC's Further Education Unit to become the first holder of the Granada Research Fellowship in Television at Leeds University. Soon after in 1966, the Centre for Mass Communication Research was founded at Leicester University, and degree programmes in media studies began to sprout at polytechnics and other universities during the 1970s and 1980s.\n\nJames Halloran at Leicester University is credited with much influence in the development of media studies and communication studies, as the head of the university's Centre for Mass Communication Research, and founder of the International Association for Media and Communication Research. Media Studies is now taught all over the UK. It is taught at Key Stages 1– 3, Entry Level, GCSE and at A level and the Scottish Qualifications Authority offers formal qualifications at a number of different levels. It is offered through a large area of exam boards including AQA and WJEC.\n\nMuch research in the field of news media studies has been led by the Reuters Institute for the Study of Journalism. Details of the research projects and results are published in the RISJ annual report.\n\nMass communication, Communication studies or simply 'Communication' may be more popular names than “media studies” for academic departments in the United States. However, the focus of such programs sometimes excludes certain media—film, book publishing, video games, etc. The title “media studies” may be used alone, to designate film studies and rhetorical or critical theory, or it may appear in combinations like “media studies and communication” to join two fields or emphasize a different focus. It is a very broad study as media has many platforms in the modern world. Social Media is an industry that has gotten a lot of attention in recent years. Our primary form of entertainment is no longer our TVs but we have access to a screen about worldwide events all the time.\nIn 1999, the MIT Comparative Media Studies program started under the leadership of Henry Jenkins, since growing into a graduate program, MIT's largest humanities major, and, following a 2012 merger with the Writing and Humanistic Studies program, a roster of twenty faculty, including Pulitzer Prize-winning author Junot Diaz, science fiction writer Joe Haldeman, games scholar T. L. Taylor, and media scholars William Uricchio (a CMS co-founder), Edward Schiappa, and Heather Hendershot. Now named Comparative Media Studies/Writing, the department places an emphasis on what Jenkins and colleagues had termed \"applied humanities\": it hosts several research groups for civic media, digital humanities, games, computational media, documentary, and mobile design, and these groups are used to provide graduate students with research assistantships to cover the cost of tuition and living expenses. The incorporation of Writing and Humanistic Studies also placed MIT's Science Writing program, Writing Across the Curriculum, and Writing and Communications Center under the same roof.\n\nFormerly an interdisciplinary major at the University of Virginia the Department of Media Studies was officially established in 2001 and has quickly grown to wide recognition. This is partly thanks to the acquisition of Professor Siva Vaidhyanathan, a cultural historian and media scholar, as well as the Inaugural Verklin Media Policy and Ethics Conference, endowed by the CEO of Canoe Ventures and UVA alumnus David Verklin. In 2010, a group of undergraduate students in the Media Studies Department established the Movable Type Academic Journal, the first ever undergraduate academic journal of its kind. The department is expanding rapidly and doubled in size in 2011.\n\nBrooklyn College, part of the City University of New York, has been offering graduate studies in television and media since 1961. Currently, the Department of Television and Radio administers an MS in Media Studies, and hosts the Center for the Study of World Television.\n\nThe University of Southern California has three distinct centers for media studies: the Center for Visual Anthropology (founded in 1984), the Institute for Media Literacy at the School of Cinematic Arts (founded in 1998) and the Annenberg School for Communication and Journalism (founded in 1971).\n\nUniversity of California, Irvine had in Mark Poster one of the first and foremost theorists of media culture in the US, and can boast a strong Department of Film & Media Studies. University of California, Berkeley has three institutional structures within which media studies can take place: the department of Film and Media (formerly Film Studies Program), including famous theorists as Mary Ann Doane and Linda Williams, the Center for New Media, and a long established interdisciplinary program formerly titled Mass Communications, which recently changed its name to Media Studies, dropping any connotations which accompany the term “Mass” in the former title. Until recently, Radford University in Virginia used the title \"media studies\" for a department that taught practitioner-oriented major concentrations in journalism, advertising, broadcast production and Web design. In 2008, those programs were combined with a previous department of communication (speech and public relations) to create a School of Communication. (A media studies major at Radford still means someone concentrating on journalism, broadcasting, advertising or Web production.)\n\nThe University of Denver has a renowned program for digital media studies. It is an interdisciplinary program combining Communications, Computer Science, and the arts.\n\n\n"}
{"id": "1791995", "url": "https://en.wikipedia.org/wiki?curid=1791995", "title": "Missing years (Jewish calendar)", "text": "Missing years (Jewish calendar)\n\nThe missing years in the Hebrew calendar refer to a chronological discrepancy between Talmudic chronologists for the destruction of the First Temple in 423 BCE (3338 AM) and the modern secular dating for it in 587 BCE.\n\nBoth the Babylonian Chronicles and the biblical Chronicles indicate that Nebuchadnezzar captured Jerusalem, but secular scholars have attempted to set a year when the event took place. The Babylonian Chronicles, which were published by Donald Wiseman in 1956, establish that Nebuchadnezzar captured Jerusalem the first time on 2 Adar (16 March) 597 BCE. The Chronicles, with the names of Jewish kings, is derived from British Museum – Cuneiform tablet with part of the Babylonian Chronicle (605–594 BC), (See also Chronicle Concerning the Early Years of Nebuchadnezzar II Reverse, lines 11'–13')\n\nIn the seventh year (of Nebuchadnezzar, 599 BCE) in the month Chislev (Nov/Dec) the king of Babylon assembled his army, and after he had invaded the land of Hatti (northern Syria and southern Anatolia) he laid siege to the city of Judah. On the second day of the month of Adar (16 March) he conquered the city and took the king (Jeconiah) prisoner. He installed in his place a king (Zedekiah) of his own choice, and after he had received rich tribute, he set forth to Babylon.\n\n6 Against him (Jehoiakim) came up Nebuchadnezzar king of Babylon, and bound him in fetters, to carry him to Babylon.\n7 Nebuchadnezzar also carried of the vessels of the house of the LORD to Babylon, and put them in his temple at Babylon.\n8 ... and Jehoiachin his son reigned in his stead.\n9 Jehoiachin was eight years old when he began to reign; and he reigned three months and ten days in Jerusalem...\n10 And at the return of the year king Nebuchadnezzar sent, and brought him to Babylon, with the goodly vessels of the house of the LORD, and made Zedekiah his brother king over Judah and Jerusalem. \n\nBefore Wiseman's publication, E. R. Thiele had determined from the biblical texts that Nebuchadnezzar's initial capture of Jerusalem occurred in the spring of 597 BCE, while other scholars, including William F. Albright, more frequently dated the event to 598 BCE.\n\nFrom the date of the first siege of Jerusalem in about 597 BCE to the date of the destruction of the First Temple requires resort to biblical sources.\n\n2 Chronicles 36:11 says:\n\n11 Zedekiah was twenty and one years old when he began to reign; and he reigned eleven years in Jerusalem\n\nThere has been some debate as to when the second siege of Jerusalem took place. Although there is no dispute that Jerusalem fell the second time in the summer month of Tammuz (), Albright dates the end of Zedekiah's reign (and the fall of Jerusalem) to 587 BCE, whereas Thiele offers 586 BCE.\n\nThiele's reckoning is based on the presentation of Zedekiah's reign on an accession basis, which was used for most but not all of the kings of Judah. In that case, the year that Zedekiah came to the throne would be his \"first\" partial year; his first full year would be 597/596 BCE, and his eleventh year, the year Jerusalem fell, would be 587/586 BCE. Since Judah's regnal years were counted from Tishrei in autumn, this would place the end of his reign and the capture of Jerusalem in the summer of 586 BCE.\n\nThe 2nd century CE rabbinic work Seder Olam Rabbah, which formed the basis of the era counting of the Hebrew calendar, interpreted the prophecy of seventy weeks in as referring to a period of 490 years, with a \"week\" being interpreted as a period of seven years, which would pass between the destruction of the First and Second Temple. This is used to date the destruction of the First Temple to 423 BCE (3338 AM) – about 165 years after the current scholarly dating of the event. The discrepancy between these two dates is referred to as \"missing years\".\n\nToday, Hebrew dating places the creation of the world near the end of \"Year One\" AM and afterwards the first year of Adam's life as \"Year Two\" AM. However, Seder Olam Rabba shows that the Hebrew dating originally counted the first year of Adam's life as \"Year Zero\" AM. This may mean that the Hebrew dating has shifted in the course of history such that traditional dating of ancient events appears two years earlier than the modern Hebrew dating would be.\n\nRabbinic tradition says that the First Temple was destroyed in \"year 3338\" AM and the Second Temple in \"year 3828\" AM. If there was no calendar shift, the Common Era equivalents would be 423 BCE and 68 CE, respectively. If there was a calendar shift, the destructions would have taken place in our years 3339 and 3829 AM, or in 3340 and 3830 AM, and the Common Era equivalents would be 422 BCE and 69 CE, respectively, or 421 BCE and 70 CE.\n\nIf there was no calendar shift, the length of the missing-years period would be 163 years (586 minus 423). If there was a calendar shift, the length of the missing-years period would be 164 or 165 years.\n\nA popular explanation for the missing years suggests that the Jewish sages interpreted the prophecy in as meaning that there would be 490 years from the destruction of the First Temple to the destruction of the Second Temple and, working backwards from the destruction of the Second Temple (in 3828 AM), wrongly dated the destruction of the First Temple (in 3338 AM).\n\nA variation on this argument states that the Jews deliberately altered the dating so that the true date of the \"anointed one\" (Mashiah) mentioned in would be hidden. Other apologists have countered with claims that the dating was indeed altered for one or another reason and should be understood as fable, not history.\n\nThese explanations come from the ambiguous meaning of the word 'week' in Hebrew, which means 'a heptad', or a group of seven. The Hebrew word for 'week' is used to refer to periods of seven days as well as seven years. The understanding of this number as referring to 490 years can also be found in Seder Olam. Christians also interpreted these verses as years and connect them to Jesus, although Rashi's interpretation is such that it upholds the tradition that the anointed one in question is the Persian king Cyrus.\n\nIf traditional dates are assumed to be based on the standard Hebrew calendar, then the differing traditional and modern secular dating of events cannot both be correct. Attempts to reconcile the two systems must show one or both to have errors.\n\nSecular scholars see the discrepancy between the traditional and secular date of the destruction of the First Temple arising as a result of Jewish sages missing out the reign lengths of several Persian kings during the Persian Empire's rule over Israel. Modern secular scholars tally ten Persian kings whose combined reigns total 208 years. By contrast, ancient Jewish sages only mention four Persian kings totaling 52 years. The reigns of several Persian kings appear to be missing from the traditional calculations.\n\nR' Azariah dei Rossi, in \"Me'or Einayim\" (c. 1573), was likely the first Jewish authority to claim that the traditional Hebrew dating is not historically precise regarding the years before the Second Temple.\n\nR' Nachman Krochmal in \"Guide to the perplexed of our times\" (Hebrew, 1851) points to the Greek name Antigonos mentioned in the beginning of Avot as proof that there must have been a longer period to account for this sign of Hellenic influence. He posits that certain books of the Bible such as Kohelet and Isaiah were written or redacted during this period.\n\nR' David Zvi Hoffman (1843–1921) points out that the Mishna in Avot (1:4) in describing the chain of tradition uses the plural \"accepted from them\" even though the previous Mishna only mentions one person. He posits that there must have been another Mishna mentioning two sages that was later removed.\n\nIt has been noted that the traditional account of Jewish history shows a discontinuity in the beginning of the 35th century: The account of Seder Olam Rabbah is complete only until this time. It has been postulated that this work was written to complement another historical work, about subsequent centuries until the time of Hadrian, which is no longer extant.\n\nIt appears that Jewish dating systems only arose in the 35th century, so that precise historical records would naturally have existed only from that time onwards. The Minyan Shtarot system, used to date official Jewish documents, started in the year 3449. According to Lerman's thesis, the year-count \"from Creation\" was established around the same time (see Birkat Hachama).\n\nIt has also been posited that certain calculations in the Talmud compute better according to the secular dating. Two possible harmonizations are proposed by modern rabbis:\n\n\nAttempts have been made to reinterpret the historical evidence to agree with the Rabbinic tradition (see \"Excursus: Rabbinic Tradition\", below), however this approach to the discrepancy is problematic. The reinterpretation of the Greek, Babylonian and Persian sources that is required to support the traditional dating has been achieved only in parts and has not yet been achieved in its entirety. Similar problems face other attempts to revise secular dating (such as those of Peter James and David Rohl) and mainstream scholarship rejects such approaches. Where and how the Gregorian or Julian calendric differential gets factored in, remains another argument entirely.\n\nAccording to the Aramaic Scroll of Antiochus, from the Second Temple's rebuilding till the 23rd year of the reign of Antiochus Eupator, son of Antiochus Epiphanes who invaded Judaea, there had transpired 213 years in total. Quoting verbatim from that ancient Aramaic record:\nבִּשׁנַת עַסרִין וּתלָת שְׁנִין לְמִמלְכֵיהּ, בִּשׁנַת מָאתַן וּתלָת עֲסַר שְׁנִין לְבִניַין בֵּית אֱלָהָא דֵיך, שַׁוִּי אַנפּוֹהִי לְמִיסַּק לִירוּשְׁלֵם\n\nLiteral translation: \"In the twenty third year of his kingdom, in the two-hundred and thirteenth year of the rebuilding of this, God's house, he (Antiochus Eupator) put his face to go up to Jerusalem.\"\n\nThis timeframe is taken in conjunction with another date in the Seleucid Era counting mentioned by Josephus, in his \"Antiquities of the Jews\" (book 12, chapter 9, section 2). Based on Josephus' record, Antiochus Eupator began his reign after his father's death (Antiochus Epiphanes) in \"anno\" 149 of the Seleucid Era (= 162 BCE). Twenty-three years into Antiochus Eupator's reign would have then been \"anno\" 172 of the Seleucid Era, or what was then 139 BCE. Since, according to the Scroll of Antiochus, the Second Temple had already been standing 213 years, this means that the Second Temple was completed in \"anno\" 352 BCE, being what was then the 6th year of the reign of Darius the king (i.e. Darius, the son of Hystaspes), the year in which the king finished its building according to Ezra 6:15. Although this date of the Temple's rebuilding largely disagrees with modern scholarship who base their chronologies upon the Babylonian Chronicles and its rebuilding in 516 BCE, it is, nonetheless, held by religious Jewish circles as being accurate and reliable, since it is founded upon a tradition passed down generation after generation. The Babylonian Chronicles, however, are known to be lacking in certain regnal years ascribed to some kings, besides disagreeing in other places with the ancient Egyptian records outlining the regnal years of eight successive Persian kings, preserved in the Third Book of Manetho. In Jewish tradition, the Second Temple stood 420 years, meaning, it was destroyed by Titus in the 2nd year of the reign of Vespasian, in 68 CE. For a discussion of subject, see Seder Olam Rabbah.\n\nThe 2nd century Jewish chronicler wrote in \"Seder Olam Rabbah\" (chapter 30): \"Rabbi Yose says: The kingdom of Persia during the time of the Temple lasted [only] 34 years.\" Many have misconstrued these words to mean that the author of \"Seder Olam Rabbah\" has contracted the entire Persian period of over 200 years into a supposed period of 34 years. According to RASHI, the 34-year Persian period must be understood in the context of their hegemony over Israel while the Second Temple stood. Meaning, 34 years is the precise timeframe between the building of the Second Temple under Darius (I) in 352 BCE (according to Jewish calculations) and Alexander the Great's rise to power in 318 BCE – collected altogether as 34 years of Persian hegemony over Israel while the Temple stood. This timeframe, therefore, does not signify the end of the dynasties in Persia, but rather of their rule and hegemony over Israel before Alexander the Great rose to power.\n\nNotes\nBibliography\n\n"}
{"id": "27232048", "url": "https://en.wikipedia.org/wiki?curid=27232048", "title": "Modern rhetoric", "text": "Modern rhetoric\n\nModern rhetoric has gone through many changes since the age of ancient Rome and Greece to fit the societal demands of the time. Kenneth Burke, who is largely credited for defining the notion of modern rhetoric, described modern rhetoric as, \"Rooted in an essential function of language itself, a function that is wholly realistic, and is continually born anew; the use of language as a symbolic means of inducing cooperation in beings that by nature respond to symbols.\" Burke’s theory of rhetoric directed attention to the division between classical and modern rhetoric. The intervention of outside academic movements, such as structuralism, semiotics, and critical theory, made important contributions to a modern sense of rhetorical studies.\n\nSome critics disagree with a changing definition of rhetoric, including Brian Vickers, who argued that modern rhetoric demeans classical rhetoric: \"It first reduces its scope, and then applies it to purposes that it never dreamt of.\" He also critiques Burke’s writing on modern rhetoric, saying it is, \"A [rhetorical] system that rearranges the components of classical rhetoric so idiosyncratically as to be virtually unusable.\"\n\nA significant event, deemed the \"linguistic turn,\" drastically changed how modern rhetoric was theorized and practiced. The linguistic turn linked different areas of study by their common concern for symbol-systems in shaping the way humans interpret the world and create meaning. Interpreting the world and creating meaning is the basis for Richard E. Vatz's \"Myth of the Rhetorical Situation,\" Philosophy and Rhetoric, Summer: 1973 and The Only Authentic Book of Persuasion, Kendall Hunt, 2012, 2013. This is a change from the traditional understanding of words being labels for ideas and concepts, to the notion of language constituting social reality.\n\nThe public sphere was studied by scholars such as Jürgen Habermas and Gerard A. Hauser. Jürgen Habermas described the public sphere as the sphere of private people coming together as a public that is accessible by all, to openly discuss the general rules governing society. Gerard Hauser described the public sphere differently in terms of rhetoric. Hauser explained it to be formed by the dialogue surrounding issues, emphasizing how the members of society that engage in the dialogue were the components of the public sphere. The public sphere grows by attaining more members who will engage in the vernacular. Hauser’s definition of the rhetorical public sphere still shares the notion of open debate and accessibility, assuming that the participants are actively engaged in the discourse.\n\nSome scholars that support the notion of modern rhetoric offer normative models that differ from classical rhetoric. Modern rhetorical study, some say, should stress two-way communication based on mutual trust and understanding to improve the speaker’s ability to persuade. Acknowledging that all communication and symbols are rhetorical, scholars of the field also call for a continued expansion of the objects of study, in order to improve communicative practices and bring about more egalitarian speech.\n"}
{"id": "23713433", "url": "https://en.wikipedia.org/wiki?curid=23713433", "title": "Novel", "text": "Novel\n\nA novel is a relatively long work of narrative fiction, normally in prose, which is typically published as a book.\n\nThe entire genre has been seen as having \"a continuous and comprehensive history of about two thousand years\", with its origins in classical Greece and Rome, in medieval and early modern romance, and in the tradition of the Italian renaissance novella. (Since the 18th century, the term \"novella\", or \"novelle\" in German, has been used in English and other European languages to describe a long short story or a short novel.)\n\nMurasaki Shikibu's \"Tale of Genji\" (1010) has been described as the world's first novel. Spread of printed books in China led to the appearance of classical Chinese novels by the Ming dynasty (1368–1644). Parallel European developments occurred after the invention of the printing press. Miguel de Cervantes, author of \"Don Quixote\" (the first part of which was published in 1605), is frequently cited as the first significant European novelist of the modern era. Ian Watt, in \"The Rise of the Novel\" (1957), suggested that the modern novel was born in the early 18th century.\n\nWalter Scott made a distinction between the novel, in which (as he saw it) \"events are accommodated to the ordinary train of human events and the modern state of society\" and the romance, which he defined as \"a fictitious narrative in prose or verse; the interest of which turns upon marvellous and uncommon incidents\". However, many such romances, including the historical romances of Scott, Emily Brontë's \"Wuthering Heights\" and Herman Melville's \"Moby-Dick\", are also frequently called novels, and Scott describes romance as a \"kindred term\". This sort of romance is in turn different from the genre fiction love romance or romance novel. Other European languages do not distinguish between romance and novel: \"a novel is \"le roman\", \"der Roman\", \"il romanzo\".\"\n\nA novel is a long, fictional narrative which describes intimate human experiences. The novel in the modern era usually makes use of a literary prose style. The development of the prose novel at this time was encouraged by innovations in printing, and the introduction of cheap paper in the 15th century.\n\nThe present English (and Spanish) word for a long work of prose fiction derives from the Italian \"novella\" for \"new\", \"news\", or \"short story of something new\", itself from the Latin \"novella\", a singular noun use of the neuter plural of \"novellus\", diminutive of \"novus\", meaning \"new\". Most European languages use the word \"romance\" (as in French, Dutch, Russian, Slovene, Serbo-Croatian, Romanian, Danish, Swedish and Norwegian \"roman\"; Finnish \"romaani\"; German \"Roman\"; Portuguese \"romance\" and Italian \"romanzo\") for extended narratives.\nFictionality is most commonly cited as distinguishing novels from historiography. However this can be a problematic criterion. Throughout the early modern period authors of historical narratives would often include inventions rooted in traditional beliefs in order to embellish a passage of text or add credibility to an opinion. Historians would also invent and compose speeches for didactic purposes. Novels can, on the other hand, depict the social, political and personal realities of a place and period with clarity and detail not found in works of history.\nWhile prose rather than verse became the standard of the modern novel, the ancestors of the modern European novel include verse epics in the Romance language of southern France, especially those by Chrétien de Troyes (late 12th century), and in Middle English (Geoffrey Chaucer's (c. 1343 – 1400) \"The Canterbury Tales\"). Even in the 19th century, fictional narratives in verse, such as Lord Byron's \"Don Juan\" (1824), Alexander Pushkin's \"Yevgeniy Onegin\" (1833), and Elizabeth Barrett Browning's \"Aurora Leigh\" (1856), competed with prose novels. Vikram Seth's \"The Golden Gate\" (1986), composed of 590 Onegin stanzas, is a more recent example of the verse novel.\nBoth in 12th-century Japan and 15th-century Europe, prose fiction created intimate reading situations. On the other hand, verse epics, including the \"Odyssey\" and \"Aeneid\", had been recited to a select audiences, though this was a more intimate experience than the performance of plays in theaters. A new world of individualistic fashion, personal views, intimate feelings, secret anxieties, \"conduct\", and \"gallantry\" spread with novels and the associated prose-romance.\nThe novel is today the longest genre of narrative prose fiction, followed by the novella. However, in the 17th century, critics saw the romance as of epic length and the novel as its short rival. A precise definition of the differences in length between these types of fiction, is, however, not possible.The requirement of length has been traditionally connected with the notion that a novel should encompass the \"totality of life.\"\n\nAlthough early forms of the novel are to be found in a number of places, including classical Rome, 10th– and 11th-century Japan, and Elizabethan England, the European novel is often said to have begun with \"Don Quixote\" in 1605.\n\nEarly works of extended fictional prose, or novels, include works in Latin like the \"Satyricon\" by Petronius (c. 50 AD), and \"The Golden Ass\" by Apuleius (c. 150 AD), works in Ancient Greek such as \"Daphnis and Chloe\" by Longus (c. late second century AD), works in Sanskrit such as the 6th– or 7th-century \"Daśakumāracarita\" by Daṇḍin, and in the 7th-century \"Kadambari\" by Banabhatta, Murasaki Shikibu's 11th-century Japanese work \"The Tale of Genji\", the 12th-century \"Hayy ibn Yaqdhan\" (or \"Philosophus Autodidactus\", the 17th-century Latin title) by Ibn Tufail, who wrote in Arabic, the 13th-century \"Theologus Autodidactus\" by Ibn al-Nafis, another Arabic novelist, and \"Blanquerna\", written in Catalan by Ramon Llull (1283), and the 14th-century Chinese \"Romance of the Three Kingdoms\" by Luo Guanzhong.\n\nMurasaki Shikibu's \"Tale of Genji\" (1010) has been described as the world's first novel and shows essentially all the qualities for which Marie de La Fayette's novel \"La Princesse de Clèves\" (1678) has been praised: individuality of perception, an interest in character development, and psychological observation. Urbanization and the spread of printed books in Song Dynasty (960–1279) China led to the evolution of oral storytelling into fictional novels by the Ming dynasty (1368–1644). Parallel European developments did not occur until after the invention of the printing press by Johannes Gutenberg in 1439, and the rise of the publishing industry over a century later allowed for similar opportunities.\n\nBy contrast, Ibn Tufail's \"Hayy ibn Yaqdhan\" and Ibn al-Nafis' \"Theologus Autodidactus\" are works of didactic philosophy and theology. In this sense, \"Hayy ibn Yaqdhan\" would be considered an early example of a philosophical novel, while \"Theologus Autodidactus\" would be considered an early theological novel. \"Hayy ibn Yaqdhan\", with its story of a human outcast surviving on an island, is also likely to have influenced Daniel Defoe's \"Robinson Crusoe\" (1719), because the work was available in an English edition in 1711.\n\nEpic poetry exhibits some similarities with the novel, and the Western tradition of the novel reaches back into the field of verse epics, though again not in an unbroken tradition. The epics of Asia, such as the Sumerian \"Epic of Gilgamesh\" (1300–1000 BC), and Indian epics such as the \"Ramayana\" (400 BCE and 200 CE), and \"Mahabharata\" (4th century BC) were as unknown in early modern Europe as was the Anglo-Saxon epic of \"Beowulf\" (c. 750–1000 AD), which was rediscovered in the late 18th century and early 19th century. Other non-European works, such as the Torah, the Koran, and the Bible, are full of stories, and thus have also had a significant influence on the development of prose narratives, and therefore the novel. Then at the beginning of the 18th century, French prose translations brought Homer's works to a wider public, who accepted them as forerunners of the novel.\n\nClassical Greek and Roman prose narratives included a didactic strand, with the philosopher Plato's (c. 425 – c. 348 BC) dialogues; a satirical dimension with Petronius' \"Satyricon\"; the incredible stories of Lucian of Samosata; and Lucius Apuleius' proto-picaresque \"The Golden Ass\", as well as the heroic romances of the Greeks Heliodorus and Longus. Longus is the author of the Greek novel, \"Daphnis and Chloe\" (2nd century AD).\n\nRomance or chivalric romance is a type of narrative in prose or verse popular in the aristocratic circles of High Medieval and Early Modern Europe. They were marvel-filled adventures, often of a knight-errant with heroic qualities, who undertakes a quest, yet it is \"the emphasis on heterosexual love and courtly manners distinguishes it from the \"chanson de geste\" and other kinds of epic, which involve heroism.\" In later romances, particularly those of French origin, there is a marked tendency to emphasize themes of courtly love.\n\nOriginally, romance literature was written in Old French, Anglo-Norman and Occitan, later, in English, Italian and German. During the early 13th century, romances were increasingly written as prose.\n\nThe shift from verse to prose dates from the early 13th century. The \"Prose Lancelot\" or \"Vulgate Cycle\" includes passages from that period. This collection indirectly led to Thomas Malory's \"Le Morte d'Arthur\" of the early 1470s. Prose became increasingly attractive because it enabled writers to associate popular stories with serious histories traditionally composed in prose, and could also be more easily translated.\n\nPopular literature also drew on themes of romance, but with ironic, satiric or burlesque intent. Romances reworked legends, fairy tales, and history, but by about 1600 they were out of fashion, and Miguel de Cervantes famously burlesqued them in \"Don Quixote\" (1605). Still, the modern image of medieval is more influenced by the romance than by any other medieval genre, and the word \"medieval\" evokes knights, distressed damsels, dragons, and such tropes.\n\nAround 1800, the connotations of \"romance\" was modified with the development Gothic fiction.\n\nThe term \"novel\" originates from the production of short stories, or novella that remained part of a European oral culture of storytelling into the late 19th century. Fairy tales, jokes, and humorous stories designed to make a point in a conversation, and the exemplum a priest would insert in a sermon belong into this tradition. Written collections of such stories circulated in a wide range of products from practical compilations of examples designed for the use of clerics to compilations of various stories such as Boccaccio's \"Decameron\" (1354) and Geoffrey Chaucer's \"Canterbury Tales\" (1386–1400). The \"Decameron\" (1354) was a compilation of one hundred novelle told by ten people—seven women and three men—fleeing the Black Death by escaping from Florence to the Fiesole hills, in 1348.\n\nThe modern distinction between history and fiction did not exist in the early sixteenth century and the grossest improbabilities pervade many historical accounts found in the early modern print market. William Caxton's 1485 edition of Thomas Malory's \"Le Morte d'Arthur\" (1471) was sold as a true history, though the story unfolded in a series of magical incidents and historical improbabilities. Sir John Mandeville's \"Voyages\", written in the 14th century, but circulated in printed editions throughout the 18th century, was filled with natural wonders, which were accepted as fact, like the one-footed Ethiopians who use their extremity as an umbrella against the desert sun. Both works eventually came to be viewed as works of fiction.\n\nIn the 16th and 17th centuries two factors led to the separation of history and fiction. The invention of printing immediately created a new market of comparatively cheap entertainment and knowledge in the form of chapbooks. The more elegant production of this genre by 17th- and 18th-century authors were \"belles lettres—\"that is, a market that would be neither low nor academic. The second major development was the first best-seller of modern fiction, the Spanish \"Amadis de Gaula\", by García Montalvo. However, it was not accepted as an example of \"belles lettres\". The \"Amadis\" eventually became the archetypical romance, in contrast with the modern novel which began to be developed in the 17th century.\n\nA chapbook is an early type of popular literature printed in early modern Europe. Produced cheaply, chapbooks were commonly small, paper-covered booklets, usually printed on a single sheet folded into books of 8, 12, 16 and 24 pages. They were often illustrated with crude woodcuts, which sometimes bore no relation to the text. When illustrations were included in chapbooks, they were considered popular prints. The tradition arose in the 16th century, as soon as printed books became affordable, and rose to its height during the 17th and 18th centuries and Many different kinds of ephemera and popular or folk literature were published as chapbooks, such as almanacs, children's literature, folk tales, nursery rhymes, pamphlets, poetry, and political and religious tracts.\n\nThe term \"chapbook\" for this type of literature was coined in the 19th century. The corresponding French and German terms are \"bibliothèque bleue\" (blue book) and \"Volksbuch\", respectively. The principal historical subject matter of chapbooks was abridgements of ancient historians, popular medieval histories of knights, stories of comical heroes, religious legends, and collections of jests and fables. The new printed books reached the households of urban citizens and country merchants who visited the cities as traders. Cheap printed histories were, in the 17th and 18th centuries, especially popular among apprentices and younger urban readers of both sexes.\n\nThe early modern market, from the 1530s and 1540s, divided into low chapbooks and high market expensive, fashionable, elegant belles lettres. The \"Amadis\" and Rabelais' \"Gargantua and Pantagruel\" were important publications with respect to this divide. Both books specifically addressed the new customers of popular histories, rather than readers of \"belles lettres\". The Amadis was a multi–volume fictional history of style, that aroused a debate about style and elegance as it became the first best-seller of popular fiction. On the other hand, \"Gargantua and Pantagruel\", while it adopted the form of modern popular history, in fact satirized that genre's stylistic achievements. The division, between low and high literature, became especially visible with books that appeared on both the popular and \"belles lettres\" markets in the course of the 17th and 18th centuries: low chapbooks included abridgments of books such as Miguel Cervantes' \"Don Quixote\" (1605/1615)\n\nThe term \"chapbook\" is also in use for present-day publications, commonly short, inexpensive booklets.\n\nHeroic Romance is a genre of imaginative literature, which flourished in the 17th century, principally in France.\n\nThe beginnings of modern fiction in France took a pseudo-bucolic form, and the celebrated \"L'Astrée\", (1610) of Honore d'Urfe (1568–1625), which is the earliest French novel, is properly styled a pastoral. Although its action was, in the main, languid and sentimental, there was a side of the Astree which encouraged that extravagant love of glory, that spirit of \" panache\", which was now rising to its height in France. That spirit it was which animated Marin le Roy de Gomberville (1603–1674), who was the inventor of what have since been known as the Heroical Romances. In these there was experienced a violent recrudescence of the old medieval elements of romance, the impossible valour devoted to a pursuit of the impossible beauty, but the whole clothed in the language and feeling and atmosphere of the age in which the books were written. In order to give point to the chivalrous actions of the heroes, it was always hinted that they were well-known public characters of the day in a romantic disguise.\n\nStories of witty cheats were an integral part of the European novella with its tradition of fabliaux. Significant examples include \"Till Eulenspiegel\" (1510), \"Lazarillo de Tormes\" (1554), Grimmelshausen's \"Simplicissimus Teutsch\" (1666–1668) and in England Richard Head's \"The English Rogue\" (1665). The tradition that developed with these titles focused on a hero and his life. The adventures led to satirical encounters with the real world with the hero either becoming the pitiable victim or the rogue who exploited the vices of those he met.\n\nA second tradition of satirical romances can be traced back to Heinrich Wittenwiler's \"Ring\" (c. 1410) and to François Rabelais' \"Gargantua and Pantagruel\" (1532–1564), which parodied and satirized heroic romances, and did this mostly by dragging them into the low realm of the burlesque. Cervantes' \"Don Quixote\" (1606/1615) modified the satire of romances: its hero lost contact with reality by reading too many romances in the Amadisian tradition.\n\nOther important works of the tradition are Paul Scarron's \"Roman Comique\" (1651–57), the anonymous French \"Rozelli\" with its satire on Europe's religions, Alain-René Lesage's \"Gil Blas\" (1715–1735), Henry Fielding's \"Joseph Andrews\" (1742) and \"Tom Jones\" (1749), and Denis Diderot's \"Jacques the Fatalist\" (1773, printed posthumously in 1796).\n\nA market of literature in the modern sense of the word, that is a separate market for fiction and poetry, did not exist until the late seventeenth century. All books were sold under the rubric of \"History and politicks\" in the early 18th century, including pamphlets, memoirs, travel literature, political analysis, serious histories, romances, poetry, and novels.\n\nThat fictional histories shared the same space with academic histories and modern journalism had been criticized by historians since the end of the Middle Ages: fictions were \"lies\" and therefore hardly justifiable at all. The climate, however, changed in the 1670s.\n\nThe romance format of the quasi–historical works of Madame d'Aulnoy, César Vichard de Saint-Réal, Gatien de Courtilz de Sandras, and Anne-Marguerite Petit du Noyer, allowed the publication of histories that dared not risk an unambiguous assertion of their truth. The literary market-place of the late 17th and early 18th century employed a simple pattern of options whereby fictions could reach out into the sphere of true histories. This permitted its authors to claim they had published fiction, not truth, if they ever faced allegations of libel.\n\nPrefaces and title pages of 17th– and early 18th-century fiction acknowledged this pattern: histories could claim to be romances, but threaten to relate true events, as in the \"Roman à clef\". Other works could, conversely, claim to be factual histories, yet earn the suspicion that they were wholly invented. A further differentiation was made between private and public history: Daniel Defoe's \"Robinson Crusoe\" was, within this pattern, neither a \"romance\" nor a \"novel\". It smelled of romance, yet the preface stated that it should most certainly be read as a true private history.\n\nThe rise of the novel as an alternative to the romance began with the publication of Cervantes' \"Novelas Exemplares\" (1613). It continued with Scarron's \"Roman Comique\" (the first part of which appeared in 1651), whose heroes noted the rivalry between French romances and the new Spanish genre.\n\nLate 17th-century critics looked back on the history of prose fiction, proud of the generic shift that had taken place, leading towards the modern novel/novella. The first perfect works in French were those of Scarron and Madame de La Fayette's \"Spanish history\" \"Zayde\" (1670). The development finally led to her \"Princesse de Clèves\" (1678), the first novel with what would become characteristic French subject matter.\n\nEurope witnessed the generic shift in the titles of works in French published in Holland, which supplied the international market and English publishers exploited the novel/romance controversy in the 1670s and 1680s. Contemporary critics listed the advantages of the new genre: brevity, a lack of ambition to produce epic poetry in prose; the style was fresh and plain; the focus was on modern life, and on heroes who were neither good nor bad. The novel's potential to become the medium of urban gossip and scandal fuelled the rise of the novel/novella. Stories were offered as allegedly true recent histories, not for the sake of scandal but strictly for the moral lessons they gave. To prove this, fictionalized names were used with the true names in a separate key. The \"Mercure Gallant\" set the fashion in the 1670s. Collections of letters and memoirs appeared, and were filled with the intriguing new subject matter and the epistolary novel grew from this and led to the first full blown example of scandalous fiction in Aphra Behn's \"Love-Letters Between a Nobleman and His Sister\" (1684/ 1685/ 1687). Before the rise of the literary novel, reading novels had only been a form of entertainment.\n\nHowever, one of the earliest English novels, Daniel Defoe's \"Robinson Crusoe\" (1719), has elements of the romance, unlike these novels, because of its exotic setting and story of survival in isolation. \"Crusoe\" lacks almost all of the elements found in these new novels: wit, a fast narration evolving around a group of young fashionable urban heroes, along with their intrigues, a scandalous moral, gallant talk to be imitated, and a brief, conciseness plot. The new developments did, however, lead to Eliza Haywood's epic length novel, \"Love in Excess\" (1719/20) and to Samuel Richardson's \"Pamela, or Virtue Rewarded\" (1741). Some literary historians date the beginning of the English novel with Richardson's \"Pamela\", rather than \"Crusoe.\"\n\nThe idea of the \"rise of the novel\" in the 18th century is especially associated with Ian Watt's influential study \"The Rise of the Novel\" (1957). In Watt's conception, a rise in fictional realism during the 18th century came to distinguish the novel from earlier prose narratives.\n\nThe rising status of the novel in 18th-century can be seen in the development of philosophical and experimental novels.\n\nPhilosophical fiction was not exactly new. Plato's dialogues were embedded in fictional narratives and his \"Republic\" is an early example of a Utopia. The tradition of works of fiction that were also philosophical texts continued with Thomas More's \"Utopia\" (1516) and Tommaso Campanella's \"City of the Sun\" (1602). However, the actual tradition of the philosophical novel came into being in the 1740s with new editions of More's work under the title \"Utopia: or the happy republic; a philosophical romance\" (1743). Voltaire wrote in this genre in \"Micromegas: a comic romance, which is a biting satire on philosophy, ignorance, and the self-conceit of mankind\" (1752, English 1753). His \"Zadig\" (1747) and \"Candide\" (1759) became central texts of the French Enlightenment and of the modern novel.\n\nAn example of the experimental novel is Laurence Sterne's \"The Life and Opinions of Tristram Shandy, Gentleman\" (1759–1767), with its rejection of continuous narration. In it the author not only addresses readers in his preface but speaks directly to them in his fictional narrative. In addition to Sterne's narrative experiments, there has visual experiments, such as a marbled page, a black page to express sorrow, and a page of lines to show the plot lines of the book. The novel as a whole focuses on the problems of language, with constant regard to John Locke's theories in \"An Essay Concerning Human Understanding\".\n\nThe rise of the word novel at the cost of its rival, the romance, remained a Spanish and English phenomenon, and though readers all over Western Europe had welcomed the novel(la) or short history as an alternative in the second half of the 17th century, only the English and the Spanish had, however, openly discredited the romance.\n\nBut the change of taste was brief and Fénelon's \"Telemachus\" (1699/1700) already exploited a nostalgia for the old romances with their heroism and professed virtue. Jane Barker explicitly advertised her \"Exilius\" as \"A new Romance\", \"written after the Manner of Telemachus\", in 1715. Robinson Crusoe spoke of his own story as a \"romance\", though in the preface to the third volume, published in 1720, Defoe attacks all who said \"that [...] the Story is feign'd, that the Names are borrow'd, and that it is all a Romance; that there never were any such Man or Place\".\n\nThe late 18th century brought an answer with the Romantic Movement's readiness to reclaim the word romance, with the gothic romance, and the historical novels of Walter Scott. \"Robinson Crusoe\" now became a \"novel\" in this period, that is a work of the new realistic fiction created in the 18th century.\n\nSentimental novels relied on emotional responses, and feature scenes of distress and tenderness, and the plot is arranged to advance emotions rather than action. The result is a valorization of \"fine feeling\", displaying the characters as models of refined, sensitive emotional effect. The ability to display such feelings was thought at this time to show character and experience, and to help shape positive social life and relationships.\n\nAn example of this genre is Samuel Richardson's \"Pamela, or Virtue Rewarded\" (1740), composed \"to cultivate the Principles of Virtue and Religion in the Minds of the Youth of Both Sexes\", which focuses on a potential victim, a heroine that has all the modern virtues and who is vulnerable because her low social status and her occupation as servant of a libertine who falls in love with her. She, however, ends in reforming her antagonist.\n\nMale heroes adopted the new sentimental character traits in the 1760s. Laurence Sterne's Yorick, the hero of the \"Sentimental Journey\" (1768) did so with an enormous amount of humour. Oliver Goldsmith's \"Vicar of Wakefield\" (1766) and Henry Mackenzie's \"Man of Feeling\" (1771) produced the far more serious role models.\n\nThees works inspired a sub- and counterculture of pornographic novels, for which Greek and Latin authors in translations had provided elegant models from the last century. Pornography includes John Cleland's \"Fanny Hill\" (1748), which offered an almost exact reversals of the plot of novel's that emphasised virtue. The prostitute Fanny Hill learns to enjoy her work and establishes herself as a free and economically independent individual, in editions one could only expect to buy under the counter.\n\nLess virtuous protagonists can also be found in satirical novels, like Richard Head's \"English Rogue\" (1665), that feature brothels, while women authors like Aphra Behn had offered their heroines alternative careers as precursors of the 19th-century femmes fatales.>\n\nThe genre evolves in the 1770s with, for example, Werther in Johann Wolfgang von Goethe's \"The Sorrows of Young Werther\" (1774) realising that it is impossible for him to integrate into the new conformist society, and Pierre Choderlos de Laclos in \"Les Liaisons dangereuses\" (1782) showing a group of aristocrats playing games of intrigue and amorality..\n\nBy around 1700, fiction was no longer a predominantly aristocratic entertainment, and printed books had soon gained the power to reach readers of almost all classes, though the reading habits differed and to follow fashions remained a privilege. Spain was a trendsetter into the 1630s but French authors superseded Cervantes, de Quevedo, and Alemán in the 1640s. As Huet was to note in 1670, the change was one of manners. The new French works taught a new, on the surface freer, gallant exchange between the sexes as the essence of life at the French court.\n\nThe situation changed again from 1660s into the 1690s when works by French authors were published in Holland out of the reach of French censors. Dutch publishing houses pirated of fashionable books from France and created a new market of political and scandalous fiction. This led to a market of European rather than French fashions in the early 18th century.\nBy the 1680s fashionable political European novels had inspired a second wave of private scandalous publications and generated new productions of local importance. Women authors reported on politics and on their private love affairs in The Hague and in London. German students imitated them to boast of their private amours in fiction. The London, the anonymous international market of the Netherlands, publishers in Hamburg and Leipzig generated new public spheres. Once private individuals, such as students in university towns and daughters of London's upper class began write novels based on questionable reputations, the public began to call for a reformation of manners.\n\nAn important development in Britain, at the beginning of the century, was that new journals like \"The Spectator\" and \"The Tatler\" reviewed novels. In Germany Gotthold Ephraim Lessing's \"Briefe, die neuste Literatur betreffend\" (1758) appeared in the middle of the century with reviews of art and fiction. By the 1780s such reviews played had an important role in introducing new works of fiction to the public.\n\nInfluenced by the new journals, reform became the main goal of the second generation of 18th-century novelists. \"The Spectator\" Number 10 had stated that the aim was now \"to enliven morality with wit, and to temper wit with morality […] to bring philosophy out of the closets and libraries, schools and colleges, to dwell in clubs and assemblies, at tea-tables and coffeehouses\"). Constructive criticism of novels had until then been rare. The first treatise on the history of the novel was a preface to Marie de La Fayette's novel \"Zayde\" (1670).\n\nA much later development was the introduction of novels into school and later university curricula.\n\nThe French churchman and scholar Pierre Daniel Huet's \"Traitté de l'origine des romans\" (1670) laid the ground for a greater acceptance of the novel as literature, that is comparable to the classics, in the early 18th century. The theologian had not only dared to praise fictions, but he had also explained techniques of theological interpretation of fiction, which was a novelty. Furthermore, readers of novels and romances could gain insight not only into their own culture, but also that of distant, exotic countries.\n\nWhen the decades around 1700 saw the appearance of new editions of the classical authors Petronius, Lucian, and Heliodorus of Emesa. the publishers equipped them with prefaces that referred to Huet's treatise. and the canon it had established. Also exotic works of Middle Eastern fiction entered the market that gave insight into Islamic culture. \"The Book of One Thousand and One Nights\" was first published in Europe from 1704 to 1715 in French, and then translated immediately into English and German, and was seen as a contribution to Huet's history of romances.\n\nThe English, \"Select Collection of Novels in six volumes\" (1720–22), is a milestone in this development of the novel's prestige. It included Huet's \"Treatise\", along with the European tradition of the modern novel of the day: that is, novella from Machiavelli's to Marie de La Fayette's masterpieces. Aphra Behn's novels had appeared in the 1680s but became classics when reprinted in collections. Fénelon's \"Telemachus\" (1699/1700) became a classic three years after its publication. New authors entering the market were now ready to use their personal names rather than pseudonyms, including Eliza Haywood, who in 1719 following in the footsteps of Aphra Behn used her name with unprecedented pride.\n\nThe very word romanticism is connected to the idea of romance, and the romance genre experienced a revival, at the end of the 18th century, with gothic fiction, that began in 1746 with English author Horace Walpole's \"The Castle of Otranto\", subtitled (in its second edition) \"A Gothic Story\". Other important works are Ann Radcliffe's \"The Mysteries of Udolpho\" (1794) and 'Monk' Lewis's \"The Monk\" (1795).\n\nThe new romances challenged the idea that the novel involved a realistic depictions of life, and destabilized the difference the critics had been trying to establish, between serious classical art and popular fiction. Gothic romances exploited the grotesque, and some critics thought that their subject matter deserved less credit than the worst medieval tales of Arthurian knighthood.\n\nThe authors of this new type of fiction were accused of exploiting all available topics to thrill, arouse, or horrify their audience. These new romantic novelists, however, claimed that they were exploring the entire realm of fictionality. And psychological interpreters, in the early 19th century, read these works as encounters with the deeper hidden truth of the human imagination: this included sexuality, anxieties, and insatiable desires. Under such readings, novels were described as exploring deeper human motives, and it was suggested that such artistic freedom would reveal what had not previously been openly visible.\n\nThe romances of de Sade, \"Les 120 Journées de Sodome\" (1785), Poe's \"Tales of the Grotesque and Arabesque\" (1840), Mary Shelley, \"Frankenstein\" (1818), and E.T.A. Hoffmann, \"Die Elixiere des Teufels\" (1815), would later attract 20th-century psychoanalysts and supply the images for 20th- and 21st-century horror films, love romances, fantasy novels, role-playing computer games, and the surrealists.\n\nThe historical romance was also important at this time. But, while earlier writers of these romances paid little attention to historical reality, Walter Scott's historical novel \"Waverley\" (1814) broke with this tradition, and he invented \"the true historical novel\". At the same time he was influenced by gothic romance, and had collaborated in 1801 with 'Monk' Lewis on \"Tales of Wonder\". With his Waverley novels Scott \"hoped to do for the Scottish border\" what Goethe and other German poets \"had done for the Middle Ages, \"and make its past live again in modern romance\". Scott's novels \"are in the mode he himself defined as romance, 'the interest of which turns upon marvelous and uncommon incidents'\". He used his imagination to re-evaluate history by rendering things, incidents and protagonists in the way only the novelist could do. His work remained historical fiction, yet it questioned existing historical perceptions. The use of historical research was an important tool: Scott, the novelist, resorted to documentary sources as any historian would have done, but as a romantic he gave his subject a deeper imaginative and emotional significance. By combining research with \"marvelous and uncommon incidents\", Scott attracted a far wider market than any historian could, and was the most famous novelist of his generation, throughout Europe.\n\nIn the 19th century the relationship between authors, publishers, and readers, changed. Authors originally had only received payment for their manuscript, however, changes in copyright laws, which began in 18th and continued into 19th century promised royalties on all future editions. Another change in the 19th century was that novelists began to read their works in theaters, halls, and bookshops. Also during the nineteenth century the market for popular fiction grew, and competed with works of literature. New institutions like the circulating library created a new market with a mass reading public.\n\nAnother difference was that novels began to deal with more difficult subjects, including current political and social issues, that were being discussed in newspapers and magazines. The idea of social responsibility became a key subject, whether of the citizen, or of the artist, with the theoretical debate concentrating on questions around the moral soundness of the modern novel. Questions about artistic integrity, as well as aesthetics, including, for example. the idea of \"art for art's sake\", proposed by writers like Oscar Wilde and Algernon Charles Swinburne, were also important.\n\nMajor British writers such as Charles Dickens and Thomas Hardy were influenced by the romance genre tradition of the novel, which had been revitalized during the Romantic period. The Brontë sisters were notable mid-19th-century authors in this tradition, with Anne Brontë's \"The Tenant of Wildfell Hall\", Charlotte Brontë's \"Jane Eyre\" and Emily Brontë's \"Wuthering Heights\". Publishing at the very end of the 19th century, Joseph Conrad has been called, \"a supreme 'romancer'\". In America \"the romance ... proved to be a serious, flexible, and successful medium for the exploration of philosophical ideas and attitudes.\" Notable examples include Nathaniel Hawthorne's \"The Scarlet Letter\", and Herman Melville's \"Moby-Dick\".\n\nA number of European novelists were similarly influenced influenced during this period by the earlier romance tradition, along with the Romanticism, including Victor Hugo, with novels like \"The Hunchback of Notre-Dame\" (1831) and \"Les Misérables\" (1862), and Mikhail Yuryevich Lermontov with \"A Hero of Our Time\" (1840).\n\nMany 19th-century authors dealt with significant social matters. Émile Zola's novels depicted the world of the working classes, which Marx and Engels's non-fiction explores. In the United States slavery and racism became topics of far broader public debate thanks to Harriet Beecher Stowe's \"Uncle Tom's Cabin\" (1852), which dramatizes topics that had previously been discussed mainly in the abstract. Charles Dickens' novels led his readers into contemporary workhouses, and provided first-hand accounts of child labor. The treatment of the subject of war changed with Leo Tolstoy's \"War and Peace\" (1868/69), where he questions the facts provided by historians. Similarly the treatment of crime is very different in Fyodor Dostoyevsky's \"Crime and Punishment\" (1866), where the point of view is that of a criminal. Women authors had dominated fiction from the 1640s into the early 18th century, but few before George Eliot so openly questioned the role, education, and status of women in society, as she did.\n\nAs the novel became a platform of modern debate, national literatures were developed that link the present with the past in the form of the historical novel. Alessandro Manzoni's \"I Promessi Sposi\" (1827) did this for Italy, while novelists in Russia and the surrounding Slavonic countries, as well as Scandinavia, did likewise.\n\nAlong with this new appreciation of history, the future also became a topic for fiction. This had been done earlier in works like Samuel Madden's \"Memoirs of the Twentieth Century\" (1733) and Mary Shelley's \"The Last Man\" (1826), a work whose plot culminated in the catastrophic last days of a mankind extinguished by the plague. Edward Bellamy's \"Looking Backward\" (1887) and H.G. Wells's \"The Time Machine\" (1895) were concerned with technological and biological developments. Industrialization, Darwin's theory of evolution and Marx's theory of class divisions shaped these works and turned historical processes into a subject of wide debate. Bellamy's \"Looking Backward\" became the second best-selling book of the 19th century after Harriet Beecher Stowe's \"Uncle Tom's Cabin\". Such works led to the development of a whole genre of popular science fiction as the 20th century approached.\n\nJames Joyce's \"Ulysses\" (1922) had a major influence on modern novelists, in the way that it replaced the 18th- and 19th-century narrator with a text that attempted to record inner thoughts, or a \"stream of consciousness\". This term was first used by William James in 1890 and is used (or the related interior monologue) by modernists like Dorothy Richardson, Marcel Proust, as well as, later Virginia Woolf and William Faulkner. Also in the 1920s expressionist Alfred Döblin went in a different direction with \"Berlin Alexanderplatz\" (1929), where interspersed non-fictional text fragments exist alongside the fictional material to create another new form of realism, which differs from that of stream-of-consciousness.\n\nLater works like Samuel Beckett's trilogy \"Molloy\" (1951), \"Malone Dies\" (1951) and \"The Unnamable\" (1953), as well as Julio Cortázar's \"Rayuela\" (1963) and Thomas Pynchon's \"Gravity's Rainbow\" (1973) all make use of the stream-of-consciousness technique. On the other hand, Robert Coover is an example of those authors who, in the 1960s, fragmented their stories and challenged time and sequentiality as fundamental structural concepts.\n\nThe 20th century novels deals with a wide range of subject matter. Erich Maria Remarque's \"All Quiet on the Western Front\" (1928) focusses on young German's experiences of World War I. The Jazz Age is explored by American F. Scott Fitzgerald, and the Great Depression by fellow American John Steinbeck. The rise of totalitarian states is the subject of British writer George Orwell. France's existentialism is the subject of French writers Jean-Paul Sartre's \"Nausea\" (1938) and Albert Camus' \"The Stranger\" (1942). The counterculture of the 1960s led to revived interest in Hermann Hesse's \"Steppenwolf\" (1927), and produced such iconic works of its own like Ken Kesey's \"One Flew Over the Cuckoo's Nest\" and Thomas Pynchon's \"Gravity's Rainbow\". Novelist have also been interested in the subject of racial and gender identity in recent decades. Jesse Kavadlo of Maryville University of St. Louis has described Chuck Palahniuk's \"Fight Club\" (1996) as \"a closeted feminist critique\". Virginia Woolf, Simone de Beauvoir, Doris Lessing, Elfriede Jelinek were feminist voices during this period.\n\nFurthermore, the major political and military confrontations of the 20th and 21st centuries have also influenced novelists. The events of World War II, from a German perspective, are dealt with by Günter Grass' \"The Tin Drum\" (1959) and an American by Joseph Heller's \"Catch-22\" (1961). The subsequent Cold War influenced popular spy novels. Latin American self-awareness in the wake of the (failing) left revolutions of the 1960s and 1970s resulted in a \"Latin American Boom\", linked to with the names of novelists Julio Cortázar, Mario Vargas Llosa, Carlos Fuentes and Gabriel García Márquez, along with the invention of a special brand of postmodern magic realism.\n\nAnother major 20th-century social events, the so-called sexual revolution is reflected in the modern novel. D.H. Lawrence's \"Lady Chatterley's Lover\" had to be published in Italy in 1928; British censorship lifted its ban as late as 1960. Henry Miller's \"Tropic of Cancer\" (1934) created the comparable US scandal. Transgressive fiction from Vladimir Nabokov's \"Lolita\" (1955) to Michel Houellebecq's \"Les Particules élémentaires\" (1998) entered a literary field that eventually led to more pornographic works such as Anne Desclos' \"Story of O\" (1954) to Anaïs Nin's \"Delta of Venus\" (1978).\n\nIn the second half of the 20th century, Postmodern authors subverted serious debate with playfulness, claiming that art could never be original, that it always plays with existing materials. The idea that language is self-referential was already an accepted truth in the world of pulp fiction. A postmodernist re-reads popular literature as an essential cultural production. Novels from Thomas Pynchon's \"The Crying of Lot 49\" (1966), to Umberto Eco's \"The Name of the Rose\" (1980) and \"Foucault's Pendulum\" (1989) made use of intertextual references.\n\nSee also: Thriller, Westerns and Speculative fiction\n\nWhile the reader of so-called serious literature will follow public discussions of novels, popular fiction production employs more direct and short-term marketing strategies by openly declarating of the work's genre. Popular novels are based entirely on the expectations for the particular genre, and this includes the creation of a series of novels with an identifiable brand name. e.g. the Sherlock Holmes series by Arthur Conan Doyle\n\nPopular literature holds a larger market share. Romance fiction had an estimated $1.375 billion share in the US book market in 2007. Inspirational literature/religious literature followed with $819 million, science fiction/fantasy with $700 million, mystery with $650 million and then classic literary fiction with $466 million.\n\nGenre literature might be seen as the successor of the early modern chapbook. Both fields share a focus on readers who are in search of accessible reading satisfaction. The 20th-century love romance is a successor of the novels Madeleine de Scudéry, Marie de La Fayette, Aphra Behn, and Eliza Haywood wrote from the 1640s into the 1740s. The modern adventure novel goes back to Daniel Defoe's \"Robinson Crusoe\" (1719) and its immediate successors. Modern pornography has no precedent in the chapbook market but originates in libertine and hedonistic belles lettres, of works like John Cleland's \"Fanny Hill\" (1749) and similar eighteenth century novels. Ian Fleming's \"James Bond\" is a descendant of the anonymous yet extremely sophisticated and stylish narrator who mixed his love affairs with his political missions in \"La Guerre d'Espagne\" (1707). Marion Zimmer Bradley's \"The Mists of Avalon\" is influenced by Tolkien, as well as Arthurian literature, including its 19th-century successors. Modern horror fiction also has no precedent on the market of chapbooks but goes back to the elitist market of early-19th-century Romantic literature. Modern popular science fiction has an even shorter history, from the 1860s.\n\nThe authors of popular fiction tend to advertise that they have exploited a controversial topic and this is a major difference between them and so-called elitist literature. Dan Brown, for example, discusses, on his website, the question whether his \"Da Vinci Code\" is an anti-Christian novel. And because authors of popular fiction have a fan community to serve, they can risk offending literary critic. However, the boundaries between popular and serious literature have blurred in recent years, with postmodernism and poststructuralism, as well as by adaptation of popular literary classics by the film and television industries.\n\nCrime became a major subject of 20th and 21st century genre novelists and crime fiction reflects the realities of modern industrialized societies. Crime is both a personal and public subject: criminals each have their personal motivations; detectives, see their moral codes challenged. Patricia Highsmith's thrillers became a medium of new psychological explorations. Paul Auster's \"New York Trilogy\" (1985–1986) is an example of experimental postmodernist literature based on this genre.\n\nFantasy is another major area of commercial fiction, and a major example is J.R.R. Tolkien's \"The Lord of the Rings\" (1954/55), a work originally written for young readers that became a major cultural artefact. Tolkien in fact revived the tradition of European epic literature in the tradition of Beowulf, the North Germanic Edda and the Arthurian Cycles.\n\nScience fiction, is another important type of genre fiction and it has developed in a variety of ways, ranging from the early, technological adventure Jules Verne had made fashionable in the 1860s, to Aldous Huxley's \"Brave New World\" (1932) about Western consumerism and technology. George Orwell's \"Nineteen Eighty-Four\" (1949) deals with totalitarianism and surveillance, among other matters, while Stanisław Lem, Isaac Asimov and Arthur C. Clarke produced modern classics which focus on the interaction between humans and machines. The surreal novels of Philip K Dick such as \"The Three Stigmata of Palmer Eldritch\" explore the nature of reality, reflecting the widespread recreational experimentation with drugs and cold-war paranoia of the 60's and 70's. Writers such as Ursula le Guin and Margaret Atwood explore feminist and broader social issues in their works. William Gibson, author of the cult classic \"Neuromancer\" (1984), is one of a new wave of authors who explore post-apocalyptic fantasies and virtual reality.\n\n\n\n\n"}
{"id": "218879", "url": "https://en.wikipedia.org/wiki?curid=218879", "title": "Pleonasm", "text": "Pleonasm\n\nPleonasm (; , ) is the use of more words or parts of words than are necessary or sufficient for clear expression: for example \"black darkness\" or \"burning fire\". Such redundancy is, by traditional rhetorical criteria, a manifestation of tautology. However, pleonasm may also be used for emphasis, or because the phrase has already become established in a certain form.\n\nMost often, \"pleonasm\" is understood to mean a word or phrase which is useless, clichéd, or repetitive, but a pleonasm can also be simply an unremarkable use of idiom. It can aid in achieving a specific linguistic effect, be it social, poetic, or literary. In particular, pleonasm sometimes serves the same function as rhetorical repetition—it can be used to reinforce an idea, contention, or question, rendering writing clearer and easier to understand. Further, pleonasm can serve as a redundancy check: If a word is unknown, misunderstood, or misheard, or the medium of communication is poor—a wireless telephone connection or sloppy handwriting—pleonastic phrases can help ensure that the entire meaning gets across even if some of the words get lost.\n\nSome pleonastic phrases are part of a language's idiom, like \"tuna fish\" and \"safe haven\" in American English. They are so common that their use is unremarkable and often even unnoticeable for native speakers, although in many cases the redundancy can be dropped with no loss of meaning.\n\nWhen expressing possibility, English speakers often use potentially pleonastic expressions such as \"It may be possible\" or \"maybe it's possible\", where both terms (verb \"may\" / adverb \"maybe\" and adjective \"possible\") have the same meaning under certain constructions. Many speakers of English use such expressions for possibility in general, such that most instances of such expressions by those speakers are in fact pleonastic. Others, however, use this expression only to indicate a distinction between ontological possibility and epistemic possibility, as in \"Both the ontological possibility of X under current conditions and the ontological impossibility of X under current conditions are epistemically possible\" (in logical terms, \"I am not aware of any facts inconsistent with the truth of proposition X, but I am likewise not aware of any facts inconsistent with the truth of the negation of X\"). The habitual use of the double construction to indicate possibility \"per se\" is far less widespread among speakers of most other languages (except in Spanish; see examples); rather, almost all speakers of those languages use one term in a single expression:\n\nIn a satellite-framed language like English, verb phrases containing particles that denote direction of motion are so frequent that even when such a particle is pleonastic, it seems natural to include it (e.g. \"enter into\").\n\nSome pleonastic phrases, when used in professional or scholarly writing, may reflect a standardized usage that has evolved or a meaning familiar to specialists but not necessarily to those outside that discipline. Such examples as \"null and void\", \"terms and conditions\", \"each and every\" are legal doublets that are part of legally operative language that is often drafted into legal documents. A classic example of such usage was that by the Lord Chancellor at the time (1864), Lord Westbury, in the English case of \" Gorely\", when he described a phrase in an Act as \"redundant and pleonastic\". Although this type of usage may be favored in certain contexts, it may also be disfavored when used gratuitously to portray false erudition, obfuscate, or otherwise introduce verbiage. This is especially so in disciplines where imprecision may introduce ambiguities (such as the natural sciences).\n\nIn addition, pleonasms can serve purposes external to meaning. For example, a speaker who is too terse is often interpreted as lacking ease or grace, because, in oral and sign language, sentences are spontaneously created without the benefit of editing. The restriction on the ability to plan often creates much redundancy. In written language, removing words not strictly necessary sometimes makes writing seem stilted or awkward, especially if the words are cut from an idiomatic expression.\n\nOn the other hand, as is the case with any literary or rhetorical effect, excessive use of pleonasm weakens writing and speech; words distract from the content. Writers wanting to conceal a thought or a purpose obscure their meaning with verbiage. William Strunk Jr. advocated concision in \"The Elements of Style\" (1918):\n\nYet, one has only to look at Baroque, Mannerist, and Victorian sources for different opinions.\n\n\nThere are two kinds of pleonasm: syntactic pleonasm and semantic pleonasm.\n\nSyntactic pleonasm occurs when the grammar of a language makes certain function words optional. For example, consider the following English sentences:\n\n\nIn this construction, the conjunction \"that\" is optional when joining a sentence to a verb phrase with \"know\". Both sentences are grammatically correct, but the word \"that\" is pleonastic in this case. By contrast, when a sentence is in spoken form and the verb involved is one of assertion, the use of \"that\" makes clear that the present speaker is making an indirect rather than a direct quotation, such that he is not imputing particular words to the person he describes as having made an assertion; the demonstrative adjective \"that\" also does not fit such an example. Also, some writers may use \"that\" for technical clarity reasons. In some languages, such as French, the word is not optional and should therefore not be considered pleonastic.\n\nThe same phenomenon occurs in Spanish with subject pronouns. Since Spanish is a null-subject language, which allows subject pronouns to be deleted when understood, the following sentences mean the same:\n\nIn this case, the pronoun ('I') is grammatically optional; both sentences mean \"I love you\" (however, they may not have the same tone or \"intention\"—this depends on pragmatics rather than grammar). Such differing but syntactically equivalent constructions, in many languages, may also indicate a difference in register.\n\nThe process of deleting pronouns is called \"pro-dropping\", and it also happens in many other languages, such as Korean, Japanese, Hungarian, Latin, Portuguese, Scandinavian languages, Swahili, some Slavic languages, and the Lao language.\n\nIn contrast, formal English requires an overt subject in each clause. A sentence may not need a subject to have valid meaning, but to satisfy the syntactic requirement for an explicit subject a pleonastic (or dummy pronoun) is used; only the first sentence in the following pair is acceptable English:\n\n\nIn this example the pleonastic \"it\" fills the subject function, however, it does not contribute any meaning to the sentence. The second sentence, which omits the pleonastic it is marked as ungrammatical although no meaning is lost by the omission. Elements such as \"it\", or \"there\" serving as empty subject markers are also called (syntactic) expletives, and also dummy pronouns. Compare:\n\n\nThe pleonastic (), expressing uncertainty in formal French, works as follows:\n\nTwo more striking examples of French pleonastic construction are the word translated as 'today', but originally meaning \"on the day of today\", and the phrase meaning 'What's that?' or 'What is it?', while literally it means \"What is it that it is?\".\n\nThere are examples of the pleonastic, or dummy, negative in English, such as the construction, heard in the New England region of the United States, in which the phrase \"So don't I\" is intended to have the same positive meaning as \"So do I.\"\n\nWhen Robert South said, \"It is a pleonasm, a figure usual in Scripture, by a multiplicity of expressions to signify one notable thing\", he was observing the Biblical Hebrew poetic propensity to repeat thoughts in different words, since written Biblical Hebrew was a comparatively early form of written language and was written using oral patterning, which has many pleonasms. In particular, very many verses of the Psalms are split into two halves, each of which says much the same thing in different words. The complex rules and forms of written language as distinct from spoken language were not as well-developed as they are today when the books making up the Old Testament were written. See also parallelism (rhetoric).\n\nThis same pleonastic style remains very common in modern poetry and songwriting (e.g., \"Anne, with her father / is out in the boat / riding the water / riding the waves / on the sea\", from Peter Gabriel's \"Mercy Street\").\n\n\nSemantic pleonasm is a question more of style and usage than of grammar. Linguists usually call this \"redundancy\" to avoid confusion with syntactic pleonasm, a more important phenomenon for theoretical linguistics. It usually takes one of two forms: Overlap or prolixity.\n\nOverlap: One word's semantic component is subsumed by the other:\n\nProlixity: A phrase may have words which add nothing, or nothing logical or relevant, to the meaning.\n\nAn expression like \"tuna fish\", however, might elicit one of many possible responses, such as:\n\nCareful speakers and writers to are aware of pleonasms, especially with cases such as \"tuna fish\", which is normally used only in some dialects of American English, and would sound strange in other variants of the language, and even odder in translation into other languages.\n\nSimilar situations are:\n\nNot all constructions that are typically pleonasms are so in all cases, nor are all constructions derived from pleonasms themselves pleonastic:\n\nMorphemes, not just words, can enter the realm of pleonasm: Some word-parts are simply optional in various languages and dialects. A familiar example to American English speakers would be the allegedly optional \"-al-\", probably most commonly seen in \"publically\" vs. \"publicly\"—both spellings are considered correct/acceptable in American English, and both pronounced the same, in this dialect, rendering the \"publically\" spelling pleonastic in US English; in other dialects it is \"required\", while it is quite conceivable that in another generation or so of American English it will be \"forbidden\". This treatment of words ending in \"-ic\", \"-ac\", etc., is quite inconsistent in US English—compare \"maniacally\" or \"forensically\" with \"stoicly\" or \"heroicly\"; \"forensicly\" doesn't look \"right\" in any dialect, but \"heroically\" looks internally redundant to many Americans. (Likewise, there are thousands of mostly American Google search results for \"eroticly\", some in reputable publications, but it does not even appear in the 23-volume, 23,000-page, 500,000-definition \"Oxford English Dictionary\" (\"OED\"), the largest in the world; and even American dictionaries give the correct spelling as \"erotically\".) In a more modern pair of words, Institute of Electrical and Electronics Engineers dictionaries say that \"electric\" and \"electrical\" mean exactly the same thing. However, the usual adverb form is \"electrically\". (For example, \"The glass rod is electrically charged by rubbing it with silk\".)\n\nSome (mostly US-based) prescriptive grammar pundits would say that the \"-ly\" not \"-ally\" form is \"correct\" in any case in which there is no \"-ical\" variant of the basic word, and vice versa; i.e. \"maniacally\", not \"maniacly\", is correct because \"maniacal\" is a word, while \"publicly\", not \"publically\", must be correct because \"publical\" is (arguably) not a real word (it does not appear in the \"OED\"). This logic is in doubt, since most if not all \"-ical\" constructions arguably are \"real\" words and most have certainly occurred more than once in \"reputable\" publications, and are also immediately understood by any educated reader of English even if they \"look funny\" to some, or do not appear in popular dictionaries. Additionally, there are numerous examples of words that have very widely accepted extended forms that have skipped one or more intermediary forms, e.g. \"disestablishmentarian\" in the absence of \"disestablishmentary\" (which does not appear in the \"OED\"). At any rate, while some US editors might consider \"-ally\" vs. \"-ly\" to be pleonastic in some cases, the majority of other English speakers would not, and many \"-ally\" words are not pleonastic to anyone, even in American English.\nThe most common definitely pleonastic morphological usage in English is \"irregardless\", which is very widely criticized as being a non-word. The standard usage is \"regardless\", which is already negative; adding the additional negative \"ir-\" is interpreted by some as logically reversing the meaning to \"with regard to/for\", which is certainly not what the speaker intended to convey. (According to most dictionaries that include it, \"irregardless\" appears to derive from confusion between \"regardless\" and \"irrespective\", which have overlapping meanings.)\n\nThere are several instances in Chinese vocabulary where pleonasms and cognate objects are present. Their presence usually indicate the plural form of the noun or the noun in formal context.\n\nIn some instances, the pleonasmic form of the verb is used with the intention as an emphasis to one meaning of the verb, isolating them from their idiomatic and figurative uses. But over time, the pseudo-object, which sometimes repeats the verb, is almost inherently coupled with the it.\n\nFor example, the word ('to sleep') is an intransitive verb, but may express different meaning when coupled with objects of prepositions as in \"to sleep with\". However, in Mandarin, is usually coupled with a pseudo-character , yet it is not entirely a cognate object, to express the act of resting.\n\n\nOne can also find a way around this verb, using another one which does not is used to express idiomatic expressions nor necessitate a pleonasm, because it only has one meaning:\nNevertheless, is a verb used in high-register diction, just like English verbs with Latin roots.\n\nThere is no relationship found between Chinese and English regarding verbs that can take pleonasms and cognate objects. Although the verb \"to sleep\" may take a cognate object as in \"sleep a restful sleep\", it is a pure coincidence, since verbs of this form are more common in Chinese than in English; and when the English verb is used without the cognate objects, its diction is natural and its meaning is clear in every level of diction, as in \"I want to sleep\" and \"I want to have a rest\".\n\nIn some cases, the redundancy in meaning occurs at a syntactic level above the word, such as at the phrase level:\n\nThe redundancy of these two well-known statements is deliberate, for humorous effect. (See Yogi_Berra#Yogi-isms.) But one does hear educated people say \"my predictions about the future of politics\" for \"my predictions about politics\", which are equivalent in meaning. While predictions are necessarily about the future (at least in relation to the time the prediction was made), the nature of this future can be subtle (e.g., \"I predict that he died a week ago\"—the prediction is about future discovery or proof of the date of death, not about the death itself). Generally \"the future\" is assumed, making most constructions of this sort pleonastic. The latter humorous quote above about not making predictions – by Yogi Berra – is not really a pleonasm, but rather an ironic play on words.\n\nBut \"It's \"déjà vu\" all over again.\" could mean that there was earlier another \"déjà vu\" of the same event or idea, which has now arisen for a third time.\n\nRedundancy, and \"useless\" or \"nonsensical\" words (or phrases, or morphemes) can also be inherited by one language from the influence of another, and are not pleonasms in the more critical sense, but actual changes in grammatical construction considered to be required for \"proper\" usage in the language or dialect in question. Irish English, for example, is prone to a number of constructions that non-Irish speakers find strange and sometimes directly confusing or silly:\n\n\nAll of these constructions originate from the application of Irish Gaelic grammatical rules to the English dialect spoken, in varying particular forms, throughout the island.\n\nSeemingly \"useless\" additions and substitutions must be contrasted with similar constructions that are used for stress, humor, or other intentional purposes, such as:\n\n\nThe latter of these is a result of Yiddish influences on modern English, especially East Coast US English.\n\nSometimes editors and grammatical stylists will use \"pleonasm\" to describe simple wordiness. This phenomenon is also called prolixity or logorrhea. Compare:\n\nor even:\n\nThe reader or hearer does not have to be told that loud music has a sound, and in a newspaper headline or other abbreviated prose can even be counted upon to infer that \"burglary\" is a proxy for \"sound of the burglary\" and that the music necessarily must have been loud to drown it out, unless the burglary was relatively quiet (this is not a trivial issue, as it may affect the legal culpability of the person who played the music); the word \"loud\" may imply that the music should have been played quietly if at all. Many are critical of the excessively abbreviated constructions of \"headline-itis\" or \"newsspeak\", so \"loud [music]\" and \"sound of the [burglary]\" in the above example should probably not be properly regarded as pleonastic or otherwise genuinely redundant, but simply as informative and clarifying.\n\nProlixity is also used simply to obfuscate, confuse, or euphemize and is not necessarily redundant or pleonastic in such constructions, though it often is. \"Post-traumatic stress disorder\" (shell shock) and \"pre-owned vehicle\" (used car) are both tumid euphemisms but are not redundant. Redundant forms, however, are especially common in business, political, and even academic language that is intended to sound impressive (or to be vague so as to make it hard to determine what is actually being promised, or otherwise misleading). For example: \"This quarter, we are presently focusing with determination on an all-new, innovative integrated methodology and framework for rapid expansion of customer-oriented external programs designed and developed to bring the company's consumer-first paradigm into the marketplace as quickly as possible.\"\n\nIn contrast to redundancy, an oxymoron results when two seemingly contradictory words are adjoined.\n\nRedundancies sometimes take the form of foreign words whose meaning is repeated in the context:\n\nThese sentences use phrases which mean, respectively, \"the restaurant restaurant\", \"the tar tar\", \"with juice sauce\" and so on. However, many times these redundancies are necessary—especially when the foreign words make up a proper noun as opposed to a common one. For example, \"We went to Il Ristorante\" is acceptable provided the audience can infer that it is a restaurant (if they understand Italian and English it might likely, if spoken rather than written, be misinterpreted as a generic reference and not a proper noun, leading the hearer to ask \"Which ristorante do you mean?\" Such confusions are common in richly bilingual areas like Montreal or the American Southwest when people mix phrases from two languages at once). But avoiding the redundancy of the Spanish phrase in the second example would only leave an awkward alternative: \"La Brea pits are fascinating\".\n\nMost find it best to not even drop articles when using proper nouns made from foreign languages:\n\n\nThis is also similar to the treatment of definite and indefinite articles in titles of books, films, etc. where the article can—some would say \"must\"—be present where it would otherwise be \"forbidden\":\n\n\nSome cross-linguistic redundancies, especially in placenames, occur because a word in one language became the title of a place in another (e.g., the Sahara Desert—\"Sahara\" is an English approximation of the word for \"deserts\" in Arabic). A supposed extreme example is Torpenhow Hill in Cumbria, if etymologized as meaning \"hill\" in the language of each of the cultures that have lived in the area during recorded history, could be translated as \"Hillhillhill Hill\". See the List of tautological place names for many more examples.\n\nAcronyms can also form the basis for redundancies; this is known humorously as RAS syndrome (for Redundant Acronym Syndrome Syndrome):\n\n\nIn all the examples listed above, the word after the acronym repeats a word represented in the acronym—respectively, \"Personal Identification Number number\", \"Automated Teller Machine machine\", \"Random Access Memory memory\", \"Human Immunodeficiency Virus virus\", \"Content Management System system\". (See RAS syndrome for many more examples.) The expansion of an acronym like PIN or HIV may be well known to English speakers, but the acronyms themselves have come to be treated as words, so little thought is given to what their expansion is (and \"PIN\" is also pronounced the same as the word \"pin\"; disambiguation is probably the source of \"PIN number\"; \"SIN number\" for \"Social Insurance Number number\" is a similar common phrase in Canada.) But redundant acronyms are more common with technical (e.g. computer) terms where well-informed speakers recognize the redundancy and consider it silly or ignorant, but mainstream users might not, since they may not be aware or certain of the full expansion of an acronym like \"RAM\".\n\nSome redundancies are simply typographical. For instance, when a short inflexional word like \"the\" occurs at the end of a line, it is very common to accidentally repeat it at the beginning of the line, and a large number of readers would not even notice it.\n\nCarefully constructed expressions, especially in poetry and political language, but also some general usages in everyday speech, may appear to be redundant but are not. This is most common with cognate objects (a verb's object that is cognate with the verb):\n\nOr, a classic example from Latin:\n\nThe words need not be etymologically related, but simply conceptually, to be considered an example of cognate object:\n\nSuch constructions are not actually redundant (unlike \"She slept a sleep\" or \"We wept tears\") because the object's modifiers provide additional information. A rarer, more constructed form is polyptoton, the stylistic repetition of the same word or words derived from the same root:\n\n\nAs with cognate objects, these constructions are not redundant because the repeated words or derivatives cannot be removed without removing meaning or even destroying the sentence, though in most cases they could be replaced with non-related synonyms at the cost of style (e.g., compare \"The only thing we have to fear is terror\".)\n\nIn many cases of semantic pleonasm, the status of a word as pleonastic depends on context. The relevant context can be as local as a neighboring word, or as global as the extent of a speaker's knowledge. In fact, many examples of redundant expressions are not inherently redundant, but can be redundant if used one way, and are not redundant if used another way. The \"up\" in \"climb up\" is not always redundant, as in the example \"He climbed up and then fell down the mountain.\" Many other examples of pleonasm are redundant only if the speaker's knowledge is taken into account. For example, most English speakers would agree that \"tuna fish\" is redundant because tuna is a kind of fish. However, given the knowledge that \"tuna\" can also refer a kind of edible prickly pear, the \"fish\" in \"tuna fish\" can be seen as non-pleonastic, but rather a disambiguator between the fish and the prickly pear.\n\nConversely, to English speakers who do not know Spanish, there is nothing redundant about \"the La Brea tar pits\" because the name \"La Brea\" is opaque: the speaker does not know that it is Spanish for \"the tar\". Similarly, even though scuba stands for \"self-contained underwater breathing apparatus\", a phrase like \"the scuba gear\" would probably not be considered pleonastic because \"scuba\" has been reanalyzed into English as a simple word, and not an acronym suggesting the pleonastic word sequence \"apparatus gear\". (Most do not even know that it is an acronym and do not spell it SCUBA or S.C.U.B.A. Similar examples are radar and laser.)\n\n"}
{"id": "24111141", "url": "https://en.wikipedia.org/wiki?curid=24111141", "title": "Post-socialist art", "text": "Post-socialist art\n\nPost-socialist art (or post-communist art) is a term used in analysis of art arriving from post-socialist (post-communist) countries taken as different in their nature from Western, Postmodern art.\n\nCrucial for such art is that:\n\nSocialist art started opening to Western markets already in the eighties, introducing Slovenian collective IRWIN, Belgrade Malevich (aka Goran Djordjevic) and other artists Peter Weibel named Retro-avantgard. In the second wave of nineties it was followed by a weakened political versions which were massively funded (Most of such art was financed by George Soros, similarly to Socialist realism which Lead thinkers as Miško Šuvaković to name such art Soros Realism.\n\n"}
{"id": "20576463", "url": "https://en.wikipedia.org/wiki?curid=20576463", "title": "Procedural drama", "text": "Procedural drama\n\nA procedural drama is a genre of television programming which focuses on how crimes are solved or some other aspect of a law enforcement agency, legislative body, or court of law. Some dramas include a lab or conference room with high-tech or state-of-the-art equipment where the main characters meet to work out the problem. Shows usually have an episodic format that does not require the viewer to have seen previous episodes. Episodes typically have a self-contained, also referred to as stand-alone, plot that is introduced and resolved within the same episode.\nThe procedural format is popular around the world. In 2011, the director of a TV consultancy said, \"The continuing trend is for procedurals because they use a predictable structure.\" Due to their stand-alone episodic nature, they are more accessible to new viewers than serials. Self-contained episodes also make it easier for viewers to return to a show if they have missed some episodes. In general, procedural dramas can usually be re-run with little concern for episode order.\n\nProcedurals are often criticized for being . Procedurals are also generally less character-driven than serialized shows. However, some procedurals have more character emphasis than is typical of the format. Some may occasionally feature a storyline stretching over several episodes (often called a story arc).\n\nA popular variant is the police procedural.\n\n\n"}
{"id": "52898110", "url": "https://en.wikipedia.org/wiki?curid=52898110", "title": "Queer of color critique", "text": "Queer of color critique\n\nQueer of color critique is a methodology that recognizes the intersections of race, gender, class, sexuality, capital, and nation, and disidentifies with the universality of social categories present in canonical sociology and historical materialism. Roderick Ferguson is credited with coining this term in his 2004 book \"Aberrations in Black, \"and draws from woman of color feminism, postcolonial studies, queer theory and African American studies. In his critique of canonical sociology, Ferguson argues that racialized heteronormativity and heteropatriarchy has played a conspicuous role in shaping sociology and social policy, and recognizes its intersection with revolutionary nationalism. Queer of color critique operates as a method for building unlikely coalitions across different identity categories. In framing queer of color critique, Ferguson draws from Barbara Smith and the Combahee River Collective's use of coalitional politics to address gender, race, and sexuality in context with capitalist expansion.\n\nQueer of color critique has been taken up by multiple scholars as an attempt at a more intersectional framework on which to build and extend their work in various, sometimes intersecting academic subjects.\n\nThough most references at the intersection of queerness and indigeneity fall on Two-Spirit identity and ideas surrounding it, queer of color critique extends the discussion to settler colonialism, future potentialities of Native identity and life, and general discourse about how Native Studies as an academic study can benefit from a sort of \"queering\". According to scholar Andrea Smith, looking at indigeneity from this perspective questions the limitations of a \"subjectless\" or \"postidentity\" analysis in regards to the shedding of a particular ethnic identity, which in itself has roots in colonialist and nationalist ideology. On the other hand, even this sort of critique does not fully acknowledge the extent of the absence of indigeneity in the context of direct and indirect investment in settler colonialism by those of color doing the critiquing.\n\nThe unraveling of colonialist ideology,—the belief in a normative society—according to scholar Emma Perez, is necessary to fully understand national histories and identities, specifically of queer individuals.\n\nQueer diasporic critique can be considered an extension of and complement to queer of color critique in that it considers ethnic and cultural identity as an underlying context when analyzing and critiquing arguments based on Eurocentric, white centered queer theories, as well as critiquing the heteronormativity of area studies. Oftentimes, these ideas are connected to ideas of nationhood and national identity. In the words of scholar Gayatri Gopinath, \"While both queer of color and queer diasporic analysis are part of a collective endeavor to reshape queer studies through a thorough engagement with questions of race, nationalism, and transnationalism, it may also be useful to explore some of the points at which the interventions and emphases of each project both intersect and diverge.\" In this respect, critique emphasizing diaspora tends to focus more on a \"global restructuring of capital and its attendant gender and sexual hierarchies\" and the creation of \"home\" in regards to diaspora and transnationalism.\n\nAn example of queer of color critique in practice can be seen in the analysis of queer Muslims in Europe done by scholar Fatima El-Tayeb, which touches on the larger themes explored in queer theory. Among these is the idea of \"coming-out\" as a person of diaspora, which challenges the currently held notion of its role in the creation of a \"normative, healthy and desirable LGBT identity\". Another describes how the idea of Islamophobia permeates into the intersectional oppressions faced by queer Muslims in the west. The ideas of migration and \"home\" are also critiqued in that the ethnic migrant laborer lives amongst an \"increasingly segregated, criminalized and policed multi-ethnic population of color\". These queer diasporic and queer of color critiques therefore act as a lens through which to view homonormativity and the various facets through which is functions.\n\nAs a type of community manifestation of this type of critique, the queer of color activist group takes an intersectional, colored approach to queer activism. This helps undermine binaries such as the \"Muslim/European dichotomy to the normative coming out narrative\", which, according to El-Tayeb, perpetuate homonormativity and racism.\n\nSome notable scholars who have incorporated queer of color critique into their work are Roderick Ferguson, Jesus Values-Morales, Andrea Smith, Gayatri Gopinath, Fatima El-Tayeb, Martin Manalansan IV, Juana María Rodríguez, José Esteban Muñoz, Emma Perez, Edward Brockenbrough, Salvador Vidal-Ortiz, Amy Villarejo, Jasbir Puar, Scott Lauria Morgensen, Kevin K. Kumashiro, Lawrence La Fountain-Stokes, Chandan Reddy, Jennifer C. Nash, and others.\n\n"}
{"id": "711230", "url": "https://en.wikipedia.org/wiki?curid=711230", "title": "Regional geography", "text": "Regional geography\n\nRegional geography is a major branch of geography. It focuses on the interaction of different cultural and natural geofactors in a specific land or landscape, while its counterpart, systematic geography, concentrates on a specific geofactor at the global level. \n\nAttention is paid to unique characteristics of a particular region such as natural elements, human elements, and regionalization which covers the techniques of delineating space into regions. Rooted in the tradition of the German speaking countries, the two pillars of regional geography are the idiographic study of \"Länder\" or spatial individuals (specific places, countries, continents) and the typological study of \"Landschaften\" or spatial types (landscapes such as coastal regions, mountain regions, border regions, etc.).\n\nRegional geography is also a certain approach to geographical study, comparable to quantitative geography or critical geography. This approach prevailed during the second half of the 19th century and the first half of the 20th century, a period when then regional geography paradigm was central within the geographical sciences. It was later criticised for its descriptiveness and the lack of theory. Strong criticism was leveled against it in particular during the 1950s and the quantitative revolution. Main critics were G. H. T. Kimble and Fred K. Schaefer. The regional geography paradigm has influenced many other geographical sciences, including economic geography and geomorphology. Regional geography is still taught in some universities as a study of the major regions of the world, such as Northern and Latin America, Europe, and Asia and their countries. In addition, the notion of a city-region approach to the study of geography, underlining urban-rural interactions, gained credence since the mid-1980s. Some geographers have also attempted to reintroduce a certain amount of regionalism since the 1980s. This involves a complex definition of regions and their interactions with other scales.\n\nNotable figures in regional geography were Alfred Hettner in Germany, with his concept of chorology; Paul Vidal de la Blache in France, with the possibilism approach (possibilism being a softer notion than environmental determinism); and, in the United States, Richard Hartshorne with his concept of areal differentiation. The school of Carl O. Sauer, strongly influenced by Alfred Hettner and Paul Vidal de la Blache, is also seen as regional geography in its broadest sense.\n\n"}
{"id": "50207524", "url": "https://en.wikipedia.org/wiki?curid=50207524", "title": "Respective case", "text": "Respective case\n\nThe respective case (also called the dedative or relative) is a grammatical case invented by J. R. R. Tolkien in his constructed language Quenya (the elven language of the book \"The Lord of the Rings\"). It is not clear if this case is used with prepositions, transitive or intransitive verbs, or has a more general use. Based on a few examples, one linguist has deduced the possibility that it is used as a kind of locative (e.g.: \"i coa i taures / i coa i tauressë\" the house in the forest).\n\nAnother possible use is to translate expressions with the preposition “about.” For example, in translating “He told me about the invasion,” the Quenya word for “invasion” might appear in the respective/dedative. The seeming absence of any other case being used to translate this preposition lends support to this conjecture. In the constructed language Ithkuil the case corresponding to the preposition about is called Referential case\n"}
{"id": "143284", "url": "https://en.wikipedia.org/wiki?curid=143284", "title": "Restoration comedy", "text": "Restoration comedy\n\n\"Restoration comedy\" is English comedy written and performed in the Restoration period from 1660 to 1710. Comedy of manners is used as a synonym of Restoration comedy. After public stage performances had been banned for 18 years by the Puritan regime, the re-opening of the theatres in 1660 signalled a renaissance of English drama. Sexually explicit language was encouraged by King Charles II (1660–1685) personally and by the rakish style of his court. Historian George Norman Clark argues:\n\nThe socially diverse audiences included both aristocrats, their servants and hangers-on, and a substantial middle-class segment. These playgoers were attracted to the comedies by up-to-the-minute topical writing, by crowded and bustling plots, by the introduction of the first professional actresses, and by the rise of the first celebrity actors. This period saw the first professional female playwright, Aphra Behn.\n\n Charles II was an active and interested patron of the drama. Soon after his restoration, in 1660, he granted exclusive play-staging rights, so-called Royal patents, to the King's Company and the Duke's Company, led by two middle-aged Caroline playwrights, Thomas Killigrew and William Davenant. The patentees scrambled for performance rights to the previous generation's Jacobean and Caroline plays, which were the first necessity for economic survival before any new plays existed. Their next priority was to build new, splendid patent theatres in Drury Lane and Dorset Gardens, respectively. Striving to outdo each other in magnificence, Killigrew and Davenant ended up with quite similar theatres, both designed by Christopher Wren, both optimally provided for music and dancing, and both fitted with moveable scenery and elaborate machines for thunder, lightning, and waves.\n\nThe dramatists of the Restoration renounced the tradition of satire, as recently embodied by Ben Jonson, and devoted themselves to the comedy of manners, which uncritically accepted the social code of the upper class.\n\nThe audience of the early Restoration period was not exclusively courtly, as has sometimes been supposed, but it was quite small and could barely support two companies. There was no untapped reserve of occasional playgoers. Ten consecutive performances constituted a smash hit. This closed system forced playwrights to be extremely responsive to popular taste. Fashions in the drama would change almost week by week rather than season by season, as each company responded to the offerings of the other, and new plays were urgently sought. The King's Company and the Duke's Company vied with one another for audience favour, for popular actors, and for new plays, and in this hectic climate the new genres of heroic drama, pathetic drama, and Restoration comedy were born and flourished.\n\nBoth the quantity and quality of the drama suffered when in 1682 the more successful Duke's Company absorbed the struggling King's Company, and the amalgamated \"United Company\" was formed. The production of new plays dropped off sharply in the 1680s, affected by both the monopoly and the political situation (see Decline of comedy below). The influence and the incomes of the actors dropped, too. In the late 80s, predatory investors (\"Adventurers\") converged on the United Company, while management was taken over by the lawyer Christopher Rich. Rich attempted to finance a tangle of \"farmed\" shares and sleeping partners by slashing salaries and, dangerously, by abolishing the traditional perks of senior performers, who were stars with the clout to fight back.\n\nThe company owners, wrote the young United Company employee Colley Cibber, \"who had made a monopoly of the stage, and consequently presum'd they might impose what conditions they pleased upon their people, did not consider that they were all this while endeavouring to enslave a set of actors whom the public were inclined to support.\" Performers like the legendary Thomas Betterton, the tragedienne Elizabeth Barry, and the rising young comedian Anne Bracegirdle had the audience on their side and, in the confidence of this, they walked out.\n\nThe actors gained a Royal \"licence to perform\", thus bypassing Rich's ownership of both the original Duke's and King's Company patents from 1660, and formed their own cooperative company. This unique venture was set up with detailed rules for avoiding arbitrary managerial authority, regulating the ten actors' shares, the conditions of salaried employees, and the sickness and retirement benefits of both categories. The cooperative had the good luck to open in 1695 with the première of William Congreve's famous \"Love For Love\" and the skill to make it a huge box-office success.\n\nLondon again had two competing companies. Their dash to attract audiences briefly revitalised Restoration drama, but also set it on a fatal downhill slope to the lowest common denominator of public taste. Rich's company notoriously offered Bartholomew Fair-type attractions – high kickers, jugglers, ropedancers, performing animals – while the co-operating actors, even as they appealed to snobbery by setting themselves up as the only legitimate theatre company in London, were not above retaliating with \"prologues recited by boys of five, and epilogues declaimed by ladies on horseback\".\n\nRestoration comedy was strongly influenced by the introduction of the first professional actresses. Before the closing of the theatres, all female roles had been taken by boy players, and the predominantly male audiences of the 1660s and 1670s were curious, censorious, and delighted at the novelty of seeing real women engage in risqué repartee and take part in physical seduction scenes. Samuel Pepys refers many times in his famous diary to visiting the playhouse to watch or re-watch the performance of particular actresses, and to how much he enjoys these experiences.\n\nDaringly suggestive comedy scenes involving women became especially common, although of course Restoration actresses were, just like male actors, expected to do justice to all kinds and moods of plays. (Their role in the development of Restoration tragedy is also important, compare She-tragedy.)\n\nA new speciality introduced almost as early as the actresses was the breeches role, which called for an actress to appear in male clothes (breeches being tight-fitting knee-length pants, the standard male garment of the time), for instance to play a witty heroine who disguises herself as a boy to hide, or to engage in escapades disallowed to girls. A quarter of the plays produced on the London stage between 1660 and 1700 contained breeches roles. Playing these cross-dressing roles, women behaved with the freedom society allowed to men, and some feminist critics, such as Jacqueline Pearson, regard them as subversive of conventional gender roles and empowering for female members of the audience. Elizabeth Howe has objected that the male disguise, when studied in relation to playtexts, prologues, and epilogues, comes out as \"little more than yet another means of displaying the actress as a sexual object\" to male patrons, by showing off her body, normally hidden by a skirt, outlined by the male outfit.\n\nSuccessful Restoration actresses included Charles II's mistress Nell Gwyn, the tragedienne Elizabeth Barry who was famous for her ability to \"move the passions\" and make whole audiences cry, the 1690s comedian Anne Bracegirdle, and Susanna Mountfort (a.k.a. Susanna Verbruggen), who had many breeches roles written especially for her in the 1680s and 90s. Letters and memoirs of the period show that both men and women in the audience greatly relished Mountfort's swaggering, roistering impersonations of young women wearing breeches and thereby enjoying the social and sexual freedom of the male Restoration rake.\n\nDuring the Restoration period, both male and female actors on the London stage became for the first time public personalities and celebrities. Documents of the period show audiences being attracted to performances by the talents of particular actors as much as by particular plays, and more than by authors (who seem to have been the least important draw, no performance being advertised by author until 1699). Although the playhouses were built for large audiences—the second Drury Lane theatre from 1674 held 2000 patrons – they were of compact design, and an actor's charisma could be intimately projected from the thrust stage.\n\nWith two companies competing for their services from 1660 to 1682, star actors were able to negotiate star deals, comprising company shares and benefit nights as well as salaries. This advantageous situation changed when the two companies were amalgamated in 1682, but the way the actors rebelled and took command of a new company in 1695 is in itself an illustration of how far their status and power had developed since 1660.\n\nThe greatest fixed stars among Restoration actors were Elizabeth Barry (\"Famous Mrs Barry\" who \"forc 'd Tears from the Eyes of her Auditory\") and Thomas Betterton, both of them active in organising the actors' revolt in 1695 and both original patent-holders in the resulting actors' cooperative.\n\nBetterton played every great male part there was from 1660 into the 18th century. After watching \"Hamlet\" in 1661, Samuel Pepys reports in his diary that the young beginner Betterton \"did the prince's part beyond imagination.\" Betterton's expressive performances seem to have attracted playgoers as magnetically as did the novelty of seeing women on the stage. He was soon established as the leading man of the Duke's Company, and played Dorimant, the seminal irresistible Restoration rake, at the première of George Etherege's \"Man of Mode\" (1676). Betterton's position remained unassailable through the 1680s, both as the leading man of the United Company and as its stage manager and \"de facto\" day-to-day leader. He remained loyal to Rich longer than many of his co-workers, but eventually it was he who headed the actors' walkout in 1695, and who became the acting manager of the new company.\n\nVariety and dizzying fashion changes are typical of Restoration comedy. Even though the \"Restoration drama\" unit taught to college students is likely to be telescoped in a way that makes the plays all sound contemporary, scholars now have a strong sense of the rapid evolution of English drama over these forty years and of its social and political causes. The influence of theatre company competition and playhouse economics is also acknowledged.\nRestoration comedy peaked twice. The genre came to spectacular maturity in the mid-1670s with an extravaganza of aristocratic comedies. Twenty lean years followed this short golden age, although the achievement of Aphra Behn in the 1680s is to be noted. In the mid-1690s a brief second Restoration comedy renaissance arose, aimed at a wider audience. The comedies of the golden 1670s and 1690s peak times are extremely different from each other. An attempt is made below to illustrate the generational taste shift by describing \"The Country Wife\" (1675) and \"The Provoked Wife\" (1697) in some detail. These two plays differ from each other in some typical ways, just as a Hollywood movie of the 1950s differs from one of the 1970s. The plays are not, however, offered as being \"typical\" of their decades. Indeed, there exist no typical comedies of the 1670s or the 1690s; even within these two short peak-times, comedy types kept mutating and multiplying.\n\nThe drama of the 1660s and 1670s was vitalised by the competition between the two patent companies created at the Restoration, as well as by the personal interest of Charles II, and the comic playwrights rose to the demand for new plays. They stole freely from the contemporary French and Spanish stage, from English Jacobean and Caroline plays, and even from Greek and Roman classical comedies, and combined the looted plotlines in adventurous ways. Resulting differences of tone in a single play were appreciated rather than frowned on, as the audience prized \"variety\" within as well as between plays. Early Restoration audiences had little enthusiasm for structurally simple, well-shaped comedies such as those of Molière; they demanded bustling, crowded multi-plot action and fast pace. Even a splash of high heroic drama might be thrown in to enrich the comedy mix, as in George Etherege's \"Love in a Tub\" (1664), which has one heroic verse \"conflict between love and friendship\" plot, one urbane wit comedy plot, and one burlesque pantsing plot. (\"See illustration, top right\".) Such incongruities contributed to Restoration comedy being held in low esteem in the 18th, 19th and early 20th centuries, but today the early Restoration total theatre experience is again valued on the stage, as well as by postmodern academic critics.\n\nThe unsentimental or \"hard\" comedies of John Dryden, William Wycherley, and George Etherege reflected the atmosphere at Court, and celebrated with frankness an aristocratic macho lifestyle of unremitting sexual intrigue and conquest. The Earl of Rochester, real-life Restoration rake, courtier and poet, is flatteringly portrayed in Etherege's \"The Man of Mode\" (1676) as a riotous, witty, intellectual, and sexually irresistible aristocrat, a template for posterity's idea of the glamorous Restoration rake (actually never a very common character in Restoration comedy). Wycherley's \"The Plain Dealer\" (1676), a variation on the theme of Molière's \"Le Misanthrope\", was highly regarded for its uncompromising satire and earned Wycherley the appellation \"Plain Dealer\" Wycherley or \"Manly\" Wycherley, after the play's main character Manly. The single play that does most to support the charge of obscenity levelled then and now at Restoration comedy is probably Wycherley's \"The Country Wife\" (1675). \n\n\"The Country Wife\" has three interlinked but distinct plots, which each project sharply different moods:\n\n1. Horner's impotence trick provides the main plot and the play's organising principle. The upper-class town rake Horner mounts a campaign for seducing as many respectable ladies as possible, first spreading a false rumour of his own impotence, to be allowed where no complete man may go. The trick is a great success and Horner has sex with many married ladies of virtuous reputation, whose husbands are happy to leave him alone with them. In one famously outrageous scene, the \"China scene\", sexual intercourse is assumed to take place repeatedly just off stage, where Horner and his mistresses carry on a sustained double entendre dialogue purportedly about Horner's china collection. \"The Country Wife\" is driven by a succession of near-discoveries of the truth about Horner's sexual prowess (and thus the truth about the respectable ladies), from which he extricates himself by quick thinking and good luck. Horner never becomes a reformed character, but keeps his secret to the end and is assumed to go on merrily reaping the fruits of his planted misinformation, past the last act and beyond.\n\n2. The married life of Pinchwife and Margery is based on Molière's \"School for Wives\". Pinchwife is a middle-aged man who has married an ignorant young country girl in the hope that she will not know to cuckold him. However, Horner teaches her, and Margery cuts a swathe through the sophistications of London marriage without even noticing them. She is enthusiastic about the virile handsomeness of town gallants, rakes, and especially theatre actors (such self-referential stage jokes were nourished by the new higher status of actors), and keeps Pinchwife in a state of continual horror with her plain-spokenness and her interest in sex. A running joke is the way Pinchwife's pathological jealousy always leads him into supplying Margery with the very type of information he wishes her not to have.\n\n3. The courtship of Harcourt and Alithea is a comparatively uplifting love story in which the witty Harcourt wins the hand of Pinchwife's sister Alithea from the hands of the Upper-class town snob Sparkish, to whom she was engaged until discovering he loved her only for her money.\n\nWhen the two companies were amalgamated in 1682 and the London stage became a monopoly, both the number and the variety of new plays being written dropped sharply. There was a swing away from comedy to serious political drama, reflecting preoccupations and divisions following on the Popish Plot (1678) and the Exclusion Crisis (1682). The few comedies produced also tended to be political in focus, the Whig dramatist Thomas Shadwell sparring with the Tories John Dryden and Aphra Behn. Behn's unique achievement as an early professional woman writer has been the subject of much recent study.\n\nDuring the second wave of Restoration comedy in the 1690s, the \"softer\" comedies of William Congreve and John Vanbrugh reflected mutating cultural perceptions and great social change. The playwrights of the 1690s set out to appeal to more socially mixed audiences with a strong middle-class element, and to female spectators, for instance by moving the war between the sexes from the arena of intrigue into that of marriage. The focus in comedy is less on young lovers outwitting the older generation, more on marital relations after the wedding bells. Thomas Southerne's dark \"The Wives' Excuse\" (1691) is not yet very \"soft\": it shows a woman miserably married to the fop Friendall, everybody's friend, whose follies and indiscretions undermine her social worth, since her honour is bound up in his. Mrs Friendall is pursued by a would-be lover, a matter-of-fact rake devoid of all the qualities that made Etherege's Dorimant charming, and she is kept from action and choice by the unattractiveness of all her options. All the humour of this \"comedy\" is in the subsidiary love-chase and fornication plots, none in the main plot.\n\nIn Congreve's \"Love for Love\" (1695) and \"The Way of the World\" (1700), the \"wit duels\" between lovers typical of 1670s comedy are underplayed. The give-and-take set pieces of couples still testing their attraction for each other have mutated into witty prenuptial debates on the eve of marriage, as in the famous \"Proviso\" scene in \"The Way of the World\" (1700). Vanbrugh's \"The Provoked Wife\" (1697) follows in the footsteps of Southerne's \"Wives' Excuse\", with a lighter touch and more humanly recognisable characters.\n\n\"The Provoked Wife\" is something of a Restoration problem play in its attention to the subordinate legal position of married women and the complexities of \"divorce\" and separation, issues that had been highlighted in the mid-1690s by some notorious cases before the House of Lords (see Stone).\n\nSir John Brute in \"The Provoked Wife\" is tired of matrimony. He comes home drunk every night and is continually rude and insulting to his wife. She is meanwhile being tempted to embark upon an affair with the witty and faithful Constant. Divorce is not an option for either of the Brutes at this time, but forms of legal separation have recently come into existence, and would entail a separate maintenance to the wife. Such an arrangement would not allow remarriage. Still, muses Lady Brute, in one of many discussions with her niece Bellinda, \"These are good times. A woman may have a gallant and a separate maintenance too.\"\n\nBellinda is at the same time being grumpily courted by Constant's friend Heartfree, who is surprised and dismayed to find himself in love with her. The bad example of the Brutes is a constant warning to Heartfree to not marry.\n\n\"The Provoked Wife\" is a talk play, with the focus less on love scenes and more on discussions between female friends (Lady Brute and Bellinda) and male friends (Constant and Heartfree). These exchanges are full of jokes, but are also thoughtful and have a dimension of melancholy and frustration.\n\nAfter a forged-letter complication, the play ends with marriage between Heartfree and Bellinda and stalemate between the Brutes. Constant continues to pay court to Lady Brute, and she continues to shilly-shally.\n\nThe tolerance for Restoration comedy even in its modified form was running out at the end of the 17th century, as public opinion turned to respectability and seriousness even faster than the playwrights did. Interconnected causes for this shift in taste were demographic change, the Glorious Revolution of 1688, William's and Mary's dislike of the theatre, and the lawsuits brought against playwrights by the Society for the Reformation of Manners (founded in 1692). When Jeremy Collier attacked Congreve and Vanbrugh in his \"Short View of the Immorality and Profaneness of the English Stage\" in 1698, he was confirming a shift in audience taste that had already taken place. At the much-anticipated all-star première in 1700 of \"The Way of the World\", Congreve's first comedy for five years, the audience showed only moderate enthusiasm for that subtle and almost melancholy work. The comedy of sex and wit was about to be replaced by the drama of obvious sentiment and exemplary morality.\n\nDuring the 18th and 19th centuries, the sexual frankness of Restoration comedy ensured that theatre producers cannibalised it or adapted it with a heavy hand, rather than actually performed it. Today, Restoration comedy is again appreciated on the stage. The classics, Wycherley's \"The Country Wife\" and \"The Plain-Dealer\", Etherege's \"The Man of Mode\", and Congreve's \"Love For Love\" and \"The Way of the World\" have competition not only from Vanbrugh's \"The Relapse\" and \"The Provoked Wife\", but from such dark, unfunny comedies as Thomas Southerne's \"The Wives Excuse\". Aphra Behn, once considered unstageable, has had a major renaissance, with \"The Rover\" now a repertory favourite.\n\nDistaste for sexual impropriety long kept Restoration comedy not only off the stage but also locked in a critical poison cupboard. Victorian critics like William Hazlitt, although valuing the linguistic energy and \"strength\" of the canonical writers Etherege, Wycherley, and Congreve, always found it necessary to temper aesthetic praise with heavy moral condemnation. Aphra Behn received the condemnation without the praise, since outspoken sex comedy was considered particularly offensive coming from a woman author. At the turn of the 20th century, an embattled minority of academic Restoration comedy enthusiasts began to appear, for example the important editor Montague Summers, whose work ensured that the plays of Aphra Behn remained in print.\n\n\"Critics remain astonishingly defensive about the masterpieces of this period\", wrote Robert D. Hume as late as 1976. It is only over the last few decades that that statement has become untrue, as Restoration comedy has been acknowledged a rewarding subject for high theory analysis and Wycherley's \"The Country Wife\", long branded the most obscene play in the English language, has become something of an academic favourite. \"Minor\" comic writers are getting a fair share of attention, especially the post-Aphra Behn generation of women playwrights which appeared just around the turn of the 18th century: Delarivier Manley, Mary Pix, Catharine Trotter, and Susannah Centlivre. A broad study of the majority of never-reprinted Restoration comedies has been made possible by Internet access (by subscription only) to the first editions at the British Library.\n\n\n\n\n\nThis section lists a selection of seminal critical studies.\n\n\n"}
{"id": "181885", "url": "https://en.wikipedia.org/wiki?curid=181885", "title": "Ribaldry", "text": "Ribaldry\n\nRibaldry, or blue comedy, is humorous entertainment that ranges from bordering on indelicacy to gross indecency. It is also referred to as \"bawdiness\", \"gaminess\" or \"bawdy\".\n\nSex is presented in ribald material more for the purpose of poking fun at the foibles and weaknesses that manifest themselves in human sexuality, rather than to present sexual stimulation either excitingly or artistically. Also, ribaldry may use sex as a metaphor to illustrate some non-sexual concern, in which case ribaldry may verge on the territory of satire.\n\nLike any humour, ribaldry may be read as conventional or subversive. Ribaldry typically depends on a shared background of sexual conventions and values, and its comedy generally depends on seeing those conventions broken.\n\nThe ritual taboo-breaking that is a usual counterpart of ribaldry underlies its controversial nature and explains why ribaldry is sometimes a subject of censorship. Ribaldry, whose usual aim is \"not\" \"merely\" to be sexually stimulating, often does address larger concerns than mere sexual appetite. However, being presented in the form of comedy, these larger concerns may be overlooked by censors.\n\nRibaldry differs from black comedy (or gallows humor) in that black comedy deals with topics which would normally considered painful or frightening whereas ribaldry deals with topics that would only be considered offensive.\n\nRibaldry is present to some degree in every culture and has likely been around for all of human history. Works like \"Lysistrata\" by Aristophanes, \"Menaechmi\" by Plautus, \"Cena Trimalchionis\" by Petronius, and \"The Golden Ass\" of Apuleius are ribald classics from ancient Greece and Rome. Geoffrey Chaucer's \"The Miller's Tale\" from his \"Canterbury Tales\" and \"The Crabfish\", one of the oldest English traditional ballads, are classic examples. The Frenchman François Rabelais showed himself to be a master of ribaldry (technically called grotesque body) in his \"Gargantua\" and other works. \"The Life and Opinions of Tristram Shandy, Gentleman\" by Laurence Sterne and \"The Lady's Dressing Room\" by Jonathan Swift are also in this genre; as is Mark Twain's long-suppressed \"1601\".\n\nAnother example of ribaldry is \"De Brevitate Vitae\", a song which in many European-influenced universities is both a student beer-drinking song and an anthem sung by official university choirs at public graduation ceremonies. The private and public versions of the song contain vastly different words. More recent works like \"Candy\", \"Barbarella\", \"L'Infermiera\", the comedic works of Russ Meyer, \"Little Annie Fanny\" and John Barth's \"The Sot-Weed Factor\" are probably better classified as ribaldry than as either pornography or erotica.\n\nA bawdy song is a humorous song that emphasises sexual themes and is often rich with innuendo. Historically these songs tend to be confined to groups of young males, either as students or in an environment where alcohol is flowing freely. An early collection was \"Wit and Mirth, or Pills to Purge Melancholy\", edited by Thomas D'Urfey and published between 1698 and 1720. Selected songs from \"Wit and Mirth\" have been recorded by the City Waites and other singers. Sailor's songs tend to be quite frank about the exploitative nature of the relationship between men and women. There are many examples of folk songs in which a man encounters a woman in the countryside. This is followed by a short conversation, and then sexual intercourse, e.g. \"The Game of All Fours\". Neither side demonstrates any shame or regret. If the woman becomes pregnant, the man will not be there anyway. Rugby songs are often bawdy. Examples of bawdy folk songs are: \"Seventeen Come Sunday\" and \"The Ballad of Eskimo Nell\". Robert Burns compiled \"The Merry Muses of Caledonia\" (the title is not Burns's), a collection of bawdy lyrics that were popular in the music halls of Scotland as late as the 20th century. In modern times Hash House Harriers have taken on the role of tradition-bearers for this kind of song. \"The Unexpurgated Folk Songs of Men\" (Arhoolie 4006) is a gramophone record containing a collection of American bawdy songs recorded in 1959.\n\nBlue comedy is comedy that is off-color, risqué, indecent or profane, largely about sex. It often contains profanity or sexual imagery that may shock and offend some audience members.\n\n\"Working blue\" refers to the act of using curse words and discussing things that people do not discuss in \"polite society\". A \"blue comedian\" or \"blue comic\" is a comedian who usually performs risqué routines layered with curse words. \n\nThere is a common belief that comedian Max Miller (1894–1963) coined the phrase, after his stage act which involved telling jokes from either a white book or a blue book, chosen by audience preference (the blue book contained ribald jokes). This is not so, as the \"Oxford English Dictionary\" contains earlier references to the use of blue to mean ribald: 1890 \"Sporting Times\" 25 Jan. 1/1 \"\"Shifter wondered whether the damsel knew any novel blue stories.\" and 1900 \"Bulletin\" (Sydney) 20 Oct. 12/4 \"Let someone propose to celebrate Chaucer by publicly reading some of his bluest productions unexpurgated. The reader would probably be locked up.\"\"\n\nPrivate events at show business clubs such as the Bob Saget Club and The Masquers often showed this blue side of otherwise cleancut Bob Saget; a recording survives of one Masquers roast from the 1950s with Jack Benny, George Jessel, George Burns, and Art Linkletter all using highly risqué material and obscenities. Many comedians who are normally family-friendly might choose to work blue when off-camera or in an adult-oriented environment; Bob Saget exemplifies this dichotomy. Bill Cosby's 1969 record album records both his family-friendly evening standup comedy show, and his blue midnight show, which included a joke about impregnating his wife \"right through the old midnight trampoline\" (her diaphragm) and other sexual references.\n\nSome comedians build their careers on blue comedy. Among the best known of these are Redd Foxx, Lawanda Page, and the team of Leroy and Skillet, all of whom later performed on the family-friendly television show Sanford and Son. Page, Leroy, and Skillet specialized in a particular African American form of blue spoken word recitation called signifying or toasting. Dave Attell has also been described by his peers as one of the greatest modern-day blue comics. \n\nOn talk radio in the United States, many commentators use blue comedy in their political programs. Examples include Neal Boortz, Eric Von Haessler, Phil Hendrie and Steve Morrison.\n\n\n\n"}
{"id": "2851778", "url": "https://en.wikipedia.org/wiki?curid=2851778", "title": "Skomorokh", "text": "Skomorokh\n\nA skomorokh (скоморох in Russian, скоморохъ in Old East Slavic, скоморaхъ in Church Slavonic) was a medieval East Slavic harlequin, or actor, who could also sing, dance, play musical instruments and compose for oral/musical and dramatic performances. The etymology of the word is not completely clear. There are hypotheses that the word is derived from the Greek σκώμμαρχος (cf. σκῶμμα, \"joke\"); from the Italian \"scaramuccia\" (\"joker\", cf. English \"scaramouch\"); from the Arabic \"masẋara\"; and many others.\n\nThe skomorokhs appeared in Kievan Rus no later than the mid-11th century, but fresco depictions of skomorokh musicians in the Saint Sophia Cathedral in Kiev date to the 11th century.\n\nThe \"Primary Chronicle\" on skomorokhs concurs with the period. The monk chronicler denounced them as devil servants. Furthermore, the Orthodox Church often railed against them and other elements of popular culture as being irreverent, detracting from the worship of God or being downright diabolical. For example Theodosius of Kiev, one of the co-founders of the Caves Monastery in the 11th century, called the skomorokhs \"evils to be shunned by good Christians\". Their art was related and addressed to the common people and usually opposed the ruling groups, who considered them not just useless but even ideologically detrimental and dangerous by both the feudalists and the clergy.\n\nThey were persecuted in the years of the Mongol yoke, when the church strenuously propagated ascetic living. Its art reached its peak in the 15th to the 17th centuries. Their repertoire included mock songs, dramatic and satirical sketches, called \"glumy\" (глумы), performed in masks and skomorokh dresses to the sounds of domra, balalaika, gudok, bagpipes or buben (a kind of tambourine). The appearance of Russian puppet theatre was directly associated with skomorokh performances.\n\nSkomorokhs performed in the streets and city squares, engaging with the spectators to draw them into their play. Usually, the main character of the skomorokh performance was a fun-loving saucy \"muzhik\" (мужик) of comic simplicity. In the 16 and 17th centuries, skomorokhs would sometimes combine their efforts and perform in a \"vataga\" (ватага, big crowd) numbering 70 to 100 people. The skomorokhs were often persecuted by the Russian Orthodox Church and civilian authorities. \n\nIn 1648 and 1657, Tsar Alexei Mikhailovich issued ukases banning skomorokh art as blasphemous, but actors would still occasionally perform during popular celebrations. In the 18th century, skomorokh art gradually died away; passing on some of its traditions to the \"\" (балаган) and \"rayoks\" (раёк).\n\n\n"}
{"id": "3565092", "url": "https://en.wikipedia.org/wiki?curid=3565092", "title": "Society of Scribes &amp; Illuminators", "text": "Society of Scribes &amp; Illuminators\n\nThe Society of Scribes & Illuminators is an organisation dedicated to the promotion and development of the arts of calligraphy and illumination.\n\nThe SSI was founded in the United Kingdom in 1921 by former students of leading calligrapher Edward Johnston and has an international reputation in its field. The SSI organises exhibitions and lectures on subjects related to its fields of interest. Membership is opened to professionals in the field as well as interested amateurs.\n\n\n"}
{"id": "32745919", "url": "https://en.wikipedia.org/wiki?curid=32745919", "title": "Stream of unconsciousness (narrative mode)", "text": "Stream of unconsciousness (narrative mode)\n\nIn literary criticism, stream of unconsciousness is a narrative mode that portrays an individual's point of view by transcribing the author's unconscious dialogue or \"somniloquy\" during sleep, in connection to his or her actions within a dream.\n\nStream of unconsciousness is characterized by disjointed leaps in ideation and story line, bizarre new word creation, loss of self-censorship, one sided conversations and punctuation that can make the prose both disturbing and difficult to follow. Despite the name, the stream of unconsciousness occurs more in the form of waves than an actual continuous running stream of dialogue. The stream of unconsciousness is one of several forms of dramatic monologue, where the speaker is addressing an audience or a third person. Such monologues are commonly used in poetry and drama, but with the stream of unconsciousness, the audience or third persons are 'unknowingly' imaginary. It is primarily a fictional device and often takes the form of a tragedy. The term was introduced to the field of literary studies from that of the independent music industry, where it was redefined by author and artist Bryan Lewis Saunders in 2005. For nearly a century the term has been both a misnomer and a malapropism for the stream of consciousness.\n\nThe audio recordings of Dion McGregor (1922–1994) can be perceived as one of the precursors of the 'stream of unconsciousness' (narrative mode), although his monlogues and somniloquy were never transcribed and presented as such. \"Dreams\" (1995) by contemporary American artist Jim Shaw is a collection of illustrations and extremely detailed dream descriptions. \"Experiment with Dreams\" by Leif Elggren and Thomas Liljenberg (Stockholm Feb. 1996) may also be attributed to the formation of this method.\n\nExamples of notable works employing the stream of unconsciousness narrative mode are:\n\n"}
{"id": "79452", "url": "https://en.wikipedia.org/wiki?curid=79452", "title": "Viruses of the Mind", "text": "Viruses of the Mind\n\n\"Viruses of the Mind\" is an essay by British evolutionary biologist Richard Dawkins, first published in the book \"Dennett and His Critics: Demystifying Mind\" (1993). Dawkins originally wrote the essay in 1991 and delivered it as a Voltaire Lecture on 6 November 1992 at the Conway Hall Humanist Centre. The essay discusses how religion can be viewed as a meme, an idea previously expressed by Dawkins in \"The Selfish Gene\" (1976). Dawkins analyzes the propagation of religious ideas and behaviors as a memetic virus, analogous to how biological and computer viruses spread. The essay was later published in \"A Devil's Chaplain\" (2003) and its ideas are further explored in the television programme, \"The Root of All Evil?\" (2006).\n\nDawkins defines the \"symptoms\" of being infected by the \"virus of religion\", providing examples for most of them, and tries to define a connection between the elements of religion and its survival value (invoking Zahavi's handicap principle of sexual selection, applied to believers of a religion). Dawkins also describes religious beliefs as \"mind-parasites\", and as \"gangs <nowiki>[that]</nowiki> will come to constitute a package, which may be sufficiently stable to deserve a collective name such as Roman Catholicism ... or ... component parts to a single virus\".\n\nDawkins suggests that religious belief in the \"faith-sufferer\" typically shows the following elements:\n\nDawkins stresses his claim that religious beliefs do not spread as a result of evidence in their support, but typically by cultural transmission, in most cases from parents or from charismatic individuals. He refers to this as involving \"epidemiology, not evidence\". Further Dawkins distinguishes this process from the spread of scientific ideas, which, he suggests, is constrained by the requirement to conform with certain virtues of standard methodology: \"testability, evidential support, precision, quantifiability, consistency, intersubjectivity, repeatability, universality, progressiveness, independence of cultural milieu, and so on\". He points out that faith \"spreads despite a total lack of every single one of these virtues\".\n\nAlister McGrath, a Christian theologian, has also commented critically on Dawkins' analysis, suggesting that \"memes have no place in serious scientific reflection\", that there is strong evidence that such ideas are not spread by random processes, but by deliberate intentional actions, that \"evolution\" of ideas is more Lamarckian than Darwinian, and suggests there is no evidence that epidemiological models usefully explain the spread of religious ideas. McGrath also cites a metareview of 100 studies and argues that \"If religion is reported as having a positive effect on human well-being by 79% of recent studies in the field, how can it conceivably be regarded as analogous to a virus?\"\n\n\n"}
