{"id": "594386", "url": "https://en.wikipedia.org/wiki?curid=594386", "title": "Age of Sail", "text": "Age of Sail\n\nThe Age of Sail (usually dated as 1571–1862) was a period roughly corresponding to the early modern period in which international trade and naval warfare were dominated by sailing ships, lasting from the 16th to the mid-19th century. \n\nLike most periodic eras the definition is inexact but close enough to serve as a general description. The age of sail runs roughly from the Battle of Lepanto in 1571, the last significant engagement in which oar-propelled galleys played a major role, to the Battle of Hampton Roads in 1862, in which the steam-powered ironclad CSS \"Virginia\" destroyed the sailing ships USS \"Cumberland\" and USS \"Congress\", demonstrating that the advance of steam power had rendered sail power in warfare obsolete.\n\nThe Suez Canal, in the Middle-East, which opened in 1869, was impractical for sailing ships, and made steamboats faster on the European-Asian sea route.\n\nThe period between 1850 and the early 20th century when sailing vessels reached their peak of size and complexity is sometimes referred to as the \"Golden Age of Sail\". During this time the efficiency and use of commercial sailing vessels was at its peak—immediately before steamboats started to take trade away from sail.\n\nSailing ships sometimes continued to be an economical way to transport bulk cargo on long voyages into the 1920s, even if steamships also were used for such transports and became more and more common. Sailing ships do not require fuel or complex engines to be powered; thus they tended to be more independent from requiring a dedicated support base on the mainland. Crucially though, steam-powered ships held a speed advantage and were rarely hindered by adverse winds, freeing steam-powered vessels from the necessity of following trade winds. As a result, cargo and supplies could reach a foreign port in half the time it took a sailing ship. \n\nSailing vessels were pushed into narrower and narrower economic niches and gradually disappeared from commercial trade. Today, sailing vessels are only economically viable for small scale coastal fishing, along with recreational uses such as yachting and passenger sail excursion ships.\n\n"}
{"id": "33645566", "url": "https://en.wikipedia.org/wiki?curid=33645566", "title": "Bachelor of Environmental Science", "text": "Bachelor of Environmental Science\n\nA Bachelor of Environmental Science is an undergraduate bachelor's degree awarded for courses taken in the study of environmental science or related disciplines, such as sustainable resource development, environmental health, or ecological sustainability, and may also be known as a Bachelor of Environmental Science and Management degree in some schools. This is a interdisciplinary degree course, some universities offers four year honors degree programme \n\nThe Bachelor of Environmental Science degree is frequently abbreviated as B.Env.Sc., (sometimes B.E.Sc, B.S.E.S, or even B.E.S - the latter of which can cause confusion with a Bachelor of Environmental \"Studies\" degree) especially in the use of post-nominals where an Environmental Studies degree may be either B.S. or B.A.\n\nIn Australia, those who graduate with a B.Env.Sc. are eligible for membership in the Environment Institute of Australian and New Zealand.\n\nThis listing only includes institutions that award a Bachelor of Environmental Science degree. Although there are similar programs that are Bachelor of Science degrees that specialize in Environmental Science or Environmental Studies, they are not included in this list. In many cases, the differences are political or administrative in nature, as opposed to content.\n\n\n\n\n\n\n\n"}
{"id": "41588566", "url": "https://en.wikipedia.org/wiki?curid=41588566", "title": "Bouldergaine", "text": "Bouldergaine\n\nBouldergaine is a variant sport combining the disciplines of bouldering and rogaining.\n\nThe sport takes place in a large boulder field, which is mapped in a similar way to a traditional rogaine. Teams of 2 people choose which checkpoints to visit within a time limit with the intent of maximizing their score. At each checkpoint is at least one extra bonus point station, which requires scrambling or climbing to access. A successful team will require a mixture of fitness, climbing ability and map skills.\n\nThe world's first bouldergaine was held on March 15, 2014 at Kura Tawhiti / Castle Hill in Canterbury, New Zealand. The event was organised by the New Zealand Alpine Club, with over 100 participants.\n"}
{"id": "337871", "url": "https://en.wikipedia.org/wiki?curid=337871", "title": "Butte", "text": "Butte\n\nIn geomorphology, a butte () is an isolated hill with steep, often vertical sides and a small, relatively flat top; buttes are smaller landforms than mesas, plateaus, and tablelands. The word \"butte\" comes from a French word meaning \"small hill\"; its use is prevalent in the Western United States, including the southwest where \"mesa\" is used for the larger landform. Because of their distinctive shapes, buttes are frequently landmarks in plains and mountainous areas. In differentiating mesas and buttes, geographers use the rule of thumb that a mesa has a top that is wider than its height, while a butte has a top that is narrower than its height.\n\nThe Mitten Buttes of Monument Valley in Arizona are two of the most distinctive and widely recognized buttes. Monument Valley and the Mittens provided backgrounds in scenes from many western-themed films, including seven movies directed by John Ford. The Devils Tower in northeastern Wyoming is a laccolithic butte composed of igneous rock rather than sandstone, limestone or other sedimentary rocks.\n\nThree other notable formations that are either named \"butte\" or may be considered buttes even though they do not conform to the formal geographer's rule are Scotts Bluff in Nebraska which is actually a collection of five bluffs, Crested Butte which is a mountain in Colorado, and Elephant Butte which is now an island in Elephant Butte Reservoir in New Mexico.\n\nAmong the well-known non-flat-topped buttes in the United States are Bear Butte, South Dakota, Black Butte, Oregon, and the Sutter Buttes in California. In many cases, buttes have been given other names that do not use the word \"butte\", for example, Courthouse Rock, Nebraska. Also, some large hills that are technically not buttes have names using the word \"butte\", examples of which are Kamiak Butte and Chelan Butte in Washington state.\n\nButtes form by weathering and erosion when hard caprock overlies a layer of less resistant rock that is eventually worn away. The harder rock on top of the butte resists erosion. The caprock provides protection for the less resistant rock below from wind abrasion which leaves it standing isolated. As the top is further eroded by abrasion and weathering, the excess material that falls off adds to the scree or talus slope around the base. On a much smaller scale, the same process forms hoodoos.\n\n\n"}
{"id": "3984096", "url": "https://en.wikipedia.org/wiki?curid=3984096", "title": "Clock code", "text": "Clock code\n\nThe clock code is a method of mentally computing the sine of an angle between zero and sixty degrees. Pilots sometimes need to do this to estimate the heading correction due to the wind, and sailors may find it useful to do the same thing to allow for the current due to the tides.\n\nThe basic assumption is that for angles up to around 60°, it is adequately accurate to assume that sine(A) = A, when A is expressed as a fraction of 60. Thus, the sine of 30° = 30/60 = 1/2 = 0.5.\n\nThe clock code is a further method of visualising fractions of 60, since we are very used to expressing fractions of an hour (60 minutes) when telling the time.\n\nThus:\n\n\nThe angle is the angle of the wind or tide as it presents itself relative to the ship or aircraft, so if the wind is coming from the left at a relative angle of 30°, we use a sine of 0.5. Once the sine has been estimated, the drift due to wind or tide can be estimated accordingly by resolving the velocity of the wind or tide into a forward component and a sideways component. The sideways component is the windspeed x the sine, and the forward component is (1 - the sine) x windspeed (i.e. the cosine). Naturally we must apply these corrections to our groundspeed in the appropriate direction according to logic - a wind from the left will blow us off course in that direction; a headwind will slow our progress, a tailwind will increase it. Converting this back to a heading correction can be done using the 1 in 60 rule.\n\nFor wind angles greater than 60°, it is adequate to assume it's at 90°, i.e. a side wind.\n\nMore accurate corrections are done when possible, but in the particular case of a VFR pilot mentally calculating an unexpected diversion, using tables or the E6B slide rule in flight is usually not an option, so the clock code is one method of handling the required calculation without excessive error. In practice the wind strength can only be guessed or based on earlier reports, and the error due to the sine calculation will usually be much less than that due to the wind itself. Fine tuning of the heading can be done en route using the usual methods for doing so.\n\n"}
{"id": "33273182", "url": "https://en.wikipedia.org/wiki?curid=33273182", "title": "Conference of Latin Americanist Geographers", "text": "Conference of Latin Americanist Geographers\n\nThe Conference of Latin Americanist Geographers (CLAG) was formed in 1970 to foster geographic education and research on Latin America. A Board of Directors governs CLAG. CLAG publishes a Newsletter and the \"Journal of Latin American Geography\". It also operates CLAGNET, an electronic Listserv for members.\n\nThe Syracuse University Archives preserve documents and photographs related to the history of the organization because the long-standing Executive Director, David J. Robinson, is a professor at that university.\nEach year CLAG makes several awards to distinguished Latin Americanist geographers and others.\n\nCLAG also makes awards to graduate students to support research and conference travel.\nCLAG organizes a conference on a regular basis. The conference typically takes place in the Americas but in 2001 met in Benicassim, Spain.\n\n"}
{"id": "56428137", "url": "https://en.wikipedia.org/wiki?curid=56428137", "title": "Dalsfjord Lighthouse Museum", "text": "Dalsfjord Lighthouse Museum\n\nThe Dalsfjord Lighthouse Museum () is a Norwegian museum dedicated to documenting and presenting the social history of the people that built lighthouses, sector lights, markers, moorings, and ports along the Norwegian coastline.\n\nThe museum is located in Dravlaus in the municipality of Volda, and it is part of the Sunnmøre Museum Foundation. The museum was opened in a new location with a new exhibit on November 8, 2012.\n\n"}
{"id": "74263", "url": "https://en.wikipedia.org/wiki?curid=74263", "title": "Frame of reference", "text": "Frame of reference\n\nIn physics, a frame of reference (or reference frame) consists of an abstract coordinate system and the set of physical reference points that uniquely fix (locate and orient) the coordinate system and standardize measurements.\n\nIn n dimensions, n+1 reference points are sufficient to fully define a reference frame. Using rectangular (Cartesian) coordinates, a reference frame may be defined with a reference point at the origin and a reference point at one unit distance along each of the n coordinate axes.\n\nIn Einsteinian relativity, reference frames are used to specify the relationship between a moving observer and the phenomenon or phenomena under observation. In this context, the phrase often becomes \"observational frame of reference\" (or \"observational reference frame\"), which implies that the observer is at rest in the frame, although not necessarily located at its origin. A relativistic reference frame includes (or implies) the coordinate time, which does not correspond across different frames moving relatively to each other. The situation thus differs from Galilean relativity, where all possible coordinate times are essentially equivalent.\n\nThe need to distinguish between the various meanings of \"frame of reference\" has led to a variety of terms. For example, sometimes the type of coordinate system is attached as a modifier, as in \"Cartesian frame of reference\". Sometimes the state of motion is emphasized, as in \"rotating frame of reference\". Sometimes the way it transforms to frames considered as related is emphasized as in \"Galilean frame of reference\". Sometimes frames are distinguished by the scale of their observations, as in \"macroscopic\" and \"microscopic frames of reference\".\n\nIn this article, the term \"observational frame of reference\" is used when emphasis is upon the \"state of motion\" rather than upon the coordinate choice or the character of the observations or observational apparatus. In this sense, an observational frame of reference allows study of the effect of motion upon an entire family of coordinate systems that could be attached to this frame. On the other hand, a \"coordinate system\" may be employed for many purposes where the state of motion is not the primary concern. For example, a coordinate system may be adopted to take advantage of the symmetry of a system. In a still broader perspective, the formulation of many problems in physics employs \"generalized coordinates\", \"normal modes\" or \"eigenvectors\", which are only indirectly related to space and time. It seems useful to divorce the various aspects of a reference frame for the discussion below. We therefore take observational frames of reference, coordinate systems, and observational equipment as independent concepts, separated as below:\n\n\n\nHere is a quotation applicable to moving observational frames formula_1 and various associated Euclidean three-space coordinate systems [\"R\", \"R′\", \"etc.\"]:\nand this on the utility of separating the notions of formula_1 and [\"R\", \"R′\", \"etc.\"]:\n\nand this, also on the distinction between formula_1 and [\"R\", \"R′\", \"etc.\"]:\n\nand from J. D. Norton:\n\nThe discussion is taken beyond simple space-time coordinate systems by Brading and Castellani. Extension to coordinate systems using generalized coordinates underlies the Hamiltonian and Lagrangian formulations of quantum field theory, classical relativistic mechanics, and quantum gravity.\n\nAlthough the term \"coordinate system\" is often used (particularly by physicists) in a nontechnical sense, the term \"coordinate system\" does have a precise meaning in mathematics, and sometimes that is what the physicist means as well.\n\nA coordinate system in mathematics is a facet of geometry or of algebra, in particular, a property of manifolds (for example, in physics, configuration spaces or phase spaces). The coordinates of a point r in an \"n\"-dimensional space are simply an ordered set of \"n\" numbers: \nIn a general Banach space, these numbers could be (for example) coefficients in a functional expansion like a Fourier series. In a physical problem, they could be spacetime coordinates or normal mode amplitudes. In a robot design, they could be angles of relative rotations, linear displacements, or deformations of joints. Here we will suppose these coordinates can be related to a Cartesian coordinate system by a set of functions:\n\nwhere \"x\", \"y\", \"z\", \"etc.\" are the \"n\" Cartesian coordinates of the point. Given these functions, coordinate surfaces are defined by the relations:\nThe intersection of these surfaces define coordinate lines. At any selected point, tangents to the intersecting coordinate lines at that point define a set of basis vectors {e, e, …, e} at that point. That is:\n\nwhich can be normalized to be of unit length. For more detail see curvilinear coordinates.\n\nCoordinate surfaces, coordinate lines, and basis vectors are components of a coordinate system. If the basis vectors are orthogonal at every point, the coordinate system is an orthogonal coordinate system.\n\nAn important aspect of a coordinate system is its metric tensor \"g\", which determines the arc length \"ds\" in the coordinate system in terms of its coordinates:\n\nwhere repeated indices are summed over.\n\nAs is apparent from these remarks, a coordinate system is a mathematical construct, part of an axiomatic system. There is no necessary connection between coordinate systems and physical motion (or any other aspect of reality). However, coordinate systems can include time as a coordinate, and can be used to describe motion. Thus, Lorentz transformations and Galilean transformations may be viewed as coordinate transformations.\n\nGeneral and specific topics of coordinate systems can be pursued following the See also links below.\n\nAn observational frame of reference, often referred to as a \"physical frame of reference\", a \"frame of reference\", or simply a \"frame\", is a physical concept related to an observer and the observer's state of motion. Here we adopt the view expressed by Kumar and Barve: an observational frame of reference is characterized \"only by its state of motion\". However, there is lack of unanimity on this point. In special relativity, the distinction is sometimes made between an \"observer\" and a \"frame\". According to this view, a \"frame\" is an \"observer\" plus a coordinate lattice constructed to be an orthonormal right-handed set of spacelike vectors perpendicular to a timelike vector. See Doran. This restricted view is not used here, and is not universally adopted even in discussions of relativity. In general relativity the use of general coordinate systems is common (see, for example, the Schwarzschild solution for the gravitational field outside an isolated sphere).\n\nThere are two types of observational reference frame: inertial and non-inertial. An inertial frame of reference is defined as one in which all laws of physics take on their simplest form. In special relativity these frames are related by Lorentz transformations, which are parametrized by rapidity. In Newtonian mechanics, a more restricted definition requires only that Newton's first law holds true; that is, a Newtonian inertial frame is one in which a free particle travels in a straight line at constant speed, or is at rest. These frames are related by Galilean transformations. These relativistic and Newtonian transformations are expressed in spaces of general dimension in terms of representations of the Poincaré group and of the Galilean group.\n\nIn contrast to the inertial frame, a non-inertial frame of reference is one in which fictitious forces must be invoked to explain observations. An example is an observational frame of reference centered at a point on the Earth's surface. This frame of reference orbits around the center of the Earth, which introduces the fictitious forces known as the Coriolis force, centrifugal force, and gravitational force. (All of these forces including gravity disappear in a truly inertial reference frame, which is one of free-fall.)\n\nA further aspect of a frame of reference is the role of the measurement apparatus (for example, clocks and rods) attached to the frame (see Norton quote above). This question is not addressed in this article, and is of particular interest in quantum mechanics, where the relation between observer and measurement is still under discussion (see measurement problem).\n\nIn physics experiments, the frame of reference in which the laboratory measurement devices are at rest is usually referred to as the laboratory frame or simply \"lab frame.\" An example would be the frame in which the detectors for a particle accelerator are at rest. The lab frame in some experiments is an inertial frame, but it is not required to be (for example the laboratory on the surface of the Earth in many physics experiments is not inertial). In particle physics experiments, it is often useful to transform energies and momenta of particles from the lab frame where they are measured, to the center of momentum frame \"COM frame\" in which calculations are sometimes simplified, since potentially all kinetic energy still present in the COM frame may be used for making new particles.\n\nIn this connection it may be noted that the clocks and rods often used to describe observers' measurement equipment in thought, in practice are replaced by a much more complicated and indirect metrology that is connected to the nature of the vacuum, and uses atomic clocks that operate according to the standard model and that must be corrected for gravitational time dilation. (See second, meter and kilogram).\n\nIn fact, Einstein felt that clocks and rods were merely expedient measuring devices and they should be replaced by more fundamental entities based upon, for example, atoms and molecules.\n\n\nConsider a situation common in everyday life. Two cars travel along a road, both moving at constant velocities. See Figure 1. At some particular moment, they are separated by 200 metres. The car in front is travelling at 22 metres per second and the car behind is travelling at 30 metres per second. If we want to find out how long it will take the second car to catch up with the first, there are three obvious \"frames of reference\" that we could choose.\n\nFirst, we could observe the two cars from the side of the road. We define our \"frame of reference\" \"S\" as follows. We stand on the side of the road and start a stop-clock at the exact moment that the second car passes us, which happens to be when they are a distance \"d\" = 200 m apart. Since neither of the cars is accelerating, we can determine their positions by the following formulas, where formula_12 is the position in meters of car one after time \"t\" in seconds and formula_13 is the position of car two after time \"t\".\n\nNotice that these formulas predict at \"t\" = 0 s the first car is 200 m down the road and the second car is right beside us, as expected. We want to find the time at which formula_15. Therefore, we set formula_15 and solve for formula_17, that is:\n\nAlternatively, we could choose a frame of reference \"S′\" situated in the first car. In this case, the first car is stationary and the second car is approaching from behind at a speed of . In order to catch up to the first car, it will take a time of , that is, 25 seconds, as before. Note how much easier the problem becomes by choosing a suitable frame of reference. The third possible frame of reference would be attached to the second car. That example resembles the case just discussed, except the second car is stationary and the first car moves backward towards it at 8 m / s.\n\nIt would have been possible to choose a rotating, accelerating frame of reference, moving in a complicated manner, but this would have served to complicate the problem unnecessarily. It is also necessary to note that one is able to convert measurements made in one coordinate system to another. For example, suppose that your watch is running five minutes fast compared to the local standard time. If you know that this is the case, when somebody asks you what time it is, you are able to deduct five minutes from the time displayed on your watch in order to obtain the correct time. The measurements that an observer makes about a system depend therefore on the observer's frame of reference (you might say that the bus arrived at 5 past three, when in fact it arrived at three).\n\nFor a simple example involving only the orientation of two observers, consider two people standing, facing each other on either side of a north-south street. See Figure 2. A car drives past them heading south. For the person facing east, the car was moving towards the right. However, for the person facing west, the car was moving toward the left. This discrepancy is because the two people used two different frames of reference from which to investigate this system.\n\nFor a more complex example involving observers in relative motion, consider Alfred, who is standing on the side of a road watching a car drive past him from left to right. In his frame of reference, Alfred defines the spot where he is standing as the origin, the road as the x-axis and the direction in front of him as the positive y-axis. To him, the car moves along the \"x\" axis with some velocity \"v\" in the positive x-direction. Alfred's frame of reference is considered an inertial frame of reference because he is not accelerating (ignoring effects such as Earth's rotation and gravity).\n\nNow consider Betsy, the person driving the car. Betsy, in choosing her frame of reference, defines her location as the origin, the direction to her right as the positive \"x\"-axis, and the direction in front of her as the positive \"y\"-axis. In this frame of reference, it is Betsy who is stationary and the world around her that is moving – for instance, as she drives past Alfred, she observes him moving with velocity \"v\" in the negative \"y\"-direction. If she is driving north, then north is the positive \"y\"-direction; if she turns east, east becomes the positive \"y\"-direction.\n\nFinally, as an example of non-inertial observers, assume Candace is accelerating her car. As she passes by him, Alfred measures her acceleration and finds it to be \"a\" in the negative x-direction. Assuming Candace's acceleration is constant, what acceleration does Betsy measure? If Betsy's velocity \"v\" is constant, she is in an inertial frame of reference, and she will find the acceleration to be the same as Alfred in her frame of reference, \"a\" in the negative \"y\"-direction. However, if she is accelerating at rate \"A\" in the negative \"y\"-direction (in other words, slowing down), she will find Candace's acceleration to be \"a′\" = \"a\" − \"A\" in the negative \"y\"-direction - a smaller value than Alfred has measured. Similarly, if she is accelerating at rate \"A\" in the positive y-direction (speeding up), she will observe Candace's acceleration as \"a′\" = \"a\" + \"A\" in the negative \"y\"-direction – a larger value than Alfred's measurement.\n\nFrames of reference are especially important in special relativity, because when a frame of reference is moving at some significant fraction of the speed of light, then the flow of time in that frame does not necessarily apply in another frame. The speed of light is considered to be the only true constant between moving frames of reference.\n\nIt is important to note some assumptions made above about the various inertial frames of reference. Newton, for instance, employed universal time, as explained by the following example. Suppose that you own two clocks, which both tick at exactly the same rate. You synchronize them so that they both display exactly the same time. The two clocks are now separated and one clock is on a fast moving train, traveling at constant velocity towards the other. According to Newton, these two clocks will still tick at the same rate and will both show the same time. Newton says that the rate of time as measured in one frame of reference should be the same as the rate of time in another. That is, there exists a \"universal\" time and all other times in all other frames of reference will run at the same rate as this universal time irrespective of their position and velocity. This concept of time and simultaneity was later generalized by Einstein in his special theory of relativity (1905) where he developed transformations between inertial frames of reference based upon the universal nature of physical laws and their economy of expression (Lorentz transformations).\n\nIt is also important to note that the definition of inertial reference frame can be extended beyond three-dimensional Euclidean space. Newton's assumed a Euclidean space, but general relativity uses a more general geometry. As an example of why this is important, let us consider the geometry of an ellipsoid. In this geometry, a \"free\" particle is defined as one at rest or traveling at constant speed on a geodesic path. Two free particles may begin at the same point on the surface, traveling with the same constant speed in different directions. After a length of time, the two particles collide at the opposite side of the ellipsoid. Both \"free\" particles traveled with a constant speed, satisfying the definition that no forces were acting. No acceleration occurred and so Newton's first law held true. This means that the particles were in inertial frames of reference. Since no forces were acting, it was the geometry of the situation which caused the two particles to meet each other again. In a similar way, it is now common to describe that we exist in a four-dimensional geometry known as spacetime. In this picture, the curvature of this 4D space is responsible for the way in which two bodies with mass are drawn together even if no forces are acting. This curvature of spacetime replaces the force known as gravity in Newtonian mechanics and special relativity.\n\nHere the relation between inertial and non-inertial observational frames of reference is considered. The basic difference between these frames is the need in non-inertial frames for fictitious forces, as described below.\n\nAn accelerated frame of reference is often delineated as being the \"primed\" frame, and all variables that are dependent on that frame are notated with primes, e.g. \"x′\", \"y′\", \"a′\".\n\nThe vector from the origin of an inertial reference frame to the origin of an accelerated reference frame is commonly notated as R. Given a point of interest that exists in both frames, the vector from the inertial origin to the point is called r, and the vector from the accelerated origin to the point is called r′.\nFrom the geometry of the situation, we get\nTaking the first and second derivatives of this with respect to time, we obtain\nwhere V and A are the velocity and acceleration of the accelerated system with respect to the inertial system and v and a are the velocity and acceleration of the point of interest with respect to the inertial frame.\n\nThese equations allow transformations between the two coordinate systems; for example, we can now write Newton's second law as\n\nWhen there is accelerated motion due to a force being exerted there is manifestation of inertia. If an electric car designed to recharge its battery system when decelerating is switched to braking, the batteries are recharged, illustrating the physical strength of manifestation of inertia. However, the manifestation of inertia does not prevent acceleration (or deceleration), for manifestation of inertia occurs in response to change in velocity due to a force. Seen from the perspective of a rotating frame of reference the manifestation of inertia appears to exert a force (either in centrifugal direction, or in a direction orthogonal to an object's motion, the Coriolis effect).\n\nA common sort of accelerated reference frame is a frame that is both rotating and translating (an example is a frame of reference attached to a CD which is playing while the player is carried). This arrangement leads to the equation (see Fictitious force for a derivation):\n\nor, to solve for the acceleration in the accelerated frame,\n\nMultiplying through by the mass \"m\" gives\nwhere\n\n\n"}
{"id": "8231648", "url": "https://en.wikipedia.org/wiki?curid=8231648", "title": "GPS-aided GEO augmented navigation", "text": "GPS-aided GEO augmented navigation\n\nThe GPS-aided GEO augmented navigation (GAGAN) is an implementation of a regional satellite-based augmentation system (SBAS) by the Indian government. It is a system to improve the accuracy of a GNSS receiver by providing reference signals. The AAI's efforts towards implementation of operational SBAS can be viewed as the first step towards introduction of modern Communication, navigation and surveillance/Air Traffic Management system over Indian airspace.\n\nThe project has established 15 Indian reference stations, 3 Indian navigation land uplink stations, 3 Indian mission control centers, and installation of all associated software and communication links. It will be able to help pilots to navigate in the Indian airspace by an accuracy of 3 m. This will be helpful for landing aircraft in marginal weather and difficult approaches like Mangalore and Leh airports.\n\nThe project was created in three phases through 2008 by the Airport Authority of India with the help of the Indian Space Research Organisation's (ISRO) technology and space support. The goal is to provide navigation system for all phases of flight over the Indian airspace and in the adjoining area. It is applicable to safety-to-life operations, and meets the performance requirements of international civil aviation regulatory bodies.\n\nThe space component became available after the launch of the GAGAN payload on the GSAT-8 communication satellite, which was successfully launched. This payload was also part of the GSAT-4 satellite that was lost when the geosynchronous satellite launch vehicle (GSLV) failed during launch in April 2010. A final system acceptance test was conducted during June 2012 followed by system certification during July 2013.\n\nTo begin implementing a satellite-based augmentation system over the Indian airspace, Wide Area Augmentation System (WAAS) codes for L1 frequency and L5 frequency were obtained from the United States Air Force and U.S Department of Defense on November 2001 and March 2005. The system will use eight reference stations located in Delhi, Guwahati, Kolkata, Ahmedabad, Thiruvananthapuram, Bangalore, Jammu and Port Blair, and a master control center at Bangalore. US defense contractor Raytheon has stated they will bid to build the system.\n\nA national plan for satellite navigation including implementation of technology demonstration system (TDS) over the Indian air space as a proof of concept had been prepared jointly by Airports Authority of India (AAI) and ISRO. TDS was successfully completed during 2007 by installing eight Indian Reference Stations (INRESs) at eight Indian airports and linked to the Master Control Center (MCC) located near Bangalore. Preliminary system acceptance testing has been successfully completed in December 2010. The ground segment for GAGAN, which has been put up by the Raytheon, has 15 reference stations scattered across the country. Two mission control centres, along with associated uplink stations, have been set up at Kundalahalli in Bangalore. One more control centre and uplink station are to come up at Delhi. As a part of the programme, a network of 18 total electron content (TEC) monitoring stations were installed at various locations in India to study and analyse the behaviour of the ionosphere over the Indian region.\n\nGAGAN's TDS signal in space provides a three-metre accuracy as against the requirement of 7.6 metres. Flight inspection of GAGAN signal is being carried out at Kozhikode, Hyderabad, Nagpur and Bangalore airports and the results have been satisfactory so far.\n\nOne essential component of the GAGAN project is the study of the ionospheric behaviour over the Indian region. This has been specially taken up in view of the uncertain nature of the behaviour of the ionosphere in the region. The study will lead to the optimisation of the algorithms for the ionospheric corrections in the region.\n\nTo study the ionospheric behaviour more effectively over entire Indian airspace, Indian universities and R&D labs, which are involved in the development of regional based ionotropic model for GAGAN, have suggested nine more TEC stations.\n\nGAGAN after its final operational phase completion, will be compatible with other SBAS systems such as the wide-area augmentation system (WAAS), the European Geostationary Navigation Overlay Service (EGNOS) and the Multi-functional Satellite Augmentation System (MSAS) and will provide seamless air navigation service across regional boundaries. While the ground segment consists of eight reference stations and a master control center, which will have sub systems such as data communication network, SBAS correction and verification system, operations and maintenance system, performance monitoring display and payload simulator, Indian land uplinking stations will have dish antenna assembly. The space segment will consist of one geo-navigation transponder.\n\nA flight-management system based on GAGAN will then be poised to save operators time and money by managing climb, descent and engine performance profiles. The FMS will improve the efficiency and flexibility by increasing the use of operator-preferred trajectories.\nIt will improve airport and airspace access in all weather conditions, and the ability to meet the environmental and obstacle clearance constraints. It will also enhance reliability and reduce delays by defining more precise terminal area procedures that feature parallel routes and environmentally optimised airspace corridors.\n\n\nThe first GAGAN transmitter was integrated into the GSAT-4 geostationary satellite, and had a goal of being operational in 2008. Following a series of delays, GSAT-4 was launched on 15 April 2010, however it failed to reach orbit after the third stage of the Geosynchronous Satellite Launch Vehicle Mk.II that was carrying it malfunctioned.\n\nIn 2009, Raytheon had won an 82 million dollar contract. It was mainly dedicated to modernise Indian air navigation system. The vice president of Command & Control Systems, Raytheon Network Centric Systems, Andy Zogg commented:\n\nGAGAN will be the world's most advanced air navigation system and further reinforces India's leadership in the forefront of air navigation. GAGAN will greatly improve safety, reduce congestion and enhance communications to meet India's growing air traffic management needs\n\nIn 2012, the Defence Research and Development Organisation received a \"miniaturised version\" of the device with all the features from global positioning systems (GPS) and global navigation satellite systems (GNSS). The module weighing just 17 gm, can be used in multiple platforms ranging from aircraft (e.g. winged or rotor-craft) to small boats, ships. Reportedly, it can also assist \"survey applications\". It is a cost-efficient device and can be of \"tremendous\" civilian use. The navigation output is composed of GPS, GLONASS and GPS+GLONASS position, speed and time data. According to a statement released by the DRDO, G3oM is a state-of-the-art technology receiver, integrating Indian GAGAN as well as both global positioning system and GLONASS systems.\n\nAccording to Deccan chronicle:\nG. Satheesh Reddy, associate director of the city-based Research Centre Imarat, said the product is bringing about a quantum leap in the area of GNSS technology and has paved the way for highly miniaturised GNSS systems for the future.\n\nOn 30 December 2012, the Directorate General of Civil Aviation (DGCA), India provisionally certified the GPS-aided geo-augmented navigation (GAGAN) system to RNP0.1 (Required Navigation Performance, 0.1 Nautical Mile) service level. The certification enabled aircraft fitted with SBAS equipment to use GAGAN signal in space for navigation purposes.\n\nGSAT-8 is an Indian geostationary satellites, which was successfully launched using Ariane 5 on 21 May 2011 and is positioned in geosynchronous orbit at 55 degrees E longitude.\n\nGSAT-10 is envisaged to augment the growing need of Ku and C-band transponders and carries 12 Ku Band, 12 C Band and 12 Extended C Band transponders and a GAGAN payload. The spacecraft employs the standard I-3K structure with power handling capability of around 6 kW with a lift off mass of 3400 kg. GSAT-10 was successfully launched by Ariane 5 on 29 September 2012.\n\nGSAT-15 carries 24 Ku band transponders with India coverage beam and a GAGAN payload. was successfully launched on 10 November 2015, 21:34:07 UTC, completing the constellation.\n\nThe Indian government has stated that it intends to use the experience of creating the GAGAN system to enable the creation of an autonomous regional navigation system called the Indian Regional Navigation Satellite System IRNSS.\n\nIRNSS-1\nIndian regional navigational satellite system (IRNSS)-1, the first of the seven satellites of the IRNSS constellation, carries a navigation payload and a C-band ranging transponder. The spacecraft employs an optimised I-1K structure with a power handling capability of around 1660W and a lift off mass of 1425 kg, and is designed for a nominal mission life of 10 years. The first satellite of IRNSS constellation was launched onboard PSLV (C22) on 1 July 2012. While the full constellation was planned to be realised during 2014 time frame, launch of subsequent satellites got delayed.\n\nCurrently all 7 satellites are in orbit but in 2017 it was announced that all three rubidium atomic clocks on board IRNSS-1A had failed, mirroring similar failures in the Galileo constellation. The first failure occurred in July 2016, following which two other clocks also failed. This rendered the satellite somewhat redundant and required replacement. Although the satellite still performs other functions, the data is coarse, and thus cannot be used for accurate measurements. ISRO plans to replace it with IRNSS-1H in July or August 2017.\n\nTwo more clocks in the navigational system had started showing signs of abnormality, thereby taking the total number of failed clocks to five.\n\nAs a precaution to extend the operational life of navigation satellite, ISRO is running only one rubidium atomic clock instead of two in the remaining six satellites. Each satellite has three clocks, therefore a total of 27 clocks for all satellites in the system (including standby satellites). The clocks of both IRNSS and GALILEO were supplied by SpectraTime. ISRO replaced the atomic clocks in two standby NavIC satellites. The setback comes at a time when IRNSS is yet to start commercial operations.\n\nKarnataka Forest Department has used GAGAN to build a new, accurate and publicly available satellite based database of its forestlands. This is a followup to the Supreme Court directive to states to update and put up their respective forest maps. The geospatial database of forestlands pilot has used data from the Cartosat-2 satellite. The maps are meant to rid authorities of ambiguities related to forest boundaries and give clarity to forest administrators, revenue officials as also the public, according to R.K. Srivastava, chief conservator of forests (headquarters).\n\nVarious Indian manufactured missiles including the BrahMos will use GAGAN for guidance.\n\n\n\n"}
{"id": "11686565", "url": "https://en.wikipedia.org/wiki?curid=11686565", "title": "GeoNames", "text": "GeoNames\n\nGeoNames is a geographical database available and accessible through various web services, under a Creative Commons attribution license.\n\nThe GeoNames database contains over 25,000,000 geographical names corresponding to over 11,800,000 unique features. All features are categorized into one of nine feature classes and further subcategorized into one of 645 feature codes. Beyond names of places in various languages, data stored include latitude, longitude, elevation, population, administrative subdivision and postal codes. All coordinates use the World Geodetic System 1984 (WGS84).\n\nThose data are accessible free of charge through a number of Web services and a daily database export.\n\nThe core of GeoNames database is provided by official public sources, the quality of which may vary. Through a wiki interface, users are invited to manually edit and improve the database by adding or correcting names, move existing features, add new features, etc.\n\nEach GeoNames feature is represented as a web resource identified by a stable URI. This URI provides access, through content negotiation, either to the HTML wiki page, or to a RDF description of the feature, using elements of the GeoNames ontology. This ontology describes the GeoNames features properties using the Web Ontology Language, the feature classes and codes being described in the SKOS language.\nThrough Wikipedia articles URL linked in the RDF descriptions, GeoNames data are linked to DBpedia data and other RDF Linked Data.\n\nAs in other crowdsourcing schemes, GeoNames edit interface allows everyone to sign in and edit the database, hence false information can be entered and such information can remain undetected especially for places that are not accessed frequently. studies these inaccuracies and classifies them into loss in the granularity of coordinates (e.g., due to truncation and low-resolution geocoding in some cases), wrong feature codes, near-identical places, and the placement of places outside their designated countries.\nManually correcting these inaccuracies is both tedious and error prone (due to the database size) and may require experts.\n\nThe literature provides very few works on automatically resolving them either.\nComputing the boundary information can help detect inconsistencies such as near-identical places and the placement of locations such as cities under wrong parents such as provinces or countries. Singh and Rafiei show that the boundary information derived in their work can move more than 20% of locations in GeoNames to better positions in the spatial hierarchy and the accuracy of those moves is over 90%.\n\n"}
{"id": "995417", "url": "https://en.wikipedia.org/wiki?curid=995417", "title": "Geodetic datum", "text": "Geodetic datum\n\nA geodetic datum or geodetic system is a coordinate system, and a set of reference points, used to locate places on the Earth (or similar objects). An approximate definition of sea level is the datum WGS 84, an ellipsoid, whereas a more accurate definition is Earth Gravitational Model 2008 (EGM2008), using at least 2,159 spherical harmonics. Other datums are defined for other areas or at other times; ED50 was defined in 1950 over Europe and differs from WGS 84 by a few hundred meters depending on where in Europe you look. \nMars has no oceans and so no sea level, but at least two martian datums have been used to locate places there.\n\nDatums are used in geodesy, navigation, and surveying by cartographers and satellite navigation systems to translate positions indicated on maps (paper or digital) to their real position on Earth. Each starts with an ellipsoid (stretched sphere), and then defines latitude, longitude and altitude coordinates. One or more locations on the Earth's surface are chosen as anchor \"base-points\".\n\nThe difference in co-ordinates between datums is commonly referred to as \"datum shift\". The datum shift between two particular datums can vary from one place to another within one country or region, and can be anything from zero to hundreds of meters (or several kilometers for some remote islands). The North Pole, South Pole and Equator will be in different positions on different datums, so True North will be slightly different. Different datums use different interpolations for the precise shape and size of the Earth (reference ellipsoids).\n\nBecause the Earth is an imperfect ellipsoid, localised datums can give a more accurate representation of the area of coverage than WGS 84. OSGB36, for example, is a better approximation to the geoid covering the British Isles than the global WGS 84 ellipsoid. However, as the benefits of a global system outweigh the greater accuracy, the global WGS 84 datum is becoming increasingly adopted.\n\nHorizontal datums are used for describing a point on the Earth's surface, in latitude and longitude or another coordinate system. Vertical datums measure elevations or depths.\n\nIn surveying and geodesy, a \"datum\" is a reference system or an approximation of the Earth's surface against which positional measurements are made for computing locations. Horizontal datums are used for describing a point on the Earth's surface, in latitude and longitude or another coordinate system. Vertical datums are used to measure elevations or underwater depths.\n\nThe horizontal datum is the model used to measure positions on the Earth. A specific point on the Earth can have substantially different coordinates, depending on the datum used to make the measurement. There are hundreds of local horizontal datums around the world, usually referenced to some convenient local reference point. Contemporary datums, based on increasingly accurate measurements of the shape of the Earth, are intended to cover larger areas. The WGS 84 datum, which is almost identical to the NAD83 datum used in North America and the ETRS89 datum used in Europe, is a common standard datum.\n\nFor example, in Sydney there is a 200 metres (700 feet) difference between GPS coordinates configured in GDA (based on global standard WGS 84) and AGD (used for most local maps), which is an unacceptably large error for some applications, such as surveying or site location for scuba diving.\n\nA vertical datum is used as a reference point for elevations of surfaces and features on the Earth including terrain, bathymetry, water levels, and man-made structures. Vertical datums are either: tidal, based on sea levels; gravimetric, based on a geoid; or geodetic, based on the same ellipsoid models of the Earth used for computing horizontal datums.\n\nIn common usage, elevations are often cited in height above sea level, although what “sea level” actually means is a more complex issue than might at first be thought: the height of the sea surface at any one place and time is a result of numerous effects, including waves, wind and currents, atmospheric pressure, tides, topography, and even differences in the strength of gravity due to the presence of mountains etc.\n\nFor the purpose of measuring the height of objects on land, the usual datum used is mean sea level (MSL). This is a tidal datum which is described as the arithmetic mean of the hourly water elevation taken over a specific 19 years cycle. This definition averages out tidal highs and lows (caused by the gravitational effects of the sun and the moon) and short term variations. It will not remove the effects of local gravity strength, and so the height of MSL, relative to a geodetic datum, will vary around the world, and even around one country. Countries tend to choose the mean sea level at one specific point to be used as the standard “sea level” for all mapping and surveying in that country. (For example, in Great Britain, the national vertical datum, Ordnance Datum Newlyn, is based on what was mean sea level at Newlyn in Cornwall between 1915 and 1921). However, zero elevation as defined by one country is not the same as zero elevation defined by another (because MSL is not the same everywhere), which is why locally defined vertical datums differ from one another.\n\nA different principle is used when choosing a datum for nautical charts. For safety reasons, a mariner must be able to know the minimum depth of water that could occur at any point. For this reason, depths and tides on a nautical chart are measured relative to chart datum, which is defined to be a level below which tide rarely falls. Exactly how this is chosen depends on the tidal regime in the area being charted and on the policy of the hydrographic office producing the chart in question; a typical definition is Lowest Astronomical Tide (the lowest tide predictable from the effects of gravity), or Mean Lower Low Water (the average lowest tide of each day), although MSL is sometimes used in waters with very low tidal ranges.\n\nConversely, if a ship is to safely pass under a low bridge or overhead power cable, the mariner must know the minimum clearance between the masthead and the obstruction, which will occur at high tide. Consequently, bridge clearances etc. are given relative to a datum based on high tide, such as Highest Astronomical Tide or Mean High Water Springs.\n\nSea level does not remain constant throughout geological time, and so tidal datums are less useful when studying very long-term processes. In some situations sea level does not apply at all — for instance for mapping Mars' surface — forcing the use of a different \"zero elevation\", such as mean radius.\n\nA geodetic vertical datum takes some specific zero point, and computes elevations based on the geodetic model being used, without further reference to sea levels. Usually, the starting reference point is a tide gauge, so at that point the geodetic and tidal datums might match, but due to sea level variations, the two scales may not match elsewhere. An example of a gravity-based geodetic datum is NAVD88, used in North America, which is referenced to a point in Quebec, Canada. Ellipsoid-based datums such as WGS 84, GRS80 or NAD83 use a theoretical surface that may differ significantly from the geoid.\n\nIn geodetic coordinates, the Earth's surface is approximated by an ellipsoid, and locations near the surface are described in terms of latitude (formula_1), longitude (formula_2), and height (formula_3).\n\nGeodetic latitude (formula_1), resp. altitude, is different from geocentric latitude (formula_5), resp. altitude. Geodetic latitude is determined by the angle between the equatorial plane and normal to the ellipsoid, whereas geocentric latitude is determined by the angle between the equatorial plane and line joining the point to the centre of the ellipsoid (see figure). Unless otherwise specified, latitude is geodetic latitude.\n\nThe ellipsoid is completely parameterised by the semi-major axis formula_6 and the flattening formula_7.\n\nFrom formula_6 and formula_7 it is possible to derive the semi-minor axis formula_10, first eccentricity formula_11 and second eccentricity formula_12 of the ellipsoid\n\nAGD66 and AGD84 both use the parameters defined by Australian National Spheroid (see below)\n\nGDA94 uses the parameters defined by GRS80 (see below)\n\nSee GDA Technical Manual document for more details; the value given above for the flattening is not exact.\n\nThe Global Positioning System (GPS) uses the World Geodetic System 1984 (WGS 84) to determine the location of a point near the surface of the Earth.\n\nSee The official World Geodetic System 1984 document for more details.\n\nA more comprehensive list of geodetic systems can be found here\n\nDatum conversion is the process of converting the coordinates of a point from one datum system to another. Datum conversion may frequently be accompanied by a change of grid projection.\n\nA reference datum is a known and constant surface which is used to describe the location of unknown points on the Earth. Since reference datums can have different radii and different center points, a specific point on the Earth can have substantially different coordinates depending on the datum used to make the measurement. There are hundreds of locally developed reference datums around the world, usually referenced to some convenient local reference point. Contemporary datums, based on increasingly accurate measurements of the shape of the Earth, are intended to cover larger areas. The most common reference Datums in use in North America are NAD27, NAD83, and WGS 84.\n\nThe North American Datum of 1927 (NAD 27) is \"the horizontal control datum for the United States that was defined by a location and azimuth on the Clarke spheroid of 1866, with origin at (the survey station) Meades Ranch (Kansas).\" ... The geoidal height at Meades Ranch was assumed to be zero, as sufficient gravity data was not available, and this was needed to relate surface measurements to the datum. \"Geodetic positions on the North American Datum of 1927 were derived from the (coordinates of and an azimuth at Meades Ranch) through a readjustment of the triangulation of the entire network in which Laplace azimuths were introduced, and the Bowie method was used.\" (http://www.ngs.noaa.gov/faq.shtml#WhatDatum ) NAD27 is a local referencing system covering North America.\n\nThe North American Datum of 1983 (NAD 83) is \"The horizontal control datum for the United States, Canada, Mexico, and Central America, based on a geocentric origin and the Geodetic Reference System 1980 (GRS80). \"This datum, designated as NAD 83 ...is based on the adjustment of 250,000 points including 600 satellite Doppler stations which constrain the system to a geocentric origin.\" NAD83 may be considered a local referencing system.\n\nWGS 84 is the World Geodetic System of 1984. It is the reference frame used by the U.S. Department of Defense (DoD) and is defined by the National Geospatial-Intelligence Agency (NGA) (formerly the Defense Mapping Agency, then the National Imagery and Mapping Agency). WGS 84 is used by DoD for all its mapping, charting, surveying, and navigation needs, including its GPS \"broadcast\" and \"precise\" orbits. WGS 84 was defined in January 1987 using Doppler satellite surveying techniques. It was used as the reference frame for broadcast GPS Ephemerides (orbits) beginning January 23, 1987. At 0000 GMT January 2, 1994, WGS 84 was upgraded in accuracy using GPS measurements. The formal name then became WGS 84 (G730), since the upgrade date coincided with the start of GPS Week 730. It became the reference frame for broadcast orbits on June 28, 1994. At 0000 GMT September 30, 1996 (the start of GPS Week 873), WGS 84 was redefined again and was more closely aligned with International Earth Rotation Service (IERS) frame ITRF 94. It was then formally called WGS 84 (G873). WGS 84 (G873) was adopted as the reference frame for broadcast orbits on January 29, 1997. Another update brought it to WGS84(G1674).\n\nThe WGS 84 datum, within two meters of the NAD83 datum used in North America, is the only world referencing system in place today. WGS 84 is the default standard datum for coordinates stored in recreational and commercial GPS units.\n\nUsers of GPS are cautioned that they must always check the datum of the maps they are using. To correctly enter, display, and to store map related map coordinates, the datum of the map must be entered into the GPS map datum field.\n\nExamples of map datums are:\n\n\n\n"}
{"id": "2794830", "url": "https://en.wikipedia.org/wiki?curid=2794830", "title": "Geovisualization", "text": "Geovisualization\n\nGeovisualization or Geovisualisation, short for \"Geographic Visualization\", refers to a set of tools and techniques supporting the analysis of geospatial data through the use of interactive visualization.\n\nLike the related fields of scientific visualization and information visualization geovisualization emphasizes knowledge construction over knowledge storage or information transmission. To do this, geovisualization communicates geospatial information in ways that, when combined with human understanding, allow for data exploration and decision-making processes.\n\nTraditional, static maps have a limited exploratory capability; the graphical representations are inextricably linked to the geographical information beneath. GIS and geovisualization allow for more interactive maps; including the ability to explore different layers of the map, to zoom in or out, and to change the visual appearance of the map, usually on a computer display. Geovisualization represents a set of cartographic technologies and practices that take advantage of the ability of modern microprocessors to render changes to a map in real time, allowing users to adjust the mapped data on the fly.\n\nThe term visualization is first mentioned in the cartographic literature at least as early as 1953, in an article by University of Chicago geographer Allen K. Philbrick. New developments in the field of computer science prompted the National Science Foundation to redefine the term in a 1987 report which placed visualization at the convergence of computer graphics, image processing, computer vision, computer-aided design, signal processing, and user interface studies and emphasized both the knowledge creation and hypothesis generation aspects of scientific visualization.\n\nGeovisualization developed as a field of research in the early 1980s, based largely on the work of French graphic theorist Jacques Bertin. Bertin’s work on cartographic design and information visualization share with the National Science Foundation report a focus on the potential for the use of “dynamic visual displays as prompts for scientific insight and on the methods through which dynamic visual displays might leverage perceptual cognitive processes to facilitate scientific thinking”.\n\nGeovisualization has continued to grow as a subject of practice and research. The International Cartographic Association (ICA) established a Commission on Visualization & Virtual Environments in 1995.\n\nGeovisualization is closely related to other visualization fields, such as scientific visualization and information visualization. Owing to its roots in cartography, geovisualization contributes to these other fields by way of the map metaphor, which “has been widely used to visualize non-geographic information in the domains of information visualization and domain knowledge visualization.\" It is also related to urban simulation.\n\nGeovisualization has made inroads in a diverse set of real-world situations calling for the decision-making and knowledge creation processes it can provide. The following list provides a summary of some of these applications as they are discussed in the geovisualization literature.\nE\n\nFirefighters have been using sandbox environments to rapidly and physically model topography and fire for wildfire incident command strategic planning. The SimTable is a 3D interactive fire simulator, bringing sandtable exercises to life. The SimTable uses advanced computer simulations to model fires in any area, including local neighborhoods, utilizing actual slope, terrain, wind speed/direction, vegetation, and other factors. SimTable Models were used in Arizona's largest fire on record, the Wallow Fire.\n\nGeovisualizers, working with European foresters, used CommonGIS and Visualization Toolkit (VTK) to visualize a large set of spatio-temporal data related to European forests, allowing the data to be explored by non-experts over the Internet. The report summarizing this effort “uncovers a range of fundamental issues relevant to the broad field of geovisualization and information visualization research”.\n\nThe research team cited the two major problems as the inability of the geovisualizers to convince the foresters of the efficacy of geovisualization in their work and the foresters’ misgivings over the dataset’s accessibility to non-experts engaging in “uncontrolled exploration”. While the geovisualizers focused on the ability of geovisualization to aid in knowledge construction, the foresters preferred the information-communication role of more traditional forms of cartographic representation.\n\nGeovisualization provides archaeologists with a potential technique for mapping unearthed archaeological environments as well as for accessing and exploring archaeological data in three dimensions.\n\nThe implications of geovisualization for archaeology are not limited to advances in archaeological theory and exploration but also include the development of new, collaborative relationships between archaeologists and computer scientists.\n\nGeovisualization tools provide multiple stakeholders with the ability to make balanced environmental decisions by taking into account “the complex interacting factors that should be taken into account when studying environmental changes\". Geovisualization users can use a georeferenced model to explore a complex set of environmental data, interrogating a number of scenarios or policy options to determine a best fit.\n\nBoth planners and the general public can use geovisualization to explore real-world environments and model ‘what if’ scenarios based on spatio-temporal data. While geovisualization in the preceding fields may be divided into two separate domains—the private domain, in which professionals use geovisualization to explore data and generate hypotheses, and the public domain, in which these professionals present their “visual thinking” to the general public—planning relies more heavily than many other fields on collaboration between the general public and professionals.\n\nPlanners use geovisualization as a tool for modeling the environmental interests and policy concerns of the general public. Jiang et al. mention two examples, in which “3D photorealistic representations are used to show urban redevelopment [and] dynamic computer simulations are used to show possible pollution diffusion over the next few years.” The widespread use of the Internet by the general public has implications for these collaborative planning efforts, leading to increased participation by the public while decreasing the amount of time it takes to debate more controversial planning decisions.\n\n\n\n"}
{"id": "47470974", "url": "https://en.wikipedia.org/wiki?curid=47470974", "title": "Glacial refugium", "text": "Glacial refugium\n\nA glacial refugium (\"plural refugia\") is a geographic region which made possible the survival of flora and fauna in times of ice ages and allowed a post-glacial re-colonization. Different types of glacial refugia can be distinguished, namely nunatak, peripheral and lowland refugia. Glacial refugia have been suggested as a major cause of the distributions of flora and fauna in both temperate and tropical latitudes. However, in spite of the continuing use of historical refugia to explain modern-day species distributions, especially in birds, doubt has been cast on the validity of such inferences, as much of the differentiation between populations observed today may have occurred before or after their restriction to refugia.\n\n"}
{"id": "22764646", "url": "https://en.wikipedia.org/wiki?curid=22764646", "title": "Hellenic Military Geographical Service", "text": "Hellenic Military Geographical Service\n\nThe Hellenic Military Geographical Service or HMGS ( or Γ.Υ.Σ.), is the Greek military mapping agency.\n\nEstablished in 1889 as the “Geodetic Mission” with the purpose of compiling the “Topographic and Cadastral Map of the Country”, the service acquired its current name in 1926. Today it supports both the Greek Army and the general public with cartographic products.\n\nFrom 1966 until the 1980s, the service housed the Armed Forces Information Service, Greece's second state-run broadcaster alongside the National Radio Foundation.\n\nHMGS supplies City Plans of scales 1:5.000, 1:10.000 and 1:25.000 in Greek and Latin series\n\n\nHMGS produces and supplies topographic maps of various scales that cover the whole country and occasionally parts of neighbouring countries. It should be emphasised that not all maps noted below are available to public, since part of them is confidential and available only upon approval of HMGS. Although not a state monopoly and not very famous among hikers, certainly HMGS maps have a much wider coverage in small scales than commercial leisure maps. The maps used the Hellenic Geodetic Reference System 1987 (HGRS87 or ΕΓΣΑ'87) which specifies a Transverse Mercatorial Projection mapping Greece in one zone. \n\n\nThe products and services of HMGS are not available in the market. They can either be purchased using the e-shop of the service or collected from the offices of the service upon submitting the relevant request form.\n\n\n"}
{"id": "2853761", "url": "https://en.wikipedia.org/wiki?curid=2853761", "title": "Intercept method", "text": "Intercept method\n\nThe intercept method, also known as Marcq St. Hilaire method, is an astronomical navigation method of calculating an observer's position on earth. It was originally called the \"azimuth intercept\" method because the process involves drawing a line which intercepts the azimuth line. This name was shortened to \"intercept\" method and the \"intercept distance\" was shortened to 'intercept'.\n\nThe method yields a line of position (LOP) on which the observer is situated. The intersection of two or more such lines will define the observer's position, called a \"fix\". Sights may be taken at short intervals, usually during hours of twilight, or they may be taken at an interval of an hour or more (as in observing the Sun during the day). In either case, the lines of position, if taken at different times, must be advanced or retired to correct for the movement of the ship during the interval between observations. If observations are taken at short intervals, a few minutes at most, the corrected lines of position by convention yield a \"fix\". If the lines of position must be advanced or retired by an hour or more, convention dictates that the result is referred to as a \"running fix\".\n\nThe intercept method is based on the following principle.\nThe actual distance from the observer to the geographical position (GP) of a celestial body (that is, the point where it is directly overhead) is \"measured\" using a sextant. The observer has already estimated his position by dead reckoning and calculated the distance from the estimated position to the body's GP; the difference between the \"measured\" and calculated distances is called the intercept.\n\nThe diagram on the right shows why the zenith distance of a celestial body is equal to the angular distance of its GP from the observer's position.\n\nThe rays of light from a celestial body are assumed to be parallel (unless the observer is looking at the moon, which is too close for such a simplification). The angle at the centre of the Earth that the ray of light passing through the body's GP makes with the line running from the observer's zenith is the same as the zenith distance. This is because they are corresponding angles. In practice it is not necessary to use zenith distances, which are 90° minus altitude, as the calculations can be done using observed altitude and calculated altitude.\n\nTaking a sight using the intercept method consists of the following process:\n\nSuitable bodies for celestial sights are selected, often using a Rude Star Finder. Using a sextant, an altitude is obtained of the sun, the moon, a star or a planet. The name of the body and the precise time of the sight in UTC is recorded. Then the sextant is read and the altitude (\"Hs\") of the body is recorded. Once all sights are taken and recorded, the navigator is ready to start the process of sight reduction and plotting.\n\nThe first step in sight reduction is to correct the sextant altitude for various errors and corrections. The instrument may have an error, IC or index correction (See article on adjusting a sextant). Refraction by the atmosphere is corrected for with the aid of a table or calculation and the observer's height of eye above sea level results in a \"dip\" correction, (as the observer's eye is raised the horizon dips below the horizontal). If the Sun or Moon was observed, a semidiameter correction is also applied to find the centre of the object. The resulting value is \"observed altitude\" (\"Ho\").\n\nNext, using an accurate clock, the observed celestial object's geographic position (\"GP\") is looked up in an almanac. That's the point on the Earth's surface directly below it (where the object is in the zenith). The latitude of the geographic position is called declination, and the longitude is usually called the hour angle.\n\nNext, the altitude and azimuth of the celestial body are computed for a selected position (assumed position or AP). This involves resolving a spherical triangle. Given the three magnitudes: local hour angle (\"LHA\"), observed body's declination (\"dec\"), and assumed latitude (\"lat\"), the altitude \"Hc\" and azimuth \"Zn\" must be computed. The local hour angle, \"LHA\", is the difference between the AP longitude and the hour angle of the observed object. It is always measured in a westerly direction from the assumed position.\n\nThe relevant formulas (derived using the spherical trigonometric identities) are:\n\nor, alternatively,\n\nWhere\n\nThese computations can be done easily using electronic calculators or computers but traditionally there were methods which used logarithm or haversine tables. Some of these methods were H.O. 211 (Ageton), Davies, haversine, etc. The relevant haversine formula for \"Hc\" is\n\nWhere \"\" is the zenith distance, or complement of \"Hc\".\n\n\"\" = 90° - \"Hc\".\n\nThe relevant formula for Zn is\n\nWhen using such tables or a computer or scientific calculator, the navigation triangle is solved directly, so any assumed position can be used. Often the dead reckoning DR position is used. This simplifies plotting and also reduces any slight error caused by plotting a segment of a circle as a straight line.\n\nWith the use of astral navigation for air navigation, faster methods needed to be developed and tables of precomputed triangles were developed. When using precomputed sight reduction tables, selection of the assumed position is one of the trickier steps for the fledgling navigator to master. Sight reduction tables provide solutions for navigation triangles of integral degree values. When using precomputed sight reduction tables, such as H.O. 229, the assumed position must be selected to yield integer degree values for \"LHA\" (local hour angle) and latitude. West longitudes are subtracted and east longitudes are added to \"GHA\" to derive \"LHA\", so AP's must be selected accordingly. When using precomputed sight reduction tables each observation and each body will require a different assumed position.\n\nProfessional navigators are divided in usage between sight reduction tables on the one hand, and handheld computers or scientific calculators on the other. The methods are equally accurate. It is simply a matter of personal preference which method is used. An experienced navigator can reduce a sight from start to finish in about 5 minutes using nautical tables or a scientific calculator.\n\nThe precise location of the assumed position has no great impact on the result, as long as it is reasonably close to the observer's actual position. An assumed position within 1 degree of arc of the observer's actual position is usually considered acceptable.\n\nThe calculated altitude (\"Hc\") is compared to the observed altitude (\"Ho\", sextant altitude (\"Hs\") corrected for various errors). The difference between \"Hc\" and \"Ho\" is called \"intercept\" and is the observer's distance from the assumed position. The resulting line of position (\"LOP\") is a small segment of the circle of equal altitude, and is represented by a straight line perpendicular to the azimuth of the celestial body. When plotting the small segment of this circle on a chart it is drawn as a straight line, the resulting tiny errors are too small to be significant.\n\nNavigators use the memory aid \"computed greater away\" to determine whether the observer is farther from the body's geographic position (measure intercept from \"Hc\" away from the azimuth). If the \"Hc\" is less than \"Ho\", then the observer is closer to the body's geographic position, and intercept is measured from the AP toward the azimuth direction.\n\nThe last step in the process is to plot the lines of position \"LOP\" and determine the vessel's location. Each assumed position is plotted first. Best practise is to then advance or retire the assumed positions to correct for vessel motion during the interval between sights. Each LOP is then constructed from its associated AP by striking off the azimuth to the body, measuring intercept toward or away from the azimuth, and constructing the perpendicular line of position.\n\nTo obtain a fix (a position) this \"LOP\" must be crossed with another \"LOP\" either from another sight or from elsewhere e.g. a bearing of a point of land or crossing a depth contour such as the 200 metre depth line on a chart.\n\nUntil the age of satellite navigation ships usually took sights at dawn, during the forenoon, at noon (meridian transit of the Sun) and dusk. The morning and evening sights were taken during twilight while the horizon was visible and the stars, planets and/or moon were visible, at least through the telescope of a sextant. Two observations are always required to give a position accurate to within a mile under favourable conditions. Three are always sufficient.\n\nA fix is called a \"running fix\" when one or more of the LOPs used to obtain it is an LOP advanced or retrieved over time. In order to get a fix the LOP must cross at an angle, the closer to 90° the better. This means the observations must have different azimuths. During the day, if only the Sun is visible, it is possible to get an LOP from the observation but not a fix as another LOP is needed. What may be done is take a first sight which yields one LOP and, some hours later, when the Sun's azimuth has changed substantially, take a second sight which yields a second LOP. Knowing the distance and course sailed in the interval, the first LOP can be advanced to its new position and the intersection with the second LOP yields a \"running fix\".\n\nAny sight can be advanced and used to obtain a \"running fix\". It may be that the navigator due to weather conditions could only obtain a single sight at dawn. The resulting LOP can then be advanced when, later in the morning, a Sun observation becomes possible. The precision of a running fix depends on the error in distance and course so, naturally, a running fix tends to be less precise than an unqualified fix and the navigator must take into account his confidence in the exactitude of distance and course to estimate the resulting error in the running fix.\n\nDetermining a fix by crossing LOPs and advancing LOPs to get running fixes are not specific to the intercept method and can be used with any sight reduction method or with LOPs obtained by any other method (bearings, etc.).\n\n\n\n"}
{"id": "8097563", "url": "https://en.wikipedia.org/wiki?curid=8097563", "title": "LRK", "text": "LRK\n\nLong Range Kinematic (LRK) technology is a sophisticated kinematic method developed by Magellan (formerly Thales) Navigation that optimises the advantages of dual-frequency GPS operation. Other conventional methods use the dual-frequency only during initialisation. LRK makes solving ambiguities during initialisation easy and continuous dual-frequency kinematic operation possible at distances up to 40 kilometres.\n\nConventional dual-frequency kinematic operation is limited to about 10 kilometres, using a combined observation on GPS L1 and L2 frequencies to produce an initial wide lane solution, ambiguous to around 86 centimetres. During a second phase, the conventional kinematic method uses measurements from the L1 frequency only. This method only allows for kinematic operation as long as the de-correlation of atmospheric errors is compatible with a pure phase single-frequency solution.\n\nSimilar to the KART process, LRK is a simple and reliable method that allows any initialisation mode, from a static or fixed reference point, to On The Fly ambiguity resolution, when performing dual-frequency GPS positioning. LRK technology reduces initialisation times to a few seconds by efficiently using L2 measurements in every mode of operation. LRK maintains optimal real-time positioning accuracy to within a centimetre at a range up to 40-50 kilometres, even with a reduced number of visible satellites.\n\n"}
{"id": "216411", "url": "https://en.wikipedia.org/wiki?curid=216411", "title": "Land ethic", "text": "Land ethic\n\nA land ethic is a philosophy or theoretical framework about how, ethically, humans should regard the land. The term was coined by Aldo Leopold (1887–1948) in his \"A Sand County Almanac\" (1949), a classic text of the environmental movement. There he argues that there is a critical need for a \"new ethic,\" an \"ethic dealing with human's relation to land and to the animals and plants which grow upon it\". \n\nLeopold offers an ecologically based land ethic that rejects strictly human-centered views of the environment and focuses on the preservation of healthy, self-renewing ecosystems. \"A Sand County Almanac\" was the first systematic presentation of a holistic or ecocentric approach to the environment. Although Leopold is credited with coining the term \"land ethic,\" there are many philosophical theories that speak to how humans should treat the land. Some of the most prominent land ethics include those rooted in economics, utilitarianism, libertarianism, egalitarianism, and ecology.\n\nThis is a land ethic based wholly upon economic self-interest. Leopold sees two flaws in this type of ethic. First, he argues that most members of an ecosystem have no economic worth. For this reason, such an ethic can ignore or even eliminate these members when they are actually necessary for the health of the biotic community of the land. And second, it tends to relegate conservation necessary for healthy ecosystems to the government and these tasks are too large and dispersed to be adequately addressed by such an institution. This ties directly into the context within which Leopold wrote \"A Sand County Almanac.\"\n\nFor example, when the US Forest Service was founded by Gifford Pinchot, the prevailing ethos was economic and utilitarian. Leopold argued for an ecological approach, becoming one of the first to popularize this term coined by Henry Chandler Cowles of the University of Chicago during his early 1900s research at the Indiana Dunes. Conservation became the preferred term for the more anthropocentric model of resource management, while the writing of Leopold and his inspiration, John Muir, led to the development of environmentalism.\n\nUtilitarianism was first put forth by British philosophers Jeremy Bentham and John Stuart Mill. Though there are many varieties of utilitarianism, generally it is the view that a morally right action is an action that produces the maximum good for people. Utilitarianism has often been used when deciding how to use land and it is closely connected with an economic-based ethic. For example, it forms the foundation for industrial farming; an increase in yield, which would increase the number of people able to receive goods from farmed land, is judged from this view to be a good action or approach. In fact, a common argument in favor of industrial agriculture is that it is a good practice because it increases the benefits for humans; benefits such as food abundance and a drop in food prices. However, a utilitarian-based land ethic is different from a purely economic one as it could be used to justify the limiting of a person's rights to make profit. For example, in the case of the farmer planting crops on a slope, if the runoff of soil into the community creek led to the damage of several neighbor's properties, then the good of the individual farmer would be overridden by the damage caused to his neighbors. Thus, while a utilitarian-based land ethic can be used to support economic activity, it can also be used to challenge this activity.\n\nAnother philosophical approach often used to guide actions when making (or not making) changes to the land is libertarianism. Roughly, libertarianism is the ethical view that agents own themselves and have particular moral rights, including the right to acquire property. In a looser sense, libertarianism is commonly identified with the belief that each individual person has a right to a maximum amount of freedom or liberty when this freedom does not interfere with other people's freedom. A well-known libertarian theorist is John Hospers. For libertarians, property rights are natural rights. Thus, it would be acceptable for the above farmer to plant on a slope as long as this action does not limit the freedom of his or her neighbors.\n\nThis view is closely connected to utilitarianism. Libertarians often use utilitarian arguments to support their own arguments. For example, in 1968, Garrett Hardin applied this philosophy to land issues when he argued that the only solution to the \"Tragedy of the Commons\" was to place soil and water resources into the hands of private citizens. Hardin supplied utilitarian justifications to support his argument. However, it can be argued that this leaves a libertarian-based land ethics open to the above critique lodged against economic-based approaches. Even excepting this, the libertarian view has been challenged by the critique that numerous people making self-interested decisions often cause large ecological disasters, such as the Dust Bowl disaster. Even so, libertarianism is a philosophical view commonly held within the United States and, especially, held by U.S. ranchers and farmers.\n\nEgalitarian-based land ethics are often developed as a response to libertarianism. This is because, while libertarianism ensures the maximum amount of human liberty, it does not require that people help others. In addition, it also leads to the uneven distribution of wealth. A well-known egalitarian philosopher is John Rawls. When focusing on land use, egalitarianism evaluates its uneven distribution and the uneven distribution of the fruits of that land. While both a utilitarian- and libertarian-based land ethic could conceivably rationalize this mal-distribution, an egalitarian approach typically favors equality, whether that be equal entitlement to land and/or access to food. However, there is also the question of negative rights when holding to an egalitarian-based ethic. In other words, if it is recognized that a person has a right to something, then someone has the responsibility to supply this opportunity or item; whether that be an individual person or the government. Thus, an egalitarian-based land ethic could provide a strong argument for the preservation of soil fertility and water because it links land and water with the right to food, with the growth of human populations, and the decline of soil and water resources.\n\nLand ethics may also be based upon the principle that the land (and the organisms that live off the land) has intrinsic value. These ethics are, roughly, based on an ecological or systems view. This position was first put forth by Ayers Brinser in \"Our Use of the Land\", published in 1939. Brinser argued that white settlers brought with them \"the seeds of a civilization which has grown by consuming the land, that is, a civilization which has used up the land in much the same way that a furnace burns coal.” Later, Aldo Leopold's posthumously published \"A Sand County Almanac\" (1949) popularized this idea. \n\nAnother example is the deep ecology view, which argues that human communities are built upon a foundation of the surrounding ecosystems or the biotic communities, and that all life is of inherent worth. Similar to egalitarian-based land ethics, the above land ethics were also developed as alternatives to utilitarian and libertarian-based approaches. Leopold's ethic is one of the most popular ecological approaches in the early 21st century. Other writers and theorists who hold this view include Wendell Berry (b. 1934), N. Scott Momaday, J. Baird Callicott, Paul B. Thompson, and Barbara Kingsolver.\n\nIn his classic essay, \"The Land Ethic,\" published posthumously in \"A Sand County Almanac\" (1949), Leopold proposes that the next step in the evolution of ethics is the expansion of ethics to include nonhuman members of the biotic community, collectively referred to as \"the land.\" Leopold states the basic principle of his land ethic as: \"A thing is right when it tends to preserve the integrity, stability, and beauty of the biotic community. It is wrong when it tends otherwise.\"\n\nHe also describes it in this way: \"The land ethic simply enlarges the boundaries of the community to include soils, waters, plants, and animals, or collectively: the land . . . [A] land ethic changes the role of \"Homo sapiens\" from conqueror of the land-community to plain member and citizen of it. It implies respect for his fellow-members, and also respect for the community as such.\"\n\nLeopold was a naturalist, not a philosopher. There is much scholarly debate about what exactly Leopold's land ethic asserts and how he argues for it. At its core, the land ethics claims (1) that humans should view themselves as plain members and citizens of biotic communities, not as \"conquerors\" of the land; (2) that we should extend ethical consideration to ecological wholes (\"soils, waters, plants, and animals\"), (3) that our primary ethical concern should not be with individual plants or animals, but with the healthy functioning of whole biotic communities, and (4) that the \"summary moral maxim\" of ecological ethics is that we should seek to preserve the integrity, stability, and beauty of the biotic community. Beyond this, scholars disagree about the extent to which Leopold rejected traditional human-centered approaches to the environment and how literally he intended his basic moral maxim to be applied. They also debate whether Leopold based his land ethic primarily on human-centered interests, as many passages in \"A Sand County Almanac\" suggest, or whether he placed significant weight on the intrinsic value of nature. One prominent student of Leopold, J. Baird Callicott, has suggested that Leopold grounded his land ethics on various scientific claims, including a Darwinian view of ethics as rooted in special affections for kith and kin, a Copernican view of humans as plain members of nature and the cosmos, and the finding of modern ecology that ecosystems are complex, interrelated wholes. However, this interpretation has recently been challenged by Roberta Millstein, who has offered evidence that Darwin's influence on Leopold was not related to Darwin's views about moral sentiments, but rather to Darwin's views about interdependence in the struggle for existence.\n\nLeopold's ecocentric land ethic is popular today with mainstream environmentalists for a number of reasons. Unlike more radical environmental approaches, such as deep ecology or biocentrism, it does not require huge sacrifices of human interests. Leopold does not, for example, believe that humans should stop eating or hunting or experimenting on animals. Nor does he call for a massive reduction in human population, or for permitting humans to interfere with nature only to satisfy vital human needs (regardless of economic or other human costs). As an environmental ethic, Leopold's land ethic is a comparatively moderate view that seeks to strike a balance between human interests and a healthy and biotically diverse natural environment. Many of the things mainstream environmentalists favor--preference for native plants and animals over invasive species, hunting or selective culling to control overpopulated species that are damaging to the environment, and a focus on preserving healthy, self-regenerating natural ecosystems both for human benefit and for their own intrinsic value--jibe with Leopold's ecocentric land ethic.\n\nA related understanding has been framed as global land as a commons. In this view biodiversity and terrestrial carbon storage - an element of climate change mitigation - are global public goods. Hence, land should be governed on a global scale as a commons, requiring increased international cooperation on nature preservation .\n\nSome critics fault Leopold for lack of clarity in spelling out exactly what the land ethic is and its specific implications for how humans should think about the environment. It is clear that Leopold did not intend his basic normative principle (\"A thing is right when it tends to preserve the integrity, stability, and beauty of the biotic community\") to be regarded as an ethical absolute. Thus construed, it would prohibit clearing land to build homes, schools, or farms, and generally require a \"hands-off\" approach to nature that Leopold plainly did not favor. Presumably, therefore, his maxim should be seen as a general guideline for valuing natural ecosystems and striving to achieve what he terms a sustainable state of \"harmony between men and land.\" But this is vague and, according to some critics, not terribly helpful.\n\nA second common criticism of Leopold is that he fails to state clearly why we should adopt the land ethic. He often cites examples of environmental damage (e.g., soil erosion, pollution, and deforestation) that result from traditional human-centered, \"conqueror\" attitudes towards nature. But it is unclear why such examples support the land ethic specifically, as opposed to biocentricism or some other nature-friendly environmental ethic. Leopold also frequently appeals to modern ecology, evolutionary theory, and other scientific discoveries to support his land ethic. Some critics have suggested that such appeals may involve an illicit move from facts to values. At a minimum, such critics claim, more should be said about the normative basis of Leopold's land ethic.\n\nOther critics object to Leopold's ecological holism. According to animal rights advocate, Tom Regan, Leopold's land ethic condones sacrificing the good of individual animals to the good of the whole, and is thus a form of \"environmental fascism.\" According to these critics, we rightly reject such holistic approaches in human affairs. Why, they ask, should we adopt them in our treatment of non-human animals?\n\nFinally, some critics have questioned whether Leopold's land ethic might require unacceptable interferences with nature in order to protect current, but transient, ecological balances. If the fundamental environmental imperative is to \"preserve\" the integrity and stability of natural ecosystems, wouldn't this require frequent and costly human interventions to prevent naturally occurring changes to natural environments? In nature, the \"stability and integrity\" of ecosystems are disrupted or destroyed all the time by drought, fire, storms, pests, newly invasive predators, etc. Must humans act to prevent such ecological changes, and if so, at what cost? Why should we place such high value on current ecological balances? Why think it is our role to be nature's steward or policeman? According to these critics, Leopold's stress on preserving existing ecological balances is overly human-centered and fails to treat nature with the respect it deserves.\n\n\n"}
{"id": "205135", "url": "https://en.wikipedia.org/wiki?curid=205135", "title": "Landscape", "text": "Landscape\n\nA landscape is the visible features of an area of land, its landforms and how they integrate with natural or man-made features.\n\nA landscape includes the physical elements of geophysically defined landforms such as (ice-capped) mountains, hills, water bodies such as rivers, lakes, ponds and the sea, living elements of land cover including indigenous vegetation, human elements including different forms of land use, buildings and structures, and transitory elements such as lighting and weather conditions.\n\nCombining both their physical origins and the cultural overlay of human presence, often created over millennia, landscapes reflect a living synthesis of people and place that is vital to local and national identity. The character of a landscape helps define the self-image of the people who inhabit it and a sense of place that differentiates one region from other regions. It is the dynamic backdrop to people’s lives. Landscape can be as varied as farmland, a landscape park, or wilderness.\n\nThe Earth has a vast range of landscapes, including the icy landscapes of polar regions, mountainous landscapes, vast arid desert landscapes, islands and coastal landscapes, densely forested or wooded landscapes including past boreal forests and tropical rainforests, and agricultural landscapes of temperate and tropical regions.\n\nThe activity of modifying the visible features of an area of land is referred to as \"landscaping\".\n\nThere are several definitions of what constitutes a landscape, depending on context. In common usage however, a landscape refers either to all the visible features of an area of land (usually rural), often considered in terms of aesthetic appeal, or to a pictorial representation of an area of countryside, specifically within the genre of landscape painting. When people deliberately improve the aesthetic appearance of a piece of land—by changing contours and vegetation, etc.—it is said to have been landscaped, though the result may not constitute a landscape according to some definitions.\n\nThe word \"landscape\" (\"landscipe\" or \"landscaef\") arrived in England—and therefore into the English language—after the fifth century, following the arrival of the Anglo-Saxons; these terms referred to a system of human-made spaces on the land. The term \"landscape\" emerged around the turn of the sixteenth century to denote a painting whose primary subject matter was natural scenery. \"Land\" (a word from Germanic origin) may be taken in its sense of something to which people belong (as in England being the land of the English). The suffix \"‑scape\" is equivalent to the more common English suffix \"‑ship.\" The roots of \"‑ship\" are etymologically akin to Old English \"sceppan\" or \"scyppan\", meaning \"to shape\". The suffix \"‑schaft\" is related to the verb \"schaffen\", so that \"‑ship\" and \"shape\" are also etymologically linked. The modern form of the word, with its connotations of scenery, appeared in the late sixteenth century when the term \"landschap\" was introduced by Dutch painters who used it to refer to paintings of inland natural or rural scenery. The word \"landscape\", first recorded in 1598, was borrowed from a Dutch painters' term. The popular conception of the \"landscape\" that is reflected in dictionaries conveys both a particular and a general meaning, the particular referring to an area of the Earth's surface and the general being that which can be seen by an observer. An example of this second usage can be found as early as 1662 in the Book of Common Prayer:\n\nThere are several words that are frequently associated with the word landscape:\n\nGeomorphology is the scientific study of the origin and evolution of topographic and bathymetric features created by physical or chemical processes operating at or near Earth's surface. Geomorphologists seek to understand why landscapes look the way they do, to understand landform history and dynamics and to predict changes through a combination of field observations, physical experiments and numerical modeling. Geomorphology is practiced within physical geography, geology, geodesy, engineering geology, archaeology and geotechnical engineering. This broad base of interests contributes to many research styles and interests within the field.\n\nThe surface of Earth is modified by a combination of surface processes that sculpt landscapes, and geologic processes that cause tectonic uplift and subsidence, and shape the coastal geography. Surface processes comprise the action of water, wind, ice, fire, and living things on the surface of the Earth, along with chemical reactions that form soils and alter material properties, the stability and rate of change of topography under the force of gravity, and other factors, such as (in the very recent past) human alteration of the landscape. Many of these factors are strongly mediated by climate. Geologic processes include the uplift of mountain ranges, the growth of volcanoes, isostatic changes in land surface elevation (sometimes in response to surface processes), and the formation of deep sedimentary basins where the surface of Earth drops and is filled with material eroded from other parts of the landscape. The Earth surface and its topography therefore are an intersection of climatic, hydrologic, and biologic action with geologic processes.\n\nDesert, Plain, Taiga, Tundra, Wetland, Mountain, Mountain range, Cliff, Coast, Littoral zone, Glacier, Polar regions of Earth, Shrubland, Forest, Rainforest, Woodland, Jungle, Moors.\n\nLandscape ecology is the science of studying and improving relationships between ecological processes in the environment and particular ecosystems. This is done within a variety of landscape scales, development spatial patterns, and organizational levels of research and policy.\n\nLandscape is a central concept in landscape ecology. It is, however, defined in quite different ways. For example: Carl Troll conceives of landscape not as a mental construct but as an objectively given ‘organic entity’, a ‘‘harmonic individuum of space’’. \nErnst Neef defines landscapes as sections within the uninterrupted earth-wide interconnection of geofactors which are defined as such on the basis of their uniformity in terms of a specific land use, and are thus defined in an anthropocentric and relativistic way.\n\nAccording to Richard Forman and Michael Godron, a landscape is a heterogeneous land area composed of a cluster of interacting ecosystems that is repeated in similar form throughout, whereby they list woods, meadows, marshes and villages as examples of a landscape’s ecosystems, and state that a landscape is an area at least a few kilometres wide. \nJohn A. Wiens opposes the traditional view expounded by Carl Troll, Isaak S. Zonneveld, Zev Naveh, Richard T. T. Forman/Michel Godron and others that landscapes are arenas in which humans interact with their environments on a kilometre-wide scale; instead, he defines 'landscape'—regardless of scale—as \"the template on which spatial patterns influence ecological processes\".\nSome define 'landscape' as an area containing two or more ecosystems in close proximity.\n\nIntegrated landscape management is a way of managing a landscape that brings together multiple stakeholders, who collaborate to integrate policy and practice for their different land use objectives, with the purpose of achieving sustainable landscapes. It recognises that, for example, one river basin can supply water for towns and agriculture, timber and food crops for smallholders and industry, and habitat for biodiversity; the way in which each one of these sectors pursues its goals can have impacts on the others. The intention is to minimise conflict between these different land use objectives and ecosystem services. This approach draws on landscape ecology, as well as many related fields that also seek to integrate different land uses and users, such as watershed management.\n\nProponents of integrated landscape management argue that it is well-suited to address complex global challenges, such as those that are the focus of the Sustainable Development Goals. Integrated landscape management is increasingly taken up at the national, local and international level, for example the UN Environment Programme states that \"UNEP champions the landscape approach de facto as it embodies the main elements of integrated ecosystem management\".\n\nLandscape archaeology or landscape history is the study of the way in which humanity has changed the physical appearance of the environment - both present and past. Landscape generally refers to both natural environments and environments constructed by human beings. Natural landscapes are considered to be environments that have not been altered by humans in any shape or form. Cultural landscapes, on the other hand, are environments that have been altered in some manner by people (including temporary structures and places, such as campsites, that are created by human beings). Among archaeologists, the term landscape can refer to the meanings and alterations people mark onto their surroundings. As such, landscape archaeology is often employed to study the human use of land over extensive periods of time.\nLandscape archaeology can be summed up by Nicole Branton's statement: \n\nThe concept of cultural landscapes can be found in the European tradition of landscape painting. From the 16th century onwards, many European artists painted landscapes in favor of people, diminishing the people in their paintings to figures subsumed within broader, regionally specific landscapes.\n\nThe geographer Otto Schlüter is credited with having first formally used \"cultural landscape\" as an academic term in the early 20th century. In 1908, Schlüter argued that by defining geography as a \"Landschaftskunde\" (landscape science) this would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (transl. original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (transl. 'cultural landscape') a landscape created by human culture. The major task of geography was to trace the changes in these two landscapes.\"\n\nIt was Carl O. Sauer, a human geographer, who was probably the most influential in promoting and developing the idea of cultural landscapes. Sauer was determined to stress the agency of culture as a force in shaping the visible features of the Earth’s surface in delimited areas. Within his definition, the physical environment retains a central significance, as the medium with and through which human cultures act. His classic definition of a 'cultural landscape' reads as follows:\nThe cultural landscape is fashioned from a natural landscape by a cultural group. Culture is the agent, the natural area is the medium, the cultural landscape is the result.\n\nA cultural landscape, as defined by the World Heritage Committee, is the \"cultural properties [that] represent the combined works of nature and of man.\"\n\nThe World Heritage Committee identifies three categories of cultural landscape, ranging from (i) those landscapes most deliberately 'shaped' by people, through (ii) full range of 'combined' works, to (iii) those least evidently 'shaped' by people (yet highly valued). The three categories extracted from the Committee's Operational Guidelines, are as follows:\n\n\nThe Chinese garden is a landscape garden style which has evolved over three thousand years. It includes both the vast gardens of the Chinese emperors and members of the Imperial Family, built for pleasure and to impress, and the more intimate gardens created by scholars, poets, former government officials, soldiers and merchants, made for reflection and escape from the outside world. They create an idealized miniature landscape, which is meant to express the harmony that should exist between man and nature.\nA typical Chinese garden is enclosed by walls and includes one or more ponds, scholar's rocks, trees and flowers, and an assortment of halls and pavilions within the garden, connected by winding paths and zig-zag galleries. By moving from structure to structure, visitors can view a series of carefully composed scenes, unrolling like a scroll of landscape paintings.\n\nThe English landscape garden, also called English landscape park or simply the 'English garden', is a style of parkland garden intended to look as though it might be a natural landscape, although it may be very extensively re-arranged. It emerged in England in the early 18th century, and spread across Europe, replacing the more formal, symmetrical \"jardin à la française\" of the 17th century as the principal style for large parks and gardens in Europe. The English garden (and later French landscape garden) presented an idealized view of nature. It drew inspiration from paintings of landscapes by Claude Lorraine and Nicolas Poussin, and from the classic Chinese gardens of the East, which had recently been described by European travellers and were realized in the Anglo-Chinese garden, and the philosophy of Jean-Jacques Rousseau (1712 – 1778).\n\nThe English garden usually included a lake, sweeps of gently rolling lawns set against groves of trees, and recreations of classical temples, Gothic ruins, bridges, and other picturesque architecture, designed to recreate an idyllic pastoral landscape. The work of Lancelot \"Capability\" Brown and Humphry Repton was particularly influential. By the end of the 18th century the English garden was being imitated by the French landscape garden, and as far away as St. Petersburg, Russia, in Pavlovsk, the gardens of the future Emperor Paul. It also had a major influence on the form of the public parks and gardens which appeared around the world in the 19th century.\n\nLandscape architecture is a multi-disciplinary field, incorporating aspects of botany, horticulture, the fine arts, architecture, industrial design, geology and the earth sciences, environmental psychology, geography, and ecology. The activities of a landscape architect can range from the creation of public parks and parkways to site planning for campuses and corporate office parks, from the design of residential estates to the design of civil infrastructure and the management of large wilderness areas or reclamation of degraded landscapes such as mines or landfills. Landscape architects work on all types of structures and external space – large or small, urban, suburban and rural, and with \"hard\" (built) and \"soft\" (planted) materials, while paying attention to ecological sustainability.\n\nFor the period before 1800, the history of landscape gardening (later called landscape architecture) is largely that of master planning and garden design for manor houses, palaces and royal properties, religious complexes, and centers of government. An example is the extensive work by André Le Nôtre at Vaux-le-Vicomte and at the Palace of Versailles for King Louis XIV of France. The first person to write of making a landscape was Joseph Addison in 1712. The term landscape architecture was invented by Gilbert Laing Meason in 1828 and was first used as a professional title by Frederick Law Olmsted in 1863. During the latter 19th century, the term landscape architect became used by professional people who designed landscapes. Frederick Law Olmsted used the term 'landscape architecture' as a profession for the first time when designing Central Park, New York City, US. Here the combination of traditional landscape gardening and the emerging field of city planning gave landscape architecture its unique focus. This use of the term landscape architect became established after Frederick Law Olmsted, Jr. and others founded the American Society of Landscape Architects (ASLA) in 1899.\n\nPossibly the earliest landscape literature is found in Australian aboriginal myths (also known as Dreamtime or Dreaming stories, songlines, or Aboriginal oral literature), the stories traditionally performed by Aboriginal peoples within each of the language groups across Australia. All such myths variously tell significant truths within each Aboriginal group's local landscape. They effectively layer the whole of the Australian continent's topography with cultural nuance and deeper meaning, and empower selected audiences with the accumulated wisdom and knowledge of Australian Aboriginal ancestors back to time immemorial.\n\nIn the West pastoral poetry represent the earliest form of landscape literature, though this literary genre presents an idealized landscape peopled by shepherds and shepherdesses, and creates \"an image of a peaceful uncorrupted existence; a kind of prelapsarian world\". The pastoral has its origins in the works of the Greek poet Theocritus (c. 316 - c. 260 BC). The Romantic period poet William Wordsworth created a modern, more realistic form of pastoral with \"Michael, A Pastoral Poem\" (1800).\n\nAn early form of landscape poetry, Shanshui poetry, developed in China during the third and fourth centuries A.D.\n\nTopographical poetry is a genre of poetry that describes, and often praises, a landscape or place. John Denham's 1642 poem \"Cooper's Hill\" established the genre, which peaked in popularity in 18th-century England. Examples of topographical verse date, however, to the Late Classical period, and can be found throughout the Medieval era and during the Renaissance. Though the earliest examples come mostly from continental Europe, the topographical poetry in the tradition originating with Denham concerns itself with the classics, and many of the various types of topographical verse, such as river, ruin, or hilltop poems were established by the early 17th century. Alexander Pope's \"Windsor Forest\" (1713) and John Dyer's \"Grongar Hill' (1762) are two other familiar examples. George Crabbe, the Suffolk regional poet, also wrote topographical poems, as did William Wordsworth, of which \"Lines written a few miles above Tintern Abbey\" is an obvious example. More recently, Matthew Arnold's \"The Scholar Gipsy\" (1853) praises the Oxfordshire countryside, and W. H. Auden's \"In Praise of Limestone\" (1948) uses a limestone landscape as an allegory.\n\nSubgenres of topographical poetry include the country house poem, written in 17th-century England to compliment a wealthy patron, and the prospect poem, describing the view from a distance or a temporal view into the future, with the sense of opportunity or expectation. When understood broadly as landscape poetry and when assessed from its establishment to the present, topographical poetry can take on many formal situations and types of places. Kenneth Baker, in his \"Introduction to \"The Faber Book of Landscape Poetry\", identifies 37 varieties and compiles poems from the 16th through the 20th centuries—from Edmund Spenser to Sylvia Plath—correspondent to each type, from \"Walks and Surveys,\" to \"Mountains, Hills, and the View from Above,\" to \"Violation of Nature and the Landscape,\" to \"Spirits and Ghosts.\"\n\nCommon aesthetic registers of which topographical poetry makes use include pastoral imagery, the sublime, and the picturesque, which include images of rivers, ruins, moonlight, birdsong, and clouds, peasants, mountains, caves, and waterscapes.\n\nThough describing a landscape or scenery, topographical poetry often, at least implicitly, addresses a political issue or the meaning of nationality in some way. The description of the landscape therefore becomes a poetic vehicle for a political message. For example, in John Denham's \"Cooper's Hill,\" the speaker discusses the merits of the recently executed Charles I.\n\nOne important aspect of British Romanticism – evident in painting and literature as well as in politics and philosophy – was a change in the way people perceived and valued the landscape. In particular, after William Gilpin's \"Observations on the River Wye\" was published in 1770, the idea of the picturesque began to influence artists and viewers. Gilpin advocated approaching the landscape \"by the rules of picturesque beauty,\" which emphasized contrast and variety. Edmund Burke's \"A Philosophical Enquiry into the Origin of Our Ideas of the Sublime and Beautiful\" (1757) was also an influential text, as was Longinus' \"On the Sublime\" (early A.D., Greece), which was translated into English from the French in 1739. From the 18th century, a taste for the sublime in the natural landscape emerged alongside the idea of the sublime in language; that is elevated rhetoric or speech. A topographical poem that influenced the Romantics, was James Thomson's \"The Seasons\" (1726–30).\nThe changing landscape, brought about by the industrial and agricultural revolutions, with the expansion of the city and depopulation of the countryside, was another influences on the growth of the Romantic movement in Britain. The poor condition of workers, the new class conflicts, and the pollution of the environment all led to a reaction against urbanism and industrialisation and a new emphasis on the beauty and value of nature and landscape. However, it was also a revolt against aristocratic social and political norms of the Age of Enlightenment, as well a reaction against the scientific rationalisation of nature.\n\nThe poet William Wordsworth was a major contributor to the literature of landscape, as was his contemporary poet and novelist Walter Scott. Scott's influence was felt throughout Europe, as well as on major Victorian novelists in Britain, such as Emily Bronte, Mrs Gaskell, George Eliot, and Thomas Hardy, as well as John Cowper Powys in the 20th-century. Margaret Drabble in \"A Writer's Britain\" suggests that Thomas Hardy \"is perhaps the greatest writer of rural life and landscape\" in English.\n\nAmong European writers influenced by Scott were Frenchmen Honoré de Balzac and Alexandre Dumas and Italian Alessandro Manzoni. Manzoni's famous novel \"The Betrothed\" was inspired by Walter Scott's \"Ivanhoe\".\n\nAlso influenced by Romanticism's approach to landscape was the American novelist Fenimore Cooper, who was admired by Victor Hugo and Balzac and characterized as the \"American Scott.\"\n\nLandscape in Chinese poetry has often been closely tied to Chinese landscape painting, which developed much earlier than in the West. Many poems evoke specific paintings, and some are written in more empty areas of the scroll itself. Many painters also wrote poetry, especially in the scholar-official or literati tradition. Landscape images were present in the early \"Shijing\" and the \"Chuci\", but in later poetry the emphasis changed, as in painting]] to the \"Shan shui\" ( lit. \"mountain-water\") style featuring wild mountains, rivers and lakes, rather than landscape as a setting for a human presence. Shanshui poetry developed in China during the third and fourth centuries AD and left most of the varied landscapes of China largely unrepresented. \"Shan shui\" painting and poetry shows imaginary landscapes, though with features typical of some parts of South China; they remain popular to the present day.\n\nFields and Gardens poetry (), in poetry) was a contrasting poetic movement which lasted for centuries, with a focused on the nature found in gardens, in backyards, and in the cultivated countryside. Fields and Gardens poetry is one of many Classical Chinese poetry genres. One of the main practitioners of the Fields and Gardens poetry genre was Tao Yuanming (also known as Tao Qian (365–427), among other names or versions of names). Tao Yuanming has been regarded as the first great poet associated with the Fields and Gardens poetry genre.\n\nMany landscape photographs show little or no human activity and are created in the pursuit of a pure, unsullied depiction of nature devoid of human influence, instead featuring subjects such as strongly defined landforms, weather, and ambient light. As with most forms of art, the definition of a landscape photograph is broad, and may include urban settings, industrial areas, and nature photography. Notable landscape photographers include Ansel Adams, Galen Rowell, Edward Weston, Ben Heine, Mark Gray and Fred Judge.\n\nThe earliest forms of art around the world depict little that could really be called landscape, although ground-lines and sometimes indications of mountains, trees or other natural features are included. The earliest \"pure landscapes\" with no human figures are frescos from Minoan Greece of around 1500 BCE. Hunting scenes, especially those set in the enclosed vista of the reed beds of the Nile Delta from Ancient Egypt, can give a strong sense of place, but the emphasis is on individual plant forms and human and animal figures rather than the overall landscape setting. For a coherent depiction of a whole landscape, some rough system of perspective, or scaling for distance, is needed, and this seems from literary evidence to have first been developed in Ancient Greece in the Hellenistic period, although no large-scale examples survive. More ancient Roman landscapes survive, from the 1st century BCE onwards, especially frescos of landscapes decorating rooms that have been preserved at archaeological sites of Pompeii, Herculaneum and elsewhere, and mosaics.\n\nThe Chinese ink painting tradition of shan shui (\"mountain-water\"), or \"pure\" landscape, in which the only sign of human life is usually a sage, or a glimpse of his hut, uses sophisticated landscape backgrounds to figure subjects, and landscape art of this period retains a classic and much-imitated status within the Chinese tradition.\n\nBoth the Roman and Chinese traditions typically show grand panoramas of imaginary landscapes, generally backed with a range of spectacular mountains – in China often with waterfalls and in Rome often including sea, lakes or rivers. These were frequently used to bridge the gap between a foreground scene with figures and a distant panoramic vista, a persistent problem for landscape artists.\n\nA major contrast between landscape painting in the West and East Asia has been that while in the West until the 19th century it occupied a low position in the accepted hierarchy of genres, in East Asia the classic Chinese mountain-water ink painting was traditionally the most prestigious form of visual art. However, in the West, history painting came to require an extensive landscape background where appropriate, so the theory did not entirely work against the development of landscape painting – for several centuries landscapes were regularly promoted to the status of history painting by the addition of small figures to make a narrative scene, typically religious or mythological.\nDutch Golden Age painting of the 17th century saw the dramatic growth of landscape painting, in which many artists specialized, and the development of extremely subtle realist techniques for depicting light and weather. The popularity of landscapes in the Netherlands was in part a reflection of the virtual disappearance of religious painting in a Calvinist society, and the decline of religious painting in the 18th and 19th centuries all over Europe combined with Romanticism to give landscapes a much greater and more prestigious place in 19th-century art than they had assumed before.\n\nIn England, landscapes had initially been mostly backgrounds to portraits, typically suggesting the parks or estates of a landowner, though mostly painted in London by an artist who had never visited the site. the English tradition was founded by Anthony van Dyck and other, mostly Flemish, artists working in England. By the beginning of the 19th century the English artists with the highest modern reputations were mostly dedicated landscapists, showing the wide range of Romantic interpretations of the English landscape found in the works of John Constable, J.M.W. Turner and Samuel Palmer. However all these had difficulty establishing themselves in the contemporary art market, which still preferred history paintings and portraits. \nIn Europe, as John Ruskin said, and Sir Kenneth Clark confirmed, landscape painting was the \"chief artistic creation of the nineteenth century\", and \"the dominant art\", with the result that in the following period people were \"apt to assume that the appreciation of natural beauty and the painting of landscape is a normal and enduring part of our spiritual activity\"\n\nThe Romantic movement intensified the existing interest in landscape art, and remote and wild landscapes, which had been one recurring element in earlier landscape art, now became more prominent. The German Caspar David Friedrich had a distinctive style, influenced by his Danish training. To this he added a quasi-mystical Romanticism. French painters were slower to develop landscape painting, but from about the 1830s Jean-Baptiste-Camille Corot and other painters in the Barbizon School established a French landscape tradition that would become the most influential in Europe for a century, with the Impressionists and Post-Impressionists for the first time making landscape painting the main source of general stylistic innovation across all types of painting.\n\nIn the United States, the Hudson River School, prominent in the middle to late 19th century, is probably the best-known native development in landscape art. These painters created works of mammoth scale that attempted to capture the epic scope of the landscapes that inspired them. The work of Thomas Cole, the school's generally acknowledged founder, has much in common with the philosophical ideals of European landscape paintings — a kind of secular faith in the spiritual benefits to be gained from the contemplation of natural beauty. Some of the later Hudson River School artists, such as Albert Bierstadt, created less comforting works that placed a greater emphasis (with a great deal of Romantic exaggeration) on the raw, even terrifying power of nature. The best examples of Canadian landscape art can be found in the works of the Group of Seven, prominent in the 1920s. Emily Carr was also closely associated with the Group of Seven, though was never an official member. Although certainly less dominant in the period after World War I, many significant artists still painted landscapes in the wide variety of styles exemplified by Neil Welliver, Alex Katz, Milton Avery, Peter Doig, Andrew Wyeth, David Hockney and Sidney Nolan.\n\nThe term neo-romanticism is applied in British art history, to a loosely affiliated school of landscape painting that emerged around 1930 and continued until the early 1950s. These painters looked back to 19th-century artists such as William Blake and Samuel Palmer, but were also influenced by French cubist and post-cubist artists such as Pablo Picasso, André Masson, and Pavel Tchelitchew (; ). This movement was motivated in part as a response to the threat of invasion during World War II. Artists particularly associated with the initiation of this movement included Paul Nash, John Piper, Henry Moore, Ivon Hitchens, and especially Graham Sutherland. A younger generation included John Minton, Michael Ayrton, John Craxton, Keith Vaughan, Robert Colquhoun, and Robert MacBryde .\n\n"}
{"id": "1375216", "url": "https://en.wikipedia.org/wiki?curid=1375216", "title": "Landscape engineering", "text": "Landscape engineering\n\nLandscape engineering is the application of mathematics and science to shape land and waterscapes. It can also be described as green engineering, but the design professionals best known for landscape engineering are landscape architects. Landscape engineering is the interdisciplinary application of engineering and other applied sciences to the design and creation of anthropogenic landscapes. It differs from, but embraces traditional reclamation. It includes scientific disciplines: Agronomy, Botany, Ecology, Forestry, Geology, Geochemistry, Hydrogeology, and Wildlife Biology. It also draws upon applied sciences: Agricultural & Horticultural Sciences, Engineering Geomorphology, landscape architecture, and Mining, Geotechnical, and Civil, Agricultural & Irrigation Engineering.\n\nLandscape engineering builds on the engineering strengths of declaring goals, determining initial conditions, iteratively designing, predicting performance based on knowledge of the design, monitoring performance, and adjusting designs to meet the declared goals. It builds on the strengths and history of reclamation practice. Its distinguishing feature is the marriage of landforms, substrates, and vegetation throughout all phases of design and construction, which previously have been kept as separate disciplines.\n\nThough landscape engineering embodies all elements of traditional engineering (planning, investigation, design, construction, operation, assessment, research, management, and training), it is focused on three main areas. The first is closure planning – which includes goal setting and design of the landscape as a whole. The second division is landscape design more focused on the design of individual landforms to reliably meet the goals as set out in the closure planning process. Landscape performance assessment is critical to both of these, and is also important for estimating liability and levels of financial assurance. The iterative process of planning, design, and performance assessment by a multidisciplinary team is the basis of landscape engineering.\n\nSource: McKenna, G.T., 2002. \"Sustainable mine reclamation and landscape engineering\". PhD Thesis, University of Alberta, Edmonton, Canada 661p.\n\nThe father of the first Irrigation Engineering Degree in the Americas was Louis George Carpenter (March 28, 1861 – September 12, 1935) He was a college Professor and later the Dean of Engineering & Physics at Colorado State University formerly known as the Colorado Agricultural College. He was also an Engineer, Mathematician and an Irrigation and Consulting Engineer.\n\nIt was there where Carpenter began the first organized and systematic college program for irrigation engineering starting in 1888. Those completing such instruction were awarded a Bachelor of Science degree in Irrigation Engineering.\n\nCarpenter was one of the foremost leading experts on irrigation systems. During his life he investigated irrigation systems not only in North America but also in Canada and Europe. This led to his engineering consulting and water law. He became Colorado's State Engineer which he held for several years while still teaching. Carpenter was involved in not only in irrigation engineering but consulting on hydraulic construction projects and the problems associated with such projects.\n\n"}
{"id": "374240", "url": "https://en.wikipedia.org/wiki?curid=374240", "title": "Landscaping", "text": "Landscaping\n\nLandscaping refers to any activity that modifies the visible features of an area of land, including:\n\nLandscaping requires expertise in horticulture and artistic design.\n\nConstruction requires study and observation. It is not the same in different parts of the world. Landscaping varies according to different regions. Therefore, normally local natural experts are recommended if it is done for the first time. Understanding of the site is one of the chief essentials for successful landscaping. Different natural features like terrain, topography, soil qualities, prevailing winds, depth of the frost line, and the system of native flora and fauna must be taken into account. Sometimes the land is not fit for landscaping. In order to landscape it, the land must be reshaped. This reshaping of land is called grading.\n\nRemoval of earth from the land is called cutting while when earth is added to the slope, it is called filling. Sometimes the grading process may involve removal of excessive waste (landfills), soil and rocks, so designers should take into account while in the planning stage.\n\nIn the start, the landscaping contractor makes a letter which is a rough design and layout of what could be done with the land in order to achieve the desired outcome. Different pencils are required to make graphics of the picture. Landscaping has become more technological than natural, as few projects begin without bulldozers, lawnmowers, or chainsaws. Different areas have different qualities of plants. When growing new grass, it should ideally be done in the spring and the fall seasons to maximize growth and to minimize the spread of weeds. It is generally agreed that Fertilizers are required for good plant growth. Some landscapers prefer to use mix gravel with rocks of varying sizes to add interest in large areas.\n\n"}
{"id": "1514091", "url": "https://en.wikipedia.org/wiki?curid=1514091", "title": "Linear referencing", "text": "Linear referencing\n\nLinear referencing (also called linear reference system or linear referencing system or LRS), is a method of spatial referencing, in which the locations of features are described in terms of measurements along a linear element, from a defined starting point, for example a milestone along a road. Each feature is located by either a point (e.g. a signpost) or a line (e.g. a no-passing zone). The system is designed so that if a segment of a route is changed, only those milepoints on the changed segment need to be updated.\nLinear referencing is suitable for management of data related to linear features like roads, railways, oil and gas transmission pipelines, power and data transmission lines, and rivers.\n\nA system for identifying the location of pipeline features and characteristics is by measuring distance from the start of the pipeline. An example linear reference address is: Engineering Station 1145 + 86 on pipeline Alpha = 114,586 feet from the start of the pipeline. With a reroute, cumulative stationing might not be the same as engineering stationing, because of the addition of the extra pipeline. Linear referencing systems compute the differences to resolve this dilemma.\n\nLinear referencing is one of a family of methods of expressing location. Coordinates such as latitude and longitude are another member of the family, as are landmark references such as \"5 km south of Ayers Rock.\" Linear referencing has traditionally been the expression of choice in engineering applications such as road and pipeline maintenance. One can more realistically dispatch a worker to a bridge 12.7 km along a road from a reference point, rather than to a pair of coordinates or a landmark. The road serves as the reference frame, just as the earth serves as the reference frame for latitude and longitude.\n\nLinear referencing can be used to define points along a linear feature with just a small amount of information such as the name of a road and the distance and bearing from a landmark along the road. This information can be communicated concisely via plaintext. For example: \"State route 4, 20 feet east of mile marker 187.\" Giving a latitude and longitude coordinate to a work crew is not meaningful unless the coordinate is plotted on a map. Often work crews work in remote areas without wireless connectivity which makes on-line digital maps not practical, and the relatively higher effort of providing offline maps or printed maps is not as economical as simply stating locations as offsets, or ranges of offsets, along a linear feature.\n\nLinear referencing systems can also be made to be both very precise and very accurate at a much lower cost than is needed to collect latitude and longitude coordinates with high accuracy, especially when the goal is sub-meter accuracy. This is highly dependent upon the width of the linear feature, its centerline, and the visibility of the landmarks and markers that are used to define linear reference offsets.\n\nOften, roads are created by engineers using CAD tools that have no geospatial reference at all, and LRS is the preferred method of defining data for linear features.\n\nConsequently, a major limitation of linear referencing is that specifying points that are not on a linear feature is troublesome and error-prone, though not entirely impossible. Consider for example a ski lodge located 100 meters to the right of the road, traveling north. The linear referencing system can be extended by specifying a lateral offset, but the absolute location (i.e. coordinates) of the lodge cannot be determined unless coordinates are specified for the road; that process is prone to error particularly on curved roads.\n\nAnother major drawback of linear referencing is that a modification in the alignment of a road (e.g. constructing a bypass around a town) changes the measurements that reference all downstream points. The system requires an extensive network of reference stations, and constant maintenance. In an era of mobile maps and GPS, this maintenance overhead for linear referencing systems challenges its long-term viability. (But see below for US Federal Highway Administration requirement that all State DOTs use LRS.)\n\nNonetheless, travel along a road is a linear experience, and at the very least, linear referencing will continue to have a conversational role. Linear referencing systems are recognized by the US Federal government as a valuable tool for specifying right of way data, and are now actually required for the States. Therefore, it is not likely to see LRS usage decline any time soon.\n\nThe US Federal Highway Administration is pushing states to move closer to standardization of LRS data with the ARNOLD requirement. To wit:\n\n\"On August 7, 2012, FHWA announced that the HPMS is expanding the requirement for State\nDepartments of Transportation (DOTs) to submit their LRS to include all public roads. This requirement\nwill be referred to as the All Road Network of Linear Referenced Data (ARNOLD)\" \n\nThe ARNOLD requirement sets the stage for systems that utilize both LRS and coordinates. Both systems are useful in different contexts, and while using latitude and longitude is becoming very popular due to the availability of practical and affordable devices for capturing and displaying global coordinate data, the use of LRS has widely been adopted for planning, engineering, and maintenance.\n\nLinear referencing is supported for example by several Geographic Information System software, including:\n\n\n"}
{"id": "47357272", "url": "https://en.wikipedia.org/wiki?curid=47357272", "title": "List of geographical tors", "text": "List of geographical tors\n\nThe following list enumerate and expand on notable tors.\n\nDartmoor represents one of the largest areas of exposed granite in the United Kingdom, covering an area of 368 square miles (954 square kilometres). It is part of a chain of granite stretching through Cornwall, as far as the Isles of Scilly.\n\nSome of the more durable granite survived to form the rocky crowns of Dartmoor tors. One of the best known is at Haytor, on the eastern part of the moor, whose granite is of unusually fine quality and was quarried from the hillside below the tor during the 19th and early 20th centuries. Its stone was used to construct the pillars outside the British Museum in London, and to build London Bridge. The last granite to be quarried there was used to build Exeter War Memorial in 1919.\n\nTen Tors is an annual weekend hike on Dartmoor.\n\n\nHills:\n\nThere are many tors in this area, notably in the Dark Peak where the host rock is Millstone Grit:\n\n\nIn addition there are hills which incorporate 'tor' in their name but yet do not feature the geomorphological feature described in this article. Examples include Mam Tor and Shining Tor.\n\n\nThere are numerous tors developed in the Cairngorm granite in the Scottish Highlands:\n\n\nTor Bay, one of the sandy beaches near Oxwich Bay on the Gower Peninsula in south Wales, is so-called because the beach is framed by a huge outcrop of Carboniferous Limestone.\n\n\n\nTors are very commonly found in the Telangana and the Rayalaseema regions of Andhra Pradesh\n\n\n\n"}
{"id": "2539168", "url": "https://en.wikipedia.org/wiki?curid=2539168", "title": "List of political and geographic borders", "text": "List of political and geographic borders\n\nBelow are separate lists of countries and dependencies with their land boundaries, and lists of which countries and dependencies border oceans and major seas. The first short section describes the borders or edges of continents and oceans/major seas. Disputed areas are not considered.\n\n\"(Notes: Dependencies and islands remote from continental land masses are not considered and are excluded from this list section; thus only continental land borders are considered. The only countries listed either straddle continents or are on a continent border.)\"\n\nSection Key: \n<br>\n( * ) = represents a country that crosses a continental land border\n\n\"(Notes: Dependencies are excluded from this section. See below. Only land boundaries are considered; maritime boundaries are excluded; see the List of countries and territories by maritime boundaries. Disputed territories are not considered, other than the inclusion by necessity, in a neutral fashion, of Western Sahara.)\"\n<br>\nSection Key: \n<br>\n( * ) = represents a sea (or other body of water)'s parent ocean; also it may represent a dependency's parent country\n\n\"(Note: All official and unofficial claims by countries and/or governments on the continent of Antarctica are excluded from this section list.)\"\n\n\"(Note: This section lists only dependencies with land boundaries)\"\nSection Key: \n<br>\n( * ) = represents a sea (or other body of water)'s parent ocean <br>\n\nSection Key: \n<br>\n( * ) = represents a dependency's parent country\n\n\"(Note: Seas are excluded from this list section.)\"\n\nSection Key: \n<br>\n( * ) = represents a dependency's parent country\n\n\"(Note: This section only includes large bodies of water that are designated as a \"sea\" by a large portion of geographers worldwide.)\"\n\n\n"}
{"id": "18596832", "url": "https://en.wikipedia.org/wiki?curid=18596832", "title": "List of political and geographic subdivisions by total area in excess of 1,000,000 square kilometers", "text": "List of political and geographic subdivisions by total area in excess of 1,000,000 square kilometers\n"}
{"id": "3710289", "url": "https://en.wikipedia.org/wiki?curid=3710289", "title": "List of popular place names", "text": "List of popular place names\n\nThis List of popular place names is derived from the US FIPS55 place name database (158,000 US place names) and the US GEOnet name server database (5.6 million non-US place names).\n\n\nThe data files were fetched on January 1, 2006.\n\n"}
{"id": "594140", "url": "https://en.wikipedia.org/wiki?curid=594140", "title": "List of postal codes", "text": "List of postal codes\n\nThis list shows an overview of postal code notation schemes for all countries that have postal or ZIP code systems.\n\n\nThe use of country codes in conjunction with postal codes started as a recommendation from CEPT (European Conference of Postal and Telecommunications Administrations) in the 1960s. In the original CEPT-recommendation the distinguishing signs of motor vehicles in international traffic (\"car codes\") were placed before the postal code, and separated from it by a \"-\" (dash). Codes were only used on international mail and were hardly ever used internally in each country.\n\nSince the late 1980s, however, a number of postal administrations have changed the recommended codes to the two-letter country codes of ISO 3166. This would allow a universal, standardized code set to be used, and bring it in line with country codes used elsewhere in the UPU (Universal Postal Union). Attempts were also made (without success) to make this part of the official address guidelines of the UPU. Recently introduced postal code systems where the UPU has been involved have included the ISO 3166 country code as an integral part of the postal code.\n\nAt present there are no universal guidelines as to which code set to use, and recommendations vary from country to country. In some cases, the applied country code will differ according to recommendations of the sender's postal administration. UPU recommends that the country name always be included as the last line of the address.\n\nIn the list above, the following principles have been applied:\n\n\n"}
{"id": "39568382", "url": "https://en.wikipedia.org/wiki?curid=39568382", "title": "List of presidential trips made by Christian Wulff", "text": "List of presidential trips made by Christian Wulff\n\nThis is a list of presidential visits to foreign countries made by Christian Wulff as President of Germany. Wulff held the office from his election on 2 July 2010 until his resignation on 17 February 2012.\n\n"}
{"id": "10397282", "url": "https://en.wikipedia.org/wiki?curid=10397282", "title": "Metres above sea level", "text": "Metres above sea level\n\nMetres above mean sea level (MAMSL) or simply metres above sea level (MASL or m a.s.l.) is a standard metric measurement in metres of the elevation or altitude of a location in reference to a historic mean sea level. Mean sea levels are affected by climate change and other factors and change over time. For this and other reasons, recorded measurements of elevation above sea level might differ from the actual elevation of a given location over sea level at a given moment.\n\nMetres above sea level is the standard measurement of the elevation or altitude of:\n\nThe elevation or altitude in metres above sea level of a location, object, or point can be determined in a number of ways. The most common include:\n\nAccurate measurement of historical mean sea levels is complex. Land mass subsidence (as occurs naturally in some islands) can give the appearance of rising sea levels. Conversely, markings on land masses that are uplifted due to geological processes can suggest a lowering of mean sea level.\n\nFeet above sea level is the most common analogue for metres above sea level in the US customary measurement system.\n\nMetres above sea level is commonly abbreviated mamsl or MAMSL, based on the abbreviation AMSL for above mean sea level. Other abbreviations are m.a.s.l. and MASL.\n\n"}
{"id": "25147432", "url": "https://en.wikipedia.org/wiki?curid=25147432", "title": "Olfactory navigation", "text": "Olfactory navigation\n\nOlfactory navigation is a hypothesis put forward to explain navigation and homing of pigeons, in particular the homing pigeon.\n\nThere are two principal versions. Papi’s mosaic model proposes that pigeons construct a map from the distribution of environmental odours, within a radius of 70-100 kilometres. Wallraff’s gradient theory overcomes the problem of distance limitation by proposing the existence of long-range, stable atmospheric odour gradients. However, the evidence to suggest that pigeons use an ‘olfactory map’ in order to home is not conclusive.\n\nHoming can be defined as the ability to return to a set point from potentially anywhere on the earth’s surface, including destinations that are unfamiliar. There are two criteria needed to coordinate this task, a compass sense (a sense of direction) and a map sense (a sense of location). It is the ability to return from unfamiliar locations that posed the question of what sensory cues are used to determine locational information as well as directional information. It has been proposed that the compass sense can be derived from a number of perspectives. Magnetic orientation as a mechanism for directional sense was first put forward in the 19th century. Equally, the sun could be used as a compass in order to navigate home. In 1972, however, Papi and his contemporaries reported that anosmic pigeons (\"Columbia livia\") were severely impaired in orientation and homing performance. On the basis of their results, the hypothesis of ‘olfactory navigation’ was proposed.\n\nTwo models for olfactory navigation have been proposed, Papi’s ‘mosaic’ model and Wallraff’s ‘gradient’ model. Papi’s mosaic hypothesis advocates that pigeons construct a map from the distribution of environmental odours, within a radius of 70-100 kilometres.\n\nFrom this information, it is possible to derive the ‘home’ direction when encountering these odours at a release site. An example of associated wind-borne scents would be pine forests, coastlines and pollution from cities. It is argued that pigeons first learn to associate specific odours with particular locations during exercise and training flights. This model has the advantage that it requires the bird only to detect the presence or absence of a range of odours. Therefore, homing is viable only if the release sites are within a proximity that can provide reliable wind-borne cues, although Papi (1990), argues the utilisation of olfactory information obtained during the outward journey.\n\nWallraff’s gradient theory overcomes the problem of distance limitation via different means. It proposes the existence of long-range, stable atmospheric odour gradients. The foundation for this navigational map is a spatial representation in which two or more environmental odours have a particular intensity. Odour gradient differs along dissimilar directional axes and, therefore, the pigeon can compare the intensity of the scent at a particular location to its concentration at the home loft. This mechanism in principle could operate over vast distances, but would require the detection and interpretation of minute differences in odour concentration. However, a more poignant question is the existence of predictable odour gradients. Meteorologists deny that odour gradients, as required by this hypothesis, exist in nature.\n\nThe olfactory navigation hypothesis states that pigeons learn an odour map by associating smells perceived at the home loft with the directions from which they are carried by winds. Therefore, attempts to manipulate the development of that have involved changing the direction of wind, shielding birds from winds of a certain direction and exposing the pigeons to artificial odorants. The predication is that the experimental pigeons should learn an altered map and thus when released, they should fly according to their distorted perception.\n\nSuch an experiment was conducted, where two groups of pigeons were reared in separate, although identical aviaries composed of bamboo. Group one had air blown from the south containing olive oil and air from the north containing synthetic turpentine. This was reversed for group two. The pigeons were then released east of the loft; half had a drop of synthetic turpentine added to the bill, while the others were given a drop of olive oil. Pigeons from group one exposed to olive oil flew north, contrary to birds sentient to synthetic turpentine, which flew south. Consistent, but reversed results were found in group two.\n\nHowever, it is important to note that there has been a failure to replicate these results in other countries, such as Germany, Italy and the United States, even when considerable effort has been made to employ identical procedures. Nevertheless, further experiments applied two different methods – namely the placement of fans near the home coop in order to reverse wind direction and usage of deflector lofts to shift the apparent direction of the wind by 90°. Deflector lofts comprised wooden or glass baffles, which deflected wind course and therefore any signature odours. Findings were that pigeons raised in such lofts oriented themselves with a magnitude of a 90° error, known as the ‘deflector loft effect’. The wind reversed experiments, too, exhibited results that favoured the olfactory hypothesis, with experimentals on average flying in the opposing direction of home, while the controls took the correct flight path, when released from the same site.\n\nIn replication of the deflector loft experiments, similar findings were produced, though when anosmic pigeons where employed, they displayed the same degree of error in orientation as had previously been observed. Therefore, suggesting that the detection of odours may not have been associated with the defector loft effect. Indeed, the flight directions could simply reflect a directional response to wind experienced in the loft or by “other non-odorous factors”, such as light reflection. Researchers support these suggestions by noting the lack of highly developed nasal apparatus and associated brain functions in seed-eating birds such as pigeons. It could be argued, therefore, that pigeons are not dominated by olfactory landmarks when constructing a navigatory map.\n\nConflicting evidence, however, was produced when pigeons were housed in open cages and exposed to a fan produced air current carrying the scent of benzaldehyde. When released with exposure only to the natural air during transport and at the release site, both experimentals and controls were homeward oriented. Contrary if their response were simply to wind direction.\n\nA consistent feature of the olfaction experiments is that anosmic pigeons that are released from familiar sites are essentially unaffected. Perhaps a common fault of the olfactory mosaic and gradient model of olfactory navigation is that each model is over simplistic and that they do not sufficiently take account of other cues that may be of importance.\n\nThe Earth’s magnetic field is a potential map cue as the field varies in both strength and direction over the Earth’s surface Manipulations of the ambient magnetic field are rather difficult, although Keeton (1971) and Ioalé (1984) did report that magnets caused disorientation in pigeons when they were released under total overcast. This first indication for magnetic compass orientation in homing was later supported by other studies, which reversed the field around the head of the pigeon using battery operated coils. Though the coils had little effect in clear conditions, their effect under overcast conditions was dependent on the direction of the current. Another observation consistent with the idea of a geomagnetic map is the shift in the initial bearings of pigeons that occurs when the field increases during magnetic storms. In magnetic anomalies too, pigeons are disoriented, even under sunny conditions.\n\nThe predictable 15° movement per hour of the sun from east to west, signifies its potential as a celestial compass. This is possible providing the time of day is known and is achievable by birds due to their internal biological clock. Experiments to test this hypothesis, using the migratory European starling, indicated that the direction of migration could be manipulated by reflecting the angle of the sun. This effect was reproduced using homing pigeons. Although this study is of value in demonstrating mechanisms other than olfaction in bird navigation, it does not refer to pigeons.\n\nThe fundamental question of olfaction map sense in pigeons is ‘can they smell?’ Available evidence suggests that pigeons lack highly developed nasal apparatus and associated brain functions – yet empirical evidence has shown that the homing ability of pigeons can be compromised by interfering with the olfactory environment. However, the variability in the effects of olfactory manipulations indicates that odours are not the sole cues on which navigation is based and that map sense appears to rely on a comparison of available cues. Odour may still, however, be one of many navigational factors playing a highly variable role, though physical limitations and inconsistent findings render the olfactory hypothesis questionable.\n\n"}
{"id": "3004764", "url": "https://en.wikipedia.org/wiki?curid=3004764", "title": "Plane sailing", "text": "Plane sailing\n\nPlane sailing (also, colloquially and historically, spelled plain sailing) is an approximate method of navigation over small ranges of latitude and longitude. With the course and distance known, the difference in latitude Δ\"φ\" between \"A\" and \"B\" can be found, as well as the departure, the distance made good east or west. The difference in longitude Δ\"λ\" is unknown and has to be calculated using meridional parts as in Mercator sailing.\n\nBoth spellings (\"plane\" and \"plain\") have been in use for several centuries,\n\nPlane sailing is based on the assumption that the meridian through the point of departure, the parallel through the destination, and the course line form a right triangle in a plane, called the \"plane sailing triangle\".\n\nThe expressions \"plane sailing\" (or more commonly \"plain sailing\") has, by analogy, taken on a more general meaning of any activity that is relatively straightforward. \n\n"}
{"id": "29854203", "url": "https://en.wikipedia.org/wiki?curid=29854203", "title": "Plantmaps", "text": "Plantmaps\n\nPlantmaps.com is reference website that contains several interactive maps and tools to assist gardeners, botanists, farmers and horticulturalists. By entering a ZIP code, users can find the USDA hardiness zone, first and last frost dates, heat zones, drought conditions and annual climatology for their area.\n\nbuilt using the google maps api, plantmaps.com has the only fully interactive USDA hardiness zones map available on the web. In addition to plant hardiness zone map covering the continental United States, there are detailed interactive hardiness zone maps for each individual state. Plantmaps allows the user to locate their hardiness zone based on current US Postal Service ZIP codes. The ZIP code search will zoom to the coverage area of the selected ZIP code.\n\nPlantmaps allows other websites to embed the USDA hardiness zone to ZIP code search tool in their websites by adding an iframe widget to their site, allowing visitors to easily find their local hardiness zone.\n\nIn response to the latency of the current official USDA hardiness zone map (produced in 1990 using data from 1974 to 1986), plantmaps has created a new hardiness zone map using historical data from the National Climatic Data Center that covers longer time periods and uses climate record data from as recently as 2009. There is an updated plantmaps hardiness zones map for each individual state in the Continental US.\n\nUsing long term climatology data from the National Climatology Data Center, Plantmaps has created interactive average annual first frost and average annual last frost date maps for each individual state. These maps break the average frost dates into 10 day/date increments.\n\nUsing shapefile data available from the National Drought Mitigation Center, plantmaps has created statewide interactive Drought Monitor Maps that are updated weekly with the latest drought monitor conditions throughout the US.\n\nPlantmaps provides an interactive Palmer Drought Index map for each state. The interactive drought index maps use the latest data supplied by the National Oceanic and Atmospheric Administration and allows the map user to select and highlight the specific climate divisions in each state.\n\nPlantmaps has created interactive versions of all available level IV Environmental Protection Agency ecoregion maps that allow the user to zoom to identify each individual region. Ecoregions denote areas within which ecosystems (and the type, quality, and quantity of environmental resources) are generally similar. Currently, level IV maps are available for all states except Arizona, California, Hawaii and Alaska.\n\nPlantmaps contains interactive heat zone maps for each state. Heat zones display the average frequency of annual days where the temperature exceeds 86 degrees fahrenheit. Heat zones are useful for determining the survivability of specific plants and trees in extreme temperature conditions. It is also useful for determining suitability for plants and trees such as palm trees that require warm temperatures for growth.\n\nPlantmaps has interactive plant and tree distribution maps for 800 North American flora. The distribution maps contain overlays of USDA hardiness zones for analysis of plant and tree range versus hardiness zone.\n\nIn January 2011, Plantmaps released an interactive Canada plant hardiness zone map using up to date climatology data from Canadian National Climate Data Archive.\n\n"}
{"id": "4207958", "url": "https://en.wikipedia.org/wiki?curid=4207958", "title": "Point of interest", "text": "Point of interest\n\nA point of interest, or POI, is a specific point location that someone may find useful or interesting. An example is a point on the Earth representing the location of the Space Needle, or a point on Mars representing the location of the mountain, Olympus Mons. Most consumers use the term when referring to hotels, campsites, fuel stations or any other categories used in modern (automotive) navigation systems. \n\nUsers of a mobile devices can be provided with geolocation and time aware POI service, that recommends geolocations nearby and with a temporal relevance (e.g. POI to special services in a ski resort are available only in winter).\n\nIn medical fields such as histology/pathology/histopathology, points of interest are selected from the general background in a field of view; for example, among hundreds of normal cells, the pathologist may find 3 or 4 neoplastic cells that stand out from the others upon staining.\n\nA region of interest (ROI) and a volume of interest (VOI) are similar in concept, denoting a region or a volume (which may contain various individual POIs).\n\nThe term is widely used in cartography, especially in electronic variants including GIS, and GPS navigation software. In this context the synonym waypoint is common.\n\nA GPS point of interest specifies, at minimum, the latitude and longitude of the POI, assuming a certain map datum. A name or description for the POI is usually included, and other information such as altitude or a telephone number may also be attached. GPS applications typically use icons to represent different categories of POI on a map graphically.\n\nDigital maps for modern GPS devices typically include a basic selection of POI for the map area.\n\nHowever websites exist that specialize in the collection, verification, management and distribution of POI which end-users can load onto their devices to replace or supplement the existing POI. While some of these websites are generic, and will collect and categorize POI for any interest, others are more specialized in a particular category (such as speed cameras) or GPS device (e.g. TomTom/Garmin). End-users also have the ability to create their own custom collections.\n\nCommercial POI collections, especially those that ship with digital maps, or that are sold on a subscription basis are usually protected by copyright. However, there are also many websites from which royalty-free POI collections can be obtained, e.g. SPOI - Smart Points of Interest, which is distributed under ODbL license.\n\nThe applications for POI are extensive. As GPS-enabled devices as well as software applications that use digital maps become more available, so too the applications for POI are also expanding. Newer digital cameras for example can automatically tag a photograph using Exif with the GPS location where a picture was taken; these pictures can then be overlaid as POI on a digital map or satellite image such as Google Earth. Geocaching applications are built around POI collections. In Vehicle tracking systems POIs are used to mark destination points and/or offices to that users of GPS tracking software would easily monitor position of vehicles according to POIs.\n\nMany different file formats, including proprietary formats, are used to store point of interest data, even where the same underlying WGS84 system is used.\n\nReasons for variations to store the same data include:\n\nThe following are some of the file formats used by different vendors and devices to exchange POI (and in some cases, also navigation tracks):\n\n\nThird party and vendor-supplied utilities are available to convert point of interest data between different formats to allow them to be exchanged between otherwise incompatible GPS devices or systems. Furthermore, many applications will support the generic ASCII text file format, although this format is more prone to error due to its loose structure as well as the many ways in which GPS co-ordinates can be represented (e.g. decimal vs degree/minute/second). POI format converters are often named after the POI file format they convert and convert to, such as KML2GPX (converts KML to GPX) and KML2OV2 (converts KML to OV2).\n\n"}
{"id": "32110355", "url": "https://en.wikipedia.org/wiki?curid=32110355", "title": "Precise Point Positioning", "text": "Precise Point Positioning\n\nPrecise Point Positioning (PPP) is a Global Navigation Satellite System (GNSS) positioning method to calculate very precise positions up to few centimeter level using a single (GNSS) receiver in a dynamic and global reference framework like International Terrestrial Reference System (ITRS). PPP methods are different from (DGNSS) positioning methods which differentiate errors using one or more reference stations with known positions. The PPP approach combines precise clocks and orbits, so-called precise ephemeris, calculated from a global network to calculate a precise position with a single receiver, which can be double or single frequency.\n\nHistorically precise positioning was associated with surveying and geodesy. It makes use of carrier-phase observables, allowing positioning precisions of a fraction of a carrier wavelength, 19 or 24 cm. It also makes use of precise ephemeris, generated by the geodetic community -- the IGS, International GNSS Service -- from measurements by a global network of tracking stations. Recently, the dissemination over the Internet in low-latency real time -- e.g., APPS, the Automatic Precise Positioning Service of NASA JPL -- has made possible to do such precise positioning also in real time, giving us the PPP technique. \n\nWith the advent of Global Navigation Satellite Systems (GNSS) such as the Global Positioning System (GPS), precise positioning has been incorporated into production processes in mining, agriculture and construction. The main application has been in machine guidance and machine automation which require high levels of precision.\n\nPrecise positioning is also increasingly used in the fields of robotics and autonomous navigation.\n\n"}
{"id": "5669256", "url": "https://en.wikipedia.org/wiki?curid=5669256", "title": "Principal meridian", "text": "Principal meridian\n\nA principal meridian is a meridian used for survey control in a large region.\n\nThe Dominion Land Survey of Western Canada took its origin at the First (or Principal) Meridian, located at 97°27′28.41″ west of Greenwich, just west of Winnipeg, Manitoba. This line is exactly ten miles west of the Red River at the Canada-United States border.\n\nSix other meridians were designated at four-degree intervals westward, with the seventh located in British Columbia; the second and fourth meridians form the general eastern border and the western border of Saskatchewan.\n\nIn the United States Public Land Survey System, a principal meridian is the principal north-south line used for survey control in a large region, and which divides townships between east and west. The meridian meets its corresponding baseline at the point of origin, or initial point, for the land survey. For example, the Mount Diablo Meridian, used for surveys in California and Nevada, runs north-south through the summit of Mount Diablo.\n\nOften, meridians are marked with roads, such as the Meridian Avenue in San Jose, California, Meridian Road in Vacaville, California, both on the Mount Diablo Meridian, Meridian Road in Wichita, Kansas on the Sixth Principal Meridian, and Meridian Avenue in several western Washington counties generally following the Willamette Meridian. Baseline Road or Base Line Street extends for about 40 miles from Highland, California east of San Bernardino to La Verne, California where it meets Foothill Boulevard.\n\n\n"}
{"id": "2519712", "url": "https://en.wikipedia.org/wiki?curid=2519712", "title": "Quantitative revolution", "text": "Quantitative revolution\n\nThe quantitative revolution (QR) was a paradigm shift that sought to develop a more rigorous and systematic methodology for the discipline of geography. It came as a response to the inadequacy of regional geography to explain general spatial dynamics. The main claim for the quantitative revolution is that it led to a shift from a descriptive (idiographic) geography to an empirical law-making (nomothetic) geography. The quantitative revolution occurred during the 1950s and 1960s and marked a rapid change in the method behind geographical research, from regional geography into a spatial science.\n\nIn the history of geography, the quantitative revolution was one of the four major turning-points of modern geography – the other three being environmental determinism, regional geography and critical geography).\n\nThe quantitative revolution had occurred earlier in economics and psychology and contemporaneously in political science and other social sciences and to a lesser extent in history.\n\nDuring the late 1940s and early 1950s:\n\nAll of these events presented a threat to geography's position as an academic subject, and thus geographers began seeking new methods to counter critique.\n\nThe quantitative revolution responded to the regional geography paradigm that was dominant at the time. Debates raged predominantly (although not exclusively) in the U.S., where regional geography was the major philosophical school. In the early 1950s, there was a growing sense that the existing paradigm for geographical research was not adequate in explaining how physical, economic, social, and political processes are spatially organized, ecologically related, or how outcomes generated by them are evidence for a given time and place. A growing number of geographers started to express their dissatisfaction with the traditional paradigm of the discipline and its focus on regional geography, deeming the work as too descriptive, fragmented, and non-generalizable. To address these concerns, early critics such as Ackerman suggested the systematization of the discipline. Soon thereafter, a series of debates regarding methodological approaches in geography took place. One of the first illustrations of this was the Schaefer vs. Hartshorne debate. In 1953 \"Exceptionalism in geography: A Methodological Examination\" was published. In this work, Schaefer rejected Hartshorne’s exceptionalist interpretations about the discipline of geography and having the region as its central object of study. Instead, Schaefer envisioned as the discipline’s main objective the establishment of morphological laws through scientific inquiry, i.e. incorporating laws and methods from other disciplines in the social sciences that place a greater emphasis on processes. Hartshorne, on the other hand, addressed Schaefer’s criticism in a series of publications, where he dismissed Schaefer’s views as subjective and contradictory. He also stressed the importance of describing and classifying places and phenomena, yet admitted that there was room for employing laws of generic relationships in order to maximize scientific understanding. In his view, however, there should be no hierarchy between these two approaches.\n\nWhile debates about methods carried on, the institutionalization of systematic geography was taking place in the U.S. academy. The geography programs at the University of Iowa, Wisconsin, and Washington were pioneering programs in that respect. At the University of Iowa, Harold McCarty led efforts to establish laws of association between geographical patterns. At the University of Wisconsin, Arthur H. Robinson led efforts to develop statistical methods for map comparison. And at the University of Washington, Edward Ullman and William Garrison worked on developing the field of economic and urban geography, and central place theory. These institutions engendered a generation of geographers that established spatial analysis as part of the research agenda at other institutions including University of Chicago, Northwestern University, Loyola University, The Ohio State University, the University of Michigan, among others.\n\nThe changes introduced during the 1950s and 1960s under the banner of bringing 'scientific thinking' to geography led to an increased use of technique-based practices, including an array of mathematical techniques and computerized statistics that improved precision, and theory-based practices to conceptualize location and space in geographical research.\n\nSome of the techniques that epitomize the quantitative revolution include:\n\nThe common factor, linking the above techniques, was a preference for numbers over words, plus a belief that numerical work had a superior scientific pedigree.\n\nThe new method of inquiry led to the development of generalizations about spatial aspects in a wide range of natural and cultural settings. Generalizations may take the form of tested hypotheses, models, or theories, and the research is judged on its scientific validity, turning geography into a nomothetic science.\n\nOne of the most significant works to provide a legitimate theoretical and philosophical foundation for the reorientation of geography into a spatial science was David Harvey’s book, \"Explanation in Geography\", published in 1969. In this work, Harvey laid out two possible methodologies to explain geographical phenomena: an inductive route where generalizations are made from observation; and a deductive one where, through empirical observation, testable models and hypothesis are formulated and later verified to become scientific laws. He placed preference on the latter method. This positivist approach was countered by critical rationalism, a philosophy advanced by Karl Popper who rejected the idea of verification and maintained that hypothesis can only be falsified. Both epistemological philosophies, however, sought to achieve the same objective: to produce scientific laws and theories.\n\nThe paradigm shift had its strongest repercussions in the sub-field of economic and urban geography, especially as it pertains to location theory. However, some geographers–such as Ian Burton–expressed their dissatisfaction with quantification while others – such as Emrys Jones, Peter Lewis, and Golledge and Amedeo – debated the feasibility of law-making. Others, such as F. Luckermann, criticized the scientific explanations offered in geography as conjectural and lacking empirical basis. As a result, even models that were tested failed to accurately depict reality.\n\nBy the mid-1960s the quantitative revolution had successfully displaced regional geography from its dominant position and the paradigm shift was evident by the myriad of publications in geographical academic journals and geography textbooks. The adoption of the new paradigm allowed the discipline to be more serviceable to the public and private sectors.\n\nThe quantitative revolution had enormous implications in shaping the discipline of geography into what it looks like today given that its effects led to the spread of positivist (post-positivist) thinking and counter-positivist responses.\n\nThe rising interest in the study of distance as a critical factor in understanding the spatial arrangement of phenomena during the revolution led to formulation of the first law of geography by Waldo Tobler. The development of spatial analysis in geography led to more applications in planning process and the further development of theoretical geography offered to geographical research a necessary theoretical background.\n\nThe greater use of computers in geography also led to many new developments in geomatics, such as the creation and application of GIS and remote sensing. These new developments allowed geographers for the first time to assess complex models on a full-scale model and over space and time and the relationship between spatial entities. To some extent, the development of geomatics helped obscure the binary between physical and human geography to some extent, as the complexities of the human and natural environments could be assessed on new computable models.\n\nThe overwhelming focus on statistical modelling would, eventually, be the undoing of the quantitative revolution. Many geographers became increasingly concerned that these techniques simply put a highly sophisticated technical gloss on an approach to study that was barren of fundamental theory. Other critics argued that it removed the 'human dimension' from a discipline that always prided itself on studying the human and natural world alike. As the 1970s dawned, the quantitative revolution came under direct challenge. The counter-positivist response came as geographers began to expose the inadequacy of quantitative methods to explain and address issues regarding race, gender, class and war. On that regard, David Harvey disregarded earlier works where he advocated for the quantitative revolution and adopted a Marxist theoretical framework. Soon new subfields would emerge in human geography to contribute a new vocabulary for addressing these issues, most notably critical geography and feminist geography.\n\n\n\n"}
{"id": "52205564", "url": "https://en.wikipedia.org/wiki?curid=52205564", "title": "Radio acoustic ranging", "text": "Radio acoustic ranging\n\nRadio acoustic ranging, occasionally written as \"radio-acoustic ranging\" and sometimes abbreviated RAR, was a method for determining a ship's precise location at sea by detonating an explosive charge underwater near the ship, detecting the arrival of the underwater sound waves at remote locations, and radioing the time of arrival of the sound waves at the remote stations to the ship, allowing the ship's crew to use triangulation to determine the ship's position. Developed by the United States Coast and Geodetic Survey in 1923 and 1924 for use in accurately fixing the position of survey ships during hydrographic survey operations, it was the first navigation technique in human history other than dead reckoning that did not require visual observation of a landmark, marker, light, or celestial body, and the first non-visual means to provide precise positions. First employed operationally in 1924, radio acoustic ranging remained in use until 1944, when new radio navigation techniques developed during World War II rendered it obsolete.\n\nTo fix their position using radio acoustic ranging, a ship's crew first ascertained the temperature and salinity of sea water in the vicinity of the ship to determine an accurate velocity of sound through the water. The crew then threw a small TNT bomb off the ship's stern. It exploded at a depth of about , and a chronograph aboard the ship automatically recorded the time the explosion was heard at the ship. The sound traveled outward from the explosion, eventually reaching hydrophones at known locations – shore stations, anchored manned station ships, or unmanned moored buoys – at a distance from the ship. Each hydrophone was connected to a radio transmitter that automatically sent a signal indicating the time its hydrophone detected the sound. At the distances involved – generally less than – each of these radio signals arrived at the ship at essentially the same instant that each of the remote hydrophones detected the sound of the explosion. The ship's chronograph automatically recorded the time each radio signal arrived at the ship. By subtracting the time of the explosion from the time of radio signal reception, the ship's crew could determine the length of time the sound wave required to travel from the point of the explosion to each remote hydrophone and, knowing the speed of sound in the surrounding sea water, could multiply the sound's travel time by the velocity of sound in sea water to determine the distance between the explosion and the hydrophone. By determining the distance to at least two remote hydrophones in known locations, the ship's crew could use triangulation to fix the ship's position.\n\nIn deep waters, such as those that prevailed in the Pacific Ocean along the United States West Coast, the Coast and Geodetic Survey could rely upon shore stations to support radio acoustic ranging because the deep water allowed sound to travel to the coast. Along the United States East Coast, where shallower waters prevailed, sound had greater difficulty in reaching the coast, and the Coast and Geodetic Survey relied more heavily on anchored manned station ships, and later unmanned moored buoys, to support radio acoustic ranging.\n\nChronographs recorded times to the hundredth of a second, and the crew of a ship using radio acoustic ranging could determine their ship's distance from the remote hydrophone stations to within , allowing them to plot their ship's position with great accuracy for the time. With sound waves traveling from the point of the explosion to the distant hydrophones at about , ships occasionally used radio acoustic ranging at distances of over between ship and hydrophone station, and distances of were common.\n\nRadio acoustic ranging had its origins in a growing understanding of underwater acoustics and their practical application during the early decades of the 20th century, and developed in parallel with echo sounding. The first step took place in the early 1900s, when the Submarine Signal Company invented a submarine bell signalling device and a hydrophone that could serve as a receiver of the underwater sounds the bells generated. The crew of a ship equipped with the receiving hydrophone could plot their ship's distance from the submarine bell mechanism and plot intersecting lines from two or more bells to determine the ship's position. The bells were installed at lighthouses, aboard lightvessels, and on buoys along the coasts of North America and Europe, and receiving hydrophones were mounted aboard hundreds of ships. It was history's first practical use of acoustics in an ocean environment.\n\nThe sinking of in 1912 spurred the Canadian inventor Reginald Fessenden (1866–1932) to begin work on a long-distance underwater sound transmission and reception system that could detect hazards in the path of a ship. This led to the invention of the Fessenden oscillator, an electro-acoustic transducer which by 1914 had a proven ability to transmit and receive sound at a distance of 31 miles across Massachusetts Bay and to detect an iceberg ahead of a ship at a range of two miles by bouncing sound off it and detecting the echo, as well as an occasional ability to detect the reflection of sound off the ocean bottom. Further impetus to developing practical applications of underwater acoustics came from World War I, which prompted the Royal Navy, United States Navy, and United States Army Coast Artillery Corps to experiment with sound as a means of detecting submerged submarines. In postwar experiments, the Coast Artillery Corps's Subaqueous Sound Ranging Section conducted experiments in shallow water in Vineyard Sound off Massachusetts in which it detonated explosive charges underwater at the ends of established baselines and measured the amount of time it took for the sound to arrive at hydrophones at the other ends of the baselines in order to establish very accurate measurements of the speed of sound through water. And in 1923, the Submarine Signal Company improved upon its underwater signaling devices by equipping them with radio transmitters that sent signals both to identify the particular device and to indicate to approaching ships that it would generate an acoustic signal at a specific time interval after it sent the radio signal, allowing ships to identify the specific navigational aid they were approaching and to take advantage of a one-way ranging capability that let their crews determine their direction and distance from the navigational aid.\n\nRealizing the potential of these applications of acoustics to hydrographic surveying and navigation, particularly along the United States West Coast, where fog frequently interfered with attempts to fix ship positions accurately, Ernest Lester Jones (1876–1929), then Director of the United States Coast and Geodetic Survey, in consultation with United States Coast and Geodetic Survey Corps officers, decided to investigate the use of acoustics in both depth finding and navigation. Nicholas H. Heck (1882–1953), a Coast and Geodetic Survey Corps officer, had been assigned from 1917 to 1919 to World War I service with the United States Naval Reserve Force, during which he had researched the use of underwater acoustics in antisubmarine warfare. He was the obvious choice to lead the new effort.\n\nBy January 1923, the Coast and Geodetic Survey had decided to install a Hayes sonic rangefinder – an early echo sounder – aboard the survey ship USC&GS \"Guide\", which the Coast and Geodetic Survey planned to commission into its fleet later that year; successful operation of the sonic rangefinder would require a precise understanding of the speed of sound through water. When Heck contacted E. A. Stephenson of the U.S. Army Coast Artillery Corps to inform him of this plan and to inquire further about the Vineyard Sound experiments, Stephenson suggested that a system of hydrophones detecting the sound of underwater explosions could allow Coast and Geodetic Survey ships to fix their position while conducting surveys. Heck agreed, but believed that existing navigation aids would not meet the needs of the Coast and Geodetic Survey in terms of the immediacy and accuracy of position fixes. He envisioned improving on the Submarine Signal Company's system of underwater noise generators and attached radio transmitters, as well as other previous concepts, by creating what would become known as the radio acoustic ranging method. Like echosounding, this method required an accurate calculation of the speed of sound through water.\n\nHeck oversaw tests at Coast and Geodetic Survey headquarters in Washington, D.C., that demonstrated that shipboard recording of the time of an explosion could be performed accurately enough for his concept to work. He worked with Dr. E. A. Eckhardt, a physicist, and M. Keiser, an electrical engineer, of the National Bureau of Standards to develop a hydrophone system that could automatically send a radio signal when it detected the sound of an underwater explosion. When the Coast and Geodetic Survey commissioned \"Guide\" in 1923, Heck had her based at New London, Connecticut. Under his direction, \"Guide\" both tested her new echo sounder's ability to make accurate depth soundings and conducted radio acoustic ranging experiments in cooperation with the U.S. Army Coast Artillery Corps. Despite many difficulties, testing of both echo sounding and radio acoustic ranging wrapped up successfully in November 1923.\n\nIn late November 1923, with Heck aboard, \"Guide\" began a voyage from New London via Puerto Rico and the Panama Canal to San Diego, California, where she would be based in the future, with her route planned to take her over a wide variety of ocean depths so that she could continue to test her echo sounder. \"Guide\" made history during the voyage, becoming the first Coast and Geodetic Survey ship to use echo sounding to measure and record the depth of the sea at points along her course; she also measured water temperatures and took water samples so that the Scripps Institution for Biological Research (now the Scripps Institution of Oceanography) at La Jolla, California, could measure salinity levels. She also compared echo sounder soundings with those made by lead lines, discovering that using a single speed of sound through water, as had been the previous practice by those conducting echo sounding experiments, yielded acoustic depth-finding results that did not match the depths found by lead lines. Before she reached San Diego in December 1923, she had accumulated much data beneficial to the study of the movement of sound waves through water and measuring their velocity under varying conditions of salinity, density, and temperature, information essential both to depth-finding and radio acoustic ranging.\n\nUpon arriving in California, Heck and \"Guide\" personnel in consultation with the Scripps Institution developed formulas that allowed accurate echo sounding of depths in all but the shallowest waters and installed hydrophones at La Jolla and Oceanside, California, to allow experimentation with radio acoustic ranging. Under Heck's direction, \"Guide\" then conducted experiments off the coast of California during the early months of 1924 that demonstrated that accurate echo sounding was possible using the new formulas. Experiments with radio acoustic ranging, despite initial difficulties, demonstrated that the method also was practical, although difficulty with getting some of the explosive charges to detonate hampered some of the experimental program. In April 1924, the Coast and Geodetic Survey concluded that both echo sounding and radio acoustic ranging were fundamentally sound, with no foundational problems left to solve, and that all that remained necessary was continued development and refinement of both techniques during their operational use. Heck turned over continued development of echo sounding and radio acoustic ranging to \"Guide\"'s commanding officer, Commander Robert Luce, and returned to his duties in Washington, D.C.\n\nOperating in the Pacific Ocean off Oregon in 1924, \"Guide\" became the first ship to employ radio acoustic ranging operationally. Off Oregon that year, she successfully employed the technique at a distance of between the ranging explosion and the remote hydrophones detecting its sound and in the process achieved the first observed indication of the ocean sound layer that was later called the sound fixing and ranging (SOFAR) channel or deep sound channel (DSC). In 1928, French investigators extended this range, detonating a 30-kg (66-pound) explosive in the Mediterranean Sea between Algiers in French Algeria and Toulon, France, and detecting the sound at a range of .\n\nInitially, Heck and others involved in the development of radio acoustic ranging thought the technique would prove least effective along the coast of the Pacific Northwest, where they assumed that the sound of wave action along the coast and the difficulty of setting up shore stations and cables would reduce the success of radio acoustic ranging; in contrast, they thought that conditions along the United States East Coast would pose no challenges. In fact, the opposite proved true: Among other problems, the relatively shallow water along the U.S. East Coast attenuated the sound of ranging explosions and shoals often blocked the sound from reaching shore at all. To overcome these difficulties, the Coast and Geodetic Survey anchored manned vessels well offshore along the U.S. East Coast to serve as hydrophone stations. In 1931, the Coast and Geodetic Survey proposed replacing the manned station ships with \"radio-sonobuoys\", and in July 1936 it began to place radio-sonobuoys in service. The 700-pound (317.5-kg) unmanned buoys – equipped with subsurface hydrophones, batteries, and radio transmitters that automatically sent a radio signal when their hydrophones detected the sound of a ranging explosion – could be deployed or recovered by Coast and Geodetic Survey ships in five minutes. Use of the buoys spread to the U.S. West Coast as well because they were cheaper to set up and operate than a shore station.\n\nRadio acoustic ranging had limitations and drawbacks. Local peculiarities in the propagation of acoustic waves in the water column could degrade its accuracy, there were problems with maintaining hydrophone stations, and handling explosive charges posed a considerable danger to personnel and ships. On one occasion a Coast and Geodetic Survey Corps ensign on board the survey ship USC&GS \"Hydrographer\" inserted a radio acoustic ranging bomb in the mouth of a shark and released the shark, only to watch in horror as it swam back to the ship and exploded next to \"Hydrographer\"′s hull; the explosion rocked the ship. Aboard \"Guide\" in 1927, tragedy almost struck when a petty officer handling a bomb lit its fuse and then fell when the ship lurched; he dropped the bomb, which rolled into a gutter. The petty officer fell again before finally reaching the bomb and heaving it overboard just in time; it exploded alongside the ship just as it hit the water. The concussion prompted half the crew to rush up from below decks to find out what had happened.\n\nAs late as 1942, radio acoustic ranging remained important enough to the Coast and Geodetic Survey for it to devote just over 100 pages of its \"Hydrographic Manual\" to it. However, World War II, which by then had been raging for three years, gave impetus to the rapid development of purely radio-based navigation systems to assist bombers in finding their targets in darkness and bad weather. Such radio navigation systems were easier to maintain than hydrophone stations and did not require the handling of explosives and, as the new systems matured, the Coast and Geodetic Survey began to apply them to maritime navigation. Radio acoustic ranging appears not to have been used after 1944, and by 1946, Coast and Geodetic Survey ships had switched over to the new SHORAN electronic navigation technology to fix their positions.\n\nThe first non-visual method of precise navigation in human history, and the first that could be used at any time of day or night and in any weather conditions, radio acoustic ranging was a major step forward in the development of modern navigation systems. Nicholas Heck revolutionized oceanic surveying through the use of radio electronic ranging to establish ship locations, one of his major contributions to oceanography. His work related to the technique also helped to develop underwater sound velocity tables allowing the establishment of \"true depths\" of up to using echo sounding.\n\nRadio acoustic ranging was an early step along the path to modern electronic navigation systems, oceanographic telemetering systems, and the development of marine seismic surveying. The technique also laid the groundwork for the development of sonars capable of looking ahead of and to the sides of vessels.\n\nThe Coast and Geodetic Survey's radio-sonobuoys, developed to support radio acoustic ranging, were the ancestors of the sonobuoys used by ships and aircraft in antisubmarine warfare and underwater acoustic research today.\n\n\n"}
{"id": "32655756", "url": "https://en.wikipedia.org/wiki?curid=32655756", "title": "Running survey", "text": "Running survey\n\nA running survey is a rough survey made by a vessel while coasting. Bearings to landmarks are taken at intervals as the vessel sails offshore, and are used to fix features on the coast and further inland. Intervening coastal detail is sketched in. \n\nThe method was used by James Cook, and subsequently by navigators who sailed under—or were influenced by—him, including George Vancouver, William Bligh and Matthew Flinders.\n"}
{"id": "32322814", "url": "https://en.wikipedia.org/wiki?curid=32322814", "title": "Sahab Geographic and Drafting Institute", "text": "Sahab Geographic and Drafting Institute\n\nSahab Geographic and Drafting Institute is the first geographical and cartographical institute in the private sector of Iran and the Middle East. It produces maps, globes, atlases and other educational materials regarding geography. The current manager of the institute is Mohammad Reza Sahab.\n\n\"Sahab Geographic and Drafting Institute\" was founded in 1935 by Abolghasem Sahab and his son Abbas Sahab.\n\nSahab Institute has produced more than 3000 titles, such as maps, atlases, globes, school text books and posters.\n\nSahab Library was established by Abolghasem Sahab, and was later developed by Abbas Sahab. There are about 32000 books and journals regarding geography. There are also around 20000 different maps.\n\n\"Sahab Geographic and Drafting Institute\" has published many books and maps regarding Persian Gulf, its historical maps and the islands in the Persian Gulf.\n"}
{"id": "1515653", "url": "https://en.wikipedia.org/wiki?curid=1515653", "title": "Satellite navigation", "text": "Satellite navigation\n\nA satellite navigation or satnav system is a system that uses satellites to provide autonomous geo-spatial positioning. It allows small electronic receivers to determine their location (longitude, latitude, and altitude/elevation) to high precision (within a few metres) using time signals transmitted along a line of sight by radio from satellites. The system can be used for providing position, navigation or for tracking the position of something fitted with a receiver (satellite tracking). The signals also allow the electronic receiver to calculate the current local time to high precision, which allows time synchronisation. Satnav systems operate independently of any telephonic or internet reception, though these technologies can enhance the usefulness of the positioning information generated.\n\nA satellite navigation system with global coverage may be termed a global navigation satellite system (GNSS). , the United States' Global Positioning System (GPS) and Russia's GLONASS are fully operational GNSSs, with China's BeiDou Navigation Satellite System (BDS) and the European Union's Galileo scheduled to be fully operational by 2020. India, France and Japan are in the process of developing regional navigation and augmentation systems as well.\n\nGlobal coverage for each system is generally achieved by a satellite constellation of 18–30 medium Earth orbit (MEO) satellites spread between several orbital planes. The actual systems vary, but use orbital inclinations of >50° and orbital periods of roughly twelve hours (at an altitude of about ).\n\nSatellite navigation systems that provide enhanced accuracy and integrity monitoring usable for civil navigation are classified as follows:\n\n\nGround based radio navigation has long been practiced. The DECCA, LORAN, GEE and Omega systems used terrestrial longwave radio transmitters which broadcast a radio pulse from a known \"master\" location, followed by a pulse repeated from a number of \"slave\" stations. The delay between the reception of the master signal and the slave signals allowed the receiver to deduce the distance to each of the slaves, providing a fix.\n\nThe first satellite navigation system was Transit, a system deployed by the US military in the 1960s. Transit's operation was based on the Doppler effect: the satellites travelled on well-known paths and broadcast their signals on a well-known radio frequency. The received frequency will differ slightly from the broadcast frequency because of the movement of the satellite with respect to the receiver. By monitoring this frequency shift over a short time interval, the receiver can determine its location to one side or the other of the satellite, and several such measurements combined with a precise knowledge of the satellite's orbit can fix a particular position. Satellite orbital position errors are induced by variations in the gravity field and radar refraction, among others. These were resolved by a team led by Harold L Jury of Pan Am Aerospace Division in Florida from 1970-1973. Using real-time data assimilation and recursive estimation, the systematic and residual errors were narrowed down to a manageable level to permit accurate navigation. \n\nPart of an orbiting satellite's broadcast included its precise orbital data. In order to ensure accuracy, the US Naval Observatory (USNO) continuously observed the precise orbits of these satellites. As a satellite's orbit deviated, the USNO would send the updated information to the satellite. Subsequent broadcasts from an updated satellite would contain its most recent ephemeris.\n\nModern systems are more direct. The satellite broadcasts a signal that contains orbital data (from which the position of the satellite can be calculated) and the precise time the signal was transmitted. The orbital ephemeris is transmitted in a data message that is superimposed on a code that serves as a timing reference. The satellite uses an atomic clock to maintain synchronization of all the satellites in the constellation. The receiver compares the time of broadcast encoded in the transmission of three (at sea level) or four different satellites, thereby measuring the time-of-flight to each satellite. Several such measurements can be made at the same time to different satellites, allowing a continual fix to be generated in real time using an adapted version of trilateration: see GNSS positioning calculation for details.\n\nEach distance measurement, regardless of the system being used, places the receiver on a spherical shell at the measured distance from the broadcaster. By taking several such measurements and then looking for a point where they meet, a fix is generated. However, in the case of fast-moving receivers, the position of the signal moves as signals are received from several satellites. In addition, the radio signals slow slightly as they pass through the ionosphere, and this slowing varies with the receiver's angle to the satellite, because that changes the distance through the ionosphere. The basic computation thus attempts to find the shortest directed line tangent to four oblate spherical shells centred on four satellites. Satellite navigation receivers reduce errors by using combinations of signals from multiple satellites and multiple correlators, and then using techniques such as Kalman filtering to combine the noisy, partial, and constantly changing data into a single estimate for position, time, and velocity.\n\nThe original motivation for satellite navigation was for military applications. Satellite navigation allows precision in the delivery of weapons to targets, greatly increasing their lethality whilst reducing inadvertent casualties from mis-directed weapons. (See Guided bomb). Satellite navigation also allows forces to be directed and to locate themselves more easily, reducing the fog of war.\n\nThe ability to supply satellite navigation signals is also the ability to deny their availability. The operator of a satellite navigation system potentially has the ability to degrade or eliminate satellite navigation services over any territory it desires.\n\nThe United States' Global Positioning System (GPS) consists of up to 32 medium Earth orbit satellites in six different orbital planes, with the exact number of satellites varying as older satellites are retired and replaced. Operational since 1978 and globally available since 1994, GPS is the world's most utilized satellite navigation system.\n\nThe formerly Soviet, and now Russian, \"Global'naya Navigatsionnaya Sputnikovaya Sistema\", (GLObal NAvigation Satellite System or GLONASS), is a space-based satellite navigation system that provides a civilian radionavigation-satellite service and is also used by the Russian Aerospace Defence Forces. GLONASS has full global coverage with 24 satellites.\n\nThe European Union and European Space Agency agreed in March 2002 to introduce their own alternative to GPS, called the Galileo positioning system. Galileo became operational on 15 December 2016 (global Early Operational Capability (EOC)) At an estimated cost of €3 billion, the system of 30 MEO satellites was originally scheduled to be operational in 2010. The original year to become operational was 2014. The first experimental satellite was launched on 28 December 2005. Galileo is expected to be compatible with the modernized GPS system. The receivers will be able to combine the signals from both Galileo and GPS satellites to greatly increase the accuracy. Galileo is expected to be in full service in 2020 and at a substantially higher cost.\nThe main modulation used in Galileo Open Service signal is the Composite Binary Offset Carrier (CBOC) modulation.\n\nChina has indicated their plan to complete the entire second generation Beidou Navigation Satellite System (BDS or BeiDou-2, formerly known as COMPASS), by expanding current regional (Asia-Pacific) service into global coverage by 2020. The BeiDou-2 system is proposed to consist of 30 MEO satellites and five geostationary satellites. A 16-satellite regional version (covering Asia and Pacific area) was completed by December 2012.\n\nChinese regional (Asia-Pacific, 16 satellites) network to be expanded into the whole BeiDou-2 global system which consists of all 35 satellites by 2020.\n\nThe NAVIC or NAVigation with Indian Constellation is an autonomous regional satellite navigation system developed by Indian Space Research Organisation (ISRO) which would be under the total control of Indian government. The government approved the project in May 2006, with the intention of the system completed and implemented on 28 April 2016. It will consist of a constellation of 7 navigational satellites. 3 of the satellites will be placed in the Geostationary orbit (GEO) and the remaining 4 in the Geosynchronous orbit(GSO) to have a larger signal footprint and lower number of satellites to map the region. It is intended to provide an all-weather absolute position accuracy of better than 7.6 meters throughout India and within a region extending approximately 1,500 km around it. A goal of complete Indian control has been stated, with the space segment, ground segment and user receivers all being built in India. All seven satellites, IRNSS-1A, IRNSS-1B, IRNSS-1C, IRNSS-1D, IRNSS-1E, IRNSS-1F, and IRNSS-1G, of the proposed constellation were precisely launched on 1 July 2013, 4 April 2014, 16 October 2014, 28 March 2015, 20 January 2016, 10 March 2016 and 28 April 2016 respectively from Satish Dhawan Space Centre. The system is expected to be fully operational by August 2016.\n\nThe Quasi-Zenith Satellite System (QZSS) is a proposed four-satellite regional time transfer system and enhancement for GPS covering Japan and the Asia-Oceania regions. QZSS services are available on a trial basis as of January 12, 2018, and are scheduled to be launched in November 2018. The first satellite was launched in September 2010.\n\nSources: \n\nGNSS augmentation is a method of improving a navigation system's attributes, such as accuracy, reliability, and availability, through the integration of external information into the calculation process, for example, the Wide Area Augmentation System, the European Geostationary Navigation Overlay Service, the Multi-functional Satellite Augmentation System, Differential GPS, GPS Aided GEO Augmented Navigation (GAGAN) and inertial navigation systems.\n\nDoppler Orbitography and Radio-positioning Integrated by Satellite (DORIS) is a French precision navigation system. Unlike other GNSS systems, it is based on static emitting stations around the world, the receivers being on satellites, in order to precisely determine their orbital position. The system may be used also for mobile receivers on land with more limited usage and coverage. Used with traditional GNSS systems, it pushes the accuracy of positions to centimetric precision (and to millimetric precision for altimetric application and also allows monitoring very tiny seasonal changes of Earth rotation and deformations), in order to build a much more precise geodesic reference system.\n\nThe two current operational low Earth orbit satellite phone networks are able to track transceiver units with accuracy of a few kilometers using doppler shift calculations from the satellite. The coordinates are sent back to the transceiver unit where they can be read using AT commands or a graphical user interface. This can also be used by the gateway to enforce restrictions on geographically bound calling plans.\n\n\n\n\n"}
{"id": "23003745", "url": "https://en.wikipedia.org/wiki?curid=23003745", "title": "Standard Interchange Format", "text": "Standard Interchange Format\n\nStandard Interchange Format, called SIF, is a geospatial data exchange format. A standard or neutral format used to move graphics files between DOD Project 2851 and is currently codified in Content Standard for Digital Geospatial Metadata maintained by the Federal Geographic Data Committee.\n\nUnit 69 of the NCGIA Core Corriculum in GIS states that SIF is a \"popular data exchange format for many GIS packages\" and was \"developed to support exchange of data between Intergraph and other systems.\"\n\nNavteq uses Standard Interchange Format (SIF) \n\nAnother example of data available in SIF format can be found online from the NASA's BOREAS project that also claims that the SIF format is \"not well documented.\"\n\nAdditional criticism of SIF, along with recognition of SIF's ubiquity and utility for exchanging data, is acknowledged in the online journal article \"Is a Standard Terrain Data Format Necessary?\"\n\n"}
{"id": "5284746", "url": "https://en.wikipedia.org/wiki?curid=5284746", "title": "Sustainable city", "text": "Sustainable city\n\nSustainable cities, urban sustainability, or eco-city (also \"ecocity\") is a city designed with consideration for social, economic, environmental impact , and resilient habitat for existing populations, without compromising the ability of future generations to experience the same. These cities are inhabited by people whom are dedicated towards minimization of required inputs of energy, water, food, waste, output of heat, air pollution - CO, methane, and water pollution. Richard Register first coined the term \"ecocity\" in his 1987 book, \"Eco city Berkeley: Building Cities for a Healthy Future\". Other leading figures who envisioned the sustainable city are architect Paul F Downton, who later founded the company Ecopolis Pty Ltd, as well as authors Timothy Beatley and Steffen Lehmann, who have written extensively on the subject. The field of industrial ecology is sometimes used in planning these cities.\n\nThere remains no completely agreed upon definition for what a sustainable city should be or completely agreed upon paradigm for what components should be included. Generally, developmental experts agree that a sustainable city should meet the needs of the present without sacrificing the ability of future generations to meet their own needs. The ambiguity within this idea leads to a great deal of variation in terms of how cities carry out their attempts to become sustainable.\n\nIdeally, a sustainable city creates an enduring way of life across the four domains of ecology, economics, politics and culture. However, minimally a sustainable city should firstly be able to feed itself with a sustainable reliance on the surrounding countryside. Secondly, it should be able to power itself with renewable sources of energy. The core of this is to create the smallest conceivable ecological footprint, while producing the lowest quantity of pollution achievable. All while efficiently using the land; composting used materials, and recycling or converting waste-to-energy. All of these contributions will lead to the city's overall impacts on climate change to be minimal and with as little impact. The Adelaide City Council states that socially sustainable cities should be equitable, diverse, connected, and democratic and provide a good quality of life. \n\nA sustainable city can feed itself with minimal reliance on the surrounding countryside, and power itself with renewable sources of energy. The crux of this is to create the smallest possible ecological footprint, and to produce the lowest quantity of pollution possible, to efficiently use land; compost used materials, recycle it or convert waste-to-energy, and thus the city's overall contribution to climate change will be minimal, if such practices are adhered to. \n\nIt is estimated that over 50% of the world’s population now lives in cities and urban areas. These large communities provide both challenges and opportunities for environmentally-conscious developers. There are distinct advantages to further defining and working towards the goals of sustainable cities. Humans are social creatures and thrive in urban spaces that foster social connections. Richard Florida, an urban studies theorist, focuses on the social impact of sustainable cities and states that cities need to be more than a competitive business climate; they need to be a great people climate that appeals to individuals and families of all types. Because of this, a shift to more dense, urban living would provide an outlet for social interaction and conditions under which humans can prosper. These types of urban areas would also promote the use of public transit, walkability and biking which would benefit citizens health wise but also be environmentally beneficial.\n\nContrary to common belief, urban systems can be more environmentally sustainable than rural or suburban living. With people and resource located so close to one another it is possible to save energy for transportation and mass transit systems, and resources such as food. Cities benefit the economy by locating human capital in one relatively small geographic area where ideas can be generated. Having a more dense, urban space would also increase people's efficiency since they wouldn't have to spend as much time commuting to places if resources are located close together, which in turn would benefit the economy since people can use this extra time on other matters; like work.\n\nThese ecological cities are achieved through various means, such as:\n\nBuildings provide the infrastructure for a functioning city and allow for many opportunities to demonstrate a commitment to sustainability. A commitment to sustainable architecture encompasses all phases of building including the planning, building, and restructuring. Sustainable Site Initiatives is used by landscape architects, designers, engineers, architects, developers, policy-makers and others to align land development and management with innovative sustainable design.\n\nThe purpose of an eco-industrial park is to connect a number of firms and organizations to work together to decrease their environmental impact while simultaneously improving their economic performance. The community of businesses accomplishes this goal through collaboration in managing environmental and resource issues, such as energy, water, and materials. The components for building an eco-industrial park include natural systems, more efficient use of energy, and more efficient material and water flows Industrial parks should be built to fit into their natural settings in order to reduce environmental impacts, which can be accomplished through plant design, landscaping, and choice of materials. For instance, there is an industrial park in Michigan built by Phoenix Designs that is made almost entirely from recycled materials. The landscaping of the building will include native trees, grasses, and flowers, and the landscaping design will also act as climate shelter for the facility. In choosing the materials for building an eco-industrial park, designers must consider the life-cycle analysis of each medium that goes into the building to assess their true impact on the environment and to ensure that they are using it from one plant to another, steam connections from firms to provide heating for homes in the area, and using renewable energy such as wind and solar power. In terms of material flows, the companies in an eco-industrial park may have common waste treatment facilities, a means for transporting by-products from one plant to another, or anchoring the park around resource recovery companies that are recruited to the location or started from scratch. To create more efficient water flows in industrial parks, the processed water from one plant can be reused by another plant and the parks infrastructure can include a way to collect and reuse storm water runoff.\n\nSee also: Urban Agriculture\n\nUrban farming is the process of growing and distributing food, as well as raising animals, in and around a city or in urban area. According to the RUAF Foundation, urban farming is different from rural agriculture because \"it is integrated into the urban economic and ecological system: urban agriculture is embedded in -and interacting with- the urban ecosystem. Such linkages include the use of urban residents as labourers, use of typical urban resources (like organic waste as compost and urban wastewater for irrigation), direct links with urban consumers, direct impacts on urban ecology (positive and negative), being part of the urban food system, competing for land with other urban functions, being influenced by urban policies and plans, etc.\" There are many motivations behind urban agriculture, but in the context of creating a sustainable city, this method of food cultivation saves energy in food transportation and saves costs. In order for urban farming to be a successful method of sustainable food growth, cities must allot a common area for community gardens or farms, as well as a common area for a farmers market in which the foodstuffs grown within the city can be sold to the residents of the urban system.\nBerms of fava beans have been planted at Hayes Valley Farm, a community-built farm on the former Central freeway ramps of San Francisco.\n\nMany cities are currently in a shift from the suburban sprawl model of development to a return to urban dense living. This shift in geographic distribution of population leads to a denser core of city residents. These residents provide a growing demand in many sectors that is reflected in the architectural fabric of the city. This new demand can be supplied by new construction or historic rehabilitation. Sustainable cities will opt for historical rehabilitation wherever possible. Having people live in higher densities not only gives economies of scale but also allows for infrastructure to be more efficient.\n\nWalkable urbanism is a development strategy in opposition to suburban sprawl. It advocates housing for a diverse population, a full mix of uses, walkable streets, positive public space, integrated civic and commercial centers, transit orientation and accessible open space. It also advocates for density and accessibility of commercial and government activity.\n\nThe most clearly defined form of walkable urbanism is known as the Charter of New Urbanism. It is an approach for successfully reducing environmental impacts by altering the built environment to create and preserve smart cities which support sustainable transport. Residents in compact urban neighborhoods drive fewer miles, and have significantly lower environmental impacts across a range of measures, compared with those living in sprawling suburbs. The concept of circular flow land use management has also been introduced in Europe to promote sustainable land use patterns that strive for compact cities and a reduction of greenfield land taken by urban sprawl.\n\nIn sustainable architecture the recent movement of New Classical Architecture promotes a sustainable approach towards construction, that appreciates and develops smart growth, walkability, architectural tradition and classical design. This in contrast to modernist and globally uniform architecture, as well as opposing solitary housing estates and suburban sprawl. Both trends started in the 1980s.\n\nMain article: Leadership in Energy and Environmental Design\n\nThe Leadership in Energy and Environmental Design (LEED) Green Building Rating System® encourages and accelerates global adoption of sustainable green building and development practices through the creation and implementation of universally understood and accepted tools and performance criteria.\n\nLEED, or Leadership in Energy and Environmental Design, is an internationally recognized green building certification system. LEED recognizes whole building sustainable design by identifying key areas of excellence including: Sustainable Sites, Water Efficiency, Energy and Atmosphere, Materials and Resources, Indoor Environmental Quality, Locations & Linkages, Awareness and Education, Innovation in Design, Regional Priority. In order for a building to become LEED certified sustainability needs to be prioritized in design, construction, and use. One example of sustainable design would be including a certified wood like bamboo. Bamboo is fast growing and has an incredible replacement rate after being harvested. By far the most credits are rewarded for optimizing energy performance. This promotes innovative thinking about alternative forms of energy and encourages increased efficiency.\n\nSustainable Sites Initiative, a combined effort of the American Society of Landscape Architects, The Lady Bird Johnson Wildflower Center at The University of Texas at Austin, and the United States Botanic Garden, is a voluntary national guideline and performance benchmark for sustainable land design, construction and maintenance practices. The building principles of SSI are to design with nature and culture, use a decision-making hierarchy of preservation, conservation, and regeneration, use a system thinking approach, provide regenerative systems, support a living process, use a collaborative and ethical approach, maintain integrity in leadership and research, and finally foster environmental stewardship. All of these help promote solutions to common environmental issues such as greenhouse gases, urban climate issues, water pollution and waste, energy consumption, and health and wellbeing of site users. The main focus is hydrology, soils, vegetation, materials, and human health and well being.\n\nIn SSI, the main goal for hydrology in sites is to protect and restore existing hydrologic functions. To design storm water features to be accessible to site users, and manage and clean water on site. For site design of soil and vegetationmany steps can be done during the construction process to help minimize the urban heat island effects, to and minimize the building heating requirements by using plants.\n\nAs major focus of the sustainable cities, sustainable transportation attempts to reduce a city’s reliance and use of greenhouse emitting gases by utilizing eco friendly urban planning, low environmental impact vehicles, and residential proximity to create an urban center that has greater environmental responsibility and social equity.\n\nDue to the significant impact that transportation services have on a city’s energy consumption, the last decade has seen an increasing emphasis on sustainable transportation by developmental experts. Currently, transportation systems account for nearly a quarter of the world’s energy consumption and carbon dioxide emission. In order to reduce the environmental impact caused by transportation in metropolitan areas, sustainable transportation has three widely agreed upon pillars that it utilizes to create more healthy and productive urban centers.\n\nThe Carbon Trust states that there are three main ways cities can innovate to make transport more sustainable without increasing journey times - better land use planning, modal shift to encourage people to choose more efficient forms of transport, and making existing transport modes more efficient.\n\nThe concept of car free cities or a city with large pedestrian areas is often part of the design of a sustainable city. A large part of the carbon footprint of a city is generated by cars so the car free concept is often considered an integral part of the design of a sustainable city.\n\nCreated by eco friendly urban planning, the concept of urban proximity is an essential element of current and future sustainable transportation systems. This requires that cities be built and added onto with appropriate population and landmark density so that destinations are reached with reduced time in transit. This reduced time in transit allows for reduced fuel expenditure and also opens the door to alternative means of transportation such as bike riding and walking.\nTransportation in downtown Chicago\nFurthermore, close proximity of residents and major landmarks allows for the creation of efficient public transportation by eliminating long sprawled out routes and reducing commute time. This in turn decreases the social cost to residents who choose to live in these cities by allowing them more time with families and friends instead by eliminating part of their commute time.\n\nSee also: Compact city and Pocket neighborhood\n\nSustainable transportation emphasizes the use of a diversity of fuel-efficient transportation vehicles in order to reduce greenhouse emissions and diversity fuel demand. Due to the increasingly expensive and volatile cost of energy, this strategy has become very important because it allows a way for city residents to be less susceptible to varying highs and lows in various energy prices.\n\nAmong the different modes of transportation, the use alternative energy cars and widespread installation of refueling stations has gained increasing importance, while the creation of centralized bike and walking paths remains a staple of the sustainable transportation movement.\n\nIn order to maintain the aspect of social responsibility inherent within the concept of sustainable cities, implementing sustainable transportation must include access to transportation by all levels of society. Due to the fact that car and fuel cost are often too expensive for lower income urban residents, completing this aspect often revolves around efficient and accessible public transportation.\n\nIn order to make public transportation more accessible, the cost of rides must be affordable and stations must be located no more than walking distance in each part of the city. As studies have shown, this accessibility creates a great increase in social and productive opportunity for city residents. By allowing lower income residents cheap and available transportation, it allows for individuals to seek employment opportunities all over the urban center rather than simply the area in which they live. This in turn reduces unemployment and a number of associated social problems such as crime, drug use, and violence.\n\nAlthough there is not an international policy regarding sustainable cities and there are not established international standards, there is an organization, the United Cities and Local Governments (UCLG) that is working to establish universal urban strategic guidelines. The UCLG a democratic and decentralized structure that operates in Africa, Asia, Eurasia, Europe, Latin America, North America, Middle East, West Asian and a Metropolitan section work to promote a more sustainable society. The 60 members of the UCLG committee evaluate urban development strategies and debate theses experiences to make the best recommendations. Additionally, the UCLG accounts for differences in regional and national context. All the organizations are making a great effort to promote this concept by media and internet, and in conferences and workshops. An International conference was held in Italy at Università del Salento and Università degli Studi della Basilicata, called 'Green Urbanism', from 12–14 October 2016.\n\nRecently, local and national governments and regional bodies such as the European Union have recognized the need for a holistic understanding of urban planning. This is instrumental to establishing an international policy that focuses on cities challenges and the role of the local authorities responses. Generally, in terms of urban planning, the responsibility of local governments are limited to land use and infrastructure provision excluding inclusive urban development strategies. The advantages of urban strategic planning include an increase in governance and cooperation that aids local governments in establishing performance based-management, clearly identifying the challenges facing local community and more effectively responding on a local level rather than national level, and improves institutional responses and local decision making. Additionally, it increases dialogue between stakeholders and develops consensus-based solutions, establishing continuity between sustainability plans and change in local government; it places environmental issues as the priority for the sustainable development of cities and serves as a platform to develop concepts and new models of housing, energy and mobility.\n\nThe City Development Strategies (CDS) addresses new challenges and provides space for innovative policies that involves all stakeholders. The inequality in spatial development and socio-economic classes paired with concerns of poverty reduction and climate change are factors in achieving global sustainable cities. According to the UCLG there are differences between regional and national conditions, framework and practice that are overcome in the international commitment to communication and negotiation with other governments, communities and the private sector to continual to develop through innovative and participatory approaches in strategic decisions, building consensus and monitoring performance management and raising investment.\n\nAccording to UN Habitat, around half of the world's population is concentrated in cities, which is set to rise to 60% within a couple decades. The UCLG has specifically identified 13 global challenges to establishing sustainable cities: demographic change and migration, globalisation of the job market, poverty and unmet Millennium Development Goals, segregation, spatial patterns and urban growth, metropolisation and the rise of urban regions, more political power for local authories, new actors for developing a city and providing services, decline in public funding for development, the environment and climate change, new and accessible building technologies, preparing for uncertainty and limits of growth and global communications and partnerships.\n\nUrban forests\n\nIn Adelaide, South Australia (a city of 1.3 million people) Premier Mike Rann (2002 to 2011) launched an urban forest initiative in 2003 to plant 3 million native trees and shrubs by 2014 on 300 project sites across the metro area. The projects range from large habitat restoration projects to local biodiversity projects. Thousands of Adelaide citizens have participated in community planting days. Sites include parks, reserves, transport corridors, schools, water courses and coastline. Only trees native to the local area are planted to ensure genetic integrity. Premier Rann said the project aimed to beautify and cool the city and make it more liveable; improve air and water quality and reduce Adelaide's greenhouse gas emissions by 600,000 tonnes of C02 a year. He said it was also about creating and conserving habitat for wildlife and preventing species loss.\n\nSolar power\n\nThe Rann government also launched an initiative for Adelaide to lead Australia in the take-up of solar power. In addition to Australia's first 'feed-in' tariff to stimulate the purchase of solar panels for domestic roofs, the government committed millions of dollars to place arrays of solar panels on the roofs of public buildings such as the museum, art gallery, Parliament, Adelaide Airport, 200 schools and Australia's biggest rooftop array on the roof of Adelaide Showgrounds' convention hall which was registered as a power station.\n\nWind power\n\nSouth Australia went from zero wind power in 2002 to wind power, making up 26% of its electricity generation by October 2011. In the five years preceding 2011 there was a 15% drop in emissions, despite strong economic growth.\n\nWaste recycling\n\nFor Adelaide the South Australian government also embraced a Zero Waste recycling strategy, achieving a recycling rate of nearly 80% by 2011 with 4.3 million tonnes of materials diverted from landfill to recycling. On a per capita basis this was the best result in Australia, the equivalent of preventing more than a million tonnes of C02 entering the atmosphere. In the 1970s container deposit legislation was introduced. Consumers are paid a 10 cent rebate on each bottle, can, or container they return to recycling. In 2009 non-reusable plastic bags used in supermarket checkouts were banned by the Rann Government, preventing 400 million plastic bags per year entering the litter stream. In 2010 Zero Waste SA was commended by a UN Habitat Report entitled 'Solid Waste Management in the World Cities'.\n\nMelbourne\n\n\nThe City of Greater Taree north of Sydney has developed a masterplan for Australia's first low-to-no carbon urban development.\n\nBelo Horizonte, Brazil was created in 1897 and is the third largest metropolis in Brazil, with 2.4 million inhabitants. The Strategic Plan for Belo Horizonte (2010–2030) is being prepared by external consultants based on similar cities' infrastructure, incorporating the role of local government, state government, city leaders and encouraging citizen participation. The need for environmental sustainable development is led by the initiative of new government following planning processes from the state government. Overall, the development of the metropolis is dependent on the land regularization and infrastructure improvement that will better support the cultural technology and economic landscape.Southern cities of Porto Alegre and Curitiba are often cited as examples of urban sustainability.\n\n\nThe GreenScore City Index studies the ecological footprints of Canadian cities and splits them into three population categories: large, medium, and small. The index studies 50 cities in Canada.\n\n\nMost cities in Canada have sustainability action plans which are easily searched and downloaded from city websites.\n\nIn 2010, Calgary ranked as the top eco-city in the planet for it's, \"excellent level of service on waste removal, sewage systems, and water drinkability and availability, coupled with relatively low air pollution.” The survey was performed in conjunction with the reputable Mercer Quality of Living Survey.\n\n\nTwo comprehensive studies were carried out for the whole of Denmark in 2010 (The IDA Climate Plan 2050) and 2011 (The Danish Commission on Climate Change Policy). The studies analysed the benefits and obstacles of running Denmark on 100% renewable energy from the year 2050. There is also a larger, ambitious plan in action: the Copenhagen 2025 Climate Plan.\n\nOn a more local level, the industrial park in Kalundborg is often cited as a model for industrial ecology. However, projects have been carried out in several Danish cities promoting 100% renewable energy. Examples include Aalborg, Ballerup and Frederikshavn. Aalborg University has launched a master education program on sustainable cities (Sustainable Cities @ Aalborg University Copenhagen). See also the Danish Wikipedia.\n\n\nLoja, Ecuador won three international prizes for the sustainability efforts begun by its mayor Dr. Jose Bolivar Castillo.\n\nOxford Residences for four seasons in Estonia, winning a prize for Sustainable Company of the Year, is arguably one of the most advanced sustainable developments, not only trying to be carbon neutral, but already carbon negativeGermany \n\nFreiburg im Breisgau is often referred to as green city. It is known for its strong solar economy. Vauban, Freiburg is a sustainable model district. All houses are built to a low energy consumption standard and the whole district is designed to be carfree. \n\nThe Finnish city of Turku has adopted a \"Carbon Neutral Turku by 2040\" strategy to achieve carbon neutrality via combining the goal with circular economy.\n\nNo other country has built more eco-city projects than Germany. Freiburg im Breisgau is often referred to as a green city. It is one of the few cities with a Green mayor and is known for its strong solar energy industry. Vauban, Freiburg is a sustainable model district. All houses are built to a low energy consumption standard and the whole district is designed to be carfree. Another green district in Freiburg is Rieselfeld, where houses generate more energy than they consume. There are several other green sustainable city projects such as Kronsberg in Hannover and current developments around Munich, Hamburg and Frankfurt.\n\n\nThe government portrays the proposed Hung Shui Kiu new town as an eco-city. The same happened with the urban development plan on the site of the former Kai Tak Airport.\n\nSouth Dublin County Council announced plans in late 2007 to develop Clonburris, a new suburb of Dublin to include up to 15,000 new homes, to be designed to achieve the highest of international standards. The plans for Clonburris include countless green innovations such as high levels of energy efficiency, mandatory renewable energy for heating and electricity, the use of recycled and sustainable building materials, a district heating system for distributing heat, the provision of allotments for growing food, and even the banning of tumble driers, with natural drying areas being provided instead.\n\nIn 2012 a energy plan was carried out by the Danish Aalborg University for the municipalities of Limerick and Clare. The project was a short-term 2020 renewable energy strategy giving a 20% reduction in CO emissions, while ensuring that short-term actions are beneficial to the long-term goal of 100% renewable energy.\n\nIndia is working on Gujarat International Finance Tec-City or GIFT which is an under-construction world-class city in the Indian state of Gujarat. It will come up on 500 acres (2.0 km) land. It will also be first of its kind fully Sustainable City.\nAuroville was founded in 1968 with the intention of realizing human unity, and is now home to approximately 2,000 individuals from over 45 nations around the world. Its focus is its vibrant community culture and its expertise in renewable energy systems, habitat restoration, ecology skills, mindfulness practices, and holistic education.\nAndhra Pradesh state New capital also coming up with a future sustainable city.\n\nHacienda - Mombasa, Kenya. It is the largest development of eco-friendly residential properties in East Africa; construction is currently ongoing, and it will eventually be one of Africa’s first self-sustaining estates.\n\nSongdo IBD is a planned city in Incheon which has incorporated a number of eco-friendly features. These include a central park irrigated with seawater, a subway line, bicycle lanes, rainwater catchment systems, and pneumatic waste collection system. 75% of the waste generated by the construction of the city will be recycled.\n\nGwanggyo City Centre is another planned sustainable city.\n\nAs of 2014 a Low Carbon Cities programme is being piloted in Malaysia by KeTTHA, the Malaysian Ministry of Energy, Green Technology and Water, Malaysian Green Technology Corporation (GreenTech Malaysia) and the Carbon Trust.\n\nMalacca has a stated ambition to become a carbon-free city, taking steps towards creating a smart electricity grid. This is being done as part of an initiative to create a Green Special Economic Zone, where it is intended that as many as 20 research and development centers will be built focusing on renewable energy and clean technology, creating up to 300,000 new green jobs. \n\nThe Federal Department of Town and Country Planning (FDTCP) in peninsular Malaysia is a focal point for the implementation of the Malaysian Urban Rural National Indicators Network for Sustainable Development (MURNInets)MURNInets includes 36 sets of compulsory indicators grouped under 21 themes under six dimensions. Most of the targets and standards for the selected indicators were adjusted according to hierarchy of local authorities. In MURNInets at least three main new features are introduced. These include the Happiness Index, an indicator under the quality of life theme to meet the current development trend that emphasizes on the well-being of the community. Another feature introduced is the customer or people satisfaction level towards local authorities' services. Through the introduction of these indicators the bottom-up approach in measuring sustainability is adopted.\n\nThe city of Waitakere, the Western part of the greater Auckland urban region, was New Zealand's first eco-city, working from the Greenprint, a guiding document that the City Council developed in the early 1990s.\n\nClark Freeport Zone is a former United States Air Force base in the Philippines. It is located on the northwest side of Angeles City and on the west side of Mabalacat City in the province of Pampanga, about 40 miles (60 km) northwest of Metro Manila. A multi-billion project will convert the 36,000 hectare former Clark Air Force Base into a mix of industrial, commercial and institutional areas of green environment. The heart of the project is a 9,450-hectare metropolis dubbed as the \"Clark Green City\". Builders will use the green building system for environmentally-friendly structures. Its facilities will tap renewable energy such as solar and hydro power.\n\nThe organization Living PlanIT is currently constructing a city from scratch near Porto, Portugal. Buildings will be electronically connected to vehicles giving the user a sense of personal eco-friendliness.\n\n\n\n\n\n\nSee also the Sustainability navigational box at the bottom of the page.\n\n\n\n"}
{"id": "20855147", "url": "https://en.wikipedia.org/wiki?curid=20855147", "title": "Tellurometer", "text": "Tellurometer\n\nThe Tellurometer was the first successful microwave electronic distance measurement equipment. The name derives from the Latin \"tellus\", meaning Earth.\n\nThe original Tellurometer, known as the Micro-Distancer MRA 1, was introduced in 1957. It was invented by Dr. Trevor Lloyd Wadley of the Telecommunications Research Laboratory of the South African Council for Scientific and Industrial Research (CSIR), also responsible for the Wadley Loop receiver, which allowed precision tuning over wide bands, a task that had previously required switching out multiple crystals.\n\nThe Tellurometer emits an electronic wave: the remote station reradiates the incoming wave in a similar wave of more complex modulation, and the resulting phase shift was a measure of the distance travelled. The results appear on a cathode ray tube with circular sweep. This instrument penetrates haze and mist in daylight or darkness and has a normal range of 30–50 km but can extend up to 70 km.\n\nThe Tellurometer design yields high accuracy distance measurements over geodetic distances, but it is also useful for second order survey work, especially in areas where the terrain was rough and/or the temperatures extreme.\n\nExamples of remote locations mapped using Tellurometer surveys are Adams Bluff, Churchill Mountains, Cook Mountains, Jacobsen Glacier, Mount Albright, Mount Predoehl, Mount Summerson, Sherwin Peak and Vogt Peak.\n\nThe MRB2 or Hydrodist was a marine version that was used in coastal surveys and calibrating ships using other survey navigation systems.\n\nThey were used by the Army of the Republic of Vietnam in the late 1960s.\n\nPlessey, the British electronics company, formed a new subsidiary known as \"Tellurometer (Pty) Limited\" in the 1960s to manufacture the product and to develop and sell derivatives. The Company subsequently introduced numerical displays, solid state transmitters, integrated circuits and eventually microprocessors for the product.\n\n"}
{"id": "36860570", "url": "https://en.wikipedia.org/wiki?curid=36860570", "title": "Trinity House of Leith", "text": "Trinity House of Leith\n\nTrinity House, 99 Kirkgate, is a category A listed building in Leith, Edinburgh, Scotland, which was a guild hall, customs house, and centre for maritime administration and poor relief. In the Late Middle Ages and Early Modern Era it also served as an almshouse and hospital. Now in state care, it houses a maritime museum.\n\nTrinity House was the headquarters of the Incorporation of Masters and Mariners, a trade incorporation and charitable organisation founded in the 14th century when the shipowners and shipmasters of Leith formed a Fraternity (from which the name, \"Trinity\", may derive). The present Trinity House is a Category A listed Georgian neoclassical house, designed by Thomas Brown and built in 1816-8, using the existing basement and vaults of the former Trinity House and mariners' hospital of 1555.\n\nConcerned to improve safety at sea, Trinity House established the first formal nautical training in the country and licensed pilots for the Forth and around the Scottish coast. By collecting Licht Money (light money), by the 17th century they were maintaining primitive coal-fired lights in the Forth. In the 19th century, Trinity House was involved in the planning and funding of new and more reliable lighthouses that took advantage of improvements in technology. These included the Bell Rock lighthouse, Fidra lighthouse and the Isle of May lighthouse.\n\nThe Masters and Mariners invested in land, which became known as Trinity Mains, near the village of Newhaven. This land later developed into a suburb of Leith and is now the modern-day district of Trinity.\n\nThe medieval Incorporation served as a blueprint for the establishment of Trinity Houses in other maritime centres, including Dundee, Hull, London and Newcastle-upon-Tyne.\n\nThe Masters and Mariners of the Trinity House in the Kirkgate was the oldest and wealthiest of the trade guilds of Leith. The guilds were divided into four groups:\n\n\nIn 1380 King Robert II granted the Incorporation of Master and Mariners of Leith the right to levy a duty, called prime gilt, of 12 pennies on each ton of goods landed at Leith. An additional voluntary contribution, called crown money, was also collected. Trade was largely conducted over the North Sea, with the Nordic and Baltic regions, the Low Countries and France. The funds raised by the prime gilt and crown money were then used for the relief of the sick, the poor and the widows and orphans of lost or captured mariners; and to care for aged mariners.\n\nIn 1555 the Incorporation had sufficient funds to build a hospital at the Kirkgate. The basement of this building survives under the current Trinity House.\n\nFollowing a series of disputes over payments, in 1566 Mary Queen of Scots confirmed the right of the Incorporation to collect payments: ratifying \"the gift, foundation, erection and institution of the hospital and of the prime gilt\". Refusal to pay would result in the confiscation of sails and anchor.\n\nIn 1680, funded by fees and by a levy on Leith shipmasters, the Masters and Mariners appointed a professor to teach the mathematics of navigation to the sons and apprentices of shipmasters. This training for future officers was subsequently extended across Scotland.\n\nOn 29 June 1797 the Corporation of the Trinity House of Leith was granted a royal charter.\n\nAs a result of the requirement for formal qualifications stipulated in the Merchant Seamen Act 1844, in 1855 Trinity House and other Leith organisations founded the Leith Navigational School (also called the Government Navigation School), based at a room at St Ninian's Church (also called the Mariners' Church), on Commercial Street. In 1903 Leith Navigation School came under the control of the Scottish Education Department, and the name was changed to Leith Nautical College (since merged to become part of Jewel & Esk College, which in 2012 merged to become Edinburgh College).\n\nPrime gilt was abolished in 1862, so Trinity House had to depend on property income to meet pension payments.\n\nCurrently Trinity House is in the care of Historic Environment Scotland.\n\n\n"}
{"id": "20193195", "url": "https://en.wikipedia.org/wiki?curid=20193195", "title": "UN-SPIDER", "text": "UN-SPIDER\n\nUN-SPIDER (\"United Nations Platform for Space-based Information for Disaster Management and Emergency Response\") is a platform which facilitates the use of space-based technologies for disaster management and emergency response. It is a programme under the auspices of the United Nations Office for Outer Space Affairs (UNOOSA).\n\nGlobal vulnerability to natural disasters is likely to increase as the impact of climate change and land degradation processes continues to rise along with rapidly growing populations. Earthquakes, floods, storms, and other natural hazards cause massive disruption to societies and overburden national economic systems. Considerable losses of life and property, however, could be avoided through better information about the risk and onset of disasters, improved risk assessment, early warning, and disaster monitoring. In recognition of these needs the United Nations General Assembly, in its resolution 61/110 of 14 December 2006, acknowledged that use of existing space technology, such as earth observation and meteorological satellites, communication and navigation satellites can play a major role in supporting disaster management by providing accurate and timely information for decision making. Space-based information are relevant in all phases of the disaster management cycle from disaster risk reduction to disaster response and recovery.\n\nIn March 2015, world leaders convened in Sendai, Japan to agree on a new global framework for disaster risk reduction for the period 2015 through 2030. The outcome document refers to the importance of Space-based and geospatial information in several paragraphs, highlighting, among others, the importance to \"develop, update periodically and disseminate, as appropriate, location-based disaster risk information, including risk maps, to decision makers, the general public and communities at risk to disaster in an appropriate format by using, as applicable, geospatial information technology\".\n\nIn its resolution 61/110 of 14 December 2006, the United Nations General Assembly agreed to establish UN-SPIDER as a new United Nations programme, with the following mission statement: \"Ensure that all countries and international and regional organizations have access to and develop the capacity to use all types of space-based information to support the full disaster management cycle\". In doing so, UN-SPIDER aims at three goals: being a gateway to space information for disaster management support; serving as a bridge to connect the disaster management and space communities; and being a facilitator of capacity-building and institutional strengthening.\n\nUN-SPIDER has three offices in Vienna, Austria, in Bonn, Germany and in Beijing, China.\n\nThe UN-SPIDER Vienna office is located at UNOOSA's headquarters in the Vienna International Centre. Its staff members are in charge of general UN-SPIDER coordination, fund-raising, Regional Support Office (RSO) coordination and Technical Advisory Support. The office is supported by the government of Austria.\n\nThe UN-SPIDER Bonn Office was established in October 2007 with the support from the German Federal Ministry of Economics and Technology (BMWi) and the German Aerospace Center (DLR). The UN-SPIDER Bonn Office is responsible for UN-SPIDER's knowledge management. The goal is to make sure that all relevant information is easily accessible and disseminated to all stakeholders in the areas of disaster management and emergency and humanitarian response. This is done mainly via UN-SPIDER's Knowledge Portal, which is managed by the Bonn staff. The Bonn Office is also dealing with Technical Advisory Support in Latin America and the Caribbean.\n\nThe Beijing office opened on November 9, 2010, and is supported by the government of the People's Republic of China. The Beijing Office is mainly dealing with Technical Advisory Support in the Asia and Pacific region and the coordination of UN-SPIDER's network of National Focal Points.\n\nThe acquisition, processing and transfer of knowledge is the central element of UN-SPIDER's activities. A knowledge base on space-based information and solutions to support risk and disaster management are therefore made available through the UN-SPIDER Knowledge Portal.\n\nThe UN-SPIDER Knowledge Portal is central to all knowledge management activities carried out within the framework of UN-SPIDER, as it provides the hosting environment and the dissemination tool for all resulting outputs and products. The Knowledge Portal went online in June 2009 and has been continuously improved and amended ever since. Its main tool is the Space Application Matrix, a sophisticated search engine to make available research papers and case studies on the application of different space-based resources in all phases of the disaster management cycle. The Portal also features the latest news from the disaster and risk management and the space communities, information on workshops, trainings and events as well as profile details on UN-SPIDER's network partners.\n\nUN-SPIDER's knowledge management activities are accompanied by its awareness-raising efforts. Since awareness-raising is a process by which the level of understanding among the persons targeted is raised, fostering change in attitudes and behaviour, it is central to the success of promoting the use of space-based information. In the framework of UN-SPIDER, awareness-raising is designed as an ongoing process accompanying, facilitating and preparing activities, as new audiences are addressed, new partnerships are formed and new technological solutions are developed, offering new opportunities for existing and new target groups. UN-SPIDER implements its awareness-raising activities mainly via its publications such as the monthly Updates and the biannual Newsletter as well as via its Knowledge Portal.\n\nExperience shows that the conduction of activities targeting the full disaster management cycle spans a variety of agencies from the public and the private sector, at different levels, and is best conducted through a coordinated approach. UN-SPIDER's outreach activities contribute to involving practitioners and experts from the disaster management and space communities in UN-SPIDER activities with the end-goal to promote the use of space-based information to support the full disaster management cycle. \n\nUN-SPIDER's outreach activities include the organization of workshops, seminars and expert meetings in all regions as well as the support to similar events organized by its partners. UN-SPIDER staff furthermore participates in relevant events all over the world to raise awareness about UN-SPIDER's activities and the opportunities that space-based information offers for disaster and risk management.\n\nTechnical Advisory Support (TAS) is one of the prime activities of the UN-SPIDER programme at the national level. It serves to identify the existing capacity to use space-based information, to analyze the institutional framework to support disaster management through spacebased information and to identify the limitations that inhibit the use of such information. TAS attempts to enable Member States to overcome these limitations through international cooperation and regional opportunities, networking with regional institutions, and setting up disaster management plans. It covers region-specific aspects such as trans-boundary issues, emergency response, risk assessment, \nGIS-based disaster management systems, and disaster-risk reduction. TAS efforts range from a simple consultative phone call to the facilitation of technical support, missions, trainings and workshops. UN-SPIDER's TAS has three pilars: Technical Advisory Missions, Capacity Building and Facilitation of Emergency Support/Technical Support.\n\nTechnical Advisory Missions (TAMs) are an instrument to identify the needs of Member States regarding their capacities to fully take advantage of space-based information. TAMs are officially requested by the respective national government and are conducted by a team of experts. The team meets with key disaster management and development authorities in the Government, United Nations organizations, regional and international organizations/initiatives and private entrepreneurs to discuss the topic in depth. It makes recommendations focusing on how to improve the access to and use of space-based information in risk and disaster management. Since 2008 various missions have been carried out to countries in Latin America, the Caribbean, Africa, Asia and the Pacific region.\n\nUN-SPIDER defines its Capacity Building as the process of facilitating the strengthening of the competency of individuals, teams, and agencies to use space-based information to prevent, mitigate, and respond effectively to the challenges posed by natural hazards and related humanitarian crises. UN-SPIDER capacity building efforts include \nfour complementary types of activities: Providing policy-relevant advice to institutions and governments regarding the use of space-based (spatial) information to support the full disaster management cycle, facilitating access to space-based data and services, facilitating the training of individuals on access to and use of such data and facilitating access to infrastructure, hardware, and software, and services for spacebased applications. Therefore, UN-SPIDER's capacity building simultaneously aims at institutions, individuals and infrastructure.\n\nIn the case of emergencies and disasters, UN-SPIDER provides Emergency Supportby taking the role of a bridge linking the disaster management agencies in charge of response operations with space agencies or the mechanisms which have been established by the space community such as the International Charter: Space and Major Disasters. UN-SPIDER provides this support through the activation of its network of Regional Support Offices (RSO) and through links with specific space agencies.\n\nUN-SPIDER created a global network in order to foster and strengthen strategic alliances and partnerships on a global and regional scale. There are two types of networks: Regional Support Offices (RSOs) and National Focal Points (NFP).\n\nA Regional Support Office is a regional or national center of expertise that is set up within an existing entity by a Member State. The establishment of a network of Regional Support Offices was agreed upon by the United Nations General Assembly in its Resolution 61/110. UN-SPIDER currently has Regional Support Offices in Algeria, Argentina, Japan, Colombia, Hungary, Indonesia, Iran, Kenya, Nepal, Nigeria, Pakistan, Panama, Romania, Russia, Sri Lanka, Ukraine, and West Indies. Detailed information on all UN-SPIDER Regional Support Offices can be found in the UN-SPIDER Knowledge Portal\n\nRegional Support Offices communicate and coordinate with UN-SPIDER staff on regular basis, covering the three following realms:\n\nAs defined by the United Nations General Assembly, a National Focal Point is a national institution, nominated by the government of the respective country, representing the disaster management and space applications communities. Among them are for example members of the space - or civil protection agencies. UN-SPIDER networks with all countries through the National Focal Points.\n\nA national Focal Point works with UN-SPIDER staff to achieve the following goals:\n\n"}
{"id": "21898118", "url": "https://en.wikipedia.org/wiki?curid=21898118", "title": "Underwater acoustic positioning system", "text": "Underwater acoustic positioning system\n\nAn underwater acoustic positioning system is a system for the tracking and navigation of underwater vehicles or divers by means of acoustic distance and/or direction measurements, and subsequent position triangulation. Underwater acoustic positioning systems are commonly used in a wide variety of underwater work, including oil and gas exploration, ocean sciences, salvage operations, marine archaeology, law enforcement and military activities.\n\nFigure 1 describes the general method of operation of an acoustic positioning system, this is an example of a long baseline (LBL) positioning system for ROV\n\n\nAcoustic positioning systems measure positions relative to a framework of baseline stations, which must be deployed prior to operations. In the case of a long-baseline (LBL) system, a set of three or more baseline transponders are deployed on the sea floor. The location of the baseline transponders either relative to each other or in global coordinates must then be measured precisely. Some systems assist this task with an automated acoustic self-survey, and in other cases GPS is used to establish the position of each baseline transponder as it is deployed or after deployment.\n\n\nFollowing the baseline deployment and survey, the acoustic positioning system is ready for operations. In the long baseline example (see figure 1), an interrogator (A) is mounted on the ROV that is to be tracked. The interrogator transmits an acoustic signal that is received by the baseline transponders (B, C, D, E). The reply of the baseline transponders is received again at the ROV. The signal time-of-flight or the corresponding distances A-B, A-C, A-D and A-E are transmitted via the ROV umbilical (F) to the surface, where the ROV position is computed and displayed on a tracking screen. The acoustic distance measurements may be augmented by depth sensor data to obtain better positioning accuracy in the three-dimensional underwater space.\n\nAcoustic positioning systems can yield an accuracy of a few centimeters to tens of meters and can be used over operating distance from tens of meters to tens of kilometers. Performance depends strongly on the type and model of the positioning system, its configuration for a particular job, and the characteristics of the underwater acoustic environment at the work site.\n\nUnderwater acoustic positioning systems are generally categorized into three broad types or classes\n\nLong-baseline (LBL) systems, as in figure 1 above, use a sea-floor baseline transponder network. The transponders are typically mounted in the corners of the operations site. LBL systems yield very high accuracy of generally better than 1 m and sometimes as good as 0.01m along with very robust positions This is due to the fact that the transponders are installed in the reference frame of the work site itself (i.e. on the sea floor), the wide transponder spacing results in an ideal geometry for position computations, and the LBL system operates without an acoustic path to the (potentially distant) sea surface.\nUltra-short-baseline (USBL) systems and the related super-short-baseline (SSBL) systems rely on a small (ex. 230 mm across), tightly integrated transducer array that is typically mounted on the bottom end of a strong, rigid transducer pole which is installed either on the side or in some cases on the bottom of a surface vessel. Unlike LBL and SBL systems, which determine position by measuring multiple distances, the USBL transducer array is used to measure the target \"distance\" from the transducer pole by using signal run time, and the target \"direction\" by measuring the phase shift of the reply signal as seen by the individual elements of the transducer array. The combination of distance and direction fixes the position of the tracked target relative to the surface vessel. Additional sensors including GPS, a gyro or electronic compass and a vertical reference unit are then used to compensate for the changing position and orientation (pitch, roll, bearing) of the surface vessel and its transducer pole. USBL systems offer the advantage of not requiring a sea floor transponder array. The disadvantage is that positioning accuracy and robustness is not as good as for LBL systems. The reason is that the fixed angle resolved by a USBL system translates to a larger position error at greater distance. Also, the multiple sensors needed for the USBL transducer pole position and orientation compensation each introduce additional errors. Finally, the non-uniformity of the underwater acoustic environment cause signal refractions and reflections that have a greater impact on USBL positioning than is the case for the LBL geometry.\n\nShort-baseline (SBL) systems use a baseline consisting of three or more individual sonar transducers that are connected by wire to a central control box. Accuracy depends on transducer spacing and mounting method. When a wider spacing is employed as when working from a large working barge or when operating from a dock or other fixed platform, the performance can be similar to LBL systems. When operating from a small boat where transducer spacing is tight, accuracy is reduced. Like USBL systems, SBL systems are frequently mounted on boats and ships, but specialized modes of deployment are common too. For example, the Woods Hole Oceanographic Institution uses a SBL system to position the Jason deep-ocean ROV relative to its associated MEDEA depressor weight with a reported accuracy of 9 cm\n\nGPS intelligent buoys (GIB) systems are inverted LBL devices where the transducers are replaced by floating buoys, self-positioned by GPS. The tracked position is calculated in realtime at the surface from the Time-Of-Arrival (TOAs) of the acoustic signals sent by the underwater device, and acquired by the buoys. Such configuration allow fast, calibration-free deployment with an accuracy similar to LBL systems. At the opposite of LBL, SBL or USBL systems, GIB systems use one-way acoustic signals from the emitter to the buoys, making it less sensitive to surface or wall reflections. GIB systems are used to track AUVs, torpedoes, or divers, may be used to localize airplanes black-boxes, and may be used to determine the impact coordinates of inert or live weapons for weapon testing and training purposes references: Sharm-El-Sheih, 2004; Sotchi, 2006; Kayers, 2005; Kayser, 2006; Cardoza, 2006 and others...).\n\nAn early use of underwater acoustic positioning systems, credited with initiating the modern day development of these systems, involved the loss of the American nuclear submarine USS \"Thresher\" on 10 April 1963 in a water depth of 2560m. An acoustic short baseline (SBL) positioning system was installed on the oceanographic vessel USNS \"Mizar\". This system was used to guide the bathyscaphe Trieste 1 to the wreck site. Yet, the state of the technology was still so poor that out of ten search dives by Trieste 1, visual contact was only made once with the wreckage. Acoustic positioning was again used in 1966, to aid in the search and subsequent recovery of a nuclear bomb lost during the crash of a B-52 bomber at sea off the coast of Spain.\n\nIn the 1970s, oil and gas exploration in deeper waters required improved underwater positioning accuracy to place drill strings into the exact position referenced earlier thorough seismic instrumentation and to perform other underwater construction tasks.\n\nBut, the technology also started to be used in other applications. In 1998, salvager Paul Tidwell and his company Cape Verde Explorations led an expedition to the wreck site of the World War 2 Japanese cargo submarine I-52 in the mid-Atlantic. Resting at a depth of 5240 meters, it had been located and then identified using side scan sonar and an underwater tow sled in 1995. War-time records indicated the I-52 was bound for Germany, with a cargo including 146 gold bars in 49 metal boxes. This time, Mr. Tidwell's company had hired the Russian oceanographic vessel, the \"Akademik Mstislav Keldysh\" with its two manned deep-ocean submersibles \"MIR-1\" and \"MIR-2\" (figure 3). In order to facilitate precise navigation across the debris field and assure a thorough search, \"MIR-1\" deployed a long baseline transponder network on the first dive. Over a series of seven dives by each submersible, the debris field was progressively searched. The LBL positioning record indicated the broadening search coverage after each dive, allowing the team to concentrate on yet unsearched areas during the following dive. No gold was found, but the positioning system had documented the extent of the search.\n\nIn recent years, several trends in underwater acoustic positioning have emerged. One is the introduction of compound systems such the combination of LBL and USBL in a so-called LUSBL configuration to enhance performance. These systems are generally used in the offshore oil & gas sector and other high-end applications. Another trend is the introduction of compact, task optimized systems for a variety of specialized purposes. For example, the California Department of Fish and Game commissioned a system (figure 4), which continually measures the opening area and geometry of a fish sampling net during a trawl. That information helps the department improve the accuracy of their fish stock assessments in the Sacramento River Delta.\n\n"}
{"id": "34330145", "url": "https://en.wikipedia.org/wiki?curid=34330145", "title": "United States Geological Survey Library", "text": "United States Geological Survey Library\n\nThe United States Geological Survey Library (USGS Library) is a program within the United States Geological Survey, a scientific bureau within the Department of Interior of the United States government. The USGS operates as a fact-finding research organization with no regulatory responsibility.\n\nThe USGS Library has major branches in Lakewood, Colorado (Denver Federal Center), and Menlo Park, California and smaller, focused research libraries in Flagstaff, Arizona and Lafayette, Louisiana.\n\nToday the United States Geological Survey Library's users have access to over 3 million items: over 1.7 million books and journals, 700,000 maps, 370,000 microforms, 270,000 pamphlets, 260,000 black-and-white photographs, 60,000 color transparencies, 15,000 field record notebooks, and 250 videocassettes. Materials include USGS publications as well as those produced by state and foreign geological surveys, scientific societies, museums, academic institutions, and government scientific agencies. The libraries in Reston and Menlo Park are designated as official depositories for selected U.S. Government publications.\nThe libraries in Reston and Menlo Park have been designated as official Federal Government Depositories providing public access to selected U.S. Government publications.\n\nThe newly revised classification system presented in this report is designed for use in the U.S.\nGeological Survey (USGS) Library and other earth science libraries. Prior to the administration of Fred Boughton Weeks, 1903-1908, the library lacked a classification scheme. The Dewey Decimal system for geologic material was not sufficiently developed to accommodate the range of specialized material collected at the USGS Library, and The Library of Congress Classification System had not yet been published. The library staff and patrons were concerned about continued development of the collection without an acceptable classification scheme.\nMr. Weeks and bibliographer John M. Nickles of the library staff, with the assistance of three\nconsultants from the New York Public Library, developed the USGS classification system designed specifically for an earth science library.\nThe U.S. Geological Survey Library classification system has been designed for earth science\nlibraries. It is a tool for assigning call numbers to earth science and allied pure science materials in order to collect these materials into related subject groups on the library shelves and arrange them alphabetically by author and title. The classification can be used as a retrieval system to access materials through the subject and geographic numbers.\nThe classification scheme has been developed over the years since 1904 to meet the ever-changing needs of increased specialization and the development of new areas of research in the earth sciences.\n\nThe Field Records Collection is an archive of unpublished field notes, correspondence, manuscripts, maps, analysis reports and other data created or collected by USGS geologists during field studies and other project work. The majority of the collection dates from 1879 and relates to work done in the contiguous United States. Materials in the collection represent almost 130 years of scientific investigations by the USGS, from the earliest days of the agency to recently completed projects. Records contributed by approximately 1,200 USGS scientists are presently archived. Located in the Central Region Library in Denver, Colorado, the collection is available for on-premises examination during normal library hours.\nField records and project archives on Alaska are kept in the Alaska Technical Data Unit Field Records Archive.\n\nThe Rare Book Collection of the USGS Library comprises unusual publications, rare books, and maps collected since 1879. Included are historical maps and publications of the Survey, as well as early publications of many federal, state and other geological surveys. Records of select geological societies are also maintained in the collection, such as the Geological Society of Washington, which was founded by John Wesley Powell and other noted scientists after the Civil War. Of special note are many 19th century maps with topics such as American political boundaries, transportation, geology, and mining.\n\nAt an auction in Paris, France, pieces of M. Jules Desnoyers's (1800–1887) library were purchased in 1885 by the USGS Library to start the foreign country collection. M. Desnoyers was a founder and later Secretary of the Société Géologique de France, and in 1834 he was appointed librarian of the Muséum National d'Histoire Naturelle in Paris.\n\nWilliam R. Halliday, M.D. (b. 1926), joined the National Speleological Society in 1947. By the late 1950s he was director of the Western Speleological Survey. His works include Adventure Is Underground (1959), Caves of California (1962), Caves of Washington (1963), Depths of the Earth (1966), Caves and Cavers of the United States (1966), American Caves and Caving (1974), Discovery and Exploration of the Oregon Caves: Oregon Caves National Monument (with Frank K. Walsh; 1976) and, Floyd Collins of Sand Cave: A Photographic Memorial (1998). He donated his private collection of rare and out-of-print books and pamphlets on caves, caving and speleology to the USGS Library in 2000.\n\nDr. Halliday has also donated several collections of private papers, correspondence and research to the University of Washington Library and to the J. Willard Marriott Library of the University of Utah.\n\nIn 1933, the Library acquired the George F. Kunz collection for $1.00 from the estate of the former USGS employee and Vice-President of Tiffany & Co. The George F. Kunz Collection is a significant special collection on gems and minerals including rare books on gemology, the folklore of gemstones through history, lapidary arts and archival gem trade records important to the provenance of named stones such as the \"Hope Diamond.\" Kunz was a former USGS employee, a vice-president of Tiffany & Co., and one of the world's preeminent gem experts. The collection was acquired through the genroisty of Dr. Kunz's heirs, Mrs. Opal Kunz and Mrs. Hans Zinsser. Mr. Walter E. Reid, mining consultant and strong friend of the library, was largely instrumental in securing the Kunz collection.\n\n\"The George F. Kunz Collection is a significant special collection of many rare books, pamphlets, and other unique materials on the acquisition, collecting and lore of jewels and precious stones. Acquired by the US Geological Survey Library in 1933 after the death of George F. Kunz, a noted mineralogist and vice-president of the Tiffany & Co. of New York, the Kunz Collection today is fairly unknown outside of a small circle of gemologists, historians, and art collectors. In this paper, a short biography of George F. Kunz is given, as well as a description of the contents and scope of his collection held in the USGS Library. Some examples of the unique materials held in the collection are shown. How the Kunz Collection is used by library patrons and museums is also discussed. Appended to the paper is a bibliography of the works by and about Dr. Kunz.\"\n\nThe library's map collections have provided invaluable aid to authorities and scientists in times of crisis (the California Northridge earthquake in 1994 and major fires in the nation's forests). Topographic maps have also been used for genealogy research to pinpoint where ancestors lived, locate forgotten cemeteries, provide information on boundary changes, and research natural and man-made changes to areas over time. Planners have used the foreign map collections to study foreign terrain, geologic conditions and natural resources. During WWII, The New York Times (June 11, 1944) reported \"A 'Commando' raid by a group of civilian scientists, a search through obscure seventeenth century French manuscripts, months of study of geological reports, experiments with model beaches – all these were part of the Allied preparations for the invasion of Normandy … The dramatic story of the preparations, which began in … libraries, shifted to laboratories and ended on the shell-swept beaches …\". The collections were consulted for the military interventions in Afghanistan and Iraq. Library maps have provided aid in international disaster areas such as the aftermath of Hurricane Mitch and volcanoes worldwide. A brief mention of a diamond found in the 1906 annual report of the USGS began a trail of research and investigation that led one geologist to prospect for diamonds in Canada.\n\nResponsibility for the Topographic Map Archive was formally transferred to the USGS Library in Reston, Virginia, in March 2003. The Archive includes each U.S. state and territory, in all scales, editions and various printings. With coverage dating from the 1880s when the USGS began publishing standard topographic quadrangles; the Archive is the most complete collection of USGS topographic maps.\n\nThe Heringen Collection, is a group of military texts and maps, looted by the Nazis from European libraries, universities, geological societies, private businesses, homes and offices. Hidden in a potash mine in Heringen, Hesse, Germany, they were transported by the U.S. military at the end of WWII as captured war materials. Some 23,000 reports, books and maps were accepted by the USGS Library and integrated into the main collection. The materials are consulted for research ranging from European road development, water resources use or mining and construction. In 1946, the Heringen Collection was transferred from the U.S. Geological Survey's Military Geology Unit to the Library.\n\nIn an article published in journal, Earth Sciences History, the author says that the results of the German theft of Russian maps influenced the way they made maps for the next two generations. \"The Heringen Collection at the US Geological Survey is a special collection of maps, books and reports stolen by units of the German Wehrmacht (armed forces) as they invaded and occupied countries during World War II. The materials in the Collection came from private, society and public library collections, and were used by military geologists in each German army to help protect its soldiers, to advance their invasion, and to consolidate their occupation. They used the maps of the occupied countries against their peoples. For a generation after the invasion of the Soviet Union, this influenced the way Russian maps were drawn and printed.\"\n\nThe U.S. Antarctic Resource Center (USARC) in Reston, Virginia, is our Nation's depository for Antarctic maps, charts, geodetic ground control, satellite images, aerial photographs, publications, slides, and video tapes. These resources are produced by Antarctic Treaty nations in support of their activities in Antarctica and provided to the USARC in compliance with a standing resolution of the treaty providing for exchange of information. The USGS maintains these materials through an interagency cooperative agreement with the National Science Foundation (NSF), which also supports the USGS Antarctic Mapping Program.\n\nThe Photographic Library is an archive of still photographs and original sketches dating from the 1870s and taken by USGS scientists as part of their field studies. Topics include USGS personnel, earthquakes, volcanoes, geologic hazards and other phenomena, historical mining operations, and earth science photographs. The works of pioneer photographers such as William Henry Jackson, Timothy H. O'Sullivan, Carleton Watkins, John Karl Hillers, Thomas Moran, Andrew J. Russell, E. O. Beaman and William Bell (photographer) are represented in the collection. Some photographs have been used to illustrate publications, but most have never been published.\n\nCongress authorized a Library for the United States Geological Survey (USGS) in 1879. The library was formally established in 1882, with the naming of the first librarian, Charles C. Darwin, and began with a staff of 3 and a collection of 1,400 books. The Act of Congress establishing the USGS authorized the creation of a program for exchanging copies of USGS reports for publications of state, national, and international organizations. The exchange program was modeled after a program used by Dr. Ferdinand V. Hayden when he was head of the Interior Department's U.S. Geological and Geographical Survey of the Territories (1867–1879). The U.S. Geological Survey Library inherited 1,000 volumes of serials from Dr. Hayden's former exchange program, which he had based on the program begun by the Smithsonian Institution in 1846.\nAfter Dr. Hayden died in 1887, his widow donated his personal collection to the U.S. Geological Survey Library. Other early gifts were made by Major John Wesley Powell, second Director of the US Geological Survey, who donated his collection of State Geological Survey reports and the family of Dr. Isaac Lea (Philadelphia publisher and gem collector whose family donated nearly 600 items of his personal library). Dr. William Halliday, a world-renowned speleologist began donating his cave collection in 2003.\n\nIn a review of the USGS Library operations in 1937, William Heers, the Chief Librarian, noted that the library had more than 200,000 books and reports, about 60,000 pamphlets, and also about 60,000 maps, most of these obtained by gift and exchange. Fully half of those researchers who used the library were outside of the US Geological Survey. In service to those outside the government, between 8-10,000 books were loaned out each year through interlibrary loans, both within the continental US and overseas.\n\nIn a review of the USGS Library during its centennial, it was noted that in 1978 the library had acquired nearly 116,000 new items. About 75% of these were journals, of which 10,000 serial, magazine, and other periodical titles were received. In 1978, the library also circulated 105,000 items, made 17,700 interlibrary loans and answered some 27,000 requests for information.\n\nIn March 2012, the USGS Library joined the Biodiversity Heritage Library with the goal of contributing important historical works related to taxonomy as well as relevant USGS publications.\n\nThe U.S. Geological Survey Library system has become the largest earth science library in the world. Materials within the library system include books and maps dating back to the 16th and 17th centuries. Other materials include a nearly complete set of the various State Geological Survey publications and a virtually complete set of USGS topographic maps.\nThe original collection was based on exchange partnerships with domestic and international scientific organizations.\n\n\n"}
{"id": "38773637", "url": "https://en.wikipedia.org/wiki?curid=38773637", "title": "Vertical and horizontal", "text": "Vertical and horizontal\n\nIn astronomy, geography, and related sciences and contexts, a \"direction\" or \"plane\" passing by a given point is said to be vertical if it contains the local gravity direction at that point.\n\nConversely, a direction or plane is said to be horizontal if it is perpendicular to the vertical direction.\n\nThe usage of the inter-related terms horizontal and vertical as well as their symmetries and asymmetries vary with context (e.g. two vs. three dimensions).\nIn general, something that is vertical can be drawn from up to down (or down to up), such as the y-axis in the Cartesian coordinate system.\n\nGirard Desargues defined the vertical to be perpendicular to the horizon in his \"Perspective\" of 1636.\n\nThe word \"horizontal\" is derived from \"horizon\", whereas \"vertical\" originates in the late Latin \"\", which is from the same root as \"vertex\" 'highest point'.\n\nIn physics, in engineering, and in construction, the direction designated as vertical is usually that along which a plumb-bob hangs. Alternatively, a spirit level that exploits the buoyancy of an air bubble and its tendency to go vertically upwards may be used to test for horizontality. Modern rotary laser levels that can level themselves automatically are robust sophisticated instruments and work on the same fundamental principle.\n\nIn the flat earth scenario, where the earth is notionally a large (infinite) flat surface with gravitational field at a right angle to the surface, the earth surface is horizontal and any plane parallel to the earth surface is also horizontal. Vertical planes, e.g., walls, may be parallel to each other or they may intersect at a vertical line. Horizontal surfaces do not intersect. Furthermore, a plane cannot both be a horizontal plane at one place and a vertical plane somewhere else.\n\nWhen the curvature of the earth is taken into account, the concepts of vertical and horizontal take on yet another meaning. On the surface of a smoothly spherical, homogenous, non-rotating planet, the plumb bob picks out as vertical the radial direction. Strictly speaking, it is now no longer possible for vertical walls to be parallel: \"all\" verticals intersect. This fact has real practical applications in construction and civil engineering, e.g., the tops of the towers of a suspension bridge are further apart than at the bottom. \n\nAlso, horizontal planes can intersect when they are tangent planes to separated points on the surface of the earth. In particular, a plane tangent to a point on the equator intersects the plane tangent to the North Pole at a right angle. (See diagram).\nFurthermore, the equatorial plane is parallel to the tangent plane at the North Pole and as such has claim to be a horizontal plane. But it is. at the same time, a vertical plane for points on the equator. In this sense, a plane can, arguably, be \"both\" horizontal and vertical, horizontal \"at one place\", and vertical \"at another\".\n\nFor a spinning earth, the plumb line deviates from the radial direction as a function of latitude. Only at the North and South Poles does the plumb line align with the local radius. The situation is actually even more complicated because earth is not a homogeneous smooth sphere. It is a non homogeneous, non spherical, knobby planet in motion, and the vertical not only need not lie along a radial, it may even be curved and be varying with time. On a smaller scale, a mountain to one side may deflect the plumb bob away from the true zenith.\n\nOn a larger scale the gravitational field of the earth, which is at least approximately radial near the earth, is not radial when it is affected by the moon at higher altitudes.\n\nOn a (horizontal) floor, one can draw a horizontal line but not a vertical line in the sense of a plumb bob line. But on a (vertical) wall, one can draw both vertical \"and\" horizontal lines. In this sense, a vertical wall allows more options. This is reflected in the tools a bricklayer uses: a plumb line for verticality and a spirit level to check that the mortar courses are horizontal.\nOn the other hand, in contrast to a wall a horizontal floor allows more options when one considers compass directions. One can draw on a floor lines going north, south, east, and west, in fact, along any compass direction. A wall allows fewer options. For instance, on a wall which runs along a longitude, an insect cannot crawl east.\n\nNeglecting the curvature of the earth, horizontal and vertical motions of a projectile moving under gravity are independent of each other. Vertical displacement of a projectile is not affected by the horizontal component of the launch velocity, and, conversely, the horizontal displacement is unaffected by the vertical component. The notion dates at least as far back as Galileo.\n\nWhen the curvature of the earth is taken into account, the independence of the two motion does \"not\" hold. For example, even a projectile fired in a horizontal direction (i.e., with a zero vertical component) may leave the surface of the spherical earth and indeed escape altogether.\n\nIn the context of a two-dimensional orthogonal Cartesian coordinate system on a Euclidean plane, to say that a line is horizontal or vertical, an initial designation has to be made. One can start off by designating the vertical direction, usually labelled the Y direction. The horizontal direction, usually labelled the X direction, is then automatically determined. Or, one can do it the other way around, i.e., nominate the \"x\"-axis, in which case the \"y\"-axis is then automatically determined. There is no special reason to choose the horizontal over the vertical as the initial designation: the two directions are on par in this respect.\n\nThe following hold in the two-dimensional case:\n\nNot all of these elementary geometric facts are true in the 3-D context.\n\nIn the three-dimensional case, the situation is more complicated as now one has horizontal and vertical planes in addition to horizontal and vertical lines. Consider a point P and designate a direction through P as vertical. A plane which contains P and is normal to the designated direction is the \"horizontal plane\" at P. Any plane going through P, normal to the horizontal plane is a \"vertical plane\" at P. Through any point P, there is one and only one horizontal plane but a \"multiplicity\" of vertical planes. This is a new feature that emerges in three dimensions. The symmetry that exists in the two-dimensional case no longer holds.\n\nIn the 2-dimension case, as mentioned already, the usual designation of the vertical coincides with the y-axis in co-ordinate geometry. This convention can cause confusion in the classroom. For the teacher, writing perhaps on a white board, the \"y\"-axis really is vertical in the sense of the plumbline verticality but for the student the axis may well lie on a horizontal table.\n\nAlthough the word horizontal is commonly used in daily life and language (see below), it is subject to many misconceptions.\n\n\n\n\nIn general, something that is horizontal can be drawn from left to right (or right to left), such as the x-axis in the Cartesian coordinate system.\n\nThe concept of a horizontal plane is thus anything but simple, although, in practice, most of these effects and variations are rather small: they are measurable and can be predicted with great accuracy, but they may not greatly affect our daily life.\n\nThis dichotomy between the apparent simplicity of a concept and an actual complexity of defining (and measuring) it in scientific terms arises from the fact that the typical linear scales and dimensions of relevance in daily life are 3 orders of magnitude (or more) smaller than the size of the Earth. Hence, the world appears to be flat locally, and horizontal planes in nearby locations appear to be parallel. Such statements are nevertheless approximations; whether they are acceptable in any particular context or application depends on the applicable requirements, in particular in terms of accuracy.\n\nIn graphical contexts, such as drawing and drafting and Co-ordinate geometry on rectangular paper, it is very common to associate one of the dimensions of the paper with a horizontal, even though the entire sheet of paper is standing on a flat horizontal (or slanted) table. In this case, the horizontal direction is typically from the left side of the paper to the right side. This is purely conventional (although it is somehow 'natural' when drawing a natural scene as it is seen in reality), and may lead to misunderstandings or misconceptions, especially in an educational context.\n\n\n\n"}
