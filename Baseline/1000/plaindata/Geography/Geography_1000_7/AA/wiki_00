{"id": "6331094", "url": "https://en.wikipedia.org/wiki?curid=6331094", "title": "Atlantic World", "text": "Atlantic World\n\nThe Atlantic World is the history of the interactions among the peoples and empires bordering the Atlantic Ocean rim from the beginning of the Age of Discovery to the early 21st century.\nAtlantic history is split between three different contexts. transatlantic history, meaning the international history of the atlantic world, circus-atlantic history meaning the transnational history of the atlantic world, and cis-atlantic history within an atlantic context.\nThe Atlantic slave trade continued into the 19th century, but the international trade was largely outlawed in 1807 by Britain. Slavery ended in 1865 in the United States and in the 1880s in Brazil (1888) and Cuba (1886). In many ways the history of the \"Atlantic world\" culminates in the \"Atlantic Revolutions\" of the late 18th century and early 19th century.\n\nThe historiography of the Atlantic World, known as Atlantic history, has grown enormously since the 1990s.\n\nThe Atlantic World comprises the histories of Europe, Africa, and the Americas. Travel over land was difficult and expensive, so settlements were made along the coast, especially where rivers allowed small boats to travel inland. Distant settlements were linked by elaborate sea-based trading networks. Since the easiest and cheapest way of long-distance travel was by sea, international trading networks emerged in the Atlantic world, with major hubs at London, Amsterdam, Boston, and Havana. Time was a factor, as sailing ships averaged about 2 knots speed (50 miles a day). Navigators had to rely on maps of currents or they would be becalmed for days or weeks. These maps were not only for navigational purposes however, but also as a way to give insight in regards to power and ownership of lands that had already been claimed, essentially creating a greater desire to finding new routes and land. One major goal for centuries was finding the Northwest Passage (through what is now Canada) from Europe to Asia.\n\nFollowing Columbus and the earliest European voyages to the New World and the west African coast Africa and the division of the Americas between the Spanish Empire and the Portuguese Empire was effected by the Treaty of Tordesillas. The West Coast of Africa played a special role as the source of slave labor. There emerged an elaborate network of economic, geopolitical and cultural exchange—an \"Atlantic World\" comparable to \"Mediterranean World.\" It linked the nations and peoples that inhabited the Atlantic litoral of North and South America, Africa and Western Europe.\n\nThe main empires that built the Atlantic world were the British, French, Spanish\n, Portuguese and Dutch; entrepreneurs from the United States played a role as well after 1789. Other countries, such as Sweden and Denmark, were active on a smaller scale.\n\nThe beginning of extensive contact between Europe, Africa, and the Americas had sweeping implications for the environmental and demographic history of all the regions involved. In the process known as the Columbian exchange, numerous plants, animals, and diseases were transplanted—both deliberately and inadvertently—from one continent to another. The epidemiological impact of this exchange on the indigenous peoples of the Americas was profound, causing very high death rates and population declines of 50% to 90% or even 100%. European and African immigrants also had very high death rates on their arrival, but they could be and were replaced by new shipments of immigrants (see the Population history of American indigenous peoples). Many foods that are common in present-day Europe, including corn (maize) and potatoes, originated in the New World and were unknown in Europe before the sixteenth century. Similarly, some staple crops of present-day West Africa, including cassava and peanuts, originated in the New World. Some of the staple crops of Latin America, such as coffee and sugarcane, were introduced by European settlers in the course of the Columbian Exchange.\n\nThe slave trade played a role in the history of the Atlantic world almost from the beginning. As European powers began to conquer and claim large territories in the Americas in the sixteenth and seventeenth centuries, the role of chattel slavery and other forced labor systems in the development of the Atlantic world expanded. European powers typically had vast territories that they wished to exploit through agriculture, mining, or other extractive industries, but they lacked the work force that they needed to exploit their lands effectively. Consequently, they turned to a variety of coercive labor systems to meet their needs. At first the goal was to use native workers. Native Americans were employed through Indian slavery and through the Spanish system of encomienda. The Indians too often preferred to die of starvation rather than be slaves, so the plantation owners turned to African slaves via the Atlantic slave trade. European workers arrived as indentured servants or transported felons who went free after a term of labor.\n\nThe Trans-Atlantic Slave trade played a massive role in shaping the demographics of the Americas, especially in areas where huge plantations were the norm, such as in South America and the Caribbean. Roughly three quarters of immigrants to the Americas before 1820 were African, and more than half of these Africans were originally from West or Central Africa. In Brazil, the population percentage of Africans was even higher, with about seven African to every one Portuguese immigrant. Because there was such a large population of Africans, it is unsurprising that African slaves aided in shaping the culture of these regions. In the early colonial period, there was a high prevalence of African spiritual practices, such as spirit possessions and healing practices. Presumably, these practices served as a point of connection and as an identity hold for slaves hailing from the same African origin. Such cultural practices allowed, at least to an extent, African slaves to maintain kinship structures similar to those that they might have seen in their homeland. In many cases, European authorities viewed spiritual positions that were highly esteemed in African societies to be socially unacceptable, morally corrupt, and heretical. This led to the disappearance or transformation of most African religious practices. For example, the practice of consulting kilundu, or Angolan spirits, was seen as homosexual by Portuguese authorities, a clear example of Eurocentrism in colonial societies, as European ideas of religion often did not match African ones. Unfortunately, there is a lack of documents written from the African point of view, so almost all information from this time period in these colonial societies is subject to cross-cultural misinterpretation, omission of facts, or other such changes that could affect the quality of description of African spiritual practices. Maintaining the integrity of cultural practices was difficult due to disagreement with European propriety and European tendency to generalize the African demographic makeup to merely “Central African,” rather than acknowledging individual cultures. Eventually, most African traditions such as Kilundu, which was ultimately reduced to the popular Brazilian dance “Lundu,” were either absorbed into other African traditions or reduced to a ritual simply resembling the original tradition.\nThe extent of voluntary immigration to the Atlantic world varied considerably by region, nationality, and time period. Many European nations, particularly the Netherlands and France, only managed to send a few thousand voluntary immigrants. Though 15,000 or so who came to New France multiplied rapidly. In New Netherland, the Dutch coped by recruiting immigrants of other nationalities. In New England, the massive Puritan migration of the first half of the seventeenth century created a large free workforce and thus obviated the need to use unfree labor on a large scale. Colonial New England's reliance on the labor of free men, women, and children, organized in individual farm households, is called the family labor system.\n\nThe French colony of Saint-Domingue was one of the first American jurisdictions to end slavery, in 1794. Brazil was last nation in the Western Hemisphere to end slavery, in 1888.\n\nThe Spanish conquistadores conquered the Aztec empire in present-day Mexico and the Inca empire in present-day Peru with ease, assisted by horses, guns, and above all by the devastating mortality inflicted by newly introduced diseases such as smallpox. To some extent the prior emergence of the Inca and Aztec empires as regional powers aided the transfer of governance to the Spanish, since these native empires had already established road systems, state bureaucracies and systems of taxation and intensive agriculture that were in some cases inherited wholesale by the Spanish. The early Spanish conquerors of these empires were also aided by political instability and internal conflict within the Aztec and Incan regimes, which they successfully exploited to their benefit.\n\nOne of the problems that most European governments faced in the Americas was how to exercise authority over vast expanses of territory. Spain, which colonized Mexico, Central America, and the greater part of South America, established a network of powerful viceroyalties to administer different regions of its New World holdings: the Viceroyalty of New Spain (1535), the Viceroyalty of Peru (1542), the Viceroyalty of New Granada (1717/1739), and the Viceroyalty of Rio de la Plata (1776). The result was strong government that became even stronger during the Bourbon reforms of the 18th century.\n\nBritain approached the task of governing its New World territories in less centralized manner, establishing about twenty distinct colonies in North America and the Caribbean from 1585 onward. Each British colony had its own governor and an assembly. The North American Thirteen Colonies developed a system of home rule and democratic self-government. Usually only property owners could vote but since so many free men owned property a majority could and did vote. It was the British threat against home rule, and its demand for control of taxation, that led to the American Revolution in the 1770s.\n\nA wave of revolutions shook the Atlantic world, 1770s-1820s, including the United States (1775–1783), France and French-controlled Europe (1789–1814), Haiti (1791–1804), and Spanish America (1810–1825). There were smaller upheavals in Switzerland, Russia, and Brazil. The revolutionaries in each country knew of the others and to some degree were inspired or emulated them.\n\nIndependence movements in the New World began with the American Revolution, 1775-1783, in which France, the Netherlands and Spain assisted the new United States of America as it secured independence from Britain. In the 1790s the Haitian Revolution broke out, with large-scale killings. With Spain tied down in European wars, the mainland Spanish colonies secured independence around 1820.\n\nIn long-term perspective, the revolutions were mostly successful. They spread widely the ideals of republicanism, the overthrow of aristocracies, kings and established churches. They emphasized the universal ideals of The Enlightenment, such as the equality of all men. They emphasized equal justice under law by disinterested courts, as opposed to particular justice handed down at the whim of a local noble. They showed that the modern notion of revolution, of starting fresh with a radically new government, could actually work in practice. Revolutionary mentalities were born and continue to flourish to the present day.\n\nHistorian Bernard Bailyn traces the concept of the Atlantic world to an editorial published by journalist Walter Lippmann in 1917. The alliance of the United States and Great Britain in World War II, and the subsequent creation of NATO, heightened historians' interest in the history of interaction between societies on both sides of the Atlantic Ocean.\n\nIn American and British universities, Atlantic World history is supplementing (and possibly supplanting) the study of specific European colonial societies in the Americas, e.g. British North America or Spanish America. Atlantic world history differs from traditional approaches to the history of colonization in its emphasis on inter-regional and international comparisons and its attention to events and trends that transcended national borders. Atlantic world history also emphasizes how the colonization of the Americas reshaped Africa and Europe.\n\n"}
{"id": "510114", "url": "https://en.wikipedia.org/wiki?curid=510114", "title": "Base station", "text": "Base station\n\nBase station (or base radio station) is – according to the International Telecommunication Union's (ITU) Radio Regulations (RR) – a \"land station in the land mobile service.\"\n\nThe term is used in the context of mobile telephony, wireless computer networking and other wireless communications and in land surveying. In surveying, it is a GPS receiver at a known position, while in wireless communications it is a transceiver connecting a number of other devices to one another and/or to a wider area.\nIn mobile telephony, it provides the connection between mobile phones and the wider telephone network. In a computer network, it is a transceiver acting as a switch for computers in the network, possibly connecting them to a/another local area network and/or the Internet. In traditional wireless communications, it can refer to the hub of a dispatch fleet such as a taxi or delivery fleet, the base of a TETRA network as used by government and emergency services or a CB shack.\n\nIn the context of external land surveying, a base station is a GPS receiver at an accurately-known fixed location which is used to derive correction information for nearby portable GPS receivers. This correction data allows propagation and other effects to be corrected out of the position data obtained by the mobile stations, which gives greatly increased location precision and accuracy over the results obtained by uncorrected GPS receivers.\n\nIn the area of wireless computer networking, a base station is a radio receiver/transmitter that serves as the hub of the local wireless network, and may also be the gateway between a wired network and the wireless network. It typically consists of a low-power transmitter and wireless router.\n\nIn radio communications, a base station is a wireless communications station installed at a fixed location and used to communicate as part of one of the following:\n\n\nIn professional two-way radio systems, a base station is used to maintain contact with a dispatch fleet of hand-held or mobile radios, and/or to activate one-way paging receivers. The base station is one end of a communications link. The other end is a movable vehicle-mounted radio or walkie-talkie. Examples of base station uses in two-way radio include the dispatch of tow trucks and taxicabs.\n\nProfessional base station radios are often one channel. In lightly used base stations, a multi-channel unit may be employed. In heavily used systems, the capability for additional channels, where needed, is accomplished by installing an additional base station for each channel. Each base station appears as a single channel on the dispatch center control console. In a properly designed dispatch center with several staff members, this allows each dispatcher to communicate simultaneously, independently of one another, on a different channel as necessary. For example, a taxi company dispatch center may have one base station on a high-rise building in Boston and another on a different channel in Providence. Each taxi dispatcher could communicate with taxis in either Boston or Providence by selecting the respective base station on his or her console.\n\nIn dispatching centers it is common for eight or more radio base stations to be connected to a single dispatching console. Dispatching personnel can tell which channel a message is being received on by a combination of local protocol, unit identifiers, volume settings, and busy indicator lights. A typical console has two speakers identified as \"select\" and \"unselect\". Audio from a primary selected channel is routed to the select speaker and to a headset. Each channel has a busy light which flashes when someone talks on the associated channel.\n\nBase stations can be local controlled or remote controlled. Local controlled base stations are operated by front panel controls on the base station cabinet. Remote control base stations can be operated over tone- or DC-remote circuits. The dispatch point console and remote base station are connected by leased private line telephone circuits, (sometimes called \"RTO circuit\"s), a DS-1, or radio links. The consoles multiplex transmit commands onto remote control circuits. Some system configurations require duplex, or four wire, audio paths from the base station to the console. Others require only a two-wire or half duplex link.\n\n\nBase stations are sometimes called \"control\" or \"fixed\" stations in US Federal Communications Commission licensing. These terms are defined in regulations inside Part 90 of the commissions regulations. In US licensing jargon, types of base stations include:\n\nIn amateur radio, a base station also communicates with mobile rigs but for hobby or family communications. Amateur systems sometimes serve as dispatch radio systems during disasters, search and rescue mobilizations, or other emergencies.\n\nAn Australian UHF CB base station is another example of part of a system used for hobby or family communications.\n\nWireless telephone differ from two-way radios in that:\n\n\nA wireless telephone base station communicates with a mobile or hand-held phone. For example, in a wireless telephone system, the signals from one or more mobile telephones in an area are received at a nearby base station, which then connects the call to the land-line network. Other equipment is involved depending on the system architecture. Mobile telephone provider networks, such as European GSM networks, may involve carrier, microwave radio, and switching facilities to connect the call. In the case of a portable phone such as a US cordless phone, the connection is directly connected to a wired land line.\n\nWhile low levels of radio-frequency power are usually considered to have negligible effects on health, national and local regulations restrict the design of base stations to limit exposure to electromagnetic fields. Technical measures to limit exposure include restricting the radio frequency power emitted by the station, elevating the antenna above ground level, changes to the antenna pattern, and barriers to foot or road traffic. For typical base stations, significant electromagnetic energy is only emitted at the antenna, not along the length of the antenna tower.\n\nBecause mobile phones and their base stations are two-way radios, they produce radio-frequency (RF) radiation in order to communicate, exposing people near them to RF radiation giving concerns about mobile phone radiation and health. Hand-held mobile telephones are relatively low power so the RF radiation exposures from them are generally low.\n\nThe World Health Organization has concluded that \"there is no convincing scientific evidence that the weak RF signals from base stations and wireless networks cause adverse health effects.\"\n\nThe consensus of the scientific community is that the power from these mobile phone base station antennas is too low to produce health hazards as long as people are kept away from direct access to the antennas. However, current international exposure guidelines (ICNIRP) are based largely on the \"thermal\" effects of base station emissions, NOT considering the \"non-thermal\" effects harmless.\n\nFuel cell backup power systems are added to critical base stations or cell sites to provide emergency power.\n\n\n"}
{"id": "8440529", "url": "https://en.wikipedia.org/wiki?curid=8440529", "title": "Border irregularities of the United States", "text": "Border irregularities of the United States\n\nBorder irregularities of the United States, particularly panhandles and highway incursions into other jurisdictions, are shown here. Often they are a result of borders which do not conform to geological features.\n\nThere are several exclaves between the United States and Canada, including the entire state of Alaska (though the state can still be accessed by sea from the United States). Other exclaves include Akwesasne, the Northwest Angle, and Point Roberts. \n\nThe status of the waters around Nunez Rocks is disputed. Nunez Rocks is a low-tide elevation (\"bare at half-tide\") area (LTE) that is south of a line known as the \"A-B\" Line, which was defined in a 1903 arbitration decision on the Alaska–Canada boundary. The court specified the initial boundary point (Point \"A\") at the northern end of Dixon Entrance and Point \"B\" to the east. Canada relies on the \"A-B\" Line as rendering nearly all of Dixon Entrance as Canadian internal waters. The U.S. does not recognize the \"A-B\" Line as an official boundary, instead regarding it as allocating sovereignty over the land masses within the Dixon Entrance, with Canada's land south of the line. The U.S. regards the waters as subject to international marine law, and in 1977 it defined an equidistant territorial sea throughout Dixon Entrance. This territory, which surrounds Nunez Rocks, extends south of the \"A-B\" line for the most part. The United States has not ratified the Law of the Sea Treaty, although it adheres to most of its principles as customary international law. Under the treaty, LTEs may be used as basepoints for a territorial sea, and the U.S. uses Nunez Rocks as a basepoint. As a non-signatory, however, there is nothing preventing the U.S. from claiming areas beyond the scope of the Law of the Sea Treaty. The fact remains that, for about half of each day, above-water territory that Canada regards as Canadian is surrounded by sea territory that the U.S. has declared to be American.\n\nAnother disputed area is the Grey Zone, including Machias Seal Island, in the Gulf of Maine.\n\nThe Aroostook Valley Country Club is a golf course which straddles the Canada–US border, between the U.S. state of Maine and the Canadian province of New Brunswick. The club, located near Perth-Andover, New Brunswick and Fort Fairfield, Maine, has its course (except part of the tee area for the ninth hole, and possibly part of a sand trap on the first hole) and clubhouse on the Canadian side of the border and its parking lot and pro shop on the American side. \n\nEast Richford Slide Road in the US state of Vermont crosses in to the Canadian province of Québec for a distance of approximately 100 meters (300 feet) before returning to the United States.\n\nIn Texas and Mexico, shifts in the course of the lower Rio Grande have created numerous pene-exclaves. Under the Boundary Treaty of 1970 and earlier treaties, the United States and Mexico have maintained the actual course of the river as the international boundary, but both must approve proposed changes. From 1989 to 2009, there were 128 locations where the river changed course, causing land that had been on one side of the river to then occupy the opposite bank. Until the boundary is officially changed, there are 60 small pene-exclaves of the state of Texas now lying on the southern side of the river, as well as 68 such pene-exclaves of Mexico on the northern side of the river.\n\nThe legal status of the US-Russian border is unclear. The United States Senate ratified a treaty setting the boundary with the Soviet Union in 1991. However, shortly after, the Soviet Union collapsed, and the Russian parliament never voted on the treaty.\n\nThe boundary between the United States and Cuba is a maritime boundary along the Straits of Florida. Despite tense relations between the countries, the United States operates a military base in Cuba, Guantanamo Bay. The land on which this military base sits is not sovereign United States territory; rather, it is leased from the Cuban Government.\n\n\nThese border irregularities were caused by changes in the Mississippi River during the 1812 New Madrid earthquake or other river changes:\n\n\nOther irregularities involving the Mississippi River: \n\n\nSome highways are not connected to the rest of their nominal highway systems:\n"}
{"id": "54094680", "url": "https://en.wikipedia.org/wiki?curid=54094680", "title": "Burg Schönburg", "text": "Burg Schönburg\n\nSchönburg Castle is in the municipality of Schönburg 4.5 kilometers east of Naumburg at the heart of Federal Republic of Germany in the State of Saxony-Anhalt. It has been proposed by Germany for inscription in the List of World Heritage. \n\nThe Late Romanesque ruins of Schönburg Castle are on top of a mottled sandstone cliff by the Saale River, which is 40 metres high and declines sharply in the west. The castle was built with the local mottled sandstone.\n\nThe castle on top of the southern slopes of the Saale was erected as the most important castle in the 12th century on commission by the bishops of Naumburg. The bishops owned several castles within their territory to secure their assets. From 1158 at the latest, a free noble family that owned property around Naumburg named itself after the castle. It was first documented in 1137. Schönburg has been preserved to this day and is an example from the time of classical castle construction.\n\nAll the walls, gates and parts of the Palas have retained their original substance from the High Middle Ages, as have parts of the interior like visible chimneys dated to 1220 and decorated windows. The architecture and decorations found here correspond to other monuments in the World Heritage nomination. \n\nSchönburg Castle is one of the eleven components of the cultural landscape “Naumburg Cathedral and the High Medieval Cultural Landscape of the Rivers Saale and Unstrut”. As a landmark the castle is an important part of the lines of sight connecting the cultural landscape as a whole. \nThe World Heritage nomination is representative for the processes that shaped the continent during the High Middle Ages between 1000 and 1300: Christianization, the so-called “Landesausbau” and the dynamics of cultural exchange and transfer characteristic for this very period.\n\n\n"}
{"id": "88284", "url": "https://en.wikipedia.org/wiki?curid=88284", "title": "Challenger expedition", "text": "Challenger expedition\n\nThe \"Challenger\" expedition of 1872–76 was a scientific exercise that made many discoveries to lay the foundation of oceanography. The expedition was named after the mother vessel, HMS \"Challenger\".\n\nPrompted by Charles Wyville Thomson—of the University of Edinburgh and Merchiston Castle School—the Royal Society of London obtained the use of \"Challenger\" from the Royal Navy and in 1872 modified the ship for scientific tasks, equipping her with separate laboratories for natural history and chemistry. The expedition, led by Captain George Nares, sailed from Portsmouth, England, on 21 December 1872. Other naval officers included Commander John Maclear. Under the scientific supervision of Thomson himself, she traveled nearly surveying and exploring. The result was the \"Report Of The Scientific Results of the Exploring Voyage of H.M.S. Challenger during the years 1873-76\" which, among many other discoveries, cataloged over 4,000 previously unknown species. John Murray, who supervised the publication, described the report as \"the greatest advance in the knowledge of our planet since the celebrated discoveries of the fifteenth and sixteenth centuries\". \"Challenger\" sailed close to Antarctica, but not within sight of it.\n\nTo enable her to probe the depths, 15 of \"Challenger's\" 17 guns were removed and her spars reduced to make more space available. Laboratories, extra cabins and a special dredging platform were installed. \"Challenger\" used mainly sail power during the expedition; the steam engine was used only for powering the dredge. She was loaded with specimen jars, filled with alcohol for preservation of samples, microscopes and chemical apparatus, trawls and dredges, thermometers, barometers, water sampling bottles, sounding leads, devices to collect sediment from the sea bed and great lengths of rope with which to suspend the equipment into the ocean depths. Because of the novelty of the expedition, some of the equipment was invented or specially modified for the occasion. In all, she was supplied with 181 miles (291 km) of Italian hemp for sounding.\n\nOn her journey circumnavigating the globe, 492 deep sea soundings, 133 bottom dredges, 151 open water trawls and 263 serial water temperature observations were taken. Also about 4,700 new species of marine life were discovered.\n\nThe scientific work was conducted by Wyville Thomson, John Murray, John Young Buchanan, Henry Nottidge Moseley, Alphonse François Renard and Rudolf von Willemoes-Suhm. Frank Evers Bed was appointed prosector. The official expedition artist was John James Wild. As well as Nares and Maclear, others that were part of the naval crew included Pelham Aldrich, Lord George Granville Campbell, and Andrew Francis Balfour (one of the sons of Scottish botanist John Hutton Balfour). Also among the officers was Thomas Henry Tizard, who had carried out important hydrographic observations on previous voyages. Though he was not among the civilian scientific staff, Tizard would later help write the official account of the expedition, and also become a Fellow of the Royal Society.\n\nThe original ship's complement included 21 officers and around 216 crew members. By the end of the voyage, this had reduced to 144 due to deaths, desertions, being left ashore due to illness, and planned departures.\n\n\"Challenger\" reached Hong Kong in December 1874, at which point Nares and Aldrich left the ship to take part in the British Arctic Expedition. The new captain was Frank Tourle Thomson. The second-in-command, and the most senior officer present throughout the entire expedition, was Commander John Maclear. Willemoes-Suhm died and was buried at sea on the voyage to Tahiti. Lord Campbell and Balfour left the ship in Valparaiso, Chile, after being promoted.\nThe first leg of the expedition took the ship from Portsmouth (December 1872) south to Lisbon (January 1873) and then on to Gibraltar. The next stops were Madeira and the Canary Islands (both February 1873). The period from February to July 1873 was spent crossing the Atlantic westwards from the Canary Islands to the Virgin Islands, then heading north to Bermuda, east to the Azores, back to Madeira, and then south to the Cape Verde Islands. During this period, there was a detour in April and May 1873, sailing from Bermuda north to Halifax and back, crossing the Gulf Stream twice with the reverse journey crossing further to the east.\n\nAfter leaving the Cape Verde Islands in August 1873, the expedition initially sailed south-east and then headed west to reach St Paul's Rocks. From here, the route went south across the equator to Fernando de Noronha during September 1873, and onwards that same month to Bahia (now called Salvador) in Brazil. The period from September to October 1873 was spent crossing the Atlantic from Bahia to the Cape of Good Hope, touching at Tristan da Cunha on the way.\n\nDecember 1873 to February 1874 was spent sailing on a roughly south-eastern track from the Cape of Good Hope to the parallel of 60 degrees south. The islands visited during this period were the Prince Edward Islands, the Crozet Islands, the Kerguelen Islands, and Heard Island. February 1874 was spent travelling south and then generally eastwards in the vicinity of the Antarctic Circle, with sightings of icebergs, pack ice and whales. The route then took the ship north-eastward and away from the ice regions in March 1874, with the expedition reaching Melbourne in Australia later that month. The journey eastward along the coast from Melbourne to Sydney took place in April 1874, passing by Wilsons Promontory and Cape Howe.\n\nWhen the voyage resumed in June 1874, the route went east from Sydney to Wellington in New Zealand, followed by a large loop north into the Pacific calling at Tonga and Fiji, and then back westward to Cape York in Australia by the end of August. The ship arrived in New Zealand in late June and left in early July. Before reaching Wellington (on New Zealand's North Island), brief stops were made at Port Hardy (on d'Urville Island) and Queen Charlotte Sound (on New Zealand's South Island) and \"Challenger\" passed through the Cook Strait to reach Wellington. The route from Wellington to Tonga went along the east coast of New Zealand's North Island, and then north and east into the open Pacific, passing by the Kermadec Islands en route to Tongatabu, the main island of the Tonga archipelago (then known as the Friendly Islands). The waters around the Fijian islands, a short distance to the north-west of Tonga, were surveyed during late July and early August 1874. The ship's course was then set westward, reaching Raine Island (on the outer edge of the Great Barrier Reef) at the end of August and thence arriving at Cape York, at the tip of Australia's Cape York Peninsula.\n\nOver the following three months (September to November 1874), the expedition visited several islands and island groups while sailing from Cape York to China and Hong Kong (then a British colony). The first part of the route passed north and west over the Arafura Sea, with New Guinea to the north-east and the Australian mainland to the south-west. The first islands visited were the Aru Islands, followed by the nearby Kai Islands. The ship then crossed the Banda Sea touching at the Banda Islands, to reach Amboina (Ambon Island) in October 1874, and then continuing to Ternate Island. All these islands are now part of modern-day Indonesia. From Ternate, the route went north-westward towards the Philippines, passing east of Celebes (Sulawesi) into the Celebes Sea. The expedition called at Samboangan (Zamboanga) on Mindanao, and then Iloilo on the island of Panay, before navigating within the interior of the archipelago en route to the bay and harbour of Manila on the island of Luzon. The crossing north-westward from Manila to Hong Kong took place in November 1874.\n\nAfter several weeks in Hong Kong, the expedition departed in early January 1875 to retrace their route south-east towards New Guinea. The first stop on this outward leg of the journey was Manila. From there, they continued on to Samboangan, but took a different route through the interior of the Philippines, this time touching at the island of Zebu (Cebu). From Samboangan the ship diverged from the inward route, this time passing south of Mindanao (in early February 1875). \"Challenger\" then headed east into the open sea, before turning to the south-east and making landfall at Humboldt Bay (now Yos Sudarso Bay) on the north coast of New Guinea. By March 1875, the expedition had reached the Admiralty Islands north-east of New Guinea. The final stage of the voyage on this side of the Pacific was a long journey across the open ocean to the north, passing mostly west of the Carolina Islands and the Mariana Islands, reaching port in Yokohama, Japan, in April 1875.\n\"Challenger\" departed Japan in mid-June 1875, heading east across the Pacific to a point due north of the Sandwich Islands (Hawaii), and then turning south, making landfall at the end of July at Honolulu on the Hawaiian island of Oahu. A couple of weeks later, in mid-August, the ship departed south-eastward, anchoring at Hilo Bay off Hawaii's Big Island, before continuing to the south and reaching Tahiti in mid-September. The expedition left Tahiti in early October, swinging to the west and south of the Tubuai Islands and then heading to the south-east before turning east towards the South American coast. The route touched at the Juan Fernández Islands in mid-November 1875, with \"Challenger\" reaching the port of Valparaiso in Chile a few days later. The next stage of the journey commenced the following month, with the route taking the ship south-westward back out into the Pacific, past the Juan Fernández Islands, before turning to the south-east and back towards South America, reaching Port Otway in the Gulf of Penas on 31 December 1875.\n\nMost of January 1876 was spent navigating around the southern tip of South America, surveying and touching at many of the bays and islands of the Patagonian archipelago, the Strait of Magellan, and Tierra del Fuego. Locations visited here include Hale Cove, Gray Harbour, Port Grappler, Tom Bay (all in the vicinity of Wellington Island), Puerta Bueno (near Hanover Island), Isthmus Bay (near the Queen Adelaide Archipelago), and Port Churruca (near Santa Ines Island). The final stops, before heading out into the Atlantic, were Port Famine, Sandy Point, and Elizabeth Island. \"Challenger\" reached the Falkland Islands towards the end of January, calling at Port Stanley and then continuing northward, reaching Montevideo in Uruguay in mid-February 1876. The ship left Montevideo at the end of February, heading first due east and then due north, arriving at Ascension Island at the end of March 1876. The period from early to mid-April was spent sailing from Ascension Island to the Cape Verde Islands (visited almost three years ago on the outward journey). From here, the route taken in late April and early May 1876 was a westward loop to the north out into the mid-Atlantic, eventually turning due east towards Europe to touch land at Vigo in Spain towards the end of May. The final stage of the voyage took the ship and its crew north-eastward from Vigo, skirting the Bay of Biscay to make landfall in England.\n\n\"Challenger\" returned to Spithead, Hampshire, on 24 May 1876, having spent 713 days at sea out of the intervening 1,250. The complete set of reports of the \"Challenger\" expedition, published in 50 volumes between 1877 and 1895, are available online.\n\nThe Royal Society stated that the voyage's scientific goals were:\n\nTo investigate the physical conditions of the deep sea in the great ocean basins (as far as the neighborhood of the Great Southern Ice Barrier) in regard to depth, temperature, circulation, specific gravity and penetration of light.\n\nTo determine the chemical composition of seawater at various depths from the surface to the bottom, the organic matter in solution and the particles in suspension.\n\nTo ascertain the physical and chemical character of deep-sea deposits and the sources of these deposits.\n\nTo investigate the distribution of organic life at different depths and on the deep seafloor.\n\nAt each of the 360 stations the crew measured the bottom depth, temperature at different depths, observed weather and surface ocean conditions, and collected seafloor, water, and biota samples.\n\n\"Challenger's\" crew used methods that were developed in prior small-scale expeditions to make observations. To measure depth, \"Challenger's\" crew would lower a line with a weight attached to it until it reached the sea floor. The line was marked in 25 fathom intervals with flags denoting depth. Because of this, the depth measurements from \"Challenger\" were at best accurate to 25 fathoms, or approximately 45m. The sinker often had a small container attached to it that would allow for the collection of bottom sediment samples.\nThe crew used a variety of dredges and trawls to collect biological samples. The dredges consisted of metal nets attached to a wooden plank and dragged across the sea floor. Mop heads attached to the wooden plank would sweep across the sea floor and release organisms from the ocean bottom to be caught in the nets. Trawls were large metal nets towed behind the ship to collect organisms at different depths of water. Upon the retrieval of a dredge or trawl, \"Challenger\" crew would sort, rinse, and store the specimens for examination upon return. The specimens were often preserved in either brine or alcohol.\n\nThe primary thermometer used throughout the \"Challenger\" expedition was the Miller-Casella thermometer, which contained two markers within a curved mercury tube to record the maximum and minimum temperature through which the instrument traveled. Several of these thermometers would be lowered at various depths for recording. However, this design assumed that the water closer to the surface of the ocean was always warmer than that below. During the voyage, \"Challenger\" crew tested the reversing thermometer, which could measure temperature at specified depths. Afterwards, this type of thermometer was used extensively until the second half of the 20th century.\n\nOn March 23, 1875, at sample station number 225 located in the southwest Pacific Ocean between Guam and Palau, the crew recorded a sounding of 4,475 fathoms, (8,184 meters) deep, which was confirmed by an additional sounding. As shown by later expeditions using modern equipment, this area represents the southern end of the Mariana trench and is one of the deepest known places on the ocean floor. Modern soundings to 10,994 meters have since been found near the site of the \"Challenger\" original sounding. \"Challenger\" discovery of this depth was a key finding of the expedition in broadening oceanographic horizons on the ocean's depth and extent and now bears the vessel's name, the Challenger Deep.\n\nFindings from the \"Challenger\" expedition continued to be published until 1895, 19 years after the completion of its journey. The report contained 50 volumes and was over 29,500 pages in length. Specimens brought back by \"Challenger\" were distributed to the world's foremost experts for examination, which greatly increased the expenses and time required to finalize the report. The report and specimens are currently held at the British Natural History Museum and the report has been made available online. Some specimens, many of which were the first discovered of their kind, are still examined by scientists today.\n\nA large number of scientists worked on categorising the material brought back from the expedition including the palaeontologist Gabriel Warton Lee.\n\nGeorge Albert Boulenger, herpetologist at the British Museum (Natural History), named a species of lizard, \"Saproscincus challengeri\", after HMS \"Challenger\".\n\nAs the first true oceanographic cruise, the \"Challenger\" expedition laid the groundwork for an entire academic and research discipline. The name of \"Challenger\" was applied to such varied phenomena as the Challenger Society for Marine Science, the oceanographic and marine geological survey ship \"Glomar Challenger\", and the Space Shuttle \"Challenger\".\n\n\n\n\n"}
{"id": "13854919", "url": "https://en.wikipedia.org/wiki?curid=13854919", "title": "Deformation monitoring", "text": "Deformation monitoring\n\nDeformation monitoring (also referred to as deformation survey) is the systematic measurement and tracking of the alteration in the shape or dimensions of an object as a result of stresses induced by applied loads. Deformation monitoring is a major component of logging measured values that may be used to for further computation, deformation analysis, predictive maintenance and alarming.\n\nDeformation monitoring is primarily related to the field of applied surveying, but may be also related to civil engineering, mechanical engineering, construction, and geology. The measuring devices used for deformation monitoring depend on the application, the chosen method, and the preferred measurement interval.\n\nMeasuring devices (or sensors) can be sorted in two main groups, geodetic and geotechnical sensors. Both measuring devices can be seamlessly combined in modern deformation monitoring. \n\nDeformation monitoring can be required for the following applications:\n\nDeformation monitoring can be manual or automatic. Manual deformation monitoring is the operation of sensors or instruments by hand or manual downloading of collected data from deformation monitoring instruments such as ShapeAccelArray. Automatic deformation monitoring operation of a group of software and hardware elements for deformation monitoring that, once set up, does not require human input to function.\n\nNote that deformation analysis and interpretation of the data collected by the monitoring system is not included in this definition.\n\nAutomated deformation monitoring requires instruments to communicate with a base station. Communication methods used include:\n\nThe monitoring regularity and time interval of the measurements must be considered depending on the application and object to be monitored. Objects can undergo both rapid, high frequency movement and slow, gradual movement. For example, a bridge might oscillates with a period of a few seconds due to the influence of traffic and wind and also be shifting gradually due to tectonic changes.\n\n\nDeformation analysis is concerned with determining if a measured displacement is significant enough to warrant a response. Deformation data must be checked for statistical significance, and then checked against specified limits, and reviewed to see if movements below specified limits imply potential risks.\n\nThe software acquires data from sensors, computes meaningful values from the measurements, records results, and can notify responsible persons should threshold value be exceeded. However, a human operator must make considered decisions on the appropriate response to the movement, e.g. independent verification though on-site inspections, re-active controls such as structural repairs and emergency responses such as shut down processes, containment processes and site evacuation.\n\n\n"}
{"id": "4876592", "url": "https://en.wikipedia.org/wiki?curid=4876592", "title": "Diver navigation", "text": "Diver navigation\n\nDiver navigation, termed \"underwater navigation\" by scuba divers, is a set of techniques—including observing natural features, the use of a compass, and surface observations—that divers use to navigate underwater. Free-divers do not spend enough time underwater for navigation to be important, and surface supplied divers are limited in the distance they can travel by the length of their umbilicals and are usually directed from the surface control point. On those occasions when they need to navigate they can use the same methods used by scuba divers.\n\nAlthough it is considered a basic skill, it is normally only taught to a limited degree as part of basic Open Water certification. Most North American diver training agencies only teach significant elements of underwater navigation as part of the Advanced Open Water Diver certification program.\n\nUnderwater navigation is usually a core component of most, if not all, advanced recreational diver training. In the PADI Advanced Open Water Diver course, it is one of the two mandatory skills (together with Deep diving) which must be taken alongside three elective skills.\n\nTraining agencies promote underwater navigation as a skill (despite the fact that it is less popular than other recreational diving specialties) on the basis that it:\n\nUnderwater compass navigation is a component of the scuba-based underwater sport, underwater orienteering.\n\nUnderwater navigation in recreational diving is broadly split into two categories. \"Natural navigation\" techniques, and \"orienteering\", which is navigation focused upon the use of an underwater magnetic compass.\n\nNatural navigation, sometimes known as pilotage, involves orienting by naturally observable phenomena, such as sunlight, water movement, bottom composition (for example, sand ripples run parallel to the direction of the wave front, which tends to run parallel to the shore), bottom contour and noise. Although natural navigation is taught on courses, developing the skills is generally more a matter of experience.\n\nOrienteering, or compass navigation, is a matter of training, practice and familiarity with the use of underwater compasses, combined with various techniques for reckoning distance underwater, including kick cycles (one complete upward and downward sweep of a kick), time, air consumption and occasionally by actual measurement. Kick cycles depend on the diver's finning technique and equipment, but are generally more reliable than time, which is critically dependent on speed, or air consumption, which is critically dependent on depth, work rate, diver fitness, and equipment drag. Techniques for direct measurement also vary, from the use of calibrated distance lines or surveyor's tape measures, to a mechanism like an impeller log, to pacing off the distance along the bottom with the arms.\n\nMany skilled underwater navigators use techniques from both of these categories in a seamless combination, using the compass to navigate between landmarks over longer distances and in poor visibility, while making use of the generic oceanographic indicators to help stay on course and as a check that there is no mistake with the bearing, and then recognising landmarks and using them with the remembered topography of a familiar site to confirm position.\n\nRecognisable topographical features may be remembered or noted and used identify position and direction. This is particularly useful if the visibility is sufficient to see the next landmark on the route before leaving the last. Landmarks are ordinarily considered permanent or semi-permanent features, such as ridges, boulders, wrecks or clumps of weed, but use can also be made of temporary marks such as anchor cables, shot lines, jackstays and guide lines.\n\nThe slope of the bottom is often a reliable indicator of the direction toward the shore, particularly when the bottom is of soft or loose material, and is not broken up greatly by rocky outcrops. This information can be checked for reliability on a sufficiently detailed chart of the area. Contours of depth running roughly parallel to the coastline indicate a slope dipping directly away from the shore, and can be used to maintain a sense of distance and orientation relative to the shore. In some places where the bottom is composed of predominantly rocky outcrops the slope may be in any direction and is not a reliable indicator of direction.\n\nIf circumstances of depth and water clarity allow the position of the sun to produce sufficient variation in brightness, this may indicate the direction of the sun, and be used as a cue to orientation. The effect is greater if the sun is relatively low in the sky, the water is clean, the depth fairly shallow and the surface fairly smooth.\n\nIn some circumstances the diver can look up at the surface, to see in which direction the land lies. These cues will not give any precise information about position, but will allow the diver to keep a mental picture of where he or she is and is going.\n\nCurrent direction can be useful as an orientation cue as long as the direction of the current is known. In rivers it tends to be fairly consistent and reliable, though localised eddies may occur. In the sea it may depend on weather conditions and local topography, as well as the state of the tide. In estuaries and harbours the currents will usually be predominantly tidal, so the state of the tide must be known, as the difference in direction between ebb and flow is usually about 180°.\n\nWave surge direction is essentially the same as wave direction, but may be felt at depths where the wave direction is no longer visible. It is useful if the offshore wave direction relative to the shore is known and does not change appreciably during the dive. In shallow water the wave crests will often be parallel to the shore. The important difference is that waves can be seen to travel in a definite direction, whereas surge is a back and forth motion, allowing a possible 180° error.\n\nA regular and distinct ripple pattern on a sand, mud or gravel bottom is an indication that it has been affected by wave action. The surge of the wave at depth causes the particles to be moved backwards and forwards in the direction the waves are travelling. This movement produces a ripple pattern on the bottom which is an indication of the wave direction on the surface. The ripple crests will be approximately parallel to the crests of the waves that formed them. It is however possible for the surface waves to change direction, and due to shorter wavelength, not reach the bottom to change the ripple pattern. When this is the case there will be no surge at the bottom. If there is a surge at the bottom, and the ripple crests are perpendicular the direction of the surge, then the wave crests will be parallel to the ripple crests. Ripple crests, like surge, may be interpreted with a 180° error.\n\nMany rock formations have characteristic angles known as dip and strike. Dip is the slope of the strata from the horizontal, and strike is the general direction of the strata in the horizontal plane (very roughly). These characteristics will usually be similar in the rocks above and below the water in a locality, so they can be used to estimate direction. Ridges above and below water are often parallel, and gullies and valleys may well extend under water for considerable distances.\n\nDifferent areas may for a wide variety of reasons have different ecologies. A diver who is familiar with an area can use the diversity variations and patterns to provide orientation cues.\n\nThere is often variation of ecological zoning with depth, but a diver is expected to be aware of the depth all the time anyway. In some places the seaward side of big rocks may have different species from the shoreward face because of the greater exposure to wave action.\n\nSea fans and sponges are filter feeders, and may grow into a fan shape at right angles to the usual current or surge direction, to get the maximum volume of water flowing past them.\n\nThe magnetic compass indicates the local direction of the ambient magnetic field, which is usually that of the Earth. This is usually a reliable and consistent feature and is very useful as a navigational aid as it is not affected by visibility, pressure, or the presence of water.\n\nAn important concept is that the compass card should not turn, even though it appears to always “swing” to magnetic north. The housing that holds the compass card turns around the card, which remains pointing in the same direction (Magnetic North) all the time. There are occasions when the card does turn, but this is when it has been stuck or the compass is turned over, and the card is unable to remain aligned with the magnetic field.\n\nTrue north is the geometrically accurate direction along the surface of the earth toward the North pole of the planet’s axis of rotation.\nThe lines of longitude on maps are in true North/South directions.\n\nThe Earth has a magnetic field which is not quite in line with the geographic directions. The difference between the magnetic and true directions is known as Variation. It differs from place to place and changes with time. Large scale charts and maps will usually include a compass rose showing variation.\n\nThe compass will indicate the magnetic field direction at the place where it happens to be at the time. If there are influences other than the Earth’s magnetic field, these may change the direction indicated by the compass. These effects are called deviation, and can be caused by a whole range of things. Any magnetic object or electrical current will have an influence, some more than others. The current in a dive computer is too small to affect the compass, even when quite near, but the hull of a ship or overhead power lines may make a difference even several meters away. It is difficult and often impossible to correct for all possible deviations, but it is worth checking a dive compass for deviation caused by dive equipment. It has been known for regulators to cause deviation, steel cylinders can cause deviation, and powerful lights may be a problem. A diver propulsion vehicle with an electric motor is also a potential problem for those who use them, though divers have been known to navogate adequately using compasses mounted on the handgrip of a DPV. A magnetic clip used to secure equipment to the diver's harness has a powerful magnet in both parts, and should not be used to hold the compass, as the part attached to the compass will produce a serious error.\n\nDeviation may be checked by comparing the compass bearing as measured with a known magnetic bearing measured by a compass with no deviation. Deviation may vary with different directions and for accurate work it is necessary to make up a table of deviations. This is done for ships, but for diving it is generally not worth the trouble. Bearings of one diver's compass may vary from those of another diver even if they have both been read correctly. The difference should not be large, but it can result in being off course and not finding something. A compass is a magnet, and will affect another compass nearby, so they can not be checked by putting them together.\n\nThe magnetic field of the earth is tilted from the horizontal. The angle is called dip and varies with place, so compasses can be corrected for different zones. This is a factory process. A compass made for the northern parts of the northern hemisphere will tilt badly in the southern hemisphere, in some cases to the extent that it will jam if held horizontal.\n\nAlso known as cave lines, distance lines, penetration lines and jackstays. These are permanent or temporary lines laid by divers to mark a route, particularly in caves, wrecks and other areas where the way out from an overhead environment may not be obvious. Guidelines are also useful in the event of silt out.\n\nDistance lines are wound on to a spool or a reel. The length of the distance line used is dependent on the plan for the dive. An open water diver using the distance line only for a surface marker buoy may only need 50 metres / 165 feet, whereas a cave diver may use multiple reels of lengths from 50 ft (15 m) to 1000+ ft (300 m).\n\nReels for distance lines may have a locking mechanism, ratchet or adjustable drag to control deployment of the line and a winding handle to help keep slack line under control and rewind line. Lines are used in open water to deploy surface marker buoys and decompression buoys and link the buoy on the surface to the submerged diver, or may be used to allow easy return navigation to a point such as a shotline or boat anchor.\n\nThe material used for any given distance line will vary based on intended use, nylon being the material of choice for cave diving. A common line used is 2 mm (0.08 inch) polypropylene line when it does not matter if the line is buoyant.\n\nThe use of guideline for navigation requires careful attention to laying and securing the line, line following, marking, referencing, positioning, teamwork, and communication.\n\nIn cave (and occasionally wreck) diving, line markers are used for orientation as a visual and tactile reference on a permanent guideline. Directional markers (commonly arrows), are also known as line arrows or Dorff arrows, and point the way to an exit. Line arrows may mark the location of a \"jump\" location in a cave when two are placed adjacent to each other. Two adjacent arrows facing away from each other, mark a point in the cave where the diver is equidistant from two exits. Arrow direction can be identified by feel in low visibility. Non-directional markers (\"cookies\") are purely personal markers that mark specific spots, or the direction of one's chosen exit at line intersections where there are options. Their shape does not provide a tactile indication of direction as this could cause confusion in low visibility. One important reason to be adequately trained before cave diving is that incorrect marking can confuse and fatally endanger not only oneself, but also other divers.\n\nIn some circumstances divers may be directed by their surface control personnel. This requires a method of communication between the surface team and the diver.\nBoth voice communications and line signals may be used to direct the movement of the diver and to provide other information.\nSurface direction may be used in scuba diving when diving under ice or conducting an underwater search, and in surface supplied diving for both these purposes and at any other time that it is useful or convenient for the dive controller to direct the movement of the diver. Surface direction is most useful when the surface personnel have a better idea of where the diver is relative to where he or she needs to be than the diver has, which can happen when the visibility is poor, or when the diver is following a search pattern controlled by the surface controller.\n\nSurface applications for compass navigation include marking a position and finding the position using compass bearings. At least two position lines are required to fix a position, as only direction can be found using a compass. When two bearings are used a large angle between the bearings will minimize error. The angle should preferably be between 60 and 120 degrees, and near 90 degrees would be ideal. Three bearings are better as they will also give an indication of probable accuracy when plotted on a chart. The \"cocked hat\" or triangle where the lines intersect, shows the probable location of the position measured, and a small triangle indicates a small probable error. The angle between the three bearings should preferably be in the order of 60 or 120 degrees where available landmarks allow. In all cases landmarks should be as close to the diver as possible and spread over a large arc for best accuracy.\n\nVarious pieces of equipment are available to assist divers navigating underwater.\nPeriodically reports are issued suggesting the development of underwater GPS technology, but no system is currently available on market. It is generally thought that the difficulty of locating satellite by signals from underwater at present is not capable of being overcome by existing technology.\n\nThe typical diving compass is made from a card with graduation in degrees, mounted on a pivot in a transparent housing filled with fluid which damps the movement and prevents pressure collapse of the housing. It may be wrist mounted, console mounted or carried some other way. It is desirable that the compass can operate accurately at significant tilt angles without sticking.\n\nOn the card there is a magnet which will interact with the ambient magnetic field so as to align itself and the card with the field provided it is free to rotate. There will be other marks on the housing which are intended to be aligned with the direction of travel of the user, so the offset of the card to the housing will indicate the direction of the magnetic field and the orientation of the user.\n\nImportant features of a diving compass are that it can easily be read in dim light, the card or needle does not easily jam if the housing is tilted slightly, and that it can be securely attached to the diver's arm or equipment and does not get lost. It is useful for any straps to be adjustable while wearing gloves, and any clips that may attached should be non-magnetic.\n\nThe strap should be long enough to go round the diver's wrist over the diving suit glove, and if it is slightly elastic it will stay in place when the suit compresses.\n\nThere may be a movable bezel which can be set to record a course and to help set a reciprocal course.\n\nThere are also electronic compasses which can provide a digital or analogue display These are based on magnetometer technology. Several models of dive computer incorporate a compass function, but this may not be accessible at the same time as the primary decompression information, and may be limited in their precision of display information.\n\nThere are two ways in which a compass may be marked, which influence the way you would read them. These are known as direct reading compasses and indirect reading compasses. Both provide the same information to the same level of accuracy. Both types may have graduations on the card which can be read through a side window to give the bearing directly.\n\nThe direct reading compass has graduations on the housing which read anti-clockwise round the face, with zero on the far side. \nThe effect of this configuration is that if the housing is aligned with a direction, the north point of the card or needle will point directly towards the number representing the bearing. No further effort is needed on the part of the operator, you just find the number the arrow points at and read off the bearing. \nThe bezel has no graduations, it is just a marker to align the card.\n\nThe indirect reading compass has graduations on the bezel. These graduations are clockwise round the face, and the zero mark coincides with the notch. To take a bearing the compass must first be aligned with the direction, then the bezel must be turned so that the notch aligns with the north point of the card or needle, and the bearing can then be read at the far side of the compass.\n\nFlux-gate compasses are built into several models of dive computer as an extra function. They may require calibration when powered up, but calibration usually lasts as long as the processor is running. They are usually insensitive to tilt as there are no moving parts to jam. The display varies, and may not be as intuitive as for a mechanical compass needle or card arrangement. They can often be calibrated to account for local deviation and give true direction. The nearby presence of a magnetic compass can cause large error, but they are not greatly affected by other electronic compasses, as can be seen from the images.\n\nSome digital cameras for underwater use also have a built in flux-gate compass (such as the Olympus TG series) which can be used for navigation as well as for recording the direction of a photograph.\n\n\n"}
{"id": "1886550", "url": "https://en.wikipedia.org/wiki?curid=1886550", "title": "Dry point", "text": "Dry point\n\nIn geography, a dry point is an area of firm or flood-free ground in an area of wetland, marsh or flood plains. The term typically applies to settlements, and dry point settlements were common in history.\n\nIn the United Kingdom extreme examples of dry point settlements include Glastonbury, situated on a low hill in the marshy, and once frequently flooded, Somerset Levels, and Wareham in Dorset surrounded by flood plains to the west and Poole Harbour to the east.\n\nA dry point has the advantages of flood protection, fertile soil (due to previous floodings which would have deposited silt on the land) and fairly flat land which is ideal for agriculture and building.\n"}
{"id": "28992001", "url": "https://en.wikipedia.org/wiki?curid=28992001", "title": "E-Navigation", "text": "E-Navigation\n\ne-Navigation is a strategy deveped by the International Maritime Organization (IMO), a UN specialized agency, to l shipping through better organization of data on ships and on shore, and better data exchange and communication between ships and the ship and shore. The concept was launched when maritime authorities from seven nations requested the IMO’s Maritime Safety Committee to add the development of an e-navigation strategy to the work programs of the IMO's NAV and COMSAR sub-committees. Working groups in three sub-committees (NAV, COMSAR and STW) and an intersessional correspondence group, led by Norway, has subsequently developed a Strategy Implementation Plan (SIP). Member states of IMO and a number of Intergovernmental and non-governmental organisations have contributed to the work, including the International Hydrographic Organization (IHO), Comité International Radio-Maritime (CIRM), the International Association of Lighthouse Authorities (IALA), the International Chamber of Shipping (ICS), the Baltic and International Maritime Council (BIMCO) and the International Electrotechnical Commission (IEC)\n\nAn input paper to IMO’s Maritime Safety Committee’s 81st session in 2005 from Japan, Marshall Islands, the Netherlands, Norway, Singapore and the United Kingdom and the United States identified that there was a clear need to equip the master of a vessel, and those responsible for the safety of shipping ashore, with modern proven tools to make marine navigation and communications more reliable and thereby reduce errors − especially those with a potential for loss of life, injury, environmental damage and undue commercial costs.\n\nIt also identified that more substantial and widespread benefits for States, shipowners and seafarers could be expected to arise from the increased safety at sea, which was identified as the core objective of e-navigation.\n\nAlso according to the United Kingdom’s Marine Accident Investigation Branch, navigational errors and failures, including those of the human element, had been significant in over half of the incidents meriting a full investigation between 2002 and 2005. \nThe input paper also noted that accidents related to navigation continue to occur despite the development and availability of a number of ship- and shore-based technologies that improve situational awareness and decision-making. These include the Automatic Identification System (AIS), Electronic Chart Display and Information System (ECDIS), Integrated Bridge Systems/Integrated Navigation Systems (IBS/INS), Automatic Radar Plotting Aids (ARPA), radio navigation, Long Range Identification and Tracking (LRIT) systems, Vessel Traffic Service (VTS) and the Global Maritime Distress Safety System (GMDSS).\n\nIt was therefore proposed to add a new item on e-navigation to the work programme of the Sub-Committee on Safety of Navigation (NAV) and also to that on Radiocommunications and Search and Rescue (COMSAR). The aim was to develop a strategic vision for the utilization of existing and new navigational tools, in particular electronic tools, in a holistic and systematic manner. e-navigation can thereby help reduce navigational accidents, errors and failures by developing standards for an accurate and cost-effective system that would make a major contribution to the IMO’s agenda of safe, secure and efficient shipping on clean oceans.\n\nThe last decades have seen huge developments in technology within navigation and communication systems. Sophisticated and advanced technology is developing rapidly. Mariners have never had more technological support systems than today and therefore there is a need to coordinate systems and more use of harmonised standards. Although ships now carry Global Satellite Navigation Systems (GNSS) and will soon all have reliable Electronic Chart Displays and Information Systems (ECDIS), their use on board is not fully integrated and harmonised with other existing systems and those of other ships and ashore.\nAt the same time it has been identified that the human element, including training, competency, language skills, workload and motivation are essential in today’s world. Administrative burden, information overload and ergonomics are prominent concerns. A clear need has been identified for the application of good ergonomic principles in a well-structured human machine interface as part of the e-navigation strategy.\n\nAt MSC 85, the Committee, taking into account inputs from the industry and other relevant organizations (e.g., IALA and IHO), approved the Strategy for the development and implementation of e-navigation and developed the following definition of e-navigation:\n\n\"e-navigation is the harmonized collection, integration, exchange, presentation and analysis of marine information on board and ashore by electronic means to enhance berth to berth navigation and related services for safety and security at sea and protection of the marine environment.\"\n\nOn a global level e-navigation will:\n\nFor Coastal states, Flag states and Port states e-navigation will: \n\nFor branches, organizations and industry e-navigation will:\n\nFor ship borne users e-navigation will:\n\nThe IMO entrusted Norway and the Norwegian Coastal Administration to coordinate the work of developing a proposal for an e-navigation strategy implementation plan. Three sub-committees within the IMO - NAV, COMSAR and STW - established working groups on e-navigation; each group was chaired by John Erik Hagen of the Norwegian Coastal Administration. Further, a correspondence group overseen by the Norwegian Coastal Administration had an ongoing role in gathering input from national maritime administrations to proposals and decisions related to the process of establishing an e-navigation Strategy Implementation Plan (SIP).\n\nThe work on an e-navigation Strategy Implementation Plan was broken down into several clear phases:\n\nWith these phases complete, five agreed solutions were proposed to provide the basis for a Strategy Implementation Plan. These are:\n\nS1: improved, harmonized and user friendly bridge design;\n\nS2: means for standardized and automated reporting;\n\nS3: improved reliability, resilience and integrity of bridge equipment and navigation information;\n\nS4: integration and presentation of available information in graphical displays received via communications equipment; and\n\nS9: improved communication of VTS Service Portfolio.\n\nS1, S3 and S4 address the equipment and its use on the ship, while S2 and S9 address improved communications between ships and ship to shore and shore to ship.\nDuring the development of the SIP a number of tasks have been identified in order to continue the further development and implementation of e-navigation. Some of these tasks may require further consideration and investigation before taking a final decision on the best way forward and subsequent tasks.\nFurther it was recognised that there is a need to identify shore-based functions and services. At present, there are many different types of services in most given situations or locations, such as ports, coastal and high seas. Harmonising and standardising these services results in the Maritime Service Portfolios (MSPs).\n\nThe final e-Navigation Strategy Implementation Plan contains eight core elements, defined thus:\n\nThe combination of the five e-navigation solutions, and the three guidelines, \"Guidelines on Human Centred Design (HCD) for e-navigation\", \"Guidelines on Usability Testing, Evaluation and Assessment (U-TEA) for e-navigation systems\" and \"Guidelines for Software Quality Assurance (SQA) in e-navigation\", proposes an e-navigation implementation that facilitates a holistic approach to the interaction between shipboard and shore-based users.\n\nThe SIP was presented to the new IMO sub-committee NCSR (Navigation Communication Search and Rescue) in June 2014 for endorsement, and forwarded from there to the Maritime Safety Committee where it was approved in November 2014. Along with work taking place under the aegis of the IMO, a number of public and private groups are working to advance e-navigation and topics related to e-navigation.\n\n"}
{"id": "11750132", "url": "https://en.wikipedia.org/wiki?curid=11750132", "title": "GeoJournal", "text": "GeoJournal\n\nGeoJournal is a peer-reviewed international academic journal on all aspects of geography founded in 1977. Twelve issues (three volumes) a year were published by Springer Netherlands (formerly Kluwer) until December 2009 and can be accessed via SpringerLink. Starting February 2010, \"GeoJournal\" was relaunched as an international journal for spatially integrated social sciences and humanities with six issues a year. The journal's editor-in-chief is currently Daniel Z. Sui (Center for Urban and Regional Analysis, Department of Geography, The Ohio State University).\n\n"}
{"id": "51208627", "url": "https://en.wikipedia.org/wiki?curid=51208627", "title": "Geographic data and information", "text": "Geographic data and information\n\nGeographic data and information are defined in the ISO/TC 211 series of standards as data and information having an implicit or explicit association with a location relative to the Earth.\n\nIt is also called geospatial data and information, georeferenced data and information, as well as geodata and geoinformation.\n\nApproximately 90% of government sourced data has a location component.\n\n\n"}
{"id": "13959310", "url": "https://en.wikipedia.org/wiki?curid=13959310", "title": "Geographical Names Board of Canada", "text": "Geographical Names Board of Canada\n\nThe Geographical Names Board of Canada (GNBC) is a national committee with a secretariat in Natural Resources Canada, part of the Government of Canada, which authorizes the names used on official federal government maps of Canada created since 1897. The board consists of 27 members, one from each of the provinces and territories, and others from departments of the Government of Canada. The board also is involved with names of areas in the Antarctic through the Antarctic Treaty.\n\nThe secretariat is provided by Natural Resources Canada. In addition to the provincial and territorial members are members from the following federal government departments: Aboriginal Affairs and Northern Development Canada, Canada Post Corporation, Fisheries and Oceans Canada, Elections Canada, Library and Archives Canada, Department of National Defence, Natural Resources Canada (including Geological Survey of Canada and Canada Centre for Mapping and Earth Observation), Parks Canada, Statistics Canada, and the Translation Bureau. The Chair of the Geographical Names Board of Canada is Connie Wyatt Anderson from The Pas, Manitoba.\n\n\n"}
{"id": "1009410", "url": "https://en.wikipedia.org/wiki?curid=1009410", "title": "Geographical segregation", "text": "Geographical segregation\n\nGeographical segregation exists whenever the proportions of population rates of two or more populations are not homogenous throughout a defined space. Populations can be considered any plant or animal species, human genders, followers of a certain religion, people of different nationalities, ethnic groups, etc.\n\nIn social geography segregation of ethnic groups, social classes and genders is often measured by the calculation of indices such as the index of dissimilarity. Different dimensions of segregation (or its contrary) are recognised: exposure, evenness, clustering, concentration, centralisation, etc. More recent studies also highlight new local indices of segregation.\n\nSegregation, as a broad concept, has appeared in all parts of the world where people exist—in different contexts and times it takes on different forms, shaped by the physical and human environments. The spatial concentration of population groups is not a new phenomenon. Since societies began to form there have been segregated inhabitants. Either segregated purposefully by force, or gradually over time, segregation was based on socio-economic, religious, educational, linguistic or ethnic grounds. Some groups choose to be segregated to strengthen social identity.\n\nSegregation can be caused by legal frameworks, such as in the extreme example of apartheid in South Africa, and even Jewish ghettoization in Germany in the 20th century. Segregation can also happen slowly, stimulated by increased land and housing prices in certain neighborhoods, resulting in segregation of rich and poor in many urban cities. Segregation can also be assigned arbitrarily. This can occur on a global scale, such as is seen in the Partition of India, instances in Ireland, and many other situations. Geographical boundaries were often put in place without much consideration for native peoples and natural geographic terrain and cultural limits that had long been in place.\n\nIn apartheid South Africa, segregation was very much a legal concept. Enforced by the government, Africans were discriminated against, and forced to comply with apartheid. Some of the legislation passed dealt with physical segregation in schools, land tenure, geographic segregation and state repression. These were very clearly legislative, but also in the case of most white South Africans, a social construct as well.\n\nSegregation can also be encouraged, using geographical boundaries, while not explicitly enforced. Public housing projects, especially in the United States, have been criticized for this. Putting cheap housing in poor black neighborhoods encouraged local African-Americans to stay in the area, keeping other richer areas white by not building public housing there. This has been changing in the last ten years.\n\nSegregation can also be caused by social factors that become evident as they happen, but are not necessarily government sanctioned. This could be things like informal ghettos, or simply rich neighborhoods. In terms of land capital, over time in a given area, humans will settle down and buy or take land. Some privileged people will acquire better land (that is, more arable, proximate to potential capital, more pleasing views). Demand for these nicer habitats drives up prices, and areas deemed “better” based solely on geography become inherently exclusionary in their population makeup.\n\nWest Point Grey, an area of Vancouver Canada, is in part rich because of the views offered of Downtown Vancouver, the Gulf Islands, and its location near the water and University of British Columbia. Wealthy people had the resources to pay for advantages, and subsequently drove up prices. Examples of this can be seen all over the world. Geographical segregation is not defined by the sightline of places, though. It also occurs around certain structures, or simply in areas that are specifically developed with an income bracket in mind.\n\nAnother segregation term, the ghetto, has been used in many different contexts over time, generally meaning any physical part of a city predominantly occupied by any particular group of people. It implies that the group may be looked down upon and segregated purposefully. This does not mean that all ghettos are built up communities and buildings specifically for a segregation purpose, although many are. In the case of the United States, segregation of the African-American community was to a degree due to white flight out of the cities, than forcing African-Americans to live in the downtown cores.\n\nGated communities could be seen as a combination of both legal frameworks and social conventions regarding segregation. A gated community today is a controlled neighborhood, inhabited by people with common interests, such as safety, or class separation, but not necessarily of the same ethnicity or religion—it is distinct from an international community (in most cases). Gated communities are very controversial, as they can be seen as encouraging distinction and separation, and therefore superiority from those who do not live with the gates community.\n\nVoluntary segregation is almost as common an occurrence as involuntary segregation is. Often, immigrants coming to a new and foreign country will band together for mutual benefit, and to keep a sense of community in the new country. These can be called ethnic enclaves and can be formed by any community or people group. Some well-known groups are Chinatowns, Little Italys and \"barrios\". These localized phenomena also come in the form of ethnoburbs, which are essentially the same concept as an ethnic enclave, but specifically located in suburbs, rather than the traditional downtowns, where Chinatowns and Little Italys are usually based.\n\n"}
{"id": "12567516", "url": "https://en.wikipedia.org/wiki?curid=12567516", "title": "Geoparsing", "text": "Geoparsing\n\nGeoparsing is the process of converting free-text descriptions of places (such as \"twenty miles northeast of Jalalabad\") into unambiguous geographic identifiers, such as geographic coordinates expressed as latitude-longitude. One can also geoparse location references from other forms of media, for examples audio content in which a speaker mentions a place. With geographic coordinates the features can be mapped and entered into Geographic Information Systems. Two primary uses of the geographic coordinates derived from unstructured content are to plot portions of the content on maps and to search the content using a map as a filter.\n\nGeoparsing goes beyond geocoding. Geocoding analyzes unambiguous structured location references, such as postal addresses and rigorously formatted numerical coordinates. Geoparsing handles ambiguous references in unstructured discourse, such as \"Al Hamra,\" which is the name of several places, including towns in both Syria and Yemen.\n\nA geoparser is a piece of software or a (web) service that helps in this process.\n\n\n"}
{"id": "33867932", "url": "https://en.wikipedia.org/wiki?curid=33867932", "title": "Geospatial topology", "text": "Geospatial topology\n\nGeospatial topology studies the rules concerning the relationships between the points, lines, and polygons that represent the features of a geographic region. For example, where two polygons represent adjacent counties, typical topological rules would require that the counties share a common boundary with no gaps and no overlaps. Similarly, it would be nonsense to allow two polygons representing lakes to overlap.\n\nIn \"spatial analysis\" the topological spatial relations are derived from the DE-9IM model, as spatial predicates about relations between points, lines, and/or areas: Equals, Contains, Covers, CoveredBy, Crosses, Disjoint, Intersects, Overlaps, Touches and Within. In network and graph representations the topology analysis is about topological objects such as faces, edges and nodes.\n\nThe ESRI White Paper \"GIS Topology\" explains that topology operations are used to manage shared geometry, define and enforce data integrity rules, support topological relationship queries and navigation, and build more complex shapes such as polygons, from primitive ones such as lines. A \"GIS for Educators\" worksheet at Linfiniti adds the detection and correction of digitising errors and carrying out network analysis. Topological error correction is explained in more detail in a paper by Ubeda and Egenhofer.\n\nUnlike GML, topologies are not directly represented in ESRI shapefiles which store individual geometric objects in isolation. Topological processing can, however, be undertaken in GIS software such as GRASS GIS or QGIS or could in principle be enforced using integrity constraints in a GIS-enabled DBMS such as PostGIS. However, as Riedemann (2004) explains, topological operators are inherently complex and their implementation requires care to be taken with usability and conformance to standards.\n\nOracle and PostGIS provide fundamental topological operators allowing applications to test for \"such relationships as contains, inside, covers, covered by, touch, and overlap with boundaries intersecting.\" Unlike the PostGIS documentation, the Oracle documentation draws a distinction between \"topological relationships [which] remain constant when the coordinate space is deformed, such as by twisting or stretching\" and \"relationships that are not topological [which] include length of, distance between, and area of.\" These operators are leveraged by applications to ensure that data sets are stored and processed in a topologically correct fashion.\n"}
{"id": "29034375", "url": "https://en.wikipedia.org/wiki?curid=29034375", "title": "Glossary of geography terms", "text": "Glossary of geography terms\n\nThis glossary of geography terms is a list of definitions of terms and concepts used in geography and related fields, which describe and identify natural phenomena, geographical locations, spatial dimension and natural resources. Geographical terms are classified according to their functions, such as description, explanation, analysing, evaluating and integrating.\n\n\nMuch of this material was copied from U.S. government works which are in the because they are not eligible for copyright protection.\n"}
{"id": "8785734", "url": "https://en.wikipedia.org/wiki?curid=8785734", "title": "Heinrich Wild", "text": "Heinrich Wild\n\nHeinrich Wild (Mitlödi, Canton of Glarus, November 15, 1877 – Baden, Switzerland, December 26, 1951) was a Swiss inventor, designer and founder of Wild Heerbrugg.\n\nAt 15 years of age, Wild became an apprentice with the engineer Legler in Glarus (hydraulic engineer for the Linth River). He bought a small theodolite, and after a short time independently made expanded measurements of the flow of the river Linth. Later Wild joined the Geometerschule (geometer school) at Winterthur and came in 1899 as a trainee to the Landestopographie (Swisstopo is a popular designation for the Swiss Federal Office of Topography) in Bern. \nDue to his bad experiences with the high mountain triangulation with a theodolite of conventional design, he tried in 1905 to design a new theodolite with rotable circle with coincidence circle-readings.\n\nIn 1907 he left the Landestopographie and moved to Jena, Germany, where he joined the company Carl Zeiss to build up a new department for producing geodetic instruments. \nHe began with the development of levelling instruments and designed later also a new theodolite, the Th I.\n\nIn 1921 Wild returned to Switzerland and founded with Dr. R. Helbling, who operated a measurement office, and with the politician Jacob Schmidheiny, the company Heinrich Wild, Werkstätte für Feinmechanik und Optik (later known as Wild Heerbrugg, Wild-Leitz AG, Leica Geosystems, Leica Microsystems, Leica Camera).\n\nAt this time he developed the first versions of the famous universal theodolite Wild T series and also the stereo autograph Wild A1 for aerial photo interpretation, besides a number of other measurement instruments.\n\nIt may be perhaps typical for the inventors like Wild that he worried little about the financial condition of his company, and this ended finally with the consequence that Wild separated in 1932 from the company he founded, in order to be able to work as a freelance technical designer and inventor. He continued designing up until his death in 1951. Among his designs were the legendary DK1, DKM1, DM2, DKM2, and DKM3, for Kern & Co, Aarau, besides other things.\n\nThe ETH Zurich honored him with the title Dr. honoris causa in 1930.\n\nHis biography was published in the Historical Dictionary of Switzerland as well as by the ETHZ.\n\n"}
{"id": "10832418", "url": "https://en.wikipedia.org/wiki?curid=10832418", "title": "History of navigation", "text": "History of navigation\n\nThe history of navigation is the history of seamanship, the art of directing vessels upon the open sea through the establishment of its position and course by means of traditional practice, geometry, astronomy, or special instruments. A few people have excelled as seafarers, prominent among them the Austronesians, their descendants the Malays, Micronesians, and Polynesians, the Harappans, the Phoenicians, the ancient Greeks, the Romans, the Arabs, the ancient Indians, the Norse, the Chinese, the Venetians, the Genoese, the Hanseatic Germans, the Portuguese, the Spanish, the English, the French, the Dutch and the Danes.\nNavigation in the Pacific began at some point between about 3000 and 1000 BC, by Austronesians from Taiwan. By about 900 BC their descendants had spread more than 6,000 kilometers across the Pacific, reaching Tonga and Samoa. In this region, a distinctive Polynesian culture developed. Within the next few centuries Polynesians reached New Zealand, Easter Island and possibly South America.\n\nPolynesian navigators used a range of tools and methods, including observation of birds, star navigation, and use of waves and swells to detect nearby land. Songs, mythological stories, and star charts were used to help people remember important navigational information. \n\nSailors navigating in the Mediterranean made use of several techniques to determine their location, including staying in sight of land and understanding of the winds and their tendencies. Minoans of Crete are an example of an early Western civilization that used celestial navigation. Their palaces and mountaintop sanctuaries exhibit architectural features that align with the rising sun on the equinoxes, as well as the rising and setting of particular stars. The Minoans made sea voyages to the island of Thera and to Egypt. Both of these trips would have taken more than a day's sail for the Minoans and would have left them traveling by night across open water. Here the sailors would use the locations of particular stars, especially those of the constellation Ursa Major, to orient the ship in the correct direction.\n\nWritten records of navigation using stars, or celestial navigation, go back to Homer's Odyssey where Calypso tells Odysseus to keep the Bear (Ursa Major) on his left hand side and at the same time to observe the position of the Pleiades, the late-setting Boötes and the Orion as he sailed eastward from her island Ogygia traversing the Ocean. The Greek poet Aratus wrote in his \"Phainomena\" in the third century BC detailed positions of the constellations as written by Eudoxos. The positions described do not match the locations of the stars during Aratus' or Eudoxos' time for the Greek mainland, but some argue that they match the sky from Crete during the Bronze Age. This change in the position of the stars is due to the wobble of the Earth on its axis which affects primarily the pole stars. Around 1000 BC the constellation Draco would have been closer to the North Pole than Polaris. The pole stars were used to navigate because they did not disappear below the horizon and could be seen consistently throughout the night.\n\nBy the third century BC the Greeks had begun to use the Little Bear, Ursa Minor, to navigate. In the mid-1st century AD Lucan writes of Pompey who questions a sailor about the use of stars in navigation. The sailor replies with his description of the use of circumpolar stars to navigate by. To navigate along a degree of latitude a sailor would have needed to find a circumpolar star above that degree in the sky. For example, Apollonius would have used β Draconis to navigate as he traveled west from the mouth of the Alpheus River to Syracuse.\n\nThe voyage of the Greek navigator Pytheas of Massalia is a particularly notable example of a very long, early voyage. A competent astronomer and geographer, Pytheas ventured from Greece through the strait of Gibraltar to Western Europe and the British Isles. Pytheas is the first known person to describe the Midnight Sun, polar ice, Germanic tribes and possibly Stonehenge. Pytheas also introduced the idea of distant \"Thule\" to the geographic imagination and his account is the earliest to state that the moon is the cause of the tides.\n\nNearchos's celebrated voyage from India to Susa after Alexander's expedition in India is preserved in Arrian's account, the Indica. Greek navigator Eudoxus of Cyzicus explored the Arabian Sea for Ptolemy VIII, king of the Hellenistic Ptolemaic dynasty in Egypt. According to Poseidonius, later reported in Strabo's \"Geography\", the monsoon wind system of the Indian Ocean was first sailed by Eudoxus of Cyzicus in 118 or 116 BC.\n\nNautical charts and textual descriptions known as sailing directions have been in use in one form or another since the sixth century BC. Nautical charts using stereographic and orthographic projections date back to the second century BC.\n\nIn 1900, the Antikythera mechanism was recovered from Antikythera wreck. This mechanism was built around 1st century BC.\n\nThe Phoenicians and their successors, the Carthaginians, were particularly adept sailors and learned to voyage further and further away from the coast in order to reach destinations faster. One tool that helped them was the sounding weight. This tool was bell shaped, made from stone or lead, with tallow inside attached to a very long rope. When out to sea, sailors could lower the sounding weight in order to determine how deep the waters were, and therefore estimate how far they were from land. Also, the tallow picked up sediments from the bottom which expert sailors could examine to determine exactly where they were. The Carthaginian Hanno the Navigator is known to have sailed through the Strait of Gibraltar c. 500 BC and explored the Atlantic coast of Africa. There is general consensus that the expedition reached at least as far as Senegal. There is a lack of agreement whether the furthest limit of Hanno's explorations was Mount Cameroon or Guinea's 890-metre (2910-foot) Mount Kakulima.\n\nIn the South China Sea and Indian Ocean, a navigator could take advantage of the fairly constant monsoon winds to judge direction. This made long one-way voyages possible twice a year.\n\nThe Arab Empire significantly contributed to navigation, and had trade networks extending from the Atlantic Ocean and Mediterranean Sea in the west to the Indian Ocean and China Sea in the east, Apart from the Nile, Tigris and Euphrates, navigable rivers in the Islamic regions were uncommon, so transport by sea was very important. Islamic geography and navigational sciences made use of a magnetic compass and a rudimentary instrument known as a kamal, used for celestial navigation and for measuring the altitudes and latitudes of the stars. The kamal itself was simple to construct. It was a rectangular piece of either bone or wood which had a string with 9 consecutive knots attached to it. Another instrument available, developed by the Arabs as well, was the quadrant. Also a celestial navigation device, it was originally developed for astronomy and later transitioned to navigation. When combined with detailed maps of the period, sailors were able to sail across oceans rather than skirt along the coast. Muslim sailors were also responsible for the use and development of the lateen sails and large three-masted merchant vessels to the Mediterranean. The origins of the caravel ship, developed and used for long-distance travel by the Portuguese, and later by the rest of Iberians, since the 15th century, also date back to the \"qarib\" used by Andalusian explorers by the 13th century.\n\nThe sea lanes between India and neighboring lands were the usual form of trade for many centuries, and are responsible for the widespread influence of Indian culture to the societies of Southeast Asia. Powerful navies included those of the Maurya, Satavahana, Chola, Vijayanagara, Kalinga, Maratha and Mughal Empire.\n\nVikings used polarization and the Sunstone to allow navigation of their ships by locating the Sun even in a completely overcast sky. This special mineral was talked about in several 13th – 14th-century written sources in Iceland, some centuries after the carbon-dated, early-11th-century Norse settlement of L'Anse aux Meadows in northernmost Newfoundland had been briefly established. \n\nIn China between 1040 and 1117, the magnetic compass was being developed and applied to navigation. This let masters continue sailing a course when the weather limited visibility of the sky. The true mariner's compass using a pivoting needle in a dry box was invented in Europe no later than 1300.\n\nNautical charts called portolan charts began to appear in Italy at the end of the 13th century. However, their use did not seem to spread quickly: there are no reports of the use of a nautical chart on an English vessel until 1489.\n\nThe commercial activities of Portugal in the early 15th century marked an epoch of distinct progress in practical navigation for Europeans. These exploration and trade expeditions sent out by Infante Henrique (later called \"Henry the Navigator\") led first to the discovery of Porto Santo Island (near Madeira) in 1418, rediscovery of the Azores in 1427, the discovery of the Cape Verde Islands in 1447 and Sierra Leone in 1462. Combined with the empirical observations gathered in oceanic seafaring, mapping winds and currents, Portuguese explorers took the lead in the long distance oceanic navigation, opening later, at the beginning of the 16th century, a network of ocean routes covering the Atlantic, the Indian and the western Pacific oceans, from the North Atlantic and South America, to Japan and Southeast Asia.\n\nThe Portuguese discovered the two large volta do mar (meaning literally \"turn of the sea\" but also \"return from the sea\") currents and trade winds of North and of South Atlantic ocean (approximately in the first half and in the late 15th century respectively), that paved the way to reach the New World and return to Europe, as well as to circumnavigate Africa in western open sea, in future voyages of discovery, avoiding contrary winds and currents.\n\nKing John II of Portugal continued this effort, forming a committee on navigation. This group computed tables of the sun's declination and improved the mariner's astrolabe, believing it a good replacement for the cross-staff. These resources improved the ability of a navigator at sea to judge his latitude. Castilian Jew Abraham Zacut, the author of an exceptional treatise on astronomy/astrology in Hebrew, with the title \"Ha-jibbur Ha-gadol\", fled to Portugal in 1492. He published in the printing press of Leiria in 1496, the book \"Biur Luhoth\", or in Latin \"Almanach Perpetuum\", which was soon translated into Latin and Spanish. In this book were the astronomical tables (ephemerides) for the years 1497 to 1500, which may have been instrumental, together with the new astrolabe, made of metal and not wood as before (created and perfected at the beginning of the Portuguese discoveries), to Vasco da Gama and Pedro Álvares Cabral in their voyages to India (also passing through South America) around the open Atlantic ocean (including the Southwest Atlantic) and in the Indian Ocean. Nevertheless, the Portuguese had to hire local pilots in the Indian Ocean for several decades to guide their ships.\n\nIn the 15th and 16th centuries, the Crown of Castile and then the \"unified\" Crown of \"Spain\" was also in the vanguard of European global exploration and colonial expansion. The Spanish Crown opened trade routes across the oceans, specially the transatlantic expeditions of Christopher Columbus on behalf of Castile, from 1492. The Crown of Castile, under Charles I of Spain, also sponsored the first expedition of world circumnavigation in 1521. The enterprise was led by Portuguese navigator Ferdinand Magellan and completed by the Spanish Basque Juan Sebastián Elcano. The trips of exploration led to trade flourishing across the Atlantic Ocean between Spain and America and across the Pacific Ocean between Asia-Pacific and Mexico via the Philippines. Later, Andrés de Urdaneta discovered the northern Pacific`s \"volta do mar\" return voyage.\n\nThe compass, a cross-staff or astrolabe, a method to correct for the altitude of Polaris and rudimentary nautical charts were all the tools available to a navigator at the time of Christopher Columbus. In his notes on Ptolemy's geography, Johannes Werner of Nurenberg wrote in 1514 that the cross-staff was a very ancient instrument, but was only beginning to be used on ships.\n\nPrior to 1577, no method of judging the ship's speed was mentioned that was more advanced than observing the size of the vessel's bow wave or the passage of sea foam or various floating objects. In 1577, a more advanced technique was mentioned: the chip log. In 1578, a patent was registered for a device that would judge the ship's speed by counting the revolutions of a wheel mounted below the ship's waterline.\n\nAccurate time-keeping is necessary for the determination of longitude. As early as 1530, precursors to modern techniques were being explored. However, the most accurate clocks available to these early navigators were water clocks and sand clocks, such as hourglass. Hourglasses were still in use by the Royal Navy of Britain until 1839 for the timing of watches.\n\nContinuous accumulation of navigational data, along with increased exploration and trade, led to increased production of volumes through the Middle Ages. \"Routiers\" were produced in France about 1500; the English referred to them as \"rutters.\" In 1584 Lucas Waghenaer published the \"Spieghel der Zeevaerdt\" (\"The Mariner's Mirror\"), which became the model for such publications for several generations of navigators. They were known as \"Waggoners\" by most sailors.\n\nIn 1537, Pedro Nunes published his \"Tratado da Sphera\". In this book he included two original treatises about questions of navigation. For the first time the subject was approached using mathematical tools. This publication gave rise to a new scientific discipline: \"theoretical or scientific navigation\".\n\nIn 1545, Pedro de Medina published the influential \"Arte de navegar\". The book was translated into French, Italian, Dutch and English.\n\nIn 1569, Gerardus Mercator published for the first time a world map in such a cartographic projection that constant-rhumb trajectories were plotted as straight lines. This Mercator projection would be widely used for nautical charts from the 18th century onward.\n\nIn 1594, John Davis published an 80-page pamphlet called \"The Seaman's Secrets\" which, among other things describes great circle sailing. It's said that the explorer Sebastian Cabot had used great circle methods in a crossing of the North Atlantic in 1495. Davis also gave the world a version of the backstaff, the Davis quadrant, which became one of the dominant instruments from the 17th century until the adoption of the sextant in the 19th century.\n\nIn 1599, Edward Wright published \"Certaine Errors in Navigation\", which for the first time explained the mathematical basis of the Mercator projection, with calculated mathematical tables which made it possible to use in practice. The book made clear why only with this projection would a constant bearing correspond to a straight line on a chart. It also analysed other sources of error, including the risk of parallax errors with some instruments; and faulty estimates of latitude and longitude on contemporary charts.\n\nIn 1599/1600, Edward Wright's World Chart of 1599 was the first map under the Mercator projection drawn by an Englishman for English navigation. The map prominently displays the Queen Elizabeth I Privy Seal; the only one of her realm to carry her private seal. The Molyneux 1592 globe is the only other cartography with her Privy Seal. Both identify \"Nova Albion\", the land Captain Francis Drake claimed for his Queen during his 1577-1580 circumnavigation, above the 40th parallel.\n\nIn 1631, Pierre Vernier described his newly invented quadrant that was accurate to one minute of arc. In theory, this level of accuracy could give a line of position within a nautical mile of the navigator's actual position.\n\nIn 1635, Henry Gellibrand published an account of yearly change in magnetic variation.\n\nIn 1637, using a specially built astronomical sextant with a 5-foot radius, Richard Norwood measured the length of a nautical mile with chains. His definition of 2,040 yards is fairly close to the modern International System of Units (SI) definition of 2,025.372 yards. Norwood is also credited with the discovery of magnetic dip 59 years earlier, in 1576.\n\nIn 1714 the British \"Commissioners for the discovery of longitude at sea\" came into prominence. This group, which existed until 1828, offered grants and rewards for the solution of navigational problems. Between 1737 and 1828, the commissioners disbursed some £101,000. The government of the United Kingdom also offered significant rewards for navigational accomplishments in this era, such as £20,000 for the discovery of the Northwest Passage and £5,000 for the navigator that could sail within a degree of latitude of the North Pole. A widespread manual in the 18th century was \"Navigatio Britannica\" by John Barrow, published in 1750 by March & Page and still being advertised in 1787.\n\nIsaac Newton invented a reflecting quadrant around 1699. He wrote a detailed description of the instrument for Edmond Halley, which was published in 1742. Due to this time lapse, credit for the invention has often been given instead to John Hadley and Thomas Godfrey. The octant eventually replaced earlier cross-staffs and Davis quadrants, and had the immediate effect of making latitude calculations much more accurate.\n\nA highly important breakthrough for the accurate determination of longitude came with the invention of the marine chronometer. The 1714 longitude prize offer for a method of determining longitude at sea, was won by John Harrison, a Yorkshire carpenter. He submitted a project in 1730, and in 1735 completed a clock based on a pair of counter-oscillating weighted beams connected by springs whose motion was not influenced by gravity or the motion of a ship. His first two sea timepieces H1 and H2 (completed in 1741) used this system, but he realised that they had a fundamental sensitivity to centrifugal force, which meant that they could never be accurate enough at sea.\nHarrison solved the precision problems with his much smaller H4 chronometer design in 1761. H4 looked much like a large five-inch (12 cm) diameter pocket watch. In 1761, Harrison submitted H4 for the £20,000 longitude prize. His design used a fast-beating balance wheel controlled by a temperature-compensated spiral spring. These features remained in use until stable electronic oscillators allowed very accurate portable timepieces to be made at affordable cost. In 1767, the Board of Longitude published a description of his work in \"The Principles of Mr. Harrison's time-keeper\".\n\nIn 1757, John Bird invented the first sextant.This replaced the Davis quadrant and the octant as the main instrument for navigation. The sextant was derived from the octant in order to provide for the lunar distance method. With the lunar distance method, mariners could determine their longitude accurately. Once chronometer production was established in the late 18th century, the use of the chronometer for accurate determination of longitude was a viable alternative. Chronometers replaced lunars in wide usage by the late 19th century.\n\nIn 1891 radios, in the form of wireless telegraphs, began to appear on ships at sea.\n\nIn 1899 the \"R.F. Matthews\" was the first ship to use wireless communication to request assistance at sea. Using radio for determining direction was investigated by \"Sir Oliver Lodge, of England; Andre Blondel, of France; De Forest, Pickard; and Stone, of the United States; and Bellini and Tosi, of Italy.\" The Stone Radio & Telegraph Company installed an early prototype radio direction finder on the naval collier \"Lebanon\" in 1906.\n\nBy 1904 time signals were being sent to ships to allow navigators to check their chronometers. The U.S. Navy Hydrographic Office was sending navigational warnings to ships at sea by 1907.\n\nLater developments included the placing of lighthouses and buoys close to shore to act as marine signposts identifying ambiguous features, highlighting hazards and pointing to safe channels for ships approaching some part of a coast after a long sea voyage. In 1912 Nils Gustaf Dalén was awarded the Nobel Prize in Physics for his invention of automatic valves designed to be used in combination with gas accumulators in lighthouses\n\n1921 saw the installation of the first radiobeacon.\n\nThe first prototype shipborne radar system was installed on the \"USS Leary\" in April 1937.\n\nOn November 18, 1940 Mr. Alfred L. Loomis made the initial suggestion for an electronic air navigation system which was later developed into LORAN (long range navigation system) by the Radiation Laboratory of the Massachusetts Institute of Technology, and on November 1, 1942 the first LORAN System was placed in operation with four stations between the Chesapeake Capes and Nova Scotia.\nIn October 1957, the Soviet Union launched the world's first artificial satellite, \"Sputnik.\" Scientists at Johns Hopkins University's Applied Physics Laboratory took a series of measurements of \"Sputnik\"'s doppler shift yielding the satellite's position and velocity. This team continued to monitor \"Sputnik\" and the next satellites into space, \"Sputnik II\" and \"Explorer I\". In March 1958 the idea of working backwards, using known satellite orbits to determine an unknown position on the Earth's surface began to be explored. This led to the \"TRANSIT\" satellite navigation system. The first \"TRANSIT\" satellite was placed in polar orbit in 1960. The system, consisting of 7 satellites, was made operational in 1962. A navigator using readings from three satellites could expect accuracy of about 80 feet.\n\nOn July 14, 1974 the first prototype Navstar GPS satellite was put into orbit, but its clocks failed shortly after launch. The \"Navigational Technology Satellite 2\", redesigned with cesium clocks, started to go into orbit on June 23, 1977. By 1985, the first 11-satellite GPS Block I constellation was in orbit.\n\nSatellites of the similar Russian GLONASS system began to be put into orbit in 1982, and the system is expected to have a complete 24-satellite constellation in place by 2010. The European Space Agency expects to have its Galileo with 30 satellites in place by 2011/12 as well.\n\nElectronic integrated bridge concepts are driving future navigation system planning. Integrated systems take inputs from various ship sensors, electronically display positioning information, and provide control signals required to maintain a vessel on a preset course. The navigator becomes a system manager, choosing system presets, interpreting system output, and monitoring vessel response.\n\n\n"}
{"id": "2853761", "url": "https://en.wikipedia.org/wiki?curid=2853761", "title": "Intercept method", "text": "Intercept method\n\nThe intercept method, also known as Marcq St. Hilaire method, is an astronomical navigation method of calculating an observer's position on earth. It was originally called the \"azimuth intercept\" method because the process involves drawing a line which intercepts the azimuth line. This name was shortened to \"intercept\" method and the \"intercept distance\" was shortened to 'intercept'.\n\nThe method yields a line of position (LOP) on which the observer is situated. The intersection of two or more such lines will define the observer's position, called a \"fix\". Sights may be taken at short intervals, usually during hours of twilight, or they may be taken at an interval of an hour or more (as in observing the Sun during the day). In either case, the lines of position, if taken at different times, must be advanced or retired to correct for the movement of the ship during the interval between observations. If observations are taken at short intervals, a few minutes at most, the corrected lines of position by convention yield a \"fix\". If the lines of position must be advanced or retired by an hour or more, convention dictates that the result is referred to as a \"running fix\".\n\nThe intercept method is based on the following principle.\nThe actual distance from the observer to the geographical position (GP) of a celestial body (that is, the point where it is directly overhead) is \"measured\" using a sextant. The observer has already estimated his position by dead reckoning and calculated the distance from the estimated position to the body's GP; the difference between the \"measured\" and calculated distances is called the intercept.\n\nThe diagram on the right shows why the zenith distance of a celestial body is equal to the angular distance of its GP from the observer's position.\n\nThe rays of light from a celestial body are assumed to be parallel (unless the observer is looking at the moon, which is too close for such a simplification). The angle at the centre of the Earth that the ray of light passing through the body's GP makes with the line running from the observer's zenith is the same as the zenith distance. This is because they are corresponding angles. In practice it is not necessary to use zenith distances, which are 90° minus altitude, as the calculations can be done using observed altitude and calculated altitude.\n\nTaking a sight using the intercept method consists of the following process:\n\nSuitable bodies for celestial sights are selected, often using a Rude Star Finder. Using a sextant, an altitude is obtained of the sun, the moon, a star or a planet. The name of the body and the precise time of the sight in UTC is recorded. Then the sextant is read and the altitude (\"Hs\") of the body is recorded. Once all sights are taken and recorded, the navigator is ready to start the process of sight reduction and plotting.\n\nThe first step in sight reduction is to correct the sextant altitude for various errors and corrections. The instrument may have an error, IC or index correction (See article on adjusting a sextant). Refraction by the atmosphere is corrected for with the aid of a table or calculation and the observer's height of eye above sea level results in a \"dip\" correction, (as the observer's eye is raised the horizon dips below the horizontal). If the Sun or Moon was observed, a semidiameter correction is also applied to find the centre of the object. The resulting value is \"observed altitude\" (\"Ho\").\n\nNext, using an accurate clock, the observed celestial object's geographic position (\"GP\") is looked up in an almanac. That's the point on the Earth's surface directly below it (where the object is in the zenith). The latitude of the geographic position is called declination, and the longitude is usually called the hour angle.\n\nNext, the altitude and azimuth of the celestial body are computed for a selected position (assumed position or AP). This involves resolving a spherical triangle. Given the three magnitudes: local hour angle (\"LHA\"), observed body's declination (\"dec\"), and assumed latitude (\"lat\"), the altitude \"Hc\" and azimuth \"Zn\" must be computed. The local hour angle, \"LHA\", is the difference between the AP longitude and the hour angle of the observed object. It is always measured in a westerly direction from the assumed position.\n\nThe relevant formulas (derived using the spherical trigonometric identities) are:\n\nor, alternatively,\n\nWhere\n\nThese computations can be done easily using electronic calculators or computers but traditionally there were methods which used logarithm or haversine tables. Some of these methods were H.O. 211 (Ageton), Davies, haversine, etc. The relevant haversine formula for \"Hc\" is\n\nWhere \"\" is the zenith distance, or complement of \"Hc\".\n\n\"\" = 90° - \"Hc\".\n\nThe relevant formula for Zn is\n\nWhen using such tables or a computer or scientific calculator, the navigation triangle is solved directly, so any assumed position can be used. Often the dead reckoning DR position is used. This simplifies plotting and also reduces any slight error caused by plotting a segment of a circle as a straight line.\n\nWith the use of astral navigation for air navigation, faster methods needed to be developed and tables of precomputed triangles were developed. When using precomputed sight reduction tables, selection of the assumed position is one of the trickier steps for the fledgling navigator to master. Sight reduction tables provide solutions for navigation triangles of integral degree values. When using precomputed sight reduction tables, such as H.O. 229, the assumed position must be selected to yield integer degree values for \"LHA\" (local hour angle) and latitude. West longitudes are subtracted and east longitudes are added to \"GHA\" to derive \"LHA\", so AP's must be selected accordingly. When using precomputed sight reduction tables each observation and each body will require a different assumed position.\n\nProfessional navigators are divided in usage between sight reduction tables on the one hand, and handheld computers or scientific calculators on the other. The methods are equally accurate. It is simply a matter of personal preference which method is used. An experienced navigator can reduce a sight from start to finish in about 5 minutes using nautical tables or a scientific calculator.\n\nThe precise location of the assumed position has no great impact on the result, as long as it is reasonably close to the observer's actual position. An assumed position within 1 degree of arc of the observer's actual position is usually considered acceptable.\n\nThe calculated altitude (\"Hc\") is compared to the observed altitude (\"Ho\", sextant altitude (\"Hs\") corrected for various errors). The difference between \"Hc\" and \"Ho\" is called \"intercept\" and is the observer's distance from the assumed position. The resulting line of position (\"LOP\") is a small segment of the circle of equal altitude, and is represented by a straight line perpendicular to the azimuth of the celestial body. When plotting the small segment of this circle on a chart it is drawn as a straight line, the resulting tiny errors are too small to be significant.\n\nNavigators use the memory aid \"computed greater away\" to determine whether the observer is farther from the body's geographic position (measure intercept from \"Hc\" away from the azimuth). If the \"Hc\" is less than \"Ho\", then the observer is closer to the body's geographic position, and intercept is measured from the AP toward the azimuth direction.\n\nThe last step in the process is to plot the lines of position \"LOP\" and determine the vessel's location. Each assumed position is plotted first. Best practise is to then advance or retire the assumed positions to correct for vessel motion during the interval between sights. Each LOP is then constructed from its associated AP by striking off the azimuth to the body, measuring intercept toward or away from the azimuth, and constructing the perpendicular line of position.\n\nTo obtain a fix (a position) this \"LOP\" must be crossed with another \"LOP\" either from another sight or from elsewhere e.g. a bearing of a point of land or crossing a depth contour such as the 200 metre depth line on a chart.\n\nUntil the age of satellite navigation ships usually took sights at dawn, during the forenoon, at noon (meridian transit of the Sun) and dusk. The morning and evening sights were taken during twilight while the horizon was visible and the stars, planets and/or moon were visible, at least through the telescope of a sextant. Two observations are always required to give a position accurate to within a mile under favourable conditions. Three are always sufficient.\n\nA fix is called a \"running fix\" when one or more of the LOPs used to obtain it is an LOP advanced or retrieved over time. In order to get a fix the LOP must cross at an angle, the closer to 90° the better. This means the observations must have different azimuths. During the day, if only the Sun is visible, it is possible to get an LOP from the observation but not a fix as another LOP is needed. What may be done is take a first sight which yields one LOP and, some hours later, when the Sun's azimuth has changed substantially, take a second sight which yields a second LOP. Knowing the distance and course sailed in the interval, the first LOP can be advanced to its new position and the intersection with the second LOP yields a \"running fix\".\n\nAny sight can be advanced and used to obtain a \"running fix\". It may be that the navigator due to weather conditions could only obtain a single sight at dawn. The resulting LOP can then be advanced when, later in the morning, a Sun observation becomes possible. The precision of a running fix depends on the error in distance and course so, naturally, a running fix tends to be less precise than an unqualified fix and the navigator must take into account his confidence in the exactitude of distance and course to estimate the resulting error in the running fix.\n\nDetermining a fix by crossing LOPs and advancing LOPs to get running fixes are not specific to the intercept method and can be used with any sight reduction method or with LOPs obtained by any other method (bearings, etc.).\n\n\n\n"}
{"id": "38262946", "url": "https://en.wikipedia.org/wiki?curid=38262946", "title": "Land systems", "text": "Land systems\n\nLand systems constitute the terrestrial component of the Earth system and encompass all processes and activities related to the human use of land, including socioeconomic, technological and organizational investments and arrangements, as well as the benefits gained from land and the unintended social and ecological outcomes of societal activities. Changes in land systems have large consequences for the local environment and human well-being and are at the same time pervasive factors of global environmental change. Land provides vital resources to society, such as food, fuel, fibres and many other ecosystem services that support production functions, regulate risks of natural hazards, or provide cultural and spiritual services. By using the land, society alters and modifies the quantity and quality of the provision of these services.\n\nLand system changes are the direct result of human decision making at multiple scales ranging from local land owners decisions to national scale land use planning and global trade agreements. The aggregate impact of many local land system changes has far reaching consequences for the Earth System, that feedback on ecosystem services, human well-being and decision making. As a consequence, land system change is both a cause and consequence of socio-ecological processes.\n\nThe Global Land Programme (GLP) of Future Earth is an interdisciplinary community of science and practice fostering the study of land systems and the co-design of solutions for global sustainability.\n"}
{"id": "1034969", "url": "https://en.wikipedia.org/wiki?curid=1034969", "title": "Levelling", "text": "Levelling\n\nLevelling (British English) or leveling (American English); is a branch of surveying, the object of which is to establish or verify or measure the height of specified points relative to a datum. It is widely used in cartography to measure geodetic height, and in construction to measure height differences of construction artifacts.\n\nOptical levelling employs an optical level,which consists of a precision telescope with crosshairs and Stadia marks. The cross hairs are used to establish the level point on the target, and the stadia allow range-finding; stadia are usually at ratios of 100:1, in which case one metre between the stadia marks on the levelling staff represents 100 metres from the target.\nThe complete unit is normally mounted on a tripod, and the telescope can freely rotate 360° in a horizontal plane. The surveyor adjusts the instrument's level by coarse adjustment of the tripod legs and fine adjustment using three precision levelling screws on the instrument to make the rotational plane horizontal. The surveyor does this with the use of a bull's eye level built into the instrument mount.\nThe surveyor looks through the eyepiece of telescope while an assistant holds a vertical level staff which is a graduated in inches or centimeters. The level staff is placed with its foot on the point for which the level measurement is required. The telescope is rotated and focused until the level staff is plainly visible in the crosshairs. In the case of a high accuracy manual level, the fine level adjustment is made by an altitude screw, using a high accuracy bubble level fixed to the telescope. This can be viewed by a mirror whilst adjusting or the ends of the bubble can be displayed within the telescope, which also allows assurance of the accurate level of the telescope whilst the sight is being taken. However, in the case of an automatic level, altitude adjustment is done automatically by a suspended prism due to gravity, as long as the coarse levelling is accurate within certain limits. When level, the staff graduation reading at the crosshairs is recorded, and an identifying mark or marker placed where the level staff rested on the object or position being surveyed.\n\nA typical procedure for a linear track of levels from a known datum is as follows. Set up the instrument within of a point of known or assumed elevation. A rod or staff is held vertical on that point and the instrument is used manually or automatically to read the rod scale. This gives the height of the instrument above the starting (backsight) point and allows the height of the instrument (H.I.) above the datum to be computed.\nThe rod is then held on an unknown point and a reading is taken in the same manner, allowing the elevation of the new (foresight) point to be computed. The procedure is repeated until the destination point is reached. It is usual practice to perform either a complete loop back to the starting point or else close the traverse on a second point whose elevation is already known. The closure check guards against blunders in the operation, and allows residual error to be distributed in the most likely manner among the stations.\n\nSome instruments provide three crosshairs which allow stadia measurement of the foresight and backsight distances. These also allow use of the average of the three readings (3-wire leveling) as a check against blunders and for averaging out the error of interpolation between marks on the rod scale.\n\nThe two main types of levelling are single-levelling as already described, and double-levelling (Double-rodding). In double-levelling, a surveyor takes two foresights and two backsights and makes sure the difference between the foresights and the difference between the backsights are equal, thereby reducing the amount of error. Double-levelling costs twice as much as single-levelling.\n\nWhen using an optical level, the endpoint may be out of the effective range of the instrument. There may be obstructions or large changes of elevation between the endpoints. In these situations, extra setups are needed. Turning is a term used when referring to moving the level to take an elevation shot from a different location.\n\nTo \"turn\" the level, one must first take a reading and record the elevation of the point the rod is located on. While the rod is being kept in exactly the same location, the level is moved to a new location where the rod is still visible. A reading is taken from the new location of the level and the height difference is used to find the new elevation of the level gun. This is repeated until the series of measurements is completed.\n\nThe level must be horizontal to get a valid measurement. Because of this, if the horizontal crosshair of the instrument is lower than the base of the rod, the surveyor will not be able to sight the rod and get a reading. The rod can usually be raised up to 25 feet high, allowing the level to be set much higher than the base of the rod.\n\nThe curvature of the earth means that a line of sight that is horizontal at the instrument will be higher and higher above a spheroid at greater distances. The effect may be significant for some work at distances under 100 meters.\n\nThe line of sight is horizontal at the instrument, but is not a straight line because of refraction in the air. The change of air density with elevation causes the line of sight to bend toward the earth.\n\nThe combined correction for refraction and curvature is approximately: \nFor precise work these effects need to be calculated and corrections applied. For most work it is sufficient to keep the foresight and backsight distances approximately equal so that the refraction and curvature effects cancel out. Refraction is generally the greatest source of error in leveling. For short level lines the effects of temperature and pressure are generally insignificant, but the effect of the temperature gradient \"dT / dh\" can lead to errors.\n\nAssuming error-free measurements, if the Earth's gravity field were completely regular and gravity constant, leveling loops would always close precisely:\n\naround a loop. In the real gravity field of the Earth, this happens only approximately; on small loops typical of engineering projects, the loop closure is negligible, but on larger loops covering regions or continents it is not.\n\nInstead of height differences, \"geopotential differences\" do close around loops:\n\nwhere formula_5 stands for gravity at the leveling interval \"i\". For precise leveling networks on a national scale, the latter formula should always be used.\n\nshould be used in all computations, producing geopotential values formula_7 for the benchmarks of the network.\n\n The wye level is the oldest and bulkiest of the older style optical instruments. A low-powered telescope is placed in a pair of clamp mounts, and the instrument then leveled using a spirit level, which is mounted parallel to the main telescope.\n\nThe dumpy level was developed by English civil engineer William Gravatt, while surveying the route of a proposed railway line from London to Dover. More compact and hence both more robust and easier to transport, it is commonly believed that dumpy levelling is less accurate than other types of levelling, but such is not the case. Dumpy levelling requires shorter and therefore more numerous sights, but this fault is compensated by the practice of making foresights and backsights equal.\n\nPrecise level designs were often used for large leveling projects where utmost accuracy was required. They differ from other levels in having a very precise spirit level tube and a micrometer adjustment to raise or lower the line of sight so that the crosshair can be made to coincide with a line on the rod scale and no interpolation is required.\n\nAutomatic levels make use of a compensator that ensures that the line of sight remains horizontal once the operator has roughly leveled the instrument (to within maybe 0.05 degree). The surveyor sets the instrument up quickly and doesn't have to relevel it carefully each time he sights on a rod on another point. It also reduces the effect of minor settling of the tripod to the actual amount of motion instead of leveraging the tilt over the sight distance. Three level screws are used to level the instrument.\n\nLaser levels project a beam which is visible and/or detectable by a sensor on the leveling rod. This style is widely used in construction work but not for more precise control work. An advantage is that one person can perform the levelling independently, whereas other types require one person at the instrument and one holding the rod.\n\nThe sensor can be mounted on earth-moving machinery to allow automated grading.\n\n\n"}
{"id": "18007820", "url": "https://en.wikipedia.org/wiki?curid=18007820", "title": "List of countries by irrigated land area", "text": "List of countries by irrigated land area\n\nThis is a list of countries by irrigated land area based on \"The World Factbook\" of the Central Intelligence Agency.\n\n\n"}
{"id": "54228300", "url": "https://en.wikipedia.org/wiki?curid=54228300", "title": "List of official trips made by Edi Rama", "text": "List of official trips made by Edi Rama\n\nThis is a list of official trips made by Edi Rama as the 33rd Prime Minister of the Republic of Albania.\n\nThe following international trips were made by Prime Minister Edi Rama in 2013:\n\nThe following international trips were made by Prime Minister Edi Rama in 2014:\n\nThe following international trips were made by Prime Minister Edi Rama in 2015:\n\nThe following international trips were made by Prime Minister Edi Rama in 2016:\n\nThe following international trips were made by Prime Minister Edi Rama in 2017:\n\nThe following international trips are scheduled to be made by Edi Rama during 2017:\n\n"}
{"id": "19152401", "url": "https://en.wikipedia.org/wiki?curid=19152401", "title": "List of political and geographic subdivisions by total area from 3,000 to 5,000 square kilometers", "text": "List of political and geographic subdivisions by total area from 3,000 to 5,000 square kilometers\n"}
{"id": "11486041", "url": "https://en.wikipedia.org/wiki?curid=11486041", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: K", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: K\n\n\n"}
{"id": "11485651", "url": "https://en.wikipedia.org/wiki?curid=11485651", "title": "List of towns and cities with 100,000 or more inhabitants/country: X-Y-Z", "text": "List of towns and cities with 100,000 or more inhabitants/country: X-Y-Z\n\n\n"}
{"id": "56176805", "url": "https://en.wikipedia.org/wiki?curid=56176805", "title": "Lists of renamed places", "text": "Lists of renamed places\n\nThese are lists of renamed places by country, sorted by continent.\n\n\n\n\n\n\n"}
{"id": "629028", "url": "https://en.wikipedia.org/wiki?curid=629028", "title": "Meridian (geography)", "text": "Meridian (geography)\n\nA (geographical) meridian (or line of longitude) is the half of an imaginary great circle on the Earth's surface, terminated by the North Pole and the South Pole, connecting points of equal longitude, as measured in angular degrees east or west of the Prime Meridian. The position of a point along the meridian is given by that longitude and its latitude, measured in angular degrees north or south of the Equator. Each meridian is perpendicular to all circles of latitude. Each is also the same length, being half of a great circle on the Earth's surface and therefore measuring 20,003.93 km (12,429.9 miles).\n\nThe first prime meridian was set by Eratosthenes in 276 BCE. This prime meridian was used to provide measurement of the earth, but had many issues because of the lack of latitude measurement. Many years later around the 19th century there was still concerns of the prime meridian. The idea of having one prime meridian came from William Parker Snow, because he realized the confusion of having multiple prime meridian locations. Many of theses geographical locations were traced back to the ancient Greeks, and others were created by several nations. Multiple locations for the geographical meridian meant that there was inconsistency, because each country had their own guidelines for where the prime meridian was located.\n\nThe term \"meridian\" comes from the Latin \"meridies\", meaning \"midday\"; the subsolar point passes through a given meridian at solar noon, midway between the times of sunrise and sunset on that meridian. Likewise, the Sun crosses the celestial meridian at the same time. The same Latin stem gives rise to the terms a.m. (ante meridiem) and p.m. (post meridiem) used to disambiguate hours of the day when utilizing the 12-hour clock.\n\nToward the ending of the 19th century there were two main locations that were acknowledged as the geographic location of the meridian, France and Britain. These two locations often conflicted and a settlement was reached only after there was an International Meridian Conference held, in which Greenwich was recognized as the 0° location.\n\nThe meridian through Greenwich (inside Greenwich Park), England, called the Prime Meridian, was set at zero degrees of longitude, while other meridians were defined by the angle at the center of the earth between where it and the prime meridian cross the equator. As there are 360 degrees in a circle, the meridian on the opposite side of the earth from Greenwich, the antimeridian, forms the other half of a circle with the one through Greenwich, and is at 180° longitude near the International Date Line (with land mass and island deviations for boundary reasons). The meridians from West of Greenwich (0°) to the antimeridian (180°) define the Western Hemisphere and the meridians from East of Greenwich (0°) to the antimeridian (180°) define the Eastern Hemisphere. Most maps show the lines of longitude.\n\nThe position of the prime meridian has changed a few times throughout history, mainly due to the transit observatory being built next door to the previous one (to maintain the service to shipping). Such changes had no significant practical effect. Historically, the average error in the determination of longitude was much larger than the change in position. The adoption of WGS84 (\"World Geodetic System 84\") as the positioning system has moved the geodetic prime meridian 102.478 metres east of its last astronomic position (measured at Greenwich). The position of the current geodetic prime meridian is not identified at all by any kind of sign or marking (as the older astronomic position was) in Greenwich, but can be located using a GPS receiver.\n\nIt was in the best interests of the nations to agree to one standard meridian to benefit their fast growing economy and production. The disorganized system they had before was not sufficient for their increasing mobility. The coach services in England had erratic timing before the GWT. U.S. and Canada were also improving their railroad system and needed a standard time as well. With a standard meridian, stage coach and trains were able to be more efficient. The argument of which meridian is more scientific was set aside in order to find the most convenient for practical reasons. They were also able to agree that the universal day was going to be the mean solar day. They agreed that the days would begin at midnight and the universal day would not impact the use of local time. In the \"Transactions of the Royal Society of Canada a report was submitted, dated 10 May 1894; on the Unification of the Astronomical, Civil and Nautical Days\" it states that:\n\ncivil day- begins at midnight and ends at midnight following\n\nastronomical day- begins at noon of civil day and continue until following noon\n\nnautical day- concludes at noon of civil day, starting at preceding noon\n\nThe magnetic meridian is an equivalent imaginary line connecting the magnetic south and north poles and can be taken as the horizontal component of magnetic force lines along the surface of the earth. Therefore, a compass needle will be parallel to the magnetic meridian. However, a compass needle will not be steady in the magnetic meridian, because of the longitude from east to west being complete geodesic. The angle between the magnetic and the true meridian is the magnetic declination, which is relevant for navigating with a compass. Navigators were able to use the azimuth (the horizontal angle or direction of a compass bearing) of the rising and setting Sun to measure the magnetic variation (difference between magnetic and true north).\n\nThe true meridian is the plane that passes through true north poles and true south poles at the spot of the observer. The difference between true meridian and magnetic meridian is that the true meridian is fixed while the magnetic meridian is formed through the movement of the needle. True bearing is the horizontal angle between true meridian and a line. Henry D. Thoreau classified this true meridian versus the magnetic meridian in order to have a more qualitative, intuitive, and abstract function. He used the true meridian since his compass varied by a few degrees. There were some variations. When he noted the sight line for the True Meridian from his family's house to the depot, he could check the declination of his compass before and after surveying throughout the day. He noted this variation down.\n\nThe meridian passage is the moment when a celestial object passes the meridian of longitude of the observer. At this point, the celestial object is at its highest point. When the sun passes two times an altitude while rising and setting can be averaged to give the time of meridian passage. Navigators utilized the sun’s declination and the sun’s altitude at local meridian passage, in order to calculate their latitude with the formula.\n\nLatitude = (90o – noon altitude + declination)\n\nThe declination of major stars are their angles north and south from the celestial equator. It is important to note that the Meridian passage will not occur exactly at 12 hours because of the inclination of the earth. The meridian passage can occur within a few minutes of variation.\n\nMany of these instruments rely on the ability to measure the longitude and latitude of the earth. These instruments also were typically effected by local gravity, which paired well with existing technologies such as the magnetic meridian.\n\nTools of Measurement\n\n\n\n"}
{"id": "31775293", "url": "https://en.wikipedia.org/wiki?curid=31775293", "title": "Mobile mapping", "text": "Mobile mapping\n\nMobile mapping is the process of collecting geospatial data from a mobile vehicle, typically fitted with a range of photographic, radar, laser, LiDAR or any number of remote sensing systems. Such systems are composed of an integrated array of time synchronised navigation sensors and imaging sensors mounted on a mobile platform. The primary output from such systems include GIS data, digital maps, and georeferenced images and video.\n\nThe development of direct reading georeferencing technologies opened the way for mobile mapping systems. GPS and Inertial Navigation Systems, have allowed rapid and accurate determination of position and attitude of remote sensing equipment, effectively leading to direct mapping of features of interest without the need for complex post-processing of observed data.\n\nTraditional techniques of geo-referencing aerial photography, ground profiling radar, or Lidar are prohibitively expensive, particularly in inaccessible areas, or where the type of data collected makes interpretation of individual features difficult. Image direct georeferencing, simplifies the mapping control for large scale mapping tasks.\n\nMobile mapping systems allow rapid collection of data to allow accurate assessment of conditions on the ground.\n\nInternet, and mobile device users, are increasingly utilising geo-spatial information, either in the form of mapping, or geo-referenced imaging. Google, Microsoft, and Yahoo have adapted both aerial photographs and satellite images to develop online mapping systems. \"Street View\" type images are also an increasing market.\n\nLocation aware PDA systems rely on geo-referenced features collated from mobile mapping sources.\n\nGPS combined with digital camera systems allow rapid update of road maps.\nThe same system can be utilised to carry out efficient road condition surveys, and facilities management. Laser scanning technologies, applied in the mobile mapping sense, allow full 3D data collection of slope, bankings, etc.\n\n"}
{"id": "14389994", "url": "https://en.wikipedia.org/wiki?curid=14389994", "title": "Natural landscape", "text": "Natural landscape\n\nA natural landscape is the original landscape that exists before it is acted upon by human culture. The natural landscape and the cultural landscape are separate parts of the landscape. However, in the twenty-first century landscapes that are totally untouched by human activity no longer exist, so that reference is sometimes now made to degrees of naturalness within a landscape.\n\nIn \"Silent Spring\" (1962) Rachel Carson describes a roadside verge as it used to look: \"Along the roads, laurel, viburnum and alder, great ferns and wildflowers delighted the traveler’s eye through much of the year\" and then how it looks now following the use of herbicides: \"The roadsides, once so attractive, were now lined with browned and withered vegetation as though swept by fire\". Even though the landscape before it is sprayed is biologically degraded, and may well contains alien species, the concept of what might constitute a natural landscape can still be deduced from the context.\n\nThe phrase \"natural landscape\" was first used in connection with landscape painting, and landscape gardening, to contrast a formal style with a more natural one, closer to nature. Alexander von Humboldt (1769 – 1859) was to further conceptualize this into the idea of a natural landscape \"separate\" from the cultural landscape. Then in 1908 geographer Otto Schlüter developed the terms original landscape (\"Urlandschaft\") and its opposite cultural landscape (\"Kulturlandschaft\") in an attempt to give the science of geography a subject matter that was different from the other sciences. An early use of the actual phrase \"natural landscape\" by a geographer can be found in Carl O. Sauer's paper \"The Morphology of Landscape\" (1925).\n\nThe concept of a natural landscape was first developed in connection with landscape painting, though the actual term itself was first used in relation to landscape gardening. In both cases it was used to contrast a formal style with a more natural one, that is closer to nature. Chunglin Kwa suggests, \"that a seventeenth-century or early-eighteenth-century person could experience natural scenery ‘just like on a painting,’ and so, with or without the use of the word itself, designate it as a landscape.\" With regard to landscape gardening John Aikin, commented in 1794: \"Whatever, therefore, there be of \"novelty\" in the singular scenery of an artificial garden, it is soon exhausted, whereas the infinite diversity of a natural landscape presents an inexhaustible flore of new forms\". Writing in 1844 the prominent American landscape gardener Andrew Jackson Downing comments: \"straight canals, round or oblong pieces of water, and all the regular forms of the geometric mode ... would evidently be in violent opposition to the whole character and expression of natural landscape\".\n\nIn his extensive travels in South America, Alexander von Humboldt became the first to conceptualize a natural landscape separate from the cultural landscape, though he does not actually use these terms. Andrew Jackson Downing was aware of, and sympathetic to, Humboldt's ideas, which therefore influenced American landscape gardening.\n\nSubsequently, the geographer Otto Schlüter, in 1908, argued that by defining geography as a \"Landschaftskunde\" (landscape science) would give geography a logical subject matter shared by no other discipline. He defined two forms of landscape: the \"Urlandschaft\" (original landscape) or landscape that existed before major human induced changes and the \"Kulturlandschaft\" (cultural landscape) a landscape created by human culture. Schlüter argued that the major task of geography was to trace the changes in these two landscapes.\n\nThe term natural landscape is sometimes used as a synonym for wilderness, but for geographers natural landscape is a scientific term which refers to the biological, geological, climatological and other aspects of a landscape, not the cultural values that are implied by the word wilderness.\n\nMatters are complicated by the fact that the words nature and natural have more than one meaning. On the one hand there is the main dictionary meaning for nature: \"The phenomena of the physical world collectively, including plants, animals, the landscape, and other features and products of the earth, as opposed to humans or human creations\". On the other hand, there is the growing awareness, especially since Charles Darwin, of humanities biological affinity with nature.\n\nThe dualism of the first definition has its roots is an \"ancient concept\", because early people viewed \"nature, or the nonhuman world […] as a divine \"Other\", godlike in its separation from humans\". In the West, Christianity's myth of the fall, that is the expulsion of humankind from the Garden of Eden, where all creation lived in harmony, into an imperfect world, has been the major influence. Cartesian dualism, from the seventeenth century on, further reinforced this dualistic thinking about nature. \nWith this dualism goes value judgement as to the superiority of the natural over the artificial. Modern science, however, is moving towards a holistic view of nature.\n\nWhat is meant by natural, within the American conservation movement, has been changing over the last century and a half.\n\nIn the mid-nineteenth century American began to realize that the land was becoming more and more domesticated and wildlife was disappearing. This led to the creation of American National Parks and other conservation sites. Initially it was believed that all that was needed to do was to separate what was seen as natural landscape and \"avoid disturbances such as logging, grazing, fire and insect outbreaks\". This, and subsequent environmental policy, until recently, was influenced by ideas of the wilderness. However, this policy was not consistently applied, and in Yellowstone Park, to take one example, the existing ecology was altered, firstly by the exclusion of Native Americans and later with the virtual extermination of the wolf population.\n\nA century later, in the mid-twentieth century, it began to be believed that the earlier policy of \"protection from disturbance was inadequate to preserve park values\", and that is that direct human intervention was necessary to restore the landscape of National Parks to its ‘’natural’’ condition. In 1963 the Leopold Report argued that \"A national park should represent a vignette of primitive America\". This policy change eventually led to the restoration of wolves in Yellowstone Park in the 1990s.\n\nHowever, recent research in various disciplines indicates that a pristine natural or \"primitive\" landscape is a myth, and it now realised that people have been changing the natural into a cultural landscape for a long while, and that there are few places untouched in some way from human influence. The earlier conservation policies were now seen as cultural interventions. The idea of what is natural and what artificial or cultural, and how to maintain the natural elements in a landscape, has been further complicated by the discovery of global warming and how it is changing natural landscapes.\n\nAlso important is a reaction recently amongst scholars against dualistic thinking about nature and culture. Maria Kaika comments: \"Nowadays, we are beginning to see nature and culture as intertwined once again – not ontologically separated anymore […].What I used to perceive as a compartmentalized world, consisting of neatly and tightly sealed, autonomous ‘space envelopes’ (the home, the city, and nature) was, in fact, a messy socio-spatial continuum”. And William Cronon argues against the idea of wilderness because it \"involves a dualistic vision in which the human is entirely outside the natural\" and affirms that \"wildness (as opposed to wilderness) can be found anywhere\" even \"in the cracks of a Manhattan sidewalk\". According to Cronon we have to \"abandon the dualism that sees the tree in the garden as artificial […] and the tree in the wilderness as natural […] Both in some ultimate sense are wild.\" Here he bends somewhat the regular dictionary meaning of wild, to emphasise that nothing natural, even in a garden, is fully under human control.\n\nThe landscape of Europe has considerably altered by people and even in an area, like the Cairngorm Mountains of Scotland, with a low population density, only \" the high summits of the Cairngorm Mountains, consist entirely of natural elements. These \"high summits\" are of course only part of the Cairngorms, and there are no longer wolves, bears, wild boar or lynx in Scotland's wilderness. The Scots pine in the form of the Caledonian forest also covered much more of the Scottish landscape than today.\n\nThe Swiss National Park, however, represent a more natural landscape. It was founded in 1914, and is one of the earliest national parks in Europe.\nVisitors are not allowed to leave the motor road, or paths through the park, make fire or camp. The only building within the park is Chamanna Cluozza, mountain hut. It is also forbidden to disturb the animals or the plants, or to take home anything found in the park. Dogs are not allowed. Due to these strict rules, the Swiss National Park is the only park in the Alps who has been categorized by the IUCN as a strict nature reserve, which is the highest protection level.\n\nNo place on the Earth is unaffected by people and their culture. People are part of biodiversity, but human activity affects biodiversity, and this alters the natural landscape. Mankind have altered landscape to such an extent that few places on earth remain pristine, but once free of human influences, the landscape can return to a natural or near natural state.\nEven the remote Yukon and Alaskan wilderness, the bi-national Kluane-Wrangell-St. Elias-Glacier Bay-Tatshenshini-Alsek park system comprising Kluane, Wrangell-St Elias, Glacier Bay and Tatshenshini-Alsek parks, a UNESCO World Heritage Site, is not free from human influence, because the Kluane National Park lies within the traditional territories of the Champagne and Aishihik First Nations and Kluane First Nation who have a long history of living in this region. Through their respective Final Agreements with the Canadian Government, they have made into law their rights to harvest in this region.\n\nCultural forces intentionally or unintentionally, have an influence upon the landscape. Cultural landscapes are places or artifacts created and maintained by people. Examples of cultural intrusions into a landscape are: fences, roads, parking lots, sand pits, buildings, hiking trails, management of plants, including the introduction of invasive species, extraction or removal of plants, management of animals, mining, hunting, natural landscaping, farming and forestry, pollution. Areas that might be confused with a natural landscape include public parks, farms, orchards, artificial lakes and reservoirs, managed forests, golf courses, nature center trails, gardens.\n\n"}
{"id": "9997671", "url": "https://en.wikipedia.org/wiki?curid=9997671", "title": "Nautical stationkeeping", "text": "Nautical stationkeeping\n\nStation-keeping in a nautical situation is when a vessel is to:\n\n"}
{"id": "7693731", "url": "https://en.wikipedia.org/wiki?curid=7693731", "title": "NavPix", "text": "NavPix\n\nNavPix is the proprietary name applied by Navman to its technology that combines an image with geographical data.\n\nThe \"NavPix\" name is used for both the software and the geo-referenced image that results from that software.\n\nThe NavPix technology enables users to take a JPEG image using the integrated digital camera on the N Series (\"N\" for NavPix), iCN 720 or iCN 750 portable Navman GPS navigation devices.\n\nThe Navman's GPS (Global Positioning System) receiver determines the latitude and longitude of where that image was taken. That information is then written into the image's Exif (Exchangeable image file format) meta data by the NavPix software. The NavPix, therefore, effectively provides a Georeference of the location where the image was taken, which is not necessarily the same georeference as the object being \"NavPix-ed\".\n\nThe NavPix image can then be used to define a destination or point of interest on compatible Navman devices.\n\nFurthermore, as the geographical information is written to the meta data, the image itself can be shared between compatible devices or uploaded to Navman's NavPix Library which offers a wide range of NavPix images that have been taken by both Navman users and sourced from professional photo providers, including Lonely Planet.\n\nThe NavPix Library also enables people to upload non-NavPix images (including other formats such as GIF) and convert them to NavPix images by using entering either the latitude and longitude they want to associate with the image or by entering the address and using the Library's software to generate the latitude and longitude values based on a Postal code look-up.\n\nUnlike some geo-referencing applications, the NavPix Library writes the georeference values to the image itself via the Exif meta data.\n\nThe photo taking abilities do not help navigation.\n\n\n"}
{"id": "6755999", "url": "https://en.wikipedia.org/wiki?curid=6755999", "title": "Normalnull", "text": "Normalnull\n\nNormalnull (\"standard zero\") or Normal-Null (short N. N. or NN ) is an outdated official vertical datum used in Germany. Elevations using this reference system were to be marked \"Meter über Normal-Null\" (“meters above standard zero”). Normalnull has been replaced by Normalhöhennull (short NHN).\n\nIn 1878 reference heights were taken from the Amsterdam Ordnance Datum and transferred to the New Berlin Observatory in order to define the Normalhöhenpunkt 1879. Normalnull has been defined as a level going through an imaginary point 37.000 m below \"Normalhöhenpunkt 1879\". When the New Berlin Observatory was demolished in 1912 the reference point was moved east to the village of Hoppegarten (now part of the town of Müncheberg, Brandenburg, Germany).\n"}
{"id": "555750", "url": "https://en.wikipedia.org/wiki?curid=555750", "title": "Permeable paving", "text": "Permeable paving\n\nPermeable paving is a method of paving vehicle and pedestrian pathways that allows for infiltration of fluids. In pavement design the base is the top portion of the roadway that pedestrians or vehicles come into contact with. The media used for the base of permeable paving may be porous, to allow for fluids to flow through it, or nonporous media that are spaced so that fluid may flow in between the gaps. In addition to reducing surface runoff, permeable paving can trap suspended solids therefore filtering pollutants from stormwater. Examples include roads, paths, and parking lots that are subject to light vehicular traffic, such as cycle-paths, service or emergency access lanes, road and airport shoulders, and residential sidewalks and driveways.\n\nAlthough some porous paving materials appear nearly indistinguishable from nonporous materials, their environmental effects are qualitatively different. Whether it is pervious concrete, porous asphalt, paving stones or concrete or plastic-based pavers, all these permeable paving systems allow stormwater to percolate and infiltrate the surface areas, bypassing the traditionally impervious materials to the soil below. The goal is to control stormwater at the source, reduce runoff and improve water quality by filtering pollutants in the substrata layers.\n\nPermeable solutions can be based on: porous asphalt and concrete surfaces, concrete pavers (permeable interlocking concrete paving systems – PICP), or polymer-based grass pavers, grids and geocells. Porous pavements and concrete pavers (actually the voids in-between them) enable stormwater to drain through a stone base layer for on-site infiltration and filtering. Polymer based grass grid or cellular paver systems provide load bearing reinforcement for unpaved surfaces of gravel or turf.\n\nGrass pavers, plastic turf reinforcing grids (PTRG), and geocells (cellular confinement systems) are honeycombed 3D grid-cellular systems, made of thin-walled HDPE plastic or other polymer alloys. These provide grass reinforcement, ground stabilization and gravel retention. The 3D structure reinforces infill and transfers vertical loads from the surface, distributing them over a wider area. Selection of the type of cellular grid depends to an extent on the surface material, traffic and loads. The cellular grids are installed on a prepared base layer of open-graded stone (higher void spacing) or engineered stone (stronger). The surface layer may be compacted gravel or topsoil seeded with grass and fertilizer. In addition to load support, the cellular grid reduces compaction of the soil to maintain permeability, while the roots improve permeability due to their root channels.\n\nIn new suburban growth, porous pavements protect watersheds by delaying and filtering the surge flow. In existing built-up areas and towns, redevelopment and reconstruction are opportunities to implement stormwater water management practices. Permeable paving is an important component in Low Impact Development (LID), a process for land development in the United States that attempts to minimize impacts on water quality and the similar concept of sustainable drainage systems (SuDS) in the United Kingdom.\n\nThe infiltration capacity of the native soil is a key design consideration for determining the depth of base rock for stormwater storage or for whether an underdrain system is needed.\n\nPermeable paving surfaces have been demonstrated as effective in managing runoff from paved surfaces. Large volumes of urban runoff causes serious erosion and siltation in surface water bodies. Permeable pavers provide a solid ground surface, strong enough to take heavy loads, like large vehicles, while at the same time they allow water to filter through the surface and reach the underlying soils, mimicking natural ground absorption. They can reduce downstream flooding and stream bank erosion, and maintain base flows in rivers to keep ecosystems self-sustaining. Permeable pavers also combat erosion that occurs when grass is dry or dead, by replacing grassed areas in suburban and residential environments.\n\nPermeable paving surfaces keep the pollutants in place in the soil or other material underlying the roadway, and allow water seepage to groundwater recharge while preventing the stream erosion problems. They capture the heavy metals that fall on them, preventing them from washing downstream and accumulating inadvertently in the environment. In the void spaces, naturally occurring micro-organisms digest car oils, leaving little but carbon dioxide and water. Rainwater infiltration is usually less than that of an impervious pavement with a separate stormwater management facility somewhere downstream. .in areas where infiltration is not possible due to unsuitable soil conditions permeable pavements are used in the attenuation mode where water is retained in the pavement and slowly released to surface water systems between storm events.\n\nPermeable pavements may give urban trees the rooting space they need to grow to full size. A \"structural-soil\" pavement base combines structural aggregate with soil; a porous surface admits vital air and water to the rooting zone. This integrates healthy ecology and thriving cities, with the living tree canopy above, the city's traffic on the ground, and living tree roots below. The benefits of permeables on urban tree growth have not been conclusively demonstrated and many researchers have observed tree growth is not increased if construction practices compact materials before permeable pavements are installed.\n\nPermeable pavements are designed to replace Effective Impervious Areas (EIAs), not to manage stormwater from other impervious surfaces on site. Use of this technique must be part of an overall on site management system for stormwater, and is not a replacement for other techniques.\n\nDuring large storm event, the water table below the porous pavement can rise to a higher level, preventing the precipitation from being absorbed into the ground. Some additional water is stored in the open graded / crushed drain rock base, and remains until the subgrade can absorb the water. For clay-based soils, or other low to 'non'-draining soils, it is important to increase the depth of the crushed drain rock base to allow additional capacity for the water as it waits to be infiltrated.\n\nThe best way to prevent this problem is to understand the soil infiltration rate, and design the pavement and base depths to meet the volume of water. Or, allow for adequate rain water run off at the pavement design stage.\n\nRunoff across some land uses may become contaminated, where pollutant concentrations exceed those typically found in stormwater. These \"hot spots\" include commercial plant nurseries, recycling facilities, fueling stations, industrial storage, marinas, some outdoor loading facilities, public works yards, hazardous materials generators (if containers are exposed to rainfall), vehicle service and maintenance areas, and vehicle and equipment washing and steam cleaning facilities. Since porous pavement is an infiltration practice, it should not be applied at stormwater hot spots due to the potential for groundwater contamination. All contaminated runoff should be prevented from entering municipal storm drain systems by using best management practices (BMPs) for the specific industry or activity.\n\nReference sources differ on whether low or medium traffic volumes and weights are appropriate for porous pavements. For example, around truck loading docks and areas of high commercial traffic, porous pavement is sometimes cited as being inappropriate. However, given the variability of products available, the growing number of existing installations in North America and targeted research by both manufacturers and user agencies, the range of accepted applications seems to be expanding. Some concrete paver companies have developed products specifically for industrial applications. Working examples exist at fire halls, busy retail complex parking lots, and on public and private roads, including intersections in parts of North America with quite severe winter conditions.\n\nPermeable pavements may not be appropriate when land surrounding or draining into the pavement exceeds a 20 percent slope, where pavement is down slope from buildings or where foundations have piped drainage at their footers. The key is to ensure that drainage from other parts of a site is intercepted and dealt with separately rather than being directed onto permeable surfaces.\n\nCold climates may present special challenges. Road salt contains chlorides that could migrate through the porous pavement into groundwater. Snow plow blades could catch block edges and damage surfaces. Sand cannot be used for snow and ice control on porous surfaces because it will plug the pores and reduce permeability. Although there are design modifications to reduce the risks, infiltrating runoff may freeze below the pavement, causing frost heave. Another issue is spalling damage. Spalling damage exclusively occurs on porous concrete pavement from salt application during the winter season. Thus porous paving is suggested for warmer climates. However, other materials have proven to be effective, even lowering winter maintenance costs by preserving salt in the pavement itself. This also reduces the amount of storm water runoff that is contaminated with salt chlorides. Porous pavement designed to reduce frost heave and spalling damage has been used successfully in Norway. Furthermore, experience suggests that preventative measures with rapid drainage below porous surfaces be taken in order to increase the rate of snow melt above ground.\n\nSome estimates put the cost of permeable paving at two to three times that of conventional asphalt paving. Using permeable paving, however, can reduce the cost of providing larger or more stormwater BMPs on site, and these savings should be factored into any cost analysis. In addition, the off-site environmental impact costs of not reducing on-site stormwater volumes and pollution have historically been ignored or assigned to other groups (local government parks, public works and environmental restoration budgets, fisheries losses, etc.) The City of Olympia, Washington is studying the use of pervious concrete quite closely and finding that new stormwater regulations are making it a viable alternative to storm water.\n\nSome permeable pavements require frequent maintenance because grit or gravel can block the open pores. This is commonly done by industrial vacuums that suck up all the sediment. If maintenance is not carried out on a regular basis, the porous pavements can begin to function more like impervious surfaces. With more advanced paving systems the levels of maintenance needed can be greatly decreased, elastomerically bound glass pavements requires less maintenance than regular concrete paving as the glass bound pavement has 50% more void space.\n\nPlastic grid systems, if selected and installed correctly, are becoming more and more popular with local government maintenance personnel owing to the reduction in maintenance efforts: reduced gravel migration and weed suppression in public park settings.\n\nSome permeable paving products are prone to damage from misuse, such as drivers who tear up patches of plastic & gravel grid systems by \"joy riding\" on remote parking lots at night. The damage is not difficult to repair but can look unsightly in the meantime. Grass pavers require supplemental watering in the first year to establish the vegetation, otherwise they may need to be re-seeded. Regional climate also means that most grass applications will go dormant during the dry season. While brown vegetation is only a matter of aesthetics, it can influence public support for this type of permeable paving.\n\nTraditional permeable concrete paving bricks tend to lose their color in relatively short time which can be costly to replace or clean and is mainly due to the problem of efflorescence.\n\nEfflorescence is a hardened crystalline deposit of salts, principally calcium carbonates, which migrate from the center of concrete or masonry materials to the surface, where they form insoluble deposits that harden on the surface. Efflorescence usually appears white, gray or black depending on the region.\n\nOver time, efflorescence begins to degrade the overall appearance of masonry/concrete and may cause the surfaces to become slippery when exposed to moisture. If left unchecked, this efflorescence will harden whereby the calcium/lime deposits begin to affect the integrity of the cementitious surface by slowly eroding away the cement paste and aggregate. In some cases it will also discolor stained or coated surfaces.\n\nEfflorescence forms more quickly in areas that are exposed to excessive amounts of moisture, such as near pool decks, spas, and fountains or where irrigation runoff is present. The affected regions become very slick when wet. This can be of serious concern especially as a public safety issue to individuals, principals and property owners by exposing them to possible injury and increased general liability claims.\n\nEfflorescence remover chemicals can be used to remove calcium/lime build-up without damaging the integrity of the paving surface.\n\nInstallation of porous pavements is no more difficult than that of dense pavements, but has different specifications and procedures which must be strictly adhered to. Nine different families of porous paving materials present distinctive advantages and disadvantages for specific applications. Here are examples:\n\nPervious concrete is widely available, can bear frequent traffic, and is universally accessible. Pervious concrete quality depends on the installer's knowledge and experience.\n\nPlastic grids allow for a 100% porous system using structural grid systems for containing and stabilizing either gravel or turf. These grids come in a variety of shapes and sizes depending on use; from pathways to commercial parking lots. These systems have been used readily in Europe for over a decade, but are gaining popularity in North America due to requirements by government for many projects to meet LEED environmental building standards. Plastic grid system are also popular with homeowners due to their lower cost to install, ease of installation, and versatility. The ideal design for this type of grid system is a closed cell system, which prevents gravel/sand/turf from migrating laterally. It is also known as Grass pavers / Turf Pavers in India.\n\nPorous asphalt is produced and placed using the same methods as conventional asphalt concrete; it differs in that fine (small) aggregates are omitted from the asphalt mixture. The remaining large, single-sized aggregate particles leave open voids that give the material its porosity and permeability. To ensure pavement strength, fiber may be added to the mix or a polymer-modified asphalt binder may be used. Generally, porous asphalt pavements are designed with a subsurface reservoir that holds water that passes through the pavement, allowing it to evaporate and/or percolate slowly into the surround soils.\n\n\"Open-graded friction courses\" (OGFC) are a porous asphalt surface course used on highways to improve driving safety by removing water from the surface. Unlike a full-depth porous asphalt pavement, OGFCs do not drain water to the base of a pavement. Instead, they allow water to infiltrate the top 3/4 to 1.5 inch of the pavement and then drain out to the side of the roadway. This can improve the friction characteristics of the road and reducing road spray.\n\nSingle-sized aggregate without any binder, e.g. loose gravel, stone-chippings, is another alternative. Although it can only be safely used in walkways and very low-speed, low-traffic settings, e.g. car-parks and drives, its potential cumulative area is great.\n\nPorous turf, if properly constructed, can be used for occasional parking like that at churches and stadia. Plastic turf reinforcing grids can be used to support the increased load. Living turf transpires water, actively counteracting the \"heat island\" with what appears to be a green open lawn.\n\nPermeable interlocking concrete pavements are concrete units with open, permeable spaces between the units. They give an architectural appearance, and can bear both light and heavy traffic, particularly interlocking concrete pavers, excepting high-volume or high-speed roads. Some products are polymer-coated and have an entirely porous face.\n\nPermeable clay brick pavements are fired clay brick units with open, permeable spaces between the units. Clay pavers provide a durable surface that allows stormwater runoff to permeate through the joints.\n\nResin bound paving is a mixture of resin binder and aggregate. Clear resin is used to fully coat each aggregate particle before laying. Enough resin is used to allow each aggregate particle to adhere to one another and to the base yet leave voids for water to permeate through. Resin bound paving provides a strong and durable surface that is suitable for pedestrian and vehicular traffic in applications such as pathways, driveways, car parks and access roads.\n\nElastomerically bound recycled glass porous pavement consisting of bonding processed post consumer glass with a mixture of resins, pigments, granite and binding agents. Approximately 75 percent of glass in the U.S. is disposed in landfills.\n\nStormwater management practices related to roadways:\n\n"}
{"id": "48134895", "url": "https://en.wikipedia.org/wiki?curid=48134895", "title": "Primary direction", "text": "Primary direction\n\nPrimary direction is a term in astronomy for the reference meridian used in a celestial coordinate system for that system's longitude.\n\n"}
{"id": "2245722", "url": "https://en.wikipedia.org/wiki?curid=2245722", "title": "QDGC", "text": "QDGC\n\nQDGC - Quarter Degree Grid Cells (or QDS - Quarter degree Squares) are a way of dividing the longitude latitude degree square cells into smaller squares, forming in effect a system of geocodes. Historically QDGC has been used in a lot of African atlases. Several African biodiversity projects uses QDGC, among which The atlas of Southern African Birds is the most prominent one. In 2009 a paper by Larsen \"et al.\" describes the QDGC standard in detail.\n\nThe squares themselves are based on the degree squares covering earth. QDGC represents a way of making approximately equal area squares covering a specific area to represent specific qualities of the area covered. However, differences in area between 'squares' enlarge along with longitudinal distance and this can violate assumptions of many statistical analyses requiring truly equal-area grids. For instance species range modelling or estimates of ecological niche could be substantilly affected if data were not appriopriatelly transformed, e.g. projected onto a plane using a special projection.\n\nAround the equator we have 360 longitudinal lines, and from the north to the south pole we have 180 latitudinal lines. Together this gives us 64800 segments or tiles covering earth. The form of the squares becomes more rectangular the longer north we come. At the poles they are not square or even rectangular at all, but end up in elongated triangles.\n\nEach degree square is designated by a full reference to the main degree square. S01E010 is a reference to a square in Tanzania. S means the square is south of equator, and E means it is East of the zero meridian. The numbers refer to longitudinal and latitudinal degree.\n\nA square with no sublevel reference is also called QDGC level 0. This is square based on a full degree longitude by a full degree latitude. The QDGC level 0 squares are themselves divided into four.\n\nTo get smaller squares the above squares are again divided in four - giving us a total of 16 squares within a degree square. The names for the new level of squares are named the same way. The full reference of a square could then be:\n\n\nThe number of squares for each QDGC level can be calculated with this formula:\n\nnumber of squares = (2)\n\nTable showing level, number of squares and an example reference:\n\nTo decide which name a specific longitude latitude value belongs to it is possible to use the code provided on this Github project:\n\nDownload shapefiles datasets here:\n\n\nRelated websites\n"}
{"id": "17660060", "url": "https://en.wikipedia.org/wiki?curid=17660060", "title": "RINEX", "text": "RINEX\n\nIn the field of geodesy, Receiver Independent Exchange Format (RINEX) is a data interchange format for raw satellite navigation system data. This allows the user to post-process the received data to produce a more accurate result — usually with other data unknown to the original receiver, such as better models of the atmospheric conditions at time of measurement.\n\nThe final output of a navigation receiver is usually its position, speed or other related physical quantities. However, the calculation of these quantities are based on a series of measurements from one or more satellite constellations. Although receivers calculate positions in real time, in many cases it is interesting to store intermediate measures for later use. RINEX is the standard format that allows the management and disposal of the measures generated by a receiver, as well as their off-line processing by a multitude of applications, whatever the manufacturer of both the receiver and the computer application.\n\nThe RINEX format is designed to evolve over time, adapting to new types of measurements and new satellite navigation systems. The first RINEX version was published by W. Gurtner and G. Mader in the CSTG GPS Bulletin of September/October 1990. Since 1993 the RINEX 2 is available, which has been revised and adopted several times. RINEX enables storage of measurements of pseudorange, carrier-phase, Doppler and signal-to-noise from GPS (including GPS modernization signals e.g. L5 and L2C), GLONASS, Galileo, Beidou, along with data from EGNOS and WAAS satellite based augmentation systems (SBAS), QZSS, simultaneously. RINEX version 3.02 was submitted in April 2013 and is capable of new measurements from GPS or Galileo systems. The most recent version is RINEX 3.03 from July 2015 with update 1 to 3.03 published in 2017.\n\nAlthough not part of the RINEX format, the \"Hatanaka compression scheme \" is commonly used to reduced the size of RINEX files, resulting in an ASCII-based CompactRINEX format.. It uses higher-order time differences to reduce the number of characters needed to store time data.\n\n"}
{"id": "673887", "url": "https://en.wikipedia.org/wiki?curid=673887", "title": "Reticle", "text": "Reticle\n\nA reticle, or reticule (), also known as a graticule (), is a pattern of fine lines or markings built into the eyepiece of a sighting device, such as a telescopic sight in a telescope, a microscope, or the screen of an oscilloscope, to provide measurement references during visual examination. Today, engraved lines or embedded fibers may be replaced by a computer-generated image superimposed on a screen or eyepiece. Both terms may be used to describe any set of lines used for optical measurement, but in modern use \"reticle\" is most commonly used for gunsights and such, while \"graticule\" is more widely used for the oscilloscope display, microscope slides, and similar roles.\n\nThere are many variations of reticles; this article concerns itself mainly with a simple reticle: crosshairs. Crosshairs are most commonly represented as intersecting lines in the shape of a cross, \"+\", though many variations exist, including dots, posts, circles, scales, chevrons, or a combination of these. Most commonly associated with telescopic sights for aiming firearms, crosshairs are also common in optical instruments used for astronomy and surveying, and are also popular in graphical user interfaces as a precision pointer. The reticle is said to have been invented by Robert Hooke, and dates to the 17th century. Another candidate as inventor is the amateur astronomer William Gascoigne, who predated Hooke.\n\nTelescopic sights for firearms, generally just called \"scopes\", are probably the device most often associated with crosshairs. Motion pictures and the media often use a view through crosshairs as a dramatic device, which has given crosshairs wide cultural exposure.\n\nWhile the traditional thin crossing lines are the original and still the most familiar cross-hair shape, they are really best suited for precision aiming at high contrast targets, as the thin lines are easily lost in complex backgrounds, such as those encountered while hunting. Thicker bars are much easier to discern against a complex background, but lack the precision of thin bars. The most popular types of cross-hair in modern scopes are variants on the \"duplex\" cross-hair, with bars that are thick on the perimeter and thin out in the middle. The thick bars allow the eye to quickly locate the center of the reticle, and the thin lines in the center allow for precision aiming. The thin bars in a duplex reticle may also be designed to be used as a measure. Called a 30/30 reticle, the thin bars on such a reticle span 30 minutes of arc (0.5º), which is approximately equal to 30 inches at 100 yards. This enables an experienced shooter to deduce, on the basis of the known size of an object in view, (as opposed to guess or estimate) the range within an acceptable error limit.\n\nOriginally crosshairs were constructed out of hair or spiderweb, these materials being sufficiently thin and strong. Many modern scopes use wire crosshairs, which can be flattened to various degrees to change the width. These wires are usually silver in color, but appear black when backlit by the image passing through the scope's optics. Wire reticles are by nature fairly simple, as they require lines that pass all the way across the reticle, and the shapes are limited to the variations in thickness allowed by flattening the wire; duplex crosshairs, and crosshairs with dots are possible, and multiple horizontal or vertical lines may be used. The advantage of wire crosshairs is that they are fairly tough and durable, and provide no obstruction to light passing through the scope.\n\nThe first suggestion for etched glass reticles was made by Philippe de La Hire in 1700. His method was based on engraving the lines on a glass plate with a diamond point. Many modern crosshairs are actually etched onto a thin plate of glass, which allows a far greater latitude in shapes. Etched glass reticles can have \"floating\" elements, which do not cross the reticle; circles and dots are common, and some types of glass reticles have complex sections designed for use in range estimation and bullet drop and drift compensation (see external ballistics). A potential disadvantage of glass reticles is that they are less durable than wire crosshairs, and the surface of the glass reflects some light (about 4% per surface on uncoated glass) lessening transmission through the scope, although this light loss is near zero if the glass is multicoated (coating being the norm for all modern high quality optical products).\n\nReticles may be illuminated, either by a plastic or fiber optic light pipe collecting ambient light or, in low light conditions, by a battery powered LED. Some sights also use the radioactive decay of tritium for illumination that can work for 11 years without using a battery, used in the British SUSAT sight for the SA80 (L85) assault rifle and in the American ACOG (Advanced Combat Optical Gunsight). Red is the most common color used, as it is the least destructive to the shooter's night vision, but some products use green or yellow illumination, either as a single colour or changeable via user selection.\n\nA graticule is another term for reticle, frequently encountered in British and British military technical manuals, and came into common use during World War One.\n\nThe reticle may be located at the front or rear focal plane (First Focal Plane (FFP) or Second Focal Plane (SFP)) of the telescopic sight. On fixed power telescopic sights there is no significant difference, but on variable power telescopic sights the front plane reticle remains at a constant size compared to the target, while rear plane reticles remain a constant size to the user as the target image grows and shrinks. Front focal plane reticles are slightly more durable, but most American users prefer that the reticle remains constant as the image changes size, so nearly all modern American variable power telescopic sights are rear focal plane designs. American and European high end optics manufacturers often leave the customer the choice between a FFP or SFP mounted reticle.\n\nCollimated reticles are produced by non-magnifying optical devices such as reflector sights (often called \"reflex sights\") that give the viewer an image of the reticle superimposed over the field of view, and blind collimator sights that are used with both eyes. Collimated reticles are created using refractive or reflective optical collimators to generate a collimated image of an illuminated or reflective reticle. These types of sights are used on surveying/triangulating equipment, to aid celestial telescope aiming, and as sights on firearms. Historically they were used on larger military weapon systems that could supply an electrical source to illuminate them and where the operator needed a wide field of view to track and range a moving target visually (i.e. weapons from the pre laser/radar/computer era). More recently sights using low power consumption durable light emitting diodes as the reticle (called \"red dot sight\"s) have become common on small arms with versions like the Aimpoint CompM2 being widely fielded by the U.S. Military.\n\nHolographic weapon sights use a holographic image of a reticle at finite set range built into the viewing window and a collimated laser diode to illuminate it. An advantage to holographic sights is that they eliminate a type of parallax problem found in some optical collimator based sights (such as the red dot sight) where the spherical mirror used induces spherical aberration that can cause the reticle to skew off the sight's optical axis. The use of a hologram also eliminates the need for image dimming narrow band reflective coatings and allows for reticles of almost any shape or mil size. A downside to the holographic weapon sight can be the weight and shorter battery life. As with red dot sights, holographic weapon sights have also become common on small arms with versions like the Eotech 512.A65 and similar models fielded by the U.S. Military and various law enforcement agencies.\n\nIn older instruments, reticle crosshairs and stadia marks were made using threads taken from the cocoon of the brown recluse spider. This very fine, strong spider silk makes for an excellent crosshair.\n\nIn surveying, reticles are designed for specific uses. Levels and theodolites would have slightly different reticles. However, both may have features such as stadia marks to allow distance measurements.\n\nFor astronomical uses, reticles could be simple crosshair designs or more elaborate designs for special purposes. Telescopes used for polar alignment could have a reticle that indicates the position of Polaris relative to the north celestial pole. Telescopes that are used for very precise measurements would have a filar micrometer as a reticle; this could be adjusted by the operator to measure angular distances between stars.\n\nFor aiming telescopes, reflex sights are popular, often in conjunction with a small telescope with a crosshair reticle. They make aiming the telescope at an astronomical object easier.\n\nThe constellation Reticulum was designated to recognize the reticle and its contributions to astronomy.\n\n"}
{"id": "2545972", "url": "https://en.wikipedia.org/wiki?curid=2545972", "title": "Rogaining", "text": "Rogaining\n\nRogaining is an orienteering sport of long distance cross-country navigation, involving both route planning and navigation between checkpoints using a variety of map types. In a rogaine, teams of 2–5 people choose which checkpoints to visit within a time limit with the intent of maximizing their score. Teamwork, endurance, competition and an appreciation for the natural environment are features of the sport. Championship rogaines are 24 hours long, but rogaines can be as short as two hours.\n\nRogaining can trace its roots back to 1947 when the first of many events with some of the features of rogaines was organized by the Melbourne University Mountaineering Club. The events from the 1940s eventually led to the birth of the sport of rogaining in April 1976, in Melbourne, Australia. The sport was named, rules were adopted and the world’s first rogaining association was formed (the Victorian Rogaining Association). Growth of the association and the sport occurred rapidly over the next decade.\n\nThe word rogaining is derived from the names of three of the founders, Rod Phillips, Gail Davis (née Phillips) and Neil Phillips (RoGaiNe, hence 'rogaining', 'rogainer' etc.) who were all members of the Surrey-Thomas Rover Crew which organized the world's first rogaine. The name was formally adopted by the Victorian Rogaining Association at its inaugural annual general meeting in August 1976 and accepted by Scouts Australia and University bushwalking groups to give the new sport an identity in its own right.\n\nWorld Rogaining Championships were first held in 1992, and then every two years from 1996 to 2012. Since then they have been held annually, with the country chosen two years in advance by the International Rogaining Federation. As of 2017 this has changed back again to every two years.\n\n\nTeams of two to five members visit as many checkpoints as possible in the time allowed. Shorter duration rogaines often allow solo competitors. Checkpoints are scored differently depending on level of difficulty in reaching them; therefore teams choose a strategy (for example, to visit many low score checkpoints). Teams travel entirely on foot, navigating by map and compass between checkpoints in terrain that varies from open farmland to hilly forest. A central base camp known as a \"hash house\" provides hot meals throughout the event and teams may return at any time to eat, rest or sleep. Teams travel at their own pace and anyone from children to grandparents can experience the personal satisfaction that comes from cross-country navigation at their own level of competition and comfort. Team members stay within earshot of each other.\n\nThe duration of a championship rogaine is 24 hours, but shorter variations such as 3-, 6-, 8-, 12- and 15-hour events are also held (sometimes concurrently with a 24-hour event). Depending on the terrain, experienced rogaining teams can cover more than one hundred kilometers over the 24-hour period. There have also been longer events (dubbed \"Endurogaines\") lasting 48 and 50 hours. In their native Australia, rogaines are usually held every month during weekends near full moons, with annual state championships and an annual national championship. They require dozens of volunteers to run, including course setters and caterers and are often attended by hundreds of participants. A competitive 24-hour rogaine requires a map area of around 250 km (c. 100 mi), for relatively flat terrain; terrain with significant climbs may require far less or far more area, depending on the placement of checkpoints.\n\nOther forms of rogaining are popular, particularly in Australia during the months when normal rogaines are not held because of the weather. Popular variations include:\n\n\n\n"}
{"id": "32655756", "url": "https://en.wikipedia.org/wiki?curid=32655756", "title": "Running survey", "text": "Running survey\n\nA running survey is a rough survey made by a vessel while coasting. Bearings to landmarks are taken at intervals as the vessel sails offshore, and are used to fix features on the coast and further inland. Intervening coastal detail is sketched in. \n\nThe method was used by James Cook, and subsequently by navigators who sailed under—or were influenced by—him, including George Vancouver, William Bligh and Matthew Flinders.\n"}
{"id": "1515653", "url": "https://en.wikipedia.org/wiki?curid=1515653", "title": "Satellite navigation", "text": "Satellite navigation\n\nA satellite navigation or satnav system is a system that uses satellites to provide autonomous geo-spatial positioning. It allows small electronic receivers to determine their location (longitude, latitude, and altitude/elevation) to high precision (within a few metres) using time signals transmitted along a line of sight by radio from satellites. The system can be used for providing position, navigation or for tracking the position of something fitted with a receiver (satellite tracking). The signals also allow the electronic receiver to calculate the current local time to high precision, which allows time synchronisation. Satnav systems operate independently of any telephonic or internet reception, though these technologies can enhance the usefulness of the positioning information generated.\n\nA satellite navigation system with global coverage may be termed a global navigation satellite system (GNSS). , the United States' Global Positioning System (GPS) and Russia's GLONASS are fully operational GNSSs, with China's BeiDou Navigation Satellite System (BDS) and the European Union's Galileo scheduled to be fully operational by 2020. India, France and Japan are in the process of developing regional navigation and augmentation systems as well.\n\nGlobal coverage for each system is generally achieved by a satellite constellation of 18–30 medium Earth orbit (MEO) satellites spread between several orbital planes. The actual systems vary, but use orbital inclinations of >50° and orbital periods of roughly twelve hours (at an altitude of about ).\n\nSatellite navigation systems that provide enhanced accuracy and integrity monitoring usable for civil navigation are classified as follows:\n\n\nGround based radio navigation has long been practiced. The DECCA, LORAN, GEE and Omega systems used terrestrial longwave radio transmitters which broadcast a radio pulse from a known \"master\" location, followed by a pulse repeated from a number of \"slave\" stations. The delay between the reception of the master signal and the slave signals allowed the receiver to deduce the distance to each of the slaves, providing a fix.\n\nThe first satellite navigation system was Transit, a system deployed by the US military in the 1960s. Transit's operation was based on the Doppler effect: the satellites travelled on well-known paths and broadcast their signals on a well-known radio frequency. The received frequency will differ slightly from the broadcast frequency because of the movement of the satellite with respect to the receiver. By monitoring this frequency shift over a short time interval, the receiver can determine its location to one side or the other of the satellite, and several such measurements combined with a precise knowledge of the satellite's orbit can fix a particular position. Satellite orbital position errors are induced by variations in the gravity field and radar refraction, among others. These were resolved by a team led by Harold L Jury of Pan Am Aerospace Division in Florida from 1970-1973. Using real-time data assimilation and recursive estimation, the systematic and residual errors were narrowed down to a manageable level to permit accurate navigation. \n\nPart of an orbiting satellite's broadcast included its precise orbital data. In order to ensure accuracy, the US Naval Observatory (USNO) continuously observed the precise orbits of these satellites. As a satellite's orbit deviated, the USNO would send the updated information to the satellite. Subsequent broadcasts from an updated satellite would contain its most recent ephemeris.\n\nModern systems are more direct. The satellite broadcasts a signal that contains orbital data (from which the position of the satellite can be calculated) and the precise time the signal was transmitted. The orbital ephemeris is transmitted in a data message that is superimposed on a code that serves as a timing reference. The satellite uses an atomic clock to maintain synchronization of all the satellites in the constellation. The receiver compares the time of broadcast encoded in the transmission of three (at sea level) or four different satellites, thereby measuring the time-of-flight to each satellite. Several such measurements can be made at the same time to different satellites, allowing a continual fix to be generated in real time using an adapted version of trilateration: see GNSS positioning calculation for details.\n\nEach distance measurement, regardless of the system being used, places the receiver on a spherical shell at the measured distance from the broadcaster. By taking several such measurements and then looking for a point where they meet, a fix is generated. However, in the case of fast-moving receivers, the position of the signal moves as signals are received from several satellites. In addition, the radio signals slow slightly as they pass through the ionosphere, and this slowing varies with the receiver's angle to the satellite, because that changes the distance through the ionosphere. The basic computation thus attempts to find the shortest directed line tangent to four oblate spherical shells centred on four satellites. Satellite navigation receivers reduce errors by using combinations of signals from multiple satellites and multiple correlators, and then using techniques such as Kalman filtering to combine the noisy, partial, and constantly changing data into a single estimate for position, time, and velocity.\n\nThe original motivation for satellite navigation was for military applications. Satellite navigation allows precision in the delivery of weapons to targets, greatly increasing their lethality whilst reducing inadvertent casualties from mis-directed weapons. (See Guided bomb). Satellite navigation also allows forces to be directed and to locate themselves more easily, reducing the fog of war.\n\nThe ability to supply satellite navigation signals is also the ability to deny their availability. The operator of a satellite navigation system potentially has the ability to degrade or eliminate satellite navigation services over any territory it desires.\n\nThe United States' Global Positioning System (GPS) consists of up to 32 medium Earth orbit satellites in six different orbital planes, with the exact number of satellites varying as older satellites are retired and replaced. Operational since 1978 and globally available since 1994, GPS is the world's most utilized satellite navigation system.\n\nThe formerly Soviet, and now Russian, \"Global'naya Navigatsionnaya Sputnikovaya Sistema\", (GLObal NAvigation Satellite System or GLONASS), is a space-based satellite navigation system that provides a civilian radionavigation-satellite service and is also used by the Russian Aerospace Defence Forces. GLONASS has full global coverage with 24 satellites.\n\nThe European Union and European Space Agency agreed in March 2002 to introduce their own alternative to GPS, called the Galileo positioning system. Galileo became operational on 15 December 2016 (global Early Operational Capability (EOC)) At an estimated cost of €3 billion, the system of 30 MEO satellites was originally scheduled to be operational in 2010. The original year to become operational was 2014. The first experimental satellite was launched on 28 December 2005. Galileo is expected to be compatible with the modernized GPS system. The receivers will be able to combine the signals from both Galileo and GPS satellites to greatly increase the accuracy. Galileo is expected to be in full service in 2020 and at a substantially higher cost.\nThe main modulation used in Galileo Open Service signal is the Composite Binary Offset Carrier (CBOC) modulation.\n\nChina has indicated their plan to complete the entire second generation Beidou Navigation Satellite System (BDS or BeiDou-2, formerly known as COMPASS), by expanding current regional (Asia-Pacific) service into global coverage by 2020. The BeiDou-2 system is proposed to consist of 30 MEO satellites and five geostationary satellites. A 16-satellite regional version (covering Asia and Pacific area) was completed by December 2012.\n\nChinese regional (Asia-Pacific, 16 satellites) network to be expanded into the whole BeiDou-2 global system which consists of all 35 satellites by 2020.\n\nThe NAVIC or NAVigation with Indian Constellation is an autonomous regional satellite navigation system developed by Indian Space Research Organisation (ISRO) which would be under the total control of Indian government. The government approved the project in May 2006, with the intention of the system completed and implemented on 28 April 2016. It will consist of a constellation of 7 navigational satellites. 3 of the satellites will be placed in the Geostationary orbit (GEO) and the remaining 4 in the Geosynchronous orbit(GSO) to have a larger signal footprint and lower number of satellites to map the region. It is intended to provide an all-weather absolute position accuracy of better than 7.6 meters throughout India and within a region extending approximately 1,500 km around it. A goal of complete Indian control has been stated, with the space segment, ground segment and user receivers all being built in India. All seven satellites, IRNSS-1A, IRNSS-1B, IRNSS-1C, IRNSS-1D, IRNSS-1E, IRNSS-1F, and IRNSS-1G, of the proposed constellation were precisely launched on 1 July 2013, 4 April 2014, 16 October 2014, 28 March 2015, 20 January 2016, 10 March 2016 and 28 April 2016 respectively from Satish Dhawan Space Centre. The system is expected to be fully operational by August 2016.\n\nThe Quasi-Zenith Satellite System (QZSS) is a proposed four-satellite regional time transfer system and enhancement for GPS covering Japan and the Asia-Oceania regions. QZSS services are available on a trial basis as of January 12, 2018, and are scheduled to be launched in November 2018. The first satellite was launched in September 2010.\n\nSources: \n\nGNSS augmentation is a method of improving a navigation system's attributes, such as accuracy, reliability, and availability, through the integration of external information into the calculation process, for example, the Wide Area Augmentation System, the European Geostationary Navigation Overlay Service, the Multi-functional Satellite Augmentation System, Differential GPS, GPS Aided GEO Augmented Navigation (GAGAN) and inertial navigation systems.\n\nDoppler Orbitography and Radio-positioning Integrated by Satellite (DORIS) is a French precision navigation system. Unlike other GNSS systems, it is based on static emitting stations around the world, the receivers being on satellites, in order to precisely determine their orbital position. The system may be used also for mobile receivers on land with more limited usage and coverage. Used with traditional GNSS systems, it pushes the accuracy of positions to centimetric precision (and to millimetric precision for altimetric application and also allows monitoring very tiny seasonal changes of Earth rotation and deformations), in order to build a much more precise geodesic reference system.\n\nThe two current operational low Earth orbit satellite phone networks are able to track transceiver units with accuracy of a few kilometers using doppler shift calculations from the satellite. The coordinates are sent back to the transceiver unit where they can be read using AT commands or a graphical user interface. This can also be used by the gateway to enforce restrictions on geographically bound calling plans.\n\n\n\n\n"}
{"id": "404571", "url": "https://en.wikipedia.org/wiki?curid=404571", "title": "Spatial mismatch", "text": "Spatial mismatch\n\nSpatial mismatch is the mismatch between where low-income households reside and suitable job opportunities. In its original formulation (see below) and in subsequent research, it has mostly been understood as a phenomenon affecting African-Americans, as a result of residential segregation, economic restructuring, and the suburbanization of employment.\n\nSpatial mismatch was first proposed by John F. Kain in a seminal 1968 article, \"Housing Segregation, Negro Employment, and Metropolitan Decentralization\". That article did not specifically use the term \"spatial mismatch\", and Kain disclaimed credit. \n\nIn 1987, William Julius Wilson was an important exponent, elaborating the role of economic restructuring, as well as the departure of the black middle-class, in the development of a ghetto underclass in the United States.\n\nAfter World War I, many wealthy Americans started decentralizing out of the cities and into the suburbs. During the second half of the 20th century, department stores followed the trend of moving into the suburbs. In 1968, Kain formulated the “Spatial Mismatch Hypothesis”, but he did not refer to it by this term. His hypothesis was that black workers reside in segregated zones that are distant and poorly connected to major centers of growth. The phenomenon has many implications for inner-city residents dependent on low-level entry jobs. For example, distance from work centers can lead to increasing unemployment rates and further dampen poverty outcomes for the region at large.\n\nIn 2007, Laurent Gobillon, Harris Selod, and Yves Zenou suggested that there are seven different factors that support the spatial mismatch phenomenon. Four factors are attributed to potential workers accessibility and initiatives. The remaining three factors stress employers' reluctance to divert away from the negative stigma of city people and in particular minorities when hiring.\n\n\nGrowth of ghost cities in China, mostly from not yet agglomerated areas between or adjacent metropolitan areas or coal mining towns, as in the case of the most famous example, Kangbashi New Area of Ordos, are an example of spatial mismatch. In the case of places near metropolitan areas, it represents less of a risk going forward than in mining areas.\n\n"}
{"id": "31521300", "url": "https://en.wikipedia.org/wiki?curid=31521300", "title": "Survey of Israel", "text": "Survey of Israel\n\nSurvey of Israel - SOI (Hebrew: מפ\"י - המרכז למיפוי ישראל) is the survey and mapping department of the Israeli Ministry of Housing and Construction. \n\nThe British Mandate established the country's first survey department in 1920. In 1930, a building was erected to house the department at the southern end of a 30-acre plot near the German Templar cemetery in Tel Aviv. During World War II, it was used by the British Army. \nToday, the main office of the Survey of Israel is located in Tel Aviv with branches in Jerusalem, Be'er Sheva, Haifa and Nazareth.\n\nThe Survey of Israel is the government agency for Mapping, Geodesy, Cadastre and Geoinformatics. The Survey is responsible for the national infrastructure in these areas as well as for a number of official functions.\n\nIn the field of geodesy, the Survey operates as the National Geodetic Institute, adopting the reference ellipsoid, developing and maintaining the geodetic control network, defining the national datum, both horizontal and vertical and establishing an appropriate projection with accompanying mapping equations and transformations, for applications in mapping, cadastre and geoinformatics. \n\nIn the realm of cadastre the Survey leads activities leading to land registration (Land Surveys Department). The registration of rights to land is based on Torrens principles, which in effect provides a state guarantee of the rights registered. The Survey is responsible for defining boundaries of blocks and parcels in terms of coordinates and plans. All these procedures are anchored by Survey Ordinances. \n95% of the country's area has undergone the settlement process. All this area is subdivided into approximately 15,000 blocks and 800,000 parcels. The Survey deals every year with the continuation of settlement and also with new subdivision (re-parcellation) which express mutation in rights to land or its use, or both.\n\nThe Survey of Israel is the National Mapping Agency, responsible for defining mapping products required, with a special attention to construction infrastructures, security and emergency services, environmental protection, tourism and research and development. \nThese tasks require the establishment of standards, coordinate networks including specifications for map projections, accuracy standards, practice of toponymy and symbology. \nThe standard map series at the present is the 1:50,000, from which the 1:100,000 series is derived. The present editions of these series were produced by analogical methods and revised by digital methods. In addition, a number of 1:25,000 maps have been produced, all based on the National GIS digital data files, managed by the Survey. It would be appropriate to say that all map series are in the stage of transition to a digital cartographic process. Additional products worth mentioning are town plans and the Atlas of Israel being produced with the cooperation of the Hebrew University of Jerusalem. \n\nThe topic of geoinformatics has been entrusted to the Survey by government decisions of 1990 and 1993, meaning that the Survey is responsible for the National GIS databank, including a uniform set of codes to ensure compatibility and an interministerial forum which is a platform for discussion and formulation of policy. \nThe National GIS includes a topographic data bank, derived from aerial photographs at the scale 1:40,000 and periodically revised. The data bank consists of 10 data layers with appropriate codes. There is also a cadastral data bank, based on cadastral maps and plans. It is used to supply information and to manage cadastral operations, but it has no legal standing equal to the original documents. In addition, there is a bank of addresses, in larger population centers, managed in cooperation with the National Census Bureau.\n\nThe Israeli Spatial data infrastructure. ISPRS 2010 at: http://www.isprs.org/proceedings/XXXVIII/4_8_2-W9/papers/final_232_ISPRS_SFB_030810-srebro-Felus-.pdf\n\n"}
{"id": "18062821", "url": "https://en.wikipedia.org/wiki?curid=18062821", "title": "Table (landform)", "text": "Table (landform)\n\nA table or tableland is a butte, flank of a mountain, or mountain, that has a flat top. \n\nThis landform has numerous names in addition to \"table\", including:\n\nThe term \"flat\" is relative when speaking of tables, and often the naming or identification of a table (or table hill or mountain) is based on the appearance of the terrain feature from a distance or from below it. An example is Mesa Verde, Colorado, where the \"flat top\" of the mountain is both rolling terrain and cut by numerous deep canyons and arroyos, but whose rims appear quite flat from almost all directions, terminating in cliffs.\n\n"}
{"id": "24474291", "url": "https://en.wikipedia.org/wiki?curid=24474291", "title": "Tide-predicting machine", "text": "Tide-predicting machine\n\nA tide-predicting machine was a special-purpose mechanical analog computer of the late 19th and early 20th centuries, constructed and set up to predict the ebb and flow of sea tides and the irregular variations in their heights – which change in mixtures of rhythms, that never (in the aggregate) repeat themselves exactly. Its purpose was to shorten the laborious and error-prone computations of tide-prediction. Such machines usually provided predictions valid from hour to hour and day to day for a year or more ahead.\n\nThe first tide-predicting machine, designed and built in 1872-3, and followed by two larger machines on similar principles in 1876 and 1879, was conceived by Sir William Thomson (who later became Lord Kelvin). Thomson had introduced the method of harmonic analysis of tidal patterns in the 1860s and the first machine was designed by Thomson with the collaboration of Edward Roberts (assistant at the UK HM Nautical Almanac Office), and of Alexander Légé, who constructed it.\n\nIn the US, another tide-predicting machine on a different pattern (shown right) was designed by William Ferrel and built in 1881-2. Developments and improvements continued in the UK, US and Germany through the first half of the 20th century. The machines became widely used for constructing official tidal predictions for general marine navigation. They came to be regarded as of military strategic importance during World War I, and again during the Second World War, when the US No.2 Tide Predicting Machine, described below, was classified, along with the data that it produced, and used to predict tides for the D-day Normandy landings and all the island landings in the Pacific war. Military interest in such machines continued even for some time afterwards. They were made obsolete by digital electronic computers that can be programmed to carry out similar computations, but the tide-predicting machines continued in use until the 1960s and 1970s.\n\nSeveral examples of tide-predicting machines remain on display as museum pieces, occasionally put into operation for demonstration purposes, monuments to the mathematical and mechanical ingenuity of their creators.\n\nModern scientific study of tides dates back to Isaac Newton's 'Principia' of 1687, in which he applied the theory of gravitation to make a first approximation of the effects of the Moon and Sun on the Earth's tidal waters. The approximation developed by Newton and his successors of the next 90 years is known as the 'equilibrium theory' of tides.\n\nBeginning in the 1770s, Pierre-Simon Laplace made a fundamental advance on the equilibrium approximation by bringing into consideration non-equilibrium dynamical aspects of the motion of tidal waters that occurs in response to the tide-generating forces due to the Moon and Sun.\n\nLaplace's improvements in theory were substantial, but they still left prediction in an approximate state. This position changed in the 1860s when the local circumstances of tidal phenomena were more fully brought into account by William Thomson's application of Fourier analysis to the tidal motions. Thomson's work in this field was then further developed and extended by George Darwin: Darwin's work was based on the lunar theory current in his time. His symbols for the tidal harmonic constituents are still used. Darwin's harmonic developments of the tide-generating forces were later brought by A. T. Doodson up to date and extended in light of the new and more accurate lunar theory of E. W. Brown that remained current through most of the twentieth century.\n\nThe state to which the science of tide-prediction had arrived by the 1870s can be summarized: Astronomical theories of the Moon and Sun had identified the frequencies and strengths of different components of the tide-generating force. But effective prediction at any given place called for measurement of an adequate sample of local tidal observations, to show the local tidal response at those different frequencies, in amplitude and phase. Those observations had then to be analyzed, to derive the coefficients and phase angles. Then, for purposes of prediction, those local tidal constants had to be recombined, each with a different component of the tide-generating forces to which it applied, and at each of a sequence of future dates and times, and then the different elements finally collected together to obtain their aggregate effects. In the age when calculations were done by hand and brain, with pencil and paper and tables, this was recognized as an immensely laborious and error-prone undertaking.\n\nThomson recognized that what was needed was a convenient and preferably automated way to evaluate repeatedly the sum of tidal terms such as:\n\ncontaining 10, 20 or even more trigonometrical terms, so that the computation could conveniently be repeated in full for each of a very large number of different chosen values of the date/time formula_2. This was the core of the problem solved by the tide-predicting machines.\n\nThomson conceived his aim as to construct a mechanism that would evaluate this trigonometrical sum physically, e.g. as the vertical position of a pen that could then plot a curve on a moving band of paper.\nThere were several mechanisms available to him for converting rotary motion into sinusoidal motion. One of them is shown in the schematic (right). A rotating drive-wheel is fitted with an off-center peg. A shaft with a horizontally-slotted section is free to move vertically up and down. The wheel's off-center peg is located in the slot. As a result, when the peg moves around with the wheel, it can make the shaft move up and down within limits. This arrangement shows that when the drive-wheel rotates uniformly, say clockwise, the shaft moves sinusoidally up and down. The vertical position of the center of the slot, at any time formula_2, can then be expressed as\nformula_4, where formula_5 is the radial distance from the wheel's center to the peg, formula_6 is the rate at which the wheel turns (in radians per unit of time), and formula_7 is the starting phase angle of the peg, measured in radians from the 12 o'clock position to the angular position where the peg was at time zero.\n\nThis arrangement makes a physical analog of just one trigonometrical term. Thomson needed to construct a physical sum of many such terms.\n\nAt first he inclined to use gears. Then he discussed the problem with engineer Beauchamp Tower before the British Association meeting in 1872, and Tower suggested the use of a device that (as he remembered) was once used by Wheatstone. It was a chain running alternately over and under a sequence of pulleys on movable shafts. The chain was fixed at one end, and the other (free) end was weighted to keep it taut. As each shaft moved up or down it would take up or release a corresponding length of the chain. The movements in position of the free (movable) end of the chain represented the sum of the movements of the different shafts. The movable end was kept taut, and fitted with a pen and a moving band of paper on which the pen plotted a tidal curve. In some designs, the movable end of the line was connected instead to a dial and scale from which tidal heights could be read off.\nOne of Thomson's designs for the calculating part of a tide-predicting machine is shown in the figure (right), closely similar to the third machine of 1879-81. A long cord, with one end held fixed, passed vertically upwards and over a first upper pulley, then vertically downwards and under the next, and so on. These pulleys were all moved up and down by cranks, and each pulley took in or let out cord according to the direction in which it moved. These cranks were all moved by trains of wheels gearing into the wheels fixed on a drive shaft. The greatest number of teeth on any wheel was 802 engaging with another of 423. All the other wheels had comparatively small numbers of teeth. A flywheel of great inertia enabled the operator to turn the machine fast, without jerking the pulleys, and so to run off a year's curve in about twenty-five minutes. The machine shown in the figure was arranged for fifteen constituents in all.\n\nThomson acknowledged that the use of an over-and-under arrangement of the flexible line that summed the motion components was suggested to him in August 1872 by engineer Beauchamp Tower.\n\nAn online demonstration is available to show the principle of operation of a 7-component version of a tide-predicting machine otherwise like Thomson's (Kelvin's) original design. The animation shows part of the operation of the machine: the motions of several pulleys can be seen, each moving up and down to simulate one of the tidal frequencies; and the animation also shows how these sinusoidal motions were generated by wheel rotations and how they were combined to form the resulting tidal curve. Not shown in the animation is the way in which the individual motions were generated in the machine at the correct relative frequencies, by gearing in the correct ratios, or how the amplitudes and starting phase angles for each motion were set in an adjustable way. These amplitudes and starting phase angles represented the local tidal constants, separately reset, and different for each place for which predictions were to be made. Also, in the real Thomson machines, to save on motion and wear of the other parts, the shaft and pulley with the largest expected motion (for the M2 tide component at twice per lunar day) was mounted nearest to the pen, and the shaft and pulley representing the smallest component was at the other end, nearest to the point of fixing of the flexible cord or chain, to minimize unnecessary motion in the most part of the flexible cord.\n\nThe first tide predicting machine, designed in 1872 and of which a model was exhibited at the British Association meeting in 1873 (for computing 8 tidal components), followed in 1875-6 by a machine on a slightly larger scale (for computing 10 tidal components), was designed by Sir William Thomson (who later became Lord Kelvin). The 10-component machine and results obtained from it were shown at the Paris Exhibition in 1878. An enlarged and improved version of the machine, for computing 20 tidal components, was built for the Government of India in 1879, and then modified in 1881 to extend it to compute 24 harmonic components.\n\nIn these machines, the prediction was delivered in the form of a continuous graphical pen-plot of tidal height against time. The plot was marked with hour- and noon-marks, and was made by the machine on a moving band of paper as the mechanism was turned. A year's tidal predictions for a given place, usually a chosen seaport, could be plotted by the 1876 and 1879 machines in about four hours (but the drives had to be rewound during that time).\n\nIn 1881-2, another tide predicting machine, operating quite differently, was designed by William Ferrel and built in Washington under Ferrel's direction by E. G. Fischer (who later designed the successor machine described below, which was in operation at the US Coast and Geodetic Survey from 1912 until the 1960s). Ferrel's machine delivered predictions by telling the times and heights of successive high and low waters, shown by pointer-readings on dials and scales. These were read by an operator who copied the readings on to forms, to be sent to the printer of the US tide-tables.\n\nThese machines had to be set with local tidal constants special to the place for which predictions were to be made. Such numbers express the local tidal response to individual components of the global tide-generating potential, at different frequencies. This local response, shown in the timing and the height of tidal contributions at different frequencies, is a result of local and regional features of the coasts and sea-bed. The tidal constants are usually evaluated from local histories of tide-gauge observations, by harmonic analysis based on the principal tide-generating frequencies as shown by the global theory of tides and the underlying lunar theory.\n\nThomson was also responsible for originating the method of harmonic tidal analysis, and for devising a harmonic analyzer machine, which partly mechanized the evaluation of the constants from the gauge readings.\n\nDevelopment and improvement based on the experience of these early machines continued through the first half of the 20th century.\n\nBritish Tide Predictor No.2, after initial use to generate data for Indian ports, was used for tide prediction for the British empire beyond India, and transferred to the National Physical Laboratory in 1903. British Tide Predictor No.3 was sold to the French Government in 1900 and used to generate French tide tables.\n\nUS Tide Predicting Machine No. 2 (\"Old Brass Brains\") was designed in the 1890s, completed and brought into service in 1912, used for several decades including during the second World War, and retired in the 1960s.\n\nTide-predicting machines were built in Germany during World War I, and again in the period 1935-8.\n\nThree of the last to be built were: \nExcluding small portable machines, a total of 33 tide-predicting machines are known to have been built, of which 2 have been destroyed and 4 are presently lost.\n\nThey can be seen in London, Washington, Liverpool, and elsewhere, including the Deutsches Museum in Munich.\n\n\n"}
{"id": "41349488", "url": "https://en.wikipedia.org/wiki?curid=41349488", "title": "Tienstra formula", "text": "Tienstra formula\n\nThe Tienstra formula is used to solve the resection problem in surveying, by which the location of a given point is determined by observations of angles to known landmarks from the unknown point.\n\nJ.M.Tienstra (1895-1951) was a professor of the Delft university of Technology where he taught the use of barycentric coordinates in solving the resection problem. It seems most probable that his name became attached to the procedure for this reason, though when, and by whom, the formula was first proposed is unknown.\n\nThe resection problem consists in finding the location of an observer by measuring the angles subtended by lines of sight from the observer to three known points. Tienstra’s formula provides the most compact and elegant solution to this problem.\nformula_1\n\nformula_2\n\nWhere:\nformula_3\nformula_4\nformula_5\n\n"}
{"id": "8331945", "url": "https://en.wikipedia.org/wiki?curid=8331945", "title": "Ultra-short baseline", "text": "Ultra-short baseline\n\nUSBL (ultra-short baseline, also sometimes known as SSBL for super short base line) is a method of underwater acoustic positioning. A complete USBL system consists of a transceiver, which is mounted on a pole under a ship, and a transponder or responder on the seafloor, on a towfish, or on an ROV. A computer, or \"topside unit\", is used to calculate a position from the ranges and bearings measured by the transceiver.\n\nAn acoustic pulse is transmitted by the transceiver and detected by the subsea transponder, which replies with its own acoustic pulse. This return pulse is detected by the shipboard transceiver. The time from the transmission of the initial acoustic pulse until the reply is detected is measured by the USBL system and is converted into a range.\n\nTo calculate a subsea position, the USBL calculates both a range and an angle from the transceiver to the subsea beacon. Angles are measured by the transceiver, which contains an array of transducers. The transceiver head normally contains three or more transducers separated by a baseline of 10 cm or less. A method called “phase-differencing” within this transducer array is used to calculate the direction to the subsea transponder.\n\nUSBLs have also begun to find use in \"inverted\" (iUSBL) configurations, with the transceiver mounted on an autonomous underwater vehicle, and the transponder on the target. In this case, the \"topside\" processing happens inside the vehicle to allow it to locate the transponder for applications such as automatic docking and target tracking.\n\n\n"}
{"id": "15131800", "url": "https://en.wikipedia.org/wiki?curid=15131800", "title": "Vautrin Lud Prize", "text": "Vautrin Lud Prize\n\nThe Prix International de Géographie Vautrin Lud, known in English as the Vautrin Lud Prize, is the highest award in the field of geography. Established in 1991, the award is modeled on the Nobel Prize, and colloquially called the \"Nobel Prize for Geography\". The award is named after the 16th Century French scholar Vautrin Lud who is credited with naming the New World America after Amerigo Vespucci. The award is given in the autumn of each year at the International Geography Festival in Saint-Dié-des-Vosges, France (the home town of Vautrin Lud) and decided upon by a five-person international jury.\n\n\n"}
{"id": "47319212", "url": "https://en.wikipedia.org/wiki?curid=47319212", "title": "Ymer (journal)", "text": "Ymer (journal)\n\nYmer is an annual peer-reviewed academic journal published by the Swedish Society for Anthropology and Geography. It was established in 1881 and published quarterly until 1965, when it converted to an annual rhythm. The journal is abstracted and indexed in Scopus.\n\n"}
