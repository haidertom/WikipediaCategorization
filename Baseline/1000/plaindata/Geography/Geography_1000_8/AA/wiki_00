{"id": "594386", "url": "https://en.wikipedia.org/wiki?curid=594386", "title": "Age of Sail", "text": "Age of Sail\n\nThe Age of Sail (usually dated as 1571–1862) was a period roughly corresponding to the early modern period in which international trade and naval warfare were dominated by sailing ships, lasting from the 16th to the mid-19th century. \n\nLike most periodic eras the definition is inexact but close enough to serve as a general description. The age of sail runs roughly from the Battle of Lepanto in 1571, the last significant engagement in which oar-propelled galleys played a major role, to the Battle of Hampton Roads in 1862, in which the steam-powered ironclad CSS \"Virginia\" destroyed the sailing ships USS \"Cumberland\" and USS \"Congress\", demonstrating that the advance of steam power had rendered sail power in warfare obsolete.\n\nThe Suez Canal, in the Middle-East, which opened in 1869, was impractical for sailing ships, and made steamboats faster on the European-Asian sea route.\n\nThe period between 1850 and the early 20th century when sailing vessels reached their peak of size and complexity is sometimes referred to as the \"Golden Age of Sail\". During this time the efficiency and use of commercial sailing vessels was at its peak—immediately before steamboats started to take trade away from sail.\n\nSailing ships sometimes continued to be an economical way to transport bulk cargo on long voyages into the 1920s, even if steamships also were used for such transports and became more and more common. Sailing ships do not require fuel or complex engines to be powered; thus they tended to be more independent from requiring a dedicated support base on the mainland. Crucially though, steam-powered ships held a speed advantage and were rarely hindered by adverse winds, freeing steam-powered vessels from the necessity of following trade winds. As a result, cargo and supplies could reach a foreign port in half the time it took a sailing ship. \n\nSailing vessels were pushed into narrower and narrower economic niches and gradually disappeared from commercial trade. Today, sailing vessels are only economically viable for small scale coastal fishing, along with recreational uses such as yachting and passenger sail excursion ships.\n\n"}
{"id": "1256241", "url": "https://en.wikipedia.org/wiki?curid=1256241", "title": "American Cordillera", "text": "American Cordillera\n\nThe American Cordillera is a chain of mountain ranges (cordilleras) that consists of an almost continuous sequence of mountain ranges that form the western \"backbone\" of North America, South America and Antarctica. It is also the backbone of the volcanic arc that forms the eastern half of the Pacific Ring of Fire.\n\nFrom north to south, this sequence of overlapping and parallel ranges begins with the Alaska Range and the Brooks Range in Alaska and runs through the Yukon into British Columbia. The main belt of the Rocky Mountains along with the parallel Columbia Mountains and Coast Ranges of mountains and islands continue through British Columbia and Vancouver Island. In the United States, the Cordillera branches include the Rockies, the Sierra Nevada, the Cascades, and various small Pacific coastal ranges. In Mexico, the Cordillera continues through the Sierra Madre Occidental and Sierra Madre Oriental, as well as the backbone mountains of the Baja California peninsula.\n\nThe ranges of the Cordillera from Mexico northwards are collectively called the North American Cordillera or Western Cordillera in the United States and Canada, and also named as the Canadian Cordillera or Pacific Cordillera in Canada.\n\nThe Cordillera continues on through the mountain ranges of Central America in Guatemala, Honduras, Nicaragua, Costa Rica, and Panama, and becomes the Andes Mountains of South America. The Andes with their parallel chains and the island chains off the coast of Chile continue through Colombia, Venezuela, Ecuador, Peru, Bolivia, Argentina, and Chile to the very tip of South America at Tierra del Fuego. The Cordillera continues along the Scotia Arc before reaching the mountains of the Antarctic Peninsula.\n"}
{"id": "1965097", "url": "https://en.wikipedia.org/wiki?curid=1965097", "title": "Autonomous province", "text": "Autonomous province\n\nAutonomous province is term for a type of administrative territorial entity. \nTwo autonomous provinces exist:\n\nTwo autonomous provinces exist:\n\n"}
{"id": "7275586", "url": "https://en.wikipedia.org/wiki?curid=7275586", "title": "Boundary marker", "text": "Boundary marker\n\nA boundary marker, border marker, boundary stone, or border stone is a robust physical marker that identifies the start of a land boundary or the change in a boundary, especially a change in direction of a boundary. There are several other types of named border markers, known as pillars, obelisks, and corners. Border markers can also be markers through which a border line runs in a straight line to determine that border. They can also be the markers from which a border marker has been fixed.\n\nAccording to Josiah Ober, boundary markers are \"a way of imposing human, cultural, social meanings upon a once-undifferentiated natural environment.\" Boundary markers are linked to social hierarchies, since they derive their meaning from the authority of a person or group to declare the limits of a given space of land for political, social or religious reasons. Ober notes that \"determining who can use parcels of arable land and for what purpose, has immediate and obvious economic implications.\"\n\nMany borders were drawn along invisible lines of latitude or longitude, which often created a need to mark these borders on the ground, as accurately as possible, using the technology of the day. Advances in GPS technology have shown that there are many borders inaccurately marked on the ground.\n\nBoundary markers have often been used to mark critical points on political boundaries, i.e. those between countries, states or local administrations, but have also been used to mark out the limits of private landholdings, especially in areas where fences or walls are impractical or unnecessary.\n\nIn developed countries the use of markers for land ownership has in many places been replaced by maps and land ownership registration. Boundary markers are not legal markers in Western countries and may have troublesome legal effects. However, boundary markers have legal meaning in Japan, and are generally installed across the country. Markers are still used extensively for marking international borders, which are traditionally classified into two categories: natural boundaries, correlating to topographical features such as rivers or mountain ranges, and artificial boundaries, which have no obvious relation to topography. The latter category includes borders defined by boundary markers such as stones and walls. International boundary markers are placed and can be maintained by mutual agreement of the bordering countries.\n\nBoundary markers, traditionally, were often made of stone, but later many have been made with concrete or a mixture of materials. They are typically placed at a notable or especially visible point. Many are inscribed with relevant information such as the abbreviation of the boundary holder and often a date.\n\nThe oldest known boundary stone in China is from Jiangsu Province. Dating from 12 A.D., it bears the inscription \"the sea area from Jiaozhou Bay to the east of Guixan county belongs to Langya Shire and the waters from the south of Guixan county to the east of the estuary of Guanhe River belongs to Donghai Shire.\" More recently, the border between Russia and China was formally demarcated with boundary stones as the result of the Treaty of Kiakhta in 1727. In the nineteenth century, stones were used to outline the limits of the International Settlement in Shanghai.\n\nIn ancient Thailand, sacred boundary stones called \"Sema Hin\" delimited Buddhist temple precincts. In some cases they feature inscriptions recounting the history of the temple; others were carved with wheels of the law, while some specimens consist of unfinished stone. In addition to temples, \"sema\" could enclose statues of Buddha or sacred mounds.\n\nAccording to B. S. Jackson, stones were put in place in ancient Israel to \"mark the boundary of a territory (public or private), and to seek to deter potential violators of that boundary through the use of threats.\" The Hebrew Bible contains a strict prohibition against the unauthorized displacement or removal of boundary markers.\n\nAn example of boundary markers in ancient Egypt were the boundary stelae of Akhenaten. They defined the limits of the sacred city of Akhet-Aten, built by Akhenaten as the center of the Aten religious cult which he founded. Egyptologists categorize the stelae based on whether they are inscribed with the \"Earlier Proclamation,\" a general explanation of why the location was selected and how the city would be designed, or the \"Later Proclamation,\" which provides additional details about the perimeters of the city.\n\nGlacial erratics and similar natural stones were often used as boundary markers between properties. Knowledge of their locations was typically maintained by oral tradition, wherein men of each house would walk the length of the border. These stones then became boundary markers for municipalities, and eventually provinces and countries. For example, Kuhankuono is a stone that marks the multipoint border between seven municipalities in Kurjenrahka National Park near Turku. Today, however, steel rods topped with a cube painted orange are usually used. Municipalities often post a traffic sign featuring their coat of arms on the border on major roads. On the Finnish-Russian border, many historical border stones, marked with Swedish and Imperial Russian symbols, are still in use. The actual Finnish-Russian border is marked by small white bollard, but on both sides of the border there are large striped bollards decorated with a coat of arms: a blue/white bollard on the Finnish side, a red/green bollard on the Russian side. Artificial cairns are found on the Norway-Russia-Finland tripoint (Treriksrøysa) and Norway-Sweden-Finland tripoint (Three-Country Cairn). The Sweden-Finland border on Märket is marked with holes drilled to the rock, because seasonal pack ice can shear off any protruding markers. In folklore, a type of haltija, \"rajahaltija\", a kind of a local spirit, was believed to haunt borders that had been unjustly moved.\n\nThe earliest reference to a boundary stone in Greek literature is in the \"Iliad\", which describes the goddess Athena using one as a projectile. Boundary stones, known as \"horos\", could be made of either carved or undressed stones, and were typically inscribed with the Greek word \"horos\". One such stone was used to indicate the edge of the Athenian \"agora\". The practice of separating areas of land with boundary stones, though common, was widely considered by classical writers to be a violation of the principle of communal land ownership.\n\nIn ancient Roman religion, the god Terminus was worshiped as the patron god of boundary markers. Ovid, in a hymn directed to the god, wrote: \"O Terminus, whether thou art a stone or a stump buried in the field, … thou dost set bounds to people and cities and vast kingdoms\". Numa Pompillius made the first Roman law requiring boundary stones around private property and instituting capital punishment for anyone found guilty of moving these stones.\n\nIn 1828, the Principality of Monaco and the Kingdom of Piedmont-Sardinia established a physical border with 91 boundary stones, each numbered 1 to 91, running along the border from present-day Fontvieille to Menton-Garavan. Prior to 1848, the Principality of Monaco included the villages of Roquebrune, Monti, Garavan and Menton. Of the original 91 boundary stones only 12 remain: 6 within the Principality of Monaco, 3 in Roquebrune-Cap-Martin, and 3 in Menton. The boundary stones numbered 9, 12, 15, and 31 are located in Monaco. Another stone has been cast in concrete in the Sainte-Cécile area of Monaco thus rendering its number illegible. Stone number 55, originally located in Roquebrune, was given as a gift from the city of Roquebrune to the Principality of Monaco and is now located in Monaco's city hall. Stones numbered 56, 57, and 58 are located in Roquebrune. Stones numbered 62, 71, and 73 are located in Menton. All the boundary stones have three engraved sides: one side with their individual numbers (1 to 91), one side with the letter \"M\" indicating Monaco's territory, and one side with a cross (+) indicating the Kingdom of Piedmont-Sardinia's territory. The cross represents the coat of arms of the House of Savoy, rulers of Piedmont-Sardinia.\n\nThe history of marking the Western Australian border on the ground states that the \"Austral Pillar\" and the \"Deakin Pillar\" are points used to determine their position east of Greenwich and then fix a border from, in this case used to determine the line of the 129th meridian east longitude, as the Western Australian border. The Deakin Obelisk and the Kimberley Obelisk in Australia are used in a slightly different way, in that a line is run north and south through a point on the obelisks, formed by a copper plug embedded into the top centre of the concrete obelisks. The \"corners\" in Australia, such as Cameron Corner, Haddon Corner, Poeppel Corner, and Surveyor Generals Corner, are where multiple borders meet or a border changes direction.\n\nThe basic unit of the ancient Hawaiian land division system was the \"ahupua'a\", a self-sustaining agricultural district. The places where a road crossed the border of an \"ahupua'a\" were marked with distinctive altars, known as \"ahu\" or (stone) piles. These altars served not only as boundary markers but also as sites for the performance of religious rituals related to land taxation. C. J. Lyons, an early surveyor of Hawaii, recorded that „[u]pon this altar at the annual progress of the akua makahiki (year god) was deposited the tax paid by the land whose boundary it marked, and also an image of a hog, \"puaa\", carved out of kukui wood and stained with red ochre. … [F]rom this came the name, \"ahupuaa\"“. Naturally occurring landscape features were also used as points of reference for district borders.\n\nThe original boundaries of the District of Columbia were marked using boundary stones. These were made of saw-cut sandstone blocks and stood two feet high when set in the ground. Ten boundary stones were placed along each side of the 100 square mile (259 square kilometer) district of Columbia. Although the original surveyors intended each side to be ten miles (16 kilometers) long, their measurements were often inaccurate, resulting in the sometimes significant misplacement of stones and the overall skewing of the District boundaries. Some of these discrepancies are intentional, because the ground at the exact mile point was covered in water; “in such cases,” Andrew Ellicott, the leader of the surveying crew, noted in 1793, “the stones are placed on the nearest firm ground and the true distance in miles and poles is marked on them”.\n\nInformation engraved on the stones includes the number (1 through 10) of the stone within the sequence on that side of the District, the date of placement, and the words \"Jurisdiction of the United States.\" In the twentieth century, the Daughters of the American Revolution voluntarily took responsibility for preserving the stones, which had fallen victim to vandalization and urban development. In the late 1990s renewed interest in the boundary stones led to increased preservation efforts by the DAR and other organizations.\n\nIn 1773, a Franciscan friar named Francisco Palou erected the first boundary marker between Alta and Baja California. Commissioned by the Spanish Crown, it consisted of a cross made from alder wood and placed standing upright on a rock. In the British colonies, milestones were shipped from England to mark the Mason–Dixon line. A block cut from sandstone was placed at the intersection of Wyoming, Colorado and Utah in 1879, and stone posts were used along the western border of South Dakota. Boundaries were occasionally resurveyed and boundary stones replaced or restored, depending on their condition.\n\nDreieckiger Pfahl, Germany\n\n\n \n"}
{"id": "54471473", "url": "https://en.wikipedia.org/wiki?curid=54471473", "title": "Cartographica", "text": "Cartographica\n\nThe Cartographica is an interdisciplinary peer-reviewed academic journal and the official publication of the Canadian Cartographic Association. \"Cartographica\" is published four times a year by the University of Toronto Press.\n\nThe journal is abstracted and indexed in:\n"}
{"id": "1527098", "url": "https://en.wikipedia.org/wiki?curid=1527098", "title": "Clairaut's theorem", "text": "Clairaut's theorem\n\nClairaut's theorem is a general mathematical law giving the surface gravity on a viscous rotating ellipsoid in equilibrium under the action of its gravitational field and centrifugal force. It was published in 1743 by Alexis Claude Clairaut in his \"Théorie de la figure de la terre, tirée des principes de l'hydrostatique\" (\"Theory of the shape of the earth, drawn from the principles of hydrostatics\") which synthesized physical and geodetic evidence that the Earth is an oblate rotational ellipsoid. It was initially used to relate the gravity at any point on the Earth's surface to the position of that point, allowing the ellipticity of the Earth to be calculated from measurements of gravity at different latitudes. Today it has been largely supplanted by the Somigliana equation.\n\nAlthough it had been known since antiquity that the Earth was spherical, by the 17th century evidence was accumulating that it was not a perfect sphere. In 1672 Jean Richer found the first evidence that gravity was not constant over the Earth (as it would be if the Earth were a sphere); he took a pendulum clock to Cayenne, French Guiana and found that it lost minutes per day compared to its rate at Paris. This indicated the acceleration of gravity was less at Cayenne than at Paris. Pendulum gravimeters began to be taken on voyages to remote parts of the world, and it was slowly discovered that gravity increases smoothly with increasing latitude, gravitational acceleration being about 0.5% greater at the poles than at the equator.\n\nBritish physicist Isaac Newton explained this in his \"Principia Mathematica\" (1687) in which he outlined his theory and calculations on the shape of the Earth. Newton theorized correctly that the Earth was not precisely a sphere but had an oblate ellipsoidal shape, slightly flattened at the poles due to the centrifugal force of its rotation. Since the surface of the Earth is closer to its center at the poles than at the equator, gravity is stronger there. Using geometric calculations, he gave a concrete argument as to the hypothetical ellipsoid shape of the Earth.\n\nThe goal of \"Principia\" was not to provide exact answer for natural phenomena, but to theorize potential solutions to these unresolved factors in science. Newton pushed for scientists to look further into the unexplained variables. Two prominent researchers that he inspired were Alexis Clairaut and Pierre Louis Maupertuis. They both sought to prove the validity of Newton's theory on the shape of the Earth. In order to do so, they went on an expedition to Lapland in an attempt to accurately measure the meridian arc. From such measurements they could calculate the eccentricity of the Earth, its degree of departure from a perfect sphere. Clairaut confirmed that Newton's theory that the Earth was ellipsoidal was correct, but his calculations were in error, and wrote a letter to the Royal Society of London with his findings. The society published an article in Philosophical Transactions the following year in 1737 that revealed his discovery. Clairaut showed how Newton's equations were incorrect, and did not prove an ellipsoid shape to the Earth. However, he corrected problems with the theory, that in effect would prove Newton's theory correct. Clairaut believed that Newton had reasons for choosing the shape that he did, but he did not support it in \"Principia.\" Clairaut's article did not provide an valid equation to back up his argument as well. This created much controversy in the scientific community.\n\nIt was not until Clairaut wrote \"Théorie de la figure de la terre\" in 1743 that a proper answer was provided. In it, he promulgated what is more formally known today as Clairaut's theorem.\n\nClairaut's formula for the acceleration due to gravity \"g\" on the surface of a spheroid at latitude φ, was:\n\nwhere formula_2 is the value of the acceleration of gravity at the equator, \"m\" the ratio of the centrifugal force to gravity at the equator, and \"f\" the flattening of a meridian section of the earth, defined as:\n(where \"a\" = semimajor axis, \"b\" = semiminor axis).\n\nClairaut derived the formula under the assumption that the body was composed of concentric coaxial spheroidal layers of constant density. \nThis work was subsequently pursued by Laplace, who relaxed the initial assumption that surfaces of equal density were spheroids.\nStokes showed in 1849 that the theorem applied to any law of density so long as the external surface is a spheroid of equilibrium. A history of the subject, and more detailed equations for \"g\" can be found in Khan.\n\nThe above expression for \"g\" has been supplanted by the Somigliana equation (after Carlo Somigliana):\n\nwhere,\n\nFor Earth, formula_2 = 9.7803253359 ms; formula_9 = 9.8321849378 ms; \"k\" = 0.00193185265241 ; \"e\" = 0.00669437999013: \n\nThe spheroidal shape of the Earth is the result of the interplay between gravity and centrifugal force caused by the Earth's rotation about its axis. In his \"Principia\", Newton proposed the equilibrium shape of a homogeneous rotating Earth was a rotational ellipsoid with a flattening \"f\" given by 1/230. As a result, gravity increases from the equator to the poles. By applying Clairaut's theorem, Laplace found from 15 gravity values that \"f\" = 1/330. A modern estimate is 1/298.25642. See Figure of the Earth for more detail.\n\nFor a detailed account of the construction of the reference Earth model of geodesy, see Chatfield.\n"}
{"id": "3394642", "url": "https://en.wikipedia.org/wiki?curid=3394642", "title": "Dioptra", "text": "Dioptra\n\nA dioptra (sometimes also named dioptre or diopter from ) is a classical astronomical and surveying instrument, dating from the 3rd century BCE. The dioptra was a sighting tube or, alternatively, a rod with a sight at both ends, attached to a stand. If fitted with protractors, it could be used to measure angles.\n\nGreek astronomers used the dioptra to measure the positions of stars; both Euclid and Geminus refer to the dioptra in their astronomical works. By the time of Ptolemy (2nd century CE), it was obsolete as an astronomical instrument, having been replaced by the armillary sphere.\n\nIt continued in use as an effective surveying tool. Adapted to surveying, the dioptra is similar to the theodolite, or surveyor's transit, which dates to the sixteenth century. It is a more accurate version of the groma.\n\nThe dioptra may have been sophisticated enough, for example, to construct a tunnel through two opposite points in a mountain. There is some speculation that it may have been used to build the Eupalinian aqueduct. Called \"one of the greatest engineering achievements of ancient times,\" it is a tunnel 1,036 meters (4,000 ft) long, \"excavated through Mount Kastro on the Greek island of Samos, in the 6th century BCE\" during the reign of Polycrates. Scholars disagree whether the dioptra was available that early.\n\nAn entire book about the construction and surveying usage of the dioptra is credited to Hero of Alexandria (also known as Heron; a brief description of the book is available online; see Lahanas link, below). Hero was \"one of history’s most ingenious engineers and applied mathematicians.\"\n\nThe dioptra was used extensively on aqueduct building projects. Screw turns on several different parts of the instrument made it easy to calibrate for very precise measurements\n\nThe dioptra was replaced as a surveying instrument by the theodolite.\n\n\n\n"}
{"id": "6786225", "url": "https://en.wikipedia.org/wiki?curid=6786225", "title": "Distance decay", "text": "Distance decay\n\nDistance decay is a geographical term which describes the effect of distance on cultural or spatial interactions. The distance decay effect states that the interaction between two locales declines as the distance between them increases. Once the distance is outside of the two locales' activity space, their interactions begin to decrease.\n\nWith the advent of faster travel, distance has less effect than it did in the past, except where places previously connected by now-abandoned railways, for example, have fallen off the beaten path. Advances in communications technology, such as telegraphs, telephones, broadcasting, and internet, have further decreased the effects of distance.\n\nDistance decay is graphically represented by a curving line that swoops concavely downward as distance along the x-axis increases. Distance decay can be mathematically represented as an Inverse-square law by the expression\n\nformula_1 \nor\nformula_2,\n\nwhere I is interaction and d is distance. It can take other forms such as negative exponential, i.e.\n\nformula_3\n\nDistance decay is evident in town/city centres. It can refer to various things which decline with greater distance from the center of the Central Business District (CBD):\n\nDistance decay weighs into the decision to migrate, leading many migrants to move less far.\n\nRelated terms include \"friction of distance\", which describes the force that creates distance decay and Waldo R. Tobler's \"First law of geography\", an informal statement that \"All things are related, but near things are more related than far things.\" \n\"Loss of Strength Gradient\" holds that the amount of a nation's military power that could be brought to bear in any part of the world depends on geographic distance.\n\n\n"}
{"id": "357568", "url": "https://en.wikipedia.org/wiki?curid=357568", "title": "Dominion Land Survey", "text": "Dominion Land Survey\n\nThe Dominion Land Survey (DLS) is the method used to divide most of Western Canada into one-square-mile (2.6 km) sections for agricultural and other purposes. It is based on the layout of the Public Land Survey System used in the United States, but has several differences. The DLS is the dominant survey method in the Prairie provinces, and it is also used in British Columbia along the Railway Belt (near the main line of the Canadian Pacific Railway), and in the Peace River Block in the northeast of the province. (Although British Columbia entered Confederation with control over its own lands, unlike the Northwest Territories and the Prairie provinces, British Columbia transferred these lands to the federal Government as a condition of the building of the Canadian Pacific Railway. The federal government then surveyed these areas under the DLS.)\n\nThe survey was begun in 1871, shortly after Manitoba and the North-West Territories became part of Canada, following the purchase of Rupert's Land from the Hudson’s Bay Company. Covering about , the survey system and its terminology are deeply ingrained in the rural culture of the Prairies. The DLS is the world's largest survey grid laid down in a single integrated system. The first formal survey done in western Canada was by Peter Fidler in 1813.\n\nThe inspiration for the Dominion Land Survey System was the plan for Manitoba (and later Saskatchewan and Alberta) to be agricultural economies. With a large amount of European settlers arriving, Manitoba was undergoing a large change so grasslands and parklands were surveyed, settled, and farmed. The Dominion Land Survey system was developed because the farm name and field position descriptions used in northern Europe were not organized or flexible enough, and the township and concession system used in eastern Canada was not satisfactory. The first meridian was chosen at 97°27′28.4″ west longitude and was established in 1869. Another 6 meridians were established after.\n\nA number of places are excluded from the survey system: these include federal lands such as Indian reserves, federal parks, and air weapon ranges. The surveys do not encroach on reserves because that land was established before the surveys began. When the Hudson's Bay Company relinquished their title to the Dominion on July 15, 1870, via the deed of surrender it received Section 8 and all of Section 26 excluding the northeast quarter. These lands were gradually sold by the company and in 1984 they donated the remaining to the Saskatchewan Wildlife Association.\n\nThe surveying of western Canada was divided into five basic surveys. Each survey's layout was slightly different from the others. The first survey began in 1871 and ended in 1879 and covers some of southern Manitoba and a little of Saskatchewan. The second and smallest survey, in 1880, was used in only small areas of Saskatchewan. This system differs from the first survey because rather than running section lines parallel to the eastern boundary they run true north-south. The largest and most important of these surveys was the third which covers more land than all the others surveys put together. This survey began in 1881. That method of surveying is still used in Saskatchewan and Manitoba. The fourth and fifth surveys were used only in some townships in British Columbia.\n\nThe reason that the Canadian government was pushing to subdivide Manitoba, Saskatchewan, and Alberta was to affirm Canadian sovereignty over these lands. The United States was undergoing rapid expansion in the 1860s, and the Canadian government was afraid that the Americans would expand into Canadian territory. Canada's introduction of a railway and surveying was a means to discourage American encroachment. Sir John A. Macdonald remarked in 1870 that the Americans \"are resolved to do all they can short of war, to get possession of our western territory, and we must take immediate and vigorous steps to counteract them.\"\n\nThe beginning of the Dominion Land Survey marked a new era for western Canada. Railways were making their way to the West and the population of western regions began to increase. The introduction of the survey system marked the end of the nomadic ways for the First Nations and Metis. This did not go over well and was a catalyst to the events of the Red River Rebellion.\n\nBeing a surveyor was not easy. The hours were long, the time away from civilization was longer, and the elements were unforgiving. A survey party generally consisted of up to 20 members, which would include a party chief, chain men, a cook, people to saw trees, a recorder, and people to turn angles. All travel was either on horseback or by foot. To begin surveying a party chief would have to buy approximately $400 worth of instruments. These instruments included an alidade, dumpy level, theodolite, Gunter's chain (which was replaced by a steel tape), and a solar compass or a vernier compass.\n\nThe Dominion Land Survey system was proposed in 1869 by John Stoughton Dennis. The initial plan, though based on the square townships of the American Public Lands Survey System, involved 9 mile townships divided into sixty-four 800 acre sections consisting of four 200 acre lots each. Work to establish the first meridian and few township outlines began and quickly ended in 1869 when a party of Metis symbolically stepped on a survey chain, beginning the Red River Resistance. Work resumed in 1871; however, the system was redesigned to use 6 mile townships with 640 acre sections based on a suggestion from Lieutenant-Governor of the North-West Territories William McDougall, who advocated that most of the settlers would come from the United States, so it was \"advisable to offer them lots of a size to which they have been accustomed.\" The Dominion Land Survey System still differed from the Public Land System because it contained road allowances.\n\nThe Dominion Land Survey was enormous. Around are estimated to have been subdivided into quarter sections, 27 million of which were surveyed by 1883 (14 years after the system's inception). The amount of work undertaken between 1871 and 1930 is given justice by the amount of paper work submitted: the maps, plans and memos transferred by the Canadian government to the provinces filled approximately 200 railway cars. This did not include the closed or dormant files which would be enough to fill 9000 filing cabinets, which would weigh about 227 tons.\n\nUntil very recently, surveying was done with manually controlled instruments to take distance and angular measurements. Distance was measured using either a chain or more recently a transit or range finder. To turn angles a theodolite was used. To find their location they used astronomical observations, and to find elevation levels and barometers were used. In order to see over long distances towers were constructed from timber in flat and wooded areas.\n\nThe most important north–south lines of the survey are the meridians:\n\nThe meridians were determined by painstaking survey observations and measurements, and in reference to other benchmarks on the continent, but were determined using 19th century technology. The only truly accurate benchmarks at that time were near the prime meridian in Europe. Benchmarks in other parts of the world had to be calculated or estimated by the positions of the sun and stars. Consequently, although they were remarkably accurate for the time, today they are known to be several hundred metres in error. Before the survey was even completed it was established that for the purposes of laws based on the survey, the results of the physical survey would take precedence over the theoretically correct position of the meridians. This precludes, for example any basis for a boundary dispute between Alberta and Saskatchewan on account of surveying errors.\n\nThe main east–west lines are the baselines. The First Baseline is at 49° north, which forms much of the Canada–United States border in the West. Each subsequent baseline is about to the north of the previous one, terminating at 60° north, which forms the boundary with Yukon, the Northwest Territories, and Nunavut.\n\nStarting at each intersection of a meridian and a baseline and working west (also working east of the First Meridian and the Coast Meridian), nearly square townships are surveyed, which are about in both north–south and east–west extent. There are two tiers of townships to the north and two tiers to the south of each baseline.\n\nBecause the east and west edges of townships, called \"range lines\", are meridians of longitude, they converge towards the North Pole. Therefore, the north edge of every township is slightly shorter than the south. Only along the baselines do townships have their nominal width from east to west. The two townships to the north of a baseline gradually narrow as one moves north, and the two to the south gradually widen as one moves south. Halfway between two base lines, wider-than-nominal townships abut narrower-than-nominal townships. The east and west boundaries of these townships therefore do not align, and north–south roads that follow the survey system have to jog to the east or west. These east–west lines halfway between baselines are called \"correction lines\".\n\nTownships are designated by their \"township number\" and \"range number\". Township 1 is the first north of the First Baseline, and the numbers increase to the north. Range numbers recommence with Range 1 at each meridian and increase to the west (also east of First Meridian and Coast Meridian). On maps, township numbers are marked in Arabic numerals, but range numbers are often marked in Roman numerals; however, in other contexts Arabic numerals are used for both. Individual townships are designated such as \"Township 52, Range 25 west of the Fourth Meridian,\" abbreviated \"52-25-W4.\" In Manitoba, the First Meridian is the only one used, so the abbreviations are even more terse, e.g., \"3-1-W\" and \"24-2-E.\". In Manitoba legislation, the abbreviations WPM and EPM are used: \"3-1 WPM\" and \"24-2 EPM\".\n\nEvery township is divided into 36 sections, each about square. Sections are numbered within townships, beginning with the southeast section, as follows (north at top):\nIn turn, each section is divided into four quarter sections (square land parcels roughly 1/2 mile on a side): southeast, southwest, northwest and northeast. This quarter-section description is primarily used by the agricultural industry. The full legal description of a particular quarter section is \"the Northeast Quarter of Section 20, Township 52, Range 25 west of the Fourth Meridian\", abbreviated \"NE-20-52-25-W4.\"\n\nA section may also be split into as many as 16 legal subdivisions (LSDs). LSDs are commonly used by the oil and gas industry as a precise way of locating wells, pipelines, and facilities. LSDs can be \"quarter-quarter sections\" (square land parcels roughly on a side, comprising roughly in area)—but this is not necessary. Many are other fractions of a section (a half-quarter section—roughly in area is common.) LSDs may be square, rectangular, and occasionally even triangular. LSDs are numbered as follows (north at top):\nIn order to fully understand how the townships, sections, quarter sections, and legal subdivisions were set out, one should refer to the \"Manual of Instructions for the Survey of Dominion Land\".\n\nOccasionally, resource companies assign further divisions within LSDs such as \"A, B, C, D etc.\" for example, to distinguish between multiple sites within an LSD. These in no way constitute an official change to the Dominion Land Survey system, but nonetheless often appear as part of the legal description.\n\nBetween certain sections of a township run \"road allowances\" (but not all road allowances have an actual road built on them). The road allowances add to the size of the township (they do not cut down the size of the sections): this is the reason base lines are not exactly apart. In townships surveyed from 1871 to 1880 (most of southern Manitoba, part of southeastern Saskatchewan and a small region near Prince Albert, Saskatchewan), there are road allowances of surrounding every section. In townships surveyed from 1881 to the present, road allowances are reduced both in width and in number. They are wide and run north–south between all sections; however, there are only three east–west road allowances in each township, on the north side of sections 7 to 12, 19 to 24 and 31 to 36. This results in a north-south road allowance every mile going west, and an east-west road allowance every two miles going north. This arrangement reduced land allocation for roads, but still provides road-access to every quarter-section. Road allowances are one of the differences between the Canadian DLS and the American Public Land Survey System, which leaves no extra space for roads.\n\nCertain sections of townships were reserved for special purposes:\n\nLegal surveys conducted before and after the Dominion Land Survey grid was laid out often have their own legal descriptions and delineations. Early settlement lots still retain their own original legal descriptions, but often have townships superimposed over them for the sake of convenience or for certain tasks. Urban developments superimpose new survey lots and plans over the older section and township grid also.\n\nCertain areas otherwise within the surveys' boundaries were not part of the survey grid, or not available for homesteading. These were Indian reserves, pre-existing \"settlements\" divided into \"river lots\" based on the French system used in Quebec, and lands around Hudson's Bay Company trading posts reserved for the company when it transferred its claim over the West to Canada in 1870.\n\nThe rights of the pre-DLS settlers was a major political issue in the West in the late nineteenth century. The settlers claimed squatters' rights over the land they had already farmed, but the sizes and boundaries of these farms were poorly defined, leading to frequent disputes. As well the Métis in the Southbranch settlements of Saskatchewan were particularly with their land rights given that they had not been well protected by the \"Manitoba Act\" as they had been promised in 1870. In the case of the closely clustered settlements of Edmonton, St. Albert, and Fort Saskatchewan in the Alberta District, a militant \"settlers' rights\" movement developed which demanded action from the federal government to grant the settlers legal title to their land and to end claim jumping. The movement even resorted to vigilante action against suspected claim jumpers. Most of these grievances were resolved by 1885, which is likely one of the reasons the area never joined the North-West Rebellion despite the fact that St. Albert, and to a lesser extent Edmonton, were largely populated by Métis people.\n\n\nDennis, John Stoughton younger (1856-1938). A short history of the surveys performed under the Dominion lands system, 1869 to 1889. Ottawa: s.n, 1892.\n\nMcKercher, Robert B and Bertram Wolf (1986). Understanding Western Canada's Dominion Land Survey System. Saskatoon: Division of Extension and Community Relations, University of Saskatchewan.\n\nOliver, J. (2007) ‘The paradox of progress: land survey and the making of agrarian society in colonial British Columbia’. In L. McAtackney, M. Palus and A. Piccini (eds.) Contemporary and Historical Archaeology in Theory, pp. 31–38. Oxford: BAR, International Series, S1677.\n\nThomson, D.W. (1966 & 1967) Men and Meridians: The History of Surveying and Mapping in Canada, 3 vols. Ottawa: Department of Energy, Mines and Resources.\n\n"}
{"id": "2855257", "url": "https://en.wikipedia.org/wiki?curid=2855257", "title": "Fluvio-glacial", "text": "Fluvio-glacial\n\nFluvio refers to things related to rivers and glacial refers to something that is of ice. Fluvio-glacial refers to the meltwater created when a glacier melts. Fluvio-glacial processes can occur on the surface and within the glacier. The deposits that happen within the glacier are revealed after the entire glacier melts or partially retreats. Fluvio-glacial landforms and erosional surfaces include: outwash plains, kames, kame terraces, kettle holes, eskers, varves, and proglacial lakes.\n\nAn outwash plain is both an erosional and depositional surface created by meltwater coming from the glacier. These plains are found in front of the glaciers and are typically characterized by small braided streams. The streams are usually small and braided because the sediment size varies and the original stream gets broken up. Since these streams meander around, the erosion happens laterally (left to right) instead of vertically (up and down). These plains are usually found beyond the end moraine deposited by the glacier.\n\nKames are small hills that consist of sediments ranging from sands to gravels. Intraglacial movement of water carries sediments and deposits them inside cavaties, or holes, in the glacier; once the glacier melts or retreats, the kames is left behind as a deposit. These hills can range in size and be up to 50 m tall and 400 m wide. A kame terrace is a relatively flat surface of sediment that was deposited between the valley surface and the glacier. When kames, kame surfaces, and other fluvio-glacial landforms are combined into one landscape, it is called a kame complex or glacial karst topography.\n\nKettle holes are typically formed when a chunk of ice that came from the glacier is buried under fluvio-glacial sediments. Once the ice melts, the sediments fall and form a depression that can later be filled with rainwater and form kettle lakes. Kettle holes can often be found in the outwash plain of a glacier. Kettle holes can be anywhere from 5 m to 30 km wide.\n\nEskers can be describes as long, curved ridges made up of sands and gravels. These surfaces were created by intraglacial streams carrying and depositing sediments as they flow through the glacier. These landforms can range from 100 m to 500 km long and 3 m to 200 m tall.\n\nVarves are a depositional feature of a fluvio-glacial movement. They are layers of annual sediment deposits. The sizes of the sediments vary and depend on the volume of the streams, but are usually associated with mud deposits (silt and clay). The color and amount of the sediment deposited also varies depending upon the season; summer deposits typically have larger volumes of deposition and are characterized as being light, whereas winter deposits are usually the opposite. Winter deposits are fairly uncommon because the water is frozen into ice again.\n\nProglacial lakes form as meltwater trapped behind a glacial feature such as an end moraine. These lakes have an intake of sediment deposited by streams within the glacier and their deposits are recorded within the varves.\n"}
{"id": "8231648", "url": "https://en.wikipedia.org/wiki?curid=8231648", "title": "GPS-aided GEO augmented navigation", "text": "GPS-aided GEO augmented navigation\n\nThe GPS-aided GEO augmented navigation (GAGAN) is an implementation of a regional satellite-based augmentation system (SBAS) by the Indian government. It is a system to improve the accuracy of a GNSS receiver by providing reference signals. The AAI's efforts towards implementation of operational SBAS can be viewed as the first step towards introduction of modern Communication, navigation and surveillance/Air Traffic Management system over Indian airspace.\n\nThe project has established 15 Indian reference stations, 3 Indian navigation land uplink stations, 3 Indian mission control centers, and installation of all associated software and communication links. It will be able to help pilots to navigate in the Indian airspace by an accuracy of 3 m. This will be helpful for landing aircraft in marginal weather and difficult approaches like Mangalore and Leh airports.\n\nThe project was created in three phases through 2008 by the Airport Authority of India with the help of the Indian Space Research Organisation's (ISRO) technology and space support. The goal is to provide navigation system for all phases of flight over the Indian airspace and in the adjoining area. It is applicable to safety-to-life operations, and meets the performance requirements of international civil aviation regulatory bodies.\n\nThe space component became available after the launch of the GAGAN payload on the GSAT-8 communication satellite, which was successfully launched. This payload was also part of the GSAT-4 satellite that was lost when the geosynchronous satellite launch vehicle (GSLV) failed during launch in April 2010. A final system acceptance test was conducted during June 2012 followed by system certification during July 2013.\n\nTo begin implementing a satellite-based augmentation system over the Indian airspace, Wide Area Augmentation System (WAAS) codes for L1 frequency and L5 frequency were obtained from the United States Air Force and U.S Department of Defense on November 2001 and March 2005. The system will use eight reference stations located in Delhi, Guwahati, Kolkata, Ahmedabad, Thiruvananthapuram, Bangalore, Jammu and Port Blair, and a master control center at Bangalore. US defense contractor Raytheon has stated they will bid to build the system.\n\nA national plan for satellite navigation including implementation of technology demonstration system (TDS) over the Indian air space as a proof of concept had been prepared jointly by Airports Authority of India (AAI) and ISRO. TDS was successfully completed during 2007 by installing eight Indian Reference Stations (INRESs) at eight Indian airports and linked to the Master Control Center (MCC) located near Bangalore. Preliminary system acceptance testing has been successfully completed in December 2010. The ground segment for GAGAN, which has been put up by the Raytheon, has 15 reference stations scattered across the country. Two mission control centres, along with associated uplink stations, have been set up at Kundalahalli in Bangalore. One more control centre and uplink station are to come up at Delhi. As a part of the programme, a network of 18 total electron content (TEC) monitoring stations were installed at various locations in India to study and analyse the behaviour of the ionosphere over the Indian region.\n\nGAGAN's TDS signal in space provides a three-metre accuracy as against the requirement of 7.6 metres. Flight inspection of GAGAN signal is being carried out at Kozhikode, Hyderabad, Nagpur and Bangalore airports and the results have been satisfactory so far.\n\nOne essential component of the GAGAN project is the study of the ionospheric behaviour over the Indian region. This has been specially taken up in view of the uncertain nature of the behaviour of the ionosphere in the region. The study will lead to the optimisation of the algorithms for the ionospheric corrections in the region.\n\nTo study the ionospheric behaviour more effectively over entire Indian airspace, Indian universities and R&D labs, which are involved in the development of regional based ionotropic model for GAGAN, have suggested nine more TEC stations.\n\nGAGAN after its final operational phase completion, will be compatible with other SBAS systems such as the wide-area augmentation system (WAAS), the European Geostationary Navigation Overlay Service (EGNOS) and the Multi-functional Satellite Augmentation System (MSAS) and will provide seamless air navigation service across regional boundaries. While the ground segment consists of eight reference stations and a master control center, which will have sub systems such as data communication network, SBAS correction and verification system, operations and maintenance system, performance monitoring display and payload simulator, Indian land uplinking stations will have dish antenna assembly. The space segment will consist of one geo-navigation transponder.\n\nA flight-management system based on GAGAN will then be poised to save operators time and money by managing climb, descent and engine performance profiles. The FMS will improve the efficiency and flexibility by increasing the use of operator-preferred trajectories.\nIt will improve airport and airspace access in all weather conditions, and the ability to meet the environmental and obstacle clearance constraints. It will also enhance reliability and reduce delays by defining more precise terminal area procedures that feature parallel routes and environmentally optimised airspace corridors.\n\n\nThe first GAGAN transmitter was integrated into the GSAT-4 geostationary satellite, and had a goal of being operational in 2008. Following a series of delays, GSAT-4 was launched on 15 April 2010, however it failed to reach orbit after the third stage of the Geosynchronous Satellite Launch Vehicle Mk.II that was carrying it malfunctioned.\n\nIn 2009, Raytheon had won an 82 million dollar contract. It was mainly dedicated to modernise Indian air navigation system. The vice president of Command & Control Systems, Raytheon Network Centric Systems, Andy Zogg commented:\n\nGAGAN will be the world's most advanced air navigation system and further reinforces India's leadership in the forefront of air navigation. GAGAN will greatly improve safety, reduce congestion and enhance communications to meet India's growing air traffic management needs\n\nIn 2012, the Defence Research and Development Organisation received a \"miniaturised version\" of the device with all the features from global positioning systems (GPS) and global navigation satellite systems (GNSS). The module weighing just 17 gm, can be used in multiple platforms ranging from aircraft (e.g. winged or rotor-craft) to small boats, ships. Reportedly, it can also assist \"survey applications\". It is a cost-efficient device and can be of \"tremendous\" civilian use. The navigation output is composed of GPS, GLONASS and GPS+GLONASS position, speed and time data. According to a statement released by the DRDO, G3oM is a state-of-the-art technology receiver, integrating Indian GAGAN as well as both global positioning system and GLONASS systems.\n\nAccording to Deccan chronicle:\nG. Satheesh Reddy, associate director of the city-based Research Centre Imarat, said the product is bringing about a quantum leap in the area of GNSS technology and has paved the way for highly miniaturised GNSS systems for the future.\n\nOn 30 December 2012, the Directorate General of Civil Aviation (DGCA), India provisionally certified the GPS-aided geo-augmented navigation (GAGAN) system to RNP0.1 (Required Navigation Performance, 0.1 Nautical Mile) service level. The certification enabled aircraft fitted with SBAS equipment to use GAGAN signal in space for navigation purposes.\n\nGSAT-8 is an Indian geostationary satellites, which was successfully launched using Ariane 5 on 21 May 2011 and is positioned in geosynchronous orbit at 55 degrees E longitude.\n\nGSAT-10 is envisaged to augment the growing need of Ku and C-band transponders and carries 12 Ku Band, 12 C Band and 12 Extended C Band transponders and a GAGAN payload. The spacecraft employs the standard I-3K structure with power handling capability of around 6 kW with a lift off mass of 3400 kg. GSAT-10 was successfully launched by Ariane 5 on 29 September 2012.\n\nGSAT-15 carries 24 Ku band transponders with India coverage beam and a GAGAN payload. was successfully launched on 10 November 2015, 21:34:07 UTC, completing the constellation.\n\nThe Indian government has stated that it intends to use the experience of creating the GAGAN system to enable the creation of an autonomous regional navigation system called the Indian Regional Navigation Satellite System IRNSS.\n\nIRNSS-1\nIndian regional navigational satellite system (IRNSS)-1, the first of the seven satellites of the IRNSS constellation, carries a navigation payload and a C-band ranging transponder. The spacecraft employs an optimised I-1K structure with a power handling capability of around 1660W and a lift off mass of 1425 kg, and is designed for a nominal mission life of 10 years. The first satellite of IRNSS constellation was launched onboard PSLV (C22) on 1 July 2012. While the full constellation was planned to be realised during 2014 time frame, launch of subsequent satellites got delayed.\n\nCurrently all 7 satellites are in orbit but in 2017 it was announced that all three rubidium atomic clocks on board IRNSS-1A had failed, mirroring similar failures in the Galileo constellation. The first failure occurred in July 2016, following which two other clocks also failed. This rendered the satellite somewhat redundant and required replacement. Although the satellite still performs other functions, the data is coarse, and thus cannot be used for accurate measurements. ISRO plans to replace it with IRNSS-1H in July or August 2017.\n\nTwo more clocks in the navigational system had started showing signs of abnormality, thereby taking the total number of failed clocks to five.\n\nAs a precaution to extend the operational life of navigation satellite, ISRO is running only one rubidium atomic clock instead of two in the remaining six satellites. Each satellite has three clocks, therefore a total of 27 clocks for all satellites in the system (including standby satellites). The clocks of both IRNSS and GALILEO were supplied by SpectraTime. ISRO replaced the atomic clocks in two standby NavIC satellites. The setback comes at a time when IRNSS is yet to start commercial operations.\n\nKarnataka Forest Department has used GAGAN to build a new, accurate and publicly available satellite based database of its forestlands. This is a followup to the Supreme Court directive to states to update and put up their respective forest maps. The geospatial database of forestlands pilot has used data from the Cartosat-2 satellite. The maps are meant to rid authorities of ambiguities related to forest boundaries and give clarity to forest administrators, revenue officials as also the public, according to R.K. Srivastava, chief conservator of forests (headquarters).\n\nVarious Indian manufactured missiles including the BrahMos will use GAGAN for guidance.\n\n\n\n"}
{"id": "7487953", "url": "https://en.wikipedia.org/wiki?curid=7487953", "title": "GSHHG", "text": "GSHHG\n\nGSHHG (Global Self-consistent, Hierarchical, High-resolution Geography Database; formerly \"Global Self-consistent, Hierarchical, High-resolution Shoreline Database\" (\"GSHHS\")) is a high-resolution shoreline data set amalgamated from two data bases (the CIA world database WDBII, and the World Vector Shoreline database) in the public domain. The data have undergone extensive processing and are free of internal inconsistencies such as erratic points and crossing segments. The shorelines are constructed entirely from hierarchically arranged closed polygons. The four-level hierarchy is as follows: seashore, lakes, islands within lakes, ponds within islands within lakes.\n\nThe data can be used to simplify data searches and data selections, or to study the statistical characteristics of shorelines and land-masses. It comes with access software and routines to facilitate decimation based on a standard line-reduction algorithm.\n\nGSHHS is developed and maintained by Dr. Paul Wessel at the University of Hawai'i, and Dr. Walter H. F. Smith at the NOAA Laboratory for Satellite Altimetry.\n\n\n\n\n\"This article contains public domain text created by the U.S. Federal government, taken from the NOAA website at \"\n"}
{"id": "995417", "url": "https://en.wikipedia.org/wiki?curid=995417", "title": "Geodetic datum", "text": "Geodetic datum\n\nA geodetic datum or geodetic system is a coordinate system, and a set of reference points, used to locate places on the Earth (or similar objects). An approximate definition of sea level is the datum WGS 84, an ellipsoid, whereas a more accurate definition is Earth Gravitational Model 2008 (EGM2008), using at least 2,159 spherical harmonics. Other datums are defined for other areas or at other times; ED50 was defined in 1950 over Europe and differs from WGS 84 by a few hundred meters depending on where in Europe you look. \nMars has no oceans and so no sea level, but at least two martian datums have been used to locate places there.\n\nDatums are used in geodesy, navigation, and surveying by cartographers and satellite navigation systems to translate positions indicated on maps (paper or digital) to their real position on Earth. Each starts with an ellipsoid (stretched sphere), and then defines latitude, longitude and altitude coordinates. One or more locations on the Earth's surface are chosen as anchor \"base-points\".\n\nThe difference in co-ordinates between datums is commonly referred to as \"datum shift\". The datum shift between two particular datums can vary from one place to another within one country or region, and can be anything from zero to hundreds of meters (or several kilometers for some remote islands). The North Pole, South Pole and Equator will be in different positions on different datums, so True North will be slightly different. Different datums use different interpolations for the precise shape and size of the Earth (reference ellipsoids).\n\nBecause the Earth is an imperfect ellipsoid, localised datums can give a more accurate representation of the area of coverage than WGS 84. OSGB36, for example, is a better approximation to the geoid covering the British Isles than the global WGS 84 ellipsoid. However, as the benefits of a global system outweigh the greater accuracy, the global WGS 84 datum is becoming increasingly adopted.\n\nHorizontal datums are used for describing a point on the Earth's surface, in latitude and longitude or another coordinate system. Vertical datums measure elevations or depths.\n\nIn surveying and geodesy, a \"datum\" is a reference system or an approximation of the Earth's surface against which positional measurements are made for computing locations. Horizontal datums are used for describing a point on the Earth's surface, in latitude and longitude or another coordinate system. Vertical datums are used to measure elevations or underwater depths.\n\nThe horizontal datum is the model used to measure positions on the Earth. A specific point on the Earth can have substantially different coordinates, depending on the datum used to make the measurement. There are hundreds of local horizontal datums around the world, usually referenced to some convenient local reference point. Contemporary datums, based on increasingly accurate measurements of the shape of the Earth, are intended to cover larger areas. The WGS 84 datum, which is almost identical to the NAD83 datum used in North America and the ETRS89 datum used in Europe, is a common standard datum.\n\nFor example, in Sydney there is a 200 metres (700 feet) difference between GPS coordinates configured in GDA (based on global standard WGS 84) and AGD (used for most local maps), which is an unacceptably large error for some applications, such as surveying or site location for scuba diving.\n\nA vertical datum is used as a reference point for elevations of surfaces and features on the Earth including terrain, bathymetry, water levels, and man-made structures. Vertical datums are either: tidal, based on sea levels; gravimetric, based on a geoid; or geodetic, based on the same ellipsoid models of the Earth used for computing horizontal datums.\n\nIn common usage, elevations are often cited in height above sea level, although what “sea level” actually means is a more complex issue than might at first be thought: the height of the sea surface at any one place and time is a result of numerous effects, including waves, wind and currents, atmospheric pressure, tides, topography, and even differences in the strength of gravity due to the presence of mountains etc.\n\nFor the purpose of measuring the height of objects on land, the usual datum used is mean sea level (MSL). This is a tidal datum which is described as the arithmetic mean of the hourly water elevation taken over a specific 19 years cycle. This definition averages out tidal highs and lows (caused by the gravitational effects of the sun and the moon) and short term variations. It will not remove the effects of local gravity strength, and so the height of MSL, relative to a geodetic datum, will vary around the world, and even around one country. Countries tend to choose the mean sea level at one specific point to be used as the standard “sea level” for all mapping and surveying in that country. (For example, in Great Britain, the national vertical datum, Ordnance Datum Newlyn, is based on what was mean sea level at Newlyn in Cornwall between 1915 and 1921). However, zero elevation as defined by one country is not the same as zero elevation defined by another (because MSL is not the same everywhere), which is why locally defined vertical datums differ from one another.\n\nA different principle is used when choosing a datum for nautical charts. For safety reasons, a mariner must be able to know the minimum depth of water that could occur at any point. For this reason, depths and tides on a nautical chart are measured relative to chart datum, which is defined to be a level below which tide rarely falls. Exactly how this is chosen depends on the tidal regime in the area being charted and on the policy of the hydrographic office producing the chart in question; a typical definition is Lowest Astronomical Tide (the lowest tide predictable from the effects of gravity), or Mean Lower Low Water (the average lowest tide of each day), although MSL is sometimes used in waters with very low tidal ranges.\n\nConversely, if a ship is to safely pass under a low bridge or overhead power cable, the mariner must know the minimum clearance between the masthead and the obstruction, which will occur at high tide. Consequently, bridge clearances etc. are given relative to a datum based on high tide, such as Highest Astronomical Tide or Mean High Water Springs.\n\nSea level does not remain constant throughout geological time, and so tidal datums are less useful when studying very long-term processes. In some situations sea level does not apply at all — for instance for mapping Mars' surface — forcing the use of a different \"zero elevation\", such as mean radius.\n\nA geodetic vertical datum takes some specific zero point, and computes elevations based on the geodetic model being used, without further reference to sea levels. Usually, the starting reference point is a tide gauge, so at that point the geodetic and tidal datums might match, but due to sea level variations, the two scales may not match elsewhere. An example of a gravity-based geodetic datum is NAVD88, used in North America, which is referenced to a point in Quebec, Canada. Ellipsoid-based datums such as WGS 84, GRS80 or NAD83 use a theoretical surface that may differ significantly from the geoid.\n\nIn geodetic coordinates, the Earth's surface is approximated by an ellipsoid, and locations near the surface are described in terms of latitude (formula_1), longitude (formula_2), and height (formula_3).\n\nGeodetic latitude (formula_1), resp. altitude, is different from geocentric latitude (formula_5), resp. altitude. Geodetic latitude is determined by the angle between the equatorial plane and normal to the ellipsoid, whereas geocentric latitude is determined by the angle between the equatorial plane and line joining the point to the centre of the ellipsoid (see figure). Unless otherwise specified, latitude is geodetic latitude.\n\nThe ellipsoid is completely parameterised by the semi-major axis formula_6 and the flattening formula_7.\n\nFrom formula_6 and formula_7 it is possible to derive the semi-minor axis formula_10, first eccentricity formula_11 and second eccentricity formula_12 of the ellipsoid\n\nAGD66 and AGD84 both use the parameters defined by Australian National Spheroid (see below)\n\nGDA94 uses the parameters defined by GRS80 (see below)\n\nSee GDA Technical Manual document for more details; the value given above for the flattening is not exact.\n\nThe Global Positioning System (GPS) uses the World Geodetic System 1984 (WGS 84) to determine the location of a point near the surface of the Earth.\n\nSee The official World Geodetic System 1984 document for more details.\n\nA more comprehensive list of geodetic systems can be found here\n\nDatum conversion is the process of converting the coordinates of a point from one datum system to another. Datum conversion may frequently be accompanied by a change of grid projection.\n\nA reference datum is a known and constant surface which is used to describe the location of unknown points on the Earth. Since reference datums can have different radii and different center points, a specific point on the Earth can have substantially different coordinates depending on the datum used to make the measurement. There are hundreds of locally developed reference datums around the world, usually referenced to some convenient local reference point. Contemporary datums, based on increasingly accurate measurements of the shape of the Earth, are intended to cover larger areas. The most common reference Datums in use in North America are NAD27, NAD83, and WGS 84.\n\nThe North American Datum of 1927 (NAD 27) is \"the horizontal control datum for the United States that was defined by a location and azimuth on the Clarke spheroid of 1866, with origin at (the survey station) Meades Ranch (Kansas).\" ... The geoidal height at Meades Ranch was assumed to be zero, as sufficient gravity data was not available, and this was needed to relate surface measurements to the datum. \"Geodetic positions on the North American Datum of 1927 were derived from the (coordinates of and an azimuth at Meades Ranch) through a readjustment of the triangulation of the entire network in which Laplace azimuths were introduced, and the Bowie method was used.\" (http://www.ngs.noaa.gov/faq.shtml#WhatDatum ) NAD27 is a local referencing system covering North America.\n\nThe North American Datum of 1983 (NAD 83) is \"The horizontal control datum for the United States, Canada, Mexico, and Central America, based on a geocentric origin and the Geodetic Reference System 1980 (GRS80). \"This datum, designated as NAD 83 ...is based on the adjustment of 250,000 points including 600 satellite Doppler stations which constrain the system to a geocentric origin.\" NAD83 may be considered a local referencing system.\n\nWGS 84 is the World Geodetic System of 1984. It is the reference frame used by the U.S. Department of Defense (DoD) and is defined by the National Geospatial-Intelligence Agency (NGA) (formerly the Defense Mapping Agency, then the National Imagery and Mapping Agency). WGS 84 is used by DoD for all its mapping, charting, surveying, and navigation needs, including its GPS \"broadcast\" and \"precise\" orbits. WGS 84 was defined in January 1987 using Doppler satellite surveying techniques. It was used as the reference frame for broadcast GPS Ephemerides (orbits) beginning January 23, 1987. At 0000 GMT January 2, 1994, WGS 84 was upgraded in accuracy using GPS measurements. The formal name then became WGS 84 (G730), since the upgrade date coincided with the start of GPS Week 730. It became the reference frame for broadcast orbits on June 28, 1994. At 0000 GMT September 30, 1996 (the start of GPS Week 873), WGS 84 was redefined again and was more closely aligned with International Earth Rotation Service (IERS) frame ITRF 94. It was then formally called WGS 84 (G873). WGS 84 (G873) was adopted as the reference frame for broadcast orbits on January 29, 1997. Another update brought it to WGS84(G1674).\n\nThe WGS 84 datum, within two meters of the NAD83 datum used in North America, is the only world referencing system in place today. WGS 84 is the default standard datum for coordinates stored in recreational and commercial GPS units.\n\nUsers of GPS are cautioned that they must always check the datum of the maps they are using. To correctly enter, display, and to store map related map coordinates, the datum of the map must be entered into the GPS map datum field.\n\nExamples of map datums are:\n\n\n\n"}
{"id": "351656", "url": "https://en.wikipedia.org/wiki?curid=351656", "title": "Geographic Names Information System", "text": "Geographic Names Information System\n\nThe Geographic Names Information System (GNIS) is a database that contains name and locative information about more than two million physical and cultural features located throughout the United States of America and its territories. It is a type of gazetteer. GNIS was developed by the United States Geological Survey in cooperation with the United States Board on Geographic Names (BGN) to promote the standardization of feature names.\n\nThe database is part of a system that includes topographic map names and bibliographic references. The names of books and historic maps that confirm the feature or place name are cited. Variant names, alternatives to official federal names for a feature, are also recorded. Each feature receives a permanent, unique feature record identifier, sometimes called the GNIS identifier. The database never removes an entry, \"except in cases of obvious duplication.\"\n\nThe GNIS accepts proposals for new or changed names for U.S. geographical features. The general public can make proposals at the GNIS web site and can review the justifications and supporters of the proposals.\n\n\n\n\n"}
{"id": "1913728", "url": "https://en.wikipedia.org/wiki?curid=1913728", "title": "Geographic information science", "text": "Geographic information science\n\nGeographic information science or geographical information science (GIScience) is the scientific discipline that studies data structures and computational techniques to capture, represent, process, and analyze geographic information. It can be contrasted with geographic information systems (GIS), which are software tools.\nBritish geographer Michael Goodchild defined this area in the 1990s and summarized its core interests, including spatial analysis, visualization, and the representation of uncertainty. GIScience is conceptually related to geography, information science, computer science, geomatics and geoinformatics, but it claims the status of an independent scientific discipline.\n\nSince its inception in the 1990s, the boundaries between GIScience and cognate disciplines are contested, and different communities might disagree on what GIScience is and what it studies. In particular, Goodchild stated that \"information science can be defined as the systematic study according to scientific principles of the nature and properties of information. Geographic information science is the subset of information science that is about geographic information.\" Another influential definition is that by GIScientist David Mark, which states:Geographic Information Science (GIScience) is the basic research field that seeks to redefine geographic concepts and their use in the context of geographic information systems. GIScience also examines the impacts of GIS on individuals and society, and the influences of society on GIS. GIScience re-examines some of the most fundamental themes in traditional spatially oriented fields such as geography, cartography, and geodesy, while incorporating more recent developments in cognitive and information science. It also overlaps with and draws from more specialized research fields such as computer science, statistics, mathematics, and psychology, and contributes to progress in those fields. It supports research in political science and anthropology, and draws on those fields in studies of geographic information and society.\n\nIn 2009, Goodchild summarized the history of GIScience and its achievements and open challenges.\n\n\n\n\n"}
{"id": "227280", "url": "https://en.wikipedia.org/wiki?curid=227280", "title": "Grid reference", "text": "Grid reference\n\nGrid references define locations in maps using Cartesian coordinates. Grid lines on maps define the coordinate system, and are numbered to provide a unique reference to features. This reference is normally based on projected easting and northings\n\nAlthough professional map-making and use of the grid had existed in China before, the Chinese cartographer and geographer Pei Xiu of the Three Kingdoms period was the first to mention a plotted geometrical grid reference and graduated scale displayed on the surface of maps to gain greater accuracy in the estimated distance between different locations. Historian Howard Nelson asserts that there is ample written evidence that Pei Xiu derived the idea of the grid reference from the map of Zhang Heng (78–139 CE), a polymath inventor and statesman of the Eastern Han dynasty. The American writer Robert K. G. Temple asserts that Zhang Heng should also be credited as the first to establish the mathematical grid in cartography, as evidenced by his work in maps, the titles of his lost books, and the hint given in the \"Book of Later Han\" (i.e. Zhang Heng \"cast a network of coordinates about heaven and earth, and reckoned on the basis of it\").\n\nGrid systems vary, but the most common is a square grid with grid lines intersecting each other at right angles and numbered sequentially from the origin at the bottom left of the map. The grid numbers on the east-west (horizontal) axis are called Eastings, and the grid numbers on the north-south (vertical) axis are called Northings.\n\nNumerical grid references consist of an even number of digits. Eastings are written before Northings. Thus in a 6 digit grid reference 123456, the Easting component is 123 and the Northing component is 456.\n\nGrids may be arbitrary, or can be based on specific distances, for example some maps use a one-kilometre square grid spacing.\n\nA grid reference locates a unique square region on the map. The precision of location varies, for example a simple town plan may use a simple grid system with single letters for Eastings and single numbers for Northings. A grid reference in this system, such as 'H3', locates a particular square rather than a single point.\n\nPoints can be located by grid references on maps that use a standard system for Eastings and Northings, such as the Universal Transverse Mercator used worldwide, or the Ordnance Survey National Grid used by Ordnance Survey in the UK. These points can then be located by someone else using grid references, even if using maps of a different scale.\n\nIn the Universal Transverse Mercator (UTM) system, grid reference is given by three numbers: zone, easting and northing. In the UTM system, the earth is divided into 60 zones. Northing values are given by the metres north, or south (in the southern hemisphere) of the equator. Easting values are established as the distance from the central meridian of a zone. The central meridian is arbitrarily set at 500,000 metres, to avoid negative numbers. A position 100 kilometres west of a central meridian would have an easting of 400,000 metres. Due to its popularity, and worldwide cover, the UTM system is used worldwide by NATO as well as many countries, including Australia and the USA.\n\nIn the United Kingdom, a proprietary grid system is used. In Ordnance Survey maps, each Easting and Northing grid line is given a two-digit code, based on the British national grid reference system with origin point just off the southwest coast of the United Kingdom. Since the Eastings and Northings are one kilometre apart, a combination of a Northing and an Easting will give a four-digit grid reference describing a one-kilometre square on the ground. The convention is the grid reference numbers call out the lower-left corner of the desired square. In the example map below, the town Little Plumpton lies in the square 6901, even though the writing which labels the town is in 6802 and 6902, most of the buildings (the orange boxed symbols) are in square 6901.\n\nThe more digits added to a grid reference, the more precise the reference becomes. To locate a specific building in Little Plumpton, a further two digits are added to the four-digit reference to create a six-digit reference. The extra two digits describe a position within the 1-kilometre square. Imagine (or draw or superimpose a Romer) a further 10x10 grid within the current grid square. Any of the 100 squares in the superimposed 10×10 grid can be accurately described using a digit from 0 to 9 (with 0 0 being the bottom left square and 9 9 being the top right square).\n\nFor the church in Little Plumpton, this gives the digits 6 and 7 (6 on the left to right axis (Eastings) and 7 on the bottom to top axis (Northings). These are added to the four-figure grid reference after the two digits describing the same coordinate axis, and thus our six-figure grid reference for the church becomes 696017. This reference describes a 100-metre by 100-metre square, and not a single point, but this precision is usually sufficient for navigation purposes. The symbols on the map are not precise in any case, for example the church in the example above would be approximately 100x200 metres if the symbol was to scale, so in fact, the middle of the black square represents the map position of the real church, independently of the actual size of the church.\n\nGrid references comprising larger numbers for greater precision could be determined using large-scale maps and an accurate Romer. This might be used in surveying but is not generally used for land navigating for walkers or cyclists, etc. The growing availability and decreasing cost of handheld GPS receivers enables determination of accurate grid references without needing a map, but it is important to know how many digits the GPS displays to avoid reading off just the first six digits. A GPS unit commonly gives a ten-digit grid reference, based on two groups of five numbers for the Eastings and Northing values. Each successive increase in precision (from 6 digit to 8 digit to 10 digit) pinpoints the location by a factor of 10. Since, in the UK at least, a 6-figure grid reference identifies a square of 100-metre sides, an 8-figure reference would identify a 10-metre square, and a 10-digit reference a 1-metre square. In order to give a standard 6-figure grid reference from a 10-figure GPS readout, the 4th, 5th, 9th and 10th digits must be omitted so it is important not to read just the first 6 digits.\n"}
{"id": "44913770", "url": "https://en.wikipedia.org/wiki?curid=44913770", "title": "Interfluve", "text": "Interfluve\n\nAn interfluve is a narrow, elongated and plateau-like or ridge-like landform between two valleys. Whittow, more generally, defines an interfluve as an area of higher ground between two rivers in the same drainage system.\n\nThese landforms are created by earth flow (\"solifluction\"). They can also be former river terraces that are subsequently bisected by fluvial erosion. In cases where there is a deposit of younger sedimentary beds (loess, colluvium) the interfluves have a rounder and less rugged appearance. A consequence of interfluve formation is the so-called \"interfluvial landscape.\"\n\n\n"}
{"id": "5533009", "url": "https://en.wikipedia.org/wiki?curid=5533009", "title": "Landscape connectivity", "text": "Landscape connectivity\n\nLandscape connectivity in ecology is, broadly, \"the degree to which the landscape facilitates or impedes movement among resource patches\". Alternatively, connectivity may be a continuous property of the landscape and independent of patches and paths. Connectivity includes both structural connectivity (the physical arrangements of disturbance and/or patches) and functional connectivity (the movement of individuals across contours of disturbance and/or among patches). The degree to which a landscape is connected determines the amount of dispersal there is among patches, which influences gene flow, local adaptation, extinction risk, colonization probability, and the potential for organisms to move as they cope with climate change.\n\nThe concept of “Landscape connectivity” was first introduced by Dr. Gray Merriam in 1984. Merriam noted that movement among habitat patches was not merely a function of an organism’s attributes, but also, a quality of the landscape elements through which it must move. To emphasize this fundamental interaction in determining a particular movement pathway, Merriam (1984), defined landscape connectivity as “the degree to which absolute isolation is prevented by landscape elements which allow organisms to move among habitat patches.” Nine years later, Merriam and colleagues, revised the definition to “the degree to which the landscape impedes or facilitates movement among resource patches. Although this definition has undoubtedly become the most accepted and cited meaning within the scientific literature, many authors have continued to create their own definitions. With et al (1997), presented their interpretation as “the functional relationship among habitat patches, owing to the spatial contagion of habitat and the movement responses of organisms to landscape structure.”, and Ament et al. (2014) defined it as “the degree to which regional landscapes, encompassing a variety of natural, semi-natural, and developed land cover types, are conducive to wildlife movement and to sustain ecological processes.” Thus, although there have been many definitions of landscape connectivity over the past 30 years, each new description emphasizes both a structural and a behavioural element to the landscape connectivity concept. The physical component is defined by the spatial and temporal configuration of the landscape elements (landform, landcover and land use types), and the behavioural component is defined by the behavioural responses, of organisms and/or processes, to the physical arrangement of the landscape elements.\n\nHabitat loss and habitat fragmentation have become ubiquitous in both natural and human modified landscapes, resulting in detrimental consequences for local species interactions and global biodiversity. Human development now modifies over 50% of the earth’s landscape, leaving only patches of isolated natural or semi-natural habitats for the millions of other species we share this planet with. Patterns of biodiversity and ecosystem functions are changing worldwide resulting in a loss of connectivity and ecological integrity for the entire global ecological network. Loss of connectivity can influence individuals, populations and communities through within species, between species, and between ecosystem interactions. These interactions affect ecological mechanisms such as nutrient and energy flows, predator-prey relationships, pollination, seed dispersal, demographic rescue, inbreeding avoidance, colonization of unoccupied habitat, altered species interactions, and spread of disease, . Accordingly, landscape connectivity facilitates the movement of biotic processes such as animal movement, plant propagation, and genetic exchange, as well as abiotic processes such as water, energy, and material movement within and between ecosystems.\n\nWithin their home range or territory most animals must move daily among multiple primary habitat patches to forage for food and obtain all the resources they need.\n\nSome species travel to different locations throughout the year to access the resources they need. These movements are usually predictable and are due to changes in the environmental conditions at the primary habitat site, or to facilitate access to breeding grounds. Migratory behaviour is seen in land animals, birds and marine species, and the routes they follow are usually the same year after year.\n\nIs the once in a lifetime movement of certain individuals from one population to another for the purpose of breeding. These exchanges maintain genetic and demographic diversity among populations.\n\nIs the unpredictable movement of individuals or populations to new locations of suitable habitat due to an environmental disturbance. Major disturbances such as fire, natural disasters, human development, and climate change can impact the quality and distribution of habitats and necessitate the movement of species to new locations of suitable habitat.\n\nMovement of species in areas that are typically used by humans. These include greenbelts, recreational trail systems, hedgerows, and golf courses.\n\nPreserving or creating landscape connectivity has become increasingly recognized as a key strategy to protect biodiversity, maintain viable ecosystems and wildlife populations, and facilitate the movement and adaptation of wildlife populations in the face of climate change. The degree to which landscapes are connected determines the overall amount of movement taking place within and between local populations. This connectivity has influences on gene flow, local adaptation, extinction risk, colonization probability, and the potential for organisms to move and adapt to climate change. With habitat loss and fragmentation increasingly deteriorating natural habitats, the sizes and isolation of the remaining habitat fragments are particularly critical to the long-term conservation of biodiversity. Thus, connectivity among these remaining fragments, as well as the characteristics of the surrounding matrix, and the permeability and structure of the habitat edges are all important for biodiversity conservation and affect the overall persistence, strength and integrity of the remaining ecological interactions.\n\nSince the definition of landscape connectivity has both a physical and a behavioural component, quantifying landscape connectivity is consequently organism-, process- and landscape-specific. According to (Wiens & Milne, 1989), the first step in the quantification process of landscape connectivity is defining the specific habitat or habitat network of the focal species, and in turn, describe the landscape elements from its point of view. The next step is to determine the scale of the landscape structure as perceived by the organism. This is defined as the scale at which the species responds to the array of landscape elements, through its fine-scale (grain), and large-scale (extent), movement behaviours. Lastly, how the species responds to the different elements of a landscape is determined. This comprises the species’ movement pattern based on behavioural reactions to the mortality risk of the landscape elements, including habitat barriers and edges.\n\nAlthough connectivity is an intuitive concept, there is no single consistently-used metric of connectivity. Theories of connectivity include consideration of both binary representations of connectivity through \"corridors\" and \"linkages\" and continuous representations of connectivity, which include the binary condition as a sub-set \n\nGenerally, connectivity metrics fall into three categories:\n\n\nTypically, the \"natural\" form of connectivity as an ecological property perceived by organisms is modeled as a continuous surface of permeability, which is the corollary to disturbance. This can be accomplished by most geographic information systems (GIS) able to model in grid/raster format. A critical component of this form of modeling is the recognition that connectivity and disturbance are perceived and responded to differently by different organisms and ecological processes. This variety in responses is one of the most challenging parts of attempting to represent connectivity in spatial modeling. Typically, the most accurate connectivity models are for single species/processes and are developed based on information about the species/process. There is little, and often no evidence that spatial models, including those described here, can represent connectivity for the many species or processes that occupy many natural landscapes. The disturbance-based models are used as the basis for the binary representations of connectivity as paths/corridor/linkages through landscapes described below.\n\nCircuitscape is an open source program that uses circuit theory to predict connectivity in heterogeneous landscapes for individual movement, gene flow, and conservation planning. Circuit theory offers several advantages over common analytic connectivity models, including a theoretical basis in random walk theory and an ability to evaluate contributions of multiple dispersal pathways. Landscapes are represented as conductive surfaces, with low resistances assigned to habitats that are most permeable to movement or best promote gene flow, and high resistances assigned to poor dispersal habitat or to movement barriers. Effective resistances, current densities, and voltages calculated across the landscapes can then be related to ecological processes, such as individual movement and gene flow.\n\nGraphab is a software application devoted to the modelling of landscape networks. It is composed of four main modules: graph building, including loading the initial landscape data and identification of the patches and the links; computation of the connectivity metrics from the graph; connection between the graph and exogenous point data set; visual and cartographical interface.\nGraphab runs on any computer supporting Java 1.6 or later (PC under Linux, Windows, Mac...). It is distributed free of charge for non-commercial use.\n\n\n"}
{"id": "252582", "url": "https://en.wikipedia.org/wiki?curid=252582", "title": "Landscape ecology", "text": "Landscape ecology\n\nLandscape ecology is the science of studying and improving relationships between ecological processes in the environment and particular ecosystems. This is done within a variety of landscape scales, development spatial patterns, and organizational levels of research and policy.\n\nAs a highly interdisciplinary field in systems science, landscape ecology integrates biophysical and analytical approaches with humanistic and holistic perspectives across the natural sciences and social sciences. Landscapes are spatially heterogeneous geographic areas characterized by diverse interacting patches or ecosystems, ranging from relatively natural terrestrial and aquatic systems such as forests, grasslands, and lakes to human-dominated environments including agricultural and urban settings. The most salient characteristics of landscape ecology are its emphasis on the relationship among pattern, process and scale, and its focus on broad-scale ecological and environmental issues. These necessitate the coupling between biophysical and socioeconomic sciences. Key research topics in landscape ecology include ecological flows in landscape mosaics, land use and land cover change, scaling, relating landscape pattern analysis with ecological processes, and landscape conservation and sustainability.\n\nThe German term \"Landschaftsökologie\"–thus \"landscape ecology\"–was coined by German geographer Carl Troll in 1939. He developed this terminology and many early concepts of landscape ecology as part of his early work, which consisted of applying aerial photograph interpretation to studies of interactions between environment and vegetation.\n\nHeterogeneity is the measure of how parts of a landscape differ from one another. Landscape ecology looks at how this spatial structure affects organism abundance at the landscape level, as well as the behavior and functioning of the landscape as a whole. This includes studying the influence of pattern, or the internal order of a landscape, on process, or the continuous operation of functions of organisms. Landscape ecology also includes geomorphology as applied to the design and architecture of landscapes. Geomorphology is the study of how geological formations are responsible for the structure of a landscape.\n\nOne central landscape ecology theory originated from MacArthur & Wilson's \"The Theory of Island Biogeography\". This work considered the biodiversity on islands as the result of competing forces of colonization from a mainland stock and stochastic extinction. The concepts of island biogeography were generalized from physical islands to abstract patches of habitat by Levins' metapopulation model (which can be applied e.g. to forest islands in the agricultural landscape). This generalization spurred the growth of landscape ecology by providing conservation biologists a new tool to assess how habitat fragmentation affects population viability. Recent growth of landscape ecology owes much to the development of geographic information systems (GIS) and the availability of large-extent habitat data (e.g. remotely sensed datasets).\n\nLandscape ecology developed in Europe from historical planning on human-dominated landscapes. Concepts from general ecology theory were integrated in North America. While general ecology theory and its sub-disciplines focused on the study of more homogenous, discrete community units organized in a hierarchical structure (typically as ecosystems, populations, species, and communities), landscape ecology built upon heterogeneity in space and time. It frequently included human-caused landscape changes in theory and application of concepts.\n\nBy 1980, landscape ecology was a discrete, established discipline. It was marked by the organization of the International Association for Landscape Ecology (IALE) in 1982. Landmark book publications defined the scope and goals of the discipline, including Naveh and Lieberman and Forman and Godron. Forman wrote that although study of \"the ecology of spatial configuration at the human scale\" was barely a decade old, there was strong potential for theory development and application of the conceptual framework. Today, theory and application of landscape ecology continues to develop through a need for innovative applications in a changing landscape and environment. Landscape ecology relies on advanced technologies such as remote sensing, GIS, and models. There has been associated development of powerful quantitative methods to examine the interactions of patterns and processes. An example would be determining the amount of carbon present in the soil based on landform over a landscape, derived from GIS maps, vegetation types, and rainfall data for a region.\n\nNowadays, at least six different conceptions of landscape ecology can be identified: one group tending toward the more disciplinary concept of ecology (subdiscipline of biology; in conceptions 2, 3, and 4) and another group—characterized by the interdisciplinary study of relations between human societies and their environment—inclined toward the integrated view of geography (in conceptions 1, 5, and 6):\n\nSome research programmes of landscape ecology theory, namely those standing in the European tradition, may be slightly outside of the \"classical and preferred domain of scientific disciplines\" because of the large, heterogeneous areas of study. However, general ecology theory is central to landscape ecology theory in many aspects. Landscape ecology consists of four main principles: the development and dynamics of spatial heterogeneity, interactions and exchanges across heterogeneous landscapes, influences of spatial heterogeneity on biotic and abiotic processes, and the management of spatial heterogeneity. The main difference from traditional ecological studies, which frequently assume that systems are spatially homogenous, is the consideration of spatial patterns.\n\nLandscape ecology not only created new terms, but also incorporated existing ecological terms in new ways. Many of the terms used in landscape ecology are as interconnected and interrelated as the discipline itself.\n\nCertainly, 'landscape' is a central concept in landscape ecology. It is, however, defined in quite different ways. For example: Carl Troll conceives of landscape not as a mental construct but as an objectively given 'organic entity', a \"harmonic individuum of space\". \nErnst Neef defines landscapes as sections within the uninterrupted earth-wide interconnection of geofactors which are defined as such on the basis of their uniformity in terms of a specific land use, and are thus defined in an anthropocentric and relativistic way.\nAccording to Richard Forman and Michael Godron, a landscape is a heterogeneous land area composed of a cluster of interacting ecosystems that is repeated in similar form throughout, whereby they list woods, meadows, marshes and villages as examples of a landscape's ecosystems, and state that a landscape is an area at least a few kilometres wide. \nJohn A. Wiens opposes the traditional view expounded by Carl Troll, Isaak S. Zonneveld, Zev Naveh, Richard T. T. Forman/Michel Godron and others that landscapes are arenas in which humans interact with their environments on a kilometre-wide scale; instead, he defines 'landscape'—regardless of scale—as \"the template on which spatial patterns influence ecological processes\".\nSome define 'landscape' as an area containing two or more ecosystems in close proximity.\n\nA main concept in landscape ecology is \"scale\". Scale represents the real world as translated onto a map, relating distance on a map image and the corresponding distance on earth. Scale is also the spatial or temporal measure of an object or a process, or amount of spatial resolution. Components of scale include composition, structure, and function, which are all important ecological concepts. Applied to landscape ecology, \"composition\" refers to the number of patch types (see below) represented on a landscape and their relative abundance. For example, the amount of forest or wetland, the length of forest edge, or the density of roads can be aspects of landscape composition. \"Structure\" is determined by the composition, the configuration, and the proportion of different patches across the landscape, while \"function\" refers to how each element in the landscape interacts based on its life cycle events. \"Pattern\" is the term for the contents and internal order of a heterogeneous area of land.\n\nA landscape with structure and pattern implies that it has spatial \"heterogeneity\", or the uneven distribution of objects across the landscape. Heterogeneity is a key element of landscape ecology that separates this discipline from other branches of ecology. Landscape heterogeneity is able to quantify with agent-based methods as well.\n\n\"Patch\", a term fundamental to landscape ecology, is defined as a relatively homogeneous area that differs from its surroundings. Patches are the basic unit of the landscape that change and fluctuate, a process called \"patch dynamics\". Patches have a definite shape and spatial configuration, and can be described compositionally by internal variables such as number of trees, number of tree species, height of trees, or other similar measurements.\n\n\"Matrix\" is the \"background ecological system\" of a landscape with a high degree of connectivity. \"Connectivity\" is the measure of how connected or spatially continuous a corridor, network, or matrix is. For example, a forested landscape (matrix) with fewer gaps in forest cover (open patches) will have higher connectivity. \"Corridors\" have important functions as strips of a particular type of landscape differing from adjacent land on both sides. A \"network\" is an interconnected system of corridors while \"mosaic\" describes the pattern of patches, corridors, and matrix that form a landscape in its entirety.\n\nLandscape patches have a boundary between them which can be defined or fuzzy. The zone composed of the edges of adjacent ecosystems is the \"boundary\". \"Edge\" means the portion of an ecosystem near its perimeter, where influences of the adjacent patches can cause an environmental difference between the interior of the patch and its edge. This edge effect includes a distinctive species composition or abundance. For example, when a landscape is a mosaic of perceptibly different types, such as a forest adjacent to a grassland, the edge is the location where the two types adjoin. In a continuous landscape, such as a forest giving way to open woodland, the exact edge location is fuzzy and is sometimes determined by a local gradient exceeding a threshold, such as the point where the tree cover falls below thirty-five percent.\n\nA type of boundary is the \"ecotone\", or the transitional zone between two communities. Ecotones can arise naturally, such as a lakeshore, or can be human-created, such as a cleared agricultural field from a forest. The ecotonal community retains characteristics of each bordering community and often contains species not found in the adjacent communities. Classic examples of ecotones include fencerows, forest to marshlands transitions, forest to grassland transitions, or land-water interfaces such as riparian zones in forests. Characteristics of ecotones include vegetational sharpness, physiognomic change, occurrence of a spatial community mosaic, many exotic species, ecotonal species, spatial mass effect, and species richness higher or lower than either side of the ecotone.\n\nAn \"ecocline\" is another type of landscape boundary, but it is a gradual and continuous change in environmental conditions of an ecosystem or community. Ecoclines help explain the distribution and diversity of organisms within a landscape because certain organisms survive better under certain conditions, which change along the ecocline. They contain heterogeneous communities which are considered more environmentally stable than those of ecotones.\n\nAn \"ecotope\" is a spatial term representing the smallest ecologically distinct unit in mapping and classification of landscapes. Relatively homogeneous, they are spatially explicit landscape units used to stratify landscapes into ecologically distinct features. They are useful for the measurement and mapping of landscape structure, function, and change over time, and to examine the effects of disturbance and fragmentation.\n\n\"Disturbance\" is an event that significantly alters the pattern of variation in the structure or function of a system. \"Fragmentation\" is the breaking up of a habitat, ecosystem, or land-use type into smaller parcels. Disturbance is generally considered a natural process. Fragmentation causes land transformation, an important process in landscapes as development occurs.\n\nAn important consequence of repeated, random clearing (whether by natural disturbance or human activity) is that contiguous cover can break down into isolated patches. This happens when the area cleared exceed a critical level, which means that landscapes exhibit two phases: connected and disconnected.\n\nLandscape ecology theory stresses the role of human impacts on landscape structures and functions. It also proposes ways for restoring degraded landscapes. Landscape ecology explicitly includes humans as entities that cause functional changes on the landscape. Landscape ecology theory includes the landscape stability principle, which emphasizes the importance of landscape structural heterogeneity in developing resistance to disturbances, recovery from disturbances, and promoting total system stability. This principle is a major contribution to general ecological theories which highlight the importance of relationships among the various components of the landscape. Integrity of landscape components helps maintain resistance to external threats, including development and land transformation by human activity. Analysis of land use change has included a strongly geographical approach which has led to the acceptance of the idea of multifunctional properties of landscapes. There are still calls for a more unified theory of landscape ecology due to differences in professional opinion among ecologists and its interdisciplinary approach (Bastian 2001).\n\nAn important related theory is hierarchy theory, which refers to how systems of discrete functional elements operate when linked at two or more scales. For example, a forested landscape might be hierarchically composed of drainage basins, which in turn are composed of local ecosystems, which are in turn composed of individual trees and gaps. Recent theoretical developments in landscape ecology have emphasized the relationship between pattern and process, as well as the effect that changes in spatial scale has on the potential to extrapolate information across scales. Several studies suggest that the landscape has critical thresholds at which ecological processes will show dramatic changes, such as the complete transformation of a landscape by an invasive species due to small changes in temperature characteristics which favor the invasive's habitat requirements.\n\nDevelopments in landscape ecology illustrate the important relationships between spatial patterns and ecological processes. These developments incorporate quantitative methods that link spatial patterns and ecological processes at broad spatial and temporal scales. This linkage of time, space, and environmental change can assist managers in applying plans to solve environmental problems. The increased attention in recent years on spatial dynamics has highlighted the need for new quantitative methods that can analyze patterns, determine the importance of spatially explicit processes, and develop reliable models. Multivariate analysis techniques are frequently used to examine landscape level vegetation patterns. Studies use statistical techniques, such as cluster analysis, canonical correspondence analysis (CCA), or detrended correspondence analysis (DCA), for classifying vegetation. Gradient analysis is another way to determine the vegetation structure across a landscape or to help delineate critical wetland habitat for conservation or mitigation purposes (Choesin and Boerner 2002).\n\nClimate change is another major component in structuring current research in landscape ecology. Ecotones, as a basic unit in landscape studies, may have significance for management under climate change scenarios, since change effects are likely to be seen at ecotones first because of the unstable nature of a fringe habitat. Research in northern regions has examined landscape ecological processes, such as the accumulation of snow, melting, freeze-thaw action, percolation, soil moisture variation, and temperature regimes through long-term measurements in Norway. The study analyzes gradients across space and time between ecosystems of the central high mountains to determine relationships between distribution patterns of animals in their environment. Looking at where animals live, and how vegetation shifts over time, may provide insight into changes in snow and ice over long periods of time across the landscape as a whole.\n\nOther landscape-scale studies maintain that human impact is likely the main determinant of landscape pattern over much of the globe. Landscapes may become substitutes for biodiversity measures because plant and animal composition differs between samples taken from sites within different landscape categories. Taxa, or different species, can “leak” from one habitat into another, which has implications for landscape ecology. As human land use practices expand and continue to increase the proportion of edges in landscapes, the effects of this leakage across edges on assemblage integrity may become more significant in conservation. This is because taxa may be conserved across landscape levels, if not at local levels.\n\nLandscape ecology has been incorporated into a variety of ecological subdisciplines. For example, a recent development has been the more explicit consideration of spatial concepts and principles applied to the study of lakes, streams, and wetlands in the field of landscape limnology. Seascape ecology is a marine and coastal application of landscape ecology. In addition, landscape ecology has important links to application-oriented disciplines such as agriculture and forestry. In agriculture, landscape ecology has introduced new options for the management of environmental threats brought about by the intensification of agricultural practices. Agriculture has always been a strong human impact on ecosystems. In forestry, from structuring stands for fuelwood and timber to ordering stands across landscapes to enhance aesthetics, consumer needs have affected conservation and use of forested landscapes. Landscape forestry provides methods, concepts, and analytic procedures for landscape forestry. Landscape ecology has been cited as a contributor to the development of fisheries biology as a distinct biological science discipline, and is frequently incorporated in study design for wetland delineation in hydrology. It has helped shape integrated landscape management. Lastly, landscape ecology has been very influential for progressing sustainability science and sustainable development planning. In example, a recent study assessed sustainable urbanization across Europe using evaluation indices, country-landscapes, and landscape ecology tools and methods.\n\nLandscape ecology has also been combined with population genetics to form the field of landscape genetics, which addresses how landscape features influence the population structure and gene flow of plant and animal populations across space and time and on how the quality of intervening landscape, known as \"matrix,\" influences spatial variation. After the term was coined in 2003, the field of landscape genetics had expanded to over 655 studies by 2010, and continues to grow today. As genetic data has become more readily accessible, it is increasingly being used by ecologists to answer novel evolutionary and ecological questions, many with regard to how landscapes effect evolutionary processes, especially in human-modified landscapes, which are experiencing biodiversity loss.\n\n\n"}
{"id": "32528928", "url": "https://en.wikipedia.org/wiki?curid=32528928", "title": "List of place names of Dutch origin", "text": "List of place names of Dutch origin\n\nThe Dutch, aided by their skills in shipping, map making, finance and trade, traveled to every corner of the world and left their language embedded in names of places they visited. A fraction of these are still in use today.\n\nTo be included in this list, the place must have an article on Wikipedia or must have inline references showing the name is or was indeed Dutch. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[Category:Geography-related lists|Toponymy]]\n[[Category:Netherlands-related lists|place]]\n[[Category:Dutch language lists]]\n[[Category:Maritime history of the Dutch Republic]]\n[[Category:Dutch Empire]]"}
{"id": "2539168", "url": "https://en.wikipedia.org/wiki?curid=2539168", "title": "List of political and geographic borders", "text": "List of political and geographic borders\n\nBelow are separate lists of countries and dependencies with their land boundaries, and lists of which countries and dependencies border oceans and major seas. The first short section describes the borders or edges of continents and oceans/major seas. Disputed areas are not considered.\n\n\"(Notes: Dependencies and islands remote from continental land masses are not considered and are excluded from this list section; thus only continental land borders are considered. The only countries listed either straddle continents or are on a continent border.)\"\n\nSection Key: \n<br>\n( * ) = represents a country that crosses a continental land border\n\n\"(Notes: Dependencies are excluded from this section. See below. Only land boundaries are considered; maritime boundaries are excluded; see the List of countries and territories by maritime boundaries. Disputed territories are not considered, other than the inclusion by necessity, in a neutral fashion, of Western Sahara.)\"\n<br>\nSection Key: \n<br>\n( * ) = represents a sea (or other body of water)'s parent ocean; also it may represent a dependency's parent country\n\n\"(Note: All official and unofficial claims by countries and/or governments on the continent of Antarctica are excluded from this section list.)\"\n\n\"(Note: This section lists only dependencies with land boundaries)\"\nSection Key: \n<br>\n( * ) = represents a sea (or other body of water)'s parent ocean <br>\n\nSection Key: \n<br>\n( * ) = represents a dependency's parent country\n\n\"(Note: Seas are excluded from this list section.)\"\n\nSection Key: \n<br>\n( * ) = represents a dependency's parent country\n\n\"(Note: This section only includes large bodies of water that are designated as a \"sea\" by a large portion of geographers worldwide.)\"\n\n\n"}
{"id": "18596875", "url": "https://en.wikipedia.org/wiki?curid=18596875", "title": "List of political and geographic subdivisions by total area from 500,000 to 1,000,000 square kilometers", "text": "List of political and geographic subdivisions by total area from 500,000 to 1,000,000 square kilometers\n"}
{"id": "45515618", "url": "https://en.wikipedia.org/wiki?curid=45515618", "title": "List of presidential trips made by Barack Obama during 2015", "text": "List of presidential trips made by Barack Obama during 2015\n\nThis is a list of presidential trips made by Barack Obama during 2015, the seventh year of his presidency as the 44th President of the United States.\n\nThis list excludes trips made within Washington, D.C., the U.S. federal capital in which the White House, the official residence and principal workplace of the President, is located. Additionally excluded are trips to Camp David, the country residence of the President, and to the private home of the Obama family in Kenwood, Chicago, Illinois.\n"}
{"id": "39568382", "url": "https://en.wikipedia.org/wiki?curid=39568382", "title": "List of presidential trips made by Christian Wulff", "text": "List of presidential trips made by Christian Wulff\n\nThis is a list of presidential visits to foreign countries made by Christian Wulff as President of Germany. Wulff held the office from his election on 2 July 2010 until his resignation on 17 February 2012.\n\n"}
{"id": "11486221", "url": "https://en.wikipedia.org/wiki?curid=11486221", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: U", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: U\n\n\n"}
{"id": "11486275", "url": "https://en.wikipedia.org/wiki?curid=11486275", "title": "List of towns and cities with 100,000 or more inhabitants/cityname: Z", "text": "List of towns and cities with 100,000 or more inhabitants/cityname: Z\n\n\n"}
{"id": "37695547", "url": "https://en.wikipedia.org/wiki?curid=37695547", "title": "List of tumps", "text": "List of tumps\n\nTump means a hillock, mound, barrow or tumulus. The Welsh words \"twmp\" and \"Twmpath\" may be related. Although some may appear similar to glacial drumlins, for the most part they are man-made, e.g. remains from mineral extraction, burial mounds (tumuli and especially bowl barrows) or motte-and-bailey castle mounds. The following geographical features in the UK are referred to using the word:\n\n"}
{"id": "42037557", "url": "https://en.wikipedia.org/wiki?curid=42037557", "title": "Lowest bridging point", "text": "Lowest bridging point\n\nThe lowest bridging point is the location on a river which is crossed by a bridge at its closest point to the sea.\n\nHistorically - that is, before the development of engineering technology that allowed the construction of tunnels and high-level road bridges - the lowest bridging point of a river was frequently the point at which an important town or city grew up, and particularly where trade and commerce took place. The place could be served by roads on either side of the river, allowing access from a wide hinterland; had river transport available upstream; and often was at a location that allowed seagoing traffic to approach it from a downstream direction.\n\nExamples of lowest bridging points in Britain include London, the lowest bridging point on the Thames; Lancaster, the lowest bridging point on the Lune; York, the lowest bridging point on the Ouse; and Gloucester, the lowest bridging point on the Severn. Glasgow grew up at the lowest bridging point on the Clyde which was about twelve miles upstream from its lowest fording point at Dumbuck. Mathew Paris’s map of 1247 appears to show only one bridge in the whole of Britain: at Stirling, the lowest bridging point on the River Forth.\n"}
{"id": "6252791", "url": "https://en.wikipedia.org/wiki?curid=6252791", "title": "Marconi Plaza, Philadelphia", "text": "Marconi Plaza, Philadelphia\n\nMarconi Plaza is an urban park square located in South Philadelphia, Philadelphia, Pennsylvania. The Plaza was named to recognize the 20th Century cultural identity in Philadelphia of the surrounding Italian-American enclave neighborhood and became the designation location of the annual Columbus Day Parade.\n\nMarconi Plaza has two main halves, East and West, which are divided in the middle by Broad Street. It is located at the most southern end of the city and within the northern border of the Sports Complex Special Services District. The park plaza is accessible via the Oregon Avenue Station of the Broad Street Subway. \nBoundaries of Marconi Plaza Neighborhood:\nThe urban park plaza itself, from which the neighborhood derives its name(Marconi East and \"Marco\" Marconi West), is a rectangular park. The Roman styled plaza is divided in the center by Broad Street and is bordered by 13th Street, 15th Street, Bigler Street, and Oregon Avenue.\n\nThe plaza design is credited to the strong influence of renowned architect Paul Philippe Cret in 1904 as part of his participation in the Art Jury reviewing the preliminary plans presented by landscape architects the Olmsted Brothers, who were then charged with a modified design to complete the work. \n. The Plaza later served as the grand pre-entrance for the 1926 Sesquicentennial Exposition, leading visitors south along a tree lined Southern Boulevard Parkway (Philadelphia, Pennsylvania) ( landscaped segment of South Broad Street) to the exhibition grounds that started at Packer Avenue and continued to League Island Park. This neighborhood twin park is mirrored on both sides of Broad Street and became property of the Fairmount Park system. It held the common name of Oregon Plaza until October 18, 1937 when it was officially named Marconi Plaza in honor of the Nobel Prize Laureate Guglielmo Marconi, the inventor of Radio.\n\nThe F. Amadee Bregy School was added to the National Register of Historic Places in 1988.\n\nThe original design of the Plaza was a two level terrace with pathways, marble trims, urns, influenced by landscaped architecture modeling after Roman gardens and English gardens. The east and west plaza reflected the same winding pathways leading to a raised stepped terrace surrounded by stone railings and entrance sculptures of large urns with two small \"reflecting\" pools of water facing Broad Street at the center point, which at that time was cut away from the curbline forming half circles open to traffic on both the east and west. This accent was used in 1926 to position a large Liberty Bell at the center of the street permitting traffic to circle around.\n\nOver the years many of the fine details have been erased including the half circled indented curbline on either side of Broad Street at the center. This location also had on both sides of the plaza, two reflecting pools of water. The pools were filled in to provide the foundation for the two statues that were later erected to support the cultural history of the immigrant Italian community and respond to Anti-Italianism.\n\nThe park is currently lushly covered with 25% trees adorned with park benches, open areas for two tot lots, a baseball field, basketball court, and country cottage style enclosed bocce court. The sidewalk border surrounding the park is densely lined with large maple trees with heights of 30–50 feet high.\n\nA bronze statue of Guglielmo Marconi, sculpted by Saleppichi Giancarlo was erected on the east Plaza in 1975 though the efforts of the Marconi Memorial Association headed by Dr. Frank P. DiDio. The statue was dedicated on April 25, 1980, to commemorate the 106th anniversary of the birthday of the world-famous Italian scientist and inventor.\n\nA statue of Christopher Columbus was erected on the west plaza in 1982. This work was originally located along Belmont Avenue in Fairmount Park, having been erected on October 12, 1876 for Philadelphia's Centennial Exposition. Thought to be the work of Emanuele Caroni, this is said to be first publicly funded monument to Christopher Columbus in the United States. It was purchased for $18,000 with money raised by Italian-Americans and the Columbus Monument Association, through the efforts of Alonzo Viti of Philadelphia and his brothers. The statue's initial installation began an annual tradition for the colony of mostly Italian Americans in South Philadelphia to march each year on Columbus Day to the statue in Fairmount Park. The journey was found to be too exhausting and in 1920 the celebration changed locations.\n\nMollbore Terraces of Marconi: The 1930s Mollbore Terrace was a unique urban change from the densely lined row houses that characterized most of South Philadelphia. The design included front porches and a rear yard with an access service roadway for trash pick-up. Three separate Mollbore Terrace sections were constructed east of the plaza within the boundaries of 13th Street to 7th Street, and from Oregon Ave to Johnston Street. The layout departed from the standard street grid, offsetting the numbered streets that permitted placing a \"mini-public-square\" of green space for houses to face inward on all four sides and directions. The center large rectangular common parks space was originally designated as a \"Terrace\" that included pathways, grass and trees with an octagon-shaped wading pool at the west end and a raised octagon sand pit platform with a flag pole at the east end.\n\nRoman Terraces of Marconi: The Greco-Roman–accented homes west of the plaza from 15th to 19th street, using the same concept but on a smaller scale, include two oval-shaped terrace streets at Smedley and Colorado. The terrace at Colorado Street became well known citywide for its annual decorations and street lighting during the Christmas holidays from 1950 to 2000.\n\nMoyamensing Avenue Parkway of Marconi: This main angular dual street with an approximately 50-foot center median landscaped area and tree-lined street, crosses the standard street grid and was designed as an alternative roadway access to the 1926 Sesquicentennial Exposition. It begins at Oregon Avenue, that once was a headhouse entrance for the 1926 Expo, through to the intersection of 20th Street, Penrose Avenue and Packer Avenue. An architectural design for a grand public square like the squares of Center City Philadelphia (inspired by the Benjamin Franklin Parkway) was planned at the parkway's end point of Penrose Avenue, which was viewed by city planners to be the significant southern gateway to the City. The 1926 square was never developed.\n\nIn 2002 the City of Philadelphia legislated boundaries of the Sports Complex Special Service District. The residential communities defined included Marconi Plaza. The Special District established an overlay providing the basis for a new definition to Marconi East as community \"three\" and Marconi West as community \"four\".\n\n\n"}
{"id": "21836", "url": "https://en.wikipedia.org/wiki?curid=21836", "title": "North Pole", "text": "North Pole\n\nThe North Pole, also known as the Geographic North Pole or Terrestrial North Pole, is (subject to the caveats explained below) defined as the point in the Northern Hemisphere where the Earth's axis of rotation meets its surface.\n\nThe North Pole is the northernmost point on the Earth, lying diametrically opposite the South Pole. It defines geodetic latitude 90° North, as well as the direction of true north. At the North Pole all directions point south; all lines of longitude converge there, so its longitude can be defined as any degree value. Along tight latitude circles, counterclockwise is east and clockwise is west. The North Pole is at the center of the Northern Hemisphere.\n\nWhile the South Pole lies on a continental land mass, the North Pole is located in the middle of the Arctic Ocean amid waters that are almost permanently covered with constantly shifting sea ice. This makes it impractical to construct a permanent station at the North Pole (unlike the South Pole). However, the Soviet Union, and later Russia, constructed a number of manned drifting stations on a generally annual basis since 1937, some of which have passed over or very close to the Pole. Since 2002, the Russians have also annually established a base, Barneo, close to the Pole. This operates for a few weeks during early spring. Studies in the 2000s predicted that the North Pole may become seasonally ice-free because of Arctic ice shrinkage, with timescales varying from 2016 to the late 21st century or later.\n\nThe sea depth at the North Pole has been measured at by the Russian Mir submersible in 2007 and at 4,087 m (13,410 ft) by USS \"Nautilus\" in 1958. The nearest land is usually said to be Kaffeklubben Island, off the northern coast of Greenland about away, though some perhaps semi-permanent gravel banks lie slightly closer. The nearest permanently inhabited place is Alert in the Qikiqtaaluk Region, Nunavut, Canada, which is located from the Pole.\n\nThe Earth's axis of rotation – and hence the position of the North Pole – was commonly believed to be fixed (relative to the surface of the Earth) until, in the 18th century, the mathematician Leonhard Euler predicted that the axis might \"wobble\" slightly. Around the beginning of the 20th century astronomers noticed a small apparent \"variation of latitude,\" as determined for a fixed point on Earth from the observation of stars. Part of this variation could be attributed to a wandering of the Pole across the Earth's surface, by a range of a few metres. The wandering has several periodic components and an irregular component. The component with a period of about 435 days is identified with the eight-month wandering predicted by Euler and is now called the Chandler wobble after its discoverer. The exact point of intersection of the Earth's axis and the Earth's surface, at any given moment, is called the \"instantaneous pole\", but because of the \"wobble\" this cannot be used as a definition of a fixed North Pole (or South Pole) when metre-scale precision is required.\n\nIt is desirable to tie the system of Earth coordinates (latitude, longitude, and elevations or orography) to fixed landforms. Of course, given plate tectonics and isostasy, there is no system in which all geographic features are fixed. Yet the International Earth Rotation and Reference Systems Service and the International Astronomical Union have defined a framework called the International Terrestrial Reference System.\n\nAs early as the 16th century, many prominent people correctly believed that the North Pole was in a sea, which in the 19th century was called the Polynya or Open Polar Sea. It was therefore hoped that passage could be found through ice floes at favorable times of the year. Several expeditions set out to find the way, generally with whaling ships, already commonly used in the cold northern latitudes.\n\nOne of the earliest expeditions to set out with the explicit intention of reaching the North Pole was that of British naval officer William Edward Parry, who in 1827 reached latitude 82°45′ North. In 1871 the Polaris expedition, a US attempt on the Pole led by Charles Francis Hall, ended in disaster. Another British Royal Navy attempt on the pole, part of the British Arctic Expedition, by Commander Albert H. Markham reached a then-record 83°20'26\" North in May 1876 before turning back. An 1879–1881 expedition commanded by US naval officer George W. DeLong ended tragically when their ship, the USS \"Jeanette\", was crushed by ice. Over half the crew, including DeLong, were lost.\nIn April 1895 the Norwegian explorers Fridtjof Nansen and Hjalmar Johansen struck out for the Pole on skis after leaving Nansen's icebound ship \"Fram\". The pair reached latitude 86°14′ North before they abandoned the attempt and turned southwards, eventually reaching Franz Josef Land.\n\nIn 1897 Swedish engineer Salomon August Andrée and two companions tried to reach the North Pole in the hydrogen balloon \"Örnen\" (\"Eagle\"), but came down north of Kvitøya, the northeasternmost part of the Svalbard archipelago. They trekked to Kvitøya but died there three months later. In 1930 the remains of this expedition were found by the Norwegian Bratvaag Expedition.\n\nThe Italian explorer Luigi Amedeo, Duke of the Abruzzi and Captain Umberto Cagni of the Italian Royal Navy (Regia Marina) sailed the converted whaler \"Stella Polare\" (\"Pole Star\") from Norway in 1899. On 11 March 1900 Cagni led a party over the ice and reached latitude 86° 34’ on 25 April, setting a new record by beating Nansen's result of 1895 by . Cagni barely managed to return to the camp, remaining there until 23 June. On 16 August the \"Stella Polare\" left Rudolf Island heading south and the expedition returned to Norway.\n\nThe US explorer Frederick Cook claimed to have reached the North Pole on 21 April 1908 with two Inuit men, Ahwelah and Etukishook, but he was unable to produce convincing proof and his claim is not widely accepted.\n\nThe conquest of the North Pole was for many years credited to US Navy engineer Robert Peary, who claimed to have reached the Pole on 6 April 1909, accompanied by Matthew Henson and four Inuit men, Ootah, Seeglo, Egingwah, and Ooqueah. However, Peary's claim remains highly disputed and controversial. Those who accompanied Peary on the final stage of the journey were not trained in [Western] navigation, and thus could not independently confirm his navigational work, which some claim to have been particularly sloppy as he approached the Pole.\n\nThe distances and speeds that Peary claimed to have achieved once the last support party turned back seem incredible to many people, almost three times that which he had accomplished up to that point. Peary's account of a journey to the Pole and back while traveling along the direct line – the only strategy that is consistent with the time constraints that he was facing – is contradicted by Henson's account of tortuous detours to avoid pressure ridges and open leads.\n\nThe British explorer Wally Herbert, initially a supporter of Peary, researched Peary's records in 1989 and found that there were significant discrepancies in the explorer's navigational records. He concluded that Peary had not reached the Pole. Support for Peary came again in 2005, however, when British explorer Tom Avery and four companions recreated the outward portion of Peary's journey with replica wooden sleds and Canadian Eskimo Dog teams, reaching the North Pole in 36 days, 22 hours – nearly five hours faster than Peary. However, Avery's fastest 5-day march was 90 nautical miles, significantly short of the 135 claimed by Peary. Avery writes on his web site that \"The admiration and respect which I hold for Robert Peary, Matthew Henson and the four Inuit men who ventured North in 1909, has grown enormously since we set out from Cape Columbia. Having now seen for myself how he travelled across the pack ice, I am more convinced than ever that Peary did indeed discover the North Pole.\"\nAnother rejection of Peary's claim arrived in 2009, when E. Myles Standish of the California Institute of Technology, an experienced referee of scientific claims, reported numerous alleged lacunae and inconsistencies.\n\nThe first claimed flight over the Pole was made on 9 May 1926 by US naval officer Richard E. Byrd and pilot Floyd Bennett in a Fokker tri-motor aircraft. Although verified at the time by a committee of the National Geographic Society, this claim has since been undermined by the 1996 revelation that Byrd's long-hidden diary's solar sextant data (which the NGS never checked) consistently contradict his June 1926 report's parallel data by over . The secret report's alleged en-route solar sextant data were inadvertently so impossibly overprecise that he excised all these alleged raw solar observations out of the version of the report finally sent to geographical societies five months later (while the original version was hidden for 70 years), a realization first published in 2000 by the University of Cambridge after scrupulous refereeing.\n\nAccording to Standish, \"Anyone who is acquainted with the facts and has any amount of logical reasoning can not avoid the conclusion that neither Cook, nor Peary, nor Byrd reached the North Pole; and they all knew it.\"\n\nThe first consistent, verified, and scientifically convincing attainment of the Pole was on 12 May 1926, by Norwegian explorer Roald Amundsen and his US sponsor Lincoln Ellsworth from the airship \"Norge\". \"Norge\", though Norwegian-owned, was designed and piloted by the Italian Umberto Nobile. The flight started from Svalbard in Norway, and crossed the Arctic Ocean to Alaska. Nobile, with several scientists and crew from the \"Norge\", overflew the Pole a second time on 24 May 1928, in the airship \"Italia\". The \"Italia\" crashed on its return from the Pole, with the loss of half the crew.\n\nIn May 1937 the world's first North Pole ice station, North Pole-1, was established by Soviet scientists by air 20 kilometres (13 mi) from the North Pole. The expedition members: oceanographer Pyotr Shirshov, meteorologist Yevgeny Fyodorov, radio operator Ernst Krenkel, and the leader Ivan Papanin conducted scientific research at the station for the next nine months. By 19 February 1938, when the group was picked up by the ice breakers \"Taimyr\" and \"Murman\", their station had drifted 2850 km to the eastern coast of Greenland.\n\nIn May 1945 an RAF Lancaster of the \"Aries\" expedition became the first Commonwealth aircraft to overfly the North Geographic and North Magnetic Poles. The plane was piloted by David Cecil McKinley of the Royal Air Force. It carried an 11-man crew, with Kenneth C. Maclure of the Royal Canadian Air Force in charge of all scientific observations. In 2006, Maclure was honoured with a spot in Canada's Aviation Hall of Fame.\n\nDiscounting Peary's disputed claim, the first men to set foot at the North Pole were a Soviet party including geophysicists Mikhail Ostrekin and Pavel Senko, oceanographers Mikhail Somov and Pavel Gordienko, and other scientists and flight crew (24 people in total) of Aleksandr Kuznetsov's \"Sever-2\" expedition (March–May 1948). It was organized by the Chief Directorate of the Northern Sea Route. The party flew on three planes (pilots Ivan Cherevichnyy, Vitaly Maslennikov and Ilya Kotov) from Kotelny Island to the North Pole and landed there at 4:44pm (Moscow Time, ) on 23 April 1948. They established a temporary camp and for the next two days conducted scientific observations. On 26 April the expedition flew back to the continent.\n\nNext year, on 9 May 1949 two other Soviet scientists (Vitali Volovich and Andrei Medvedev) became the first people to parachute onto the North Pole. They jumped from a Douglas C-47 Skytrain, registered CCCP H-369.\n\nOn 3 May 1952 U.S. Air Force Lieutenant Colonel Joseph O. Fletcher and Lieutenant William Pershing Benedict, along with scientist Albert P. Crary, landed a modified Douglas C-47 Skytrain at the North Pole. Some Western sources considered this to be the first landing at the Pole until the Soviet landings became widely known.\nThe United States Navy submarine \"USS Nautilus\" (SSN-571) crossed the North Pole on 3 August 1958. On 17 March 1959 \"USS Skate\" (SSN-578) surfaced at the Pole, breaking through the ice above it, becoming the first naval vessel to do so.\n\nSetting aside Peary's claim, the first confirmed surface conquest of the North Pole was that of Ralph Plaisted, Walt Pederson, Gerry Pitzl and Jean Luc Bombardier, who traveled over the ice by snowmobile and arrived on 19 April 1968. The United States Air Force independently confirmed their position.\n\nOn 6 April 1969 Wally Herbert and companions Allan Gill, Roy Koerner and Kenneth Hedges of the British Trans-Arctic Expedition became the first men to reach the North Pole on foot (albeit with the aid of dog teams and airdrops). They continued on to complete the first surface crossing of the Arctic Ocean – and by its longest axis, Utqiagvik, Alaska to Svalbard – a feat that has never been repeated. Because of suggestions (later proven false) of Plaisted's use of air transport, some sources classify Herbert's expedition as the first confirmed to reach the North Pole over the ice surface by any means. In the 1980s Plaisted's pilots Weldy Phipps and Ken Lee signed affidavits asserting that no such airlift was provided. It is also said that Herbert was the first person to reach the pole of inaccessibility.\nOn 17 August 1977 the Soviet nuclear-powered icebreaker \"Arktika\" completed the first surface vessel journey to the North Pole.\n\nIn 1982 Ranulph Fiennes and Charles R. Burton became the first people to cross the Arctic Ocean in a single season. They departed from Cape Crozier, Ellesmere Island, on 17 February 1982 and arrived at the geographic North Pole on 10 April 1982. They travelled on foot and snowmobile. From the Pole, they travelled towards Svalbard but, due to the unstable nature of the ice, ended their crossing at the ice edge after drifting south on an ice floe for 99 days. They were eventually able to walk to their expedition ship \"MV Benjamin Bowring\" and boarded it on 4 August 1982 at position 80:31N 00:59W. As a result of this journey, which formed a section of the three-year Transglobe Expedition 1979–1982, Fiennes and Burton became the first people to complete a circumnavigation of the world via both North and South Poles, by surface travel alone. This achievement remains unchallenged to this day.\n\nIn 1985 Sir Edmund Hillary (the first man to stand on the summit of Mount Everest) and Neil Armstrong (the first man to stand on the moon) landed at the North Pole in a small twin-engined ski plane. Hillary thus became the first man to stand at both poles and on the summit of Everest.\n\nIn 1986 Will Steger, with seven teammates, became the first to be confirmed as reaching the Pole by dogsled and without resupply.\n\nUSS \"Gurnard\" (SSN-662) operated in the Arctic Ocean under the polar ice cap from September to November 1984 in company with one of her sister ships, the attack submarine USS \"Pintado\" (SSN-672). On 12 November 1984 \"Gurnard\" and \"Pintado\" became the third pair of submarines to surface together at the North Pole. In March 1990, \"Gurnard\" deployed to the Arctic region during exercise Ice Ex '90 and completed only the fourth winter submerged transit of the Bering and Seas. \"Gurnard\" surfaced at the North Pole on April 18, in the company of the USS \"Seahorse\" (SSN-669).\n\nOn 6 May 1986 USS \"Archerfish\" (SSN 678), USS \"Ray\" (SSN 653) and USS \"Hawkbill\" (SSN-666) surfaced at the North Pole, the first tri-submarine surfacing at the North Pole.\n\nOn 21 April 1987 Shinji Kazama of Japan became the first person to reach the North Pole on a motorcycle.\n\nOn 18 May 1987 USS \"Billfish\" (SSN 676), USS \"Sea Devil\" (SSN 664) and HMS \"Superb\" (S 109) surfaced at the North Pole, the first international surfacing at the North Pole.\n\nIn 1988 a 13-man strong team (9 Soviets, 4 Canadians) skied across the arctic from Siberia to northern Canada. One of the Canadians, Richard Weber became the first person to reach the Pole from both sides of the Arctic Ocean.\n\nOn 4 May 1990 Børge Ousland and Erling Kagge became the first explorers ever to reach the North Pole unsupported, after a 58-day ski trek from Ellesmere Island in Canada, a distance of 800 km.\n\nOn 7 September 1991 the German research vessel \"Polarstern\" and the Swedish icebreaker \"Oden\" reached the North Pole as the first conventional powered vessels. Both scientific parties and crew took oceanographic and geological samples and had a common tug of war and a football game on an ice floe. Polarstern again reached the pole exactly 10 years later with the \"Healy\".\n\nIn 1998, 1999, and 2000 Lada Niva Marshs (special very large wheeled versions made by BRONTO, Lada/Vaz's experimental product division) were driven to the North Pole. The 1998 expedition was dropped by parachute and completed the track to the North Pole. The 2000 expedition departed from a Russian research base around 114 km from the Pole and claimed an average speed of 20–15 km/h in an average temperature of −30 °C.\n\nCommercial airliner flights on the Polar routes may pass within viewing distance of the North Pole. For example, the flight from Chicago to Beijing may come close as latitude 89° N, though because of prevailing winds return journeys go over the Bering Strait. In recent years journeys to the North Pole by air (landing by helicopter or on a runway prepared on the ice) or by icebreaker have become relatively routine, and are even available to small groups of tourists through adventure holiday companies. Parachute jumps have frequently been made onto the North Pole in recent years. The temporary seasonal Russian camp of Barneo has been established by air a short distance from the Pole annually since 2002, and caters for scientific researchers as well as tourist parties. Trips from the camp to the Pole itself may be arranged overland or by helicopter.\n\nThe first attempt at underwater exploration of the North Pole was made on 22 April 1998 by Russian firefighter and diver Andrei Rozhkov with the support of the Diving Club of Moscow State University, but ended in fatality. The next attempted dive at the North Pole was organized the next year by the same diving club, and ended in success on 24 April 1999. The divers were Michael Wolff (Austria), Brett Cormick (UK), and Bob Wass (USA).\n\nIn 2005 the United States Navy submarine USS \"Charlotte\" (SSN-766) surfaced through of ice at the North Pole and spent 18 hours there.\n\nIn July 2007 British endurance swimmer Lewis Gordon Pugh completed a swim at the North Pole. His feat, undertaken to highlight the effects of global warming, took place in clear water that had opened up between the ice floes. His later attempt to paddle a kayak to the North Pole in late 2008, following the erroneous prediction of clear water to the Pole, was stymied when his expedition found itself stuck in thick ice after only three days. The expedition was then abandoned.\n\nBy September 2007 the North Pole had been visited 66 times by different surface ships: 54 times by Soviet and Russian icebreakers, 4 times by Swedish \"Oden\", 3 times by German \"Polarstern\", 3 times by USCGC \"Healy\" and USCGC \"Polar Sea\", and once by CCGS \"Louis S. St-Laurent\" and by Swedish \"Vidar Viking\".\n\nOn 2 August 2007 a Russian scientific expedition Arktika 2007 made the first ever manned descent to the ocean floor at the North Pole, to a depth of , as part of the research programme in support of Russia's 2001 extended continental shelf claim to a large swathe of the Arctic Ocean floor. The descent took place in two MIR submersibles and was led by Soviet and Russian polar explorer Artur Chilingarov. In a symbolic act of visitation, the Russian flag was placed on the ocean floor exactly at the Pole.\n\nThe expedition was the latest in a series of efforts intended to give Russia a dominant influence in the Arctic according to \"The New York Times\". The warming Arctic climate and summer shrinkage of the iced area has attracted the attention of many countries, such as China and the United States, toward the top of the world, where resources and shipping routes may soon be exploitable.\n\nIn 2009 the Russian Marine Live-Ice Automobile Expedition (MLAE-2009) with Vasily Elagin as a leader and a team of Afanasy Makovnev, Vladimir Obikhod, Alexey Shkrabkin, Sergey Larin, Alexey Ushakov and Nikolay Nikulshin reached the North Pole on two custom-built 6 x 6 low-pressure-tire ATVs — Yemelya-1 and Yemelya-2, designed by Vasily Elagin, a known Russian mountain climber, explorer and engineer. The vehicles reached the North Pole on 26 April 2009, 17:30 (Moscow time). The expedition was partly supported by Russian State Aviation. The Russian Book of Records recognized it as the first successful vehicle trip from land to the Geographical North Pole.\n\nOn 1 March 2013 the Russian Marine Live-Ice Automobile Expedition (MLAE 2013) with Vasily Elagin as a leader, and a team of Afanasy Makovnev, Vladimir Obikhod, Alexey Shkrabkin, Andrey Vankov, Sergey Isayev and Nikolay Kozlov on two custom-built 6 x 6 low-pressure-tire ATVs — Yemelya-3 and Yemelya-4,— started from Golomyanny Island (the Severnaya Zemlya Archipelago) to the North Pole across drifting ice of the Arctic Ocean. The vehicles reached the Pole on 6 April and then continued to the Canadian coast. The coast was reached on 30 April 2013 (83°08N, 075°59W Ward Hant Island), and on 5 May 2013 the expedition finished in Resolute Bay, NU. The way between the Russian borderland (Machtovyi Island of the Severnaya Zemlya Archipelago, 80°15N, 097°27E) and the Canadian coast (Ward Hant Island, 83°08N, 075°59W) took 55 days; it was ~2300 km across drifting ice and about 4000 km in total. The expedition was totally self-dependent and used no external supplies. The expedition was supported by the Russian Geographical Society.\n\nThe sun at the North Pole is continuously above the horizon during the summer and continuously below the horizon during the winter. Sunrise is just before the March equinox (around 20 March); the sun then takes three months to reach its highest point of near 23½° elevation at the summer solstice (around 21 June), after which time it begins to sink, reaching sunset just after the September equinox (around 23 September). When the sun is visible in the polar sky, it appears to move in a horizontal circle above the horizon. This circle gradually rises from near the horizon just after the vernal equinox to its maximum elevation (in degrees) above the horizon at summer solstice and then sinks back toward the horizon before sinking below it at the autumnal equinox. Hence the North and South Poles experience the slowest rates of sunrise and sunset on Earth.\n\nA civil twilight period of about two weeks occurs before sunrise and after sunset, a nautical twilight period of about five weeks occurs before sunrise and after sunset and an astronomical twilight period of about seven weeks occurs before sunrise and after sunset.\n\nThese effects are caused by a combination of the Earth's axial tilt and its revolution around the sun. The direction of the Earth's axial tilt, as well as its angle relative to the plane of the Earth's orbit around the sun, remains very nearly constant over the course of a year (both change very slowly over long time periods). At northern midsummer the North Pole is facing towards the sun to its maximum extent. As the year progresses and the Earth moves around the sun, the North Pole gradually turns away from the sun until at midwinter it is facing away from the Sun to its maximum extent. A similar sequence is observed at the South Pole, with a six-month time difference.\n\nIn most places on Earth, local time is determined by longitude, such that the time of day is more-or-less synchronised to the position of the sun in the sky (for example, at midday the sun is roughly at its highest). This line of reasoning fails at the North Pole, where the sun rises and sets only once per year, and all lines of longitude, and hence all time zones, converge. There is no permanent human presence at the North Pole and no particular time zone has been assigned. Polar expeditions may use any time zone that is convenient, such as Greenwich Mean Time, or the time zone of the country from which they departed.\n\nThe North Pole is substantially warmer than the South Pole because it lies at sea level in the middle of an ocean (which acts as a reservoir of heat), rather than at altitude on a continental land mass. Despite being an ice cap, it shares some characteristics with a tundra climate (\"ETf\") due to the July and August temperatures peaking just above freezing.\n\nWinter temperatures at the northernmost weather station in Greenland can range from about , averaging around , with the North Pole being slightly colder. However, a freak storm caused the temperature to reach for a time at a World Meteorological Organization buoy, located at 87.45°N, on December 30, 2015. It was estimated that the temperature at the North Pole was between during the storm. Summer temperatures (June, July, and August) average around the freezing point (). The highest temperature yet recorded is , much warmer than the South Pole's record high of only . A similar spike in temperatures occurred on November 15, 2016 when temperatures hit freezing. Yet again, February 2018 featured a storm so powerful that temperatures at Cape Morris Jesup, the world's northernmost weather station in Greenland, reached and spent 24 straight hours above freezing. Meanwhile, the pole itself was estimated to reach a high temperature of . This same temperature of was also recorded at the Hollywood Burbank Airport in Los Angeles at the very same time.\n\nThe sea ice at the North Pole is typically around thick, although ice thickness, its spatial extent, and the fraction of open water within the ice pack can vary rapidly and profoundly in response to weather and climate. Studies have shown that the average ice thickness has decreased in recent years. It is likely that global warming has contributed to this, but it is not possible to attribute the recent abrupt decrease in thickness entirely to the observed warming in the Arctic. Reports have also predicted that within a few decades the Arctic Ocean will be entirely free of ice in the summer. This may have significant commercial implications; see \"Territorial Claims,\" below.\n\nThe retreat of the Arctic sea ice will accelerate global warming, as less ice cover reflects less solar radiation, and may have serious climate implications by contributing to Arctic cyclone generation.\nPolar bears are believed to travel rarely beyond about 82° North owing to the scarcity of food, though tracks have been seen in the vicinity of the North Pole, and a 2006 expedition reported sighting a polar bear just from the Pole. The ringed seal has also been seen at the Pole, and Arctic foxes have been observed less than away at 89°40′ N.\n\nBirds seen at or very near the Pole include the snow bunting, northern fulmar and black-legged kittiwake, though some bird sightings may be distorted by the tendency of birds to follow ships and expeditions.\n\nFish have been seen in the waters at the North Pole, but these are probably few in number. A member of the Russian team that descended to the North Pole seabed in August 2007 reported seeing no sea creatures living there. However, it was later reported that a sea anemone had been scooped up from the seabed mud by the Russian team and that video footage from the dive showed unidentified shrimps and amphipods.\n\nCurrently, under international law, no country owns the North Pole or the region of the Arctic Ocean surrounding it. The five surrounding Arctic countries, Russian Federation, Canada, Norway, Denmark (via Greenland), and the United States, are limited to a exclusive economic zone off their coasts, and the area beyond that is administered by the International Seabed Authority.\n\nUpon ratification of the United Nations Convention on the Law of the Sea, a country has 10 years to make claims to an extended continental shelf beyond its 200-mile exclusive economic zone. If validated, such a claim gives the claimant state rights to what may be on or beneath the sea bottom within the claimed zone. Norway (ratified the convention in 1996), Russia (ratified in 1997), Canada (ratified in 2003) and Denmark (ratified in 2004) have all launched projects to base claims that certain areas of Arctic continental shelves should be subject to their sole sovereign exploitation.\n\nIn 1907 Canada invoked a \"sector principle\" to claim sovereignty over a sector stretching from its coasts to the North Pole. This claim has not been relinquished, but was not consistently pressed until 2013.\n\nIn some children's Western cultures, the geographic North Pole is described as the location of Santa Claus' workshop and residence, although the depictions have been inconsistent between the geographic and magnetic North Pole. Canada Post has assigned postal code H0H 0H0 to the North Pole (referring to Santa's traditional exclamation of \"Ho ho ho!\").\n\nThis association reflects an age-old esoteric mythology of Hyperborea that posits the North Pole, the otherworldly world-axis, as the abode of God and superhuman beings. The popular figure of the pole-dwelling Santa Claus thus functions as an archetype of spiritual purity and transcendence.\n\nAs Henry Corbin has documented, the North Pole plays a key part in the cultural worldview of Sufism and Iranian mysticism. \"The Orient sought by the mystic, the Orient that cannot be located on our maps, is in the direction of the north, beyond the north.\"\n\nOwing to its remoteness, the Pole is sometimes identified with a mysterious mountain of ancient Iranian tradition called Mount Qaf (Jabal Qaf), the \"farthest point of the earth\". According to certain authors, the Jabal Qaf of Muslim cosmology is a version of Rupes Nigra, a mountain whose ascent, like Dante's climbing of the Mountain of Purgatory, represents the pilgrim's progress through spiritual states. In Iranian theosophy, the heavenly Pole, the focal point of the spiritual ascent, acts as a magnet to draw beings to its \"palaces ablaze with immaterial matter.\" \n\n\n"}
{"id": "4329386", "url": "https://en.wikipedia.org/wiki?curid=4329386", "title": "Philosophy of environment", "text": "Philosophy of environment\n\nThe philosophy of environment is a trend of free thought located between philosophy, epistemology and anthropology. It combines various schools of philosophy such as humanist ecology, philosophy of evolution and environmental humanism. It is also meant to be a cultural trend having an influence in society.\n\nThe philosophical current indicated under the name philosophy of evolution has been developed since the 1970s by Humanist Ecology (also called Environmental or Evolutive Humanism) and by the neo-darwinian school. Its purpose is the long-term evolution of the complex living being in its universal environment, analysed through its Human cultural expression.\n\nThis trend is unique to the quasi religious Evolutionist Humanism advocated by Julian Huxley, notably during the founders congress of the IHEU (International Humanist and Ethical Union) in 1952.\n\nFurther than the basic scientific theory of evolution (notably neo-darwinian, Evolutive Humanism, developed by Richard Dawkins and Stephen Jay Gould), looks at the necessity for Man of a permanent adaptation of both his organism and his thoughts to his universal environment. And from there to the relativized relation to belief and to uncertainty, because human ideas evolve and are transformed, as are physical qualities, in a universal process of evolution and adaptive improvement in their environment. But this does not suppose determinism, genetic or cultural, it refers to the evolutive interactivity and reactivity, partly random, of the living being with his surroundings. According to Marc Carl, \"Man must necessarily learn to manage the insufficient appearance and the uncertainty of information to fit and to develop in his relational interactions with his environment, as well as physically and culturally\". In a universal environment still as much as 90% unknown, this evolutive cogitation therefore tries not to be locked in premature schemata and answers. It encourages humility in proposals and intuitive boldness in investigations.\nA subjacent humanist thought encourages human beings to deliberately take his destiny in his hands, in careful correlation with his environment, knowing that human thought is one of the most impacting manifestations of the living being, as an agent of transformation of the environment, and consequently not only of the Earth's environment. In terms of philosophy of evolution, the organized collective thought of humanity, and in general his evolutive culture, appears as the key to development of our species in its universal environment, and the key to a possibly significant interactive modification, in the long term, of this environment. It is principally a prospective step.\n\nThis cultural trend seems to have emerged because the concept of progressive permanent adaptation, which is no longer appreciated only by its scientific aspects, also took a metaphysical dimension (in a sense of research of the essence of the human), which consequently encourages analysis of human evolution, an agent which potentially modifies our environment, in philosophical terms. The possibilities and the risks of this evolution, non-deterministic because of its integration into a dynamic complex living system, gives the human existence and destiny a new sense. And to be involved in such a cogitation opens a philosophical way in which curious minds could not miss advancing sooner or later, a way which rests on a regenerated metaphysics, encouraging us to take back in an evolutive way Aristoteles original concept of physis and its substance, and to search with modern conceptual tools the essence and the sense of our life.\n\nSupported by the philosophy of evolution, this international emerging concept has expressed since the 1970s an evolutive humanism, extending the naturalist tradition of ancient Greek philosophers. Humanist ecology encourages us to better understand and situate the place and the destiny of humanity in its environment in permanent evolution. Human destiny is put in perspective in a universal context where many things remain to be learned. Because it encourages every human in a responsibility in front of his conscience, humanist ecology can be defined too as a will of ethical responsibility of civilized humanity, favouring its permanent improvement and its happiness, in useful interaction with its evolutive environment, in a beneficial way as much for a human being particularly as for mankind in general, and in common symbiosis with their local and global bioscape in evolution. That serves to optimize human society in its own interactions and in its interactions with its bioscape, notably by preserving the planetary equilibrium of the Earth. This solidarity of all of mankind is necessary to preserve its environment and its best development in this environment inspired a particular political expression of humanist ecology, taken up notably by Statesmen such as Jacques Chirac (France) or Mohammed VI (Morocco) in the main meetings of the United Nations.\n\nHumanist ecology naturally favours the permanent adaptation and the best possible development of humanity, and of the human being, in an uncertain universal environment in permanent evolution, with a mind open enough to consider all the possibilities. In humanist ecological comprehension, it is vain to want to freeze an arbitrary cultural schemata and choose the apparent equilibrium and the supposed future of one moment of evolution. A permanent evolutive adaptation is necessary, as much biologically as mentally. That requires relativity and a caution in the analysis. According to this concept, for the human mind, any representation belongs to the domain of belief, considering the uncertainty of the relation of man to the universe, and the natural imperfection of his senses to represent his environment and his interactions with this environment; the reality perceived by Man being only one representation of reality, particular in mankind. Humanist ecology admits this relativized relation with belief but refutes any final and locking form, knowing that no truth can be final for the human mind without upsetting its natural evolutive necessity. This school of thought accepts belief in the present, for want of anything better, but dictates that you must take care to verify and update beliefs.\n\n"}
{"id": "3064243", "url": "https://en.wikipedia.org/wiki?curid=3064243", "title": "Proportional navigation", "text": "Proportional navigation\n\nProportional navigation (also known as PN or Pro-Nav) is a guidance law (analogous to proportional control) used in some form or another by most homing air target missiles. It is based on the fact that two vehicles are on a collision course when their direct Line-of-Sight does not change direction as the range closes. PN dictates that the missile velocity vector should rotate at a rate proportional to the rotation rate of the line of sight (Line-Of-Sight rate or LOS-rate), and in the same direction.\n\nWhere formula_2 is the acceleration perpendicular to instantaneous line of sight, formula_3 is the proportionality constant generally having an integer value 3-5 (dimensionless), formula_4 is the line of sight rate, and V is the closing velocity.\n\nFor example, if the line of sight rotates slowly from north to east, the missile should turn to the right by a certain factor faster than the LOS-rate. This factor is called the Navigation Constant.\nSince the line of sight is not in general co-linear with the missile velocity vector, the applied acceleration does not necessarily preserve the missile kinetic energy. In practice, in the absence of engine throttling capability, this type of control may not be possible.\n\nProportional navigation can also be achieved using an acceleration normal to the instantaneous velocity difference:\n\nwhere formula_6 is the rotation vector of the line of sight:\n\nand formula_8 is the target velocity relative to the missile and formula_9 is the range from missile to target. This acceleration depends explicitly on the velocity difference vector, which may be difficult to obtain in practice. By contrast, in the expressions that follow, dependence is only on the change of the line of sight and the magnitude of the closing velocity. If acceleration normal to the instantaneous line of sight is desired (as in the initial description), then the following expression is valid:\n\nIf energy conserving control is required (as is the case when only using control surfaces), the following acceleration, which is orthogonal to the missile velocity, may be used:\n\nA rather simple hardware implementation of this guidance law can be found in early AIM-9 Sidewinder missiles. These missiles use a rapidly rotating parabolic mirror as a seeker. Simple electronics detect the directional error the seeker has with its target (an IR source), and apply a moment to this gimballed mirror to keep it pointed at the target. Since the mirror is in fact a gyroscope it will keep pointing at the same direction if no external force or moment is applied, regardless of the movements of the missile. The voltage applied to the mirror while keeping it locked on the target is then also used (although amplified) to deflect the control surfaces that steer the missile, thereby making missile velocity vector rotation proportional to line of sight rotation. Although this does not result in a rotation rate that is always exactly proportional to the LOS-rate (which would require a constant airspeed), this implementation is equally effective.\n\nThe basis of proportional navigation was first discovered at sea, and was used by navigators on ships to \"avoid\" collisions. Commonly referred to as Constant Bearing Decreasing Range (CBDR), the concept continues to prove very useful for conning officers (the person in control of navigating the vessel at any point in time) because CBDR will result in a collision or near miss if action is not taken by one of the two vessels involved. Simply altering course until a change in bearing (obtained by compass sighting) occurs, will provide some assurance of avoidance of collision. Obviously not foolproof, the conning officer of the vessel having made the course change, must continually monitor bearing lest the other vessel does the same. Significant course change, rather than a modest alteration, is prudent. International Regulations for Preventing Collisions at Sea dictate which vessel must give way but they, of course, provide no guarantee that action will be taken by that vessel.\n\n\n"}
{"id": "22642052", "url": "https://en.wikipedia.org/wiki?curid=22642052", "title": "Q-guidance", "text": "Q-guidance\n\nQ-guidance is a method of missile guidance used in some U.S. ballistic missiles and some civilian space flights. It was developed in the 1950s by J. Halcombe Laning and Richard Battin at the MIT Instrumentation Lab.\n\nQ-guidance is used for missiles whose trajectory consists of a relatively short boost phase (or powered phase) during which the missile's propulsion system operates, followed by a ballistic phase during which the missile coasts to its target under the influence of gravity. (Cruise missiles use different guidance methods). The objective of Q-guidance is to hit a specified target at a specified time (if there is some flexibility as to the time the target should be hit then other types of guidance can be used).\n\nAt the time Q-guidance was developed the main competitive method was called Delta-guidance. According to Mackenzie, Titan, some versions of Atlas, Minuteman I and II used Delta-guidance, while Q-guidance was used for Thor IRBM and Polaris, and presumably Poseidon. It appears, from monitoring of test launches, that early Soviet ICBMs used a variant of Delta-guidance.\n\nDelta-guidance is based on adherence to a planned reference trajectory, which is developed before the flight using ground-based computers and stored in the missile's guidance system. In flight, the actual trajectory is modeled mathematically as a Taylor series expansion around the reference trajectory. The guidance system attempts to zero the linear terms of this expression, i.e. to bring the missile back to the planned trajectory. For this reason, Delta-guidance is sometimes referred to as \"fly [along] the wire\", where the (imaginary) wire refers to the reference trajectory.\n\nIn contrast, Q-guidance is a dynamic method, reminiscent of the theories dynamic programming or state based feedback. In essence, it says \"never mind where we were supposed to be, given where we are what should we do to make progress towards the goal of reaching the required target at the required time\". To do this it relies on the concept of \"velocity to be gained\".\n\nAt a given time \"t\" and for a given vehicle position \"r\", the correlated velocity vector \"V\" is defined as follows: if the vehicle had the velocity \"V\" and the propulsion system was turned off, then the missile would reach the desired target at the desired time under the influence of gravity. In some sense, \"V\" is the desired velocity.\n\nThe actual velocity of the missile is denoted by \"V\" and the missile is subject to both the acceleration due to gravity \"g\" and that due to the engines \"a\". The velocity to be gained is defined as the difference between \"V\" and \"V\":\n\nA simple guidance strategy is to apply acceleration (i.e. engine thrust) in the direction of \"V\". This will have the effect of making the actual velocity come closer to \"V\". When they become equal (i.e. when \"V\" becomes identically zero) it is time to shut off the engines, since the missile is by definition able to reach the desired target at the desired time on its own.\n\nThe only remaining issue is how to compute \"V\" easily from information available on board the vehicle.\n\nA remarkably simple differential equation can be used to compute the velocity to be gained:\n\nwhere the \"Q\" matrix is defined by\n\nwhere \"Q\" is a symmetric 3 by 3 time-varying matrix. (The vertical bar refers to the fact that the derivative must be evaluated for a given target position \"r\" and time of free flight \"t\".) The calculation of this matrix is non-trivial, but can be performed offline before the flight; experience shows that the matrix is only slowly time varying, so only a few values of Q corresponding to different times during the flight need to be stored on board the vehicle.\n\nIn early applications the integration of the differential equation was performed using analog hardware, rather than a digital computer. Information about vehicle acceleration, velocity and position is supplied by the onboard Inertial measurement unit.\n\nDerivation of the equation\n\nNotation:\n\n\"t\" the current time\n\n\"r\" the current vehicle position vector\n\n\"V\" the current vehicle velocity vector\n\n\"T\" the time the vehicle will reach the target\n\n\"t\" the time of free flight for the correlated vehicle, i.e. t-T\n\nA reasonable strategy to gradually align the thrust vector to the \"V\" vector is to steer at a rate proportional to the cross product between them. A simple control strategy that does this is to steer at the rate\n\nwhere formula_5 is a constant. This implicitly assumes that \"V\" remains roughly constant during the maneuver. A somewhat more clever strategy can be designed that takes into account the rate of time change of \"V\" as well, since this is available from the differential equation above.\n\nThis second control strategy is based on Battin's insight that \"If you want to drive a vector to zero, it is [expedient] to align the time rate of change of the vector with the vector itself\". This suggests setting the auto-pilot steering rate to\n\nEither of these methods are referred to as cross-product steering, and they are easy to implement in analog hardware.\n\nFinally, when all components of \"V\" are small, the order to cut-off engine power can be given.\n\n"}
{"id": "13352174", "url": "https://en.wikipedia.org/wiki?curid=13352174", "title": "Quadrant (instrument)", "text": "Quadrant (instrument)\n\nA quadrant is an instrument that is used to measure angles up to 90°. Different versions of this instrument could be used to calculate various readings, such as longitude, latitude, and time of day. It was originally proposed by Ptolemy as a better kind of astrolabe. Several different variations of the instrument were later produced by medieval Muslim astronomers.\n\nThe term “quadrant”, meaning one fourth, refers to the fact that early versions of the instrument were derived from astrolabes. The quadrant condensed the workings of the astrolabe into an area one fourth the size of the astrolabe face; it was essentially a quarter of an astrolabe.\n\nOne of the earliest accounts of a quadrant comes from Ptolemy's Almagest around 150 CE. He described a “plinth” that could measure the altitude of the noon sun by projecting the shadow of a peg on a graduated arc of 90 degrees. This quadrant was unlike later versions of the instrument; it was larger and consisted of several moving parts. Ptolemy’s version was a derivative of the astrolabe and the purpose of this rudimentary device was to measure the meridian angle of the sun.\n\nIslamic astronomers in the Middle Ages improved upon these ideas and constructed quadrants throughout the Middle East, in observatories such as Marageh, Rey and Samarkand. At first these quadrants were usually very large and stationary, and could be rotated to any bearing to give both the altitude and azimuth for any celestial body. As Islamic astronomers made advancements in astronomical theory and observational accuracy they are credited with developing four different types of quadrants during the Middle Ages and beyond. The first of these, the sine quadrant, was invented by Muhammad ibn Musa al-Khwarizmi in the 9th century at the House of Wisdom in Baghdad. The other types were the universal quadrant, the horary quadrant and the astrolabe quadrant.\n\nDuring the Middle Ages the knowledge of these instruments spread to Europe. In the 13th century Jewish astronomer Jacob ben Machir ibn Tibbon was crucial in further developing the quadrant. He was a skilled astronomer and wrote several volumes on the topic, including an influential book detailing how to build and use an improved version of the quadrant. The quadrant that he invented came to be known as the “novus quadrans”, or new quadrant. This device was revolutionary because it was the first quadrant to be built that did not involve several moving parts and thus could be much smaller and more portable.\n\nTibbon’s Hebrew manuscripts were translated into Latin and improved upon by French scholar Peter Nightingale several years later. Because of the translation, Tibbon, or Prophatius Judaeus as he was known in Latin, became an influential name in astronomy. His new quadrant was based upon the idea that the stereographic projection that defines a planispheric astrolabe can still work if the astrolabe parts are folded into a single quadrant. The result was a device that was far cheaper, easier to use and more portable than a standard astrolabe. Tibbon’s work had a far reach and influenced Copernicus, Christopher Clavius and Erasmus Reinhold; and his manuscript was referenced in Dante’s Divine Comedy.\n\nAs the quadrant became smaller and thus more portable, its value for navigation was soon realized. The first documented use of the quadrant to navigate at sea is in 1461, by Diogo Gomes. Sailors began by measuring the height of Polaris to ascertain their latitude. This application of quadrants is generally attributed to Arab sailors who traded along the east coast of Africa and often travelled out of sight of land. It soon became more common to take the height of the sun at a given time due to the fact that Polaris disappears south of the equator.\n\nIn 1618 English Mathematician Edmund Gunter further adapted the quadrant with an invention that came to be known as the Gunter quadrant. This pocket sized quadrant was revolutionary because it was inscribed with projections of the tropics, the equator, the horizon and the ecliptic. With the correct tables one could use the quadrant to find the time, the date, the length of the day or night, the time of sunrise and sunset and the meridian. The Gunter quadrant was extremely useful but it had its drawbacks; the scales only applied to a certain latitude so the instrument's use was limited at sea.\n\nThere are several types of quadrants:\n\n\nThey can also be classified as:\n\nThe geometric quadrant is a quarter-circle panel usually of wood or brass. Markings on the surface might be printed on paper and pasted to the wood or painted directly on the surface. Brass instruments had their markings scribed directly into the brass.\n\nFor marine navigation, the earliest examples were found around 1460. They were not graduated in degrees but rather had the latitudes of the most common destinations directly scribed on the limb. When in use, the navigator would sail north or south until the quadrant indicated he was at the destination's latitude, turn in the direction of the destination and sail to the destination maintaining a course of constant latitude. After 1480, more of the instruments were made with limbs graduated in degrees.\n\nAlong one edge there were two sights forming an alidade. A plumb bob was suspended by a line from the centre of the arc at the top.\n\nIn order to measure the altitude of a star, the observer would view the star through the sights and hold the quadrant so that the plane of the instrument was vertical. The plumb bob was allowed to hang vertical and the line indicated the reading on the arc's graduations. It was not uncommon for a second person to take the reading while the first concentrated on observing and holding the instrument in proper position.\n\nThe accuracy of the instrument was limited by its size and by the effect the wind or observer's motion would have on the plumb bob. For navigators on the deck of a moving ship, these limitations could be difficult to overcome.\n\nIn order to avoid staring into the sun to measure its altitude, navigators could hold the instrument in front of them with the sun to their side. By having the sunward sighting vane cast its shadow on the lower sighting vane, it was possible to align the instrument to the sun. Care would have to be taken to ensure that the altitude of the centre of the sun was determined. This could be done by averaging the elevations of the upper and lower umbra in the shadow.\n\nIn order to perform measurements of the altitude of the sun, a back observation quadrant was developed.\n\nWith such a quadrant, the observer viewed the horizon from a \"sight vane\" (C in the figure on the right) through a slit in the \"horizon vane\" (B). This ensured the instrument was level. The observer moved the \"shadow vane\" (A) to a position on the graduated scale so as to cause its shadow to appear coincident with the level of the horizon on the horizon vane. This angle was the elevation of the sun.\n\nLarge frame quadrants were used for astronomical measurements, notably determining the altitude of celestial objects. They could be permanent installations, such as mural quadrants. Smaller quadrants could be moved. Like the similar astronomical sextants, they could be used in a vertical plane or made adjustable for any plane.\n\nWhen set on a pedestal or other mount, they could be used to measure the angular distance between any two celestial objects.\n\nThe details on their construction and use are essentially the same as those of the astronomical sextants; refer to that article for details.\n\nNavy: Used to gauge elevation on ships cannon, the quadrant had to be placed on each gun's trunnion in order to judge range, after the loading. The reading was taken at the top of the ship's roll, the gun adjusted,and checked, again at the top of the roll, and he went to the next gun, until all that were going to be fired were ready. The ship's Gunner was informed, who in turn informed the captain...You may fire when ready...at the next high roll, the cannon would be fired.\n\nIn more modern applications, the quadrant is attached to the trunion ring or of a large naval gun to align it to benchmarks welded to the ship's deck. This is done to ensure firing of the gun hasn't \"warped the deck.\" A flat surface on the mount gunhouse or turret is also checked against benchmarks, also, to ensure large bearings and/or bearing races haven't changed... to \"calibrate\" the gun.\n\nDuring the Middle Ages, makers often added customization to impress the person for whom the quadrant was intended. In large, unused spaces on the instrument, a sigil or badge would often be added to denote the ownership by an important person or the allegiance of the owner. \n\n\n\n"}
{"id": "153095", "url": "https://en.wikipedia.org/wiki?curid=153095", "title": "Radio navigation", "text": "Radio navigation\n\nRadio navigation or radionavigation is the application of radio frequencies to determine a position of an object on the Earth. Like radiolocation, it is a type of radiodetermination.\n\nThe basic principles are measurements from/to electric beacons, especially\n\nThese systems used some form of directional radio antenna to determine the location of a broadcast station on the ground. Conventional navigation techniques are then used to take a radio fix. These were introduced prior to World War I, and remain in use today.\n\nThe first system of radio navigation was the \"Radio Direction Finder\", or RDF. By tuning in a radio station and then using a directional antenna, one could determine the direction to the broadcasting antenna. A second measurement using another station was then taken. Using triangulation, the two directions can be plotted on a map where their intersection reveals the location of the navigator. Commercial AM radio stations can be used for this task due to their long range and high power, but strings of low-power radio beacons were also set up specifically for this task, especially near airports and harbours.\n\nEarly RDF systems normally used a loop antenna, a small loop of metal wire that is mounted so it can be rotated around a vertical axis. At most angles the loop has a fairly flat reception pattern, but when it is aligned perpendicular to the station the signal received on one side of the loop cancels the signal in the other, producing a sharp drop in reception known as the \"null\". By rotating the loop and looking for the angle of the null, the relative bearing of the station can be determined. Loop antennas can be seen on most pre-1950s aircraft and ships.\n\nThe main problem with RDF is that it required a special antenna on the vehicle, which may not be easy to mount on smaller vehicles or single-crew aircraft. A smaller problem is that the accuracy of the system is based to a degree on the size of the antenna, but larger antennas would likewise make the installation more difficult.\n\nDuring the era between World War I and World War II, a number of systems were introduced that placed the rotating antenna on the ground. As the antenna rotated through a fixed position, typically due north, the antenna was keyed with the morse code signal of the station's identification letters so the receiver could ensure they were listening to the right station. Then they waited for the signal to either peak or disappear as the antenna briefly pointed in their direction. By timing the delay between the morse signal and the peak/null, then dividing by the known rotational rate of the station, the bearing of the station could be calculated.\n\nThe first such system was the German Telefunken Kompass Sender, which began operations in 1907 and was used operationally by the Zeppelin fleet until 1918. An improved version was introduced by the UK as the Orfordness Beacon in 1929 and used until the mid-1930s. A number of improved versions followed, replacing the mechanical motion of the antennas with phasing techniques that produced the same output pattern with no moving parts. One of the longest lasting examples was Sonne, which went into operation just before World War II and was used operationally under the name Consol until 1991. The modern VOR system is based on the same principles (see below).\n\nA great advance in the RDF technique was introduced in the form of phase comparisons of a signal as measured on two or more small antennas, or a single highly directional solenoid. These receivers were dramatically smaller, more accurate, and simpler to operate. Combined with the introduction of the transistor and integrated circuit, RDF systems were so reduced in size and complexity that they once again became quite common during the 1960s, and were known by the new name, automatic direction finder, or ADF.\n\nThis also led to a revival in the operation of simple radio beacons for use with these RDF systems, now referred to as \"non-directional beacons\" (NDB). As the LF/MF signals used by NDBs can follow the curvature of earth, NDB has a much greater range than VOR which travels only in \"line of sight\". NDB can be categorized as \"long range\" or \"short range\" depending on their power. The frequency band allotted to non-directional beacons is 190–1750 kHz, but the same system can be used with any common AM-band commercial station.\n\nVHF omnidirectional range, or VOR, is an implementation of the reverse-RDF system, but one that is more accurate and able to be completely automated.\n\nInstead of a single signal, the VOR transmitter sends out three signals – one is a simple voice channel that sends morse code to identify the station, another is a continuous signal sent in all directions, and the last is a signal that is rotated at 30 RPM. Like the Orfordness concept, the bearing of the station is measured by finding the rotating signal's peak or null. But instead of timing the signal, the rotating signal is changed in phase in synchronicity with its rotation, such that it is in-phase when pointed north, 90 degrees off when it points east, and so forth. By comparing the phase of the received signal with the one being broadcast omnidirectionally, the angle can be determined using simple electronics. This angle is then displayed in the cockpit of the aircraft, and can be used to take a fix just like the earlier RDF systems, although it is easier to use.\n\nAs VOR required two VHF receivers as well as a conventional radio for station identification, the system did not become popular until the era of miniaturized electronics, first with small tubes in the 1950s, and then transistorized systems in the 1960s. During this period it quickly took over from the older Radio Range system (see below). The signals from the stations could be received anywhere, as opposed to the beams which were only broadcast in certain directions, so in theory the VOR system could be used for free navigation from any to any point. In practice, the older Radio Range procedures were so widely used and standardized that VOR was used to produce a similar set of airways that remain in use today.\n\nThe US military also introduced a VOR-like system known as TACAN. It differed from VOR primarily in its modulation system, adding a Lorentz-like signal to accurately define the center of the rotating beam and thereby improve accuracy. It requires five receiver channels and additional electronics, an expensive requirement when it was introduced.\n\nBeam systems broadcast narrow signals in the sky, and navigation is accomplished by keeping the aircraft centred in the beam. A number of stations are used to create an airway, with the navigator tuning in different stations along the direction of travel. These systems were common in the era when electronics were large and expensive, as they placed minimum requirements on the receivers – they were simply voice radio sets tuned to the selected frequencies. However, they did not provide navigation outside of the beams, and were thus less flexible in use. The rapid miniaturization of electronics during and after World War II made systems like VOR practical, and most beam systems rapidly disappeared.\n\nIn the post-World War I era, the Lorenz company of Germany developed a means of projecting two narrow radio signals with a slight overlap in the center. By broadcasting different audio signals in the two beams, the receiver could position themselves very accurately down the centreline by listening to the signal in their headphones. The system was accurate to less than a degree in some forms.\n\nOriginally known as \"Ultrakurzwellen-Landefunkfeuer\" (LFF), or simply \"Leitstrahl\" (guiding beam), little money was available to develop a network of stations. Deployment was instead led by the US, where it formed the basis of a wide-area navigation system through the 1930s and 40s (see LFF, below). Development was restarted in Germany in the 1930s as a short-range system deployed at airports as a blind landing aid. Although there was some interest in deploying a medium-range system like the US LFF, deployment had not yet started when the beam system was combined with the Orfordness timing concepts to produce the highly accurate Sonne system. In all of these roles, the system was generically known simply as a \"Lorenz beam\".\n\nIn the immediate pre-World War II era the same concept was also developed as a blind-bombing system. This used very large antennas to provide the required accuracy at long distances (over England), and very powerful transmitters. Two such beams were used, crossing over the target to triangulate it. Bombers would enter one of the beams and use it for guidance until they heard the second one in a second radio receiver, using that signal to time the dropping of their bombs. The system was highly accurate, and the 'Battle of the Beams' broke out when United Kingdom intelligence services attempted, and then succeeded, in rendering the system useless through electronic warfare. Sonne, however, proved just as useful to the UK as Germany, and was left to operate unhindered throughout the war.\n\nThe low-frequency radio range (LFR, also other names) was the main navigation system used by aircraft for instrument flying in the 1930s and 1940s in the U.S. and other countries, until the advent of the VOR in the late 1940s. It was used for both en route navigation as well as instrument approaches.\n\nThe ground stations consisted of a set of four antennas that projected Lorenz beams in four cardinal directions. One of the beams was \"keyed\" with the morse code signal \"A\", dit-dah, with the second beam \"N\", dah-dit. Flying down the centreline produced a steady tone. The beams were pointed to the next station to produce a set of airways, allowing an aircraft to travel from airport to airport by following a selected set of stations. Effective course accuracy was about three degrees, which near the station provided sufficient safety margins for instrument approaches down to low minimums. At its peak deployment, there were nearly 400 LFR stations in the US.\n\nThe remaining widely used beam systems are glide path and the localizer of the \"instrument landing system\" (ILS). ILS uses a \"localizer\" to provide horizontal position, distance to the runway, and airport information, and \"glide path\" to provide vertical positioning. ILS can provide enough accuracy and redundancy to allow automated landings.\nFor more information see also: \n\nPositions can be determined with any two measures of angle or distance. The introduction of radar in the 1930s provided a way to directly determine the distance to an object even at long distances. Navigation systems based on these concepts soon appeared, and remained in widespread use until recently. Today they are used primarily for aviation, although GPS has largely supplanted this role.\n\nEarly radar systems, like the UK's Chain Home, consisted of large transmitters and separate receivers. The transmitter periodically sends out a short pulse of a powerful radio signal, which is sent into space through broadcast antennas. When the signal reflects off a target, some of that signal is reflected back in the direction of the station, where it is received. The received signal is a tiny fraction of the broadcast power, and has to be powerfully amplified in order to be used.\n\nThe same signals are also sent over local electrical wiring to the operator's station, which is equipped with an oscilloscope. Electronics attached to the oscilloscope provides a signal that increases in voltage over a short period of time, a few microseconds. When sent to the X input of the oscilloscope, this causes a horizontal line to be displayed on the scope. This \"sweep\" is triggered by a signal tapped off the broadcaster, so the sweep begins when the pulse is sent. Amplified signals from the receiver are then sent to the Y input, where any received reflection causes the beam to move upward on the display. This causes a series of \"blips\" to appear along the horizontal axis, indicating reflected signals. By measuring the distance from the start of the sweep to the blip, which corresponds to the time between broadcast and reception, the distance to the object can be determined.\n\nSoon after the introduction of radar, the radio transponder appeared. Transponders are a combination of receiver and transmitter whose operation is automated – upon reception of a particular signal, normally a pulse on a particular frequency, the transponder sends out a pulse in response, typically delayed by some very short time. Transponders were initially used as the basis for early IFF systems; aircraft with the proper transponder would appear on the display as part of the normal radar operation, but then the signal from the transponder would cause a second blip to appear a short time later. Single blips were enemies, double blips friendly.\n\nTransponder-based distance-distance navigation systems have a significant advantage in terms of positional accuracy. Any radio signal spreads out over distance, forming the fan-like beams of the Lorenz signal, for instance. As the distance between the broadcaster and receiver grows, the area covered by the fan increases, decreasing the accuracy of location within it. In comparison, transponder-based systems measure the timing between two signals, and the accuracy of that measure is largely a function of the equipment and nothing else. This allows these systems to remain accurate over very long range.\n\nThe latest transponder systems (mode S) can also provide position information, possibly derived from GNSS, allowing for even more precise positioning of targets.\n\nThe first distance-based navigation system was the German Y-Gerät blind-bombing system. This used a Lorenz beam for horizontal positioning, and a transponder for ranging. A ground-based system periodically sent out pulses which the airborne transponder returned. By measuring the total round-trip time on a radar's oscilloscope, the aircraft's range could be accurately determined even at very long ranges. An operator then relayed this information to the bomber crew over voice channels, and indicated when to drop the bombs.\n\nThe British introduced similar systems, notably the Oboe system. This used two stations in England that operated on different frequencies and allowed the aircraft to be triangulated in space. To ease pilot workload only one of these was used for navigation – prior to the mission a circle was drawn over the target from one of the stations, and the aircraft was directed to fly along this circle on instructions from the ground operator. The second station was used, as in Y-Gerät, to time the bomb drop. Unlike Y-Gerät, Oboe was deliberately built to offer very high accuracy, as good as 35 m, much better than even the best optical bombsights.\n\nOne problem with Oboe was that it allowed only one aircraft to be guided at a time. This was addressed in the later Gee-H system by placing the transponder on the ground and broadcaster in the aircraft. The signals were then examined on existing Gee display units in the aircraft (see below). Gee-H did not offer the accuracy of Oboe, but could be used by as many as 90 aircraft at once. This basic concept has formed the basis of most distance measuring navigation systems to this day.\n\nThe key to the transponder concept is that it can be used with existing radar systems. The ASV radar introduced by RAF Coastal Command was designed to track down submarines and ships by displaying the signal from two antennas side by side and allowing the operator to compare their relative strength. Adding a ground-based transponder immediately turned the same display into a system able to guide the aircraft towards a transponder, or \"beacon\" in this role, with high accuracy.\n\nThe British put this concept to use in their Rebecca/Eureka system, where battery-powered \"Eureka\" transponders were triggered by airborne \"Rebecca\" radios and then displayed on ASV Mk. II radar sets. Eureka's were provided to French resistance fighters, who used them to call in supply drops with high accuracy. The US quickly adopted the system for paratroop operations, dropping the Eureka with pathfinder forces or partisans, and then homing in on those signals to mark the drop zones.\n\nThe beacon system was widely used in the post-war era for blind bombing systems. Of particular note were systems used by the US Marines that allowed the signal to be delayed in such a way to offset the drop point. These systems allowed the troops at the front line to direct the aircraft to points in front of them, directing fire on the enemy. Beacons were widely used for temporary or mobile navigation as well, as the transponder systems were generally small and low-powered, able to be man portable or mounted on a Jeep.\n\nIn the post-war era, a general navigation system using transponder-based systems was deployed as the distance measuring equipment (DME) system.\n\nDME was identical to Gee-H in concept, but used new electronics to automatically measure the time delay and display it as a number, rather than having the operator time the signals manually on an oscilloscope. This led to the possibility that DME interrogation pulses from different aircraft might be confused, but this was solved by having each aircraft send out a different series of pulses which the ground-based transponder repeated back.\n\nDME is almost always used in conjunction with VOR, and is normally co-located at a VOR station. This combination allows a single VOR/DME station to provide both angle and distance, and thereby provide a single-station fix. DME is also used as the distance-measuring basis for the military TACAN system, and their DME signals can be used by civilian receivers.\n\nHyperbolic navigation systems are a modified form of transponder systems which eliminate the need for an airborne transponder. The name refers to the fact that they do not produce a single distance or angle, but instead indicate a location along any number of hyperbolic lines in space. Two such measurements produces a fix. As these systems are almost always used with a specific navigational chart with the hyperbolic lines plotted on it, they generally reveal the receiver's location directly, eliminating the need for manual triangulation. As these charts were digitized, they became the first true location-indication navigational systems, outputting the location of the receiver as latitude and longitude. Hyperbolic systems were introduced during World War II and remained the main long-range advanced navigation systems until GPS replaced them in the 1990s.\n\nThe first hyperbolic system to be developed was the British Gee system, developed during World War II. Gee used a series of transmitters sending out precisely timed signals, with the signals leaving the stations at fixed delays. An aircraft using Gee, RAF Bomber Command's heavy bombers, examined the time of arrival on an oscilloscope at the navigator's station. If the signal from two stations arrived at the same time, the aircraft must be an equal distance from both transmitters, allowing the navigator to determine a line of position on his chart of all the positions at that distance from both stations. More typically, the signal from one station would be received earlier than the other. The \"difference\" in timing between the two signals would reveal them to be along a curve of possible locations. By making similar measurements with other stations, additional lines of position can be produced, leading to a fix. Gee was accurate to about 165 yards (150 m) at short ranges, and up to a mile (1.6 km) at longer ranges over Germany. Gee remained in use long after World War II, and equipped RAF aircraft as late as the 1960s (approx freq was by then 68 MHz).\n\nWith Gee entering operation in 1942, similar US efforts were seen to be superfluous. They turned their development efforts towards a much longer-ranged system based on the same principles, using much lower frequencies that allowed coverage across the Atlantic Ocean. The result was LORAN, for \"LOng-range Aid to Navigation\". The downside to the long-wavelength approach was that accuracy was greatly reduced compared to the high-frequency Gee. LORAN was widely used during convoy operations in the late war period.\n\nAnother British system from the same era was Decca Navigator. This differed from Gee primarily in that the signals were not pulses delayed in time, but continuous signals delayed in phase. By comparing the phase of the two signals, the time difference information as Gee was returned. However, this was far easier to display; the system could output the phase angle to a pointer on a dial removing any need for visual interpretation. As the circuitry for driving this display was quite small, Decca systems normally used three such displays, allowing quick and accurate reading of multiple fixes. Decca found its greatest use post-war on ships, and remained in use into the 1990s.\n\nAlmost immediately after the introduction of LORAN, in 1952 work started on a greatly improved version. LORAN-C (the original retroactively became LORAN-A) combined the techniques of pulse timing in Gee with the phase comparison of Decca.\n\nThe resulting system (operating in the low frequency (LF) radio spectrum from 90 to 110 kHz) that was both long-ranged (for 60 kW stations, up to 3400 miles) and accurate. To do this, LORAN-C sent a pulsed signal, but modulated the pulses with an AM signal within it. Gross positioning was determined using the same methods as Gee, locating the receiver within a wide area. Finer accuracy was then provided by measuring the phase difference of the signals, overlaying that second measure on the first. By 1962, high-power LORAN-C was in place in at least 15 countries.\n\nLORAN-C was fairly complex to use, requiring a room of equipment to pull out the different signals. However, with the introduction of integrated circuits, this was quickly reduced further and further. By the late 1970s, LORAN-C units were the size of a stereo amplifier and were commonly found on almost all commercial ships as well as some larger aircraft. By the 1980s, this had been further reduced to the size of a conventional radio, and it became common even on pleasure boats and personal aircraft. It was the most popular navigation system in use through the 1980s and 90s, and its popularity led to many older systems being shut down, like Gee and Decca. However, like the beam systems before it, civilian use of LORAN-C was short-lived when GPS technology drove it from the market.\n\nSimilar hyperbolic systems included the US global-wide VLF/Omega Navigation System, and the similar Alpha deployed by the USSR. These systems determined pulse timing not by comparison of two signals, but by comparison of a single signal with a local atomic clock. The expensive-to-maintain Omega system was shut down in 1997 as the US military migrated to using GPS. Alpha is still in use.\n\nSince the 1960s, navigation has increasingly moved to satellite navigation systems. These are essentially DME systems located in space. The fact that the satellites are in orbit and normally move with respect to the receiver means that the calculation of the position of the satellite needs to be taken into account as well, which can only be handled effectively with a computer.\n\nThe Global Positioning System, better known simply as GPS, sends several signals that are used to decode the position and distance of the satellite. One signal encodes the satellite's \"ephemeris\" data, which is used to accurately calculate the satellite's location at any time. Space weather and other effects causes the orbit to change over time so the ephemeris has to be updated periodically. Other signals send out the time as measured by the satellite's onboard atomic clock. By measuring this signal from several satellites, the receiver can re-build an accurate clock signal of its own. Comparing the two produces the distance to the satellite, and several such measurements allows a form of triangulation to be carried out.\n\nGPS has better accuracy that any previous land-based system, is available at almost all locations on the Earth, can be implemented in a few cents of modern electronics, and requires only a few dozen satellites to provide worldwide coverage. As a result of these advantages, GPS has led to almost all previous systems falling from use. LORAN, Omega, Decca, Consol and many other systems disappeared during the 1990s and 2000s. The only other systems still in use are aviation aids, which are also being turned off for long-range navigation while new differential GPS systems are being deployed to provide the local accuracy needed for blind landings.\n\n\n\n"}
{"id": "44115663", "url": "https://en.wikipedia.org/wiki?curid=44115663", "title": "Reduced level", "text": "Reduced level\n\nReduced Level in surveying refers to equating elevations of survey points with reference to a common assumed datum. It is a vertical distance between survey point and adopted datum plane. Thus it is considered as the base elevation which is used as reference to reckon heights or depths of other important places. Reduced here means equating and Level means Elevations. Datum may be real or imaginary location with a nominated elevation of straight zero.\n\nThe most common and convenient datum which is internationally accepted is mean sea level. Countries take their nearby sea levels as datum planes for calculations of Reduced levels . For example, Pakistan takes sea near Karachi as its datum while India takes sea near Mumbai as its datum for calculation of Reduced levels of different places in their respective countries. The term Reduced Level is denoted shortly by ‘RL’. National survey departments of each country determines RL’s of significantly important locations or points. These points are called as \"permanent benchmarks\" and this survey process is known as Great Trigonometrical Surveying (GTS). The permanent bench marks act as reference points for determining RL’s of other locations in a particular country.\n\nThe instruments used to determine reduced level include:-\nand Coke’s reversible level. Anyone of these can be used at a time for the purpose.\n\nRL of a survey point can be determined by two methods:-\n\n\n"}
{"id": "18779063", "url": "https://en.wikipedia.org/wiki?curid=18779063", "title": "SK-42 reference system", "text": "SK-42 reference system\n\nThe SK-42 reference system also known as the Krasovsky 1940 ellipsoid, is a coordinate system established in the Soviet Union in 1942 as \"Systema koordinat\" (), and provides parameters which are linked to the geocentric Cartesian coordinate system PZ-90. It was used in geodetic calculations, notably in military mapping and determining state borders.\n\nThe Krasovsky 1940 ellipsoid uses a semi-major axis (equatorial radius) of 6,378,245 m, and an inverse flattening of 298.3.\n\n"}
{"id": "491876", "url": "https://en.wikipedia.org/wiki?curid=491876", "title": "Sector light", "text": "Sector light\n\nA sector light is a man-made pilotage and position fixing aid that consists of strictly de-limited horizontal angle light beams to guide water-borne traffic through a safe channel at night in reasonable visibility. Sector lights are most often used for safe passage through shallow or dangerous waters. This may be when leaving or entering harbour. Nautical charts (paper and electronic) give all the required information.\n\nSectors of colored glass (or plastic) are placed in the lanterns of these lights. The light will then show these colors when observed certain bearings. Bearings referring to a sector are given in degrees true as observed from sea. Though the colors of the light will change, the characteristics will not. The change of color is not abrupt. The transition is made through an arc of uncertainty of 2° or greater.\n\nThe colors that are used, are conform to the IALA Maritime Bouyage system that is designed by the International Association of Lighthouse Authorities:\nA ship that is sailing in safe water and then sees the red (or green) color of the light has to make an alteration in course.\n\nThe world has different navigation stereotypes managed by IALA (International Association of Lighthouse Authorities). For example, the United States uses a signalling stereotype which is the opposite of Europe. In USA, the red light indicates the starboard side of the channel for harbour bound vessels, while the green light indicates the port side of the same channel. An expression to remind of this is \"red right returning\".\nAn example of a sector light is the light of the Fisgard Lighthouse in British Columbia. The lighthouse as built to guide ships through the entrance of Esquimalt harbour. The white sector is an isophase light of 2s from 322° to 195°. If the ship sees this white light, it can pass safely. The rest shows a red light from 195 to 322°. If a vessel sees this light, it should alter its course.\n\nAnother example is the PEL sector light at Diego Garcia. The PEL sector light was constructed to guide US Navy vessels into the Diego Garcia port through a narrow entrance in the lagoon. The PEL sector light produces 3.5 million candela and is visible for 10 nautical miles (18.5 km) by day. The beam with 3 colors is narrow at 1.6°. This was required because the PEL sector light was built over 7NM (13 km) from the atoll entrance and it had to illuminate a safe entrance which is only 228 meters wide.\n"}
{"id": "47069448", "url": "https://en.wikipedia.org/wiki?curid=47069448", "title": "Sight reduction", "text": "Sight reduction\n\nSight reduction is the process of deriving from a sight the information needed for establishing a line of position.\n\nSight is defined as the observation of the altitude, and sometimes also the azimuth, of a celestial body for a line of position; or the data obtained by such observation.\n\nNowadays sight reduction uses the equation of the circle of equal altitude to calculate the altitude of the celestial body,\n\nformula_1\n\nand the azimuth \"Zn\" is obtained from \"Z\" by:\n\nformula_2\n\nWith the observed altitude \"Ho\", \"Hc\" and \"Zn\" are the parameters of the Marcq St Hilaire intercept for the line of position:\n\nWith \"B\" the latitude (+N/S), \"L\" the longitude (+E/−W), \"LHA\" = \"GHA\" + \"L\" is the local hour angle, \"Dec\" and \"GHA\" are the declination and Greenwich hour angle of the star observed, and \"Hc\" the calculated altitude. \"Z\" is the calculated azimuth of the body.\n\nBasic procedures involved computer sight reduction or longhand tabular methods.\n\nThe methods included are:\n\nThis method is a practical procedure to reduce celestial sights with the needed accuracy, without using electronic tools such as calculator or a computer. And it could serve as a backup in case of malfunction of the positioning system aboard.\n\nThe first approach of a compact and concise method was published by R. Doniol in 1955 The altitude is derived from sin(\"Hc\") = \"n\" − \"a\" (\"m\" + \"n\"), in which \"n\" = cos(\"B\" − \"Dec\"), \"m\" = cos(\"B\" + \"Dec\"), \"a\" = hav(\"LHA\").\n\nThe calculation is:\n\nA practical and friendly method using only haversines was developed between 2014 and 2015, and published in NavList.\n\nA compact expression for the altitude was derived using haversines, hav, for all the terms of the equation:\n\nhav(\"ZD\") = hav(\"B\" − \"Dec\") + (1 − hav(\"B\" − \"Dec\") − hav(\"B\" + \"Dec\")) hav(\"LHA\")\n\nwhere \"ZD\" is the zenith distance\n\n\"Hc\" = (90 − \"ZD\") the calculated altitude\n\nThe algorithm if absolute values are used is:\n\nFor the azimuth a diagram was developed for a faster solution without calculation, and with an accuracy of 1°.\n\nThis diagram could be used also for star identification.\n\nAn ambiguity in the value of azimuth may arise since in the diagram 0 ≤ \"Z\" ≤ 90°. \"Z\" is E/W as the name of the meridian angle, but the N/S name is not determined. In most situations azimuth ambiguities are resolved simply by observation.\n\nWhen there are reasons for doubt or for the purpose of checking the following formula should be used.\nhav(\"Z\") = [hav(90° − \"Dec\") − hav(\"B\" − \"Hc\")] / (1 − hav(\"B\" − \"Hc\") − hav(\"B\" + \"Hc\"))\n\nThe algorithm if absolute values are used is:\n\nThis computation of the altitude and the azimuth needs a haversine table. For a precision of 1 minute of arc, a four figure table is enough.\n\n Data:\n\n\n"}
{"id": "22603476", "url": "https://en.wikipedia.org/wiki?curid=22603476", "title": "Sink (geography)", "text": "Sink (geography)\n\nA geographic sink is a depression within an endorheic basin where water collects with no visible outlet. Instead of discharging, the collected water is lost due to evaporation and/or penetration (water sinking underground, e.g., to become groundwater in an aquifer). If the sink has karstic terrain, water will sink at a higher rate than the surface evaporation, and conversely if the lakebed or sink bed has a layer of soil that is largely impervious to water (hardpan), evaporation will predominate. Since dry lakes in sinks with hardpan have little penetration, they require more severe aridity/heat to eliminate collected water at a comparable rate as for a similar sink with appreciable penetration.\n\nDepending on losses, precipitation, and inflow (e.g., a spring, a tributary, or flooding); the temporal result of a lake in a sink may be a persistent lake, an intermittent lake, a playa lake (temporarily covered with water), or an ephemeral lake.\n\n\n"}
{"id": "17373153", "url": "https://en.wikipedia.org/wiki?curid=17373153", "title": "Standard Geographical Classification code (Canada)", "text": "Standard Geographical Classification code (Canada)\n\nThe Standard Geographical Classification (SGC) is a system maintained by Statistics Canada for categorizing and enumerating the census geographic units of Canada. Each geographic area receives a unique numeric code ranging from 1 to 10 digits, which extend telescopically to refer to increasingly small areas. This geocode is roughly analogous to the ONS coding system in use in the United Kingdom.\n\nThe SGC code format for regions is X, where \"X\" is a unique identifier incrementing from east to west, then north.\n\n1: Atlantic Canada\n2: Quebec\n3: Ontario\n4: Prairies\n5: British Columbia\n6: Northern Canada\n\nThe SGC code format for provinces and territories is XY, where\n\"X\" is the above regional prefix, and \"Y\" is a further identifier incrementing from east to west. Taken as a single digit, each value of \"Y\" is unique within the province group, or unique within the territory group.\n\n10: Newfoundland and Labrador\n11: Prince Edward Island\n12: Nova Scotia\n13: New Brunswick\n24: Quebec\n35: Ontario\n46: Manitoba\n47: Saskatchewan\n48: Alberta\n59: British Columbia\n60: Yukon\n61: Northwest Territories\n62: Nunavut\n\nThe SGC code format for census divisions is XX YY, where \"XX\" is the above province/territory code, and \"YY\" is the census division's code, unique within its own province. Census divisions are generally numbered from east to west. In some locations, a similar policy to American FIPS county codes has been adopted, with even-numbered slots being left vacant for future expansion.\n\nExamples:\n10 04: Division No. 4, Newfoundland and Labrador\n10 05: Division No. 5, Newfoundland and Labrador\n13 08: Kent County, New Brunswick\n13 09: Northumberland County, New Brunswick\n13 10: York County, New Brunswick\n24 64: Les Moulins Regional County Municipality, Quebec\n24 65: \"Territoire équivalent\" of Laval, Quebec\n24 66: \"Territoire équivalent\" of Montreal, Quebec\n24 67: Roussillon Regional County Municipality, Quebec\n24 68: Les Jardins-de-Napierville Regional County Municipality, Quebec\n35 07: Leeds and Grenville United Counties, Ontario\n35 08: \"[vacant slot]\"\n35 09: Lanark County, Ontario\n35 10: Frontenac Census Division, Ontario\n47 04: Division No. 4, Saskatchewan\n48 05: Division No. 5, Alberta\n59 01: Regional District of East Kootenay, British Columbia\n59 02: \"[vacant slot]\"\n59 03: Regional District of Central Kootenay, British Columbia\n59 04: \"[vacant slot]\"\n59 05: Regional District of Kootenay Boundary, British Columbia\n\nThe SGC code format for census subdivisions is XX YY ZZZ, where \"XX\" is the province/territory code, \"YY\" is the census division code, and \"ZZZ\" is the census subdivision's code, unique within its own census division. Census subdivisions are again generally numbered from east to west, and the practice has been to leave even-numbered slots vacant for future expansion.\n\nExamples:\n35 12 001: Tyendinaga, Ontario\n35 12 002: Deseronto, Ontario\n35 12 003: \"[vacant slot]\"\n35 12 004: Tyendinaga Mohawk Territory, Ontario\n35 12 005: Belleville, Ontario\n35 12 006: \"[vacant slot]\"\n62 04 001: Sanikiluaq, Nunavut\n62 04 002: \"[vacant slot]\"\n62 04 003: Iqaluit, Nunavut\n62 04 004: \"[vacant slot]\"\n62 04 005: Kimmirut, Nunavut\n62 04 006: \"[vacant slot]\"\n\n"}
{"id": "6985614", "url": "https://en.wikipedia.org/wiki?curid=6985614", "title": "Synekism", "text": "Synekism\n\nSynekism is a concept in urban studies coined by Edward Soja. It refers to the dynamic formation of the polis state — the union of several small urban settlements under the rule of a \"capital\" city (or so-called city-state or urban system). Soja's definition of synekism, mentioned in \"Writing the city spatially\", is \"the stimulus of urban agglomeration.\"\n\nFrom the social sciences' view, it is also a \"nucleated and hierarchically nested process of political governance, economic development, social order, and cultural identity\" Soja.\nIn densely settled urban places, a critical-mass provides potential for innovation that is not typically available in rural environments, therefore synekism can be thought of as the geographical relationships that create and give importance to cities.\n\n"}
{"id": "20855147", "url": "https://en.wikipedia.org/wiki?curid=20855147", "title": "Tellurometer", "text": "Tellurometer\n\nThe Tellurometer was the first successful microwave electronic distance measurement equipment. The name derives from the Latin \"tellus\", meaning Earth.\n\nThe original Tellurometer, known as the Micro-Distancer MRA 1, was introduced in 1957. It was invented by Dr. Trevor Lloyd Wadley of the Telecommunications Research Laboratory of the South African Council for Scientific and Industrial Research (CSIR), also responsible for the Wadley Loop receiver, which allowed precision tuning over wide bands, a task that had previously required switching out multiple crystals.\n\nThe Tellurometer emits an electronic wave: the remote station reradiates the incoming wave in a similar wave of more complex modulation, and the resulting phase shift was a measure of the distance travelled. The results appear on a cathode ray tube with circular sweep. This instrument penetrates haze and mist in daylight or darkness and has a normal range of 30–50 km but can extend up to 70 km.\n\nThe Tellurometer design yields high accuracy distance measurements over geodetic distances, but it is also useful for second order survey work, especially in areas where the terrain was rough and/or the temperatures extreme.\n\nExamples of remote locations mapped using Tellurometer surveys are Adams Bluff, Churchill Mountains, Cook Mountains, Jacobsen Glacier, Mount Albright, Mount Predoehl, Mount Summerson, Sherwin Peak and Vogt Peak.\n\nThe MRB2 or Hydrodist was a marine version that was used in coastal surveys and calibrating ships using other survey navigation systems.\n\nThey were used by the Army of the Republic of Vietnam in the late 1960s.\n\nPlessey, the British electronics company, formed a new subsidiary known as \"Tellurometer (Pty) Limited\" in the 1960s to manufacture the product and to develop and sell derivatives. The Company subsequently introduced numerical displays, solid state transmitters, integrated circuits and eventually microprocessors for the product.\n\n"}
{"id": "22694540", "url": "https://en.wikipedia.org/wiki?curid=22694540", "title": "The Exploration of the Colorado River and Its Canyons", "text": "The Exploration of the Colorado River and Its Canyons\n\nThe Exploration of the Colorado River and Its Canyons by John Wesley Powell is a classic of American exploration literature. It is about the Powell Geographic Expedition of 1869 which was the first trip down the Colorado River by boat, including the first trip through the Grand Canyon. \n\nPowell's first written accounts of his exploration appeared in the January, February and March 1875 editions of \"Scribner’s Monthly\" as \"The Canons of the Colorado\". The Smithsonian published it in book form in 1875 under title \"Report of the Exploration of the Colorado River of the West and Its Tributaries. Explored in 1869, 1870, 1871, and 1872, under the direction of the Secretary of the Smithsonian Institution\". It was revised and published in 1895 as \"The Exploration of the Colorado River and Its Canyons\".\n\nIt includes hundreds of wood engravings based on photographs by E.O. Beaman, James Fennemore and John Karl Hillers, and drawings by Thomas Moran.\n\n"}
{"id": "4878962", "url": "https://en.wikipedia.org/wiki?curid=4878962", "title": "Thomas Larcom", "text": "Thomas Larcom\n\nMajor-General Sir Thomas Aiskew Larcom, 1st Baronet PC FRS (24 December 1801 – 15 June 1879) was a leading official in the early Irish Ordnance Survey that started in 1824. He later became a poor law commissioner, census commissioner and finally executive head of the British administration in Ireland as under-secretary to the Lord-Lieutenant of Ireland, a position the government of the day was eager for him to take.\n\nThe longest-serving under-secretary (1853–1868), and a man of unusual abilities, Larcom had a distinguished career in his adopted country and acted with an impartiality that won him respect from all parties. In 1868 he was admitted to the Irish Privy Council and created a Baronet.\n\n\n"}
{"id": "8339163", "url": "https://en.wikipedia.org/wiki?curid=8339163", "title": "Townsite", "text": "Townsite\n\nA townsite is a legal subdivision of land for the development of a town or community. In the historical development of the United States, Canada, and other former British colonial nations, the filing of a townsite plat (United States) or plan (Canada) was often the first legal act in the establishment of a new town or community.\n\nNumerous townsites were filed in British Columbia, Canada, in the early 19th century. Some of those filed in what is now Metro Vancouver included:\n\nAlthough most of these townsites were incorporated into newly created cities and municipalities of Metro Vancouver, official survey plans still continue to designate lots as being part of these townsites. For example, a 2008 strata plan in Vancouver's Gastown is indicated as being in the Granville Townsite.\n\nTownsite planning was either done by government authorities or by private developers. In the case of North Vancouver townsite in 1907 most of the land was owned by the North Vancouver Land and Improvement Company.\nBy owning large amount of land they were able to plan on a grand scale. New Westminster was surveyed by the Royal Engineers under government direction.\n\nAs existing towns grow, they develop Official Community Plans (OCPs) that expand upon the already existing townsites. However, periodically new communities are still created. In 1975, the Whistler townsite was created and eventually incorporated as a municipality. The new municipality was given of Crown land to develop the townsite.\n"}
{"id": "52760151", "url": "https://en.wikipedia.org/wiki?curid=52760151", "title": "Tunnel network", "text": "Tunnel network\n\nIn transport, tunnels can be connected together to form a tunnel network. These can be used in mining to reach ore below ground, in cities for underground rapid transit systems, in sewer systems, in warfare to avoid enemy detection or attacks, as maintenance access routes beneath sites with high ground-traffic such as airports and amusement parks, or to extend public living areas or commercial access while avoiding outdoor weather.\n\nTunnel networks were sometimes developed during siege warfare, even dating back to classical antiquity. Starting with a single tunnels being dug to undermine a wall that might be detected by the defenders and met with counter-tunnels, leading to tunnel warfare. Defenders might first create a series of underground listing posts to preempt such mining attacks.\n\nAny time the use of trenches becomes extensive, this naturally leads to connecting them with tunnel networks for safe passage both along the trench lines and with rear areas. In World War I, when given enough time and resources, the underground components of the defenses could become more extensive than those above ground.\n\nThe French Maginot Line, constructed from 1929 to 1939, was a chain of fortresses, bunkers, retractable turrets, outposts, obstacles, and sunken artillery emplacements, all linked by an extensive shell-proof underground tunnel network. It included underground barracks, shelters, ammo dumps and depots, and even had its own underground narrow gauge railways.\n\nThe tunnels of Củ Chi are an immense network of connecting underground tunnels located in the Củ Chi District of Ho Chi Minh City (Saigon), Vietnam, and are part of a much larger network of tunnels that underlie much of the country. The Củ Chi tunnels were the location of several military campaigns during the Vietnam War, and were the Viet Cong's base of operations for the Tết Offensive in 1968.\nThe tunnels were used by Viet Cong soldiers as hiding spots during combat, as well as serving as communication and supply routes, hospitals, food and weapon caches and living quarters for numerous North Vietnamese fighters. The tunnel systems were of great importance to the Viet Cong in their resistance to American forces, and helped to counter the growing American military effort.\nThe Vịnh Mốc tunnels are a tunnel complex in Quảng Trị, Vietnam. During the Vietnam War it was strategically located on the border of North Vietnam and South Vietnam. The tunnels were built to shelter people from the intense bombing of Son Trung and Son Ha communes in Vinh Linh county of Quảng Trị Province in the Vietnamese Demilitarized Zone. The American forces believed the villagers of Vinh Moc were supplying food and armaments to the North Vietnamese garrison on the island of Con Co which was in turn hindering the American bombers on their way to bomb Hanoi. The idea was to force the villagers of Vinh Moc to leave the area but as is typical in Vietnam there was nowhere else to go. The villagers initially dug the tunnels to move their village 10 metres underground but the American forces designed bombs that burrowed down 10 metres. Eventually against these odds, the villagers moved the village to a depth of 30 metres. It was constructed in several stages beginning in 1966 and used until early 1972. The complex grew to include wells, kitchens, rooms for each family and spaces for healthcare. Around 60 families lived in the tunnels; as many as 17 children were born inside the tunnels.\nThe tunnels were a success and no villagers lost their lives. The only direct hit was from a bomb that failed to explode; the resulting hole was utilized as a ventilation shaft.\nThree levels of tunnels were eventually built.\n\nDuring the 2014 Israel–Gaza conflict, the Israeli military uncovered and destroyed 32 cross-border tunnels that went on for miles beneath Gaza and reached into Israeli territory. According to intelligence officials, Israeli engineers are developing a system that could detect and destroy cross-border tunnels for which the Israeli government has reportedly spent more than $250 million since 2004.\n\nA network of caves beneath the cities of Mosul and Badana were built by ISIS. The terrorist group avoids battlefield engagements, preferring to hide in such tunnels safe from satellite detection, drone strikes and artillery, managed to maintain supply lines and communication with other areas under their control.\n\nA Texas investor group is building a $300 million luxury community replete that includes a navigable tunnel network with underground homes and air-lock blast doors designed for people worried about a dirty bomb or other disasters.\n\n"}
{"id": "44041754", "url": "https://en.wikipedia.org/wiki?curid=44041754", "title": "United States Geography Olympiad", "text": "United States Geography Olympiad\n\nThe United States Geography Olympiad, often abbreviated as USGO, is a nationwide academic geography competition for primary and secondary school students in the United States. It was introduced during the 2012-2013 school year. It currently consist of approximately 105 National Qualifying Exam sites held around the United States and its territories, along with a certain number of international schools, and a National Championships held in Arlington, VA at the end of April. The National Qualifying Exam sites are held in conjunction with the high school-level regional and state tournaments of The National History Bee and The National History Bowl.\n\nThe United States Geography Olympiad is also the official qualifying competition for American students looking to qualify for the International Geography Olympiad, an international geography competition, which is held every summer in a different country. The international competition consists of three parts: a written test, a multimedia test and some fieldwork-related activity. James Mullen of Cupertino, CA, won the 2014 International Geography Olympiad while the USA finished 10th out of 36 countries overall.\n\nThe United States Geography Olympiad currently includes two categories: a Junior Varsity category and a Varsity category. For each academic school year, students who are younger than 16 years of age, as of July 1 of the summer following the school year, can participate in the Junior Varsity category - there is no younger age limit. Students who are 16 or older (up to 19), as July 1 of the summer following the school year, must participate in the Varsity category. Only students in the Varsity category are eligible to qualify for the US National Team which will participate during that summer in the International Geography Olympiad.\n\nThe US Geography Olympiad is not associated with National Geographic or the National Geographic Bee.\n\nThe US Geography Olympiad was founded in the summer of 2012 under the name \"United States Geography Challenge\" by David Madden and Enrico Contolini. David Madden is a former 19-day champion on Jeopardy!, who also serves as Executive Director for both The National History Bee and The National History Bowl. Enrico Contolini is an engineer and self-taught geography enthusiast and researcher who coached his daughter Isabella to be the top female student nationally in the National Geographic Bee in both 2010 and 2011. The name of the competition was changed to the United States Geography Olympiad in October 2013.\n\nAlong with acting as the qualifying competition for American students looking to attend the International Geography Olympiad, the US Geography Olympiad was founded to test the geography skills of American students and to help promote geography education. The competition aims to reward students who have devoted themselves to studying the world, and its countries and peoples, as a thorough knowledge of geography is essential to being an educated citizen today. The US Geography Olympiad aims at being more than a competition, and for this reason it also runs a USGeoChallenge Facebook page where geography-related articles and news, quizzes, links to relevant websites and blogs are posted on a regular bases to foster educational discussions and opportunities for those interested in geography as a subject.\n\nThe Elementary and Middle School divisions were introduced in 2015. The Middle School division was divided into 7th and 8th grade divisions in 2016.\n\nThe Regional and/or State Qualifying event is currently organized as a 50 multiple-choice question written test to be completed in 20 minutes. Students may take the Qualifying Exam up to three times, as three different versions of the Exam are offered at different regional tournaments. To qualify for Nationals, students must finish in either the top half of the students in their division (Varsity or JV) at their site, or in the top 50% of students taking that particular version of the Qualifying Exam nationally. \n\nThe National competition currently consists of three different events: A written exam, a map skills exam, and an 80 question multiple choice exam. Participants are given one hour to finish the first set of 40 multiple choice questions and the map skills exam. After a break, participants are given another hour to finish the second set of 40 multiple choice questions and the written exam. The written exam is based on the written exam at the International Geography Olympiad (iGeo), and the multiple choice exam is based on the multimedia exam at iGeo. \n\nAfter the competition, participants are given a rank score in each event, based on how their scores placed within that event. Any ties are shown by a .5 ranking; thus, a student that is tied for 1st place would have a rank score of 1.5. To determine participants' final rank, rank values in the three events are added together and ranked from greatest to least. The most favorable rank score would be 3.0, signifying that the participant ranked 1st in all three events. Rankings for Junior Varsity and Varsity divisions are separate, because the top 4 students in Varsity earn a seat in the US Team going to iGeo.\n\nThe JV and Varsity Championships have always been held at the Crystal Gateway Marriott in the Crystal City neighborhood of Arlington, Virginia.\n\nIn 2015, the Elementary and Middle School Championships were held at the Marriott Downtown Louisville in Louisville, Kentucky. In 2016, the Elementary, 7th Grade, and 8th Grade Championships were held at the Hyatt Regency O'Hare in Rosemont, Illinois. The Hyatt Regency O'Hare also hosted the Elementary and Middle School Championships for 2017 on Memorial Day weekend. In 2018, the Middle School and Elementary National Championships were held at the Atlanta Marriott Marquis.\n\n<nowiki>*</nowiki>Do note that in 2018, 7th and 8th grades were split again\n\n\n"}
{"id": "5329197", "url": "https://en.wikipedia.org/wiki?curid=5329197", "title": "Visual geography", "text": "Visual geography\n\nVisual geography is the study of geography as represented in paintings, photos and other forms of the visual arts. Both human geography and physical geography in the visual arts are studied. Typical studies in Visual Geography include the geographical symbols within visual arts. Another is cross referencing the date a piece of visual art was produced with known weather records to see if the art was drawn accurately in the field or from memory in a studio.\n\n\n"}
{"id": "5860082", "url": "https://en.wikipedia.org/wiki?curid=5860082", "title": "Wild Heerbrugg", "text": "Wild Heerbrugg\n\nThe Wild (Heerbrugg) company was founded in 1921 in Switzerland. The company manufactured optical instruments, such as surveying instruments, microscopes and instruments for photogrammetry among others. The company changed its name several times, first being known as \"Heinrich Wild, Werkstätte für Feinmechanik und Optik\", then \"Verkaufs-Aktiengesellschaft Heinrich Wild's Geodätische Instrumente\", later \"Wild Heerbrugg AG\", later \"Wild-Leitz\". The company was linked with Leica in 1989, then it became part of Leica Holding B.V. Its subsidiary Leica Geosystems AG became part of the Swedish Hexagon AB Group of companies in 2005.\n\nOn 26 April 1921 the company Heinrich Wild, Werkstätte für Feinmechanik und Optik was founded in Heerbrugg by three Swiss personalities: \n\nHeinrich Wild (1877–1951), a leading designer of geodetic and astronomical instruments, was born in Switzerland and began his career as an apprentice surveyor. In 1908, having invented a military rangefinder and convinced Zeiss to manufacture it, Wild moved to Jena and became head of GEO, the new Zeiss branch responsible for surveying instruments. Wild returned to Switzerland after the First World War. In 1921, with the help of Swiss financiers, he established a Werkstätte für Feinmechanik und Optik in Heerbrugg, in the Rhine Valley. As the first major product, Wild developed the Theodolite Wild T2. Later models were launched when Wild already had left the company, still bearing his name, such as the theodolite WILD Heerbrugg T4. Another important new product engineered by Wild was an aerial camera, later called \"Aviophot\" for cartography.\n\nIn the early 1930s, having recognized that he was not cut out to be a factory manager, Wild moved to Zurich, severed his connections with the firm in Heerbrugg, and designed instruments for Kern & Co in Aarau. His old firm became Wild Heerbrugg in 1937. It merged with the optical firm of Ernst Leitz GmbH of Wetzlar in 1987. It also acquired a majority interest in Kern in 1988, was renamed Wild-Leitz AG in 1989. The combined company had about 8000 employees and revenues of approx. one billion Swiss Francs (CHF). Then, it became part of the Leica holding company in 1990.\n\nStarting from 1996 the company was divided gradually again into smaller units. \nThus 1996 the Leica Camera AG developed, in October 1997 the Leica Geosystems AG and on 1 April 1998 the Leica Microsystems AG. Leica Geosystems produced—in continuation of Wild Heerbrugg—the geodetic instruments and is the global market leader in this sector as part of the new parent company Hexagon AB.\n\nThe brand name \"Wild Heerbrugg\" is still in use for stereo microscopes and precision level measuring such as the WILD Heerbrugg N3 Level.\n\n"}
