{"id": "20871763", "url": "https://en.wikipedia.org/wiki?curid=20871763", "title": "Anthropization", "text": "Anthropization\n\nIn geography and ecology, anthropization is the conversion of open spaces, landscapes, and natural environments by human action.\n\nAnthropic erosion is the process of human action degrading terrain and soil.\n\nAn area may be classified as anthropized even though it looks natural, such as grasslands that have been deforested by humans. It can be difficult to determine how much a site has been anthropized in the case of urbanization because one must be able to estimate the state of the landscape before significant human action.\n\nThe earliest known stages of anthropization can be found as early as the Neolithic era and the basic farmland created in that time..\nWith the continually-growing population of humans, the land that the Earth provides has been appropriated over the years. The ecological footprint created by anthropization is continually growing despite efficiency and technique improvements made in anthropization..\n\nWhether anthropized or not, all land seldom a few locations has been claimed. Outside of the largely inhospitable Arctic and Antarctic circles and large portions of other uninhabitable landscapes, much of the globe has been used or altered in some direct way by humans. Land has been appropriated for many different reasons, but ultimately the outcome is typically a short-term benefit for humans. An area is anthropized is some way to make land available for housing, to harvest the resources, to create space for some anthropological reason, or many other possibilities.\n\nThe root of many early forms of civilization, agriculture has been a primary reason for anthropization. To cultivate food or breed animals, humans must alter land—till soil or build structures—to facilitate agriculture. This can lead to soil erosion and pollution (pesticides, greenhouse gas emissions, etc.), and subsequently habitat fragmentation and overall an increased ecological footprint. Agriculture and industry often overlap, and industry produces many of these effects too.\n\nEspecially with approximately 7.5 Billion humans inhabiting the Earth, this typically aligns with an increase in residences worldwide. Over the years, humans have built on land to meet their needs and wants. These actions range from small villages to massive factories, water parks, and apartments. Urbanization and development of human residences can significantly affect the environment. Not only does the physical space of buildings fragment habitats and possibly endanger species, but it fundamentally alters the habitat for any other living being. For some species, this effect can be inconsequential, but for many this can have a dramatic impact. The biosphere is very much interconnected, and this means that if one organism is affected, then as a result the other organisms within this ecosystem and food chain are also affected. \n\nAs well, within the last century, any urbanized area requires roads for transportation. This transportation is a continued source of pollution, and the roads can be a source of soil erosion.\n\nTo support humans, industrial buildings and processes are apparently essential. Urban development and agriculture require that people produce, refined, or construct many things. Key to this is that factories require that people gather the materials they need to create a product. The wide range of products in this anthropological age use a plethora of substances that must be harvested or produced. Many of these materials are non-renewable (e.g., fossil fuel, metal ores, etc.) and the harvest of these results in relatively permanent anthropization. For resources that depend on in high quantity, this can also mean temporary depletion or damage to the source of the resource (e.g., depletion or pollution of fresh water reserves, improper or inefficient silviculture, etc.). Even sustainable or renewable industrial anthropization still affects the environment. While the resource in question may not be in jeopardy, the harvest and processing can still change and damage the environment.\n\nAnthropization can also be a result of scientific endeavours. This can manifest as construction of structures to aid in scientific discovery and observation. This can range from structures such as observatories, or on the opposite scale the Large Hadron Collider. These and many other things are built and used to enhance knowledge of sciences. They do however require space and energy.\n\nTo power the ever-growing human race, energy is needed. Power-harvesting structures are built to harness energy, such as dams, windmills, and nuclear reactors. These sources of energy ultimately fuel the rest of anthropological activity and are essential in this way. However many of these methods have consequences. With dams, construction aside, they can cause flooding, habitat fragmentation, and other effects. With nuclear reactors, they have a lasting effect in that typically a lifespan of one of these is around 50 years and afterwards the nuclear waste must be dealt with, and the structure itself must be shut down and cannot be used further. To safely dispose of this even low-level waste can take hundreds of years, ranging upwards with increased radioactivity. To produce and as a result of this production of energy, it requires a lot of anthropized land.\n\nChanges in population directly effect anthropological impact—but changes in technology and knowledge have greatly changed anthropization throughout the Holocene. The tools and methods that humans use to anthropize have changed drastically. For examples, the great pyramids in Egypt were not constructed by some large machine, but instead by thousands of humans. They were still able to build massive monuments, but the efficiency of their efforts and environmental damage is much different than today. This shows that the environmental effect of modern anthropization is generally greater, not just because of the increase in population. Pollution and loss of biodiversity in Egypt was largely natural, not man-made, and anthropization existed on a much lower level.\n\nAs the human population of Earth increase, this anthropization will continue to evolve.\n\n"}
{"id": "7878118", "url": "https://en.wikipedia.org/wiki?curid=7878118", "title": "Atlantis Expedition", "text": "Atlantis Expedition\n\nExpedition Atlantis is the name given to the crossing of the Atlantic Ocean made by five Argentines in 1984, leaving from the port of Tenerife in the Canary Islands and 52 days later arriving in La Guaira, Venezuela.\n\nThe aim was to prove that 3500 years before Christopher Columbus, African sailors may have accidentally reached the shores of America led by specific ocean currents.\n\nThe boat was a raft long by wide, built out of logs, rudderless, and with only a single sail. The crew consisted of Alfredo Barragan, Jorge Iriberri, Horacio Giaccaglia, Daniel Sanchez and Felix Arrieta Magariños.\n\nThe departure date was May 22, 1984, and landfall was made 52 days later, on July 12, 1984. The distance traveled was approximately .\n\n\n"}
{"id": "5471354", "url": "https://en.wikipedia.org/wiki?curid=5471354", "title": "Australian Geography Competition", "text": "Australian Geography Competition\n\nThe Australian Geography Competition is an Australia-wide competition run by the Royal Geographic Society of Queensland and the Australian Geography Teachers Association and sponsored by the National Geographic Channel. It tests the geographic knowledge of high school students from years 8-12. It starts with a written multiple choice test in early March. In the under 16 competition the winners from each state & the territories (Australian Capital Territory, Northern Territory, Cocos Islands) are taken as well as a runner up who has the highest score after these winners. These people are flown to Sydney at the start of June for a weekend of sightseeing followed by the national final. The winners go on to the National Geographic World Championship to represent Australia. The competition is also used to select the members of the Australian team for the International Geography Olympiad.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1965097", "url": "https://en.wikipedia.org/wiki?curid=1965097", "title": "Autonomous province", "text": "Autonomous province\n\nAutonomous province is term for a type of administrative territorial entity. \nTwo autonomous provinces exist:\n\nTwo autonomous provinces exist:\n\n"}
{"id": "39935679", "url": "https://en.wikipedia.org/wiki?curid=39935679", "title": "Center for Urban and Regional Analysis", "text": "Center for Urban and Regional Analysis\n\nCenter for Urban and Regional Analysis (CURA) \nThe Center for Urban & Regional Analysis (CURA) is an interdisciplinary research organization of The Ohio State University. CURA's mission is to serve as a bridge across academia, industry, and the policy sector by providing spatial analysis of economic, social, environmental, and health issues in urban and regional settings in Ohio and beyond. The organization explores various social, political, economic, geographic, economic, and public health issues using Geographic Information Systems (GIS) and other spatial analysis techniques. CURA also offers a speaker series which brings a wide range of speakers to campus each semester. \n\nThe movement for an urban center began in 1995 with a recommendation from the Committee on Applied Social and Public Policy. Funding followed three years later in 1998. CURA then was formed at The Ohio State University in 1999 as the Urban and Regional Analysis Initiative (URAI). Two years later, in April 2001, the organization officially renamed itself the Center for Urban & Regional Analysis and was given final approval by the Board of Trustees. The change was announced in the University's newspaper, The Lantern. The organization was projected to receive $240,000 per year in funding from the state of Ohio along with an additional $150,000 from the Office of Academic Affairs.\n\nCURA has conducted extensive research and analysis on various topics such as air traffic, urban growth, land use, food accessibility, crime, housing, and data mapping through GIS technology. It is one of just eight Ohio Urban University Program (UUP) research institutes. The center provides a place for researchers from many fields to collaborate on critical urban issues.\n\nThe Center for Urban and Regional Analysis is closely associated with Ohio State's Austin E. Knowlton School of Architecture and the Geography Department but collaborates with a wide range of departments and centers at Ohio State including:\n\nIn June 2013, CURA, in collaboration with the Chadwick Arboretum and Learning Gardens, completed a web-based Tree Tour of Ohio State's historic Oval. The Story Map, hosted on ArcGIS.com, provides the user with a guided walking tour of 32 significant and historic trees along with information about each tree's environmental and financial benefits.\n\nIn partnership with the Kirwan Institute of Ohio State, CURA is working to digitize Ohio's redlining maps from the 1930s.\n\nOhio State's Food Innovation Center and the Mid-Ohio Foodbank are working with CURA and using GIS to find ways to increase the equity of fresh food distribution.\n\nCURA partnered with Center for Aviation Studies at OSU to create maps displaying the accessibility of domestic air service in the United States. The maps show data for over 400 airports with an index describing the level of air service of a given region. The project requires the continued input of data to show how the accessibility of given airports changes over time. Graduate Affiliate Kejing Peng created a detailed web application displaying the results of the air traffic analysis. The T100 Airport Traffic Analysis is both interactive and informative to users, providing a great deal of information on individual airports across the country.\n\nWith Ohio State's Facilities Operations and Development, CURA Graduate Associate Shaun Fontanella is planning to develop digital signs for central buildings on campus. The signs will display and track energy consumption of each building in order to encourage decreased consumption.\n\n"}
{"id": "1886550", "url": "https://en.wikipedia.org/wiki?curid=1886550", "title": "Dry point", "text": "Dry point\n\nIn geography, a dry point is an area of firm or flood-free ground in an area of wetland, marsh or flood plains. The term typically applies to settlements, and dry point settlements were common in history.\n\nIn the United Kingdom extreme examples of dry point settlements include Glastonbury, situated on a low hill in the marshy, and once frequently flooded, Somerset Levels, and Wareham in Dorset surrounded by flood plains to the west and Poole Harbour to the east.\n\nA dry point has the advantages of flood protection, fertile soil (due to previous floodings which would have deposited silt on the land) and fairly flat land which is ideal for agriculture and building.\n"}
{"id": "623774", "url": "https://en.wikipedia.org/wiki?curid=623774", "title": "Environmental impact of electricity generation", "text": "Environmental impact of electricity generation\n\nElectric power systems consist of generation plants of different energy sources, transmission networks, and distribution lines. Each of these components can have environmental impacts at multiple stages of their development and use including in their construction, during the generation of electricity, and in their decommissioning and disposal. We can split these impacts into operational impacts (fuel sourcing, global atmospheric and localized pollution) and construction impacts (manufacturing, installation, decommissioning, and disposal). This page looks exclusively at the operational environmental impact of electricity generation. The page is organized by energy source and includes impacts such as water usage, emissions, local pollution, and wildlife displacement.\n\nMore detailed information on electricity generation impacts for specific technologies and on other environmental impacts of electric power systems in general can be found under the .\n\nWater usage is one of the most obvious environmental impacts of electricity generation. All thermal cycles (coal, natural gas, nuclear, geothermal, and biomass) use water as a cooling fluid to drive the thermodynamic cycles that allow electricity to be extracted from heat energy. Other energy sources such as wind and solar use water for cleaning equipment, while hydroelectricity has water usage from evaporation from the reservoirs. The amount of water usage is often of great concern for electricity generating systems as populations increase and droughts become a concern. In addition, changes in water resources may impact the reliability of electricity generation. The power sector in the United States withdraws more water than any other sector and is heavily dependent on available water resources. According to the U.S. Geological Survey, in 2005, thermo-electric power generation water withdrawals accounted for 41 percent (201 Bgal/d) of all freshwater withdrawals. Nearly all of the water withdrawn for thermoelectric power was surface water used for once-through cooling at power plants. Withdrawals for irrigation and public supply in 2005 were 37% and 13% of all freshwater withdrawals respectively. Likely future trends in water consumption are covered here.\n\nDiscussions of water usage of electricity generation distinguish between water withdrawal and water consumption. According to the USGS, “withdrawal” is defined as the amount of water removed from the ground or diverted from a water source for use, while “consumption” refers to the amount of water that is evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment. Both water withdrawal and consumption are important environmental impacts to evaluate.\n\nGeneral numbers for fresh water usage of different power sources are shown below.\n\nSteam-cycle plants (nuclear, coal, NG, solar thermal) require a great deal of water for cooling, to remove the heat at the steam condensors. The amount of water needed relative to plant output will be reduced with increasing boiler temperatures. Coal- and gas-fired boilers can produce high steam temperatures and so are more efficient, and require less cooling water relative to output. Nuclear boilers are limited in steam temperature by material constraints, and solar is limited by concentration of the energy source.\n\nThermal cycle plants near the ocean have the option of using seawater. Such a site will not have cooling towers and will be much less limited by environmental concerns of the discharge temperature since dumping heat will have very little effect on water temperatures. This will also not deplete the water available for other uses. Nuclear power in Japan for instance, uses no cooling towers at all because all plants are located on the coast. If dry cooling systems are used, significant water from the water table will not be used. Other, more novel, cooling solutions exist, such as sewage cooling at the Palo Verde Nuclear Generating Station.\n\nHydroelectricity's main cause of water usage is both evaporation and seepage into the water table.\n\nReference: Nuclear Energy Institute factsheet using EPRI data and other sources.\n\nSource(s): Adapted from US Department Of Energy, Energy Demand on Water Resources. Report to Congress on the Interdependence of Energy and Water, December 2006 (except where noted).*Cambridge Energy Research Associates (CERA) estimate. #Educated estimate.Water Requirements for Existing and Emerging Thermoelectric Plant Technologies. US Department Of Energy, National Energy Technology Laboratory, August 2008.Note(s): 3.6 GJ = gigajoule(s) == 1 MW·h = megawatt-hour(s), thus 1 L/GJ = 3.6 L/MW·h. B = Black coal (supercritical)-(new subcritical), Br = Brown coal (new subcritical), H = Hard coal, L = Lignite, cc = combined cycle, oc = open cycle, T = low-temperature/closed-circuit (geothermal doublet), T = high-temperature/open-circuit.\n\nMost electricity today is generated by burning fossil fuels and producing steam which is then used to drive a steam turbine that, in turn, drives an electrical generator.\n\nSuch systems allow electricity to be generated where it is needed, since fossil fuels can readily be transported. They also take advantage of a large infrastructure designed to support consumer automobiles. The world's supply of fossil fuels is large, but finite. Exhaustion of low-cost fossil fuels will have significant consequences for energy sources as well as for the manufacture of plastics and many other things. Various estimates have been calculated for exactly when it will be exhausted (see Peak oil). New sources of fossil fuels keep being discovered, although the rate of discovery is slowing while the difficulty of extraction simultaneously increases.\n\nMore serious are concerns about the emissions that result from fossil fuel burning. Fossil fuels constitute a significant repository of carbon buried deep underground. Burning them results in the conversion of this carbon to carbon dioxide, which is then released into the atmosphere. The estimated CO2 emission from the world's electrical power industry is 10 billion tonnes yearly. This results in an increase in the Earth's levels of atmospheric carbon dioxide, which enhances the greenhouse effect and contributes to global warming. The linkage between increased carbon dioxide and global warming is well accepted, though fossil-fuel producers vigorously contest these findings.\n\nDepending on the particular fossil fuel and the method of burning, other emissions may be produced as well. Ozone, sulfur dioxide, NO and other gases are often released, as well as particulate matter. Sulfur and nitrogen oxides contribute to smog and acid rain. In the past, plant owners addressed this problem by building very tall flue-gas stacks, so that the pollutants would be diluted in the atmosphere. While this helps reduce local contamination, it does not help at all with global issues.\n\nFossil fuels, particularly coal, also contain dilute radioactive material, and burning them in very large quantities releases this material into the environment, leading to low levels of local and global radioactive contamination, the levels of which are, ironically, higher than a nuclear power station as their radioactive contaminants are controlled and stored.\n\nCoal also contains traces of toxic heavy elements such as mercury, arsenic and others. Mercury vaporized in a power plant's boiler may stay suspended in the atmosphere and circulate around the world. While a substantial inventory of mercury exists in the environment, as other man-made emissions of mercury become better controlled, power plant emissions become a significant fraction of the remaining emissions. Power plant emissions of mercury in the United States are thought to be about 50 tons per year in 2003, and several hundred tons per year in China. Power plant designers can fit equipment to power stations to reduce emissions.\n\nAccording to Environment Canada:\n\"The electricity sector is unique among industrial sectors in its very large contribution to emissions associated with nearly all air issues. Electricity generation produces a large share of Canadian nitrogen oxides and sulphur dioxide emissions, which contribute to smog and acid rain and the formation of fine particulate matter. It is the largest uncontrolled industrial source of mercury emissions in Canada. Fossil fuel-fired electric power plants also emit carbon dioxide, which may contribute to climate change. In addition, the sector has significant impacts on water and habitat and species. In particular, hydro dams and transmission lines have significant effects on water and biodiversity.\"\nCoal mining practices in the United States have also included strip mining and removing mountain tops. Mill tailings are left out bare and have been leached into local rivers and resulted in most or all of the rivers in coal producing areas to run red year round with sulfuric acid that kills all life in the rivers.\n\nThe efficiency of some of these systems can be improved by cogeneration and geothermal (combined heat and power) methods. Process steam can be extracted from steam turbines. Waste heat produced by thermal generating stations can be used for space heating of nearby buildings. By combining electric power production and heating, less fuel is consumed, thereby reducing the environmental effects compared with separate heat and power systems.\n\nElectric cars burn no petroleum, thereby shifting any environmental impact from the car user to the electric utility. In South Africa an electric car, will be powered by coal generated electricity and harm the environment. In Norway an electric car will be powered by hydroelectricity and be harmless. Electric cars by themselves are neither beneficial or harmful, it depends how your region generates electricity.\n\nHomeowners can get 90% efficiency using natural gas to heat their home. Heat pumps are very efficient and burn no natural gas, shifting the environmental impacts from homeowners to electric utilities. Switching from natural gas to electricity in Alberta Canada burns natural gas and coal at about a 40% efficiency to supply the heat pump. In Quebec Canada where electric resistance heating is common, the heat pump will use 70% less hydroelectricity. Heat pumps may be beneficial for the environment or not, it depends how your region generates electricity.\n\nNuclear power plants do not burn fossil fuels and so do not directly emit carbon dioxide; because of the high energy yield of nuclear fuels, the carbon dioxide emitted during mining, enrichment, fabrication and transport of fuel is small when compared with the carbon dioxide emitted by fossil fuels of similar energy yield.\n\nA large nuclear power plant may reject waste heat to a natural body of water; this can result in undesirable increase of the water temperature with adverse effect on aquatic life.\n\nEmission of radioactivity from a nuclear plant is controlled by regulations. Abnormal operation may result in release of radioactive material on scales ranging from minor to severe, although these scenarios are very rare.\n\nMining of uranium ore can disrupt the environment around the mine. Disposal of spent fuel is controversial, with many proposed long-term storage schemes under intense review and criticism. Diversion of fresh or spent fuel to weapons production presents a risk of nuclear proliferation. Finally, the structure of the reactor itself becomes radioactive and will require decades of storage before it can be economically dismantled and in turn disposed of as waste.\n\nRenewable power technologies can have significant environmental benefits. Unlike coal and natural gas, they can generate electricity and fuels without releasing significant quantities of CO2 and other greenhouse gases that contribute to climate change, however the greenhouse gas savings from a number of biofuels have been found to be much less than originally anticipated, as discussed in the article Indirect land use change impacts of biofuels.\n\nBoth solar and wind have been criticized from an aesthetic point of view. However, methods and opportunities exist to deploy these renewable technologies efficiently and unobtrusively: fixed solar collectors can double as noise barriers along highways, and extensive roadway, parking lot, and roof-top area is currently available; amorphous photovoltaic cells can also be used to tint windows and produce energy. Advocates of renewable energy also argue that current infrastructure is less aesthetically pleasing than alternatives, but sited further from the view of most critics.\n\nThe major advantage of conventional hydroelectric dams with reservoirs is their ability to store potential power for later electrical production. The combination of a natural supply of energy and production on demand has made hydro power the largest source of renewable energy by far. Other advantages include longer life than fuel-fired generation, low operating costs, and the provision of facilities for water sports. Some dams also operate as pumped-storage plants balancing supply and demand in the generation system. Overall, hydroelectric power can be less expensive than electricity generated from fossil fuels or nuclear energy, and areas with abundant hydroelectric power attract industry.\n\nHowever, in addition to the advantages above, there are several disadvantages to dams that create large reservoirs. These may include: dislocation of people living where the reservoirs are planned, release of significant amounts of carbon dioxide at construction and flooding of the reservoir, disruption of aquatic ecosystems and bird life, adverse impacts on the river environment, potential risks of sabotage and terrorism, and in rare cases catastrophic failure of the dam wall.\n\nSome dams only generate power and serve no other purpose, but in many places large reservoirs are needed for flood control and/or irrigation, adding a hydroelectric portion is a common way to pay for a new reservoir. Flood control protects life/property and irrigation supports increased agriculture. Without power turbines, the downstream river environment would improve in several ways, however dam and reservoir concerns would remain unchanged.\n\nSmall hydro and run-of-the-river are two low impact alternatives to hydroelectric reservoirs, although they may produce intermittent power due to a lack of stored water.\n\nLand constrictions such as straits or inlets can create high velocities at specific sites, which can be captured with the use of turbines. These turbines can be horizontal, vertical, open, or ducted and are typically placed near the bottom of the water column.\n\nThe main environmental concern with tidal energy is associated with blade strike and entanglement of marine organisms as high speed water increases the risk of organisms being pushed near or through these devices. As with all offshore renewable energies, there is also a concern about how the creation of EMF and acoustic outputs may affect marine organisms. Because these devices are in the water, the acoustic output can be greater than those created with offshore wind energy. Depending on the frequency and amplitude of sound generated by the tidal energy devices, this acoustic output can have varying effects on marine mammals (particularly those who echolocate to communicate and navigate in the marine environment such as dolphins and whales). Tidal energy removal can also cause environmental concerns such as degrading farfield water quality and disrupting sediment processes. Depending on the size of the project, these effects can range from small traces of sediment build up near the tidal device to severely affecting nearshore ecosystems and processes.\n\nTidal barrages are dams built across the entrance to a bay or estuary that captures potential tidal energy with turbines similar to a conventional hydrokinetic dam. Energy is collected while the height difference on either side of the dam is greatest, at low or high tide. A minimum height fluctuation of 5 meters is required to justify the construction, so only 40 locations worldwide have been identified as feasible.\n\nInstalling a barrage may change the shoreline within the bay or estuary, affecting a large ecosystem that depends on tidal flats. Inhibiting the flow of water in and out of the bay, there may also be less flushing of the bay or estuary, causing additional turbidity (suspended solids) and less saltwater, which may result in the death of fish that act as a vital food source to birds and mammals. Migrating fish may also be unable to access breeding streams, and may attempt to pass through the turbines. The same acoustic concerns apply to tidal barrages. Decreasing shipping accessibility can become a socio-economic issue, though locks can be added to allow slow passage. However, the barrage may improve the local economy by increasing land access as a bridge. Calmer waters may also allow better recreation in the bay or estuary.\n\nElectrical power can be generated by burning anything which will combust. Some electrical power is generated by burning crops which are grown specifically for the purpose. Usually this is done by fermenting plant matter to produce ethanol, which is then burned. This may also be done by allowing organic matter to decay, producing biogas, which is then burned. Also, when burned, wood is a form of biomass fuel.\n\nBurning biomass produces many of the same emissions as burning fossil fuels. However, growing biomass captures carbon dioxide out of the air, so that the net contribution to global atmospheric carbon dioxide levels is small.\n\nThe process of growing biomass is subject to the same environmental concerns as any kind of agriculture. It uses a large amount of land, and fertilizers and pesticides may be necessary for cost-effective growth. Biomass that is produced as a by-product of agriculture shows some promise, but most such biomass is currently being used, for plowing back into the soil as fertilizer if nothing else.\n\nWind power harnesses mechanical energy from the constant flow of air over the surface of the earth. Wind power stations generally consist of wind farms, fields of wind turbines in locations with relatively high winds. A primary publicity issue regarding wind turbines are their older predecessors, such as the Altamont Pass Wind Farm in California. These older, smaller, wind turbines are rather noisy and densely located, making them very unattractive to the local population. The downwind side of the turbine does disrupt local low-level winds. Modern large wind turbines have mitigated these concerns, and have become a commercially important energy source. Many homeowners in areas with high winds and expensive electricity set up small wind turbines to reduce their electric bills.\n\nA modern wind farm, when installed on agricultural land, has one of the lowest environmental impacts of all energy sources:\n\nLandscape and heritage issues may be a significant issue for certain wind farms. However, when appropriate planning procedures are followed, the heritage and landscape risks should be minimal. Some people may still object to wind farms, perhaps on the grounds of aesthetics, but there is still the supportive opinions of the broader community and the need to address the threats posed by climate change.\n\nOffshore wind is similar to terrestrial wind technologies, as a large windmill-like turbine located in a fresh or saltwater environment. Wind causes the blades to rotate, which is then turned into electricity and connected to the grid with cables. The advantages of offshore wind are that winds are stronger and more consistent, allowing turbines of much larger size to be erected by vessels. The disadvantages are the difficulties of placing a structure in a dynamic ocean environment.\n\nThe turbines are often scaled-up versions of existing land technologies. However, the foundations are unique to offshore wind and are listed below:\n\nMonopile foundations are used in shallow depth applications (0–30 m) and consist of a pile being driven to varying depths into the seabed (10–40 m) depending on the soil conditions. The pile-driving construction process is an environmental concern as the noise produced is incredibly loud and propagates far in the water, even after mitigation strategies such as bubble shields, slow start, and acoustic cladding. The footprint is relatively small, but may still cause scouring or artificial reefs. Transmission lines also produce an electromagnetic field that may be harmful to some marine organisms.\n\nTripod fixed bottom foundations are used in transitional depth applications (20–80 m) and consist of three legs connecting to a central shaft that supports the turbine base. Each leg has a pile driven into the seabed, though less depth is necessary because of the wide foundation. The environmental effects are a combination of those for monopile and gravity foundations.\n\nGravity foundations are used in shallow depth applications (0–30 m) and consist of a large and heavy base constructed of steel or concrete to rest on the seabed. The footprint is relatively large and may cause scouring, artificial reefs, or physical destruction of habitat upon introduction. Transmission lines also produce an electromagnetic field that may be harmful to some marine organisms.\n\nGravity tripod foundations are used in transitional depth applications (10–40 m) and consist of two heavy concrete structures connected by three legs, one structure sitting on the seabed while the other is above the water. As of 2013, no offshore windfarms are currently using this foundation. The environmental concerns are identical to those of gravity foundations, though the scouring effect may be less significant depending on the design.\n\nFloating structure foundations are used in deep depth applications (40–900 m) and consist of a balanced floating structure moored to the seabed with fixed cables. The floating structure may be stabilized using buoyancy, the mooring lines, or a ballast. The mooring lines may cause minor scouring or a potential for collision. Transmission lines also produce an electromagnetic field that may be harmful to some marine organisms.\n\nGeothermal energy is the heat of the Earth, which can be tapped into to produce electricity in power plants. Warm water produced from geothermal sources can be used for industry, agriculture, bathing and cleansing. Where underground steam sources can be tapped, the steam is used to run a steam turbine. Geothermal steam sources have a finite life as underground water is depleted. Arrangements that circulate surface water through rock formations to produce hot water or steam are, on a human-relevant time scale, renewable.\n\nWhile a geothermal power plant does not burn any fuel, it will still have emissions due to substances other than steam which come up from the geothermal wells. These may include hydrogen sulfide, and carbon dioxide. Some geothermal steam sources entrain non-soluble minerals that must be removed from the steam before it is used for generation; this material must be properly disposed. Any (closed cycle) steam power plant requires cooling water for condensers; diversion of cooling water from natural sources, and its increased temperature when returned to streams or lakes, may have a significant impact on local ecosystems.\n\nRemoval of ground water and accelerated cooling of rock formations can cause earth tremors. Enhanced geothermal systems (EGS) fracture underground rock to produce more steam; such projects can cause earthquakes. Certain geothermal projects (such as one near Basel, Switzerland in 2006) have been suspended or canceled owing to objectionable seismicity induced by geothermal recovery. However, risks associated with \"hydrofracturing induced seismicity are low compared to that of natural earthquakes, and can be reduced by careful management and monitoring\" and \"should not be regarded as an impediment to further development of the Hot Rock geothermal energy resource\".\n\nCurrently solar photovoltaic power is used primarily in Germany and Spain where the governments offer financial incentives. In the U.S., Washington State also provides financial incentives. Photovoltaic power is also more common, as one might expect, in areas where sunlight is abundant.\n\nIt works by converting the sun's radiation into direct current (DC) power by use of photovoltaic cells. This power can then be converted into the more common AC power and fed to the power grid.\n\nSolar photovoltaic power offers a viable alternative to fossils fuels for its cleanliness and supply, although at a high production cost. Future technology improvements are expected to bring this cost down to a more competitive range.\n\nIts negative impact on the environment lies in the creation of the solar cells which are made primarily of silica (from sand) and the extraction of silicon from silica may require the use of fossil fuels, although newer manufacturing processes have eliminated CO production. Solar power carries an upfront cost to the environment via production, but offers clean energy throughout the lifespan of the solar cell.\n\nLarge scale electricity generation using photovoltaic power requires a large amount of land, due to the low power density of photovoltaic power. Land use can be reduced by installing on buildings and other built up areas, though this reduces efficiency.\n\nAlso known as solar thermal, this technology uses various types of mirrors to concentrate sunlight and produce heat. This heat is used to generate electricity in a standard Rankine cycle turbine. Like most thermoelectric power generation, this consumes water. This can be a problem, as solar powerplants are most commonly located in a desert environment due to the need for sunlight and large amounts of land. Many concentrated solar systems also use exotic fluids to absorb and collect heat while remaining at low pressure. These fluids could be dangerous if spilled.\n\nNegawatt power refers to investment to reduce electricity consumption rather than investing to increase supply capacity. In this way investing in Negawatts can be considered as an alternative to a new power station and the costs and environmental concerns can be compared.\n\nNegawatt investment alternatives to reduce consumption by improving efficiency include:\n\nNegawatt investment alternatives to reduce peak electrical load by time shifting demand include:\n\nNote that time shifting does not reduce total energy consumed or system efficiency; however, it can be used to avoid the need to build a new power station to cope with a peak load.\n\n\n"}
{"id": "31642145", "url": "https://en.wikipedia.org/wiki?curid=31642145", "title": "European and American voyages of scientific exploration", "text": "European and American voyages of scientific exploration\n\nThe era of European and American voyages of scientific exploration followed the Age of Discovery and were inspired by a new confidence in science and reason that arose in the Age of Enlightenment. Maritime expeditions in the Age of Discovery were a means of expanding colonial empires, establishing new trade routes and extending diplomatic and trade relations to new territories, but with the Enlightenment scientific curiosity became a new motive for exploration to add to the commercial and political ambitions of the past. See also List of Arctic expeditions and List of Antarctic expeditions.\n\nFrom the early 15th century to the early 17th century the Age of Discovery had, through Spanish and Portuguese seafarers, opened up southern Africa, the Americas (New World), Asia and Oceania to European eyes: Bartholomew Dias had sailed around the Cape of southern Africa in search of a trade route to India; Christopher Columbus, on four journeys across the Atlantic, had prepared the way for European colonisation of the New World; Ferdinand Magellan had commanded the first expedition to sail across the Atlantic and Pacific oceans to complete the first circumnavigation of the Earth. Over this period colonial power shifted from the Portuguese and Spanish to the Dutch and then the British and French. The new era of scientific exploration began in the late 17th century as scientists, and in particular natural historians, established scientific societies that published their researches in specialist journals. The British Royal Society was founded in 1660 and encouraged the scientific rigour of empiricism with its principles of careful observation and deduction. Activities of early members of the Royal Society served as models for later maritime exploration. Hans Sloane (1650–1753) was elected a member in 1685 and travelled to Jamaica from 1687 to 1689 as physician to the Duke of Albemarle (1653–1688) who had been appointed Governor of Jamaica. In Jamaica Sloane collected numerous specimens which were carefully described and illustrated in a published account of his stay. Sloane bequeathed his vast collection of natural history 'curiosities' and library of over 50,000 bound volumes to the nation, prompting the establishment in 1753 of the British Museum. His travels also made him an extremely wealthy man as he patented a recipe that combined milk with the fruit of \"Theobroma cacao\" (cocoa) he saw growing in Jamaica, to produce milk chocolate. Books of distinguished social figures like the intellectual commentator Jean Jacques Rousseau, Director of the Paris Museum of Natural History Comte de Buffon, and scientist-travellers like Joseph Banks, and Charles Darwin, along with the romantic and often fanciful travelogues of intrepid explorers, increased the desire of European governments and the general public for accurate information about the newly discovered distant lands.\n\nOne of the earliest French expeditions on the coasts of Africa, South America and through the Strait of Magellan was made by a squadron of French men-of-war under the command of M. de Gennes in 1695–97. The young French explorer, engineer and hydrographer François Froger described this expedition in his \"A Relation of a Voyage\" (1699).\n\nBy the 18th century maritime exploration had become safer and more efficient with technical innovations that vastly improved navigation and cartography: improvements were made to the theodolite, octant, precision clocks, as well as the compass, telescope, and general shipbuilding techniques.\nFrom the mid-18th century through the 19th century scientific missions mapped the newly discovered regions, brought back to Europe the newly discovered fauna and flora, made hydrological, astronomical and meteorological observations and improved the methods of navigation. This stimulated great advances in the scientific disciplines of natural history, botany, zoology, ichthyology, conchology, taxonomy, medicine, geography, geology, mineralogy, hydrology, oceanography, physics, meteorology etc. – all contributing to the sense of \"improvement\" and \"progress\" that characterized the Enlightenment. Artists were used to record landscapes and indigenous peoples, while natural history illustrators captured the appearance of organisms before they deteriorated after collection. Some of the worlds finest natural history illustrations were produced at this time and the illustrators changed from informed amateurs to fully trained professionals acutely aware of the need for scientific accuracy.\n\nBy the middle of the 19th century all of the world's major land masses, and most of the minor ones, had been discovered by Europeans and their coastlines charted. This marked the end of this phase of science as the of 1872–1876 began exploring the deep seas beyond a depth of 20 or 30 meters. In spite of the growing community of scientists, for nearly 200 years science had been the preserve of wealthy amateurs, educated middle classes and clerics. At the start of the 18th century most voyages were privately organized and financed but by the second half of the century these scientific expeditions, like James Cook's three Pacific voyages under the auspices of the British Admiralty, were instigated by government. In the late 19th century, when this phase of science was drawing to a close, it became possible to earn a living as a professional scientist although photography was beginning to replace the illustrators. The exploratory sailing ship had gradually evolved into the modern research vessels. From now on maritime research in new European colonies in America, Africa, Australia, India and elsewhere, would be carried out by researchers within the occupied territories themselves.\n\nThis compendium of voyages of scientific exploration provides an overview of maritime scientific research carried out at the time of the Enlightenment in Europe.\nPublished journals and accounts are included with the individual voyages.\n\nConsidered the first scientific voyage undertaken by the Royal Navy, its primary purpose was the discovery of new lands in the South Atlantic Ocean. It was during this trip that several islands of the Tuamotu archipelago were discovered. was a 24-gun post ship launched in 1751 and used as a survey ship from 1764, making two circumnavigations under the command of John Byron and Samuel Wallis. She was broken up in 1777.\n\n\nA circumnavigation by the English navigator Samuel Wallis, on board , accompanied by Philip Carteret on the consort ship . In August 1766, the two ships passed through the Strait of Magellan. In December 1766, conflicts between the two captains led to the separation of the ships. \"Dolphin\" reached Tahiti in June 1767. Samuel Wallis studied the customs of the Polynesians, reaching the Dutch East Indies at Batavia, returning to London in May 1768. Meanwhile, Philip Carteret in \"Swallow\" explored and studied the Solomon Islands, New Ireland (island) (now part of Papua New Guinea) and the islands of the Indonesian archipelago (Sulawesi among others). The expedition also stopped in Batavia from June to September 1768 and returned to London in March 1769.\n\n\nThis British ship explored Newfoundland and Labrador with Constantine Phipps aboard and Thomas Adams (Captain?), and with Joseph Banks also aboard. HMS \"Niger\" was a 33-gun fifth-rate launched in 1759, converted to a prison ship in 1810 and renamed \"Negro\" in 1813. She was sold in 1814.\n\n\nOrdered by Louis XV, it is the first trip around the world initiated by the French. The discovery and description of Tahiti by Louis Antoine de Bougainville in his trip will have a very significant impact on the philosophers of the Enlightenment including Jean-Jacques Rousseau (1712–1778). The expedition was organised by Louis Antoine de Bougainville and received the support of such prominent figures of the time as Charles de Brosses (1709–1777), Comte de Buffon (1707–1788), Pierre Louis Moreau de Maupertuis (1698–1759) and Jérôme Lalande (1732–1807).\n\nThe purpose of the expedition is to discover new territories available for settlement, to open a new route to reach China, to found new outlets for the French East India Company and, finally, discover acclimatable spices for the Isle de France (now Mauritius).\n\n\nAn expedition to observe the transit of Venus across the Sun (in 1769) that included the discovery of new Islands, Tuamotu and Society Islands, the first circumnavigation of New Zealand and charting of the East coast of New Holland.\n\n\nExpedition to harvest spices for production on Mauritius, to prevent the monopoly of their trade by the Dutch.\n\n\nAn expedition in the brig \"Sir Lawrence\" exploring Iceland and the islands along the West coast of Scotland.\n\n\nCook's second voyage in and around the world. He again visited New Zealand, sailed near the Antarctic and discovered many islands in the Pacific. Swedish Sparrman embarked during a stopover at the Cape.\n\n\nExploration of the southern Indian Ocean and the shipping routes to India.\n\n\nExploration of the southern Indian Ocean.\n\n\nA British expedition to explore the Arctic Sea. The two ships reached Svalbard before turning back because of the ice. Horatio Nelson was involved with the trip.\n\n\nCook's Third Voyage to find the North-West passage by crossing the Bering Strait. Cook was killed in the Hawaiian archipelago.\n\n\nFrench King Louis XVI inspired by Cook's voyages mounted his own expedition under the direction of La Pérouse. Cook's anti-scorbutic remedies to eradicate scurvy were applied successfully. Lamanon and twelve other members of the expedition were massacred by natives at Vanuatu where they were looking for water. The two ships disappeared in the Solomon Islands, at Vanikoro, during a violent storm.\n\n\nGlobal circumnavigation.\n\n\nA Russian expedition commanded by the British Captain Joseph Billings, astronomer on Cook's third voyage. This expedition lasted more than ten years attempting, unsuccessfully, to find the northwest passage that had remained undiscovered after Cook's explorations.\n\n\nThe Solide expedition was the second successful circumnavigation by the French, after that by Bougainville. It occurred from 1790 to 1792 but remains little known due to its mostly commercial aims in the fur trade between the northwest American coast and China.\n\n\nThe Spanish Malaspina Expedition explored the coasts of Spanish possessions in America and Alaska, always looking for the northwest passage. More than 70 crates of natural history specimens were sent to Madrid. On return Captain Malaspina was forced into exile because of his ideas, suggesting, among other things, that Spain abandon the military domination of its colonies in favour of a Federation. The scientific journal of the trip was lost but recovered in 1885.\n\n\nAn expedition to find the two vessels commanded by Jean-François de La Pérouse (1741–1788) and of which there was no news after they left Port Jackson heading for southern Tasmania and southern Australia. Captain Kermadec died in May 1793 and Captain d'Entrecasteaux in July of the same year. The expedition was headed by a royalist and heard of the terror in France when putting into the Dutch colonies. The crew was arrested and collections of natural history confiscated and offered by the Dutch to the British. These were however, on the express request of Joseph Banks (1743–1820), returned to France\n\n\nThe Royal Society of Arts, Manufactures and Commerce offered a reward of fifty pounds for living Bread-fruit plants. Bligh completed this in , his second mission to collect breadfruit plants and other botanical specimens from the Pacific. These he transported to the West Indies, specimens being given to the Royal Botanic Gardens in St. Vincent. This expedition was a success, returning to the Royal Botanic Gardens Kew with 1,283 plants including varieties of apple, pear, oranges and mangoes. In addition to these specimens, the expedition accomplished many observations and cartographic surveys in the South Seas.\n\n\nA mission to the South Seas and Pacific Northwest coast of America. In 1791, left England with . Both ships anchored at Cape Town before exploring the south coast of Australia. In King George Sound, the Discovery's naturalist and surgeon Archibald Menzies collected various plant species including \"Banksia grandis\", the first recording of the genus \"Banksia\" from Western Australia. The two ships sailed to Hawaii where Vancouver named Kamehameha I. Chatham and Discovery then sailed on to the Northwest Pacific. Over the course of the next four years, Vancouver surveyed the northern Pacific Ocean coast in Discovery wintering in Spanish California or Hawaii. Discovery's primary mission was to exert British sovereignty over this part of the Northwest Coast following the hand-over of the Spanish Fort San Miguel at Nootka Sound, although exploration in co-operation with the Spanish was seen as an important secondary objective. Exploration work was successful as relations with the Spanish went well; resupply in California was especially helpful. Vancouver and the Spanish commandant Juan Francisco de la Bodega y Quadra were on such good terms that the original name of Vancouver Island was actually \"Quadra and Vancouver's Island\".\n\n\nThis expedition was organised to establish a permanent colonial presence in the South seas before the British, concentrating on the mapping of the coast of the Australia and New Guinea. Nicolas Baudin died in Mauritius in 1803, another naturalist on the island of Timor, two other naturalists chose to stay on the island and two astronomers died of dysentery. Péron, assisted by his friend Lesueur, managed to gather a vast zoological collection. \"\"Naturaliste\"\" returned to France in 1803 with a part of the collections. Captain Baudin bought a schooner, the then at Port Jackson. Baudin was replaced by Pierre Bernard Milius (1773–1829).\n\n\nThe first circumnavigation of Australia. The work of scientific observation was interrupted due to damage and many specimens transferred to were lost when it sank. The observations of Brown on the flora of this continent were the most extensive at this time.\n\n\nThe first Russian circumnavigation of the world was intended to establish a link with Russian possessions in America, the transport of goods at that time being via Siberia (a journey lasting about two years). The second objective, which was not achieved, was to establish trade and diplomatic links with Japan. This expedition took place during the rule of emperor Alexander I (1777–1825).\n\n\"Nadezhda\" and \"Neva\" explored the Aleutian Islands, Sakhalin and discovered the mouth of the Love River. They also visited the Marquesas Islands and Hawaii. Baron von Langsdorff left the expedition in 1805 to explore the Interior of Alaska and California. Thirteen cases of natural history specimens were shipped to the St. Petersburg Academy of Sciences.\n\n\nA Russian expedition funded by the Chancellor of Russia, count Nikolai P. Romanzof to investigate the northeast passage in the Bering Sea. The coast of Alaska was studied and the South Pacific, also the cartography of 36 islands including the Marshall Islands. Also natural history collections made.\n\n\nA French expedition exploring Western Australia and islands of Timor, Molucca, Samoa and Hawaii. \"L'Uranie\" visited Rio de Janeiro to take a series of pendulum measurements as well as other observations, not only in geography and ethnology, but in astronomy, terrestrial magnetism, and meteorology, and for the collection of specimens in natural history.\n\n\nOne of the missions of this expedition and recruit workers to Java and Philippines to French Guiana. Botanist Samuel Perrottet (1793–1870) settled in Guyana to investigate the acclimation of plants reported to Asia. \"La Durance\" returned to France in 1820, \"Le Rhône\" the following year.\n\n\nLouis Isidore Duperrey commanded the expedition in \"La Coquille\" with Jules Dumont d'Urville as second in command. The naturalists appointed to the expedition were the surgeon, pharmacist and zoologist René Primevère Lesson and surgeon-major Prosper Garnot. Doctor Garnot had a severe attack of dysentery and was sent back on the \"Castle Forbes\" with some of the specimens collected in South America and the Pacific. The specimens were lost when the ship was wrecked off the Cape of Good Hope in July 1824. Garnot and Lesson wrote the zoological section of the voyage's report.\n\n\nAn expedition of two ships of war, the main object of which was to take reinforcements to Kamchatka. There was, however, a staff of scientists on board the Russian sailing sloop \"Predpriyatiye\" (Russian: \"Enterprise\"), who collected much valuable information and material on geography, ethnography and natural history. The expedition, proceeding by Cape Horn, visited the Radak and Society Islands, and reached Petropavlovsk in July 1824. Many positions along the coast were mapped more accurately, the Navigator islands visited, and several discoveries made. The expedition returned by the Marianas, Philippines, New Caledonia and the Hawaiian Islands, reaching Kronstadt on July 10, 1826.\n\n\nIn 1824 Byron was chosen to accompany homewards the bodies of Hawaiian monarchs Liholiho (known as King Kamehameha II) and Queen Kamāmalu, who had died of measles during a state visit to England. He sailed in in September 1824, accompanied by several naturalists and, amongst others, his lieutenant, Edward Belcher.\nHe toured the islands and made observations. With the consent of Christian missionaries to the islands, he also removed wooden carvings and other artifacts of the chiefs of ancient Hawaii from the temple ruins of Puuhonua O Hōnaunau.\nOn his return journey in 1825, Lord Byron discovered and charted Malden Island, which he named after his surveying officer, Mauke; and Starbuck Island. Starbuck was named in honour of Captain Valentine Starbuck, an American whaler who had sighted the island while carrying the Hawaiian royal couple to England in 1823–1824, but which had probably been previously sighted by his cousin and fellow-whaler Captain Obed Starbuck in 1823.\n\n\nA mission to establish diplomatic relations with Indochina and make geographical observations. On 12 January 1825, Hyacinthe de Bougainville led an embassy to Vietnam with Captain Courson de la Ville-Hélio, arriving in Da Nang, with the warships \"Thétis\" and \"L'Espérance\". Although they had a 28 January 1824 letter from Louis XVIII, the ambassadors could not obtain an audience with Minh Mạng.\n\n\nA British expedition to the Bering Sea attempting a rendezvous with the expedition of Sir John Franklin (1786–1847) at the mouth of Mackenzie River. reached as far north as Point Barrow, Alaska, the furthest point into the Arctic any non-Inuit had been at the time, but was unable to join the Franklin expedition. With Lay ill it was Beechey and Collie that performed most of the specimen collection but many could not be preserved.\n\n\nThe mission was the hydrographic survey of Patagonia and Tierra del Fuego, under the overall command of the surveyor Commander Phillip Parker King, in HMS \"Adventure\".\n\nIn the desolate waters of Tierra del Fuego Stokes, the captain of , became depressed and shot himself on 2 August 1828 dying a few days later. Parker King replaced Stokes with Lieutenant W.G. Skyring as commander of the ship, and both ships sailed to Montevideo. After the ships arrived at Rio de Janeiro for repairs and provisioning, Rear Admiral Sir Robert Otway, the Commander-in-chief of the South American station, gave command of \"Beagle\" to his aide, Lieutenant Robert FitzRoy. Fuegians were taken back with them when the \"Beagle\" returned. During this survey, the Beagle Channel was identified and named after the ship.\n\n\nThis mission, led by Dumont d'Urville, searched for the two vessels of La Pérouse (1741–1788). The coasts of Australia, of New Zealand, of Fiji and the Loyalty Islands were explored. Dumont d'Urville renamed \"La Coquille\" as \"L'Astrolabe\" as a tribute to the ship of La Pérouse.\n\n\nA Russian circumnavigation on the ship \"\", sailing from Cronstadt and rounding Cape Horn accompanied by Captain Mikhail Nikolaievich Staniukovich in command of the sloop \"Moller\". During the voyage Litke and his team described the western coastline of the Bering Sea, the Bonin Islands off Japan, and the Carolines, discovered 12 new islands. An expedition to strengthen Russian presence near Alaska. A large collection of natural history specimens was made including 1,000 new speciess of insects, fish, birds and other animals and 2,500 plant specimens including algae and minerals.\n\n\nThe first expedition to map the coast of India.\n\n\nDutch exploration of New Guinea.\n\nScientific exploration was placed under the direction of Jean-Baptiste Bory de Saint-Vincent (1778–1846).\n\n\nAs British, American and Dutch voyages consolidated their interest in Australia, Hawaii and New Guinea, the French government sought to secure the religious freedoms and rights of French residents in the South Pacific. The expedition passed the Cape of Good Hope, stopping at Pondicherry and Madras, and then exploring the coast of Cochinchina and Tonkin, stopping in the Philippines, Australia, Tasmania and New Zealand. The expedition was considered a great success, many hydrological observations were completed and natural history collections assembled.\n\n\nA world circumnavigation to make a hydrographic survey of the coast of Patagonia, Tierra del Fuego, Chile and Peru, and establish accurate longitude measurements. Charles Darwin paid his own way as a naturalist/companion to the captain, and found the voyage a stimulus both to his career as a geologist and to the formulation of his theory of evolution.\n\n\nTwo expeditions to the coasts of Iceland and Greenland in an attempt to trace the \"Bordelaise\" commanded by Jules de Blosseville (1802–1833) which had been missing since 1833.\n\n\nExpedition (circumnavigation) in the frigate Vénus to assess the economic viability of whaling in the North Pacific.\n\n\nA global circumnavigation sailing the coast of South America, back along the West Coast to California, across the Pacific, reaching Manila, China, India, the Isla Borbón and returning to France. More than 1,000 new plant species were collected and many geographical and meteorological observations made.\n\n\nExploration of the Pacific coast of America and interior of Nicaragua and El Salvador. participated in the First Opium War between 1840 and 1841 and was later used to survey the harbour of Hong Kong in 1841, returning to England in 1842.\n\n\nThe second voyage of \"L'Astrolabe\", this time accompanied by \"La Zélée\", sailed on 7 September 1837 and at the end of November, the ships reached the Strait of Magellan. Dumont thought there was sufficient time to explore the strait for three weeks, taking into account the precise maps drawn by Phillip Parker King between 1826 and 1830, before heading south again but two weeks after seeing their first iceberg, the ships were encased in pack ice for a while. After reaching the South Orkney Islands, the expedition headed directly to the South Shetland Islands and the Bransfield Strait. Then located some land which was named \"Terre de Louis-Philippe\" (now called Graham Land), the Joinville Island group and \"Rosamel Island\" (now called Andersson Island. In poor shape the two ships headed for Talcahuano in Chile. Turning south they led for the first time some experiments to determine the approximate position of the South magnetic pole, discovered the Terre Adélie on January 20, 1840 and landed two days later on an islet of the Géologie Archipelago () 4 km from the mainland to take mineral and animal samples.\n\nFor all other publications by themes and authors, refer to in the Publications part.\n\nThe mission was the hydrographic survey of the coasts of Australia. In 1839 Lieutenant Stokes sighted a natural harbour which Wickham named Port Darwin, the later settlement nearby eventually became the city of Darwin, Northern Territory. In 1841 Wickham fell ill, and Stokes took command.\n\n\nThe \"Wilkes Expedition\", included naturalists, botanists, a mineralogist, taxidermists, artists and a philologist in the ships \"Vincennes\", \"Peacock\", the brig \"Porpoise\", the store-ship \"Relief\", and two schooners, \"Sea Gull\", and \"Flying Fish\".\n\nDeparting Hampton Roads on 18 August 18, 1838, the expedition stopped at Madeira and Rio de Janeiro, Argentina; visited Tierra del Fuego, Chile, Peru, the Tuamotu Archipelago, Samoa, and New South Wales. From Sydney, Australia, the fleet sailed into the Antarctic Ocean in December 1839 and reported the discovery \"of an Antarctic continent west of the Balleny Islands\" of which it sighted the coast on January 25, 1840. Next, the expedition visited Fiji and the Hawaiian Islands in 1840. In July 1840, two sailors, one of whom was Wilkes' nephew, Midshipman Wilkes Henry, were killed while bartering for food on Malolo, in Fiji. Wilkes retribution was swift and severe. According to an old man of Malolo Island, nearly 80 Fijians were killed in the incident.\n\nFrom December 1840 to March 1841, his men with native Hawaiian porters hauled a pendulum to the summit of Mauna Loa to measure gravity. He explored the west coast of North America, including the Strait of Juan de Fuca, Puget Sound, the Columbia River, San Francisco Bay and the Sacramento River, in 1841.\nThe expedition returned by way of the Philippines, the Sulu Archipelago, Borneo, Singapore, Polynesia and the Cape of Good Hope, reaching New York City on 10 June 1842. This was the first circumnavigation of the world funded by the Government of the United States and the last by a sailing vessel. The expedition was poorly prepared and of five vessels which left, only two returned to port. The natural history collections were very rich with 50,000 plant specimens (approximately 10 000 species) and 4,000 specimens of animals (half being new species).\n\n\nThis British trip, sponsored by the Royal Society, was to discover magnetic and geographic features of the Antarctic. The expedition was prepared with great care by James Clark Ross, already familiar with Polar navigation. The two ships left the United Kingdom on 19 September 1839, stopping to explore the Kerguelen Islands in 1840, and then on Tasmania to build a magnetic observatory for the Antarctic and to conduct cartographic work. Mount Erebus and the Ross Sea were discovered during this journey. After three attempts, Ross admitted that the magnetic pole lay in land that he could not reach. Following the footsteps of his uncle John Ross, he performed the first deep sea surveys up to 4800 m (2677 fathoms), using ropes. Unfortunately biological specimens collected decomposed.\n\n\nA scientific exploration in the China Sea and Indian Ocean.\n\n\nDuring the early to mid-1840s, charted numerous trade and other routes between many locations, primarily off Australia's North-east coast and nearby islands. Such islands included Whitsunday Island and the Capricorn Islands. After being discovered during the survey of the Gulf of Papua, New Guinea, the Fly River was named after \"Fly\". For the most of its seaworthy existence, \"Fly\" was captained by Francis Price Blackwood.\n\n\nAn expedition to the Cape York and Torres Strait areas of northern Australia.\n\n\nA French expedition circumnavigating the world via Cape Horn, stopping in Tahiti and Ualan to determine an astronomical Meridian intended for future travel in the Pacific, then arriving in China. There, the ship performed several missions of exploration including, in July–August 1852, in the seas of Korea and Japan (then very little known in Europe) and on the coasts of Kamchatkato completely unknown the Lapérouse expedition. The \"Capricieuse\" then returned to France via the Cape of Good Hope. This was the last French global circumnavigation by sail.\n\n\nA Swedish natural history excursion, contributing to the capture of Manuel Briones, a robber who seized an American whaler \"George Howland\" and who was the terror on the coast of the Ecuador.\n\n\nA survey of the Australian coast and Fiji Islands, continuing the mission of HMS \"Rattlesnake\". Following disagreements with the captain, naturalist John MacGillivray disembarks at Sydney in January 1854. was a 500-ton, 28-gun sixth-rate, launched as \"Termagant\" in 1822 and renamed in 1824. She served as a survey ship under Henry Kellett and Henry Mangles Denham and was sold in 1864.\n\n\nThis American expedition explored the coasts of Japan, China, Siberia and Kamchatka before putting in at the Cape of Good Hope and returning to the United States. \"Porpoise\" sank in a typhoon in 1854.\n\n\nAn expedition organized by the Emperor of Austria to demonstrate the power of the Crown. \"Novara\" departed Trieste in April 1857, passing the Cape of Good Hope to reach the Philippines, Australia, and New Zealand. Fourteen of the forty-four guns were dumped to make more room for the scientific collections.\n\n\nAn oceanographic survey in for the laying of a submarine telegraph cable in the North Atlantic.\n\n\nAn Italian circumnavigation of the globe that made important scientific observations in South America. The purpose of the trip was also to establish diplomatic relations with China and Japan, but without success. De Filippi set out in 1866 on a government-sponsored scientific voyage to circumnavigate the globe. The ship, the Italian warship Magenta, sailed under the command of Vittorio Arminjon, departing Montevideo on February 2, 1866. It reached Naples on March 28, 1868. However, De Filippi himself died en route at Hong Kong, on February 9, 1867, from serious dysentery and liver problems. The scientific report was completed by his assistant, Professor Enrico Hillyer Giglioli. Giglioli returned to Italy in 1868.\n\n\nAn expedition embarked in leaving Sydney in June 1865 to explore the Pacific Islands. One of the objectives is to punish the inhabitants of the islands of Tanna for mistreating a missionary.\n\n\nTwo oceanographic expeditions in the Atlantic Ocean and Mediterranean Sea.\n\n\nThe Challenger Expedition was a grand tour of the world during covering 68,000 nautical miles (125,936 km) organized by the Royal Society in collaboration with the University of Edinburgh. Charles Thomson was the leader of a large scientific team.\n\n\nThe British Arctic Expedition in and seeking to establish the geographic and magnetic north pole.\n\n\nSeveral expeditions were conducted in the Bering Sea in 1881 to find the \"Jeannette\" and two whaling ships. Wrangell Island was discovered and made part of the United States in August 1881 with the landing of famed explorer John Muir and the crew of U. S. Revenue Marine ship Thomas Corwin under the command of Captain Calvin Leighton Hooper. The landing at the mouth of the Clark River was illustrated by Muir in his book \"The Cruise of the Corwin\". Two weeks after the Corwin took possession, USS \"John Rodgers\" conducted a complete survey of the island, which turned out to equal the size of Rhode Island and Delaware combined.\n\n\nThe building of the French Navy \"La Romanche\" was for a French multidisciplinary expedition on a Scientific Mission to Cape Horn. (See also Romanche Glacier)\n\n\nThe \"Vettor Pisani\" was an Italian naval corvette equipped for scientific exploration.\n\n belonged to the Committee on Fisheries of the United States and it carried out numerous scientific expeditions under the direction of Alexander Emanuel Agassiz (1835–1910). The primary goal was an inventory of the Pacific fishery reserves but many other observations are carried out by Townsend and other scientists.\n\n\nZoologist Walter Rothschild commissioned the Webster-Harris Expedition to the Galápagos Islands from June 1897 to February 1898. This expedition on the schooner \"Lila & Mattie\" is well-described in the 1983 book titled \"Dear Lord Rothschild\" by Miriam Rothschild. In the 1936 book \"Oceanic Birds of South America\" by Robert Cushman Murphy, Rollo Beck describes the seminal telegram from C.M Harris that started his long and important association with the Galápagos Islands. The original of this telegram is in the Rollo Beck Collection in the California Academy of Sciences Archives. There is also a photo from Beck's Sierra Nevada collecting trip in the archives of the Museum of Vertebrate Zoology on the University of California, Berkeley campus. The story of buried treasure on Tower Island connected with this trip was apparently known to Captain Lindbridge during this voyage, but the information was not revealed until after the group had left Tower Island. This trip lasted from June 1897 to February 1898, after having started on a tragic note with the deaths of three of the original crew to Yellow Fever, and having to reconstitute the expedition in San Francisco, California.\n\n\nAdrien de Gerlache was an officer in the Belgian Royal Navy who led the Belgian Antarctic Expedition of 1897 to 1899. He acquired \"Le Patria\" in 1896 renaming it \"Belgica\". He left Antwerp on 16 August 1897 passing winter in the Antarctic before returning to Belgium on 5 November 1898.\n\n\nA German deep-sea expedition exploring in Antarctic regions, the \"Valdivia\" being a steamship in the Hamburg-American line of steamers. The subscription was launched by Georg von Neumayer (1826–1909) and only consisted of a single vessel instead of the two planned. The expedition quickly reached the Cape of Good Hope where the study of deep waters began. The ship reached Antarctic pack ice and rediscovered Bouvet Island followed by the Kerguelen Islands. For the first time, evidence of deep water in this region was provided by survey. The \"Valdivia\" then passed to the Indian Ocean, studying the coast of Sumatra before returning to its port of origin 29 April 1899.\n\n\n\n\"This article incorporates text from the French language Wikipedia article\" .\n"}
{"id": "32759787", "url": "https://en.wikipedia.org/wiki?curid=32759787", "title": "Exploration of the Pacific", "text": "Exploration of the Pacific\n\nPolynesians reached nearly all the Pacific islands by about 1200 AD, followed by Asian navigation in Southeast Asia and West Pacific. Around the Middle Ages Muslim traders linked the Middle East and East Africa to the Asian Pacific coasts (to southern China and much of the Malay Archipelago). The direct contact of European fleets with the Pacific began in 1512, with the Portuguese, on its western edges, followed by the Spanish discovery of the Pacific from the American coast.\n\nIn 1521 a Spanish expedition led by the Portuguese navigator Ferdinand Magellan was the first known crossing of the Pacific Ocean, who then named it the \"peaceful sea\". Starting in 1565 with the voyage of Andres de Urdaneta and for the next 250 years, the Spanish controlled the transpacific trade with the Manila galleons that crossed from Mexico to the Philippines and vice versa, until 1815. Other expeditions from Mexico and Peru discovered various archipelagos in the North and South Pacific. In the 17th and 18th centuries, other European powers sent expeditions to the Pacific, namely the Dutch Republic, England, France, and Russia.\n\nHumans reached Australia by at least 40,000 BC which implies some degree of water crossing. People were in the Americas before 10,000 BC. One theory holds that they travelled along the coast by canoe.\n\nAbout 3000 BC speakers of the Austronesian languages, probably on the island of Taiwan, mastered the art of long-distance canoe travel and spread themselves, or their languages, south to the Philippines and Indonesia and east to the islands of Micronesia and Melanesia. The Polynesians branched off and occupied Polynesia to the east. Dates and routes are uncertain, but they seem to have started from the Bismarck Archipelago, went west past Fiji to Samoa and Tonga about 1500 BC. By 100 AD they were in the Marquesas Islands and 300-800 AD in Tahiti (Tahiti is west of the Marquesas.) 300-800 AD is also given for their arrival at Easter Island, their easternmost point and the same date range for Hawaii, which is far to the north and distant from other islands. Far to the southwest, New Zealand was reached about 1250 AD. The Chatham Islands, about 500 miles east of New Zealand were reached about 1500. The fact that some Polynesians possessed the South American Sweet potato implies that they may have reached the Americas or, conversely, that people from the Americas may have reached Polynesia. Thor Heyerdahl's \"Kon-Tiki\" expedition successfully demonstrated that the trip from the Americas to Polynesia using only materials and technology available at the time was at least possible.\n\nOn the Asian side long-distance trade developed all along the coast from Mozambique to Japan. Trade, and therefore knowledge, extended to the Indonesian Islands but apparently not Australia. By at the latest 878 when there was a significant Islamic settlement in Canton much of this trade was controlled by Arabs or Muslims. In 219 BC Xu Fu sailed out into the Pacific searching for the elixir of immortality. From 1404-33 Zheng He led expeditions into the Indian Ocean.\n\nAn interesting issue is Japanese fishing boats. If one was blown out to sea and lacked proper equipment it could be carried by the current all the way to North America. Japanese boats reached Acapulco in 1617, the Aleutians in 1782, Alaska in 1805, the mouth of the Columbia River in 1820, and Cape Flattery in 1833. Such trips may have taken place before Europeans were present in those areas to make detailed records of them.\n\nThe first contact of European navigators with the western edge of the Pacific Ocean was made by the Portuguese expeditions of António de Abreu and Francisco Serrão, via the Lesser Sunda Islands, to the Maluku Islands, in 1512, and with Jorge Álvares's expedition to southern China in 1513, both ordered by Afonso de Albuquerque from Malacca.\n\nSpanish explorer Balboa was the first European to sight the Pacific from America in 1513 after his expedition crossed the Isthmus of Panama and reached a new ocean. He named it \"Mar del Sur\" (literally, \"Sea of the South\" or \"South Sea\") because the ocean was to the south of the coast of the isthmus where he first observed the Pacific. Later, Portuguese explorer Ferdinand Magellan sailed the Pacific East to West on a Castilian (\"Spanish\") expedition of world circumnavigation starting in 1519. Magellan called the ocean \"Pacífico\" (or \"Pacific\" meaning, \"peaceful\") because, after sailing through the stormy seas off Cape Horn, the expedition found calm waters. The ocean was often called the \"Sea of Magellan\" in his honor until the eighteenth century.\n\nFrom 1565 to 1815, a Spanish transpacific route known as the Manila galleons regularly crossed from Mexico to the Philippines and back. On the Asian side the Portuguese and later the Dutch built a regular trade from the East Indies to Japan. On the American side Spanish power stretched thousands of miles from Mexico to Chile. The vast central Pacific was visited only by the Manila galleons and an occasional explorer. The south Pacific was first crossed by Spanish expeditions in the 16th century who discovered many islands including Tuvalu, the Marquesas, the Cook Islands, the Solomon Islands, and the Admiralty Islands, and later the Pitcairn and Vanuatu archipelagos.\n\nThe Pacific recognized: Europeans knew that there was a vast ocean to the west, and the Chinese knew that there was one to the east. Learned Europeans thought that the world was round and that the two oceans were one. In 1492 Columbus sailed west to what he thought was Asia. When Pedro Álvares Cabral, en route to Asia via the Atlantic and the Indian oceans, reached Brazil, in 1500, the true extent of the Americas began to become known. The Martin Waldseemüller map of 1507 was the first to show the Americas separating two distinct oceans. This guess was confirmed in 1513 when Balboa crossed Panama and found salt water. The Magellan expedition of 1519-22 proved that there was one continuous ocean from the Americas to Asia. The Diogo Ribeiro map of 1529 was the first to show the Pacific at about its proper size.\n\nThe coast of Asia: The first European to see the Pacific Ocean was probably Marco Polo about 1292. The Portuguese reached India in 1498, conquered Malacca in 1511 and in 1512 António de Abreu and Francisco Serrão reached the Spice Islands. In May 1513 Jorge Álvares reached southern China and in the same year Balboa crossed Panama. In 1525 Diogo da Rocha and Gomes de Sequeira reached the Caroline Islands, and Jorge de Menezes in 1526-27 landed on the \"Islands of Don Jorge de Menezes\", in the northwest coast of New Guinea (now part of Indonesia), and named the region \"Ilhas dos Papuas\" and is thus credited with the European \"discovery\" of Papua. In 1542 Fernão Mendes Pinto reached Japan. From about 1543 until 1614, the Portuguese monopolize the trade between China and Japan, through the nanban trade. In 1589, João da Gama reached Hokkaido and possibly sighted the Kuril islands, crossing the Pacific further north of the routes usually taken until then. The land that he eventually discovered northeast of Japan, has since become a matter of legend and controversy. \n\nOne hundred years after the Spanish and Portuguese the Dutch Republic began its remarkable expansion. The Dutch reached the East Indies in 1596, the Spice Islands in 1602 and in 1619 founded Batavia. In 1600 a Dutch fleet reached Japan from the Strait of Magellan. The Dutch had little success in China but established themselves at Hirado, Nagasaki in 1609 and monopolized the Japan trade from 1639. In 1639 Matthijs Quast and Abel Tasman searched the empty ocean east of Japan looking for two islands called 'Rica de Oro' and 'Rica de Plata'. In 1643 Maarten Gerritsz Vries reached and charted Sakhalin and the Kuril Islands. In 1653 Hendrick Hamel was shipwrecked in Korea. At about this time the Russians reached the Pacific overland via Siberia (see below). It is significant that the Russian and Dutch trades were never linked since Siberian furs might easily have been exported to China at great profit.\nMagellan and the Manila Galleons: In 1519 Ferdinand Magellan sailed down the east coast of South America, found and sailed through the strait that bears his name and on 28 November 1520 entered the Pacific. He then sailed north and caught the trade winds which carried him across the Pacific to the Philippines where he was killed. One surviving ship returned west across the Indian Ocean and the other went north in the hope of finding the westerlies and reaching Mexico. Unable to find the right winds, it was forced to return to the East Indies. In 1565 (44 years later) Andrés de Urdaneta found a wind system that would reliably blow a ship eastward back to the Americas. From then until 1815 the annual Manila Galleons crossed the Pacific from Mexico to the Philippines and back, exchanging Mexican silver for spices and porcelain. Until the time of Captain Cook these were the only large ships to regularly cross the Pacific. The route was purely commercial and there was no exploration of the areas to the north and south. In 1668 the Spanish founded a colony on Guam as a resting place for west-bound galleons. For a long time this was the only non-coastal European settlement in the Pacific.\n\nSouth America: In 1513, six years before Magellan, Spanish explorer Vasco Núñez de Balboa crossed the Isthmus of Panama and saw the Pacific Ocean. In 1517-18 two ships were built on the Pacific coast. In 1522 Pascual de Andagoya sailed the coast as far as Ecuador. In 1532 Francisco Pizarro conquered Peru. A regular trade developed that carried Peruvian silver up the coast to Panama where it was carried overland to the Caribbean and part to Spain. Spanish settlement extended as far south as central Chile. In 1557-8 Juan Fernández Ladrillero discovered the Juan Fernandez islands and explored the Chilean coast down to the Strait of Magellan.\n\nThe South Pacific: Several Spanish expeditions were sent from South America across the Pacific Ocean in the 16th and early 17th centuries. They all used the southern trade winds. In 1567/68 Álvaro de Mendaña de Neira sailed from Peru to the Solomon Islands. In 1595 he tried again and reached the Santa Cruz Islands (eastern Solomons toward Fiji). He died there and the survivors reached the Philippines. In 1606 Pedro Fernandes de Queirós reached Vanuatu south of the Solomons. He continued exploring and eventually sailed back to Mexico. One of his separated ships under Luis Vaz de Torres sailed west and discovered the strait that bears his name sighting the northern tip of Australia. Other Spanish expeditions discovered Tuvalu, the Marquesas, the Cook Islands, the Admiralty Islands and the Pitcairn. In 1722 the Dutchman Jacob Roggeveen sailed from Cape Horn to Batavia and discovered Easter Island and Samoa.\n\nCape Horn: Six years after Magellan, in 1526, one of the ships of the Loaísa Expedition sailed through the Strait of Magellan and followed the coast north to Mexico. In 1578 Francis Drake passed through the Strait, sailed north raiding Spanish ships and put in somewhere on the coast of California. In 1580 Pedro Sarmiento de Gamboa, who was hunting for Drake, was the first to sail from the Strait to Europe. In 1587 Thomas Cavendish followed Drake, captured a Manila galleon and returned via the Indian Ocean. In 1599 the first Dutch ships passed through the Strait of Magellan (Will Adams, the first Englishman to reach Japan, was on board). Olivier van Noort followed and became the first Dutch circumnavigator.\n\nIn 1525 Francisco de Hoces, while trying to enter the Strait as part of the Loaisa Expedition, was blown south by a storm and saw what he thought was land's end. In 1578 Drake was blown south on the west side and saw what he thought was open water. In 1616 Willem Schouten sought a more southerly passage and rounded Cape Horn. In 1619 the Garcia de Nodal expedition followed the Dutch and proved that Tierra del Fuego was an island by circumnavigating it. Since the Strait of Magellan is narrow and hard to navigate Cape Horn became the standard route until the opening of the Panama Canal. It is a measure of the difficulty of these seas that it was not until 1820 that anyone went as far south as Antarctica.\n\nNorth America: When the Spanish conquered Mexico in 1521 they gained a stretch of Pacific coast. In 1533, Fortún Ximénez reached Baja California and in 1539 Francisco de Ulloa showed that it was a peninsula, but the myth of an Island of California continued for many years. In 1542 Juan Rodriguez Cabrillo reached a point north of San Francisco. In 1578 Drake landed somewhere on the coast. In 1587 Pedro de Unamuno, coming from the Philippines, stopped at Morro Bay, California. In 1592, Juan de Fuca may have reached Puget Sound.\n\nIn 1595, Sebastian Rodriguez Cermeño (Sebastião Rodrigues Soromenho), commander of the Manila galleon \"San Agustín\", attempted an exploration of the California coast. He reached the continent between Point St. George and Trinidad Head in California, but the galleon was later wrecked in a storm off Drake's Bay and the survivors had to sail the rest of the way back to Mexico in a small launch. The smaller vessel, however, allowed Cermeño to sail closer to the coast and to make useful observations of coastal features. In 1602, Sebastián Vizcaíno re-explored the California coast, one of his ships reaching Oregon. His was the last northward exploration for the next 150 years.\n\nThe Portolà expedition of 1769 began the land exploration of Alta California, following the coast as far north as San Francisco Bay and using the reports of Cermeño and Vizcaíno for guidance.\n\nAfter conquering Mexico the Spanish occupied the southern two thirds of Mexico, all of Central America and the South American coast down to Chile. North of this the land was too dry to support a dense population that could be ruled and taxed. The only exception was the Pueblo peoples far to the north in New Mexico. People like Francisco Vásquez de Coronado penetrated far into the interior and found nothing that the Spanish valued. The Chichimeca country of northern Mexico was slowly absorbed and Baja California began to be settled in 1687. The returning Manila galleons followed the westerlies to the coast of California, but immediately turned south, making only a few attempts to explore the coast. For more see History of the West Coast of North America and Early knowledge of the Pacific Northwest.\n\nAustralia and the southwest: Australia is remarkable for the number of explorers who missed it. There seems to be no record of Indonesian sailors reaching Australia. Some think that the Portuguese reached Australia before 1600 but these theories are difficult to prove. The 1567–1606 Spanish voyages from South America stopped at islands to the east before reaching Australia. The first European to definitely see Australia was Willem Janszoon who in February 1606 reached the Cape York Peninsula and thought it was part of New Guinea. Also in 1606 (June to October) Luis Váez de Torres of the Quiros expedition from South America followed the south coast of New Guinea and passed through the Torres Strait without recognizing Australia. His voyage, and therefore the separation between Australia and New Guinea, was not generally known until 1765. From about 1611 the standard Dutch route to the East Indies was to follow the roaring forties as far east as possible and then turn sharply north to Batavia. Since it was difficult to know longitude some ships would reach the west coast or be wrecked on it. 1616 Dirk Hartog bumped into the west coast and did some exploring. Frederick de Houtman did the same in 1619. In 1623 Jan Carstenszoon followed the south coast of New Guinea, missed Torres Strait and went along the north coast of Australia. In 1643 Abel Tasman left Mauritius, missed Australia, found Tasmania, continued east and found New Zealand, missed the strait between the north and south islands, turned northwest, missed Australia again and sailed along the north coast of New Guinea. In 1644 he followed the south coast of New Guinea, missed the Torres Strait, turned south and mapped the north coast of Australia. In 1688 the English buccaneer William Dampier beached a ship on the northwest coast. In 1696 Willem de Vlamingh explored the southwest coast. In 1699 Dampier was sent to find the east coast of Australia. He sailed along the west coast, went north to Timor, followed the north coast of New Guinea to the Bismarck Archipelago and abandoned his search because his ship had become rotten. Until Captain Cook the east coast was completely unknown and New Zealand had only been seen once.\n\nPacific Islands: See also History of the Pacific Islands\n\nMythical Lands: Europeans had long believed in a Strait of Anian somewhere near Bering Strait. A large and distorted Hokkaido was called 'Ezo', 'Jesso' and many other spellings. One of the Kuril Islands named \"Companies Landt\" by Vries grew into a large mass attached to North America. Joao-da-Gama-Land was thought to be east of Japan. There was an overgrown Puget Sound called \"Grande Mer de l'Ouest\" possibly connected to Hudson Bay. In the far south was a Terra Australis. The map published in Diderot's \"Encyclopédie\" in 1755 is filled with nonsense. In 1875 no less than 123 mythical islands were removed from the Royal Navy chart of the North Pacific.\n\nAlaska and the Russians: The modern period begins with Russian expeditions. They crossed Siberia and reached the Pacific in 1639 (Ivan Moskvitin). In 1644 Vassili Poyarkov found the Amur River. In 1648 Semyon Dezhnyov (probably) entered the Pacific from the Arctic Ocean. In 1652 Mikhail Stadukhin followed the coast of the Sea of Okhotsk. In 1697 Vladimir Atlasov entered the Kamchatka Peninsula overland from the north. In 1716 the first seagoing boats were built to reach Kamchatka from the mainland. In 1728 Vitus Bering sailed from Kamchatka through the strait that bears his name without seeing America. In 1732 Mikhail Gvozdev and Ivan Fedorov (navigator) saw the tip of Alaska from the Bering Strait. In 1741 Vitus Bering and Alexei Chirikov sailed just south of the Aleutian Islands and reached the Alaska panhandle. Peter Kuzmich Krenitzin mapped the Aleutians before 1769. The myth of a land mass north of the Aleutians took a long time to dispel. Russians fur hunters island-hopped along the Aleutians and then along the south coast of Alaska looking mainly for sea otter (Attu at the west end of the Aleutians in 1745, Unalaska Island at the east end in 1759, Kodiak Island 1784, Kenai Peninsula 1785, Yakutat, 1795, Sitka 1799, Fort Ross 1812). North of the Aleutians posts appeared on the west coast after 1819. Spaniards from Mexico met the Russians in 1788. (see below). Russian America was sold to the United States in 1867.\nCaptain Cook: On his first voyage (1768–71) James Cook went to Tahiti from Cape Horn, circumnavigated New Zealand, followed the east coast of Australia for the first time and returned via the Torres Strait and the Cape of Good Hope. On his second voyage (1772–75) he sailed from west to east keeping as far south as possible and showed that there was probably no Terra Australis. On his third voyage (1776–80) he found the Hawaiian Islands and followed the North American coast from Oregon to the Bering Strait, mapping this coast for the first time and showing that there was probably no Northwest passage. Cook was killed in Hawaii in 1779. The expedition made a second attempt at the Bering Strait, stopped at Kamchatka and China and reached England in 1780. Cook set a high standard of scientific exploration, showed that there was no large land mass in the southern ocean, mapped the two largest island groups in the Pacific and by following the east coast of Australia and the west coast of North America closed the last gaps in European knowledge of the Pacific coasts. After Cook everything was detail.\n\nCook's rivals and successors: Several governments sponsored Pacific expeditions, often in rivalry or emulation of Captain Cook. At the time of Cook's first voyage, in 1766-69 Louis Antoine de Bougainville crossed the Pacific and publicized Tahiti and in 1767 Samuel Wallis and Philip Carteret separately crossed the Pacific. In 1785-88 Jean-François de Galaup, comte de Lapérouse followed the American coast from Chile to Alaska, crossed to China, explored northern Japan and Kamchatka, went south to Australia and lost his life in the Santa Cruz Islands. The Malaspina Expedition (1789–1794) visited the American coast, Manila, New Zealand and Australia. In 1792-93 George Vancouver more thoroughly mapped the west coast of Canada. In 1803/6 Adam Johann von Krusenstern led the first Russian circumnavigation and investigated both sides of the North Pacific. In 1820 Fabian Gottlieb von Bellingshausen saw Antarctica. A number of other voyages are listed in \"European and American voyages of scientific exploration.\"\n\nSpain on the west coast of North America: For Europeans in the Age of Exploration western North America was one of the most distant places on earth (9 to 12 months of sailing). Spain had long claimed the entire west coast of the Americas. The area north of Mexico however was given little attention in the early years. This changed when the Russians appeared in Alaska. The Spanish moved north to California and built a series of missions along the Pacific coast including: San Diego in 1767, Monterey, California in 1770 and San Francisco in 1776. San Francisco Bay was discovered in 1769 by Gaspar de Portolà from the landward side because its mouth is not obvious from the sea. The Spanish settlement of San Francisco remained the northern limit of land occupation. By sea, from 1774 to 1793 the Spanish expeditions to the Pacific Northwest tried to assert Spanish claims against the Russians and British. In 1774 Juan José Pérez Hernández reached what is now the south end of the Alaska panhandle. In 1778 Captain Cook sailed the west coast and spent a month at Nootka Sound on Vancouver Island. An expedition led by Juan Francisco de la Bodega y Quadra sailed north to Nootka and reached Prince William Sound. In 1788 Esteban José Martínez went north and met the Russians for the first time (Unalaska and Kodiak Island) and heard that the Russians were planning to occupy Nootka Sound. In 1789 Martinez went north to build a fort at Nootka and found British and American merchant ships already there. He seized a British ship which led to the Nootka Crisis and Spanish recognition of non-Spanish trade on the northwest coast. In 1791 the Malaspina expedition mapped the Alaska coast. In 1792 Dionisio Alcalá Galiano circumnavigated Vancouver Island. In 1792-93 George Vancouver also mapped the complex coast of British Columbia. Vancouver Island was originally named Quadra's and Vancouver's Island in commemoration of the friendly negotiations held by the Spanish commander of the Nootka Sound settlement, Juan Francisco de la Bodega y Quadra and British naval captain George Vancouver in Nootka Sound in 1792. In 1793 Alexander Mackenzie reached the Pacific overland from Canada. By this time Spain was becoming involved in the French wars and increasingly unable to assert its claims on the Pacific coast. In 1804 the Lewis and Clark expedition reached the Pacific overland from the Mississippi River. By the Adams–Onís Treaty of 1819 Spain gave up its claims north of California. Canadian fur traders, and later a smaller number of Americans, crossed the mountains and built posts on the coast. In 1846 the Oregon Treaty divided the Oregon country between Britain and the United States. The United States conquered California in 1848 and purchased Alaska in 1867.\n\nNortheast: The Russians moved south and the Japanese moved north and explored the Kuril Islands and Sakhalin. About 1805 Adam Johann von Krusenstern was apparently the first Russian to reach eastern Siberia by sea from European Russia. In 1808 Mamiya Rinzo explored the coast of Sakhalin. During the Crimean War a British fleet failed to capture Petropavlovsk-Kamchatsky. In 1860 Russia annexed the southeast corner of Siberia from China.\n\nThe Pacific opened to trade and imperialism: After Captain Cook large numbers of European merchant vessels began to enter the Pacific. The reasons for this are not completely clear. On Cook's third voyage furs bought at Nootka were sold in China at a 1,800 percent profit - enough to pay for a trading voyage. The first to do this was James Hanna from Macao in 1785. Robert Gray in 1787 was the first American. This Maritime fur trade reached its peak about 1810, drew many ships into the Pacific and drew Canadians and Americans to the coast. The first Pacific whaling ship left London in 1788 and by the nineteenth century there were hundreds of whaleships in the Pacific each year. Clipper ships cut the sailing time from Europe to the Pacific. England founded a colony in Australia in 1788 and New Zealand in 1840. After about 1800 England began to replace the Dutch Republic along the Asian coast. Hong Kong became a colony in 1839 during the First Opium War, which was also the first time that a large European military and naval force appeared in the Pacific. European ships and sailors disrupted life on the Pacific islands. Most of the Pacific islands were soon claimed by one European power or another.\n\n"}
{"id": "2826958", "url": "https://en.wikipedia.org/wiki?curid=2826958", "title": "GEOnet Names Server", "text": "GEOnet Names Server\n\nThe GEOnet Names Server (GNS) provides access to the National Geospatial-Intelligence Agency's (NGA) and the U.S. Board on Geographic Names's (BGN) database of geographic feature names and locations for locations outside the United States. The database is the official repository of foreign place-name decisions approved by the US BGN. Approximately 20,000 of the database's features are updated monthly. The database never removes an entry, \"except in cases of obvious duplication\".\n\n\n"}
{"id": "11450984", "url": "https://en.wikipedia.org/wiki?curid=11450984", "title": "Garden festival", "text": "Garden festival\n\nA garden / flora festival or exposition is a festival and exposition held to celebrate the arts of gardening, garden design, landscaping and landscape architecture. There are local garden festivals, regional garden festivals, National Garden Festivals and International Garden Festivals. The idea probably originated with Germany's Bundesgartenschau. The UK held five garden festivals in the period 1984-92 but blundered through not planning an after-use for the festival grounds during their design and planning phase.\n\nTo qualify as an International Exhibition, an Expo must be recognised by the Bureau International des Expositions (BIE), which was established by a diplomatic international Convention, signed in Paris, in 1928. Horticultural Expos can also be recognised by the International Association of Horticultural Producers (IAHP / AIPH). To qualify as a National Exhibition, a garden festival must be recognised by a national government.\n\nBecause garden design is becoming more popular and featuring on TV, there is an ever-growing number of garden festivals: permanent and temporary, official and non-official. One of the best known is International Garden Festival held on a permanent site at Chaumont in France. Despite the name, Chaumont does not come within the BIE definition of an 'international' festival. Other shows feature garden design but describe themselves as 'flower shows'. The best-known example in this category, the Chelsea Flower Show, emphasises garden design. It spun off a Chelsea Fringe events in 2012 which featured a variety of unusual gardens and gardening across London.\n\n\n\n"}
{"id": "26732377", "url": "https://en.wikipedia.org/wiki?curid=26732377", "title": "Geodat", "text": "Geodat\n\nGeodat was a commercial project, begun in 1980 and completed by 1991, that provided digital geographic mapping data for commercial users at scales equal to or greater than 1:1,000,000. The term \"Geodat\" was derived from \"GEOgraphic DATa\". Geodat data was primarily \"medium scale\", a nominal 1:100,000, but ranged from 1:50,000 to 1:250,000. The cartographic data was vector-based digitisation of coastline, hydrography, internal and international political boundaries, primary transportation routes and city locations. The data was intended to be used on its own to produce quick, cheap, consistent maps, initially for oil exploration firms. Harry Wassall, the founder of Petroconsultants SA, a Geneva-based energy information services firm, began the project in 1979 by hiring a researcher from the Harvard Laboratory for Computer Graphics and Spatial Analysis, Michael Mainelli, to explore how to automate Petroconsultants' extensive paper map series. Mainelli became Project Director in 1981. Petroconsultants concluded that a cooperative project among the oil firms acknowledged the high degree of overlap in their computer mapping interests.\n\nPetroconsultants SA assessed client interest at a meeting in Geneva on 20–21 August 1981 with attendees from Amoco, BP, Cities Service, Deminex, Elf Acquitaine, Exxon, Gulf and Shell. The need for computerised mapping data was high and the response positive enough to form an advisory committee with paid sponsorship. The sponsors commissioned Petroconsultants to produce four sample digitised maps of the Ivory Coast. The Ivorian pilot project resulted in four 1:200,000 maps with 800 features and 40,000 data points. The pilot established Common Geographic Format (CGF) records, for a time the industry standard for computer cartographic information exchange. These digitised map files, and their attendant file structures, feature codes, segment records, map records, annotation records and set records were reviewed at a meeting in Dublin on 10–11 November 1981 with participation from Amoco, BP, Chevron, Cities Service, Elf Acquitaine, Exxon, Gulf, Phillips Petroleum and Shell. Geodat was formally launched in Houston on 9–10 February 1982 with attendees from Amoco, BP, Chevron, Elf Acquitaine, Exxon, Getty, Gulf, Texas Easter and Union Texas. Four primary sponsors were Shell, BP, Elf Acquitaine and Chevron, while ten other firms were partial sponsors. Full sponsors received a guarantee of six million digitised points (approximately 360 maps) digitised to a specified quality level.\n\nFirst data delivery was in June 1983, consisting of 57 maps and 1.24 million points. By the end of 1985, Geodat had delivered twenty million data points and 750 map sheets in the 1:50,000 to 1:250,000 scale range. Alongside mainstream production for the project sponsors, the Geodat project produced a complete digital map of the world at a scale of 1:1,000,000, MundoCart, in 1985. MundoCart was based on digitisation of paper prints of the United States Defense Mapping Agency's (DMA) Operational Navigation Chart (ONC) 1:1,000,000 scale paper map series, produced by the USA, Australia, Canada, and the United Kingdom. ONC charts were designed to meet the needs of pilots and air crews in medium-and low-altitude navigation and to support military operational planning, intelligence briefings, and other needs. Some 270 1:1million maps, plus six 1:2million maps for Antarctica, resulted in 30 million data points. MundoCart provided numerous commercial and academic Geographic information system (GIS) users with their first complete vector map of the world. The data was sold, along with a complete set of FORTRAN mapping software, by Petroconsultants (CES), a UK subsidiary of Petroconsultants SA, subsequently sold to IHS in 1996, that sponsored the project.\n\nPrior to Geodat, the only complete digital map of the world was World Data Bank 2 (WDB-II), a dataset digitized between 1972 and 1977 by the US Department of State's (DoS) Central Intelligence Agency (CIA). WDBII was of variable scale, nominally 1:3,000,000 but digitised from sources ranging from 1:1,000,000 to 1:4,000,000, and of variable quality. WDB-II was released at nominal cost from 1977, but users desired higher resolution and more consistent quality. Until the release of the US Department of Defense's Digital Chart of the World in 1992, and subsequent free issue from 2006, MundoCart was the primary global GIS database for commercial users. \n\nGeodat was unusual in the 1980s in that the software was often given away free, while data downloads or tapes were charged. Geodat set out a quality management process for digitisation covering acquisition, cataloguing, map stability, transformation algorithms, merging and node coalescing. Geodat also set out a quality standard for comparing digitised maps with source maps, based on using generalisation and interpolation against a maximum orthogonal offset distance. Originally delivered as five large tapes, MundoCart was burned on CD-ROM in 1987. MundoCart was one of the first CD-ROM applications not brought out by the creators, Philips and Sony. In the early releases of MundoCart, a CD-ROM reader accompanied each purchase.\nProduction was based from late 1982 to 1991 in Burleigh House, Newmarket Road in Cambridge, England. Coincidentally, Burleigh House had been the former Star Brewery headquarters and the cellars provided good storage facilities for paper maps. At its height, the project employed 30 people, principally programmers and digitisers. Processing was based on hand digitisation using DEC PDP-11 mincomputers alongside digitisation tables. Further processing was done on DEC VAX computers. The computer programs were almost entirely written in FORTRAN, though some PL/1 and assembler programming was used. At the time, scanning systems were not suitable for large-scale production, but the project did help design, build and use a laser line-following digitiser, combining a laser photovoltaic feedback system with stepper motors, mounted on a cowboy boot sewing machine chassis from the USA. The effect of the laser line-following digitiser was that the operator felt as if he or she was directing the laser along a \"groove\" that followed the line being acquired.\n\nThe Geodat project drew heavily on the experience of the Harvard Laboratory for Computer Graphics and Spatial Analysis, founded in 1963 and disbanded in 1991. Unable to purchase the Laboratory's software while Harvard University was exploring licensing options, Geodat developed its own software. The Laboratory and Geodat both employed \"flat\" computer files and streamed processing, instead of hierarchical and direct access processing. Wherever possible, the software operated in FORTRAN processing sequential files to enhance portability among operating systems. Geodat also emulated other Laboratory ideas, most notably the idea of \"cycling\", using software to resolve disconnected vectors.\n"}
{"id": "11750971", "url": "https://en.wikipedia.org/wiki?curid=11750971", "title": "Geoportal", "text": "Geoportal\n\nA geoportal is a type of web portal used to find and access geographic information (geospatial information) and associated geographic services (display, editing, analysis, etc.) via the Internet. Geoportals are important for effective use of geographic information systems (GIS) and a key element of Spatial Data Infrastructure (SDI).\n\nGeographic information providers, including government agencies and commercial sources, use geoportals to publish descriptions (geospatial metadata) of their geographic information. Geographic information consumers, professional or casual, use geoportals to search and access the information they need. Thus geoportals serve an increasingly important role in the sharing of geographic information and can avoid duplicated efforts, inconsistencies, delays, confusion, and wasted resources.\n\nThe United States National Spatial Data Infrastructure (NSDI), started in 1994 (see OMB Circular A-16), is considered the earliest geoportal concept. The U.S. Federal Geospatial Data Committee (FGDC) coordinated development of the NSDI Clearinghouse Network, the first large geoportal. It has many distributed catalogs that can be searched via a client interface.\n\nFirst released in 2003, the Geospatial One-Stop (GOS) geoportal was developed as part of a U.S. e-government initiative. Unlike the NSDI Clearinghouse Network, GOS was built around a centralized metadata catalog database, with an architecture that links users to data providers through a Web-based geoportal. The user of GOS may employ a simple Web browser (thin client) or may interface directly with a GIS (thick client).\n\nMore recently, there has been a proliferation of geoportals for sharing of geographic information based on region or theme. Examples include the INSPIRE, or Infrastructure for Spatial Information in the European Community geoportal, the NatCarb geoportal, which provides geographic information concerning carbon sequestration in the United States, and UNSDI, the United Nations Spatial Data Infrastructure.\n\nModern web-based geoportals include direct access to raw data in multiple formats, complete metadata, online visualization tools so users can create maps with data in the portal, automated provenance linkages across users, datasets and created maps, commenting mechanisms to discuss data quality and interpretation, and sharing or exporting created maps in various formats. Open portals allow user contribution of datasets as well.\n\nIn September 2011, GOS was retired and the content it included by then became part of the broader open data site (Geo.)Data.gov. At the same time, the United States federal government launched the Geospatial Platform, which represents a shift from focusing on cataloging references to resources, to providing shared web services for national significant datasets, API for developers, and end-user applications (built on those web services and API).\n\n\nFu, P., and J. Sun. 2010. \"Web GIS: Principles and Applications\". ESRI Press. Redlands, CA. .\n\nGoodchild, M.F., P. Fu, and P.M. Rich. 2007. Geographic information sharing: the case of the Geospatial One-Stop portal. \"Annals of the Association of American Geographers\" 97(2):250-266.\n\nMaguire, D.J., and P.A. Longley. 2005. The emergence of geoportals and their role in spatial data infrastructures. \"Computers, Environment and Urban Systems\" 29: 3-14.\n\nTang, W. and Selwood, J. 2005. \"Spatial Portals: Gateways to Spatial Information\". ESRI Press, Redlands, CA.\n"}
{"id": "22764646", "url": "https://en.wikipedia.org/wiki?curid=22764646", "title": "Hellenic Military Geographical Service", "text": "Hellenic Military Geographical Service\n\nThe Hellenic Military Geographical Service or HMGS ( or Γ.Υ.Σ.), is the Greek military mapping agency.\n\nEstablished in 1889 as the “Geodetic Mission” with the purpose of compiling the “Topographic and Cadastral Map of the Country”, the service acquired its current name in 1926. Today it supports both the Greek Army and the general public with cartographic products.\n\nFrom 1966 until the 1980s, the service housed the Armed Forces Information Service, Greece's second state-run broadcaster alongside the National Radio Foundation.\n\nHMGS supplies City Plans of scales 1:5.000, 1:10.000 and 1:25.000 in Greek and Latin series\n\n\nHMGS produces and supplies topographic maps of various scales that cover the whole country and occasionally parts of neighbouring countries. It should be emphasised that not all maps noted below are available to public, since part of them is confidential and available only upon approval of HMGS. Although not a state monopoly and not very famous among hikers, certainly HMGS maps have a much wider coverage in small scales than commercial leisure maps. The maps used the Hellenic Geodetic Reference System 1987 (HGRS87 or ΕΓΣΑ'87) which specifies a Transverse Mercatorial Projection mapping Greece in one zone. \n\n\nThe products and services of HMGS are not available in the market. They can either be purchased using the e-shop of the service or collected from the offices of the service upon submitting the relevant request form.\n\n\n"}
{"id": "501118", "url": "https://en.wikipedia.org/wiki?curid=501118", "title": "Hermit kingdom", "text": "Hermit kingdom\n\nThe term hermit kingdom can be used to refer to any country, organization or society which willfully walls itself off, either metaphorically or physically, from the rest of the world - The country of North Korea is a prime example of a hermit kingdom.\n\nKorea in the age of Joseon dynasty was the subject of the first use of the term, in William Elliot Griffis' 1882 book \"Corea: The Hermit Nation\", and Korea was frequently described as a hermit kingdom until 1905 when it became a protectorate of Japan. The term is still commonplace throughout Korea and it is often used by Koreans themselves to describe pre-modern Korea. Today, the term is often applied to North Korea in news and social media, and in 2009 it was used by United States former Secretary of State Hillary Clinton. \n"}
{"id": "37280343", "url": "https://en.wikipedia.org/wiki?curid=37280343", "title": "Hügelkultur", "text": "Hügelkultur\n\nHügelkultur is a horticultural technique where a mound constructed from decaying wood debris and other compostable biomass plant materials is later (or immediately) planted as a raised bed. Adopted by permaculture advocates, it is suggested the technique helps to improve soil fertility, water retention, and soil warming, thus benefiting plants grown on or near such mounds.\n\nHügelkultur is a German word meaning mound culture or hill culture. It is said to have been practiced in German and Eastern European societies for hundreds of years. \n\nThe term is first published in a 1962 German gardening booklet by Herrman Andrä. Inspired by observation of the diversity and success of plants growing in a pile of woody debris, \"mound culture\" is suggested (as opposed to \"flatland culture\"). This was also posited as an easy way to utilise woody debris without burning, which was illegal. Andrä appears to have been influenced by Rudolf Steiner's biodynamic agriculture. Steiner developed his biodynamic philosophy through meditation and clairvoyance, and rejected scientific inquiry on the grounds that his methods were “true and correct unto themselves.” Andrä quotes a 1924 lecture on biodynamics by Steiner, which describes mixing of soil with composting or decaying material in earthen hillocks. Joined by author Hans Beba, another German gardener, \"Hill Culture - the horticultural method of the future\" was revised and republished several times in the 1970s and 1980s.\n\nThe technique was later adopted and developed by Sepp Holzer, an Austrian permaculture advocate. More recent permaculture advocates such as Paul Wheaton and Geoff Lawton strongly promote Hügelkultur beds as a perfect permaculture design.\n\nIn its basic form, mounds are constructed by piling logs, branches, plant waste, compost and additional soil directly on the ground. The pile has the form of a pyramid. The sides of the two slopes both have a grade of between 65 and 80 degrees. The beds are usually about by in area and about high. However, this height reduces as decomposition progresses.\n\nWhen positioned on sloped terrain, the beds need to be put at an angle to the hillside (rather than having them parallel to it). This makes sure the beds do not receive unequal amounts of water. In most cases, it is useful to have the beds positioned against the prevailing wind direction.\n\nThe raised bed can form light-duty swales, circles and mazes. Mounds may also be made from alternating layers of wood, sod, compost, straw, and soil. Although their construction is straightforward, planning is necessary to prevent steep slopes that would result in erosion. \n\nIn his book \"Desert or Paradise: Restoring Endangered Landscapes Using Water Management, Including Lake and Pond Construction\", Holzer describes a method of constructing Hügelkultur which incorporates rubbish such as cardboard, clothes and kitchen waste. He recommends building mounds that are wide and any length. Mounds are built in a trench in sandy soil, and without a trench if the ground is wet.\n\nThe mound is left to rest for several months before planting, although some advise immediate planting.\n\nAnything can be grown on the raised beds, but if the bed will decompose/release its nutrients quickly (so long as it is not made of bulky materials like tree trunks), more demanding crops such as pumpkins, courgettes, cucumbers, cabbages, tomatoes, sweet corn, celery, or potatoes are grown in the first year, after which the bed is used for less demanding crops like beans, peas, and strawberries.\nThe original German publications described the mounds as having a lifespan of 5-6 years, after which they had to be rebuilt from scratch.\n\nAs of 2017 there are no peer-reviewed scientific studies available regarding the efficacy of the technique. A few university student projects investigate Hügelkultur but have not been published in scientific journals. \n\nOne small scale and short term student project investigated the Hügelkultur method as a potential use for yard trimmings waste, and also if lima beans, kale and okra planted on a Hügelkultur mound showed any signs of nutrient deficiency compared to a non-raised control bed. It was found that over 11 tons of yard trimmings were used in the mound, and no evidence of macronutrient deficiency could be detected in the crops in the short term. Indeed, despite concerns that incorporation of large quantities of high carbon woody matter would lead to nitrogen immobilization and hence nitrogen deficiency in the crop, a higher level of nitrogen was found in the raised bed. However the micronutrient iron was lower relative to the control bed. The author speculated that no nitrogen deficiency occurred since the roots of the plants did not penetrate past the superficial layers of the mound into the deeper wood-containing region.\n\nA student thesis investigated the water holding capacity of Hügelkultur beds and whether the technique could be useful to prevent karst rocky desertification in China. Over 3 months of measurements, water concentration in hügel mounds remained high. Samples from hugel sites contained almost twice as much water as those from flat control plots. It was suggested that 1 Hectare of hügels has 3-10 times more water than a flat plot affected by karst rocky desertification.\n\nMany publications and websites advocate the technique based on personal experience of the authors. Some have criticised the technique as lacking genuine scientific principles, and running counter to the ecological principles of soil building with litterfall. \n\nHügelkultur is said to replicate the natural process of decomposition that occurs on forest floors, however in natural ecosystems wood would be present at the soil surface. Trees that fall in a forest often become nurse logs decaying and providing ecological facilitation to seedlings. As the wood decays, its porosity increases, allowing it to store water like a sponge. The water is slowly released back into the environment, benefiting nearby plants.\n\nHügelkultur beds are said to be ideal for areas where the underlying soil is of poor quality or compacted. They tend to be easier to maintain due to their relative height above the ground. \n\nDecomposition speed of organic material depends on the carbon to nitrogen ratio of the material, among other factors. Wood breaks down relatively slowly because it has one of the highest carbon to nitrogen ratios of all organic matter that is used in composting. If the wood is not processed into smaller pieces with larger surface area to speed up chemical reactions, breakdown is even slower. The decomposition process may in the short term take more nitrogen from the soil through microbial activity (nitrogen immobilization), if not enough nitrogen is available. Thus in the short term the fertility of the soil may be decreased before eventually, perhaps after 1-2 years, the nitrogen level is increased past the original level. Traditionally therefore, it is said to be advantageous to balance \"browns\" (e.g. woodchippings) with \"greens\" (e.g. leaves) for efficient composting, and to allow compost to become well-rotted before applying it a bed to prevent competition between soil bacteria and plants for nitrogen, reducing yield.\n\nAlthough Hügelkultur beds can safely retain water in light-duty applications (for example, conserving the moisture of rain that falls on the bed), creating heavy-duty rainwater retention areas behind Hügelkultur beds on contour, to catch surface runoff from surrounding areas, can be dangerous. Some designers conflate the Hügelkultur bed's appearance with that of solid earthworks, but Hügelkultur beds cannot predictably control large amounts of stormwater in the way that solid earthworks can. Whereas embankment dams or the hillsides of swales can be relied on to hold back many thousands of gallons of water for weeks to allow it to seep into the ground, and berms can slow runoff, Hügelkultur beds are different in two ways: earthworks have no buoyant core (whereas Hügelkultur mounds contain logs), and the soil that they are made of is compacted. If fresh or dried timber is used in the bed, it may become buoyant in the water-saturated substrate, bursting from the soil covering and releasing all the sitting water through a breach. This can be an issue for years, until the wood is sufficiently rotten and infused with water. Another consideration is that Hügelkultur beds will degrade, shrinking over time into much lower mounds of soft, rich soil. This means that the retention area will have less depth as time goes on, but it also means that the uncompacted soil will remain a threat to breaching even if the logs become saturated.\n\nThere is a recorded instance of a breach occurring in a new project. Upon the first rainstorm, the retention areas behind the Hügelkultur beds filled with water and broke through. The released water carried the freshly-buried logs and dirt downhill, smashing a hole in a building being used as a church and filling the space with mud. No injuries were reported.\n\nSome permaculturists have taken mild positions against the \"hügel swales\" still being promoted by other permaculturists, citing the danger and cross-purposes of Hügelkultur beds and swales.\n\nOver-fertilized plants are said to have less flavor, and too much nitrogen can be consumed by eating certain plants which have been over-fertilised (e.g. spinach). Advocates state that overfertilization is a risk in the first year if woodchips are used, which will break down too fast. Instead raised beds made with whole logs release nutrients slowly over a period of years. It has been suggested that excessive use of decomposing organic matter in Hügelkultur could leach out and contaminate and disrupt soil and water habitats.\n\n"}
{"id": "36134078", "url": "https://en.wikipedia.org/wiki?curid=36134078", "title": "Ice Warrior Project", "text": "Ice Warrior Project\n\nThe Ice Warrior Project is an organisation founded in 2001 by the explorer Jim McNeill. Its remit is to develop people from all walks of life and echelons of society into modern-day explorers; to discover change in the world’s most remote regions under the guidance of partner leading scientific authorities; and deliver these discoveries in an engaging, human manner to audiences around the globe.\n\nThe organisation provides raw, scientific data for others to go on and interpret. Its rationale for doing so is that if we do not monitor these regions, which were described by Nobel peace prize nominee Sheila Watt-Cloutier as \"clearly one of the most important regions when it comes to climate change.\", then how will we know that any action to mitigate the human impact on our Earth is actually working?\n\nThe project's founder Jim McNeill maintains that without such knowledge how can we be true guardians of the planet on which we live? And if we are not, then this ultimately affects our chances of survival as a species. \n\nAs an organisation it has an altruistic project side which relies on corporate sponsorship and involves charitable work and also an underpinning commercial side.\n\n\n"}
{"id": "2550354", "url": "https://en.wikipedia.org/wiki?curid=2550354", "title": "Knotted cord", "text": "Knotted cord\n\nA knotted cord was a primitive surveyor's tool for measuring distances. It is a length of cord with knots at regular intervals. They were eventually replaced by surveyor's chains, which being made of metal were less prone to stretching and thus were more accurate and consistent.\n\nKnotted cords were used by many ancient cultures. The Greek schoenus is referred to as a rope used to measure land. Ropes generally became cables and chains with Pythagoras making the Greek agros a chain of 10 stadia equal to a nautical mile c 540 BC. The Romans used a waxed cord for measuring distances. \n\nA knotted cord 12 lengths long (the units do not matter) closed into a loop can be used to lay out a right angle by forming the loop of cord into a 3–4–5 triangle. This could be used for laying out the corner of a field or a building foundation, for instance.\n\nKnotted cords were used by rope stretchers, royal surveyors who measured out the sides of fields (Egyptian \"3ht\"). The knotted cords (Egyptian \"ht\") were 100 royal cubits in length, with a knot every \"hayt\" or 10 royal cubits. The rope stretchers stretched the rope in order to take the sag out it, as well as to keep the measures uniform.\n\nSince land in ancient Egypt was measured using several different units, there would have been knotted cords with the knots spaced in each unit. Among these were the \"mh t3\" or land cubits, remen royal cubits, rods or \"ha3t\", generally the lengths in multiples of 100 units. The longest measured length listed in the Rhind Mathematical Papyrus is a circumference of about a Roman mile with a diameter of 9 khet.\n\nDespite many popular claims, there is no reliable evidence that the 3-4-5 triangle, and by implication the Pythagoras' theorem, was used in Ancient Egypt to lay out right angles, such as for the pyramids. The historian Moritz Cantor first made the conjecture in 1882. Right angles were certainly laid out accurately in Ancient Egypt; their surveyors did use knotted cords for measurement; Plutarch recorded in \"Isis and Osiris\" (around 100 AD) that the Egyptians admired the 3-4-5 triangle; and the Berlin Papyrus 6619 from the Middle Kingdom (before 1700 BC) made statements that suggest knowledge of Pythagoras' theorem. However, no Egyptian text before 300 BC actually mentions the use of the theorem to find the length of a triangle's sides, and the historian of mathematics Roger Cooke notes that there are simpler ways to construct a right angle. He guesses that the Ancient Egyptians probably did know the Pythagorean theorem, but concludes that \"there is no evidence that they used it to construct right angles\".\n\n"}
{"id": "32637830", "url": "https://en.wikipedia.org/wiki?curid=32637830", "title": "Lae Garden and Landscapes", "text": "Lae Garden and Landscapes\n\nLae Garden and Landscapes is a landscaping company in the Philippines specializing in designing, installing, and maintaining fine indoor and outdoor landscapes.\n\n It was founded in 1987 primarily as a residential landscaping company.\n\nFarmlae Garden Bazaar is the main farm and nursery of Lae Garden and Landscapes. It is located at Bay, Laguna Philippines just under 20 km from South Luzon Expressway.\n"}
{"id": "2861044", "url": "https://en.wikipedia.org/wiki?curid=2861044", "title": "Land description", "text": "Land description\n\nA land description location of the written words which delineate a specific piece of real property. Also known as a \"Legal Description\". In the written transfer of real property, it is universally required that the instrument of conveyance (deed) include a written description of the property.\n\n\n"}
{"id": "38263069", "url": "https://en.wikipedia.org/wiki?curid=38263069", "title": "Land systems of Western Australia", "text": "Land systems of Western Australia\n\nLand systems of Western Australia are systematic land resource concepts, where the tying in of geographical, geological, and ecological data is processed to provide land planning and land management systems with sets of information that can process large areas of land in terms of agricultural and other usages.\n\nAt times land systems are not referred to directly, but used within terms of \"land resources\" and \"landform resources\" analysis and appraisal. The government departments from which reports and papers emanate include the Agricultural and Mining departments, as well as Land administration bodies.\n\n"}
{"id": "8838023", "url": "https://en.wikipedia.org/wiki?curid=8838023", "title": "Line marker", "text": "Line marker\n\nIn cave (and occasionally wreck) diving, line markers are used for orientation as a visual and tactile reference on a permanent guideline. Directional markers (commonly a notched acute isosceles triangle in basic outline), are also known as line arrows or Dorff arrows, and point the way to an exit. Line arrows may mark the location of a \"jump\" location in a cave when two are placed adjacent to each other. Two adjacent arrows facing away from each other, mark a point in the cave where the diver is equidistant from two exits. Arrow direction can be identified by feel in low visibility.\n\nNon-directional markers (\"cookies\") are purely personal markers that mark specific spots, or the direction of one's chosen exit at line intersections where there are options. Their shape does not provide a tactile indication of direction as this could cause confusion in low visibility. One important reason to be adequately trained before cave diving is that incorrect marking can confuse and fatally endanger not only oneself, but also other divers.\n\nThe line arrow was invented by Lewis Holzendorf and developed by Forrest Wilson at the Cave diving NSS workshop, inspired by Sheck Exley and other cave diving pioneers, and later, a few hundreds of the handmade markers were sold through Branford Dive Center in North Florida. Soon they became very popular and today are commonly used by underwater cave explorers.\n\nLine markers are generally used on permanent guidelines to provide critical information to divers following the line. The slots and notches provided are used to wrap the line to secure the marker in place. A simple passage of the line through the enlarged area at the base of the two slots will allow the marker to slide along the line, or even fall off if brushed by a diver. To more securely fasten the marker, an extra wrap may be added at each slot. The basic function of these markers is fairly consistent internationally, but procedures may differ by region, and between teams. The protocol for placement and removal should be well understood by the members of a specific team. Temporary line markers are only for the use of the team who placed them and are removed during exit to avoid littering a cave system with irrelevant and potentially confusing information.\n\nCave markers may be carried on a short length of bungee with a knot at one end and a clip at the other, but other methods are also used. Reasonable security and easy access are all that is required.\n\nLine arrows are used to indicate the direction along the line to the nearest air source. In cave systems with only one opening, line arrows will always indicate the route back to the entry, but in cave systems with multiple openings, they may not point to the entry used by the dive team, and in some cases this may not be a usable exit.\n\nSome popular cave diving systems have permanent line arrows at regular intervals pointing to the nearest exit, and may be marked with a distance to the exit. Permanent arrows may also be used to identify secondary passages and midpoints in a cave.\n\nLine arrows are placed to help the divers to establish the direction they come from, when they later need to find the way out, but should not alter the general navigation information of the system, so line arrows are added only where they cannot cause other groups to become confused about the direction to the nearest air source. An arrow would be added at the point of attachment of a reel line when tying off to explore new areas, and may be removed after return and removal of the temporary line. If the place is to be marked for future reference, the arrow may be left in place.\n\nThe original non-directional markers in cave diving were clothes pegs, but the circular plastic \"cookies\" were developed later, based on the successful line-arrow design, and are more secure when placed on a guideline. They can be used to mark reference points, to distinguish lines at an intersection, or can be placed by each member of a team on a jump or gap line to identify who is still on the line if the team gets separated. As they are commonly used to identify the presence of a diver, they generally have some kind of personal identification, such as initials or drawings and may have notches, or holes added so they can be identified by feel. \n\nA non-directional marker indicates the presence of a diver or team beyond a point and is mainly useful for that team. They may be used to indicate the direction the team came from and intend to exit by when the intersection is already marked by arrows, particularly if there are alternatives available which the team does not want to use. To avoid confusing other divers, these temporary markers are removed on the way out.\n\nA hybrid marker, also known as a referencing exit marker or REM, is a more recently developed line marker used as a directional marker by the diver placing it, and seen as a cookie by others. They are rectangular with slots like the arrow and cookie forms for attachment to a line, and have a relatively large blank space on one end to add identification on one side and to write on the other side. Referencing exit markers are asymmetrical so can be used to show the direction which the diver came from. They are personal markers, and should not be used by others to judge the suitability of a route as the practicability of an exit may depend on the size of the diver and the equipment carried.\n\nOriginally hand made from available materials. Commercially marketed line markers are commonly injection-moulded plastic and are available in a few colours. Small variations in size and shape allow personal markers to be identified more easily, and unique markings may be added by the user.\n\n\n"}
{"id": "3989609", "url": "https://en.wikipedia.org/wiki?curid=3989609", "title": "List of countries and dependencies by area", "text": "List of countries and dependencies by area\n\nThis is a list of the world's countries and their dependent territories by area, ranked by total area.\n\nEntries in this list include, but are not limited to, those in the ISO 3166-1 standard, which includes sovereign states and dependent territories. Largely unrecognised states not in ISO 3166-1 are included in the list in ranked order, but are not given a rank number. The areas of such largely unrecognised states are in most cases also included in the areas of the more widely recognised states that claim the same territory; see the notes in the \"notes\" column for each country for clarification.\n\nNot included in the list are individual country claims to parts of the continent of Antarctica, entities such as the European Union that have some degree of sovereignty but do not consider themselves to be sovereign countries or dependent territories, and unrecognised micronations such as the Principality of Sealand.\n\nThis list includes three measurements of area:\n\nData is taken from the United Nations Statistics Division unless otherwise noted.\n\nThe charts below are based on the \"CIA World Factbook\" as of February 15, 2005.\n\nSovereign states with areas greater than 100,000 km are shown in green. In addition, non-sovereign territories are included for purposes of comparison, and are shown in gray. Areas include inland water bodies (lakes, reservoirs, rivers). Claims to parts of Antarctica by various countries are not included.\n\n\n"}
{"id": "20586173", "url": "https://en.wikipedia.org/wiki?curid=20586173", "title": "List of countries by westernmost point", "text": "List of countries by westernmost point\n\nThis is a list of countries by westernmost point on land (dependent territories included). A selection of dependent territories are listed in \"italics\" and are not ranked.\n\nThere are five countries with territory on both sides of the 180th meridian, and thus can be said to be both the westernmost and easternmost countries: Russia, New Zealand, Fiji, United States, and Kiribati (as well as Antarctica). Russia, New Zealand, and Fiji have most of their territories west of the 180th meridian, in the Eastern Hemisphere, so they are considered in this article to belong to the easternmost countries with their territory stretching east beyond the 180th meridian into the Western Hemisphere. Conversely, the United States and Kiribati have most of their territories east of the 180th meridian, into the Western Hemisphere, so they are considered to belong to the westernmost countries, with their territory stretching west beyond the 180th meridian into the Eastern Hemisphere.\n"}
{"id": "21003502", "url": "https://en.wikipedia.org/wiki?curid=21003502", "title": "List of firsts in the Geographic North Pole", "text": "List of firsts in the Geographic North Pole\n\nThis is a list of firsts in the Geographic North Pole.\n\n"}
{"id": "57758502", "url": "https://en.wikipedia.org/wiki?curid=57758502", "title": "List of international presidential trips made by Armen Sarkissian", "text": "List of international presidential trips made by Armen Sarkissian\n\nBelow is a list of international presidential trips made by Armen Sarkissian as the 4th President of Armenia.\n\n"}
{"id": "33317494", "url": "https://en.wikipedia.org/wiki?curid=33317494", "title": "List of international presidential trips made by Dilma Rousseff", "text": "List of international presidential trips made by Dilma Rousseff\n\nThis is a list of international presidential trips made by Dilma Rousseff, the 36th President of Brazil. During her presidency, which began with her inauguration on 1 January 2011 and ended with her impeachment on 31 August 2016, Rousseff visited 24 countries as of July 2012.\n\nThe following international trips were made by President Dilma Rousseff in 2011:\nThe following international trips have been made by President Dilma Rousseff during her second year in office as of December 2012:\nOn 17 September 2013, President Rousseff cancelled her state visit to Washington, D.C. on 3 October 2013, because of alleged spying by the United States that targeted Brazil.\n\nThe following international trips have been made by President Dilma Rousseff in 2015:\n"}
{"id": "51848502", "url": "https://en.wikipedia.org/wiki?curid=51848502", "title": "List of international presidential trips made by Ronald Reagan", "text": "List of international presidential trips made by Ronald Reagan\n\nThis is a list of international presidential trips made by Ronald Reagan, the 40th president of the United States. Ronald Reagan made 25 international trips to 26 different countries on four continents—Europe, Asia, North America, and South America—during his presidency, which began on January 20, 1981 and ended on January 20, 1989.\n\nThe number of visits per country where he travelled are:\n\n"}
{"id": "254069", "url": "https://en.wikipedia.org/wiki?curid=254069", "title": "List of words derived from toponyms", "text": "List of words derived from toponyms\n\nThis is a list of English language words derived from toponyms, followed by the place name it derives from. \n\n\n\n\n\nNote: Saskatoon, Saskatchewan is named after the local Saskatoon berry, rather than vice versa.\n\n\n\nThere are some corporations whose name is simply the same as their original location.\n\n\"See\": Chemical elements named after places\n\n\n\n"}
{"id": "10528515", "url": "https://en.wikipedia.org/wiki?curid=10528515", "title": "Location intelligence", "text": "Location intelligence\n\nLocation intelligence (LI), or spatial intelligence, is the process of deriving meaningful insight from geospatial data relationships to solve a particular problem. It involves layering multiple data sets spatially and/or chronologically, for easy reference on a map, and its applications span industries, categories and organizations.\n\nMaps have been used to represent information throughout the ages, but what might be referenced as the first example of true location 'intelligence' was in London in 1854 when John Snow was able to debunk theories about the spread of cholera by overlaying a map of the area with the location of water pumps and was able to narrow the source to a single water pump. This layering of information over a map was able to identify relationships between different sets of geospatial data.\n\nLocation or geographical information system (GIS) tools enable spatial experts to collect, store, analyze and visualize data. Location intelligence experts can use a variety of spatial and business analytical tools to measure optimal locations for operating a business or providing a service. Location intelligence experts begin with defining the business ecosystem which has many interconnected economic influences. Such economic influences include but are not limited to culture, lifestyle, labor, healthcare, cost of living, crime, economic climate and education.\n\nThe term \"location intelligence\" is often used to describe the people, data and technology employed to geographically \"map\" information. These mapping applications can transform large amounts of data into color-coded visual representations that make it easy to see trends and generate meaningful intelligence. The creation of location intelligence is directed by domain knowledge, formal frameworks, and a focus on decision support. Location cuts across through everything i.e. devices, platforms, software and apps, and is one of the most important ingredient of understanding context in sync with social data, mobile data, user data, sensor data.\n\nLocation intelligence is also used to describe the integration of a geographical component into business intelligence processes and tools, often incorporating spatial database and spatial OLAP tools.\n\nIn 2012, Wayne Gearey from the real estate industry (JLL) offered the first applied course on location intelligence at the University of Texas at Dallas in which he defined location intelligence as the process for selecting the optimal location that will support workplace success and address a variety of business and financial objectives.\n\nPitney Bowes MapInfo Corporation describes location intelligence as follows: \"Spatial information, commonly known as \"Location\", relates to involving, or having the nature of where. Spatial is not constrained to a geographic location however most common business uses of spatial information deal with how spatial information is tied to a location on the earth. Miriam-Webster® defines Intelligence as \"The ability to learn or understand, or the ability to apply knowledge to manipulate one`s environment.\" Combining these terms alludes to how you achieve an understanding of the spatial aspect of information and apply it to achieve a significant competitive advantage.\"\n\nDefinition by ESRI is as follows: \"Location Intelligence is defined as the capacity to organize and understand complex data through the use of geographic relationships. LI organizes business and geographically referenced data to reveal the relationship of location to people, events, transactions, facilities, and assets.\"\n\nDefinition by Yankee Group within their White Paper \"Location Intelligence in Retail Banking: \"...a business management term that refers to spatial data visualization, contextualization and analytical capabilities applied to solve a business problem.\"\n\nNear defines location intelligence as the gamut of geo-spatial analytic models, algorithms, techniques and tools that harness location-indexed streaming and static data to provide requisite answers to questions around location such as the attributes of i) a place, ii) people visiting a place, and iii) products consumed at a place. Location intelligence platforms encapsulate these technologies, ingest relevant data sources, and provide it as data-as-a-service, or use it to power data-driven products.\n\nInstarea defines location intelligence as \"Using machine learning algorithms to uncover trends that would be otherwise unknown using human based rules in a large location data set.\"\n\nLocation intelligence is used by a broad range of industries to improve overall business results. Applications include:\n\n"}
{"id": "3627810", "url": "https://en.wikipedia.org/wiki?curid=3627810", "title": "National Conservation Lands", "text": "National Conservation Lands\n\nNational Conservation Lands, formally known as the National Landscape Conservation System, is a collection of lands in 873 federally recognized areas considered to be the crown jewels of the American West. These lands represent 10% of the managed by the Bureau of Land Management (BLM). The BLM is the largest federal public land manager and is responsible for over 40% of all the federal public land in the nation. The other major federal public land managers include the US Forest Service (USFS), National Park Service (NPS), and the US Fish and Wildlife Service (USFWS).\n\nOver the years, the Bureau of Land Management has had to adjust its approach to public land management to fit the changing needs of the nation. The BLM historically has managed lands under its jurisdiction for extractive uses, such as mining, logging, grazing, and oil and gas production. In 1983, Congress acknowledged the value of watersheds, wildlife habitat, recreation, scenery, scientific exploration and other non-extractive uses with the designation of the first BLM-managed wilderness area—the Bear Trap Canyon unit of the Lee Metcalf Wilderness in Montana. In 1996, President Clinton underscored non-extractive priorities on BLM lands when he established the first national monument to be administered by the BLM—the Grand Staircase-Escalante National Monument in southern Utah. With this and several similar designations, a new focus emerged that would become part of how the agency looks at the land it manages: the protection of special areas where conservation and restoration of the landscape and its biological or cultural resources is the overriding objective.\n\nThe Bureau of Land Management's National Landscape Conservation System, better known as the National Conservation Lands, was created in 2000 with the mission to \"conserve, protect, and restore these nationally significant landscapes that have outstanding cultural, ecological, and scientific values for the benefit of current and future generations.\"\n\nThere are ten different federal conservation designations for the units that make up the National Conservation Lands:\n\nThe Conservation System was created in 2000, but without Congressional authorization, there was no guarantee that the System would be permanent. The National Landscape Conservation System Act was signed into law in March 2009. The Act permanently unified the individual units as a public lands System, protecting the System in law so that it would no longer exist at the pleasure of each president. This marked the first new congressionally authorized public lands system in decades. \n\nThe Conservation System act was included in the Omnibus Public Land Management Act of 2009, which also added of new designations to the System, including a National Monument, three National Conservation Areas, Wilderness, Wild and Scenic Rivers and National Scenic Trails.\n\nThese 25 sites, total \n\nThese 16 sites, total \n\nThese 221 sites, total , excludes wilderness associated with the U.S. Forest Service, National Park Service and other agencies \n\nThere are 545 wilderness study areas with a total area of .\n\nThese 11 sites, total \n\nDistances and states are noted for BLM lands only.\n\n5 units, total \n\nDistances and states are noted for BLM lands only.\n\nThese 38 sites, total \n\nSee List of National Wild and Scenic Rivers\n\nThese 6 sites total (note: more sites exist than are listed here)\n\n"}
{"id": "32374980", "url": "https://en.wikipedia.org/wiki?curid=32374980", "title": "Nautical measured mile", "text": "Nautical measured mile\n\nA nautical measured mile is a nautical mile which is marked by two pairs of towers. A mile is measure by sailing on a given bearing and lining up the pairs of towers. The start of the mile is recorded when the first pair of towers line up and the end of the mile recorded when the second pair line up.\n\nTo accurately measure performance ships must make at least four to six runs in both directions to allow for the wind and tide. \nThere are several nautical measured miles around the British Isles:\n\n"}
{"id": "21994", "url": "https://en.wikipedia.org/wiki?curid=21994", "title": "Navigation research", "text": "Navigation research\n\nWhereas originally the term Navigation applies to the process of directing a ship to a destination, Navigation research deals with fundamental aspects of navigation in general. It can be defined as \"The process of determining and maintaining a course or trajectory to a goal location\" (Franz, Mallot, 2000).\nIt concerns basically all moving agents, biological or artificial, or remote-controlled.\n\nFranz and Mallot proposed a navigation hierarchy in \"Robotics and Autonomous Systems 30\" (2006):\nThere are two basic methods for navigation:\n\n\nIn human navigation people visualize different routes in their minds to plan how to get from one place to another. The things which they rely on to plan these routes vary from person to person and are the basis of the differing navigational strategies.\nSome people use measures of distance and absolute directional terms (north, south, east, and west) in order to visualize the best pathway from point to point. The use of these more general, external cues as directions is considered part of an allocentric navigation strategy. Allocentric navigation is typically seen in males and is beneficial primarily in large and/or unfamiliar environments. This likely has some basis in evolution when males would have to navigate through large and unfamiliar environments while hunting. The use of allocentric strategies when navigating primarily activates the hippocampus and parahippocampus in the brain. This navigation strategy relies more on a mental, spatial map than visible cues, giving it an advantage in unknown areas but a flexibility to be used in smaller environments as well. The fact that it is mainly males that favor this strategy is likely related to the generalization that males are better navigators than females as it is better able to be applied in a greater variety of settings.\n\nEgocentric navigation relies on more local landmarks and personal directions (left/right) to navigate and visualize a pathway. This reliance on more local and well-known stimuli for finding their way makes it difficult to apply in new locations, but is instead most effective in smaller, familiar environments. Evolutionarily, egocentric navigation likely comes from our ancestors who would forage for their food and need to be able to return to the same places daily to find edible plants. This foraging usually occurred in relatively nearby areas and was most commonly done by the females in hunter-gatherer societies. Females, today, are typically better at knowing where various landmarks are and often rely on them when giving directions. Egocentric navigation causes high levels of activation in the right parietal lobe and prefrontal regions of the brain that are involved in visuospatial processing.\n\nOutdoor robots can use GPS in a similar way to automotive navigation systems.\nAlternative systems can be used with floor plan instead of maps for indoor robots, combined with localization wireless hardware.\n\n\n"}
{"id": "1749642", "url": "https://en.wikipedia.org/wiki?curid=1749642", "title": "Olmsted Brothers", "text": "Olmsted Brothers\n\nThe Olmsted Brothers company was an influential landscape architectural firm in the United States, established in 1898 by brothers John Charles Olmsted (1852–1920) and Frederick Law Olmsted, Jr. (1870–1957), sons of the eminent landscape architect Frederick Law Olmsted.\n\nThe Olmsted Brothers inherited the nation's first landscape architecture business from their father Frederick Law Olmsted. This firm was a successor to the earlier firm of Olmsted, Olmsted and Eliot after the death of their partner Charles Eliot in 1897. The two brothers were among the founding members of the American Society of Landscape Architects (ASLA) and played an influential role in creating the National Park Service. Prior to their takeover of the firm, Frederick Law Olmsted Jr. had worked as an apprentice under his father, helping to design projects such as Biltmore Estate and the World's Columbian Exposition before graduating from Harvard University. The firm employed nearly 60 staff at its peak in the early 1930s. Notable landscape architects in the firm included James Frederick Dawson and Percival Gallagher. The last Olmsted family member in the firm, Frederick Law Olmsted, Jr., retired in 1949. The firm itself remained in operation, moving from Brookline in 1980 and continuing in Fremont, NH until 2000. This created one continuous firm from 1858-2000.\n\n\"Fairsted\" -- the firm's 100-year-old business headquarters and design office -- has been carefully preserved as the Frederick Law Olmsted National Historic Site, located on of landscaped grounds at 99 Warren St., Brookline, Massachusetts. It offers excellent insights into the practice of large-scale landscape design and engineering. The site also houses an archive (access by appointment only) of the firm's designs, plant lists, and photos for hundreds of projects.\n\nThe Olmsted Brothers completed numerous high-profile projects, many of which remain popular to this day, including park systems, universities, exposition grounds, libraries, hospitals, residential neighborhoods and state capitols. Notable commissions include the roadways in the Great Smoky Mountains and Acadia National Parks, Yosemite Valley, Atlanta's Piedmont Park, a residential neighborhood in Oak Bay, British Columbia, Canada: Uplands; entire park systems in cities such as Cleveland, Portland, Seattle, and Washington state's Northern State Hospital. The Olmsted Brothers also co-authored, with Harland Bartholomew, a 1930 report for the Los Angeles Chamber of Commerce entitled \"Parks, Playgrounds, and Beaches for the Los Angeles Region\" encouraging the preservation of outdoor public space in southern California. The report was largely ignored by the city, but became an important urban planning reference.\n\n\n"}
{"id": "2006073", "url": "https://en.wikipedia.org/wiki?curid=2006073", "title": "Outline of environmental studies", "text": "Outline of environmental studies\n\nThe following outline is provided as an overview of and topical guide to environmental studies:\n\nEnvironmental studies –\n\n\nPrimary undergraduate and graduate degrees in Environmental studies include the following.\n\n\n\n\n\nListed here are primary, secondary, and non-degree granting environmental education institutions and organizations.\n\n\n"}
{"id": "3084362", "url": "https://en.wikipedia.org/wiki?curid=3084362", "title": "Party wall", "text": "Party wall\n\nA party wall (occasionally parti-wall or parting wall, also known as common wall) is a dividing partition between two adjoining buildings that is shared by the occupants of each residence or business. Typically, the builder lays the wall along a property line dividing two terraced houses, so that one half of the wall's thickness lies on each side. This type of wall is usually structural. Party walls can also be formed by two abutting walls built at different times. The term can be also used to describe a division between separate units within a multi-unit apartment complex. Very often the wall in this case is non-structural but designed to meet established criteria for sound and/or fire protection, i.e. a firewall.\n\nWhile party walls are effectively in common ownership of two or more immediately adjacent owners, there are various possibilities for legal ownership: the wall may belong to both tenants (in common), to one tenant or the other, or partly to one, partly to the other. In cases where the ownership is not shared, both parties have \"use\" of the wall, if not ownership. Other party structures can exist, such as floors dividing flats or apartments.\n\nApart from special statutory definitions, the term \"Party Wall\" may be used in four different legal senses. \n\nIn English law the party wall does not confirm a boundary at its median point and there are instances where the legal boundary between adjoining lands actually comes at one face or the other of a wall or part of it, and sometimes at some odd measurement within the thickness of the wall. The legal position is, however, clear insofar as a party using or benefiting from a party wall or structure abutting, on or in its land has rights to use the wall and for it to be retained should the other side no longer wish it to be there. For this reason, expert surveyors are used in the main to issue notices, deal with the response from someone receiving a notice and settling any dispute by an Award. Details can be obtained from the Royal Institution of Chartered Surveyors.\n\nOriginating in London as early as the 11th century, requirements for terraced houses to have a dividing wall substantially capable of acting as a fire break have been applied in some form or other. Evidently, this was not enough to prevent the several great fires of London, and the most famous of which being the Great Fire of 1666.\n\nIn England and Wales, the Party Wall etc. Act 1996, confers rights on those whose property adjoins a party wall or other 'party structure' irrespective of ownership of the wall or structure.\n\nThe Hopps Partnership - A firm of RICS registered Surveyors whom specialize in matters pertaining to the Party Wall etc. Act 1996\n\nThe principles of the party wall in Paris under the common law in 1765 are the following: \n\n\nIn the USA, the term may refer to a fire wall that separates and is used in common by two adjoining buildings (condominium, row house), or the wall between two adjacent commercial buildings that were often built using common walls, or built walls onto existing walls. Rights and obligations are governed by state statutes, and common law.\n\nThe wall starts at the foundation and continues up to a parapet, creating two separate and structurally independent buildings on either side.\n\n\n\n"}
{"id": "4329386", "url": "https://en.wikipedia.org/wiki?curid=4329386", "title": "Philosophy of environment", "text": "Philosophy of environment\n\nThe philosophy of environment is a trend of free thought located between philosophy, epistemology and anthropology. It combines various schools of philosophy such as humanist ecology, philosophy of evolution and environmental humanism. It is also meant to be a cultural trend having an influence in society.\n\nThe philosophical current indicated under the name philosophy of evolution has been developed since the 1970s by Humanist Ecology (also called Environmental or Evolutive Humanism) and by the neo-darwinian school. Its purpose is the long-term evolution of the complex living being in its universal environment, analysed through its Human cultural expression.\n\nThis trend is unique to the quasi religious Evolutionist Humanism advocated by Julian Huxley, notably during the founders congress of the IHEU (International Humanist and Ethical Union) in 1952.\n\nFurther than the basic scientific theory of evolution (notably neo-darwinian, Evolutive Humanism, developed by Richard Dawkins and Stephen Jay Gould), looks at the necessity for Man of a permanent adaptation of both his organism and his thoughts to his universal environment. And from there to the relativized relation to belief and to uncertainty, because human ideas evolve and are transformed, as are physical qualities, in a universal process of evolution and adaptive improvement in their environment. But this does not suppose determinism, genetic or cultural, it refers to the evolutive interactivity and reactivity, partly random, of the living being with his surroundings. According to Marc Carl, \"Man must necessarily learn to manage the insufficient appearance and the uncertainty of information to fit and to develop in his relational interactions with his environment, as well as physically and culturally\". In a universal environment still as much as 90% unknown, this evolutive cogitation therefore tries not to be locked in premature schemata and answers. It encourages humility in proposals and intuitive boldness in investigations.\nA subjacent humanist thought encourages human beings to deliberately take his destiny in his hands, in careful correlation with his environment, knowing that human thought is one of the most impacting manifestations of the living being, as an agent of transformation of the environment, and consequently not only of the Earth's environment. In terms of philosophy of evolution, the organized collective thought of humanity, and in general his evolutive culture, appears as the key to development of our species in its universal environment, and the key to a possibly significant interactive modification, in the long term, of this environment. It is principally a prospective step.\n\nThis cultural trend seems to have emerged because the concept of progressive permanent adaptation, which is no longer appreciated only by its scientific aspects, also took a metaphysical dimension (in a sense of research of the essence of the human), which consequently encourages analysis of human evolution, an agent which potentially modifies our environment, in philosophical terms. The possibilities and the risks of this evolution, non-deterministic because of its integration into a dynamic complex living system, gives the human existence and destiny a new sense. And to be involved in such a cogitation opens a philosophical way in which curious minds could not miss advancing sooner or later, a way which rests on a regenerated metaphysics, encouraging us to take back in an evolutive way Aristoteles original concept of physis and its substance, and to search with modern conceptual tools the essence and the sense of our life.\n\nSupported by the philosophy of evolution, this international emerging concept has expressed since the 1970s an evolutive humanism, extending the naturalist tradition of ancient Greek philosophers. Humanist ecology encourages us to better understand and situate the place and the destiny of humanity in its environment in permanent evolution. Human destiny is put in perspective in a universal context where many things remain to be learned. Because it encourages every human in a responsibility in front of his conscience, humanist ecology can be defined too as a will of ethical responsibility of civilized humanity, favouring its permanent improvement and its happiness, in useful interaction with its evolutive environment, in a beneficial way as much for a human being particularly as for mankind in general, and in common symbiosis with their local and global bioscape in evolution. That serves to optimize human society in its own interactions and in its interactions with its bioscape, notably by preserving the planetary equilibrium of the Earth. This solidarity of all of mankind is necessary to preserve its environment and its best development in this environment inspired a particular political expression of humanist ecology, taken up notably by Statesmen such as Jacques Chirac (France) or Mohammed VI (Morocco) in the main meetings of the United Nations.\n\nHumanist ecology naturally favours the permanent adaptation and the best possible development of humanity, and of the human being, in an uncertain universal environment in permanent evolution, with a mind open enough to consider all the possibilities. In humanist ecological comprehension, it is vain to want to freeze an arbitrary cultural schemata and choose the apparent equilibrium and the supposed future of one moment of evolution. A permanent evolutive adaptation is necessary, as much biologically as mentally. That requires relativity and a caution in the analysis. According to this concept, for the human mind, any representation belongs to the domain of belief, considering the uncertainty of the relation of man to the universe, and the natural imperfection of his senses to represent his environment and his interactions with this environment; the reality perceived by Man being only one representation of reality, particular in mankind. Humanist ecology admits this relativized relation with belief but refutes any final and locking form, knowing that no truth can be final for the human mind without upsetting its natural evolutive necessity. This school of thought accepts belief in the present, for want of anything better, but dictates that you must take care to verify and update beliefs.\n\n"}
{"id": "5298959", "url": "https://en.wikipedia.org/wiki?curid=5298959", "title": "Public open space", "text": "Public open space\n\nPublic open space is often referred to by urban planners and landscape architects by the acronym 'POS'. Varied interpretations of the term are possible.\n\n'Public' can mean:\n\n'Open' can mean:\n\nDepending on which of these definitions are adopted, any of the following could be called Public Open Space:\n\n\n\n"}
{"id": "52205564", "url": "https://en.wikipedia.org/wiki?curid=52205564", "title": "Radio acoustic ranging", "text": "Radio acoustic ranging\n\nRadio acoustic ranging, occasionally written as \"radio-acoustic ranging\" and sometimes abbreviated RAR, was a method for determining a ship's precise location at sea by detonating an explosive charge underwater near the ship, detecting the arrival of the underwater sound waves at remote locations, and radioing the time of arrival of the sound waves at the remote stations to the ship, allowing the ship's crew to use triangulation to determine the ship's position. Developed by the United States Coast and Geodetic Survey in 1923 and 1924 for use in accurately fixing the position of survey ships during hydrographic survey operations, it was the first navigation technique in human history other than dead reckoning that did not require visual observation of a landmark, marker, light, or celestial body, and the first non-visual means to provide precise positions. First employed operationally in 1924, radio acoustic ranging remained in use until 1944, when new radio navigation techniques developed during World War II rendered it obsolete.\n\nTo fix their position using radio acoustic ranging, a ship's crew first ascertained the temperature and salinity of sea water in the vicinity of the ship to determine an accurate velocity of sound through the water. The crew then threw a small TNT bomb off the ship's stern. It exploded at a depth of about , and a chronograph aboard the ship automatically recorded the time the explosion was heard at the ship. The sound traveled outward from the explosion, eventually reaching hydrophones at known locations – shore stations, anchored manned station ships, or unmanned moored buoys – at a distance from the ship. Each hydrophone was connected to a radio transmitter that automatically sent a signal indicating the time its hydrophone detected the sound. At the distances involved – generally less than – each of these radio signals arrived at the ship at essentially the same instant that each of the remote hydrophones detected the sound of the explosion. The ship's chronograph automatically recorded the time each radio signal arrived at the ship. By subtracting the time of the explosion from the time of radio signal reception, the ship's crew could determine the length of time the sound wave required to travel from the point of the explosion to each remote hydrophone and, knowing the speed of sound in the surrounding sea water, could multiply the sound's travel time by the velocity of sound in sea water to determine the distance between the explosion and the hydrophone. By determining the distance to at least two remote hydrophones in known locations, the ship's crew could use triangulation to fix the ship's position.\n\nIn deep waters, such as those that prevailed in the Pacific Ocean along the United States West Coast, the Coast and Geodetic Survey could rely upon shore stations to support radio acoustic ranging because the deep water allowed sound to travel to the coast. Along the United States East Coast, where shallower waters prevailed, sound had greater difficulty in reaching the coast, and the Coast and Geodetic Survey relied more heavily on anchored manned station ships, and later unmanned moored buoys, to support radio acoustic ranging.\n\nChronographs recorded times to the hundredth of a second, and the crew of a ship using radio acoustic ranging could determine their ship's distance from the remote hydrophone stations to within , allowing them to plot their ship's position with great accuracy for the time. With sound waves traveling from the point of the explosion to the distant hydrophones at about , ships occasionally used radio acoustic ranging at distances of over between ship and hydrophone station, and distances of were common.\n\nRadio acoustic ranging had its origins in a growing understanding of underwater acoustics and their practical application during the early decades of the 20th century, and developed in parallel with echo sounding. The first step took place in the early 1900s, when the Submarine Signal Company invented a submarine bell signalling device and a hydrophone that could serve as a receiver of the underwater sounds the bells generated. The crew of a ship equipped with the receiving hydrophone could plot their ship's distance from the submarine bell mechanism and plot intersecting lines from two or more bells to determine the ship's position. The bells were installed at lighthouses, aboard lightvessels, and on buoys along the coasts of North America and Europe, and receiving hydrophones were mounted aboard hundreds of ships. It was history's first practical use of acoustics in an ocean environment.\n\nThe sinking of in 1912 spurred the Canadian inventor Reginald Fessenden (1866–1932) to begin work on a long-distance underwater sound transmission and reception system that could detect hazards in the path of a ship. This led to the invention of the Fessenden oscillator, an electro-acoustic transducer which by 1914 had a proven ability to transmit and receive sound at a distance of 31 miles across Massachusetts Bay and to detect an iceberg ahead of a ship at a range of two miles by bouncing sound off it and detecting the echo, as well as an occasional ability to detect the reflection of sound off the ocean bottom. Further impetus to developing practical applications of underwater acoustics came from World War I, which prompted the Royal Navy, United States Navy, and United States Army Coast Artillery Corps to experiment with sound as a means of detecting submerged submarines. In postwar experiments, the Coast Artillery Corps's Subaqueous Sound Ranging Section conducted experiments in shallow water in Vineyard Sound off Massachusetts in which it detonated explosive charges underwater at the ends of established baselines and measured the amount of time it took for the sound to arrive at hydrophones at the other ends of the baselines in order to establish very accurate measurements of the speed of sound through water. And in 1923, the Submarine Signal Company improved upon its underwater signaling devices by equipping them with radio transmitters that sent signals both to identify the particular device and to indicate to approaching ships that it would generate an acoustic signal at a specific time interval after it sent the radio signal, allowing ships to identify the specific navigational aid they were approaching and to take advantage of a one-way ranging capability that let their crews determine their direction and distance from the navigational aid.\n\nRealizing the potential of these applications of acoustics to hydrographic surveying and navigation, particularly along the United States West Coast, where fog frequently interfered with attempts to fix ship positions accurately, Ernest Lester Jones (1876–1929), then Director of the United States Coast and Geodetic Survey, in consultation with United States Coast and Geodetic Survey Corps officers, decided to investigate the use of acoustics in both depth finding and navigation. Nicholas H. Heck (1882–1953), a Coast and Geodetic Survey Corps officer, had been assigned from 1917 to 1919 to World War I service with the United States Naval Reserve Force, during which he had researched the use of underwater acoustics in antisubmarine warfare. He was the obvious choice to lead the new effort.\n\nBy January 1923, the Coast and Geodetic Survey had decided to install a Hayes sonic rangefinder – an early echo sounder – aboard the survey ship USC&GS \"Guide\", which the Coast and Geodetic Survey planned to commission into its fleet later that year; successful operation of the sonic rangefinder would require a precise understanding of the speed of sound through water. When Heck contacted E. A. Stephenson of the U.S. Army Coast Artillery Corps to inform him of this plan and to inquire further about the Vineyard Sound experiments, Stephenson suggested that a system of hydrophones detecting the sound of underwater explosions could allow Coast and Geodetic Survey ships to fix their position while conducting surveys. Heck agreed, but believed that existing navigation aids would not meet the needs of the Coast and Geodetic Survey in terms of the immediacy and accuracy of position fixes. He envisioned improving on the Submarine Signal Company's system of underwater noise generators and attached radio transmitters, as well as other previous concepts, by creating what would become known as the radio acoustic ranging method. Like echosounding, this method required an accurate calculation of the speed of sound through water.\n\nHeck oversaw tests at Coast and Geodetic Survey headquarters in Washington, D.C., that demonstrated that shipboard recording of the time of an explosion could be performed accurately enough for his concept to work. He worked with Dr. E. A. Eckhardt, a physicist, and M. Keiser, an electrical engineer, of the National Bureau of Standards to develop a hydrophone system that could automatically send a radio signal when it detected the sound of an underwater explosion. When the Coast and Geodetic Survey commissioned \"Guide\" in 1923, Heck had her based at New London, Connecticut. Under his direction, \"Guide\" both tested her new echo sounder's ability to make accurate depth soundings and conducted radio acoustic ranging experiments in cooperation with the U.S. Army Coast Artillery Corps. Despite many difficulties, testing of both echo sounding and radio acoustic ranging wrapped up successfully in November 1923.\n\nIn late November 1923, with Heck aboard, \"Guide\" began a voyage from New London via Puerto Rico and the Panama Canal to San Diego, California, where she would be based in the future, with her route planned to take her over a wide variety of ocean depths so that she could continue to test her echo sounder. \"Guide\" made history during the voyage, becoming the first Coast and Geodetic Survey ship to use echo sounding to measure and record the depth of the sea at points along her course; she also measured water temperatures and took water samples so that the Scripps Institution for Biological Research (now the Scripps Institution of Oceanography) at La Jolla, California, could measure salinity levels. She also compared echo sounder soundings with those made by lead lines, discovering that using a single speed of sound through water, as had been the previous practice by those conducting echo sounding experiments, yielded acoustic depth-finding results that did not match the depths found by lead lines. Before she reached San Diego in December 1923, she had accumulated much data beneficial to the study of the movement of sound waves through water and measuring their velocity under varying conditions of salinity, density, and temperature, information essential both to depth-finding and radio acoustic ranging.\n\nUpon arriving in California, Heck and \"Guide\" personnel in consultation with the Scripps Institution developed formulas that allowed accurate echo sounding of depths in all but the shallowest waters and installed hydrophones at La Jolla and Oceanside, California, to allow experimentation with radio acoustic ranging. Under Heck's direction, \"Guide\" then conducted experiments off the coast of California during the early months of 1924 that demonstrated that accurate echo sounding was possible using the new formulas. Experiments with radio acoustic ranging, despite initial difficulties, demonstrated that the method also was practical, although difficulty with getting some of the explosive charges to detonate hampered some of the experimental program. In April 1924, the Coast and Geodetic Survey concluded that both echo sounding and radio acoustic ranging were fundamentally sound, with no foundational problems left to solve, and that all that remained necessary was continued development and refinement of both techniques during their operational use. Heck turned over continued development of echo sounding and radio acoustic ranging to \"Guide\"'s commanding officer, Commander Robert Luce, and returned to his duties in Washington, D.C.\n\nOperating in the Pacific Ocean off Oregon in 1924, \"Guide\" became the first ship to employ radio acoustic ranging operationally. Off Oregon that year, she successfully employed the technique at a distance of between the ranging explosion and the remote hydrophones detecting its sound and in the process achieved the first observed indication of the ocean sound layer that was later called the sound fixing and ranging (SOFAR) channel or deep sound channel (DSC). In 1928, French investigators extended this range, detonating a 30-kg (66-pound) explosive in the Mediterranean Sea between Algiers in French Algeria and Toulon, France, and detecting the sound at a range of .\n\nInitially, Heck and others involved in the development of radio acoustic ranging thought the technique would prove least effective along the coast of the Pacific Northwest, where they assumed that the sound of wave action along the coast and the difficulty of setting up shore stations and cables would reduce the success of radio acoustic ranging; in contrast, they thought that conditions along the United States East Coast would pose no challenges. In fact, the opposite proved true: Among other problems, the relatively shallow water along the U.S. East Coast attenuated the sound of ranging explosions and shoals often blocked the sound from reaching shore at all. To overcome these difficulties, the Coast and Geodetic Survey anchored manned vessels well offshore along the U.S. East Coast to serve as hydrophone stations. In 1931, the Coast and Geodetic Survey proposed replacing the manned station ships with \"radio-sonobuoys\", and in July 1936 it began to place radio-sonobuoys in service. The 700-pound (317.5-kg) unmanned buoys – equipped with subsurface hydrophones, batteries, and radio transmitters that automatically sent a radio signal when their hydrophones detected the sound of a ranging explosion – could be deployed or recovered by Coast and Geodetic Survey ships in five minutes. Use of the buoys spread to the U.S. West Coast as well because they were cheaper to set up and operate than a shore station.\n\nRadio acoustic ranging had limitations and drawbacks. Local peculiarities in the propagation of acoustic waves in the water column could degrade its accuracy, there were problems with maintaining hydrophone stations, and handling explosive charges posed a considerable danger to personnel and ships. On one occasion a Coast and Geodetic Survey Corps ensign on board the survey ship USC&GS \"Hydrographer\" inserted a radio acoustic ranging bomb in the mouth of a shark and released the shark, only to watch in horror as it swam back to the ship and exploded next to \"Hydrographer\"′s hull; the explosion rocked the ship. Aboard \"Guide\" in 1927, tragedy almost struck when a petty officer handling a bomb lit its fuse and then fell when the ship lurched; he dropped the bomb, which rolled into a gutter. The petty officer fell again before finally reaching the bomb and heaving it overboard just in time; it exploded alongside the ship just as it hit the water. The concussion prompted half the crew to rush up from below decks to find out what had happened.\n\nAs late as 1942, radio acoustic ranging remained important enough to the Coast and Geodetic Survey for it to devote just over 100 pages of its \"Hydrographic Manual\" to it. However, World War II, which by then had been raging for three years, gave impetus to the rapid development of purely radio-based navigation systems to assist bombers in finding their targets in darkness and bad weather. Such radio navigation systems were easier to maintain than hydrophone stations and did not require the handling of explosives and, as the new systems matured, the Coast and Geodetic Survey began to apply them to maritime navigation. Radio acoustic ranging appears not to have been used after 1944, and by 1946, Coast and Geodetic Survey ships had switched over to the new SHORAN electronic navigation technology to fix their positions.\n\nThe first non-visual method of precise navigation in human history, and the first that could be used at any time of day or night and in any weather conditions, radio acoustic ranging was a major step forward in the development of modern navigation systems. Nicholas Heck revolutionized oceanic surveying through the use of radio electronic ranging to establish ship locations, one of his major contributions to oceanography. His work related to the technique also helped to develop underwater sound velocity tables allowing the establishment of \"true depths\" of up to using echo sounding.\n\nRadio acoustic ranging was an early step along the path to modern electronic navigation systems, oceanographic telemetering systems, and the development of marine seismic surveying. The technique also laid the groundwork for the development of sonars capable of looking ahead of and to the sides of vessels.\n\nThe Coast and Geodetic Survey's radio-sonobuoys, developed to support radio acoustic ranging, were the ancestors of the sonobuoys used by ships and aircraft in antisubmarine warfare and underwater acoustic research today.\n\n\n"}
{"id": "10063629", "url": "https://en.wikipedia.org/wiki?curid=10063629", "title": "Rank-size distribution", "text": "Rank-size distribution\n\nRank-size distribution is the distribution of size by rank, in decreasing order of size. For example, if a data set consists of items of sizes 5, 100, 5, and 8, the rank-size distribution is 100, 8, 5, 5 (ranks 1 through 4). This is also known as the rank-frequency distribution, when the source data are from a frequency distribution. These are particularly of interest when the data vary significantly in scale, such as city size or word frequency. These distributions frequently follow a power law distribution, or less well-known ones such as a stretched exponential function or parabolic fractal distribution, at least approximately for certain ranges of ranks; see below.\n\nA rank-size distribution is not a probability distribution or cumulative distribution function. Rather, it is a discrete form of a quantile function (inverse cumulative distribution) in reverse order, giving the size of the element at a given rank.\n\nIn the case of city populations, the resulting distribution in a country, a region, or the world will be characterized by its largest city, with other cities decreasing in size respective to it, initially at a rapid rate and then more slowly. This results in a few large cities and a much larger number of cities orders of magnitude smaller. For example, a rank 3 city would have one-third the population of a country's largest city, a rank 4 city would have one-fourth the population of the largest city, and so on.\n\nWhen any log-linear factor is ranked, the ranks follow the Lucas numbers, which consist of the sequentially additive numbers 1, 3, 4, 7, 11, 18, 29, 47, 76, 123, 199, etc. Like the more famous Fibonacci sequence, each number is approximately 1.618 (the Golden ratio) times the preceding number. For example, the third term in the sequence above, 4, is approximately 1.618, or 4.236; the fourth term, 7, is approximately 1.618, or 6.854; the eighth term, 47, is approximately 1.618, or 46.979. With higher values, the figures converge. An equiangular spiral is sometimes used to visualize such sequences.\n\nA rank-size (or rank-frequency) distribution is often segmented into ranges. This is frequently done somewhat arbitrarily or due to external factors, particularly for market segmentation, but can also be due to distinct behavior as rank varies.\n\nMost simply and commonly, a distribution may be split in two, termed the head and tail. If a distribution is broken into three pieces, the third (middle) piece has several terms, generically middle, also belly, torso, and body. These frequently have some adjectives added, most significantly \"long tail\", also \"fat belly\", \"chunky middle\", etc. In more traditional terms, these may be called \"top-tier\", \"mid-tier\", and \"bottom-tier\".\n\nThe relative sizes and weights of these segments (how many ranks in each segment, and what proportion of the total population is in a given segment) qualitatively characterizes a distribution, analogously to the skewness or kurtosis of a probability distribution. Namely: is it dominated by a few top members (head-heavy, like profits in the recorded music industry), or is it dominated by many small members (tail-heavy, like internet search queries), or distributed in some other way? Practically, this determines strategy: where should attention be focused?\n\nThese distinctions may be made for various reasons. For example, they may arise from differing properties of the population, as in the 90–9–1 principle, which posits that in an internet community, 90% of the participants of a community only view content, 9% of the participants edit content, and 1% of the participants actively create new content. As another example, in marketing one may pragmatically consider the head as all members that receive personalized attention, such as personal phone calls; while the tail is everything else, which does not receive personalized attention, for example receiving form letters; and the line is simply as far as resources allow, or where it makes business sense to stop.\n\nPurely quantitatively, a conventional way of splitting a distribution into head and tail is to consider the head to be the first \"p\" portion of ranks, which account for formula_1 of the overall population, as in the 80:20 Pareto principle, where the top 20% (head) comprises 80% of the overall population. The exact cutoff depends on the distribution – each distribution has a single such cutoff point—and for power laws can be computed from the Pareto index.\n\nSegments may arise naturally due to actual changes in behavior of the distribution as rank varies. Most common is the king effect, where behavior of the top handful of items does not fit the pattern of the rest, as illustrated at top for country populations, and above for most common words in English Wikipedia. For higher ranks, behavior may change at some point, and be well-modeled by different relations in different regions; on the whole by a piecewise function. For example, if two different power laws fit better in different regions, one can use a broken power law for the overall relation; the word frequency in English Wikipedia (above) also demonstrates this.\n\nThe Yule–Simon distribution that results from preferential attachment (intuitively, \"the rich get richer\" and \"success breeds success\") simulates a broken power law and has been shown to \"very well capture\" word frequency versus rank distributions. It originated from trying to explain the population verses rank in different species. It has also been show to fit city population versus rank better.\n\nThe rank-size rule (or law), describes the remarkable regularity in many phenomena, including the distribution of city sizes, the sizes of businesses, the sizes of particles (such as sand), the lengths of rivers, the frequencies of word usage, and wealth among individuals.\n\nAll are real-world observations that follow power laws, such as Zipf's law, the Yule distribution, or the Pareto distribution. If one ranks the population size of cities in a given country or in the entire world and calculates the natural logarithm of the rank and of the city population, the resulting graph will show a log-linear pattern. This is the rank-size distribution.\n\nOne study claims that the rank size rule \"works\" because it is a \"shadow\" or coincidental measure of the true phenomenon. The true value of rank size is thus not as an accurate mathematical measure (since other power-law formulas are more accurate, especially at ranks lower than 10) but rather as a handy measure or \"rule of thumb\" to spot power laws. When presented with a ranking of data, is the third-ranked variable approximately one-third the value of the highest-ranked one? Or, conversely, is the highest-ranked variable approximately ten times the value of the tenth-ranked one? If so, the rank size rule has possibly helped spot another power law relationship.\n\nWhile Zipf's law works well in many cases, it tends to not fit the largest cities in many countries; one type of deviation is known as the King effect. A 2002 study found that Zipf's law was rejected for 53 of 73 countries, far more than would be expected based on random chance. The study also found that variations of the Pareto exponent are better explained by political variables than by economic geography variables like proxies for economies of scale or transportation costs. A 2004 study showed that Zipf's law did not work well for the five largest cities in six countries. In the richer countries, the distribution was flatter than predicted. For instance, in the United States, although its largest city, New York City, has more than twice the population of second-place Los Angeles, the two cities' metropolitan areas (also the two largest in the country) are much closer in population. In metropolitan-area population, New York City is only 1.3 times larger than Los Angeles. In other countries, the largest city would dominate much more than expected. For instance, in the Democratic Republic of the Congo, the capital, Kinshasa, is more than eight times larger than the second-largest city, Lubumbashi. When considering the entire distribution of cities, including the smallest ones, the rank-size rule does not hold. Instead, the distribution is log-normal. This follows from Gibrat's law of proportionate growth.\n\nBecause exceptions are so easy to find, the function of the rule for analyzing cities today is to compare the city-systems in different countries. The rank-size rule is a common standard by which urban primacy is established. A distribution such as that in the United States or China does not exhibit a pattern of primacy, but countries with a dominant \"primate city\" clearly vary from the rank-size rule in the opposite manner. Therefore, the rule helps to classify national (or regional) city-systems according to the degree of dominance exhibited by the largest city. Countries with a primate city, for example, have typically had a colonial history that accounts for that city pattern. If a normal city distribution pattern is expected to follow the rank-size rule (i.e. if the rank-size principle correlates with central place theory), then it suggests that those countries or regions with distributions that do not follow the rule have experienced some conditions that have altered the normal distribution pattern. For example—the presence of multiple regions within large nations such as China and the United States tends to favor a pattern in which more large cities appear than would be predicted by the rule. By contrast, small countries that had been connected (e.g. colonially/economically) to much larger areas will exhibit a distribution in which the largest city is much larger than would fit the rule, compared with the other cities—the excessive size of the city theoretically stems from its connection with a larger system rather than the natural hierarchy that central place theory would predict within that one country or region alone.\n\n"}
{"id": "32655756", "url": "https://en.wikipedia.org/wiki?curid=32655756", "title": "Running survey", "text": "Running survey\n\nA running survey is a rough survey made by a vessel while coasting. Bearings to landmarks are taken at intervals as the vessel sails offshore, and are used to fix features on the coast and further inland. Intervening coastal detail is sketched in. \n\nThe method was used by James Cook, and subsequently by navigators who sailed under—or were influenced by—him, including George Vancouver, William Bligh and Matthew Flinders.\n"}
{"id": "226761", "url": "https://en.wikipedia.org/wiki?curid=226761", "title": "Set and drift", "text": "Set and drift\n\nThe term “set and drift” is used to describe external forces that affect a boat and keep it from following an intended course. To understand and calculate set and drift, one needs to first understand currents. Ocean currents are the horizontal movements of water from one location to another. The movement of water is impacted by: meteorological effects, wind, temperature differences and gravity and on occasion earthquakes. Set is referred to as the current’s direction, expressed in true degrees, and the drift is referred to as the current’s speed, which is usually measured in knots. Ignoring set and drift can cause a mariner to get off their desired course, sometimes by hundreds of miles. A mariner needs to be able to steer the ship and compensate for the effects of set and drift upon their vessel while underway. A vessel’s actual course that is travels is referred to as the course over the ground and the current of the ocean alters this course whether pushing it away from its desired course of in the same direction. The vessel’s speed through the water is also referred to as the speed over the ground and the current can affect how fast or slow the vessel moves through the water.\n\nIn order to utilize set and drift in navigation, navigators much first set the course using Dead Reckoning.\n\nA Dead Reckoning, DR, is calculated by using a previously determined position on a chart, and advancing that position based on known or estimated speed over a set amount of time. This can be calculated by using the formula Speed = Distance / Time.\n\nOnce an advance position has been plotted, then set and drift can be factored in. If there is a known set and drift, then the corrections can be applied to the Dead Reckoning position to then get an Estimated Position on a chart.\n\nThe Course Made Good is the direction in which a ship or vessel has traveled with the effects of current, wind, and helmsman ship. If a current is flowing in the same direction as the ships heading, then the Course Made Good remains the same, but the current speed and ship speed are added together. If the speeds are in opposite directions, then the smaller speed is subtracted from the larger speed. If a vessel runs directly against a current or directly with a current, the speed of the vessel and speed of the current can be added or subtracted from each other. Such as, a vessel has a speed of 8 knots through the water and the vessel is traveling with the current, which is at 2 knots, then the speed over ground is 8+2 and the vessel’s true speed is 10 knots. \n\nSet and Drift can be calculated by using a vector diagram and can be drawn and measured on a chart, maneuvering board or even a plain piece of paper.\n\nYou are underway on course 150 degrees true at 8 knots. Your vessel is making good a course of 166 degrees true and a speed made good of 8.8 knots. What is your set and drift? \n\nStep 1. Plot out course of 150 degrees true on Radar Plotting Sheet\n\nStep 2. Measure length of course by using speed of 8 knots and converting into nautical miles via the time, speed, and distance scale\n\nStep 3. Plot the length of speed on Radar Plotting sheet\n\nStep 4. Plot out desired course of 166 degrees true on Radar Plotting Sheet\n\nStep 5. Measure length of course by using speed of 8.8 knots and converting into nautical miles via the time, speed, and distance scale\n\nStep 6. Connect the two ends of the vectors from the current course to the course made good. Thus creating your set and drift vector\n\nStep 7. Using the navigational triangle, place on set and drift vector. Then drag to center crosshairs keeping the same angle and find the degrees on the outer circle\n\nStep 8. Measure the length of the set and drift vector with the compass. Convert from nautical miles into knots using the time, speed, and distance scale.\n\nStep 9. Your set is 230 degrees true at a drift of 2.5 knots\n\nTo use Navigation Triangles, a navigator need two of them to be able to navigate correctly. Navigation Triangles can be used to find Lines of Positions, Dead Reckoning, ranges, Estimated Positions, Running Fixes and so on. They are also used to connect all the types of fixes to be able to determine a course.\n\nDividers can be used for the measurement of lengths of lines on a chart and approximating the lengths of non-linear lines on a chart. Dividers can also be turned into a compass by replacing the metal leg of the divider with a piece of lead. A compass can measure and draw arc and circles on charts and maneuvering boards.\n"}
{"id": "27860462", "url": "https://en.wikipedia.org/wiki?curid=27860462", "title": "Social space", "text": "Social space\n\nA social space is physical or virtual space such as a social center, online social media, or other gathering place where people gather and interact. Some social spaces such as town squares or parks are public places; others such as pubs, websites. or shopping malls are privately owned and regulated.\n\nHenri Lefebvre emphasised that in human society all 'space is social: it involves assigning more or less appropriated places to social relations...social space has thus always been a social product'. \nSocial space becomes thereby a metaphor for the very experience of social life - 'society experienced alternatively as a deterministic environment or force (\"milieu\") and as our very element or beneficent shell (\"ambience\")'. In this sense 'social space spans the dichotomy between \"public\" and \"private\" space...is also linked to subjective and phenomenological space'.\n\nAs metaphor, 'social space contributes a relational rather than an abstract dimension...has received a large variety of attributes, interpretations, and metaphors'. Such 'social space...i[s] an intricate space of obligations, duties, entitlements, prohibitions, debts, affections, insults, allies, contracts, enemies, infatuations, compromises, mutual love, legitimate expectations, and collective ideals'.\n\nFor Lefebvre, 'the family, the school, the workplace, the church, and so on - each possesses an \"appropriate\" space...for a use specified within the social division of labor'. Within such social spaces 'a system of \"adapted\" expectations and responses - rarely articulated as such because they seem obvious - acquire a quasi-natural self-evidence in everyday life and common sense': thus everybody consensually 'knows what he is talking about when he refers to the town hall, the post office, the police station, the grocery store, the bus and the train, train stations, and bistros' - all underlying aspects of 'a \"social space\" as such. an (artificial) edifice of hierarchically ordered institutions, of laws and conventions'.\n\nDefining a stratified morphology as a series of 'discrete units embedded within one another in a definite order', one can see that a distinct 'morphology exists in social space - from the \"room\" or hut to the house and the building; from the building to the group of houses, to the village and the neighbourhood; from the neighbourhood to the city, the region, the nation, and the State...continent [&] planet'.\n\nThe interaction of different levels may be symbiotic or it may be conflictual: as a Michigan cabinet member put it to two Southern colleagues shortly before the American Civil War, '\"I see how it is; you are a Virginian and you are a South Carolinian; I am not a Michigander, I am an American\"'.\n\nFor the individual, as well as the social institution, different levels of social space come to the fore at different times. To a Britisher, for example, '\"we\" sometimes narrows to southern England, sometimes broadens to refer to \"Britain and America\" or \"Europe\" or \"the west\"'. In precisely the same way, 'a resident of Rome may define himself with varying degrees of intensity as a Roman, an Italian, a Catholic, a Christian, a European, a Westerner' - a sequence of stratified social spaces.\n\n'In premodern societies, space and place largely coincided...Modernity increasingly tears space away from place'. Whereas in the premodern 'every thing has its assigned place in social space', postmodernists would proudly proclaim that 'we need to substitute for the magisterial space of the past...a less upright, less Euclidean space where no one would ever be \"in his final place\" '.\n\nThe way 'migration, seen as a metaphor, is everywhere' in postmodernity - 'we are migrants and perhaps hybrids, in but not of any situation in which we find ourselves' - is rooted in the postmodern forms of production of social space.\n\nLefebvre considered globalization as the creation, and superimposition on nature, of 'worldwide-social space...with strong points (the centers) and weaker and dominated bases (the peripheries)'.\n\nEducation, formal and informal, might be described as in large part a process whereby the new recruit to the human race 'must learn to represent the many dimensions of the local social space...through the veil of degraded inputs, chronic ambiguity, and the occasional deliberate deception'. Faced with such intricacies, R. D. Laing concluded that 'it is just as well that man is a social animal, since the sheer complexity and contradiction of the social field in which he has to live is so formidable'.\n\nThe madman, by contrast, is not 'someone to be counted on to know his place': in many ways, 'mental symptoms are wilful situational improprieties'. Whereas in 'public and semi-public places - streets, shops, neighbourhoods, public transportation, and the like -...a fine mesh of obligations obtains which ensures the orderly traffic and co-mingling of participants...many classic symptoms of psychosis are precise and pointed violations of these territorial arrangements'.\n\nLacan considered that 'it would be worthwhile mapping the places in social space that our culture has assigned to these [psychotic] subjects', and saw their difficulties as in part 'the effects of the breakdown produced by the symbolic discordances that characterize the complex structures of civilization': what Goffman termed '\"The Insanity of Place\"'.\n\n\n"}
{"id": "352353", "url": "https://en.wikipedia.org/wiki?curid=352353", "title": "Spirit level", "text": "Spirit level\n\nA spirit level, bubble level or simply a level is an instrument designed to indicate whether a surface is horizontal (level) or vertical (plumb). Different types of spirit levels may be used by carpenters, stonemasons, bricklayers, other building trades workers, surveyors, millwrights and other metalworkers, and in some photographic or videographic work.\n\nEarly tubular spirit levels had very slightly curved glass vials with constant inner diameter at each viewing point. These vials are incompletely filled with a liquid, usually a mercury colored spirit or alcohol, leaving a bubble in the tube. They have a slight upward curve, so that the bubble naturally rests in the center, the highest point. At slight inclinations the bubble travels away from the marked center position. Where a spirit level must also be usable upside-down or on its side, the curved constant-diameter tube is replaced by an uncurved barrel-shaped tube with a slightly larger diameter in its middle.\n\nAlcohols such as ethanol are often used rather than water. Alcohols have low viscosity and surface tension, which allows the bubble to travel the tube quickly and settle accurately with minimal interference with the glass surface. Alcohols also have a much wider liquid temperature range, and won't break the vial as water could due to ice expansion. A colorant such as fluorescein, typically yellow or green, may be added to increase the visibility of the bubble. \n\nA variant of the linear spirit level is the bull's eye level: a circular, flat-bottomed device with the liquid under a slightly convex glass face with a circle at the center. It serves to level a surface across a plane, while the tubular level only does so in the direction of the tube.\n\nTo check the accuracy of a carpenter's type level, a perfectly horizontal surface is not needed. The level is placed on a flat and \"roughly\" level surface and the reading on the bubble tube is noted. This reading indicates to what extent the surface is parallel to the horizontal plane, according to the level, which at this stage is of unknown accuracy. The spirit level is then rotated through 180 degrees in the horizontal plane, and another reading is noted. If the level is accurate, it will indicate the same orientation with respect to the horizontal plane. A difference implies that the level is inaccurate.\n\nAdjustment of the spirit level is performed by successively rotating the level and moving the bubble tube within its housing to take up roughly half of the discrepancy, until the magnitude of the reading remains constant when the level is flipped.\n\nA similar procedure is applied to more sophisticated instruments such as a surveyor's optical level or a theodolite and is a matter of course each time the instrument is set up. In this latter case, the plane of rotation of the instrument is levelled, along with the spirit level. This is done in two horizontal perpendicular directions.\n\nThe sensitivity is an important specification for a spirit level; its accuracy depends on its sensitivity. The sensitivity of a level is given as the change of angle or gradient required to move the bubble by unit distance. If the bubble housing has graduated divisions, then the sensitivity is the angle or gradient change that moves the bubble by one of these divisions. is the usual spacing for graduations; on a surveyor's level, the bubble will move 2 mm when the vial is tilted about 0.005 degree. For a precision machinist level with 2 mm divisions, when the vial is tilted 5 arc seconds the bubble will move one graduation. This is equivalent to movement of measured one foot from the pivot point, referred to as 5 ten-thousandths per foot.\n\nThere are different types of spirit levels for different uses:\n\nA spirit level is usually found on the head of combination squares.\n\n\"Tilting level\", dumpy level or \"automatic level\" are terms used to refer to types of \"leveling instruments\" as used in surveying to measure height differences over larger distances. It has a spirit level mounted on a telescope (perhaps 30 power) with cross-hairs, itself mounted on a tripod. The observer reads height values off two graduated vertical rods, one 'behind' and one 'in front', to obtain the height difference between the ground points on which the rods are resting. Starting from a point with a known elevation and going cross country (successive points being perhaps apart) height differences can be measured cumulatively over long distances and elevations can be calculated. Precise levelling is supposed to give the difference in elevation between two points apart correct to within a few millimeters.\n\nA traditional carpenter's spirit level looks like a short plank of wood and often has a wide body to ensure stability, and that the surface is being measured correctly. In the middle of the spirit level is a small window where the bubble and the tube is mounted. Two notches (or rings) designate where the bubble should be if the surface is level. Often an indicator for a 45 degree inclination is included.\n\nA line level is a level designed to hang on a builder's string line. The body of the level incorporates small hooks to allow it to attach and hang from the string line. The body is lightweight, so as not to weigh down the string line, it is also small in size as the string line in effect \"becomes\" the body; when the level is hung in the center of the string, each \"leg\" of the string line extends the level's plane.\n\nAn engineer's precision level permits leveling items to greater accuracy than a plain spirit level. They are used to level the foundations, or beds of machines to ensure the machine can output workpieces to the accuracy pre-built in the machine.\n\nMelchisédech Thévenot, a French scientist, invented the instrument some time before February 2, 1661. This date can be established from Thevenot's correspondence with scientist Christiaan Huygens. Within a year of this date the inventor circulated details of his invention to others, including Robert Hooke in London and Vincenzo Viviani in Florence. It is occasionally argued that these bubble levels did not come into widespread use until the beginning of the eighteenth century, the earliest surviving examples being from that time, but Adrien Auzout had recommended that the Académie Royale des Sciences take \"levels of the Thevenot type\" on its expedition to Madagascar in 1666. It is very likely that these levels were in use in France and elsewhere long before the turn of the century.\n\nThevenot is often confused with his nephew, the traveler Jean de Thevenot (born 1633; died 1667). There is evidence to suggest that both Huygens and Hooke later laid claim to the invention, although only within their own countries.\n\nThe Fell All-Way precision level, one of the first successful American made bull's eye levels for machine tool use, was invented by William B. Fell, Rockford, Illinois prior to WWII in 1939. The device was unique in that it could be placed on a machine bed and show tilt on the x-y axes simultaneously; eliminating the need to rotate the level 90 degrees. The level was so accurate it was restricted from export during World War II. The device set a new standard of .0005 inches per foot resolution (five ten thousands per foot or five arc seconds tilt). The level's production stopped around 1970. Production restarted in the 1980s by Thomas Butler Technology, Rockford, Illinois, but finally ended in the mid 1990s. However, there are still hundreds of the highly prized devices in existence.\n\nToday level tools are available in most smartphones by using the device's accelerometer. These mobile apps come with various features and easy designs. Also new web standards allow websites to get orientation of devices.\n\nAlternatives include:\n\n\nDigital levels are increasingly common in replacing conventional spirit levels, particularly in civil engineering applications such as traditional building construction and steel structure erection, for on-site angle alignment and leveling tasks. The industry practitioners often refer to those levelling tools as a “construction level”, “heavy duty level”, \"inclinometer\", or “protractor”. These modern electronic levels are (i) capable of displaying precise numeric angles within 360° with high accuracy, (ii) digital readings can be read from a distance with clarity, (iii) affordably priced due to mass adoption. They provide features that traditional levels are unable to match. Typically, these features enable steel beam frames under construction to be precisely aligned and levelled to the required orientation, which is vital to ensure the stability, strength and rigidity of steel structures on sites. Digital levels, embedded with angular MEMS technology effectively improve productivity and quality of many modern civil structures. Some recent models feature waterproof IP65 and impact resistance features for harsh working environments.\n\n\n"}
{"id": "41624490", "url": "https://en.wikipedia.org/wiki?curid=41624490", "title": "Surveying in early America", "text": "Surveying in early America\n\nThe history of surveying in early America included the mapping of large, unknown territories and the layout of the District of Columbia. Several presidents were involved, including George Washington.\n\nGeorge Washington was not only a founding political father of the U.S., he was a founding surveyor of Virginia, as well. At the age of eleven, he inherited Ferry Farm. When George reached school age, instead of a career in the Royal Navy, George went to school to study surveying and geometry. His first surveying tools were from his own storehouse on Ferry Farm. At the age of 17, under the tutelage of Joshua Fry, he surveyed the northern neck of Virginia and became the county surveyor for Culpeper County, Virginia. By the time of the French and Indian War, he had laid out most of northern Virginia, and this knowledge would contribute to his success during the war.\n\nFrom 1747 to 1799, he surveyed 200 tracts of land, and due to his also being a land speculator, he amassed of land.\n\nDuring the Revolutionary War, he appointed the first geographer of the Continental Army, Robert Erskine.\n\nSurveying was not only for the wealthy plantation owners, but the entire new nation needed to be surveyed, and resurveyed. Most of all, the proposed new capital city, bearing Washington’s name, needed to be surveyed. A two-man team would survey what became the District of Columbia in 1791. The first was Benjamin Banneker, a free ex-slave, who learned to read, write, and do math from his grandmother. Banneker would go on to be a leading astronomer, mathematician, clock maker, and most of all, a surveyor. The second man was Andrew Ellicott. He would go on to do several prominent surveys of the area and assist Lewis and Clark in planning their expedition.\n\nPrior to independence, Peter Jefferson, along with his son Thomas Jefferson, were land surveyors for the crown. At this time, surveyors used a system known as the metes and bounds system, which used \"monuments\"; identifiable objects such as rocks, trees etc., as property markers. The surveyor would measure from monument to monument. The major problem with this system was the fact that these monuments were not necessarily permanent. As a result, Thomas Jefferson was involved in the creation the Public Land Survey System. A comparison of county boundaries in the various states graphically displays the difference between the systems, as counties in the Eastern states are irregularly shaped whereas counties in the Midwest tend to be square or rectangular.\n\nNeeding money to pay the debts for the Revolutionary War, Jefferson began selling land in the Northwest Territory in plots of for $2.50 an acre. Soon after, he sold the land in plots of for 1.25 an acre. The NW Territory was surveyed using the Rectangular System. This system used a central point determined by a Principal North-South Meridian Line and an East-West Base line.\n\nJefferson convinced Congress to accept the land deal with Napoleon. As a result of the Louisiana Purchase and Jefferson’s love for nature, Jefferson organized the Lewis and Clark expedition. Andrew Ellicott taught Lewis and Clark how to use a sextant to map their position. Lewis and Clark would leave from Wood River, Illinois and document the wilderness all the way to the Pacific Ocean.\n\nAbraham Lincoln came to New Salem in 1831, and shortly after in 1832, he lost in his bid to become a state representative. The Sangamon County Surveyor, John Calhoun, then offered Lincoln a job as Deputy Surveyor due to the high volume of resurveying.\n\nAs Deputy Surveyor, Lincoln surveyed five towns, four roads, and thirty properties. The first was the plat for Huron, a proposed town North of Springfield that never came to be. The proposal was that county would build a canal to straighten the Sangamon River, but the canal was never built. The last town Lincoln laid out was New Boston, a town at the confluence of the Iowa River and the Mississippi River. Instead of payment for his work, Lincoln had his surveying equipment repossessed and sold. Unknown to Lincoln, Jimmy Short, a friend, bought all of his equipment, his horse, and his saddlebags. Short returned Lincoln’s surveying equipment and later, as president, Lincoln returned the favor by making Short the Indian Agent of the Round Lake Indian Reservation.\n"}
{"id": "3544390", "url": "https://en.wikipedia.org/wiki?curid=3544390", "title": "Tacheometry", "text": "Tacheometry\n\nTacheometry (; from Greek for \"quick measure\"), is a system of rapid surveying, by which the horizontal and vertical positions of points on the earth's surface relative to one another are determined without using a chain or tape, or a separate levelling instrument.\nInstead of the pole formerly employed to mark a point, a staff similar to a level staff is used. This is marked with heights from the base or foot, and is graduated according to the form of tacheometer in use. \n\nThe horizontal distance is inferred from the vertical angle included between two well-defined points on the staff and the known vertical distance between them. Alternatively, also by readings of the staff indicated by two fixed stadia wires in the diaphragm (reticle) of the telescope. The difference of height is computed from the angle of depression or elevation of a fixed point on the staff and the horizontal distance already obtained. \nThe azimuth angle is determined as formerly. Thus all the measurements requisite to locate a point both vertically and horizontally with reference to the point where the tacheometer is centred are determined by an observer at the instrument without any assistance beyond that of a man to hold the staff.\n\nThe ordinary methods of surveying with a theodolite, chain, and levelling instrument are fairly satisfactory when the ground is relatively clear of obstructions and not very precipitous, but it becomes extremely cumbersome when the ground is covered with bush, or broken up by ravines. Chain measurements then become slow and liable to considerable error; the levelling, too, is carried on at great disadvantage in point of speed, though without serious loss of accuracy. These difficulties led to the introduction of tacheometry.\n\nIn western countries, tacheometry is primarily of historical interest in surveying, as professional measurement nowadays is usually carried out using total stations and recorded using data collectors. Location positions are also determined using GNSS. Traditional methods and instruments are still in use in many areas of the world and by users who are not primarily surveyors.\n\nA \"tachymeter\" or \"tacheometer\" is a type of theodolite used for rapid measurements and determines, electronically or electro-optically, the distance to target. The principles of action are similar to those of rangefinders.\n\nOther forms of tacheometry in surveying include the use of stadia rods with theodolites or plane-table alidades. These use stadia marks on the instrument's reticle to measure the distance between two points on the stadia rod (the stadia interval). This is converted to distance from the instrument to the stadia rod by multiplying the stadia interval by the \"stadia interval factor\". If the stadia rod is not at the same elevation as the instrument, the value must be corrected for the angle of elevation between the instrument and the rod.\n\nThe formula most widely used for finding the distances is:\n\nformula_1\n\nHere, formula_2 is the stadia interval (top intercept minus bottom intercept); formula_3 and formula_4 are multiplicative and additive constants. Generally, the instrument is made so that formula_5 and formula_6 exactly, to simplify calculations.\n\nAnother device used in tacheometry is the \"subtense bar\". This is a rigid rod, usually of a material insensitive to change in temperature such as invar, of fixed length (typically two metres). The subtense bar is mounted on a tripod over the station to which the distance is desired. It is brought to level and a small telescope on the bar enables the bar to be oriented perpendicular to the line of sight to the angle measuring station.\n\nA theodolite is used to measure the angle between indicators on the two ends of the subtense bar. The distance from the telescope to the subtense bar is the height of an isosceles triangle formed with the theodolite at the upper vertex and the subtense bar length at its base, determined by trigonometry.\n"}
{"id": "2059246", "url": "https://en.wikipedia.org/wiki?curid=2059246", "title": "Terraserver.com", "text": "Terraserver.com\n\nTerraServer is a commercial website specializing in aerial and satellite imagery which was originally launched in 1997. It is owned and operated by TerraServer.com, Inc. in Raleigh, North Carolina. The company was previously named Aerial Images until May 2002 when the assets of Aerial Images were sold and the company renamed itself TerraServer.com, Inc.\n\nAerial Images was a part of the original project that involved Microsoft and Compaq as a demonstration of the real-world scalability of SQL Server and Microsoft's Windows NT Server. Aerial Images brought in satellite imagery from Sovinformsputnik (the Russian Federal Space Agency) and GeoEye. TerraServer expanded its partnerships and became an image provider for LandVoyage and the DigitalGlobe family of companies: GlobeXplorer and AirPhotoUSA, as well as imagery from the USGS and USDA.\n\nAfter an agreement that ended in January 2000, the operations split into two pieces. Aerial Images kept the TerraServer.com domain name to create a commercial site selling custom selected imagery. Microsoft went forward with their own imagery project, which provides access to United States Geological Survey imagery. The Microsoft project used the TerraServer brand name in a variety of ways including terraserver.homeadvisor.com, terraserver.microsoft.com, and terraserver.msn.com. In 2003, Microsoft rebranded its research service as TerraServer-USA, and then Microsoft Research Maps. There may exist confusion between the two sites, because of the name similarity. However, TerraServer.com, Inc. is the sole owner of the registered trademark TerraServer. The \"TerraServer\" name is a reference to 'Terra', which is Latin for 'earth' or 'land'.\n\nThe TerraServer viewer allows users to select imagery from different dates and of different resolutions. While most of the TerraServer imagery is focused on high resolution, color, satellite and aerial imagery, TerraServer.com also offers Color Infrared Imagery, Panchromatic Imagery, Low Resolution Satellite Imagery, and topographic maps. Subscribers to TerraServer.com gain access to a drawing and measuring tool that can measure surface distance and area on the images. TerraServer also makes a number of image overlays available, including road names, FEMA flood data, earthquake data, landslide data, real estate parcels, parks, schools, hospitals, airports, and zip codes.\n"}
{"id": "8331945", "url": "https://en.wikipedia.org/wiki?curid=8331945", "title": "Ultra-short baseline", "text": "Ultra-short baseline\n\nUSBL (ultra-short baseline, also sometimes known as SSBL for super short base line) is a method of underwater acoustic positioning. A complete USBL system consists of a transceiver, which is mounted on a pole under a ship, and a transponder or responder on the seafloor, on a towfish, or on an ROV. A computer, or \"topside unit\", is used to calculate a position from the ranges and bearings measured by the transceiver.\n\nAn acoustic pulse is transmitted by the transceiver and detected by the subsea transponder, which replies with its own acoustic pulse. This return pulse is detected by the shipboard transceiver. The time from the transmission of the initial acoustic pulse until the reply is detected is measured by the USBL system and is converted into a range.\n\nTo calculate a subsea position, the USBL calculates both a range and an angle from the transceiver to the subsea beacon. Angles are measured by the transceiver, which contains an array of transducers. The transceiver head normally contains three or more transducers separated by a baseline of 10 cm or less. A method called “phase-differencing” within this transducer array is used to calculate the direction to the subsea transponder.\n\nUSBLs have also begun to find use in \"inverted\" (iUSBL) configurations, with the transceiver mounted on an autonomous underwater vehicle, and the transponder on the target. In this case, the \"topside\" processing happens inside the vehicle to allow it to locate the transponder for applications such as automatic docking and target tracking.\n\n\n"}
{"id": "5322293", "url": "https://en.wikipedia.org/wiki?curid=5322293", "title": "Western Australia border", "text": "Western Australia border\n\nThe Western Australian border was originally designated as 129th meridian east longitude (129° east). However, the border marked on the ground is some distance from this line. Kununurra is the closest town to the Western Australian border, being about 25 km west of the border. The closest settlement is Border Village, 1,734 km to the south on the South Australian side of the border, on Eyre Highway.\n\nThe Western Australian (WA) border marked on the ground, is not as straight as it looks on a map. The Northern Territory border with Western Australia and the South Australian border with Western Australia are displaced east–west by approximately 127 metres, due to errors within the limits of surveying technology available in the 1920s.\n\nWhere Western Australia meets the Northern Territory (NT) and South Australia (SA) borders is a 127-metre segment running east–west along the 26th parallel south latitude (26° south).\n\nIn June 1968, two monuments were erected to mark each end of this 127-metre segment, At the easternmost of these concrete border markers all three borders meet, at Surveyor Generals Corner.\n\nIn 1788 Governor Phillip claimed the continent of Australia only as far west as the 135th meridian east (135° east) in accordance with his commission. (26 January 1788 – MAP)\n\nIt has been suggested that the 1788 claim by the British of 135° east was in reference to Spain's claims under the Treaty of Tordesillas. Spain was seen as no longer having an interest in the area. On the other hand, the other signatories to the treaty, the Portuguese still had a presence in Macau and East Timor. Adoption of 135° east as a boundary would minimise provocation of the Portuguese. By 1825, however, Britain was powerful enough and found it convenient to adopt the original line of the Portuguese under the treaty, 129° east.\n\nThe line of 129° east first became a border in Australia as the western border of New South Wales (NSW) in 1825 (16 July 1825 – MAP).\n\nOn 16 July 1825, the western boundary of New South Wales was relocated at 129° east to take in the new settlement at Melville Island.\n\nFrom 1825 to 1829 129° east was the NSW border, except that the settlement of King George's Sound, now Albany, was part of New South Wales from its establishment on 26 December 1826, until 7 March 1831 when it was made part of the Swan River Colony.\n\nFollowing the settlement of the Swan River Colony (SRC) in 1829 (2 May 1829 – MAP), the eastern boundary was declared to be 129° east, that is coinciding with the western boundary of New South Wales at the time.\n\nThe Swan River Colony, started in 1829, was commissioned as the colony of Western Australia in March 1831.\n\nFrom 1829 to 1832 129° east was the SRC/NSW border.\n\nThe name of the Swan River Colony changed to Western Australia in 1832 (6 February 1832 – MAP).\n\nFrom 1832 to 1846 129° east was the WA/NSW border.\n\nIn 1846 the colony of North Australia (NA) was proclaimed by Letters Patent, which was all of New South Wales north of 26° south. (17 February 1846 – Map).\n\nFrom 1846 to 1847 129° east was the WA/NA border north of 26° south and the WA/NSW border south of the 26th parallel.\n\nIn 1847 the colony of North Australia was revoked and reincorporated into New South Wales. (15 April 1847 – MAP).\n\nFrom 1847 to 1860 129° east was once again the WA/NSW border.\n\nIn 1860 South Australia, which had been proclaimed a colony in 1836 (28 December 1836 – MAP), west to the 132° east, changed their western border from 132° east to 129° east (1860 – MAP).\n\nFrom 1860 to 1863 129° east was the WA/NSW border north of 26° south and the WA/SA border south of the 26th parallel.\n\nIn 1863 that part of New South Wales to the north of South Australia was annexed to South Australia by Letters Patent and became known as the Northern Territory of South Australia (NToSA). (6 July 1863 – MAP).\n\nFrom 1863 to 1911 129° east was the WA/NToSA border north of 26° south and the WA/SA border south of the 26th parallel\n\nIn 1911 the Northern Territory (NT) was split off from South Australia to be administered by the Commonwealth. (1 January 1911 – MAP).\n\nFrom 1911 to 1927 129° east was the WA/NT border north of 26° south and the WA/SA border south of the 26th parallel.\n\nIn 1927 the Northern Territory was split into two territories, North Australia (NA) and Central Australia (CA). (1 March 1927 – MAP).\n\nFrom 1927 to 1931 129° east was once again the WA/NA border and WA/CA border, both north of 26° south and the WA/SA border south of the 26th parallel.\n\nIn 1931 North Australia and Central Australia were reunited as the Northern Territory.\n(12 June 1931 – MAP).\n\nFrom 1931 to the present 129° east has been the WA/NT border north of 26° south and the WA/SA border south of the 26th parallel.\n\nFixing the position of the border of Western Australia on the ground has a rich history. In March 1920 the Western Australian Government Astronomer, H. B. Curlewis gave a talk at the WA Museum about the history of the determination of longitude, in relation to using what was at that time a new technology, by using wireless time signals to determine the position of the border between South Australia and Western Australia, as close to the 129th east meridian as possible.\n\nPreliminary work on the border determinations began in November 1920 when the Government Astronomer for South Australia, G.F. Dodwell and the Government Astronomer of Western Australia, H.B. Curlewis met at Deakin, Western Australia on the East-West Trans-Australian Railway.\n\nThe other members of the party were Messrs. Clive Melville Hambidge and J. Crabb, of the Survey Department; Warrant Officer V. D. Bowen, in charge of wireless apparatus lent by the Defence Department; and Mr. C. A. Maddern, of the Adelaide Observatory, all from Adelaide.\n\nConcrete piers for the astronomical observing instruments were erected in readiness for the final determinations that were to be held in 1921. Observations were made for the purpose of testing under field conditions the instruments and methods to be used in 1921.\n\nThis expedition, to determine 129° east on the ground, created worldwide scientific interest and involved the cooperation of the Astronomer Royal and the Royal Observatory, Greenwich, with wireless time signals sent by the French wireless Service, that were transmitted from the at Saint-Genis-Laval, near Lyons, France, between 17 and 24 November 1920. Wireless time signals were also sent from the Adelaide Observatory, transmitted by the Adelaide Radio Station, to enable the beats of the Adelaide sidereal clock to be used as a control on the rate of the chronometer used for the boundary observation.\n\nAfter these initial tests a comprehensive program was then arranged for the second stage of the border determinations, which were to take place during the following year and dates were then set for that to happen, from 20 April to 10 May 1921.\n\nOne of the concrete piers mentioned, which were cubic concrete blocks slightly smaller than a cubic metre, would later be named as the Deakin Pillar (1921), being from where the larger border marker, the Deakin Obelisk (1926), would be set out from.\n\nThe Deakin Pillar is approximately 2.82 km west of the Deakin Obelisk. The Deakin Obelisk was erected as closely as was possible with the technology of 1926 to 129° east.\n\nThe Deakin Obelisk has a copper plug embedded into the top centre of the concrete obelisk, which determines, on the ground, the South Australian border with Western Australia by a line drawn south to the coastline of the Great Australian Bight and north through this point to 26° south.\n\nShortly after the 1921 determinations of the border of South Australia and Western Australia, the Government Astronomer for South Australia, G.F. Dodwell and the Government Astronomer of Western Australia, H.B. Curlewis and party travelled by the State Ship Bambra to the port of Wyndham, Western Australia.\nFrom Wyndham they were guided by Michael Patrick (\"M.P.\") Durack to a point he perceived as the northern boundary between his Argyle Downs Station and Jack Kilfoyle's Rosewood Station, which was also Western Australia's border with the Northern Territory or 129° east. Most of Rosewood station is in the Northern Territory but some distance further south Rosewood also extends into the East Kimberley Region of Western Australia.\n\nFrom the chosen position, two concrete pillars were erected similar to those described above and portable radio masts set up, before the determinations were carried out by the scientists using the same methods of wireless time signals as were used at Deakin.\n\nOne of the concrete pillars erected, which was the one used as the point of the determinations, was marked by the expedition party to show how far east of Greenwich they were in hours, minutes and seconds, and became known as the Austral Pillar.\n\nThe Austral Pillar, the point selected for the scientific determinations of 1921 would later be found to be about 2 km east from the border of 129° east on that part of Rosewood Station, therefore inside the Northern Territory.\n\nThe Kimberley Obelisk was erected as closely as was possible with the technology of 1927 to 129° east. Over several weeks during 1927, a Western Australian survey crew from the WA Department of Lands and Surveys travelled to Wyndham, then to the Austral Pillar site to set out from that point to the border, where they then erected the much more substantial Kimberley Obelisk.\n\nThe Kimberley Obelisk has a copper plug embedded into the top of the concrete obelisk, which officially determines the WA/NT border on the ground, near 129° east, by a line drawn north to the northern coastline near the Joseph Bonaparte Gulf and south through this point at the Kimberley Obelisk to the 26th parallel.\n\n"}
{"id": "3469713", "url": "https://en.wikipedia.org/wiki?curid=3469713", "title": "World Geographic Reference System", "text": "World Geographic Reference System\n\nThe World Geographic Reference System (GEOREF) is a grid-based method of specifying locations on the surface of the Earth. GEOREF is essentially based on the geographic system of latitude and longitude, but using a simpler and more flexible notation. GEOREF was used primarily for air navigation, particularly in military or inter-service applications, but it is rarely seen today. However, GEOREF can be used with any map or chart that has latitude and longitude printed on it.\n\nGEOREF is based on the standard system of latitude and longitude, but uses a simpler and more concise notation. GEOREF divides the Earth's surface into successively smaller quadrangles, with a notation system used to identify each quadrangle within its parent. Unlike latitude/longitude, GEOREF runs in one direction horizontally, east from the 180° meridian; and one direction vertically, north from the South Pole. GEOREF can easily be adapted to give co-ordinates with varying degrees of precision, using a 2–12 character geocode.\n\nGEOREF co-ordinates are defined by successive divisions of the Earth's surface, as follows:\n\n\nThe initial two letters of a GEOREF reference, designating the 15 degree quadrangle, can be omitted, if it is clear which 15 degree quadrangle the reference applies to (e.g., when working within a restricted geographical area).\n\nFor example, on a GEOREF chart, Naval Air Station Patuxent River (38°17′10″N 76°24′42″W) / (38.286108, -76.4291704) is located (to the nearest minute) at position GJPJ3716. \n\nTo locate the position from the coordinates, proceed as follows:\n\n\nThe same co-ordinate shown in 6-digit (1/10 minute) format is GJPJ370160 and in 8-digit (1/100 minute) format is GJPJ37001600.\n\nExtensions to the above notation allow the GEOREF system to be used to designate an area around a reference point. This is achieved by adding an area designation to a base GEOREF co-ordinate. The area designation can be the letter S, to specify the sides of a rectangle (separated by the letter X); or the letter R, to specify the radius of a circle. In both cases the units are nautical miles. In addition, the letter H can be added, followed by an altitude in thousands of feet.\n\nFor example, the reference GJQJ0207S6X8 designates a rectangle centered on Deal Island (GJQJ0207), running east–west and north–south. Designation GJPJ4103R5 means a circle around Point Lookout (GJPJ4103) with a radius of . Designation GJPJ3716H17 means a height of 17,000 feet over GJPJ3716.\n\n\n"}
