{"id": "541317", "url": "https://en.wikipedia.org/wiki?curid=541317", "title": "Alternative technology", "text": "Alternative technology\n\nAlternative technology is a term used to refer to technologies that are more environmentally friendly than the functionally equivalent technologies dominant in current practice.\n\nThe term was coined by Peter Harper, one of the founders of the Centre for Alternative Technology, North Wales (a.k.a. \"The Quarry\"), in Undercurrents (magazine) in the 1970s.\n\nSome \"alternative technologies\" have in the past or may in the future become widely adopted, after which they might no longer be considered \"alternative.\" For example, the use of wind turbines to produce electricity.\n\nAlternative technologies include the following:\n\n\n"}
{"id": "35981916", "url": "https://en.wikipedia.org/wiki?curid=35981916", "title": "Architectural technology", "text": "Architectural technology\n\nArchitectural technology, or building technology, is the application of technology to the design of buildings. It is a component of architecture and building engineering and is sometimes viewed as a distinct discipline or sub-category. New materials and technologies generated new design challenges and construction methods throughout the evolution of building, especially since the advent of industrialisation in the 19th century. Architectural technology is related to the different elements of a building and their interactions, and is closely aligned with advances in building science.\n\nArchitectural technology can be summarised as the \"technical design and expertise used in the application and integration of construction technologies in the building design process.\" or as \"The ability to analyse, synthesise and evaluate building design factors in order to produce efficient and effective technical design solutions which satisfy performance, production and procurement criteria.\"\n\nMany specialists and professionals, consider Vitruvius's theories as the foundations of architectural technology. Vitruvius's attempt to classify building types, styles, materials and construction methods influenced the creation of many disciplines such as civil engineering, structural engineering, architectural technology and other practices which, now and since the 19th century, form a conceptual framework for architectural design.\n\nIn his published research, Stephen Emmitt explains that in our modern society, \"The relationship between building technology and design can be traced back to the Enlightenment and the industrial revolution, a period when advances in technology and science were seen as the way forward, and times of solid faith in progress... As technologies multiply in number and complexity the building profession started to fragment\".\n\nArchitectural technology is a discipline that spans architecture, building science and engineering. It is practiced by architects, architectural technologists, structural engineers, architectural / building engineers and others who develop the design / concept into a buildable reality. Specialist manufacturers who develop products used to construct buildings, are also involved in the discipline.\n\nPaul Nuttall director at ARUP compares the role of the building engineer to the role of the technical architect and architect. The practice of architectural technology cannot be limited to technology itself; it also relates to the space, the building, the light, the environment served and created by the design. The professional in charge of the technical design leads specialist engineers to provide the best technical and engineered solutions for the project.\n\nIn practice, architectural technology is developed, understood and integrated into a building by producing architectural drawings and schedules. Computer technology is now used on all but the simplest building types: in the twentieth century the use of computer aided design (CAD) became mainstream, allowing for highly accurate drawings that can be shared electronically, so that for example the architectural plans can be used as the basis for designing electrical and air handling services. As the design develops, that information can be shared with the whole design team. That process is currently taken to a logical conclusion with the widespread use of Building Information Modeling (BIM), which uses a three dimensional model of the building, created with input from all the disciplines to build up an integrated design.\n\nArchitectural technology is informed by both practical constraints, and building regulations, as well as standards relating to safety, environmental performance, fire resistance, etc.\n\nUntil the twentieth century, the materials used for building were limited to brick, stone, timber and steel to form structures, slate and tiles for roof coverings, lead and sometimes copper for waterproofing details and decorative roofing effects. The Romans used concrete, but it was virtually unknown as a building material until the invention of reinforced concrete in 1849. Modern construction is much more complex, with walls, floors and roofs all built up from many elements to include structure, insulation and waterproofing often as separate layers or elements.\n\n"}
{"id": "28244922", "url": "https://en.wikipedia.org/wiki?curid=28244922", "title": "Creative technology", "text": "Creative technology\n\nCreative technology is a broadly interdisciplinary and transdisciplinary field combining computing, design, art and the humanities. The field of creative technology encompasses art, digital product design, digital media or an advertising and media made with a software-based, electronic and/or data-driven engine. Examples of creative technology include multi-sensory experiences made using computer graphics, video production, digital cinematography, virtual reality, augmented reality, video editing, software engineering, 3D printing, the Internet of Things, CAD/CAM and wearable technology. In the art world, new media art and internet art are examples of work being done in the creative technology field. Performances, interactive installations and other immersive experiences take museum-going to the next level and may serve as research processes for humans' artistic and emotional integration with machines. Some believe that \"creativity has the potential to be revolutionised with technology\", or view the field of creative technology as helping to \"disrupt\" the way people today interact with computers, and usher in a more integrated, immersive experience.\n\nCreative technology facilities may be organized as arts, research or job development entities, such as the UK's Foundation for Art and Creative Technology which has presented hundreds of new media and digital artworks from around the world, or a recently established $20.5m project in Hawaii specializing in film industry job training and workforce development programs which plans to offer robotics, computer labs, recording studios and editing bays, pitched as a \"game-changing\" opportunity to bring new skills and jobs to Kauai. Degrees in this field were designed to address needs for cross-disciplinary interaction and aim to develop lateral thinking skills across more rigidly defined academic areas. Some educators have complained that creative technology tools, though \"widely available\", are difficult to use for young populations. \n\nThe first major corporation to have a corporate officer bearing the title creative technology was The Walt Disney Company, which gave it first to the imagineer, Bran Ferren in 1993, who eventually became Disney's president of creative technology in 1998. At about the same time, the first educational research center in the United States was created to bridge these disciplines across industry, academia and the defense communities, designated the University of Southern California's, Institute for Creative Technologies. The ICT was established with funding by the US Army.\n\nMarketers and advertisers are also looking toward the power of creative technology to re-engage customers. The UK's Marketing Agencies Association is promoting creative technology as a way to build a more connected and personalized engagement with prospective customers, which launched a Creative Technology Initiative in early 2015. Industry associations and developers, arts organizations and agency creatives alike call for more investment in technology, which has lagged behind the sea change in the industry that is introducing more technology into creative fields. Many advertising agencies and other businesses have begun to create internal labs for research in creative technology. For example, Unilever created its Foundry Project as a way for the company to \"embrace the mentality of hacking, deploying and scaling\"; they share their discoveries and view the lab as a way to incorporate technology into the company, drive experimentation and engage with strategic partners. The Adobe Creative Technologies lab collaborated with the MIT Media Lab, one of the most notable endeavors in the creative technology field, to give artists the ability to draw geometric designs with a computer without having to master text-based programming or math.\n\nCreative technology is best seen as the intersection of new technology with creative initiatives such as fashion, art, advertising, media and entertainment. As such, it is a way to make connections between countries seeking to update their culture; a winter 2015 \"Forbes\" article tells of 30 creative technology startups from the UK making the rounds in Singapore, Kuala Lampur and New York City in an effort to raise funds and make connections.\n\n\nProfessionals who work in the field of creative technology tend to have a background as developers and may work in digital or entertainment media, with an advertising agency or in a new electronic product development role. In an advertising agency setting, a professional with a job description including creative technology may be a designer who became interested in technology, or a developer who focuses on the bigger picture of experience design. Department heads in creative technology may be charged with integrating new technologies into the agency's departments, leveraging partnerships with cutting edge providers and platforms. For example, the head of creative technology at Grey Global Group in New York \"created an in-house lab... which highlights new tech each month with exhibits, events and workshops.\" Members of the team may have the ability to both write computer code and build electronics for prototypes.\n\nThe creative technologist job title is likely to refer to a developer who understands the creative process and the world of advertising. The person is actually making and coding and may be building web projects, mobile apps and other digital experiences. They are trying out new concepts and ideas, and modifying; this is recognized as similar to the artistic process but applied to media, advertising and other creative industries. Creative technologists have been referred to as technology-focussed individuals who either sit within or work closely with the creative team, recognizing that siloed departments of technology and design have historically led to bad agency work. Responsibilities described in a 2014 job posting for \"Creative Technologist\" at Google included \"collaborating on the ideation and development of 'never been done before' digital experiences in partnership with top brands and agencies\", and \"contributing to the development of cutting edge prototypes in the field of creative technology\".\n\nA Master or Bachelor degree in creative technologies or creative technology is a broadly interdisciplinary and transdisciplinary course of study combining fields of computer technology, design, art and the humanities. Established as a modern degree addressing needs for cross-disciplinary interaction, one of its fundamental objectives is to develop lateral thinking skills across more rigidly defined academic areas recognized as a valuable component in expanding technological horizons. The Creative Technology & Design (CT&D) subject area at Fashion Institute of Technology offers specialized courses and both credit and non-credit programs. According to FIT's web site, the mission of this transdisciplinary subject area is to elevate students’ understanding of advanced design concepts, as well as their command of cutting-edge technologies. The Creative Technology 2-year portfolio program at Miami Ad School description reads, \"You are a techie with creative passion and talent – or – a creative with a knack for tech...It's about how we integrate machine learning and artificial intelligence into a creative environment\". Creative Technology is also seen as an industry and skill set for the emerging economy, as in this quote by a University of Texas at Austin dean at the opening of a new school at the University presumed to become the largest academic unit in the college: \"The School of Design and Creative Technologies moves UT Austin more assertively into emerging creative, commercial disciplines that are driving culture and economies in the 21st century\".\n\n\n"}
{"id": "13315514", "url": "https://en.wikipedia.org/wiki?curid=13315514", "title": "Design technology", "text": "Design technology\n\nDesign technology, or D.T., is the study, design, development, application, implementation, support and management of computer and non-computer based technologies for the express purpose of communicating product design intent and constructability. Design technology can be applied to the problems encountered in construction, operation and maintenance of a product.\n\nAt times there is cross-over between D.T. and Information Technology, whereas I.T. is primarily focused on overall network infrastructure, hardware & software requirements, and implementation, D.T. is specifically focused on supporting, maintaining and training design and engineering applications and tools and working closely with I.T. to provide necessary infrastructure, for the most effective use of these applications and tools.\n\nWithin the building design, construction and maintenance industry (also known as AEC/O/FM), the product is the building and the role of D.T., is the effective application of technologies within all phases and aspects of building process. D.T. processes have adopted Building Information Modeling (BIM) to quicken construction, design and facilities management using technology. So though D.T. encompasses BIM and Integrated Project Delivery, I.P.D., it is more overarching in its directive and scope and likewise looks for ways to leverage and more effectively utilize C.A.D., Virtual Design & Construction, V.D.C., as well as historical and legacy data and systems.\n\nD.T. is also applicable to industrial and product design and the manufacturing and fabrication processes therein.\n\nThere are formal courses of study in some countries known as design and technology that focus on particular areas. In this case the above definition still remains valid, if for instance one takes the subject textiles technology and replace product in the above definition with textile.\n\n"}
{"id": "58794547", "url": "https://en.wikipedia.org/wiki?curid=58794547", "title": "Direct Autonomous Authentication", "text": "Direct Autonomous Authentication\n\nDirect Autonomous Authentication (DAA) is a cybersecurity platform developed by San Francisco-based technology company Averon.\n\nThe DAA platform enables secure authentication of a mobile user whilst simultaneously preserving privacy of the user.\n\nThe technology was developed in stealth from late 2015, first publicly introduced by Averon in 2018, and featured at the 2018 Consumer Electronics Show as a new technology that combats the increasing threats of cybercrime and consumer account hacking.\nIn contrast to legacy methods of cybersecurity, the DAA platform bypasses end user actions, and rather than focusing on the authentication of a user's device, DAA instead provides autonomous authentication of a user's mobile phone number, since the mobile phone number continues to be associated with the user even when they lose, destroy or upgrade their mobile phone. \n\nThe DAA method uses a proprietary mix of technology developed by Averon that works inside the secure mobile network data pipelines together with encrypted technology already within every smartphone. The combination of these autonomous authentication methods has been described by research analysts as a faster, more secure, and stronger method of cybersecurity than traditional methods.\n\nBlockchain technology incorporated into the DAA platform ensures the privacy of end users. No identifiable personal data is maintained on the platform, therefore public disclosure of one's authentic identity (such as for the purpose of verified social media interactions) is voluntary. DAA technology affords the end user full control over identity disclosure in any given online interaction, which can be controlled by the end user in varying degrees from fully anonymous to fully identified publicly. In cases involving the need for anonymity with regard to an end user's safety, such as in cases of whistleblowers or political activists, the DAA platform's blockchain technology provides a method for both complete anonymity with the option of voluntary verification of limited but often needed data (such as verifying an anonymous user's general location). Thus DAA technology alleviates the heretofore insurmountable challenge of protecting user privacy with the need for authentication.\n\nThe DAA technology platform was designed to be seamlessly adopted for utilization in a wide variety of industries and use cases in which mobile authentication of users is required.\n\nSince its introduction to the market in 2018, the DAA platform has been recognized by a number of industry groups for its innovation, including winning the Gold prize at the 2018 Edison Awards, a Cybersecurity Excellence Award, and a BIG Innovation 2018 Award.\n"}
{"id": "47886", "url": "https://en.wikipedia.org/wiki?curid=47886", "title": "Disruptive innovation", "text": "Disruptive innovation\n\nIn business, a disruptive innovation is an innovation that creates a new market and value network and eventually disrupts an existing market and value network, displacing established market-leading firms, products, and alliances. The term was defined and first analyzed by the American scholar Clayton M. Christensen and his collaborators beginning in 1995, and has been called the most influential business idea of the early 21st century.\n\nNot all innovations are disruptive, even if they are revolutionary. For example, the first automobiles in the late 19th century were not a disruptive innovation, because early automobiles were expensive luxury items that did not disrupt the market for horse-drawn vehicles. The market for transportation essentially remained intact until the debut of the lower-priced Ford Model T in 1908. The \"mass-produced\" automobile was a disruptive innovation, because it changed the transportation market, whereas the first thirty years of automobiles did not.\n\nDisruptive innovations tend to be produced by outsiders and entrepreneurs in startups, rather than existing market-leading companies. The business environment of market leaders does not allow them to pursue disruptive innovations when they first arise, because they are not profitable enough at first and because their development can take scarce resources away from sustaining innovations (which are needed to compete against current competition). A disruptive process can take longer to develop than by the conventional approach and the risk associated to it is higher than the other more incremental or evolutionary forms of innovations, but once it is deployed in the market, it achieves a much faster penetration and higher degree of impact on the established markets.\n\nBeyond business and economics disruptive innovations can also be considered to disrupt complex systems, including economic and business-related aspects.\n\nThe term disruptive technologies was coined by Clayton M. Christensen and introduced in his 1995 article \"Disruptive Technologies: Catching the Wave\", which he cowrote with Joseph Bower. The article is aimed at management executives who make the funding or purchasing decisions in companies, rather than the research community. He describes the term further in his book \"The Innovator's Dilemma\". \"Innovator's Dilemma\" explored the cases of the disk drive industry (which, with its rapid generational change, is to the study of business what fruit flies are to the study of genetics, as Christensen was advised in the 1990s) and the excavating equipment industry (where hydraulic actuation slowly displaced cable-actuated movement). In his sequel with Michael E. Raynor, \"The Innovator's Solution\", Christensen replaced the term \"disruptive technology\" with \"disruptive innovation\" because he recognized that few technologies are intrinsically disruptive or sustaining in character; rather, it is the \"business model\" that the technology enables that creates the disruptive impact. However, Christensen's evolution from a technological focus to a business-modelling focus is central to understanding the evolution of business at the market or industry level. Christensen and Mark W. Johnson, who cofounded the management consulting firm Innosight, described the dynamics of \"business model innovation\" in the 2008 \"Harvard Business Review\" article \"Reinventing Your Business Model\". The concept of disruptive technology continues a long tradition of identifying radical technical change in the study of innovation by economists, and the development of tools for its management at a firm or policy level.\n\nThe term “disruptive innovation” is misleading when it is used to refer to a product or service at one fixed point, rather than to the evolution of that product or service over time.\n\nIn the late 1990s, the automotive sector began to embrace a perspective of \"constructive disruptive technology\" by working with the consultant David E. O'Ryan, whereby the use of current off-the-shelf technology was integrated with newer innovation to create what he called \"an unfair advantage\". The process or technology change as a whole had to be \"constructive\" in improving the current method of manufacturing, yet disruptively impact the whole of the business case model, resulting in a significant reduction of waste, energy, materials, labor, or legacy costs to the user.\n\nIn keeping with the insight that what matters economically is the business model, not the technological sophistication itself, Christensen's theory explains why many disruptive innovations are \"not\" \"advanced technologies\", which a default hypothesis would lead one to expect. Rather, they are often novel combinations of existing off-the-shelf components, applied cleverly to a small, fledgling value network.\n\nOnline news site TechRepublic suggests to end using the term, and similar related terms, being that it is overused jargon as of 2014.\n\nThe current theoretical understanding of disruptive innovation is different from what might be expected by default, an idea that Clayton M. Christensen called the \"technology mudslide hypothesis\". This is the simplistic idea that an established firm fails because it doesn't \"keep up technologically\" with other firms. In this hypothesis, firms are like climbers scrambling upward on crumbling footing, where it takes constant upward-climbing effort just to stay still, and any break from the effort (such as complacency born of profitability) causes a rapid downhill slide. Christensen and colleagues have shown that this simplistic hypothesis is wrong; it doesn't model reality. What they have shown is that good firms are usually aware of the innovations, but their business environment does not allow them to pursue them when they first arise, because they are not profitable enough at first and because their development can take scarce resources away from that of sustaining innovations (which are needed to compete against current competition). In Christensen's terms, a firm's existing \"value networks\" place insufficient value on the disruptive innovation to allow its pursuit by that firm. Meanwhile, start-up firms inhabit different value networks, at least until the day that their disruptive innovation is able to invade the older value network. At that time, the established firm in that network can at best only fend off the market share attack with a me-too entry, for which survival (not thriving) is the only reward.\n\nChristensen defines a disruptive innovation as a product or service designed for a new set of customers.\nChristensen argues that disruptive innovations can hurt successful, well-managed companies that are responsive to their customers and have excellent research and development. These companies tend to ignore the markets most susceptible to disruptive innovations, because the markets have very tight profit margins and are too small to provide a good growth rate to an established (sizable) firm. Thus, disruptive technology provides an example of an instance when the common business-world advice to \"focus on the customer\" (or \"stay close to the customer\", or \"listen to the customer\") can be strategically counterproductive.\n\nWhile Christensen argued that disruptive innovations can hurt successful, well-managed companies, O'Ryan countered that \"constructive\" integration of existing, new, and forward-thinking innovation could improve the economic benefits of these same well-managed companies, once decision-making management understood the systemic benefits as a whole.\nChristensen distinguishes between \"low-end disruption\", which targets customers who do not need the full performance valued by customers at the high end of the market, and \"new-market disruption\", which targets customers who have needs that were previously unserved by existing incumbents.\n\n\"Low-end disruption\" occurs when the rate at which products improve exceeds the rate at which customers can adopt the new performance. Therefore, at some point the performance of the product overshoots the needs of certain customer segments. At this point, a disruptive technology may enter the market and provide a product that has lower performance than the incumbent but that exceeds the requirements of certain segments, thereby gaining a foothold in the market.\n\nIn low-end disruption, the disruptor is focused initially on serving the least profitable customer, who is happy with a good enough product. This type of customer is not willing to pay premium for enhancements in product functionality. Once the disruptor has gained a foothold in this customer segment, it seeks to improve its profit margin. To get higher profit margins, the disruptor needs to enter the segment where the customer is willing to pay a little more for higher quality. To ensure this quality in its product, the disruptor needs to innovate. The incumbent will not do much to retain its share in a not-so-profitable segment, and will move up-market and focus on its more attractive customers. After a number of such encounters, the incumbent is squeezed into smaller markets than it was previously serving. And then, finally, the disruptive technology meets the demands of the most profitable segment and drives the established company out of the market.\n\n\"New market disruption\" occurs when a product fits a new or emerging market segment that is not being served by existing incumbents in the industry.\n\nThe extrapolation of the theory to all aspects of life has been challenged, as has the methodology of relying on selected case studies as the principal form of evidence. Jill Lepore points out that some companies identified by the theory as victims of disruption a decade or more ago, rather than being defunct, remain dominant in their industries today (including Seagate Technology, U.S. Steel, and Bucyrus). Lepore questions whether the theory has been oversold and misapplied, as if it were able to explain everything in every sphere of life, including not just business but education and public institutions.\n\nIn 2009, Milan Zeleny described high technology as disruptive technology and raised the question of what is being disrupted. The answer, according to Zeleny, is the \"support network\" of high technology. For example, introducing electric cars disrupts the support network for gasoline cars (network of gas and service stations). Such disruption is fully expected and therefore effectively resisted by support net owners. In the long run, high (disruptive) technology bypasses, upgrades, or replaces the outdated support network.\nQuestioning the concept of a disruptive technology, Haxell (2012) questions how such technologies get named and framed, pointing out that this is a positioned and retrospective act.\n\nTechnology, being a form of social relationship, always evolves. No technology remains fixed. Technology starts, develops, persists, mutates, stagnates, and declines, just like living organisms. The evolutionary life cycle occurs in the use and development of any technology. A new high-technology core emerges and challenges existing technology support nets (TSNs), which are thus forced to coevolve with it. New versions of the core are designed and fitted into an increasingly appropriate TSN, with smaller and smaller high-technology effects. High technology becomes regular technology, with more efficient versions fitting the same support net. Finally, even the efficiency gains diminish, emphasis shifts to product tertiary attributes (appearance, style), and technology becomes TSN-preserving appropriate technology. This technological equilibrium state becomes established and fixated, resisting being interrupted by a technological mutation; then new high technology appears and the cycle is repeated.\n\nRegarding this evolving process of technology, Christensen said:\nThe World Bank's 2019 World Development Report on \"The Changing Nature of Work\" examines how technology shapes the relative demand for certain skills in labor markets and expands the reach of firms - robotics and digital technologies, for example, enable firms to automate, replacing labor with machines to become more efficient, and innovate, expanding the number of tasks and products. Joseph Bower explained the process of how disruptive technology, through its requisite support net, dramatically transforms a certain industry.\nFor example, the automobile was high technology with respect to the horse carriage; however, it evolved into technology and finally into appropriate technology with a stable, unchanging TSN. The main high-technology advance in the offing is some form of electric car—whether the energy source is the sun, hydrogen, water, air pressure, or traditional charging outlet. Electric cars preceded the gasoline automobile by many decades and are now returning to replace the traditional gasoline automobile. The printing press was a development that changed the way that information was stored, transmitted, and replicated. This allowed empowered authors but it also promoted censorship and information overload in writing technology.\n\nMilan Zeleny described the above phenomenon. He also wrote that:\nSocial media could be considered a disruptive innovation within sports. More specifically, the way that news in sports circulates nowadays versus the pre-internet era where sports news was mainly on T.V., radio, and newspapers. Social media has created a new market for sports that was not around before in the sense that players and fans have instant access to information related to sports.\n\nHigh technology is a technology core that changes the very architecture (structure and organization) of the components of the technology support net. High technology therefore transforms the qualitative nature of the TSN's tasks and their relations, as well as their requisite physical, energy, and information flows. It also affects the skills required, the roles played, and the styles of management and coordination—the organizational culture itself.\n\nThis kind of technology core is different from regular technology core, which preserves the qualitative nature of flows and the structure of the support and only allows users to perform the same tasks in the same way, but faster, more reliably, in larger quantities, or more efficiently. It is also different from appropriate technology core, which preserves the TSN itself with the purpose of technology implementation and allows users to do the same thing in the same way at comparable levels of efficiency, instead of improving the efficiency of performance.\n\nAs for the difference between high technology and low technology, Milan Zeleny once said:\nHowever, not all modern technologies are high technologies. They have to be used as such, function as such, and be embedded in their requisite TSNs. They have to empower the individual because only through the individual can they empower knowledge. Not all information technologies have integrative effects. Some information systems are still designed to improve the traditional hierarchy of command and thus preserve and entrench the existing TSN. The administrative model of management, for instance, further aggravates the division of task and labor, further specializes knowledge, separates management from workers, and concentrates information and knowledge in centers.\n\nAs knowledge surpasses capital, labor, and raw materials as the dominant economic resource, technologies are also starting to reflect this shift. Technologies are rapidly shifting from centralized hierarchies to distributed networks. Nowadays knowledge does not reside in a super-mind, super-book, or super-database, but in a complex relational pattern of networks brought forth to coordinate human action.\n\nIn the practical world, the popularization of personal computers illustrates how knowledge contributes to the ongoing technology innovation. The original centralized concept (one computer, many persons) is a knowledge-defying idea of the prehistory of computing, and its inadequacies and failures have become clearly apparent. The era of personal computing brought powerful computers \"on every desk\" (one person, one computer). This short transitional period was necessary for getting used to the new computing environment, but was inadequate from the vantage point of producing knowledge. Adequate knowledge creation and management come mainly from networking and distributed computing (one person, many computers). Each person's computer must form an access point to the entire computing landscape or ecology through the Internet of other computers, databases, and mainframes, as well as production, distribution, and retailing facilities, and the like. For the first time, technology empowers individuals rather than external hierarchies. It transfers influence and power where it optimally belongs: at the loci of the useful knowledge. Even though hierarchies and bureaucracies do not innovate, free and empowered individuals do; knowledge, innovation, spontaneity, and self-reliance are becoming increasingly valued and promoted.\n\nMajor opportunities according to researchers and consultants\n\n\n"}
{"id": "7118210", "url": "https://en.wikipedia.org/wiki?curid=7118210", "title": "Electrical engineering technology", "text": "Electrical engineering technology\n\nElectrical/Electronics engineering technology (EET) is an engineering technology field that implements and applies the principles of electrical engineering. Like electrical engineering, EET deals with the \"design, application, installation, manufacturing, operation or maintenance of electrical/electronic(s) systems.\" However, EET is a specialized discipline that has more focus on application, theory, and applied design, and implementation, while electrical engineering may focus more of a generalized emphasis on theory and conceptual design. Electrical/Electronic engineering technology is the largest branch of engineering technology and includes a diverse range of sub-disciplines, such as applied design, electronics, embedded systems, control systems, instrumentation, telecommunications, and power systems.\n\nThe Accreditation Board for Engineering and Technology (ABET) is the recognized organization for accrediting both undergraduate engineering and engineering technology programs in the United States.\n\nEET curricula can vary widely by institution type, degree type, program objective, and expected student outcome. Each year, however, ABET publishes a set of minimum criteria that a given EET program (either associate degree or bachelor's degree) must meet in order to maintain its ABET accreditation. These criteria may be classified as either general criteria, which apply to all ABET accredited programs, or as program criteria, which apply to discipline-specific criteria.\n\nAssociate degree programs emphasize the practical field knowledge that is needed to maintain or troubleshoot existing electrical/electronic systems or to build and test new design prototypes.\n\nDiscipline-specific program outcomes include the application of circuit analysis and design, analog and digital electronics, computer programming, associated software, and relevant engineering standards\n\nCoursework must be at a minimum algebra and trigonometry based.\n\nBachelor’s degree programs emphasize the analysis, design, and implementation of electrical/electronic systems. Some programs may focus on a specific sub-discipline, such as control systems or communications systems, while others may take a broader approach, introducing the student to several different sub-disciplines.\n\nMath to differential equations is a minimum requirement for ABET accredited bachelor’s level EET degrees. In addition, graduates must demonstrate an understanding of basic project management skills.\n\nThe United States Department of Commerce classifies the bachelor of science in electrical engineering technology (BSEET) as a STEM undergraduate engineering degree field.\n\nIn many states, recent graduates and students who are close to finishing an undergraduate BSEET degree are qualified to sit-in for the Fundamentals of Engineering exam while those BSEETs who have already gained at least four years’ post-college experience are qualified to sit-in for the Professional Engineer exam for their licensure in the United States. The importance of the licensing board requirements depend upon location, level of education, required years of experience, and the BSEETs sub-discipline are the passageways for becoming a licensed engineer. The knowledge obtained by a TAC/ABET accredited program is one pathway that may help students prepare for and pass the FE/PE exam. For example, in the United States and Canada, \"only a licensed engineer may seal engineering work for public and private clients\".\n\nGraduates of electrical/electronics engineering technology programs work in a wide range of career fields. Some examples include:\n\nElectrical/electronic engineering technicians may have a two-year associate degree and considered craftsman technicians. Eventually, with additional experience and certifications obtained then the craftsman technicians may advance to master craftsman technicians.\n\nElectrical/electronic engineering technologists are broad specialists, rather than central technicians. EETs have a bachelor's degree and are considered applied electrical or electronic engineers because they have electrical engineering concepts to use in their work. Entry-level jobs in electrical or electronics engineering generally require a bachelor's degree in electrical engineering, electronics engineering, or electrical engineering technology.\n\n\n"}
{"id": "525028", "url": "https://en.wikipedia.org/wiki?curid=525028", "title": "High tech", "text": "High tech\n\nHigh technology, often abbreviated to high tech (adjective forms high-technology, high-tech or hi-tech) is technology that is at the cutting edge: the most advanced technology available. The opposite of high tech is \"low technology\", referring to simple, often traditional or mechanical technology; for example, a slide rule is a low-tech calculating device.\n\nThe phrase was used in a 1958 \"The New York Times\" story advocating \"atomic energy\" for Europe: \"... Western Europe, with its dense population and its high technology ...\" Robert Metz used the term in a financial column in 1969: \"Arthur H. Collins of Collins Radio] controls a score of high technology patents in variety of fields.\" and in a 1971 article used the abbreviated form, \"high tech.\"\n\nA widely-used classification of high-technological manufacturing industries is provided by the OECD. It is based on the intensity of research and development activities used in these industries within OECD coutries, resulting in four distinct categories.\n\n"}
{"id": "7796225", "url": "https://en.wikipedia.org/wiki?curid=7796225", "title": "Industrial technology", "text": "Industrial technology\n\nIndustrial technology is the use of engineering and manufacturing technology to make production faster, simpler and more efficient.The industrial technology field employs creative and technically proficient individuals who can help a company achieve efficient and profitable productivity.\n\nIndustrial Technology programs typically include instruction in optimization theory, human factors, organizational behavior, industrial processes, industrial planning procedures, computer applications, and report and presentation preparation.\n\nPlanning and designing manufacturing processes and equipment is a main aspect of being an industrial technologist. An Industrial Technologist is often responsible for implementing certain designs and processes.\n\nThe USA based Association of Technology, Management, and Applied Engineering (ATMAE), accredits selected collegiate programs in Industrial Technology in the USA. An instructor or graduate of an Industrial Technology program may choose to become a Certified Technology Manager (CTM) by sitting for a rigorous exam administered by ATMAE covering Production Planning & Control, Safety, Quality, and Management/Supervision.\n\nATMAE program accreditation is recognized by the Council for Higher Education Accreditation (CHEA) for accrediting Industrial Technology programs. CHEA recognizes ATMAE in the U.S. for accrediting associate, baccalaureate, and master's degree programs in technology, applied technology, engineering technology, and technology-related disciplines delivered by national or regional accredited institutions in\nthe United States.(2011)\n\n\"A career in industrial technology typically entails formal education from an accredited college or university. Opportunities are available to professionals with all levels of education. Those who hold associate degrees typically qualify for entry-level technician and technologist positions, such as in the maintenance and operation of machinery. Bachelor's degree-holders could fill management and engineering positions, such as plant manager, production supervisor and quality systems engineering technologist. A graduate degree in industrial technology could qualify individuals for jobs in research, teaching and upper-level management\".\n\nIndustrial Technology includes wide-ranging subject matter and could be viewed as an amalgamation of industrial engineering and business topics with a focus on practicality and management of technical systems with less focus on actual engineering of those systems.\n\nTypical curriculum at a four-year university might include courses on manufacturing process, technology and impact on society, mechanical and electronic systems, quality assurance and control, materials science, packaging, production and operations management, and manufacturing facility planning and design. In addition, the Industrial Technologist may have exposure to more vocational-style education in the form of courses on CNC manufacturing, welding, and other tools-of-the-trade in manufacturing.\n\nIndustrial technology program graduates obtain a majority of positions which are applied engineering and/or management oriented. Since \"industrial technologist\" is not a common job title in the United States, the actual bachelor's degree or associate degree earned by the individual is obscured by the job title he/she receives. Typical job titles for industrial technologists having a bachelor's degree include quality systems engineering technologist, manufacturing engineering technologist, industrial engineering technologist, plant manager, production supervisor, etc. Typical job titles for industrial technologists having a two-year associate degree include project technologist, manufacturing technologist, process technologist, etc.\n\nA technologist curriculum may focus or specialize in a certain technical area of study. Examples of this includes electronics, manufacturing, construction, graphics, automation/robotics, CADD, nanotechnology, aviation, etc.\n\nA major subject of study is technological development in industry. This has been defined as:\nStudies in this area often employ a multi-disciplinary research methodology and shade off into the wider analysis of business and economic growth (development, performance). The studies are often based on a mixture of industrial field research and desk-based data analysis and aim to be of interest and use to practitioners in business management and investment (etc.) as well as academics. In engineering, construction, textiles, food and drugs, chemicals and petroleum, and other industries, the focus has been on not only the nature and factors facilitating and hampering the introduction and utilization of new technologies but also the impact of new technologies on the production organization (etc.) of firms and various social and other wider aspects of the technological development process.\n\nHow and When Technological development in industry Performed :\n"}
{"id": "1721135", "url": "https://en.wikipedia.org/wiki?curid=1721135", "title": "Low technology", "text": "Low technology\n\nLow technology, often abbreviated low tech (adjective forms low-technology, low-tech, lo-tech) is simple technology, often of a traditional or non-mechanical kind, such as crafts and tools that pre-date the Industrial Revolution. It is the opposite of high technology.\n\nLow technology can typically be practised or fabricated with a minimum of capital investment by an individual or small group of individuals. Also, the knowledge of the practice can be completely comprehended by a single individual, free from increasing specialization and compartmentalization. Low-tech techniques and designs may fall into disuse due to changing socio-economic conditions or priorities.\n\nNote: almost all of the entries in this section should be prefixed by the word \"traditional\".\n\n\n\nNote: home canning is a counter example of a low technology since some of the supplies needed to pursue this skill rely on a global trade network and an existing manufacturing infrastructure.\n\n\nBy federal law in the United States, only those articles produced with little or no use of machinery or tools with complex mechanisms may be stamped with the designation \"hand-wrought\" or \"hand-made\". Lengthy court-battles are currently underway over the precise definition of the terms \"organic\" and \"natural\" as applied to foodstuffs.\n\n\n\n"}
{"id": "31885413", "url": "https://en.wikipedia.org/wiki?curid=31885413", "title": "Marine technology", "text": "Marine technology\n\nMarine technology is defined by WEGEMT (a European association of 40 universities in 17 countries) as \"technologies for the safe use, exploitation, protection of, and intervention in, the marine environment.\" In this regard, according to WEGEMT, the technologies involved in marine technology are the following: naval architecture, marine engineering, ship design, ship building and ship operations; oil and gas exploration, exploitation, and production; hydrodynamics, navigation, sea surface and sub-surface support, underwater technology and engineering; marine resources (including both renewable and non-renewable marine resources); transport logistics and economics; inland, coastal, short sea and deep sea shipping; protection of the marine environment; leisure and safety.\n\nAccording to the Cape Fear Community College of Wilmington, North Carolina, the curriculum for a marine technology program provides practical skills and academic background that are essential in succeeding in the area of marine scientific support. Through a marine technology program, students aspiring to become marine technologists will become proficient in the knowledge and skills required of scientific support technicians. \n\nThe educational preparation includes classroom instructions and practical training aboard ships, such as how to use and maintain electronic navigation devices, physical and chemical measuring instruments, sampling devices, and data acquisition and reduction systems aboard ocean-going and smaller vessels, among other advanced equipment.\n\nAs far as marine technician programs are concerned, students learn hands-on to trouble shoot, service and repair four- and two-stroke outboards, stern drive, rigging, fuel & lube systems, electrical including diesel engines.\n\nMarine technology is related to the marine science and technology industry, also known as maritime commerce. The Executive Office of Housing and Economic Development (EOHED) of the government of Massachusetts in the United States defined marine science and technology industry as any business that deals primarily with or relates to the sea. A marine science industry includes businesses and technologies, research facilities, and higher education learning institutions. Companies and businesses involved in marine science and industry produce products such as ropes used for commercial fishing, undersea robotics, and stabilized sensor systems. The marine science industry has five sub-sectors, namely marine instrumentation and equipment, marine services, marine research and education, marine materials and supply, and shipbuilding and design.\n\n"}
{"id": "156428", "url": "https://en.wikipedia.org/wiki?curid=156428", "title": "Microtechnology", "text": "Microtechnology\n\nMicrotechnology is technology with features near one micrometre (one millionth of a metre, or 10 metre, or 1μm).\n\nAround 1970, scientists learned that by arraying large numbers of microscopic transistors on a single chip, microelectronic circuits could be built that dramatically improved performance, functionality, and reliability, all while reducing cost and increasing volume. This development led to the Information Revolution.\n\nMore recently, scientists have learned that not only electrical devices, but also mechanical devices, may be miniaturized and batch-fabricated, promising the same benefits to the mechanical world as integrated circuit technology has given to the electrical world. While electronics now provide the ‘brains’ for today’s advanced systems and products, micro-mechanical devices can provide the sensors and actuators — the eyes and ears, hands and feet — which interface to the outside world.\n\nToday, micromechanical devices are the key components in a wide range of products such as automobile airbags, ink-jet printers, blood pressure monitors, and projection display systems. It seems clear that in the not-too-distant future these devices will be as pervasive as electronics.\n\nThe term MEMS, for Micro Electro Mechanical Systems, was coined in the 1980s to describe new, sophisticated mechanical systems on a chip, such as micro electric motors, resonators, gears, and so on. Today, the term MEMS in practice is used to refer to any microscopic device with a mechanical function, which can be fabricated in a batch process (for example, an array of microscopic gears fabricated on a microchip would be considered a MEMS device but a tiny laser-machined stent or watch component would not). In Europe, the term MST for Micro System Technology is preferred, and in Japan MEMS are simply referred to as \"micromachines\". The distinctions in these terms are relatively minor and are often used interchangeably. \n\nThough MEMS processes are generally classified into a number of categories – such as surface machining, bulk machining, LIGA, and EFAB – there are indeed thousands of different MEMS processes. Some produce fairly simple geometries, while others offer more complex 3-D geometries and more versatility. A company making accelerometers for airbags would need a completely different design and process to produce an accelerometer for inertial navigation. Changing from an accelerometer to another inertial device such as a gyroscope requires an even greater change in design and process, and most likely a completely different fabrication facility and engineering team.\n\nMEMS technology has generated a tremendous amount of excitement, due to the vast range of important applications where MEMS can offer previously unattainable performance and reliability standards. In an age where everything must be smaller, faster, and cheaper, MEMS offers a compelling solution. MEMS have already had a profound impact on certain applications such as automotive sensors and inkjet printers. The emerging MEMS industry is already a multibillion-dollar market. It is expected to grow rapidly and become one of the major industries of the 21st century. Cahners In-Stat Group has projected sales of MEMS to reach $12B by 2005. The European NEXUS group projects even larger revenues, using a more inclusive definition of MEMS.\n\nMicrotechnology is often constructed using photolithography. Lightwaves are focused through a mask onto a surface. They solidify a chemical film. The soft, unexposed parts of the film are washed away. Then acid etches away the material not protected.\n\nMicrotechnology's most famous success is the integrated circuit. It has also been used to construct micromachinery.\n\nThe following items have been constructed on a scale of 1 micrometre using photolithography:\n\n\n"}
{"id": "59091880", "url": "https://en.wikipedia.org/wiki?curid=59091880", "title": "Multimodal anthropology", "text": "Multimodal anthropology\n\nMultimodal anthropology is an emerging subfield of social cultural anthropology that encompasses anthropological research and knowledge production across multiple traditional and new media platforms and practices including film, video, photography, theatre, design, podcast, mobile apps, interactive games, web-based social networking, immersive 360 video and augmented reality. As characterized in American Anthropologist\",\" multimodal anthropology is an \"anthropology that works across multiple media, but one that also engages in public anthropology and collaborative anthropology through a field of differentially linked media platforms\" (Collins, Durington & Gill). A multimodal approach also encourages anthropologist to reconsider the ways in which they conduct their research, to pay close attention to the role various media technologies and digital devices plays in the lives of their interlocutors, and how they these technologies redefine what fieldwork looks like.\n\nMultimodal anthropology is not a new concept. It has been a fundamental part of anthropological research and fieldwork from the early days of the disciple. Anthropologists have been experimenting with different forms media technologies throughout the twentieth century whenever confronted with the limitation of text-based ethnography. Multimodal is a term that has readily been used since the 1970s in varied disciplines as psychotherapy, phonetics, genetics, literature and medicine to characterize different approaches to carrying out scientific research that involves to a certain degree, thinking outside of the box. In the early 1990s, semioticians used the terms to discuss different forms of communication across different media, eventually including digital media.\n\nTechnological advances in the later part of the twentieth century, the accessibility to photography, film cameras and audio recorders led to the emergence of visual anthropology as a sub discipline dedicated to the study and production of ethnographic photography, film and media. Building in this legacy, multimodal anthropology seeks to expand the boundaries of visual anthropology to incorporate emerging technologies of twenty-first century including mobile networking, social media, geo-mapping, virtual reality, podcasting, interactive design, along with other traditional forms of learning and knowledge production like art and drawing that were often sidelined within visual anthropology, such as interactive gaming, theatre, performance, graphic novels, ethnofiction and experimental ethnography. As Samuel Collins, Matthew Durington and Harjant Gill note in their introductory essay on title \"Multimodality: An Invitation,\" published in American Anthropologist, \"multimodal anthropologies does not attempt – or desire – to supplant visual anthropology. Rather it seeks to include traditional forms of visual anthropology while simultaneously broadening the purview of the discipline to engage in variety of media forms that exist today.\"\n\n"}
{"id": "27352320", "url": "https://en.wikipedia.org/wiki?curid=27352320", "title": "Open-source appropriate technology", "text": "Open-source appropriate technology\n\nOpen-source appropriate technology (OSAT) is appropriate technology developed through the principles of the open-design movement. OSAT refers to, on the one hand, technology designed with special consideration to the environmental, ethical, cultural, social, political, and economic aspects of the community it is intended for. On the other hand, OSAT is developed in the open and licensed in such a way as to allow their designs to be used, modified and distributed freely.\n\nOpen source is a development method for appropriate technology that harnesses the power of distributed peer review and transparency of process. is an example of open-source appropriate technology. There anyone can both learn how to make and use AT free of concerns about patents. At the same time anyone can also add to the collective open-source knowledge base by contributing ideas, observations, experimental data, deployment logs, etc. It has been claimed that the potential for open-source-appropriate technology to drive applied sustainability is enormous. The built in continuous peer-review can result in better quality, higher reliability, and more flexibility than conventional design/patenting of technologies. The free nature of the knowledge also obviously provides lower costs, particularly for those technologies that do not benefit to a large degree from scale of manufacture. Finally, OSAT also enables the end to predatory intellectual property lock-in. This is particularly important in the context of technology focused on relieving suffering and saving lives in the developing world.\n\nThe \"open-source\" model can act as a driver of sustainable development. Reasons include:\n\n\nFor solutions, many researchers, companies, and academics do work on products meant to assist sustainable development. Vinay Gupta has suggested that those developers agree to three principles:\n\n\nThe ethics of information sharing in this context has been explored in depth.\n\n\n\n\nAppropriate technology is designed to promote decentralized, labor-intensive, energy-efficient and environmentally sound businesses. Carroll Pursell says that the movement declined from 1965 to 1985, due to an inability to counter advocates of agribusiness, large private utilities, and multinational construction companies. Recently (2011), several barriers to OSAT deployment have been identified:\n\n"}
{"id": "6504692", "url": "https://en.wikipedia.org/wiki?curid=6504692", "title": "Outline of technology", "text": "Outline of technology\n\nThe following outline is provided as an overview of and topical guide to technology:\n\nTechnology – collection of tools, including machinery, modifications, arrangements and procedures used by humans. Engineering is the discipline that seeks to study and design new technologies. Technologies significantly affect human as well as other animal species' ability to control and adapt to their natural environments.\n\n\n\n\nHistory of technology\n\n\n\n\n\n\nPotential technology of the future includes:\n\nHypothetical technology – \n\nPhilosophy of technology – \n\n\n\n\n\n\n\n\n\n\n\nFictional technology – \n\n\n\n\n\n\n\n"}
{"id": "17062218", "url": "https://en.wikipedia.org/wiki?curid=17062218", "title": "Product teardown", "text": "Product teardown\n\nA product teardown, or simply teardown, is the act of disassembling a product, such that it helps to identify its component parts, chip & system functionality, and component costing information. For products having 'secret' technology, such as the Mikoyan-Gurevich MiG-25, the process may be secret. For others, including consumer electronics, the results are typically disseminated through photographs and component lists so that others can make use of the information without having to disassemble the product themselves. This information is important to designers of semiconductors, displays, batteries, packaging companies, integrated design firms, and semiconductor fabs, and the systems they operate within.\n\nThis information can be of interest to hobbyists, but can also be used commercially by the technical community to find out, for example, what semiconductor components are being utilized in consumer electronic products, such as the Wii video game console or Apple's iPhone. Such knowledge can aid understanding of how the product works, including innovative design features, and can facilitate estimating the bill of materials (BOM). The financial community therefore has an interest in teardowns, as knowing how a company's products are built can help guide a stock valuation. Manufacturers are often not allowed to announce what components are present in a product due to non-disclosure agreements (NDA). Teardowns can also play a part in evidence of use in court and litigation proceedings where a companies parts may have been used without their permission, counterfeited, or to show where intellectual property or patents might be infringed by another firms part or system.\n\nIdentifying semiconductor components in systems has become more difficult over the past years. The most notable change started with Apple's 8GB iPod nano, which were repackaged with Apple branding. This makes it more difficult to identify the actual device manufacturer and function of the component without performing a 'decap' – removing the outer packaging to analyze the die within it. Typically there are markings on the die inside the package that can lead experienced engineers to see who actually created the device and what functionality it performs in the system.\n\n"}
{"id": "29816", "url": "https://en.wikipedia.org/wiki?curid=29816", "title": "Technology", "text": "Technology\n\nTechnology (\"science of craft\", from Greek , \"techne\", \"art, skill, cunning of hand\"; and , \"-logia\") is the collection of techniques, skills, methods, and processes used in the production of goods or services or in the accomplishment of objectives, such as scientific investigation. Technology can be the knowledge of techniques, processes, and the like, or it can be embedded in machines to allow for operation without detailed knowledge of their workings.\n\nThe simplest form of technology is the development and use of basic tools. The prehistoric discovery of how to control fire and the later Neolithic Revolution increased the available sources of food, and the invention of the wheel helped humans to travel in and control their environment. Developments in historic times, including the printing press, the telephone, and the Internet, have lessened physical barriers to communication and allowed humans to interact freely on a global scale.\n\nTechnology has many effects. It has helped develop more advanced economies (including today's global economy) and has allowed the rise of a leisure class. Many technological processes produce unwanted by-products known as pollution and deplete natural resources to the detriment of Earth's environment. Innovations have always influenced the values of a society and raised new questions of the ethics of technology. Examples include the rise of the notion of efficiency in terms of human productivity, and the challenges of bioethics.\n\nPhilosophical debates have arisen over the use of technology, with disagreements over whether technology improves the human condition or worsens it. Neo-Luddism, anarcho-primitivism, and similar reactionary movements criticize the pervasiveness of technology, arguing that it harms the environment and alienates people; proponents of ideologies such as transhumanism and techno-progressivism view continued technological progress as beneficial to society and the human condition.\n\nThe use of the term \"technology\" has changed significantly over the last 200 years. Before the 20th century, the term was uncommon in English, and it was used either to refer to the description or study of the useful arts or to allude to technical education, as in the Massachusetts Institute of Technology (chartered in 1861).\n\nThe term \"technology\" rose to prominence in the 20th century in connection with the Second Industrial Revolution. The term's meanings changed in the early 20th century when American social scientists, beginning with Thorstein Veblen, translated ideas from the German concept of \"\" into \"technology.\" In German and other European languages, a distinction exists between \"technik\" and \"technologie\" that is absent in English, which usually translates both terms as \"technology.\" By the 1930s, \"technology\" referred not only to the study of the industrial arts but to the industrial arts themselves.\n\nIn 1937, the American sociologist Read Bain wrote that \"technology includes all tools, machines, utensils, weapons, instruments, housing, clothing, communicating and transporting devices and the skills by which we produce and use them.\" Bain's definition remains common among scholars today, especially social scientists. Scientists and engineers usually prefer to define technology as applied science, rather than as the things that people make and use. More recently, scholars have borrowed from European philosophers of \"technique\" to extend the meaning of technology to various forms of instrumental reason, as in Foucault's work on technologies of the self (\"techniques de soi\").\n\nDictionaries and scholars have offered a variety of definitions. The \"Merriam-Webster Learner's Dictionary\" offers a definition of the term: \"the use of science in industry, engineering, etc., to invent useful things or to solve problems\" and \"a machine, piece of equipment, method, etc., that is created by technology.\" Ursula Franklin, in her 1989 \"Real World of Technology\" lecture, gave another definition of the concept; it is \"practice, the way we do things around here.\" The term is often used to imply a specific field of technology, or to refer to high technology or just consumer electronics, rather than technology as a whole. Bernard Stiegler, in \"Technics and Time, 1\", defines technology in two ways: as \"the pursuit of life by means other than life,\" and as \"organized inorganic matter.\"\n\nTechnology can be most broadly defined as the entities, both material and immaterial, created by the application of mental and physical effort in order to achieve some value. In this usage, technology refers to tools and machines that may be used to solve real-world problems. It is a far-reaching term that may include simple tools, such as a crowbar or wooden spoon, or more complex machines, such as a space station or particle accelerator. Tools and machines need not be material; virtual technology, such as computer software and business methods, fall under this definition of technology. W. Brian Arthur defines technology in a similarly broad way as \"a means to fulfill a human purpose.\"\n\nThe word \"technology\" can also be used to refer to a collection of techniques. In this context, it is the current state of humanity's knowledge of how to combine resources to produce desired products, to solve problems, fulfill needs, or satisfy wants; it includes technical methods, skills, processes, techniques, tools and raw materials. When combined with another term, such as \"medical technology\" or \"space technology,\" it refers to the state of the respective field's knowledge and tools. \"State-of-the-art technology\" refers to the high technology available to humanity in any field.\nTechnology can be viewed as an activity that forms or changes culture. Additionally, technology is the application of math, science, and the arts for the benefit of life as it is known. A modern example is the rise of communication technology, which has lessened barriers to human interaction and as a result has helped spawn new subcultures; the rise of cyberculture has at its basis the development of the Internet and the computer. Not all technology enhances culture in a creative way; technology can also help facilitate political oppression and war via tools such as guns. As a cultural activity, technology predates both science and engineering, each of which formalize some aspects of technological endeavor.\n\nThe distinction between science, engineering, and technology is not always clear. Science is systematic knowledge of the physical or material world gained through observation and experimentation. Technologies are not usually exclusively products of science, because they have to satisfy requirements such as utility, usability, and safety.\n\nEngineering is the goal-oriented process of designing and making tools and systems to exploit natural phenomena for practical human means, often (but not always) using results and techniques from science. The development of technology may draw upon many fields of knowledge, including scientific, engineering, mathematical, linguistic, and historical knowledge, to achieve some practical result.\n\nTechnology is often a consequence of science and engineering, although technology as a human activity precedes the two fields. For example, science might study the flow of electrons in electrical conductors by using already-existing tools and knowledge. This new-found knowledge may then be used by engineers to create new tools and machines such as semiconductors, computers, and other forms of advanced technology. In this sense, scientists and engineers may both be considered technologists; the three fields are often considered as one for the purposes of research and reference.\n\nThe exact relations between science and technology in particular have been debated by scientists, historians, and policymakers in the late 20th century, in part because the debate can inform the funding of basic and applied science. In the immediate wake of World War II, for example, it was widely considered in the United States that technology was simply \"applied science\" and that to fund basic science was to reap technological results in due time. An articulation of this philosophy could be found explicitly in Vannevar Bush's treatise on postwar science policy, \"Science – The Endless Frontier\": \"New products, new industries, and more jobs require continuous additions to knowledge of the laws of nature ... This essential new knowledge can be obtained only through basic scientific research.\" In the late-1960s, however, this view came under direct attack, leading towards initiatives to fund science for specific tasks (initiatives resisted by the scientific community). The issue remains contentious, though most analysts resist the model that technology simply is a result of scientific research.\n\nThe use of tools by early humans was partly a process of discovery and of evolution. Early humans evolved from a species of foraging hominids which were already bipedal, with a brain mass approximately one third of modern humans. Tool use remained relatively unchanged for most of early human history. Approximately 50,000 years ago, the use of tools and complex set of behaviors emerged, believed by many archaeologists to be connected to the emergence of fully modern language.\n\nHominids started using primitive stone tools millions of years ago. The earliest stone tools were little more than a fractured rock, but approximately 75,000 years ago, pressure flaking provided a way to make much finer work.\n\nThe discovery and utilization of fire, a simple energy source with many profound uses, was a turning point in the technological evolution of humankind. The exact date of its discovery is not known; evidence of burnt animal bones at the Cradle of Humankind suggests that the domestication of fire occurred before 1 Ma; scholarly consensus indicates that \"Homo erectus\" had controlled fire by between 500 and 400 ka. Fire, fueled with wood and charcoal, allowed early humans to cook their food to increase its digestibility, improving its nutrient value and broadening the number of foods that could be eaten.\n\nOther technological advances made during the Paleolithic era were clothing and shelter; the adoption of both technologies cannot be dated exactly, but they were a key to humanity's progress. As the Paleolithic era progressed, dwellings became more sophisticated and more elaborate; as early as 380 ka, humans were constructing temporary wood huts. Clothing, adapted from the fur and hides of hunted animals, helped humanity expand into colder regions; humans began to migrate\nout of Africa by 200 ka and into other continents such as Eurasia.\n\nHuman's technological ascent began in earnest in what is known as the Neolithic Period (\"New Stone Age\"). The invention of polished stone axes was a major advance that allowed forest clearance on a large scale to create farms. This use of polished stone axes increased greatly in the Neolithic, but were originally used in the preceding Mesolithic in some areas such as Ireland. Agriculture fed larger populations, and the transition to sedentism allowed simultaneously raising more children, as infants no longer needed to be carried, as nomadic ones must. Additionally, children could contribute labor to the raising of crops more readily than they could to the hunter-gatherer economy.\n\nWith this increase in population and availability of labor came an increase in labor specialization. What triggered the progression from early Neolithic villages to the first cities, such as Uruk, and the first civilizations, such as Sumer, is not specifically known; however, the emergence of increasingly hierarchical social structures and specialized labor, of trade and war amongst adjacent cultures, and the need for collective action to overcome environmental challenges such as irrigation, are all thought to have played a role.\n\nContinuing improvements led to the furnace and bellows and provided, for the first time, the ability to smelt and forge of gold, copper, silver, and lead native metals found in relatively pure form in nature. The advantages of copper tools over stone, bone, and wooden tools were quickly apparent to early humans, and native copper was probably used from near the beginning of Neolithic times (about 10 ka). Native copper does not naturally occur in large amounts, but copper ores are quite common and some of them produce metal easily when burned in wood or charcoal fires. Eventually, the working of metals led to the discovery of alloys such as bronze and brass (about 4000 BCE). The first uses of iron alloys such as steel dates to around 1800 BCE.\n\nMeanwhile, humans were learning to harness other forms of energy. The earliest known use of wind power is the sailing ship; the earliest record of a ship under sail is that of a Nile boat dating to the 8th millennium BCE. From prehistoric times, Egyptians probably used the power of the annual flooding of the Nile to irrigate their lands, gradually learning to regulate much of it through purposely built irrigation channels and \"catch\" basins. The ancient Sumerians in Mesopotamia used a complex system of canals and levees to divert water from the Tigris and Euphrates rivers for irrigation.\n\nAccording to archaeologists, the wheel was invented around 4000 BCE probably independently and nearly simultaneously in Mesopotamia (in present-day Iraq), the Northern Caucasus (Maykop culture) and Central Europe. Estimates on when this may have occurred range from 5500 to 3000 BCE with most experts putting it closer to 4000 BCE. The oldest artifacts with drawings depicting wheeled carts date from about 3500 BCE; however, the wheel may have been in use for millennia before these drawings were made. More recently, the oldest-known wooden wheel in the world was found in the Ljubljana marshes of Slovenia.\n\nThe invention of the wheel revolutionized trade and war. It did not take long to discover that wheeled wagons could be used to carry heavy loads. The ancient Sumerians used the potter's wheel and may have invented it. A stone pottery wheel found in the city-state of Ur dates to around 3429 BCE, and even older fragments of wheel-thrown pottery have been found in the same area. Fast (rotary) potters' wheels enabled early mass production of pottery, but it was the use of the wheel as a transformer of energy (through water wheels, windmills, and even treadmills) that revolutionized the application of nonhuman power sources. The first two-wheeled carts were derived from travois and were first used in Mesopotamia and Iran in around 3000 BCE.\n\nThe oldest known constructed roadways are the stone-paved streets of the city-state of Ur, dating to circa 4000 BCE and timber roads leading through the swamps of Glastonbury, England, dating to around the same time period. The first long-distance road, which came into use around 3500 BCE, spanned 1,500 miles from the Persian Gulf to the Mediterranean Sea, but was not paved and was only partially maintained. In around 2000 BCE, the Minoans on the Greek island of Crete built a fifty-kilometer (thirty-mile) road leading from the palace of Gortyn on the south side of the island, through the mountains, to the palace of Knossos on the north side of the island. Unlike the earlier road, the Minoan road was completely paved.\n\nAncient Minoan private homes had running water. A bathtub virtually identical to modern ones was unearthed at the Palace of Knossos. Several Minoan private homes also had toilets, which could be flushed by pouring water down the drain. The ancient Romans had many public flush toilets, which emptied into an extensive sewage system. The primary sewer in Rome was the Cloaca Maxima; construction began on it in the sixth century BCE and it is still in use today.\n\nThe ancient Romans also had a complex system of aqueducts, which were used to transport water across long distances. The first Roman aqueduct was built in 312 BCE. The eleventh and final ancient Roman aqueduct was built in 226 CE. Put together, the Roman aqueducts extended over 450 kilometers, but less than seventy kilometers of this was above ground and supported by arches.\n\nInnovations continued through the Middle Ages with innovations such as silk, the horse collar and horseshoes in the first few hundred years after the fall of the Roman Empire. Medieval technology saw the use of simple machines (such as the lever, the screw, and the pulley) being combined to form more complicated tools, such as the wheelbarrow, windmills and clocks. The Renaissance brought forth many of these innovations, including the printing press (which facilitated the greater communication of knowledge), and technology became increasingly associated with science, beginning a cycle of mutual advancement. The advancements in technology in this era allowed a more steady supply of food, followed by the wider availability of consumer goods.\nStarting in the United Kingdom in the 18th century, the Industrial Revolution was a period of great technological discovery, particularly in the areas of agriculture, manufacturing, mining, metallurgy, and transport, driven by the discovery of steam power. Technology took another step in a second industrial revolution with the harnessing of electricity to create such innovations as the electric motor, light bulb, and countless others. Scientific advancement and the discovery of new concepts later allowed for powered flight and advancements in medicine, chemistry, physics, and engineering. The rise in technology has led to skyscrapers and broad urban areas whose inhabitants rely on motors to transport them and their food supply. Communication was also greatly improved with the invention of the telegraph, telephone, radio and television. The late 19th and early 20th centuries saw a revolution in transportation with the invention of the airplane and automobile.\nThe 20th century brought a host of innovations. In physics, the discovery of nuclear fission has led to both nuclear weapons and nuclear power. Computers were also invented and later miniaturized utilizing transistors and integrated circuits. Information technology subsequently led to the creation of the Internet, which ushered in the current Information Age. Humans have also been able to explore space with satellites (later used for telecommunication) and in manned missions going all the way to the moon. In medicine, this era brought innovations such as open-heart surgery and later stem cell therapy along with new medications and treatments.\n\nComplex manufacturing and construction techniques and organizations are needed to make and maintain these new technologies, and entire industries have arisen to support and develop succeeding generations of increasingly more complex tools. Modern technology increasingly relies on training and education – their designers, builders, maintainers, and users often require sophisticated general and specific training. Moreover, these technologies have become so complex that entire fields have been created to support them, including engineering, medicine, and computer science, and other fields have been made more complex, such as construction, transportation, and architecture.\n\nGenerally, technicism is the belief in the utility of technology for improving human societies. Taken to an extreme, technicism \"reflects a fundamental attitude which seeks to control reality, to resolve all problems with the use of scientific–technological methods and tools.\" In other words, human beings will someday be able to master all problems and possibly even control the future using technology. Some, such as Stephen V. Monsma, connect these ideas to the abdication of religion as a higher moral authority.\n\nOptimistic assumptions are made by proponents of ideologies such as transhumanism and singularitarianism, which view technological development as generally having beneficial effects for the society and the human condition. In these ideologies, technological development is morally good.\n\nTranshumanists generally believe that the point of technology is to overcome barriers, and that what we commonly refer to as the human condition is just another barrier to be surpassed.\n\nSingularitarians believe in some sort of \"accelerating change\"; that the rate of technological progress accelerates as we obtain more technology, and that this will culminate in a \"Singularity\" after artificial general intelligence is invented in which progress is nearly infinite; hence the term. Estimates for the date of this Singularity vary, but prominent futurist Ray Kurzweil estimates the Singularity will occur in 2045.\n\nKurzweil is also known for his history of the universe in six epochs: (1) the physical/chemical epoch, (2) the life epoch, (3) the human/brain epoch, (4) the technology epoch, (5) the artificial intelligence epoch, and (6) the universal colonization epoch. Going from one epoch to the next is a Singularity in its own right, and a period of speeding up precedes it. Each epoch takes a shorter time, which means the whole history of the universe is one giant Singularity event.\n\nSome critics see these ideologies as examples of scientism and techno-utopianism and fear the notion of human enhancement and technological singularity which they support. Some have described Karl Marx as a techno-optimist.\n\nOn the somewhat skeptical side are certain philosophers like Herbert Marcuse and John Zerzan, who believe that technological societies are inherently flawed. They suggest that the inevitable result of such a society is to become evermore technological at the cost of freedom and psychological health.\n\nMany, such as the Luddites and prominent philosopher Martin Heidegger, hold serious, although not entirely, deterministic reservations about technology (see \"The Question Concerning Technology\"). According to Heidegger scholars Hubert Dreyfus and Charles Spinosa, \"Heidegger does not oppose technology. He hopes to reveal the essence of technology in a way that 'in no way confines us to a stultified compulsion to push on blindly with technology or, what comes to the same thing, to rebel helplessly against it.' Indeed, he promises that 'when we once open ourselves expressly to the essence of technology, we find ourselves unexpectedly taken into a freeing claim.' What this entails is a more complex relationship to technology than either techno-optimists or techno-pessimists tend to allow.\"\n\nSome of the most poignant criticisms of technology are found in what are now considered to be dystopian literary classics such as Aldous Huxley's \"Brave New World\", Anthony Burgess's \"A Clockwork Orange\", and George Orwell's \"Nineteen Eighty-Four\". In Goethe's \"Faust\", Faust selling his soul to the devil in return for power over the physical world is also often interpreted as a metaphor for the adoption of industrial technology. More recently, modern works of science fiction such as those by Philip K. Dick and William Gibson and films such as \"Blade Runner\" and \"Ghost in the Shell\" project highly ambivalent or cautionary attitudes toward technology's impact on human society and identity.\n\nThe late cultural critic Neil Postman distinguished tool-using societies from technological societies and from what he called \"technopolies,\" societies that are dominated by the ideology of technological and scientific progress to the exclusion or harm of other cultural practices, values, and world-views.\n\nDarin Barney has written about technology's impact on practices of citizenship and democratic culture, suggesting that technology can be construed as (1) an object of political debate, (2) a means or medium of discussion, and (3) a setting for democratic deliberation and citizenship. As a setting for democratic culture, Barney suggests that technology tends to make ethical questions, including the question of what a good life consists in, nearly impossible because they already give an answer to the question: a good life is one that includes the use of more and more technology.\n\nNikolas Kompridis has also written about the dangers of new technology, such as genetic engineering, nanotechnology, synthetic biology, and robotics. He warns that these technologies introduce unprecedented new challenges to human beings, including the possibility of the permanent alteration of our biological nature. These concerns are shared by other philosophers, scientists and public intellectuals who have written about similar issues (e.g. Francis Fukuyama, Jürgen Habermas, William Joy, and Michael Sandel).\n\nAnother prominent critic of technology is Hubert Dreyfus, who has published books such as \"On the Internet\" and \"What Computers Still Can't Do\".\n\nA more infamous anti-technological treatise is \"\", written by the Unabomber Ted Kaczynski and printed in several major newspapers (and later books) as part of an effort to end his bombing campaign of the techno-industrial infrastructure. There are also subcultures that disapprove of some or most technology, such as self-identified off-gridders.\n\nThe notion of appropriate technology was developed in the 20th century by thinkers such as E.F. Schumacher and Jacques Ellul to describe situations where it was not desirable to use very new technologies or those that required access to some centralized infrastructure or parts or skills imported from elsewhere. The ecovillage movement emerged in part due to this concern.\n\n\"This section mainly focuses on American concerns even if it can reasonably be generalized to other Western countries. \"\n\nIn his article, Jared Bernstein, a Senior Fellow at the Center on Budget and Policy Priorities, questions the widespread idea that automation, and more broadly, technological advances, have mainly contributed to this growing labor market problem.\nHis thesis appears to be a third way between optimism and skepticism. Essentially, he stands for a neutral approach of the linkage between technology and American issues concerning unemployment and declining wages.\n\nHe uses two main arguments to defend his point.\nFirst, because of recent technological advances, an increasing number of workers are losing their jobs. Yet, scientific evidence fails to clearly demonstrate that technology has displaced so many workers that it has created more problems than it has solved. Indeed, automation threatens repetitive jobs but higher-end jobs are still necessary because they complement technology and manual jobs that \"requires flexibility judgment and common sense\" remain hard to replace with machines. Second, studies have not shown clear links between recent technology advances and the wage trends of the last decades.\n\nTherefore, according to Bernstein, instead of focusing on technology and its hypothetical influences on current American increasing unemployment and declining wages, one needs to worry more about \"bad policy that fails to offset the imbalances in demand, trade, income, and opportunity.\"\n\nFor people who use both the Internet and mobile devices in excessive quantities it is likely for them to experience fatigue and over exhaustion as a result of disruptions in their sleeping patterns. Continuous studies have shown that increased BMI and weight gain are associated with people who spend long hours online and not exercising frequently. Heavy Internet use is also displayed in the school lower grades of those who use it in excessive amounts. It has also been noted that the use of mobile phones whilst driving has increased the occurrence of road accidents — particularly amongst teen drivers. Statistically, teens reportedly have fourfold the amount of road traffic incidents as those who are 20 years or older, and a very high percentage of adolescents write (81%) and read (92%) texts while driving. In this context, mass media and technology have a negative impact on people, on both their mental and physical health.\n\nThomas P. Hughes stated that because technology has been considered as a key way to solve problems, we need to be aware of its complex and varied characters to use it more efficiently. What is the difference between a wheel or a compass and cooking machines such as an oven or a gas stove? Can we consider all of them, only a part of them, or none of them as technologies?\n\nTechnology is often considered too narrowly; according to Hughes, \"Technology is a creative process involving human ingenuity\". This definitio's emphasis on creativity avoids unbounded definitions that may mistakenly include cooking \"technologies,\" but it also highlights the prominent role of humans and therefore their responsibilities for the use of complex technological systems.\n\nYet, because technology is everywhere and has dramatically changed landscapes and societies, Hughes argues that engineers, scientists, and managers have often believed that they can use technology to shape the world as they want. They have often supposed that technology is easily controllable and this assumption has to be thoroughly questioned. For instance, Evgeny Morozov particularly challenges two concepts: \"Internet-centrism\" and \"solutionism.\" Internet-centrism refers to the idea that our society is convinced that the Internet is one of the most stable and coherent forces. Solutionism is the ideology that every social issue can be solved thanks to technology and especially thanks to the internet. In fact, technology intrinsically contains uncertainties and limitations. According to Alexis Madrigal's review of Morozov's theory, to ignore it will lead to “unexpected consequences that could eventually cause more damage than the problems they seek to address.\" Benjamin R. Cohen and Gwen Ottinger also discussed the multivalent effects of technology.\n\nTherefore, recognition of the limitations of technology, and more broadly, scientific knowledge, is needed – especially in cases dealing with environmental justice and health issues. Ottinger continues this reasoning and argues that the ongoing recognition of the limitations of scientific knowledge goes hand in hand with scientists and engineers’ new comprehension of their role. Such an approach of technology and science \"[require] technical professionals to conceive of their roles in the process differently. [They have to consider themselves as] collaborators in research and problem solving rather than simply providers of information and technical solutions.\"\n\nTechnology is properly defined as any application of science to accomplish a function. The science can be leading edge or well established and the function can have high visibility or be significantly more mundane, but it is all technology, and its exploitation is the foundation of all competitive advantage.\n\nTechnology-based planning is what was used to build the US industrial giants before WWII (e.g., Dow, DuPont, GM) and it is what was used to transform the US into a superpower. It was not economic-based planning.\n\nThe use of basic technology is also a feature of other animal species apart from humans. These include primates such as chimpanzees, some dolphin communities, and crows. Considering a more generic perspective of technology as ethology of active environmental conditioning and control, we can also refer to animal examples such as beavers and their dams, or bees and their honeycombs.\n\nThe ability to make and use tools was once considered a defining characteristic of the genus Homo. However, the discovery of tool construction among chimpanzees and related primates has discarded the notion of the use of technology as unique to humans. For example, researchers have observed wild chimpanzees utilising tools for foraging: some of the tools used include leaf sponges, termite fishing probes, pestles and levers. West African chimpanzees also use stone hammers and anvils for cracking nuts, as do capuchin monkeys of Boa Vista, Brazil.\n\nTheories of technology often attempt to predict the future of technology based on the high technology and science of the time. As with all predictions of the future, however, technology's is uncertain.\n\nIn 2005, futurist Ray Kurzweil predicted that the future of technology would mainly consist of an overlapping \"GNR Revolution\" of genetics, nanotechnology and robotics, with robotics being the most important of the three.\n\n"}
{"id": "74007", "url": "https://en.wikipedia.org/wiki?curid=74007", "title": "Technology assessment", "text": "Technology assessment\n\nTechnology assessment (TA, German: , French: ) is a scientific, interactive, and communicative process that aims to contribute to the formation of public and political opinion on societal aspects of science and technology.\n\nTA is the study and evaluation of new technologies. It is based on the conviction that new developments within, and discoveries by, the scientific community are relevant for the world at large rather than just for the scientific experts themselves, and that technological progress can never be free of ethical implications. Also, technology assessment recognizes the fact that scientists normally are not trained ethicists themselves and accordingly ought to be very careful when passing ethical judgement on their own, or their colleagues, new findings, projects, or work in progress.\n\nTechnology assessment assumes a global perspective and is future-oriented, not anti-technological. TA considers its task as an interdisciplinary approach to solving already existing problems and preventing potential damage caused by the uncritical application and the commercialization of new technologies.\n\nTherefore, any results of technology assessment studies must be published, and particular consideration must be given to communication with political decision-makers.\n\nAn important problem concerning technology assessment is the so-called Collingridge dilemma: on the one hand, impacts of new technologies cannot be easily predicted until the technology is extensively developed and widely used; on the other hand, control or change of a technology is difficult as soon as it is widely used.\n\nTechnology assessments, which are a form of cost–benefit analysis, are difficult if not impossible to carry out in an objective manner since subjective decisions and value judgments have to be made regarding a number of complex issues such as (a) the boundaries of the analysis (i.e., what costs are internalized and externalized), (b) the selection of appropriate indicators of potential positive and negative consequences of the new technology, (c) the monetization of non-market values, and (d) a wide range of ethical perspectives. Consequently, most technology assessments are neither objective nor value-neutral exercises but instead are greatly influenced and biased by the values of the most powerful stakeholders, which are in many cases the developers and proponents (i.e., corporations and governments) of new technologies under consideration. In the most extreme view, as expressed by Ian Barbour in '’Technology, Environment, and Human Values'’, technology assessment is \"a one-sided apology for contemporary technology by people with a stake in its continuation.\"\n\nSome of the major fields of TA are: information technology, hydrogen technologies, nuclear technology, molecular nanotechnology, pharmacology, organ transplants, gene technology, artificial intelligence, the Internet and many more. Health technology assessment is related, but profoundly different, despite the similarity in the name.\n\nThe following types of concepts of TA are those that are most visible and practiced. There are, however, a number of further TA forms that are only proposed as concepts in the literature or are the label used by a particular TA institution.\n\nMany TA institutions are members of the European Parliamentary Technology Assessment (EPTA) network, some are working for the STOA panel of the European Parliament and formed the European Technology Assessment Group (ETAG).\n\n\n"}
{"id": "13549505", "url": "https://en.wikipedia.org/wiki?curid=13549505", "title": "Technology fusion", "text": "Technology fusion\n\nTechnology fusion involves a transformation of core technologies through a combination process facilitated by technological advances such as the phone and the Internet, which ensure that labs are no longer isolated. This results in profitable advances that can be made cheaply by combining knowledge from different fields, companies, industries, and geographies. The technological fusion is distinguished from the so-called breakthrough approach, which is the linear technological development that replaces an older generation of technology through its focus on combining existing technologies into hybrid products that can revolutionize markets. \n\nThe fusion of technologies goes beyond mere combination. Fusion is more than complementarism, because it creates a new market and new growth opportunities for each participant in the innovation. It blends incremental improvements from several (often previously separate) fields to create a product.\n\nAn example is the fusion of mechanical and electronic engineering to create mechatronics. There is also the case of fusing chemical and electronics technology to produce the Liquid Crystal display (LCD) technology.\n\n"}
{"id": "58306941", "url": "https://en.wikipedia.org/wiki?curid=58306941", "title": "Technology readiness level", "text": "Technology readiness level\n\nTechnology readiness levels (TRL) are a method of estimating technology maturity of Critical Technology Elements (CTE) of a program during the acquisition process. They are determined during a Technology Readiness Assessment (TRA) that examines program concepts, technology requirements, and demonstrated technology capabilities. TRL are based on a scale from 1 to 9 with 9 being the most mature technology. The use of TRLs enables consistent, uniform discussions of technical maturity across different types of technology. TRL has been in widespread use at NASA since the 1980s where it was originally invented. In 1999 the US Department of Defense was advised by GAO to use the scale for procurement which it did from the early 2000s. By 2008 the scale was also in use at the European Space Agency (ESA) as it is evidenced by their handbook. The European Commission advised EU-funded research and innovation projects to adopt the scale in 2010 which they did from 2014 in its Horizon 2020 program. In 2013 TRL was further canonized by the ISO 16290:2013 standard. A comprehensive approach and discussion about TRLs has been published by the European Association of Research and Technology Organisations (EARTO). Extensive criticism of the adoption of TRL scale by the European Union was published in The Innovation Journal, in that \"concreteness and sophistication of the TRL scale gradually diminished as its usage spread outside its original context (space programs)\".\n\nTechnology Readiness Levels were originally conceived at NASA in 1974 and formally defined in 1989. The original definition included seven levels, but in the 1990s NASA adopted the current nine-level scale that subsequently gained widespread acceptance.\n\nOriginal NASA TRL Definitions (1989)\n\nThe TRL methodology was originated by Stan Sadin at NASA Headquarters in 1974. At that time, Ray Chase was the JPL Propulsion Division representative on the Jupiter Orbiter design team. At the suggestion of Stan Sadin, Mr Chase used this methodology to assess the technology readiness of the proposed JPL Jupiter Orbiter spacecraft design. Later Mr Chase spent a year at NASA Headquarters helping Mr Sadin institutionalize the TRL methodology. Mr Chase joined ANSER in 1978, where he used the TRL methodology to evaluate the technology readiness of proposed Air Force development programs. He published several articles during the 1980s and 90s on reusable launch vehicles utilizing the TRL methodology. These documented an expanded version of the methodology that included design tools, test facilities, and manufacturing readiness on the Air Force Have Not program. The Have Not program manager, Greg Jenkins, and Ray Chase published the expanded version of the TRL methodology, which included design and manufacturing. Leon McKinney and Mr Chase used the expanded version to assess the technology readiness of the ANSER team's Highly Reusable Space Transportation (\"HRST\") concept. ANSER also created an adapted version of the TRL methodology for proposed Homeland Security Agency programs.\n\nThe United States Air Force adopted the use of Technology Readiness Levels in the 1990s.\n\nIn 1995, John C. Mankins, NASA, wrote a paper that discussed NASA's use of TRLs and proposed expanded descriptions for each TRL. In 1999, the United States General Accounting Office produced an influential report that examined the differences in technology transition between the DOD and private industry. It concluded that the DOD takes greater risks and attempts to transition emerging technologies at lesser degrees of maturity than does private industry. The GAO concluded that use of immature technology increased overall program risk. The GAO recommended that the DOD make wider use of Technology Readiness Levels as a means of assessing technology maturity prior to transition. In 2001, the Deputy Under Secretary of Defense for Science and Technology issued a memorandum that endorsed use of TRLs in new major programs. Guidance for assessing technology maturity was incorporated into the \"Defense Acquisition Guidebook\". Subsequently, the DOD developed detailed guidance for using TRLs in the 2003 DOD Technology Readiness Assessment Deskbook.\n\nThe European Space Agency adopted the TRL scale in the mid-2000s. Its handbook closely follows the NASA definition of TRLs. The universal usage of TRL in EU policy was proposed in the final report of the first High Level Expert Group on Key Enabling Technologies, and it was indeed implemented in the subsequent EU framework program, called H2020, running from 2013 to 2020. This means not only space and weapons programs, but everything from nanotechnology to informatics and communication technology.\n\nA \"Technology Readiness Level Calculator\" was developed by the United States Air Force. This tool is a standard set of questions implemented in Microsoft Excel that produces a graphical display of the TRLs achieved. This tool is intended to provide a snapshot of technology maturity at a given point in time.\n\nThe \"Technology Program Management Model\" was developed by the United States Army. The TPMM is a TRL-gated high-fidelity activity model that provides a flexible management tool to assist Technology Managers in planning, managing, and assessing their technologies for successful technology transition. The model provides a core set of activities including systems engineering and program management tasks that are tailored to the technology development and management goals. This approach is comprehensive, yet it consolidates the complex activities that are relevant to the development and transition of a specific technology program into one integrated model.\n\nThe primary purpose of using technology readiness levels is to help management in making decisions concerning the development and transitioning of technology. It should be viewed as one of several tools that are needed to manage the progress of research and development activity within an organization.\n\nAmong the advantages of TRLs:\n\n\nSome of the characteristics of TRLs that limit their utility:\n\n\nCurrent TRL models tend to disregard negative and obsolescence factors. There have been suggestions made for incorporating such factors into assessments.\n\nFor complex technologies that incorporate various development stages, a more detailed scheme called the Technology Readiness Pathway Matrix has been developed going from basic units to applications in society. This tool aims to show that a readiness level of a technology is based on a less linear process but on a more complex pathway through its application in society.. \n\n\n\n"}
