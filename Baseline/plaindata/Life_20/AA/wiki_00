{"id": "32952970", "url": "https://en.wikipedia.org/wiki?curid=32952970", "title": "3D Virtual Creature Evolution", "text": "3D Virtual Creature Evolution\n\n3D Virtual Creature Evolution, abbreviated to 3DVCE, is an artificial evolution simulation program created by Lee Graham. The website is currently down. Its purpose is to visualize and research common themes in body plans and strategies to achieve a fitness function of the artificial organisms generated and maintained by the system in their given environment. The program was inspired by Karl Sims’ 1994 artificial evolution program, Evolved Virtual Creatures. The program is run through volunteers who download the program from the home website and return information from completed simulations. It is currently only available on Windows and in some cases Linux.\n\n3DVCE uses evolutionary algorithms to simulate evolution. The user sets the body plan restrictions (maximum number of segment types, branching segments’ length and depth limits, and size limits) and whether fitness score is scaled in relation to size. Limb interpenetration is also an option. Reproduction / population settings include the size of each population and their run time (how long each individual has to attain a fitness score), percentage of individuals who get to reproduce (tournament size), what percentage sexually or asexually reproduce, and selection type is then determined. Crossover rate determines what percentage of an individual is created via crossover of parents and mutation. Mutation rate in body and brain is then determined. Specific mathematical operations and values can be attributed to the creature’s brain as well.\n\nFitness function is then determined. Artificial organisms’ fitness score is determined by how well they achieve their fitness goal within their evaluation time. Fitness functions include distance traveled, maximum height, average height, “TOG” (determined by amount of time creature is in contact with ground), and “Sphere” (determined by creature’s ability to catch and hold spheres). These goals are not individualized and can be set to specific strengths (from zero, as not having an influence on fitness, to one, or having maximum influence) to determine the fitness goal. What generations the fitness function applies to can also be set. The environment, or “Terrain”, is then determined. This includes a flat plain, bumpy terrain (in which a hill is generated around creature that constantly inclines as distance is traveled from the creature’s spawning point), water (a low gravity simulator, non-functional), and “spheres” (spheres are generated above creature to catch).\n\nEverything in the simulation is viewed from a first person viewpoint. After settings are determined, the first generation is generated from randomly created individuals. All creatures appear at the same spawning point and are made of segments or rectangular prisms connected to others at joints. Colors are assigned to segment types randomly. Segment type is determined by the size and joints a segment has. Colors indicate nothing else than that. These first generation creatures move randomly, with no influence from the fitness goal. Creatures with the largest fitness value reproduce and the following generation is based on this reproduction. Eventually, patterns in the population form and fitness increases even further. Fitness function can be changed during the simulation to simulate environmental changes and individual runs can be duplicated to simulate different lineages or speciation.\n\n3DVCE is not only for evolutionary research. Objects can also be spawned for graphics and simulated physics tests. This includes pre-installed blocks, spheres, grenades, and structures that can either be thrown from camera or generated at a spawning point. Artificial gravity can also be manipulated. Random and archived creatures can also be re-spawned to manipulate or view. Lee Graham has also included a TARDIS in the simulation, which when moved into can teleport the camera back to the original spawning point.\n\nConvergent evolution occurs often in 3DVCE, as similar structures and behaviors of the creatures form to maximize fitness. Two-Armed Jumpers consist of a small core and two large symmetrical \"wings\", and evolve in response to jumping and distance requirement. These creatures propel themselves forward using their limbs by jiggling or flapping them. Jumping Ribbons and Springs consist of a chain of segments and evolve in response to max height and distance. They contract or curl up and stretch out their body to leap into the air. Rolling Ribbons and Springs are very similar to the previous group, except they are often larger and segments are more repetitive. They evolve in response to average height, distance, and TOG (touching the ground). They roll on the ground to propel their head into the air to attain height while still touching the ground. Some simply roll in a horizontal fashion like a cylinder. Single-Joint Powered Creatures have more erratic structures and evolve in response to distance on bumpy terrain. They have one large segment in back which kicks the creature forward, but being poorly balanced they use the rest of their bodies to get back up after stumbling or prevent stumbles altogether.\n\nMany other types of creatures also form that do not necessarily fit the four main groups previously described by Lee Graham. Tall stick-like creatures also evolve to attain maximum height. Some users have been able to fix the water simulator to evolve creatures that swim. Many other creatures evolve that share traits of multiple groups. There are currently over 220 creatures archived on the main website, which can be found on YouTube by visiting the \"Creature Mann\" channel.\n"}
{"id": "985619", "url": "https://en.wikipedia.org/wiki?curid=985619", "title": "Agent-based model", "text": "Agent-based model\n\nAn agent-based model (ABM) is a class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness. Particularly within ecology, ABMs are also called individual-based models (IBMs), and individuals within IBMs may be simpler than fully autonomous agents within ABMs. A review of recent literature on individual-based models, agent-based models, and multiagent systems shows that ABMs are used on non-computing related scientific domains including biology, ecology and social science. Agent-based modeling is related to, but distinct from, the concept of multi-agent systems or multi-agent simulation in that the goal of ABM is to search for explanatory insight into the collective behavior of agents obeying simple rules, typically in natural systems, rather than in designing agents or solving specific practical or engineering problems.\n\nAgent-based models are a kind of microscale model that simulate the simultaneous operations and interactions of multiple agents in an attempt to re-create and predict the appearance of complex phenomena. The process is one of emergence from the lower (micro) level of systems to a higher (macro) level. As such, a key notion is that simple behavioral rules generate complex behavior. This principle, known as K.I.S.S. (\"Keep it simple, stupid\"), is extensively adopted in the modeling community. Another central tenet is that the whole is greater than the sum of the parts. Individual agents are typically characterized as boundedly rational, presumed to be acting in what they perceive as their own interests, such as reproduction, economic benefit, or social status, using heuristics or simple decision-making rules. ABM agents may experience \"learning\", adaptation, and reproduction.\n\nMost agent-based models are composed of: (1) numerous agents specified at various scales (typically referred to as agent-granularity); (2) decision-making heuristics; (3) learning rules or adaptive processes; (4) an interaction topology; and (5) an environment. ABMs are typically implemented as computer simulations, either as custom software, or via ABM toolkits, and this software can be then used to test how changes in individual behaviors will affect the system's emerging overall behavior.\n\nThe idea of agent-based modeling was developed as a relatively simple concept in the late 1940s. Since it requires computation-intensive procedures, it did not become widespread until the 1990s.\n\nThe history of the agent-based model can be traced back to the Von Neumann machine, a theoretical machine capable of reproduction. The device von Neumann proposed would follow precisely detailed instructions to fashion a copy of itself. The concept was then built upon by von Neumann's friend Stanislaw Ulam, also a mathematician; Ulam suggested that the machine be built on paper, as a collection of cells on a grid. The idea intrigued von Neumann, who drew it up—creating the first of the devices later termed cellular automata.\nAnother advance was introduced by the mathematician John Conway. He constructed the well-known Game of Life. Unlike von Neumann's machine, Conway's Game of Life operated by tremendously simple rules in a virtual world in the form of a 2-dimensional checkerboard.\n\nOne of the earliest agent-based models in concept was Thomas Schelling's segregation model, which was discussed in his paper \"Dynamic Models of Segregation\" in 1971. Though Schelling originally used coins and graph paper rather than computers, his models embodied the basic concept of agent-based models as autonomous agents interacting in a shared environment with an observed aggregate, emergent outcome.\n\nIn the early 1980s, Robert Axelrod hosted a tournament of Prisoner's Dilemma strategies and had them interact in an agent-based manner to determine a winner. Axelrod would go on to develop many other agent-based models in the field of political science that examine phenomena from ethnocentrism to the dissemination of culture.\nBy the late 1980s, Craig Reynolds' work on flocking models contributed to the development of some of the first biological agent-based models that contained social characteristics. He tried to model the reality of lively biological agents, known as artificial life, a term coined by Christopher Langton.\n\nThe first use of the word \"agent\" and a definition as it is currently used today is hard to track down. One candidate appears to be John Holland and John H. Miller's 1991 paper \"Artificial Adaptive Agents in Economic Theory\", based on an earlier conference presentation of theirs.\n\nAt the same time, during the 1980s, social scientists, mathematicians, operations researchers, and a scattering of people from other disciplines developed Computational and Mathematical Organization Theory (CMOT). This field grew as a special interest group of The Institute of Management Sciences (TIMS) and its sister society, the Operations Research Society of America (ORSA).\n\nWith the appearance of StarLogo in 1990, Swarm and NetLogo in the mid-1990s and RePast and AnyLogic in 2000, or GAMA in 2007 as well as some custom-designed code, modelling software became widely available and the range of domains that ABM was applied to, grew. Bonabeau (2002) is a good survey of the potential of agent-based modeling as of the time\n\nThe 1990s were especially notable for the expansion of ABM within the social sciences, one notable effort was the large-scale ABM, Sugarscape, developed by\nJoshua M. Epstein and Robert Axtell to simulate and explore the role of social phenomena such as seasonal migrations, pollution, sexual reproduction, combat, and transmission of disease and even culture. Other notable 1990s developments included Carnegie Mellon University's Kathleen Carley ABM, to explore the co-evolution of social networks and culture.\nDuring this 1990s timeframe Nigel Gilbert published the first textbook on Social Simulation: Simulation for the social scientist (1999) and established a journal from the perspective of social sciences: the \"Journal of Artificial Societies and Social Simulation\" (JASSS). Other than JASSS, agent-based models of any discipline are within scope of SpringerOpen journal \"Complex Adaptive Systems Modeling\" (CASM).\n\nThrough the mid-1990s, the social sciences thread of ABM began to focus on such issues as designing effective teams, understanding the communication required for organizational effectiveness, and the behavior of social networks. CMOT—later renamed Computational Analysis of Social and Organizational Systems (CASOS)—incorporated more and more agent-based modeling. Samuelson (2000) is a good brief overview of the early history, and Samuelson (2005) and Samuelson and Macal (2006) trace the more recent developments.\n\nIn the late 1990s, the merger of TIMS and ORSA to form INFORMS, and the move by INFORMS from two meetings each year to one, helped to spur the CMOT group to form a separate society, the North American Association for Computational Social and Organizational Sciences (NAACSOS). Kathleen Carley was a major contributor, especially to models of social networks, obtaining National Science Foundation funding for the annual conference and serving as the first President of NAACSOS. She was succeeded by David Sallach of the University of Chicago and Argonne National Laboratory, and then by Michael Prietula of Emory University. At about the same time NAACSOS began, the European Social Simulation Association (ESSA) and the Pacific Asian Association for Agent-Based Approach in Social Systems Science (PAAA), counterparts of NAACSOS, were organized. As of 2013, these three organizations collaborate internationally. The First World Congress on Social Simulation was held under their joint sponsorship in Kyoto, Japan, in August 2006. The Second World Congress was held in the northern Virginia suburbs of Washington, D.C., in July 2008, with George Mason University taking the lead role in local arrangements.\n\nMore recently, Ron Sun developed methods for basing agent-based simulation on models of human cognition, known as cognitive social simulation. Bill McKelvey, Suzanne Lohmann, Dario Nardi, Dwight Read and others at UCLA have also made significant contributions in organizational behavior and decision-making. Since 2001, UCLA has arranged a conference at Lake Arrowhead, California, that has become another major gathering point for practitioners in this field. In 2014, Sadegh Asgari from Columbia University and his colleagues developed an agent-based model of the construction competitive bidding. While his model was used to analyze the low-bid lump-sum construction bids, it could be applied to other bidding methods with little modifications to the model.\n\nMost computational modeling research describes systems in equilibrium or as moving between equilibria. Agent-based modeling, however, using simple rules, can result in different sorts of complex and interesting behavior. The three ideas central to agent-based models are agents as objects, emergence, and complexity.\n\nAgent-based models consist of dynamically interacting rule-based agents. The systems within which they interact can create real-world-like complexity. Typically agents are\nsituated in space and time and reside in networks or in lattice-like neighborhoods. The location of the agents and their responsive behavior are encoded in algorithmic form in computer programs. In some cases, though not always, the agents may be considered as intelligent and purposeful. In ecological ABM (often referred to as \"individual-based models\" in ecology), agents may, for example, be trees in forest, and would not be considered intelligent, although they may be \"purposeful\" in the sense of optimizing access to a resource (such as water).\nThe modeling process is best described as inductive. The modeler makes those assumptions thought most relevant to the situation at hand and then watches phenomena emerge from the agents' interactions. Sometimes that result is an equilibrium. Sometimes it is an emergent pattern. Sometimes, however, it is an unintelligible mangle.\n\nIn some ways, agent-based models complement traditional analytic methods. Where analytic methods enable humans to characterize the equilibria of a system, agent-based models allow the possibility of generating those equilibria. This generative contribution may be the most mainstream of the potential benefits of agent-based modeling. Agent-based models can explain the emergence of higher-order patterns—network structures of terrorist organizations and the Internet, power-law distributions in the sizes of traffic jams, wars, and stock-market crashes, and social segregation that persists despite populations of tolerant people. Agent-based models also can be used to identify lever points, defined as moments in time in which interventions have extreme consequences, and to distinguish among types of path dependency.\n\nRather than focusing on stable states, many models consider a system's robustness—the ways that complex systems adapt to internal and external pressures so as to maintain their functionalities. The task of harnessing that complexity requires consideration of the agents themselves—their diversity, connectedness, and level of interactions.\n\nRecent work on the Modeling and simulation of Complex Adaptive Systems has demonstrated the need for combining agent-based and complex network based models. describe a framework consisting of four levels of developing models of complex adaptive systems described using several example multidisciplinary case studies:\nOther methods of describing agent-based models include code templates and text-based methods such as the ODD (Overview, Design concepts, and Design Details) protocol.\n\nThe role of the environment where agents live, both macro and micro, is also becoming an important factor in agent-based modelling and simulation work. Simple environment affords simple agents, but complex environments generates diversity of behaviour.\n\nAgent-based modeling has been used extensively in biology, including the analysis of the spread of epidemics, and the threat of biowarfare, biological applications including population dynamics, vegetation ecology, landscape diversity, the growth and decline of ancient civilizations, evolution of ethnocentric behavior, forced displacement/migration, language choice dynamics, cognitive modeling, and biomedical applications including modeling 3D breast tissue formation/morphogenesis, the effects of ionizing radiation on mammary stem cell subpopulation dynamics, inflammation,\nand the human immune system. Agent-based models have also been used for developing decision support systems such as for breast cancer. Agent-based models are increasingly being used to model pharmacological systems in early stage and pre-clinical research to aid in drug development and gain insights into biological systems that would not be possible \"a priori\". Military applications have also been evaluated. Moreover, agent-based models have been recently employed to study molecular-level biological systems.\n\nAgent-based models have been used since the mid-1990s to solve a variety of business and technology problems. Examples of applications include the modeling of organizational behaviour and cognition, team working, supply chain optimization and logistics, modeling of consumer behavior, including word of mouth, social network effects, distributed computing, workforce management, and portfolio management. They have also been used to analyze traffic congestion.\n\nRecently, agent based modelling and simulation has been applied to various domains such as studying the impact of publication venues by researchers in the computer science domain (journals versus conferences). In addition, ABMs have been used to simulate information delivery in ambient assisted environments. A November 2016 article in arXiv analyzed an agent based simulation of posts spread in the Facebook online social network. In the domain of peer-to-peer, ad-hoc and other self-organizing and complex networks, the usefulness of agent based modeling and simulation has been shown. The use of a computer science-based formal specification framework coupled with wireless sensor networks and an agent-based simulation has recently been demonstrated.\n\nAgent based evolutionary search or algorithm is a new research topic for solving complex optimization problems.\n\nPrior to, and in the wake of the financial crisis, interest has grown in ABMs as possible tools for economic analysis. ABMs do not assume the economy can achieve equilibrium and \"representative agents\" are replaced by agents with diverse, dynamic, and interdependent behavior including herding. ABMs take a \"bottom-up\" approach and can generate extremely complex and volatile simulated economies. ABMs can represent unstable systems with crashes and booms that develop out of non-linear (disproportionate) responses to proportionally small changes. A July 2010 article in \"The Economist\" looked at ABMs as alternatives to DSGE models. The journal \"Nature\" also encouraged agent-based modeling with an editorial that suggested ABMs can do a better job of representing financial markets and other economic complexities than standard models along with an essay by J. Doyne Farmer and Duncan Foley that argued ABMs could fulfill both the desires of Keynes to represent a complex economy and of Robert Lucas to construct models based on microfoundations. Farmer and Foley pointed to progress that has been made using ABMs to model parts of an economy, but argued for the creation of a very large model that incorporates low level models. By modeling a complex system of analysts based on three distinct behavioral profiles – imitating, anti-imitating, and indifferent – financial markets were simulated to high accuracy. Results showed a correlation between network morphology and the stock market index.\n\nSince the beginning of the 21st century ABMs have been deployed in architecture and urban planning to evaluate design and to simulate pedestrian flow in the urban environment. There is also a growing field of socio-economic analysis of infrastructure investment impact using ABM's ability to discern systemic impacts upon a socio-economic network.\n\nThe agent-directed simulation (ADS) metaphor distinguishes between two categories, namely \"Systems for Agents\" and \"Agents for Systems.\" Systems for Agents (sometimes referred to as agents systems) are systems implementing agents for the use in engineering, human and social dynamics, military applications, and others. Agents for Systems are divided in two subcategories. Agent-supported systems deal with the use of agents as a support facility to enable computer assistance in problem solving or enhancing cognitive capabilities. Agent-based systems focus on the use of agents for the generation of model behavior in a system evaluation (system studies and analyses).\n\nMany agent-based modeling software are designed for serial von-Neumann computer architectures. This limits the speed and scalability of these systems. A recent development is the use of data-parallel algorithms on Graphics Processing Units GPUs for ABM simulation. The extreme memory bandwidth combined with the sheer number crunching power of multi-processor GPUs has enabled simulation of millions of agents at tens of frames per second.\n\nVerification and validation (V&V) of simulation models is extremely important. Verification involves the model being debugged to ensure it works correctly, whereas validation ensures that the right model has been built. Face validation, sensitivity analysis, calibration and statistical validation have also been demonstrated. A discrete-event simulation framework approach for the validation of agent-based systems has been proposed. A comprehensive resource on empirical validation of agent-based models can be found here.\n\nAs an example of V&V technique, consider VOMAS (virtual overlay multi-agent system), a software engineering based approach, where a virtual overlay multi-agent system is developed alongside the agent-based model. The agents in the multi-agent system are able to gather data by generation of logs as well as provide run-time validation and verification support by watch agents and also agents to check any violation of invariants at run-time. These are set by the Simulation Specialist with help from the SME (subject-matter expert). Muazi et al. also provide an example of using VOMAS for verification and validation of a forest fire simulation model.\n\nVOMAS provides a formal way of validation and verification. To develop a VOMAS, one must design VOMAS agents along with the agents in the actual simulation, preferably from the start. In essence, by the time the simulation model is complete, one can essentially consider it to be one model containing two models:\n\nUnlike all previous work on verification and validation, VOMAS agents ensure that the simulations are validated in-simulation i.e. even during execution. In case of any exceptional situations, which are programmed on the directive of the Simulation Specialist (SS), the VOMAS agents can report them. In addition, the VOMAS agents can be used to log key events for the sake of debugging and subsequent analysis of simulations. In other words, VOMAS allows for a flexible use of any given technique for the sake of verification and validation of an agent-based model in any domain.\n\nDetails of validated agent-based modeling using VOMAS along with several case studies are given in. This thesis also gives details of \"exploratory agent-based modeling\", \"descriptive agent-based modeling\" and \"validated agent-based modeling\", using several worked case study examples.\n\nMathematical models of complex systems are of three types: black-box (phenomenological), white-box (mechanistic, based on the first principles) and grey-box (mixtures of phenomenological and mechanistic models).\n\n\n\n"}
{"id": "1660757", "url": "https://en.wikipedia.org/wiki?curid=1660757", "title": "Animat", "text": "Animat\n\nAnimat (\"third-person indicative present of the Latin verb animō\"), meaning to \"animate, give or bring life\".\n\nAnimats are artificial animals, a contraction of animal-materials. The term includes physical robots and virtual simulations. Animat research, a subset of Artificial Life studies, has become rather popular since Rodney Brooks' seminal paper \"Intelligence without representation\". The word was coined by S.W. Wilson in 1985,\nin \"Knowledge growth in an artificial animal\", published in the first \"Proceedings\" \n\"of an International Conference on Genetic Algorithms and Their Applications\".\n\nAn example using the Animat model as proposed by Wilson is discussed at some length in chapter 9 of Stan Franklin's book, Artificial Minds. In this implementation, the animat is capable of independent learning about its environment through application and evolution of pattern-matching rules called \"taxons\".\n\nIn 2001 Thomas DeMarse performed studies on 'neurally controlled animat'. Every two years, the Society of Adaptive Behaviour meets and produces a proceedings on this topic. They also have a journal, \"Adaptive Behavior\".\n\nAlan H Goldstein has proposed that, because nanobiotechnology is in the process of creating real animal-materials, speculative use of this term should be discouraged and its application become purely phenomenological. Based on the Animat Test (contained in the reference \"I, Nanobot.\") any nonbiological material or entity that exhibits the minimum set of behaviors that define a life form is, de facto, an Animat. Goldstein's basic premise is that in the age of nanobiotechnology it is necessary to follow the chemistry and molecular engineering rather than watching for the emergence of some pre-conceived minimum level of 'intelligence' such as an artificial neural network capable of adaptive phenomena. Goldstein has cautioned that there is a serious disconnect between the fields of nanobiotechnology and A-life based on profound differences in scientific training, experimental systems, and the different sets of terminology (jargon) these two fields have produced. Nanobiotechnologists (really molecular engineers who work with both biological and nonbiological molecules) are generally not concerned with complex systems per se; even when they are building molecular interconnects between such systems, e.g. neuroelectronic splices. A-Life researchers mainly take a systems-level approach. The enormous transformative power of novel molecular engineering has the potential to create Animats, true nonbiological life forms, whose relatively simple behavior would not fit into most standard A-Life paradigms. As a result, Goldstein argues, the first Animats may come into being completely unrecognized by either scientific community.\n\n\n\n"}
{"id": "17404409", "url": "https://en.wikipedia.org/wiki?curid=17404409", "title": "Artificial Life (journal)", "text": "Artificial Life (journal)\n\nArtificial Life is a peer-reviewed scientific journal that covers the study of man-made systems that exhibit the behavioral characteristics of natural living systems. Its articles cover system synthesis in software, hardware, and wetware. \"Artificial Life\" was established in 1993 and is the official journal of the International Society of Artificial Life. It is published online and in hard copy by the MIT Press.\n\n\"Artificial Life\" is abstracted and indexed in Academic Search, Biological Abstracts, BIOSIS Previews, CSA Mechanical & Transportation Engineering Abstracts, Compendex, Current Contents, EMBASE, Excerpta Medica, Inspec, MEDLINE, METADEX, PubMed, Referativny Zhurnal, Science Citation Index Expanded, Scopus, and The Zoological Record. \n\n"}
{"id": "1000441", "url": "https://en.wikipedia.org/wiki?curid=1000441", "title": "Artificial chemistry", "text": "Artificial chemistry\n\nAn artificial chemistry\n\nis a chemical-like system that usually consists of objects, called molecules, that interact according to rules resembling chemical reaction rules. Artificial chemistries are created and studied in order to understand fundamental properties of chemical systems, including prebiotic evolution, as well as for developing chemical computing systems. Artificial chemistry is a field within computer science wherein chemical reactions—often biochemical ones—are computer-simulated, yielding insights on evolution, self-assembly, and other biochemical phenomena. The field does not use actual chemicals, and should not be confused with either synthetic chemistry or computational chemistry. Rather, bits of information are used to represent the starting molecules, and the end products are examined along with the processes that led to them. The field originated in artificial life but has shown to be a versatile method with applications in many fields such as chemistry, economics, sociology and linguistics.\n\nAn artificial chemistry is defined in general as a triple (S,R,A). In some cases it is sufficient to define it as a tuple (S,I).\n\n\n\n\nArtificial chemistries emerged as a sub-field of artificial life, in particular from strong artificial life. The idea behind this field was that if one wanted to build something alive, it had to be done by a combination of non-living entities. For instance, a cell is itself alive, and yet is a combination of non-living molecules. Artificial chemistry enlists, among others, researchers that believe in an extreme bottom-up approach to artificial life. In artificial life, bits of information were used to represent bacteria or members of a species, each of which moved, multiplied, or died in computer simulations. In artificial chemistry bits of information are used to represent starting molecules capable of reacting with one another. The field has pertained to artificial intelligence by virtue of the fact that, over billions of years, non-living matter evolved into primordial life forms which in turn evolved into intelligent life forms.\n\nThe first reference about Artificial Chemistries come from a Technical paper written by John McCaskill\nWalter Fontana working with Leo Buss then took up the work developing the AlChemy model\nThe model was presented at the second International Conference of Artificial Life.\nIn his first papers he presented the concept of organization, as a set of molecules that is algebraically closed and self-maintaining.\nThis concept was further developed by Dittrich and Speroni di Fenizio into a theory of chemical organizations\n\nTwo main schools of artificial chemistries have been in Japan and Germany.\nIn Japan the main researchers have been Takashi Ikegami\n\nHideaki Suzuki\nand Yasuhiro Suzuki\nIn Germany, it was Wolfgang Banzhaf, who, together with his students Peter Dittrich and Jens Ziegler, developed various artificial chemistry models.\nTheir 2001 paper 'Artificial Chemistries - A Review' became a standard in the field.\nJens Ziegler, as part of his PhD thesis, proved that an artificial chemistry could be used to control a small Khepera robot\nAmong other models, Peter Dittrich developed the Seceder model which is able to explain group formation in society through some simple rules. Since then he became a professor in Jena where he investigates artificial chemistries as a way to define a general theory of constructive dynamical systems.\n\nArtificial Chemistries are often used in the study of protobiology, in trying to bridge the gap between chemistry and biology.\nA further motivation to study artificial chemistries is the interest in constructive dynamical systems. Yasuhiro Suzuki has modeled various systems such as membrane systems, signaling pathways (P53), ecosystems, and enzyme systems by using his method, abstract rewriting system on multisets (ARMS).\n\n\n\n"}
{"id": "4061360", "url": "https://en.wikipedia.org/wiki?curid=4061360", "title": "Artificial creation", "text": "Artificial creation\n\nArtificial creation is a field of research that studies the primary synthesis of complex lifelike structures from primordial lifeless origins. \n\nThe field bears some similarity to artificial life, but unlike artificial life, artificial creation focuses on the primary emergence of complex structures and processes of abiogenesis. Artificial creation does not rely exclusively on the application of evolutionary computation and genetic algorithms to optimize artificial creatures or grow synthetic life forms. Artificial creation instead studies systems of rules of interaction, initial conditions and primordial building blocks that can generate complex lifelike structures, based exclusively on repeated application of rules of interaction.\nAn essential difference that distinguishes artificial creation from other related fields is that no explicit fitness function is used to select for fit structures. Structures exist based only on their ability to persist as entities that do not violate the system's rules of interaction. Artificial creation studies the way in which complex emergent properties can arise to form a self-organizing system.\n\nAlthough concepts and elements of artificial creation are represented to some degree in many areas, the field itself is less than a decade old. There are models of self-organizing systems that produce emergent properties in biology, computer science, mathematics, engineering and other fields. Artificial creation differs from these in that it focuses on underlying properties of systems that can generate the endless environmentally interactive complexity of living systems. \n\nOne of the primary impetuses for the exploration of artificial creation comes from the realization in the artificial life and evolutionary computing fields that some basic assumptions common in these fields represent subtle mischaracterizations of natural evolution. These fall into two general classes: 1) there are fundamental problems with the use of fitness functions for the primary synthesis of complex systems, and 2) the process of self-replication used in almost all artificial life research is not a fundamental property of self-organizing systems.\n\nThe concept of a fitness function (or objective function) is central to artificial life systems. The fitness function is used to drive a given artificial evolution process to create synthetic organisms capable of performing a given task. However, fitness functions, even aggregate high-level ones, drive evolving systems to particular solutions and place a ceiling on innovation and complexity. Natural evolution does not drive organisms toward particular solutions to problems. There is no motive force for evolution in our universe beyond that of adherence to physical law. This is a subtle point: when modern organisms are viewed in a local context it does appear that they are evolving to optimize particular abilities that will help them survive. This is not the case in a global sense. In fact, the pressure to optimize a given ability comes from the current state of a given population. The only fundamental selective pressure is the ability of structures to adhere to physical law. Stochastic events initiated the appearance of features that we might view as adaptive traits, and it is the local evolution of these traits that much of evolutionary theory is based upon. \n\nThe universe maintains its own consistency. However, when trying to model the universe, or create artificial organisms by mimicking natural evolution, the application of fitness functions that select for particular abilities force an unnatural bias onto the evolving organisms. \n\nIn evolutionary biology, it is well understood that all properties of organisms and ecosystems are emergent properties of fundamental physical law. The conceptual problems related to generations, offspring and populations that occur in artificial life systems stem from the fact that self-replication is itself an innovation of life on Earth. The dynamics observed in modern self-replicating life forms on Earth are extremely complex emergent properties of the organisms themselves, not driving forces imposed by an external controlling entity.\n\nAlife: http://alife.org/conferences\nGECCO: http://www.sigevo.org/gecco-2016/\n"}
{"id": "36122619", "url": "https://en.wikipedia.org/wiki?curid=36122619", "title": "Artificial life", "text": "Artificial life\n\nArtificial life (often abbreviated ALife or A-Life) is a field of study wherein researchers examine systems related to natural life, its processes, and its evolution, through the use of simulations with computer models, robotics, and biochemistry. The discipline was named by Christopher Langton, an American theoretical biologist, in 1986. There are three main kinds of alife, named for their approaches: \"soft\", from software; \"hard\", from hardware; and \"wet\", from biochemistry. Artificial life researchers study traditional biology by trying to recreate aspects of biological phenomena.\n\nArtificial life studies the fundamental processes of living systems in artificial environments in order to gain a deeper understanding of the complex information processing that define such systems. These topics are broad, but often include evolutionary dynamics, emergent properties of collective systems, biomimicry, as well as related issues about the philosophy of the nature of life and the use of lifelike properties in artistic works.\n\nThe modeling philosophy of artificial life strongly differs from traditional modeling by studying not only \"life-as-we-know-it\" but also \"life-as-it-might-be\".\n\nA traditional model of a biological system will focus on capturing its most important parameters. In contrast, an alife modeling approach will generally seek to decipher the most simple and general principles underlying life and implement them in a simulation. The simulation then offers the possibility to analyse new and different lifelike systems.\n\nVladimir Georgievich Red'ko proposed to generalize this distinction to the modeling of any process, leading to the more general distinction of \"processes-as-we-know-them\" and \"processes-as-they-could-be\".\n\nAt present, the commonly accepted definition of life does not consider any current alife simulations or software to be alive, and they do not constitute part of the evolutionary process of any ecosystem. However, different opinions about artificial life's potential have arisen:\n\n\n\nThis is a list of artificial life/digital organism simulators, organized by the method of creature definition.\n\nProgram-based simulations contain organisms with a complex DNA language, usually Turing complete. This language is more often in the form of a computer program than actual biological DNA. Assembly derivatives are the most common languages used. An organism \"lives\" when its code is executed, and there are usually various methods allowing self-replication. Mutations are generally implemented as random changes to the code. Use of cellular automata is common but not required. Another example could be an artificial intelligence and multi-agent system/program.\n\nIndividual modules are added to a creature. These modules modify the creature's behaviors and characteristics either directly, by hard coding into the simulation (leg type A increases speed and metabolism), or indirectly, through the emergent interactions between a creature's modules (leg type A moves up and down with a frequency of X, which interacts with other legs to create motion). Generally these are simulators which emphasize user creation and accessibility over mutation and evolution.\n\nOrganisms are generally constructed with pre-defined and fixed behaviors that are controlled by various parameters that mutate. That is, each organism contains a collection of numbers or other \"finite\" parameters. Each parameter controls one or several aspects of an organism in a well-defined way.\n\nThese simulations have creatures that learn and grow using neural nets or a close derivative. Emphasis is often, although not always, more on learning than on natural selection.\n\nMathematical models of complex systems are of three types: black-box (phenomenological), white-box (mechanistic, based on the first principles) and grey-box (mixtures of phenomenological and mechanistic models). In black-box models, the individual-based (mechanistic) mechanisms of a complex dynamic system remain hidden. Black-box models are completely nonmechanistic. They are phenomenological and ignore a composition and internal structure of a complex system. We cannot investigate interactions of subsystems of such a non-transparent model. A white-box model of complex dynamic system has ‘transparent walls’ and directly shows underlying mechanisms. All events at micro-, meso- and macro-levels of a dynamic system are directly visible at all stages of its white-box model evolution. In most cases mathematical modelers use the heavy black-box mathematical methods, which cannot produce mechanistic models of complex dynamic systems. Grey-box models are intermediate and combine black-box and white-box approaches. Creation of a white-box model of complex system is associated with the problem of the necessity of an a priori basic knowledge of the modeling subject. The deterministic logical cellular automata are necessary but not sufficient condition of a white-box model. The second necessary prerequisite of a white-box model is the presence of the physical ontology of the object under study. The white-box modeling represents an automatic hyper-logical inference from the first principles because it is completely based on the deterministic logic and axiomatic theory of the subject. The purpose of the white-box modeling is to derive from the basic axioms a more detailed, more concrete mechanistic knowledge about the dynamics of the object under study. The necessity to formulate an intrinsic axiomatic system of the subject before creating its white-box model distinguishes the cellular automata models of white-box type from cellular automata models based on arbitrary logical rules. If cellular automata rules have not been formulated from the first principles of the subject, then such a model may have a weak relevance to the real problem.\n\nHardware-based artificial life mainly consist of \"robots\", that is, automatically guided machines able to do tasks on their own.\n\nBiochemical-based life is studied in the field of synthetic biology. It involves e.g. the creation of synthetic DNA. The term \"wet\" is an extension of the term \"wetware\".\n\n\n\n\n\nAlife has had a controversial history. John Maynard Smith criticized certain artificial life work in 1994 as \"fact-free science\".\n\n"}
{"id": "22945287", "url": "https://en.wikipedia.org/wiki?curid=22945287", "title": "Artificial life framework", "text": "Artificial life framework\n\nArtificial Life is a free and open sourced Java framework created to simulate Life. It is a multi-agents framework where each agent runs its own Thread.\n\nThe agents are split into two different categories: the services and the processes. The services deliver services to other agents and the processes execute specific tasks.\n\nThe agents are organized in a tree structure called Instance. Within an Instance, the services and the processes are grouped together and unlimited groups can be defined. Each node of the Instance tree can have a collection of views used to monitor the activity of the node or interact with the node. The definition of Java classes used within the Instance (i.e. processes, services or views) is defined in the Instance Model.\n\nIn order to avoid dead locking, Artificial Life implements a messaging system, a method invocation mechanism based on the messaging system and an event mechanism also based on the messaging system.\n\n"}
{"id": "8553751", "url": "https://en.wikipedia.org/wiki?curid=8553751", "title": "Biological organisation", "text": "Biological organisation\n\nBiological organization is the hierarchy of complex biological structures and systems that define life using a reductionistic approach. The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organization concept, or as the field, hierarchical ecology.\n\nEach level in the hierarchy represents an increase in organizational complexity, with each \"object\" being primarily composed of the previous level's basic unit. The basic principle behind the organization is the concept of \"emergence\"—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.\n\nThe biological organization of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organization, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct insecticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.\n\nThe simple standard biological organization scheme, from the lowest level to the highest level, is as follows:\n\nMore complex schemes incorporate many more levels. For example, a molecule can be viewed as a grouping of elements, and an atom can be further divided into subatomic particles (these levels are outside the scope of biological organization). Each level can also be broken down into its own hierarchy, and specific types of these biological objects can have their own hierarchical scheme. For example, genomes can be further subdivided into a hierarchy of genes.\n\nEach level in the hierarchy can be described by its lower levels. For example, the organism may be described at any of its component levels, including the atomic, molecular, cellular, histological (tissue), organ and organ system levels. Furthermore, at every level of the hierarchy, new functions necessary for the control of life appear. These new roles are not functions that the lower level components are capable of and are thus referred to as \"emergent properties\".\n\nEvery organism is organised, though not necessarily to the same degree. An organism can not be organised at the histological (tissue) level if it is not composed of tissues in the first place.\n\nEmpirically, a large proportion of the (complex) biological systems we observe in nature exhibit hierarchic structure. On theoretical grounds we could expect complex systems to be hierarchies in a world in which complexity had to evolve from simplicity. System hierarchies analysis performed in the 1950s, laid the empirical foundations for a field that would be, from the 1980s, hierarchical ecology.\n\nThe theoretical foundations are summarized by thermodynamics.\nWhen biological systems are modeled as physical systems, in its most general abstraction, they are thermodynamic open systems that exhibit self-organised behavior, and the set/subset relations between dissipative structures can be characterized in a hierarchy.\n\nA simpler and more direct way to explain the fundamentals of the \"hierarchical organization of life\", was introduced in Ecology by Odum and others as the \"Simon's hierarchical principle\"; Simon emphasized that hierarchy \"emerges almost inevitably through a wide variety of evolutionary processes, for the simple reason that hierarchical structures are stable\".\n\nTo motivate this deep idea, he offered his \"parable\" about imaginary watchmakers.\n\n\n"}
{"id": "3447756", "url": "https://en.wikipedia.org/wiki?curid=3447756", "title": "Carbon-based life", "text": "Carbon-based life\n\nCarbon is a key component of all known life on Earth, representing approximately 45-50% of all dry biomass. Complex molecules are made up of carbon bonded with other elements, especially oxygen and hydrogen and frequently also with nitrogen, phosphorus and sulfur. Carbon is abundant on Earth. It is also lightweight and relatively small in size, making it easier for enzymes to manipulate carbon molecules. It is frequently assumed in astrobiology that if life exists elsewhere in the universe, it will also be carbon-based. Critics refer to this assumption as carbon chauvinism.\n\nCarbon forms a vast number of compounds, more than any other element, with almost ten million compounds described to date, and yet that number is but a fraction of the number of theoretically possible compounds under standard conditions. For this reason, carbon has often been referred to as the \"king of the elements\".\n\nCarbon is the 15th most abundant element in the Earth's crust, and the fourth most abundant element in the universe by mass after hydrogen, helium, and oxygen. Carbon's abundance, its unique diversity of organic compounds, and its unusual ability to form polymers at the temperatures commonly encountered on Earth enables this element to serve as a common element of all known life. It is the second most abundant element in the human body by mass (about 18.5%) after oxygen.\n\n\"What we normally think of as 'life' is based on chains of carbon atoms, with a few other atoms, such as nitrogen or phosphorus\", per Stephen Hawking in a 2008 lecture, \"carbon [...] has the richest chemistry.\" \nThe most important characteristics of carbon as a basis for the chemistry of life are, that it has four valence bonds, and that the energy required to make or break a bond is at an appropriate level for building molecules, which are stable and reactive. Carbon atoms bond readily to other carbon atoms; this allows the building of arbitrarily long complex molecules and polymers.\n\nThere are not many other elements which even appear to be promising candidates for supporting life, for example, processes such as metabolism. The most frequently suggested alternative is silicon. Silicon is in the same group in the Periodic Table of elements, and has four valence bonds, and bonds to itself, generally in the form of crystal lattices rather than long chains. It is considerably more electropositive than carbon. Silicon compounds do not readily recombine into different permutations in a manner that would plausibly support lifelike processes.\n\nThe most notable groups of chemicals used in the processes of living organisms include:\n\n\nSilicon has been a theme of non-carbon-based-life since it is somewhat similar to carbon in its chemical characteristics. \nIn cinematic and literary science fiction, when man-made machines cross from non-living to living, this new form would be an example of non-carbon-based life. Since the advent of the microprocessor in the late 1960s, these machines are often classed as \"silicon-based life\". Another example of \"silicon-based life\" is the episode \"The Devil in the Dark\" from , in which a living rock creature's biochemistry is based on silicon. Also in The X-Files episode \"Firewalker\" where silicon based organism was discovered in a volcano.\n\nIn the movie adaptation of Arthur C. Clarke's \"2010\" (1984) a character argues, \"Whether we are based on carbon or on silicon makes no fundamental difference; we should each be treated with appropriate respect\". This quote may be the basis of Steve Jobs's quip when he introduced Carbon within MacOS X, \"Carbon. All life forms will be based on it.\"\n\n\n"}
{"id": "32703814", "url": "https://en.wikipedia.org/wiki?curid=32703814", "title": "Chirality", "text": "Chirality\n\nChirality is a property of asymmetry important in several branches of science. The word \"chirality\" is derived from the Greek (\"kheir\"), \"hand,\" a familiar chiral object.\n\nAn object or a system is \"chiral\" if it is distinguishable from its mirror image; that is, it cannot be superposed onto it. Conversely, a mirror image of an \"achiral\" object, such as a sphere, cannot be distinguished from the object. A chiral object and its mirror image are called \"enantiomorphs\" (Greek, \"opposite forms\") or, when referring to molecules, \"enantiomers\". A non-chiral object is called \"achiral\" (sometimes also \"amphichiral\") and can be superposed on its mirror image. If the object is non-chiral and is imagined as being colored blue and its mirror image is imagined as colored yellow, then by a series of rotations and translations the two can be superposed, producing green, with none of the original colors remaining.\n\nThe term was first used by Lord Kelvin in 1893 in the second Robert Boyle Lecture at the Oxford University Junior Scientific Club which was published in 1894:\n\nHuman hands are perhaps the most universally recognized example of chirality. The left hand is a non-superimposable mirror image of the right hand; no matter how the two hands are oriented, it is impossible for all the major features of both hands to coincide across all axes. This difference in symmetry becomes obvious if someone attempts to shake the right hand of a person using their left hand, or if a left-handed glove is placed on a right hand. In mathematics, \"chirality\" is the property of a figure that is not identical to its mirror image.\n\nIn mathematics, a figure is chiral (and said to have chirality) if it cannot be mapped to its mirror image by rotations and translations alone. For example, a right shoe is different from a left shoe, and clockwise is different from anticlockwise. See for a full mathematical definition.\n\nA chiral object and its mirror image are said to be enantiomorphs. The word \"enantiomorph\" stems from the Greek (enantios) 'opposite' + (morphe) 'form'. A non-chiral figure is called achiral or amphichiral.\n\nThe helix (and by extension a spun string, a screw, a propeller, etc.) and Möbius strip are chiral two-dimensional objects in three-dimensional ambient space. The J, L, S and Z-shaped \"tetrominoes\" of the popular video game Tetris also exhibit chirality, but only in a two-dimensional space.\n\nMany other familiar objects exhibit the same chiral symmetry of the human body, such as gloves, glasses (where two lenses differ in prescription), and shoes. A similar notion of chirality is considered in knot theory, as explained below.\n\nSome chiral three-dimensional objects, such as the helix, can be assigned a right or left handedness, according to the right-hand rule.\n\nIn geometry a figure is achiral if and only if its symmetry group contains at least one \"orientation-reversing\" isometry.\nIn two dimensions, every figure that possesses an axis of symmetry is achiral, and it can be shown that every \"bounded\" achiral figure must have an axis of symmetry.\nIn three dimensions, every figure that possesses a plane of symmetry or a center of symmetry is achiral. There are, however, achiral figures lacking both plane and center of symmetry.\nIn terms of point groups, all chiral figures lack an improper axis of rotation (S). This means that they cannot contain a center of inversion (i) or a mirror plane (σ). Only figures with a point group designation of C, C, D, T, O, or I can be chiral.\n\nA knot is called achiral if it can be continuously deformed into its mirror image, otherwise it is called chiral. For example, the unknot and the figure-eight knot are achiral, whereas the trefoil knot is chiral.\n\nIn physics, chirality may be found in the spin of a particle, where the handedness of the object is determined by the direction in which the particle spins. Not to be confused with helicity, which is the projection of the spin along the linear momentum of a subatomic particle, chirality is a purely quantum mechanical phenomenon like spin. Although both can have left-handed or right-handed properties, only in the massless case do they have a simple relation. In particular for a massless particle the helicity is the same as the chirality while for an antiparticle they have opposite sign.\n\nThe \"handedness\" in both chirality and helicity relate to the rotation of a particle while it proceeds in linear motion with reference to the human hands. The thumb of the hand points towards the direction of linear motion whilst the fingers curl into the palm, representing the direction of rotation of the particle (i.e. clockwise and counterclockwise). Depending on the linear and rotational motion, the particle can either be defined by left-handedness (ex. translating leftwards and rotating counterclockwise) or right-handedness (ex. translating in the right direction and rotating clockwise). A symmetry transformation between the two is called parity. Invariance under parity by a Dirac fermion is called \"chiral symmetry\".\n\nElectromagnetic wave propagation as handedness is wave polarization and described in terms of helicity (occurs as a helix). Polarization of an electromagnetic wave is the property that describes the orientation, i.e., the time-varying, direction (vector), and amplitude of the electric field vector. For a depiction, see the adjacent image.\n\nA \"chiral molecule\" is a type of molecule that has a non-superposable mirror image. The feature that is most often the cause of chirality in molecules is the presence of an asymmetric carbon atom.\n\nThe term \"chiral\" in general is used to describe the object that is non-superposable on its mirror image.\n\nIn chemistry, chirality usually refers to molecules. Two mirror images of a chiral molecule are called enantiomers or optical isomers. Pairs of enantiomers are often designated as \"right-\", \"left-handed\" or if it has no bias achiral. As polarized light passes through a chiral molecule, the plane of polarization, when viewed along the axis toward the source, will be rotated in a clockwise (to the right) or anticlockwise (to the left). A right handed rotation is dextrorotary (d); that to the left is levorotary (l). The d- and l-isomers are the same compound but are called enantiomers. An equimolar mixture of the two optical isomers will produce no net rotation of polarized light as it passes through. Left handed molecules have l- prefixed to their names; d- is prefixed to right handed molecules.\n\nMolecular chirality is of interest because of its application to stereochemistry in inorganic chemistry, organic chemistry, physical chemistry, biochemistry, and supramolecular chemistry.\n\nMore recent developments in chiral chemistry include the development of chiral inorganic nanoparticles that may have the similar tetrahedral geometry as chiral centers associated with sp3 carbon atoms traditionally associated with chiral compounds, but at larger scale. Helical and other symmetries of chiral nanomaterials were also obtained.\n\nAll of the known life-forms show specific chiral properties in chemical structures as well as macroscopic anatomy, development and behavior. In any specific organism or evolutionarily related set thereof, individual compounds, organs, or behavior are found in the same single enantiomorphic form. Deviation (having the opposite form) could be found in a small number of chemical compounds, or certain organ or behavior but that variation strictly depends upon the genetic make up of the organism. From chemical level (molecular scale), biological systems show extreme stereospecificity in synthesis, uptake, sensing, metabolic processing. A living system usually deals with two enantiomers of same compound in a drastically different way.\n\nIn biology, homochirality is a common property of amino acids and carbohydrates. The chiral protein-making amino acids, which are translated through the ribosome from genetic coding, occur in the form. However, -amino acids are also found in nature. The monosaccharides (carbohydrate-units) are commonly found in -configuration. DNA double helix is chiral (as any kind of helix is chiral), and B-form of DNA shows a right-handed turn.\n\nSometimes, when two enantiomers of a compound found in organisms, they significantly differ in their taste, smell and other biological actions. For example, (+)-Limonene found in orange (causing its smell), and (–)-Limonene found in Lemons (causing its smell), show different smells due to different biochemical interactions at human nose. (+)-Carvone is responsible for the smell of Caraway seed oil whereas (–)-carvone is responsible for smell of Spearmint oil.\n\nAlso, for artificial compounds, including medicines, in case of chiral drugs, the two enantiomers sometimes show remarkable difference in effect of their biological actions. Darvon (Dextropropoxyphene) is a painkiller, whereas its enantiomer, Novrad (Levopropoxyphene) is an anti-cough agent. In case of Penicillamine, the S-isomer used in treatment of primary chronic arthritis, Whereas the R-isomer has no therapeutic effect as well as being highly toxic. In some cases the less therapeutically active enantiomer can cause side effects. For example, S-naproxen is an analgesic but the R-isomer cause renal problems. The naturally occurring plant form of alpha-tocopherol (vitamin E) is RRR-α-tocopherol whereas the synthetic form (all-racemic vitamin E, or dl-tocopherol) is equal parts of the stereoisomers RRR, RRS, RSS, SSS, RSR, SRS, SRR and SSR with progressively decreasing biological equivalency, so that 1.36 mg of dl-tocopherol is considered equivalent to 1.0 mg of d-tocopherol.\nMacroscopic example of Chirality is found in plant kingdom, animal kingdom and all other groups of organism. A simple example is the coiling direction of any climber plants. It may be one of two possible type of helix.\nIn anatomy, chirality is found in the imperfect mirror image symmetry of many kinds of animal bodies. Organisms such as gastropods exhibit chirality in their coiled shells, resulting in an asymmetrical appearance. Over 90% of gastropod species have \"dextral\" (right-handed) shells in their coiling, but a small minority of species and genera are virtually always \"sinistral\" (left-handed). A very few species (for example \"Amphidromus perversus\") show an equal mixture of dextral and sinistral individuals.\n\nIn humans, chirality (also referred to as \"handedness\" or \"laterality\") is an attribute of humans defined by their unequal distribution of fine motor skill between the left and right hands. An individual who is more dexterous with the right hand is called \"right-handed\", and one who is more skilled with the left is said to be \"left-handed\". Chirality is also seen in the study of facial asymmetry.\n\nIn flatfish, the Summer flounder or fluke are left-eyed, while halibut are right-eyed.\n\n\n"}
{"id": "1170166", "url": "https://en.wikipedia.org/wiki?curid=1170166", "title": "Chirality (chemistry)", "text": "Chirality (chemistry)\n\nChirality is a geometric property of some molecules and ions. A chiral molecule/ion is non-superposable on its mirror image. The presence of an asymmetric carbon center is one of several structural features that induce chirality in organic and inorganic molecules. The term \"chirality\" is derived from the Ancient Greek word for hand, χεῖρ (\"kheir\").\n\nThe mirror images of a chiral molecule or ion are called enantiomers or optical isomers. Individual enantiomers are often designated as either right-handed or left-handed. Chirality is an essential consideration when discussing the stereochemistry in organic and inorganic chemistry. The concept is of great practical importance because most biomolecules and pharmaceuticals are chiral.\n\nChiral molecules and ions are described by various ways of designating their absolute configuration, which codify either the entity's geometry or its ability to rotate plane-polarized light, a common technique in studying chirality.\n\nChirality is based on molecular symmetry elements. Specifically, a chiral compound can contain no improper axis of rotation (S), which includes planes of symmetry and inversion center. Chiral molecules are always dissymmetric (lacking S) but not always asymmetric (lacking all symmetry elements except the trivial identity). Asymmetric molecules are always chiral.\n\nIn general, chiral molecules have point chirality at a single \"stereogenic\" atom, which has four different substituents. The two enantiomers of such compounds are said to have different absolute configurations at this center. This center is thus stereogenic (i.e., a grouping within a molecular entity that may be considered a focus of stereoisomerism). The stereogenic atom (also known as the \"stereocenter\") is usually carbon, as in many biological molecules. However a stereocenter can coincide with any atom, including metals (as in many chiral coordination compounds), phosphorus, or sulfur. The low barrier of nitrogen inversion make most \"N\"-chiral amines (NRR′R″) impossible to resolve, but \"P\"-chiral phosphines (PRR′R″) as well as S-chiral sulfoxides (OSRR′) are optically stable.\n\nWhile the presence of a stereogenic atom describes the great majority of chiral molecules, many variations and exceptions exist. For instance it is not necessary for the chiral substance to have a stereogenic atom. Examples include 1-bromo-3-chloro-5-fluoroadamantane, methylethylphenyltetrahedrane, certain calixarenes and fullerenes, which have inherent chirality. The C-symmetric species 1,1′-bi-2-naphthol (BINOL), 1,3-dichloroallene have axial chirality. (\"E\")-cyclooctene and many ferrocenes have planar chirality.\n\nWhen the optical rotation for an enantiomer is too low for practical measurement, the species is said to exhibit cryptochirality.\n\nEven isotopic differences must be considered when examining chirality. Illustrative is the derivative of benzyl alcohol PhCHDOH, which is chiral. The \"S\" enantiomer has [\"α\"] = +0.715°.\n\nMany biologically active molecules are chiral, including the naturally occurring amino acids (the building blocks of proteins) and sugars. In biological systems, most of these compounds are of the same chirality: most amino acids are levorotatory () and sugars are dextrorotatory (). Typical naturally occurring proteins are made of -amino acids and are known as \"left-handed proteins\"; the comparably rarer -amino acids produce \"right-handed proteins\".\n\nThe origin of this homochirality in biology is the subject of much debate. Most scientists believe that Earth life's \"choice\" of chirality was purely random, and that if carbon-based life forms exist elsewhere in the universe, their chemistry could theoretically have opposite chirality. However, there is some suggestion that early amino acids could have formed in comet dust. In this case, circularly polarised radiation (which makes up 17% of stellar radiation) could have caused the selective destruction of one chirality of amino acids, leading to a selection bias which ultimately resulted in all life on Earth being homochiral.\n\nEnzymes, which are chiral, often distinguish between the two enantiomers of a chiral substrate. One could imagine an enzyme as having a glove-like cavity that binds a substrate. If this glove is right-handed, then one enantiomer will fit inside and be bound, whereas the other enantiomer will have a poor fit and is unlikely to bind.\n\n-forms of amino acids tend to be tasteless, whereas -forms tend to taste sweet. Spearmint leaves contain the -enantiomer of the chemical carvone or \"R\"-(−)-carvone and caraway seeds contain the -enantiomer or \"S\"-(+)-carvone. These smell different to most people because our olfactory receptors are chiral.\n\nChirality is important in context of ordered phases as well, for example the addition of a small amount of an optically active molecule to a nematic phase (a phase that has long range orientational order of molecules) transforms that phase to a chiral nematic phase (or cholesteric phase). Chirality in context of such phases in polymeric fluids has also been studied in this context.\n\nChirality is a symmetry property, not a characteristic of any part of the periodic table. Thus many inorganic materials, molecules, and ions are chiral. Quartz is an example from the mineral kingdom. Such noncentric materials are of interest for applications in nonlinear optics.\n\nIn the areas of coordination chemistry and organometallic chemistry, chirality is pervasive and of practical importance. A famous example is tris(bipyridine)ruthenium(II) complex in which the three bipyridine ligands adopt a chiral propeller-like arrangement. The two enantiomers of complexes such as [Ru(2,2′-bipyridine)] may be designated as Λ (capital lambda, the Greek version of \"L\") for a left-handed twist of the propeller described by the ligands, and Δ (capital delta, Greek \"D\") for a right-handed twist (pictured).\n\nChiral ligands confer chirality to a metal complex, as illustrated by metal-amino acid complexes. If the metal exhibits catalytic properties, its combination with a chiral ligand is the basis of asymmetric catalysis.\n\nThe term \"optical activity\" is derived from the interaction of chiral materials with polarized light. In a solution, the (−)-form, or levorotatory form, of an optical isomer rotates the plane of a beam of linearly polarized light counterclockwise. The (+)-form, or dextrorotatory form, of an optical isomer does the opposite. The rotation of light is measured using a polarimeter and is expressed as the optical rotation.\n\n\nThe rotation of plane polarized light by chiral substances was first observed by Jean-Baptiste Biot in 1815, and gained considerable importance in the sugar industry, analytical chemistry, and pharmaceuticals. Louis Pasteur deduced in 1848 that this phenomenon has a molecular basis. The term \"chirality\" itself was coined by Lord Kelvin in 1894. Different enantiomers or diastereomers of a compound were formerly called optical isomers due to their different optical properties. At one time, chirality was thought to be associated with organic chemistry, but this misconception was overthrown by the resolution of a purely inorganic compound, hexol, by Alfred Werner.\n\n\n"}
{"id": "44262036", "url": "https://en.wikipedia.org/wiki?curid=44262036", "title": "Coleridge's theory of life", "text": "Coleridge's theory of life\n\nRomanticism grew largely out of an attempt to understand not just inert nature, but also vital nature. Romantic works in the realm of art and Romantic medicine were a response to the general failure of the application of method of inertial science to reveal the foundational laws and operant principles of vital nature. German romantic science and medicine sought to understand the nature of the life principle identified by John Hunter as distinct from matter itself via Johan Friedrich Blumenbach's \"Bildungstrieb\" and Romantic medicine's \"Lebenskraft\", as well as Röschlaub's development of the Brunonian system of medicine system of John Brown, in his \"excitation theory\" of life (German:\"Erregbarkeit theorie\"), working also with Schelling's \"Naturphilosophie\", the work of Goethe regarding morphology, and the first dynamic conception of physiology of Richard Saumarez. But it is in Samuel Taylor Coleridge that we find the question of life and vital nature most intensely and comprehensively examined, particularly in his Hints towards the Formation of a more Comprehensive Theory of Life (1818), providing the foundation for Romantic philosophy, science and medicine. The work is key to understanding the relationship of Romantic literature and science.\n\nThe Enlightenment had developed a philosophy and science supported by formidable twin pillars: the first the Cartesian split of mind and matter, the second Newtonian physics, with its conquest of inert nature, both of which focused the mind's gaze on things or objects. For Cartesian philosophy, life existed on the side of matter, not mind; and for the physical sciences, the method that had been so productive for revealing the secrets of inert nature, should be equally productive in examining vital nature. The initial attempt to seek the cause and principle of life in matter was challenged by John Hunter, who held that the principle of life was not to be found nor confined within matter, but existed independently of matter itself, and informed or animated it, that is, he implied, it was the unifying or antecedent cause of the things or what Aristotelean philosophy termed \"natura naturata\" (the outer appearances of nature).\nThis reduction of the question of life to matter, and the corollary, that the method of the inertial sciences was the way to understand the very phenomenon of life, that is, its very nature and essence as a power (\"natura naturans\"), not as manifestations through sense-perceptible appearances (\"natura naturata\"), also reduced the individual to a material-mechanical 'thing' and seemed to render human freedom an untenable concept. It was this that Romanticism challenged, seeking instead to find an approach to the essence of nature as being also vital not simply inert, through a systematic method involving not just physics, but physiology (living functions). For Coleridge, quantitative analysis was anti-realist and needed to be grounded in qualitative analysis ('-ologies') (as was the case with Goethe's approach).\nAt the same time, the Romantics had to deal with the idealistic view that life was a 'somewhat' outside of things, such that the things themselves lost any real existence, a stream coming through Hume and Kant, and also infusing the German natural philosophical stream, German idealism, and in particular \"naturphilosophie\", eventuating scientifically in the doctrine of 'vitalism'. For the Romantics, life is independent of and antecedent to nature, but also infused and suspended in nature, not apart from it, As David Bohm expresses it in more modern terms \"In nature nothing remains constant…everything comes from other things and gives rise to other things. This principle is…at the foundation of the possibility of our understanding nature in a rational way.\"\n\nAnd as Coleridge explained, 'this antecedent unity, or cause and principle of each union, it has since the time of Bacon and Kepler been customary to call a law.\" And as law, \"we derive from it a progressive insight into the necessity and generation of the phenomena of which it is the law.\"\n\nColeridge's was the dominant mind on many issues involving the philosophy and science in his time, as John Stuart Mill acknowledged, along with others since who have studied the history of Romanticism.\n\nFor Coleridge, as for many of his romantic contemporaries, the idea that matter itself can begat life only dealt with the various changes in the arrangement of particles and did not explain life itself as a principle or power that lay behind the material manifestations, \"natura naturans\" or \"the productive power suspended and, as it were, quenched in the product\" Until this were addressed, according to Coleridge, \"we have not yet attained to a science of nature.\"\n\nThis productive power is above sense experience, but not above nature herself, that is, supersensible, but not supernatural, and, thus, not 'occult' as was the case with vitalism. Vitalism failed to distinguish between spirit and nature, and then within nature, between the visible appearances and the invisible, yet very real and not simply hypothesized notion, essence or motivating principle (\"natura naturans\"). Even Newton spoke of things invisible in themselves (though not in their manifestations), such as force, though Comte, the thorough materialist, complained of the use of such terms as the 'force of gravity' as being relics of animism.\n\nMatter was not a 'datum' or thing in and of itself, but rather a product or effect, and for Coleridge, looking at life in its broadest sense, it was the product of a polarity of forces and energies, but derived from a unity which is itself a power, not an abstract or nominal concept, that is Life, and this polar nature of forces within the power of Life is the very law or 'Idea' (in the Platonic sense) of Creation.\n\nFor Coleridge the essence of the universe is motion and motion is driven by a dynamic polarity of forces that is both inherent in the world as potential and acting inherently in all manifestations. This polarity is the very dynamic that acts throughout all of nature, including into the more particular form of 'life biological', as well as of mind and consciousness.\n\nAnd this polarity is dynamic, that is real, though not visible, and not simply logical or abstract. Thus, the polarity results in manifestations that are real, as the opposite powers are not contradictory, but counteracting and inter-penetrating.\n\nThus, then, Life itself is not a thing—a self-subsistent hypostasis—but an act and process...\n\nAnd in that sense Coleridge re-phrases the question \"What is Life?\" to \"What is not Life that really is?\"\n\nThis dynamic polar essence of nature in all its functions and manifestations is a universal law in the order of the law of gravity and other physical laws of inert nature. And, critically, this dynamic polarity of constituent powers of life at all levels is not outside or above nature, but is within nature (\"natura naturans\"), not as a part of the visible product, but as the ulterior natural functions that produce such products or things.\n\nIt is these functions that provided the bridge being sought by Romantic science and medicine, in particular by Andreas Röschlaub and the Brunonian system of medicine, between the inertial science of inert nature (physics) and the vital science of vital nature (physiology) and its therapeutic application or physic (the domain of the physician).\n\nColeridge was influenced by German philosophy, in particular Kant, Fichte and Schelling (Naturphilosophie), as well as the physiology of Blumenbach and the dynamic excitation theory of life of the Brunonian system. He sought a path that was neither the mystical tendency of the earlier vitalists nor the materialistic reductionist approach to natural science, but a dynamic one.\n\nColeridge's challenge was to describe something that was dynamic neither in mystical terms not materialistic ones, but via analogy, drawing from the examples of inertial science. As one writer explains, he uses the examples of electricity, magnetism and gravity not because they are like life, but because they offer a way of understanding powers, forces and energies, which lie at the heart of life. And using these analogies, Coleridge seeks to demonstrate that life is not a material force, but a product of relations amongst forces. Life is not linear and static, but dynamic process of self-regulation and Emergent evolution that results in increasing complexity and individuation. This spiral, upward movement (cf. Goethe's ideas) creates a force for organization that unifies, and is most intense and powerful in that which is most complex and most individual - the self-regulating, enlightened, developed individual mind. But at the same time, this process of life increases interdependence (like the law of comparative advantage in economics) and associational powers of the mind. Thus, he is not talking about an isolated, individual subjective mind, but about the evolution of a higher level of consciousness and thought at the core of the process of life.\n\nAnd the direction of this motion is towards increasing individuation, that is the creation of specific, individual units of things. At the same time, given the dynamic polarity of the world, there must always be an equal and opposite tendency, in this case, that of connection. So, a given of our experience is that man is both an individual, tending in each life and in history generally to greater and greater individualization, and a social creature seeking interaction and connection. It is the dynamic interplay between the individuation and connecting forces that leads to higher and higher individuation.\n\nColeridge makes a further distinction between mathematics and life, the latter being productive or creative, that is, living, and the former ideal. Thus, the mathematical approach that works so well with inert nature, is not suitable for vital nature.\n\nThe counteraction then of the two assumed forces does not depend on their meeting from opposite directions; the power which acts in them is indestructible; it is therefore inexhaustibly re-ebullient; and as something must be the result of these two forces, both alike infinite, and both alike indestructible; and as rest or neutralization cannot be this result; no other conception is possible, but that the product must be a tertium aliquid, [a third thing] or finite generation. Consequently this conception is necessary. Now this tertium aliquid can be no other than an inter-penetration of the counteracting powers, partaking of both… Consequently the 'constituent powers', that have given rise to a body, may then reappear in it as its function: \"a Power, acting in and by its Product or Representative to a predetermined purpose is a Function...the first product of its energy is the thing itself: \"ipsa se posuit et iam facta est ens positum\". Still, however, its productive energy is not exhausted in this product, but overflows, or is effluent, as the specific forces, properties, faculties, of the product. It reappears, in short, as the function of the body...The vital functions are consequents of the \"Vis Vitae Principium Vitale\", and presuppose the Organs, as the Functionaries.\n\nLife, that is, the essential polarity in unity (multeity in unity) in Coleridge’s sense also has a four beat cycle, different from the arid dialectics of abstraction - namely the tension of the polar forces themselves, the charge of their synthesis, the discharge of their product (indifference) and the resting state of this new form (predominance). The product is not a neutralization, but a new form of the essential forces, these forces remaining within, though now as the functions of the form.\n\nTo make it adequate, we must substitute the idea of positive production for that of rest, or mere neutralization. To the fancy alone it is the null-point, or zero, but to the reason it is the punctum saliens, and the power itself in its eminence.\n\nThis dynamic polarity that is Life is expressed at different levels. At its most basic it is Space-Time, with its product - motion. The interplay of both gives us either a line or a circle, and then there are different degrees possible within a given form or “predominance” of forces. Geometry is not conceivable except as the dynamic interplay of space (periphery) and time (point). Space, time and motion are also geometrically represented by width, length (breadth) and depth. And this correspondence is repeated throughout the scale of Life.\n\nMatter, then, is the product of the dynamic forces - repulsion (centrifugal), and attraction (centripetal); it is not itself a productive power. It is also the mass of a given body.\n\nColeridge’s understanding of life is contrasted with the materialist view which is essentially reduced to defining life as that which is the opposite of not-life, or that which resists death, that is, that which is life.\n\nThe problem for Coleridge and the Romantics was that the intellect, 'left to itself' as Bacon stated, was capable of apprehending only the outer forms of nature (natura naturata) and not the inmost, living functions (natura naturans) giving rise to these forms. Thus, effects can only be 'explained' in terms of other effects, not causes. It takes a different capacity to 'see' these living functions, which is an imaginative activity. For Coleridge, there is an innate, primitive or 'primary' imagination that configures invisibly sense-experience into perception, but a rational perception, that is, one raised into consciousness and awareness and then rationally presentable, requires a higher level, what he termed 'secondary imagination', which is able to connect with the thing being experienced, penetrate to its essence in terms of the living dynamics upholding its outer form, and then present the phenomena as and within its natural law, and further, using reason, develop the various principles of its operation.\n\nThis cognitive capacity involved what Coleridge termed the 'inmost sense' or what Goethe termed the Gemüt. It also involved the reactivation of the old Greek noetic capacity, and the ability to 'see' or produce the theory (Greek \"theoria\" from the verb 'to see') dynamic polarities, or natural Laws, the dynamic transcendent (foundational) entities that Plato termed 'Ideas' (\"eidos\").\n\nSince \"natura naturata\" is sustained by \"natura naturans\", and the creative power of \"natura naturans\" is one of a kind with the human mind, itself creative, then there must be a correspondence or connection between the mind and the things we perceive, such that we can overcome the apparent separation between the object and the representation in the mind of the object that came to bedevil Enlightenment thought (Hume, Kant). As one commentator noted \"to speak at all of the unity of intelligence and nature is of course flatly to contradict Descartes.\"\n\nFor Coleridge the power of life lies in every seed as a potential to be unfolded as a result of interaction with the environment (heat, light, air, moisture, etc.), an insight which allowed him to see in the Brunonian system a dynamic polarity in excitation theory.\nColeridge also saw that there was a progressive movement through time and space of life or the law of polarity, from the level of physics (space and time) and the mineral or inert nature (law of gravity, operating through forces of attraction and repulsion), up to man, with his law of resonance in terms of his innate desire to be himself (force of individuation) and to also connect with like-minded (force of connection), as Goethe expressed in his novel \"Elective Affinities\" (\"Wahlverwandschaften\") as well as in his own life's experience.\nEvolution occurred because the original polarity of creation, the very 'Law of Creation', itself gives birth to subsequent polarities, as each pole is itself a unity that can be further polarized (what Wilhelm Reich later termed 'orgonomic functionalism' and what at the biological level constitutes physiology), an insight that would later be taken up by the concept of emergent evolution, including the emergence of mind and consciousness.\n\nAnd that this is so, is also an intimate and shared experience of all humans, as is set out in Reid's Common Sense philosophy. As Coleridge states\n\nThat nature evolves towards a purpose, and that is the unfolding of the human mind and consciousness in all its levels and degrees, is not teleological but a function of the very nature of the law of polarity or creation itself, namely that of increasing individuation of an original unity, what Coleridge termed 'multeity in unity'. As he states, \"without assigning to nature as nature, a conscious purpose\" we must still \"distinguish her agency from a blind and lifeless mechanism.\"\n\nWhile man contains and is subject to the various laws of nature, man as a self-conscious being is also the summa of a process of creation leading to greater mind and consciousness, that is, a creative capacity of imagination. Instead of being a creature of circumstance, man is the creator of them, or at least has that potential.\n\n"}
{"id": "2169038", "url": "https://en.wikipedia.org/wiki?curid=2169038", "title": "Homochirality", "text": "Homochirality\n\nHomochirality is a uniformity of chirality, or handedness. Objects are \"chiral\" when they cannot be superposed on their mirror images. For example, the left and right hands of a human are approximately mirror images of each other but are not their own mirror images, so they are \"chiral\". In biology, 19 of the 20 natural amino acids are homochiral, being -chiral (left-handed), while sugars are -chiral (right-handed). \"Homochirality\" can also refer to \"enantiomerically pure\" substances in which all the constituents are the same enantiomer (a right-handed or left-handed version of an atom or molecule), but some sources discourage this use of the term.\n\nIt is unclear whether homochirality has a purpose, however, it appears to be a form of information storage. One suggestion is that it reduces entropy barriers in the formation of large organized molecules. It has been experimentally verified that amino acids form large aggregates in larger abundance from enantiopure substrates than from racemic ones.\n\nIt is not clear whether homochirality emerged before or after life, and many mechanisms for its origin have been proposed. Some of these models propose three distinct steps: \"mirror-symmetry breaking\" creates a minute enantiomeric imbalance, \"chiral amplification\" builds on this imbalance, and \"chiral transmission\" is the transfer of chirality from one set of molecules to another.\n\nAmino acids are the building blocks of peptides and enzymes while sugar-peptide chains are the backbone of RNA and DNA. In biological organisms, amino acids appear almost exclusively in the left-handed form (-amino acids) and sugars in the right-handed form (R-sugars). Since the enzymes catalyze reactions, they enforce homochirality on a great variety of other chemicals, including hormones, toxins, fragrances and food flavors. Glycine is achiral, as are some other non-proteinogenic amino acids are either achiral (such as dimethylglycine) or of the enantiomeric form.\n\nBiological organisms easily discriminate between molecules with different chiralities. This can affect physiological reactions such as smell and taste. Carvone, a terpenoid found in essential oils, smells like mint in its L-form and caraway in its R-form. Limonene tastes like lemons when right-handed and oranges when left-handed.\n\nHomochirality also affects the response to drugs. Thalidomide, in its left-handed form, cures morning sickness; in its right-handed form, it causes birth defects. Unfortunately, even if a pure left-handed version is administered, some of it can convert to the right-handed form in the patient. Many drugs are available as both a racemic mixture (equal amounts of both chiralities) and an enantiopure drug (only one chirality). Depending on the manufacturing process, enantiopure forms can be more expensive to produce than stereochemical mixtures.\n\nChiral preferences can also be found at a macroscopic level. Snail shells can be right-turning or left-turning helices, but one form or the other is strongly preferred in a given species. In the edible snail \"Helix pomatia\", only one out of 20,000 is left-helical. The coiling of plants can have a preferred chirality and even the chewing motion of cows has a 10% excess in one direction.\n\nKnown mechanisms for the production of non-racemic mixtures from racemic starting materials include: asymmetric physical laws, such as the electroweak interaction; asymmetric environments, such as those caused by circularly polarized light, quartz crystals, or the Earth's rotation; and statistical fluctuations during racemic synthesis. Once established, chirality would be selected for. A small enantiomeric excess can be amplified into a large one by asymmetric autocatalysis, such as in the Soai reaction. In asymmetric autocatalysis, the catalyst is a chiral molecule, which means that a chiral molecule is catalysing its own production. An initial enantiomeric excess, such as can be produced by polarized light, then allows the more abundant enantiomer to outcompete the other.\n\nOne supposition is that the discovery of an enantiomeric imbalance in molecules in the Murchison meteorite supports an extraterrestrial origin of homochirality: there is evidence for the existence of circularly polarized light originating from Mie scattering on aligned interstellar dust particles which may trigger the formation of an enantiomeric excess within chiral material in space. Interstellar and near-stellar magnetic fields can align dust particles in this fashion. Another speculation (the Vester-Ulbricht hypothesis) suggests that fundamental chirality of physical processes such as that of the beta decay (see Parity violation) leads to slightly different half-lives of biologically relevant molecules. Homochirality may also result from spontaneous absolute asymmetric synthesis.\n\nIt is also possible that homochirality is simply a result of the natural autoamplification process of life—that either the formation of life as preferring one chirality or the other was a chance rare event which happened to occur with the chiralities we observe, or that all chiralities of life emerged rapidly but due to catastrophic events and strong competition, the other unobserved chiral preferences were wiped out by the preponderance and metabolic, enantiomeric enrichment from the 'winning' chirality choices. The emergence of chirality consensus as a natural autoamplification process has been associated with the 2nd law of thermodynamics.\n\nIn 1953, Charles Frank proposed a model to demonstrate that homochirality is a consequence of autocatalysis. In his model the and enantiomers of a chiral molecule are autocatalytically produced from an achiral molecule A\n\nwhile suppressing each other through a reaction that he called \"mutual antagonism\"\nIn this model the racemic state is unstable in the sense that the slightest enantiomeric excess will be amplified to a completely homochiral state. This can be shown by computing the reaction rates from the law of mass action:\nwhere formula_2 is the rate constant for the autocatalytic reactions, formula_3 is the rate constant for mutual antagonism reaction, and the concentration of A is kept constant for simplicity. By defining the enantiomeric excess formula_4 as\nwe can compute the rate of change of enatiomeric excess using chain rule from the rate of change of the concentrations of enantiomeres and .\nLinear stability analysis of this equation shows that the racemic state formula_7 is unstable. Starting from almost everywhere in the concentration space, the system evolves to a homochiral state.\n\nIt is generally understood that autocatalysis alone does not yield to homochirality, and the presence of the mutually antagonistic relationship between the two enantiomers is necessary for the instability of the racemic mixture. However, recent studies show that homochirality could be achieved from autocatalysis in the absence of the mutually antagonistic relationship, but the underlying mechanism for symmetry-breaking is different.\n\nThere are several laboratory experiments that demonstrate how a small amount of one enantiomer at the start of a reaction can lead to a large excess of a single enantiomer as the product. For example, the Soai reaction is autocatalytic. If the reaction is started with some of one of the product enantiomers already present, the product acts as an enantioselective catalyst for production of more of that same enantiomer. The initial presence of just 0.2 equivalent one enantiomer can lead to up to 93% enantiomeric excess of the product.\n\nAnother study concerns the proline catalyzed aminoxylation of propionaldehyde by nitrosobenzene. In this system, a small enantiomeric excess of catalyst leads to a large enantiomeric excess of product.\n\nSerine octamer clusters are also contenders. These clusters of 8 serine molecules appear in mass spectrometry with an unusual homochiral preference, however there is no evidence that such clusters exist under non-ionizing conditions and amino acid phase behavior is far more prebiotically relevant. The recent observation that partial sublimation of a 10% enantioenriched sample of leucine results in up to 82% enrichment in the sublimate shows that enantioenrichment of amino acids could occur in space. Partial sublimation processes can take place on the surface of meteors where large variations in temperature exist. This finding may have consequences for the development of the Mars Organic Detector scheduled for launch in 2013 which aims to recover trace amounts of amino acids from the Mars surface exactly by a sublimation technique.\n\nA high asymmetric amplification of the enantiomeric excess of sugars are also present in the amino acid catalyzed asymmetric formation of carbohydrates\n\nOne classic study involves an experiment that takes place in the laboratory. When sodium chlorate is allowed to crystallize from water and the collected crystals examined in a polarimeter, each crystal turns out to be chiral and either the form or the form. In an ordinary experiment the amount of crystals collected equals the amount of crystals (corrected for statistical effects). However, when the sodium chlorate solution is stirred during the crystallization process the crystals are either exclusively or exclusively . In 32 consecutive crystallization experiments 14 experiments deliver -crystals and 18 others -crystals. The explanation for this symmetry breaking is unclear but is related to autocatalysis taking place in the nucleation process.\n\nIn a related experiment, a crystal suspension of a racemic amino acid derivative continuously stirred, results in a 100% crystal phase of one of the enantiomers because the enantiomeric pair is able to equilibrate in solution (compare with dynamic kinetic resolution).\n\nMany strategies in asymmetric synthesis are built on chiral transmission. Especially important is the so-called organocatalysis of organic reactions by proline for example in Mannich reactions.\n\nThere exists no theory elucidating correlations among -amino acids. If one takes, for example, alanine, which has a small methyl group, and phenylalanine, which has a larger benzyl group, a simple question is in what aspect, -alanine resembles -phenylalanine more than -phenylalanine, and what kind of mechanism causes the selection of all -amino acids. Because it might be possible that alanine was and phenylalanine was .\n\nIt was reported in 2004 that excess racemic ,-asparagine (Asn), which spontaneously forms crystals of either isomer during recrystallization, induces asymmetric resolution of a co-existing racemic amino acid such as arginine (Arg), aspartic acid (Asp), glutamine (Gln), histidine (His), leucine (Leu), methionine (Met), phenylalanine (Phe), serine (Ser), valine (Val), tyrosine (Tyr), and tryptophan (Trp). The enantiomeric excess of these amino acids was correlated almost linearly with that of the inducer, i.e., Asn. When recrystallizations from a mixture of 12 ,-amino acids (Ala, Asp, Arg, Glu, Gln, His, Leu, Met, Ser, Val, Phe, and Tyr) and excess ,-Asn were made, all amino acids with the same configuration with Asn were preferentially co-crystallized. It was incidental whether the enrichment took place in - or -Asn, however, once the selection was made, the co-existing amino acid with the same configuration at the α-carbon was preferentially involved because of thermodynamic stability in the crystal formation. The maximal ee was reported to be 100%. Based on these results, it is proposed that a mixture of racemic amino acids causes spontaneous and effective optical resolution, even if asymmetric synthesis of a single amino acid does not occur without an aid of an optically active molecule.\n\nThis is the first study elucidating reasonably the formation of chirality from racemic amino acids with experimental evidences.\n\nThis term was introduced by Kelvin in 1904, the year that he published his Baltimore Lecture of 1884. Kelvin used the term homochirality as a relationship between two molecules, i.e. two molecule are homochiral if they have the same chirality. Recently, however, homochiral has been used in the same sense as enantiomerically pure. This is permitted in some journals (but not encouraged), its meaning changing into the preference of a process or system for a single optical isomer in a pair of isomers in these journals.\n\n\n"}
{"id": "18393", "url": "https://en.wikipedia.org/wiki?curid=18393", "title": "Life", "text": "Life\n\nLife is a characteristic that distinguishes physical entities that have biological processes, such as signaling and self-sustaining processes, from those that do not, either because such functions have ceased (they have died), or because they never had such functions and are classified as inanimate. Various forms of life exist, such as plants, animals, fungi, protists, archaea, and bacteria. The criteria can at times be ambiguous and may or may not define viruses, viroids, or potential synthetic life as \"living\". Biology is the science concerned with the study of life.\n\nThe definition of life is controversial. The current definition is that organisms are open systems that maintain homeostasis, are composed of cells, have a life cycle, undergo metabolism, can grow, adapt to their environment, respond to stimuli, reproduce and evolve. However, several other biological definitions have been proposed, and there are some borderline cases of life, such as viruses or viroids. In the past, there have been many attempts to define what is meant by \"life\" through obsolete concepts such as odic force, hylomorphism, spontaneous generation and vitalism, that have now been disproved by biological discoveries. Abiogenesis describes the natural process of life arising from non-living matter, such as simple organic compounds. Properties common to all organisms include the need for certain core chemical elements to sustain biochemical functions.\n\nLife on Earth first appeared as early as 4.28 billion years ago, soon after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. Earth's current life may have descended from an RNA world, although RNA-based life may not have been the first. The mechanism by which life began on Earth is unknown, though many hypotheses have been formulated and are often based on the Miller–Urey experiment. The earliest known life forms are microfossils of bacteria. 3.45 billion year old Australian rocks are reported to have contained microorganisms. In 2016, scientists reported identifying a set of 355 genes thought to be present in the last universal common ancestor (LUCA) of all living organisms, already a complex organism and not the first living thing.\n\nSince its primordial beginnings, life on Earth has changed its environment on a geologic time scale. To survive in most ecosystems, life must often adapt to a wide range of conditions. Some microorganisms, called extremophiles, thrive in physically or geochemically extreme environments that are detrimental to most other life on Earth. Aristotle was the first person to classify organisms. Later, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Eventually new groups and categories of life were discovered, such as cells and microorganisms, forcing dramatic revisions of the structure of relationships between living organisms. The cell is considered the structural and functional unit of life. There are two kinds of cells, prokaryotic and eukaryotic, both of which consist of cytoplasm enclosed within a membrane and contain many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division, in which the parent cell divides into two or more daughter cells.\n\nThough currently only known on Earth, life need not be restricted to it, and many scientists speculate in the existence of extraterrestrial life. Artificial life is a computer simulation or man-made reconstruction of any aspect of life, which is often used to examine systems related to natural life. Death is the permanent termination of all biological functions which sustain an organism, and as such, is the end of its life. Extinction is the process by which an entire group or taxon, normally a species, dies out. Fossils are the preserved remains or traces of organisms.\n\nThe definition of life has long been a challenge for scientists and philosophers, with many varied definitions put forward. This is partially because life is a process, not a substance. This is complicated by a lack of knowledge of the characteristics of living entities, if any, that may have developed outside of Earth. Philosophical definitions of life have also been put forward, with similar difficulties on how to distinguish living things from the non-living. Legal definitions of life have also been described and debated, though these generally focus on the decision to declare a human dead, and the legal ramifications of this decision.\n\nSince there is no unequivocal definition of life, most current definitions in biology are descriptive. Life is considered a characteristic of something that preserves, furthers or reinforces its existence in the given environment. This characteristic exhibits all or most of the following traits:\n\nThese complex processes, called physiological functions, have underlying physical and chemical bases, as well as signaling and control mechanisms that are essential to maintaining life.\n\nFrom a physics perspective, living beings are thermodynamic systems with an organized molecular structure that can reproduce itself and evolve as survival dictates. Thermodynamically, life has been described as an open system which makes use of gradients in its surroundings to create imperfect copies of itself. Hence, life is a self-sustained chemical system capable of undergoing Darwinian evolution. A major strength of this definition is that it distinguishes life by the evolutionary process rather than its chemical composition.\n\nOthers take a systemic viewpoint that does not necessarily depend on molecular chemistry. One systemic definition of life is that living things are self-organizing and autopoietic (self-producing). Variations of this definition include Stuart Kauffman's definition as an autonomous agent or a multi-agent system capable of reproducing itself or themselves, and of completing at least one thermodynamic work cycle. This definition is extended by the apparition of novel functions over time.\n\nWhether or not viruses should be considered as alive is controversial. They are most often considered as just replicators rather than forms of life. They have been described as \"organisms at the edge of life\" because they possess genes, evolve by natural selection, and replicate by creating multiple copies of themselves through self-assembly. However, viruses do not metabolize and they require a host cell to make new products. Virus self-assembly within host cells has implications for the study of the origin of life, as it may support the hypothesis that life could have started as self-assembling organic molecules.\n\nTo reflect the minimum phenomena required, other biological definitions of life have been proposed, with many of these being based upon chemical systems. Biophysicists have commented that living things function on negative entropy. In other words, living processes can be viewed as a delay of the spontaneous diffusion or dispersion of the internal energy of biological molecules towards more potential microstates. In more detail, according to physicists such as John Bernal, Erwin Schrödinger, Eugene Wigner, and John Avery, life is a member of the class of phenomena that are open or continuous systems able to decrease their internal entropy at the expense of substances or free energy taken in from the environment and subsequently rejected in a degraded form.\n\nLiving systems are open self-organizing living things that interact with their environment. These systems are maintained by flows of information, energy, and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory would arise out of the ecological and biological sciences and attempt to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nThe idea that the Earth is alive is found in philosophy and religion, but the first scientific discussion of it was by the Scottish scientist James Hutton. In 1785, he stated that the Earth was a superorganism and that its proper study should be physiology. Hutton is considered the father of geology, but his idea of a living Earth was forgotten in the intense reductionism of the 19th century. The Gaia hypothesis, proposed in the 1960s by scientist James Lovelock, suggests that life on Earth functions as a single organism that defines and maintains environmental conditions necessary for its survival. This hypothesis served as one of the foundations of the modern Earth system science.\n\nThe first attempt at a general living systems theory for explaining the nature of life was in 1978, by American biologist James Grier Miller. Robert Rosen (1991) built on this by defining a system component as \"a unit of organization; a part with a function, i.e., a definite relation between part and whole.\" From this and other starting concepts, he developed a \"relational theory of systems\" that attempts to explain the special properties of life. Specifically, he identified the \"nonfractionability of components in an organism\" as the fundamental difference between living systems and \"biological machines.\"\n\nA systems view of life treats environmental fluxes and biological fluxes together as a \"reciprocity of influence,\" and a reciprocal relation with environment is arguably as important for understanding life as it is for understanding ecosystems. As Harold J. Morowitz (1992) explains it, life is a property of an ecological system rather than a single organism or species. He argues that an ecosystemic definition of life is preferable to a strictly biochemical or physical one. Robert Ulanowicz (2009) highlights mutualism as the key to understand the systemic, order-generating behavior of life and ecosystems.\n\nComplex systems biology (CSB) is a field of science that studies the emergence of complexity in functional organisms from the viewpoint of dynamic systems theory. The latter is also often called systems biology and aims to understand the most fundamental aspects of life. A closely related approach to CSB and systems biology called relational biology is concerned mainly with understanding life processes in terms of the most important relations, and categories of such relations among the essential functional components of organisms; for multicellular organisms, this has been defined as \"categorical biology\", or a model representation of organisms as a category theory of biological relations, as well as an algebraic topology of the functional organization of living organisms in terms of their dynamic, complex networks of metabolic, genetic, and epigenetic processes and signaling pathways. Alternative but closely related approaches focus on the interdependance of constraints, where constraints can be either molecular, such as enzymes, or macroscopic, such as the geometry of a bone or of the vascular system.\n\nIt has also been argued that the evolution of order in living systems and certain physical systems obeys a common fundamental principle termed the Darwinian dynamic. The Darwinian dynamic was formulated by first considering how macroscopic order is generated in a simple non-biological system far from thermodynamic equilibrium, and then extending consideration to short, replicating RNA molecules. The underlying order-generating process was concluded to be basically similar for both types of systems.\n\nAnother systemic definition called the operator theory proposes that \"life is a general term for the presence of the typical closures found in organisms; the typical closures are a membrane and an autocatalytic set in the cell\" and that an organism is any system with an organisation that complies with an operator type that is at least as complex as the cell. Life can also be modeled as a network of inferior negative feedbacks of regulatory mechanisms subordinated to a superior positive feedback formed by the potential of expansion and reproduction.\n\nSome of the earliest theories of life were materialist, holding that all that exists is matter, and that life is merely a complex form or arrangement of matter. Empedocles (430 BC) argued that everything in the universe is made up of a combination of four eternal \"elements\" or \"roots of all\": earth, water, air, and fire. All change is explained by the arrangement and rearrangement of these four elements. The various forms of life are caused by an appropriate mixture of elements.\n\nDemocritus (460 BC) thought that the essential characteristic of life is having a soul (\"psyche\"). Like other ancient writers, he was attempting to explain what makes something a \"living\" thing. His explanation was that fiery atoms make a soul in exactly the same way atoms and void account for any other thing. He elaborates on fire because of the apparent connection between life and heat, and because fire moves.\n\nThe mechanistic materialism that originated in ancient Greece was revived and revised by the French philosopher René Descartes, who held that animals and humans were assemblages of parts that together functioned as a machine. In the 19th century, the advances in cell theory in biological science encouraged this view. The evolutionary theory of Charles Darwin (1859) is a mechanistic explanation for the origin of species by means of natural selection.\n\nHylomorphism is a theory first expressed by the Greek philosopher Aristotle (322 BC). The application of hylomorphism to biology was important to Aristotle, and biology is extensively covered in his extant writings. In this view, everything in the material universe has both matter and form, and the form of a living thing is its soul (Greek \"psyche\", Latin \"anima\"). There are three kinds of souls: the \"vegetative soul\" of plants, which causes them to grow and decay and nourish themselves, but does not cause motion and sensation; the \"animal soul\", which causes animals to move and feel; and the \"rational soul\", which is the source of consciousness and reasoning, which (Aristotle believed) is found only in man. Each higher soul has all of the attributes of the lower ones. Aristotle believed that while matter can exist without form, form cannot exist without matter, and that therefore the soul cannot exist without the body.\n\nThis account is consistent with teleological explanations of life, which account for phenomena in terms of purpose or goal-directedness. Thus, the whiteness of the polar bear's coat is explained by its purpose of camouflage. The direction of causality (from the future to the past) is in contradiction with the scientific evidence for natural selection, which explains the consequence in terms of a prior cause. Biological features are explained not by looking at future optimal results, but by looking at the past evolutionary history of a species, which led to the natural selection of the features in question.\n\nSpontaneous generation was the belief that living organisms can form without descent from similar organisms. Typically, the idea was that certain forms such as fleas could arise from inanimate matter such as dust or the supposed seasonal generation of mice and insects from mud or garbage.\n\nThe theory of spontaneous generation was proposed by Aristotle, who compiled and expanded the work of prior natural philosophers and the various ancient explanations of the appearance of organisms; it held sway for two millennia. It was decisively dispelled by the experiments of Louis Pasteur in 1859, who expanded upon the investigations of predecessors such as Francesco Redi. Disproof of the traditional ideas of spontaneous generation is no longer controversial among biologists.\n\nVitalism is the belief that the life-principle is non-material. This originated with Georg Ernst Stahl (17th century), and remained popular until the middle of the 19th century. It appealed to philosophers such as Henri Bergson, Friedrich Nietzsche, and Wilhelm Dilthey, anatomists like Marie François Xavier Bichat, and chemists like Justus von Liebig. Vitalism included the idea that there was a fundamental difference between organic and inorganic material, and the belief that organic material can only be derived from living things. This was disproved in 1828, when Friedrich Wöhler prepared urea from inorganic materials. This Wöhler synthesis is considered the starting point of modern organic chemistry. It is of historical significance because for the first time an organic compound was produced in inorganic reactions.\n\nDuring the 1850s, Hermann von Helmholtz, anticipated by Julius Robert von Mayer, demonstrated that no energy is lost in muscle movement, suggesting that there were no \"vital forces\" necessary to move a muscle. These results led to the abandonment of scientific interest in vitalistic theories, although the belief lingered on in pseudoscientific theories such as homeopathy, which interprets diseases and sickness as caused by disturbances in a hypothetical vital force or life force.\n\nThe age of the Earth is about 4.54 billion years. Evidence suggests that life on Earth has existed for at least 3.5 billion years, with the oldest physical traces of life dating back 3.7 billion years; however, some theories, such as the Late Heavy Bombardment theory, suggest that life on Earth may have started even earlier, as early as 4.1–4.4 billion years ago, and the chemistry leading to life may have begun shortly after the Big Bang, 13.8 billion years ago, during an epoch when the universe was only 10–17 million years old.\n\nMore than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct.\n\nAlthough the number of Earth's catalogued species of lifeforms is between 1.2 million and 2 million, the total number of species in the planet is uncertain. Estimates range from 8 million to 100 million, with a more narrow range between 10 and 14 million, but it may be as high as 1 trillion (with only one-thousandth of one percent of the species described) according to studies realized in May 2016. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10 and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon). In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\n\nAll known life forms share fundamental molecular mechanisms, reflecting their common descent; based on these observations, hypotheses on the origin of life attempt to find a mechanism explaining the formation of a universal common ancestor, from simple organic molecules via pre-cellular life to protocells and metabolism. Models have been divided into \"genes-first\" and \"metabolism-first\" categories, but a recent trend is the emergence of hybrid models that combine both categories.\n\nThere is no current scientific consensus as to how life originated. However, most accepted scientific models build on the Miller–Urey experiment and the work of Sidney Fox, which show that conditions on the primitive Earth favored chemical reactions that synthesize amino acids and other organic compounds from inorganic precursors, and phospholipids spontaneously form lipid bilayers, the basic structure of a cell membrane.\n\nLiving organisms synthesize proteins, which are polymers of amino acids using instructions encoded by deoxyribonucleic acid (DNA). Protein synthesis entails intermediary ribonucleic acid (RNA) polymers. One possibility for how life began is that genes originated first, followed by proteins; the alternative being that proteins came first and then genes.\n\nHowever, because genes and proteins are both required to produce the other, the problem of considering which came first is like that of the chicken or the egg. Most scientists have adopted the hypothesis that because of this, it is unlikely that genes and proteins arose independently.\n\nTherefore, a possibility, first suggested by Francis Crick, is that the first life was based on RNA, which has the DNA-like properties of information storage and the catalytic properties of some proteins. This is called the RNA world hypothesis, and it is supported by the observation that many of the most critical components of cells (those that evolve the slowest) are composed mostly or entirely of RNA. Also, many critical cofactors (ATP, Acetyl-CoA, NADH, etc.) are either nucleotides or substances clearly related to them. The catalytic properties of RNA had not yet been demonstrated when the hypothesis was first proposed, but they were confirmed by Thomas Cech in 1986.\n\nOne issue with the RNA world hypothesis is that synthesis of RNA from simple inorganic precursors is more difficult than for other organic molecules. One reason for this is that RNA precursors are very stable and react with each other very slowly under ambient conditions, and it has also been proposed that living organisms consisted of other molecules before RNA. However, the successful synthesis of certain RNA molecules under the conditions that existed prior to life on Earth has been achieved by adding alternative precursors in a specified order with the precursor phosphate present throughout the reaction. This study makes the RNA world hypothesis more plausible.\n\nGeological findings in 2013 showed that reactive phosphorus species (like phosphite) were in abundance in the ocean before 3.5 Ga, and that Schreibersite easily reacts with aqueous glycerol to generate phosphite and glycerol 3-phosphate. It is hypothesized that Schreibersite-containing meteorites from the Late Heavy Bombardment could have provided early reduced phosphorus, which could react with prebiotic organic molecules to form phosphorylated biomolecules, like RNA.\n\nIn 2009, experiments demonstrated Darwinian evolution of a two-component system of RNA enzymes (ribozymes) \"in vitro\". The work was performed in the laboratory of Gerald Joyce, who stated \"This is the first example, outside of biology, of evolutionary adaptation in a molecular genetic system.\"\n\nPrebiotic compounds may have originated extraterrestrially. NASA findings in 2011, based on studies with meteorites found on Earth, suggest DNA and RNA components (adenine, guanine and related organic molecules) may be formed in outer space.\n\nIn March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.\n\nAccording to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe.\n\nThe diversity of life on Earth is a result of the dynamic interplay between genetic opportunity, metabolic capability, environmental challenges, and symbiosis. For most of its existence, Earth's habitable environment has been dominated by microorganisms and subjected to their metabolism and evolution. As a consequence of these microbial activities, the physical-chemical environment on Earth has been changing on a geologic time scale, thereby affecting the path of evolution of subsequent life. For example, the release of molecular oxygen by cyanobacteria as a by-product of photosynthesis induced global changes in the Earth's environment. Because oxygen was toxic to most life on Earth at the time, this posed novel evolutionary challenges, and ultimately resulted in the formation of Earth's major animal and plant species. This interplay between organisms and their environment is an inherent feature of living systems.\n\nThe biosphere is the global sum of all ecosystems. It can also be termed as the zone of life on Earth, a closed system (apart from solar and cosmic radiation and heat from the interior of the Earth), and largely self-regulating. By the most general biophysiological definition, the biosphere is the global ecological system integrating all living beings and their relationships, including their interaction with the elements of the lithosphere, geosphere, hydrosphere, and atmosphere.\n\nLife forms live in every part of the Earth's biosphere, including soil, hot springs, inside rocks at least deep underground, the deepest parts of the ocean, and at least high in the atmosphere. Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space and to survive in the vacuum of outer space. Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans. Other researchers reported related studies that life forms thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States, as well as beneath the seabed off Japan. In August 2014, scientists confirmed the existence of life forms living below the ice of Antarctica. According to one researcher, \"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are.\"\n\nThe biosphere is postulated to have evolved, beginning with a process of biopoesis (life created naturally from non-living matter, such as simple organic compounds) or biogenesis (life created from living matter), at least some 3.5 billion years ago. The earliest evidence for life on Earth includes biogenic graphite found in 3.7 billion-year-old metasedimentary rocks from Western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone from Western Australia. More recently, in 2015, \"remains of biotic life\" were found in 4.1 billion-year-old rocks in Western Australia. In 2017, putative fossilized microorganisms (or microfossils) were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that were as old as 4.28 billion years, the oldest record of life on earth, suggesting \"an almost instantaneous emergence of life\" after ocean formation 4.4 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. According to biologist Stephen Blair Hedges, \"If life arose relatively quickly on Earth ... then it could be common in the universe.\"\n\nIn a general sense, biospheres are any closed, self-regulating systems containing ecosystems. This includes artificial biospheres such as Biosphere 2 and BIOS-3, and potentially ones on other planets or moons.\n\nThe inert components of an ecosystem are the physical and chemical factors necessary for life—energy (sunlight or chemical energy), water, heat, atmosphere, gravity, nutrients, and ultraviolet solar radiation protection. In most ecosystems, the conditions vary during the day and from one season to the next. To live in most ecosystems, then, organisms must be able to survive a range of conditions, called the \"range of tolerance.\" Outside that are the \"zones of physiological stress,\" where the survival and reproduction are possible but not optimal. Beyond these zones are the \"zones of intolerance,\" where survival and reproduction of that organism is unlikely or impossible. Organisms that have a wide range of tolerance are more widely distributed than organisms with a narrow range of tolerance.\n\nTo survive, selected microorganisms can assume forms that enable them to withstand freezing, complete desiccation, starvation, high levels of radiation exposure, and other physical or chemical challenges. These microorganisms may survive exposure to such conditions for weeks, months, years, or even centuries. Extremophiles are microbial life forms that thrive outside the ranges where life is commonly found. They excel at exploiting uncommon sources of energy. While all organisms are composed of nearly identical molecules, evolution has enabled such microbes to cope with this wide range of physical and chemical conditions. Characterization of the structure and metabolic diversity of microbial communities in such extreme environments is ongoing.\n\nMicrobial life forms thrive even in the Mariana Trench, the deepest spot in the Earth's oceans. Microbes also thrive inside rocks up to below the sea floor under of ocean.\n\nInvestigation of the tenacity and versatility of life on Earth, as well as an understanding of the molecular systems that some organisms utilize to survive such extremes, is important for the search for life beyond Earth. For example, lichen could survive for a month in a simulated Martian environment.\n\nAll life forms require certain core chemical elements needed for biochemical functioning. These include carbon, hydrogen, nitrogen, oxygen, phosphorus, and sulfur—the elemental macronutrients for all organisms—often represented by the acronym CHNOPS. Together these make up nucleic acids, proteins and lipids, the bulk of living matter. Five of these six elements comprise the chemical components of DNA, the exception being sulfur. The latter is a component of the amino acids cysteine and methionine. The most biologically abundant of these elements is carbon, which has the desirable attribute of forming multiple, stable covalent bonds. This allows carbon-based (organic) molecules to form an immense variety of chemical arrangements. Alternative hypothetical types of biochemistry have been proposed that eliminate one or more of these elements, swap out an element for one not on the list, or change required chiralities or other chemical properties.\n\nDeoxyribonucleic acid is a molecule that carries most of the genetic instructions used in the growth, development, functioning and reproduction of all known living organisms and many viruses. DNA and RNA are nucleic acids; alongside proteins and complex carbohydrates, they are one of the three major types of macromolecule that are essential for all known forms of life. Most DNA molecules consist of two biopolymer strands coiled around each other to form a double helix. The two DNA strands are known as polynucleotides since they are composed of simpler units called nucleotides. Each nucleotide is composed of a nitrogen-containing nucleobase—either cytosine (C), guanine (G), adenine (A), or thymine (T)—as well as a sugar called deoxyribose and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. According to base pairing rules (A with T, and C with G), hydrogen bonds bind the nitrogenous bases of the two separate polynucleotide strands to make double-stranded DNA. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10, and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).\n\nDNA stores biological information. The DNA backbone is resistant to cleavage, and both strands of the double-stranded structure store the same biological information. Biological information is replicated as the two strands are separated. A significant portion of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences.\n\nThe two strands of DNA run in opposite directions to each other and are therefore anti-parallel. Attached to each sugar is one of four types of nucleobases (informally, \"bases\"). It is the sequence of these four nucleobases along the backbone that encodes biological information. Under the genetic code, RNA strands are translated to specify the sequence of amino acids within proteins. These RNA strands are initially created using DNA strands as a template in a process called transcription.\n\nWithin cells, DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.\n\nDNA was first isolated by Friedrich Miescher in 1869. Its molecular structure was identified by James Watson and Francis Crick in 1953, whose model-building efforts were guided by X-ray diffraction data acquired by Rosalind Franklin.\n\nLife is usually classified by eight levels of taxa—domains, kingdoms, phyla, class, order, family, genus, and species. In May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.\n\nThe first known attempt to classify organisms was conducted by the Greek philosopher Aristotle (384–322 BC), who classified all living organisms known at that time as either a plant or an animal, based mainly on their ability to move. He also distinguished animals with blood from animals without blood (or at least without red blood), which can be compared with the concepts of vertebrates and invertebrates respectively, and divided the blooded animals into five groups: viviparous quadrupeds (mammals), oviparous quadrupeds (reptiles and amphibians), birds, fishes and whales. The bloodless animals were also divided into five groups: cephalopods, crustaceans, insects (which included the spiders, scorpions, and centipedes, in addition to what we define as insects today), shelled animals (such as most molluscs and echinoderms), and \"zoophytes\" (animals that resemble plants). Though Aristotle's work in zoology was not without errors, it was the grandest biological synthesis of the time and remained the ultimate authority for many centuries after his death.\n\nThe exploration of the Americas revealed large numbers of new plants and animals that needed descriptions and classification. In the latter part of the 16th century and the beginning of the 17th, careful study of animals commenced and was gradually extended until it formed a sufficient body of knowledge to serve as an anatomical basis for classification. In the late 1740s, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Linnaeus attempted to improve the composition and reduce the length of the previously used many-worded names by abolishing unnecessary rhetoric, introducing new descriptive terms and precisely defining their meaning.\n\nThe fungi were originally treated as plants. For a short period Linnaeus had classified them in the taxon Vermes in Animalia, but later placed them back in Plantae. Copeland classified the Fungi in his Protoctista, thus partially avoiding the problem but acknowledging their special status. The problem was eventually solved by Whittaker, when he gave them their own kingdom in his five-kingdom system. Evolutionary history shows that the fungi are more closely related to animals than to plants.\n\nAs new discoveries enabled detailed study of cells and microorganisms, new groups of life were revealed, and the fields of cell biology and microbiology were created. These new organisms were originally described separately in protozoa as animals and protophyta/thallophyta as plants, but were united by Haeckel in the kingdom Protista; later, the prokaryotes were split off in the kingdom Monera, which would eventually be divided into two separate groups, the Bacteria and the Archaea. This led to the six-kingdom system and eventually to the current three-domain system, which is based on evolutionary relationships. However, the classification of eukaryotes, especially of protists, is still controversial.\n\nAs microbiology, molecular biology and virology developed, non-cellular reproducing agents were discovered, such as viruses and viroids. Whether these are considered alive has been a matter of debate; viruses lack characteristics of life such as cell membranes, metabolism and the ability to grow or respond to their environments. Viruses can still be classed into \"species\" based on their biology and genetics, but many aspects of such a classification remain controversial.\n\nIn the 1960s a trend called cladistics emerged, arranging taxa based on clades in an evolutionary or phylogenetic tree.\n\nIn systems of scientific classification, Biota is the superdomain that classifies all life.\n\nCells are the basic unit of structure in every living thing, and all cells arise from pre-existing cells by division. Cell theory was formulated by Henri Dutrochet, Theodor Schwann, Rudolf Virchow and others during the early nineteenth century, and subsequently became widely accepted. The activity of an organism depends on the total activity of its cells, with energy flow occurring within and between them. Cells contain hereditary information that is carried forward as a genetic code during cell division.\n\nThere are two primary types of cells. Prokaryotes lack a nucleus and other membrane-bound organelles, although they have circular DNA and ribosomes. Bacteria and Archaea are two domains of prokaryotes. The other primary type of cells are the eukaryotes, which have distinct nuclei bound by a nuclear membrane and membrane-bound organelles, including mitochondria, chloroplasts, lysosomes, rough and smooth endoplasmic reticulum, and vacuoles. In addition, they possess organized chromosomes that store genetic material. All species of large complex organisms are eukaryotes, including animals, plants and fungi, though most species of eukaryote are protist microorganisms. The conventional model is that eukaryotes evolved from prokaryotes, with the main organelles of the eukaryotes forming through endosymbiosis between bacteria and the progenitor eukaryotic cell.\n\nThe molecular mechanisms of cell biology are based on proteins. Most of these are synthesized by the ribosomes through an enzyme-catalyzed process called protein biosynthesis. A sequence of amino acids is assembled and joined together based upon gene expression of the cell's nucleic acid. In eukaryotic cells, these proteins may then be transported and processed through the Golgi apparatus in preparation for dispatch to their destination.\n\nCells reproduce through a process of cell division in which the parent cell divides into two or more daughter cells. For prokaryotes, cell division occurs through a process of fission in which the DNA is replicated, then the two copies are attached to parts of the cell membrane. In eukaryotes, a more complex process of mitosis is followed. However, the end result is the same; the resulting cell copies are identical to each other and to the original cell (except for mutations), and both are capable of further division following an interphase period.\n\nMulticellular organisms may have first evolved through the formation of colonies of identical cells. These cells can form group organisms through cell adhesion. The individual members of a colony are capable of surviving on their own, whereas the members of a true multi-cellular organism have developed specializations, making them dependent on the remainder of the organism for survival. Such organisms are formed clonally or from a single germ cell that is capable of forming the various specialized cells that form the adult organism. This specialization allows multicellular organisms to exploit resources more efficiently than single cells. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule, called GK-PID, may have allowed organisms to go from a single cell organism to one of many cells.\n\nCells have evolved methods to perceive and respond to their microenvironment, thereby enhancing their adaptability. Cell signaling coordinates cellular activities, and hence governs the basic functions of multicellular organisms. Signaling between cells can occur through direct cell contact using juxtacrine signalling, or indirectly through the exchange of agents as in the endocrine system. In more complex organisms, coordination of activities can occur through a dedicated nervous system.\n\nThough life is confirmed only on Earth, many think that extraterrestrial life is not only plausible, but probable or inevitable. Other planets and moons in the Solar System and other planetary systems are being examined for evidence of having once supported simple life, and projects such as SETI are trying to detect radio transmissions from possible alien civilizations. Other locations within the Solar System that may host microbial life include the subsurface of Mars, the upper atmosphere of Venus, and subsurface oceans on some of the moons of the giant planets.\nBeyond the Solar System, the region around another main-sequence star that could support Earth-like life on an Earth-like planet is known as the habitable zone. The inner and outer radii of this zone vary with the luminosity of the star, as does the time interval during which the zone survives. Stars more massive than the Sun have a larger habitable zone, but remain on the Sun-like \"main sequence\" of stellar evolution for a shorter time interval. Small red dwarfs have the opposite problem, with a smaller habitable zone that is subject to higher levels of magnetic activity and the effects of tidal locking from close orbits. Hence, stars in the intermediate mass range such as the Sun may have a greater likelihood for Earth-like life to develop. The location of the star within a galaxy may also affect the likelihood of life forming. Stars in regions with a greater abundance of heavier elements that can form planets, in combination with a low rate of potentially habitat-damaging supernova events, are predicted to have a higher probability of hosting planets with complex life. The variables of the Drake equation are used to discuss the conditions in planetary systems where civilization is most likely to exist. Use of the equation to predict the amount of extraterrestrial life, however, is difficult; because many of the variables are unknown, the equation functions as more of a mirror to what its user already thinks. As a result, the number of civilizations in the galaxy can be estimated as low as 9.1 x 10 or as high as 156 million; for the calculations, see Drake equation.\n\nArtificial life is the simulation of any aspect of life, as through computers, robotics, or biochemistry. The study of artificial life imitates traditional biology by recreating some aspects of biological phenomena. Scientists study the logic of living systems by creating artificial environments—seeking to understand the complex information processing that defines such systems. While life is, by definition, alive, artificial life is generally referred to as data confined to a digital environment and existence.\n\nSynthetic biology is a new area of biotechnology that combines science and biological engineering. The common goal is the design and construction of new biological functions and systems not found in nature. Synthetic biology includes the broad redefinition and expansion of biotechnology, with the ultimate goals of being able to design and build engineered biological systems that process information, manipulate chemicals, fabricate materials and structures, produce energy, provide food, and maintain and enhance human health and the environment.\n\nDeath is the permanent termination of all vital functions or life processes in an organism or cell. It can occur as a result of an accident, medical conditions, biological interaction, malnutrition, poisoning, senescence, or suicide. After death, the remains of an organism re-enter the biogeochemical cycle. Organisms may be consumed by a predator or a scavenger and leftover organic material may then be further decomposed by detritivores, organisms that recycle detritus, returning it to the environment for reuse in the food chain.\n\nOne of the challenges in defining death is in distinguishing it from life. Death would seem to refer to either the moment life ends, or when the state that follows life begins. However, determining when death has occurred is difficult, as cessation of life functions is often not simultaneous across organ systems. Such determination therefore requires drawing conceptual lines between life and death. This is problematic, however, because there is little consensus over how to define life. The nature of death has for millennia been a central concern of the world's religious traditions and of philosophical inquiry. Many religions maintain faith in either a kind of afterlife or reincarnation for the soul, or resurrection of the body at a later date.\n\nExtinction is the process by which a group of taxa or species dies out, reducing biodiversity. The moment of extinction is generally considered the death of the last individual of that species. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively after a period of apparent absence. Species become extinct when they are no longer able to survive in changing habitat or against superior competition. In Earth's history, over 99% of all the species that have ever lived are extinct; however, mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.\n\nFossils are the preserved remains or traces of animals, plants, and other organisms from the remote past. The totality of fossils, both discovered and undiscovered, and their placement in fossil-containing rock formations and sedimentary layers (strata) is known as the \"fossil record\". A preserved specimen is called a fossil if it is older than the arbitrary date of 10,000 years ago. Hence, fossils range in age from the youngest at the start of the Holocene Epoch to the oldest from the Archaean Eon, up to 3.4 billion years old.\n\n\n"}
{"id": "183290", "url": "https://en.wikipedia.org/wiki?curid=183290", "title": "Life extension", "text": "Life extension\n\nLife extension is the idea of extending the human lifespan, either modestly – through improvements in medicine – or dramatically by increasing the maximum lifespan beyond its generally settled limit of 125 years. The ability to achieve such dramatic changes, however, does not currently exist.\n\nSome researchers in this area, and \"life extensionists\", \"immortalists\" or \"longevists\" (those who wish to achieve longer lives themselves), believe that future breakthroughs in tissue rejuvenation, stem cells, regenerative medicine, molecular repair, gene therapy, pharmaceuticals, and organ replacement (such as with artificial organs or xenotransplantations) will eventually enable humans to have indefinite lifespans (agerasia) through complete rejuvenation to a healthy youthful condition. The ethical ramifications, if life extension becomes a possibility, are debated by bioethicists.\n\nThe sale of purported anti-aging products such as supplements and hormone replacement is a lucrative global industry. For example, the industry that promotes the use of hormones as a treatment for consumers to slow or reverse the aging process in the US market generated about $50 billion of revenue a year in 2009. The use of such products has not been proven to be effective or safe.\n\nDuring the process of aging, an organism accumulates damage to its macromolecules, cells, tissues, and organs. Specifically, aging is characterized as and thought to be caused by \"genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication.\" Oxidation damage to cellular contents caused by free radicals is believed to contribute to aging as well.\n\nThe longest a human has ever been proven to live is 122 years, the case of Jeanne Calment who was born in 1875 and died in 1997, whereas the maximum lifespan of a wildtype mouse, commonly used as a model in research on aging, is about three years. Genetic differences between humans and mice that may account for these different aging rates include differences in efficiency of DNA repair, antioxidant defenses, energy metabolism, proteostasis maintenance, and recycling mechanisms such as autophagy.\n\nAverage lifespan in a population is lowered by infant and child mortality, which are frequently linked to infectious diseases or nutrition problems. Later in life, vulnerability to accidents and age-related chronic disease such as cancer or cardiovascular disease play an increasing role in mortality. Extension of expected lifespan can often be achieved by access to improved medical care, vaccinations, good diet, exercise and avoidance of hazards such as smoking.\n\nMaximum lifespan is determined by the rate of aging for a species inherent in its genes and by environmental factors. Widely recognized methods of extending maximum lifespan in model organisms such as nematodes, fruit flies, and mice include caloric restriction, gene manipulation, and administration of pharmaceuticals. Another technique uses evolutionary pressures such as breeding from only older members or altering levels of extrinsic mortality.\nSome animals such as hydra, planarian flatworms, and certain sponges, corals, and jellyfish do not die of old age and exhibit potential immortality.\n\nMuch life extension research focuses on nutrition—diets or supplements— although there is little evidence that they have an effect. The many diets promoted by anti-aging advocates are often contradictory. \n\nIn some studies calorie restriction has been shown to extend the life of mice, yeast, and rhesus monkeys. However, a more recent study did not find calorie restriction to improve survival in rhesus monkeys. In humans the long-term health effects of moderate caloric restriction with sufficient nutrients are unknown.\n\nThe free-radical theory of aging suggests that antioxidant supplements might extend human life. However, evidence suggest that β-carotene supplements and high doses of vitamin E increase mortality rates. Resveratrol is a sirtuin stimulant that has been shown to extend life in animal models, but the effect of resveratrol on lifespan in humans is unclear as of 2011.\n\nThe anti-aging industry offers several hormone therapies. Some of these have been criticized for possible dangers and a lack of proven effect. For example, the American Medical Association has been critical of some anti-aging hormone therapies.\n\nWhile growth hormone (GH) decreases with age, the evidence for use of growth hormone as an anti-aging therapy is mixed and based mostly on animal studies. There are mixed reports that GH or IGF-1 modulates the aging process in humans and about whether the direction of its effect is positive or negative.\n\nThe extension of life has been a desire of humanity and a mainstay motif in the history of scientific pursuits and ideas throughout history, from the Sumerian Epic of Gilgamesh and the Egyptian Smith medical papyrus, all the way through the Taoists, Ayurveda practitioners, alchemists, hygienists such as Luigi Cornaro, Johann Cohausen and Christoph Wilhelm Hufeland, and philosophers such as Francis Bacon, René Descartes, Benjamin Franklin and Nicolas Condorcet. However, the beginning of the modern period in this endeavor can be traced to the end of the 19th – beginning of the 20th century, to the so-called \"fin-de-siècle\" (end of the century) period, denoted as an \"end of an epoch\" and characterized by the rise of scientific optimism and therapeutic activism, entailing the pursuit of life extension (or life-extensionism). Among the foremost researchers of life extension at this period were the Nobel Prize winning biologist Elie Metchnikoff (1845-1916) -- the author of the cell theory of immunity and vice director of Institut Pasteur in Paris, and Charles-Édouard Brown-Séquard (1817-1894) -- the president of the French Biological Society and one of the founders of modern endocrinology.\n\nSociologist James Hughes claims that science has been tied to a cultural narrative of conquering death since the Age of Enlightenment. He cites Francis Bacon (1561–1626) as an advocate of using science and reason to extend human life, noting Bacon's novel \"New Atlantis\", wherein scientists worked toward delaying aging and prolonging life. Robert Boyle (1627–1691), founding member of the Royal Society, also hoped that science would make substantial progress with life extension, according to Hughes, and proposed such experiments as \"to replace the blood of the old with the blood of the young\". Biologist Alexis Carrel (1873–1944) was inspired by a belief in indefinite human lifespan that he developed after experimenting with cells, says Hughes.\n\nIn 1970, the American Aging Association was formed under the impetus of Denham Harman, originator of the free radical theory of aging. Harman wanted an organization of biogerontologists that was devoted to research and to the sharing of information among scientists interested in extending human lifespan.\n\nIn 1976, futurists Joel Kurtzman and Philip Gordon wrote \"No More Dying. The Conquest Of Aging And The Extension Of Human Life\", () the first popular book on research to extend human lifespan. Subsequently, Kurtzman was invited to testify before the House Select Committee on Aging, chaired by Claude Pepper of Florida, to discuss the impact of life extension on the Social Security system.\n\nSaul Kent published \"The Life Extension Revolution\" () in 1980 and created a nutraceutical firm called the Life Extension Foundation, a non-profit organization that promotes dietary supplements. The Life Extension Foundation publishes a periodical called \"Life Extension Magazine\". The 1982 bestselling book \"\" () by Durk Pearson and Sandy Shaw further popularized the phrase \"life extension\".\n\nRegulatory and legal struggles between the Food and Drug Administration (FDA) and the Life Extension Foundation included seizure of merchandise and court action. In 1991, Saul Kent and Bill Faloon, the principals of the Foundation, were jailed. The LEF accused the FDA of perpetrating a \"Holocaust\" and \"seeking gestapo-like power\" through its regulation of drugs and marketing claims.\n\nIn 2003, Doubleday published \"The Immortal Cell: One Scientist's Quest to Solve the Mystery of Human Aging,\" by Michael D. West. West emphasised the potential role of embryonic stem cells in life extension.\n\nOther modern life extensionists include writer Gennady Stolyarov, who insists that death is \"the enemy of us all, to be fought with medicine, science, and technology\"; transhumanist philosopher Zoltan Istvan, who proposes that the \"transhumanist must safeguard one's own existence above all else\"; futurist George Dvorsky, who considers aging to be a problem that desperately needs to be solved; and recording artist Steve Aoki, who has been called \"one of the most prolific campaigners for life extension\".\n\nIn 1991, the American Academy of Anti-Aging Medicine (A4M) was formed. The American Board of Medical Specialties recognizes neither anti-aging medicine nor the A4M's professional standing.\n\nIn 2003, Aubrey de Grey and David Gobel formed the Methuselah Foundation, which gives financial grants to anti-aging research projects. In 2009, de Grey and several others founded the SENS Research Foundation, a California-based scientific research organization which conducts research into aging and funds other anti-aging research projects at various universities. In 2013, Google announced Calico, a new company based in San Francisco that will harness new technologies to increase scientific understanding of the biology of aging. It is led by Arthur D. Levinson, and its research team includes scientists such as Hal V. Barron, David Botstein, and Cynthia Kenyon. In 2014, biologist Craig Venter founded Human Longevity Inc., a company dedicated to scientific research to end aging through genomics and cell therapy. They received funding with the goal of compiling a comprehensive human genotype, microbiome, and phenotype database.\n\nAside from private initiatives, aging research is being conducted in university laboratories, and includes universities such as Harvard and UCLA. University researchers have made a number of breakthroughs in extending the lives of mice and insects by reversing certain aspects of aging.\n\nPolitics relevant to the substances of life extension pertain mostly to communications and availability.\n\nIn the United States, product claims on food and drug labels are strictly regulated. The First Amendment (freedom of speech) protects third-party publishers' rights to distribute fact, opinion and speculation on life extension practices. Manufacturers and suppliers also provide informational publications, but because they market the substances, they are subject to monitoring and enforcement by the Federal Trade Commission (FTC), which polices claims by marketers. What constitutes the difference between truthful and false claims is hotly debated and is a central controversy in this arena.\n\nSome critics dispute the portrayal of aging as a disease. For example, Leonard Hayflick, who determined that fibroblasts are limited to around 50 cell divisions, reasons that aging is an unavoidable consequence of entropy. Hayflick and fellow biogerontologists Jay Olshansky and Bruce Carnes have strongly criticized the anti-aging industry in response to what they see as unscrupulous profiteering from the sale of unproven anti-aging supplements.\n\nResearch by Sobh and Martin (2011) suggests that people buy anti-aging products to obtain a hoped-for self (e.g., keeping a youthful skin) or to avoid a feared-self (e.g., looking old). The research shows that when consumers pursue a hoped-for self, it is expectations of success that most strongly drive their motivation to use the product. The research also shows why doing badly when trying to avoid a feared self is more motivating than doing well. When product use is seen to fail it is more motivating than success when consumers seek to avoid a feared-self.\n\nThough many scientists state that life extension and radical life extension are possible, there are still no international or national programs focused on radical life extension. There are political forces staying for and against life extension. By 2012, in Russia, the United States, Israel, and the Netherlands, the Longevity political parties started. They aimed to provide political support to radical life extension research and technologies, and ensure the fastest possible and at the same time soft transition of society to the next step – life without aging and with radical life extension, and to provide access to such technologies to most currently living people.\n\nSome tech innovators and Silicon Valley entrepreneurs have invested heavily into anti-aging research. This includes Larry Ellison (founder of Oracle), Peter Thiel (former Paypal CEO), Larry Page (co-founder of Google), and Peter Diamandis.\n\nLeon Kass (chairman of the US President's Council on Bioethics from 2001 to 2005) has questioned whether potential exacerbation of overpopulation problems would make life extension unethical. He states his opposition to life extension with the words:\nJohn Harris, former editor-in-chief of the Journal of Medical Ethics, argues that as long as life is worth living, according to the person himself, we have a powerful moral imperative to save the life and thus to develop and offer life extension therapies to those who want them.\n\nTranshumanist philosopher Nick Bostrom has argued that any technological advances in life extension must be equitably distributed and not restricted to a privileged few. In an extended metaphor entitled \"The Fable of the Dragon-Tyrant\", Bostrom envisions death as a monstrous dragon who demands human sacrifices. In the fable, after a lengthy debate between those who believe the dragon is a fact of life and those who believe the dragon can and should be destroyed, the dragon is finally killed. Bostrom argues that political inaction allowed many preventable human deaths to occur.\n\nControversy about life extension is due to fear of overpopulation and possible effects on society. Biogerontologist Aubrey De Grey counters the overpopulation critique by pointing out that the therapy could postpone or eliminate menopause, allowing women to space out their pregnancies over more years and thus \"decreasing\" the yearly population growth rate. Moreover, the philosopher and futurist Max More argues that, given the fact the worldwide population growth rate is slowing down and is projected to eventually stabilize and begin falling, superlongevity would be unlikely to contribute to overpopulation.\n\nA Spring 2013 Pew Research poll in the United States found that 38% of Americans would want life extension treatments, and 56% would reject it. However, it also found that 68% believed most people would want it and that only 4% consider an \"ideal lifespan\" to be more than 120 years. The median \"ideal lifespan\" was 91 years of age and the majority of the public (63%) viewed medical advances aimed at prolonging life as generally good. 41% of Americans believed that radical life extension (RLE) would be good for society, while 51% said they believed it would be bad for society. One possibility for why 56% of Americans claim they would reject life extension treatments may be due to the cultural perception that living longer would result in a longer period of decrepitude, and that the elderly in our current society are unhealthy.\n\nReligious people are no more likely to oppose life extension than the unaffiliated, though some variation exists between religious denominations.\n\nMainstream medical organizations and practitioners do not consider aging to be a disease. David Sinclair says: \"I don't see aging as a disease, but as a collection of quite predictable diseases caused by the deterioration of the body\". The two main arguments used are that aging is both inevitable and universal while diseases are not. However, not everyone agrees. Harry R. Moody, director of academic affairs for AARP, notes that what is normal and what is disease strongly depend on a historical context. David Gems, assistant director of the Institute of Healthy Ageing, argues that aging should be viewed as a disease. In response to the universality of aging, David Gems notes that it is as misleading as arguing that Basenji are not dogs because they do not bark. Because of the universality of aging he calls it a \"special sort of disease\". Robert M. Perlman, coined the terms \"aging syndrome\" and \"disease complex\" in 1954 to describe aging.\n\nThe discussion whether aging should be viewed as a disease or not has important implications. One view is, this would stimulate pharmaceutical companies to develop life extension therapies and in the United States of America, it would also increase the regulation of the anti-aging market by the FDA. Anti-aging now falls under the regulations for cosmetic medicine which are less tight than those for drugs.\n\nTheoretically, extension of maximum lifespan in humans could be achieved by reducing the rate of aging damage by periodic replacement of damaged tissues, molecular repair or rejuvenation of deteriorated cells and tissues, reversal of harmful epigenetic changes, or the enhancement of enzyme telomerase activity.\n\nResearch geared towards life extension strategies in various organisms is currently under way at a number of academic and private institutions. Since 2009, investigators have found ways to increase the lifespan of nematode worms and yeast by 10-fold; the record in nematodes was achieved through genetic engineering and the extension in yeast by a combination of genetic engineering and caloric restriction. A 2009 review of longevity research noted: \"Extrapolation from worms to mammals is risky at best, and it cannot be assumed that interventions will result in comparable life extension factors. Longevity gains from dietary restriction, or from mutations studied previously, yield smaller benefits to Drosophila than to nematodes, and smaller still to mammals. This is not unexpected, since mammals have evolved to live many times the worm's lifespan, and humans live nearly twice as long as the next longest-lived primate. From an evolutionary perspective, mammals and their ancestors have already undergone several hundred million years of natural selection favoring traits that could directly or indirectly favor increased longevity, and may thus have already settled on gene sequences that promote lifespan. Moreover, the very notion of a \"life-extension factor\" that could apply across taxa presumes a linear response rarely seen in biology.\"\n\nThere are a number of chemicals intended to slow the aging process currently being studied in animal models. One type of research is related to the observed effects of a calorie restriction (CR) diet, which has been shown to extend lifespan in some animals. Based on that research, there have been attempts to develop drugs that will have the same effect on the aging process as a caloric restriction diet, which are known as Caloric restriction mimetic drugs. Some drugs that are already approved for other uses have been studied for possible longevity effects on laboratory animals because of a possible CR-mimic effect; they include rapamycin, metformin and other geroprotectors. MitoQ, resveratrol and pterostilbene are dietary supplements that have also been studied in this context.\n\nOther attempts to create anti-aging drugs have taken different research paths. One notable direction of research has been research into the possibility of using the enzyme telomerase in order to counter the process of telomere shortening. However, there are potential dangers in this, since some research has also linked telomerase to cancer and to tumor growth and formation.\n\nFuture advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair machines, including ones operating within cells and utilizing as yet hypothetical molecular computers, in his 1986 book Engines of Creation. Raymond Kurzweil, a futurist and transhumanist, stated in his book \"The Singularity Is Near\" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a \"medical\" use for Feynman's theoretical nanomachines (see biological machine). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"swallow the doctor\". The idea was incorporated into Feynman's 1959 essay \"There's Plenty of Room at the Bottom.\"\n\nSome life extensionists suggest that therapeutic cloning and stem cell research could one day provide a way to generate cells, body parts, or even entire bodies (generally referred to as reproductive cloning) that would be genetically identical to a prospective patient. Recently, the US Department of Defense initiated a program to research the possibility of growing human body parts on mice. Complex biological structures, such as mammalian joints and limbs, have not yet been replicated. Dog and primate brain transplantation experiments were conducted in the mid-20th century but failed due to rejection and the inability to restore nerve connections. As of 2006, the implantation of bio-engineered bladders grown from patients' own cells has proven to be a viable treatment for bladder disease. Proponents of body part replacement and cloning contend that the required biotechnologies are likely to appear earlier than other life-extension technologies.\n\nThe use of human stem cells, particularly embryonic stem cells, is controversial. Opponents' objections generally are based on interpretations of religious teachings or ethical considerations. Proponents of stem cell research point out that cells are routinely formed and destroyed in a variety of contexts. Use of stem cells taken from the umbilical cord or parts of the adult body may not provoke controversy.\n\nThe controversies over cloning are similar, except general public opinion in most countries stands in opposition to reproductive cloning. Some proponents of therapeutic cloning predict the production of whole bodies, lacking consciousness, for eventual brain transplantation.\n\nReplacement of biological (susceptible to diseases) organs with mechanical ones could extend life. This is the goal of the 2045 Initiative.\n\nFor cryonicists (advocates of cryopreservation), storing the body at low temperatures after death may provide an \"ambulance\" into a future in which advanced medical technologies may allow resuscitation and repair. They speculate cryogenic temperatures will minimize changes in biological tissue for many years, giving the medical community ample time to cure all disease, rejuvenate the aged and repair any damage that is caused by the cryopreservation process.\n\nMany cryonicists do not believe that legal death is \"real death\" because stoppage of heartbeat and breathing—the usual medical criteria for legal death—occur before biological death of cells and tissues of the body. Even at room temperature, cells may take hours to die and days to decompose. Although neurological damage occurs within 4–6 minutes of cardiac arrest, the irreversible neurodegenerative processes do not manifest for hours. Cryonicists state that rapid cooling and cardio-pulmonary support applied immediately after certification of death can preserve cells and tissues for long-term preservation at cryogenic temperatures. People, particularly children, have survived up to an hour without heartbeat after submersion in ice water. In one case, full recovery was reported after 45 minutes underwater. To facilitate rapid preservation of cells and tissue, cryonics \"standby teams\" are available to wait by the bedside of patients who are to be cryopreserved to apply cooling and cardio-pulmonary support as soon as possible after declaration of death.\n\nNo mammal has been successfully cryopreserved and brought back to life, with the exception of frozen human embryos. Resuscitation of a postembryonic human from cryonics is not possible with current science. Some scientists still support the idea based on their expectations of the capabilities of future science.\n\nAnother proposed life extension technology would combine existing and predicted future biochemical and genetic techniques. SENS proposes that rejuvenation may be obtained by removing aging damage via the use of stem cells and tissue engineering, telomere-lengthening machinery, allotopic expression of mitochondrial proteins, targeted ablation of cells, immunotherapeutic clearance, and novel lysosomal hydrolases.\n\nWhile many biogerontologists find these ideas \"worthy of discussion\" and SENS conferences feature important research in the field, some contend that the alleged benefits are too speculative given the current state of technology, referring to it as \"fantasy rather than science\".\n\nGenome editing, in which nucleic acid polymers are delivered as a drug and are either expressed as proteins, interfere with the expression of proteins, or correct genetic mutations, has been proposed as a future strategy to prevent aging.\n\nA large array of genetic modifications have been found to increase lifespan in model organisms such as yeast, nematode worms, fruit flies, and mice. As of 2013, the longest extension of life caused by a single gene manipulation was roughly 50% in mice and 10-fold in nematode worms.\n\nIn \"The Selfish Gene\", Richard Dawkins describes an approach to life-extension that involves \"fooling genes\" into thinking the body is young. Dawkins attributes inspiration for this idea to Peter Medawar. The basic idea is that our bodies are composed of genes that activate throughout our lifetimes, some when we are young and others when we are older. Presumably, these genes are activated by environmental factors, and the changes caused by these genes activating can be lethal. It is a statistical certainty that we possess more lethal genes that activate in later life than in early life. Therefore, to extend life, we should be able to prevent these genes from switching on, and we should be able to do so by \"identifying changes in the internal chemical environment of a body that take place during aging... and by simulating the superficial chemical properties of a young body\".\n\nOne hypothetical future strategy that, as some suggest, \"eliminates\" the complications related to a physical body, involves the copying or transferring (e.g. by progressively replacing neurons with transistors) of a conscious mind from a biological brain to a non-biological computer system or computational device. The basic idea is to scan the structure of a particular brain in detail, and then construct a software model of it that is so faithful to the original that, when run on appropriate hardware, it will behave in essentially the same way as the original brain. Whether or not an exact copy of one's mind constitutes actual life extension is matter of debate.\n\nSome scientists believe that the dead may one day be \"resurrected\" through simulation technology.\n\nSome clinics currently offer injection of blood products from young donors. The alleged benefits of the treatment, none of which have been demonstrated in a proper study, include a longer life, darker hair, better memory, better sleep, curing heart diseases, diabetes and Alzheimer. The approach is based on parabiosis studies such as Irina Conboy do on mice, but Conboy says young blood does not reverse aging (even in mice) and that those who offer those treatments have misunderstood her research. Neuroscientist Tony Wyss-Coray, who also studied blood exchanges on mice as recently as 2014, said people offering those treatments are \"basically abusing people's trust\" and that young blood treatments are \"the scientific equivalent of fake news\". The treatment appeared in HBO's Silicon Valley fiction series.\n\nTwo clinics in California, run by Jesse Karmazin and David C. Wright, offer $8,000 injections of plasma extracted from the blood of young people. Karmazin has not published in any peer-reviewed journal and his current study does not use a control group.\n\n\n"}
{"id": "16554664", "url": "https://en.wikipedia.org/wiki?curid=16554664", "title": "Living systems", "text": "Living systems\n\nLiving systems are open self-organizing life forms that interact with their environment. These systems are maintained by flows of information, energy and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nLiving systems theory is a general theory about the existence of all living systems, their structure, interaction, behavior and development. This work is created by James Grier Miller, which was intended to formalize the concept of life. According to Miller's original conception as spelled out in his magnum opus \"Living Systems\", a \"living system\" must contain each of twenty \"critical subsystems\", which are defined by their functions and visible in numerous systems, from simple cells to organisms, countries, and societies. In \"Living Systems\" Miller provides a detailed look at a number of systems in order of increasing size, and identifies his subsystems in each. \nMiller considers living systems as a subset of all systems. Below the level of living systems, he defines space and time, matter and energy, information and entropy, levels of organization, and physical and conceptual factors, and above living systems ecological, planetary and solar systems, galaxies, etc.\n\nLiving systems according to Parent (1996) are by definition \"open self-organizing systems that have the special characteristics of life and interact with their environment. This takes place by means of information and material-energy exchanges. Living systems can be as simple as a single cell or as complex as a supranational organization such as the European Union. Regardless of their complexity, they each depend upon the same essential twenty subsystems (or processes) in order to survive and to continue the propagation of their species or types beyond a single generation\".\n\nMiller said that systems exist at eight \"nested\" hierarchical levels: cell, organ, organism, group, organization, community, society, and supranational system. At each level, a system invariably comprises twenty critical subsystems, which process matter–energy or information except for the first two, which process both matter–energy and information: reproducer and boundary.\n\nThe processors of matter–energy are: \n\nThe processors of information are:\n\nJames Grier Miller in 1978 wrote a 1,102-page volume to present his living systems theory. He constructed a general theory of living systems by focusing on concrete systems—nonrandom accumulations of matter–energy in physical space–time organized into interacting, interrelated subsystems or components. Slightly revising the original model a dozen years later, he distinguished eight \"nested\" hierarchical levels in such complex structures. Each level is \"nested\" in the sense that each higher level contains the next lower level in a nested fashion.\n\nHis central thesis is that the systems in existence at all eight levels are open systems composed of twenty critical subsystems that process inputs, throughputs, and outputs of various forms of matter–energy and information. Two of these subsystems—reproducer and boundary—process both matter–energy and information. Eight of them process only matter–energy. The other ten process information only. \nAll nature is a continuum. The endless complexity of life is organized into patterns which repeat themselves—theme and variations—at each level of system. These similarities and differences are proper concerns for science. From the ceaseless streaming of protoplasm to the many-vectored activities of supranational systems, there are continuous flows through living systems as they maintain their highly organized steady states.\nSeppänen (1998) says that Miller applied general systems theory on a broad scale to describe all aspects of living systems.\n\nMiller's theory posits that the mutual interrelationship of the components of a system extends across the hierarchical levels. Examples: Cells and organs of a living system thrive on the food the organism obtains from its suprasystem; the member countries of a supranational system reap the benefits accrued from the communal activities to which each one contributes. Miller says that his eclectic theory \"ties together past discoveries from many disciplines and provides an outline into which new findings can be fitted\".\n\nMiller says the concepts of space, time, matter, energy, and information are essential to his theory because the living systems exist in space and are made of matter and energy organized by information. Miller's theory of living systems employs two sorts of spaces: physical or geographical space, and conceptual or abstracted spaces. Time is the fundamental \"fourth dimension\" of the physical space–time continuum/spiral. Matter is anything that has mass and occupies physical space. Mass and energy are equivalent as one can be converted into the other. Information refers to the degrees of freedom that exist in a given situation to choose among signals, symbols, messages, or patterns to be transmitted.\n\nOther relevant concepts are system, structure, process, type, level, echelon, suprasystem, subsystem, transmissions, and steady state. A system can be conceptual, concrete or abstracted. The structure of a system is the arrangement of the subsystems and their components in three-dimensional space at any point of time. Process, which can be reversible or irreversible, refers to change over time of matter–energy or information in a system. Type defines living systems with similar characteristics. Level is the position in a hierarchy of systems. Many complex living systems, at various levels, are organized into two or more echelons. The suprasystem of any living system is the next higher system in which it is a subsystem or component. The totality of all the structures in a system which carry out a particular process is a subsystem. Transmissions are inputs and outputs in concrete systems. Because living systems are open systems, with continually altering fluxes of matter–energy and information, many of their equilibria are dynamic—situations identified as steady states or flux equilibria.\n\nMiller identifies the comparable matter–energy and information processing critical subsystems. Elaborating on the eight hierarchical levels, he defines society, which constitutes the seventh hierarchy, as \"a large, living, concrete system with [community] and lower am levels of living systems as subsystems and components\". Society may include small, primitive, totipotential communities; ancient city–states, and kingdoms; as well as modern nation–states and empires that are not supranational systems. Miller provides general descriptions of each of the subsystems that fit all eight levels.\n\nA supranational system, in Miller's view, \"is composed of two or more societies, some or all of whose processes are under the control of a decider that is superordinate to their highest echelons\". However, he contends that no supranational system with all its twenty subsystems under control of its decider exists today. The absence of a supranational decider precludes the existence of a concrete supranational system. Miller says that studying a supranational system is problematical because its subsystems\n...tend to consist of few components besides the decoder. These systems do little matter-energy processing. The power of component societies [nations] today is almost always greater than the power of supranational deciders. Traditionally, theory at this level has been based upon intuition and study of history rather than data collection. Some quantitative research is now being done, and construction of global-system models and simulations is currently burgeoning.\n\nAt the supranational system level, Miller's emphasis is on international organizations, associations, and groups comprising representatives of societies (nation–states). Miller identifies the subsystems at this level to suit this emphasis. Thus, for example, the reproducer is \"any multipurpose supranational system which creates a single purpose supranational organization\" (p. 914); and the boundary is the \"supranational forces, usually located on or near supranational borders, which defend, guard, or police them\" (p. 914).\n\nNot just those specialized in international communication, but all communication science scholars could pay particular attention to the major contributions of living systems theory (LST) to social systems approaches that Bailey has pointed out:\n\nBailey says that LST, perhaps the \"most integrative\" social systems theory, has made many more contributions that may be easily overlooked, such as: providing a detailed analysis of types of systems; making a distinction between concrete and abstracted systems; discussion of physical space and time; placing emphasis on information processing; providing an analysis of entropy; recognition of totipotential systems, and partipotential systems; providing an innovative approach to the structure–process issue; and introducing the concept of joint subsystem—a subsystem that belongs to two systems simultaneously; of dispersal—lateral, outward, upward, and downward; of inclusion—inclusion of something from the environment that is not part of the system; of artifact—an animal-made or human-made inclusion; of adjustment process, which combats stress in a system; and of critical subsystems, which carry out processes that all living systems need to survive.\n\nLST's analysis of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter–energy-processing and information-processing, as well as LST's analysis of the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or \"organizational pathologies\" of systems functioning (e.g., system stress and strain, feedback irregularities, information–input overload). It explicates the role of entropy in social research while it equates negentropy with information and order. It emphasizes both structure and process, as well as their interrelations.\n\nIt omits the analysis of subjective phenomena, and it overemphasizes concrete Q-analysis (correlation of objects) to the virtual exclusion of R-analysis (correlation of variables). By asserting that societies (ranging from totipotential communities to nation-states and non-supranational systems) have greater control over their subsystem components than supranational systems have, it dodges the issue of transnational power over the contemporary social systems. Miller's supranational system bears no resemblance to the modern world-system that Immanuel Wallerstein (1974) described, although both of them were looking at the same living (dissipative) structure.\n\n\n\n"}
{"id": "4585070", "url": "https://en.wikipedia.org/wiki?curid=4585070", "title": "Non-cellular life", "text": "Non-cellular life\n\nNon-cellular life is life that exists without a cellular structure for at least part of its life cycle. Historically, most (descriptive) definitions of life postulated that a living organism must be composed of one or more cells, but this is no longer considered necessary, and modern criteria allow for forms of life based on other structural arrangements.\n\nThe primary candidates for non-cellular life are viruses. A minority of biologists consider viruses to be living organisms, but most do not. Their primary objection is that no known viruses are capable of autopoiesis, which means they cannot reproduce themselves: they must rely on cells to copy them. However, the recent discovery of giant viruses that possess genes for part of the required translation machinery has raised the prospect that they may have had extinct ancestors that could evolve and replicate independently. Most biologists agree that such an ancestor would be a \"bona fide\" non-cellular lifeform, but its existence and characteristics are still uncertain.\n\nEngineers sometimes use the term \"artificial life\" to refer to software and robots inspired by biological processes, but these do not satisfy any biological definition of life.\n\nThe nature of viruses was unclear for many years following their discovery as pathogens. They were described as poisons or toxins at first, then as \"infectious proteins\", but with advances in microbiology it became clear that they also possessed genetic material, a defined structure, and the ability to spontaneously assemble from their constituent parts. This spurred extensive debate as to whether they should be regarded as fundamentally organic or inorganic — as very small biological organisms or very large biochemical molecules — and since the 1950s many scientists have thought of viruses as existing at the border between chemistry and life; a gray area between living and nonliving.\n\nThe recent discovery of giant viruses (aka giruses, nucleocytoplasmic large DNA viruses, NCLDVs) has reignited this debate, since they are not only physically larger than previously known viruses, but also possess much more extensive genomes, including genes coding for aminoacyl tRNA synthetases, key proteins involved in translation, which were previously thought to be exclusive to cellular organisms. This raises the prospect that the giant viruses may have had extinct ancestors (or even undiscovered ones) capable of engaging in life processes (such as evolution and replication) independent of cells, and that modern viruses lost those abilities secondarily. The viral lineage including this ancestor would be ancient and may have originated alongside the earliest archaea or before the LUCA. If such a virus is discovered, or its past existence supported by further genetic evidence, most biologists agree that it would constitute a \"bona fide\" lifeform, and its descendants (at least the giant viruses, and possibly including all known viruses) could be phylogenetically classified in a fourth domain of life. Discovery of the pandoraviruses, with genomes are even larger than the other giant viruses and exhibiting particularly low homology with the three existing domains, further discredited the traditional view that viruses simply \"pick-pocketed\" all of their genes from cellular organisms, and further supported the \"complex ancestors\" hypothesis.\n\nOngoing research is being conducted in this area, using techniques such as phylogenetic bracketing on the giant viruses to infer characteristics of their proposed progenitor.\n\nFurthermore, Viral replication and self-assembly has implications for the study of the origin of life, as it lends further credence to the hypothesis that life could have started as self-assembling organic molecules.\n\nViroids are the smallest infectious pathogens known to biologists, consisting solely of short strands of circular, single-stranded RNA without protein coats. They are mostly plant pathogens and some are animal pathogens, from which some are of commercial importance. Viroid genomes are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection by themselves are around 2,000 nucleobases in size. Viroids are the first known representatives of a new biological realm of sub-viral pathogens.\n\nViroid RNA does not code for any protein. Its replication mechanism hijacks RNA polymerase II, a host cell enzyme normally associated with synthesis of messenger RNA from DNA, which instead catalyzes \"rolling circle\" synthesis of new RNA using the viroid's RNA as a template. Some viroids are ribozymes, having catalytic properties which allow self-cleavage and ligation of unit-size genomes from larger replication intermediates.\n\nViroids attained significance beyond plant virology since one possible explanation of their origin is that they represent “living relics” from a hypothetical, ancient, and non-cellular RNA world before the evolution of DNA or protein. This view was first proposed in the 1980s, and regained popularity in the 2010s to explain crucial intermediate steps in the evolution of life from inanimate matter (Abiogenesis).\n\nIn discussing the taxonomic domains of life, the terms \"Acytota\" or \"Aphanobionta\" are occasionally used as the name of a viral kingdom, domain, or empire. The corresponding cellular life name would be Cytota. Non-cellular organisms and cellular life would be the two top-level subdivisions of life, whereby life as a whole would be known as organisms, Naturae, or Vitae. The taxon Cytota would include three top-level subdivisions of its own, the domains Bacteria, Archaea, and Eukarya.\n\n"}
{"id": "41077350", "url": "https://en.wikipedia.org/wiki?curid=41077350", "title": "OpenWorm", "text": "OpenWorm\n\nOpenWorm is an international open science project to simulate the roundworm \"Caenorhabditis elegans\" at the cellular level as a simulation. Although the long-term goal is to model all 959 cells of the \"C. elegans\", the first stage is to model the worm's locomotion by simulating the 302 neurons and 95 muscle cells. This bottom up simulation is being pursued by the OpenWorm community. As of this writing, a physics engine called Sibernetic has been built for the project and models of the neural connectome and a muscle cell have been created in NeuroML format. A 3D model of the worm anatomy can be accessed through the web via the OpenWorm browser. The OpenWorm project is also contributing to develop Geppetto, a web-based multi-algorithm, multi-scale simulation platform engineered to support the simulation of the whole organism.\n\nThe roundworm \"C. elegans\" has one of the simplest nervous systems of any organism, with its hermaphrodite type having only 302 neurons. Furthermore, the structural connectome of these neurons is fully worked out. There are fewer than one thousand cells in the whole body of a \"C. elegans\" worm, and because \"C. Elegans\" is a model organism, each has a unique identifier and comprehensive supporting literature. Being a model organism, the genome is fully known, along with many well characterized mutants readily available, a comprehensive literature of behavioural studies, etc. With so few neurons and new calcium 2 photon microscopy techniques it should soon be possible to record the complete neural activity of a living organism. By manipulating the neurons through optogenetic techniques, combined with the above recording capacities the project is in an unprecedented position to be able to fully characterize the neural dynamics of an entire organism.\n\nIn the process of trying to build an \"in silico\" model of a relatively simple organism like \"C. elegans\", new tools are being developed which will make it easier to model progressively more complex organisms.\n\nProject Nemaload was created as a research program trying to empirically establish the relevant biological facts which are necessary for a true bottom-up simulation. The project founder, David Dalrymple, is a collaborator on the OpenWorm project.\n\nAlthough the ultimate goal is to simulate all features of \"C. elegans\"' behaviour, the project is new and the first behaviour the Open Worm community decided to simulate is a simple motor response: teaching the worm to crawl. To do so, the virtual worm must be placed in a virtual environment. A full feedback loop must be established: Environmental Stimulus > Sensory Transduction > Interneuron Firing > Motor Neuron Firing > Motor Output > Environmental Change > Sensory Transduction.\n\nThere are two main technical challenges here: modelling the neural/electrical properties of the brain as it processes the information, and modelling the mechanical properties of the body as it moves. The neural properties are being modeled by a Hodgkin-Huxley model, and the mechanical properties are being modeled by a Smoothed Particle Hydrodynamic algorithm.\n\nThe OpenWorm team built an engine called Geppetto which could integrate these algorithms and due to its modularity will be able to model other biological systems (like digestion) which the team will tackle at a later time.\n\nThe team also built an environment called NeuroConstruct which is able to output neural structures in NeuroML. Using NeuroConstruct the team reconstructed the full connectome of \"C. elegans\".\n\nUsing NeuroML the team has also built a model of a muscle cell. Note that these models currently only model the relevant properties for the simple motor response: the neural/electrical and the mechanical properties discussed above.\n\nThe next step is to connect this muscle cell to the six neurons which synapse on it and approximate their effect.\n\nThe rough plan is to then both:\n\nAs of January 2015, the project is still awaiting peer review, and researchers involved in the project are reluctant to make bold claims about its current resemblance to biological behavior; project coordinator Stephen Larson estimates that they are \"only 20 to 30 percent of the way towards where we need to get\".\n\nIn 1998 Japanese researchers announced the Perfect C. elegans Project. A proposal was submitted, but the project appears to have been abandoned.\n\nIn 2004 a group from Hiroshima began the Virtual C. elegans Project. They released two papers which showed how their simulation would retract from virtual prodding.\n\nIn 2005 a Texas researcher described a simplified C. elegans simulator based on a 1-wire network incorporating a digital Parallax Basic Stamp processor, sensory inputs and motor outputs. Inputs employed 16-bit A/D converters attached to operational amplifier simulated neurons and a 1-wire temperature sensor. Motor outputs were controlled by 256-position digital potentiometers and 8-bit digital ports. Artificial muscle action was based on Nitinol actuators. It used a \"sense-process-react\" operating loop which recreated several instinctual behaviors.\n\nThese early attempts of simulation have been criticized for not being biologically realistic. Although we have the complete structural connectome, we do not know the synaptic weights at each of the known synapses. We do not even know whether the synapses are inhibitory or excitatory. To compensate for this the Hiroshima group used machine learning to find some weights of the synapses which would generate the desired behaviour. It is therefore no surprise that the model displayed the behaviour, and it may not represent true understanding of the system.\n\nThe Open Worm community is committed to the ideals of open science. Generally this means that the team will try to publish in open access journals and include all data gathered (to avoid the file drawer problem). Indeed, all the biological data the team has gathered is publicly available, and the five publications the group has made so far are available for free on their website. All the software that OpenWorm has produced is completely free and open source.\n\nOpen Worm is also trying a radically open model of scientific collaboration. The team consists of anyone who wishes to be a part of it. There are over one hundred \"members\" who are signed up for the high volume technical mailing list. Of the most active members who are named on a publication there are collaborators from Russia, Brazil, England, Scotland, Ireland and the United States. To coordinate this international effort, the team uses \"virtual lab meetings\" and other online tools that are detailed in the resources section.\n\n"}
{"id": "18789195", "url": "https://en.wikipedia.org/wiki?curid=18789195", "title": "The Seven Pillars of Life", "text": "The Seven Pillars of Life\n\nThe Seven Pillars of Life are the essential principles of life described by Daniel E. Koshland in 2002 in order to create a universal definition of life. One stated goal of this universal definition is to aid in understanding and identifying artificial and extraterrestrial life. The seven pillars are Program, Improvisation, Compartmentalization, Energy, Regeneration, Adaptability, and Seclusion. These can be abbreviated as PICERAS.\n\nKoshland defines \"Program\" as an \"organized plan that describes both the ingredients themselves and the kinetics of the interactions among ingredients as the living system persists through time.\" In natural life as it is known on Earth, the program operates through the mechanisms of nucleic acids and amino acids, but the concept of program can apply to other imagined or undiscovered mechanisms.\n\n\"Improvisation\" refers to the living system's ability to change its program in response to the larger environment in which it exists. An example of improvisation on earth is natural selection.\n\n\"Compartmentalization\" refers to the separation of spaces in the living system that allow for separate environments for necessary chemical processes. Compartmentalization is necessary to protect the concentration of the ingredients for a reaction from outside environments.\n\nBecause living systems involve net movement in terms of chemical movement or body movement, and lose energy in those movements through entropy, energy is required for a living system to exist. The main source of energy on Earth is the sun, but other sources of energy exist for life on Earth, such as hydrogen gas or methane, used in chemosynthesis.\n\n\"Regeneration\" in a living system refers to the general compensation for losses and degradation in the various components and processes in the system. This covers the thermodynamic loss in chemical reactions, the wear and tear of larger parts, and the larger decline of components of the system in ageing. Living systems replace these losses by importing molecules from the outside environment, synthesizing new molecules and components, or creating new generations to start the system over again.\n\n\"Adaptability\" is the ability of a living system to respond to needs, dangers, or changes. It is distinguished from improvisation because the response is timely and does not involve a change of the program. Adaptability occurs from a molecular level to a behavioral level through feedback and feedforward systems. For example, an animal seeing a predator will respond to the danger with hormonal changes and escape behavior.\n\nY. N. Zhuravlev and V. A. Avetisov have analyzed Koshland's seven pillars from the context of primordial life and, though calling the concept \"elegant,\" point out that the pillars of compartmentalization, program, and seclusion don't apply well to the non-differentiated earliest life.\n\n"}
