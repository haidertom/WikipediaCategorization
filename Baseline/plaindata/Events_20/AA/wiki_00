{"id": "14697061", "url": "https://en.wikipedia.org/wiki?curid=14697061", "title": "AIDSRide", "text": "AIDSRide\n\nThe AIDSRides were a series of fundraising events organised by Pallotta TeamWorks which raised more than $105 million for critical AIDS services and medical research. About half of the money raised directly benefited AIDS patients.\n\nIn 2001, the San Francisco AIDS Foundation and the Los Angeles Gay and Lesbian Center ended their partnership with the California AIDSRide over concerns that not enough money was going to the charities. They hired Honeycutt Group, a consulting firm started by three former Pallotta TeamWorks employees, to organize AIDS/LifeCycle, a similar event. Dan Pallotta unsuccessfully sued the \"copycat\" event, but the competition and surrounding controversy made 2002 the event's last year.\n\nPallotta TeamWorks has organised several AIDSRides in North America:\n\n\n\n"}
{"id": "21188248", "url": "https://en.wikipedia.org/wiki?curid=21188248", "title": "Analog-to-digital timeline", "text": "Analog-to-digital timeline\n\nThis page serves as a timeline to show when analog devices were first made with digital circuits and systems.\n\n\n"}
{"id": "26241411", "url": "https://en.wikipedia.org/wiki?curid=26241411", "title": "Chase's Calendar of Events", "text": "Chase's Calendar of Events\n\nChase's Calendar of Events is an annual American publication, started in 1957 by brothers William (Bill) D. Chase (a journalist and publisher from Michigan), and Harrison V. Chase (a university social scientist from Florida). It includes special events, holidays, federal and state observances, historic anniversaries, and more unusual celebratory traditions. Bill Chase worked as a newspaper librarian and saw a need for \"a single reference source for calendar dates, and for authoritative and current information about various observances throughout the year\".\n\nThe brothers gathered information on events and the first edition of 2,000 copies was printed for 1958. \"It was 32 pages, contained 364 entries and sold for $1\", while recent editions are 752 pages and contain more than 12,000 entries. A promotion sponsored by the US Chamber of Commerce was added in 1958: a pamphlet listed commercial promotions as Special Days, Weeks and Months, and remained in future editions. Contemporary Books in Chicago, Illinois, took over publication in 1983 and the Chases retired in 1987 from compiling the calendar, which is now handled by an in-house staff of editors and researchers. Contemporary Books was acquired by Tribune in 1993 and sold to McGraw-Hill Companies in September 2000. McGraw-Hill sold the property to Rowman and Littlefield Publishing Group in January 2015, where it is now published by Bernan Press, an imprint of RLPG. \nSections of the calendar include \n\n"}
{"id": "20261829", "url": "https://en.wikipedia.org/wiki?curid=20261829", "title": "Chronology of the Great Famine", "text": "Chronology of the Great Famine\n\nThe Chronology of the Great Famine ( or \"An Drochshaol\", litt: \"The Bad Life\") documents a period of Irish history between 1845 and 1852 during which time the population of Ireland was reduced by 20 to 25 percent. The proximate cause was famine resulting from a potato disease commonly known as late blight. Although blight ravaged potato crops throughout Europe during the 1840s, the impact and human cost in Irelandwhere a third of the population was entirely dependent on the potato for foodwas exacerbated by a host of political, social and economic factors which remain the subject of historical debate.\n\nAn important and controversial point of debate, that is notable amongst Irish people, was that Irish food supplies were sufficient until it was taken away by the British the feed the British, and the landlords kept economic benefits to themselves. Some commentators and writers controversially compared this explanation to \"genocide\".\n\nAt the beginning of August, Sir Robert Peel, the British Prime Minister, received news of a potato disease in the South of England. This was the first recorded evidence that the 'blight' which had ravaged the potato crop in North America in 1843-1844 had crossed the Atlantic. Cecil Woodham-Smith would write that a failure in England would be serious, but for Ireland, it would be a disaster.\n\nFollowing earlier reports of incidences of the blight in England, on 13 September 1845 potato blight was first reported in Ireland. The crops at Dublin were suddenly perishing, it was reported in the \"Gardeners' Chronicle\", asking \"where will Ireland be in the event of a universal potato rot?\" The British Government were nevertheless optimistic through the next few weeks.\n\n\nThe principle of the Corn Laws had been to keep the price of home-grown grain up. Duties on imported grain assured English farmers a minimum and profitable price. The burden of a higher price for bread was carried by the labouring classes, in particular factory workers and operatives. It was claimed that if the Corn Laws were repealed all those connected with the land would be ruined and the established social organisation of the country destroyed.\n\nAccording to Cecil Woodham-Smith, the rising wrath of Tories and landlords ensured \"all interest in Ireland was submerged.\" She writes that the Tory Mayor of Liverpool refused to call a meeting for the relief of Irish distress. She continues that the Mansion House Committee in Dublin was accused of 'deluding the public with a false alarm', and the blight itself 'was represented as the invention of agitators on the other side of the water'. The entanglement of the Irish famine with the repeal of the Corn Laws, she says, was a key misfortune for Ireland. The potato failure was eclipsed by the domestic issue of Corn Law repeal. The Irish famine, she writes, \"slipped into the background.\"\n\n\nThe first deaths from hunger took place in early 1846. In March Peel set up a programme of public works in Ireland but was forced to resign as Prime Minister on 29 June. The new Whig administration under Lord Russell, influenced by their laissez-faire belief that the market would provide the food needed then halted government food and relief works, leaving many hundreds of thousands of people without any work, money or food. Grain continued to be exported from the country. Private initiatives such as The Central Relief Committee of the Society of Friends (Quakers) attempted to fill the gap caused by the end of government relief and eventually the government reinstated the relief works, although bureaucracy slowed the release of food supplies. The blight almost totally destroyed the 1846 crop and the Famine worsened considerably. By December a third of a million destitute people were employed in public works.\n\nThere were average crop yields in the 1847 harvest, but due to lack of seed potatoes to plant, the crop was low. Crowds began to throng the public works during the last months of 1846 and the start of 1847, which promoted exactly the social conditions for the spread of 'famine fever.' In late January and February, legislation called the Temporary Relief Act went through the British parliament; it became popularly known as the Soup Kitchen Act and occasionally as Burgoyne's Act. This system of relief was designed to deliver cheap food directly and gratuitously to the destitute masses. This system of relief would be terminated in September. The government also announced an additional change in the system of relief. After August 1847, the permanent Poor Law was to be extended and was to become responsible for providing relief and as a result, all relief would be financed by the local Poor Law rates. This put impossible loads on local poor rates, particularly in the rural west and south. With the mass emigration of the famine era, the horrors of the 'coffin ships' and 1847 have ever since been associated in the popular mind, according to James S. Donnelly.\n\nIn December 1847 The Crime and Outrage Bill (Ireland) 1847 was enacted due to growing Irish nationalist agitation that was causing the British government concern about a possible violent rebellion against British rule in Ireland.\n\nThe bill gave the Lord Lieutenant of Ireland the power to organise the island into districts and bring police forces into them at the districts' expense. It limited who could own guns, and required all of the men in the district between the ages of 16 and 60 to assist in apprehending suspected murderers when landlords were killed, or else be guilty of a misdemeanour themselves.\n\nThe blight returned in 1848 and outbreaks of cholera were reported. Evictions became common among the Irish who could not keep up with the demands of their British landlords. Famine victims on outdoor relief peaked in July at almost 840,000 people. On 29 July an uprising against the government was led by William Smith O'Brien. After a skirmish at \"Widow McCormack's house\" in the village of Ballingarry, County Tipperary the leaders of the rebellion fled to America or were sentenced to transportation.\n\nThe potato crop failed again in 1849 and famine was accompanied by cholera outbreaks. This deadly cholera epidemic killed one of Ireland's greatest poets: James Clarence Mangan.\n\nThe Famine ended.\n\nBy 1851 census figures showed that the population of Ireland had fallen to 6,575,000 – a drop of 1,600,000 in ten years. Cormac Ó Gráda and Joel Mokyr have described the 1851 census as a famous but flawed source. They contend that the combination of institutional and individuals figures gives \"an incomplete and biased count\" of fatalities during the famine. The famine left in its wake up to a million dead and another million emigrated. The famine caused a sense of lasting bitterness by the Irish towards the British government, whom many blamed — then and now — for the starvation of so many people. The fall-out of the famine continued for decades afterwards.\n\n\n\n"}
{"id": "21219455", "url": "https://en.wikipedia.org/wiki?curid=21219455", "title": "Chronology of the Shunzhi reign", "text": "Chronology of the Shunzhi reign\n\nThis is a chronicle of important events that took place under the Shunzhi Emperor of the Qing Dynasty (1636–1912) in what is now China. It spans from the death of his predecessor Hong Taiji (r. 1626–1643) in September 1643, to the emperor's own death on 5 February 1661, seven days into the eighteenth year of the Shunzhi reign period. These dates do not correspond perfectly with the Shunzhi era itself, which started on 8 February 1644—on New Year's Day of the lunisolar year following the emperor's accession—and ended on 17 February 1662 (the last day of the 18th year of Shunzhi), more than one solar year after the emperor's death. The posthumous events related to the Shunzhi Emperor's burial and posthumous cult are also included.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19167840", "url": "https://en.wikipedia.org/wiki?curid=19167840", "title": "Chronology of the universe", "text": "Chronology of the universe\n\nThe chronology of the universe describes the history and future of the universe according to Big Bang cosmology. The earliest stages of the universe's existence are estimated as taking place 13.8 billion years ago, with an uncertainty of around 21 million years at the 68% confidence level.\n\nFor the purposes of this summary, it is convenient to divide the chronology of the universe since it originated, into five parts. It is generally considered meaningless or unclear whether time existed before this chronology:\n\nEarliest stages of chronology shown below (before neutrino decoupling) are an active area of research and based on ideas which are still speculative and subject to modification as scientific knowledge improves.\n\n\"Time\" column is based on extrapolation of observed metric expansion of space back in the past. For the earliest stages of chronology this extrapolation may be invalid. To give one example, eternal inflation theories propose that inflation lasts forever throughout most of the universe, making the notion of \"N seconds since Big Bang\" ill-defined.\n\nThe radiation temperature refers to the cosmic background radiation and is given by 2.725·(1+\"z\"), where \"z\" is the redshift.\n\nThe Planck epoch is an era in traditional (non-inflationary) Big Bang cosmology immediately after the event which began our known universe. During this epoch, the temperature and average energies within the universe were so inconceivably high compared to any temperature we can observe today, that everyday subatomic particles could not form, and even the four fundamental forces that shape our universe—electromagnetism, gravitation, weak nuclear interaction, and strong nuclear interaction—were combined and formed one fundamental force. Little is understood about physics at this temperature; different hypotheses propose different scenarios. Traditional big bang cosmology predicts a gravitational singularity before this time, but this theory relies on the theory of general relativity, which is thought to break down for this epoch due to quantum effects.\n\nIn inflationary models of cosmology, times before the end of inflation (roughly 10 second after the Big Bang) do not follow the same timeline as in traditional big bang cosmology. Models that aim to describe the universe and physics during the Planck epoch are generally speculative and fall under the umbrella of \"New Physics\". Examples include the Hartle–Hawking initial state, string landscape, string gas cosmology, and the ekpyrotic universe.\n\nAs the universe expanded and cooled, it crossed transition temperatures at which forces separated from each other. These phase transitions can be visualised as similar to condensation and freezing phase transitions of ordinary matter. At certain temperatures/energies, water molecules change their behaviour and structure, and they will behave completely differently. Like steam turning to water, the fields which define our universe's fundamental forces and particles also completely change their behaviors and structures when the temperature/energy falls below a certain point. This is not apparent in everyday life, because it only happens at much, much, higher temperatures than we usually see in our present universe.\n\nThese phase transitions are believed to be caused by a phenomenon of quantum fields called \"symmetry breaking\".\n\nIn everyday terms, as the universe cools, it becomes possible for the quantum fields that create the forces and particles around us, to settle at lower energy levels and with higher levels of stability. In doing so, they completely shift how they interact. Forces and interactions arise due to these fields, so the universe can behave very differently above and below a phase transition. For example, in a later epoch, a side effect of one phase transition is that suddenly, many particles that had no mass at all acquire a mass (they begin to interact with the Higgs boson), and a single force begins to manifest as two separate forces.\n\nThe grand unification epoch began with a phase transitions of this kind, when gravitation separated from the universal combined gauge force. This caused two forces to now exist: gravity, and an electrostrong interaction. There is no hard evidence yet, that such a combined force existed, but many physicists believe it did. The physics of this electrostrong interaction would be described by a so-called grand unified theory (GUT).\n\nThe grand unification epoch ended with a second phase transition, as the electrostrong interaction in turn separated, and began to manifest as two separate interactions, called the strong and electroweak interactions.\n\nDepending on how epochs are defined, and the model being followed, the electroweak epoch may be considered to start before or after the inflationary epoch. In some models it is described as including the inflationary epoch. In other models, the electroweak epoch is said to begin after the inflationary epoch ended, at roughly 10 seconds.\n\nAccording to traditional big bang cosmology, the electroweak epoch began 10 seconds after the Big Bang, when the temperature of the universe was low enough (10 K) for the Electronuclear Force to begin to manifest as two separate interactions, called the strong and the electroweak interactions. (The electroweak interaction will also separate later, dividing into the electromagnetic and weak interactions). The exact point where electrostrong symmetry was broken is not certain, because of the very high energies of this event.\n\nAt this point, the very early universe suddenly and very rapidly expanded to at least 10 times its previous volume (and possibly much more). This is equivalent to a linear increase of at least 10 times in every spatial dimension – equivalent to an object 1 nanometer (10 m, about half the width of a molecule of DNA) in length, expanding to one approximately 10.6 light years (about 62 trillion miles) long in a tiny fraction of a second. This change is known as inflation.\n\nAlthough light and objects within spacetime cannot travel faster than the speed of light, in this case it was the metric governing the size and geometry of spacetime itself that changed in scale. Changes to the metric are not limited by the speed of light.\n\nIn some models, it is thought to have been triggered by the separation of the strong and electroweak interactions which ended the grand unification epoch. One of the theoretical products of this phase transition was a scalar field called the inflaton field. As this field settled into its lowest energy state throughout the universe, it generated an enormous repulsive force that led to a rapid expansion of space itself. Inflation explains several observed properties of the current universe that are otherwise difficult to account for, including explaining how today's universe has ended up so exceedingly homogeneous (similar) on a very large scale, even though it was highly disordered in its earliest stages.\n\nIt is not known exactly when the inflationary epoch ended, but it is thought to have been between 10 and 10 seconds after the Big Bang. The rapid expansion of space meant that elementary particles remaining from the grand unification epoch were now distributed very thinly across the universe. However, the huge potential energy of the inflation field was released at the end of the inflationary epoch, as the inflaton field decayed into other particles, known as \"reheating\". This heating effect led to the universe being repopulated with a dense, hot mixture of quarks, anti-quarks and gluons. In other models, reheating is often considered to mark the start of the electroweak epoch, and some theories, such as warm inflation, avoid a reheating phase entirely.\n\nIn non-traditional versions of Big Bang theory (known as \"inflationary\" models), inflation ended at a temperature corresponding to roughly 10 second after the Big Bang, but this does \"not\" imply that the inflationary era lasted less than 10 second. To explain the observed homogeneity of the universe, the duration in these models must be longer than 10 second. Therefore, in inflationary cosmology, the earliest meaningful time \"after the Big Bang\" is the time of the \"end\" of inflation. \n\nAfter inflation ended, the universe continued to expand, but at a very slow rate. The slow expansion began to speed up after several billion years, believed to be due to dark energy, and is still expanding today.\n\nOn March 17, 2014, astrophysicists of the BICEP2 collaboration announced the detection of inflationary gravitational waves in the B-mode power spectrum which was interpreted as clear experimental evidence for the theory of inflation. However, on June 19, 2014, lowered confidence in confirming the cosmic inflation findings was reported and finally, on February 2, 2015, a joint analysis of data from BICEP2/Keck and Planck satellite concluded that the statistical \"significance [of the data] is too low to be interpreted as a detection of primordial B-modes\" and can be attributed mainly to polarized dust in the Milky Way.\n\nAs the universe's temperature continued to fall below a certain very high energy level, a third symmetry breaking occurs. So far as we currently know, it was the final symmetry breaking event in the formation of our universe. It is believed that below some energies unknown yet, the Higgs field spontaneously acquires a vacuum expectation value. When this happens, it breaks electroweak gauge symmetry. This has two related effects:\n\n\nAfter electroweak symmetry breaking, the fundamental interactions we know of – gravitation, electromagnetism, the strong interaction and the weak interaction – have all taken their present forms, and fundamental particles have mass, but the temperature of the universe is still too high to allow the formation of many fundamental particles we now see in the universe.\n\nIf supersymmetry is a property of our universe, then it must be broken at an energy that is no lower than 1 TeV, the electroweak scale. The masses of particles and their superpartners would then no longer be equal. This very high energy could explain why no superpartners of known particles have ever been observed.\n\nAfter cosmic inflation ends, the universe is filled with a hot quark–gluon plasma, the remains of reheating. From this point onwards the physics of the early universe is much better understood, and the energies involved in the Quark epoch are directly amenable to experiment.\n\nThe quark epoch began approximately 10 seconds after the Big Bang. This was the period in the evolution of the early universe immediately after electroweak symmetry breaking, when the fundamental interactions of gravitation, electromagnetism, the strong interaction and the weak interaction had taken their present forms, but the temperature of the universe was still too high to allow quarks to bind together to form hadrons.\n\nDuring the quark epoch the universe was filled with a dense, hot quark–gluon plasma, containing quarks, leptons and their antiparticles. Collisions between particles were too energetic to allow quarks to combine into mesons or baryons.\n\nThe quark epoch ended when the universe was about 10 seconds old, when the average energy of particle interactions had fallen below the binding energy of hadrons.\n\n\"Perhaps by 10 seconds.\"\n\nBaryons are subatomic particles such as protons and neutrons, that are composed of three quarks. It would be expected that both baryons, and particles known as antibaryons would have formed in equal numbers. However, this does not seem to be what happened – as far as we know, the universe was left with far more baryons than antibaryons. In fact, almost no antibaryons are observed in nature. It is not clear how this came about. Any explanation for this phenomenon must allow the Sakharov conditions related to baryogenesis to have been satisfied at some time after the end of cosmological inflation. Current particle physics suggests asymmetries under which these conditions would be met, but these asymmetries appear to be too small to account for the observed baryon-antibaryon asymmetry of the universe.\n\nThe quark–gluon plasma that composes the universe cools until hadrons, including baryons such as protons and neutrons, can form.\nInitially, hadron/anti-hadron pairs could form, so matter and anti-matter were in thermal equilibrium. However, as the temperature of the universe continued to fall, new hadron/anti-hadron pairs were no longer produced, and most of the newly formed hadrons and anti-hadrons annihilated each other, giving rise to pairs of high-energy photons. A comparatively small residue of hadrons remained at about 1 second of cosmic time, when this epoch ended. \n\nTheory predicts that about 1 neutron remained for every 7 protons. We believe this to be correct because, at a later stage, all the neutrons and some of the protons fused, leaving hydrogen, a hydrogen isotope called deuterium, helium and other elements, which we can measure. A 1:7 ratio of hadrons at the end of this epoch would indeed produce the observed element ratios in the early as well as current universe.\n\nAt approximately 1 second after the Big Bang neutrinos decouple and begin traveling freely through space. As neutrinos rarely interact with matter, these neutrinos still exist today, analogous to the much later cosmic microwave background emitted during recombination, around 377,000 years after the Big Bang. The neutrinos from this event have a very low energy, around 10 times smaller than is possible with present-day direct detection. Even high energy neutrinos are notoriously difficult to detect, so this cosmic neutrino background (CNB) may not be directly observed in detail for many years, if at all.\n\nHowever, Big Bang cosmology makes many predictions about the CNB, and there is very strong indirect evidence that the cosmic neutrino background exists, both from Big Bang nucleosynthesis predictions of the helium abundance, and from anisotropies in the cosmic microwave background. One of these predictions is that neutrinos will have left a subtle imprint on the cosmic microwave background (CMB). It is well known that the CMB has irregularities. Some of the CMB fluctuations were roughly regularly spaced, because of the effect of baryonic acoustic oscillations. In theory, the decoupled neutrinos should have had a very slight effect on the phase of the various CMB fluctuations.\n\nIn 2015, it was reported that such shifts had been detected in the CMB. Moreover, the fluctuations corresponded to neutrinos of almost exactly the temperature predicted by Big Bang theory ( compared to a prediction of 1.95K), and exactly three types of neutrino, the same number of neutrino flavours currently predicted by the Standard Model.\n\nPrimordial black holes are a hypothetical type of black hole proposed in 1966, that may have formed during the so-called \"radiation dominated era\", due to the high densities and inhomogeneous conditions within the first second of cosmic time. Random fluctuations could lead to some regions becoming dense enough to undergo gravitational collapse, forming black holes. Current understandings and theories place tight limits on the abundance and mass of these objects.\n\nTypically, primordial black hole formation requires density contrasts (regional variations in the Universe's density) of around formula_1 (10%), where formula_2 is the average density of the Universe. Several mechanisms could produce dense regions meeting this criterion during the early universe, including reheating, cosmological phase transitions and (in so-called \"hybrid inflation models\") axion inflation. Since primordial black holes didn't form from stellar gravitational collapse, their masses can be far below stellar mass (~2×10 g). Stephen Hawking calculated in 1971 that primordial black holes could weigh as little as 10 g. But they can have any size, so they could also be large, and may have contributed to the formation of galaxies.\n\nThe majority of hadrons and anti-hadrons annihilate each other at the end of the hadron epoch, leaving leptons (such as the electron, muons and certain neutrinos) and anti-leptons, dominating the mass of the universe. \n\nThe lepton epoch follows a similar path to the earlier hadron epoch. Initially leptons and anti-leptons are produced in pairs. About 10 seconds after the Big Bang the temperature of the universe falls to the point at which new lepton/anti-lepton pairs are no longer created and most remaining leptons and anti-leptons quickly annihilate each other, giving rise to pairs of high energy photons, and leaving a small residue of non-annihilated leptons.\n\nAfter most leptons and anti-leptons are annihilated at the end of the lepton epoch, most of the mass-energy in the universe is left in the form of photons. (Much of the rest of its mass-energy is in the form of neutrinos and other relativistic particles). Therefore the energy of the universe, and its overall behavior, is dominated by its photons. These photons continue to interact frequently with charged protons, electrons and (eventually) nuclei. They continue to do so for about the next 377,000 years.\n\nBetween about 2 and 20 minutes after the Big Bang, the temperature and pressure of the universe allow nuclear fusion to occur, giving rise to nuclei of a few light elements beyond hydrogen (\"Big Bang nucleosynthesis\"). About 25% of the protons, and all the neutrons fuse to form deuterium, a hydrogen isotope, and most of the deuterium quickly fuses to form helium-4. \n\nAtomic nuclei will easily unbind (break apart) above a certain temperature, related to their binding energy. From about 2 minutes, the falling temperature means that deuterium no longer unbinds, and is stable, and starting from about 3 minutes, helium and other elements formed by the fusion of deuterium also no longer unbind and are stable.\n\nThe short duration and falling temperature means that only the simplest and fastest fusion processes can occur. Only tiny amounts of nuclei beyond helium are formed, because nucleosynthesis of heavier elements is difficult and requires thousands of years even in stars. Small amounts of tritium (another hydrogen isotope) and beryllium-7 and -8 are formed, but these are unstable and are quickly lost again. A small amount of deuterium is left unfused because of the very short duration.\n\nTherefore, the only stable nuclides created by the end of Big Bang nucleosynthesis are protium (single proton/hydrogen nucleus), deuterium, helium-3, helium-4, and lithium-7. By mass, the resulting matter is about 75% hydrogen nuclei, 25% helium nuclei, and perhaps 10 by mass of Lithium-7. The next most common stable isotopes produced are lithium-6, beryllium-9, boron-11, carbon, nitrogen and oxygen (\"CNO\"), but these have predicted abundances of between 5 and 30 parts in 10 by mass, making them essentially undetectable and negligible.\n\nThe amounts of each light element in the early universe can be estimated from old galaxies, and is strong evidence for the Big Bang. For example, the Big Bang should produce about 1 neutron for every 7 protons, allowing for 25% of all nucleons to be fused into helium-4 (2 protons and 2 neutrons out of every 16 nucleons), and this is the amount we find today, and far more than can be easily explained by other processes. Similarly, deuterium fuses extremely easily; any alternative explanation must also explain how conditions existed for deuterium to form, but also left some of that deuterium unfused and not immediately fused again into helium. Any alternative must also explain the proportions of the various light elements and their isotopes. A few isotopes, such as lithium-7, were found to be present in amounts that differed from theory, but over time, these differences have been resolved by better observations.\n\nUntil now, the universe's large scale dynamics and behavior have been determined mainly by radiation – meaning, those constituents that move relativistically (at or near the speed of light), such as photons and neutrinos. As the universe cools, from around 47,000 years (z=3600), the universe's large scale behavior becomes dominated by matter instead. This occurs because the energy density of matter begins to exceed both the energy density of radiation and the vacuum energy density. Around or shortly after this time, the densities of non-relativistic matter (atomic nuclei) and relativistic radiation (photons) become equal, the Jeans length, which determines the smallest structures that can form (due to competition between gravitational attraction and pressure effects), begins to fall and perturbations, instead of being wiped out by free-streaming radiation, can begin to grow in amplitude.\n\nAccording to the Lambda-CDM model, by this stage, the matter in the universe is around 84.5% cold dark matter and 15.5% \"ordinary\" matter. (However the total matter in the universe is only 31.7%, much smaller than the 68.3% of dark energy). There is overwhelming evidence that dark matter exists and dominates our universe, but since the exact nature of dark matter is still not understood, Big Bang theory does not presently cover any stages in its formation.\n\nFrom this point on, and for several billion years to come, the presence of dark matter accelerates the formation of structure in our universe. In the early universe, dark matter gradually gathers in huge filaments under the effects of gravity. This amplifies the tiny inhomogeneities (irregularities) in the density of the universe which was left by cosmic inflation. Over time, slightly denser regions become denser and slightly rarefied (emptier) regions become more rarefied. Ordinary matter eventually gathers together faster than it would otherwise do, because of the presence of these concentrations of dark matter.\n\nAbout 377,000 years after the Big Bang, two connected events occurred: recombination and photon decoupling. Recombination describes the ionized particles combining to form the first neutral atoms, and decoupling refers to the photons released (\"decoupled\") as the newly formed atoms settle into more stable energy states.\n\nJust before recombination, the baryonic matter in the universe was at a temperature where it formed a hot ionized plasma. Most of the photons in the universe interacted with electrons and protons, and could not travel significant distances without interacting with ionized particles. As a result, the universe was opaque or \"foggy\". Although there was light, it was not possible to see, nor can we observe that light through telescopes.\n\nAt around 377,000 years, the universe has cooled to a point where free electrons can combine with the hydrogen and helium nuclei to form neutral atoms. This process is relatively fast (and faster for the helium than for the hydrogen), and is known as recombination. The name is slightly inaccurate and is given for historical reasons: in fact the electrons and atomic nuclei were combining for the first time.\n\nDirectly combining in a low energy state (ground state) is less efficient, so these hydrogen atoms generally form with the electrons still in a high energy state, and once combined, the electrons quickly release energy in the form of one or more photons as they transition to a low energy state. This release of photons is known as photon decoupling. Some of these decoupled photons are captured by other hydrogen atoms, the remainder remain free. By the end of recombination, most of the protons in the universe have formed neutral atoms. This change from charged to neutral particles means that the mean free path photons can travel before capture in effect becomes infinite, so any decoupled photons that have not been captured can travel freely over long distances (see Thomson scattering). The universe has become transparent to visible light, radio waves and other electromagnetic radiation for the first time in its history.\nThe photons released by these newly formed hydrogen atoms initially had a temperature/energy of around ~ 4000 K. This would have been visible to the eye as a pale yellow/orange tinted, or \"soft\", white color. Over billions of years since decoupling, as the universe has expanded, the photons have been red-shifted from visible light to radio waves (microwave radiation corresponding to a temperature of about 2.7 K). Red shifting describes the photons acquiring longer wavelengths and lower frequencies as the universe expanded over billions of years, so that they gradually changed from visible light to radio waves. These same photons can still be detected as radio waves today. They form the cosmic microwave background (\"CMB\"), and they provide crucial evidence of the early universe and how it developed.\n\nAround the same time as recombination, existing pressure waves within the electron-baryon plasma – known as baryon acoustic oscillations – became embedded in the distribution of matter as it condensed, giving rise to a very slight preference in distribution of large-scale objects. Therefore, the cosmic microwave background is a picture of the universe at the end of this epoch including the tiny fluctuations generated during inflation (see diagram), and the spread of objects such as galaxies in the universe is an indication of the scale and size of the universe as it developed over time.\n\nAfter recombination and decoupling, the universe was transparent and had cooled enough to allow light to travel long distances, but there were no light-producing structures such as stars and galaxies. Stars and galaxies are formed when dense regions of gas form due to the action of gravity, and this takes a long time within a near-uniform density of gas and on the scale required, so it is estimated that stars did not exist for perhaps hundreds of millions of years after recombination.\n\nThis period, known as the Dark Ages, began around 377,000 years after the Big Bang. During the Dark Ages, the temperature of the universe cooled from some 4000 K down to about 60 K, and only two sources of photons existed: the photons released during recombination/decoupling (as neutral hydrogen atoms formed), which we can still detect today as the cosmic microwave background (CMB), and photons occasionally released by neutral hydrogen atoms, known as the 21 cm spin line of neutral hydrogen. The hydrogen spin line is in the microwave range of frequencies, and within 3 million years, the CMB photons had redshifted out of visible light to infrared; from that time until the first stars, there were no visible light photons. Other than perhaps some rare statistical anomalies, the universe was truly dark.\n\nThe October 2010 discovery of UDFy-38135539, the first observed galaxy to have existed during the following reionization epoch, gives us a window into these times. The galaxy earliest in this period observed and thus also the most distant galaxy ever observed is currently on the record of Leiden University's Richard J. Bouwens and Garth D. Illingsworth from UC Observatories/Lick Observatory. They found the galaxy UDFj-39546284 to be at a time some 480 million years after the Big Bang or about halfway through the Cosmic Dark Ages at a distance of about 13.2 billion light-years. More recently, the UDFy-38135539, EGSY8p7 and GN-z11 galaxies were found to be around 380–550 million years after the Big Bang and at a distance of around 13.4 billion light-years. There is also currently an observational effort underway to detect the faint 21 cm spin line radiation, as it is in principle an even more powerful tool than the cosmic microwave background for studying the early universe.\n\nStructures may have begun to emerge from around 150 million years, and stars and early galaxies gradually emerged from around 400 to 700 million years. As they emerged, the Dark Ages gradually ended. Because this process was gradual, the Dark Ages only fully ended around 1 billion (1000 million) years, as the universe took its present appearance.\n\nFor about 6.6 million years, between about 10 to 17 million years after the Big Bang (redshift 137–100), the background temperature was between 373 K and 273 K, a temperature compatible with liquid water and common biological chemical reactions. Loeb (2014) speculated that primitive life might in principle have appeared during this window, which he called \"the Habitable Epoch of the Early Universe\". Loeb argues that carbon-based life might have evolved in a hypothetical pocket of the early universe that was dense enough both to generate at least one massive star that subsequently releases carbon in a supernova, and that was also dense enough to generate a planet. (Such dense pockets, if they existed, would have been extremely rare.) Life would also have required a heat differential, rather than just uniform background radiation; this could be provided by naturally-occurring geothermal energy. Such life would likely have remained primitive; it is highly unlikely that intelligent life would have had sufficient time to evolve before the hypothetical oceans freeze over at the end of the habitable epoch.\n\nThe matter in the universe is around 84.5% cold dark matter and 15.5% \"ordinary\" matter. Since the start of the matter-dominated era, the dark matter has gradually been gathering in huge spread out (diffuse) filaments under the effects of gravity. Ordinary matter eventually gathers together faster than it would otherwise do, because of the presence of these concentrations of dark matter. It is also slightly more dense at regular distances due to early baryon acoustic oscillations (BAO) which became embedded into the distribution of matter when photons decoupled. Unlike dark matter, ordinary matter can lose energy by many routes, which means that as it collapses, it can lose the energy which would otherwise hold it apart, and collapse more quickly, and into denser forms. Ordinary matter gathers where dark matter is denser, and in those places it collapses into clouds of mainly hydrogen gas. The first stars and galaxies form from these clouds. Where numerous galaxies have formed, galaxy clusters and superclusters will eventually arise. Large voids with few stars will develop between them, marking where dark matter became less common.\n\nStructure formation in the big bang model proceeds hierarchically, due to gravitational collapse, with smaller structures forming before larger ones. The earliest structures to form are the first stars (known as population III stars), dwarf galaxies, and quasars (which are thought to be bright, early active galaxies containing a supermassive black hole surrounded by a inward-spiralling accretion disk of gas). Before this epoch, the evolution of the universe could be understood through linear cosmological perturbation theory: that is, all structures could be understood as small deviations from a perfect homogeneous universe. This is computationally relatively easy to study. At this point non-linear structures begin to form, and the computational problem becomes much more difficult, involving, for example, \"N\"-body simulations with billions of particles. The Bolshoi Cosmological Simulation is a high precision simulation of this era.\n\nThese Population III stars are also responsible for turning the few light elements that were formed in the Big Bang (hydrogen, helium and small amounts of lithium) into many heavier elements. They can be huge as well as perhaps small – and non-metallic (no elements except hydrogen and helium). The larger stars have very short lifetimes compared to most Main Sequence stars we see today, so they commonly finish burning their hydrogen fuel and explode as supernovae after mere millions of years, seeding the universe with heavier elements over repeated generations. They mark the start of the Stelliferous (starry) era.\n\nAs yet, no Population III stars have been found, so our understanding of them is based on computational models of their formation and evolution. Fortunately, observations of the Cosmic Microwave Background radiation can be used to date when star formation began in earnest. Analysis of such observations made by the European Space Agency's Planck telescope in 2016 concluded that the first generation of stars formed 700 million years after the Big Bang.\n\nQuasars provides some additional evidence of early structure formation. Their light shows evidence of elements such as carbon, magnesium, iron and oxygen. This is evidence that by the time quasars formed, a massive phase of star formation had already taken place, including sufficient generations of population III stars to give rise to these elements.\n\nAs the first stars, dwarf galaxies and quasars gradually form, the intense radiation they emit reionizes much of the surrounding universe; splitting the neutral hydrogen atoms back into a plasma of free electrons and protons for the first time since recombination and decoupling.\n\nReionization is evidenced from observations of quasars. Quasars are a form of active galaxy, and the most luminous objects observed in the universe. Electrons in neutral hydrogen have a specific patterns of absorbing photons, related to electron energy levels and called the Lyman series. Ionized hydrogen does not have electron energy levels of this kind. Therefore, light travelling through ionized hydrogen and neutral hydrogen shows different absorption lines. In addition, the light will have travelled for billions of years to reach us, so any absorption by neutral hydrogen will have been redshifted by varied amounts, rather than by one specific amount, indicating when it happened. These features make it possible to study the state of ionization at many different times in the past. They show that reionization began as \"bubbles\" of ionized hydrogen which became larger over time. They also show that the absorption was due to the general state of the universe (the intergalactic medium) and not due to passing through galaxies or other dense areas. Reionization might have started as early as \"z\"=16 (250 million years of cosmic time) and was complete by around \"z\"=9 or 10 (500 million years). The epoch of reionization probably ended by around \"z\"=5 or 6 (1 billion years) as the era of Population III stars and quasars – and their intense radiation – came to an end, and the ionized hydrogen gradually reverted to neutral atoms.\n\nThese observations have narrowed down the period of time during which reionization took place, but the source of the photons that caused reionization is still not completely certain. To ionize neutral hydrogen, an energy larger than 13.6 eV is required, which corresponds to ultraviolet photons with a wavelength of 91.2 nm or shorter, implying that the sources must have produced significant amount of ultraviolet and higher energy. Protons and electrons will recombine if energy is not continuously provided to keep them apart, which also sets limits on how numerous the sources were and their longevity. With these constraints, it is expected that quasars and first generation stars and galaxies were the main sources of energy. The current leading candidates from most to least significant are currently believed to be population III stars (the earliest stars) (possibly 70%), dwarf galaxies (very early small high-energy galaxies) (possibly 30%), and a contribution from quasars (a class of active galactic nuclei).\n\nHowever, by this time, matter had become far more spread out due to the ongoing expansion of the universe. Although the neutral hydrogen atoms were again ionized, the plasma was much more thin and diffuse, and photons were much less likely to be scattered. Despite being reionized, the universe remained largely transparent during reionization. As the universe continued to cool and expand, reionization gradually ended.\n\nMatter continues to draw together under the influence of gravity, to form galaxies. The stars from this time period, known as Population II stars, are formed early on in this process, with more recent Population I stars formed later. Gravitational attraction also gradually pulls galaxies towards each other to form groups, clusters and superclusters. The Hubble Ultra Deep Field observatory has identified a number of small galaxies merging to form larger ones, at 800 million years of cosmic time (13 billion years ago) (this age estimate is now believed to be slightly overstated).\n\nJohannes Schedler's project has identified a quasar CFHQS 1641+3755 at 12.7 billion light-years away, when the universe was just 7% of its present age. On July 11, 2007, using the 10-metre Keck II telescope on Mauna Kea, Richard Ellis of the California Institute of Technology at Pasadena and his team found six star forming galaxies about 13.2 billion light years away and therefore created when the universe was only 500 million years old. Only about 10 of these extremely early objects are currently known. More recent observations have shown these ages to be shorter than previously indicated. The most distant galaxy observed as of October 2016, GN-z11, has been reported to be 32 billion light years away, a vast distance made possible through space-time expansion (redshift z=11.1; comoving distance of 32 billion light-years; lookback time of 13.4 billion years).\n\nThe universe has appeared much the same as it does now, for many billions of years. It will continue to look similar for many more billions of years into the future.\n\nBased upon the emerging science of nucleocosmochronology, the Galactic thin disk of the Milky Way is estimated to have been formed 8.8 ± 1.7 billion years ago.\n\nFrom about 9.8 billion years of cosmic time, the universe's large-scale behavior is believed to have gradually changed for the third time in its history. Its behavior had originally been dominated by radiation (relativistic constituents such as photons and neutrinos) for the first 47,000 years, and since about 377,000 years of cosmic time, its behavior had been dominated by matter. During its matter-dominated era, the expansion of the universe had begun to slow down, as gravity reigned in the initial outward expansion. But from about 9.8 billion years of cosmic time, observations show that that the expansion of the universe slowly stops decelerating, and gradually begins to accelerate again, instead. \n\nWhile the precise cause is not known, the observation is accepted as correct by the cosmologist community. By far the most accepted understanding is that this is due to an unknown form of energy which has been given the name \"dark energy\". \"Dark\" in this context means that it is not directly observed, but can currently only be studied by examining the effect it has on the universe. Research is ongoing to understand this dark energy. Dark energy is now believed to be the single largest component of the universe, as it constitutes about 68.3% of the entire mass-energy of the physical universe.\n\nDark energy is believed to act like a cosmological constant - a scalar field that exists throughout space. Unlike gravity, the effects of such a field do not diminish (or only diminish slowly) as the universe grows. While matter and gravity have a greater effect initially, their effect quickly diminishes as the universe continues to expand. Objects in the universe, which are initially seen to be moving apart as the universe expands, continue to move apart, but their outward motion gradually slows down. This slowing effect becomes smaller as the universe becomes more spread out. Eventually, the outward and repulsive effect of dark energy begins to dominate over the inward pull of gravity. Instead of slowing down and perhaps beginning to move inward under the influence of gravity, from about 9.8 billion years of cosmic time, the expansion of space starts to slowly accelerate \"outward\" at a gradually \"increasing\" rate.\n\nThe universe has existed for around 13.8 billion years, and we believe that we understand it well enough to predict its large-scale development for many billions of years into the future – perhaps as much as 100 billion years of cosmic time (about 86 billion years from now). Beyond that, we need to better understand the universe to make any accurate predictions. Therefore, the universe could follow a variety of different paths beyond this time.\n\nThere are several competing scenarios for the possible long-term evolution of the universe. Which of them will happen, if any, depends on the precise values of physical constants such as the cosmological constant, the possibility of proton decay, the energy of the vacuum (meaning, the energy of \"empty\" space itself), and the natural laws beyond the Standard Model.\nIf the expansion of the universe continues and it stays in its present form, eventually all but the nearest galaxies will be carried away from us by the expansion of space at such a velocity that our observable universe will be limited to our own gravitationally bound local galactic cluster. In the very long term (after many trillions – thousands of billions – of years, cosmic time), the Stelliferous Era will end, as stars cease to be born and even the longest-lived stars gradually die. Beyond this, all objects in the universe will cool and (with the possible exception of protons) gradually decompose back to their constituent particles and then into subatomic particles and very low level photons and other fundamental particles, by a variety of possible processes. But this will take a duration of time that is almost inconceivable to most people, compared to which the entire 13.8 billion years of the universe would be a tiny instant in time.\n\nUltimately, in the extreme future, the following scenarios have been proposed for the ultimate fate of the universe.\n\nIn this kind of extreme timescale, extremely rare quantum phenomena may also occur that are extremely unlikely to be seen on a timescale smaller than trillions of years. These may also lead to unpredictable changes to the state of the universe which would not be likely to be significant on any smaller timescale. For example, on a timescale of millions of trillions of years, black holes might appear to evaporate almost instantly, uncommon quantum tunneling phenomena would appear to be common, and quantum (or other) phenomena so unlikely that they might occur just once in a trillion years may occur many times.\n\n"}
{"id": "2154755", "url": "https://en.wikipedia.org/wiki?curid=2154755", "title": "Dates of Epoch-Making Events", "text": "Dates of Epoch-Making Events\n\nDates of Epoch-Making Events is an entry in The Nuttall Encyclopaedia for its listing of the most important turning points in history, particularly western history. The work's list illustrates western culture's turning points and James Wood's views from the early 20th century. The events are listed as in the original listing, with modern footnotes.\n\nThe events chosen, with few errors, are:\n\n\n"}
{"id": "9451", "url": "https://en.wikipedia.org/wiki?curid=9451", "title": "Event", "text": "Event\n\nEvent may refer to:\n\n\n\n"}
{"id": "2770842", "url": "https://en.wikipedia.org/wiki?curid=2770842", "title": "Event (philosophy)", "text": "Event (philosophy)\n\nIn philosophy, events are objects in time or instantiations of properties in objects.\n\nJaegwon Kim theorized that events are structured.<br>\nThey are composed of three things:\nEvents are defined using the operation [x, P, t].<br>\nA unique event is defined by two principles:<br>\nThe existence condition states “[x, P, t] exists if and only if object x exemplifies the n-adic P at time t”. This means a unique event exists if the above is met. The identity condition states “[x, P, t] is [y, Q, t`] if and only if x=y, P=Q and t=t`].\n\nKim uses these to define events under five conditions:\n\nOther problems exist within Kim’s theory, as he never specified what properties were (e.g. universals, tropes, natural classes, etc.). In addition, it is not specified if properties are few or abundant. The following is Kim’s response to the above.\n\nThere is also a major debate about the essentiality of a constitutive object. There are two major questions involved in this: If one event occurs, could it have occurred in the same manner if it were another person, and could it occur in the same manner if it would have occurred at a different time? Kim holds that neither are true and that different conditions (i.e. a different person or time) would lead to a separate event. However, some consider it natural to assume the opposite.\n\nDonald Davidson and John Lemmon proposed a theory of events that had two major conditions, respectively: a causal criterion and a spatiotemporal criterion.\n\nThe causal criterion defines an event as two events being the same if and only if they have the same cause and effect.\n\nThe spatiotemporal criterion defines an event as two events being the same if and only if they occur in the same space at the same time. Davidson however provided this scenario; if a metal ball becomes warmer during a certain minute, and during the same minute rotates through 35 degrees, must we say that these are the same event? However, one can argue that the warming of the ball and the rotation are possibly temporally separated and are therefore separate events.\n\nDavid Lewis theorized that events are merely spatiotemporal regions and properties (i.e. membership of a class). He defines an event as “e is an event only if it is a class of spatiotemporal regions, both thisworldly (assuming it occurs in the actual world) and otherworldly.” The only problem with this definition is it only tells us what an event could be, but does not define a unique event. This theory entails modal realism, which assumes possible worlds exist; worlds are defined as sets containing all objects that exist as a part of that set. However, this theory is controversial. Some philosophers have attempted to remove possible worlds, and reduce them to other entities. They hold that the world we exist in is the only world that actually exists, and that possible worlds are only possibilities.\n\nLewis’ theory is composed of four key points. Firstly, the non-duplication principle; it states that x and y are separate events if and only if there is one member of x that is not a member of y (or vice versa). Secondly, there exist regions that are subsets of possible worlds and thirdly, events are not structured by an essential time.\n\nIn \"Being and Event\", Alain Badiou writes that the event (\"événement\") is a multiple which basically does not make sense according to the rules of the \"situation,\" in other words existence. Hence, the event \"is not,\" and therefore, in order for there to be an event, there must be an \"intervention\" which changes the rules of the situation in order to allow that particular event to be (\"to be\" meaning to be a multiple which belongs to the multiple of the situation — these terms are drawn from or defined in reference to set theory). In his view, there is no \"one,\" and everything that a \"multiple.\" \"One\" happens when the situation \"counts,\" or accounts for, acknowledges, or defines something: it \"counts it as one.\" For the event to be counted as one by the situation, or counted in the one of the situation, an intervention needs to decide its belonging to the situation. This is because his definition of the event violates the prohibition against self-belonging (in other words, it is a set-theoretical definition which violates set theory's rules of consistency), thus does not count as extant on its own.\n\nGilles Deleuze lectured on the concept of \"event\" on March 10, 1987. A sense of the lecture is described by James Williams. Williams also wrote, \"From the point of view of the difference between two possible worlds, the event is all important\". He also stated, \"Every event is revolutionary due to an integration of signs, acts and structures through the whole event. Events are distinguished by the intensity of this revolution, rather than the types of freedom or chance.\" In 1988 Deleuze published a magazine article \"Signes et événements\"\n\nIn his book \"Nietszche and Philosophy\", he addresses the question \"Which one is beautiful?\" In the preface to the English translation he wrote:\n\nThe Danish philosopher Ole Fogh Kirkeby deserves mentioning, as he has written a comprehensive trilogy about the event, or in Danish \"begivenheden\". In the first work of the trilogy \"Eventum tantum – begivenhedens ethos\" (Eventum tantum - the ethos of the event) he distinguishes between three levels of the event, inspired from Nicola Cusanus: Eventum tantum as non aliud, the alma-event and the proto-event.\n\n\n"}
{"id": "1647073", "url": "https://en.wikipedia.org/wiki?curid=1647073", "title": "Event videography", "text": "Event videography\n\nEvent videography is a video production, the art of capturing social and special events onto video by a videographer. The term is used to describe the videography of any event, aside from weddings and wedding videography.\n\nEvent videography is an offshoot of wedding videography and encompasses the video documentation of social functions, such as First Communions, anniversaries, dance recitals, bar mitzvahs, color guard contests, proms, concerts, etc.\n\nEvent videography started shortly after the introduction of consumer-based video cameras, or camcorders, in the late 1970s, as videographers, who had businesses documenting weddings, began to look for other markets to offer their services.\n\nThe art of event videography is somewhat similar today as it was back when the camcorder was first introduced. The main differences lie in the improved video camera technology and equipment. Advances in high definition technology are being applied to event videography.\n\n"}
{"id": "10150504", "url": "https://en.wikipedia.org/wiki?curid=10150504", "title": "List of auto shows and motor shows by continent", "text": "List of auto shows and motor shows by continent\n\nAn auto show (also: motor show or car show) is a public exhibition of current automobile models, debuts, concept cars, or out-of-production classics. The five most prestigious auto shows, sometimes called the \"Big Five\", are generally considered to be held in Frankfurt, Geneva, Detroit, Paris and Tokyo.\n\n\n\n\n\n\n\n"}
{"id": "29752766", "url": "https://en.wikipedia.org/wiki?curid=29752766", "title": "List of fan conventions by date of founding", "text": "List of fan conventions by date of founding\n\nThe list of modern fan conventions for various genres of entertainment extends to the first conventions held in the 1930s. \n\nSome fan historians claim that the 1936 Philadelphia Science Fiction Conference, a.k.a. Philcon, was the first science fiction convention ever held. Others, such as Fred Patten and Rob Hansen, make this claim for the January 1937 event in Leeds, England, organized by the Leeds Science Fiction League, which was specifically organised as a conference, with a program and speakers. Out of this came the first incarnation of the British Science Fiction Association.\n\nWhile a few conventions were created in various parts of the world within the period between 1935-1960, the number of convention establishments increased slightly in the 1960s and then increased dramatically in the 1970s, with many of the largest conventions in the modern era being established during the latter decade. Impeti for further establishment of local fan conventions include:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3981713", "url": "https://en.wikipedia.org/wiki?curid=3981713", "title": "List of free-software events", "text": "List of free-software events\n\nThe following is a list of computer conferences and other events focused on the development and usage of free and open-source software (FOSS).\n\nSome events with \"Linux\" in their name are in fact general-purpose free-software events, often because they began as Linux-only events before broadening their focus.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10108021", "url": "https://en.wikipedia.org/wiki?curid=10108021", "title": "List of health-related charity fundraisers", "text": "List of health-related charity fundraisers\n\nThis list of health related charity fundraisers includes events designed to raise funds to fight disease and improve health.\n\nNote that all runs allow jogging and walking.\n"}
{"id": "5531650", "url": "https://en.wikipedia.org/wiki?curid=5531650", "title": "List of helicopter prison escapes", "text": "List of helicopter prison escapes\n\nA helicopter prison escape is made when an inmate escapes from a prison by means of a helicopter. This list includes prisoner escapes where a helicopter was used in an attempt to free prisoners from a place of internment, a prison or correctional facility.\n\nOne of the earliest instances of using a helicopter to escape a prison was the escape of Joel David Kaplan, nicknamed \"Man Fan\", on August 19, 1971 from the Santa Martha Acatitla in Mexico. Kaplan was a New York businessman who not only successfully escaped the prison but eventually escaped Mexico and went on to write a book about his experience, \"The 10-Second Jailbreak\".\n\nFrance has had more recorded helicopter escape attempts than any other country, with at least 11. One of the most notable French jail breaks occurred in 1986, when the wife of bank robber Michel Vaujour studied for months to learn how to fly a helicopter. Using her newly acquired skills, she rented a white helicopter and flew low over Paris to pluck her husband off the roof of his fortress prison. Vaujour was later seriously wounded in a shootout with police, and his pilot wife was arrested.\n\nThe record for most helicopter escapes goes to convicted murderer Pascal Payet, who has used helicopters to escape from prisons in 2001, 2003, and most recently 2007.\n\nAnother multiple helicopter escapee is Vasilis Paleokostas who on February 22, 2009 escaped for the second time from the same prison. Because of this, many prisons have taken applicable precautions, such as nets or cables strung over open prison courtyards.\n\n"}
{"id": "2281261", "url": "https://en.wikipedia.org/wiki?curid=2281261", "title": "List of historical reenactment events", "text": "List of historical reenactment events\n\nThis is a list of historical reenactment events.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "23563446", "url": "https://en.wikipedia.org/wiki?curid=23563446", "title": "List of largest funerals", "text": "List of largest funerals\n\n\n"}
{"id": "21396853", "url": "https://en.wikipedia.org/wiki?curid=21396853", "title": "List of science and engineering blunders", "text": "List of science and engineering blunders\n\nThis is a list of engineering blunders, i.e., gross errors or mistakes resulting from grave lack of proper consideration, such as stupidity, confusion, carelessness, or culpable ignorance, which resulted in notable incidents.\n\n\n"}
{"id": "56504292", "url": "https://en.wikipedia.org/wiki?curid=56504292", "title": "Spanish Cybersecurity Research Conference", "text": "Spanish Cybersecurity Research Conference\n\nThe Spanish Cybersecurity Research Conference (Spanish: Jornadas Nacionales de Investigación en Ciberseguridad (JNIC)), is a scientific congress that works as a meeting point where different actors working in the field of cybersecurity research (universities, technological and research centres, companies and public authorities) can exchange knowledge and experience with the shared goal of strengthening research in the Cybersecurity field at the national level.\n\nThe need to run these kind of conferences was identified during the drafting of the Summary report of the feasibility study and design of a network of centers of excellence in R&D in cybersecurity, with the consensus of participants.\n\nThe strategic plan of the Spanish Network of Excellence on Cybersecurity Research included on its measure #17, the creation of national cybsersecurity R&D+i conferences, intended to be the scientific meeting point in which both the Network of Excellence in particular and the research ecosystem in general could demonstrate their capacities, both in terms of knowledge and talent and in terms of research findings and their potential for transference to market.\n\nEqually, the measure #12 of the same study, proposed the design of an open call for proposals with mechanisms to evaluate and select candidates in order to grant awards and acknowledgement for research excellence.\n\nEach edition of the conferences is organised by the institution selected according to the procedure laid out in the regulation of the JNIC.\n\nAn organising committee is named based on the regulations established for the JNIC, with the General chair of the committee being the representative from the organising institution who is responsible for the event.\n\nThe Spanish National Cybersecurity Institute (INCIBE) in its mission to support research in cybersecurity for strengthening the cybersecurity sector, participates in the organization of this conference.\n\nWith the aim of converting the JNIC into a scientific forum of excellence in national cybersecurity field that promotes the innovation, for the first time in the 2017 edition, a complete Technological Transfer Program has been designed, that is an instrument to bring final users (companies, organisms, etc.) in contact with researchers in order to solve cybersecurity problems that are currently unresolved, formulated as scientific challenges.\n\nAfter the good reception and the success of participation in this new initiative by challengers and research teams, it is expected that the Program will continue to grow and the Transfer Track will be part of the JNIC in future editions.\n\n\n"}
{"id": "29192344", "url": "https://en.wikipedia.org/wiki?curid=29192344", "title": "Tragedy (event)", "text": "Tragedy (event)\n\nA tragedy is an event of great loss, usually of human life. Such an event is said to be \"tragic\". Traditionally the event would require \"some element of moral failure, some flaw in character, or some extraordinary combination of elements\" to be tragic.\n\nNot all death is considered a tragedy. Rather it is a precise set of symptoms surrounding the loss that define it as such. There are a variety of factors that define a death as \"tragic\".\n\nAn event in which a massive number of deaths occur may be seen as a tragedy. This can be re-enforced by media attention or other public outcry.\n\nA tragedy does not necessarily involve massive death. The death of a single person, e.g., a public figure or a child, may be seen as a tragedy. The person need not necessarily have been famous before death.\n\nGenerally, the label of \"tragedy\" is given to an event based on public perception. There are a number of factors that can make a death be considered a tragedy.\n\nThe scope of an event can affect the public view, and make it appear tragic. This can be the case whether the death toll is high, or if a single, unexpected death occurs in a well-beloved person.\n\nThe degree of attachment in the public eye may also impact whether or not the event is publicly labeled as a tragedy. For example, the unexpected death of a preparatory school student that receives heavy media attention may be seen as more tragic than that of a recidivist prisoner who is beaten to death by fellow inmates.\n\nA death may be viewed as a tragedy when it is premature in nature. An old person dying of old age is an expectation, but the death of a child or of a young, healthy adult that is not expected by others can be viewed as tragic.\n\nPublicity is a factor in making the public view an event as a tragedy. With publicity of a large number of deaths or even a single death, this plays on the emotions of the general public, and thereby impacts perception.\n\nThe range of coverage affects the number of people in whose eyes the event is viewed as tragic. While local coverage may garner sympathy from those in the hometown of the deceased, international coverage may lead the whole world to mourn.\n\nThe resulting consequences from one or more deaths can be seen as a tragedy. For example, if a large number of persons are killed in a terrorist attack, not only is life lost, but others may lose their sense of security, and this impacts the lives of others in other ways.\n\nThe long-term effects of an event can render it as tragic. Tragedies often have effects that shape those affected, and are remembered even long after, as they clearly impact the future for those involved. They may also be commemorated on anniversaries or whenever they otherwise come to mind. A public tragedy often leads to measures being taken to prevent similar tragic events in the future.\n\nThere are various ways tragedy can affect people.\n\nThe typical reaction to tragedy is heavy grief, followed by a slow recovery. Common feelings following a tragedy include sadness, depression, crying, blame, and guilt. Some people wonder what they did to deserve such suffering.\n\nFor some, their faith may be a source of comfort in the wake of tragedy.\n\n"}
