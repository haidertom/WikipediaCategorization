{"id": "1230569", "url": "https://en.wikipedia.org/wiki?curid=1230569", "title": "Acronym Finder", "text": "Acronym Finder\n\nAcronym Finder (AF) is a free online searchable dictionary and database of abbreviations (acronyms, initialisms, and others) and their meanings.\n\nThe entries are classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes. For abbreviations with multiple meanings they are listed by popularity with the most common one being listed first. it claims to have over a million \"human-edited\" and verified definitions.\n\nAcronym Finder was registered and the database put online by Michael K. Molloy of Colorado in 1997 but he began compiling it in 1985 working as a computer systems officer for the USAF. Molloy first saw the need of an acronym list while integrating computers at the Randolph Air Force Base in Texas His first acronym list running up-to 30 pages. When he had retired and put AF online in 1997, his list already had 43,000 acronyms. It began mainly as a list of Military/Government abbreviations before expanding to other areas.\n\nMolloy and his wife served as the editors of the website verifying user submissions for abbreviations and adding others they found to the database. Molloy has also provided opinions on abbreviations such as \"MSG\" which Madison Square Garden wanted as a domain name (\"msg.com\") claiming trademark to the abbreviated letters. He stated that MSG also stood for more common things such as monosodium glutamate and message among others. The Garden in the end settled out of court and came to own msg.com.\n\nThe website was maintained under Mountain Data Systems, LLC by Molloy before being sold off and eventually coming under the ownership of Farlex, Inc. publishers of Thefreedictionary.com.\n\nThe website contains a database of meanings and expansions for abbreviations, acronyms, initialisms mainly in English but includes some entries in other languages such as French, German, Spanish etc. as well. It is freely accessible. The entries are further classified into categories such as \"Information Technology, Military/Government, Science, Slang/Pop Culture\" etc. It also contains a database of US and Canadian postal codes which are shown on a Map along with location information. Abbreviations with multiple expansions are listed by popularity with the most common one being presented first, these can be sorted alphabetically as well.\n\nAnyone can contribute to the database by submitting abbreviations and their meanings, these are reviewed by an by editor and categorized before being added to the database. While the database has been described as fairly accurate errors have been found in the meanings and expansions of abbreviations. The website does not list sources for the abbreviations and their meanings but it does identify people who have contributed more than 50 abbreviations to the database.\n\nThe database only contains abbreviations and their expansions and does not list other data such as grammatical category, context, source, field of the abbreviation etc.\n\nFarlex, Inc. the current owner of the website also publishes mobile apps for the Android and iOS operating systems.\n\nAcronym Finder also includes a \"Systematic Buzz Phrase Projector\", a light-hearted tool that randomly generates jargon-like phrases and abbreviations — usually initialisms that would be unpronounceable as acronyms — and meanings from 30 cleverly chosen buzz words.\n\nThe website is supported through advertisements.\n\nThe website is listed as a quick reference tool in directories like Stanford Library, Library of Congress, USC Library. It has been cited as the largest database of acronyms and has been used in computational studies for its database.\n\nListings of abbreviations on the website have also been used as a defense that an abbreviation is in public use and cannot be trademarked. While in some trademark cases citations for AF have been accepted it has been described as an unreliable reference in others.\n\nIt has garnered criticism for the fact that anyone can submit abbreviations to the site and the content is user generated. Mike Molloy the site's original owner had defended that each submission is verified before being added to the database.\n\n"}
{"id": "9721437", "url": "https://en.wikipedia.org/wiki?curid=9721437", "title": "Ask a Librarian", "text": "Ask a Librarian\n\nAsk a Librarian is a live virtual reference service that offers online reference assistance to residents in the state of Florida. Ask a Librarian is an official service of the Florida Electronic Library and is administered by the Tampa Bay Library Consortium (TBLC).\n\nParticipating libraries provide users with virtual reference services via live chat software, text messaging, and e-mail forms which users can access through embedded links and widgets on their library’s official website. Live chat and text messaging are available from 10:00 a.m. to midnight EST from Sunday through Thursday, and from 10:00 a.m. to 5:00 p.m. EST on Friday and Saturday; the e-mail form is available to patrons 24 hours per day, seven days per week. As of March 2016, Ask a Librarian has 133 participating institutions including public libraries and library systems, K-12 libraries, and university and college libraries.\n\nAsk a Librarian began as a partnership between the College Center for Library Automation (CCLA) and TBLC in the interest of creating a statewide virtual reference service that would increase the presence of librarians on the internet. In 2002, the organizations successfully applied for a joint grant through the Library Services and Technology Act (LSTA) and were awarded $339,000 for the development and implementation of their pilot project as a service of the Florida Electronic Library.\n\nThe Ask a Librarian virtual reference service was officially activated on July 28, 2003. Tampa Bay-area libraries were the first to participate, with Pasco County libraries joining shortly afterward. Within the first year of operation, nearly 7,500 Floridians had used the service to get answers from a reference librarian. By August 19, 2007, Ask a Librarian had answered its 100,000th reference question. As of February 2011, Ask a Librarian had logged over 274,000 live virtual reference sessions and e-mail questions.\n\nAsk a Librarian has made several efforts meet user reference needs on smartphones and other mobile devices. In October 2010, Ask a Librarian introduced a text messaging service to accompany their traditional chat service. In 2012, the service introduced a mobile-friendly website interface for tablets and phones. In April 2013, the service also launched the Ask A Librarian Mobile App, a mobile-friendly interface geared toward improving the user chat experience on smartphones and tablets.\n\n"}
{"id": "398493", "url": "https://en.wikipedia.org/wiki?curid=398493", "title": "BRD (Germany)", "text": "BRD (Germany)\n\nBRD (; English: Federal Republic of Germany); () is an unofficial abbreviation commonly used between 1968 and 1990 by the communist regime of the German Democratic Republic (East Germany) to refer to the Federal Republic of Germany, informally known at the time as West Germany. The East German regime previously used the term \"German Federal Republic\" to refer to its western counterpart.\n\nUnlike the English equivalent FRG, which was used as an IOC country code and a FIFA trigramme, the use of \"BRD\" was strongly discouraged by the authorities of the Federal Republic of Germany itself, because it was considered to be a derogatory communist term. The term was not banned by law, but its use was discouraged or forbidden in schools in Western Germany. After German reunification, the country is usually referred to simply as Germany (\"\"), and hence the need for abbreviations is greatly diminished. The most widely used abbreviation for West Germany was its ISO 3166-1 alpha-2 country code \"DE\", which has remained the country code of reunified Germany.\n\nThe official name was and is \"Bundesrepublik Deutschland\" (\"Federal Republic of Germany\"). The name, even though in the beginning referring only to the republic established in the Trizone, was to reflect a name for all of Germany, therefore it was particularly to include the term \"Deutschland\" (\"Germany\"). This corresponded to the spirit of the then West German constitution, the Basic Law, allowing all states or \"Länder\", then under Allied control, to join the new Federal Republic. In 1949 the original eleven states in the Trizone and West Berlin did so. However the latter was prevented by Allied objection on account of the city being a quadripartite allied occupation area. The Saarland joined with effect from 1 January 1957, while the \"new states\" of the East did so with effect from 3 October 1990, including reunited Berlin.\n\nTherefore, the term Germany had an importance as part of the official name, which is reflected in the naming conventions which developed in the Cold War. Starting in June 1949 the abbreviation was sometimes used in the Federal Republic of Germany without any special connotations. The initialism \"BRD\" began to enter into such regular usage in West German scientific and ministerial circles, that it was added to the western edition of the German language dictionary Duden in 1967. The German Democratic Republic at first used the name \"Westdeutschland\" or \"West Germany\" (abbreviated \"WD\") for the Federal Republic of Germany, but since the 1950s the East German government insisted on calling West Germany \"Deutsche Bundesrepublik\" or \"German Federal Republic\" (abbreviated \"DBR\"), because they also considered East Germany part of Germany, and thus would not permit the West German government to use the name \"Germany\".\n\nThis changed in 1968 with the new constitution of the German Democratic Republic. The communists no longer strove for German reunification, and the name \"BRD\" was introduced as a propaganda counter-term to the term \"DDR\", trying to express the equality of the states. Conversely, the West would speak of the \"sogenannte DDR\" or \"so-called 'DDR'\" when it had to be belittled.\n\nAt that time, the initialism \"BRD\" had been adopted by \"Neues Deutschland\", the ruling Socialist Unity Party's daily newspaper, while East German official sources adopted that initialism as standard in 1973.\n\nThe East German decision to abandon the idea of a single German nation was accompanied by omitting the terms \"Deutschland\" (\"Germany\") and \"deutsch\" (\"German\") in a number of terms, for example:\n\n\nHowever, the ruling party's full name, \"Sozialistische Einheitspartei Deutschlands\" or \"Socialist Unity Party of Germany\" remained unchanged, as did that of its newspaper \"Neues Deutschland\" (\"New Germany\") .\nTherefore, using the abbreviation \"BRD\" fitted perfectly into the official East German policy of downplaying the concept of a united Germany. In 1974, the GDR had replaced the vehicle registration code \"D\", hitherto shared with the Federal Republic, for \"DDR\" and demanded that West Germany recognise the division by likewise accepting \"BRD\". \nThis was rejected by the West, where some motorists displayed bumper stickers with the slogan \"BRD - Nein Danke!\" (\"BRD? No Thanks!\"). Thus in the West the initialism became even more objectionable and using it was often considered either unreflecting or even expressing naïve Communist sympathies.\nAs a result, the initialism reached only occasional frequency in West German parlance. In order to be precise West Germans increasingly used the terms \"Bundesrepublik\" or \"Bundesgebiet\" (\"Federal Republic\", or \"Federal Territory\") to refer to the country and \"Bundesbürger\" (\"Federal Citizen[s]\") as to its citizens, with the pertaining adjective \"bundesdeutsch\" (federally German).\n\nTo distance themselves from the term \"BRD\", until German reunification, the government of the Federal Republic of Germany and media sometimes used the abbreviations \"BR Deutschland,\" \"BR-Dt.\", \"BRDt.\",\nWest Germany had always claimed to be \"the\" Germany, and did not like the comparison to \"DDR\", or two separate German states. This claim was also reflected in the Hallstein Doctrine determining its foreign and interior policy until the early 1970s. Named after Walter Hallstein, State secretary at the Foreign Office, this was a key doctrine in the foreign policy of West Germany after 1955, which prescribed that the Federal Republic of Germany would not establish or maintain diplomatic relations with any state that recognised the GDR. Although this changed after 1973, with the Federal Republic no longer asserting an exclusive mandate over the whole of Germany, West Germany only established \"de facto\" diplomatic relations with East Germany. Under the terms of the Basic Treaty in 1972, Bonn and East Berlin exchanged \"permanent missions\", headed by \"permanent representatives\", rather than \"de jure\" embassies headed by ambassadors. Similarly, relations with the GDR were not conducted through the Foreign Office, but through a separate Federal Ministry for Intra-German Relations, to which the East German mission was accredited.\n\nIn 1965 the Federal Minister of All-German Affairs (later Intra-German Relations) issued the \"Directives for the appellation of Germany\" recommending that the use of \"BRD\" be avoided. On 31 May 1974 the heads of the federal and state governments recommended that the full name should always be used in official publications. In November 1979 the federal government informed the Bundestag that the West German public broadcasters ARD and ZDF agreed not to use the initialism.\n\nUnder the West German federal system, the states were generally responsible for school education, and by the 1970s, some of them had either already recommended omitting the initialism, or, in the case of Bavaria, forbidden it. Similarly, a decree by the educational authorities in the state of Schleswig-Holstein of 4 October 1976 declared the term to be \"nicht wünschenswert\" or \"undesirable\". The conference of all the states ministers for school education decided on 12 February 1981 to not print the initialism in books, maps, and atlases for schools. with pupils being required to write \"Bundesrepublik Deutschland\" in full and use of the term being deemed an error. The different usages were so ingrained that one could deduce a person's or source's political leaning from the name used for West Germany, with far-left movements in the country using \"BRD\".\n\nHowever, as the Association for the German Language found, this debate on the initialism had little influence on changing the West German parlance with the usage of the initialism - in any event limited - unaffected by the debate.\n\nA similar ideological question was the question whether to use \"Berlin (West)\" (the officially preferred name) or \"West Berlin\", and even whether to write \"West Berlin\" in German as two hyphenated words - \"West-Berlin\" - or as one word - \"Westberlin\".\n\nMost Westerners called the Western sectors \"Berlin\", unless further distinction was necessary. The West German Federal government initially called West Berlin \"Groß-Berlin\" or \"Greater Berlin\", but changed this \"Berlin (West)\", although it also used the hyphenated \"West-Berlin\". However, the East German government commonly referred to it as \"Westberlin\". Starting from 31 May 1961, East Berlin was officially called \"Berlin, Hauptstadt der DDR\" (Berlin, Capital of the GDR), replacing the formerly used term \"Democratic Berlin\", or simply \"Berlin\", by East Germany, and \"Berlin (Ost)\" by the West German Federal government. Other names used by West German media included \"Ost-Berlin\" and \"Ostberlin\" (both meaning \"East Berlin\") as well as \"Ostsektor\" or \"Eastern Sector\". These different naming conventions for the divided parts of Berlin, when followed by individuals, governments, or media, commonly indicated their political leanings, with the centre-right \"Frankfurter Allgemeine Zeitung\" using \"Ost-Berlin\" and the centre-left \"Süddeutsche Zeitung\" using \"Ostberlin\".\n\nThe naming of the German Democratic Republic was also a controversial issue, West Germans at first preferring the names \"Mitteldeutschland\" (\"Middle Germany\") and \"Sowjetische Besatzungszone\" (Soviet Occupation Zone) abbreviated as \"SBZ\". This only changed under Willy Brandt when West German authorities started using the official name, \"Deutsche Demokratische Republik\" or \"DDR\", but many conservative German newspapers, like \"Bild\", owned by the Springer company, always wrote \"DDR\" in scare quotes until 1 August 1989.\n\nIn 1995, a disagreement arose between reunified Germany and newly independent Slovakia, as Germany objected to the use of the Slovak language name \"Nemecká spolková republika\" (literally \"German Federal Republic\") owing to its Cold War connotations, instead of \"Spolková republika Nemecko\". This was almost identical to the equivalent \"Spolková republika Německo\" in Czech, a language closely related to Slovak, but the Slovak authorities claimed that \"Federal Republic of Germany\" could not be translated grammatically into Slovak. However, the Slovak government had used it until the previous year, leading to suggestions in the Bratislava newspaper \"Narodna Obroda\" that they were using \"German Federal Republic\" to show their displeasure with German attitudes to the country.\n"}
{"id": "46842956", "url": "https://en.wikipedia.org/wiki?curid=46842956", "title": "Bekker numbering", "text": "Bekker numbering\n\nBekker numbering or Bekker pagination is the standard form of citation to the works of Aristotle. It is based on the page numbers used in the Prussian Academy of Sciences edition of the complete works of Aristotle and takes its name from the editor of that edition, the classical philologist August Immanuel Bekker (1785-1871); because the Academy was located in Berlin, the system is occasionally referred to by the alternative name Berlin numbering or Berlin pagination.\n\nBekker numbers consist of up to three ordered coordinates, or pieces of information: a number, the letter a or b, and another number, which refer respectively to the page number of Bekker's edition of the Greek text of Aristotle's works, the page column (a standard page of Bekker's edition has exactly two columns), and the line number (total lines typically ranging from 20-40 on a given column or page in Bekker's edition). For example, the Bekker number denoting the beginning of Aristotle's Nicomachean Ethics is \"1094a1\", which corresponds to page 1094 of Bekker's edition, first column (column a), line 1.\n\nAll modern editions or translations of Aristotle intended for scholarly readers use Bekker numbers, in addition to or instead of page numbers. Contemporary scholars writing on Aristotle use the Bekker number so that the author's citations can be checked by readers without having to use the same edition or translation that the author used.\n\nWhile Bekker numbers are the dominant method used to refer to the works of Aristotle, Catholic or Thomist scholars often use the medieval method of reference by book, chapter, and sentence, albeit generally in addition to Bekker numbers.\n\nStephanus pagination is the comparable system for referring to the works of Plato, and Diels-Kranz numbering is the comparable system for Pre-Socratic philosophy. Unlike Stephanus pagination, which is based upon a three-volume translation of Plato's works and which recycles low page numbers across the three volumes, introducing the possibility for ambiguity if the Platonic work or volume is not specified, Bekker page numbers cycle from 1 through the end of the \"Corpus Aristotelicum\" regardless of volume, without starting over for some other given volume. Bekker numbering therefore has the advantage that its notation is unambiguous as compact numerical information, although it relies upon the ordering of Aristotle's works as presented in Bekker's edition.\n\nThe following list is complete. The titles are given in accordance with the standard set by the Revised Oxford Translation. Latin titles, still often used by scholars, are also given.\n\nThe \"Constitution of the Athenians\" (or \"Athenaiōn Politeia\") was not included in Bekker's edition because it was first edited in 1891 from papyrus rolls acquired in 1890 by the British Museum. The standard reference to it is by section (and subsection) numbers.\n\nSurviving fragments of the many lost works of Aristotle were included in the fifth volume of Bekker's edition, edited by Valentin Rose. These are not cited by Bekker numbers, however, but according to fragment numbers. Rose's first edition of the fragments of Aristotle was \"Aristoteles Pseudepigraphus\" (1863). As the title suggests, Rose considered these all to be spurious. The numeration of the fragments in a revised edition by Rose, published in the Teubner series, \"Aristotelis qui ferebantur librorum fragmenta\", Leipzig, 1886, is still commonly used (indicated by \"R\"), although there is a more current edition with a different numeration by Olof Gigon (published in 1987 as a new vol. 3 in Walter de Gruyter's reprint of the Bekker edition), and a new de Gruyter edition by Eckart Schütrumpf is in preparation.\n\nFor a selection of the fragments in English translation, see W.D. Ross, \"Select Fragments\" (Oxford 1952), and Jonathan Barnes (ed.), \"The Complete Works of Aristotle: The Revised Oxford Translation\", vol. 2, Princeton 1984, pp. 2384–2465.\n\nThe works surviving only in fragments include the dialogues \"On Philosophy\" (or \"On the Good\"), \"Eudemus\" (or \"On the Soul\"), \"On Justice\", and \"On Good Birth\". The possibly spurious work, \"On Ideas\" survives in quotations by Alexander of Aphrodisias in his commentary on Aristotle's \"Metaphysics\". For the dialogues, see also the editions of Richard Rudolf Walzer, \"Aristotelis Dialogorum fragmenta, in usum scholarum\" (Florence 1934), and Renato Laurenti, \"Aristotele: I frammenti dei dialoghi\" (2 vols.), Naples: Luigi Loffredo, 1987.\n\n"}
{"id": "2887701", "url": "https://en.wikipedia.org/wiki?curid=2887701", "title": "Bible (screenwriting)", "text": "Bible (screenwriting)\n\nA bible (also known as a story bible, show bible, series bible, or pitch bible) is a reference document used by screenwriters for information on a television series' characters, settings, and other elements.\n\nShow bibles are updated with information on the characters after the information has been established on screen. For example, the \"Frasier\" show bible was \"scrupulously maintained\", and anything established on air — \"the name of Frasier's mother, Niles' favorite professor, Martin's favorite bar...even a list of Maris' [dozens of] food allergies\" — was reflected in the bible. The updated bible then serves as a resource for writers to keep everything within the series consistent. \n\nOther show bibles are used as sales documents to help a television network or studio understand a series, and are sometimes given to new writers when they join the writing staff for the same reason. These types of bibles discuss the backstories of the main characters and the history of the series' fictional universe.\n\nTelevision series often rely on writers' assistants and script coordinators to serve as \"walking bibles\" in remembering details about a series.\n\nIn the United States, writing the show bible of a produced series earns that writer the 24 units of required credit necessary to qualify for membership in the Writers Guild of America.\n\n\n"}
{"id": "27778631", "url": "https://en.wikipedia.org/wiki?curid=27778631", "title": "Breviograph", "text": "Breviograph\n\nA breviograph or brevigraph (from , short, and Greek \"grapho\", to write) is a type of scribal abbreviation in the form of an easily written symbol, character, flourish or stroke, based on a modified letter form to take the place of a common letter combination, especially those occurring at the beginning or end of a word. Breviographs were used frequently by stenographers, law clerks and scriveners, and they were also found in early printed books and tracts. Their use declined after the 17th century.\n\nExamples of breviographs:\n\n\n"}
{"id": "1250078", "url": "https://en.wikipedia.org/wiki?curid=1250078", "title": "Cf.", "text": "Cf.\n\nThe abbreviation cf. (short for the , both meaning \"compare\") is used in writing to refer the reader to other material to make a comparison with the topic being discussed. It is used to form a contrast, for example: \"Abbott (2010) found supportive results in her memory experiment, unlike those of previous work (cf. Zeller & Williams, 2007).\" It is recommended that \"cf.\" be used only to suggest a comparison, and the word \"see\" be used to point to a source of information.\n\nIn biological naming conventions, cf. is commonly placed between the genus name and the species name to describe a specimen that is difficult to identify because of practical difficulties, such as the specimen being poorly preserved. For example, \"' cf. '\" indicates that the specimen is in the genus \"Barbus\", and believed to be \"\" but the actual species-level identification cannot be certain.\n\nCf. can also be used to express a possible identity, or at least a significant resemblance, such as between a newly observed specimen and a known species or taxon. Such a usage might suggest a specimen's membership of the same genus or possibly of a shared higher taxon, such as in, \", cf. \"\"\", where the author is confident of the order and family (Diptera: Tabanidae), but can only offer the genus (\"Tabanus\") as a suggestion and has no information favouring a particular species.\n\n"}
{"id": "12185843", "url": "https://en.wikipedia.org/wiki?curid=12185843", "title": "Comparative cognition", "text": "Comparative cognition\n\nComparative cognition is the comparative study of the mechanisms and origins of cognition in various species, and is sometimes seen as more general than, or similar to, comparative psychology.\nFrom a biological point of view, work is being done on the brains of fruit flies that should yield techniques precise enough to allow an understanding of the workings of the human brain on a scale appreciative of individual groups of neurons rather than the more regional scale previously used. Similarly, gene activity in the human brain is better understood through examination of the brains of mice by the Seattle-based Allen Institute for Brain Science (see link below), yielding the freely available Allen Brain Atlas. This type of study is related to comparative cognition, but better classified as one of comparative genomics. Increasing emphasis in psychology and ethology on the biological aspects of perception and behavior is bridging the gap between genomics and behavioral analysis.\n\nIn order for scientists to better understand cognitive function across a broad range of species they can systematically compare cognitive abilities between closely and distantly related species Through this process they can determine what kinds of selection pressure has led to different cognitive abilities across a broad range of animals. For example, it has been hypothesized that there is convergent evolution of the higher cognitive functions of corvids and apes, possibly due to both being omnivorous, visual animals that live in social groups.\n\n\n"}
{"id": "2051798", "url": "https://en.wikipedia.org/wiki?curid=2051798", "title": "Comparative contextual analysis", "text": "Comparative contextual analysis\n\nComparative contextual analysis is a methodology for comparative research where contextual interrogation precedes any analysis of similarity and difference. It is a thematic process directed and designed to explore relationships of agency rather than institutional or structural frameworks. See structure and agency and theory of structuration.\n\n\n"}
{"id": "917868", "url": "https://en.wikipedia.org/wiki?curid=917868", "title": "Comparative genomics", "text": "Comparative genomics\n\nComparative genomics is a field of biological research in which the genomic features of different organisms are compared. The genomic features may include the DNA sequence, genes, gene order, regulatory sequences, and other genomic structural landmarks. In this branch of genomics, whole or large parts of genomes resulting from genome projects are compared to study basic biological similarities and differences as well as evolutionary relationships between organisms. The major principle of comparative genomics is that common features of two organisms will often be encoded within the DNA that is evolutionarily conserved between them. Therefore, comparative genomic approaches start with making some form of alignment of genome sequences and looking for orthologous sequences (sequences that share a common ancestry) in the aligned genomes and checking to what extent those sequences are conserved. Based on these, genome and molecular evolution are inferred and this may in turn be put in the context of, for example, phenotypic evolution or population genetics.\n\nVirtually started as soon as the whole genomes of two organisms became available (that is, the genomes of the bacteria \"Haemophilus influenzae\" and \"Mycoplasma genitalium\") in 1995, comparative genomics is now a standard component of the analysis of every new genome sequence. With the explosion in the number of genome projects due to the advancements in DNA sequencing technologies, particularly the next-generation sequencing methods in late 2000s, this field has become more sophisticated, making it possible to deal with many genomes in a single study. Comparative genomics has revealed high levels of similarity between closely related organisms, such as humans and chimpanzees, and, more surprisingly, similarity between seemingly distantly related organisms, such as humans and the yeast \"Saccharomyces cerevisiae\". It has also showed the extreme diversity of the gene\ncomposition in different evolutionary lineages.\n\n\"See also\": History of genomics\n\nComparative genomics has a root in the comparison of virus genomes in the early 1980s. For example, small RNA viruses infecting animals (picornaviruses) and those infecting plants (cowpea mosaic virus) were compared and turned out to share significant sequence similarity and, in part, the order of their genes. In 1986, the first comparative genomic study at a larger scale was published, comparing the genomes of varicella-zoster virus and Epstein-Barr virus that contained more than 100 genes each.\n\nThe first complete genome sequence of a cellular organism, that of \"Haemophilus influenzae\" Rd, was published in 1995. The second genome sequencing paper was of the small parasitic bacterium \"Mycoplasma genitalium\" published in the same year. Starting from this paper, reports on new genomes inevitably became comparative-genomic studies.\n\nThe first high-resolution whole genome comparison system was developed in 1998 by Art Delcher, Simon Kasif and Steven Salzberg and applied to the comparison of entire highly related microbial organisms with their collaborators at the Institute for Genomic Research (TIGR). The system is called MUMMER and was described in a publication in Nucleic Acids Research in 1999. The system helps researchers to identify large rearrangements, single base mutations, reversals, tandem repeat expansions and other polymorphisms. In bacteria, MUMMER enables the identification of polymorphisms that are responsible for virulence, pathogenicity, and anti-biotic resistance. The system was also applied to the Minimal Organism Project at TIGR and subsequently to many other comparative genomics projects.\n\n\"Saccharomyces cerevisiae\", the baker's yeast, was the first eukaryote to have its complete genome sequence published in 1996. After the publication of the roundworm \"Caenorhabditis elegans\" genome in 1998 and together with the fruit fly \"Drosophila melanogaster\" genome in 2000, Gerald M. Rubin and his team published a paper titled \"Comparative Genomics of the Eukaryotes\", in which they compared the genomes of the eukaryotes \"D. melanogaster\", \"C. elegans\", and \"S. cerevisiae\", as well as the prokaryote \"H. influenzae\". At the same time, Bonnie Berger, Eric Lander, and their team published a paper on whole-genome comparison of human and mouse.\n\nWith the publication of the large genomes of vertebrates in the 2000s, including human, the Japanese pufferfish \"Takifugu rubripes\", and mouse, precomputed results of large genome comparisons have been released for downloading or for visualization in a genome browser. Instead of undertaking their own analyses, most biologists can access these large cross-species comparisons and avoid the impracticality caused by the size of the genomes.\n\nNext-generation sequencing methods, which were first introduced in 2007, have produced an enormous amount of genomic data and have allowed researchers to generate multiple (prokaryotic) draft genome sequences at once. These methods can also quickly uncover single-nucleotide polymorphisms, insertions and deletions by mapping unassembled reads against a well annotated reference genome, and thus provide a list of possible gene differences that may be the basis for any functional variation among strains.\n\nOne character of biology is evolution, evolutionary theory is also the theoretical foundation of comparative genomics, and at the same time the results of comparative genomics unprecedentedly enriched and developed the theory of evolution. When two or more of the genome sequence are compared, one can deduce the evolutionary relationships of the sequences in a phylogenetic tree. Based on a variety of biological genome data and the study of vertical and horizontal evolution processes, one can understand vital parts of the gene structure and its regulatory function.\n\nSimilarity of related genomes is the basis of comparative genomics. If two creatures have a recent common ancestor, the differences between the two species genomes are evolved from the ancestors’ genome. The closer the relationship between two organisms, the higher the similarities between their genomes. If there is close relationship between them, then their genome will display a linear behaviour (synteny), namely some or all of the genetic sequences are conserved. Thus, the genome sequences can be used to identify gene function, by analyzing their homology (sequence similarity) to genes of known function.\n\nOrthologous sequences are related sequences in different species: a gene exists in the original species, the species divided into two species, so genes in new species are orthologous to the sequence in the original species. Paralogous sequences are separated by gene cloning (gene duplication): if a particular gene in the genome is copied, then the copy of the two sequences is paralogous to the original gene. A pair of orthologous sequences is called orthologous pairs (orthologs), a pair of paralogous sequence is called collateral pairs (paralogs). Orthologous pairs usually have the same or similar function, which is not necessarily the case for collateral pairs. In collateral pairs, the sequences tend to evolve into having different functions.\nComparative genomics exploits both similarities and differences in the proteins, RNA, and regulatory regions of different organisms to infer how selection has acted upon these elements. Those elements that are responsible for similarities between different species should be conserved through time (stabilizing selection), while those elements responsible for differences among species should be divergent (positive selection). Finally, those elements that are unimportant to the evolutionary success of the organism will be unconserved (selection is neutral).\n\nOne of the important goals of the field is the identification of the mechanisms of eukaryotic genome evolution. It is however often complicated by the multiplicity of events that have taken place throughout the history of individual lineages, leaving only distorted and superimposed traces in the genome of each living organism. For this reason comparative genomics studies of small model organisms (for example the model Caenorhabditis elegans and closely related Caenorhabditis briggsae) are of great importance to advance our understanding of general mechanisms of evolution.\n\nComputational approaches to genome comparison have recently become a common research topic in computer science. A public collection of case studies and demonstrations is growing, ranging from whole genome comparisons to gene expression analysis. This has increased the introduction of different ideas, including concepts from systems and control, information theory, strings analysis and data mining. It is anticipated that computational approaches will become and remain a standard topic for research and teaching, while multiple courses will begin training students to be fluent in both topics.\n\nComputational tools for analyzing sequences and complete genomes are developing quickly due to the availability of large amount of genomic data. At the same time, comparative analysis tools are progressed and improved. In the challenges about these analyses, it is very important to visualize the comparative results.\n\nVisualization of sequence conservation is a tough task of comparative sequence analysis. As we know, it is highly inefficient to examine the alignment of long genomic regions manually. Internet-based genome browsers provide many useful tools for investigating genomic sequences due to integrating all sequence-based biological information on genomic regions. When we extract large amount of relevant biological data, they can be very easy to use and less time-consuming.\n\n\nAn advantage of using online tools is that these websites are being developed and updated constantly. There are many new settings and content can be used online to improve efficiency.\n\nAgriculture is a field that reaps the benefits of comparative genomics. Identifying the loci of advantageous genes is a key step in breeding crops that are optimized for greater yield, cost-efficiency, quality, and disease resistance. For example, one genome wide association study conducted on 517 rice landraces revealed 80 loci associated with several categories of agronomic performance, such as grain weight, amylose content, and drought tolerance. Many of the loci were previously uncharacterized. Not only is this methodology powerful, it is also quick. Previous methods of identifying loci associated with agronomic performance required several generations of carefully monitored breeding of parent strains, a time consuming effort that is unnecessary for comparative genomic studies.\n\nThe medical field also benefits from the study of comparative genomics. Vaccinology in particular has experienced useful advances in technology due to genomic approaches to problems. In an approach known as reverse vaccinology, researchers can discover candidate antigens for vaccine development by analyzing the genome of a pathogen or a family of pathogens. Applying a comparative genomics approach by analyzing the genomes of several related pathogens can lead to the development of vaccines that are multiprotective. A team of researchers employed such an approach to create a universal vaccine for Group B Streptococcus, a group of bacteria responsible for severe neonatal infection. Comparative genomics can also be used to generate specificity for vaccines against pathogens that are closely related to commensal microorganisms. For example, researchers used comparative genomic analysis of commensal and pathogenic strains of E. coli to identify pathogen specific genes as a basis for finding antigens that result in immune response against pathogenic strains but not commensal ones.\n\nComparative genomics also opens up new avenues in other areas of research. As DNA sequencing technology has become more accessible, the number of sequenced genomes has grown. With the increasing reservoir of available genomic data, the potency of comparative genomic inference has grown as well. A notable case of this increased potency is found in recent primate research. Comparative genomic methods have allowed researchers to gather information about genetic variation, differential gene expression, and evolutionary dynamics in primates that were indiscernible using previous data and methods. The Great Ape Genome Project used comparative genomic methods to investigate genetic variation with reference to the six great ape species, finding healthy levels of variation in their gene pool despite shrinking population size. Another study showed that patterns of DNA methylation, which are a known regulation mechanism for gene expression, differ in the prefrontal cortex of humans versus chimps, and implicated this difference in the evolutionary divergence of the two species.\n\n\n\n"}
{"id": "39275628", "url": "https://en.wikipedia.org/wiki?curid=39275628", "title": "Comparison of professional wrestling and mixed martial arts", "text": "Comparison of professional wrestling and mixed martial arts\n\nProfessional wrestling and mixed martial arts (Also known as MMA) both combine grappling and strikes. In MMA fights are competed over, in contrast to professional wrestling where the outcomes and moves performed are often scripted or predetermined. Despite the differences in competition, there have been people that have competed in both professional wrestling and MMA. \nProfessional wrestling could be considered a performance art which combines athletics with theatrical performance. Matches are contested on a pre-determined basis, where the fight on display is merely for entertainment purposes. Viewers are integral to a professional wrestling match, as the audience is who the action is for. Professional wrestling mixes many styles of amateur wrestling, striking and showmanship to display a fight, whilst the two performers work together to achieve a \"worked\" fight. Professional wrestling was first popularized in the 19th century Europe as a carnival attraction.\n\nMixed Martial Arts, is a hybrid of many types of physical full-contact sports; including wrestling, boxing and Martial Arts, such as Brazilian Jiu-Jitsu. The early 1990s saw the Ultimate Fighting Championship popularize the term \"Mixed Martial Arts\", for such a bout. Competitors in MMA are generally skilled in many different styles, however, it is possible to be only proficient in one type of combat. As some professional wrestling moves are simply worked versions of legitimate holds, there can be a crossover between the two.\n\nPossibly the most successful pro wrestler and MMA star is Brock Lesnar, who has won the world championships in both the WWE and UFC. Other wrestlers to have won a championship in both MMA and Pro Wrestling include Bobby Lashley (Shark Fights Heavyweight Championship as well as the Impact Global Championship), and Dan Severn who won the UFC Superfight Championship, and the NWA World Heavyweight Championship.\n\nSince MMA's rise to prominence in the 1990s, some pro wrestlers or MMA fighters have been cynical against the other's profession. This can be an attack on the sport as a whole, or acting as a fan dismissing certain actions within that sport.\n\nBelow is a list of professional wrestlers to have appeared in an official Mixed Martial Arts match; as well as their career win/loss record. All records are written as W-L-D. \n"}
{"id": "16759434", "url": "https://en.wikipedia.org/wiki?curid=16759434", "title": "Comparison of the Amundsen and Scott Expeditions", "text": "Comparison of the Amundsen and Scott Expeditions\n\nBetween December 1911 and January 1912, both Roald Amundsen (leading his South Pole expedition) and Robert Falcon Scott (leading the Terra Nova Expedition) reached the South Pole within a month of each other. But while Scott and his four companions died on the return journey, Amundsen's party managed to reach the geographic south pole first and subsequently return to their base camp at Framheim without loss of life, suggesting that they were better prepared for the expedition. The contrasting fates of the two teams seeking the same prize at the same time invites comparison.\n\nThe outcomes of the two expeditions were as follows.\n\nHistorically, several factors have been discussed and many contributing factors claimed, including:\n\nSullivan states that it was the last factor that probably was decisive. he states \"Man is a poor beast of burden, as was shown in the terrible experience of Scott, Shackleton, and Wilson in their thrust to the south of 1902–3. However, Scott relied chiefly on man-hauling in 1911–12 because ponies could not ascend the glacier midway to the Pole. The Norwegians correctly estimated that dog teams could go all the way. Furthermore, they used a simple plan, based on their native skill with skis and on dog-driving methods that were tried and true. In a similar fashion to the way the moon was reached by expending a succession of rocket stages and then casting each aside; the Norwegians used the same strategy, sacrificing the weaker animals along the journey to feed the other animals and the men themselves.\"\n\nScott and his financial backers saw the expedition as having a scientific basis, while also wishing to reach the pole. However, it was recognised by all involved that the South Pole was the primary objective (\"The Southern Journey involves the most important object of the Expedition\" – Scott), and had priority in terms of resources, such as the best ponies and all the dogs and motor sledges as well as involvement of the vast majority of the expedition personnel. Scott and his team knew the expedition would be judged on his attainment of the pole (\"The ... public will gauge the result of the scientific work of the expedition largely in accordance with the success or failure of the main object\" – Scott). He was prepared to make a second attempt the following year (1912–13) if this attempt failed and had Indian Army mules and additional dogs delivered in anticipation. In fact the mules were used by the team that discovered the dead bodies of Scott, Henry Robertson Bowers, and Edward Adrian Wilson in November 1912, but proved even less useful than the ponies, according to Cherry-Garrard.\n\nAmundsen's expedition was planned to reach the South Pole. This was a plan he conceived in 1909. Amundsen's expedition did conduct geographical work under Kristian Prestrud who conducted an expedition to King Edward VII Land while Amundsen was undertaking his attempt at the pole.\n\nAmundsen camped on the Ross Ice Shelf at the Bay of Whales which is 60 miles (96 km) closer to the pole than Scott's camp (which was 350 miles west of Amundsen). Amundsen had deduced that, as the Trans-Antarctic Mountains ran northwest to southeast then if he were to meet a mountain range on his route then the time spent at the high altitude of the Antarctic plateau would be less than Scott's.\nScott's base was at Cape Evans on Ross Island, with access to the Trans-Antarctic mountain range to the west, and was a better base for geological exploration. He had based his previous expedition in the same area. However, he knew it to be poor as a route to the pole as he had to start before sea ice melted and had suffered delay in returning while waiting for the sea ice to freeze. They also had to make detours around Ross Island and its known crevassed areas which meant a longer journey. The crossing of the Ross Ice Shelf was an onerous task for the ponies. Scott had advanced considerable stores across the ice shelf the year before to allow the ponies to carry lighter loads over the early passage across the ice. Even so, he had to delay the departure of the ponies until 1 November rather than 24 October when the dogs and motor sledges set off.\nConsequently, the Motor Party spent 6 days at the Mount Hooper Depot waiting for Scott to arrive.\n\nThe major comparison between Scott and Amundsen has focused on the choice of draft transport —dog versus pony/man-hauling. In fact Scott took dogs, ponies and three \"motor sledges\". Scott spent nearly seven times the amount of money on his motor sledges than on the dogs and horses combined. They were therefore a vital part of the expedition. Unfortunately, Scott decided to leave behind the engineer, Lieutenant Commander Reginald William Skelton who had created and trialled the motor sledges. This was due to the selection of Lieutenant E.R.G.R. \"Teddy\" Evans as the expedition's second in command. As Evans was junior in rank to Skelton, he insisted that Skelton could not come on the expedition. Scott agreed to this request and Skelton's experience and knowledge was lost. One of the original three motor sledges was a failure even before the expedition set out; the heavy sledge was lost through thin ice on unloading it from the ship. The two remaining motor sledges failed relatively early in the main expedition because of repeated faults. Skelton's experience might have been valuable in overcoming the failures.\n\nScott had used dogs on his first (Discovery) expedition and felt they had failed. On that journey, Scott, Shackleton, and Wilson started with three sledges and 13 dogs. But on that expedition, the men had not properly understood how to travel on snow with the use of dogs. The party had skis but were too inexperienced to make good use of them. As a result, the dogs travelled so fast that the men could not keep up with them. The Discovery expedition had to increase their loads to slow the dogs down. Additionally, the dogs were fed Norwegian dried fish, which did not agree with them and soon they began to deteriorate. The whole team of dogs eventually died (and were eaten), and the men took over hauling the sleds.\n\nScott's opinion was reinforced by Shackleton's experience on his Nimrod expedition that got to within of the pole. Shackleton used ponies. Scott planned to use ponies only to the base of the Beardmore Glacier (one-quarter of the total journey) and man-haul the rest of the journey. Scott's team had developed snow shoes for his ponies, and trials showed they could significantly increase daily progress. However, Lawrence Oates, whom Scott had made responsible for the ponies, was reluctant to use the snow shoes and Scott failed to insist on their use.\n\nThere was plenty of evidence that dogs could succeed in the achievements of William Speirs Bruce in his Arctic, Antarctic, and Scottish National Antarctic Expedition, Amundsen in the \"Gjøa\" North West passage expedition, Fridtjof Nansen's crossing of Greenland, Robert Peary's three attempts at the North Pole, Eivind Astrup's work supporting Peary, Frederick Cook's discredited North Pole expedition, and Otto Sverdrup's explorations of Ellesmere Island. Moreover, Scott ignored the direct advice he received (while attending trials of the motor sledges in Norway) from Nansen, the most famous explorer of the day, who told Scott to take \"dogs, dogs and more dogs\".\n\nAt the time of the events, the expert view in England had been that dogs were of dubious value as a means of Antarctic transport. Broadly speaking, Scott saw two ways in which dogs may be used—they may be taken with the idea of bringing them all back safe and sound, or they may be treated as pawns in the game, from which the best value is to be got regardless of their lives. He stated that if, and only if, the comparison was made with a dog sledge journey which aimed to preserve the dogs' lives, 'I am inclined to state my belief that in the polar regions properly organised parties of men will perform as extended journeys as teams of dogs.' On the other hand, if the lives of the dogs were to be sacrificed, then 'the dog-team is invested with a capacity for work which is beyond the emulation of men. To appreciate this is a matter of simple arithmetic'. But efficiency notwithstanding, he expressed \"reluctance\" to use dogs in this way: \"One cannot calmly contemplate the murder of animals which possess such intelligence and individuality, which have frequently such endearing qualities, and which very possibly one has learnt to regard as friends and companions.\"\n\nAmundsen, by contrast, took an entirely utilitarian approach. Amundsen planned from the start to have weaker animals killed to feed the other animals and the men themselves. He expressed the opinion that it was less cruel to feed and work dogs correctly before shooting them, than it would be to starve and overwork them to the point of collapse. Amundsen and his team had similar affection for their dogs as those expressed above by the English, but they \"also had agreed to shrink from nothing in order to achieve our goal\". The British thought such a procedure was distasteful, though they were willing to eat their ponies.\n\nAmundsen had used the opportunity of learning from the Inuit while on his \"Gjøa\" North West passage expedition of 1905. He recruited experienced dog drivers. To make the most of the dogs he paced them and deliberately kept daily mileages shorter than he need have for 75 percent of the journey, and his team spent up to 16 hours a day resting. His dogs could eat seals and penguins hunted in the Antarctic while Scott's pony fodder had to be brought all the way from England in their ship. It has been later shown that seal meat with the blubber attached is the ideal food for a sledge dog. Amundsen went with 52 dogs, and came back with 11.\n\nWhat Scott did not realise is a sledge dog, if it is to do the same work as a man, will require the same amount of food. Furthermore, when sledge dogs are given insufficient food they become difficult to handle. The advantage of the sledge dog is its greater mobility. Not only were the Norwegians accustomed to skiing, which enabled them to keep up with their dogs, but they also understood how to feed them and not overwork them.\n\nScott took the Norwegian pilot and skier Tryggve Gran to the Antarctic on the recommendation of Nansen to train his expedition to ski, but although a few of his party began to learn, he made no arrangements for compulsory training for the full party. Gran (possibly because he was Norwegian) was not included in the South Pole party, which could have made a difference. Gran was, one year later, the first to locate the deceased Scott and his remaining companions in their tent just some 18 km (11 miles) short of One Ton depot, that might have saved their lives had they reached it.\n\nScott would subsequently complain in his diary, while well into his journey and therefore too late to take any corrective action and after over 10 years since the Discovery expedition, that \"Skis are the thing, and here are my tiresome fellow countrymen too prejudiced to have prepared themselves for the event\".\n\nAmundsen on his side recruited a team of well experienced skiers, all Norwegians who had skied from an early age. He also recruited a champion skier, Olav Bjaaland, as the front runner. The Amundsen party gained weight on their return travel from the South Pole.\n\nScott and Shackleton's experience in 1903 and 1907 gave them first-hand experience of average conditions in Antarctica. Simpson, Scott's meteorologist 1910–1912, charted the weather during their expedition, often taking two readings a day. On their return to the Ross Ice Shelf, Scott's group experienced prolonged low temperatures from 27 February until 10 March which have only been matched once in 15 years of current records. The exceptional severity of the weather meant they failed to make the daily distances they needed to get to the next depot. This was a serious position as they were short of fuel and food. When Scott, Wilson, and Bowers died (Petty Officer Edgar Evans and Lawrence Oates had died earlier during the return from the South Pole) they were short of One-Ton Depot, which was from Corner Camp, where they would have been safe.\n\nOn the other hand, Cherry-Garrard had travelled nearly in the same area, during the same time period and same temperatures, using a dog team. Scott also blamed \"a prolonged blizzard\". But while there is evidence to support the low temperatures, there is only evidence for a \"normal\" two- to four-day blizzard, and not the ten days that Scott claims.\n\nDuring depot laying in February 1911, Roald Amundsen had his first (and last) of his route marked like a Norwegian ski course using marker flags initially every eight miles. He added to this by using food containers painted black, resulting in a marker every mile. From 82 degrees on, Amundsen built a cairn every three miles with a note inside recording the cairn's position, the distance to the next depot, and direction to the next cairn. In order not to miss a depot considering the snow and great distances, Amundsen took precautions. Each depot laid out up to 85 degrees (laid out every degree of latitude) had a line of bamboo flags laid out transversely every half-mile for five miles on either side of the depot, ensuring that the returning party could locate the designated depot.\n\nScott relied on depots much less frequently laid out. For one distance where Amundsen laid seven depots, Scott laid only two. Routes were marked by the walls made at lunch and evening stops to protect the ponies. Depots had a single flag. As a result, Scott has much concern recorded in his diaries over route finding, and experienced close calls about finding depots. It is also clear that Scott's team did not travel on several days, because the swirling snow hid their three-month-old outward tracks. With better depot and route marking they would have been able to travel on more days with a following wind which would have filled the sail attached to their sledge, and so travel further, and might have reached safety.\n\nBy the time they arrived at the pole, the health of Scott's team had significantly deteriorated, whereas Amundsen's team actually gained weight during the expedition. While Scott's team managed to maintain the scheduled pace for most of the return leg, and hence was virtually always on full rations, their condition continued to worsen rapidly. (The only delay occurred when they were held for four days by a blizzard, and had to open their summit rations early as a consequence.)\n\nApsley Cherry-Garrard in his analysis of the expedition estimated that even under optimistic assumptions the summit rations contained only a little more than half the calories actually required for the man-hauling of sledges. A carefully planned 2006 re-enactment of both Amundsen's and Scott's travels, sponsored by the BBC, confirmed Cherry-Garrard's theory. The British team had to abort their tour due to the severe weight loss of all members. The experts hinted that Scott's reports of unusually bad surfaces and weather conditions might in part have been due to their exhausted state which made them feel the sledge weights and the chill more severely.\n\nScott's calculations for the supply requirements were based on a number of expeditions, both by members of his team (e.g., Wilson's trip with Cherry-Garrard and Bowers to the Emperor penguin colony which had each man on a different type of experimental ration), and by Shackleton. Apparently, Scott didn't take the strain of prolonged man-hauling at high altitudes sufficiently into account.\n\nSince the rations contained no B and C vitamins, the only source of these vitamins during the trek was from the slaughter of ponies or dogs. This made the men progressively malnourished, manifested most clearly in the form of scurvy.\n\nScott also had to fight with a shortage of fuel due to leakage from stored fuel cans which used leather washers. This was a phenomenon that had been noticed previously by other expeditions, but Scott took no measures to prevent it. Amundsen, in contrast, had learned the lesson and had his fuel cans soldered closed. A fuel depot he left on Betty's Knoll was found 50 years later still full.\n\nDehydration may also have been a factor. Amundsen's team had plenty of fuel due to better planning and soldered fuel cans. Scott had a shortage of fuel and was unable to melt as much water as Amundsen. At the same time Scott's team were more physically active in man-hauling the sledges.\n\nIt has been said (by the present-day explorer Ranulph Fiennes amongst others) that Scott's team was appropriately dressed for man-hauling in their woolen and wind-proof clothing, and as Amundsen was skiing it was appropriate he wore furs. Skiing at the pace of a dog team is a strenuous activity. Yet Amundsen never complained about the clothing being too hot. That is because the furs are worn loosely so air circulates and sweat evaporates. Scott's team, on the other hand, made regular complaints about the cold.\n\nAmundsen's team did initially have problems with their boots. However, the depot-laying trips of January and February 1911 and an abortive departure to the South Pole on 8 September 1911 allowed changes to be made before it was too late.\n\nScott's team suffered regularly from snow blindness and sometimes this affected over half the team at any one time. By contrast, there was no recorded case of snow blindness during the whole of Amundsen's expedition. On the return journey, Amundsen's team rested during the \"day\" (when the sun was in front of them) and travelled during the \"night\" (when the sun was behind them) to minimise the effects of snow blindness.\n\nIn 1921, 'Teddy' Evans wrote in his book \"South with Scott\" that Scott had left the following written orders at Cape Evans.\n\nHe did however place a lesser importance upon this journey than that of replenishing the food rations at One Ton Depot.\n\nHe continued his instructions in the next paragraph \"You will of course understand that whilst the object of your third journey is important, that of the second is vital. At all hazards three X.S. units of provision must be got to One Ton Camp by the date named (19th January), and if the dogs are unable to perform this task, a man party must be organised.\" with that qualification he closed his notes regarding his instructions for the dogs.\n\nExpedition member Apsley Cherry-Garrard did not mention Scott's order in his 1922 book \"The Worst Journey in the World\". However, in the 1948 preface to his book, he discusses Scott's order. Cherry-Garrard writes that he and Edward Atkinson reached Cape Evans on 28 January. Scott had estimated Atkinson would reach camp by 13 January. Atkinson, now the senior officer discovered that the dog handler Cecil Meares had resigned from the expedition and that neither Meares nor anyone else had resupplied dog food to the depots. Cherry-Garrard also wrote \"In my opinion he [Atkinson] would not have been fit to take out the dogs in the first week of February\".\n\nOn 13 February, Atkinson set off on the first lap southwards to Hut Point with the dog assistant, Dimitri Gerov, and the dogs to avoid being cut off by disintegrating sea ice. Atkinson and Gerov were still at Hut Point when, on 19 February, Tom Crean arrived on foot from the Barrier and reported that Lt Edward Evans was lying seriously ill in a tent some to the south, and in urgent need of rescue. Atkinson decided that this mission was his priority, and set out with the dogs to bring Evans back. This was achieved; the party was back at Hut Point on 22 February.\n\nAtkinson sent a note back to the Cape Evans base camp requesting either the meteorologist Wright or Cherry-Garrard to take over the task of meeting Scott with the dogs. Chief meteorologist Simpson was unwilling to release Wright from his scientific work, and Atkinson therefore selected Apsley Cherry-Garrard. It was still not in Atkinson's mind that Cherry-Garrard's was a relief mission, and according to Cherry-Garrard's account, told him to \"use his judgement\" as to what to do in the event of not meeting the polar party by One Ton, and that Scott's orders were that the dogs must not be risked. Cherry-Garrard left with Gerov and the dogs on 26 February, carrying extra rations for the polar party to be added to the depot and 24 days' of dog food. They arrived at One Ton Depot on 4 March and did not proceed further south. Instead, he and Gerov, after waiting there for Scott for several days, apparently mostly in blizzard conditions (although no blizzard was recorded by Scott some 100 miles further south until 10 March), they returned to Hut Point on 16 March, in poor physical condition and without news of the polar party.\n\nOn the return journey from the pole, Scott reached the 82.30°S meeting point for the dog teams three days ahead of schedule, around 27 February 1912. Scott's diary for that day notes \"We are naturally always discussing possibility of meeting dogs, where and when, etc. It is a critical position. We may find ourselves in safety at the next depot, but there is a horrid element of doubt.\" By 10 March it became clear that the dog teams were not coming: \"The dogs which would have been our salvation have evidently failed. Meares [the dog-driver] had a bad trip home I suppose. It's a miserable jumble.\"\n\nAround 25 March, awaiting death in his tent at latitude 79.30°S, Scott speculated, in a farewell letter to his expedition treasurer Sir Edgar Speyer, that he had overshot the meeting point with the dog relief teams, writing \"We very nearly came through, and it's a pity to have missed it, but lately I have felt that we have overshot our mark. No-one is to blame and I hope no attempt will be made to suggest that we had lacked support.\" (Farewell letter to Sir Edgar Speyer, cited from Karen May 2012.)\n\n"}
{"id": "506639", "url": "https://en.wikipedia.org/wiki?curid=506639", "title": "Compendium", "text": "Compendium\n\nA compendium (plural: compendia) is a concise compilation of a body of knowledge. A compendium may summarize a larger work. In most cases the body of knowledge will concern a specific field of human interest or endeavour (for example: hydrogeology, logology, ichthyology, phytosociology or myrmecology), while a general encyclopedia can be referred to as a \"compendium of all human knowledge\".\n\nThe word compendium arrives from the Latin word \"compenso\", meaning \"to weigh together or balance\". The 21st century has seen the rise of democratized, online compendia in various fields.\nAn example would be the \"Compendium of the Catechism of the Catholic Church\", a concise 598-question-and-answer book which summarises the teachings of the Catholic Faith and Morals.\n\nThe Bible is another example of a compendium—a group of many writings of the prophets and apostles over a period of time, whose books are put together to form the Old Testament and the New Testament.\n\nSome well known literary figures have written their own compendium. An example would be Alexandre Dumas, author of The Three Musketeers, and an enthusiastic gourmand. His compendium on food titled \"From Absinthe to Zest\" serves as an alphabet for food lovers.\n\nThe bestiary, popular in the Middle Ages, is another example of a compendium. Bestiaries cataloged animals and facts about natural history and were particularly popular in England and France around the 12th century.\n\n\n"}
{"id": "1902180", "url": "https://en.wikipedia.org/wiki?curid=1902180", "title": "Digital reference", "text": "Digital reference\n\nDigital reference (or virtual reference) is a service by which a library reference service is conducted online, and the reference transaction is a computer-mediated communication. It is the remote, NextNextcomputer-mediated delivery of reference information provided by library professionals to users who cannot access or do not want face-to-face communication. Virtual reference service is most often an extension of a library's existing reference service program. The word \"reference\" in this context refers to the task of providing assistance to library users in finding information, answering questions, and otherwise fulfilling users’ information needs. Reference work often but not always involves using reference works, such as dictionaries, encyclopedias, etc. This form of reference work expands reference services from the physical reference desk to a \"virtual\" reference desk where the patron could be writing from home, work or a variety of other locations.\n\nThe terminology surrounding virtual reference services may involve multiple terms used for the same definition. The preferred term for remotely delivered, computer-mediated reference services is \"virtual reference\", with the secondary non-preferred term \"digital reference\" having gone out of use in recent years. \"Chat reference\" is often used interchangeably with virtual reference, although it represents only one aspect of virtual reference. Virtual reference includes the use of both synchronous (i.e., IM, videoconferencing) and asynchronous communication (i.e., texting and email). Here, \"synchronous virtual reference\" refers to any real-time computer-mediated communication between patron and information professional. Asynchronous virtual reference is all computer-mediated communication that is sent and received at different times.\n\nThe earliest digital reference services were launched in the mid-1980s, primarily by academic and medical libraries, and provided by e-mail. These early-adopter libraries launched digital reference services for two main reasons: to extend the hours that questions could be submitted to the reference desk, and to explore the potential of campus-wide networks, which at that time was a new technology.\n\nWith the advent of the graphical World Wide Web, libraries quickly adopted webforms for question submission. Since then, the percentage of questions submitted to services via webforms has outstripped the percentage submitted via email.\n\nIn the early- to mid-1990s, digital reference services began to appear that were not affiliated with any library. These digital reference services are often referred to as \"AskA\" services. Examples of AskA services are the Internet Public Library, Ask Dr. Math, and Ask Joan of Art.\n\nProviding remote-based services for patrons has been a steady practice of libraries over the years. For example, before the widespread use of chat software, reference questions were often answered via phone, fax, email and audio conferencing. Email is the oldest type of virtual reference service used by libraries. Library services in America and the UK are just now gaining visibility in their use of virtual reference services using chat software. However, a survey in America revealed that by 2001 over 200 libraries were using chat reference services. \nThe rapid global proliferation of information technology (IT) often leaves libraries at a disadvantage in terms of keeping their services current. However, libraries are always striving to understand their user demographics in order to provide the best possible services. Therefore, libraries continue to take notes from current cyberculture and are continually incorporating a diversified range of interactive technologies in their service repertoires. Virtual reference represents only one small part of a larger library mission to meet the needs of a new generation, sometimes referred to as the \"Google Generation\", of users who have grown up with the internet. For instance, virtual reference may be used in conjunction with embedded Web 2.0 (online social media such as Facebook, YouTube, blogs, del.icio.us, Flickr, etc.) applications in a library's suite of online services. As technological innovations continue, libraries will be watching to find new, more personalized ways of interacting with remote reference users.\n\nThe range of cost-per-transaction of reference interactions has been found to be large, due to the differences in librarian salaries and infrastructural costs required by reference interviews.\n\nWebforms are created for digital reference services in order to help the patron be more productive in asking their question. This document helps the librarian locate exactly what the patron is asking for. Creation of webforms requires design consideration. Because webforms substitute for the reference interview, receiving as much information as possible from the patron is a key function.\n\nAspects commonly found within webforms:\n\n\nSeveral applications exist for providing chat-based reference. Some of these applications are: QuestionPoint, OmniReference, Tutor.com, LibraryH3lp, AspiringKidz.com, and Vienova.com. These applications bear a resemblance to commercial help desk applications. These applications possess functionality such as: chat, co-browsing of webpages, webpage and document pushing, customization of pre-scripted messages, storage of chat transcripts, and statistical reporting.\n\nInstant messaging (IM) services are used by some libraries as a low-cost means of offering chat-based reference, since most IM services are free. Utilizing IM for reference services allows a patron to contact the library from any location via the internet. This service is like the traditional reference interview because it is a live interaction between the patron and the librarian. On the other side the reference interview is different because the conversation does not float away but instead is in print on the screen for the librarian to review if needed to better understand the patron. IM reference services may be for the use of in-house patrons as well as patrons unable to go to the library. If library computers support IM chat programs, patrons may IM from within the library to avoid losing their use of a computer or avoid making embarrassing questions public.\n\nSuccessful IM reference services will:\n\nAt times, IM becomes challenging because of lack of non-verbal cues such as eye contact, and the perceived time pressure. Moreover, formulating the question online without the give and take of nonverbal cues and face to face conversation presents an added obstacle. In addition, to provide effective reference service through IM, it is important to meet higher level of information literacy standards. These standards include evaluating the information and its source, synthesizing the information to create new ideas or products, and understanding the societal, legal, and economic issues surrounding its use.\n\nThe article Live, Digital Reference Marketplace by Buff Hirko contains a comparison of the features of applications for chat-based reference.\n\nSee the entries in the Library Success Wiki's Online Reference Section, including software recommended for web-based chat reference, IM reference, SMS (text messaging) reference, and other types like digital audio or video reference.\n\nVirtual service software programs offered by libraries are often unique, and tailored to the individual library's needs. However, each program may have several distinct features. A knowledge base is a chunk of information that users can access independently. An example of this is a serialized listing of frequently asked questions (FAQ) that a user can read and use at his or her leisure.\n\nOnline chat, or instant messaging (IM) has become a very popular Web-based feature. Instant messaging is a real time conversation that utilizes typed text instead of language. Users may feel a sense of satisfaction with the use of this tool because of their personalized interaction with staff.\n\nThe use of electronic mail (email) in responding to reference questions in libraries has been in use for years. Also, in some cases with the IM feature, a question may be asked that cannot be resolved in online chat. In this instance the staff member may document the inquiring patron’s email address and will the user a response.\n\nWith the increase in use of text messaging (Short Message Service or SMS), some libraries are also adopting text messaging in their virtual reference services. Librarians can use mobile phones, text-to-instant messaging or web-based services to respond to reference questions via text messaging.\n\nCo-browsing, or cooperative browsing, is a virtual reference function that involves interactive control of a user’s web browser. This function enables the librarian to see what the patron has on his or her computer screen. Several types of co-browsing have been offered in mobile devices of late; libraries may have software that incorporates dual modes of co-browsing in a variety of formats. For instance, it is possible to browse on a mobile device within and between documents (such as Word), webpages, and images.\n\nVirtual reference services are growing in popularity in the UK with more institutions accepting queries via email, instant messaging and other chat based services. A study of the use of virtual reference within UK academic institutions showed that 25% currently offer a form of virtual reference, with 54% of academic institutions surveyed considering adding this service.\n\nUK public libraries were instrumental in some of the first steps towards UK-wide internet collaboration amongst libraries with the EARL Consortium (Electronic Access to Resources in Libraries) in 1995, in a time where internet access was a rare commodity for both library staff and the public. Resources were collated and lines of communication opened between libraries across the UK, paving the way for services all over the world to follow suit. There are now a number of area-specific reference services across the UK including Ask A Librarian (UK-wide, established in 1997), Ask Cymru (Welsh and English language service), Enquire (Government funded through the People's Network, also UK-wide), and Ask Scotland. Ask Scotland was created by the Scottish Government's advisory body on libraries, SLIC (Scottish Library and Information Council), and funded by the Public Library Quality Improvement Fund (PLQIF) in June 2009. It uses the Online Computer Library Center's QuestionPoint software.\n\nThe definition formulated by the American Library Association's (ALA) 2004 MARS Digital Reference Guidelines Ad Hoc Committee contains three components:\n\n\nIn January 2011 QuestionPoint and the American Library Association were in talks about offering a National Ask A Librarian service across the whole United States of America. At present the Ask services in the US are run at a local level.\n\nIn Europe some countries offer services in both their own national language and in English. European countries include: Finland, the Netherlands (in Dutch only), Denmark, and France.\n\nOther countries which offer virtual reference services include: Australia, New Zealand, Canada, and the state of Colorado in the United States.\n\nA collaboration between UK and Australian library services, entitled Chasing the Sun, has been initiated using QuestionPoint software so that an all-hours digital reference chat service can be offered. Targeted at health libraries where reference queries from health professionals could occur at any time of the day or night due to medical emergencies, the collaboration between the two countries means that someone will be on hand to field the query at any time. Although the UK libraries involved are currently based in England the programme may expand to other countries and health services if successful.\n\n\n\n\nThe following provide software and technology infrastructure for digital/virtual reference.\n\n\n\n\n\n"}
{"id": "1054566", "url": "https://en.wikipedia.org/wiki?curid=1054566", "title": "Ditloid", "text": "Ditloid\n\nA ditloid is a type of word puzzle, in which a phrase, quotation, date, or fact must be deduced from the numbers and abbreviated letters in the clue. Common words such as 'the', 'in', 'a', 'an', 'of', 'to', etc. are not normally abbreviated. The name 'ditloid' was given by the \"Daily Express\" newspaper, originating from the clue: 1 = DitLoID ≡ \"1 Day in the Life of Ivan Denisovich\".\n\nWill Shortz originated the current form of this puzzle and first published it in the May–June 1981 issue of \"Games\" magazine, calling it the Equation Analysis Test. In its annual 1981 issue of \"What's hot and what's not,\" \"Us\" magazine named the Equation Analysis Test in the \"what's hot\" category – the only nonperson so recognized. Shortz reports:\nSome anonymous person had retyped the puzzle from \"Games\" (word for word, except for my byline),\nphotocopied it, and passed it along. This page was then rephotocopied ad infinitum, like a chain letter,\nand circulated around the country. \"Games\" readers who hadn't seen the original even started sending\nit back to \"Games\" as something the magazine ought to consider publishing!\nShortz based the puzzle on the Formula Analysis Test - Revised Form published in Morgan Worthy's 1975 book \"AHA! A Puzzle Approach to Creative Thinking\" (Chicago: Nelson Hall). Worthy's equations were in a different format, for example:\n\nWorthy gives the source of his inspiration and speculates about the perennial popularity\nof this puzzle:\nI got the idea for linguistic equations from graffiti someone had\nwritten in the form of an obscene formula on a restroom wall at the\nUniversity of Florida. When the answer suddenly came to me, I realized\nthe format was a good one for eliciting the \"aha effect\". After that I\nused such items as exercise material when teaching workshops on\ncreative thinking.\nMy guess is that one reason a person enjoys linguistic equations is\nthat the answer hits him or her all at once rather than being solved in\nan incremental fashion. It is similar to what happens when we suddenly\nsee an embedded figure pop into focus; the satisfaction is visceral\nrather than just intellectual. My experience was that people often had\nthe answer to an item come to them when they were not consciously\nthinking about the puzzles, but relaxed, such as in the shower or about\nto fall asleep.\nAnother factor is that with well-written items, success does not hinge\non obscure information. Ideally, a person should never have to feel, \"I\ncould never have gotten that one no matter how long I worked on it.\"\nThere is something ego enhancing about knowing you have the answer\ninside and just need to find it.\n"}
{"id": "33487458", "url": "https://en.wikipedia.org/wiki?curid=33487458", "title": "Guide to information sources", "text": "Guide to information sources\n\nA Guide to information sources (or a bibliographic guide, a literature guide, a guide to reference materials, a subject gateway, etc.) is a kind of metabibliography. Ideally it is not just a listing of bibliographies, reference works and other information sources, but more like a textbook introducing users to the information sources in a given field (in general).\n\nSuch guides may have many different forms: Comprehensive or highly selective, printed or electronic sources, annoteted listings or written chapters etc.\n\nOften used as curriculum tools for bibliographic instruction, the guides help library users find materials or help those unfamiliar with a discipline understand the key sources.\n\nAby, Stephen H., Nalen, James & Fielding, Lori (2005). Sociology; a guide to reference and information sources. 3rd ed. Westport, Conn.: Libraries Unlimited.\n\nAdams, Stephen R. (2005). \"Information Sources in Patents\"; 2nd ed. (Guides to Information Sources). München: K. G. Saur \n\nBlewett, Daniel K (2008). American military history; a guide to reference and information sources. 2nd ed. Westport, CT : Libraries Unlimited.\n\nJacoby, JoAnn & Kibbee, Josephine Z. (2007). Cultural anthropology; a guide to reference and information sources. 2nd ed. Westport, Conn.: Libraries Unlimited.\n\nSchmidt, Diane & Bell, George H. (2003). Guide to reference and information sources in the zoological sciences. Westport, Conn. : Libraries Unlimited.\n\nO'Hare, Christine (2007). \"Business Information Sources\". London: Library Assn Pub Ltd\n\nOstwald, W (1919). Die chemische Literatur und die Organisation der Wissenschaft. Leipzig : W. Ostwald & C. Drucker. (This is considered the first \"guide to information sources\").\n\nStebbins, Leslie F. (2006). Student guide to research in the digital age; how to locate and evaluate information sources. Westport, Conn.: Libraries Unlimited.\n\nWebb, W. H. et al. (Ed.). (1986). Sources of information in the social sciences. A Guide to the literature. 3. ed. Chicago : American Library Association.\n\nZell, Hans M. (ed.). (2003). The African studies companion; a guide to information sources. 3rd rev. and expanded ed. Glais Bheinn : Hans Zell.\n\n\n"}
{"id": "57442907", "url": "https://en.wikipedia.org/wiki?curid=57442907", "title": "Handbook of Middle American Indians", "text": "Handbook of Middle American Indians\n\nHandbook of Middle American Indians (HMAI) is a sixteen-volume compendium on Mesoamerica , from the prehispanic to the late twentieth century. Volumes on particular topics were published from the 1960s and 1970s under the general editorship of Robert Wauchope. Separate volumes with particular volume editors deal with a number of general topics, including archeology, cultural anthropology, physical anthropology, linguistics, with the last four substantive volumes treating various topics in Mesoamerican ethnohistory, under the editorship of Howard F. Cline. Select volumes have become available in e-book format.\n\nA retrospective review of the HMAI by two anthropologists discusses its history and evaluates it. One review calls it a fundamental work. Another reviewer says \"since the first volume of the HMAI appeared in 1964 is far and away the most comprehensive and erudite coverage of native cultures of any region in the Americas.\" A review in the journal \"Science\" says that \"There can be little doubt that, like the \"Handbook of South American Indians\", this monumental synthesis will provide a sound basis for new generalizations and will stimulate additional research to fill the gaps in knowledge and understanding that will become apparent.\n\nStarting in 1981, six volumes in the Supplement to the Handbook of Middle American Indians were published under the general editorship of Victoria Bricker.\n\nVolume 1. Natural Environment and Early Cultures, Robert C. West, volume editor. 1. Geohistory and Paleogeography of Middle America (Manuel Maldonado-Koerdell); 2. Surface Configuration and Associated Geology of Middle America (Robert C. West); 3. The Hydrography of Middle America (Jorge L. Tamayo, in collaboration with Robert C. West); 4. The American Mediterranean (Albert Collier); 5. Oceanography and Marine Life along the Pacific Coast (Carl L. Hubbs and Gunnar I. Roden); 6. Weather and Climate of Mexico and Central America (Jorge A. Vivo Escoto); 7. Natural Vegetation of Middle America (Philip L. Wagner); 8. The Soils of Middle America and their Relation to Indian Peoples and Cultures (Rayfred L. Stevens); 9. Fauna of Middle America (L. C. Stuart); 10. The Natural Regions of Middle America (Robert C. West); 11. The Primitive Hunters (Luis Aveleyra Arroyo de Anda); 12. The Food-gathering and Incipient Agriculture Stage of Prehistoric Middle America (Richard S. MacNeish); 13. Origins of Agriculture in Middle America (Paul C. Mangelsdorf, Richard S. MacNeish, and Gordon R. Willey); 14. The Patterns of Farming Life and Civilization (Gordon R. Willey, Gordon F. Ekholm, and Rene F. Millon)\n\nVolumes 2-3. Archeology of Southern Mesoamerica, Gordon R. Wiley, volume editor.\n\nVolume 4. ‘’Archeological Frontiers and External Connections G.F. Ekholm and G. R. Wiley, volume editors.\n\nVolume 5. ‘’Linguistics, Norman A. McQuown, volume editor.\n\nVolume 6. Social Anthropology, Manning Nash, volume editor. 1.Introduction, Manning Nash; 2. Indian Population and its Identification, Anselmo Marino Flores; 3.Agricultural Systems and Food Patterns, Angel Palerm; 4. Settlement Patterns, William T. Sanders; 5. Indian Economies, Manning Nash; 6. Contemporary Pottery and Basketry, George M. Foster; 7. Laquer, Katharine D. Jenkins; 8. Textiles and Costume, A.H. Gayton; 9. Drama, Dance and Music, Gertrude Prokosch Kurath; 10. Play: Games, Gossip, and Humor; 11. Kinship and Family, A. Kimball Romney; 12. Compadrinazgo, Robert Ravicz; 13. Local and Territoria Units, Eva Hunt and June Nash; 14. Political and Religious Organizations, Frank Cancian; 15. Levels of Communal Relations, Eric R. Wolf; 16. Annual Cycle and Fiesta Cycle, Ruben E. Reina; 17. Sickness and Social Relations, Richard N. Adams and Arthur J. Rubel; 18. Narrative Folklore, Munro S. Edmonson; 19. Religious Syncretism, William Madsen; 20. Ritual and Mythology, E. Michael Mendelson; 21. Psychological Orientations, Benjamin N. Colby; 22. Ethnic Relationships, Julio de la Fuente; 23. Acculturation, Ralph L. Beals; 24. Nationalization, Richard N. Adams; 25. Directed Change, Robert H. Ewald; 26. Urbanization and Industrialization, Arden R. King\n\nVolumes 7-8, Ethnology, Evan Z. Vogt, volume editor. Volume 7. Introduction (Evon Z. Vogt)Section I: The Maya 2; The Maya: Introduction (Evon Z. Vogt); 3. Guatemalan Highlands (Manning Nash); 4. The Maya of Northwestern Guatemala (Charles Wagley); 5. The Maya of the Midwestern Highlands (Sol Tax and Robert Hinshaw); 6. Eastern Guatemalan Highlands: The Pokomames and Chorti (Ruben E. Reina); 7. Chiapas Highlands (Evon Z. Vogt); 8. The Tzotzil (Robert M. Laughlin); 9. The Tzeltal (Alfonso Villa Rojas); 10. The Tojolabal (Roberta Montagu); 11. Maya Lowlands: The Chontal, Chol, and Kekchi (Alfonso Villa Rojas); 12. The Maya of Yucatan (Alfonso Villa Rojas); 13. The Lacandon (Gertrude Duby and Frans Blom); 14. The Huastec (Robert M. Laughlin); Section II: Southern Mexican Highlands and Adjacent Coastal Regions15. Southern Mexican Highlands and Adjacent Coastal Regions: Introduction (Ralph L. Reals); 16. The Zapotec of Oaxaca (Laura Nader); 17. The Chatino (Gabriel DeCicco); 18. The Mixtec (Robert Ravicz and A. Kimball Romney); 19. The Trique of Oaxaca (Laura Nader);20. The Amuzgo (Robert Ravicz and A. Kimball Romney); 21. The Cuicatec (Roberto J. Weitlaner); 22. The Mixe, Zoque, and Popoluca (George M. Foster); 23. The Huave (A. Richard Diebold, Jr.); 24. The Popoloca (Walter A. Hoppe, Andres Medina, and Roberto J. Weitlaner); 25. The Ichcatec (Walter A. Hoppe and Roberto J. Weitlaner); 26. The Chocho (Walter A. Hoppe and Roberto J. Weitlaner); 27. The Mazatec (Roberto J. Weitlaner and Walter A. Hoppe); 28. The Chinantec (Roberto J. Weitlaner and Howard F. Cline); 29. The Tequistlatec and Tlapanec (D. L. Olmsted); 30. The Cuitlatec (Susana Drucker, Roberto Escalante, and Roberto J. Weitlaner); Volume 8, Section III: Central Mexican Highlands; 31. Central Mexican Highlands: Introduction (Pedro Carrasco); 32. The Nahua (William Madsen); 33. The Totonac (H. R. Harvey and Isabel Kelly); 34. The Otomi (Leonardo Manrique C.); Section IV: Western Mexico 35. The Tarascans (Ralph L. Beals); Section V: Northwest Mexico; 36. Northwest Mexico: Introduction (Edward H. Spicer); 37. The Huichol and Cora (Joseph E. Grimes and Thomas B. Hinton); 38. The Southern Tepehuan and Tepecano (Carroll L. Riley); 39. The Northern Tepehuan (Elman R. Service); 40. The Yaqui and Mayo (Edward H. Spicer); 41. The Tarahumara (Jacob Fried); 42. Contemporary Ethnography of Baja California, Mexico (Roger C. Owen); 43. Remnant Tribes of Sonora: Opata, Pima, Papago, and Seri (Thomas B. Hinton).\n\nVolumes 6 & 7 were reviewed when the appeared. One reviewer highlights several articles, including those by Eric R. Wolf, Angel Palerm, and Willilam Sanders, but he goes on to say \"These volumes are ... more valuable for reference than for reading. Sections dealing with distribution, history, and bibliography are very useful, but sections dealing with social structure or the character of the peoples generally fail to provide integrated analyses indicating the essential features.\"\n\nVolume 9. Physical Anthropology, T.D. Stewart, volume editor.\n\nVolume 10-11. Archeology of Northern Mesoamerica, G. F. Ekholm and Ignacio Bernal, volume editors.\n\nVolumes 12-15, Guide to Ethnohistorical Sources, Howard F. Cline, Volume editor.\n\nVolume 12, Guide to Ethnohistorical Sources, Part 1. (1972) 1.“Introductory Notes on Territorial Divisions of Middle America” , Howard F. Cline, pp. 17–62; 2. “Colonial New Spain, 1519-1786: Historical Notes on the Evolution of Minor Political Jurisdictions”, Peter Gerhard, pp. 63–137; 3. “Viceroyalty to Republics, 1786-1952: Historical Notes on the Evolution of Middle American Political Units,” Howard F. Cline, pp. 138–165; 4.“Ethnohistorical Regions of Middle America,” Howard F. Cline, pp. 166–182; 5.“The \"Relaciones Geográficas\" of the Spanish Indies, 1577-1648,” Howard F. Cline, pp. 183–242; 6.“The Pinturas (Maps) of the Relaciones Geográficas, with Catalogue,” Donald Robertson, pp. 243–278; 7.“The Relaciones Geográficas, 1579-1586: Native Languages,” H.R. Harvey, pp. 279–323; 8.“A Census of the Relaciones Geográficas of New Spain, 1579-1612,” Howard F. Cline, pp. 324–369; 9.“The Relaciones Geográficas of Spain, New Spain, and the Spanish Indies: An Annotated Bibliography,” Howard F. Cline, pp. 370–395; 10.“The Relaciones Geográficas of Mexico and Central America, 1740-1792,” Robert C. West, pp. 396–452.\nVolume 13. Guide to Ethnohistorical Sources, Part 2. (1973) 11, “Published Collections of Documents Relating to Middle American Ethnohistory”, Charles Gibson; 12, “An Introductory Survey of Secular Writings in the European Tradition on Colonial Middle America, 1503-1818,” J. Benedict Warren, pp. 42–137; 13. “Religious Chronicles and Historians: A Summary and Annotated Bibliography,” Ernest J. Burrus, S.J.; 14. “Bernardino de Sahagún, 1499-1590A. “Sahagún and His Works,” Nicolau d’Olwer and Howard F. Cline, 186-206; B. “Sahagún’s “Primeros Memoriales.” Tepepulco, H. B. Nicholson, pp. 207–217; C. “Sahagún’s Materials and Studies,” Howard F. Cline, pp. 218–239; 15. “Antonio de Herrera, 1549-1625,” Manuel Ballesteros Gaibrois, pp. 240–255; 16. “Juan de Torquemada, 1564-1624,” José Alcina Franch, pp. 256–275; 17. “Francisco Javier Clavigero, 1731-1787, “ Charles F. Ronan, S. J., pp. 276–297; 18. “Charles Etienne Brasseur de Bourbourg, 1814-1874,” Carroll Edward Mace, pp. 298–325; 19. “Hubert Howe Bancroft, 1832-1918,” Howard F. Cline, pp. 326–347; 20. “Eduard Georg Seler, 1849-1922,” H. B. Nicholson, pp. 348–369; 21, “Select Nineteenth-Century Mexican Writers on Ethnohistory,” Howard F. Cline, pp. 370–403. Carlos María de Bustamante, José Fernando Ramírez, Manuel Orozco y Berra, Joaquín García Icazbalceta, Alfredo Chavero, Francisco del Paso y Troncoso\n\nVolume 14. Guide to Ethnohistorical Sources Part 3. (1975) 22. “A Survey of Native Middle American Pictorial Manuscripts,” John B. Glass, pp. 3–80; 23. “A Census of Native Middle American Pictorial Manuscripts,” John B. Glass with Donald Robertson, pp. 81–252; 24. “Techialoyan Manuscripts and Paintings with a Catalog,” Donald Robertson, pp. 253–280; 25. “A Census of Middle American Testerian Manuscripts,” John B. Glass, pp. 281–296; 26. “A Catalogue of Falsified Middle American Pictorial Manuscripts,” John B. Glass, pp. 297–309; Illustrations and maps, 1-103\n\nVolume 15. Guide to Ethnohistorical Sources Part 4. (1975) 27A. “Prose Sources in the Native Historical Tradition,” Charles Gibson, pp 312–319; 27B. “A Census of Middle American Prose Manuscripts in the Native Historical Tradition,” Charles Gibson and John B. Glass, pp. 322–400; 28. “A Checklist of Institutional Holdings of Middle American Manuscripts in the Native Historical Tradition,” John B. Glass, pp. 401–472; 29. “The Boturini Collection,” John B. Glass, pp. 473–486; 30. “Middle American Ethnohistory: An Overview,” H. B. Nicholson, pp. 487–505; 31.”Index of Authors, Titles, and Synonyms,” John B. Glass, pp. 506–536; 32. “Annotated References,” John B. Glass, pp. 537–724.\nVolume 16. Handbook of Middle American Indians. Margaret A.L. Harrison, volume editor. (1976) – Bibliography for all volumes.\n\nGeneral Editor, Victoria Bricker\n\n"}
{"id": "20819040", "url": "https://en.wikipedia.org/wiki?curid=20819040", "title": "Hashtag", "text": "Hashtag\n\nA hashtag is a type of metadata tag used on social networks such as Twitter and other microblogging services, allowing users to apply dynamic, user-generated tagging which makes it possible for others to easily find messages with a specific theme or content. Users create and use hashtags by placing the number sign or pound sign codice_1 usually in front of a word or unspaced phrase in a message. The hashtag may contain letters, digits, and underscores. Searching for that hashtag will yield each message that has been tagged with it. A hashtag archive is consequently collected into a single stream under the same hashtag. For example, on the photo-sharing service Instagram, the hashtag \"#bluesky\" allows users to find all the posts that have been tagged using that hashtag. \n\nThe use of hashtags was first proposed by Chris Messina in a 2007 tweet that, although initially decried by Twitter as a \"thing for nerds\", eventually led to their use spreading like wild-fire through the platform. Messina, who made no attempt to copyright the use because he felt \"they were born of the internet, and owned by no one\", has subsequently been credited as the godfather of the hashtag. By the end of the decade hashtags could be seen in most emerging as well as established social media platforms including Instagram, Facebook, Reddit, and YouTube. So much so that Instagram had to officially place a \"30 hashtags\" limit on its posts to prevent people from abusing their use, a limit which Instagrammers eventually circumvented by posting hashtags in the comments section of their posts. As of 2018 more than 85% of the top 50 websites by traffic on the Internet use hashtags and their use is highly common with millennials, Gen Z, politicians, influencers, and celebrities worldwide. Because of its widespread use, \"hashtag\" was added to the \"Oxford English Dictionary\" in June 2014. The term \"hashtag\" is also sometimes erroneously used to refer to the hash symbol itself when used in the context of a hashtag. Formal taxonomies can be developed from the folk taxonomy rendered machine-readable by the markup that hashtags provide; this process is called folksonomy.\nThe US pound sign, number sign or hash symbol \"#\" is often used in information technology to highlight a special meaning. (\"Pound sign\" in the UK means \"£\"; \"#\" is called hash, gate, and occasionally octothorpe.) In 1970, for example, the number sign was used to denote \"immediate\" address mode in the assembly language of the PDP-11 when placed next to a symbol or a number. In 1978, Brian Kernighan and Dennis Ritchie used \"#\" in the C programming language for special keywords that had to be processed first by the C preprocessor. In the 1986 SGML standard, ISO 8879:1986 (q.v.), # is a reserved name indicator (rni) which precedes keyword syntactic literals, --e.g., the primitive content token #PCDATA, used for parsed character data.\n\nThe International Telecommunication Union approved in November 1988 recommendation E.161 that put the hash sign on the right side of the 0 in the 4 x 3 button arrangement for push buttons on telephones. This same arrangement is still used today in most software phones (see Android dialer for example). The ITU recommendation had 2 design options for the hash: a European version where the hash sign was built with a 90-degree angle and a North-American version with an 80-degree angle. The North-American version seems to have prevailed as most hash signs in Europe now follow the 80-degree inclination.\n\nThe pound sign was adopted for use within IRC networks circa 1988 to label groups and topics. Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an ampersand '&').\n\nThe use of the pound sign in IRC inspired Chris Messina to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network. He posted the first hashtag on Twitter:\n\nMessina’s suggestion to use the hashtag was not adopted by Twitter, but the practice took off after hashtags were widely used in tweets relating to the 2007 San Diego forest fires in Southern California.\n\nAccording to Messina, he suggested use of the hashtag to make it easy for \"lay\" users to search for content and find specific relevant updates; they were for people who do not have the technological knowledge to navigate the site. Therefore, the hashtag \"was created organically by Twitter users as a way to categorize messages.\" Today they are for anyone, either with or without technical knowledge, to easily impose enough annotation to be useful without needing a more formal system or adhering to many technical details.\n\nInternationally, the hashtag became a practice of writing style for Twitter posts during the 2009–2010 Iranian election protests; Twitter users inside and outside Iran used both English- and Persian-language hashtags in communications during the events.\n\nThe first published use of the term \"hash tag\" was in a blog post by Stowe Boyd, \"Hash Tags = Twitter Groupings,\" on August 26, 2007, according to lexicographer Ben Zimmer, chair of the American Dialect Society's New Words Committee.\n\nBeginning July 2, 2009, Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced \"Trending Topics\" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to spam the trending list and ensure that hashtags trend naturally.\n\nAlthough the hashtag started out most popularly on Twitter as the main social media platform for this use, the use has extended to other social media sites including Instagram, Facebook, Flickr, Tumblr, and Google+.\n\nA hashtag must begin with a hash character followed by other characters, and is terminated by a space, or end of message. It is always safe to precede the “#” with a space, and to include letters without diacritics, digits, and underscores. In many cases other characters are also allowed, in particular accented characters used in many languages, but handling may vary from one client to another, and from time to time as standards evolve. A discussion of hashtag standards suggests that if #Romeo&Juliet is used, different Twitter clients might link to #Romeo, #Romeo&, or #Romeo&Juliet. Hashtags are not case sensitive; a search for “#hashtag” will find “#HashTag”. The use of embedded capitals (CamelCase) increases readability and avoids confusion; a (real) pen shop would be advised to use #PenIsland rather than all lower-case. On microblogging and social networking sites hashtags can be inserted anywhere within a text, often at the beginning or the end, but also within the text, usually as a word (e.g. “It is #sunny today”).\n\nLanguages which do not use letters are handled slightly differently. In China, microblogs Sina Weibo and Tencent Weibo use a double-hashtag-delimited #HashName# format, since the lack of spacing between Chinese characters necessitates a closing tag. Twitter uses a different syntax for Chinese characters and orthographies with similar spacing conventions: the hashtag contains unspaced characters, separated from preceding and following text by spaces (e.g. '我 #爱 你' instead of '我#爱你') or by zero-width non-joiner characters before and after the hashtagged element, to retain a linguistically natural appearance (displaying as unspaced '我‌#爱‌你', but with invisible non-joiners delimiting the hashtag).\n\nIt is considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the \"absolute maximum\", and any contribution exceeding this risks \"raising the ire of the community.\"\n\nAs well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or suspended.\n\nJimmy Fallon and Justin Timberlake performed a sketch parodying the often incorrect and misunderstood use of hashtags on \"Late Night with Jimmy Fallon\" in September 2013.\n\nHashtags are mostly used in unmoderated, ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can \"trend\" and attract more individual users to discussion. On Twitter, when a hashtag becomes extremely popular, it will appear in the \"Trending Topics\" area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users. They cannot be \"retired\" from public usage, meaning that any given hashtag can theoretically be used in perpetuity. They do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes, as chosen by the creators of them.\n\nHashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using #cakefestival rather than simply #cake. However, this can also make it difficult for topics to become \"trending topics\" because people often use different spelling or words to refer to the same topic. For topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.\n\nHashtags also function as beacons in order for users to find and \"follow\" (subscribe) or \"list\" (organize into public contact lists) other users of similar interest.\n\nTelevision broadcasters such as Channel 4 have employed the hashtag during the transmission of programmes such as First Dates and The Undateables. Research has shown that audience numbers go up when individuals can be interactive by tweeting while viewing a programme.\n\nHashtags can be used on the social network Instagram, by posting a picture and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic, such as #photography #iPhone #iphoneography, and therefore do not fulfill a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use. The ban against certain hashtags has a consequential role in the way that particular subaltern communities are built and maintained on Instagram. Despite Instagram's content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.\n\nFamous Youtube bloggers often use hashtags to promote their videos to a wide audience. Thus, by leaving various hashtags under the video, they are trying to increase their views and gain as many likes as possible. Usually, hashtags are left under the video itself in a special line. By clicking on the hashtag you go directly to the link to the video, which are similar in topic.\n\nHashtags are also used informally to express context around a given message, with no intent to categorize the message for later searching, sharing, or other reasons. One of the functions of the hashtag is to serve as a reflexive meta-commentary, which contributes to the idea of how written communication in new media can be paralleled to how pragmatic methodology is applied to speech.\n\nThis can help express contextual cues or offer more depth to the information or message that appears with the hashtag. \"My arms are getting darker by the minute. #toomuchfaketan\". Another function of the hashtag can be used to express personal feelings and emotions. For example, with \"It's Monday!! #excited #sarcasm\" in which the adjectives are directly indicating the emotions of the speaker. It can also be used as a disclaimer of the information that the hashtag accompanies, as in, \"BREAKING: US GDP growth is back! #kidding\". In this case, the hashtag provides an essential piece of information in which the meaning of the utterance is changed entirely by the disclaimer hashtag. This may also be conveyed with #sarcasm, as in the previous example. Self-mockery is another informal function of the hashtag used by writers, as in this tweet: \"Feeling great about myself till I met an old friend who now races at the Master's level. Yup, there's today's #lessoninhumility,\" where the informality of the hashtag provides commentary on the tweet itself.\n\nThe feature has been added to other, non-short-message-oriented services, such as the user comment systems on YouTube and Gawker Media. In the case of the latter, hashtags for blog comments and directly submitted comments were used to maintain a more constant rate of user activity even when paid employees were not logged into the website. Real-time search aggregators such as the former Google Real-Time Search also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the \"river\" of Twitter posts that can result from search terms or hashtags.\n\nThe use of hashtags has extended to televisiona concept that began rising in prominence in the early 2010s. Broadcasters may display a hashtag as an on-screen bug, encouraging viewers to participate in a backchannel of discussion via social media prior to, during, or after the program. Television commercials have sometimes contained hashtags for similar purposes. Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement.\n\nWhile personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames to receive mentions and replies to posts, usage of related or \"branded\" hashtags alongside Twitter usernames (e.g., #edshow as well as @edshow) is increasingly encouraged as a microblogging style to \"trend\" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. #SunnyFX) and instantaneous, \"temporary\" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts. Some have speculated that hashtags might take the place of (or co-exist with) the Nielsen television ratings system.\n\nAn example of trending \"temporary\" hashtags garnering viewers during broadcasts is observed on \"The Tonight Show\" with Jimmy Fallon, a variety talk show on NBC. Every Wednesday, Fallon hosts a segment on his show called \"Tonight Show Hashtags,\" which engages viewers by inviting them via Twitter to post humorous stories based on a specific hashtag topic, such as #WhydidIsaythat, #Worstfirstdate, to #Onetimeinclass, reflecting on funny experiences in daily life. By using hashtags, Fallon creates a sense of community and solidarity among his viewers and draws a wider range of viewers through an online platform while they watch a classic, non-interactive television program. Because of its popularity, the \"Tonight Show Hashtags\" are usually the 'most tweeted hashtag' on Twitter, which promotes the show. By engaging viewers with a lighthearted subject and simple hashtags, Fallon can gauge topical responses from viewers during broadcasts and also use the hashtags to brand his show.\n\nThe increased usage of hashtags as brand promotion devices has been compared to the promotion of branded \"keywords\" by AOL in the late 1990s and early 2000s, as such keywords were also promoted at the end of television commercials and series episodes.\n\nThe late-night television comedy game show @midnight with Chris Hardwick on Comedy Central features a daily game entitled \"Hashtag Wars,\" in which three comedians compete against one another to come up with phrases based on a given hashtag theme.\n\nSome hashtags have become famous worldwide. For instance the slogan \"Je suis Charlie,\" which was first used on Twitter as the hashtag #jesuischarlie and #iamcharlie to indicate solidarity with \"Charlie Hebdo\" offices attacked in Paris, spread to the internet at large.\n\nSince February 2013 Twitter and American Express have collaborated to enable users to pay for discounted goods online by tweeting a special hashtag. American Express members can sync their card with Twitter and pay for offers by tweeting; American Express tweets a response to the member that confirms the purchase.\n\nOrganized real-world events have used hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants to find each other, both on Twitter and, in many cases, during actual physical events.\n\nCompanies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.\n\nPolitical protests and campaigns in the early 2010s, such as #OccupyWallStreet and #LibyaFeb17, have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion. Hashtags have also been used to promote official events; the Finnish Ministry of Foreign Affairs officially titled the 2018 Russia–United States summit as the \"#HELSINKI2018 Meeting\".\n\nHashtags are often used by consumers on social media platforms to complain about the customer service experience with large companies. The term \"bashtag\" has been created to describe situations in which a user refers to a corporate social media hashtag to criticise the company or to tell others about poor customer service. For example, in January 2012, McDonald's created the #McDStories hashtag so that customers could share positive experiences about the restaurant chain. But, the marketing effort was cancelled after two hours when McDonald's received numerous complaint tweets rather than the positive stories they were anticipating.\n\nThe use of hashtags also reveals what feelings or sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is sarcastic—a difficult AI problem.\n\nThe YouTuber Spencer FC used the hashtag for the name and crest of his YouTube-based association football team, Hashtag United F.C..\n\nSince the 2012–13 season, the National Basketball Association (NBA) has allowed fans to vote players in as All-Star Game starters on Twitter and Facebook using #NBAVOTE. The tweets and Facebook posts must include #NBAVOTE along with the player's first and last name or Twitter handle.\n\nDuring the April 2011 Canadian party leader debate, Jack Layton, then-leader of the New Democratic Party, referred to Conservative Prime Minister Stephen Harper's crime policies as \"a hashtag fail\" (presumably #fail).\n\nThe term \"hashtag rap\", coined by Kanye West, was developed in the 2010s to describe a style of rapping which, according to Rizoh of the \"Houston Press,\" uses \"three main ingredients: a metaphor, a pause, and a one-word punch line, often placed at the end of a rhyme\". Rappers Nicki Minaj, Big Sean, Drake, and Lil Wayne are credited with the popularization of hashtag rap, while the style has been criticized by Ludacris, The Lonely Island, and various music writers.\n\nOn September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a \"New York Times\" front-page article regarding Twitter's initial public offering.\n\nBird's Eye foods released in 2014 a shaped mashed potato food that included forms of @-symbols and hashtags, called \"Mashtags\".\n\nHashtags have been used verbally to make a humorous point in informal conversations, such as \"I’m hashtag confused!\" In August 2012, British journalist Tom Meltzer reported in \"The Guardian\" about a new hand gesture that mimicked the hashtag, sometimes called the \"finger hashtag\", in which both hands form a peace sign, and then the fingers are crossed to form the symbol of a hashtag. The emerging gesture was reported about in \"Wired\" by Nimrod Kamer, and during 2013, it was seen on TV as used by Jimmy Fallon, and on \"The Colbert Report,\" among other programs. Writing in 2015, Paola Maria Caleff considered this usage a fad, but noted that people talking the way that they write was a consequence of computer-mediated communication.\n\n\n\n"}
{"id": "190975", "url": "https://en.wikipedia.org/wiki?curid=190975", "title": "Ibid.", "text": "Ibid.\n\nIbid is an abbreviation for the Latin word \"ibīdem\", meaning \"in the same place\", commonly used in an endnote, footnote, bibliography citation, or scholarly reference to refer to the source cited in the preceding note or list item. This is similar to \"īdem\", literally meaning \"the same\", abbreviated \"Id.\", which is commonly used in legal citation.\n\nIbid. may also be used in the Harvard (name-date) system for in-text references where there has been a close previous citation from the same source material. The previous reference should be immediately visible, e.g. within the same paragraph or page. Some academic publishers now prefer that \"ibid.\" not be italicized, as it is a commonly found term.\n\nSince ibid. is an abbreviation where the last two letters of the word are omitted, it takes a full stop (period) in both British and American usage.\n\nReference 2 is the same as reference 1: E. Vijh, \"Latin for Dummies\" on page 23, whereas reference 3 refers to the same work but at a different location, namely page 29. Intervening entries require a reference to the original citation in the form Ibid. <citation #>, as in reference 5.\n\n\n\n"}
{"id": "161388", "url": "https://en.wikipedia.org/wiki?curid=161388", "title": "Indirect self-reference", "text": "Indirect self-reference\n\nIndirect self-reference describes an object referring to itself \"indirectly\".\n\nFor example, define the function f such that f(x) = x(x). Any function passed as an argument to f is invoked with itself as an argument, and thus in any use of that argument is indirectly referring to itself.\n\nThis example is similar to the Scheme expression \"((lambda(x)(x x)) (lambda(x)(x x)))\", which is expanded to itself by beta reduction, and so its evaluation loops indefinitely despite the lack of explicit looping constructs. An equivalent example can be formulated in lambda calculus.\n\nIndirect self-reference is special in that its self-referential quality is not explicit, as it is in the sentence \"this sentence is false.\" The phrase \"this sentence\" refers directly to the sentence as a whole. An indirectly self-referential sentence would replace the phrase \"this sentence\" with an expression that effectively still referred to the sentence, but did not use the pronoun \"this.\"\n\nAn example will help to explain this. Suppose we define the quine of a phrase to be the quotation of the phrase followed by the phrase itself. So, the quine of:\nwould be:\nwhich, incidentally, is a true statement.\n\nNow consider the sentence:\n\nThe quotation here, plus the phrase \"when quined,\" indirectly refers to the entire sentence. The importance of this fact is that the remainder of the sentence, the phrase \"makes quite a statement,\" can now make a statement about the sentence as a whole. If we had used a pronoun for this, we could have written something like \"this sentence makes quite a statement.\"\n\nIt seems silly to go through this trouble when pronouns will suffice (and when they make more sense to the casual reader), but in systems of mathematical logic, there is generally no analog of the pronoun. It is somewhat surprising, in fact, that self-reference can be achieved at all in these systems.\n\nUpon closer inspection, it can be seen that in fact, the Scheme example above uses a quine, and f(x) is actually the quine function itself.\n\nIndirect self-reference was studied in great depth by W. V. Quine (after whom the operation above is named), and occupies a central place in the proof of Gödel's incompleteness theorem. Among the paradoxical statements developed by Quine is the following:\n\n"}
{"id": "15293025", "url": "https://en.wikipedia.org/wiki?curid=15293025", "title": "Informationsdienst Wissenschaft", "text": "Informationsdienst Wissenschaft\n\nInformationsdienst Wissenschaft e.V. or idw (The Science Information Service) operates an Internet platform, which bundles the press reports and dates of important events from about 1,000 scientific institutions, including universities, technical colleges, governmental and non-governmental research institutes and institutes to support research or scientific administration. idw (a registered charitable society) also operates an expert broker, the idw expert finder, which is exclusively for journalists. This makes idw one of the most comprehensive sources of science news in the German-speaking area. Foreign journalists and institutions (mostly European) now use idw as well. \n\nThe two main objectives of idw are:\n\nThe information in idw can be accessed free of charge - either directly on idw’s www pages, or by using an individually configurable RSS feed or as an e-mail subscriber. Any user can request the information covering the topics and regions which interest him. All idw services can be used free of cost - the current news ticker, the science calendar, research in the archive (which contains more than 350,000 press releases), and the list of institutions linked to idw. idw also provides journalists with instruments for contacting experts, and maintains a database with science photos.\nThe members' press offices have various possibilities of communicating with journalists. Membership is only offered to German or foreign institutions which perform research or teaching, or which support science or are active in science in some other way.\n\nThe original idea of idw was to provide experts for journalists. Using the American ProfNet as example, the press officers of Universitaet Bayreuth, the Ruhr University Bochum and the Clausthal University of Technology, in collaboration with Computing Centre of Clausthal University of Technology/TU Clausthal, developed a concept for a German language network, by means of the new media. The concept was technically implemented by the staff of the Computing Centre of the Clausthal University of Technology. A total of nine staff members in Bayreuth, Bochum and Clausthal are responsible for programming, maintaining and developing the idw operating system, for user services and further development of the content.\n\nThe initial phase (1996–1999) was guaranteed by project support from the Federal Ministry for Education and Research (BMBF). The technical development of the idw was supported by the Ministry, together with the Stifterverband fuer die Deutsche Wissenschaft (Donor Association for German Science). idw has been working closely for years with the initiative Wissenschaft im Dialog (Science in Dialogue). idw has been economically independent since 2000 and is financed by contributions from member institutions. It has been organised as a registered charitable society (gemeinnütziger e. V.) since 2002.\n\nidw has developed as a recognised and accepted source for German language science and for science journalism. It has become an instrument for public relations work for scientific institutions. \nAbout 37,000 subscribers (figure for June 2018) receive regular reports from idw, including some 7,900 journalists. About 1,000 institutions publish their press reports and dates of important events via idw.\n\n"}
{"id": "5995840", "url": "https://en.wikipedia.org/wiki?curid=5995840", "title": "L. G. Pine", "text": "L. G. Pine\n\nLeslie Gilbert Pine (22 December 1907 – 15 May 1987) was a British author, lecturer, and researcher in the areas of genealogy, nobility, history, heraldry and animal welfare. He was born in 1907 in Bristol, England and died in Bury St. Edmunds, Suffolk in 1987. He was the son of Lilian Grace Beswetherick and Henry Moorshead Pine (a tea merchant).\n\nFrom 1935 to 1940 he served as an assistant editor at Burke's Peerage Ltd. During World War II he was an officer in the Royal Air Force intelligence branch, serving in North Africa, Italy, Greece, and India; he retired with the rank of Squadron Leader. After the war and until 1960, he was Burke's executive director. Pine edited \"Burke's Peerage,\" 1949-1959; \"Burke's Landed Gentry (of Great Britain),\" 1952; \"Burke's Landed Gentry (of Ireland),\" 1958; and, \"Burke's Distinguished Families of America,\" 1939, 1947. He also edited \"The International Year Book and Statesmen's Who's Who,\" 1953-1960; \"Author's and Writer's Who's Who,\" 1948, 1960; \"Who's Who in Music,\" 1949; and, \"Who's Who in the Free Churches,\" 1951.\n\nA graduate of London University, he became a Barrister-at-Law, Inner Temple, in 1953. Pine was a member of the International Institute of Genealogy and Heraldry, Fellow of the Society of Antiquaries of Scotland, a Fellow of the Ancient Monuments Society, a Life Fellow of the Institute of Journalists, a Freeman of the City of London, and a Liveryman of the Glaziers' Company. In 1959 he was the unsuccessful Conservative candidate for Bristol Central.\n\nHe was managing editor of a British hunting magazine, \"Shooting Times\", from 1960 to 1964. He later authored an important book highly critical of sport hunting, \"After Their Blood\", in which he wrote: \"It is our duty as men and women of God’s redeemed creation to try not to increase the suffering of the world, but to lessen it. To get rid of bloodsports will be a great step toward this end.\"\n\nIn 1948 Leslie Pine married Grace V. Griffin (20 August 1914- ). Their only child, Richard Pine, was born in London on 21 August 1949.\n\nHis books include:\n\n\nPine is also the primary contributor to the article \"genealogy\" in \"Encyclopædia Britannica\".\n\n"}
{"id": "6487324", "url": "https://en.wikipedia.org/wiki?curid=6487324", "title": "Leishu", "text": "Leishu\n\nThe leishu () is a genre of reference books historically compiled in China and other countries of the Sinosphere. The term is generally translated as \"encyclopedia\", although the \"leishu\" are quite different from the modern notion of encyclopedia.\n\nThe \"leishu\" are composed of sometimes lengthy citations from other works, and often contain copies of entire works, not just excerpts. The works are classified by a systematic set of categories, which are further divided into subcategories. \"Leishu\" may be considered anthologies, but are encyclopedic in the sense that they may comprise the entire realm of knowledge at the time of compilation.\n\nApproximately 600 \"leishu\" were compiled from the early third century until the eighteenth century, of which 200 have survived. The largest \"leishu\" ever compiled was the 1408 \"Yongle Dadian\", containing 370 million Chinese characters, and the largest ever printed was the \"Gujin Tushu Jicheng\", containing 100 million characters and 852,408 pages.\n\nThe genre first appeared in the early third century. The earliest known was the \"Huanglan\" (\"Emperor's mirror\"). Sponsored by the emperor of Cao Wei, it was compiled around 220, but has since been lost. However, the term \"leishu\" was not used until the Song dynasty (960–1279).\n\nIn later imperial China dynasties, such as the Ming and Qing, emperors sponsored monumental projects to compile all known human knowledge into a single \"leishu\", in which entire works, rather than excerpts, were copied and classified by category. The largest \"leishu\" ever compiled, on the order of the Yongle Emperor of Ming, was the \"Yongle Dadian\" containing a total of 370 million Chinese characters. The project involved 2,169 scholars, who worked for four years under general editor Yao Guangxiao. It was completed in 1408, but never printed, as the imperial treasury had run out of money.\n\nThe \"Qinding Gujin Tushu Jicheng\" (Imperially approved synthesis of books and illustrations past and present) is by far the largest \"leishu\" ever printed, containing 100 million characters and 852,408 pages. It was compiled by a team of scholars led by Chen Menglei, and printed between 1726 and 1728, during the Qing dynasty.\n\nThe \"riyong leishu\" (encyclopedias for daily use), containing practical information for people who were literate but below the Confucian elite, were also compiled in the later imperial era. Today, they provide scholars with valuable information on non-elite culture and attitudes.\n\nAccording to Jean-Pierre Diény, the Jiaqing reign (1796–1820) of the Qing dynasty saw the end of the publication of \"leishu\".\n\nOther countries of the Sinosphere also adopted the genre of \"leishu\". In 1712, the \"Sancai Tuhui\", a richly illustrated \"leishu\" compiled by Ming scholar Wang Qi (王圻) in the early 17th century, was printed in Japan as \"Wakan Sansai Zue\". The Japanese version was edited by Terajima Ryōan (寺島良安), a physician born in Osaka.\n\nThe \"leishu\" have played an important role in the preservation of ancient works, many of which have been lost, only preserved completely or partially as part of a \"leishu\" compilation. The 7th-century \"Yiwen Leiju\" is especially valuable. It contains excerpts from 1,400 pre-7th century works, 90% of which have been otherwise lost. Even though the \"Yongle Dadian\" is itself largely lost, the remnants still contain 385 complete books that have been otherwise lost. The \"leishu\" also provide a unique view of the transmission of knowledge and education, and an easy way to locate traditional materials on any given subject.\n\nApproximately 600 \"leishu\" were compiled, from the Cao Wei period (early third century) until the 18th century, of which 200 have survived. Among the most important, in chronological order, are:\n\n\n"}
{"id": "330432", "url": "https://en.wikipedia.org/wiki?curid=330432", "title": "Note (typography)", "text": "Note (typography)\n\nA note is a string of text placed at the bottom of a page in a book or document or at the end of a chapter, volume or the whole text. The note can provide an author's comments on the main text or citations of a reference work in support of the text, or both.\n\nFootnotes are notes at the foot of the page while endnotes are collected under a separate heading at the end of a chapter, volume, or entire work. Unlike footnotes, endnotes have the advantage of not affecting the layout of the main text, but may cause inconvenience to readers who have to move back and forth between the main text and the endnotes.\n\nIn some editions of the Bible, notes are placed in a narrow column in the middle of each page between two columns of biblical text.\n\nIn English, a footnote is normally flagged by a superscripted number immediately following that portion of the text the note references, each such footnote being numbered sequentially. Occasionally a number between brackets or parentheses is used instead, thus: [1], which can also be superscripted, as in Wikipedia's own citation style rendered within this very page.\n\nTypographical devices such as the asterisk (*) or dagger (†) may also be used to point to footnotes; the traditional order of these symbols in English is *, †, ‡, §, ‖, ¶. Other symbols, including the #, Δ, ◊, ↓, and ☞, have also been used. In documents like timetables, many different symbols, letters and numbers may be used to refer the reader to particular notes.\n\nNotes are most often used as an alternative to long explanatory notes that can be distracting to readers. Most literary style guidelines (including the Modern Language Association and the American Psychological Association) recommend limited use of foot and endnotes. However, publishers often encourage note references in lieu of parenthetical references. Aside from use as a bibliographic element, notes are used for additional information or explanatory notes that might be too digressive for the main text. Footnotes are heavily utilized in academic institutions to support claims made in academic essays covering myriads of topics.\n\nIn particular, footnotes are the normal form of citation in historical journals. This is due, firstly, to the fact that the most important references are often to archive sources or interviews which do not readily fit standard formats, and secondly, to the fact that historians expect to see the exact nature of the evidence which is being used at each stage.\n\nThe MLA (Modern Language Association) requires the superscript numbers in the main text to be placed following the punctuation in the phrase or clause the note is in reference to. The exception to this rule occurs when a sentence contains a dash, in which case the superscript would precede it.\n\nAside from their technical use, authors use notes for a variety of reasons:\n\nThe US Government Printing Office Style Manual devotes over 660 words to the topic of footnotes. NASA has guidance for footnote usage in its historical documents.\nAssociate Justice Stephen Breyer of the Supreme Court of the United States is famous in the American legal community for his writing style, in which he never uses notes. He prefers to keep all citations within the text (which is permitted in American legal citation). Richard A. Posner has also written against the use of notes in judicial opinions. Bryan A. Garner, however, advocates using notes instead of inline citations.\n\nHTML, the predominant markup language for web pages, has no mechanism for marking up notes. Despite a number of different proposals over the years, and repeated pleas from the user base, the working group has been unable to reach a consensus on it. Because of this, MediaWiki, for example, has had to introduce its own codice_1 tag for citing references in notes, an idea which has since also been implemented for generic use by the \"Nelson\" HTML preprocessor.\n\nIt might be argued that the hyperlink partially eliminates the need for notes, being the web's way to refer to another document. However, it does not allow citing to offline sources and if the destination of the link changes, the link can become dead or irrelevant.\n\nThe sign is historically equal to the asterisks used by Aristarchus of Samothrace at the Mouseion at Alexandria. It was used for the critical editions of Homer's writings where it \"marked a verse incorrectly repeated in another passage\" and was used together with other signs such as the obelus.\n\nThe London printer Richard Jugge is generally credited as the inventor of the footnote, first used in the Bishops' Bible of 1568.\n\nEarly printings of the Douay Bible used two closely spaced colons (actually squared four dot punctuation mark U+2E2C) to indicate a marginal note.\n\nAt times, notes have been used for their comical effect, or as a literary device.\n\n\n\n"}
{"id": "10550174", "url": "https://en.wikipedia.org/wiki?curid=10550174", "title": "Observer's Books", "text": "Observer's Books\n\nThe Observer's Books were a series of small, pocket-sized books, published by Frederick Warne & Co in the United Kingdom from 1937 to 2003. They covered a variety of topics including hobbies, art, history and wildlife. The aim of these books was to interest the observer and they have also been popular amongst children. Some of them have become collector's items. For the dedicated collector this could be a lifetime's work as there are over 800 variations, some of which are now rare. The values of the books can vary from 50 pence to hundreds of pounds. \n\nThe books were produced with paper dust covers up until 1969. Each one had a unique pattern of squiggly lines at the top but these were not especially practical because they were easy to rip and stain. From 1970, the covers were protected with a glossy coating. These types are often referred to as \"Glossies\". From the late 1970s, Warne decided to laminate the covers to the actual books to make them sturdier and more resistant to wear.\n\nThe first Observer's guide was published in 1937, and was on the subject of British birds. This is now rare, and a mint copy with a dust cover is worth hundreds of pounds. The same year, Warne published a second Observer's book on British wild flowers. A mint copy of this book is worth around £220. When the popularity of these was recognized, several more titles were added 'uniform in the series', but during World War II production was limited due to paper and labour shortages. Even so, by 1941 Warne had published the first six Observer's books.\n\nIn 1942 a special edition book was brought out on \"airplanes\" . This book had no number in the series, as it was brought out to help people spot enemy warplanes. It was reprinted in 1943 and 1945. \n\nThe first few Observer's titles had focused on nature, but gradually subjects like geology, music and architecture were introduced. 'Spotter' titles like \"Aircraft\", \"Automobiles\" and \"Railway Locomotives\" proved popular. During the 1950s and 60s collecting sets of these books was popular among children and adults alike. \n\nWhen Warne was acquired by Penguin books in 1983, Warne brought out new editions of the Observer's books. These were slightly bigger than the original books, and were in paperback, not hardback. The same year Penguin, with permission of Warne, started printing their own, more up-to-date Observer's books. These again were slightly larger than the originals, but were hardbacks. Like the later original Observer's books, the dust covers were laminated to the actual book. There were two types of the Penguin Observer's books: Bloomsbury Observer's, and Claremont Observer's, (of which there were only 12 different editions).\n\nAfter a hiatus of 17 years, Peregrine Books published the appropriately titled \"Observer's Book of Observer's Books\" in 1999, in a format that matched the original editions and was numbered 99 so as to follow on from the last 'official' title. As the title implies, it is a guide to the series with details of its history, authors, and print-runs. As a sign of the series' popularity, this potentially obscure book has been reprinted no fewer than six times. More recently the series has been rounded up to 100 with the publication of \"Wayside and Woodland\" in 2003.\n"}
{"id": "177891", "url": "https://en.wikipedia.org/wiki?curid=177891", "title": "Primary source", "text": "Primary source\n\nIn the study of history as an academic discipline, a primary source (also called an original source) is an artifact, document, diary, manuscript, autobiography, recording, or any other source of information that was created at the time under study. It serves as an original source of information about the topic. Similar definitions can be used in library science, and other areas of scholarship, although different fields have somewhat different definitions. In journalism, a primary source can be a person with direct knowledge of a situation, or a document written by such a person.\n\nPrimary sources are distinguished from secondary sources, which cite, comment on, or build upon primary sources. Generally, accounts written after the fact with the benefit (and possible distortions) of hindsight are secondary. A secondary source may also be a primary source depending on how it is used. For example, a memoir would be considered a primary source in research concerning its author or about his or her friends characterized within it, but the same memoir would be a secondary source if it were used to examine the culture in which its author lived. \"Primary\" and \"secondary\" should be understood as relative terms, with sources categorized according to specific historical contexts and what is being studied.\n\nIn scholarly writing, an important objective of classifying sources is to determine their independence and reliability. In contexts such as historical writing, it is almost always advisable to use primary sources and that \"if none are available, it is only with great caution that [the author] may proceed to make use of secondary sources.\" Sreedharan believes that primary sources have the most direct connection to the past and that they \"speak for themselves\" in ways that cannot be captured through the filter of secondary sources.\n\nIn scholarly writing, the objective of classifying sources is to determine the independence and reliability of sources. Though the terms \"primary source\" and \"secondary source\" originated in historiography as a way to trace the history of historical ideas, they have been applied to many other fields. For example, these ideas may be used to trace the history of scientific theories, literary elements and other information that is passed from one author to another.\n\nIn scientific literature, a primary source is the original publication of a scientist's new data, results and theories. In political history, primary sources are documents such as official reports, speeches, pamphlets, posters, or letters by participants, official election returns and eyewitness accounts. In the history of ideas or intellectual history, the main primary sources are books, essays and letters written by intellectuals; these intellectuals may include historians, whose books and essays are therefore considered primary sources for the intellectual historian, though they are secondary sources in their own topical fields. In religious history, the primary sources are religious texts and descriptions of religious ceremonies and rituals.\n\nA study of cultural history could include fictional sources such as novels or plays. In a broader sense primary sources also include artifacts like photographs, newsreels, coins, paintings or buildings created at the time. Historians may also take archaeological artifacts and oral reports and interviews into consideration. Written sources may be divided into three types.\n\n\nIn historiography, when the study of history is subject to historical scrutiny, a secondary source becomes a primary source. For a biography of a historian, that historian's publications would be primary sources. Documentary films can be considered a secondary source or primary source, depending on how much the filmmaker modifies the original sources.\n\nThe Lafayette College Library, provides a synopsis of primary sources in several areas of study:\n\"The definition of a primary source varies depending upon the academic discipline and the context in which it is used.<br>\n\nAlthough many primary sources remain in private hands, others are located in archives, libraries, museums, historical societies, and special collections. These can be public or private. Some are affiliated with universities and colleges, while others are government entities. Materials relating to one area might be spread over a large number of different institutions. These can be distant from the original source of the document. For example, the Huntington Library in California houses a large number of documents from the United Kingdom.\n\nIn the US, digital copies of primary sources can be retrieved from a number of places. The Library of Congress maintains several digital collections where they can be retrieved. Some examples are American Memory and Chronicling America. The National Archives and Records Administration also has digital collections in Digital Vaults. The Digital Public Library of America searches across the digitized primary source collections of many libraries, archives, and museums. The Internet Archive also has primary source materials in many formats.\n\nIn the UK, the National Archives provides a consolidated search of its own catalogue and a wide variety of other archives listed on the Access to Archives index. Digital copies of various classes of documents at the National Archives (including wills) are available from DocumentsOnline. Most of the available documents relate to England and Wales. Some digital copies of primary sources are available from the National Archives of Scotland. Many County Record Offices collections are included in Access to Archives, while others have their own on-line catalogues. Many County Record Offices will supply digital copies of documents.\n\nIn other regions, Europeana has digitized materials from across Europe while the World Digital Library and Flickr Commons have items from all over the world. Trove has primary sources from Australia.\n\nMost primary source materials are not digitized and may only be represented online with a record or finding aid. Both digitized and not digitized materials can be found through catalogs such as WorldCat, the Library of Congress catalog, the National Archives catalog, and so on.\n\nHistory as an academic discipline is based on primary sources, as evaluated by the community of scholars, who report their findings in books, articles and papers. Arthur Marwick says \"Primary sources are absolutely fundamental to history.\" Ideally, a historian will use all available primary sources that were created by the people involved at the time being studied. In practice some sources have been destroyed, while others are not available for research. Perhaps the only eyewitness reports of an event may be memoirs, autobiographies, or oral interviews taken years later. Sometimes the only evidence relating to an event or person in the distant past was written or copied decades or centuries later. Manuscripts that are sources for classical texts can be copies of documents, or fragments of copies of documents. This is a common problem in classical studies, where sometimes only a summary of a book or letter has survived. Potential difficulties with primary sources have the result that history is usually taught in schools using secondary sources.\n\nHistorians studying the modern period with the intention of publishing an academic article prefer to go back to available primary sources and to seek new (in other words, forgotten or lost) ones. Primary sources, whether accurate or not, offer new input into historical questions and most modern history revolves around heavy use of archives and special collections for the purpose of finding useful primary sources. A work on history is not likely to be taken seriously as scholarship if it only cites secondary sources, as it does not indicate that original research has been done.\n\nHowever, primary sources – particularly those from before the 20th century – may have hidden challenges. \"Primary sources, in fact, are usually fragmentary, ambiguous and very difficult to analyse and interpret.\" Obsolete meanings of familiar words and social context are among the traps that await the newcomer to historical studies. For this reason, the interpretation of primary texts is typically taught as part of an advanced college or postgraduate history course, although advanced self-study or informal training is also possible.\n\nThe following questions are asked about primary sources:\n\nIn many fields and contexts, such as historical writing, it is almost always advisable to use primary sources if possible, and \"if none are available, it is only with great caution that [the author] may proceed to make use of secondary sources.\" In addition, primary sources avoid the problem inherent in secondary sources in which each new author may distort and put a new spin on the findings of prior cited authors.\n\nHowever, a primary source is not necessarily more of an authority or better than a secondary source. There can be bias and tacit unconscious views which twist historical information.\n\nParticipants and eyewitnesses may misunderstand events or distort their reports, deliberately or not, to enhance their own image or importance. Such effects can increase over time, as people create a narrative that may not be accurate. For any source, primary or secondary, it is important for the researcher to evaluate the amount and direction of bias. As an example, a government report may be an accurate and unbiased description of events, but it may be censored or altered for propaganda or cover-up purposes. The facts can be distorted to present the opposing sides in a negative light. Barristers are taught that evidence in a court case may be truthful but may still be distorted to support or oppose the position of one of the parties.\n\nMany sources can be considered either primary or secondary, depending on the context in which they are examined. Moreover, the distinction between \"primary\" and \"secondary\" sources is subjective and contextual, so that precise definitions are difficult to make. A book review, when it contains the opinion of the reviewer about the book rather than a summary of the book, becomes a primary source.\n\nIf a historical text discusses old documents to derive a new historical conclusion, it is considered to be a primary source for the new conclusion. Examples in which a source can be both primary and secondary include an obituary or a survey of several volumes of a journal counting the frequency of articles on a certain topic.\n\nWhether a source is regarded as primary or secondary in a given context may change, depending upon the present state of knowledge within the field. For example, if a document refers to the contents of a previous but undiscovered letter, that document may be considered \"primary\", since it is the closest known thing to an original source; but if the letter is later found, it may then be considered \"secondary\"\n\nIn some instances, the reason for identifying a text as the \"primary source\" may devolve from the fact that no copy of the original source material exists, or that it is the oldest extant source for the information cited.\n\nHistorians must occasionally contend with forged documents that purport to be primary sources. These forgeries have usually been constructed with a fraudulent purpose, such as promulgating legal rights, supporting false pedigrees, or promoting particular interpretations of historic events. The investigation of documents to determine their authenticity is called diplomatics.\n\nFor centuries, Popes used the forged Donation of Constantine to bolster the Papacy's secular power. Among the earliest forgeries are false Anglo-Saxon charters, a number of 11th- and 12th-century forgeries produced by monasteries and abbeys to support a claim to land where the original document had been lost or never existed. One particularly unusual forgery of a primary source was perpetrated by Sir Edward Dering, who placed false monumental brasses in a parish church. In 1986, Hugh Trevor-Roper \"authenticated\" the Hitler Diaries, which were later proved to be forgeries. Recently, forged documents have been placed within the UK National Archives in the hope of establishing a false provenance. However, historians dealing with recent centuries rarely encounter forgeries of any importance.\n\n</div>\n\n\n\n"}
{"id": "25407", "url": "https://en.wikipedia.org/wiki?curid=25407", "title": "Recursion", "text": "Recursion\n\nRecursion occurs when a thing is defined in terms of itself or of its type. Recursion is used in a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, where a function being defined is applied within its own definition. While this apparently defines an infinite number of instances (function values), it is often done in such a way that no loop or infinite chain of references can occur.\n\nIn mathematics and computer science, a class of objects or methods exhibit recursive behavior when they can be defined by two properties:\n\nFor example, the following is a recursive definition of a person's ancestors:\n\nThe Fibonacci sequence is a classic example of recursion:\n\nformula_1\n\nformula_2\n\nformula_3\n\nMany mathematical axioms are based upon recursive rules. For example, the formal definition of the natural numbers by the Peano axioms can be described as: \"0 is a natural number, and each natural number has a successor, which is also a natural number.\" By this base case and recursive rule, one can generate the set of all natural numbers.\n\nRecursively defined mathematical objects include functions, sets, and especially fractals.\n\nThere are various more tongue-in-cheek \"definitions\" of recursion; see recursive humor.\n\nRecursion is the process a procedure goes through when one of the steps of the procedure involves invoking the procedure itself. A procedure that goes through recursion is said to be 'recursive'.\n\nTo understand recursion, one must recognize the distinction between a procedure and the running of a procedure. A procedure is a set of steps based on a set of rules. The running of a procedure involves actually following the rules and performing the steps. An analogy: a procedure is like a written recipe; running a procedure is like actually preparing the meal.\n\nRecursion is related to, but not the same as, a reference within the specification of a procedure to the execution of some other procedure. For instance, a recipe might refer to cooking vegetables, which is another procedure that in turn requires heating water, and so forth. However, a recursive procedure is where (at least) one of its steps calls for a new instance of the very same procedure, like a sourdough recipe calling for some dough left over from the last time the same recipe was made. This immediately creates the possibility of an endless loop; recursion can only be properly used in a definition if the step in question is skipped in certain cases so that the procedure can complete, like a sourdough recipe that also tells you how to get some starter dough in case you've never made it before. Even if properly defined, a recursive procedure is not easy for humans to perform, as it requires distinguishing the new from the old (partially executed) invocation of the procedure; this requires some administration of how far various simultaneous instances of the procedures have progressed. For this reason recursive definitions are very rare in everyday situations. An example could be the following procedure to find a way through a maze. Proceed forward until reaching either an exit or a branching point (a dead end is considered a branching point with 0 branches). If the point reached is an exit, terminate. Otherwise try each branch in turn, using the procedure recursively; if every trial fails by reaching only dead ends, return on the path that led to this branching point and report failure. Whether this actually defines a terminating procedure depends on the nature of the maze: it must not allow loops. In any case, executing the procedure requires carefully recording all currently explored branching points, and which of their branches have already been exhaustively tried.\n\nLinguist Noam Chomsky among many others has argued that the lack of an upper bound on the number of grammatical sentences in a language, and the lack of an upper bound on grammatical sentence length (beyond practical constraints such as the time available to utter one), can be explained as the consequence of recursion in natural language. This can be understood in terms of a recursive definition of a syntactic category, such as a sentence. A sentence can have a structure in which what follows the verb is another sentence: \"Dorothy thinks witches are dangerous\", in which the sentence \"witches are dangerous\" occurs in the larger one. So a sentence can be defined recursively (very roughly) as something with a structure that includes a noun phrase, a verb, and optionally another sentence. This is really just a special case of the mathematical definition of recursion.\n\nThis provides a way of understanding the creativity of language—the unbounded number of grammatical sentences—because it immediately predicts that sentences can be of arbitrary length: \"Dorothy thinks that Toto suspects that Tin Man said that...\". There are many structures apart from sentences that can be defined recursively, and therefore many ways in which a sentence can embed instances of one category inside another. Over the years, languages in general have proved amenable to this kind of analysis.\n\nRecently, however, the generally accepted idea that recursion is an essential property of human language has been challenged by Daniel Everett on the basis of his claims about the Pirahã language. Andrew Nevins, David Pesetsky and Cilene Rodrigues are among many who have argued against this. Literary self-reference can in any case be argued to be different in kind from mathematical or logical recursion.\n\nRecursion plays a crucial role not only in syntax, but also in natural language semantics. The word \"and\", for example, can be construed as a function that can apply to sentence meanings to create new sentences, and likewise for noun phrase meanings, verb phrase meanings, and others. It can also apply to intransitive verbs, transitive verbs, or ditransitive verbs. In order to provide a single denotation for it that is suitably flexible, \"and\" is typically defined so that it can take any of these different types of meanings as arguments. This can be done by defining it for a simple case in which it combines sentences, and then defining the other cases recursively in terms of the simple one. \n\nA recursive grammar is a formal grammar that contains recursive production rules.\n\nRecursion is sometimes used humorously in computer science, programming, philosophy, or mathematics textbooks, generally by giving a circular definition or self-reference, in which the putative recursive step does not get closer to a base case, but instead leads to an infinite regress. It is not unusual for such books to include a joke entry in their glossary along the lines of:\n\nA variation is found on page 269 in the index of some editions of Brian Kernighan and Dennis Ritchie's book \"The C Programming Language\"; the index entry recursively references itself (\"recursion 86, 139, 141, 182, 202, 269\"). The earliest version of this joke was in \"Software Tools\" by Kernighan and Plauger, and also appears in \"The UNIX Programming Environment\" by Kernighan and Pike. It did not appear in the first edition of \"The C Programming Language\".\n\nAnother joke is that \"To understand recursion, you must understand recursion.\" In the English-language version of the Google web search engine, when a search for \"recursion\" is made, the site suggests \"Did you mean: \"recursion\".\" An alternative form is the following, from Andrew Plotkin: \"If you already know what recursion is, just remember the answer. Otherwise, find someone who is standing closer to Douglas Hofstadter than you are; then ask him or her what recursion is.\"\n\nRecursive acronyms can also be examples of recursive humor. PHP, for example, stands for \"PHP Hypertext Preprocessor\", WINE stands for \"WINE Is Not an Emulator.\" and GNU stands for \"GNU's not Unix\".\n\nThe canonical example of a recursively defined set is given by the natural numbers:\n\nAnother interesting example is the set of all \"true reachable\" propositions in an axiomatic system.\n\n\nThis set is called 'true reachable propositions' because in non-constructive approaches to the foundations of mathematics, the set of true propositions may be larger than the set recursively constructed from the axioms and rules of inference. See also Gödel's incompleteness theorems.\n\nFinite subdivision rules are a geometric form of recursion, which can be used to create fractal-like images. A subdivision rule starts with a collection of polygons labelled by finitely many labels, and then each polygon is subdivided into smaller labelled polygons in a way that depends only on the labels of the original polygon. This process can be iterated. The standard `middle thirds' technique for creating the Cantor set is a subdivision rule, as is barycentric subdivision.\n\nA function may be partly defined in terms of itself. A familiar example is the Fibonacci number sequence: \"F\"(\"n\") = \"F\"(\"n\" − 1) + \"F\"(\"n\" − 2). For such a definition to be useful, it must lead to non-recursively defined values, in this case \"F\"(0) = 0 and \"F\"(1) = 1.\n\nA famous recursive function is the Ackermann function, which—unlike the Fibonacci sequence—cannot easily be expressed without recursion.\n\nApplying the standard technique of proof by cases to recursively defined sets or functions, as in the preceding sections, yields structural induction, a powerful generalization of mathematical induction widely used to derive proofs in mathematical logic and computer science.\n\nDynamic programming is an approach to optimization that restates a multiperiod or multistep optimization problem in recursive form. The key result in dynamic programming is the Bellman equation, which writes the value of the optimization problem at an earlier time (or earlier step) in terms of its value at a later time (or later step).\n\nIn set theory, this is a theorem guaranteeing that recursively defined functions exist. Given a set \"X\", an element \"a\" of \"X\" and a function formula_7, the theorem states that there is a unique function formula_8 (where formula_4 denotes the set of natural numbers including zero) such that\nfor any natural number \"n\".\n\nTake two functions formula_8 and formula_13 such that:\n\nwhere \"a\" is an element of \"X\".\n\nIt can be proved by mathematical induction that formula_18 for all natural numbers \"n\":\n\nBy induction, formula_18 for all formula_25.\n\nA common method of simplification is to divide a problem into subproblems of the same type. As a computer programming technique, this is called divide and conquer and is key to the design of many important algorithms. Divide and conquer serves as a top-down approach to problem solving, where problems are solved by solving smaller and smaller instances. A contrary approach is dynamic programming. This approach serves as a bottom-up approach, where problems are solved by solving larger and larger instances, until the desired size is reached.\n\nA classic example of recursion is the definition of the factorial function, given here in C code:\n\nIf not having reached the base case and returning with value every instantiation of the above function creates a new instance of the function, passing to it an input reduced by (), and returns the result of this (recursive) call, multiplied by its own value of , analogously to the mathematical definition of the factorial.\n\nRecursion in computer programming is exemplified when a function is defined in terms of simpler, often smaller versions of itself. The solution to the problem is then devised by combining the solutions obtained from the simpler versions of the problem. One example application of recursion is in parsers for programming languages. The great advantage of recursion is that an infinite set of possible sentences, designs or other data can be defined, parsed or produced by a finite computer program.\n\nRecurrence relations are equations to define one or more sequences recursively. Some specific kinds of recurrence relation can be \"solved\" to obtain a non-recursive definition.\n\nUse of recursion in an algorithm has both advantages and disadvantages. The main advantage is usually simplicity. The main disadvantage is often that the algorithm may require large amounts of memory if the depth of the recursion is very large.\n\nThe Russian Doll or Matryoshka Doll is a physical artistic example of the recursive concept.\n\nRecursion has been used in paintings since Giotto's \"Stefaneschi Triptych\", made in 1320. Its central panel contains the kneeling figure of Cardinal Stefaneschi, holding up the triptych itself as an offering.\n\nM. C. Escher's \"Print Gallery\" (1956) is a print which depicts a distorted city which contains a gallery which recursively contains the picture, and so \"ad infinitum\".\n\n\n"}
{"id": "20110874", "url": "https://en.wikipedia.org/wiki?curid=20110874", "title": "Reference", "text": "Reference\n\nReference is a relation between objects in which one object designates, or acts as a means by which to connect to or link to, another object. The first object in this relation is said to \"refer to\" the second object. It is called a \"name\" for the second object. The second object, the one to which the first object refers, is called the \"referent\" of the first object. A name is usually a phrase or expression, or some other symbolic representation. Its referent may be anything – a material object, a person, an event, an activity, or an abstract concept.\n\nReferences can take on many forms, including: a thought, a sensory perception that is audible (onomatopoeia), visual (text), olfactory, or tactile, emotional state, relationship with other, spacetime coordinate, symbolic or alpha-numeric, a physical object or an energy projection. In some cases, methods are used that intentionally hide the reference from some observers, as in cryptography.\n\nReferences feature in many spheres of human activity and knowledge, and the term adopts shades of meaning particular to the contexts in which it is used. Some of them are described in the sections below.\n\nThe word \"reference\" is derived from Middle English \"referren\", from Middle French \"référer\", from Latin \"referre\", \"to carry back\", formed from the prefix \"re\"- and \"ferre\", \"to bear\". A number of words derive from the same root, including \"refer\", \"referee\", \"referential\", \"referent\", \"referendum\".\n\nThe verb \"refer (to)\" and its derivatives may carry the sense of \"link to\" or \"connect to\", as in the meanings of \"reference\" described in this article. Another sense is \"consult\"; this is reflected in such expressions as reference work, reference desk, job reference, etc.\n\nIn semantics, reference is generally construed as the relationships between nouns or pronouns and objects that are named by them. Hence, the word \"John\" refers to the person John. The word \"it\" refers to some previously specified object. The object referred to is called the \"referent\" of the word. Sometimes the word-object relation is called \"denotation\"; the word denotes the object. The converse relation, the relation from object to word, is called \"exemplification\"; the object exemplifies what the word denotes. In syntactic analysis, if a word refers to a previous word, the previous word is called the \"antecedent\".\n\nGottlob Frege argued that reference cannot be treated as identical with meaning: \"Hesperus\" (an ancient Greek name for the evening star) and \"Phosphorus\" (an ancient Greek name for the morning star) both refer to Venus, but the astronomical fact that '\"Hesperus\" is \"Phosphorus\"' can still be informative, even if the \"meanings\" of \"Hesperus\" and \"Phosphorus\" are already known. This problem led Frege to distinguish between the sense and reference of a word. Some cases seem to be too complicated to be classified within this framework; the acceptance of the notion of secondary reference may be necessary to fill the gap. See also Opaque context.\n\nThe very concept of the linguistic sign is the combination of content and expression, the former of which may refer entities in the world or refer more abstract concepts, e.g. thought.\nCertain parts of speech exist only to express reference, namely anaphora such as pronouns. The subset of reflexives expresses co-reference of two participants in a sentence. These could be the agent (actor) and patient (acted on), as in \"The man washed himself\", the theme and recipient, as in \"I showed Mary to herself\", or various other possible combinations.\n\nIn computer science, references are data types that refer to an object elsewhere in memory and are used to construct a wide variety of data structures, such as linked lists. Generally, a reference is a value that enables a program to directly access the particular data item. Most programming languages support some form of reference. For the specific type of reference used in the C++ language, see reference (C++).\n\nThe notion of reference is also important in relational database theory; see referential integrity.\n\nReferences to many types of printed matter may come in an electronic or machine-readable form. For books, there exists the ISBN and for journal articles, the Digital object identifier (DOI) is gaining relevance. Information on the Internet may be referred to by a Uniform Resource Identifier (URI).\n\nIn terms of mental processing, a self-reference is used in psychology to establish identification with a mental state during self-analysis. This seeks to allow the individual to develop own frames of reference in a greater state of immediate awareness. However, it can also lead to circular reasoning, preventing evolution of thought.\n\nAccording to Perceptual Control Theory (PCT), a reference condition is the state toward which a control system's output tends to alter a controlled quantity. The main proposition is that \"All behavior is oriented all of the time around the control of certain quantities with respect to specific reference conditions.\"\n\nIn academics and scholarship, an author-title-date information in bibliographies and footnotes, specifying complete works of other people. Copying of material by another author without proper citation or without required permissions is plagiarism.\n\nKeeping a diary allows an individual to use references for personal organization, whether or not anyone else understands the systems of reference used. However, scholars have studied methods of reference because of their key role in communication and co-operation between \"different\" people, and also because of misunderstandings that can arise. Modern academic study of reference has been developing since the 19th century.\n\nIn scholarship, a reference may be a citation of a text that has been used in the creation of a piece of work such as an essay, report, or oration. Its primary purpose is to allow people who read such work to examine the author's sources, either for validity or to learn more about the subject. Such items are often listed at the end of an article or book in a section marked \"Bibliography\" or \"References\". A bibliographical section often contains works not cited by the author, but used as background reading or listed as potentially useful to the reader. A reference section contains only those works cited by the author(s) in the main text.\n\nIn patent law, a reference is a document that can be used to show the state of knowledge at a given time and that therefore may make a claimed invention obvious or anticipated. Examples of references are patents of any country, magazine articles, Ph.D. theses that are indexed and thus accessible to those interested in finding information about the subject matter, and to some extent Internet material that is similarly accessible.\n\nIn art, a reference is an item from which a work is based. This may include:\nAnother example of reference is samples of various musical works being incorporated into a new one.\n\n\n"}
{"id": "8912106", "url": "https://en.wikipedia.org/wiki?curid=8912106", "title": "Reference scenario", "text": "Reference scenario\n\nA reference scenario is an imagined situation where a library patron brings a question to a librarian and there is then a conversation, called in the field a reference interview, where the librarian works to help the patron find what he or she wants. These scenarios are used in training future librarians how to help patrons. Basically, a scenario is as short as a couple of sentences, including a question and a situation that underlies that question.\n\nA great deal of reference teaching puts students to researching the answers to made-up questions. This focuses the student on learning about the reference sources at hand by using them to answer those questions. Scenarios are something different. They focus the student on the interaction with patrons. In class practice sessions, one student can be the patron and the other the librarian, as long as the one practicing as the librarian doesn't know the whole scenario in advance.\n\nScenarios are valued because often the question asked is not the end of the patron's information hunt, but the start. Patrons often start by voicing a question that they think the library can answer, rather than the question they are really seeking to answer. Or they pose a question that the librarian doesn't understand. Reference librarian skills are very much about mediating a gap between what the patron wants and what the library can provide. This can involve the librarian making him or herself a partner in the patron's search, teaching them what the library really has to offer, or even just clarifying a confusing word: Does the patron want information about soaps to clean with or soaps as in soap operas?\n\n\n"}
{"id": "56067306", "url": "https://en.wikipedia.org/wiki?curid=56067306", "title": "SDS-PAGE", "text": "SDS-PAGE\n\nSDS-PAGE (sodium dodecyl sulfate–polyacrylamide gel electrophoresis) is a variant of polyacrylamide gel electrophoresis, an analytical method in biochemistry for the separation of charged molecules in mixtures by their molecular masses in an electric field. It uses sodium dodecyl sulfate (SDS) molecules to help identify and isolate protein molecules.\n\nSDS-PAGE is a discontinuous electrophoretic system developed by Ulrich K. Laemmli which is commonly used as a method to separate proteins with molecular masses between 5 and 250 KDa. The publication describing it is the most frequently cited paper by a single author, and the second most cited overall.\n\nSDS-PAGE is an electrophoresis method that allows protein separation by mass. The medium (also referred to as ′matrix′) is a polyacrylamide-based discontinuous gel. In addition, SDS (sodium dodecyl sulfate) is used. About 1.4 grams of SDS bind to a gram of protein, corresponding to one SDS molecule per two amino acids. SDS acts as a surfactant, covering the proteins' intrinsic charge and conferring them very similar charge-to-mass ratios. The intrinsic charges of the proteins are negligible in comparison to the SDS loading, and the positive charges are also greatly reduced in the basic pH range of a separating gel. Upon application of a constant electric field, the protein migrate towards the anode, each with a different speed, depending on its mass. This simple procedure allows precise protein separation by mass.\n\nSDS tends to form spherical micelles in aqueous solutions above a certain concentration called the critical micellar concentration (CMC). Above the critical micellar concentration of 7 to 10 millimolar in solutions, the SDS simultaneously occurs as single molecules (monomer) and as micelles, below the CMC SDS occurs only as monomers in aqueous solutions. At the critical micellar concentration, a micelle consists of about 62 SDS molecules. However, only SDS monomers bind to proteins via hydrophobic interactions, whereas the SDS micelles are anionic on the outside and do not adsorb any protein. SDS is amphipathic in nature, which allows it to unfold both polar and nonpolar sections of protein structure. In SDS concentrations above 0.1 millimolar, the unfolding of proteins begins, and above 1 mM, most proteins are denatured. Due to the strong denaturing effect of SDS and the subsequent dissociation of protein complexes, quaternary structures can generally not be determined with SDS. Exceptions are e.g. proteins that were previously stabilised by covalent cross-linking and the SDS-resistant protein complexes, which are stable even in the presence of SDS (the latter, however, only at room temperature). To denature the SDS-resistant complexes a high activation energy is required, which is achieved by heating. SDS resistance is based on a metastability of the protein fold. Although the native, fully folded, SDS-resistant protein does not have sufficient stability in the presence of SDS, the chemical equilibrium of denaturation at room temperature occurs slowly. Stable protein complexes are characterised not only by SDS resistance but also by stability against proteases and an increased biological half-life.\n\nAlternatively, polyacrylamide gel electrophoresis can also be performed with the cationic surfactants CTAB in a CTAB-PAGE, or 16-BAC in a BAC-PAGE.\n\nThe SDS-PAGE method is composed of gel preparation, sample preparation, electrophoresis, protein staining or western blotting and analysis of the generated banding pattern.\n\nWhen using different buffers in the gel (discontinuous gel electrophoresis), the gels are made up to one day prior to electrophoresis, so that the diffusion does not lead to a mixing of the buffers. The gel is produced by radical polymerisation in a mold consisting of two sealed glass plates with spacers between the glass plates. In a typical mini-gel setting, the spacers have a thickness of 0.75 mm or 1.5 mm, which determines the loading capacity of the gel. For pouring the gel solution, the plates are usually clamped in a stand which temporarily seals the otherwise open underside of the glass plates with the two spacers. For the gel solution, acrylamide is mixed as gel-former (usually 4% V/V in the stacking gel and 10-12 % in the separating gel), methylenebisacrylamide as a cross-linker, stacking or separating gel buffer, water and SDS. By adding the catalyst TEMED and the radical initiator ammonium persulfate (APS) the polymerisation is started. The solution is then poured between the glass plates without creating bubbles. Depending on the amount of catalyst and radical starter and depending on the temperature, the polymerisation lasts between a quarter of an hour and several hours. The lower gel (separating gel) is poured first and covered with a few drops of a barely water-soluble alcohol (usually buffer-saturated butanol or isopropanol), which eliminates bubbles from the meniscus and protects the gel solution of the radical scavenger oxygen. After the polymerisation of the separating gel, the alcohol is discarded and the residual alcohol is removed with filter paper. After addition of APS and TEMED to the stacking gel solution, it is poured on top of the solid separation gel. Afterwards, a suitable sample comb is inserted between the glass plates without creating bubbles. The sample comb is carefully pulled out after polymerisation, leaving pockets for the sample application. For later use of proteins for protein sequencing, the gels are often prepared the day before electrophoresis to reduce reactions of unpolymerised acrylamide with cysteines in proteins.\n\nBy using a gradient mixer, gradient gels with a gradient of acrylamide (usually from 4 to 12%) can be cast, which have a larger separation range of the molecular masses. Commercial gel systems (so-called \"pre-cast gels\") usually use the buffer substance Bis-tris methane with a pH value between 6.4 and 7.2 both in the stacking gel and in the separating gel. These gels are delivered cast and ready-to-use. Since they use only one buffer (continuous gel electrophoresis) and have a nearly neutral pH, they can be stored for several weeks. The more neutral pH slows the hydrolysis and thus the decomposition of the polyacrylamide. Furthermore, there are fewer acrylamide-modified cysteines in the proteins. Due to the constant pH in collecting and separating gel there is no stacking effect. Proteins in BisTris gels can not be stained with ruthenium complexes. This gel system has a comparatively large separation range, which can be varied by using MES or MOPS in the running buffer.\n\nDuring sample preparation, the sample buffer, and thus SDS, is added in excess to the proteins, and the sample is then heated to 95 °C for five minutes, or alternatively 70°C for ten minutes. Heating disrupts the secondary and tertiary structures of the protein by disrupting hydrogen bonds and stretching the molecules. Optionally, disulfide bridges can be cleaved by reduction. For this purpose, reducing thiols such as β-mercaptoethanol (β-ME, 5% by volume), dithiothreitol (DTT, 10 millimolar) or dithioerythritol (DTE, 10 millimolar) are added to the sample buffer. After cooling to room temperature, each sample is pipetted into its own well in the gel, which was previously immersed in electrophoresis buffer in the electrophoresis apparatus.\n\nIn addition to the samples, a molecular-weight size marker is usually loaded onto the gel. This consists of proteins of known sizes and thereby allows the estimation (with an error of ± 10%) of the sizes of the proteins in the actual samples, which migrate in parallel in different tracks of the gel. The size marker is often pipetted into the first or last pocket of a gel.\n\nFor separation, the denatured samples are loaded onto a gel of polyacrylamide, which is placed in an electrophoresis buffer with suitable electrolytes. Thereafter, a voltage (usually around 100 V, 10-20 V per cm gel length) is applied, which causes a migration of negatively charged molecules through the gel in the direction of the positively charged anode. The gel acts like a sieve. Small proteins migrate relatively easily through the mesh of the gel, while larger proteins are more likely to be retained and thereby migrate more slowly through the gel, thereby allowing proteins to be separated by molecular size. The electrophoresis lasts between half an hour to several hours depending on the voltage and length of gel used.\n\nThe fastest-migrating proteins (with a molecular weight of less than 5 KDa) form the buffer front together with the anionic components of the electrophoresis buffer, which also migrate through the gel. The area of the buffer front is made visible by adding the comparatively small, anionic dye bromophenol blue to the sample buffer. Due to the relatively small molecule size of bromophenol blue, it migrates faster than proteins. By optical control of the migrating colored band, the electrophoresis can be stopped before the dye and also the samples have completely migrated through the gel and leave it.\n\nThe most commonly used method is the discontinuous SDS-PAGE. In this method, the proteins migrate first into a collecting gel with neutral pH, in which they are concentrated and then they migrate into a separating gel with basic pH, in which the actual separation takes place. Stacking and separating gels differ by different pore size (4-6 % T and 10-20 % T), ionic strength and pH values (pH 6.8 or pH 8.8). The electrolyte most frequently used is an SDS-containing Tris-glycine-chloride buffer system. At neutral pH, glycine predominantly forms the zwitterionic form, at high pH the glycines lose positive charges and become predominantly anionic. In the collection gel, the smaller, negatively charged chloride ions migrate in front of the proteins (as leading ions) and the slightly larger, negatively and partially positively charged glycinate ions migrate behind the proteins (as initial trailing ions), whereas in the comparatively basic separating gel both ions migrate in front of the proteins. The pH gradient between the stacking and separation gel buffers leads to a stacking effect at the border of the stacking gel to the separation gel, since the glycinate partially loses its slowing positive charges as the pH increases and then, as the former trailing ion, overtakes the proteins and becomes a leading ion, which causes the bands of the different proteins (visible after a staining) to become narrower and sharper - the stacking effect. For the separation of smaller proteins and peptides, the TRIS-Tricine buffer system of Schägger and von Jagow is used due to the higher spread of the proteins in the range of 0.5 to 50 KDa.\n\nAt the end of the electrophoretic separation, all proteins are sorted by size and can then be analyzed by other methods, e. g. protein staining such as Coomassie staining (most common and easy to use), silver staining (highest sensitivity), stains all staining, Amido black 10B staining, Fast green FCF staining, fluorescent stains such as epicocconone stain and SYPRO orange stain, and immunological detection such as the Western Blot. The fluorescent dyes have a comparatively higher linearity between protein quantity and color intensity of about three orders of magnitude above the detection limit, i. e. the amount of protein can be estimated by color intensity. When using the fluorescent protein dye trichloroethanol, a subsequent protein staining is omitted if it was added to the gel solution and the gel was irradiated with UV light after electrophoresis.\n\nProtein staining in the gel creates a documentable banding pattern of the various proteins. Glycoproteins have differential levels of glycosylations and adsorb SDS more unevenly at the glycosylations, resulting in broader and blurred bands. Membrane proteins, because of their transmembrane domain, are often composed of the more hydrophobic amino acids, have lower solubility in aqueous solutions, tend to bind lipids, and tend to precipitate in aqueous solutions due to hydrophobic effects when sufficient amounts of detergent are not present. This precipitation manifests itself for membrane proteins in a SDS-PAGE in \"tailing\" above the band of the transmembrane protein. In this case, more SDS can be used (by using more or more concentrated sample buffer) and the amount of protein in the sample application can be reduced. An overloading of the gel with a soluble protein creates a semicircular band of this protein (e. g. in the marker lane of the image at 66 KDa), allowing other proteins with similar molecular weights to be covered. A low contrast (as in the marker lane of the image) between bands within a lane indicates either the presence of many proteins (low purity) or, if using purified proteins and a low contrast occurs only below one band, it indicates a proteolytic degradation of the protein, which first causes degradation bands, and after further degradation produces a homogeneous color (\"smear\") below a band. The documentation of the banding pattern is usually done by photographing or scanning. For a subsequent recovery of the molecules in individual bands, a gel extraction can be performed.\n\nAfter protein staining and documentation of the banding pattern, the polyacrylamide gel can be dried for archival storage. Proteins can be extracted from it at a later date. The gel is either placed in a drying frame (with or without the use of heat) or in a vacuum dryer. The drying frame consists of two parts, one of which serves as a base for a wet cellophane film to which the gel and a one percent glycerol solution are added. Then a second wet cellophane film is applied bubble-free, the second frame part is put on top and the frame is sealed with clips. The removal of the air bubbles avoids a fragmentation of the gel during drying. The water evaporates through the cellophane film. In contrast to the drying frame, a vacuum dryer generates a vacuum and heats the gel to about 50 °C.\n\nFor a more accurate determination of the molecular weight, the relative migration distances of the individual protein bands are measured in the separating gel. The measurements are usually performed in triplicate for increased accuracy. The relative mobility (called Rf value or Rm value) is the quotient of the distance of the band of the protein and the distance of the buffer front. The distances of the bands and the buffer front are each measured from the beginning of the separation gel. The distance of the buffer front roughly corresponds to the distance of the bromophenol blue contained in the sample buffer. The relative distances of the proteins of the size marker are plotted semi-logarithmically against their known molecular weights. By comparison with the linear part of the generated graph or by a regression analysis, the molecular weight of an unknown protein can be determined by its relative mobility. Bands of proteins with glycosylations can be blurred. Proteins with many basic amino acids (e. g. histones) can lead to an overestimation of the molecular weight or even not migrate into the gel at all, because they move slower in the electrophoresis due to the positive charges or even to the opposite direction. Accordingly, many acidic amino acids can lead to accelerated migration of a protein and an underestimation of its molecular mass.\n\nThe SDS-PAGE in combination with a protein stain is widely used in biochemistry for the quick and exact separation and subsequent analysis of proteins. It has comparatively low instrument and reagent costs and is an easy-to-use method. Because of its low scalability, it is mostly used for analytical purposes and less for preparative purposes, especially when larger amounts of a protein are to be isolated.\n\nAdditionally, SDS-PAGE is used in combination with the western blot for the determination of the presence of a specific protein in a mixture of proteins - or for the analysis of post-translational modifications. Post-translational modifications of proteins can lead to a different relative mobility (i.e. a \"band shift\") or to a change in the binding of a detection antibody used in the western blot (i.e. a band disappears or appears).\n\nIn mass spectrometry of proteins, SDS-PAGE is a widely used method for sample preparation prior to spectrometry, mostly using in-gel digestion. In regards to determining the molecular mass of a protein, the SDS-PAGE is a bit more exact than an analytical ultracentrifugation, but less exact than a mass spectrometry or - ignoring post-translational modifications - a calculation of the protein molecular mass from the DNA sequence.\n\nIn medical diagnostics, SDS-PAGE is used as part of the HIV test and to evaluate proteinuria. In the HIV test, HIV proteins are separated by SDS-PAGE and subsequently detected by Western Blot with HIV-specific antibodies of the patient, if they are present in his blood serum. SDS-PAGE for proteinuria evaluates the levels of various serum proteins in the urine, e.g. Albumin, Alpha-2-macroglobulin and IgG.\n\nSDS-PAGE is the most widely used method for gel electrophoretic separation of proteins. Two-dimensional gel electrophoresis sequentially combines isoelectric focusing or BAC-PAGE with a SDS-PAGE. Native PAGE is used if native protein folding is to be maintained. For separation of membrane proteins, BAC-PAGE or CTAB-PAGE may be used as an alternative to SDS-PAGE. For electrophoretic separation of larger protein complexes, agarose gel electrophoresis can be used, e.g. the SDD-AGE. Some enzymes can be detected via their enzyme activity by zymography.\n\nWhile being one of the more precise and low-cost protein separation and analysis methods, the SDS-PAGE denatures proteins. Where non-denaturing conditions are necessary, proteins are separated by a native PAGE or different chromatographic methods with subsequent photometric quantification, for example affinity chromatography (or even tandem affinity purification), size exclusion chromatography, ion exchange chromatography. Proteins can also be separated by size in a tangential flow filtration or a ultrafiltration. Single proteins can be isolated from a mixture by affinity chromatography or by a pull-down assay. Some historically early and cost effective but crude separation methods usually based upon a series of extractions and precipitations using kosmotropic molecules, for example the ammonium sulfate precipitation and the polyethyleneglycol precipitation.\n\nIn 1948, Arne Tiselius was awarded the Nobel Prize in Chemistry for the discovery of the principle of electrophoresis as the migration of charged and dissolved atoms or molecules in an electric field. The use of a solid matrix (initially paper discs) in a zone electrophoresis improved the separation. The discontinuous electrophoresis of 1964 by L. Ornstein and B. J. Davis made it possible to improve the separation by the stacking effect. The use of cross-linked polyacrylamide hydrogels, in contrast to the previously used paper discs or starch gels, provided a higher stability of the gel and no microbial decomposition. The denaturing effect of SDS in continuous polyacrylamide gels and the consequent improvement in resolution was first described in 1965 by David F. Summers in the working group of James E. Darnell to separate poliovirus proteins. The current variant of the SDS-PAGE was described in 1970 by Ulrich K. Laemmli and initially used to characterise the proteins in the head of bacteriophage T4.\n\n"}
{"id": "23499848", "url": "https://en.wikipedia.org/wiki?curid=23499848", "title": "Secondary source", "text": "Secondary source\n\nIn scholarship, a secondary source is a document or recording that relates or discusses information originally presented elsewhere. A secondary source contrasts with a primary source, which is an original source of the information being discussed; a primary source can be a person with direct knowledge of a situation, or a document created by such a person. \n\nA secondary source is one that gives information about a primary source. In this source, the original information is selected, modified and arranged in a suitable format. Secondary sources involve generalization, analysis, interpretation, or evaluation of the original information. \nThe most accurate classification for any given source is not always obvious. \"Primary\" and \"secondary\" are relative terms, and some sources may be classified as primary or secondary, depending on how they are used. A third level, the tertiary source, such as an encyclopedia or dictionary, resembles a secondary source in that it contains analysis, but attempts to provide a broad introductory overview of a topic.\n\nInformation can be taken from a wide variety of objects, but this classification system is only useful for a class of sources that are called symbolic sources. Symbolic sources are sources that are intended to communicate information to someone. Common symbolic sources include written documents such as letters and notes, but not, for example, bits of broken pottery and scraps of food excavated from a midden, regardless of how much information can be extracted from an ancient trash heap, or how little can be extracted from a written document. \n\nMany sources can be considered either primary or secondary, depending on the context in which they are used. Moreover, the distinction between \"primary\" and \"secondary\" sources is subjective and contextual, so that precise definitions are difficult to make. For example, if a historical text discusses old documents to derive a new historical conclusion, it is considered to be a primary source for the new conclusion, but a secondary source of information found in the old documents. Other examples in which a source can be both primary and secondary include an obituary or a survey of several volumes of a journal counting the frequency of articles on a certain topic.\n\nWhether a source is regarded as primary or secondary in a given context may change, depending upon the present state of knowledge within the field. For example, if a document refers to the contents of a previous but undiscovered letter, that document may be considered \"primary\", since it is the closest known thing to an original source, but if the letter is later found, it may then be considered \"secondary\".\n\nAttempts to map or model scientific and scholarly communication need the concepts of primary, secondary and further \"levels\". One such model is the UNISIST model of information dissemination. Within such a model these concepts are defined in relation to each other, and the acceptance of this way of defining the concepts are connected to the acceptance of the model.\n\nSome other modern languages use more than one word for the English word \"source\". German usually uses \"Sekundärliteratur\" (\"secondary literature\") for secondary sources for historical facts, leaving \"Sekundärquelle\" (\"secondary source\") to historiography. A \"Sekundärquelle\" is a source which can tell about a lost \"Primärquelle\" (\"primary source\"), such as a letter quoting from minutes which are no longer known to exist, and so cannot be consulted by the historian.\n\nIn general, secondary sources are self-described as review articles or meta-analysis.\n\nPrimary source materials are typically defined as \"original research papers written by the scientists who actually conducted the study.\" An example of primary source material is the Purpose, Methods, Results, Conclusions sections of a research paper (in IMRAD style) in a scientific journal by the authors who conducted the study. In some fields, a secondary source may include a summary of the literature in the Introduction of a scientific paper, a description of what is known about a disease or treatment in a chapter in a reference book, or a synthesis written to review available literature. A survey of previous work in the field in a primary peer-reviewed source is secondary source information. This allows secondary sourcing of recent findings in areas where full review articles have not yet been published.\n\nA book review that contains the judgment of the reviewer about the book is a primary source for the reviewer's opinion, and a secondary source for the contents of the book. A summary of the book within a review is a secondary source.\n\nIn library and information sciences, secondary sources are generally regarded as those sources that summarize or add commentary to primary sources in the context of the particular information or idea under study.\n\nAn important use of secondary sources in the field of mathematics has been to make difficult mathematical ideas and proofs from primary sources more accessible to the public; in other sciences tertiary sources are expected to fulfill the introductory role.\n\nSecondary sources in history and humanities are usually books or scholarly journals, from the perspective of a later interpreter, especially by a later scholar. In the humanities, a peer reviewed article is always a secondary source.\nThe delineation of sources as primary and secondary first arose in the field of historiography, as historians attempted to identify and classify the sources of historical writing. In scholarly writing, an important objective of classifying sources is to determine the independence and reliability of sources. In original scholarly writing, historians rely on primary sources, read in the context of the scholarly interpretations.\n\nFollowing the Rankean model established by German scholarship in the 19th century, historians use archives of primary sources. Most undergraduate research projects rely on secondary source material, with perhaps snippets of primary sources.\n\nIn the legal field, source classification is important because the persuasiveness of a source usually depends upon its history. Primary sources may include cases, constitutions, statutes, administrative regulations, and other sources of binding legal authority, while secondary legal sources may include books, the headnotes of case reports, articles, and encyclopedias. Legal writers usually prefer to cite primary sources because only primary sources are authoritative and precedential, while secondary sources are only persuasive at best.\n\n\"A secondary source is a record or statement of an event or circumstance made by a non-eyewitness or by someone not closely connected with the event or circumstances, recorded or stated verbally either at or sometime after the event, or by an eye-witness at a time after the event when the fallibility of memory is an important factor.\" Consequently, according to this definition, a first-hand account written long after the event \"when the fallibility of memory is an important factor\" is a secondary source, even though it may be the first published description of that event.\n\nAn autobiography can be a secondary source in history or the humanities when used for information about topics other than its subject. For example, many first hand accounts of events in World War I written in the post-war years were influenced by the then prevailing perception of the war which was significantly different from contemporary opinion.\n\n\n"}
{"id": "2636884", "url": "https://en.wikipedia.org/wiki?curid=2636884", "title": "Source criticism", "text": "Source criticism\n\nSource criticism (or information evaluation) is the process of evaluating an information source, i.e. a document, a person, a speech, a fingerprint, a photo, an observation, or anything used in order to obtain knowledge. In relation to a given purpose, a given information source may be more or less valid, reliable or relevant. Broadly, \"source criticism\" is the interdisciplinary study of how information sources are evaluated for given tasks.\n\nProblems in translation: The Danish word \"kildekritik\" like the Norwegian word \"kildekritikk\" and the Swedish word \"källkritik\" derived from the German \"Quellenkritik\" and is closely associated with the German historian Leopold von Ranke (1795–1886). Hardtwig writes: \"His [Ranke's] first work \"Geschichte der romanischen und germanischen Völker\" von 1494–1514 (History of the Latin and Teutonic Nations from 1494 to 1514) (1824) was a great success. It already showed some of the basic characteristics of his conception of Europe, and was of historiographical importance particularly because Ranke made an exemplary critical analysis of his sources in a separate volume, \"Zur Kritik neuerer Geschichtsschreiber\" (On the Critical Methods of Recent Historians). In this work he raised the method of textual criticism used in the late eighteenth century, particularly in classical philology to the standard method of scientific historical writing\" (Hardtwig, 2001, p. 12739).\nThe larger part of the nineteenth and twentieth\ncenturies would be dominated by the research-oriented\nconception of historical method of the so-called\nHistorical School in Germany, led by historians as\nLeopold Ranke and Berthold Niebuhr. Their conception\nof history, long been regarded as the beginning\nof modern, 'scientific' history, harked back to the\n'narrow' conception of historical method, limiting the\nmethodical character of history to source criticism\" (Lorenz, 2001).\nBible studies dominate the use of \"source criticism\" in America (cf. Hjørland, 2008). The term is thus relatively seldom used in English about historical methods and historiography (cf. Hjørland, 2008). This difference between European and American use of \"source criticism\" is somewhat strange considering the influence of Ranke on both sides of the Atlantic Ocean. It has been suggested that differences in the use of the term are not accidental but due to different views of the historical method. In the German/Scandinavian tradition this subject is seen as important, whereas in the Anglo-American tradition it is believed that historical methods must be specific and associated with the subject studied, for which reason there is no general field of \"source criticism\".\n\nIn the Scandinavian countries and elsewhere source evaluation (or information evaluation) is also studied interdisciplinarily from many different points of view, partly caused by the influence of the Internet. It is a growing field in, among other fields, library and information science. In this context source criticism is studied from a broader perspective than just, for example, history or biblical studies.\n\nThe following principles are cited from two Scandinavian textbooks on source criticism, Olden-Jørgensen (1998) and Thurén (1997) written by historians:\n\nWe may add the following principles:\n\n\"Because each source teaches you more and more about your subject, you will be able to judge with ever-increasing precision the usefulness and value of any prospective source. In other words, the more you know about the subject, the more precisely you can identify what you must still find out\". (Bazerman, 1995, p. 304).\n\"The empirical case study showed that most people find it difficult to assess questions of cognitive authority and media credibility in a general sense, for example, by comparing the overall credibility of newspapers and the Internet. Thus these assessments tend to be situationally sensitive. Newspapers, television and the Internet were frequently used as sources of orienting information, but their credibility varied depending on the actual topic at hand\" (Savolainen, 2007).\nThe following questions are often good ones to ask about any source according to the American Library Association (1994) and Engeldinger (1988):\n\n\nFor literary sources we might add complementing criteria:\n\n\nHow general are principles of source criticism?\nSome principles are universal, other principles are specific for certain kinds of information sources. One may ask whether principles of source criticism are unique to the humanities?\n\nThere is today no consensus about the similarities and differences between natural science and humanities. Logical positivism claimed that all fields of knowledge were based on the same principles. Much of the criticism of logical positivism claimed that positivism is the basis of the sciences, whereas hermeneutics is the basis of the humanities. This was, for example, the position of Jürgen Habermas. A newer position, in accordance with, among others, Hans-Georg Gadamer and Thomas Kuhn understands both science and humanities as determined by researchers' preunderstanding and paradigms. Hermeneutics is thus a universal theory. The difference is, however, that the sources of the humanities are themselves products of human interests and preunderstanding, whereas the sources of the natural sciences are not. Humanities are thus \"doubly hermeneutic\".\n\nNatural scientists, however, are also using human products (such as scientific papers) which are products of preunderstanding (and, for example, academic fraud).\n\nEpistemological theories are the basic theories about how knowledge is obtained and thus the most general theories about how to evaluate information sources. Empiricism evaluates sources by considering the observations (or sensations) on which they are based. Sources without basis in experience are not seen as valid. Rationalism provides low priority to sources based on observations. In order to be meaningful observations must be grasped by clear ideas or concepts. It is the logical structure and the well definedness that is in focus in evaluating information sources from the rationalist point of view. Historicism evaluates information sources on the basis of their reflection of their sociocultural context and their theoretical development. Pragmatism evaluate sources on the basis of how their values and usefulness to accomplish certain outcomes. Pragmatism is skeptical about claimed neutral information sources.\n\nThe evaluation of knowledge or information sources cannot be more certain than is the construction of knowledge. If we accept the principle of fallibilism we also have to accept that source criticism can never 100% verify knowledge claims. As discussed in the next section is source criticism intimately linked to scientific methods.\n\nThe presence of fallacies of argument in sources is another kind of philosophical criteria for evaluating sources. Fallacies are presented by Walton (1998). Among the fallacies are the 'ad hominem fallacy' (the use of personal attack to try to undermine or refute a person's argument) and the 'straw man fallacy' (when one arguer misrepresents another's position to make it appear less plausible than it really is, in order more easily to criticize or refute it.) See also fallacy.\n\nResearch methods are methods used to produce scholarly knowledge. The methods that are relevant for producing knowledge are also relevant for evaluating knowledge. An example of a book that turns methodology upside-down and uses it to evaluate produced knowledge is Katzer; Cook & Crouch (1998). See also Unobtrusive measures, Triangulation (social science).\n\nStudies of quality evaluation processes such as peer review, book reviews and of the normative criteria used in evaluation of scientific and scholarly research. Another field is the study of scientific misconduct.\n\nHarris (1979) provides a case study of how a famous experiment in psychology, Little Albert, has been distorted throughout the history of psychology, starting with the author (Watson) himself, general textbook authors, behavior therapists, and a prominent learning theorist. Harris proposes possible causes for these distortions and analyzes the Albert study as an example of myth making in the history of psychology. Studies of this kind may be regarded a special kind of reception history (how Watson's paper was received). It may also be regarded as a kind of critical history (opposed to ceremonial history of psychology, cf. Harris, 1980). Such studies are important for source criticism in revealing the bias introduced by referring to classical studies.\n\nSee also Hjørland (2008): Empirical studies of the quality of science.\n\nTextual criticism (or broader: text philology) is a part of philology, which is not just devoted to the study of texts, but also to edit and produce \"scientific editions\", \"scholarly editions\", \"standard editions\", \"historical editions\", \"reliable editions\", \"reliable texts\", \"text editions\" or \"critical editions\", which are editions in which careful scholarship has been employed to ensure that the information contained within is as close to the author's/composer's original intentions as possible (and which allows the user to compare and judge changes in editions published under influence by the author/composer). The relation between these kinds of works and the concept \"source criticism\" is evident in Danish, where they may be termed \"kildekritisk udgave\" (directly translated \"source critical edition\").\n\nIn other words, it is assumed that most editions of a given works is filled with noise and errors provided by publishers, why it is important to produce \"scholarly editions\". The work provided by text philology is an important part of source criticism in the humanities.\n\n\ncomplete works and monumental editions\n\nThe study of eyewitness testimony is an important field of study used, among other purposes, to evaluate testimony in courts. The basics of eyewitness fallibility includes factors such as poor viewing conditions, brief exposure, and stress. More subtle factors, such as expectations, biases, and personal stereotypes can intervene to create erroneous reports. Loftus (1996) discuss all such factors and also shows that eyewitness memory is chronically inaccurate in surprising ways. An ingenious series of experiments reveals that memory can be radically altered by the way an eyewitness is questioned after the fact. New memories can be implanted and old ones unconsciously altered under interrogation.\n\nAnderson (1978) and Anderson & Pichert (1977) reported an elegant experiment demonstrating how change in perspective affected people's ability to recall information that was unrecallable from another perspective.\n\nIn psychoanalysis the concept of defence mechanism is important and may be considered a contribution to the theory of source criticism because it explains psychological mechanisms, which distort the reliability of human information sources.\n\nIn schools of library and information science (LIS) is source criticism of taught as part of the growing field Information literacy.\nStudy issues like relevance, quality indicators for documents, kinds of documents and their qualities (e.g. scholarly editions) and related issues are studied in LIS and are relevant for source criticism. Bibliometrics is often used to find the most influential journal, authors, countries and institutions. The study of book reviews and their function in evaluating books should also be mentioned. The well-known comparison of Wikipedia and Encyclopædia Britannica (Giles, 2005) - although not done by information scientists - contained an interview with an information scientist (Michael Twidale) and should be obvious to include in LIS.\n\nIt could be argued that library and information education should provide teaching in source criticism at least at the same level as is taught in Upper Secondary School (see Gudmundsson, 2007).\n\nIn library and information science the checklist approach has often been used. A criticism of this approach is given by Meola (2004): \"Chucking the checklist\".\n\nLibraries sometimes provide advice on how their users may evaluate sources.\n\nThe Library of Congress has a \"Teaching with Primary Sources\" (TPS) program.\n\nSource criticism is also about ethical behavior and culture. It is about a free press and an open society, including the protecting information sources from being persecuted (cf., Whistleblower).\n\nPhotos are often manipulated during wars and for political purposes. One well known example is Joseph Stalin's manipulation of a photograph from May 5, 1920 on which Stalin's predecessor Lenin held a speech for Soviet troops that Leon Trotsky attended. Stalin had later Trotsky retouched out of this photograph. (cf. King, 1997). A recent example is reported by Healy (2008) about North Korean leader Kim Jong Il.\n\nMuch interest in evaluating Internet sources (such as Wikipedia) is reflected in the scholarly literature of Library and information science and in other fields. Mintz (2002) is an edited volume about this issue. Examples of literature examining Internet sources include Chesney (2006), Fritch & Cromwell (2001), Leth & Thurén (2000) and Wilkinson, Bennett, & Oliver (1997).\n\n\"In history, the term historical method was first introduced in a systematic way in the sixteenth century by Jean Bodin in his treatise of source criticism, \"Methodus ad facilem historiarium cognitionem\" (1566). Characteristically, Bodin's treatise intended to establish the ways by which reliable knowledge of the past could be established by checking sources against one another and by so assessing the reliability of the information conveyed by them, relating them to the interests involved.\" (Lorenz, 2001, p. 6870).\n\nAs written above, modern source criticism in history is closely associated with the German historian Leopold von Ranke (1795–1886), who influenced historical methods on both sides of the Atlantic Ocean, although in rather different ways. American history developed in a more empirist and antiphilosophical way (cf., Novick, 1988).\n\nTwo of the best-known rule books from History's childhood are Bernheim (1889) and Langlois & Seignobos (1898). These books provided a seven-step procedure (here quoted from Howell & Prevenier, 2001, p. 70-71):\n\nGudmundsson (2007, p. 38) writes: \"Source criticism should not totally dominate later courses. Other important perspectives, for example, philosophy of history/view of history, should not suffer by being neglected\" (Translated by BH). This quote makes a distinction between source criticism on the one hand and historical philosophy on the other hand. However, different views of history and different specific theories about the field being studied may have important consequences for how sources are selected, interpreted and used. Feminist scholars may, for example, select sources made by women and may interpret sources from a feminist perspective. Epistemology should thus be considered a part of source criticism. It is in particular related to \"tendency analysis\".\n\nIn archaeology, radiocarbon dating is an important technique to establish the age of information sources. Methods of this kind were the ideal when history established itself as both a scientific discipline and as a profession based on \"scientific\" principles in the last part of the 1880s (although radiocarbon dating is a more recent example of such methods). The empiricist movement in history brought along both \"source criticism\" as a research method and also in many countries large scale publishing efforts to make valid editions of \"source materials\" such as important letters and official documents (e.g. as facsimiles or transcriptions).\n\nHistoriography and Historical method include the study of the reliability of the sources used, in terms of, for example, authorship, credibility of the author, and the authenticity or corruption of the text.\n\nSource criticism, as the term is used in biblical criticism, refers to the attempt to establish the sources used by the author and/or redactor of the final text. The term \"literary criticism\" is occasionally used as a synonym.\n\nBiblical source criticism originated in the 18th century with the work of Jean Astruc, who adapted the methods already developed for investigating the texts of Classical antiquity (Homer's Iliad in particular) to his own investigation into the sources of the Book of Genesis. It was subsequently considerably developed by German scholars in what was known as \"the Higher Criticism\", a term no longer in widespread use. The ultimate aim of these scholars was to reconstruct the history of the biblical text, as well as the religious history of ancient Israel.\n\nRelated to Source Criticism is Redaction Criticism which seeks to determine how and why the redactor (editor) put the sources together the way he did. Also related is form criticism and tradition history which try to reconstruct the oral prehistory behind the identified written sources.\n\nJournalists often work with strong time pressure and have access to only a limited number of information sources such as news bureaus, persons which may be interviewed, newspapers, journals and so on (see journalism sourcing). Journalists' possibility for conducting serious source criticism is thus limited compared to, for example, historians' possibilities.\n\nThe most important legal sources are created by parliaments, governments, courts, and legal researchers. They may be written or informal and based on established practices. Views concerning the quality of sources differ among legal philosophies: Legal positivism is the view that the text of the law should be considered in isolation, while legal realism, interpretivism (legal), critical legal studies and feminist legal criticism interprets the law on a broader cultural basis.\n\n\n"}
{"id": "4159251", "url": "https://en.wikipedia.org/wiki?curid=4159251", "title": "Source text", "text": "Source text\n\nA source text is a text (sometimes oral) from which information or ideas are derived. In translation, a source text is the original text that is to be translated into another language.\n\nIn historiography, distinctions are commonly made between three kinds of source texts:\n\nPrimary sources are firsthand written evidence of history made at the time of the event by someone who was present. They have been described as those sources closest to the origin of the information or idea under study. These types of sources have been said to provide researchers with \"direct, unmediated information about the object of study.\" Primary sources are sources which, usually, are recorded by someone who participated in, witnessed, or lived through the event. These are also usually authoritative and fundamental documents concerning the subject under consideration. This includes published original accounts, published original works, or published original research. They may contain original research or new information not previously published elsewhere. They have been distinguished from secondary sources, which often cite, comment on, or build upon primary sources. They serve as an original source of information or new ideas about the topic. \"Primary\" and \"secondary\", however, are relative terms, and any given source may be classified as primary or secondary, depending on how it is used. Physical objects can be primary sources.\n\nSecondary sources are written accounts of history based upon the evidence from primary sources. These are sources which, usually, are accounts, works, or research that analyze, assimilate, evaluate, interpret, and/or synthesize primary sources. These are not as authoritative and are supplemental documents concerning the subject under consideration. These documents or people summarize other material, usually primary source material. They are academics, journalists, and other researchers, and the papers and books they produce. This includes published accounts, published works, or published research. For example a history book drawing upon diary and newspaper records. \n\nTertiary sources are compilations based upon primary and secondary sources. These are sources which, on average, do not fall into the above two levels. They consist of generalized research of a specific subject under consideration. Tertiary sources are analyzed, assimilated, evaluated, interpreted, and/or synthesized from secondary sources, also. These are not authoritative and are just supplemental documents concerning the subject under consideration. These are often meant to present known information in a convenient form with no claim to originality. Common examples are encyclopedias and textbooks.\n\nThe distinction between \"primary source\" and \"secondary source\" is standard in historiography, while the distinction between these sources and \"tertiary sources\" is more peripheral, and is more relevant to the scholarly research work than to the published content itself.\n\nBelow are types of sources that most generally, but not absolutely, fall into a certain level. The letters after an item describes \"generally\" the type it is (though this can vary pending the exact source). \"P\" is for Primary sources, \"S\" is for Secondary sources, and \"T\" is for Tertiary sources. (ed., those with \"?\"s are indeterminate.)\nIn translation, a source text (ST) is a text written in a given source language which is to be, or has been, translated into another language. In translation the source text (ST) is transformed into a target text (TT), written in a given target language. According to Jeremy Munday's definition of translation, \"the process of translation between two different written languages involves the changing of an original written text (the source text or ST) in the original verbal language (the source language or SL) into a written text (the target text or TT) in a different verbal language (the target language or TL)\". \n\nTranslation scholars including Eugene Nida and Peter Newmark have represented the different approaches to translation as falling broadly into source-text-oriented or target-text-oriented categories.\n"}
{"id": "40849944", "url": "https://en.wikipedia.org/wiki?curid=40849944", "title": "Sources for the historicity of Jesus", "text": "Sources for the historicity of Jesus\n\nChristian sources, such as the New Testament books in the Christian Bible, include detailed stories about Jesus but scholars differ on the historicity of specific episodes described in the Biblical accounts of Jesus. The only two events subject to \"almost universal assent\" are that Jesus was baptized by John the Baptist and was crucified by the order of the Roman Prefect Pontius Pilate.\n\nNon-Christian sources that are used to study and establish the historicity of Jesus include Jewish sources such as Josephus, and Roman sources such as Tacitus. These sources are compared to Christian sources such as the Pauline Epistles and the Synoptic Gospels. These sources are usually independent of each other (e.g. Jewish sources do not draw upon Roman sources), and similarities and differences between them are used in the authentication process.\n\nIn a review of the state of research, the Jewish scholar Amy-Jill Levine stated that \"no single picture of Jesus has convinced all, or even most scholars\" and that all portraits of Jesus are subject to criticism by some group of scholars.\n\nThe writings of the 1st century Romano-Jewish historian Flavius Josephus include references to Jesus and the origins of Christianity. Josephus' \"Antiquities of the Jews\", written around 93–94 CE, includes two references to Jesus in Books and .\n\nOf the two passages, the James passage in Book 20 is used by scholars to support the existence of Jesus, the \"Testimonium Flavianum\" in Book 18 his crucifixion. Josephus' James passage attests to the existence of Jesus as a historical person and that some of his contemporaries considered him the Messiah. According to Bart Ehrman, Josephus' passage about Jesus was altered by a Christian scribe, including the reference to Jesus as the Messiah.\n\nA textual argument against the authenticity of the James passage is that the use of the term \"Christos\" there seems unusual for Josephus. An argument based on the flow of the text in the document is that, given that the mention of Jesus appears in the \"Antiquities\" before that of the John the Baptist, a Christian interpolator may have inserted it to place Jesus in the text before John. A further argument against the authenticity of the James passage is that it would have read well even without a reference to Jesus.\n\nThe passage deals with the death of \"James the brother of Jesus\" in Jerusalem. Whereas the works of Josephus refer to at least twenty different people with the name Jesus, this passage specifies that this Jesus was the one \"who was called Christ\". Louis Feldman states that this passage, above others, indicates that Josephus did say something about Jesus.\n\nModern scholarship has almost universally acknowledged the authenticity of the reference in of the \"Antiquities\" to \"the brother of Jesus, who was called Christ, whose name was James\", and considers it as having the highest level of authenticity among the references of Josephus to Christianity.\n\nThe \"Testimonium Flavianum\" (meaning the testimony of Flavius [Josephus]) is the name given to the passage found in of the \"Antiquities\" in which Josephus describes the condemnation and crucifixion of Jesus at the hands of the Roman authorities. Scholars have differing opinions on the total or partial authenticity of the reference in the passage to the execution of Jesus by Pontius Pilate. The general scholarly view is that while the \"Testimonium Flavianum\" is most likely not authentic in its entirety, it is broadly agreed upon that it originally consisted of an authentic nucleus with a reference to the execution of Jesus by Pilate which was then subject to Christian interpolation. Although the exact nature and extent of the Christian redaction remains unclear, there is broad consensus as to what the original text of the \"Testimonium\" by Josephus would have looked like.\n\nThe references found in \"Antiquities\" have no parallel texts in the other work by Josephus such as the \"Jewish War\", written twenty years earlier, but some scholars have provided explanations for their absence, such as that the \"Antiquities\" covers a longer time period and that during the twenty-year gap between the writing of the \"Jewish Wars\" (c. 70 CE) and \"Antiquities\" (after 90 CE) Christians had become more important in Rome and were hence given attention in the \"Antiquities\".\n\nA number of variations exist between the statements by Josephus regarding the deaths of James and the New Testament accounts. Scholars generally view these variations as indications that the Josephus passages are not interpolations, because a Christian interpolator would more likely have made them correspond to the Christian traditions. Robert Eisenman provides numerous early Christian sources that confirm the Josephus testament, that James was the brother of Jesus.\n\nThe Roman historian and senator Tacitus referred to Christ, his execution by Pontius Pilate and the existence of early Christians in Rome in his final work, \"Annals\" (c. AD 116), . The relevant passage reads: \"called Christians by the populace. Christus, from whom the name had its origin, suffered the extreme penalty during the reign of Tiberius at the hands of one of our procurators, Pontius Pilatus.\"\n\nScholars generally consider Tacitus's reference to the execution of Jesus by Pontius Pilate to be both authentic, and of historical value as an independent Roman source about early Christianity that is in unison with other historical records. William L. Portier has stated that the consistency in the references by Tacitus, Josephus and the letters to Emperor Trajan by Pliny the Younger reaffirm the validity of all three accounts.\n\nTacitus was a patriotic Roman senator and his writings shows no sympathy towards Christians. Andreas Köstenberger and separately Robert E. Van Voorst state that the tone of the passage towards Christians is far too negative to have been authored by a Christian scribe – a conclusion shared by John P. Meier Robert E. Van Voorst states that \"of all Roman writers, Tacitus gives us the most precise information about Christ\".\n\nJohn Dominic Crossan considers the passage important in establishing that Jesus existed and was crucified, and states: \"That he was crucified is as sure as anything historical can ever be, since both Josephus and Tacitus... agree with the Christian accounts on at least that basic fact.\" Bart D. Ehrman states: \"Tacitus's report confirms what we know from other sources, that Jesus was executed by order of the Roman governor of Judea, Pontius Pilate, sometime during Tiberius's reign.\" Eddy and Boyd state that it is now \"firmly established\" that Tacitus provides a non-Christian confirmation of the crucifixion of Jesus.\n\nAlthough the majority of scholars consider it to be genuine, a few scholars question the authenticity of the passage given that Tacitus was born 25 years after Jesus' death.\n\nSome scholars have debated the historical value of the passage given that Tacitus does not reveal the source of his information. Gerd Theissen and Annette Merz argue that Tacitus at times had drawn on earlier historical works now lost to us, and he may have used official sources from a Roman archive in this case; however, if Tacitus had been copying from an official source, some scholars would expect him to have labeled Pilate correctly as a \"prefect\" rather than a \"procurator\". Theissen and Merz state that Tacitus gives us a description of widespread prejudices about Christianity and a few precise details about \"Christus\" and Christianity, the source of which remains unclear. However, Paul R. Eddy has stated that given his position as a senator Tacitus was also likely to have had access to official Roman documents of the time and did not need other sources.\n\nMichael Martin notes that the authenticity of this passage of the Annals has also been disputed on the grounds that Tacitus would not have used the word “messiah” in an authentic Roman document.\n\nWeaver notes that Tacitus spoke of the persecution of Christians, but no other Christian author wrote of this persecution for a hundred years.\n\nHotema notes that this passage was not quoted by any Church father up to the 15th century, although the passage would have been very useful to them in their work; and that the passage refers to the Christians in Rome being a multitude, while at that time the Christian congregation in Rome would actually have been very small.\n\nRichard Carrier has put forward the ideas that the 'Christ, the author of this name, was executed by the procurator Pontius Pilate in the reign of Tiberius' line is a Christian interpolation and that Tacitus wrote about Chrestians not Christians.\n\nScholars have also debated the issue of hearsay in the reference by Tacitus. Charles Guignebert argued that \"So long as there is that possibility [that Tacitus is merely echoing what Christians themselves were saying], the passage remains quite worthless\". R. T. France states that the Tacitus passage is at best just Tacitus repeating what he had heard through Christians. However, Paul R. Eddy has stated that as Rome's preeminent historian, Tacitus was generally known for checking his sources and was not in the habit of reporting gossip. Tacitus was a member of the Quindecimviri sacris faciundis, a council of priests whose duty it was to supervise foreign religious cults in Rome, which as Van Voorst points out, makes it reasonable to suppose that he would have acquired knowledge of Christian origins through his work with that body.\n\nMara (son of Sarapion) was a Stoic philosopher from the Roman province of Syria. Sometime between 73 AD and the 3rd century, Mara wrote a letter to his son (also called Sarapion) which may contain an early non-Christian reference to the crucifixion of Jesus.\n\nThe letter refers to the unjust treatment of \"three wise men\": the murder of Socrates, the burning of Pythagoras, and the execution of \"the wise king\" of the Jews. The author explains that in all three cases the wrongdoing resulted in the future punishment of those responsible by God and that when the wise are oppressed, not only does their wisdom triumph in the end, but God punishes their oppressors.\n\nThe letter includes no Christian themes and the author is presumed to be a pagan. Some scholars see the reference to the execution of the \"wise king\" of the Jews as an early non-Christian reference to Jesus. Criteria that support the non-Christian origin of the letter include the observation that \"king of the Jews\" was not a Christian title, and that the letter's premise that Jesus lives on based on the wisdom of his teachings is in contrast to the Christian concept that Jesus continues to live through his resurrection.\n\nScholars such as Robert Van Voorst see little doubt that the reference to the execution of the \"king of the Jews\" is about the death of Jesus. Others such as Craig A. Evans see less value in the letter, given its uncertain date, and the possible ambiguity in the reference.\n\nThe Roman historian Suetonius (c. 69 – after 122 CE) made references to early Christians and their leader in his work \"Lives of the Twelve Caesars\" (written 121 CE). The references appear in and which describe the lives of Roman Emperors Claudius and Nero. The Nero 16 passage refers to the abuses by Nero and mentions how he inflicted punishment on Christians – which is generally dated to around AD 64. This passage shows the clear contempt of Suetonius for Christians - the same contempt expressed by Tacitus and Pliny the younger in their writings, but does not refer to Jesus himself.\n\nThe earlier passage in Claudius, may include a reference to Jesus, but is subject to debate among scholars. In Suetonius refers to the expulsion of Jews by Claudius and states:\n\nThe reference in Claudius 25 involves the agitations in the Jewish community which led to the expulsion of some Jews from Rome by Claudius, and is likely the same event mentioned in the Acts of the Apostles (). Most historians date this expulsion to around AD 49–50. Suetonius refers to the leader of the Christians as \"Chrestus\", a term also used by used by Tacitus, referred in Latin dictionaries as a (amongst other things) version of 'Christus'. However, the wording used by Suetonius implies that Chrestus was alive at the time of the disturbance and was agitating the Jews in Rome. This weakens the historical value of his reference as a whole, and there is no overall scholarly agreement about its value as a reference to Jesus. However, the confusion of Suetonius also points to the lack of Christian interpolation, for a Christian scribe would not have confused the Jews with Christians.\n\nMost scholars assume that in the reference Jesus is meant and that the disturbances mentioned were due to the spread of Christianity in Rome. However, scholars are divided on the value of the Suetonius' reference. Some scholars such as Craig A. Evans, John Meier and Craig S. Keener see it as a likely reference to Jesus. Others such as Stephen Benko and H. Dixon Slingerland see it as having little or no historical value.\n\nMenahem Stern states Suetonius definitely was referring to Jesus; because he would have added \"a certain\" to Chrestus if he had meant some unknown agitator.\n\nThe Babylonian Talmud in a few cases includes possible references to Jesus using the terms \"Yeshu\", \"Yeshu ha-Notzri\", \"ben Stada\", and \"ben Pandera\". Some of these references probably date back to the Tannaitic period (70–200 CE). In some cases, it is not clear if the references are to Jesus, or other people, and scholars continue to debate their historical value, and exactly which references, if any, may be to Jesus.\n\nRobert Van Voorst states that the scarcity of Jewish references to Jesus is not surprising, given that Jesus was not a prominent issue for the Jews during the first century, and after the devastation caused by the Siege of Jerusalem in the year 70, Jewish scholars were focusing on preserving Judaism itself, rather than paying much attention to Christianity.\n\nRobert Eisenman argues that the derivation of Jesus of Nazareth from \"ha-Notzri\" is impossible on etymological grounds, as it would suggest rather \"the Nazirite\" rather than \"the Nazarene\".\n\nVan Voorst states that although the question of who was referred to in various points in the Talmud remains subject to debate among scholars, in the case of \"Sanhedrin 43a\" (generally considered the most important reference to Jesus in rabbinic literature), Jesus can be confirmed as the subject of the passage, not only from the reference itself, but from the context that surrounds it, and there is little doubt that it refers to the death of Jesus of Nazareth. Christopher M. Tuckett states that if it is accepted that death narrative of Sanhedrin 43a refers to Jesus of Nazareth then it provides evidence of Jesus' existence and execution.\n\nAndreas Kostenberger states that the passage is a Tannaitic reference to the trial and death of Jesus at Passover and is most likely earlier than other references to Jesus in the Talmud. The passage reflects hostility toward Jesus among the rabbis and includes this text:\n\nIt is taught: On the eve of Passover they hung Yeshu and the crier went forth for forty days beforehand declaring that \"[Yeshu] is going to be stoned for practicing witchcraft, for enticing and leading Israel astray. Anyone who knows something to clear him should come forth and exonerate him.\" But no one had anything exonerating for him and they hung him on the eve of Passover. \n\nPeter Schäfer states that there can be no doubt that the narrative of the execution of Jesus in the Talmud refers to Jesus of Nazareth, but states that the rabbinic literature in question are not Tannaitic but from a later Amoraic period and may have drawn on the Christian gospels, and may have been written as responses to them. Bart Ehrman and separately Mark Allan Powell state that given that the Talmud references are quite late, they can give no historically reliable information about the teachings or actions of Jesus during his life.\n\nAnother reference in early second century Rabbinic literature (Tosefta Hullin II 22) refers to Rabbi Eleazar ben Dama who was bitten by a snake, but was denied healing in the name of Jesus by another Rabbi for it was against the law, and thus died. This passage reflects the attitude of Jesus' early Jewish opponents, i.e. that his miracles were based on evil powers.\n\nEddy and Boyd, who question the value of several of the Talmudic references state that the significance of the Talmud to historical Jesus research is that it never denies the existence of Jesus, but accuses him of sorcery, thus indirectly confirming his existence. R. T. France and separately Edgar V. McKnight state that the divergence of the Talmud statements from the Christian accounts and their negative nature indicate that they are about a person who existed. Craig Blomberg states that the denial of the existence of Jesus was never part of the Jewish tradition, which instead accused him of being a sorcerer and magician, as also reflected in other sources such as Celsus. Andreas Kostenberger states that the overall conclusion that can be drawn from the references in the Talmud is that Jesus was a historical person whose existence was never denied by the Jewish tradition, which instead focused on discrediting him.\n\nPliny the Younger (c. 61 – c. 112), the provincial governor of Pontus and Bithynia, wrote to Emperor Trajan \"c\". 112 concerning how to deal with Christians, who refused to worship the emperor, and instead worshiped \"Christus\". Charles Guignebert, who does not doubt that Jesus of the Gospels lived in Gallilee in the 1st century, nevertheless dismisses this letter as acceptable evidence for a historical Jesus.\n\nThallus, of whom very little is known, and none of whose writings survive, wrote a history allegedly around the middle to late first century CE, to which Eusebius referred. Julius Africanus, writing \"c\" 221, links a reference in the third book of the \"History\" to the period of darkness described in the crucifixion accounts in three of the Gospels . It is not known whether Thallus made any mention to the crucifixion accounts; if he did, it would be the earliest noncanonical reference to a gospel episode, but its usefulness in determining the historicity of Jesus is uncertain. The dating of Thallus is dependent on him writing about an event during the 207th Olympiad (49–52 AD), which means he wrote after that date, not near that date. This depends on the text being corrupt, which would mean Thallus could have been writing after the 217th Olympiad (89–92 AD), or even the 167th Olympiad (112–109 BC). He is first referenced by Theophilus, writing around 180 AD, which means Thallus could have written any time between 109 BC and 180 AD. All we know is Thallus mentioned a solar eclipse, and as solar eclipses are not possible at Passover, that would mean Thallus was not talking about the crucifixion of Jesus at all.\n\nPhlegon of Tralles, AD 80–140, similar to Thallus, Julius Africanus mentions a historian named Phlegon who wrote a chronicle of history around AD 140, where he records:\n“Phlegon records that, in the time of Tiberius Caesar, at full moon, there was a full eclipse of the sun from the sixth to the ninth hour.” (Africanus, Chronography, 18:1) Phlegon is also mentioned by Origen (an early church theologian and scholar, born in Alexandria):\n“Now Phlegon, in the thirteenth or fourteenth book, I think, of his Chronicles, not only ascribed to Jesus a knowledge of future events . . . but also testified that the result corresponded to His predictions.” (Origen Against Celsus, Book 2, Chapter 14)\n“And with regard to the eclipse in the time of Tiberius Caesar, in whose reign Jesus appears to have been crucified, and the great earthquakes which then took place … ” (Origen Against Celsus, Book 2, Chapter 33)\n“Jesus, while alive, was of no assistance to himself, but that he arose after death, and exhibited the marks of his punishment, and showed how his hands had been pierced by nails.” (Origen Against Celsus, Book 2, Chapter 59). However, Eusebius in The Chronicon (written in the 4th century AD) records what Phlegon said verbatim. \"Now, in the fourth year of the 202nd Olympiad [32 AD], a great eclipse of the sun occurred at the sixth hour [noon] that excelled every other before it, turning the day into such darkness of night that the stars could be seen in heaven, and the earth moved in Bithynia, toppling many buildings in the city of Nicaea.\" Phlegon never mentions Jesus or the 3 hour darkness. He also mentions a solar eclipse, which can not occur at Passover. Apart from the year (which may be a corruption), this description fits an earthquake and eclipse that occurred in North West Turkey on November, 29 AD.\n\nCelsus writing late in the second century produced the first full-scale attack on Christianity. Celsus' document has not survived but in the third century Origen replied to it, and what is known of Celsus' writing is through the responses of Origen. According to Origen, Celsus accused Jesus of being a magician and a sorcerer. While the statements of Celsus may be seen as valuable, they have little historical value, given that the wording of the original writings can not be examined.\n\nThe Dead Sea Scrolls are first century or older writings that show the language and customs of some Jews of Jesus' time. Scholars such as Henry Chadwick see the similar uses of languages and viewpoints recorded in the New Testament and the Dead Sea Scrolls as valuable in showing that the New Testament portrays the first century period that it reports and is not a product of a later period. However, the relationship between the Dead Sea scrolls and the historicity of Jesus has been the subject of highly controversial theories, and although new theories continue to appear, there is no overall scholarly agreement about their impact on the historicity of Jesus, despite the usefulness of the scrolls in shedding light on first-century Jewish traditions.\n\nThe following sources are disputed, and of limited historical value, but they are at least proof of Christians existing and being known and talked about in the first and second centuries.\n\nThere is a limestone burial box from the 1st century known as the James Ossuary with the Aramaic inscription, \"James, son of Joseph, brother of Jesus.\" The authenticity of the inscription was challenged by the Israel Antiquities Authority, who filed a complaint with the Israeli police. In 2012, the owner of the ossuary was found not guilty, with the judge ruling that the authenticity of the ossuary inscription had not been proven either way. It has been suggested it was a forgery.\n\nVarious books, memoirs and stories were written about Jesus by the early Christians. The most famous are the gospels of Matthew, Mark, Luke and John. All but one of these are believed to have been written within 50–70 years of the death of Jesus, with the Gospel of Mark believed to be the earliest, and the last the Gospel of John. Blainey writes that the oldest surviving record written by an early Christian is a short letter by St Paul: the First Epistle to the Thessalonians, which appeared about 25 years after the death of Jesus. This letter, while important in describing issues for the development of Gentilic Christianity, contains little of significance for understanding the life of the historic Jesus.\n\nBart Ehrman, Robert Eisenman and others critical of traditional Christian views, in assessing the problems involved in conducting historical Jesus research, say the Gospels are full of discrepancies, were written decades after Jesus' death, by authors who had not witnessed any events in Jesus' life. They go on to say the Gospels were authored not by eyewitnesses who were contemporary with the events that they narrate but rather by people who did not know Jesus, see anything he did, or hear anything he taught, and that the authors did not even share a language with Jesus. The accounts they produced are not disinterested; they are narratives produced by Christians who actually believed in Jesus, and were not immune from slanting the stories in light of their biases. Ehrman points out that the texts are widely inconsistent, full of discrepancies and contradictions in both details and larger portraits of who Jesus was.\n\nIn the context of Christian sources, even if all other texts are ignored, the Pauline epistles can provide some information regarding Jesus. This information does not include a narrative of the life of Jesus, and refers to his existence as a person, but adds few specific items apart from his death by crucifixion. This information comes from those letters of Paul whose authenticity is not disputed. Paul was not a companion of Jesus and claims his information comes from the holy spirit acquired after Jesus' death.\n\nOf the thirteen letters that bear Paul's name, seven are considered authentic by almost all scholars, and the others are generally considered pseudepigraphic. The 7 undisputed letters (and their approximate dates) are: 1 Thessalonians (c. 51 CE), Philippians (c. 52–54 CE), Philemon (c. 52–54 CE), 1 Corinthians (c. 53–54 CE), Galatians (c. 55 CE), 2 Corinthians (c. 55–56 CE) and Romans (c. 55–58 CE). The authenticity of these letters is accepted by almost all scholars, and they have been referenced and interpreted by early authors such as Origen and Eusebius.\n\nGiven that the Pauline epistles are generally dated to AD 50 to AD 60, they are the earliest surviving Christian texts that include information about Jesus. These letters were written approximately twenty to thirty years after the generally accepted time period for the death of Jesus, around AD 30–36. The letters were written during a time when Paul recorded encounters with the disciples of Jesus, e.g. states that several years after his conversion Paul went to Jerusalem and stayed with Apostle Peter for fifteen days. During this time, Paul disputed the nature of Jesus' message with Jesus's brother James, concerning the importance of adhering to kosher food restrictions and circumcision, important features of determining Jewish identity.\n\nThe Pauline letters were not intended to provide a narrative of the life of Jesus, but were written as expositions of Christian teachings. In Paul's view, the earthly life of Jesus was of a lower importance than the theology of his death and resurrection,a theme that permeates Pauline writings. However, the Pauline letters clearly indicate that for Paul Jesus was a real person (born of a woman as in Gal 4.4), a Jew (\"born under the law\", Romans 1.3) who had disciples (1 Corinthians 15.5), who was crucified (as in 1 Corinthians 2.2 and Galatians 3.1) and who resurrected from the dead (1 Corinthians 15.20, Romans 1.4 and 6.5, Philippians 3:10–11). And the letters reflect the general concept within the early Gentillic Christian Church that Jesus existed, was crucified and was raised from the dead.\n\nThe references by Paul to Jesus do not in themselves prove the existence of Jesus, but they do establish that the existence of Jesus was the accepted norm within the early Christians (including the Christian community in Jerusalem, given the references to collections there) twenty to thirty years after the death of Jesus, at a time when those who could have been acquainted with him could still be alive.\n\nThe seven Pauline epistles that are widely regarded as authentic include the following information that along with other historical elements are used to study the historicity of Jesus:\n\nThe existence of only these references to Jesus in the Pauline epistles has given rise to criticism of them by G. A. Wells, who is generally accepted as a leader of the movement to deny the historicity of Jesus. When Wells was still denying the existence of Jesus, he criticized the Pauline epistles for not mentioning items such as John the Baptist or Judas or the trial of Jesus and used that argument to conclude that Jesus was not a historical figure.\n\nJames D. G. Dunn addressed Wells' statement and stated that he knew of no other scholar that shared that view, and most other scholars had other and more plausible explanations for the fact that Paul did not include a narrative of the life of Jesus in his letters, which were primarily written as religious documents rather than historical chronicles at a time when the life story of Jesus could have been well known within the early Church. Dunn states that despite Wells' arguments, the theories of the non-existence of Jesus are a \"thoroughly dead thesis\".\n\nWhile Wells no longer denies the existence of Jesus, he has responded to Dunn, stating that his arguments from silence not only apply to Paul but all early Christian authors, and that he still has a low opinion of early Christian texts, maintaining that for Paul Jesus may have existed a good number of decades before.\n\nThe Pauline letters sometimes refer to creeds, or confessions of faith, that predate their writings. For instance reads: \"For what I received I passed on to you as of first importance: that Christ died for our sins according to the Scriptures, that he was buried, that he was raised on the third day according to the Scriptures.\" refers to Romans 1:2 just before it which mentions an existing gospel, and in effect may be treating it as an earlier creed.\n\nOne of the keys to identifying a pre-Pauline tradition is given in \n\nHere Paul refers to others before him who preached the creed. James Dunn states that indicates that in the 30s Paul was taught about the death of Jesus a few years earlier.\n\nThe Pauline letters thus contain Christian creed elements of pre-Pauline origin. The antiquity of the creed has been located by many Biblical scholars to less than a decade after Jesus' death, originating from the Jerusalem apostolic community. Concerning this creed, Campenhausen wrote, \"This account meets all the demands of historical reliability that could possibly be made of such a text,\" whilst A. M. Hunter said, \"The passage therefore preserves uniquely early and verifiable testimony. It meets every reasonable demand of historical reliability.\"\n\nThese creeds date to within a few years of Jesus' death, and developed within the Christian community in Jerusalem. Although embedded within the texts of the New Testament, these creeds are a distinct source for Early Christianity. This indicates that existence and death of Jesus was part of Christian belief a few years after his death and over a decade before the writing of the Pauline epistles.\n\nThe four canonical gospels, Matthew, Mark, Luke, and John, are the main sources for the biography of Jesus' life, the teachings and actions attributed to him. Three of these (Matthew, Mark, and Luke) are known as the synoptic Gospels, from the Greek σύν (syn \"together\") and ὄψις (opsis \"view\"), given that they display a high degree of similarity in content, narrative arrangement, language and paragraph structure. The presentation in the fourth canonical gospel, i.e. John, differs from these three in that it has more of a thematic nature rather than a narrative format. Scholars generally agree that it is impossible to find any direct literary relationship between the synoptic gospels and the Gospel of John.\n\nThe authors of the New Testament generally showed little interest in an absolute chronology of Jesus or in synchronizing the episodes of his life with the secular history of the age. The gospels were primarily written as theological documents in the context of early Christianity with the chronological timelines as a secondary consideration. One manifestation of the gospels being theological documents rather than historical chronicles is that they devote about one third of their text to just seven days, namely the last week of the life of Jesus in Jerusalem. Although the gospels do not provide enough details to satisfy the demands of modern historians regarding exact dates, scholars have used them to reconstruct a number of portraits of Jesus. However, as stated in the gospels do not claim to provide an exhaustive list of the events in the life of Jesus.\n\nScholars have varying degrees of certainty about the historical reliability of the accounts in the gospels, and the only two events whose historicity is the subject of almost universal agreement among scholars are the baptism and crucifixion of Jesus. Scholars such as E.P. Sanders and separately Craig A. Evans go further and assume that two other events in the gospels are historically certain, namely that Jesus called disciples, and caused a controversy at the Temple.\n\nEver since the Augustinian hypothesis, scholars continue to debate the order in which the gospels were written, and how they may have influenced each other, and several hypothesis exist in that regard, e.g. the Markan priority hypothesis holds that the Gospel of Mark was written first c. 70 CE. In this approach, Matthew is placed at being sometime after this date and Luke is thought to have been written between 70 and 100 CE. However, according to the competing, and more popular, Q source hypothesis, the gospels were not independently written, but were derived from a common source called Q. The two-source hypothesis then proposes that the authors of Matthew and Luke drew on the Gospel of Mark as well as on Q.\n\nThe gospels can be seen as having three separate lines: A literary line which looks at it from a textual perspective, secondly a historical line which observes how Christianity started as a renewal movement within Judaism and eventually separated from it, and finally a theological line which analyzes Christian teachings. Within the historical perspective, the gospels are not simply used to establish the existence of Jesus as sources in their own right alone, but their content is compared and contrasted to non-Christian sources, and the historical context, to draw conclusions about the historicity of Jesus.\n\nTwo possible patristic sources that may refer to eye witness encounters with Jesus are the early references of Papias and Quadratus, reported by Eusebius of Caesarea in the 4th century.\n\nThe works of Papias have not survived, but Eusebius quotes him as saying:\n\nRichard Bauckham states that while Papias was collecting his information (c. 90), Aristion and the elder John (who were Jesus' disciples) were still alive and teaching in Asia minor, and Papias gathered information from people who had known them. However, the exact identity of the \"elder John\" is wound up in the debate on the authorship of the Gospel of John, and scholars have differing opinions on that, e.g. Jack Finegan states that Eusebius may have misunderstood what Papias wrote, and the elder John may be a different person from the author of the fourth gospel, yet still a disciple of Jesus. Gary Burge, on the other hand sees confusion on the part of Eusebius and holds the elder John to be different person from the apostle John.\n\nThe letter of Quadratus (possibly the first Christian apologist) to emperor Hadrian (who reigned 117 – 138) is likely to have an early date and is reported by Eusebius in his \"Ecclesiastical History\" 4.3.2 to have stated:\n\nBy \"our Savior\" Quadratus means Jesus and the letter is most likely written before AD 124. Bauckham states that by \"our times\" he may refer to his early life, rather than when he wrote (117–124), which would be a reference contemporary with Papias. Bauckham states that the importance of the statement attributed to Quadratus is that he emphasizes the \"eye witness\" nature of the testimonies to interaction with Jesus. Such \"eye witness statements\" abound in early Christian writings, particularly the pseudonymous Christian Apocrypha, Gospels and Letters, in order to give them credibility.\n\nA number of later Christian texts, usually dating to the second century or later, exist as New Testament apocrypha, among which the gnostic gospels have been of major recent interest among scholars. The 1945 discovery of the Nag Hammadi library created a significant amount of scholarly interest and many modern scholars have since studied the gnostic gospels and written about them. However, the trend among the 21st century scholars has been to accept that while the gnostic gospels may shed light on the progression of early Christian beliefs, they offer very little to contribute to the study of the historicity of Jesus, in that they are rather late writings, usually consisting of sayings (rather than narrative, similar to the hypothesised Q documents), their authenticity and authorship remain questionable, and various parts of them rely on components of the New Testament. The focus of modern research into the historical Jesus has been away from gnostic writings and towards the comparison of Jewish, Greco-Roman and canonical Christian sources.\n\nAs an example, Bart Ehrman states that gnostic writings of the Gospel of Thomas (part of the Nag Hammadi library) have very little value in historical Jesus research, because the author of that gospel placed no importance on the physical experiences of Jesus (e.g. his crucifixion) or the physical existence of believers, and was only interested in the secret teachings of Jesus rather than any physical events. Similarly, the Apocryphon of John (also part of the Nag Hammadi library) has been useful in studying the prevailing attitudes in the second century, and questions of authorship regarding the Book of revelation, given that it refers to , but is mostly about the post ascension teachings of Jesus in a vision, not a narrative of his life. Some scholars such as Edward Arnal contend that the Gospel of Thomas continues to remain useful for understanding how the teachings of Jesus were transmitted among early Christians, and sheds light on the development of early Christianity.\n\nThere is overlap between the sayings of Jesus in the apocryphal texts and canonical Christian writings, and those not present in the canonical texts are called agrapha. There are at least 225 agrapha but most scholars who have studied them have drawn negative conclusions about the authenticity of most of them and see little value in using them for historical Jesus research. Robert Van Voorst states that the vast majority of the agrapha are certainly inauthentic. Scholars differ on the number of authentic agrapha, some estimating as low as seven as authentic, others as high as 18 among the more than 200, rendering them of little value altogether. While research on apocryphal texts continues, the general scholarly opinion holds that they have little to offer to the study of the historicity of Jesus given that they are often of uncertain origin, and almost always later documents of lower value.\n\n\n"}
{"id": "1707086", "url": "https://en.wikipedia.org/wiki?curid=1707086", "title": "Tag (metadata)", "text": "Tag (metadata)\n\nIn information systems, a tag is a keyword or term assigned to a piece of information (such as an Internet bookmark, digital image, database record, or computer file). This kind of metadata helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item's creator or by its viewer, depending on the system, although they may also be chosen from a controlled vocabulary.\n\nTagging was popularized by websites associated with Web 2.0 and is an important feature of many Web 2.0 services. It is now also part of other database systems, desktop applications, and operating systems.\n\nPeople use tags to aid classification, mark ownership, note boundaries, and indicate online identity. Tags may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. People were using textual keywords to classify information and objects long before computers. Computer based search algorithms made the use of such keywords a rapid way of exploring records.\n\nTagging gained popularity due to the growth of social bookmarking, image sharing, and social networking websites. These sites allow users to create and manage labels (or \"tags\") that categorize content using simple keywords. Websites that include tags often display collections of tags as tag clouds, as do some desktop applications. On websites that aggregate the tags of all users, an individual user's tags can be useful both to them and to the larger community of the website's users.\n\nTagging systems have sometimes been classified into two kinds: \"top-down\" and \"bottom-up\". Top-down taxonomies are created by an authorized group of designers (sometimes in the form of a controlled vocabulary), whereas bottom-up taxonomies (called folksonomies) are created by all users. This definition of \"top down\" and \"bottom up\" should not be confused with the distinction between a \"single hierarchical\" tree structure (in which there is one correct way to classify each item) versus \"multiple non-hierarchical\" sets (in which there are multiple ways to classify an item); the structure of both top-down and bottom-up taxonomies may be either hierarchical, non-hierarchical, or a combination of both. Some researchers and applications have experimented with combining hierarchical and non-hierarchical tagging to aid in information retrieval. Others are combining top-down and bottom-up tagging, including in some large library catalogs (OPACs) such as WorldCat.\n\nWhen tags or other taxonomies have further properties (or semantics) such as relationships and attributes, they constitute an ontology.\n\nMetadata tags as described in this article should not be confused with the use of the word \"tag\" in some software to refer to an automatically generated cross-reference; examples of the latter are \"tags tables\" in Emacs and \"smart tags\" in Microsoft Office.\n\nThe use of keywords as part of an identification and classification system long predates computers. Paper data storage devices, notably edge-notched cards, that permitted classification and sorting by multiple criteria were already in use prior to the twentieth century, and faceted classification has been used by libraries since the 1930s.\n\nIn the late 1970s and early 1980s, the Unix text editor Emacs offered a companion software program called \"Tags\" that could automatically build a table of cross-references called a \"tags table\" that Emacs could use to jump between a function call and that function's definition. This use of the word \"tag\" did not refer to metadata tags, but was an early use of the word \"tag\" in software to refer to a word index.\n\nOnline databases and early websites deployed keyword tags as a way for publishers to help users find content. In the early days of the World Wide Web, the codice_1 meta element was used by web designers to tell web search engines what the web page was about, but these keywords were only visible in a web page's source code and were not modifiable by users.\n\nIn 2003, the social bookmarking website Delicious provided a way for its users to add \"tags\" to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag. Within a couple of years, the photo sharing website Flickr allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable. The success of Flickr and the influence of Delicious popularized the concept, and other social software websites—such as YouTube, Technorati, and Last.fm—also implemented tagging. In 2005, the Atom web syndication standard provided a \"category\" element for inserting subject categories into web feeds, and in 2007 Tim Bray proposed a \"tag\" URN.\n\nMany blog systems (and other web content management systems) allow authors to add free-form tags to a post, along with (or instead of) placing the post into a predetermined category. For example, a post may display that it has been tagged with codice_2 and codice_3. Each of those tags is usually a web link leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.\n\nSome desktop applications and web applications feature their own tagging systems, such as email tagging in Gmail and Mozilla Thunderbird, bookmark tagging in Firefox, audio tagging in iTunes or Winamp, and photo tagging in various applications. Some of these applications display collections of tags as tag clouds.\n\nThere are various systems for applying tags to the files in a computer's file system. In Apple's macOS, the operating system has allowed users to assign multiple arbitrary tags as extended file attributes to any file or folder ever since OS X 10.9 was released in 2013, and before that time the open-source OpenMeta standard provided similar tagging functionality in macOS. Several semantic file systems that implement tags are available for the Linux kernel, including Tagsistant. Microsoft Windows allows users to set tags only on Microsoft Office documents and some kinds of picture files.\n\nCross-platform file tagging standards include Extensible Metadata Platform (XMP), an ISO standard for embedding metadata into popular image, video and document file formats, such as JPEG and PDF, without breaking their readability by applications that do not support XMP. XMP largely supersedes the earlier IPTC Information Interchange Model. Exif is a standard that specifies the image and audio file formats used by digital cameras, including some metadata tags. TagSpaces is an open-source cross-platform application for tagging files; it inserts tags into the filename.\n\nAn \"official tag\" is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a controlled vocabulary.\n\nA researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, freeform classification allows the author to manage what would otherwise be unwieldy amounts of information.\n\nA triple tag or machine tag uses a special syntax to define extra semantic information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a namespace, a predicate, and a value. For example, codice_4 is a tag for the geographical longitude coordinate whose value is 50.123456. This triple structure is similar to the Resource Description Framework model for information.\n\nThe triple tag format was first devised for geolicious in November 2004, to map Delicious bookmarks, and gained wider acceptance after its adoption by Mappr and GeoBloggers to map Flickr photos. In January 2007, Aaron Straup Cope at Flickr introduced the term \"machine tag\" as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.\n\nSpecialized metadata for geographical identification is known as \"geotagging\"; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using binomial nomenclature.\n\nA hashtag is a kind of metadata tag marked by the prefix codice_5, sometimes known as a \"hash\" symbol. This form of tagging is used on microblogging and social networking services such as Twitter, Facebook, Google+, VK and Instagram.\n\nA knowledge tag is a type of meta-information that describes or defines some aspect of a piece of information (such as a document, digital image, database table, or web page). Knowledge tags are more than traditional non-hierarchical keywords or terms; they are a type of metadata that captures knowledge in the form of descriptions, categorizations, classifications, semantics, comments, notes, annotations, hyperdata, hyperlinks, or references that are collected in tag profiles (a kind of ontology). These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository.\n\nKnowledge tags are part of a knowledge management discipline that leverages Enterprise 2.0 methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. Different kinds of knowledge can be captured in knowledge tags, including factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies). These forms of knowledge often exist outside the data itself and are derived from personal experience, insight, or expertise. Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information. Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turnover, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.\n\nIn a typical tagging system, there is no explicit information about the meaning or semantics of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them; in contrast, the flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.\n\nWhen users can freely choose tags (creating a folksonomy, as opposed to selecting terms from a controlled vocabulary), the resulting metadata can include homonyms (the same tags used with different meanings) and synonyms (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject. For example, the tag \"orange\" may refer to the fruit or the color, and items related to a version of the Linux kernel may be tagged \"Linux\", \"kernel\", \"Penguin\", \"software\", or a variety of other terms. Users can also choose tags that are different inflections of words (such as singular and plural), which can contribute to navigation difficulties if the system does not include stemming of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of \"tag terms\" within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies may collectively develop a partial set of tagging conventions.\n\nDespite the apparent lack of control, research has shown that a simple form of shared vocabulary emerges in social bookmarking systems. Collaborative tagging exhibits a form of complex systems dynamics (or self-organizing dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags converges over time to stable power law distributions. Once such stable distributions form, simple folksonomic vocabularies can be extracted by examining the correlations that form between different tags. In addition, research has suggested that it is easier for machine learning algorithms to learn tag semantics when users tag \"verbosely\"—when they annotate resources with a wealth of freely associated, descriptive keywords.\n\nTagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a YouTube video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items. The number of tags allowed may also be limited to reduce spam.\n\nSome tagging systems provide a single text box to enter tags, so to be able to tokenize the string, a must be used. Two popular separators are the space character and the comma. To enable the use of separators in the tags, a system may allow for higher-level separators (such as quotation marks) or escape characters. Systems can avoid the use of separators by allowing only one tag to be added to each input widget at a time, although this makes adding multiple tags more time-consuming.\n\nA syntax for use within HTML is to use the rel-tag microformat which uses the \"rel\" attribute with value \"tag\" (i.e., codice_6) to indicate that the linked-to page acts as a tag for the current context.\n"}
{"id": "1007243", "url": "https://en.wikipedia.org/wiki?curid=1007243", "title": "Tertiary source", "text": "Tertiary source\n\nA tertiary source is an index or textual consolidation of primary and secondary sources. Some tertiary sources are not to be used for academic research, unless they can also be used as secondary sources, or to find other sources.\n\nDepending on the topic of research, a scholar may use a bibliography, dictionary, or encyclopedia as either a tertiary or a secondary source. This causes difficulty in defining many sources as either one type or the other.\n\nIn some academic disciplines the differentiation between a secondary and tertiary source is relative. \n\nIn the United Nations International Scientific Information System (UNISIST) model, a secondary source is a bibliography, whereas a tertiary source is a synthesis of primary sources.\n\nAs tertiary sources, encyclopedias, textbooks, and compendia attempt to summarize, collect, and consolidate the source materials into an overview, but may also present subjective, or biased commentary and analysis (which are characteristics of secondary sources).\n\nIndexes, bibliographies, concordances, and databases may not provide much textual information, but as aggregates of primary and secondary sources, they are often considered tertiary sources. So although tertiary sources are both primary and secondary, they are more towards a secondary source because of commentary and bias.\n\nAlmanacs, travel guides, field guides, and timelines are also examples of tertiary sources.\n\nSurvey or overview articles are usually tertiary, though review articles in peer-reviewed academic journals are secondary (not be confused with film, book, etc. reviews, which are primary-source opinions).\n\nSome usually primary sources, such as user guides and manuals, are secondary or tertiary (depending on the nature of the material) when written by third parties.\n\n"}
{"id": "26681002", "url": "https://en.wikipedia.org/wiki?curid=26681002", "title": "Text annotation", "text": "Text annotation\n\nText Annotation is the practice and the result of adding a note or gloss to a text, which may include highlights or underlining, comments, footnotes, tags, and links. Text annotations can include notes written for a reader's private purposes, as well as shared annotations written for the purposes of collaborative writing and editing, commentary, or social reading and sharing. In some fields, text annotation is comparable to metadata insofar as it is added post hoc and provides information about a text without fundamentally altering that original text. Text annotations are sometimes referred to as marginalia, though some reserve this term specifically for hand-written notes made in the margins of books or manuscripts. Annotations are extremely useful and help to develop knowledge of English literature.\n\nThis article covers both private and socially shared text annotations, including hand-written and information technology-based annotation. For information on annotation of Web content, including images and other non-textual content, see also Web annotation.\n\nText annotation may be as old as writing on media, where it was possible to produce an additional copy with a reasonable effort. It became a prominent activity around 1000 AD in Talmudic commentaries and Arabic rhetorics treaties. In the Medieval era, scribes who copied manuscripts often made marginal annotations that then circulated with the manuscripts and were thus shared with the community; sometimes annotations were copied over to new versions when such manuscripts were later recopied.\n\nWith the rise of the printing press and the relative ease of circulating and purchasing individual (rather than shared) copies of texts, the prevalence of socially shared annotations declined and text annotation became a more private activity consisting of a reader interacting with a text. Annotations made on shared copies of texts (such as library books) are sometimes seen as devaluing the text, or as an act of defacement. Thus, print technologies support the circulation of annotations primarily as formal scholarly commentary or textual footnotes or endnotes rather than marginal, handwritten comments made by private readers, though handwritten comments or annotations were common in collaborative writing or editing.\n\nComputer-based technologies have provided new opportunities for individual and socially shared text annotations that support multiple purposes, including readers’ individual reading goals, learning, social reading, writing and editing, and other practices. Text annotation in Information Technology (IT) systems raises technical issues of access, linkage, and storage that are generally not relevant to paper-based text annotation, and thus research and development of such systems often addresses these areas.\n\nText annotations can serve a variety of functions for both private and public reading and communication practices. In their article \"From the Margins to the Center: The Future of Annotation,\" scholars Joanna Wolfe and Christine Neuwirth identify four primary functions that text annotations commonly serve in the modern era, including: (1)\"facilitat[ing] reading and later writing tasks,\" which includes annotations that support reading for both personal and professional purposes; (2)\"eavesdrop[ping] on the insights of other readers,\" which involves sharing of annotations; (3)\"provid[ing] feedback to writers or promote communication with collaborators,\" which can include personal, professional, and education-related feedback; and (4)\"call[ing] attention to topics and important passages,\" for which scholarly annotations, footnotes, and call-outs often function. Regarding the ways that annotations can support individual reading tasks, Catherine Marshall points out that the ways that readers annotate texts depends on the purpose, motivation, and context of reading. Readers may annotate to help interpret a text, to call attention to a section for future reference or reading, to support memory and recall, to help focus attention on the text as they read, to work out a problem related to the text, or create annotations not specifically related to the text at all.\n\nEducational research in text annotation has examined the role that both private and shared text annotations can play in supporting learning goals and communication. Much educational research examines how students’ private annotation of texts supports comprehension and memory; for example, research indicates that annotating texts causes more in-depth processing of information, which results in greater recall of information.\n\nOther areas of educational research investigate the benefits of socially shared text annotations for collaborative learning, both for paper-based and IT-based annotation sharing. For example, studies by Joanna Wolfe have investigated the benefits of exposure to others’ annotations on student readers and writers. In a 2000 study, Wolfe found that exposing students to others’ annotations influenced their perceptions of the annotators, which in turn shaped their responses to the material and their written products. In a later study, Wolfe found that viewing others’ written comments on a paper text, especially pairs of annotations that present opposing responses to the text, can help students engage in the type of critical reading and stance-taking necessary for effective argumentative writing.\n\nWhile shared annotations can benefit individual readers, it is important to note that, \"since the 1920s, literacy theory has increasingly emphasized the importance of social factors in the development of literacy.\" Thus, shared annotations can not only help one to better understand the content of a particular text, but may also aid in the acquirement of literacy skills. For example, a mother may leave marks inside a book to draw the attention of her child to a particular theme or concept; thanks to the development of audio annotations, parents may now leave notes for children who are just starting to read and may struggle with textual annotations.\n\nMore recent research in the effects of shared text annotations has focused on the learning applications for web-based annotation systems, some of which were developed based on design recommendations from studies outlined above. For example, Ananda Gunawardena, Aaron Tan, and David Kaufer conducted a pilot study to examine whether annotating documents in Classroom Salon, a web-based annotation and social reading platform, encouraged active reading, error detection, and collaboration in a computer science course at Carnegie Mellon University. This study suggested a correlation between students’ overall performance in the course and their ability to identify errors in a text that they annotated in Classroom Salon; it also found that students were likely to change their annotations in response to annotations made by others in the course.\n\nSimilarly, the web-based annotation tool HyLighter was used in a first-year writing course and shown to improve the development of students’ mental models of texts, including supporting reading comprehension, critical thinking, and the ability to develop a thesis. The collaboration with peers and experts around a shared text improved these skills and brought the communities’ understanding closer together.\n\nA meta-analysis of empirical studies into the higher-education uses of social annotation (SA) tools indicates such tools have been tested in several courses, among them English, sport psychology, and hypermedia. Studies have indicated that social annotation functions, including commenting, information sharing, and highlighting, can support instruction designed to foster collaborative learning and communication, as well as reading comprehension, metacognition, and critical analysis. Several studies indicated that students enjoyed using social annotation tools, and that it improved motivation in the course.\n\nText annotations have long been used in writing and revision processes as a way for reviewers to suggest changes and communicate about a text. In book publishing, for example, the collaboration of authors and editors to develop and revise a manuscript frequently involves exchanges of both in-line revisions or notes as well as marginal annotations. Similarly, copyeditors often make marginal annotations or notes that explain or suggest revisions or are directed at the author as questions or suggestions (commonly called \"queries\"). Asynchronous collaborative writing and document development often depend on text annotations as a way not only to suggest revisions but also to exchange ideas during document development or to facilitate group decision making, though such processes are often complicated by the use of different communication technologies (such as phone calls or emails as well as document sharing) for distinct tasks. Text annotations can also function to allow group or community members to communicate about a shared text, such as a doctor annotating a patient's chart.\n\nMuch research into the functionality and design of collaborative IT-based writing systems, which often support text annotation, has occurred in the area of computer-supported cooperative work.\n\nResearch in the design and development of annotation systems uses specific terminology to refer to distinct structural components of annotations and also distinguishes among options for digital annotation displays.\n\nThe structural components of any annotation can be roughly divided into three primary elements: a \"body\", an \"anchor\", and a \"marker\". The body of an annotation includes reader-generated symbols and text, such as handwritten commentary or stars in the margin. The anchor is what indicates the extent of the original text to which the body of the annotation refers; it may include circles around sections, brackets, highlights, underlines, and so on. Annotations may be anchored to very broad stretches of text (such as an entire document) or very narrow sections (such as a specific letter, word, or phrase). The marker is the visual appearance of the anchor, such as whether it is a grey underline or a yellow highlight. An annotation that has a body (such as a comment in the margin) but no specific anchor has no marker.\n\nIT-based annotation systems utilize a variety of display options for annotations, including:\nAnnotation interfaces may also allow highlighting or underlining, as well as threaded discussions. Sharing and communicating through annotations anchored to specific documents is sometimes referred to as \"anchored discussion\".\n\nIT-based annotation systems include standalone and client-server systems. In the 1980s and 1990s, a number of such systems were built in the context of libraries, patent offices, and legal text processing. Their design led researchers to produce taxonomies of annotation forms. Text annotation research has taken place at several institutions, including Xerox research centers in Palo Alto and Grenoble (France), the Hitachi Central Research Lab (in particular for annotation of patents), and in relation with the construction of the new French National Library between 1989 and 1995 at the Institut de Recherche en Informatique de Toulouse and in the company AIS (Advanced Innovation Systems).\n\nAnnotation functionality has been present in text processing software for many years through inline notes displayed as pop-ups, footnotes, and endnotes; however, it is only recently that functionality for displaying annotations as marginalia has appeared in programs such as OpenOffice.org/LibreOffice Writer and Microsoft Word. Personal or standalone annotation include word processing software that supports embedded or anchored text annotations as well as Adobe Acrobat, which in addition to commenting allows highlights, stamps, and other types of markup.\n\nTim Berners-Lee had already implemented the concept of directly editing web documents in 1990 in WorldWideWeb, the first web browser, but later ported versions removed this collaborative ability. An early version of NCSA Mosaic in 1993 also included a collaborative annotation capability, though it was quickly removed. Web Distributed Authoring and Versioning, WebDAV, was then reintroduced as an extension.\n\nA different approach to distributed authoring consists in first gathering many annotations from a wide public, and then integrate them all in order to produce a further version of a document. This approach was pioneered by Stet, the system put in place to gather comments on drafts of version 3 of the GNU General Public License. This system arose after a specific requirement, which it served egregiously, but was not so easily configurable as to be convenient for annotating any other document on the web. The co-ment system uses annotation interface concepts similar to Stet's, but it is based on an entirely new implementation, using Django/Python on the server side and various AJAX libraries such as JQuery on the client side. Both Stet and co-ment are licensed under the GNU Affero General Public License.\n\nSince 2011, the non-profit Hypothes Is Project has offered the free, open web annotation service Hypothes.is. The service features annotation via a Chrome extension, bookmarklet or proxy server, as well as integration into a LMS or CMS. Both webpages and PDFs can be annotated. Other web-based text annotation systems are collaborative software for distributed text editing and versioning, which also feature annotation and commenting interfaces. For example, HyLighter supports synchronous and asynchronous interactions, general commenting, comment tagging, threaded discussions and comment filtering. Other annotation tools under these category are more focused on NLP tasks as Named-entity recognition, relationship extraction or normalization. Some tools support manual tagging of data or automatic annotations via supervised learning.\n\nSpecialized Web-based text annotations exist in the context of scientific publication, either for refereeing or post-publication. The on-line journal PLoS ONE, published by the Public Library of Science, has developed its own Web-based system where scientists and the public can comment on published articles. The annotations are displayed as pop-ups with an anchor in the text.\n\n\n"}
{"id": "5928902", "url": "https://en.wikipedia.org/wiki?curid=5928902", "title": "The Map Library", "text": "The Map Library\n\nThe Map Library is a project of The Map Maker Trust charity, and supported by Map Maker Ltd., for the supplying of free GIS data. The project website also hosts free conversion software for raster and vector files. As of November 2008, the only data sets available were for the continents of Africa and Central America.\n\nFrom the website...\n\nThe project data is managed in part by two pieces of software, each supporting different file formats and conversions.\n\n\nThe project uses data from NASA mapping projects, Famine Early Warning Systems Network, and the National Geospatial-Intelligence Agency.\n\n"}
{"id": "270906", "url": "https://en.wikipedia.org/wiki?curid=270906", "title": "Three-letter acronym", "text": "Three-letter acronym\n\nA three-letter acronym (TLA), or three-letter abbreviation, is an abbreviation, specifically an acronym, alphabetism, or initialism, consisting of three letters. These are usually the initial letters of the words of the phrase abbreviated, and are written in capital letters (upper case); three-letter abbreviations such as \"etc.\" and \"Mrs.\" are not three-letter acronyms, but \"TLA\" is a TLA (an example of an autological abbreviation).\n\nMost three-letter abbreviations are \"initialisms\": all the letters are pronounced as the names of letters, as in \"APA\" . Some are acronyms pronounced as a word; computed axial tomography, CAT, is almost always pronounced as the animal's name in \"CAT scan\".\n\n\nThe exact phrase \"three-letter acronym\" appeared in the sociology literature in 1975. Three-letter acronyms were used as mnemonics in biological sciences, from 1977 and their practical advantage was promoted by Weber in 1982. They are used in many other fields, but the term TLA is particularly associated with computing. In 1980, the manual for the Sinclair ZX81 home computer used and explained TLA. The specific generation of three-letter acronyms in computing was mentioned in a JPL report of 1982. In 1988, in a paper titled \"On the cruelty of really teaching computer science\", eminent computer scientist Edsger W. Dijkstra wrote \n\"Because no endeavour is respectable these days without a TLA ...\" By 1992 it was in a Microsoft handbook.\n\nThe number of possible three-letter abbreviations (or permutations) using the 26 letters of the alphabet from A to Z (AAA, AAB ... to ZZY, ZZZ) is 26 × 26 × 26 = 17,576. Another 26 × 26 × 10 = 6760 can be produced if the third element is allowed to be a digit 0-9, giving a total of 24,336.\n\nIn English, WWW is the longest possible TLA to pronounce, typically requiring nine syllables. The usefulness of TLAs typically comes from how it is quicker to say the acronym instead of than the phrase they represent, however saying 'WWW' in English requires three times as many syllables than the phrase it is meant to abbreviate (World Wide Web). Consequently, \"www\" is sometimes abbreviated as \"dubdubdub\" in speech.\n\n\n"}
{"id": "9032406", "url": "https://en.wikipedia.org/wiki?curid=9032406", "title": "Tupper's self-referential formula", "text": "Tupper's self-referential formula\n\nTupper's self-referential formula is a formula that visually represents itself when graphed at a specific location in the (\"x\", \"y\") plane.\n\nThe formula was defined by Jeff Tupper and appears as an example in Tupper's 2001 SIGGRAPH paper on reliable two-dimensional computer graphing algorithms.\n\nAlthough the formula is called \"self-referential\", Tupper did not name it as such.\n\nThe formula is an inequality defined as:\n\nor, as plaintext,\nwhere ⌊ ⌋ denotes the floor function, and mod is the modulo operation.\n\nLet \"k\" equal the following 543-digit integer:\n\nIf one graphs the set of points (\"x\", \"y\") in 0 ≤ \"x\" < 106 and \"k\" ≤ \"y\" < \"k\" + 17 satisfying the inequality given above, the resulting graph looks like this (the axes in this plot have been reversed, otherwise the picture would be upside-down and mirrored):\n\nThe formula is a general-purpose method of decoding a bitmap stored in the constant \"k\", and it could actually be used to draw any other image. When applied to the unbounded positive range 0 ≤ \"y\", the formula tiles a vertical swath of the plane with a pattern that contains all possible 17-pixel-tall bitmaps. One horizontal slice of that infinite bitmap depicts the drawing formula itself, but this is not remarkable, since other slices depict all other possible formulae that might fit in a 17-pixel-tall bitmap. Tupper has created extended versions of his original formula that rule out all but one slice.\n\nThe constant \"k\" is a simple monochrome bitmap image of the formula treated as a binary number and multiplied by 17. If \"k\" is divided by 17, the least significant bit encodes the upper-right corner (\"k\", 0); the 17 least significant bits encode the rightmost column of pixels; the next 17 least significant bits encode the 2nd-rightmost column, and so on.\n\n\n"}
