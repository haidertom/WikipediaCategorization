{"id": "30875386", "url": "https://en.wikipedia.org/wiki?curid=30875386", "title": "Agência Brasil", "text": "Agência Brasil\n\nAgência Brasil (ABR) is the national public news agency, run by the Brazilian government. It is a part of the public media corporation Empresa Brasil de Comunicação (EBC), created in 2007 to unite two government media enterprises Radiobrás and TVE (Televisão Educativa). It is publishing contents under CC-BY.\n\nABr is one of the most important Brazilian news agencies, that feeds thousands of regional newspapers and websites throughout Brazil but also national media outlets like Estadao, O Globo, Folha de S.Paulo, UOL and Terra.\n\nIts headquarters are located in Brazilian capital, Brasília. There are also two regional offices located in São Paulo and Rio de Janeiro.\n\n"}
{"id": "7133058", "url": "https://en.wikipedia.org/wiki?curid=7133058", "title": "Association for International Broadcasting", "text": "Association for International Broadcasting\n\nThe Association for International Broadcasting - AIB - is the not-for-profit, non-governmental trade association that represents and supports international television and radio broadcasters and online broadcasters.\n\nFounded in 1993, the AIB has developed into a truly global organisation whose membership extends from New Zealand west through to the USA. The AIB provides its members with market intelligence, lobbying, networking and marketing support. It publishes the international media magazine, The Channel, that has a regular subscriber base of more than 7,000 senior executives in broadcasting and electronic media organisations in over 120 countries. The AIB also produces regular electronic news letters that reach the desktops of more than 27,000 people worldwide.\n\nThe AIB has an immense collection of data about broadcasting and electronic media covering territories throughout the world. \n\nThe AIB runs an awards festival that celebrates the best in factual TV and radio broadcasting. Called the AIBs, this annual festival attracts entries from broadcasters and independent production companies on every continent. In 2017, the AIBs will be hosted by Matthew Amroliwala, host of Global on BBC World News.\n\nThe AIB is a non-governmental, not-for-profit organisation with its headquarters in the United Kingdom. It is governed by an Executive Council of six members elected from the AIB's membership including representatives of BBC World News, Bloomberg Television, France Medias Monde, DD News, RT channel, Deutsche Welle and Radio Taiwan International. The AIB's permanent staff is led by Chief Executive Simon Spanswick.\n\n"}
{"id": "12822286", "url": "https://en.wikipedia.org/wiki?curid=12822286", "title": "Conflict resource", "text": "Conflict resource\n\nConflict resources are natural resources extracted in a conflict zone and sold to perpetuate the fighting. There is both statistical and anecdotal evidence that belligerent accessibility to precious commodities can prolong conflicts (a \"resource curse\"). The most prominent contemporary example has been the eastern provinces of the Democratic Republic of the Congo (DRC), where various armies, rebel groups, and outside actors have profited from mining while contributing to violence and exploitation during wars in the region.\n\nThe four most commonly mined conflict minerals (known as 3TGs, from their initials) are cassiterite (for tin), wolframite (for tungsten), coltan (for tantalum), and gold ore, which are extracted from the eastern Congo, and passed through a variety of intermediaries before being purchased. These minerals are essential in the manufacture of a variety of devices, including consumer electronics such as mobile phones, laptops, and MP3 players.\n\nThe extraction and sale of blood diamonds, also known as \"conflict diamonds\", is a better-known phenomenon which occurs under virtually identical conditions. Even petroleum can be a conflict resource; ISIS used oil revenue to finance its military and terrorist activities.\n\nInternational efforts to reduce trade in conflict resources, tried to reduce incentives to extract and fight over them. For example, in the United States, the 2010 Dodd–Frank Wall Street Reform and Consumer Protection Act required manufacturers to audit their supply chains and report use of conflict minerals. In 2015 a US federal appeals court struck down some aspects of the reporting requirements as a violation of corporations’ freedom of speech, but left others in place.\n\nThe concept of 'conflict resource', or 'conflict commodity' emerged in the late 1990s, initially in relation to the 'conflict diamonds' that were financing rebellions in Angola and Sierra Leone. (The media often called these 'blood diamonds'.) Then 'conflict timber' financed hostilities in Cambodia and Liberia.\n\nThe concept was first officially discussed by the UN General Assembly in the context of 'conflict diamonds': The UN Security Council has since referred to conflict resources in several resolutions.\n\nGlobal Witness has called for an international standardized definition to facilitate a more systematic application of UN resolutions, the prevention of complicity in abuses during hostilities by commercial entities exploiting or trading in conflict resources, and the prosecution of war profiteers suspected of supporting or abetting war criminals.\"\nSince 1996 the Bonn International Center for Conversion has tracked \"resource governance\" and \"conflict intensity\" by country. Aside from fossil fuels, metals, diamonds, and timber it tracks the governance of other primary goods that might fund conflicts, including: poppy seeds and talc (Afghanistan), rubber (Côte d'Ivoire), cotton (Zambia), and cocoa (Indonesia).\n\nThe four most prominent conflict minerals, for example codified in the U.S. Conflict Minerals Law, are:\n\n\nThese are sometimes referred to as \"\"the 3T's and gold\", \"3TG\", or even simply the \"3T's\"\". Under the US Conflict Minerals Law, additional minerals may be added to this list in the future.\n\nAs of 2010, the conflict resource fueling the world's deadliest war is gold in the Congo. Gold bars are less traceable than diamonds, and gold is abundant in the Kivu conflict region. In any case, no jewellery industry standard exists for verifying gold origination, as it does for diamonds (though jeweler's total outlay on gold is five times that on diamonds). Other conflict minerals being illicitly exported from the Congo include cobalt, tungsten, cassiterite, and coltan (which provides the tantalum for mobile phones, and is also said to be directly sustaining the conflict).\n\nArmed conflict and mineral resource looting by the Congolese National Army and various armed rebel groups, including the Democratic Forces for the Liberation of Rwanda (FDLR) and the National Congress for the Defense of the People (CNDP), a proxy Rwandan militia group, has occurred throughout the late 20th century and the early 21st century. Additionally, the looting of the Congo's natural resources is not limited to domestic actors. During the Congo Wars (First Congo War (1996–1997) and Second Congo War (1998–2003)), Rwanda, Uganda and Burundi particularly profited from the Congo's resources. These governments continued to smuggle resources out of the Congo to this day.\n\nThe profits from the sale of these minerals have financed fighting in the Second Congo War and ongoing follow-on conflicts. Control of lucrative mines has also itself become a military objective.\n\nMines, in eastern Congo, are often located far from populated areas in remote and dangerous regions. A recent study by International Peace Information Service (IPIS) indicates that armed groups are present at more than 50% of mining sites. At many sites, armed groups illegally tax, extort, and coerce civilians to work. Miners, including children, work up to 48-hour shifts amidst mudslides and tunnel collapses that kill many. The groups are often affiliated with rebel groups, or with the Congolese National Army, but both use rape and violence to control the local population.\n\nIn April 2009, Senator Sam Brownback (R-KS) introduced the Congo Conflict Minerals Act of 2009 (S. 891) to require electronics companies to verify and disclose their sources of cassiterite, wolframite, and tantalum. This legislation died in committee. However, Brownback added similar language as Section 1502 of the Dodd–Frank Wall Street Reform and Consumer Protection Act, which passed Congress and was signed into law by President Barack Obama on July 21, 2010.\n\nThe U.S. Securities and Exchange Commission (SEC) draft regulations to implement the Conflict Mineral Law, published in the Federal Register of December 23, 2010. would have required U.S. and certain foreign companies to report and make public their use of so-called \"conflict minerals\" from the Democratic Republic of the Congo or adjoining countries in their products. Comments on this proposal were extended until March 2, 2011. The comments on the proposal were reviewable by the public.\n\nOne report on the proposal stated the following statistics for the submitted comments:\nThat report also contained what it calls a \"preview of the final SEC regulations\" synthesized from their detailed research and analysis of a large body of documents, reports and other information on the law, proposed regulation and the current budget/political setting facing the SEC in the current administration.\n\nThe final rule went into effect 13 November 2012.\n\nThe SEC rule did not go unnoticed by the international community, including entities seeking to undermine traceability efforts. A report published by a metals trading publication illustrated one DRC ore/mineral flow method that has apparently been devised to thwart detection.\n\nOn July 15, 2011, the US State Department issued a statement on the subject. Section 1502(c) of the Law mandates that the State Department work in conjunction with SEC on certain elements of conflict minerals policy development and support.\n\nOn October 23, 2012 U.S. State Dept Officials asserted that ultimately, it falls on the U.S. State Dept. to determine when this rule would no longer apply.\n\nIn April 2014, the United States Court of Appeals for the District of Columbia Circuit struck down several parts of the SEC Rules as unconstitutional.\n\nUS Conflict Minerals Law contains two requirements that are closely connected: \n\nEven companies not directly regulated by the SEC will be impacted by the audit requirements because they will be pushed down through entire supply chains, including privately held and foreign-owned companies.\n\nSEC estimated that 1,199 \"issuers\" (i.e., companies subject to filing other SEC reports) will be required to submit full conflict mineral reports. This estimate was developed by finding the amount of tantalum produced by the DRC in comparison to global production (15% – 20%). The Commission selected the higher figure of 20% and multiplied that by 6,000 (the total number of \"issuers\" SEC will be required to do initial product/process evaluations). This estimate does not account for the companies who supply materials to the \"issuers\" (but are not themselves SEC-regulated) but who will almost certainly be required to conduct conflict minerals audits to meet the demands of those customers. Other estimates indicate that the total number of US companies likely impacted may exceed 12,000.\n\nA study of the potential impact of the regulation in early 2011 by the \"IPC – Association Connecting Electronic Industries\" trade association. was submitted with the association's comments to the SEC. The study states that the IPC survey respondents had a median of 163 direct suppliers. Applying that number to the SEC's estimated number of impacted issuers results in the possibility of over 195,000 businesses that could be subject to some level of supply chain traceability effort.\n\nUnder the law, companies have to submit an annual conflict minerals report to the SEC if: \n\nA company would be deemed to contract an item to be manufactured if it:\n\nThis language implied that some retailers who are not manufacturers might be subject to the audit and disclosure requirements.\n\n\"Contracting to manufacture\" a product requires some actual influence over the manufacturing of process that product, a determination based on facts and circumstances. A company is not to be deemed to have influence over the manufacturing process if it merely:\n\nThe proposed regulations attempted to clarify that tools used in assembly and manufacturing will not trigger the law. The intent was to cover minerals/metals in the final product only. Nothing specifically addresses intermediate chemical processes that use chemicals that contain conflict minerals. Additionally, neither the law nor the proposed regulation established a de minimis quantity or other form of materiality threshold that would preclude the applicability of the auditing/reporting requirements.\n\nThe law mandates the use of an \"independent private sector auditor\" to conduct the audits. SEC has proposed two different standards for the audits: the \"reasonable inquiry\" and the \"due diligence\". Should the final rule include this structure, the reasonable inquiry would be the first step to determine if the company can on its own, using reasonable efforts and trustworthy information, make a reliable determination as to the source/origin of its tin, tantalum, tungsten and/or gold. Where companies are unable to make such a determination for any reason, they would then be required to take the next step of the \"due diligence\", which is the independent private sector audit.\n\nThe statute specified that the audits be \"conducted in accordance with standards established by the Comptroller General of the United States, in accordance with rules promulgated by the Commission.\" This means that the same auditing standards that apply to other SEC auditing requirements will apply to conflict minerals audits Because of this language, SEC will have little discretion to allow companies to issue self-generated statements or certifications to satisfy the law.\n\nThird party audits for conflict minerals supply chain traceability began in summer 2010 under the Electronic Industry Citizenship Coalition (EICC), a US-based electronics manufacturing trade association. Under this program, EICC selected three audit firms to conduct the actual audits, with two of the three participating in the pilot audits in 2010. After concluding the pilot, one of the two firms involved in 2010 withdrew from the program specifically in response to the SEC's proposal and to reduce potential legal risks to the audited entities.\n\nNeither the law nor the proposed regulations provide guidance on what will be considered an acceptable audit scope or process, preferring to allow companies the flexibility meeting the requirement in a manner that is responsive to their own individual business and supply chain. At the same time, the law contains a provision that preserves the government's rights to deem any report, audit or other due diligence processes as being unreliable, and in such cases, the report shall not satisfy the requirements of the regulations, further emphasizing the need for such audits to conform to established SEC auditing standards. Comments on the proposed regulation pointed out that, should SEC not specify an applicable audit standard, it cannot also be silent or ambiguous on the auditor standards as well, or the Commission will violate the plain language of the Law mandating \"standards established by the Comptroller General of the United States\". It is generally expected that SEC will provide specificity on both the audit standard and the auditor standard. SEC's proposal attempted to clarify its position on auditor requirements.\n\nThe Organisation for Economic Co-operation and Development (OECD) published its Guidance on conflict minerals supply chain traceability. This guidance is gaining much momentum as \"the\" standard within US policy. However, a recent critical analysis of the standard in comparison to existing US auditing standards under SEC highlighted a number of significant inconsistencies and conflict with relevant US standards. Companies subject to the US law who implement the OECD Guidance without regard for the SEC auditing standards may face legal compliance risks.\n\nCompanies subject to the SEC reporting requirement would be required to disclose whether the minerals used in their products originated in the DRC or adjoining countries (as defined above). The law mandates that this reporting be submitted/made available annually. Many comments to the proposed regulation asked SEC to clarify whether the report must be \"furnished\"—meaning it is made available to SEC but not directly incorporated within the company's formal financial report—or \"submitted\"—meaning the report is directly incorporated into the financial report. At first glance, this may appear to be a minor point; however, this difference is very important in determining the audit/auditor standards and related liabilities.\n\nIf it is determined that none of the minerals originated in the DRC or adjoining countries, the report must include a statement to that effect and provide an explanation of the country of origin analysis that was used to arrive at the ultimate conclusion. On the other hand, if conflict minerals originating in the DRC or adjoining countries were used (or if it is not possible to determine the country of origin of the conflict minerals used), companies would be required to state as such in the annual report. In either case, companies would also be required to make this information public by posting their annual conflict minerals report on their websites, and providing the SEC with the internet addresses where the reports may be found. Further, the proposed regulations would require companies to maintain records relating to the country of origin of conflict minerals used in their products.\n\nMedia outlets have reported that many companies required to file Specialized Disclosure Reports to the U.S. Securities and Exchange Commission (SEC) and any necessary conflict minerals reports for 2013 under the SEC’s conflict minerals rule are struggling to meet the June 2, 2014 report filing deadline. Many impacted companies were hoping for clarification regarding filing requirements, from the United States Court of Appeals for the District of Columbia Circuit from a lawsuit filed by the National Association of Manufacturers. The appellate court’s ruling left the necessary conflict minerals reporting requirements largely intact and it has been suggested that impacted companies should review the SEC’s Division of Corporation Finance’s response to the court’s ruling which provides guidance regarding the effect of the appellate court’s ruling.\n\nOn August 18, 2015 the divided D.C. Circuit Court again held the SEC's conflict materials rule violates the First Amendment. Senior Circuit Judge A. Raymond Randolph, joined by Senior Circuit Judge David B. Sentelle, weighed if the required disclosures were effective and uncontroversial. Citing news reports and a Congressional hearing, the court decided the policy was ineffective. The court next found the required label was controversial because it \"is a metaphor that conveys moral responsibility for the Congo war.\" As such, the court struck down the conflict materials rule’s disclosure requirements as a violation of corporations’ freedom of speech. Circuit Judge Sri Srinivasan dissented, writing that the required disclosures were not controversial because they were truthful.\n\nThe law has been criticised for not addressing the root causes of the conflict, leaving to the Congolese government the responsibility for providing an environment in which companies can practice due diligence and legitimately purchase the minerals they need, when the reality is that mechanisms for transparency do not exist. The effect has been to halt legitimate mining ventures that provided livelihoods for people, reducing the Congo's legal exports of tantalum by 90%.\n\nAn investigation by the U.S. Government Accountability Office (GAO) found that most companies were unable to determine the source of their conflict minerals.\n\nTechnology manufacturers criticized a law which required them to label a product as not \"DRC Conflict Free\" as compelled speech, and in violation of the First Amendment.\n\nThe European Parliament passed legislation in 2015; negotiations are currently underway among member states as to specific wording details.\n\nOn 16 June 2016 the European Parliament confirmed that \"mandatory due diligence\" would be required for \"all but the smallest EU firms importing tin, tungsten, tantalum, gold and their ores\".\n\nOn May 17, 2017 the EU passed Regulation (EU) 2017/821 of the Parliament and of the Council on the supply chain due diligence obligations for importers of tin, tantalum, tungsten, their ores, and gold from conflict-affected and high risk areas. The regulation will take effect in January 2021, and will directly apply to companies that import 3TG metals into the EU, no matter where they originate.\n\nOn August 10, 2018 The European Commission published their non-binding guidelines for the identification of conflict-affected and high-risk areas and other supply chain risks under Regulation (EU) 2017/821 of the European Parliament and of the Council.\n\nIncreases in business process outsourcing to globally dispersed production facilities means that social problems and human rights violations are no longer only an organization matter, but also often occur in companies’ supply chains, and challenge for supply chain managers. Besides the harm conflict minerals do where they are produced, human rights violations also raise an enormous risk to corporate reputations. Consumers, mass media and employees expect companies to behave responsibly and have become intolerant of those who don't.\n\nConsequently, firms that are located downstream in the supply chain and that are more visible to stakeholders are particularly threatened by social supply chain problems. The recent debate concerning conflict minerals illustrates the importance of social and human rights issues in supply chain management practice as well as the emerging need to react to social conflicts. Conflict minerals are processed in many different components throughout various industries and hence have a high overall impact on business.\nInitiatives like the Dodd–Frank Wall Street Reform and Consumer Protection Act or the OECD Due Diligence Guidance for Responsible Supply Chains of Minerals from Conflict-Affected and High-Risk Areas demand that supply chain managers verify purchased goods as ‘‘conflict-free’’ or implement measures to better manage any inability to do so.\n\nMinerals mined in Eastern Congo pass through the hands of numerous middlemen as they are shipped out of Congo, through neighboring countries such as Rwanda or Burundi, to East Asian processing plants. Because of this, the US Conflict Minerals Law applies to materials originating (or claimed to originate) from the DRC as well as the nine adjoining countries: Angola, Burundi, Central African Republic, Congo Republic, Rwanda, South Sudan, Zimbabwe, Uganda, and Zambia.\n\nFirms have begun to apply governance mechanisms to avoid adverse effects of conflict mineral sourcing. However, the mere transfer of responsibilities upstream in the supply chain apparently will not stop the trade with conflict minerals, notably due to two reasons: \n\nIn the context of mineral supply chains, due diligence represents a holistic concept that aims at providing a chain of custody tracking from mine to export at country level, regional tracking of mineral flows through the creation of a database on their purchases, independent audits on all actors in the supply chain, and a monitoring of the whole mineral chain by a mineral chain auditor. In this sense, due diligence transcends conventional risk management approaches that usually focus on the prevention of direct impacts on the core business activities of companies. Moreover, due diligence focuses on a maximum of transparency as an end itself while risk management is always directed towards the end of averting direct damages. However, besides the Dodd–Frank Wall Street Reform and Consumer Protection Act and the OECD Guidance, there is still a gap in due diligence practices as international norms are just emerging. Studies found that the motivation for supply chain due diligence as well as expected outcomes of these processes vary among firms. Furthermore, different barriers, drivers, and implementation patterns of supply chain due diligence have been identified in scholarly research.\n\nA number of organizations and celebrities working to find solutions and raise awareness of conflict minerals. These include:\n\n\nMoreover, FairPhone Foundation raises awareness of conflict minerals in the mobile industry and is a company which tries to produce a smart phone with 'fair' conditions along the supply chain. Various industry and trade associations are also monitoring developments in conflict minerals laws and traceability frameworks. Some of these represent electronics, retailers, jewelry, mining, electronics components, and general manufacturing sectors. One organization – ITRI (a UK-based international non-profit organization representing the tin industry and sponsored/supported by its members, principally miners and smelters.) had spearheaded efforts for the development and implementation of a \"bag and tag\" scheme at the mine as a key element of credible traceability. The program and related efforts were initially not likely to extend beyond the pilot phase due to a variety of implementation and funding problems that occurred. In the end however, the device did enter the market.\n\nIn late March 2011, the UK government launched an informational section on its Foreign & Commonwealth Office website dedicated to conflict minerals. This information resource is intended to assist British companies in understanding the issues and, specifically, the US requirements.\n\nOn Jan 6th 2014, the semiconductor giant Intel announced that it would distance itself from conflict minerals. As a result, all Intel microprocessors henceforth will be conflict-free.\n\nManufacturers and supply chain partners needing to comply with the ever-increasing reporting regulations have a few commercial options available.\n\nA major research report from November 2012 by the Southern Africa Resource Watch revealed that gold miners in the east of the Democratic Republic of Congo were being exploited by corrupt government officials, bureaucrats and security personnel, who all demand illegal tax, fees and levies from the miners without delivering any services in return. Despite the alleged gold rush in regions of the country, none of the population and workforce is benefiting from this highly lucrative industry.\n\n\n\n"}
{"id": "2104998", "url": "https://en.wikipedia.org/wiki?curid=2104998", "title": "DXing", "text": "DXing\n\nDXing is the hobby of receiving and identifying distant radio or television signals, or making two way radio contact with distant stations in amateur radio, citizens' band radio or other two way radio communications. Many DXers also attempt to obtain written verifications of reception or contact, sometimes referred to as \"QSLs\" or \"veries\". The name of the hobby comes from DX, telegraphic shorthand for \"distance\" or \"distant\".\n\nThe practice of DXing arose during the early days of radio broadcasting. Listeners would mail \"reception reports\" to radio broadcasting stations in hopes of getting a written acknowledgement or a QSL card that served to officially verify they had heard a distant station. Collecting these cards became popular with radio listeners in the 1920s and 1930s, and reception reports were often used by early broadcasters to gauge the effectiveness of their transmissions. Although international shortwave broadcasts are on the decline, DXing remains popular among dedicated shortwave listeners. The pursuit of two-way contact between distant amateur radio operators is also a significant activity within the amateur radio hobby.\n\nEarly radio listeners, often using home made crystal sets and long wire antennas, found radio stations few and far between. With the broadcast bands uncrowded, signals of the most powerful stations could be heard over hundreds of miles, but weaker signals required more precise tuning or better receiving gear.\n\nBy the 1950s, and continuing through the mid-1970s, many of the most powerful North American \"clear channel\" stations such as KDKA, WLW, CKLW, CHUM, WABC, WJR, WLS, WKBW, KFI, KAAY, KSL and a host of border blasters from Mexico pumped out Top 40 music played by popular disc jockeys. As most smaller, local AM radio stations had to sign off at night, the big 50 kW stations had loyal listeners hundreds of miles away.\n\nThe popularity of DXing the medium-wave band has diminished as the popular music formats quickly migrated to the clearer, though less propagating, FM radio beginning in the 1970s. Meanwhile, the MW band in the United States was getting more and more crowded with new stations and existing stations receiving FCC authorization to operate at night. In Canada, just the opposite occurred as AM stations began moving to FM beginning in the 1980s and continuing through today.\n\nOutside of the Americas and Australia, most AM radio broadcasting was in the form of synchronous networks of government-operated stations, operating with hundreds, even thousands of kilowatts of power. Still, the lower powered stations and occasional trans-oceanic signal were popular DX targets.\n\nEspecially during wartime and times of conflict, reception of international broadcasters, whose signals propagate around the world on the shortwave bands has been popular with both casual listeners and DXing hobbyists.\n\nWith the rise in popularity of streaming audio over the internet, many international broadcasters (including the BBC and Voice of America) have cut back on their shortwave broadcasts. Missionary Religious broadcasters still make extensive use of shortwave radio to reach less developed countries around the world.\n\nIn addition to international broadcasters, the shortwave bands also are home to military communications, RTTY, amateur radio, pirate radio, and the mysterious broadcasts of numbers stations. Many of these signals are transmitted in single side band mode, which requires the use of specialized receivers more suitable to DXing than to casual listening.\n\nThough sporadic in nature, signals on the FM broadcast and VHF television bands - especially those stations at the lower end of these bands - can \"skip\" for hundreds, even thousands of miles. North American FM stations have been received in Western Europe, and European TV signals have been received on the West Coast of the U.S.\n\nPolice, fire, and military communications on the VHF bands are also DX'ed to some extent on multi-band radio scanners, though they are mainly listened to strictly on a local basis. One difficulty is in identifying the exact origins of communications of this nature, as opposed to commercial broadcasters which must identify themselves at the top of each hour, and can often be identified through mentions of sponsors, slogans, etc. throughout their programming.\n\nAmateur radio operators who specialize in making two way radio contact with other amateurs in distant countries are also referred to as \"DXers\". On the HF (also known as shortwave) amateur bands, DX stations are those in foreign countries. On the VHF/UHF amateur bands, DX stations can be within the same country or continent, since making a long-distance VHF contact, without the help of a satellite, can be very difficult. DXers collect QSL cards as proof of contact and can earn special certificates and awards from amateur radio organizations. \n\nIn addition, many clubs offer awards for communicating with a certain number of DX stations. For example, the ARRL offers the DX Century Club award, or DXCC. The basic certificate is awarded for working and confirming at least 100 entities on the ARRL DXCC List. For award purposes, other areas than just political countries can be classified as \"DX countries\". For example, the French territory of Reunion Island in the Indian Ocean is counted as a DX country, even though it is a region of France. The rules for determining what is a DX country can be quite complex and to avoid potential confusion, radio amateurs often use the term \"entity\" instead of country. In addition to entities, some awards are based on island groups in the world's oceans. On the VHF/UHF bands, many radio amateurs pursue awards based on Maidenhead grid locators. \n\nIn order to give other amateurs a chance to confirm contacts at new or exotic locations, amateurs have mounted DXpeditions to countries or regions that have no permanent base of amateur radio operators. There are also frequent contests where radio amateurs operate their stations on certain dates for a fixed period of time to try to communicate with as many DX stations as possible.\n\nMany radio enthusiasts are members of DX clubs. There are many DX clubs in many countries around the world. They are useful places to find information about up-to-date news relating to international radio. Many people also enjoy social events, which can form a large part of the enjoyment that people can get out of the radio hobby.\n\nOne of the interesting sides of DXing as a hobby is collecting QSL cards (acknowledgement cards from the broadcaster) confirming the listener's reception report (sometimes called SINPO report, see next section).\n\nUsually a QSL card will have a picture on one side and the reception data on the other. Most of the broadcasters will use pictures and messages indicating their country's culture or technological life.\n\nSINPO stands for the following qualities, graded on a scale of 1 to 5, where '1' means the quality was very bad and '5' very good.\n\nS - Signal strength\nI - Interference with other stations or broadcasters\nN - Noise ratio in the received signal\nP - Propagation (ups and downs of the reception)\nO - Overall merit\n\nAlthough this is a subjective measure, with practise the grading becomes more consistent, and a particular broadcast may be assessed by several listeners from the same area, in which case the broadcaster could assess correspondence between reports.\n\nAfter listening to a broadcast, the listener writes a report with SINPO values, typically including his geographical location (called QTH in amateur radio terminology) in longitude and latitude, the types of receiver and antennae used, the frequency the transmission was heard on, a brief description of the programme listened to, their opinion about it, suggestions if any, and so on.\n\nThe listener can send the report to the broadcaster either by post or email, and request verification (QSL) from them.\n\nVariants of this report are:\na) the SIO report which omits the Noise and Propagation,\nb) grading on a scale of 1 to 3 (instead of 1 to 5) and\nc) the SINFO report where the F stands for fading.\n\nDX communication is communication over large or relatively uncommon distances. On the UHF or VHF bands which are typically used for short range or line of sight communications, DX may represent communication with stations 50 or 100 miles away. The UHF and microwave bands have also been used to accomplish Earth–Moon–Earth communication between stations worldwide. On the low frequency bands (30 to 300 kHz), contacts between stations separated by more than 100 miles are often considered DX.\n\nAmong amateur radio operators and shortwave listeners, most traditional DX communication occurs on the HF bands, where the ionosphere is used to refract the transmitted radio beam. The beam returns to the Earth's surface, and may then be reflected back into the ionosphere for a second bounce. Ionospheric refraction is generally only feasible for frequencies below about 50 MHz, and is highly dependent upon atmospheric conditions, the time of day, and the eleven-year sunspot cycle. It is also affected by solar storms and some other solar events, which can alter the Earth's ionosphere by ejecting a shower of charged particles.\n\nThe angle of refraction places a minimum on the distance at which the refracted beam will first return to Earth. This distance increases with frequency. As a result, any station employing DX will be surrounded by an annular \"dead zone\" where they can't hear other stations or be heard by them.\n\nThis is the phenomenon that allows short wave radio reception to occur beyond the limits of line of sight. It is utilized by amateur radio enthusiasts (hams), shortwave broadcast stations (such as BBC and Voice of America) and others, and is what allows one to hear AM (MW) stations from areas far from their location. It is one of the backups to failure of long distance communication by satellites, when their operation is affected by electromagnetic storms from the sun.\n\nFor example, in clear ionosphere conditions, one can hear France Inter on 711 kHz, far into the UK and as far as Reading, Berkshire. It is also possible to hear Radio Australia from Melbourne as far away as Lansing, Michigan, a distance of some 9835 miles (15,827 kilometers).\n\nRadio equipment used in DXing ranges from inexpensive portable receivers to deluxe equipment costing thousands of dollars. Using just a simple AM radio, one can easily hear signals from the most powerful stations propagating hundreds of miles at night. Even inexpensive shortwave radio receivers can receive signals emanating from several countries during any time of day.\n\nSerious hobbyists use more elaborate receivers designed specifically for pulling in distant signals, and often build their own antennas designed for a specific frequency band. There is much discussion and debate in the hobby about the relative merits of lesser priced shortwave receivers vs. their multi-thousand dollar \"big brother\" radios. In general, a good desktop or \"PC Radio\" will be able to \"hear\" just about what a very expensive high-performance receiver can receive. The difference between the two types comes into play during difficult band or reception conditions. The expensive receiver will have more filtering options and usually better adjacent channel interference blocking, sometimes resulting in the difference of being able to receive or not receive a signal under poor conditions. Reception of international broadcasting seldom shows a noticeable difference between the two radios. Car radios are also used for DXing the broadcast bands.\n\nAnother recent trend is for the hobbyist to employ multiple radios and antennas connected to a personal computer. Through advanced radio control software, the radios can be automatically ganged together, so that tuning one radio can tune all the others in the group. This DXing technique is sometimes referred to as diversity reception and facilitates easy \"A to B\" comparison of different antennas and receivers for a given signal. For more details on \"PC Radios\" or computer controlled shortwave receivers see the discussion in Shortwave listening.\n\nHaving a minimum of two Dipole antennas at right angles to each other (for example, one running North-South and one running East-West) can produce dramatically different reception patterns. These simple antennas can be made for a few dollars worth of wire and a couple of insulators.\n\n\n"}
{"id": "1065199", "url": "https://en.wikipedia.org/wiki?curid=1065199", "title": "Encyclopedia of Life Support Systems", "text": "Encyclopedia of Life Support Systems\n\nThe Encyclopedia of Life Support Systems (EOLSS) is an encyclopedia on the science of sustainable development and conservation of life support systems on earth. The extensive publication is published under the patronage of UNESCO.\n\nIn 1996, international scientists, engineers, and policy makers were invited by the UNESCO to Washington, Tokyo, Moscow, Mexico City, Beijing and the Bahamas to generate a detailed list of EOLSS contents and achieve a global consensus and acceptance of its structure. Life support system means a natural or human constructed system that furthers the life of the biosphere in a sustainable fashion. The knowledge is structured in 21 topic areas: Earth and atmospheric sciences; Mathematical sciences; Biological, physiological and health sciences; Biotechnology; Land use, land cover and soil sciences; Tropical biology and conservation management; Social sciences and humanities; Physical sciences, engineering and technology resources; Control systems, robotics and automation; Chemical sciences engineering and technology resources; Water sciences, engineering and technology resources; Energy sciences, engineering and technology resources; Environmental and ecological sciences, engineering and technology resources; Food and agricultural sciences, engineering and technology resources; Human resources policy, development and management; Natural resources policy and management; Development and economic sciences; Institutional and infrastructural resources; Technology, information and system management resources; Area studies (Africa, Brazil, Canada and USA, China, Europe, Japan, Russia); Desalination and water resources.\n\nThe way the encyclopedia is being compiled can be defined as crowdsourcing, where thousands of intellectuals from all over the world and across various academic institutions are contributing. The encyclopedia relies on strict standards of peer review and the joint Paris-based UNESCO and EOLSS secretariat manages the encyclopedia in progress. It has got a publishing wing in Oxford as well which specializes in publishing various materials such as books, e-books or scholarly subscriptions.\n\nThe web portal of EOLSS receives maximum hits from the United States and India. If each hit is assumed to originate from one unique user, then USA has about 28% and India has about 10% of the users of this portal. The number of hits falls dramatically after we consider USA and India, whereas countries like Australia, Canada, and the United Kingdom each contribute almost about 5% of the total hits. This shows that people familiar with English are feeling more comfortable in utilizing this project for their educational or knowledge seeking purposes and researches. The proportion of users from states of the European Union who do not speak English as native language is about 8 percent in total. The statistics are updated as of May 2018.\n\n1. Earth and Atmospheric Sciences\n2. Mathematical Sciences\n3. Biological, Physiological and Health Sciences\n4. Biotechnology\n5. Tropical Biology and Conservation Management\n6. Social Sciences and Humanities\n7. Physical Sciences, Engineering and Technology Resources\n8. Control Systems, Robotics and Automation\n9. Chemical Sciences, Engineering and Technology Resources\n10. Water Sciences, Engineering and Technology Resources\n11. Compendium of Desalination and Water Resources\n12. Energy Sciences, Engineering and Technology Resources\n13. Environmental and Ecological Sciences, Engineering and Technology Resources\n14. Food and Agricultural Sciences, Engineering and Technology Resources\n15. Human Resources Policy, Development and Management\n16. Natural Resources Policy and Management\n17. Development and Economic sciences\n18. Institutional and Infrastructural Resources\n19. Technology, Information and Sytems Management Resources\n20. Land Use, Land Cover and Soil Sciences\n21. Area Studies (Regional Sustainable Development Reviews)\n\n"}
{"id": "50816353", "url": "https://en.wikipedia.org/wiki?curid=50816353", "title": "Environmental globalization", "text": "Environmental globalization\n\nEnvironmental globalization refers to the internationally coordinated practices and regulations (often in the form of international treaties) regarding environmental protection. An example of environmental globalization would be the series of International Tropical Timber Agreement treaties (1983, 1994, 2006), establishing International Tropical Timber Organization and promoting sustainable management of tropical forests. Environmental globalization is usually supported by non-governmental organizations and governments of developed countries, but opposed by governments of developing countries which see pro-environmental initiatives as hindering their economic development.\n\nKarl S. Zimmerer defined it as \"the increased role of globally organized management institutions, knowledge systems and monitoring, and coordinated strategies aimed at resource, energy, and conservation issues.\" Alan Grainger in turn wrote that it can be understood as \"an increasing spatial uniformity and contentedness in regular environmental management practices\". Steven Yearley has referred to this concept as \"globalization of environmental concern\". Grainger also cited a study by Clark (2000), which he noted was an early treatment of the concept, and distinguished three aspects of environmental globalization: \"global flows of energy, materials and organisms; formulation and global acceptance of ideas about global environment; and environmental governance\" (a growing web of institutions concerned with global environment).\n\nEnvironmental globalization is related to economic globalization, as economic development on a global scale has environmental impacts on such scale, which is of concern to numerous organizations and individuals. While economic globalization has environmental impacts, those impacts should not be confused with the concept of environmental globalization. In some regards, environmental globalization is in direct opposition to economic globalization, particularly when the latter is described as encouraging trade, and the former, as promoting pro-environment initiatives that are an impediment to trade. For that reason, an environmental activist might be opposed to economic globalization, but advocate environmental globalization.\n\nGrainger has discussed that environmental globalization in the context of international agreements on pro-environmental initiatives. According to him, precursors to modern environmental globalization can be found in the colonial era scientific forestry (research into how to create and restore forests). Modern initiatives contributing to environmental globalization include the 1972 United Nations Conference on the Human Environment, came from the World Bank 1980s requirements that development projects need to protect indigenous peoples and conserve biodiversity. Other examples of such initiative include treaties such as the series of International Tropical Timber Agreement treaties (1983, 1994, 2006). Therefore, unlike other main forms of globalization economic, political and cultural which were already strong in the 19th century, environmental globalization is a more recent phenomena, one that begun in earnest only in the later half of the 20th century. Similarly, Steven Yearley states that it was around that time that the environmental movement started to organize on the international scale focus on the global dimension of the issues (the first Earth Day was celebrated on 1970).\n\nAccording to Grainger, environmental globalization (in the form of pro-environmental international initiatives) is usually supported by various non-governmental organizations and governments of developed countries, and opposed by governments of developing countries (Group of 77), which see pro-environmental initiatives as hindering their economic development. Governmental resistance to environmental globalization takes form or policy ambiguity (exemplified by countries which sign international pro-environmental treaties and pass domestic pro-environmental laws, but then proceed to not enforce them) and collective resistance in forums such as United Nations to projects that would introduce stronger regulations or new institutions policing environmental issues worldwide (such as opposition to the forest-protection agreement during the Earth Summit in 1992, which was eventually downgraded from a binding to a non-binding set of Forest Principles).\n\nWorld Trade Organization has also been criticized as focused on economic globalization (liberalizing trade) over concerns of environmental protection, which are seen as impeding the trade. Steven Yearley states that WTO should not be described as \"anti-environmental\", but its decisions have major impact on environment worldwide, and they are based primarily on economic concerns, with environmental concerns being given secondary weight.\n\n"}
{"id": "723774", "url": "https://en.wikipedia.org/wiki?curid=723774", "title": "FIFA Women's World Rankings", "text": "FIFA Women's World Rankings\n\nThe FIFA Women's World Rankings for football were introduced in 2003, with the first rankings published in March of that year, as a follow-on to the existing FIFA World Rankings for men. They attempt to compare the strength of internationally active women's national teams at any given time.\n\n\nThe first two points result from the FIFA Women's World Rankings system being based on the Elo rating system adjusted for football; in 2018, FIFA modified the men's ranking system to similarly be based on Elo systems after continued criticism. FIFA considers the ratings for teams with fewer than 5 matches provisional and at the end of the list. Also any team that plays no matches for 18 months becomes unranked.\n\nTo date Germany and the United States have been the only two teams to have led the rankings. They have also held the top two spots in all but five releases, when Germany was ranked third: Norway was in second position in the first two rankings until Germany overtook them by winning the 2003 FIFA Women's World Cup, Brazil was ranked second in March and June 2009 until Germany won 2009 Euro and rejoined the top two, and England reached the #2 ranking in March 2018.\n\nThe United States holds the record for the longest period being ranked first: almost 7 years, from March 2008 to December 2014.\n\nIn the latest rankings, action from Women's World Cup qualifying tournaments in UEFA and CONCACAF, as well as the 2018 Tournament of Nations, caused movement throughout the rankings, though three of the top five did not change. The United States (#1), Germany (#2), and Canada (#5) retained their positions while England (#3) and France (#4) swapped. Australia rose to #6 on a 33-point gain, the largest gain among any of the top 50 teams, though Uganda earned the most points of any team with 44.\n\nThe rankings are based on the following formulae:\n\nWhere\nThe average points of all teams are about 1300 points. The top nations usually exceed 2000 points. In order to be ranked, a team must have played at least 5 matches against officially ranked teams, and have not been inactive for more than 18 months. Even if teams are not officially ranked, their points rating is kept constant until they play their next match.\n\nThe main component of the actual result is whether the team wins, loses, or draws, but goal difference is also taken into account.\n\nIf the match results in a winner and loser, the loser is awarded a percentage given by the accompanying table, with the result always less than or equal to 20% (for goal differences greater than zero). The result is based on the goal difference and the number of goals they scored. The remaining percentage points are awarded to the winner. For example, a 2–1 match has the result awarded 84%–16% respectively, a 4–3 match has the result awarded 82%–18%, and an 8–3 match has the result awarded 96.2%–3.8%. As such, it is possible for a team to lose points even if they win a match, assuming they did not \"win by enough\".\n\nIf the match ends in a draw the teams are awarded the same result, but the number depends on the goals scored so the results will not necessarily add up to 100%. For example, a 0–0 draws earns both teams 47% each, a 1–1 draw earns 50% each, and a 4–4 draw earns 52.5% each.\n\nThe following is from a non-winning perspective (loss or draw). The factor for the winning team adds up to 100.\nAnd from a winning perspective.\n\"Source\"\n\nHistorically, home teams earn 66% of the points available to them, with away teams earning the other 34%. To account for this, when two teams are not playing on neutral ground, the home team has its formula_4 inflated by 100 points for the purposes of calculation. That is, if two equally ranked teams playing at one team's home ground, the home team would be expected to win at the same rate a team playing on neutral ground with a 100-point advantage. This 100 point difference corresponds to a 64%–36% advantage in terms of expected result.\n\nThis also helps define the scaling constant formula_5, which has a value of 200. In addition to a 100-point difference causing an expected result difference of 64%–36%, it also results in a 300-point difference causing expected results of 85%–15%.\n\nRankings are published four times a year, usually on a Friday.\n\n\n"}
{"id": "36060804", "url": "https://en.wikipedia.org/wiki?curid=36060804", "title": "FIH World Rankings", "text": "FIH World Rankings\n\nThe FIH World Ranking is a ranking system for men's and women's national teams in field hockey. The teams of the member nations of International Hockey Federation (FIH), field hockey's world governing body, are ranked based on their game results. The rankings were introduced in October 2003.\n\nThe rankings were introduced to overcome the criticism of fixing when drawing the pools for each tournament. It also determines the quotas for tournaments such as the Olympic Games and also the World Cup.\n\nAll of the FIH-recognised, including qualifying and continental tournaments for last four years are included in ranking points calculation. However, the past results will be deducted by the percentage set by the FIH as shown by the tabulated below.\n\nFIH had set the total allocated points for continental tournament. However, a different percentage was set to differ the standard of regional field hockey. Currently, only Europe had full 100% points allocation for all classification while the others had only several finishers with full points allocation. Africa is the sole continent with neither men's or women's tournament had full points allocation.\n\n"}
{"id": "41687529", "url": "https://en.wikipedia.org/wiki?curid=41687529", "title": "FISA Improvements Act", "text": "FISA Improvements Act\n\nThe FISA Improvements Act is a proposed act by Senator Dianne Feinstein, Chair of the Senate Intelligence Committee. Prompted by the disclosure of NSA surveillance by Edward Snowden, it would establish the surveillance program as legal, but impose some limitations on availability of the data. Opponents say the bill would codify warrantless access to many communications of American citizens for use by domestic law enforcement.\n\nIn the wake of the Snowden disclosures, President Obama and many lawmakers believed that restoration of public trust would require legislative changes. More than 20 bills have been written with the goal of reining in government surveillance powers.\n\nOn October 28, 2013, Senator Dianne Feinstein, long described as a staunch defender of the National Security Agency (NSA), announced that a \"total review of all intelligence programs is necessary\".\n\nA bill was introduced by Feinstein on October 31, 2013. Amendments were offered and rejected. The same day it was introduced, the bill passed the United States Senate Select Committee on Intelligence by a vote of 11-4. The committee's report on the bill was published on November 12.\n\nFeinstein issued a press release saying that the bill would impose restrictions on how data is collected, including prohibiting bulk collection of the content of communications, and place a limit of five years on retention of the data. It would make unauthorized access to data obtained under the FISA orders punishable by ten years in prison. The bill would make the FISA court require \"reasonable articulable suspicion\" of association with international terrorism before records are reviewed. It also would set limits on the number of people with access to the data, and set limits on the number of \"hops\" (contact intermediaries) that can be searched. It would require the NSA to make an annual report of the number of queries made and the number of FBI investigations or probable cause orders issued. The bill would also require intelligence agencies to report violations of law to Congress, require a review once per by the Attorney General of collection procedures, and permit the FISA court to invite independent amicus curiae perspectives on cases. It would require Senate confirmation of appointments of the NSA director and NSA Inspector General.\n\nMSNBC reported that the bill \"purports to ban the NSA's controversial bulk collection of communications records under Section 215 of the Patriot Act\" but \"basically allows the NSA to continue bulk collection.\" Feinstein defended data collection in her press release, saying that \"The threats we face—from terrorism, proliferation and cyber attack, among others—are real, and they will continue. Intelligence is necessary to protect our national and economic security, as well as to stop attacks against our friends and allies around the world.\"\n\nThe Senate Intelligence Committee report recommended the bill, saying that the program was \"an effective counterterrorism tool\" and \"was determined by the Department of Justice in two Administrations and by at least fifteen different judges serving on the Foreign Intelligence Surveillance Court (FISC) to be lawful.\" While noting that the committee had encountered inadvertent violations of the law, the majority reported \"It remains the case that, through seven years of oversight of this metadata program under Section 215, the Committee has not identified a single case in which a government official engaged in a willful effort to circumvent or violate Section 215 in the conduct of the bulk telephone metadata program.\" The committee endorsed measures to codify and enhance privacy protections, saying these measures \"could not have been enacted absent the declassification of lawful intelligence activities that were, until recently, properly classified, as to do so would have revealed the programs to our adversaries and thereby compromised their effectiveness.\" However, it condemned the disclosures and said that the leaks \"have not exposed government wrongdoing.\"\n\nIn a minority view attached to the report, Senators Ron Wyden, Mark Udall, and Martin Heinrich wrote \"this bill would codify the government's authority to collect the phone records of huge numbers of law-abiding Americans, and also to conduct warrantless searches for individual Americans' phone calls and emails. We respectfully but firmly disagree with this approach.\" Feinstein's response in the report was that the bill \"does not provide any new legislative authority with which the government may acquire call records or any other information under Section 215--in fact, it narrows the existing authority for it.\"\n\nThe American Civil Liberties Union (ACLU) urged opposition, calling the bill a \"dream come true for the NSA\". The Electronic Frontier Foundation (EFF) called the bill a \"fake fix\" that would \"permanently entrench\" current surveillance practices.\n\nThe ACLU and EFF were among fifty-four \"civil liberties and public interest groups\" that authored a coalition letter to Congressional leadership urging them to oppose the act.\n\nOne area of concern raised by \"The Guardian\", crediting blogger Marcy Wheeler, regards a \"backdoor search provision\" which could allow domestic U.S. law enforcement agencies warrantless access to the data. A FISA court document declassified in 2011 and a leak by Edward Snowden previously published by the newspaper indicated that generally searches of the database for \"U. S. Persons\" was not permitted, but contained a provision that:\nAccording to \"The Guardian\", Section 6 of the Act \"blesses\" such a procedure, permitting intelligence agencies to search \"the contents of communications\" collected overseas for U.S. persons provided that the purpose pertained to \"foreign intelligence information\". The provision was also criticized by Senator Ron Wyden, who said that the bill \"would give intelligence agencies wide latitude to conduct warrantless searches for American phone calls and emails\", instead supporting the USA Freedom Act by Senators Patrick Leahy and F. James Sensenbrenner that would require a search warrant to obtain the information. Sensenbrenner called Feinstein's bill an effort \"for the first time in our country's history to allow unrestrained spying on the American people.\"\n\n\n"}
{"id": "1508112", "url": "https://en.wikipedia.org/wiki?curid=1508112", "title": "Four continents", "text": "Four continents\n\nEuropeans in the 16th century divided the world into four continents: Africa, America, Asia and Europe. Each of the four continents was seen to represent its quadrant of the world—Europe in the north, Asia in the east, Africa in the south, and America in the west. This division fit the Renaissance sensibilities of the time, which also divided the world into four seasons, four classical elements, four cardinal directions, four classical virtues, etc.\n\nThe four parts of the world or the four corners of the world refers to the Americas (the \"west\"), Europe (the \"north\"), Asia (the \"east\"), and Africa (the \"south\"). \n\nBefore the discovery of the New World a commonplace of classical and medieval geography had been the \"three parts\" in which, from Mediterranean and European perspectives, the world was divided: Europe, Asia and Africa. As Laurent de Premierfait, the pre-eminent French translator of Latin literature in the early fifteenth century, informed his readers:\nAsia is one of the three parts of the world, which the authors divide in Asia, Africa and Europe. Asia extends towards the Orient as far as the rising sun (\"devers le souleil levant\"), towards the south (\"midi\") it ends at the great sea, towards the occident it ends at our sea, and towards the north (\"septentrion\") it ends in the Maeotian marshes and the river named \"Thanaus\".\n\nFor Laurent's French readers, Asia ended at \"our sea\", the Mediterranean; Europeans were only dimly aware of the Ural Mountains, which divide Europe from Asia in the eyes of the modern geographer, and which represent the geological suture between two fragmentary continents, or cratons. Instead, the division between these continents in the European-centered picture was the Hellespont, which neatly separated Europe from Asia. From the European perspective, into the Age of Discovery, Asia began beyond the Hellespont with Asia Minor, where the Roman province of Asia had lain, and stretched away to unimaginably exotic and distant places— \"the Orient\".\n\nIn the sixteenth century America too was full of exotic promise: the \"New World\".\n\nIn 1603, Cesare Ripa published a book of emblems for the use of artists and artisans who might be called upon to depict allegorical figures. He covered an astonishingly wide variety of fields, and his work was reprinted many times. It was still being brought up-to-date in the 18th century. The illustrations reveal fixed Eurocentric perceptions of the nature of the \"four corners of the world.\" Ripa's \"Europe\" (\"illustration, left\") is the land of abundance (cornucopia) of kings and the pope, whose crowns and the papal tiara lie at her feet, and of cities.\n\n\"Africa\", by contrast (\"illustration, below right\") wears the elephant headdress (worn by rulers depicted on Hellenistic Bactrian coins) and is accompanied by a lion, the scorpion of the desert sands and Cleopatra's asps. \"Asia\" (\"illustration, right\"), the seat of Religion, carries a smoking censer as a camel takes its ease.\n\nAnd the iconic image of \"America\" (\"illustration, below left\") shows a Native American maiden in a feathered headdress, with bow and arrow. Perhaps she represents a fabled Amazon from the river that already carried the name.\n\nThe American millionaire philanthropist James Hazen Hyde, who inherited a majority share in Equitable Life Assurance Society, formed a collection of allegorical prints illustrating the Four Continents that are now at the New-York Historical Society; Hyde's drawings and a supporting collection of sets of porcelain table ornaments and other decorative arts illustrating the Four Continents were shared by various New York City museums.\nThe Renaissance associated one major river to each of the continents.\nThe Four Rivers theme appears for example in the Fontana dei Quattro Fiumi in the Piazza Navona in Rome.\n\nWith the confirmed discovery that Australia was an island continent, the theme of the \"Four Continents\" lost much of its drive, long before a sixth continent, Antarctica, was discovered. The iconography survived as the Four Corners of the World, however, generally in self-consciously classicizing contexts: for instance, in New York, in front of the Beaux-Arts Alexander Hamilton U.S. Custom House (1907), four sculptural groups by Daniel Chester French symbolize the \"Four Corners of the World.\"\n\n\n\n"}
{"id": "38346167", "url": "https://en.wikipedia.org/wiki?curid=38346167", "title": "Gavi, the Vaccine Alliance", "text": "Gavi, the Vaccine Alliance\n\nGavi, the Vaccine Alliance (Gavi for short; previously the GAVI Alliance, and before that the Global Alliance for Vaccines and Immunization) is a public–private global health partnership committed to increasing access to immunisation in poor countries.\n\nGavi brings together developing country and donor governments, the World Health Organization, UNICEF, the World Bank, the vaccine industry in both industrialised and developing countries, research and technical agencies, civil society, the Bill & Melinda Gates Foundation and other private philanthropists.\n\nGavi was created in 2000 as a successor to the Children's Vaccine Initiative, which had been launched in 1990. \n\nDr. Seth Berkley has been the CEO of GAVI since 2011.\n\nThe Bill and Melinda Gates Foundation has donated $1.5 billion to the alliance as of January 2013.\n\nIn August 2014, Gavi changed its name from \"GAVI Alliance\" and introduced a new logo. Both changes were revealed in August; it had by then acquired the gavi.org domain name and changed its primary domain from gavialliance.org to gavi.org.\n\n"}
{"id": "44023814", "url": "https://en.wikipedia.org/wiki?curid=44023814", "title": "German Parliamentary Committee investigation of the NSA spying scandal", "text": "German Parliamentary Committee investigation of the NSA spying scandal\n\nThe German Parliamentary Committee investigation of the NSA spying scandal (official title: \"1. Untersuchungsausschuss „NSA“\") was started on March 20, 2014, by the German Parliament in order to investigate the extent and background of foreign secret services spying in Germany in the light of the Global surveillance disclosures (2013–present).\nThe Committee is also in search of strategies on how to protect telecommunication with technical means.\n\nThe committee is formed by eight members of the German Parliament. The parliamentarian of the Christian Democratic Union (CDU) Clemens Binninger was head of the committee but stepped down after six days. In a statement, Binninger clarified that the other committee members had insisted on inviting Edward Snowden to testify; Binninger objected to this and resigned in protest. Patrick Sensburg (CDU) succeeded him. \n\nOn May 8, 2014, the committee unanimously decided to let US whistleblower Edward Snowden testify as a witness.\n\nOn September 23, 2014, the Green Party and The Left filed a constitutional complaint against the Christian Democratic Union, the Social Democrats and the NSA Parliamentary Committee because of the Christian Democrats' and the Social Democrats' refusal to let the witness Edward Snowden testify in Berlin. The accused proposed a video conference from Moscow which Snowden had refused.\n\nOn September 28, 2014, the Green Party and The Left filed a constitutional complaint against German chancellor Merkel. According to them, she refuses to comply with her duty according to Chapter 44 of the German constitution to ensure a real investigation; especially by refusing to ensure the legal requirements to allow the witness Edward Snowden to testify.\n\nOn July 3, 2014, the former Technical Director of the NSA, William Binney, who had become a whistleblower after the terrorist attacks of September 11, 2001, testified to the committee. He said that the NSA has a totalitarian approach that has previously only been known from dictatorships and that there is no longer such a thing as privacy. Former NSA employee Thomas Andrews Drake described the close cooperation between the NSA and the German foreign intelligence service BND.\n\nThe US journalist Glenn Greenwald was asked to testify in September, 2014. On August 1, 2014, he wrote in a letter that he was willing to support the Parliament's investigation on the espionage in Germany by the NSA. By their refusal to let the crucial witness Edward Snowden testify, German politicians had however shown that it is more important to them not to annoy the US than to really investigate and he was not willing to take part in a \"ritual\" that \"shall seem like an investigation\".\n\nIn Operation Eikonal German BND agents received \"Selector Lists\" from the NSA − search terms for their dragnet surveillance. They contain IP addresses, mobile phone numbers and email accounts with the BND surveillance system containing hundreds of thousands and possibly more than a million such targets. These lists have been subject of controversy as in 2008 it was revealed that they contained some terms targeting the European Aeronautic Defence and Space Company (EADS), the Eurocopter project as well as French administration, which were first noticed by BND employees in 2005. Other selectors were found to target the administration of Austria. After the revelations made by whistleblower Edward Snowden the BND decided to investigate the issue whose October 2013 conclusion was that at least 2,000 of these selectors were aimed at Western European or even German interests which has been a violation of the Memorandum of Agreement that the US and Germany signed in 2002 in the wake of the 9/11 terror attacks. After reports emerged in 2014 that EADS and Eurocopter had been surveillance targets the Left Party and the Greens filed an official request to obtain evidence of the violations.\n\nThe investigative Parliamentary Committee was set up in spring 2014 and reviewed the selectors and discovered 40,000 suspicious search parameters, including espionage targets in Western European governments and numerous companies. The group also confirmed suspicions that the NSA had systematically violated German interests and concluded that the Americans could have perpetrated economic espionage directly under the Germans' noses. The investigative parliamentary committee was not granted access to the NSA's selectors list as an appeal led by opposition politicians failed at Germany's top court - instead the ruling coalition appointed an administrative judge, Kurt Graulich, as a \"person of trust\" who was granted access to the list and briefed the investigative commission on its contents after analyzing the 40,000 parameters. In his almost 300-paged report Graulich concluded that European government agencies were targeted massively and that Americans hence broke contractual agreements. He also found that German targets which received special protection from surveillance of domestic intelligence agencies by Germany's Basic Law (Grundgesetz) − including numerous enterprises based in Germany − were featured in the NSA's wishlist in a surprising plenitude. While the magnitude differs there have also been problematic BND-internal selectors which have been used until end of 2013 - around two thirds of 3300 targets were related to EU and NATO states. Klaus Landefeld, member of the board at the Internet industry association Eco International, has met intelligence officials and legislators to present suggestions for improvement, like streamlining the selector system.\nOn July 4, 2014, it was revealed to the public that BND agent Markus R. had been arrested on July 2, 2014, for apparently having spied. The 31-year-old German is accused of having worked for the CIA. After his arrest, the US ambassador John B. Emerson was summoned for talks at the German Foreign Office.\n\nIt was revealed that the BND-agent had saved 218 secret BND documents on USB sticks since 2012 and sold them to US agents for a sum of 25,000 Euro in Salzburg, Austria. At least three of the documents were about the NSA Parliamentary Committee.\nThe Federal Office for the Protection of the Constitution had mistaken him for a Russian spy and asked US colleagues to help uncover him.\n\nOn July 9, 2014, a second US spy was discovered, who worked for the Secretary of Defense.\n\nIn July 2014 a Parliament technician found out that the mobile phone of Roderich Kiesewetter, representative of the Christian Democratic Union in the committee, was spied on. Kiesewetter said there is evidence that all four Party representatives in the committee have been spied on.\n\nIn the months following May 2015, Peter Pilz, a member of the Austrian parliament for the Green Party, disclosed several documents and transcripts related to operation Eikonal, in which NSA and BND cooperated for tapping transit cables at a facility of Deutsche Telekom in Frankfurt. These documents were highly sensitive and handed over to the committee for investigating the Eikonal operation. Therefore, it seems likely someone from the committee leaked these documents to Pilz. Among them are lists of communication channels from many European countries, including most of Germany's neighbours. Peter Pilz also discovered NSA spying facilities in Austria, and therefore wants a parliamentary committee on the NSA spying in his own country too.\n\nOn December 1, 2016, WikiLeaks released over 2,400 documents which it claims are from the investigation.\n\n"}
{"id": "1940832", "url": "https://en.wikipedia.org/wiki?curid=1940832", "title": "Global governance", "text": "Global governance\n\nGlobal governance or world governance is a movement towards political cooperation among transnational actors, aimed at negotiating responses to problems that affect more than one state or region. Institutions of global governance—the United Nations, the International Criminal Court, the World Bank, etc.—tend to have limited or demarcated power to enforce compliance. The modern question of world governance exists in the context of globalization and globalizing regimes of power: politically, economically and culturally. In response to the acceleration of worldwide interdependence, both between human societies and between humankind and the biosphere, the term \"global governance\" may name the process of designating laws, rules, or regulations intended for a global scale.\n\nGlobal governance is not a singular system. There is no \"world government\" but the many different regimes of global \"governance\" do have commonalities:\n\nThe term world governance is broadly used to designate all regulations intended for organization and centralization of human societies on a global scale. The Forum for a new World Governance defines world governance simply as \"collective management of the planet\".\n\nTraditionally, \"government\" has been associated with \"governing,\" or with political authority, institutions, and, ultimately, control. \"Governance\" denotes a process through which institutions coordinate and control independent social relations, and that have the ability to enforce, by force, their decisions. However, authors like James Rosenau have also used \"governance\" to denote the regulation of interdependent relations in the absence of an overarching political authority, such as in the international system. Some now speak of the development of \"global public policy\".\n\nAdil Najam, a scholar on the subject at the Pardee School of Global Studies, Boston University has defined global governance simply as \"the management of global processes in the absence of global government.\" According to Thomas G. Weiss, director of the Ralph Bunche Institute for International Studies at the Graduate Center (CUNY) and editor (2000–05) of the journal \"\", \"'Global governance'—which can be good, bad, or indifferent—refers to concrete cooperative problem-solving arrangements, many of which increasingly involve not only the United Nations of states but also 'other UNs,' namely international secretariats and other non-state actors.\" In other words, global governance refers to the way in which global affairs are managed.\n\nThe definition is flexible in scope, applying to general subjects such as global security and order or to specific documents and agreements such as the World Health Organization's Code on the Marketing of Breast Milk Substitutes. The definition applies whether the participation is bilateral (e.g. an agreement to regulate usage of a river flowing in two countries), function-specific (e.g. a commodity agreement), regional (e.g. the Treaty of Tlatelolco), or global (e.g. the Non-Proliferation Treaty). These \"cooperative problem-solving arrangements\" may be formal, taking the shape of laws or formally constituted institutions for a variety of actors (such as state authorities, intergovernmental organizations (IGOs), non-governmental organizations (NGOs), private sector entities, other civil society actors, and individuals) to manage collective affairs. They may also be informal (as in the case of practices or guidelines) or ad hoc entities (as in the case of coalitions).\n\nHowever, a single organization may take the nominal lead on an issue, for example the World Trade Organization (WTO) in world trade affairs. Therefore, global governance is thought to be an international process of consensus-forming which generates guidelines and agreements that affect national governments and international corporations. Examples of such consensus would include WHO policies on health issues.\n\nIn short, global governance may be defined as \"the complex of formal and informal institutions, mechanisms, relationships, and processes between and among states, markets, citizens and organizations, both inter- and non-governmental, through which collective interests on the global plane are articulated, Duties, obligations and privileges are established, and differences are mediated through educated professionals.\"\n\nTitus Alexander, author of \"Unravelling Global Apartheid, an Overview of World Politics\", has described the current institutions of global governance as a system of global apartheid, with numerous parallels with minority rule in the formal and informal structures of South Africa before 1991.\n\nThe dissolution of the Soviet Union in 1991 marked the end of a long period of international history based on a policy of balance of powers. Since this historic event, the planet has entered a phase of geostrategic breakdown. The national-security model, for example, while still in place for most governments, is gradually giving way to an emerging collective conscience that extends beyond the restricted framework it represents.\n\nThe post-Cold War world of the 1990s saw a new paradigm emerge based on a number of issues:\n\n\nGlobal governance can be roughly divided into four stages:\n\nWorld authorities including international organizations and corporations achieve deference to their agenda through different means. Authority can derive from institutional status, expertise, moral authority, capacity, or perceived competence.\n\nIn its initial phase, world governance was able to draw on themes inherited from geopolitics and the theory of international relations, such as peace, defense, geostrategy, diplomatic relations, and trade relations. But as globalization progresses and the number of interdependences increases, the global level is also highly relevant to a far wider range of subjects. Following are a number of examples.\n\n\"The crisis brought about by the accelerated pace and the probably irreversible character of the effect of human activities on nature requires collective answers from governments and citizens. Nature ignores political and social barriers, and the global dimension of the crisis cancels the effects of any action initiated unilaterally by state governments or sectoral institutions, however powerful they may be. Climate change, ocean and air pollution, nuclear risks and those related to genetic manipulation, the reduction and extinction of resources and biodiversity, and above all a development model that remains largely unquestioned globally are all among the various manifestations of this accelerated and probably irreversible effect.\n\nThis effect is the factor, in the framework of globalization, that most challenges a system of states competing with each other to the exclusion of all others: among the different fields of global governance, environmental management is the most wanting in urgent answers to the crisis in the form of collective actions by the whole of the human community. At the same time, these actions should help to model and strengthen the progressive building of this community.\"\n\nProposals in this area have discussed the issue of how collective environmental action is possible. Many multilateral, environment-related agreements have been forged in the past 30 years, but their implementation remains difficult. There is also some discussion on the possibility of setting up an international organization that would centralize all the issues related to international environmental protection, such as the proposed World Environment Organization (WEO). The United Nations Environment Program (UNEP) could play this role, but it is a small-scale organization with a limited mandate. The question has given rise to two opposite views: the European Union, especially France and Germany, along with a number of NGOs, is in favor of creating a WEO; the United Kingdom, the USA, and most developing countries prefer opting for voluntary initiatives.\n\nThe International Institute for Sustainable Development proposes a \"reform agenda\" for global environmental governance. The main argument is that there seems to exist an unspoken but powerful consensus on the essential objectives of a system of global environmental governance. These goals would require top-quality leadership, a strong environmental policy based on knowledge, effective cohesion and coordination, good management of the institutions constituting the environmental governance system, and spreading environmental concerns and actions to other areas of international policy and action.\n\nThe focus of environmental issues shifted to climate change from 1992 onwards. Due to the transboundary nature of climate change, various calls have been made for a World Environment Organisation (WEO) (sometimes referred to as a Global Environment Organisation) to tackle this global problem on a global scale. At present, a single worldwide governing body with the powers to develop and enforce environmental policy does not exist. The idea for the creation of a WEO was discussed thirty years ago but is receiving fresh attention in the light of arguably disappointing outcomes from recent, ‘environmental mega-conferences’(e.g.Rio Summit and Earth Summit 2002).\n\nInternational environmental organisations do exist. The United Nations Environmental Programme (UNEP), created in 1972, coordinates the environmental activity of countries in the UN. UNEP and similar international environmental organisations are seen as not up to the task. They are criticised as being institutionally weak, fragmented, lacking in standing and providing non-optimal environmental protection. It has been stated that the current decentralised, poorly funded and strictly intergovernmental regime for global environmental issues is sub-standard. However, the creation of a WEO may threaten to undermine some of the more effective aspects of contemporary global environmental governance; notably its fragmented nature, from which flexibility stems. This also allows responses to be more effective and links to be forged across different domains. Even though the environment and climate change are framed as global issues, Levin states that ‘it is precisely at this level that government institutions are least effective and trust most delicate’ while Oberthur and Gehring argue that it would offer little more than institutional restructuring for its own sake.\n\nMany proposals for the creation of a WEO have emerged from the trade and environment debate. It has been argued that instead of creating a WEO to safeguard the environment, environmental issues should be directly incorporated into the World Trade Organization (WTO). The WTO has “had success in integrating trade agreements and opening up markets because it is able to apply legal pressure to nation states and resolve disputes”. Greece and Germany are currently in discussion about the possibility of solar energy being used to repay some of Greece’s debt after their economy crashed in 2010. This exchange of resources, if it is accepted, is an example of increased international cooperation and an instance where the WTO could embrace energy trade agreements. If the future holds similar trade agreements, then an environmental branch of the WTO would surely be necessary. However critics of a WTO/WEO arrangement say that this would neither concentrate on more directly addressing underlying market failures, nor greatly improve rule-making.\n\nThe creation of a new agency, whether it be linked to the WTO or not, has now been endorsed by Renato Ruggiero, the former head of the World Trade Organization (WTO), as well as by the new WTO director-designate, Supachai Panitchpakdi. The debate over a global institutional framework for environmental issues will undoubtedly rumble on but at present there is little support for any one proposal.\n\nThe 2008 financial crisis may have undermined faith that laissez-faire capitalism will correct all serious financial malfunctioning on its own, as well as belief in the presumed independence of the economy from politics. It has been stated that, lacking in transparency and far from democratic, international financial institutions may be incapable of handling financial collapses. There are many who believe free-market capitalism may be incapable of forming the economic policy of a stable society, as it has been theorised that it can exacerbate inequalities.\n\nNonetheless, the debate on the potential failings of the system has led the academic world to seek solutions. According to Tubiana and Severino, \"refocusing the doctrine of international cooperation on the concept of public goods offers the possibility . . . of breaking the deadlock in international negotiations on development, with the perception of shared interests breathing new life into an international solidarity that is running out of steam.\"\n\nJoseph Stiglitz argues that a number of global public goods should be produced and supplied to the populations, but are not, and that a number of global externalities should be taken into consideration, but are not. On the other hand, he contends, the international stage is often used to find solutions to completely unrelated problems under the protection of opacity and secrecy, which would be impossible in a national democratic framework.\n\nOn the subject of international trade, Susan George states that \". . . in a rational world, it would be possible to construct a trading system serving the needs of people in both North and South. . . . Under such a system, crushing third world debt and the devastating structural adjustment policies applied by the World Bank and the IMF would have been unthinkable, although the system would not have abolished capitalism.\"\n\nBuilding a responsible world governance that would make it possible to adapt the political organization of society to globalization implies establishing a democratic political legitimacy at every level: local, national, regional and global.\n\nObtaining this legitimacy requires rethinking and reforming, all at the same time:\n\nThe political aspect of world governance is discussed in greater detail in the section Problems of World Governance and Principles of Governance\n\nArmed conflicts have changed in form and intensity since the Berlin wall came down in 1989. The events of 9/11, the wars in Afghanistan and in Iraq, and repeated terrorist attacks all show that conflicts can repercuss well beyond the belligerents directly involved. The major powers and especially the United States, have used war as a means of resolving conflicts and may well continue to do so. If many in the United States believe that fundamentalist Muslim networks are likely to continue to launch attacks, in Europe nationalist movements have proved to be the most persistent terrorist threat. The Global War on Terrorism arguably presents a form of emerging global governance in the sphere of security with the United States leading cooperation among the Western states, non-Western nations and international institutions. Beyer argues that participation in this form of 'hegemonic governance' is caused both by a shared identity and ideology with the US, as well as cost-benefit considerations. Pesawar school attack 2014 is a big challenge to us. Militants from the Pakistani Taliban have attacked an army-run school in Peshawar, killing 141 people, 132 of them children, the military say.\n\nAt the same time, civil wars continue to break out across the world, particularly in areas where civil and human rights are not respected, such as Central and Eastern Africa and the Middle East. These and other regions remain deeply entrenched in permanent crises, hampered by authoritarian regimes, many of them being supported by the United States, reducing entire swathes of the population to wretched living conditions. The wars and conflicts we are faced with have a variety of causes: economic inequality, social conflict, religious sectarianism, Western imperialism, colonial legacies, disputes over territory and over control of basic resources such as water or land. They are all illustrations a deep-rooted crisis of world governance.\n\nThe resulting bellicose climate imbues international relations with competitive nationalism and contributes, in rich and poor countries alike, to increasing military budgets, siphoning off huge sums of public money to the benefit of the arms industry and military-oriented scientific innovation, hence fueling global insecurity. Of these enormous sums, a fraction would be enough to provide a permanent solution for the basic needs of the planet's population hence practically eliminating the causes of war and terrorism.\n\nAndrée Michel argues that the arms race is not only proceeding with greater vigor, it is the surest means for Western countries to maintain their hegemony over countries of the South. Following the break-up of the Eastern bloc countries, she maintains, a strategy for the manipulation of the masses was set up with a permanent invention of an enemy (currently incarnated by Iraq, Iran, Libya, Syria, and North Korea) and by kindling fear and hate of others to justify perpetuating the Military–industrial complex and arms sales. The author also recalls that the \"Big Five\" at the UN who have the veto right are responsible for 85% of arms sales around the world.\n\nProposals for the governance of peace, security, and conflict resolution begin by addressing prevention of the causes of conflicts, whether economic, social, religious, political, or territorial. This requires assigning more resources to improving people's living conditions—health, accommodation, food, and work—and to education, including education in the values of peace, social justice, and unity and diversity as two sides of the same coin representing the global village.\n\nResources for peace could be obtained by regulating, or even reducing military budgets, which have done nothing but rise in the past recent years. This process could go hand in hand with plans for global disarmament and the conversion of arms industries, applied proportionally to all countries, including the major powers. Unfortunately, the warlike climate of the last decade has served to relegate all plans for global disarmament, even in civil-society debates, and to pigeonhole them as a long-term goal or even a Utopian vision. This is definitely a setback for the cause of peace and for humankind, but it is far from being a permanent obstacle.\n\nInternational institutions also have a role to play in resolving armed conflicts. Small international rapid deployment units could intervene in these with an exclusive mandate granted by a reformed and democratic United Nations system or by relevant regional authorities such as the European Union. These units could be formed specifically for each conflict, using armies from several countries as was the case when the UNIFIL was reinforced during the 2006 Lebanon War. On the other hand, no national army would be authorized to intervene unilaterally outside its territory without a UN or regional mandate.\n\nAnother issue that is worth addressing concerns the legitimate conditions for the use of force and conduct during war. Jean-Réné Bachelet offers an answer with the conceptualization of a military ethics corresponding to the need for a \"principle of humanity.\" The author defines this principle as follows: \"All human beings, whatever their race, nationality, gender, age, opinion, or religion, belong to one same humanity, and every individual has an inalienable right to respect for his life, integrity, and dignity.\"\n\nThe World Trade Organization's (WTO) agenda of liberalizing public goods and services are related to culture, science, education, health, living organisms, information, and communication. This plan has been only partially offset by the alter-globalization movement, starting with the events that took place at the 1999 Seattle meeting, and on a totally different and probably far more influential scale in the medium and long term, by the astounding explosion of collaborative practices on the Internet. However, lacking political and widespread citizen support as well as sufficient resources, civil society has not so far been able to develop and disseminate alternative plans for society as a whole on a global scale, even though plenty of proposals and initiatives have been developed, some more successful than others, to build a fairer, more responsible, and more solidarity-based world in all of these areas.\n\nAbove all, each country tries to impose their values and collective preferences within international institutions such like WTO or UNESCO, particularly in the Medias sector. This is an excellent opportunity to promote their soft power, for instance with the promotion of the cinema\n\nAs far as science is concerned, \"[r]esearch increasingly bows to the needs of financial markets, turning competence and knowledge into commodities, making employment flexible and informal, and establishing contracts based on goals and profits for the benefit of private interests in compliance with the competition principle. The directions that research has taken in the past two decades and the changes it has undergone have drastically removed it from its initial mission (producing competence and knowledge, maintaining independence) with no questioning of its current and future missions. Despite the progress, or perhaps even as its consequence, humankind continues to face critical problems: poverty and hunger are yet to be vanquished, nuclear arms are proliferating, environmental disasters are on the rise, social injustice is growing, and so on.\n\nNeoliberal commercialization of the commons favors the interests of pharmaceutical companies instead of the patients', of food-processing companies instead of the farmers' and consumers'. Public research policies have done nothing but support this process of economic profitability, where research results are increasingly judged by the financial markets. The system of systematically patenting knowledge and living organisms is thus being imposed throughout the planet through the 1994 WTO agreements on intellectual property. Research in many areas is now being directed by private companies.\"\n\nOn the global level, \"[i]nstitutions dominating a specific sector also, at every level, present the risk of reliance on technical bodies that use their own references and deliberate in an isolated environment. This process can be observed with the 'community of patents' that promotes the patenting of living organisms, as well as with authorities controlling nuclear energy. This inward-looking approach is all the more dangerous that communities of experts are, in all complex technical and legal spheres, increasingly dominated by the major economic organizations that finance research and development.\"\n\nOn the other hand, several innovative experiments have emerged in the sphere of science, such as: conscience clauses and citizens' panels as a tool for democratizing the production system: science shops and community-based research. Politically committed scientists are also increasingly organizing at the global level.\n\nAs far as education is concerned, the effect of commoditization can be seen in the serious tightening of education budgets, which affects the quality of general education as a public service. The Global Future Online report reminds us that \". . . at the half-way point towards 2015 \"(author's note: the deadline for the Millennium Goals)\", the gaps are daunting: 80 million children (44 million of them girls) are out of school, with marginalized groups (26 million disabled and 30 million conflict-affected children) continuing to be excluded. And while universal access is critical, it must be coupled with improved learning outcomes—in particular, children achieving the basic literacy, numeracy and life skills essential for poverty reduction.\"\n\nIn addition to making the current educational system available universally, there is also a call to improve the system and adapt it to the speed of changes in a complex and unpredictable world. On this point, Edgar Morin asserts that we must \"[r]ethink our way of organizing knowledge. This means breaking down the traditional barriers between disciplines and designing new ways to reconnect that which has been torn apart.\" The UNESCO report drawn up by Morin contains \"seven principles for education of the future\": detecting the error and illusion that have always parasitized the human spirit and human behavior; making knowledge relevant, i.e. a way of thinking that makes distinctions and connections; teaching the human condition; teaching terrestrial identity; facing human and scientific uncertainties and teaching strategies to deal with them; teaching understanding of the self and of others, and an ethics for humankind.\n\nThe exponential growth of new technologies, the Internet in particular, has gone hand in hand with the development over the last decade of a global community producing and exchanging goods. This development is permanently altering the shape of the entertainment, publishing, and music and media industries, among others. It is also influencing the social behavior of increasing numbers of people, along with the way in which institutions, businesses, and civil society are organized. Peer-to-peer communities and collective knowledge-building projects such as Wikipedia have involved millions of users around the world. There are even more innovative initiatives, such as alternatives to private copyright such as Creative Commons, cyber democracy practices, and a real possibility of developing them on the sectoral, regional, and global levels.\n\nRegional players, whether regional conglomerates such as Mercosur and the European Union, or major countries seen as key regional players such as China, the United States, and India, are taking a growing interest in world governance. Examples of discussion of this issue can be found in the works of: Martina Timmermann \"et al.\", \"Institutionalizing Northeast Asia: Regional Steps toward Global Governance\"; Douglas Lewis, \"Global Governance and the Quest for Justice - Volume I: International and Regional Organizations\"; Olav Schram Stokke, \"Examining the Consequences of International Regimes,\" which discusses Northern, or Arctic region building in the context of international relations; Jeffery Hart and Joan Edelman Spero, \"Globalization and Global Governance in the 21st Century,\" which discusses the push of countries such as Mexico, Brazil, India, China, Taiwan, and South Korea, \"important regional players\" seeking \"a seat at the table of global decision-making\"; Dr. Frank Altemöller, “International Trade: Challenges for Regional and Global Governance: A comparison between Regional Integration Models in Eastern Europe and Africa – and the role of the WTO”, and many others.\n\nInterdependence among countries and regions hardly being refutable today, regional integration is increasingly seen not only as a process in itself, but also in its relation to the rest of the world, sometimes turning questions like \"What can the world bring to my country or region?\" into \"What can my country or region bring to the rest of the world?\" Following are a few examples of how regional players are dealing with these questions.\n\nOften seen as a problem to be solved rather than a people or region with an opinion to express on international policy, Africans and Africa draw on a philosophical tradition of community and social solidarity that can serve as inspiration to the rest of the world and contribute to building world governance. One example is given by Sabelo J. Ndlovu-Gathseni when he reminds us of the relevance of the Ubuntu concept, which stresses the interdependence of human beings.\n\nAfrican civil society has thus begun to draw up proposals for governance of the continent, which factor in all of the dimensions: local, African, and global. Examples include proposals by the network \"Dialogues sur la gouvernance en Afrique\" for \"the construction of a local legitimate governance,\" state reform \"capable of meeting the continent's development challenges,\" and \"effective regional governance to put an end to Africa's marginalization.\"\n\nForeign-policy proposals announced by President Barack Obama include restoring the Global Poverty Act, which aims to contribute to meeting the UN Millennium Development Goals to reduce by half the world population living on less than a dollar a day by 2015. Foreign aid is expected to double to 50 billion dollars. The money will be used to help build educated and healthy communities, reduce poverty and improve the population's health.\n\nIn terms of international institutions, The White House Web site advocates reform of the World Bank and the IMF, without going into any detail.\n\nBelow are further points in the Obama-Biden plan for foreign policy directly related to world governance:\n\nThe 21st century has seen the arrival of a new and diverse generation of left-wing governments in Latin America. This has opened the door to initiatives to launch political and governance renewal. A number of these initiatives are significant for the way they redefine the role of the state by drawing on citizen participation, and can thus serve as a model for a future world governance built first and foremost on the voice of the people. The constituent assemblies in Ecuador and Bolivia are fundamental examples of this phenomenon.\n\nIn Ecuador, social and indigenous movements were behind the discussions that began in 1990 on setting up a constituent assembly. In the wake of Rafael Correa's arrival at the head of the country in November 2006, widespread popular action with the slogan \"que se vayan todos\" (let them all go away) succeeded in getting all the political parties of congress to accept a convocation for a referendum on setting up the assembly.\n\nIn April 2007, Rafael Correa's government organized a consultation with the people to approve setting up a constituent assembly. Once it was approved, 130 members of the assembly were elected in September, including 100 provincial members, 24 national members and 6 for migrants in Europe, Latin America and the USA. The assembly was officially established in November. Assembly members belonged to traditional political parties as well as the new social movements. In July 2008, the assembly completed the text for the new constitution and in September 2008 there was a referendum to approve it. Approval for the new text won out, with 63.9% of votes for compared to 28.1% of votes against and a 24.3% abstention rate.\n\nThe new constitution establishes the rule of law on economic, social, cultural and environmental rights (ESCER). It transforms the legal model of the social state subject to the rule of law into a \"constitution of guaranteed well-being\" (\"Constitución del bienestar garantizado\") inspired by the ancestral community ideology of \"good living\" propounded by the Quechuas of the past, as well as by 21st century socialist ideology. The constitution promotes the concept of food sovereignty by establishing a protectionist system that favors domestic production and trade. It also develops a model of public aid for education, health, infrastructures and other services.\n\nIn addition, it adds to the three traditional powers, a fourth power called the Council of Citizen Participation and Social Control, made up of former constitutional control bodies and social movements, and mandated to assess whether public policies are constitutional or not.\n\nThe new Bolivian constitution was approved on 25 January 2009 by referendum, with 61.4% votes in favor, 38.6% against and a 90.2% turnout. The proposed constitution was prepared by a constituent assembly that did not only reflect the interests of political parties and the elite, but also represented the indigenous peoples and social movements. As in Ecuador, the proclamation of a constituent assembly was demanded by the people, starting in 1990 at a gathering of indigenous peoples from the entire country, continuing with the indigenous marches in the early 2000s and then with the Program Unity Pact (\"Pacto de Unidad Programático\") established by family farmers and indigenous people in September 2004 in Santa Cruz.\n\nThe constitution recognizes the autonomy of indigenous peoples, the existence of a specific indigenous legal system, exclusive ownership of forest resources by each community and a quota of indigenous members of parliament. It grants autonomy to counties, which have the right to manage their natural resources and elect their representatives directly. The \"latifundio\" system has been outlawed, with maximum ownership of 5,000 hectares allowed per person. Access to water and sanitation are covered by the constitution as human rights that the state has to guarantee, as well as other basic services such as electricity, gas, postal services, and telecommunications that can be provided by either the state or contracting companies. The new constitution also establishes a social and community economic model made up of public, private, and social organizations, and cooperatives. It guarantees private initiative and freedom of enterprise, and assigns public organizations the task of managing natural resources and related processes as well as developing public services covered by the constitution. National and cooperative investment is favored over private and international investment.\nThe \"unitary plurinational\" state of Bolivia has 36 official indigenous languages along with Spanish. Natural resources belong to the people and are administered by the state. The form of democracy in place is no longer considered as exclusively representative and/or based on parties. Thus, \"the people deliberate and exercise government via their representatives and the constituent assembly, the citizen legislative initiative and the referendum . . .\" and \"popular representation is exercised via the political parties, citizen groups, and indigenous peoples.\" This way, \"political parties, and/or citizen groups and/or indigenous peoples can present candidates directly for the offices of president, vice-president, senator, house representative, constituent-assembly member, councilor, mayor, and municipal agent. The same conditions apply legally to all. . . .\"\n\nAlso in Latin America: \"Amazonia . . . is an enormous biodiversity reservoir and a major climate-regulation agent for the planet but is being ravaged and deteriorated at an accelerated pace; it is a territory almost entirely devoid of governance, but also a breeding place of grassroots organization initiatives.\". \"Amazonia can be the fertile field of a true school of 'good' governance if it is looked after as a common and valuable good, first by Brazilians (65% of Amazonia is within Brazilian borders) and the people of the South American countries surrounding it, but also by all the Earth's inhabitants.\"\nAccordingly, \"[f]rom a world-governance perspective, [Amazonia] is in a way an enormous laboratory. Among other things, Amazonia enables a detailed examination of the negative effects of productivism and of the different forms of environmental packaging it can hide behind, including 'sustainable development.' Galloping urbanization, Human Rights violations, the many different types of conflicts (14 different types of conflicts have been identified within the hundreds of cases observed in Amazonia), protection of indigenous populations and their active participation in local governance: these are among the many Amazonian challenges also affecting the planet as a whole, not to mention the environment. The hosts of local initiatives, including among the indigenous populations, are however what may be most interesting in Amazonia in that they testify to the real, concrete possibility of a different form of organization that combines a healthy local economy, good social cohesion, and a true model of sustainable development—this time not disguised as something else. All of this makes Amazonia 'a territory of solutions.'\"\n\nAccording to Arnaud Blin, the Amazonian problem helps to define certain fundamental questions on the future of humankind. First, there is the question of social justice: \"[H]ow do we build a new model of civilization that promotes social justice? How do we set up a new social architecture that allows us to live together?\" The author goes on to refer to concepts such as the concept of \"people's territory \" or even \"life territory\" rooted in the indigenous tradition and serving to challenge private property and social injustice. He then suggests that the emerging concept of the \"responsibility to protect,\" following up on the \"right of humanitarian intervention\" and until now used to try to protect populations endangered by civil wars, could also be applied to populations threatened by economic predation and to environmental protection.\n\nThe growing interest in world governance in Asia represents an alternative approach to official messages, dominated by states' nationalist visions. An initiative to develop proposals for world governance took place in Shanghai in 2006, attended by young people from every continent. The initiative produced ideas and projects that can be classified as two types: the first and more traditional type, covering the creation of a number of new institutions such as an International Emissions Organization, and a second more innovative type based on organizing network-based systems. For example, a system of cooperative control on a worldwide level among states and self-organization of civil society into networks using new technologies, a process that should serve to set up a \"Global Calling-for-Help Center\" or a new model based on citizens who communicate freely, share information, hold discussions, and seek consensus-based solutions. They would use the Internet and the media, working within several types of organizations: universities, NGOs, local volunteers and civil-society groups.\n\nGiven the demographic importance of the continent, the development of discussion on governance and practices in Asia at the regional level, as well as global-level proposals, will be decisive in the years ahead in the strengthening of global dialog among all sorts of stakeholders, a dialog that should produce a fairer world order.(See Kishore Mahbubani).\n\nAccording to Michel Rocard, Europe does not have a shared vision, but a collective history that allows Europeans to opt for projects for gradual political construction such as the European Union. Drawing on this observation, Rocard conceives of a European perspective that supports the development of three strategies for constructing world governance: reforming the UN, drawing up international treaties to serve as the main source of global regulations, and \"the progressive penetration of the international scene by justice.\"\n\nRocard considers that there are a number of \"great questions of the present days\" including recognition by all nations of the International Criminal Court, the option of an international police force authorized to arrest international criminals, and the institution of judicial procedures to deal with tax havens, massively polluting activities, and states supporting terrorist activities. He also outlines \"new problems\" that should foster debate in the years to come on questions such as a project for a Declaration of Interdependence, how to re-equilibrate world trade and WTO activities, and how to create world regulations for managing collective goods (air, drinking water, oil, etc.) and services (education, health, etc.).\n\nMartin Ortega similarly suggests that the European Union should make a more substantial contribution to global governance, particularly through concerted action in international bodies. European states, for instance, should reach an agreement on the reform of the United Nations Security Council.\n\nIn 2011, the European Strategy and Policy Analysis System (ESPAS), an inter-institutional pilot project of the European Union which aims to assist EU policy formulation through the identification and critical analysis of long-term global trends, highlighted the importance of expanding global governance over the next 20 years.\n\nIt is too soon to give a general account of the view of world-governance stakeholders, although interest in world governance is on the rise on the regional level, and we will certainly see different types of stakeholders and social sectors working to varying degrees at the international level and taking a stand on the issue in the years to come.\n\nThe World Parliamentary Forum, open to members of parliament from all nations and held every year at the same time as the World Social Forum, drew up a declaration at the sixth forum in Caracas in 2006. The declaration contains a series of proposals that express participants' opinion on the changes referred to.\n\nThe European Commission referred to global governance in its White Paper on European Governance. It contends that the search for better global governance draws on the same set of shared challenges humanity is currently facing. These challenges can be summed up by a series of goals: sustainable development, security, peace and equity (in the sense of \"fairness\").\n\nThe freedom of thought enjoyed by non-state stakeholders enables them to formulate truly alternative ideas on world-governance issues, but they have taken little or no advantage of this opportunity.\n\nPierre Calame believes that \"[n]on-state actors have always played an essential role in global regulation, but their role will grow considerably in this, the beginning of the twenty-first Century . . . Non-state actors play a key role in world governance in different domains . . . To better understand and develop the non-state actors' role, it should be studied in conjunction with the general principles of governance.\" \"Non-state actors, due to their vocation, size, flexibility, methods of organization and action, interact with states in an equal manner; however this does not mean that their action is better adapted.\"\n\nOne alternative idea encapsulated by many not-for-profit organisations relates to ideas in the 'Human Potential Movement' and might be summarised as a mission statement along these lines: 'To create an accepted framework for all humankind, that is self-regulating and which enables every person to achieve their fullest potential in harmony with the world and its place in existence.'\n\nSince the Rio Earth Summit in 1992, references to the universal collective of humanity have begun using the term 'humankind' rather than 'mankind', given the gender neutral quality of the former.\n\n'Self-regulation' is meant to invoke the concept of regulation which includes rule-making such as laws, and related ideas e.g. legal doctrine as well as other frameworks. However its scope is wider than this and intended to encompass cybernetics which allows for the study of regulation in as many varied contexts as possible from the regulation of gene expression to the Press Complaints Commission for example.\n\nSince 2005, religious leaders from a diverse array of faith traditions have engaged in dialogue with G8 leaders around issues of global governance and world risk. Drawing on the cultural capital of diverse religious traditions, they seek to strengthen democratic norms by influencing political leaders to include the interests of the most vulnerable when they make their decisions. Some have argued that religion is a key to transforming or fixing global governance.\n\nSeveral stakeholders have produced lists of proposals for a new world governance that is fairer, more responsible, solidarity-based, interconnected and respectful of the planet's diversity. Some examples are given below.\n\nJoseph E. Stiglitz proposes a list of reforms related to the internal organization of international institutions and their external role in the framework of global-governance architecture. He also deals with global taxation, the management of global resources and the environment, the production and protection of global knowledge, and the need for a global legal infrastructure.\n\nA number of other proposals are contained in the World Governance Proposal Paper: giving concrete expression to the principle of responsibility; granting civil society greater involvement in drawing up and implementing international regulations; granting national parliaments greater involvement in drawing up and implementing international regulations; re-equilibrating trade mechanisms and adopting regulations to benefit the southern hemisphere; speeding up the institution of regional bodies; extending and specifying the concept of the commons; redefining proposal and decision-making powers in order to reform the United Nations; developing independent observation, early-warning, and assessment systems; diversifying and stabilizing the basis for financing international collective action; and engaging in a wide-reaching process of consultation, a new Bretton Woods for the United Nations.\n\nThis list provides more examples of proposals:\n\nDr. Rajesh Tandon, president of the FIM (Montreal International Forum) and of PRIA (Participatory Research in Asia), prepared a framework document entitled \"Democratization of Global Governance for Global Democracy: Civil Society Visions and Strategies (G05) conference.\" He used the document to present five principles that could provide a basis for civil society actions: \"Global institutions and agenda should be subjected to democratic political accountability.\"\n\nVijaya Ramachandran, Enrique Rueda-Sabater and Robin Kraft also define principles for representation of nations and populations in the system of global governance. They propose a \"Two Percent Club\" that would provide for direct representation of nations with at least two percent of global population or global GDP; other nations would be represented within international fora through regional blocs.\n\nIn the light of the unclear meaning of the term \"global governance\" as a concept in international politics, some authors have proposed defining it not in substantive, but in disciplinary and methodological terms. For these authors, global governance is better understood as an analytical concept or optic that provides a specific perspective on world politics different from that of conventional international relations theory. Thomas G. Weiss and Rorden Wilkinson have even argued that global governance has the capacity to overcome some of the fragmentation of international relations as a discipline particularly when understood as a set of questions about the governance of world orders.\n\nSome universities, including those offering courses in international relations, have begun to establish degree programmes in global governance.\n\nThere are those who believe that world architecture depends on establishing a system of world governance. However, the equation is currently becoming far more complicated: Whereas the process used to be about regulating and limiting the individual power of states to avoid disturbing or overturning the status quo, the issue for today's world governance is to have a collective influence on the world's destiny by establishing a system for regulating the many interactions that lie beyond the province of state action. The political homogenization of the planet that has followed the advent of what is known as liberal democracy in its many forms should make it easier to establish a world governance system that goes beyond market laissez-faire and the democratic peace originally formulated by Immanuel Kant, which constitutes a sort of geopolitical laissez-faire.\n\nAnother view regarding the establishment of global governance is based on the difficulties to achieve equitable development at the world scale. \"To secure for all human beings in all parts of the world the conditions allowing a decent and meaningful life requires enormous human energies and far-reaching changes in policies. The task is all the more demanding as the world faces numerous other problems, each related to or even part of the development challenge, each similarly pressing, and each calling for the same urgent attention. But, as Arnold Toynbee has said, 'Our age is the first generation since the dawn of history in which mankind dares to believe it practical to make the benefits of civilization available to the whole human race'.\"\n\nBecause of the heterogeneity of preferences, which are enduring despite globalization, are often perceived as an implacable homogenization process. Americans and Europeans provide a good example of this point: on some issues they have differing common grounds in which the division between the public and private spheres still exist. Tolerance for inequalities and the growing demand for redistribution, attitudes toward risk, and over property rights vs human rights, set the stage. In certain cases, globalization even serves to accentuate differences rather than as a force for homogenization. Responsibility must play its part with respect to regional and International governments, when balancing the needs of its citizenry.\n\nWith the growing emergence of a global civic awareness, comes opposition to globalization and its effects. A rapidly growing number of movements and organizations have taken the debate to the international level. Although it may have limitations, this trend is one response to the increasing importance of world issues, that effect the planet.\n\nPierre Jacquet, Jean Pisani-Ferry, and Laurence Tubiana argue that \"[t]o ensure that decisions taken for international integration are sustainable, it is important that populations see the benefits, that states agree on their goals and that the institutions governing the process are seen as legitimate. These three conditions are only partially being met. Taklya\"\n\nThe authors refer to a \"crisis of purpose\" and international institutions suffering from \"imbalance\" and inadequacy. They believe that for these institutions, \"a gap has been created between the nature of the problems that need tackling and an institutional architecture which does not reflect the hierarchy of today's problems. For example, the environment has become a subject of major concern and central negotiation, but it does not have the institutional support that is compatible with its importance.\"\n\nGlobal governance is not world government, and even less democratic globalization. In fact, global governance would not be necessary, were there a world government. Domestic governments have monopolies on the use of force—the power of enforcement. Global governance refers to the political interaction that is required to solve problems that affect more than one state or region when there is no power to enforce compliance. Problems arise, and networks of actors are constructed to deal with them in the absence of an international analogue to a domestic government. This system has been termed disaggregated sovereignty.\n\nImproved global problem solving need not involve the establishment of additional powerful formal global institutions. It does involve building consensus on norms and practices. One such area, currently under construction, is the development and improvement of accountability mechanisms. For example, the UN Global Compact brings together companies, UN agencies, labor organizations, and civil society to support universal environmental and social principles. Participation is entirely voluntary, and there is no enforcement of the principles by an outside regulatory body. Companies adhere to these practices both because they make economic sense, and because stakeholders, especially shareholders, can monitor their compliance easily. Mechanisms such as the Global Compact can improve the ability of affected individuals and populations to hold companies accountable. However, corporations participating in the UN Global Compact have been criticized for their merely minimal standards, the absence of sanction-and-control measures, their lack of commitment to social and ecological standards, minimal acceptance among corporations around the world, and the high cost involved in reporting annually to small and medium-sized business\n\nBitcoin & Beyond: Blockchains, Globalization, and Global Governance workshop brings together an interdisciplinary group of researchers to examine the implications that blockchains pose for globalization and global governance.\n\nOne effect of globalization is the increasing regulation of businesses in the global marketplace. Jan Aart Scholte asserts, however, that these changes are inadequate to meet the needs: \"Along with the general intensified globalization of social relations in contemporary history has come an unprecedented expansion of regulatory apparatuses that cover planetary jurisdictions and constituencies. On the whole, however, this global governance remains weak relative to pressing current needs for global public policy. Shortfalls in moral standing, legal foundations, material delivery, democratic credentials and charismatic leadership have together generated large legitimacy deficits in existing global regimes.\"\n\nProposals and initiatives have been developed by various sources to set up networks and institutions operating on a global scale: political parties, unions, regional authorities, and members of parliament in sovereign states.\n\nOne of the conditions for building a world democratic governance should be the development of platforms for citizen dialogue on the legal formulation of world governance and the harmonization of objectives.\n\nThis legal formulation could take the form of a Global Constitution. According to Pierre Calame and Gustavo Marin, \"[a] Global Constitution resulting from a process for the institution of a global community will act as the common reference for establishing the order of rights and duties applicable to United Nations agencies and to the other multilateral institutions, such as the International Monetary Fund, the World Bank, and the World Trade Organization.\" As for formulating objectives, the necessary but insufficient ambition of the United Nations Millennium Development Goals, which aim to safeguard humankind and the planet, and the huge difficulties in implementing them, illustrates the inadequacy of institutional initiatives that do not have popular support for having failed to invite citizens to take part in the elaboration process.\n\nFurthermore, the Global Constitution \"must clearly express a limited number of overall objectives that are to be the basis of global governance and are to guide the common action of the U.N. agencies and the multilateral institutions, where the specific role of each of these is subordinated to the pursuit of these common objectives.\"\n\nCalame proposes the following objectives:\n\nIs the UN capable of taking on the heavy responsibility of managing the planet's serious problems? More specifically, can the UN reform itself in such a way as to be able to meet this challenge? At a time when the financial crisis of 2008 is raising the same questions posed by the climate disasters of previous years regarding the unpredictable consequences of disastrous human management, can international financial institutions be reformed in such a way as to go back to their original task, which was to provide financial help to countries in need?\n\nLack of political will and citizen involvement at the international level has also brought about the submission of international institutions to the \"neoliberal\" agenda, particularly financial institutions such as the World Bank, the International Monetary Fund, and the World Trade Organization (WTO). Pierre Calame gives an account of this development, while Joseph E. Stiglitz points out that the need for international institutions like the IMF, the World Bank, and the WTO has never been so great, but people's trust in them has never been so low.\n\nOne of the key aspects of the United Nations reform is the problem of the representativeness of the General Assembly. The Assembly operates on the principle of \"one state, one vote,\" so that states of hugely varying sizes have the same effect on the vote, which distorts representativeness and results in a major loss of credibility. Accordingly, \"the General Assembly has lost any real capacity to influence. This means that the mechanisms for action and consultation organized by rich countries have the leading role.\"\n\nGustave Massiah advocates defining and implementing a radical reform of the UN. The author proposes building new foundations that can provide the basis for global democracy and the creation of a Global Social Contract, rooted in the respect and protection of civil, political, economic, social, and cultural rights, as well as in the recognition of the strategic role of international law.\n\nThere is the jurisdictional gap, between the increasing need for global governance in many areas - such as health - and the lack of an authority with the power, or jurisdiction, to take action.\nMoreover, the gap of incentive between the need for international cooperation and the motivation to undertake it. The incentive gap is said to be closing as globalization provides increasing impetus for countries to cooperate. However, there are concerns that, as Africa lags further behind economically, its influence on global governance processes will diminish. At last, the participation gap, which refers to the fact that international cooperation remains primarily the affair of governments, leaving civil society groups on the fringes of policy-making. On the other hand, globalization of communication is facilitating the development of global civil society movements.\n\nInadequate global institutions, agreements or networks as well as political and national interests may impede global governance and lead to failures. Such are the consequence of ineffective global governance processes. Qin calls it a necessity to \"reconstruct ideas for effective global governance and sustainable world order, which should include the principles of pluralism, partnership, and participation\" for a change to this phenomenon. The 2012 Global Risks Report places global governance failure at the center of gravity in its geopolitical category.\n\nStudies of global governance are conducted at several academic institutions such as the LSE Department of International Relations (with a previous institution LSE Global Governance closed as a formal research centre of the LSE on 31 July 2011), the Leuven Centre for Global Governance Studies, the Global Governance Programme at the European University Institute, and the Center for Global Governance at Columbia Law School.\n\nJournals dedicated to the studies of global governance include the Chinese Journal of Global Governance, the Global Policy Journal at Durham University, , and Kosmos Journal for Global Transformation.\n\n\n\n"}
{"id": "241074", "url": "https://en.wikipedia.org/wiki?curid=241074", "title": "Greenwich Time Signal", "text": "Greenwich Time Signal\n\nThe Greenwich Time Signal (GTS), popularly known as the pips, is a series of six short tones broadcast at one-second intervals by many BBC Radio stations. The pips were introduced in 1924 and have been generated by the BBC since 1990 to mark the precise start of each hour. Their utility in calibration is diminishing as digital broadcasting entails time lags.\n\nThere are six pips (short beeps) in total, which occur on each of the 5 seconds leading up to the hour and on the hour itself. Each pip is a 1 kHz tone (about halfway between musical B5 and C6) the first five of which last a tenth of a second each, while the final pip lasts half a second. The actual moment when the hour changes – the \"on-time marker\" – is at the very beginning of the last pip.\n\nWhen a leap second occurs (exactly one second before midnight UTC), it is indicated by a seventh pip. In this case the first pip occurs at 23:59:55 (as usual) and there is a sixth short pip at 23:59:60 (the leap second) followed by the long pip at 00:00:00. The possibility of an extra pip for the leap second thus justifies the final pip being longer than the others, so that it is always clear which pip is on the hour. Before leap seconds were conceived, the final pip was the same length as the others. Although \"negative\" leap seconds can also be used to make the year shorter, this has never happened in practice.\n\nAlthough normally broadcast only on the hour by BBC domestic radio, BBC World Service use the signal at other times as well. The signal is generated at each quarter-hour and has on occasion been broadcast in error.\n\nUp to 1972 the pips were of equal length and confusion arose as to which was the final pip, hence the last pip was of extended length.\n\nThe pips are available to BBC radio stations every 15 minutes but except in rare cases, they are only broadcast on the hour, usually before news bulletins or news programmes. On BBC Radio 4, the pips are broadcast every hour except at 18:00 and 00:00 and at 22:00 on Sundays (at the start of the Westminster Hour) when they are replaced by the Westminster chimes of Big Ben at the Palace of Westminster. No time signal is broadcast at 15:00 on Saturdays and at 10:00 and 11:00 on Sundays. This is caused by the scheduling of the afternoon play on Saturday and the omnibus edition of \"The Archers\" on Sunday. On BBC Radio 2, the pips are used at 07:00, 08:00, 17:00 and 19:00 on weekdays, at 07:00 and 08:00 on Saturdays and at 08:00 and 09:00 on Sundays.\n\nThe pips were used on Radio 1 during \"The Chris Moyles Show\" at 06:30 just after the news, 09:00 as part of the Tedious Link feature, 10 am (at the end of the show) and often before \"Newsbeat\". As most stations only air the pips on the hour, \"The Chris Moyles Show\" was the only show where the pips were broadcast on the half-hour. Chris Moyles continues to use the pips at the beginning of his show on Radio X. Zane Lowe's Masterpieces, the playing of an album in its entirety, is begun with pips, and they also feature at 19:00 on Fridays to signify the start of the weekend and at 16:00 on Sundays to mark the start of \"The Official Chart Show\". \"The Weekend Breakfast Show with Dev\" begins with the pips at 06:00, and they sometimes feature on the hour at other points during the show, and \"Gemma Cairney's Early Breakfast Show\" begins with the pips. Dev's previous \"Early Breakfast Show\" also featured the pips at the beginning, and on the half-hour/hour at other points, particularly at 06:00 before or after the \"I'm Here All Week\" track. The pips are also used at 19:00 on Saturday evenings at the start of Radio 1's 12-hour simulcast with digital station BBC Radio 1Xtra.\n\nBBC Radio 3 and BBC Radio Five Live do not currently feature the Greenwich Time Signal in their scheduled programming.\n\nThe BBC World Service broadcasts the pips every hour.\n\nPips can also be heard on many BBC Local Radio stations although their use is up to the discretion of individual stations. A rare quarter-hour Greenwich Time Signal can be heard at 05:15 weekdays on Wally Webb's programme on six BBC Local Radio stations in the east of England, as part of his \"sychronised cup of tea\" feature.\n\nIn 1999, pip-like sounds were incorporated into the themes written by composer David Lowe to introduce BBC Television News programmes. They are still used today on BBC One, BBC World News and BBC News.\n\nThe BBC does not allow the pips to be broadcast except as a time signal. Radio plays and comedies which have fictional news programmes use various methods to avoid playing the full six pips, ranging from simply fading in the pips to a version played on \"On the Hour\" in which the sound was made into a small tune between the pips. \"The News Quiz\" also featured a special Christmas pantomime edition where the pips went missing, and the problem was avoided there by only playing individual pips and not the whole set. The 2012 project \"Radio Reunited\", however, did use the pips not as a time signal, but simply to commemorate 90 years of BBC Radio.\n\nThe pips for national radio stations and some local radio stations are timed relative to UTC, from an atomic clock in the basement of Broadcasting House synchronised with the National Physical Laboratory's Time from NPL and GPS. On other stations, the pips are generated locally from a GPS-synchronised clock.\n\nThe BBC compensates for the time delay in both broadcasting and receiving equipment, as well as the time for the actual transmission. The pips are timed so that they are accurately received on long wave as far as from the Droitwich AM transmitter, which is the distance to Central London.\n\nAs a pre-IRIG and pre-NTP time transfer and transmission system, the pips have been a great technological success. In modern times, however, time can be transferred to systems with CPUs and operating systems by using BCD or some Unix Time variant.\n\nNewer digital broadcasting methods have introduced even greater problems for the accuracy of use of the pips. On digital platforms such as DVB, DAB, satellite and the internet, the pips—although \"generated\" accurately—are not \"heard\" by the listener exactly on the hour. The encoding and decoding of the digital signal causes a delay, of usually between 2 and 8 seconds. In the case of satellite broadcasting, the travel time of the signal to and from the satellite adds about another 0.25 seconds.\n\nThe pips have been broadcast daily since 5 February 1924,\nand were the idea of the Astronomer Royal, Sir Frank Watson Dyson, and the head of the BBC, John Reith. The pips were originally controlled by two mechanical clocks located in the Royal Greenwich Observatory that had electrical contacts attached to their pendula. Two clocks were used in case of a breakdown of one. These sent a signal each second to the BBC, which converted them to the audible oscillatory tone broadcast.\n\nThe Royal Greenwich Observatory moved to Herstmonceux Castle in 1957 and the GTS equipment followed a few years later in the form of an electronic clock. Reliability was improved by renting two lines for the service between Herstmonceux and the BBC, with a changeover between the two at Broadcasting House if the main line became disconnected.\n\nThe tone sent on the lines was inverted: the signal sent to the BBC was a steady 1 kHz tone when no pip was required, and no tone when a pip should be sounded. This let faults on the line be detected immediately by automated monitoring for loss of audio.\n\nThe Greenwich Time Signal was the first sound heard in the handover to the London 2012 Olympics during the Beijing 2008 Olympics closing ceremony.\n\nThe pips were also broadcast by the BBC Television Service, but this practice was discontinued by the 1960s.\n\nTo celebrate the 90th birthday of the pips on 5 February 2014, the \"Today\" programme broadcast a sequence that included a re-working of the Happy Birthday melody using the GTS as its base sound.\n\nThe BBC discourages any other sound being broadcast at the same time as the pips; doing so is commonly known as 'crashing the pips'. This was most often referred to on Terry Wogan's Radio 2 Breakfast show, although usually only in jest since the actual event happened rarely. Different BBC Radio stations approach this issue differently. Both BBC Radio 1 and Radio 2 generally take a more laid-back approach with the pips, usually playing them over the closing seconds of a currently playing song or a jingle 'bed' (background music from a jingle), followed by their respective news jingles. Many BBC local radio stations also play the pips over the station's jingle. BBC Radio 4 is stricter. It is an almost entirely speech-based network; incidents at the end of the \"Today\" programme regularly cause listeners' complaints.\n\nAs a contribution to Comic Relief's 2005 Red Nose Day, the BBC developed a \"pips\" ring-tone which can be downloaded.\n\nBill Bailey's BBC Rave includes the BBC News theme, which incorporates a variant of the pips (though not actually broadcast exactly on the hour). The footage can be seen on his DVD \"Part Troll\".\n\nIn the late 1980s Radio 1 featured the pips played over a station jingle during Jakki Brambles' early show and Simon Mayo's breakfast show. This was not strictly crashing the pips as they were not intended to be, or mistaken for, an accurate time signal.\n\nAt 8 am on 17 September 2008, to the surprise of John Humphrys, the day's main presenter on the \"Today\" programme, and Johnnie Walker, who was standing in for Terry Wogan on Radio 2, the pips went adrift by 6 seconds, and broadcast seven pips rather than six. This was traced to a problem with the pip generator, which was 'repaired' by switching it off and on again. Part of Humphrys' surprise was probably because of his deliberate avoidance of crashing the pips with the help of an accurate clock in the studio.\n\nA sudden total failure in the generation of the audio pulses that constitute the pips was experienced on 31 May 2011 and silence was unexpectedly broadcast in place of the 17:00 signal. The problem was traced to the power supply of the equipment which converts the signal from the atomic clocks into an audible signal. Whilst repairs were underway the BBC elected to broadcast a \"dignified silence\" in place of the pips at 19:00. By 19:45 the same day the power supply was repaired and the 20:00 pips were broadcast as normal.\n\nMany radio broadcasters around the world use the Greenwich Time Signal as a means to mark the start of the hour. The pips are used in both domestic and international commercial and public broadcasting. Many radio stations use six tones similar to those used by the BBC World Service; some shorten it to five, four, or three tones.\n\n\n\n"}
{"id": "14098", "url": "https://en.wikipedia.org/wiki?curid=14098", "title": "History of the Americas", "text": "History of the Americas\n\nThe prehistory of the Americas (North, South, and Central America, and the Caribbean) begins with people migrating to these areas from Asia during the height of an Ice Age. These groups are generally believed to have been isolated from peoples of the \"Old World\" until the coming of Europeans in the 10th century from Norway and with the voyages of Christopher Columbus in 1492.\n\nThe ancestors of today's American Indigenous peoples were the Paleo-Indians; they were hunter-gatherers who migrated into North America. The most popular theory asserts that migrants came to the Americas via Beringia, the land mass now covered by the ocean waters of the Bering Strait. Small lithic stage peoples followed megafauna like bison, mammoth (now extinct), and caribou, thus gaining the modern nickname \"big-game hunters.\" Groups of people may also have traveled into North America on shelf or sheet ice along the northern Pacific coast.\n\nCultural traits brought by the first immigrants later evolved and spawned such cultures as Iroquois on North America and Pirahã of South America. These cultures later developed into civilizations. In many cases, these cultures expanded at a later date than their Old World counterparts. Cultures that may be considered advanced or civilized include Norte Chico, Cahokia, Zapotec, Toltec, Olmec, Maya, Aztec, Chimor, Mixtec, Moche, Mississippian, Puebloan, Totonac, Teotihuacan, Huastec people, Purépecha, Izapa, Mazatec, Muisca, and the Inca.After the voyages of Christopher Columbus in 1492, Spanish, Portuguese and later English, French and Dutch colonial expeditions arrived in the New World, conquering and settling the discovered lands, which led to a transformation of the cultural and physical landscape in the Americas. Spain colonized most of the Americas from present-day Southwestern United States, Florida and the Caribbean to the southern tip of South America. Portugal settled in what is mostly present-day Brazil while England established colonies on the Eastern coast of the United States, as well as the North Pacific coast and in most of Canada. France settled in Quebec and other parts of Eastern Canada and claimed an area in what is today the central United States. The Netherlands settled New Netherland (administrative centre New Amsterdam - now New York), some Caribbean islands and parts of Northern South America.\n\nEuropean colonization of the Americas led to the rise of new cultures, civilizations and eventually states, which resulted from the fusion of Native American and European traditions, peoples and institutions. The transformation of American cultures through colonization is evident in architecture, religion, gastronomy, the arts and particularly languages, the most widespread being Spanish (376 million speakers), English (348 million) and Portuguese (201 million). The colonial period lasted approximately three centuries, from the early 16th to the early 19th centuries, when Brazil and the larger Hispanic American nations declared independence. The United States obtained independence from England much earlier, in 1776, while Canada formed a federal dominion in 1867. Others remained attached to their European parent state until the end of the 19th century, such as Cuba and Puerto Rico which were linked to Spain until 1898. Smaller territories such as Guyana obtained independence in the mid-20th century, while certain Caribbean islands and French Guiana remain part of a European power to this day.\n\n \nThe specifics of Paleo-Indian migration to and throughout the Americas, including the exact dates and routes traveled, are subject to ongoing research and discussion. The traditional theory has been that these early migrants moved into the Beringia land bridge between eastern Siberia and present-day Alaska around 40,000 – 17,000 years ago, when sea levels were significantly lowered due to the Quaternary glaciation. These people are believed to have followed herds of now-extinct Pleistocene megafauna along \"ice-free corridors\" that stretched between the Laurentide and Cordilleran ice sheets. Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific Northwest coast to South America. Evidence of the latter would since have been covered by a sea level rise of a hundred meters following the last ice age.\n\nArchaeologists contend that the Paleo-Indian migration out of Beringia (eastern Alaska), ranges from 40,000 to around 16,500 years ago. This time range is a hot source of debate. The few agreements achieved to date are the origin from Central Asia, with widespread habitation of the Americas during the end of the last glacial period, or more specifically what is known as the late glacial maximum, around 16,000 – 13,000 years before present.\n\nThe American Journal of Human Genetics released an article in 2007 stating \"Here we show, by using 86 complete mitochondrial genomes, that all Indigenous American haplogroups, including Haplogroup X (mtDNA), were part of a single founding population.\" Amerindian groups in the Bering Strait region exhibit perhaps the strongest DNA or mitochondrial DNA relations to Siberian peoples. The genetic diversity of Amerindian indigenous groups increase with distance from the assumed entry point into the Americas. Certain genetic diversity patterns from West to East suggest, particularly in South America, that migration proceeded first down the west coast, and then proceeded eastward. Geneticists have variously estimated that peoples of Asia and the Americas were part of the same population from 42,000 to 21,000 years ago.\n\nNew studies shed light on the founding population of indigenous Americans, suggesting that their ancestry traced to both east Asian and western Eurasians who migrated to North America directly from Siberia. A 2013 study in the journal Nature reported that DNA found in the 24,000-year-old remains of a young boy in Mal’ta Siberia suggest that up to one-third of the indigenous Americans may have ancestry that can be traced back to western Eurasians, who may have \"had a more north-easterly distribution 24,000 years ago than commonly thought\" Professor Kelly Graf said that \"Our findings are significant at two levels. First, it shows that Upper Paleolithic Siberians came from a cosmopolitan population of early modern humans that spread out of Africa to Europe and Central and South Asia. Second, Paleoindian skeletons with phenotypic traits atypical of modern-day Native Americans can be explained as having a direct historical connection to Upper Paleolithic Siberia.\" A route through Beringia is seen as more likely than the Solutrean hypothesis.\n\nOn October 3, 2014, the Oregon cave where the oldest DNA evidence of human habitation in North America was found was added to the National Register of Historic Places. The DNA, radiocarbon dated to 14,300 years ago, was found in fossilized human coprolites uncovered in the Paisley Five Mile Point Caves in south central Oregon.\n\nThe Lithic stage or \"Paleo-Indian period\", is the earliest classification term referring to the first stage of human habitation in the Americas, covering the Late Pleistocene epoch. The time period derives its name from the appearance of \"Lithic flaked\" stone tools. Stone tools, particularly projectile points and scrapers, are the primary evidence of the earliest well known human activity in the Americas. Lithic reduction stone tools are used by archaeologists and anthropologists to classify cultural periods.\n\nSeveral thousand years after the first migrations, the first complex civilizations arose as hunter-gatherers settled into semi-agricultural communities. Identifiable sedentary settlements began to emerge in the so-called Middle Archaic period around 6000 BCE. Particular archaeological cultures can be identified and easily classified throughout the Archaic period.\n\nIn the late Archaic, on the north-central coastal region of Peru, a complex civilization arose which has been termed the Norte Chico civilization, also known as Caral-Supe. It is the oldest known civilization in the Americas and one of the five sites where civilization originated independently and indigenously in the ancient world, flourishing between the 30th and 18th centuries BC. It pre-dated the Mesoamerican Olmec civilization by nearly two millennia. It was contemporaneous with the Egypt following the unification of its kingdom under Narmer and the emergence of the first Egyptian hieroglyphics.\n\nMonumental architecture, including earthwork platform mounds and sunken plazas have been identified as part of the civilization. Archaeological evidence points to the use of textile technology and the worship of common god symbols. Government, possibly in the form of theocracy, is assumed to have been required to manage the region. However, numerous questions remain about its organization. In archaeological nomenclature, the culture was pre-ceramic culture of the pre-Columbian Late Archaic period. It appears to have lacked ceramics and art.\n\nOngoing scholarly debate persists over the extent to which the flourishing of Norte Chico resulted from its abundant maritime food resources, and the relationship that these resources would suggest between coastal and inland sites.\n\nThe role of seafood in the Norte Chico diet has been a subject of scholarly debate. In 1973, examining the Aspero region of Norte Chico, Michael E. Moseley contended that a maritime subsistence (seafood) economy had been the basis of society and its early flourishing. This theory, later termed \"maritime foundation of Andean Civilization\" was at odds with the general scholarly consensus that civilization arose as a result of intensive grain-based agriculture, as had been the case in the emergence of civilizations in northeast Africa (Egypt) and southwest Asia (Mesopotamia).\n\nWhile earlier research pointed to edible domestic plants such as squash, beans, lucuma, guava, pacay, and camote at Caral, publications by Haas and colleagues have added avocado, achira, and corn (Zea Mays) to the list of foods consumed in the region. In 2013, Haas and colleagues reported that maize was a primary component of the diet throughout the period of 3000 to 1800 BC.\n\nCotton was another widespread crop in Norte Chico, essential to the production of fishing nets and textiles. Jonathan Haas noted a mutual dependency, whereby \"The prehistoric residents of the Norte Chico needed the fish resources for their protein and the fishermen needed the cotton to make the nets to catch the fish.\"\n\nIn the 2005 book \"\", journalist Charles C. Mann surveyed the literature at the time, reporting a date \"sometime before 3200 BC, and possibly before 3500 BC\" as the beginning date for the formation of Norte Chico. He notes that the earliest date securely associated with a city is 3500 BC, at Huaricanga in the (inland) Fortaleza area.\n\nThe Norte Chico civilization began to decline around 1800 BC as more powerful centers appeared to the south and north along its coast, and to the east within the Andes Mountains.\n\nAfter the decline of the Norte Chico civilization, several large, centralized civilizations developed in the Western Hemisphere: Chavin, Nazca, Moche, Huari, Quitus, Cañaris, Chimu, Pachacamac, Tiahuanaco, Aymara and Inca in the Central Andes (Ecuador, Peru and Bolivia); Muisca in Colombia ; Taínos in Dominican Republic (Hispaniola, Española) and part of Caribbean; and the Olmecs, Maya, Toltecs, Mixtecs, Zapotecs, Aztecs and Purepecha in southern North America (Mexico, Guatemala).\n\nThe Olmec civilization was the first Mesoamerican civilization, beginning around 1600-1400 BC and ending around 400 BC. Mesoamerica is considered one of the six sites around the globe in which civilization developed independently and indigenously. This civilization is considered the mother culture of the Mesoamerican civilizations. The Mesoamerican calendar, numeral system, writing, and much of the Mesoamerican pantheon seem to have begun with the Olmec.\n\nSome elements of agriculture seem to have been practiced in Mesoamerica quite early. The domestication of maize is thought to have begun around 7,500 to 12,000 years ago. The earliest record of lowland maize cultivation dates to around 5100 BC. Agriculture continued to be mixed with a hunting-gathering-fishing lifestyle until quite late compared to other regions, but by 2700 BC, Mesoamericans were relying on maize, and living mostly in villages. Temple mounds and classes started to appear. By 1300/ 1200 BC, small centres coalesced into the Olmec civilization, which seems to have been a set of city-states, united in religious and commercial concerns. The Olmec cities had ceremonial complexes with earth/clay pyramids, palaces, stone monuments, aqueducts and walled plazas. The first of these centers was at San Lorenzo (until 900 bc). La Venta was the last great Olmec centre. Olmec artisans sculpted jade and clay figurines of Jaguars and humans. Their iconic giant heads - believed to be of Olmec rulers - stood in every major city.\n\nThe Olmec civilization ended in 400 BC, with the defacing and destruction of San Lorenzo and La Venta, two of the major cities. It nevertheless spawned many other states, most notably the Mayan civilization, whose first cities began appearing around 700-600 BC. Olmec influences continued to appear in many later Mesoamerican civilizations.\n\nCities of the Aztecs, Mayas, and Incas were as large and organized as the largest in the Old World, with an estimated population of 200,000 to 350,000 in Tenochtitlan, the capital of the Aztec empire. The market established in the city was said to have been the largest ever seen by the conquistadors when they arrived. The capital of the Cahokians, Cahokia, located near modern East St. Louis, Illinois, may have reached a population of over 20,000. At its peak, between the 12th and 13th centuries, Cahokia may have been the most populous city in North America. Monk's Mound, the major ceremonial center of Cahokia, remains the largest earthen construction of the prehistoric New World.\n\nThese civilizations developed agriculture as well, breeding maize (corn) from having ears 2–5 cm in length to perhaps 10–15 cm in length. Potatoes, tomatoes, pumpkins, beans, avocados, and chocolate are now the most popular of the pre-Columbian agricultural products. The civilizations did not develop extensive livestock as there were few suitable species, although alpacas and llamas were domesticated for use as beasts of burden and sources of wool and meat in the Andes. By the 15th century, maize was being farmed in the Mississippi River Valley after introduction from Mexico. The course of further agricultural development was greatly altered by the arrival of Europeans.\n\n\nCahokia was a major regional chiefdom, with trade and tributary chiefdoms located in a range of areas from bordering the Great Lakes to the Gulf of Mexico.\n\n\nThe Iroquois League of Nations or \"People of the Long House\", based in present-day upstate and western New York, had a confederacy model from the mid-15th century. It has been suggested that their culture contributed to political thinking during the development of the later United States government. Their system of affiliation was a kind of federation, different from the strong, centralized European monarchies.\n\nLeadership was restricted to a group of 50 sachem chiefs, each representing one clan within a tribe; the Oneida and Mohawk people had nine seats each; the Onondagas held fourteen; the Cayuga had ten seats; and the Seneca had eight. Representation was not based on population numbers, as the Seneca tribe greatly outnumbered the others. When a sachem chief died, his successor was chosen by the senior woman of his tribe in consultation with other female members of the clan; property and hereditary leadership were passed matrilineally. Decisions were not made through voting but through consensus decision making, with each sachem chief holding theoretical veto power. The Onondaga were the \"firekeepers\", responsible for raising topics to be discussed. They occupied one side of a three-sided fire (the Mohawk and Seneca sat on one side of the fire, the Oneida and Cayuga sat on the third side.)\n\nElizabeth Tooker, an anthropologist, has said that it was unlikely the US founding fathers were inspired by the confederacy, as it bears little resemblance to the system of governance adopted in the United States. For example, it is based on inherited rather than elected leadership, selected by female members of the tribes, consensus decision-making regardless of population size of the tribes, and a single group capable of bringing matters before the legislative body.\n\nLong-distance trading did not prevent warfare and displacement among the indigenous peoples, and their oral histories tell of numerous migrations to the historic territories where Europeans encountered them. The Iroquois invaded and attacked tribes in the Ohio River area of present-day Kentucky and claimed the hunting grounds. Historians have placed these events as occurring as early as the 13th century, or in the 17th century Beaver Wars.\n\nThrough warfare, the Iroquois drove several tribes to migrate west to what became known as their historically traditional lands west of the Mississippi River. Tribes originating in the Ohio Valley who moved west included the Osage, Kaw, Ponca and Omaha people. By the mid-17th century, they had resettled in their historical lands in present-day Kansas, Nebraska, Arkansas and Oklahoma. The Osage warred with Caddo-speaking Native Americans, displacing them in turn by the mid-18th century and dominating their new historical territories.\n\n\nThe Pueblo people of what is now the Southwestern United States and northern Mexico, living conditions were that of large stone apartment like adobe structures. They live in Arizona, New Mexico, Utah, Colorado, and possibly surrounding areas.\n\nChichimeca was the name that the Mexica (Aztecs) generically applied to a wide range of semi-nomadic peoples who inhabited the north of modern-day Mexico, and carried the same sense as the European term \"barbarian\". The name was adopted with a pejorative tone by the Spaniards when referring especially to the semi-nomadic hunter-gatherer peoples of northern Mexico.\n\nThe Zapotec emerged around 1500 years BCE. Their writing system influenced the later Olmec. They left behind the great city Monte Alban.\n\nThe Olmec civilization emerged around 1200 BCE in Mesoamerica and ended around 400 BCE. Olmec art and concepts influenced surrounding cultures after their downfall. This civilization was thought to be the first in America to develop a writing system. After the Olmecs abandoned their cities for unknown reasons, the Maya, Zapotec and Teotihuacan arose.\n\nThe Purepecha civilization emerged around 1000 CE in Mesoamerica . They flourished from 1100 CE to 1530 CE. They continue to live on in the state of Michoacán. Fierce warriors, they were never conquered and in their glory years, successfully sealed off huge areas from Aztec domination.\n\n\nMaya history spans 3,000 years. The Classic Maya may have collapsed due to changing climate in the end of the 10th century.\n\nThe Toltec were a nomadic people, dating from the 10th - 12th century, whose language was also spoken by the Aztecs.\n\nTeotihuacan (4th century BCE - 7/8th century CE) was both a city, and an empire of the same name, which, at its zenith between 150 and the 5th century, covered most of Mesoamerica.\n\nThe Aztec having started to build their empire around 14th century found their civilization abruptly ended by the Spanish conquistadors. They lived in Mesoamerica, and surrounding lands. Their capital city Tenochtitlan was one of the largest cities of all time.\n\nThe oldest known civilization of the Americas was established in the Norte Chico region of modern Peru. Complex society emerged in the group of coastal valleys, between 3000 and 1800 BCE. The Quipu, a distinctive recording device among Andean civilizations, apparently dates from the era of Norte Chico's prominence.\n\nThe Chavín established a trade network and developed agriculture by as early as (or late compared to the Old World) 900 BCE according to some estimates and archaeological finds. Artifacts were found at a site called Chavín in modern Peru at an elevation of 3,177 meters. Chavín civilization spanned from 900 BCE to 300 BCE.\n\nHolding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533.\nKnown as \"Tahuantinsuyu\", or \"the land of the four regions\", in Quechua, the Inca culture was highly distinct and developed. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture. There is evidence of excellent metalwork and even successful trepanation of the skull in Inca civilization.\n\nAround 1000, the Vikings established a short-lived settlement in Newfoundland, now known as L'Anse aux Meadows. Speculations exist about other Old World discoveries of the New World, but none of these are generally or completely accepted by most scholars.\n\nSpain sponsored a major exploration led by Italian explorer Christopher Columbus in 1492; it quickly led to extensive European colonization of the Americas. The Europeans brought Old World diseases which are thought to have caused catastrophic epidemics and a huge decrease of the native population. Columbus came at a time in which many technical developments in sailing techniques and communication made it possible to report his voyages easily and to spread word of them throughout Europe. It was also a time of growing religious, imperial and economic rivalries that led to a competition for the establishment of colonies.\n\n15th to 19th century colonies in the New World:\n\nThe formation of sovereign states in the New World began with the United States Declaration of Independence of 1776. The American Revolutionary War lasted through the period of the Siege of Yorktown — its last major campaign — in the early autumn of 1781, with peace being achieved in 1783.\nThe Spanish colonies won their independence in the first quarter of the 19th century, in the Spanish American wars of independence. Simón Bolívar and José de San Martín, among others, led their independence struggle. Although Bolivar attempted to keep the Spanish-speaking parts of Latin America politically allied, they rapidly became independent of one another as well, and several further wars were fought, such as the Paraguayan War and the War of the Pacific. (See Latin American integration.) In the Portuguese colony Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese king Dom João VI, proclaimed the country's independence in 1822 and became Brazil's first Emperor. This was peacefully accepted by the crown in Portugal, upon compensation.\n\nSlavery has had a significant role in the economic development of the New World after the colonization of the Americas by the Europeans. The cotton, tobacco, and sugar cane harvested by slaves became important exports for the United States and the Caribbean countries.\n\nAs a part of the British Empire, Canada immediately entered World War I when it broke out in 1914. Canada bore the brunt of several major battles during the early stages of the war, including the use of poison gas attacks at Ypres. Losses became grave, and the government eventually brought in conscription, despite the fact this was against the wishes of the majority of French Canadians. In the ensuing Conscription Crisis of 1917, riots broke out on the streets of Montreal. In neighboring Newfoundland, the new dominion suffered a devastating loss on July 1, 1916, the First day on the Somme.\n\nThe United States stayed out of the conflict until 1917, when it joined the Entente powers. The United States was then able to play a crucial role at the Paris Peace Conference of 1919 that shaped interwar Europe. Mexico was not part of the war, as the country was embroiled in the Mexican Revolution at the time.\n\nThe 1920s brought an age of great prosperity in the United States, and to a lesser degree Canada. But the Wall Street Crash of 1929 combined with drought ushered in a period of economic hardship in the United States and Canada. From 1936 to 1949, there was a popular uprising against the anti-Catholic Mexican government of the time, set off specifically by the anti-clerical provisions of the Mexican Constitution of 1917.\n\nOnce again, Canada found itself at war before its neighbors, however even Canadian contributions were slight before the Japanese attack on Pearl Harbor. The entry of the United States into the war helped to tip the balance in favour of the allies. Two Mexican tankers, transporting oil to the United States, were attacked and sunk by the Germans in the Gulf of Mexico waters, in 1942. The incident happened in spite of Mexico's neutrality at that time. This led Mexico to enter the conflict with a declaration of war on the Axis nations. The destruction of Europe wrought by the war vaulted all North American countries to more important roles in world affairs, especially the United States, which emerged as a \"superpower\".\n\nThe early Cold War era saw the United States as the most powerful nation in a Western coalition of which Mexico and Canada were also a part. In Canada, Quebec was transformed by the Quiet Revolution and the emergence of Quebec nationalism. Mexico experienced an era of huge economic growth after World War II, a heavy industrialization process and a growth of its middle class, a period known in Mexican history as \"El Milagro Mexicano\" (the Mexican miracle). The Caribbean saw the beginnings of decolonization, while on the largest island the Cuban Revolution introduced Cold War rivalries into Latin America.\n\nThe civil rights movement in the U.S. ended Jim Crow and empowered black voters in the 1960s, which allowed black citizens to move into high government offices for the first time since Reconstruction. However, the dominant New Deal coalition collapsed in the mid 1960s in disputes over race and the Vietnam War, and the conservative movement began its rise to power, as the once dominant liberalism weakened and collapsed. Canada during this era was dominated by the leadership of Pierre Elliot Trudeau. In 1982, at the end of his tenure, Canada enshrined a new constitution.\n\nCanada's Brian Mulroney not only ran on a similar platform but also favored closer trade ties with the United States. This led to the Canada-United States Free Trade Agreement in January 1989. Mexican presidents Miguel de la Madrid, in the early 1980s and Carlos Salinas de Gortari in the late 1980s, started implementing liberal economic strategies that were seen as a good move. However, Mexico experienced a strong economic recession in 1982 and the Mexican peso suffered a devaluation. In the United States president Ronald Reagan attempted to move the United States back towards a hard anti-communist line in foreign affairs, in what his supporters saw as an attempt to assert moral leadership (compared to the Soviet Union) in the world community. Domestically, Reagan attempted to bring in a package of privatization and regulation to stimulate the economy.\n\nThe end of the Cold War and the beginning of the era of sustained economic expansion coincided during the 1990s. On January 1, 1994, Canada, Mexico and the United States signed the North American Free Trade Agreement, creating the world's largest free trade area. In 2000, Vicente Fox became the first non-PRI candidate to win the Mexican presidency in over 70 years. The optimism of the 1990s was shattered by the 9/11 attacks of 2001 on the United States, which prompted military intervention in Afghanistan, which also involved Canada. Canada did not support the United States' later move to invade Iraq, however.\n\nIn the U.S. the Reagan Era of conservative national policies, deregulation and tax cuts took control with the election of Ronald Reagan in 1980. By 2010, political scientists were debating whether the election of Barack Obama in 2008 represented an end of the Reagan Era, or was only a reaction against the bubble economy of the 2000s (decade), which burst in 2008 and became the Late-2000s recession with prolonged unemployment.\n\nDespite the failure of a lasting political union, the concept of Central American reunification, though lacking enthusiasm from the leaders of the individual countries, rises from time to time. In 1856–1857 the region successfully established a military coalition to repel an invasion by United States adventurer William Walker. Today, all five nations fly flags that retain the old federal motif of two outer blue bands bounding an inner white stripe. (Costa Rica, traditionally the least committed of the five to regional integration, modified its flag significantly in 1848 by darkening the blue and adding a double-wide inner red band, in honor of the French tricolor).\n\nIn 1907, a Central American Court of Justice was created. On December 13, 1960, Guatemala, El Salvador, Honduras, and Nicaragua established the Central American Common Market (\"CACM\"). Costa Rica, because of its relative economic prosperity and political stability, chose not to participate in the CACM. The goals for the CACM were to create greater political unification and success of import substitution industrialization policies. The project was an immediate economic success, but was abandoned after the 1969 \"Football War\" between El Salvador and Honduras. A Central American Parliament has operated, as a purely advisory body, since 1991. Costa Rica has repeatedly declined invitations to join the regional parliament, which seats deputies from the four other former members of the Union, as well as from Panama and the Dominican Republic.\n\nIn the 1960s and 1970s, the governments of Argentina, Brazil, Chile, and Uruguay were overthrown or displaced by U.S.-aligned military dictatorships. These dictatorships detained tens of thousands of political prisoners, many of whom were tortured and/or killed (on inter-state collaboration, see Operation Condor). Economically, they began a transition to neoliberal economic policies. They placed their own actions within the United States Cold War doctrine of \"National Security\" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict (see Túpac Amaru Revolutionary Movement and Shining Path). Revolutionary movements and right-wing military dictatorships have been common, but starting in the 1980s a wave of democratization came through the continent, and democratic rule is widespread now. Allegations of corruption remain common, and several nations have seen crises which have forced the resignation of their presidents, although normal civilian succession has continued.\n\nInternational indebtedness became a notable problem, as most recently illustrated by Argentina's default in the early 21st century. In recent years, South American governments have drifted to the left, with socialist leaders being elected in Chile, Bolivia, Brazil, Venezuela, and a leftist president in Argentina and Uruguay. Despite the move to the left, South America is still largely capitalist. With the founding of the Union of South American Nations, South America has started down the road of economic integration, with plans for political integration in the European Union style.\n\n"}
{"id": "48714730", "url": "https://en.wikipedia.org/wiki?curid=48714730", "title": "Ideoscape", "text": "Ideoscape\n\nIdeoscape is a term used to describe one of Arjun Appadurai’s five dimensions of global cultural flows. The five dimensions consist of ethnoscapes, mediascapes, technoscapes, financescapes, and ideoscapes. The suffix -\"scape\" denotes that these terms are perspectival constructs inflected by the historical, linguistic, and political situatedness of different kinds of actors: \"nation-states, multinationals, diasporic communities, as well as subnational groupings and movements\". This can either be religious, political, or economic. Because cultural exchange and transactions have typically been restricted in the past due to geographical and economical aspects, Appadurai’s five dimensions give the opportunity for cultural transactions to occur. Ideoscapes is the movement of ideologies. It is often political and usually has to do with the ideologies of states and the counterideologies of movements explicitly oriented to capturing state power or a piece of it. Ideoscapes are usually composed of ideas, terms, and images including “freedom, welfare, rights, sovereignty, representation, and democracy”.\n"}
{"id": "10892972", "url": "https://en.wikipedia.org/wiki?curid=10892972", "title": "Integer factorization records", "text": "Integer factorization records\n\nInteger factorization is the process of determining which prime numbers divide a given positive integer. Doing this quickly has applications in cryptography. The difficulty depends on both the size and form of the number and its prime factors; it is currently very difficult to factorize large semiprimes (and, indeed, most numbers which have no small factors).\n\nThe first very large distributed factorisation was RSA129, a challenge number described in the Scientific American article of 1977 which first popularised the RSA cryptosystem. It was factorised between September 1993 and April 1994, using MPQS, with relations contributed by about 600 people from all over the Internet, and the final stages of the calculation performed on a MasPar supercomputer at Bell Labs.\n\nBetween January and August 1999, RSA-155, a challenge number prepared by the RSA company, was factorised using GNFS with relations again contributed by a large group, and the final stages of the calculation performed in just over nine days on the Cray C916 supercomputer at the SARA Amsterdam Academic Computer Center.\n\nIn January 2002, Franke et al. announced the factorisation of a 158-digit cofactor of 2+1, using a couple of months on about 25 PCs at the University of Bonn, with the final stages done using a cluster of six Pentium-III PCs.\n\nIn April 2003, the same team factored RSA-160 using about a hundred CPUs at BSI, with the final stages of the calculation done using 25 processors of an SGI Origin supercomputer.\n\nThe 174-digit RSA-576 was factored by Franke, Kleinjung and members of the NFSNET collaboration in December 2003, using resources at BSI and the University of Bonn; soon afterwards, Aoki, Kida, Shimoyama, Sonoda and Ueda announced that they had factored a 164-digit cofactor of 2+1.\n\nA 176-digit cofactor of 11+1 was factored by Aoki, Kida, Shimoyama and Ueda between February and May 2005 using machines at NTT and Rikkyo University in Japan.\n\nThe RSA-200 challenge number was factored by Franke, Kleinjung et al. between December 2003 and May 2005, using a cluster of 80 Opteron processors at BSI in Germany; the announcement was made on 9 May 2005. They later (November 2005) factored the slightly smaller RSA-640 challenge number.\n\nOn December 12, 2009, a team including researchers from the CWI, the EPFL, INRIA and NTT in addition to the authors of the previous record factored RSA-768, a 232-digit semiprime. They used the equivalent of almost 2000\nyears of computing on a single core 2.2 GHz AMD Opteron.\n\n12 − 1, of 542 bits (163 digits), was factored between April and July 1993 by a team at CWI and Oregon State University.\n\n2 + 1, of 774 bits (233 digits), was factored between April and November 2000 by 'The Cabal', with the matrix step done over 250 hours on the Cray also used for RSA-155.\n\n2 − 1, of 809 bits (244 digits), had its factorisation announced at the start of January 2003. Sieving was done at the CWI, at the Scientific Computing Institute and the Pure Mathematics Department at Bonn University, and using private resources of J. Franke, T. Kleinjung and the family of F. Bahr. The linear algebra step was done by P. Montgomery at SARA in Amsterdam.\n\n6 − 1, of 911 bits (275 digits), was factored by Aoki, Kida, Shimoyama and Ueda between September 2005 and January 2006 using SNFS.\n\n2 − 1, of 1039 bits (313 digits) (though a factor of 23 bits was already known) was factored between September 2006 and May 2007 by a group including K. Aoki, J. Franke, T. Kleinjung, A. K. Lenstra and D. A. Osvik, using computers at NTT, EPFL and the University of Bonn.\n\n2 − 1, of 1061 bits (320 digits) was factored between early 2011 and 4 August 2012 by a group headed by Greg Childers at CSU Fullerton, using the nfs@home BOINC project for about 300 CPU-years of sieving; the linear algebra was run at the Trestles cluster at SDSC and the Lonestar cluster at TACC and needed additional 35 CPU-years.\n\nAll unfactored parts of the numbers 2 − 1 with n between 1000 and 1200 were factored by a multiple-number-sieve approach in which much of the sieving step could be done simultaneously for multiple numbers, by a group including T. Kleinjung, J. Bos and A. K. Lenstra, starting in 2010. To be precise, n=1081 was completed on 11 March 2013; n=1111 on 13 June 2013; n=1129 on 20 September 2013; n=1153 on 28 October 2013; n=1159 on 9 February 2014; 1177 on May 29, 2014, n=1193 on 22 August 2014, and n=1199 on December 11, 2014; the first detailed announcement was made in late August 2014. The total effort for the project is of the order of 7500 CPU-years on 2.2 GHz Opterons, with roughly 5700 years spent sieving and 1800 years on linear algebra.\n\nAs of the end of 2007, thanks to the constant decline in memory prices, the ready availability of multi-core 64-bit computers, and the availability of the efficient sieving code (developed by Thorsten Kleinjung of the Bonn group) via ggnfs and of robust open-source software such as msieve for the finishing stages, special-form numbers of up to 750 bits and general-form numbers of up to about 520 bits can be factored in a few months on a few PCs by a single person without any special mathematical experience. These bounds increase to about 950 and 600 if it were possible to secure the collaboration of a few dozen PCs for sieving; currently the amount of memory and the CPU power of a single machine for the finishing stage are equal barriers to progress.\n\nIn 2009, Benjamin Moody factored a 512-bit RSA key used to sign the TI-83 graphing calculator using software found on the internet; this eventually led to the Texas Instruments signing key controversy.\n\nIn September 2013, the 696-bit RSA-210 was factored by Ryan Propper using institutional resources; between March 2013 and October 2014, another 210-digit number (the 117th term in the 'home prime sequence' starting with 49) was completed by a user known as WraithX, using $7600 worth of processing time on Amazon EC2 machines for the sieving, and four months on a dual Xeon E5-2687W v1 for the linear algebra.\n\nThe largest number factored by Shor's algorithm is 21 in 2012. 15 had previously been factored by several labs.\n\nIn April 2012, the factorization of formula_1 by a room temperature (300K) NMR adiabatic quantum computer was reported by a group led by Xinhua Peng. In November 2014 it was discovered by Nike Dattani and Nathan Bryans that the 2012 experiment had in fact also factored much larger numbers without knowing it. In April 2016 the 18-bit number 200099 was factored using quantum annealing on a D-Wave 2X quantum processor. Shortly after, 291311 was factored using NMR at higher than room temperature.\n\n"}
{"id": "825959", "url": "https://en.wikipedia.org/wiki?curid=825959", "title": "International Table Tennis Federation", "text": "International Table Tennis Federation\n\nThe International Table Tennis Federation (ITTF) is the governing body for all national table tennis associations. The role of the ITTF includes overseeing rules and regulations and seeking technological improvement for the sport of table tennis. The ITTF is responsible for the organization of numerous international competitions, including the World Table Tennis Championships that has continued since 1926.\n\nThe ITTF was founded in 1926 by William Henry Lawes of Wymondham, the nine founding members being Austria, Czechoslovakia, Denmark, England, Germany, Hungary, India, Sweden and Wales. The first international tournament was held in January 1926 in Berlin while the first World Table Tennis Championships was held in December 1926 in London.\n\nToward the end of 2000, the ITTF instituted several rules changes aimed at making table tennis more viable as a televised spectator sport. The older 38 mm balls were officially replaced by 40 mm balls. This increased the ball's air resistance and effectively slowed down the game.\n\nOn 29 February 2008, the ITTF announced several rules changes after an ITTF Executive Meeting in Guangzhou, Guangdong, China with regards to a player's eligibility to play for a new association. The new ruling is to encourage associations to develop their own players.\n\nThe headquarters of the ITTF is in Lausanne, Switzerland. The previous president of the ITTF was Adham Sharara from Canada; the current president since 2014 is from Germany.\n\nThe ITTF recognises six continental federations. Each continental federation has a president as its top official and owns its constitution. The following are recognised federations:\n\nThere are currently 226 member associations within the ITTF.\n\nAll member associations of the ITTF attend annual general meeting (AGM). Agendas on changes of the constitution, laws of table tennis, applications for membership etc. are discussed and finalised through votes. Also, the president of ITTF, 8 executive vice-presidents, and 32 or less continental representatives are elected at an AGM, serving for a four-year term. The president, executive vice-presidents, and the chairman of the athletes' commission compose executive committee.\n\nThe executive committee, continental representatives and presidents of the six continental federations or their appointees compose the board of directors (Board). The Board manages the work of the ITTF between AGMs. Several committees, commissions, working groups or panels work under the constitution of ITTF or under the Board.\n\nUnlike the organisations for more popular sports, the ITTF tends to recognise teams from generally unrecognised governing bodies for disputed territory. For example, it currently recognises the Table Tennis Federation of Kosovo even though Kosovo is excluded from most other sports. It recognised the People's Republic of China in 1953 and allowed some basic diplomacy\nwhich lead to an opening for U.S. President Richard Nixon, called \"Ping Pong Diplomacy\", in the early 1970s.\n\nFor ITTF World Title events, a player is eligible to play for his association by registering with the ITTF. If the player chooses to play for a new association, he shall register with the ITTF, through the new association.\n\n\nThe table tennis point system was reduced from a 21 to an 11-point scoring system in 2001. A game shall be won by the player or pair first scoring 11 points unless both players or pairs score 10 points, when the game shall be won by the first player or pair subsequently gaining a lead of 2 points. This was intended to make games more fast-paced and exciting. The ITTF also changed the rules on service to prevent a player from hiding the ball during service, in order to increase the average length of rallies and to reduce the server's advantage. Today, the game changes from time to time mainly to improve on the excitement for television viewers.\n\nIn 2007, ITTF's board of directors in Zagreb decided to implement the VOC-free glue rule at Junior events, starting from 1 January 2008, as a transitional period before the full implementation of the VOC ban on 1 September 2008.\n\nAs of 1 January 2009, all speed glue was to have been banned.\n\nConventions: MT/WT: Men's/Women's Teams; MS/WS: Men's/Women's Singles; MD/WD: Men's/Women's Doubles; XD: Mixed Doubles\nThe ITTF maintains an official World Ranking list based on players' results in tournaments throughout the year.\n\nThe tables below show the current ITTF World Ranking for men and women:\n\n\n"}
{"id": "53825654", "url": "https://en.wikipedia.org/wiki?curid=53825654", "title": "Jeremy Lent", "text": "Jeremy Lent\n\nJeremy Lent (born 1960) is an author whose writings investigate the patterns of thought that have led civilization to its current crisis of sustainability. He is the founder of the non-profit Liology Institute, which is dedicated to a worldview that could enable humanity to thrive sustainably. He is the author of \"The Patterning Instinct\" and \"Requiem of the Human Soul\". Earlier in his career, Lent was the founder, chairman and CEO of the internet company NextCard.\n\nLent was born and raised in London. He graduated from the University College School and earned an undergraduate degree in English Literature at Emmanuel College, Cambridge University in 1981. Lent left the UK to live in the United States. In 1986, he earned an MBA from the University of Chicago. He is married and currently lives in the San Francisco Bay Area.\n\nIn addition to his writing, Lent is a practitioner of meditation, Qigong, and Tai Chi. He is a Level II certified teacher of Radiant Heart Qigong.\n\nLent joined Strategic Planning Associates, a strategy consulting company based in Washington, D.C. In 1989, he joined First Deposit Corporation (later renamed Providian), a direct mail credit card company in San Francisco. Lent was named Chief Financial Officer of Providian in 1991 and left the company in 1994.\n\nIn 1996, Lent founded NextCard, an internet financial services company. NextCard was the first company to enable consumers to apply for a credit card over the internet and be approved in real time and the first company to offer consumers the ability to design their own card by uploading a personalized image during the application process.\n\nAs chairman and CEO, Lent took NextCard public in 1999 and led a secondary offering in 2000. However, due to his first wife’s illness, Lent stepped down as CEO later in 2000, to care for her. After Lent’s departure, NextCard suffered serious setbacks. It was announced in late 2001, that the company was undercapitalized, and it was taken over by the Federal Deposit Insurance Corporation (FDIC) in 2002. Along with the other board members, Lent was involved for several years in shareholder lawsuits and investigations by the FDIC and U.S. Securities and Exchange Commission (SEC). These were eventually settled, and in 2005 the SEC dismissed fraud charges that it had levied against Lent.\n\n\"The Patterning Instinct: A Cultural History of Humanity’s Search for Meaning\" (Prometheus Books, May 2017) is a cognitive history of humanity, tracing how different cultures patterned meaning into the universe and how that has affected history. The result of ten years of research, the book offers a thesis that “culture shapes values and those values shape history.”\n\nThis approach is in contrast to the predominant geographic determinist approach to history, exemplified by historians such as Jared Diamond, Ian Morris and Kenneth Pomeranz. Lent argues that “The cognitive frames through which different cultures perceive reality have had a profound effect on their historical direction. The worldview of a given civilization—the implicit beliefs and values that create a pattern of meaning in people’s lives—has, in my opinion, been a significant driver of the historical path each civilization has taken.”\n\nThe book conducts what Lent calls an “archaeological exploration of the mind,” using findings from cognitive science and systems theory to reveal the implicit layers of values that form cultural norms. In a departure from the mainstream science-religion debate, \"The Patterning Instinct\" shows how medieval Christian rationalism acted as an incubator for scientific thought, which in turn, shaped the modern vision of the “conquest of nature.” Evaluating the sustainability crisis, Lent argues that it is culturally driven: a product of particular patterns that could be reshaped.\n\nThe book concludes by exploring scenarios for humanity’s future, foreseeing a coming struggle between two contrasting views: one driving to a technological world of artificially enhanced humans, the other enabling a sustainable future arising from intrinsic connectedness among people and to the natural world.\n\n\"The Patterning Instinct\" contains a foreword by Fritjof Capra. Prior to publication, the book received favorable endorsements from notables such as Paul Ehrlich, Thom Hartmann, Rick Hanson, J. R. McNeill, and Jonathon Porritt. Guardian journalist, George Monbiot, has called it \"the most profound and far-reaching book I have ever read.\" The book received a Silver Award from Nautilus Book Awards in April, 2018 in the Social Sciences & Education category.\n\nBeginning around 2005, Lent began an inquiry into the various constructions of meaning formed by cultures around the world and throughout history. The first expression of this investigation was his science fiction novel, \"Requiem of the Human Soul\" (Libros Libertad, 2009)..\n\nThe novel is set in the late 22nd century when most people are genetically enhanced. The minority that remains genetically unadulterated, known as Primals, consists mostly of the impoverished global underclass. The UN is holding a hearing in New York to consider whether to make the Primals extinct. The novel is written from the viewpoint of a Primal, Eusebio, who has been picked to represent his race in a last-ditch legal effort to save the Primals from extinction.\n\nThe novel raises questions about spirituality, history and global politics: Can the human race enhance itself to a higher plane? At what cost and benefit? If some “essence” of humanity was lost as a result, would that be so bad, given our sordid and shameful history? On the other hand, is there something special—a human soul—worth keeping at any price? Ultimately, the novel invites the reader to grapple with a fundamental question: what does it mean to be human?\n\nLent founded the nonprofit Liology Institute in 2012, with the aim of fostering a worldview that could enable humanity to thrive sustainably. The institute, according to its website, “is dedicated to fostering a worldview in which the human discovery and experience of meaning in our lives is compatible with the findings of scientific investigation, offering a deeply integrated and coherent understanding of humanity’s place in our cosmos which could enable us to thrive on our planet harmoniously and sustainably.”\n\nLent coined the term \"liology\" from the Chinese word \"li,\" meaning organizing principles of the universe, and “ology” of Greek etymology meaning “the study of.” The institute is intended to integrate traditional East Asian practices with the findings of modern systems science.\n\nIts stated objectives are to:\nIn 2014, the institute began an annual series of workshops, exploring how the values and insights of Liology apply to people’s everyday lives.\n\nLent asserts that humanity is entering a period of transformation, of a scale that has occurred twice before in history: the Agricultural Revolution twelve thousand years ago, and the Scientific and Industrial Revolutions several hundred years ago. The trajectories he consider include a collapse of civilization, a posthuman techno society and sustainable human flourishing (a \"Great Transformation\").\n\nLent claims that, to secure the latter humanity needs a new suite of values based on a sense of intrinsic connectedness. These values emphasize:\n\n\n"}
{"id": "46383774", "url": "https://en.wikipedia.org/wiki?curid=46383774", "title": "List of Madonna records and achievements", "text": "List of Madonna records and achievements\n\nAmerican singer Madonna, throughout her career, spanning three decades, has obtained a remarkable series of statistical achievements, setting and breaking several world records with her participation in entrepreneurial activities, of acting and her performance in the musical scene for her videos, singles, albums, and tours.\n\nHer first appearance in the \"Guinness Book of World Records\" was in 1986 with her third studio album, \"True Blue\". Since then, she has earned multiple appearances, some 20 at least in February 2012, including her insigne record title as the top-selling female recording artist of all time. Madonna's commerciality has achieved scrutiny studies and analysis from different point of view in the academia world or by marketers, through her marketing strategies, controversies and reinvention, surpassing to others new or contemporary artists. In popular culture, many international artists have been called \"Madonna\", many times, by their impact and success in their respective countries or genres. Also, her presence in popular culture, has been led to create world records about Madonna. Dubbed as the most successful and most influential female artist of all time,\n\nWhen she reached number one with \"Music\" in 2000 at Billboard Hot 100, it made Madonna the second artist to achieve number one hits on the Hot 100 in the 1980s, 1990s and 2000s. Also, when she released \"Who's That Girl\" in 1987 it became Madonna's sixth number-one single in the United States, making her the first artist to accumulate six number-one singles in the 1980s, and the first female performer to get that many number-ones as a solo act. In 2012, with \"Give Me All Your Luvin'\" she became the second female with highest number of \"Billboard\" Hot 100 singles by female artists, only behind Aretha Franklin. The song is her 56th appearance on the chart. In 1991, with \"Rescue Me\" when it debuted at No. 15 on the March 2, 1991 Hot 100 chart, it marked the highest-ever bow for a single by a woman. Further, it was —at the time— one of only four titles to debut in the top 20. Madonna is the third oldest female to place a number one at the Billboard Hot 100. The single \"Music\" reached the number-one spot when Madonna was forty-two years one month old (September 2000). She holds the record with 38 singles within the Top 10, most for any artist.\n\nShe has 46 number-one songs in the chart —the most for any artist. Also, is the only active artist to chart continuously since 1982, spanning four decades. \"Billboard\" said: \"comparing her chart champs by decade, Madonna scored nine Dance/Club Songs No. 1s in the '80s and 13 in the '90s. Since 2000, she has almost doubled her total, adding 18 No. 1s in that span\". In 2012, when she release \"Girl Gone Wild\", became her 42nd No. 1 on the record chart, \"Billboard\" said that she has the quickest span of back-to-back No. 1s. Also, the magazine declared that \"essentially, if Madonna releases a single and it charts on Dance/Club Play Songs, it's a safe bet to assume it'll go to No. 1. Since 2000, she's placed 26 hits on the survey. Of those, all but six have gone all the way to No. 1. Also, Madonna is the only artist in history to achieve seven top-ten hits from one album (\"American Life\") and the only artist ever to achieve seven consecutive number-one hits on this chart twice.\n\n\"Give Me All Your Luvin'\" set a record chart, when it debuted at 24 in the Dance/Club Play Songs chart and rose to 9 the next week, having the fastest rising to the top 10 for a song ever \"Hung Up\" became the most successful dance song of the 2000s in the United States, by topping the Dance/Club Play Songs Decade-end tally and \"Music\" became the second most-successful dance song of decade, reaching number two on the record chart. Furthermore, \"Music\" was also the longest-running number one song on Dance/Club Play Songs in the 2000s, with a longevity of five weeks at number one.\n\nShe is the second female artist with most number ones albums, with 8, behind only Barbra Streisand, who has 10 number one albums. Her other achievements on the record chart, tied with Beyoncé as the female artist with most consecutive number-one albums. Also, in 1984 Madonna was the first female to sell over 5 million albums in the US in a single year with \"Like a Virgin\". Madonna is the artist with most top 10 albums on the Billboard 200, with 21 (20 in 2012, with \"MDNA\") George Strait places second with 17 top 10s, followed by Mariah Carey (16). Also, she is the female artist with most number-two albums on Billboard's Top 200 Albums, with six, tied with The Beatles and behind Frank Sinatra's seven\n\nHer albums \"The Immaculate Collection\", \"Like a Virgin\" and \"True Blue\" are among the top 100 certified albums according to the RIAA, with \"The Immaculate Collection\" and \"Like a Virgin\" becoming in one of the best-selling albums in United States with a diamond status. As of 2015, she is the eighth artist with most number-one albums. However, she has a record for the biggest second week sales drop in history in Nielsen SoundScan era with her twelfth studio album, \"MDNA\".\n\nWhen she released \"Open Your Heart\" in Australia, it reached a peak of number 16, breaking a run of nine consecutive top ten singles for Madonna in that country. She is tied with Australian singer Kylie Minogue, having 10 number one singles.\nShe have debuted her albums at number one for nine occasions, with \"Rebel Heart\" her last album, it became Madonna's 19th week atop the chart, ranking her at number 24 on the list of artists with most accumulated weeks at the top. She has 11 number one albums, tied with U2.\n\nShe has a total of 25 #1 singles in Canada — the most for any artist in spanning four decades: 1980s, 1990s, 2000s and 2010s. Only two of these number one singles belong to Canadian Hot 100 and the rest of Canadian RPM singles chart. Her 2000 single \"Music\" was the last number one hit on the Canadian RPM singles chart. Also, she has sixty-eight Top 40, fifty-nine Top 20, fifty-one Top 10 and thirty-nine Top 5 singles. \"4 Minutes\" debuted at the top of the Canadian Contemporary Hit Radio chart. This marked the first time any song entered at the top of the CHR chart in BDS history. With \"Music and \"American Pie\" was the first time in Canadian chart history that an artist held the top two positions on the year-end musical charts.\n\nShe has to date, two album with diamond status in Canada with \"True Blue\" (in June 1987) and \"Like a Virgin\" (July 1992). Making her one of the few artists that have this certification. Also, she has 21 albums in the Top 5, with only 3 on the Top 20.\n\nMadonna had only three number one singles in France, but spanning three decades: 1980s, 1990s and 2000s. She has twenty-two Top 10 singles on the French Top 100 Singles charts and forty-nine Top 40, during four decades: 1980s, 1990s, 2000s and 2010s.\n\nShe has eight number one albums in France. Also, she has three albums with diamond status in France with \"True Blue\" (1,000,000), \"The Immaculate Collection\" (1,000,000) and \"Confessions on a Dancefloor\" (750,000). Also, she has ten albums number-two and three number-three albums. Generally, her albums always topped the Top 10 places in France.\n\nMadonna had four number one singles in Germany, but has fifty-eight Top 40, thirty-seven Top 20, twenty-five Top 10 and nineteen Top 5 singles.\n\nShe has one album in the top 40 best-selling albums ever in Germany, with \"Ray of Light\" sold 1.5 million units. She has number ones albums during four decades: 1980s, 1990s, 2000s and 2010s\n\nSource: Media Control Charts\n\nShe has a total of twenty-five number ones singles on Italian charts, spanning for three decades: 1980s, 1990s and 2000s. Also she has sixty-eight Top 40, sixty-five Top 20, fifty-six Top 10 and forty-five Top 5 singles.\n\nShe has a total of 15 number-one albums in Italy and 8 albums number two. Most of her albums have entered the top 5, with only two studio albums, topping the two.\n\nIn Japan, she was the only foreign artist with two albums into the best selling of the 1980 decade, with \"Like a Virgin\" (#33) and \"True Blue\" (#36).\n\nShe has a total of twenty-two singles number one and two number-two singles: \"Angel\", \"The Power of Good-Bye\", \"Don't Tell Me\", \"American Life\" and \"Give Me All Your Luvin'\".\n\nShe has ten number one albums in Spain, spanning during four decades: 1980s, 1990s, 2000s and 2010s. Also, she has twenty Top 5 albums and generally, her albums always topped the Top 20 in Spain.\n\nMadonna has 13 number one hits in United Kingdom, a record for a female recording artist, spanning her number one for three decades: 1980s, 1990s and 2000s\n\nIn 1985, Madonna became the first female artist in UK chart history to hold the top-two positions of the chart simultaneously, with \"Into the Groove\" topping number one and \"Holiday\" the second place By the end of 1985, Madonna achieved up another record with the song, becoming the first female artist to have eight UK top-ten singles in one calendar year. In 1987 when she published \"La Isla Bonita\", she became the female artist with the most number-one singles in the British chart history—a record that has since been maintained by Madonna to date Every single reached the UK Top 20 until 2008's \"Miles Away\". Also, Madonna simultaneously topped the albums and singles charts 4 times in her career, which is unmatched by any other female artist. In 2014, Official Charts Company compiled her Top 40 biggest selling singles, with \"Into the Groove\" in the top with 870,000 copies. Furthermore, Madonna had 71st Top 40 U.K. singles, the most for any female artist. Her others milestones including, eleven Top 2 singles, sixty Top 10 and forty four Top 20 singles.\n\nSource: Official Charts Company \n\nShe has 12 number-one albums with 9 studio albums —the most for any solo artist. Also, she has five number-two albums and twenty Top 5 albums, more than any other female artist. In 1986, \"True Blue\" opened at the top of the UK Albums Chart on July 12, 1986, making it the first album by American artist to debut at number one in British chart history. In 2012, with \"MDNA\" she became the first female in have an album in the number one during the 1980, 1990, 2000 and 2010. Also, Madonna and Kylie Minogue are the only artists ever to have a number-one album and a number-one single in three different decades.\n\nAccording to \"New Musical Express\" her albums \"The Immaculate Collection\" and \"Confessions on a Dance Floor\" are one of the 50 fastest selling albums ever in United Kingdom, selling 340,000 and 217,610 respectively. Making in the female artist with most entries. Both Michael Jackson, Madonna had two of the best selling albums of the decade 1980 with \"True Blue\" (#9) and \"Like a Virgin\" (#17). Also during the 2000s, she figured with two albums in the top 100 with \"Music\" and \"Confessions on a Dance Floor\" and she topped the UK chart Christmas with \"The Immaculate Collection\", making the first to the decade 1990s and occupied the record number one during nine weeks.\n\nSource: Official Charts Company \n\nIn popular culture, Madonna has generated records titles by others ways in \"Guinness Book of World Records\". They said: You might not be able to run as fast as Michael Johnson, or sell as many records as Madonna.\n\n\n\nGeneral\n\nSpecific\n\n\n\n"}
{"id": "54818248", "url": "https://en.wikipedia.org/wiki?curid=54818248", "title": "List of current boxing rankings", "text": "List of current boxing rankings\n\nThis is a list of current professional boxing rankings, which includes the latest rankings by each one of the sport's four major sanctioning bodies, as well as other well-regarded sites and entities.\n\nAs professional boxing has four major sanctioning bodies (WBA, WBC, IBF, WBO) each with their own champions, the sport doesn't have a centralized ranking system. The rankings published by these organizations share the trait of not ranking the other organizations' champions, as each one of the sanctioning bodies expects their champion to frequently defend their title against their top-ranked contender. The WBA often has more than one champion, none of which are ranked by the other 3 sanctioning bodies. Their \"Super\" and \"Regular\" champions are excluded from the rankings but their \"Interim\" champion is affixed to the #1 spot. The IBF's protocol is for the top 2 spots in its rankings to remain vacant until two of its other top-ranked contenders face off, at which point the winner takes one of those two places.\n\nIn addition to the rankings published by the major sanctioning bodies, the TBRB and \"The Ring\" each publish their own independent rankings, not excluding any organizations' champions. The aim of both the TBRB and \"The Ring\" is to crown a single champion for each division. Every single one of these lists are assembled by a committee but since the 90s, other parties have experimented with computerized rankings, but these are sometimes regarded as incapable of accounting for all of boxing's quirks and subtleties. The most widely known computerized rankings are published by BoxRec and updated daily. The following is a list compiling the latest installment of all the previously mentioned rankings.\n\n\n"}
{"id": "356095", "url": "https://en.wikipedia.org/wiki?curid=356095", "title": "Longest English sentence", "text": "Longest English sentence\n\nThere have been several claims for the longest sentence in the English language, usually with claims that revolve around the longest \"printed\" sentence, because there is no limit on the possible length of a written English sentence. \n\nAt least one linguistics textbook concludes that, in theory, \"there is no longest English sentence.\" A sentence can be made arbitrarily long by successive iterations, such as \n\"Someone thinks that someone thinks that someone thinks that...,\" or by combining shorter clauses in various ways. \n\nFor example, sentences can be extended by recursively embedding clauses one into another, such as \n\nThe ability to embed structures within larger ones is called recursion. This also highlights the difference between linguistic performance and linguistic competence, because the language can support more variation than can reasonably be created or recorded.\n\nOne of the longest sentences in literature is contained in William Faulkner's \"Absalom, Absalom!\" (1936). The sentence is composed of 1,288 words (In the 1951 Random House version).\n\nAnother sentence that is often claimed to be the longest sentence ever written is Molly Bloom's soliloquy in the James Joyce novel \"Ulysses\" (1922), which contains a sentence of 3,687 words. However, this sentence is simply many sentences without punctuation.\n\nJonathan Coe's \"The Rotters' Club\" appears to hold the record at 13,955 words. It was inspired by Bohumil Hrabal's \"Dancing Lessons for the Advanced in Age\": a Czech language novel written in one long sentence.\n\n"}
{"id": "15389216", "url": "https://en.wikipedia.org/wiki?curid=15389216", "title": "Macroregion", "text": "Macroregion\n\nA macroregion is a geopolitical subdivision that encompasses several traditionally or politically defined regions. The meaning may vary, with the common denominator being cultural, economical, historical or social similarity within a macroregion. The term is often used in the context of globalization.\n\nThe term \"macroregion\" may be also used in the context of natural regions, like in Slovenia.\n\n"}
{"id": "27784030", "url": "https://en.wikipedia.org/wiki?curid=27784030", "title": "Media Network", "text": "Media Network\n\nMedia Network was the name of a weekly radio programme broadcast on Radio Netherlands Worldwide from 7 May 1981 until 26 October 2000. When the programme began the station was known as Radio Nederland, but was renamed Radio Netherlands shortly thereafter. The programme concentrated on communications topics with particular reference to international shortwave broadcasting, but also went on to cover mediumwave and longwave, television, satellite, internet, reviews of shortwave receivers and other electronic devices. It was produced and presented by Jonathan Marks. In the course of 1994, he was joined by colleague Diana Janssen, who was working as a media researcher at the station, who co-hosted the show until shortly before its end. Media Network ran for over 1000 editions.\n\nJonathan Marks, a British radio producer arrived in the Netherlands in August 1980. He had previously freelanced for Radio Austria International in the period 1976-1980 and briefly for the World Radio Club programme running on the BBC World Service. He was hired as the fifth host of the Radio Nederland \"DX Jukebox\" programme, a technical show that had been running on the English service since the 1958. His first show was broadcast on 7 August 1980. The content of \"DX Jukebox\" also revolved around shortwave broadcasting, giving information on schedules for various other stations as well as a variety of details on improving reception. As the name suggested, a portion of the show was devoted to music.\n\nMarks decided that the show needed to be updated to reflect a new era. Fewer listeners were building their own radios. The music was shortened and a name change was also planned for the middle of 1981, with Media Network being the name suggested by Roger Broadbent, a colleague producer. Audience reactions suggested a demand for a new style of programme to be not just about when and on what frequency one could listen to a given station on shortwave, but also about \"why\" one should listen.\n\nDuring the early 1980s, a time when international telephony was still of a potentially mediocre sound quality (making it unsuitable for shortwave broadcasts), Media Network went against the established wisdom and made extensive use of the telephone. Calls were made to the show's various contributors. On many occasions where telephone quality was deemed too poor for broadcast, a tape recording was made at both ends of the line, then the person with whom Marks was speaking would post the tape to the Netherlands. The two recordings were subsequently edited together resulting in a conversation with much better sound quality.\n\nOver the years the show had many regular contributors who provided valuable information on stations that went on or off the air, radio propagation conditions, new communications technology and the development of digital formats such as DAT, CD and DVD. Listeners were also encouraged to telephone an answer line and leave their comments and questions. These messages became an integral part of the programme.\n\nThe show prided itself on being less predictable than its predecessor by reporting from international conferences, compiling thematic documentaries and organising its own research surveys into radio related publications and receivers. On some occasions, the reviews of receivers conducted by the show had an influence on the manufacturers who later contacted Media Network for advice on improving their products. \n\nSpecials covered historic events as conveyed by the media, such as the first Gulf War, the Falklands War and propaganda during World War II and in more recent conflicts. There were also many DXpeditions or media safari documentaries to profile stations in different parts of the world (e.g. South Africa, Argentina, Australia, New Zealand, Bonaire).\n\nAfter Diana Janssen left the show in September, 2000 to pursue another career with Forrester Research, Jonathan Marks (who was also employed as Radio Netherlands' Director of Programmes) made the decision to end the show. He cited the increasingly demanding task of fulfilling a corporate role at Radio Netherlands combined with producing a show of a \"high standard that we strive for and you deserve\". The four editions that followed were replays of popular specials with a finale on 26 October 2000. The editorship of the program was handed over to Andy Sennitt (a frequent contributor also working at Radio Netherlands) who continued the brand as a weblog until 24 March 2012, shortly before Sennitt retired. The Media Network blog has now been removed from the Radio Netherlands Worldwide site. A copy is available at archive.org. No new material or comments will be published there. The Media News aspects of that site have been transferred to the Critical Distance blog, edited by Jonathan Marks. This is a non-commercial venture.\n\nIn February, 2010, Jonathan Marks, now an independent media strategist, launched a project called the Media Network Vintage Vault which allows the general public to download studio quality editions in Mp3 format. As of July 2015, over 460 editions have been digitized and made available free of charge. Some other programmes made by Jonathan Marks have also been uploaded to the same website, such as \"Marks on Mechanics\" and \"The Hitchhiker's Guide to DXing\", a comedic spoof of The Hitchhiker's Guide to the Galaxy which had shortwave radio as its basis.\n\n"}
{"id": "13924377", "url": "https://en.wikipedia.org/wiki?curid=13924377", "title": "Molecular epidemiology", "text": "Molecular epidemiology\n\nMolecular epidemiology is a branch of epidemiology and medical science that focuses on the contribution of potential genetic and environmental risk factors, identified at the molecular level, to the etiology, distribution and prevention of disease within families and across populations. This field has emerged from the integration of molecular biology into traditional epidemiological research. Molecular epidemiology improves our understanding of the pathogenesis of disease by identifying specific pathways, molecules and genes that influence the risk of developing disease. More broadly, it seeks to establish understanding of how the interactions between genetic traits and environmental exposures result in disease.\n\nThe term \"molecular epidemiology\" was first coined by Kilbourne in a 1973 article entitled \"The molecular epidemiology of influenza\". The term became more formalized with the formulation of the first book on \"Molecular Epidemiology: Principles and Practice\" by Schulte and Perera. At the heart of this book is the impact of advances in molecular research that have given rise to and enable the measurement and exploitation of the biomarker as a vital tool to link traditional molecular and epidemiological research strategies to understand the underlying mechanisms of disease in populations.\n\nWhile most molecular epidemiology studies are using conventional disease designation system for an outcome (with the use of exposures at the molecular level), compelling evidence indicates that disease evolution represents inherently heterogeneous process differing from person to person. Conceptually, each individual has a unique disease process different from any other individual (\"the unique disease principle\"), considering uniqueness of the exposome and its unique influence on molecular pathologic process in each individual. Studies to examine the relationship between an exposure and molecular pathologic signature of disease (particularly, cancer) became increasingly common throughout the 2000s. However, the use of molecular pathology in epidemiology posed unique challenges including lack of standardized methodologies and guidelines as well as paucity of interdisciplinary experts and training programs. The use of \"molecular epidemiology\" for this type of research masked the presence of these challenges, and hindered the development of methods and guidelines. Furthermore, the concept of disease heterogeneity appears to conflict with the premise that individuals with the same disease name have similar etiologies and disease processes.\n\nThe genome of a bacterial species fundamentally determines its identity. Thus, gel electrophoresis techniques like pulsed-field gel electrophoresis can be used in molecular epidemiology to comparatively analyze patterns of bacterial chromosomal fragments and to elucidate the genomic content of bacterial cells. Due to its widespread use and ability to analyse epidemiological information about most bacterial pathogens based on their molecular markers, pulsed-field gel electrophoresis is relied upon heavily in molecular epidemiological studies.\n\nMolecular epidemiology allows for an understanding of the molecular outcomes and implications of diet, lifestyle, and environmental exposure, particularly how these choices and exposures result in acquired genetic mutations and how these mutations are distributed throughout selected populations through the use of biomarkers and genetic information. Molecular epidemiological studies are able to provide additional understanding of previously-identified risk factors and disease mechanisms. Specific applications include:\n\nWhile the use of advanced molecular analysis techniques within the field of molecular epidemiology is providing the larger field of epidemiology with greater means of analysis, Miquel Porta identified several challenges that the field of molecular epidemiology faces, particularly selecting and incorporating requisite applicable data in an unbiased manner. Limitations of molecular epidemiological studies are similar in nature to those of generic epidemiological studies, that is, samples of convenience - both of the target population and genetic information, small sample sizes, inappropriate statistical methods, poor quality control, and poor definition of target populations.\n\n"}
{"id": "6324816", "url": "https://en.wikipedia.org/wiki?curid=6324816", "title": "Mutual recognition agreement", "text": "Mutual recognition agreement\n\nA mutual recognition agreement (MRA) is an international agreement by which two or more countries agree to recognize one another's conformity assessments.\nA mutual recognition arrangement is an international arrangement based on such an agreement.\n\nMRAs have become increasingly common since the formation of the World Trade Organization in 1995. They have been forged within and among various trade blocs, including APEC and the European Union.\n\nMRAs are most commonly applied to goods, such as various quality control MRAs. However, the term is sometimes applied to agreements on the recognition of professional qualifications as well.\n\nAccreditation Bodies, under the International Accreditation Forum use the term \"Multilateral Recognition Agreements\" in a similar sense.\n\n\n"}
{"id": "42780", "url": "https://en.wikipedia.org/wiki?curid=42780", "title": "Nordic Council", "text": "Nordic Council\n\nThe Nordic Council is the official body for formal inter-parliamentary co-operation among the Nordic countries. Formed in 1952, it has 87 representatives from Denmark, Finland, Iceland, Norway, and Sweden as well as from the autonomous areas of the Faroe Islands, Greenland, and the Åland Islands. The representatives are members of parliament in their respective countries or areas and are elected by those parliaments. The Council holds ordinary sessions each year in October/November and usually one extra session per year with a specific theme.\n\nIn 1971, the Nordic Council of Ministers, an intergovernmental forum, was established to complement the Council. The official and working languages of both the Nordic Council and the Nordic Council of Ministers are Danish, Norwegian, and Swedish, which comprise the first language of around 80% of the region's population and learned as a foreign language by the remaining 20%.\n\nThe Nordic Council and the Nordic Council of Ministers are involved in various forms of cooperation with neighbouring areas, amongst them being the Baltic Assembly and the Benelux, as well as Russia and Schleswig-Holstein.\n\nDuring World War II, Denmark and Norway were occupied by Germany; Finland was under assault by the Soviet Union; while Sweden, though neutral, still felt the war's effects. Following the war, the Nordic countries pursued the idea of a Scandinavian defence union to ensure their mutual defence. However, Finland, due to its Paasikivi-Kekkonen policy of neutrality and FCMA treaty with the USSR, could not participate.\n\nIt was proposed that the Nordic countries would unify their foreign policy and defence, remain neutral in the event of a conflict and not ally with NATO, which some were planning at the time. The United States, keen on getting access to bases in Scandinavia and believing the Nordic countries incapable of defending themselves, stated it would not ensure military support for Scandinavia if they did not join NATO. As Denmark and Norway sought US aid for their post-war reconstruction, the project collapsed, with Denmark, Norway and Iceland joining NATO.\n\nFurther Nordic co-operation, such as an economic customs union, also failed. This led Danish Prime Minister Hans Hedtoft to propose, in 1951, a consultative inter-parliamentary body. This proposal was agreed by Denmark, Iceland, Norway, and Sweden in 1952. The Council's first session was held in the Danish Parliament on 13 February 1953 and it elected Hans Hedtoft as its president. When Finnish-Soviet relations thawed following the death of Joseph Stalin, Finland joined the council in 1955.\n\nOn 2 July 1954, the Nordic labour market was created and in 1958, building upon a 1952 passport-free travel area, the Nordic Passport Union was created. These two measures helped ensure Nordic citizens' free movement around the area. A Nordic Convention on Social Security was implemented in 1955. There were also plans for a single market but they were abandoned in 1959 shortly before Denmark, Norway, and Sweden joined the European Free Trade Area (EFTA). Finland became an associated member of EFTA in 1961 and Denmark and Norway applied to join the European Economic Community (EEC).\n\nThis move towards the EEC led to desire for a formal Nordic treaty. The Treaty of Helsinki outlined the workings of the Council and came into force on 24 March 1962. Further advancements on Nordic cooperation were made in the following years: a Nordic School of Public Health, a Nordic Cultural Fund, and Nordic House in Reykjavík were created. Danish Prime Minister Hilmar Baunsgaard proposed full economic cooperation (\"Nordek\") in 1968. Nordek was agreed in 1970, but Finland then backtracked, stating that its ties with the Soviet Union meant it could not form close economic ties with potential members of the EEC (Denmark and Norway). Nordek was then abandoned.\n\nAs a consequence, Denmark and Norway applied to join the EEC and the Nordic Council of Ministers was set up in 1971 to ensure continued Nordic cooperation. In 1970 representatives of the Faroe Islands and Åland were allowed to take part in the Nordic Council as part of the Danish and Finnish delegations. Norway turned down EEC membership in 1972 while Denmark acted as a bridge builder between the EEC and the Nordics. Also in 1973, although it did not opt for full membership of the EEC, Finland negotiated a free trade treaty with the EEC that in practice removed customs duties from 1977 on, although there were transition periods up to 1985 for some products. Sweden did not apply due to its non-alliance policy, which was aimed at preserving neutrality. Greenland subsequently left the EEC and has since sought a more active role in circumpolar affairs.\n\nIn the 1970s, the Nordic Council founded the Nordic Industrial Fund, Nordtest and the Nordic Investment Bank. The Council's remit was also expanded to include environmental protection and, in order to clean up the pollution in the Baltic Sea and the North Atlantic, a joint energy network was established. The Nordic Science Policy Council was set up in 1983 and, in 1984, representatives from Greenland were allowed to join the Danish delegation.\n\nFollowing the collapse of the Soviet Union in 1991, the Nordic Council began to cooperate more with the Baltic states and new Baltic Sea organisations. Sweden and Finland joined the European Union (EU), the EEC's successor, in 1995. Norway had also applied, but once again voted against membership. However, Norway and Iceland did join the European Economic Area (EEA) which integrated them economically with the EU. The Nordic Passport Union was also subsumed into the EU's Schengen Area in 1996.\n\nThe Nordic Council became more outward-looking, to the Arctic, Baltic, Europe, and Canada. The Øresund Bridge linking Sweden and Denmark led to a large amount of cross-border travel, which in turn led to further efforts to reduce barriers. However, the initially envisioned tasks and functions of the Nordic Council have become partially dormant due to the significant overlap with the EU and EEA. In 2008 Iceland began EU membership talks, but decided to annul these in 2015.\n\nThe Nordic Council and the Nordic Council of Ministers have a particular focus on strengthening the Nordic language community; the main focus of their work to promote language understanding in the Nordic countries is on children and young people's understanding of written and oral Danish, Norwegian, and Swedish, the three mutually intelligible Scandinavian languages. Representatives of the Council have the ability to issue proposals in their own languages, and official documents are translated to cater to all five of the major Nordic languages. Discussions for bringing Finnish and Icelandic into equal footing with the three other languages has been proposed.\n\nThe Nordic Council consists of 87 representatives, elected from its members' parliaments and reflecting the relative representation of the political parties in those parliaments. It holds its main session in the autumn, while a so-called \"theme session\" is arranged in the spring. Each of the national delegations has its own secretariat in the national parliament. The autonomous territoriesGreenland, the Faroe Islands and Ålandalso have Nordic secretariats.\n\nThe Nordic Council uses the three Continental Scandinavian languages (Danish, Norwegian, and Swedish) as its official working languages, but also publishes material in Finnish, Icelandic, and English for information purposes. The council refers to Danish, Norwegian, and Swedish collectively as Scandinavian and considers them to be different forms of the same language forming a common language community. Since 1987, under the Nordic Language Convention, citizens of the Nordic countries have the opportunity to use their native language when interacting with official bodies in other Nordic countries without being liable to any interpretation or translation costs. The Convention covers visits to hospitals, job centres, the police and social security offices. The languages included are Swedish, Danish, Norwegian, Finnish, and Icelandic.\n\nThe Council does not have any formal power on its own, but each government has to implement any decisions through its national legislature. With Denmark, Norway, and Iceland being members of NATO and Finland and Sweden being neutral, the Nordic Council has not been involved in any military cooperation.\n\nThe original Nordic Council concentrates on inter-parliamentary cooperation. The \"Nordic Council of Ministers\", founded in 1971, is responsible for inter-governmental cooperation. Prime Ministers have ultimate responsibility but this is usually delegated to the Minister for Nordic Cooperation and the Nordic Committee for Co-operation, which co-ordinates the day-to-day work. The autonomous territories have the same representation as states.\n\n\nThe Nordic Council and the Council of Ministers have their headquarters in Copenhagen and various installations in each separate country, as well as many offices in neighbouring countries. The headquarters are located at Ved Stranden No. 18, close to Slotsholmen.\n\nThe Nordic Council has 8 members, 5 sovereign states and 3 self-governing regions.\n\nIn accordance with § 13 of the Rules of Procedure for the Nordic Council the Sámi Parliamentary Council is the only institution with observer status with the Nordic Council. In accordance with § 14, the Nordic Youth Council has the status of \"guest\" on a permanent basis, and the Presidium \"may invite representatives of popularly elected bodies and other persons to a session and grant them speaking rights\" as guests. According to the council, \"within the last couple of years, guests from other international and Nordic organisations have been able to take part in the debates at the Sessions. Visitors from the Baltic States and Northwest Russia are those who mostly take up this opportunity. Guests who have a connection to the theme under discussion are invited to the Theme Session.\"\n\nThe Nordic Council of Ministers has established four \"Offices outside the Nordic Region\", namely in all the Baltic states – Estonia, Latvia and Lithuania – and the German state of Schleswig-Holstein. The offices form part of the secretariat of the Nordic Council of Ministers; according to the Council of Ministers their primary mission is to promote cooperation between the Nordic countries and the Baltic states and to promote the Nordic countries in cooperation with their embassies within the Baltic states.\n\nThe Nordic Council and the Council of Ministers define Estonia, Latvia, Lithuania and Russia as \"Adjacent Areas\" and has formal cooperation with them under the Adjacent Areas policies framework; in recent years the cooperation has focused increasingly on Russia.\n\nThe Nordic Council had historically been a strong supporter of Baltic independence from the Soviet Union. During the move towards independence in the Baltic States in 1991, Denmark and Iceland pressed for the Observer Status in the Nordic Council for the then-nonsovereign Estonia, Latvia and Lithuania. The move in 1991 was opposed by Norway and Finland. The move was heavily opposed by the Soviet Union, accusing the Nordic Council of getting involved in its internal affairs. In the same year, the Nordic Council refused to give observer status for the three, at the time nonsovereign, Baltic states.\n\nWhile the Nordic Council rejected the Baltic states' application for formal observer status, the council nevertheless has extensive cooperation on different levels with all neighbouring countries, including the Baltic states and Germany, especially the state of Schleswig-Holstein. Representatives of Schleswig-Holstein were present as informal guests during a session for the first time in 2016. The state has historical ties to Denmark and cross-border cooperation with Denmark and has a Danish minority population. As parliamentary representatives from Schleswig-Holstein, a member of the South Schleswig Voter Federation and a member of the Social Democrats with ties to the Danish minority were elected. \n\nThe Sámi political structures long desired formal representation in the Nordic Council's structures, and were eventually granted observer status through the Sámi Parliamentary Council. In addition, representatives of the Sámi people are de facto included in activities touching upon their interests. In addition, the Faroe Islands have expressed their wishes for full membership in the Nordic Council instead of the current associate membership.\n\nRecently, three of the members of the Nordic Council (Sweden, Denmark and Finland, all EU-member states), the Baltic Assembly and the Benelux sought intensifying cooperation in the Digital Single Market, as well as discussing social matters, the Economic and Monetary Union of the European Union, the European migrant crisis and defense cooperation. Relations with Russia, Turkey and the United Kingdom was also on the agenda.\n\nSome desire the Nordic Council's promotion of Nordic cooperation to go much further than at present. If the states of Iceland, Sweden, Norway, Denmark and Finland were to merge in such an integration as some desire, it would command a gross domestic product of US$1.60 trillion, making it the twelfth largest economy in the world, larger than that of Australia, Spain, Mexico or South Korea. Gunnar Wetterberg, a Swedish historian and economist, wrote a book entered into the Nordic Council's year book that proposes the creation of a Nordic Federation from the Council in a few decades.\n\n\n"}
{"id": "23527766", "url": "https://en.wikipedia.org/wiki?curid=23527766", "title": "Ohio State University Health Sciences Center for Global Health", "text": "Ohio State University Health Sciences Center for Global Health\n\nThe Health Sciences Center for Global Health (HSCGH) at The Ohio State University (OSU) is a collaborative program among the OSU Colleges of Dentistry, Medicine, Nursing, Optometry, Pharmacy, Public Health, School of Health and Rehabilitation Sciences and Veterinary Medicine. The HSCGH is led jointly by the Colleges of Medicine (COM) and Public Health (CPH).\n\nThe HSCGH was created to increase student interest in global careers, prepare students for those careers and to promote, develop and coordinate interdisciplinary global health education and research throughout the health sciences colleges and the larger community. The Board of Trustees approved the creation of the HSCGH in July 2007.\n\nThe National Institutes of Health (NIH) Fogarty Framework Grant was awarded to OSU in September 2008. The NIH John E. Fogarty International Center grant supports the creation of new, multidisciplinary educational programs as well as an administrative infrastructure to support activities.\n\n\nThe Graduate Interdisciplinary Specialization in Global Health (GISGH) is a university-wide program that offers current OSU graduate and professional students advanced educational opportunities in the field of global health. The goal of the GISGH is to help prepare graduates to be active participants in the advancement of global health through academic enrichment, service-learning, and research pertaining to issues of global health. The specialization's core course, Introduction to Global Health, focuses on the basic components of population health, while the electives allow students to pursue topics across the other health sciences colleges for a truly interdisciplinary experience.\n\nDiane L. Gorgas, MD, a professor of Emergency Medicine at The Ohio State University’s Wexner Medical Center. She currently serves as the Executive Director of the Office of Global Health. and of OSU's Health Sciences Center for Global Health. She is nationally involved as an item writer and case developer and case administrator for the American Board of Emergency Medicine (ABEM) and sits on the Emergency Medicine Review Committee for the Accreditation Council for Graduate Medical Education (ACGME). She was the Residency Training Program Director in Emergency Medicine for many years, and has a long standing interest in and study of educational methods. Her other associated research interests include global health, emotional intelligence, competency assessment and learning styles. Dr. Gorgas currently directs the OSU Greif Neonatal Survival program, which works to improve the lives of mothers and infants in low-income countries through self-sustaining education and training programs to increase the in-country capacity of healthcare workers.\n\nPamela Potter serves as the administrative director for the Health Sciences Center for Global Health at The Ohio State University and in that role provides oversight of the daily administration and operation of the center. She is also the associate director of the Office of Global Health in the College of Medicine. Prior to her role at OSU, she served as the Chief of Staff to the Executive Dean for the Georgetown University School of Medicine.\n\n"}
{"id": "41227473", "url": "https://en.wikipedia.org/wiki?curid=41227473", "title": "Origins of global surveillance", "text": "Origins of global surveillance\n\nThe origins of global surveillance can be traced back to the late 1940s, when the UKUSA Agreement was jointly enacted by the United Kingdom and the United States, whose close cooperation eventually culminated in the creation of the global surveillance network, code-named \"ECHELON\", in 1971.\n\nIn the aftermath of the 1970s Watergate affair and a subsequent congressional inquiry led by Sen. Frank Church, it was revealed that the NSA, in collaboration with Britain's GCHQ, had routinely intercepted the international communications of prominent anti-Vietnam War leaders such as Jane Fonda and Dr. Benjamin Spock. Decades later, a multi-year investigation by the European Parliament highlighted the NSA's role in economic espionage in a report entitled 'Development of Surveillance Technology and Risk of Abuse of Economic Information', in 1999.\n\nHowever, for the general public, it was a series of detailed disclosures of internal NSA documents in June 2013 that first revealed the massive extent of the NSA's spying, both foreign and domestic. Most of these were leaked by an ex-contractor, Edward Snowden. Even so, a number of these older global surveillance programs such as PRISM, XKeyscore, and Tempora were referenced in the 2013 release of thousands of documents. As confirmed by the NSA's director Keith B. Alexander on September 26, 2013, the NSA collects and stores all phone records of all American citizens. Much of the data is kept in large storage facilities such as the Utah Data Center, a US$1.5 billion megaproject referred to by \"The Wall Street Journal\" as a \"symbol of the spy agency's surveillance prowess.\"\n\nWartime censorship of communications during the World Wars was paralleled by peacetime decipherment of communications by the Black Chamber (Cipher Bureau, MI-8), operating with the approval of the U.S. State Department from 1919 to 1929. In 1945 the now-defunct Project SHAMROCK was created to gather all telegraphic data entering into or exiting from the United States. Major communication companies such as Western Union, RCA Global and ITT World Communications actively aided the U.S. government in the latter's attempt to gain access to international message traffic.\n\nIn 1952, the NSA was officially established. According to \"The New York Times\", the NSA was created in \"absolute secrecy\" by President Truman. Six weeks after President Truman took office, he ordered wiretaps on the telephones of Thomas Gardiner Corcoran, a close advisor of Franklin D. Roosevelt. The recorded conversations are currently kept at the Harry S. Truman Presidential Library and Museum, along with other sensitive documents (~233,600 pages).\n\nUnder J. Edgar Hoover, the Federal Bureau of Investigation (FBI) carried out wide-ranging surveillance of communications and political expression, targeting many well-known speakers such as Albert Einstein, Frank Sinatra, First Lady Eleanor Roosevelt, Marilyn Monroe, John Lennon, Martin Luther King, Jr., A FBI memo recognized King to be the \"most dangerous and effective Negro leader in the country.\", and Daniel Ellsberg, Some of these activities were eventually uncovered in the aftermath of the Watergate scandal, leading to the Resignation of Richard Nixon.\n\nDuring World War II the U.K. and U.S. governments entered into a series of agreements for sharing of signals intelligence of enemy communications traffic. In March 1946, a secret agreement, the \"British-US Communication Intelligence Agreement\", known as BRUSA, was established, based on the wartime agreements. The agreement \"tied the two countries into a worldwide network of listening posts run by Government Communications Headquarters (GCHQ), the U.K.'s biggest spying organisation, and its U.S. equivalent, the National Security Agency.\"\n\nIn 1988, an article titled \"Somebody's listening\" by Duncan Campbell in the \"New Statesman\", described the signals intelligence gathering activities of a program code-named \"ECHELON. The program was engaged by English-speaking World War II Allied powers Australia, Canada, New Zealand, the United Kingdom and the United States (collectively known as AUSCANNZUKUS). Based on the UKUSA Agreement, it was created to monitor the military and diplomatic communications of the Soviet Union and its Eastern Bloc allies during the Cold War in the early 1960s. Though its existence had long been known, the UKUSA agreement only became public in 2010. It enabled the U.S. and the U.K. to exchange \"knowledge from operations involving intercepting, decoding and translating foreign communications.\" The agreement forbade the parties to reveal its existence to any third party.\n\nBy the late 1990s the ECHELON system was capable of intercepting satellite transmissions, public switched telephone network (PSTN) communications (including most Internet traffic), and transmissions carried by microwave. A detailed description of ECHELON was provided by New Zealand journalist Nicky Hager in his 1996 book \"Secret Power\". While the existence of ECHELON was denied by some member governments, a report by a committee of the European Parliament in 2001 confirmed the program's use and warned Europeans about its reach and effects. The European Parliament stated in its report that the term \"ECHELON\" was used in a number of contexts, but that the evidence presented indicated it was a signals intelligence collection system capable of interception and content inspection of telephone calls, fax, e-mail and other data traffic globally. The report to the European Parliament confirmed that this was a \"global system for the interception of private and commercial communications.\"\n\nIn the aftermath of the September 11 attacks in 2001 on the World Trade Center and the Pentagon, the scope of domestic spying in the United States increased significantly. The bid to prevent future attacks of this scale led to the passage of the Patriot Act. Later acts include the Protect America Act (which removes the warrant requirement for government surveillance of foreign targets) and the FISA Amendments Act (which relaxed some of the original FISA court requirements).\n\nIn 2005, the existence of STELLARWIND was revealed by Thomas Tamm. On January 1, 2006, days after \"The New York Times\" wrote that \"\"Bush Lets U.S. Spy on Callers Without Courts\", the President emphasized that \"This is a limited program designed to prevent attacks on the United States of America. And I repeat, limited.\"\n\nIn 2006, Mark Klein revealed the existence of Room 641A that he had wired back in 2003. In 2008, Babak Pasdar, a computer security expert, and CEO of Bat Blue publicly revealed the existence of the \"Quantico circuit\", that he and his team found in 2003. He described it as a back door to the federal government in the systems of an unnamed wireless provider; the company was later independently identified as Verizon. Additional disclosures regarding a mass surveillance program involving U.S. citizens had been made in the U.S. media in 2006.\n\nOn November 28, 2010, WikiLeaks and five major news outlets in Spain (\"El País\"), France (\"Le Monde\"), Germany (\"Der Spiegel\"), the United Kingdom (\"The Guardian\"), and the United States (\"The New York Times\") began publishing the first 220 of 251,287 leaked U.S. State department diplomatic \"cables\" simultaneously.\n\nOn March 15, 2012, the American magazine \"Wired\" published an article with the headline \"The NSA Is Building the Country's Biggest Spy Center (Watch What You Say)\", which was later mentioned by U.S. Rep. Hank Johnson during a congressional hearing. In response to Johnson's inquiry, NSA director Keith B. Alexander testified that these allegations made by \"Wired\" magazine were untrue.\n\nIn early 2013, Edward Snowden handed over 200,000 top secret documents to various media outlets, triggering one of the biggest news leaks in the modern history of the United States.\n\n"}
{"id": "9291245", "url": "https://en.wikipedia.org/wiki?curid=9291245", "title": "Pandemic severity index", "text": "Pandemic severity index\n\nThe pandemic severity index (PSI) is a proposed classification scale for reporting the severity of influenza pandemics in the United States. The PSI was accompanied by a set of guidelines intended to help communicate appropriate actions for communities to follow in potential pandemic situations. Released by the United States Department of Health and Human Services (HHS) on 1 February 2007, the PSI was designed to resemble the Saffir-Simpson Hurricane Scale classification scheme.\n\nThe PSI was developed by the Centers for Disease Control and Prevention (CDC) as a new pandemic influenza planning tool for use by states, communities, businesses and schools, as part of a drive to provide more specific community-level prevention measures. Although designed for domestic implementation, the HHS has not ruled out sharing the index and guidelines with interested international parties.\n\nThe index and guidelines were developed by applying principles of epidemiology to data from the history of the last three major flu pandemics and seasonal flu transmission, mathematical models, and input from experts and citizen focus groups. Many \"tried and true\" practices were combined together in a more structured manner:-\n\nDuring the onset of a growing pandemic, local communities cannot rely upon widespread availability of antiviral drugs and vaccines (See Influenza research).\nThe goal of the index is to provide guidance as to what measures various organizations can enact that will slow down the progression of a pandemic, easing the burden of stress upon community resources while definite solutions, like drugs and vaccines, can be brought to bear on the situation. The CDC expects adoption of the PSI will allow early co-ordinated use of community mitigation measures to affect pandemic progression.\n\nThe index focuses less on how likely a disease will spread worldwide – that is, become a pandemic – and more upon how severe the epidemic actually is.\nThe main criterion used to measure pandemic severity will be case-fatality ratio (CFR), the percentage of deaths out of the total reported cases of the disease.\n\nThe actual implementation of PSI alerts is expected to occur after the World Health Organization (WHO) announces phase 6 influenza transmission (human to human) in the United States. This would probably result in immediate announcement of a PSI level 3–4 situation.\n\nThe analogy of \"category\" levels were introduced to provide an understandable connection to hurricane classification schemes, with specific reference to the recent aftermath of Hurricane Katrina.\nLike the Saffir–Simpson Hurricane Scale, the PSI ranges from 1 to 5, with Category 1 pandemics being most mild (equivalent to seasonal flu) and level 5 being reserved for the most severe \"worst-case\" scenario pandemics (such as the 1918 Spanish flu).\n\nThe report recommends four primary social distancing measures for slowing down a pandemic:\n\nThese actions, when implemented, can have an overall effect of reducing the number of new cases of the disease; but they can carry potentially adverse consequences in terms of community and social disruption. The measures should have the most noticeable impact if implemented uniformly by organizations and governments across the US.\n\nWhile unveiling the PSI, Dr. Martin Cetron, Director for the Division of Global Migration and Quarantine at the CDC, reported that early feedback to the idea of a pandemic classification scale has been \"uniformly positive\".\n\nThe University of Minnesota's Center for Infectious Disease Research and Policy (CIDRAP) reports that the PSI has been \"drawing generally high marks from public health officials and others, but they say the plan spells a massive workload for local planners\". One MD praised that the PSI were \"a big improvement over the previous guidance\"; while historical influenza expert and author John M. Barry was more critical of the PSI, saying not enough emphasis was placed on basic health principles that could have an impact at the community level, adding \"I'd feel a lot more comfortable with a lot more research [supporting them]\".\n\nDuring the initial press releases in 2007, the CDC acknowledge that the PSI and the accompanying guidelines were a work in progress and will likely undergo revision in the months following their release.\n\n"}
{"id": "24458151", "url": "https://en.wikipedia.org/wiki?curid=24458151", "title": "Planetary boundaries", "text": "Planetary boundaries\n\nPlanetary boundaries is a concept involving Earth system processes which contain environmental boundaries, proposed in 2009 by a group of Earth system and environmental scientists led by Johan Rockström from the Stockholm Resilience Centre and Will Steffen from the Australian National University. The group wanted to define a \"safe operating space for humanity\" for the international community, including governments at all levels, international organizations, civil society, the scientific community and the private sector, as a precondition for sustainable development. The framework is based on scientific evidence that human actions since the Industrial Revolution have become the main driver of global environmental change. \n\nAccording to the paradigm, \"transgressing one or more planetary boundaries may be deleterious or even catastrophic due to the risk of crossing thresholds that will trigger non-linear, abrupt environmental change within continental-to planetary-scale systems.\" The Earth system process boundaries mark the safe zone for the planet to the extent that they are not crossed. As of 2009, two boundaries have already been crossed, while others are in imminent danger of being crossed.\n\nIn 2009, a group of Earth system and environmental scientists led by Johan Rockström from the Stockholm Resilience Centre and Will Steffen from the Australian National University collaborated with 26 leading academics, including Nobel laureate Paul Crutzen, Goddard Institute for Space Studies climate scientist James Hansen and the German Chancellor's chief climate adviser Hans Joachim Schellnhuber and identified nine \"planetary life support systems\" essential for human survival, attempting to quantify how far seven of these systems had been pushed already. They estimated how much further humans can go before planetary habitability is threatened. \nEstimates indicated that three of these boundaries—climate change, biodiversity loss, and the biogeochemical flow boundary—appear to have been crossed. The boundaries were \"rough, first estimates only, surrounded by large uncertainties and knowledge gaps\" which interact in complex ways that are not yet well understood. Boundaries were defined to help define a \"safe space for human development\", which was an improvement on approaches aiming at minimizing human impacts on the planet. The 2009 report was presented to the General Assembly of the Club of Rome in Amsterdam. An edited summary of the report was published as the featured article in a special 2009 edition of \"Nature\". \nalongside invited critical commentary from leading academics like Nobel laureate Mario J. Molina and biologist Cristián Samper.\n\nIn 2015, a second paper was published in \"Science\" to update the Planetary Boundaries concept and findings were presented at the World Economic Forum in Davos, January 2015.\n\nA 2018 study, co-authored by Rockström, calls into question the international agreement to limit warming to 2 degrees above pre-industrial temperatures set forth in the Paris Agreement. The scientists raise the possibility that even if greenhouse gas emissions are substantially reduced to limit warming to 2 degrees, that might be the \"threshold\" at which self-reinforcing climate feedbacks add additional warming until the climate system stabilizes in a hothouse climate state. This would make parts of the world uninhabitable, raise sea levels by up to , and raise temperatures by to levels that are higher than any interglacial period in the past 1.2 million years. Rockström notes that whether this would occur \"is one of the most existential questions in science.\" Study author Katherine Richardson stresses, \"We note that the Earth has never in its history had a quasi-stable state that is around 2 °C warmer than the preindustrial and suggest that there is substantial risk that the system, itself, will ‘want’ to continue warming because of all of these other processes – even if we stop emissions. This implies not only reducing emissions but much more.”\n\nThe idea that our planet has limits, including the burden placed upon it by human activities, has been around for some time. In 1972, \"The Limits to Growth\" was published. It presented a model in which five variables: world population, industrialization, pollution, food production, and resources depletion, are examined, and considered to grow exponentially, whereas the ability of technology to increase resources availability is only linear. Subsequently, the report was widely dismissed, particularly by economists and businessmen, and it has often been claimed that history has proved the projections to be incorrect. In 2008, Graham Turner from the Commonwealth Scientific and Industrial Research Organisation (CSIRO) published \"A comparison of \"The Limits to Growth\" with thirty years of reality\". Turner found that the observed historical data from 1970 to 2000 closely matches the simulated results of the \"standard run\" limits of growth model for almost all the outputs reported. \"The comparison is well within uncertainty bounds of nearly all the data in terms of both magnitude and the trends over time.\" Turner also examined a number of reports, particularly by economists, which over the years have purported to discredit the limits-to-growth model. Turner says these reports are flawed, and reflect misunderstandings about the model. In 2010, Nørgård, Peet and Ragnarsdóttir called the book a \"pioneering report\", and said that it \"has withstood the test of time and, indeed, has only become more relevant.\"\n\"Our Common Future\" was published in 1987 by United Nations' World Commission on Environment and Development. It tried to recapture the spirit of the Stockholm Conference. Its aim was to interlock the concepts of development and environment for future political discussions. It introduced the famous definition for sustainable development:\n\nOf a different kind is the approach made by James Lovelock. In the 1970s he and microbiologist Lynn Margulis presented the Gaia theory or hypothesis, that states that all organisms and their inorganic surroundings on Earth are integrated into a single self-regulating system. The system has the ability to react to perturbations or deviations, much like a living organism adjusts its regulation mechanisms to accommodate environmental changes such as temperature (homeostasis). Nevertheless, this capacity has limits. For instance, when a living organism is subjected to a temperature that is lower or higher than its living range, it can perish because its regulating mechanism cannot make the necessary adjustments. Similarly the Earth may not be able to react to large deviations in critical parameters. In his book \"The Revenge of Gaia\", he affirms that the destruction of rainforests and biodiversity, compounded with the increase of greenhouse gases made by humans, is producing global warming.\n\nThe Holocene began about 10,000 years ago. It is the current interglacial period, and it has proven to be a relatively stable environment of the Earth. There have been natural environmental fluctuations during the Holocene, but the key atmospheric and biogeochemical parameters have been relatively stable. This stability and resilience has allowed agriculture to develop and complex societies to thrive. According to Rockström \"et al.\", we \"have now become so dependent on those investments for our way of life, and how we have organized society, technologies, and economies around them, that we must take the range within which Earth System processes varied in the Holocene as a scientific reference point for a desirable planetary state.\"\n\nSince the industrial revolution, according to Paul Crutzen, Will Steffen and others, the planet has entered a new epoch, the Anthropocene. In the Anthropocene, humans have become the main agents of change to the Earth system. There have been well publicized scientific warnings about risks in the areas of climate change and stratospheric ozone. However, other biophysical processes are also important. For example, since the advent of the Anthropocene, the rate at which species are being extinguished has increased over 100 times, and humans are now the driving force altering global river flows as well as water vapor flows from the land surface. Continuing pressure on the Earth's biophysical systems from human activities raises concerns that further pressure could be destabilizing, and precipitate sudden or irreversible changes to the environment. According to Rockström et al., \"Up to 30% of all mammal, bird, and amphibian species will be threatened with extinction this century.\" It is difficult to address the issue, because the predominant paradigms of social and economic development are largely indifferent to the looming possibilities of large scale environmental disasters triggered by humans. Legal boundaries can help keep human activities in check, but are only as effective as the political will to make and enforce them.\n\nThresholds and boundaries\n\nThe \"threshold\", or climatological tipping point, is the value at which a very small increment for the control variable (like CO) produces a large, possibly catastrophic, change in the response variable (global warming).\n\nThe threshold points are difficult to locate, because the Earth System is very complex. Instead of defining the threshold value, the study establishes a range, and the threshold is supposed to lie inside it. The lower end of that range is defined as the \"boundary\". Therefore, it defines a safe space, in the sense that as long as we are below the boundary, we are below the threshold value. If the boundary is crossed, we enter into a danger zone.\n\nThe proposed framework lays the groundwork for shifting approach to governance and management, away from the essentially sectoral analyses of limits to growth aimed at minimizing negative externalities, toward the estimation of the safe space for human development. Planetary boundaries define, as it were, the boundaries of the \"planetary playing field\" for humanity if major human-induced environmental change on a global scale is to be avoided\n\nTransgressing one or more planetary boundaries may be highly damaging or even catastrophic, due to the risk of crossing thresholds that trigger non-linear, abrupt environmental change within continental- to planetary-scale systems. The 2009 study identified nine planetary boundaries and, drawing on current scientific understanding, the researchers proposed quantifications for seven of them. These seven are climate change (CO concentration in the atmosphere < 350 ppm and/or a maximum change of +1 W/m in radiative forcing); ocean acidification (mean surface seawater saturation state with respect to aragonite ≥ 80% of pre-industrial levels); stratospheric ozone (less than 5% reduction in total atmospheric O from a pre-industrial level of 290 Dobson Units); biogeochemical nitrogen (N) cycle (limit industrial and agricultural fixation of N to 35 Tg N/yr) and phosphorus (P) cycle (annual P inflow to oceans not to exceed 10 times the natural background weathering of P); global freshwater use (< 4000 km/yr of consumptive use of runoff resources); land system change (< 15% of the ice-free land surface under cropland); and the rate at which biological diversity is lost (annual rate of < 10 extinctions per million species). The two additional planetary boundaries for which the group had not yet been able to determine a boundary level are chemical pollution and atmospheric aerosol loading.\n\nChristopher Field, director of the Carnegie Institution's Department of Global Ecology, is impressed: \"This kind of work is critically important. Overall, this is an impressive attempt to define a safety zone.\" But the conservation biologist Stuart Pimm is not impressed: \"I don’t think this is in any way a useful way of thinking about things... The notion of a single boundary is just devoid of serious content. In what way is an extinction rate 10 times the background rate acceptable?\" and the environmental policy analyst Bill Clark thinks: \"Tipping points in the earth system are dense, unpredictable... and unlikely to be avoidable through early warning indicators. It follows that... 'safe operating spaces' and 'planetary boundaries' are thus highly suspect and potentially the new 'opiates'.\"\n\nThe biogeochemist William Schlesinger queries whether thresholds are a good idea for pollutions at all. He thinks waiting until we near some suggested limit will just permit us to continue to a point where it is too late. \"Management based on thresholds, although attractive in its simplicity, allows pernicious, slow and diffuse degradation to persist nearly indefinitely.\"\n\nThe hydrologist David Molden thinks planetary boundaries are a welcome new approach in the 'limits to growth' debate. \"As a scientific organizing principle, the concept has many strengths ... the numbers are important because they provide targets for policymakers, giving a clear indication of the magnitude and direction of change. They also provide benchmarks and direction for science. As we improve our understanding of Earth processes and complex inter-relationships, these benchmarks can and will be updated ... we now have a tool we can use to help us think more deeply—and urgently—about planetary limits and the critical actions we have to take.\"\n\nThe ocean chemist Peter Brewer queries whether it is \"truly useful to create a list of environmental limits without serious plans for how they may be achieved ... they may become just another stick to beat citizens with. Disruption of the global nitrogen cycle is one clear example: it is likely that a large fraction of people on Earth would not be alive today without the artificial production of fertilizer. How can such ethical and economic issues be matched with a simple call to set limits? ... food is not optional.\"\n\nThe environment advisor Steve Bass says the \"description of planetary boundaries is a sound idea. We need to know how to live within the unusually stable conditions of our present Holocene period and not do anything that causes irreversible environmental change ... Their paper has profound implications for future governance systems, offering some of the 'wiring' needed to link governance of national and global economies with governance of the environment and natural resources. The planetary boundaries concept should enable policymakers to understand more clearly that, like human rights and representative government, environmental change knows no borders.\"\n\nThe climate change policy advisor Adele Morris thinks that price-based policies are also needed to avoid political and economic thresholds. \"Staying within a 'safe operating space' will require staying within all the relevant boundaries, including the electorate’s willingness to pay.\"\n\nIn their report (2012) entitled \"Resilient People, Resilient Planet: A future worth choosing\", The High-level Panel on Global Sustainability called for bold global efforts, \"including launching a major global scientific initiative, to strengthen the interface between science and policy. We must define, through science, what scientists refer to as \"planetary boundaries\", \"environmental thresholds\" and \"tipping points\".\"\n\nIn 2011, at their second meeting, the High-level Panel on Global Sustainability of the United Nations had incorporated the concept of planetary boundaries into their framework, stating that their goal was: \"To eradicate poverty and reduce inequality, make growth inclusive, and production and consumption more sustainable while combating climate change and respecting the range of other planetary boundaries.\"\n\nElsewhere in their proceedings, panel members have expressed reservations about the political effectiveness of using the concept of \"planetary boundaries\": \"Planetary boundaries are still an evolving concept that should be used with caution [...] The planetary boundaries question can be divisive as it can be perceived as a tool of the \"North\" to tell the \"South\" not to follow the resource intensive and environmentally destructive development pathway that rich countries took themselves... This language is unacceptable to most of the developing countries as they fear that an emphasis on boundaries would place unacceptable brakes on poor countries.\"\n\nHowever, the concept is routinely used in the proceedings of the United Nations, and in the \"UN Daily News\". For example, the UNEP Executive Director Achim Steiner states that the challenge of agriculture is to \"feed a growing global population without pushing humanity's footprint beyond planetary boundaries.\" The United Nations Environment Programme (UNEP) Yearbook 2010 also repeated Rockström's message, conceptually linking it with ecosystem management and environmental governance indicators.\n\nThe planetary boundaries concept is also used in proceedings by the European Commission, and was referred to in the European Environment Agency synthesis report \"The European environment – state and outlook 2010\".\n\nRadiative forcing is a measure of the difference between the incoming radiation energy and the outgoing radiation energy acting across the boundary of the earth. Positive radiative forcing results in warming. From the start of the industrial revolution in 1750 to 2005, the increase in atmospheric carbon dioxide has led to a positive radiative forcing, averaging about 1.66 W/m².\n\nThe climate scientist Myles Allen thinks setting \"a limit on long-term atmospheric carbon dioxide concentrations merely distracts from the much more immediate challenge of limiting warming to 2 °C.\" He says the concentration of carbon dioxide is not a control variable we can \"meaningfully claim to control\", and he questions whether keeping carbon dioxide levels below 350 ppm will avoid more than 2 °C of warming.\n\nAdele Morris, policy director, Climate and Energy Economics Project, Brookings Institution, makes a criticism from the economical-political point of view. She puts emphasis in choosing policies that minimize costs and preserve consensus. She favors a system of green-house gas emissions tax, and emissions trading, as ways to prevent global warming. She thinks that too-ambitious objectives, like the boundary limit on CO, may discourage such actions.\n\nAccording to the biologist Cristián Samper, a \" boundary that expresses the probability of families of species disappearing over time would better reflect our potential impacts on the future of life on Earth.\"\n\nThe conservation ecologist Gretchen Daily claims that \"it is time to confront the hard truth that traditional approaches to conservation, taken alone, are doomed to fail. Nature reserves are too small, too few, too isolated and too subject to change to support more than a tiny fraction of Earth’s biodiversity. The challenge is to make conservation attractive—from economic and cultural perspectives. We cannot go on treating nature like an all-you-can-eat buffet. We depend on nature for food security, clean water, climate stability, seafood, timber, and other biological and physical services. To maintain these benefits, we need not just remote reserves but places everywhere—more like 'ecosystem service stations.' A few pioneers are integrating conservation and human development. The Costa Rican government is paying landowners for ecosystem services from tropical forests, including carbon offsets, hydropower production, biodiversity conservation and scenic beauty. China is investing $100 billion in \"ecocompensation,\" including innovative policy and finance mechanisms that reward conservation and restoration. The country is also creating \"ecosystem function conservation areas\" that make up 18 percent of its land area. Colombia and South Africa have made dramatic policy changes, too. Three advances would help the rest of the world scale such models of success. One: new science and tools to value and account for natural capital, in biophysical, economic and other terms [...] Two: compelling demonstrations of such tools in resource policy. Three: cooperation among governments, development organizations, corporations and communities to help nations build more durable economies while also maintaining critical ecosystem services.\"\n\nSince the industrial revolution, the Earth's nitrogen cycle has been disturbed even more than the carbon cycle. \"Human activities now convert more nitrogen from the atmosphere into reactive forms than all of the Earth´s terrestrial processes combined. Much of this new reactive nitrogen pollutes waterways and coastal zones, is emitted back to the atmosphere in changed forms, or accumulates in the terrestrial biosphere.\" Only a small part of the fertilizers applied in agriculture is used by plants. Most of the nitrogen and phosphorus ends up in rivers, lakes and the sea, where excess amounts stress aquatic ecosystems. For example, fertilizer which discharges from rivers into the Gulf of Mexico has damaged shrimp fisheries because of hypoxia.\n\nThe biogeochemist William Schlesinger thinks waiting until we near some suggested limit for nitrogen deposition and other pollutions will just permit us to continue to a point where it is too late. He says the boundary suggested for phosphorus is not sustainable, and would exhaust the known phosphorus reserves in less than 200 years.\n\nWith regard to nitrogen, the biogeochemist and ecosystem scientist Robert Howarth says: \"Human activity has greatly altered the flow of nitrogen across the globe. The single largest contributor is fertilizer use. But the burning of fossil fuels actually dominates the problem in some regions, such as the northeastern U.S. The solution in that case is to conserve energy and use it more efficiently. Hybrid vehicles are another excellent fix; their nitrogen emissions are significantly less than traditional vehicles because their engines turn off while the vehicle is stopped. (Emissions from conventional vehicles actually rise when the engine is idling.) Nitrogen emissions from U.S. power plants could be greatly reduced, too, if plants that predate the Clean Air Act and its amendments were required to comply; these plants pollute far out of proportion to the amount of electricity they produce.\n\nIn agriculture, many farmers could use less fertilizer, and the reductions in crop yields would be small or nonexistent. Runoff from corn fields is particularly avoidable because corn’s roots penetrate only the top few inches of soil and assimilate nutrients for only two months of the year. In addition, nitrogen losses can be reduced by 30 percent or more if farmers plant winter cover crops, such as rye or wheat, which can help the soil hold nitrogen. These crops also increase carbon sequestration in soils, mitigating climate change. Better yet is to grow perennial plants such as grasses rather than corn; nitrogen losses are many times lower. Nitrogen pollution from concentrated animal feeding operations (CAFOs) is a huge problem.\n\nAs recently as the 1970s, most animals were fed local crops, and the animals’ wastes were returned to the fields as fertilizer. Today most U.S. animals are fed crops grown hundreds of miles away, making it “uneconomical” to return the manure. The solution? Require CAFO owners to treat their wastes, just as municipalities must do with human wastes. Further, if we ate less meat, less waste would be generated and less synthetic fertilizer would be needed to grow animal feed. Eating meat from animals that are range-fed on perennial grasses would be ideal. The explosive growth in the production of ethanol as a biofuel is greatly aggravating nitrogen pollution. Several studies have suggested that if mandated U.S. ethanol targets are met, the amount of nitrogen flowing down the Mississippi River and fueling the Gulf of Mexico dead zone may increase by 30 to 40 percent. The best alternative would be to forgo the production of ethanol from corn. If the country wants to rely on biofuels, it should instead grow grasses and trees and burn these to co-generate heat and electricity; nitrogen pollution and greenhouse gas emissions would be much lower.\"\n\nWith regard to phosphorus, the ocean engineer David Vaccari says that the most sustainable environmental flow of phosphorus \"would be the natural flux: seven million metric tons per year (Mt/yr). To hit that mark yet satisfy our usage of 22 Mt/yr, we would have to recycle or reuse 72 percent of our phosphorus [...] The flow could be reduced with existing technologies... [lowering] the loss to waterways from 22 to 8.25 Mt/yr, not very much above the natural flux.\"\n\nPeak phosphorus is a concept to describe the point in time at which the maximum global phosphorus production rate is reached. Phosphorus is a scarce finite resource on earth and means of production other than mining are unavailable because of its non-gaseous environmental cycle. According to some researchers, Earth's phosphorus reserves are expected to be completely depleted in 50–100 years and peak phosphorus to be reached in approximately 2030.\n\nSurface ocean acidity has increased thirty percent since the industrial revolution. About one quarter of the additional carbon dioxide generated by humans is dissolved in the oceans, where it forms carbonic acid. This acidity inhibits the ability of corals, shellfish and plankton to build shells and skeletons. Knock-on effects could have serious consequences for fish stocks. This boundary is clearly interconnected with the climate change boundaries, since the concentration of carbon dioxide in the atmosphere is also the underlying control variable for the ocean acidification boundary.\n\nThe ocean chemist Peter Brewer thinks \"ocean acidification has impacts other than simple changes in pH, and these may need boundaries too.\"\n\nThe marine chemist Scott Doney thinks \"the main tactics are raising energy efficiency, switching to renewable and nuclear power, protecting forests and exploring carbon sequestration technologies. Regionally, nutrient runoff to coastal waters not only creates dead zones but also amplifies acidification. The excess nutrients cause more phytoplankton to grow, and as they die the added CO2 from their decay acidifies the water. We have to be smarter about how we fertilize fields and lawns and treat livestock manure and sewage ... Locally, acidic water could be buffered with limestone or chemical bases produced electrochemically from seawater and rocks. More practical may be protecting specific shellfish beds and aquaculture fisheries. Larval mollusks such as clams and oysters appear to be more susceptible to acidification than adults, and recycling old clamshells into the mud may help buffer pH and provide better substrate for larval attachment. The drop in ocean pH is expected to accelerate in coming decades, so marine ecosystems will have to adapt. We can enhance their chances for success by reducing other insults such as water pollution and overfishing, making them better able to withstand some acidification while we transition away from a fossil-fuel energy economy.\"\n\nAcross the planet, forests, wetlands and other vegetation types are being converted to agricultural and other land uses, impacting freshwater, carbon and other cycles, and reducing biodiversity.\n\nThe environment advisor Steve Bass says research tells us that \"the sustainability of land use depends less on percentages and more on other factors. For example, the environmental impact of 15 per cent coverage by intensively farmed cropland in large blocks will be significantly different from that of 15 per cent of land farmed in more sustainable ways, integrated into the landscape. The boundary of 15 per cent land-use change is, in practice, a premature policy guideline that dilutes the authors' overall scientific proposition. Instead, the authors might want to consider a limit on soil degradation or soil loss. This would be a more valid and useful indicator of the state of terrestrial health.\"\n\nThe Earth systems scientist Eric Lambin thinks that \"intensive agriculture should be concentrated on land that has the best potential for high-yield crops ... We can avoid losing the best agricultural land by controlling land degradation, freshwater depletion and urban sprawl. This step will require zoning and the adoption of more efficient agricultural practices, especially in developing countries. The need for farmland can be lessened, too, by decreasing waste along the food distribution chain, encouraging slower population growth, ensuring more equitable food distribution worldwide and significantly reducing meat consumption in rich countries.\"\n\nHuman pressures on global freshwater systems are having dramatic effects. The freshwater cycle is another boundary significantly affected by climate change. Freshwater resources, such as lakes and aquifers, are usually renewable resources which naturally recharge (the term fossil water is sometimes used to describe aquifers which don't recharge). Overexploitation occurs if a water resource is mined or extracted at a rate that exceeds the recharge rate. Recharge usually comes from area streams, rivers and lakes. Forests enhance the recharge of aquifers in some locales, although generally forests are a major source of aquifer depletion. Depleted aquifers can become polluted with contaminants such as nitrates, or permanently damaged through subsidence or through saline intrusion from the ocean. This turns much of the world's underground water and lakes into finite resources with peak usage debates similar to oil. Though Hubbert's original analysis did not apply to renewable resources, their overexploitation can result in a Hubbert-like peak. A modified Hubbert curve applies to any resource that can be harvested faster than it can be replaced.\n\nThe hydrologist Peter Gleick comments: \"Few rational observers deny the need for boundaries to freshwater use. More controversial is defining where those limits are or what steps to take to constrain ourselves within them. Another way to describe these boundaries is the concept of peak water. Three different ideas are useful. 'Peak renewable' water limits are the total renewable flows in a watershed. Many of the world's major rivers are already approaching this threshold—when evaporation and consumption surpass natural replenishment from precipitation and other sources. 'Peak nonrenewable' limits apply where human use of water far exceeds natural recharge rates, such as in fossil groundwater basins of the Great Plains, Libya, India, northern China and parts of California's Central Valley. 'Peak ecological' water is the idea that for any hydrological system, increasing withdrawals eventually reach the point where any additional economic benefit of taking the water is outweighed by the additional ecological destruction that causes. Although it is difficult to quantify this point accurately, we have clearly passed the point of peak ecological water in many basins around the world where huge damage has occurred ... The good news is that the potential for savings, without hurting human health or economic productivity, is vast. Improvements in water-use efficiency are possible in every sector. More food can be grown with less water (and less water contamination) by shifting from conventional flood irrigation to drip and precision sprinklers, along with more accurately monitoring and managing soil moisture. Conventional power plants can change from water cooling to dry cooling, and more energy can be generated by sources that use extremely little water, such as photovoltaics and wind.\"\n\nThe hydrologist David Molden says \"a global limit on water consumption is necessary, but the suggested planetary boundary of 4,000 cubic kilometres per year is too generous.\"\n\nThe stratospheric ozone layer protectively filters ultraviolet radiation (UV) from the Sun, which would otherwise damage biological systems. The actions taken after the Montreal Protocol appeared to be keeping the planet within a safe boundary. However, in 2011, according to a paper published in \"Nature\", the boundary was unexpectedly pushed in the Arctic; \"... the fraction of the Arctic vortex in March with total ozone less than 275 Dobson units (DU) is typically near zero, but reached nearly 45%\".\n\nThe Nobel laureate in chemistry, Mario Molina, says \"five per cent is a reasonable limit for acceptable ozone depletion, but it doesn't represent a tipping point\".\n\nThe physicist David Fahey says that as a result of the Montreal Protocol \"stratospheric ozone depletion will largely reverse by 2100. The gain has relied, in part, on intermediate substitutes, notably hydrochlorofluorocarbons (HCFCs), and the growing use of compounds that cause no depletion, such as hydrofluorocarbons (HFCs). Ongoing success depends on several steps:\n\"The two panels will also have to evaluate climate change and ozone recovery together. Climate change affects ozone abundance by altering the chemical composition and dynamics of the stratosphere, and compounds such as HCFCs and HFCs are greenhouse gases. For example, the large projected demand for HFCs could significantly contribute to climate change.\"\n\nAerosol particles in the atmosphere impact the health of humans and influence monsoon and global atmospheric circulation systems. Some aerosols produce clouds which cool the Earth by reflecting sunlight back to space, while others, like soot, produce thin clouds in the upper stratosphere which behave like a greenhouse, warming the Earth. On balance, anthropogenic aerosols probably produce a net negative radiative forcing (cooling influence). Worldwide each year, aerosol particles result in about 800,000 premature deaths. Aerosol loading is sufficiently important to be included among the planetary boundaries, but it is not yet clear whether an appropriate safe threshold measure can be identified.\n\nSome chemicals, such as persistent organic pollutants, heavy metals and radionuclides, have potentially irreversible additive and synergic effects on biological organisms, reducing fertility and resulting in permanent genetic damage. Sublethal uptakes are drastically reducing marine bird and mammal populations. This boundary seems important, although it is hard to quantify.\n\nA Bayesian emulator for persistent organic pollutants has been developed which can potentially be used to quantify the boundaries for chemical pollution. To date, critical exposure levels of polychlorinated biphenyls (PCBs) above which mass mortality events of marine mammals are likely to occur, have been proposed as a chemical pollution planetary boundary.\n\nA planetary boundary may interact in a manner that changes the safe operating level of other boundaries. Rockström \"et al.\" 2009 did not analyze such interactions, but they suggested that many of these interactions will reduce rather than expand the proposed boundary levels.\n\nFor example, the land use boundary could shift downward if the freshwater boundary is breached, causing lands to become arid and unavailable for agriculture. At a regional level, water resources may decline in Asia if deforestation continues in the Amazon. Such considerations suggest the need for \"extreme caution in approaching or transgressing any individual planetary boundaries.\"\n\nAnother example has to do with coral reefs and marine ecosystems. In 2009, showed that, since 1990, calcification in the reefs of the Great Barrier that they examined decreased at a rate unprecedented over the last 400 years (14% in less than 20 years). Their evidence suggests that the increasing temperature stress and the declining ocean saturation state of aragonite is making it difficult for reef corals to deposit calcium carbonate. explored how multiple stressors, such as increased nutrient loads and fishing pressure, move corals into less desirable ecosystem states. showed that ocean acidification will significantly change the distribution and abundance of a whole range of marine life, particularly species \"that build skeletons, shells, and tests of biogenic calcium carbonate. \"Increasing temperatures, surface UV radiation levels and ocean acidity all stress marine biota, and the combination of these stresses may well cause perturbations in the abundance and diversity of marine biological systems that go well beyond the effects of a single stressor acting alone.\"\n\nIn 2012 Kate Raworth from Oxfam noted the Rockstrom concept does not take human population growth into account. She suggested social boundaries should be incorporated into the planetary boundary structure, such as jobs, education, food, access to water, health services and energy and to accommodate an environmentally safe space compatible with poverty eradication and \"rights for all\". Within planetary limits and an equitable social foundation lies a doughnut shaped area which is the area where there is a \"safe and just space for humanity to thrive in\".\n\nIn 2012, Steven Running suggested a tenth boundary, the annual net global primary production of all terrestrial plants, as an easily determinable measure integrating many variables that will give \"a clear signal about the health of ecosystems\".\n\nThe United Nations secretary general Ban Ki-moon endorsed the concept of planetary boundaries on 16 March 2012, when he presented the key points of the report of his High Level Panel on Global Sustainability to an informal plenary of the UN General Assembly. Ban stated: \"The Panel’s vision is to eradicate poverty and reduce inequality, to make growth inclusive and production and consumption more sustainable, while combating climate change and respecting a range of other planetary boundaries.\" The concept was incorporated into the so-called \"zero draft\" of the outcome of the United Nations Conference on Sustainable Development to be convened in Rio de Janeiro 20–22 June 2012. However, the use of the concept was subsequently withdrawn from the text of the conference, \"partly due to concerns from some poorer countries that its adoption could lead to the sidelining of poverty reduction and economic development. It is also, say observers, because the idea is simply too new to be officially adopted, and needed to be challenged, weathered and chewed over to test its robustness before standing a chance of being internationally accepted at UN negotiations.\"\n\nThe planetary boundary framework was updated in 2015. It was suggested that three of the boundaries (including climate change) might push the Earth system into a new state if crossed; these also strongly influence the remaining boundaries. In the paper, the framework is developed to make it more applicable at the regional scale.\n\nHuman activities related to agriculture and nutrition globally contribute to the transgression of four out of nine planetary boundaries. Surplus nutrient flows (N, P) into aquatic and terrestrial ecosystems are of highest importance, followed by excessive land-system change and biodiversity loss. Whereas in the case of biodiversity loss, P cycle and land-system change, the transgression is in the zone of uncertainty—indicating an increasing risk (yellow circle in the figure), the N boundary related to agriculture is more than 200% transgressed—indicating a high risk (red marked circle in the figure).\n\n\n\n\n----\n\n\n"}
{"id": "266344", "url": "https://en.wikipedia.org/wiki?curid=266344", "title": "Space debris", "text": "Space debris\n\nInitially, the term space debris referred to the natural debris found in the solar system: asteroids, comets, and meteoroids. However, with the 1979 beginning of the NASA Orbital Debris Program, the term also refers to the debris (alt. space waste or space garbage) from the mass of defunct, artificially created objects in space, especially Earth orbit. These include old satellites and spent rocket stages, as well as the fragments from their disintegration and collisions. \n\nAs of December 2016, five satellite collisions have generated space debris. Space debris is also known as \"orbital debris\", \"space junk\", \"space waste\", \"space trash\", \"space litter\" or \"space garbage\".\n, the United States Strategic Command tracked a total of 17,852 artificial objects in orbit above the Earth, including 1,419 operational satellites. However, these are just objects large enough to be tracked. , more than 170 million bits of debris smaller than , about 670,000 pieces of debris 1–10 cm, and around 29,000 larger pieces were estimated to be in orbit around the earth. Collisions with debris have become a hazard to spacecraft; they cause damage akin to sandblasting, especially to solar panels and optics like telescopes or star trackers that cannot be covered with a ballistic Whipple shield (unless it is transparent).\n\nBelow Earth-altitude, pieces of debris are denser than meteoroids; most are dust from solid rocket motors, surface erosion debris like paint flakes, and frozen coolant from RORSAT (nuclear-powered satellites). \nFor comparison, the International Space Station orbits in the range, and the 2009 satellite collision and 2007 antisat test occurred at altitude. The ISS has Whipple shielding; however, known debris with a collision chance over 1/10,000 are avoided by maneuvering the station.\n\nThe Kessler syndrome, a runaway chain reaction of collisions exponentially increasing the amount of debris, has been hypothesized to ensue beyond a critical density. This could affect useful polar-orbiting bands, increases the cost of protection for spacecraft missions and could destroy live satellites. Whether Kessler syndrome is already underway has been debated. The measurement, mitigation, and potential removal of debris are conducted by some participants in the space industry.\n\nThere are estimated to be over 51 million pieces of debris smaller than as of July 2013. There are approximately 670,000 pieces from one to ten cm. The current count of large debris (defined as 10 cm across or larger) is 29,000. The technical measurement cutoff is c. . Over 98 percent of the 1,900 tons of debris in low Earth orbit (as of 2002) was accounted for by about 1,500 objects, each over . Total mass is mostly constant despite addition of many smaller objects, since they reenter the atmosphere sooner. Using a 2008 figure of 8,500 known items, it is estimated at .\n\nIn LEO there are few \"universal orbits\" which keep spacecraft in particular rings (in contrast to GEO, a single widely used orbit). The closest are sun-synchronous orbits that keep a constant angle between the Sun and the orbital plane; they are polar, meaning they cross over the polar regions. LEO satellites orbit in many planes, up to 15 times a day, causing frequent approaches between objects (the density of objects is much higher in LEO).\n\nOrbits are further changed by perturbations (which in LEO include unevenness of the Earth's gravitational field), and collisions can occur from any direction. For these reasons, the Kessler syndrome applies mostly to the LEO region; impacts occur at up to 16 km/s (twice the orbital speed) if head-on – the 2009 satellite collision occurred at 11.7 km/s, creating much spall in the critical size range. These can cross other orbits and lead to a cascade effect. A large-enough collision (e.g. between a space station and a defunct satellite) could make low Earth orbit impassable.\n\nManned missions are mostly at and below, where air drag helps clear zones of fragments. Atmospheric expansion as a result of space weather raises the critical altitude by increasing drag; in the 90s, it was a factor in reduced debris density. Another was fewer launches by Russia; the USSR made most of their launches in the 1970s and 1980s.\n\nAt higher altitudes, where air drag is less significant, orbital decay takes longer. Slight atmospheric drag, lunar perturbations, Earth's gravity perturbations, solar wind and solar radiation pressure can gradually bring debris down to lower altitudes (where it decays), but at very high altitudes this may take millennia. Although high-altitude orbits are less commonly used than LEO and the onset of the problem is slower, the numbers progress toward the critical threshold more quickly.\n\nMany communications satellites are in geostationary orbits (GEO), clustering over specific targets and sharing the same orbital path. Although velocities are low between GEO objects, when a satellite becomes derelict (such as Telstar 401) it assumes a geosynchronous orbit; its orbital inclination increases about .8° and its speed increases about per year. Impact velocity peaks at about . Orbital perturbations cause longitude drift of the inoperable spacecraft and precession of the orbital plane. Close approaches (within 50 meters) are estimated at one per year. The collision debris pose less short-term risk than from an LEO collision, but the satellite would likely become inoperable. Large objects, such as solar-power satellites, are especially vulnerable to collisions.\n\nAlthough the ITU now requires proof a satellite can be moved out of its orbital slot at the end of its lifespan, studies suggest this is insufficient. Since GEO orbit is too distant to accurately measure objects under , the nature of the problem is not well known. Satellites could be moved to empty spots in GEO, requiring less maneuvring and making it easier to predict future motion. Satellites or boosters in other orbits, especially stranded in geostationary transfer orbit, are an additional concern due to their typically high crossing velocity.\n\nDespite efforts to reduce risk, spacecraft collisions have occurred. The European Space Agency telecom satellite Olympus-1 was struck by a meteoroid on 11 August 1993 and eventually moved to a graveyard orbit. On 29 March 2006, the Russian Express-AM11 communications satellite was struck by an unknown object and rendered inoperable; its engineers had enough contact time with the satellite to send it into a graveyard orbit.\n\nIn 1958, the United States launched Vanguard I into a medium Earth orbit (MEO). , it, and the upper stage of its launch rocket, are the oldest surviving man-made space objects still in orbit. In a catalog of known launches until July 2009, the Union of Concerned Scientists listed 902 operational satellites from a known population of 19,000 large objects and about 30,000 objects launched.\n\nAn example of additional dead satellite debris are the remains of the 1970s/80s Soviet RORSAT naval surveillance satellite program. The satellite's BES-5 nuclear reactor were cooled with a coolant loop of sodium-potassium alloy, creating a potential problem when the satellite reached end of life. While many satellites were nominally boosted into medium-altitude graveyard orbits, not all were. Even satellites which had been properly moved to a higher orbit had an eight-percent probability of puncture and coolant release over a 50-year period. The coolant freezes into droplets of solid sodium-potassium alloy, forming additional debris.\n\nThese events continue to occur. For example, in February 2015, the USAF Defense Meteorological Satellite Program Flight 13 (DMSP-F13) exploded on orbit, creating at least 149 debris objects, which were expected to remain in orbit for decades.\n\nAccording to Edward Tufte's book \"Envisioning Information\", space debris includes a glove lost by astronaut Ed White on the first American space-walk (EVA); a camera lost by Michael Collins near Gemini 10; a thermal blanket lost during STS-88; garbage bags jettisoned by Soviet cosmonauts during Mir's 15-year life, a wrench and a toothbrush. Sunita Williams of STS-116 lost a camera during an EVA. During an STS-120 EVA to reinforce a torn solar panel, a pair of pliers was lost, and in an STS-126 EVA, Heidemarie Stefanyshyn-Piper lost a briefcase-sized tool bag.\n\nIn characterizing the problem of space debris, it was learned that much debris was due to rocket upper stages (e.g. the Inertial Upper Stage) which end up in orbit, and break up due to decomposition of unvented unburned fuel. However, a major known impact event involved an (intact) Ariane booster. Although NASA and the United States Air Force now require upper-stage passivation, other launchers do not.\nLower stages, like the Space Shuttle's solid rocket boosters or Apollo program's Saturn IB launch vehicles, do not reach orbit.\n\nOn 11 March 2000 a Chinese Long March 4 CBERS-1 upper stage exploded in orbit, creating a debris cloud.\nA Russian Briz-M booster stage exploded in orbit over South Australia on 19 February 2007. Launched on 28 February 2006 carrying an Arabsat-4A communications satellite, it malfunctioned before it could use up its propellant. Although the explosion was captured on film by astronomers, due to the orbit path the debris cloud has been difficult to measure with radar. By 21 February 2007, over 1,000 fragments were identified. A 14 February 2007 breakup was recorded by Celestrak. Eight breakups occurred in 2006, the most since 1993. Another Briz-M broke up on 16 October 2012 after a failed 6 August Proton-M launch. The amount and size of the debris was unknown. A Long March 7 rocket booster created a fireball visible from portions of Utah, Nevada, Colorado, Idaho and California on the evening of 27 July 2016; its disintegration was widely reported on social media.\n\nA past debris source was the testing of anti-satellite weapons (ASATs) by the U.S. and Soviet Union during the 1960s and 1970s. North American Aerospace Defense Command (NORAD) files only contained data for Soviet tests, and debris from U.S. tests were only identified later. By the time the debris problem was understood, widespread ASAT testing had ended; the U.S. Program 437 was shut down in 1975.\n\nThe U.S. restarted their ASAT programs in the 1980s with the Vought ASM-135 ASAT. A 1985 test destroyed a satellite orbiting at , creating thousands of debris larger than . Due to the altitude, atmospheric drag decayed the orbit of most debris within a decade. A \"de facto\" moratorium followed the test.\n\nChina's government was condemned for the military implications and the amount of debris from the 2007 anti-satellite missile test, the largest single space debris incident in history (creating over 2,300 pieces golf-ball size or larger, over 35,000 or larger, and one million pieces or larger). The target satellite orbited between and , the portion of near-Earth space most densely populated with satellites. Since atmospheric drag is low at that altitude the debris is slow to return to Earth, and in June 2007 NASA's Terra environmental spacecraft maneuvered to avoid impact from the debris.\n\nOn 20 February 2008, the U.S. launched an SM-3 missile from the USS \"Lake Erie\" to destroy a defective U.S. spy satellite thought to be carrying of toxic hydrazine propellant. The event occurred at about , and the resulting debris has a perigee of or lower. The missile was aimed to minimize the amount of debris, which (according to Pentagon Strategic Command chief Kevin Chilton) had decayed by early 2009.\n\nThe vulnerability of satellites to debris and the possibility of attacking LEO satellites to create debris clouds, has triggered speculation that it is possible for countries unable to make a precision attack. An attack on a satellite of 10 tonnes or more would heavily damage the LEO environment.\n\nSpace junk is a threat to active satellites and spaceships. The Earth's orbit may even become impassable as the risk of collision grows too high.\n\nAlthough spacecraft are protected by Whipple shields, solar panels, which are exposed to the Sun, wear from low-mass impacts. These produce a cloud of plasma which is an electrical risk to the panels.\n\nSatellites are believed to have been destroyed by micrometeorites and orbital debris (MMOD). The earliest suspected loss was of Kosmos 1275, which disappeared on 24 July 1981 (a month after launch). Kosmos contained no volatile propellant, therefore, there appeared to be nothing internal to the satellite which could have caused the destructive explosion which took place. However the case has not been proven and another hypothesis forwarded is that the battery exploded. Tracking showed it broke up, into 300 new objects.\n\nMany impacts have been confirmed since. Olympus-1 was struck by a meteoroid on 11 August 1993, and left adrift. On 24 July 1996, the French microsatellite Cerise was hit by fragments of an Ariane-1 H-10 upper-stage booster which exploded in November 1986. On 29 March 2006, the Russian Ekspress AM11 communications satellite was struck by an unknown object and rendered inoperable; its engineers had sufficient time in contact with the spacecraft to send it to a parking orbit out of GEO. On October 13, 2009, Terra suffered a single battery cell failure anomaly and a battery heater control anomaly which were likely the result of an MMOD strike. On March 12, 2010, Aura lost power from one-half of one of its 11 solar panels and this was also attributed to an MMOD strike. On May 22, 2013 GOES-13 was hit by an MMOD which caused it to lose track of the stars that it uses to maintain attitude. It took nearly a month for the spacecraft to return to operation.\n\nThe first major satellite collision occurred on 10 February 2009 at 16:56 UTC. The deactivated Kosmos 2251 and the operational Iridium 33 collided, over northern Siberia. The relative speed of impact was about , or about . Both satellites were destroyed, with accurate estimates of the number of debris unavailable. On 22 January 2013 BLITS (a Russian laser-ranging satellite) was struck by debris suspected to be from the 2007 Chinese anti-satellite missile test, changing its orbit and spin rate.\n\nSatellites frequently have to perform Collision Avoidance Maneuvers and managers have to monitor debris as part of maneuver planning. For example, in January 2017, the European Space Agency planned to alter orbit of one of its $319 million Swarm mission spacecrafts, based on data from the US Joint Space Operations Center, to end the risk of collision from Cosmos-375, an old Russian satellite. Cosmos-375, which was destroyed by Soviet operators when its mission was complete, had previously threatened to impact the International Space Station in 2011.\n\nFrom the early Space Shuttle missions, NASA used NORAD to monitor the Shuttle's orbital path for debris. In the 1980s, this used much of its capacity. The first collision-avoidance maneuver occurred during STS-48 in September 1991, a seven-second thruster burn to avoid debris from Kosmos 955. Similar maneuvers followed on missions 53, 72 and 82.\n\nOne of the first events to publicize the debris problem occurred on \"Challenger\"'s second flight, STS-7. A fleck of paint struck its front window, creating a pit over wide. On STS-59 in 1994, \"Endeavour\"'s front window was pitted about half its depth. Minor debris impacts increased from 1998.\n\nWindow chipping and minor damage to thermal protection system tiles (TPS) was already common by the 1990s. The Shuttle was later flown tail-first to take the debris load mostly on the engines and rear cargo bay (not used in orbit or during descent, and less critical for post-launch operation). When flying to the ISS, the two connected spacecraft were flipped around so the better-armored station shielded the orbiter.\nNASA's study concluded that debris accounted for half of the overall risk to the Shuttle. Executive-level decision to proceed was required if catastrophic impact was likelier than 1 in 200. On a normal (low-orbit) mission to the ISS the risk was c. 1 in 300, but STS-125 (the Hubble repair mission) at was initially calculated at a 1-in-185 risk (due to the 2009 satellite collision). A re-analysis with better debris numbers reduced the estimated risk to 1 in 221, and the mission went ahead.\n\nDebris incidents continued on later Shuttle missions. During STS-115 in 2006 a fragment of circuit board bored a small hole through the radiator panels in \"Atlantis\"' cargo bay. On STS-118 in 2007 debris blew a bullet-like hole through \"Endeavour\"s radiator panel.\n\nImpact wear was notable on Mir, the Soviet space station, since it remained in space for long periods with its original module panels.\n\nAlthough the ISS uses Whipple shielding to protect itself from minor debris, portions (notably its solar panels) cannot be protected easily. In 1989, the ISS panels were predicted to degrade c. 0.23% in four years, and they were overdesigned by 1%. A maneuver is performed if \"there is a greater than one-in-10,000 chance of a debris strike\". , there have been sixteen maneuvers in the fifteen years the ISS had been in orbit.\n\nThe crew sheltered in the Soyuz on three occasions due to late debris-proximity warnings. In addition to the sixteen firings and three Soyuz-capsule shelter orders, one attempted maneuver failed (due to not having the several days' warning necessary to upload the manoeuvre timeline to the station's computer). A March 2009 close call involved debris believed to be a piece of the Kosmos 1275 satellite. In 2013, the ISS did not maneuver to avoid debris, after a record four debris maneuvers the previous year.\n\nAlthough most manned space activity takes place at altitudes below , a Kessler syndrome cascade in that region would rain down into lower altitudes and the decay time scale is such that \"the resulting [low Earth orbit] debris environment is likely to be too hostile for future space use\".\n\nIn a Kessler syndrome, satellite lifetimes would be measured in years or months. New satellites could be launched through the debris field into higher orbits or placed in lower orbits (where decay removes the debris), but the utility of the region between is the reason for its amount of debris.\n\nAlthough most debris burns up in the atmosphere, larger objects can reach the ground intact. According to NASA, an average of one cataloged piece of debris has fallen back to Earth each day for the past 50 years. Despite their size, there has been no significant property damage from the debris.\n\nIn 1969 five sailors on a Japanese ship were injured by space debris. In 1997 an Oklahoma woman, Lottie Williams, was injured when she was hit in the shoulder by a piece of blackened, woven metallic material confirmed as part of the propellant tank of a Delta II rocket which launched a U.S. Air Force satellite the year before.\n\nThe original re-entry plan for Skylab called for the station to remain in space for eight to ten years after its final mission in February 1974. High solar activity expanded the upper atmosphere, resulting in higher-than-expected drag and bringing its orbit closer to Earth than planned. On 11 July 1979 Skylab re-entered the Earth's atmosphere and disintegrated, raining debris along a path over the southern Indian Ocean and Western Australia.\n\nOn 12 January 2001, a Star 48 Payload Assist Module (PAM-D) rocket upper stage re-entered the atmosphere after a \"catastrophic orbital decay\", crashing in the Saudi Arabian desert. It was identified as the upper-stage rocket for NAVSTAR 32, a GPS satellite launched in 1993.\n\nIn the 2003 \"Columbia\" disaster, large parts of the spacecraft reached the ground and entire equipment systems remained intact. More than 83,000 pieces, along with the remains of the six astronauts, were recovered in an area from three to 10 miles around Hemphill in Sabine County, TX. More pieces were found in a line from west Texas to east Louisiana, with the westernmost piece found in Littlefield, TX and the easternmost found southwest of Mora, LA. Although there is significant evidence that debris fell in Nevada, Utah, and New Mexico, debris was only found in Texas, Arkansas and Louisiana. In a rare case of property damage, a foot-long metal bracket smashed through the roof of a dentist office. NASA warned the public to avoid contact with the debris because of the possible presence of hazardous chemicals. 15 years after the failure, people were still sending in pieces with the last,as of February 1, 2018, found in the spring of 2017.\n\nOn 27 March 2007, airborne debris from a Russian spy satellite was seen by the pilot of a LAN Airlines Airbus A340 carrying 270 passengers whilst flying over the Pacific Ocean between Santiago and Auckland. The debris was within of the aircraft.\n\nRadar and optical detectors such as lidar are the main tools for tracking space debris. Although objects under have reduced orbital stability, debris as small as 1 cm can be tracked, however determining orbits to allow re-acquisition is difficult. Most debris remain unobserved. The NASA Orbital Debris Observatory tracked space debris with a liquid mirror transit telescope. FM Radio waves can detect debris, after reflecting off them onto a receiver. Optical tracking may be a useful early-warning system on spacecraft.\n\nThe U.S. Strategic Command keeps a catalog of known orbital objects, using ground-based radar and telescopes, and a space-based telescope (originally to distinguish from hostile missiles). The 2009 edition listed about 19,000 objects. Other data come from the ESA Space Debris Telescope, TIRA, the Goldstone, Haystack, and EISCAT radars and the Cobra Dane phased array radar, to be used in debris-environment models like the ESA Meteoroid and Space Debris Terrestrial Environment Reference (MASTER).\n\nReturned space hardware is a valuable source of information on the directional distribution and composition of the (sub-millimetre) debris flux. The LDEF satellite deployed by mission STS-41-C \"Challenger\" and retrieved by STS-32 \"Columbia\" spent 68 months in orbit to gather debris data. The EURECA satellite, deployed by STS-46 \"Atlantis\" in 1992 and retrieved by STS-57 \"Endeavour\" in 1993, was also used for debris study.\n\nThe solar arrays of Hubble were returned by missions STS-61 \"Endeavour\" and STS-109 \"Columbia\", and the impact craters studied by the ESA to validate its models. Materials returned from Mir were also studied, notably the Mir Environmental Effects Payload (which also tested materials intended for the ISS).\n\nA debris cloud resulting from a single event is studied with scatter plots known as Gabbard diagrams, where the perigee and apogee of fragments are plotted with respect to their orbital period. Gabbard diagrams of the early debris cloud prior to the effects of perturbations, if the data were available, are reconstructed. They often include data on newly observed, as yet uncatalogued fragments. Gabbard diagrams can provide important insights into the features of the fragmentation, the direction and point of impact.\n\nAn average of about one tracked object per day has been dropping out of orbit for the past 50 years, averaging almost three objects per day at solar maximum (due to the heating and expansion of the Earth's atmosphere), but one about every three days at solar minimum, usually 5½ yr later. In addition to natural atmospheric effects, corporations, academics and government agencies have proposed plans and technology to deal with space debris, but , most of these are theoretical, and there is no extant business plan for debris reduction.\n\nA number of scholars have also observed that institutional factors—political, legal, economic and cultural \"rules of the game\"—are the greatest impediment to the cleanup of near-Earth space. There is no commercial incentive, since costs aren't assigned to polluters, but a number of suggestions have been made. However, effects to date are limited. In the US, governmental bodies have been accused of backsliding on previous commitments to limit debris growth, \"let alone tackling the more complex issues of removing orbital debris.\"\n\nUpper stage passivation (e.g. of Delta boosters) by releasing residual propellants reduces debris from orbital explosions; however not all boosters implement this. Although there is no international treaty minimizing space debris, the United Nations Committee on the Peaceful Uses of Outer Space (COPUOS) published voluntary guidelines in 2007. As of 2008, the committee is discussing international \"rules of the road\" to prevent collisions between satellites.\nBy 2013, various legal regimes existed, typically instantiated in the launch licenses that are required for a launch in all spacefaring nations.\n\nThe U.S. has a set of standard practices for civilian (NASA) and military (DoD and USAF) orbital-debris mitigation, as has the European Space Agency. In 2007, the ISO began preparing an international standard for space-debris mitigation. Germany and France have posted bonds to safeguard property from debris damage.\n\nWhen originally proposed in 2015, the OneWeb constellation, initially planned to have ~700 satellites anticipated on orbit after 2018, would only state that they would re-enter the atmosphere within 25 years of retirement.\nBy October 2017, both OneWeb—and also SpaceX, with their large Starlink constellation—had filed documents with the US FCC with more aggressive space debris mitigation plans. Both companies committed to a deorbit plan for post-mission satellites which will explicitly move the satellites into orbits where they will reenter the Earth's atmosphere within approximately one year following end-of-life.\n\nWith a \"one-up, one-down\" launch-license policy for Earth orbits, launchers would rendezvous with, capture and de-orbit a derelict satellite from approximately the same orbital plane. Another possibility is the robotic refueling of satellites. Experiments have been flown by NASA, and SpaceX is developing large-scale on-orbit propellant transfer technology and tanker spacecraft.\n\nAnother approach to debris mitigation is to explicitly design the mission architecture to always leave the rocket second-stage in an elliptical geocentric orbit with a low-perigee, thus ensuring rapid orbital decay and avoiding long-term orbital debris from spent rocket bodies. Such missions require the use of a small kick stage to circularize the orbit, but the kick stage itself may be designed with the excess-propellant capability to be able to self-deorbit.\n\nAlthough the ITU requires geostationary satellites to move to a graveyard orbit at the end of their lives, the selected orbital areas do not sufficiently protect GEO lanes from debris. Rocket stages (or satellites) with enough propellant may make a direct, controlled de-orbit, or if this would require too much propellant, a satellite may be brought to an orbit where atmospheric drag would cause it to eventually de-orbit. This was done with the French Spot-1 satellite, reducing its atmospheric re-entry time from a projected 200 years to about 15 by lowering its altitude from to about .\n\nPassive methods of increasing the orbital decay rate of spacecraft debris have been proposed. Instead of rockets, an electrodynamic tether could be attached to a spacecraft at launch; at the end of its lifetime, the tether would be rolled out to slow the spacecraft. Other proposals include a booster stage with a sail-like attachment and a large, thin, inflatable balloon envelope.\n\nA consensus of speakers at a meeting in Brussels on 30 October 2012 organized by the Secure World Foundation (a U.S. think tank) and the French International Relations Institute reported that removal of the largest debris would be required to prevent the risk to spacecraft becoming unacceptable in the foreseeable future (without any addition to the inventory of dead spacecraft in LEO). Removal costs and legal questions about ownership and the authority to remove defunct satellites have stymied national or international action. Current space law retains ownership of all satellites with their original operators, even debris or spacecraft which are defunct or threaten active missions.\n\nA well-studied solution uses a remotely controlled vehicle to rendezvous with, capture and return debris to a central station.\nOne such system is Space Infrastructure Servicing, a commercially developed refueling depot and service spacecraft for communications satellites in geosynchronous orbit originally scheduled for a 2015 launch. The SIS would be able to \"push dead satellites into graveyard orbits.\" The Advanced Common Evolved Stage family of upper stages is being designed with a high leftover-propellant margin (for derelict capture and de-orbit) and in-space refueling capability for the high delta-v required to de-orbit heavy objects from geosynchronous orbit. A tug-like satellite to drag debris to a safe altitude for it to burn up in the atmosphere has been researched. When debris is identified the satellite creates a difference in potential between the debris and itself, then using its thrusters to move itself and the debris to a safer orbit.\n\nA variation of this approach is for the remotely controlled vehicle to rendezvous with debris, capture it temporarily to attach a smaller de-orbit satellite and drag the debris with a tether to the desired location. The \"mothership\" would then tow the debris-smallsat combination for atmospheric entry or move it to a graveyard orbit. One such system is the proposed Busek ORbital DEbris Remover (ORDER), which would carry over 40 SUL (satellite on umbilical line) de-orbit satellites and propellant sufficient for their removal.\nOn 7 January 2010 Star, Inc. reported that it received a contract from the Space and Naval Warfare Systems Command for a feasibility study of the ElectroDynamic Debris Eliminator (EDDE) propellantless spacecraft for space-debris removal.\nIn February 2012 the Swiss Space Center at École Polytechnique Fédérale de Lausanne announced the Clean Space One project, a nanosatellite demonstration project for matching orbit with a defunct Swiss nanosatellite, capturing it and de-orbiting together. The mission has seen several evolutions to reach a pac-man inspired capture model.\n\nThe laser broom uses a ground-based laser to ablate the front of the debris, producing a rocket-like thrust which slows the object. With continued application, the debris would fall enough to be influenced by atmospheric drag. During the late 1990s, the U.S. Air Force's Project Orion was a laser-broom design. Although a test-bed device was scheduled to launch on a Space Shuttle in 2003, international agreements banning powerful laser testing in orbit limited its use to measurements. The Space Shuttle \"Columbia\" disaster postponed the project and according to Nicholas Johnson, chief scientist and program manager for NASA's Orbital Debris Program Office, \"There are lots of little gotchas in the Orion final report. There's a reason why it's been sitting on the shelf for more than a decade.\"\n\nThe momentum of the laser-beam photons could directly impart a thrust on the debris sufficient to move small debris into new orbits out of the way of working satellites. NASA research in 2011 indicates that firing a laser beam at a piece of space junk could impart an impulse of per second, and keeping the laser on the debris for a few hours per day could alter its course by per day. One drawback is the potential for material degradation; the energy may break up the debris, adding to the problem. A similar proposal places the laser on a satellite in Sun-synchronous orbit, using a pulsed beam to push satellites into lower orbits to accelerate their reentry. A proposal to replace the laser with an Ion Beam Shepherd has been made, and other proposals use a foamy ball of aerogel or a spray of water,\ninflatable balloons,\nelectrodynamic tethers,\nboom electroadhesion,\nand dedicated anti-satellite weapons.\n\nOn 28 February 2014, Japan's Japan Aerospace Exploration Agency (JAXA) launched a test \"space net\" satellite. The launch was an operational test only. In December 2016 the country sent a space junk collector via Kounotori 6 to the ISS by which JAXA scientists experiment to pull junk out of orbit using a tether. The system failed to extend a 700-meter tether from a space station resupply vehicle that was returning to Earth. On 6 February the mission was declared a failure and leading researcher Koichi Inoue told reporters that they \"believe the tether did not get released\".\n\nSince 2012, the European Space Agency has designed a mission to remove large space debris from orbit. The mission, e.Deorbit, is scheduled for launch during 2023 with an objective to remove debris heavier than from LEO. Several capture techniques are being studied, including a net, a harpoon and a combination robot arm and clamping mechanism.\n\nHolger Krag of the European Space Agency states that as of 2017 there is no binding international regulatory framework with no progress occurring at the respective UN body in Vienna.\n\nIn 1946 during the Giacobinid meteor shower, Helmut Landsberg collected several small magnetic particles that were apparently associated with the shower. Fred Whipple was intrigued by this and wrote a paper that demonstrated that particles of this size were too small to maintain their velocity when they encountered the upper atmosphere. Instead, they quickly decelerated and then fell to Earth unmelted. In order to classify these sorts of objects, he coined the term \"micro-meteorite\".\n\nWhipple, in collaboration with Fletcher Watson of the Harvard Observatory, led an effort to build an observatory to directly measure the velocity of the meteors that could be seen. At the time the source of the micro-meteorites was not known. Direct measurements at the new observatory were used to locate the source of the meteors, demonstrating that the bulk of material was left over from comet tails, and that none of it could be shown to have an extra-solar origin. Today it is understood that meteoroids of all sorts are leftover material from the formation of the Solar System, consisting of particles from the interplanetary dust cloud or other objects made up from this material, like comets.\n\nThe early studies were based on optical measurements only. In 1957, Hans Pettersson conducted one of the first direct measurements of the fall of space dust on the Earth, estimating it to be 14,300,000 tons per year. This suggested that the meteoroid flux in space was much higher than the number based on telescope observations. Such a high flux presented a very serious risk to missions deeper in space, specifically the high-orbiting Apollo capsules. To determine whether the direct measurement was accurate, a number of additional studies followed, including the Pegasus satellite program. These showed that the rate of meteors passing into the atmosphere, or flux, was in line with the optical measurements, at around 10,000 to 20,000 tons per year.\n\nWhipple's work pre-dated the space race and it proved useful when space exploration started only a few years later. His studies had demonstrated that the chance of being hit by a meteoroid large enough to destroy a spacecraft was extremely remote. However, a spacecraft would be almost constantly struck by micrometeorites, about the size of dust grains.\n\nWhipple had already developed a solution to this problem in 1946. Originally known as a \"meteor bumper\" and now termed the Whipple shield, this consists of a thin foil film held a short distance away from the spacecraft's body. When a micrometeoroid strikes the foil, it vaporizes into a plasma that quickly spreads. By the time this plasma crosses the gap between the shield and the spacecraft, it is so diffused that it is unable to penetrate the structural material below. The shield allows a spacecraft body to be built to just the thickness needed for structural integrity, while the foil adds little additional weight. Such a spacecraft is lighter than one with panels designed to stop the meteoroids directly.\n\nFor spacecraft that spend the majority of their time in orbit, some variety of the Whipple shield has been almost universal for decades. Later research showed that ceramic fibre woven shields offer better protection to hypervelocity (~7 km/s) particles than aluminium shields of equal weight. Another modern design uses multi-layer flexible fabric, as in NASA's design for its never-flown TransHab expandable space habitation module,\nand the Bigelow Expandable Activity Module, which was launched in April 2016 and attached to the ISS for two years of orbital testing.\n\nTo avoid artificial space debris, many—but not all—research satellites are launched on elliptical orbits with perigees inside Earth's atmosphere so they will destroy themselves. Willy Ley predicted in 1960 that \"In time, a number of such accidentally too-lucky shots will accumulate in space and will have to be removed when the era of manned space flight arrives\". After the launch of Sputnik 1 in 1957, the North American Aerospace Defense Command (NORAD) began compiling a database (the Space Object Catalog) of all known rocket launches and objects reaching orbit: satellites, protective shields and upper- and lower-stage booster rockets. NASA published modified versions of the database in two-line element set, and during the early 1980s the CelesTrak bulletin board system re-published them.\nThe trackers who fed the database were aware of other objects in orbit, many of which were the result of in-orbit explosions. Some were deliberately caused during 1960s anti-satellite weapon (ASAT) testing, and others were the result of rocket stages blowing up in orbit as leftover propellant expanded and ruptured their tanks. To improve tracking, NORAD employee John Gabbard kept a separate database. Studying the explosions, Gabbard developed a technique for predicting the orbital paths of their products, and Gabbard diagrams (or plots) are now widely used. These studies were used to improve the modelling of orbital evolution and decay.\n\nWhen the NORAD database became publicly available during the 1970s, NASA scientist Donald J. Kessler applied the technique developed for the asteroid-belt study to the database of known objects. In 1978 Kessler and Burton Cour-Palais co-authored \"Collision Frequency of Artificial Satellites: The Creation of a Debris Belt\", demonstrating that the process controlling asteroid evolution would cause a similar collision process in LEO in decades rather than billions of years. They concluded that by about 2000, space debris would outpace micrometeoroids as the primary ablative risk to orbiting spacecraft.\n\nAt the time, it was widely thought that drag from the upper atmosphere would de-orbit debris faster than it was created. However, Gabbard was aware that the number and type of objects in space were under-represented in the NORAD data and was familiar with its behaviour. In an interview shortly after the publication of Kessler's paper, Gabbard coined the term \"Kessler syndrome\" to refer to the accumulation of debris; it became widely used after its appearance in a 1982 \"Popular Science\" article, which won the Aviation-Space Writers Association 1982 National Journalism Award.\n\nThe lack of hard data about space debris prompted a series of studies to better characterize the LEO environment. In October 1979, NASA provided Kessler with funding for further studies. Several approaches were used by these studies.\n\nOptical telescopes or short-wavelength radar was used to measure the number and size of space objects, and these measurements demonstrated that the published population count was at least 50% too low. Before this, it was believed that the NORAD database accounted for the majority of large objects in orbit. Some objects (typically, U.S. military spacecraft) were found to be omitted from the NORAD list, and others were not included because they were considered unimportant. The list could not easily account for objects under in size—in particular, debris from exploding rocket stages and several 1960s anti-satellite tests.\n\nReturned spacecraft were microscopically examined for small impacts, and sections of Skylab and the Apollo Command/Service Module which were recovered were found to be pitted. Each study indicated that the debris flux was higher than expected and debris was the primary source of collisions in space. LEO already demonstrated the Kessler syndrome.\n\nIn 1978 Kessler found that 42 percent of cataloged debris was the result of 19 events, primarily explosions of spent rocket stages (especially U.S. Delta rockets). He discovered this by first identifying those launches that were described having a large number of objects associated with a payload, then researching the literature to determine the rockets used in the launch. In 1979, this finding resulted in establishment of the NASA Orbital Debris Program after a briefing to NASA senior management, overturning the previously held belief that most unknown debris was from old ASAT tests, but from US upper stage rocket explosions and could be easily managed by depleting the unused fuel following the payload injection the upper stage Delta rocket. Beginning in 1986, when it was discovered that other international agencies were possibly experiencing the same type of problem, NASA expanded its program to include international agencies, the first being the European Space Agency. A number of other Delta components in orbit (Delta was a workhorse of the U.S. space program) had not yet exploded.\n\nDuring the 1980s, the U.S. Air Force conducted an experimental program to determine what would happen if debris collided with satellites or other debris. The study demonstrated that the process differed from micrometeoroid collisions, with large chunks of debris created which would become collision threats.\n\nIn 1991, Kessler published \"Collisional cascading: The limits of population growth in low Earth orbit\" with the best data then available. Citing the USAF conclusions about creation of debris, he wrote that although almost all debris objects (such as paint flecks) were lightweight, most of its mass was in debris about or heavier. This mass could destroy a spacecraft on impact, creating more debris in the critical-mass area. According to the National Academy of Sciences:\n\nA 1-kg object impacting at 10 km/s, for example, is probably capable of catastrophically breaking up a 1,000-kg spacecraft if it strikes a high-density element in the spacecraft. In such a breakup, numerous fragments larger than 1 kg would be created.\n\nKessler's analysis divided the problem into three parts. With a low-enough density, the addition of debris by impacts is slower than their decay rate and the problem is not significant. Beyond that is a critical density, where additional debris leads to additional collisions. At densities beyond this critical mass production exceeds decay, leading to a cascading chain reaction reducing the orbiting population to small objects (several cm in size) and increasing the hazard of space activity. This chain reaction is known as the Kessler syndrome.\n\nIn an early 2009 historical overview, Kessler summed up the situation:\n\nAggressive space activities without adequate safeguards could significantly shorten the time between collisions and produce an intolerable hazard to future spacecraft. Some of the most environmentally dangerous activities in space include large constellations such as those initially proposed by the Strategic Defense Initiative in the mid-1980s, large structures such as those considered in the late-1970s for building solar power stations in Earth orbit, and anti-satellite warfare using systems tested by the USSR, the U.S., and China over the past 30 years. Such aggressive activities could set up a situation where a single satellite failure could lead to cascading failures of many satellites in a period much shorter than years.\n\nDuring the 1980s, NASA and other U.S. groups attempted to limit the growth of debris. One effective solution was implemented by McDonnell Douglas on the Delta booster, by having the booster move away from its payload and vent any propellant remaining in its tanks. This eliminated the pressure buildup in the tanks which caused them to explode in the past. Other countries were slower to adopt this measure and, due especially to a number of launches by the Soviet Union, the problem grew throughout the decade.\n\nA new battery of studies followed as NASA, NORAD and others attempted to better understand the orbital environment, with each adjusting the number of pieces of debris in the critical-mass zone upward. Although in 1981 (when Schefter's article was published) the number of objects was estimated at 5,000, new detectors in the Ground-based Electro-Optical Deep Space Surveillance system found new objects. By the late 1990s, it was thought that most of the 28,000 launched objects had already decayed and about 8,500 remained in orbit. By 2005 this was adjusted upward to 13,000 objects, and a 2006 study increased the number to 19,000 as a result of an ASAT test and a satellite collision. In 2011, NASA said that 22,000 objects were being tracked.\n\nThe growth in the number of objects as a result of the late-1990s studies sparked debate in the space community on the nature of the problem and the earlier dire warnings. According to Kessler's 1991 derivation and 2001 updates, the LEO environment in the altitude range should be cascading. However, only one major incident has occurred: the 2009 satellite collision between Iridium 33 and Cosmos 2251. The lack of obvious short-term cascading has led to speculation that the original estimates overstated the problem. According to Kessler a cascade would not be obvious until it was well advanced, which might take years.\n\nA 2006 NASA model suggested that if no new launches took place the environment would retain the then-known population until about 2055, when it would increase on its own. Richard Crowther of Britain's Defence Evaluation and Research Agency said in 2002 that he believed the cascade would begin about 2015. The National Academy of Sciences, summarizing the professional view, noted widespread agreement that two bands of LEO space—900 to and —were already past critical density.\n\nIn the 2009 European Air and Space Conference, University of Southampton researcher Hugh Lewis predicted that the threat from space debris would rise 50 percent in the next decade and quadruple in the next 50 years. , more than 13,000 close calls were tracked weekly.\n\nA 2011 report by the U.S. National Research Council warned NASA that the amount of orbiting space debris was at a critical level. According to some computer models, the amount of space debris \"has reached a tipping point, with enough currently in orbit to continually collide and create even more debris, raising the risk of spacecraft failures\". The report called for international regulations limiting debris and research of disposal methods.\n\nThe plot of episode 4 (\"Conflict\") of Gerry Anderson's 1970 TV series \"UFO\" includes routine missions for the disposal of spent satellites by bombing.\n\n\"Salvage 1\" (1979 TV series) deals humorously with a scrap dealer who establish a space junk salvage company.\n\n\"Planetes\" is a manga (1999-2004) and anime series (2003-2004) that gives focus on a team which is responsible for the collection and disposal of space debris. The DVDs for the TV series include interviews with NASA's Orbital Debris Program Office.\n\nIn 2009, Rhett & Link wrote a song called \"Space Junk\" and made an accompanying music video for the TV series \"Brink\". The lyrics refer to two men tasked to clean up debris such as satellites and expended rockets.\n\n\"Gravity\" is a 2013 survival film, directed by Alfonso Cuaron, about a disaster on a space mission caused by Kessler syndrome.\n\n\n\n"}
{"id": "26692723", "url": "https://en.wikipedia.org/wiki?curid=26692723", "title": "Study of global communication", "text": "Study of global communication\n\nGlobal communication is the term used to describe ways to connect, share, relate and mobilize across geographic, political, economic, social and cultural divides. It redefines soft and hard power as well as information power and diplomacy in ways not considered by traditional theories of international relations. \n\nGlobal Communication implies a transfer of knowledge and ideas from centers of power to peripheries and the imposition of a new intercultural hegemony by means of the \"soft power\" of global news and entertainment.\n\nThe study of global communication is an interdisciplinary field that studies the continuous flows of information used in transferring values, opinions, knowledge, culture across boundaries. \n\nWith the end of the twentieth century and the turn of a new millennium, the global arena and the field of international communication were undergoing significant changes. Some authors started to use the term global communication because it goes beyond the bounds of individual states and emphasizes communication between and among peoples across borders and, importantly, the rise of transnational media corporations.\n\nInternational communication traditionally refers to communication between and among nation-states and connotes issues of national sovereignty, control of national information resources, and the supremacy of national governments.\n\nNevertheless, earlier International communication theories have failed to develop models or research agendas that match the reality of the contemporary role of global communication . The old theories only explain part of the global picture and the theories of modernization, dependency, and cultural imperialism have failed to satisfactorily explain global communication.\n\nThe term \"global\", implies a declining role of the state and state sovereignty. As a term, \"international\" has within it notions of bilateral or multilateral decisions. \"Global\" could be seen as an aspiration, also as a fear, of the weakening of the state. In addition, global may imply something more pervasive, more geographically inclusive than international. \n\nThe study of global communication increased dramatically after World War II due to military considerations coupled with their economic and political implications. Earlier attempts at theorizing have failed to develop models or research agendas that match the reality of the contemporary role of global communication .\n\nMore global communication research was written in the decade from 1945–1955; most of the research of the 1950s dealt with propaganda and the cold war. By 1970, global communication research had grown to include a great variety of subjects, especially comparative mass communication systems, communication and national development and propaganda and public opinion.\n\nFrom the point of view of global communication scholars, previous theories of modernization, dependency, and cultural imperialism have failed to satisfactorily explain global communication. The old theories only explain part of the global picture.\n\nThe emergence of global communication technologies may be considered the origin of the field of global communication in the nineteenth century. Numerous technical advances such as the creation of a new major global communication phenomenon, convergence, digital environments and the internet are some of the major engines driving the change from international communication to global communication.\n\nWith the collapse of the Soviet Union, the shadow of Cold War has lifted to reveal shifting political, economic, and cultural alliances and conflicts. The increasing importance of these currents, especially in the cultural sphere, demands a reconsideration of the nature of the international communication field within the rubric of international relations.\n\nThree key players are usually recognized as the founders of the international news agencies. In 1835, Charles-Louis Havas created the world's first news agency; In 1849, Bernhard Wolff started publishing stock market news and daily reports from Paris, London, Amsterdam, and Frankfurt; In 1849, Paul Julius Freiherr von Reuter established his own commercial service, the Reuter agency, and organized a worldwide exchange of news in 1870.\n\nIn 1859, Reuter, Havas and the German Wolff agency reached an agreement to exchange news from all over the world, which was known as the League of Allied Agencies, or the \" Ring Combination\". In 1848, American News Agency Associated Press was founded and was formally admitted into the \"Ring Combination\" in 1887.\n\nThere are some major factors that point to the growing importance of global communication in the world of the twenty-first century:\n\n\n\nTranscultural Political Economy is a concept that is presented in Global Communications by Paula Chakravartty and Yeuzhi Zhao. This concept looks at global communications and media studies in three major areas: global flows of information and culture, decentralizing the conceptual parameters of global information and media studies, and the normative debates in global communications in the context of neoliberalism. Transcultural Political Economy is a multidisciplinary study that focuses on the tensions between political economy and cultural studies. It \"integrate[s] institutional and cultural analyzes and address urgent questions in global communications in the context of economic integration, empire formation, and the tensions associated with adapting new privatized technologies, neoliberalized and globalized institutional structures, and hybrid cultural forms and practices\". Transcultural Political Economy addresses the issues surrounding the practice of neoliberalism and its creation of unequal power structures within the world system. \nGlobalization theory was popularized in the 1990s as a model for understanding global communication. The concept of globalization inspired a number of theories from various schools of thought in communication studies that each emphasize different aspects of globalization. Many globalization theories highlight actors in the business sector as leaders in the processes of global integration. Transnationalizing business is often celebrated as progression toward a more interconnected world. Globalization theories are often associated with theories of modernity. Some scholars view globalization as the social, political, economic, and cultural integration of societies into a capitalist system; Others see globalization as a successor to modernity, while some see it as an iteration of imperialism. Some question the usefulness and legitimacy of globalization theory, arguing that it does not adequately conceptualize current international relations or function as a lens through which to examine everyday events. Many scholars criticize globalization theories as overzealous toward and unrealistic about the extent of global integration. Some scholars criticize social theorists for offering opinions and predictions based on theory with little practical evidence. In contrast, some scholars work to dispute the pessimistic views of globalization theory.\n\nWorld-system theory is a macro-sociological perspective that seeks to explain the dynamics of the \"capitalist world economy\" as a \"total social system\". A world-system is what Wallerstein terms a \"world-economy\", integrated through the market rather than a political centre, in which two or more regions are interdependent with respect to necessities like food, fuel, and two or more polities compete for domination without the emergence of one single centre forever. World-system theory was first articulated by Immanuel Wallerstein. There are three major sources of the world-system theory which conceived by Wallerstein: the Annales school's general methodology, Marx's focus on accumulation process and competitive class struggles and so on, and dependence theory's neo-Marxist explanation of development processes.\n\nReferring to the transnational division of labor, world-system divides the world into core countries, peripheral countries, semi-peripheral countries and external areas. The core countries usually developed a strong central governments, extensive bureaucracies and large mercenary armies, which permit the local bourgeoisie to obtain control over international commerce and extract capital surpluses from the trade for benefits. The peripheral countries often lack strong central governments or been controlled by core countries, they export raw materials and rely on coercive labor practices. Semi-peripheries which served as buffers between the core and the peripheries. They retain limited but declining access to international banking and the production of high-cost high-quality manufactured goods.[3] External areas such as Russia maintain their own economic systems, they want to remain outside the modern world economy.\n\nThe theory of modernization was developed by Daniel Lerner (1958) in the \"Passing of traditional society.\" Lerner's description of \"modernised\" is an individual having the ability to be empathetic and being able to see oneself in another person's situation. This concept has come from the transition of traditional societies to modern societies, where modern societies is distinctively industrial, urban, literate, and participant. This theory looks at development in a linear fashion, concluding that nations need to develop into a modern society to make it a sustainable and flourishing nation. Developing modernized societies include technological advancement, and developing media sectors to help create a participatory culture. \n\"See also:\" Post-structuralism, imperialism, modernity,\n\nPost-colonialism is a theoretical approach to looking at literature that examines the colonizer-colonized experience. It deals with the adaptation of formerly colonized nations and their development in cultural, political, economical aspects. Some Notable theoreticians include: Frantz Fanon, Edward Said, Gayatri Spivak, R Siva Kumar, Dipesh Chakrabarty, Derek Gregory.\n\nCultural imperialism is a mighty civilization exerts culture influence over another. Less economically prominent cultures often import culture from Western countries, which have the economic means to produce a majority of the world's cultural media, mostly via the global transmission of media. The weak civilization adopts the mighty civilization's customs, philosophies, worldviews and general ways of life. The theoretical foundations of the academic study of cultural imperialism mostly come from Michel Foucault's concept of biopower, governmentality and Edward Saïd's concept of Post-colonialism, which theories see cultural imperialism as the cultural legacy of colonialism or forms of Western hegemony. Media effect study which integrated with political-economy traditional is the core argument of cultural imperialism. There are two opposite effects of media study. The negative one is that Western media imposes socio-political conflicts to the developing country and the latter one's resistance to the media effects to preserve their traditional cultural identities. The positive effects are the issues of the process of civilization such as women's right or racial equality with exposing to Western media. Now the term of cultural imperialism usually refers to America's global culture expansion to the rest of world, which include brand name products, video media, fast food and so on.\n\nCommunication for Development (C4D) is a praxis oriented aspect of global communication studies that approaches global development with a focus on action and participation for social change enacted through communication systems. C4D underlines \"voice, citizenship and collective action\" as central values that promote citizen-led development where the visiting party provides guidance rather than direction within the host community. C4D often incorporates bottom-up theories of social change with the aim to create sustainable change which is believed to be more likely to occur if the efforts are planned, implemented, and sustained by community members themselves. Some development workers and academics suggest that a shared definition of communication for development should be clarified, because disagreement within the field can detract from the characteristics that most scholars view as central to current development, including participatory action research (PAR). Many C4D projects revolve around media systems as a central site for social change, which differentiates C4D from other approaches to development. Theories behind C4D highlight that development projects should be contextually situated and that communication technology will affect different types of social change accordingly.\n\nGlobal media studies is a field of media study in a global scope. Media study deals with the content, history and effects of media. Media study often draws on theories and methods from the disciplines of cultural studies, rhetoric, philosophy, communication studies, feminist theory, political economy and sociology. Among these study approaches, political economic analysis is non-ignorable in understanding the current media and communication developments. But the political economic research has become more resilient because of stronger empirical studies, and the potential connections to policy-making and alternative praxis.\n\nEach country has its own distinct media ecosystem. The media of mainland China is state-run, so the political subjects are under the strict regulations set by the government while other areas such as sports, finance, and increasingly lucrative entertainment industry face less regulation from government. Canada has a well-developed media sector, but the mass media is threatened by the direct outcome of American economic and cultural imperialism which hinder the form of Canada's media identity. Many of the media in America are controlled by large for-profit corporations who reap revenues from advertisings, subscriptions and the sale of copyrighted materials. Currently, six corporations (Comcast, The Walt Disney Company, News Corporation, Time Warner, Viacom and CBS Corporation) have controlled roughly 90% of the America media. Such successes come from the policies of the federal government or the tendency to natural monopolies in the industry.\n\nImmanuel Wallerstein's world system theory develops a basic framework to understand global power shifts in the rise of the modern world. Wallerstein proposes four different categories: core, semi-periphery, periphery, and external, in terms of different region's relative position in the world system. The core regions are the ones that benefited the most from the capitalist world economy, such as England and France. The peripheral areas relied on and exported raw materials to the core, such as Eastern Europe and Latin America. The semi-peripheries are either core regions in decline or peripheries attempting to improve their relative position in the world system, such as Portugal and Spain. The external areas managed to remain outside the modern world economy, such as Russia.\n\nThere are two basic types of global power shifts in the 21st century. One is traditional power transition amongst states, which follows Wallerstein's world system theory. For instance, the global power shifts from the West to the East since the rise of Asia. The other is power diffusion, the way that power move from states to non-states actors. For instance, \"climate change, drug trade, financial flows, pandemics, all these things that cross borders outsider the control of governments.\"\n\nPublic sphere theory, attributed to Jurgen Habermas, is a theory that in its basic premise conceives of democratic governments as those that can stand criticism that comes from public spheres. Public spheres are places, physical or imagined, where people discuss any kind of topic, particularly topics of a societal or political nature. Global public sphere is, therefore, a public that is made of people from across the globe, who come together to discuss and act on issues that concern them. The concept of global public sphere is linked to the shift of public sphere, from restricted to nation-state, to made of individuals and groups connected across as well as within borders.\n\nSince Plato, it can be argued that philosophers have been thinking about versions of a common space for all people to debate in; however, a global public sphere that can fit the description above began to appear much later. In the second half of the 20th century, the legacy of World War II and technological advancements created a new sense of the global and started the economic and political phenomena that we now call globalization. This includes the expansion of humankind into space, which gave individuals the sense of a global unity, the growth of satellite technology, which allowed for people across the globe to view the same television channels, and the internet, which can provide access to an unprecedented amount of information and spaces to connect with other people.\n\nThe term \"culture industry\" appeared in the post-war period. At that time, culture and industry were argued to be opposites. Cultural industries\" are also referred to as the \"Creative industries.\n\nIn the present day, there remain different interpretations of culture as an industry. For some, cultural industries are simply those industries that produce cultural goods and services.\n\nIn the United Nations Educational, Scientific and Cultural Organization (UNESCO), the cultural industries are regarded as those industries that \"combine the creation, production and commercialization of contents which are intangible and cultural in nature. These contents are typically protected by copyright and they can take the form of goods or services\". According to UNESCO, an essential part of cultural industries is that they are \"central in promoting and maintaining cultural diversity and in ensuring democratic access to culture\". \"Cultural industries\" combines the cultural and economic, which gives the cultural industries a distinctive profile.\n\nIn France, the \"cultural industries\" have recently been defined as a set of economic activities that combine the functions of conception, creation and production of culture with more industrial functions in the large-scale manufacture and commercialization cultural products.\n\nIn Canada, economic activities involved in the preservation of heritage are also included in the definition of culture.\n\nSince the rise of the cultural industries has occurred simultaneously with economic globalization, cultural industries have close connections with globalization and global communication.\n\nHerbert Schiller argued that the 'entertainment, communications and information (ECI) complex were having a direct impact on culture and human consciousness. As Schiller argued, the result of transnational corporate expansion is the perpetuation of cultural imperialism, defined as \"the sum of the processes by which a society is brought into the modern world system and how its dominating stratum is attracted, pressured, forced, and sometimes bribed into shaping social institutions to correspond to, or even promote, the values and structures of the dominating centre of the system\".\n\nThe second wave of transnational corporate expansion, which began in the 1970s with the emergence of Export Processing Zones in developing countries, is focused on the development of global production networks. This process was described as a \"new international division of labour\" (NIDL) by the German political economists Frӧbel et al. (1980).\n\nErnst and Kim have argued that GPNs are changing the nature of the multinational corporation itself, from \"stand alone overseas investment projects, to \"global network flagships\" that integrate their dispersed supply, knowledge and customer bases into global and regional production networks\", entailing a shift from top-down hierarchical models of corporate control to increasingly networked and collective forms of organization.\n\nThe largest firms in media and media-related industries have a very high international profile. Global media empires such as Disney, News Corporation, Time-Warner and Viacom-CBS now derive 25-45 per cent of their revenues outside of the United States.\n\nIt is often argued that the global media are dominated by a small number of powerful media conglomerates. Herman and McChesney (1997) argued that the global media were \"dominated by three or four dozen large transnational corporations (TNCs) with fewer than ten mostly US-based media conglomerates towering over the global market.\" Similarly, Manfred Steger has observed that \" to a very large extent, the global cultural flows of our time are generated and directed by global media empires that rely on powerful communication technologies to spread their message.\" He also argued that during the last two decades, a few very large TNCs would come to dominate the global market for entertainment, news, television, and film.\n\nDiaspora is often confused with exodus. Diasporas are minority groups that have a sense of connection with a larger community outside of the borders they currently inhabit, and through diasporic media create a sense of a larger identity and community, whether imagined or real. In scholarly work about diaspora in communication studies, the view of nation and culture as interchangeable terms is no longer prevalent. Stuart Hall theorized of hybridity, which he distinguished from \"old style pluralism\", \"nomadic voyaging of the postmodern\", and \"global homogenization\". Hybridity is the retention of an original identity and strong ties to an original country and tradition, but with the understanding that there is no unchanged, ideal nation of the past that they can return to. To be hybrid is to also adapt to a new culture and tradition without simply assimilating in it, but rather negotiating a place between the \"original\" and \"new\" cultures. In Communication studies, diaspora is discussed as the identity that unifies people across time and space, sometimes existing in physical spaces and other times existing in imagined 'non-spaces'. However, it has been argued that the concept of 'diaspora' implies ethnic homogeneity and essentializes identity to only ethnicity. One of the most cited and well-known works in the field of diasporic media is Hamid Naficy's work on exiled Iranian Americans' creation of cable television in the United States.\n\nDiasporic media refer to media that address the needs of particular ethnic, religious, and/or linguistic groups that live in multicultural settings . Diasporic media can be in the diaspora's traditional language or in another language, and they can include news or media from the \"origin\" country or they can contain the diaspora's local news or media. Diasporic media can be created in radio, television, film, music, in newspapers, magazines, and other publishing, as well as online. It can be argued that the development and spread of satellite television is an instrumental element of the growth of diasporic media today. Satellite television allowed migrants to access the news and popular culture from their homeland, as well as allowing people who speak the same language to access the same channels that might be produced outside of the \"homeland\"\n\nContemporary studies of diaspora show that diasporic media are part of the change in the tendency Immanuel Wallerstein described in his world systems theory. The world systems theory postulates that much of the flow of people in the world has been from the 'periphery', or economically-developing states, towards the centre; which are often metropolitan, economically-wealthy states that grew their wealth in colonialist entrepreneurship. However, contrary to the movement of people, the flow of information (including media products), has tended to be from the centre to the periphery.\n\nThe advancement of media and technology have played the pivotal role in process of globalization and global communication. Cable television, ISDN, digitalization, direct broadcast satellites as well as the Internet have created a situation where vast amounts of information can be transferred around the globe in a matter of seconds.\n\nDuring the early 20th century, telegraph, telephony, and radio started the process of global communication. As media technologies developed intensely, they were thought to create, in Marshall McLuhan’ s famous words, a ‘‘global village.’’ The launch of Sputnik, the world’ s first artificial satellite, on October 4, 1957, marked the beginning of technologies that would further interconnect the world. The first live global television broadcast occurred when Neil Armstrong stepped onto the moon in July 1969. In November 1972, pay TV caused expansion of cable when Service Electric offered Home Box Office over its cable system. By 2000, over direct broadcast satellite, a household could receive channels from all over the world. Now with the World Wide Web, smart phones, tablet devices, smart televisions and other digital media devices, billions of people are now able to access media content that was once tied to particular communications media (print, broadcast) or platforms (newspapers, magazines, radio, television, cinema).\n\nJustice in communication studies includes, but is not limited to, the concern with democratic process and fostering democratic publics . Jurgen Habermas theorized of public sphere (in \"The Structural Transformation of the Public Sphere\") as the space that is created whenever matters of common concern are discussed between the state and civil society. Thus, public sphere includes not only the media, but also public protest in the form of marches, demonstrations, et cetera. There are, however, critiques of political economy in whose view it is impossible to work within the current system to produce democratic publics. Such a critique is that produced by Karl Marx, who saw institutions such as parliament, the state, the 'acceptable' public sphere, economic enterprises, and so on as structurally produced and perpetuated by a capitalist system, and thus they can not be mobilized to change it. In such a system, there can only be illusory justice, which is fair only within the logic of the system. This illusion of justice is produced through dominating ideology.\n\nAnother issue of justice in communication studies is the question of decolonizing research methods and theoretical discourse . The idea of decolonizing research comes from a rejection of the functionalist approach, which assumed that research can be conducted in a vacuum, free of ideology or the researcher's biases. This approach assumed cultures to be unchanging, homogenous, and isolated from each other. The purpose of decolonizing research and discourse is to 'uncloak' research as an unbiased power structure, and produce research that is more self-aware. The approach in decolonizing research methods attempts to create methodologies that treat the people in the study as participants or partners, rather than subjects - which is a term that in itself carries strong connotations of colonialism. Decolonizing research also involves moving away from Eurocentric models that are assumed to work anywhere else, and instead to create work that is more useful in local contexts. Decolonial approaches specifically seek to produce knowledge about the mechanisms and effects of colonialism. These approaches allow former subjects to 'talk back', which is a reflection of independent agency, on the colonizer's own terms of research, rather than to be 'given' a voice, which is an unequal power structure.\n\n"}
{"id": "2268267", "url": "https://en.wikipedia.org/wiki?curid=2268267", "title": "The Filipino Channel", "text": "The Filipino Channel\n\nThe Filipino Channel, commonly known as TFC, is a global subscription television network based in Daly City, California with studio in Redwood City, California and offices in Australia, Canada, Cayman Islands, UAE, Hungary, Japan, Middle East, the Netherlands, and the United Kingdom. It is owned and operated by the Filipino media conglomerate ABS-CBN. Its programming is composed primarily of imported programs from the ABS-CBN television network. \n\nTFC is available globally on direct-to-home satellite, cable, IPTV, online streaming, video on demand and on other over-the-top content platforms. TFC is targeted to the Filipino diaspora, and was launched on September 24, 1994, becoming the world's first trans-Pacific Asian broadcaster.\n\nAs of 2014, The Filipino Channel has over three million subscribers worldwide, most of whom are in the United States, Middle East, Europe, Australia, Canada, and Japan.\n\nOn September 24, 1994, then ABS-CBN Broadcasting Corporation (now ABS-CBN Corporation) through its newly established subsidiary ABS-CBN International signed a historic deal with the PanAmSat to bring the first trans-Pacific Asian programming service to some two million Filipino immigrants in the United States using the then-newly launched PAS 2 satellite.\n\nThe first headquarters of TFC was built in a garage in Daly City, California with only eight employees doing all the tasks from managing the phones, the computers, and the like. By 1995, TFC has grown to 25,000 subscribers in the United States. Airings of Mara Clara and other programming aired during the daytime and night in 1997 Esperanza and Mula Sa Puso where the biggest programs on TFC \n\nIn 1996, TFC Direct! was launched, an independently operated direct-to-home television service that incorporates the TV channels Sarimanok News Network (now ABS-CBN News Channel), Pinoy Blockbuster Channel (now Cinema One), Pinoy Central (later renamed as Kapamilya Channel, then it was replaced by Bro, and now it was replaced again by ABS-CBN Sports + Action (S+A)), and the radio channels DZMM Radyo Patrol 630 and WRR 101.9 For Life! (now MOR 101.9). \n\nBy 2004, TFC has grown to 250,000 subscribers in the United States. This growth led to the expansion of TFC to other territories in the world.\n\nIn 2005, ABS-CBN International signed an affiliation agreement with DirecTV, one of the leading DTH providers in the United States. Under the deal, DirecTV has the exclusive right to distribute the TFC package on its DTH platform. In return, DirecTV will pay license fees to ABS-CBN and to ABS-CBN International. Later that year, the now defunct and award-winning internet television service TFC Now! was launched. This was later replaced by TFC.tv video streaming website. In this year, ABS-CBN International acquired San Francisco International Gateway from Loral Space & Communications. SFIG is a telecommunications port company based in Richmond, California. SFIG provides satellite communications services through its 2.5 acre (1 hectare) facility consisting of 19 satellite dish antennas and 9 modular equipment buildings. ABS-CBN International received Federal Communications Commission licensing approval in April 2006. Also in this year, ABS-CBN International opened its state-of-the-art studio and office in Redwood City, California. In 2006, SFIG successfully handled the pay per view distribution to In Demand and DirecTV for the Manny Pacquiao vs. Oscar Larios super featherweight championship title fight. SFIG's customers include Discovery Communications, CBS, ESPN, Playboy among others. SFIG is a member of the World Teleport Association.\n\nIn 2007, ABS-CBN International launched Myx (now Myx TV), the first and only television channel in the United States that is targeted to the Asian-American youth audience. As of 2011, TFC has over 2.47 million subscribers worldwide. As of 2015, The Filipino Channel has over three million subscribers worldwide most of which are in United States, Middle East, Australia, Japan, Europe, and Canada.\n\nThe program line-up of The Filipino Channel is composed primarily of programs from ABS-CBN, a national television network in the Philippines. Programming ranging from news, documentaries, public service, reality shows, soap operas, teleseryes, talk shows, sitcoms, gag shows, live events, and other formats and genres are shown on TFC in a slightly delayed basis and are synchronized automatically depending on the location of the broadcast. TFC also shows original programs not shown in the Philippines and these are produced abroad by ABS-CBN's foreign subsidiaries in United States, Middle East, and Europe.\nIn 2007 the short lived Hero TV on TFC aired in April 2007-August 2007\n\nTFC Video On Demand is an IPTV service distributed around the world under the brand of The Filipino Channel. To avail of the service, users must acquire the TFC IPTV set-top box, connect the box to a TV set (HDMI for HDTVs, composite for SDTVs) and broadband internet through WiFi or Ethernet (requires 2 Mbps minimum speed), and subscribe to monthly subscription. The channel line up will depend on the subscription package availed and may include the live-streaming of TV channels The Filipino Channel, Myx TV, the international feeds of ABS-CBN Sports+Action, ABS-CBN News Channel, Cinema One, Lifestyle, as well as radio channels DZMM and My Only Radio (Manila station), and an in-house interactive Karaoke channel. Aside from the live-streaming of ABS-CBN channels, the service also includes a video on demand feature that allow users to watch, pause, rewind, fast-forward select programs of the channels anytime for a period of one month and in high-definition format (for select programs and internet speed). The service also offers a wide selection of fresh movies from the Philippines and other pay per view contents like concerts and sporting events also in high-definition format.\n\n\n"}
{"id": "3870084", "url": "https://en.wikipedia.org/wiki?curid=3870084", "title": "The Globalization of World Politics", "text": "The Globalization of World Politics\n\nThe Globalization of World Politics: An Introduction to International Relations is a book by John Baylis, Patricia Owens, and Steve Smith.\n\nJohn Baylis, Steve Smith and Patricia Owens: Introduction\nThe historical context\nTheories of world politics\nStructures and processes\nInternational issues\nGlobalization in the future\n"}
{"id": "50429661", "url": "https://en.wikipedia.org/wiki?curid=50429661", "title": "The State of the World's Children", "text": "The State of the World's Children\n\nThe State of the World's Children is an annual report published by the United Nations Children's Emergency Fund (UNICEF). It is the flagship publication of the organization. The first report was published in 1980, having been introduced by James P. Grant (the executive director of UNICEF at the time). Peter Adamson was the author of the report for 15 years. The publication of the 1982–1983 \"The State of the World's Children\" report marked the start of the child survival revolution.\n\nFollowing the end of Grant's tenure at UNICEF and his death in 1995, \"The State of the World's Children\" has received significantly less attention.\n\n\"The State of the World's Children 1982–1983\" launches the child survival revolution and pushed for GOBI (growth monitoring, oral rehydration therapy, breastfeeding, and immunization).\n\nThe 1988 report argues for the need of a \"Grand Alliance\" for children between governments, schools, mass media, etc., to continue the child survival and development revolution.\n\nThe 1991 report features the World Summit for Children, which happened the previous year. The report covers the commitment made by the Summit and serves as a record.\n\nThe 2015 \"The State of the World's Children\" report, titled \"Reimagine the Future\", reviews the work on children's health and rights in the world in the context of the 25th anniversary of the Convention on the Rights of the Child. The report also argued that more innovation is necessary, and highlighted examples including Solar Ear (a solar-rechargeable hearing aid battery charger) and community-based management of acute malnutrition.\n\n"}
{"id": "50293718", "url": "https://en.wikipedia.org/wiki?curid=50293718", "title": "Timeline of deworming", "text": "Timeline of deworming\n\nThis is a timeline of deworming, and specifically mass deworming.\n\n"}
{"id": "50166890", "url": "https://en.wikipedia.org/wiki?curid=50166890", "title": "Timeline of global health", "text": "Timeline of global health\n\nThis page is a timeline of global health, including major conferences, interventions, cures, and crises.\n\nDuring this pre-WWII era, there are three big trends that operate separately, but sometimes affect each other in development and outcomes.\n\nFirst, a trend of urbanization (fueled by the Industrial Revolution) as well as greater global trade and migration leads to new challenges, including those in urban sanitation and infectious diseases/pandemics. Six global cholera pandemics happen in this period because of increased commerce and migration.\n\nSecond, there is a lot of development on the underlying theory of disease, advancements in vaccine and antibiotic development, and a variety of experimental large-scale eradication and control programs. One big example: the germ theory of diseases begins to become accepted and popularized starting around 1850. Another big example is the development of the smallpox vaccine by Edward Jenner in 1796. Systematic eradication and control efforts include the Rockefeller Sanitary Commission and efforts to eradicate smallpox. Antitoxins and vaccines for numerous diseases including cholera and tuberculosis are developed during this period, building on a trend of greater understanding of and control over microorganisms.\n\nA third theme during this era is the formation of various preliminary international alliances and conferences, including the International Sanitary Conferences, Pan American Health Organization, Office International d'Hygiène Publique, and the League of Nations Health Committee. This is closely intertwined with the other two trends. For instance, the cholera pandemics mentioned above, as well as the growing scientific understanding of the germ theory of disease, are both key impetuses for the International Sanitary Conferences.\n\nFollowing the end of World War II, the first batch of big organizations, both international and national (with international cooperation), including the United Nations and World Health Organization (WHO), form. Beginning with the United Nations Relief and Rehabilitation Administration for relief of victims of war in 1943, there is a big push to begin creating large scale health initiatives, non-governmental organizations, and worldwide global health programs by the United Nations to improve quality of life around the world. UNICEF, the World Health Organization, as well as the UNRRA are all part of United Nations efforts to benefit global health beginning with developing countries. These various programs aim to aid in economic endeavors by providing loans, direct disease prevention programs, health education, etc.\n\nAfter wrapping up complications caused by the end of the war, there is an international energy put in into eradication, beginning with the complete smallpox eradication in 1979. There is greater dissatisfaction with WHO for its focus on disease/infection control at the expense of trying to improve general living conditions, as well as disappointment at its low budget and staffing. This atmosphere spurs other organizations to provide their own forms of aid. The Alma Ata Declaration and selective primary healthcare are created to express urgent action by all governments and citizens to protect and promote the health of all people equally. More organizations form following these new active attitudes toward global health, including the International Agency for Research on Cancer and the Doctors Without Borders organization. Publications like the WHO Model List of Essential Medicines highlight basic medicines required by most adults and children to survive, and set priorities for healthcare fund allocation in developing countries. Generally, there is more buy-in for the idea that direct, targeted efforts to address healthcare could be worthwhile and benefit many countries.\n\nCertain specific efforts increase in efficiency and productivity, including improvement in maternal and child health and a focus on HIV/AIDS, tuberculosis, and malaria (the 'Big Three') in developing countries. During this time period, the child survival revolution (CSR), which helps reduce child mortality in the developing world, and GOBI-FFF are both advocated by James P. Grant. The World Summit for Children also takes place, becoming one of the largest ever gathering of heads of states and government to commit a set of goals to improve the well-being of children. Finally, HIV/AIDS becomes the focus of many governmental and non-governmental organizations, leading to the formation of the Global Programme on AIDS (GPA) by efforts of the World Health Organization. However, these health organizations also make significant advancements to tuberculosis treatments, including the DOTS strategy and the formation of the Stop TB Partnership.\n\nUN's Millennium Development Goals establishes health care as an important goal (not just combating infectious diseases). Later in 2015, the Sustainable Development Goals build on the MDGs to outline the objectives that will transform our world by ending poverty, helping the environment, and improving health and education. More specific disease-targeting organizations are created primarily to fund healthcare plans in developing countries, including the President's Emergency Plan for AIDS Relief and The Global Fund to Fight AIDS, Tuberculosis and Malaria. These organizations (especially the WHO) adopt new strategies and initiatives, including the 3 by 5 Initiative to widen the access to antiretroviral treatment, the WHO Framework Convention on Tobacco Control, etc. Private large donors such as the Bill & Melinda Gates Foundation begin to play an important role in shaping the funding landscape and direction of efforts in global health.\n\nThe following events are selected for inclusion in the timeline:\n\n\nWe do \"not\" include:\n\n\n\n\n\n"}
{"id": "51099085", "url": "https://en.wikipedia.org/wiki?curid=51099085", "title": "To Save Humanity", "text": "To Save Humanity\n\nTo Save Humanity is a 2015 collection of 96 essays on global health from a collection of authors who range from heads of states, movie stars, scientists at leading universities, activists, and Nobel Prize winners. Each contributor was asked the same question: \"What is the single most important thing for the future of global health over the next fifty years?\" The collection was edited by Julio Frenk and Steven J. Hoffman.\n\nThe Global Strategy Lab called the collection \"unparalleled\" and \"a primer on the major issues of our time and a blueprint for post-2015 health and development,\" and featured it in their annual conference.\n\nThe Health Impact Fund also featured the collection at their conference.\n\nThe Lancet described the book as \"testimony to the complexity of global health politics,\" and called it \"a reminder that the breadth of individual and institutional engagement with global health cannot be fully captured by one set of global goals.\"\n\nVox has republished several of the articles for free online as part of a series entitled \"One Change to Save the World.\"\n"}
{"id": "15380061", "url": "https://en.wikipedia.org/wiki?curid=15380061", "title": "Water scarcity", "text": "Water scarcity\n\nWater scarcity is the lack of fresh water resources to meet water demand. It affects every continent and was listed in 2015 by the World Economic Forum as the largest global risk in terms of potential impact over the next decade. It is manifested by partial or no satisfaction of expressed demand, economic competition for water quantity or quality, disputes between users, irreversible depletion of groundwater, and negative impacts on the environment. One-third of the global population (2 billion people) live under conditions of severe water scarcity at least 1 month of the year. Half a billion people in the world face severe water scarcity all year round. Half of the world’s largest cities experience water scarcity.\n\nA mere 0.014% of all water on Earth is both fresh and easily accessible. Of the remaining water, 97% is saline and a little less than 3% is hard to access. Technically, there is a sufficient amount of freshwater on a global scale. However, due to unequal distribution (exacerbated by climate change) resulting in some very wet and some very dry geographic locations, plus a sharp rise in global freshwater demand in recent decades driven by industry, humanity is facing a water crisis. Demand is expected to outstrip supply by 40% in 2030, if current trends continue.\n\nThe essence of global water scarcity is the geographic and temporal mismatch between freshwater demand and availability. The increasing world population, improving living standards, changing consumption patterns, and expansion of irrigated agriculture are the main driving forces for the rising global demand for water. Climate change, such as altered weather-patterns (including droughts or floods), deforestation, increased pollution, green house gases, and wasteful use of water can cause insufficient supply. At the global level and on an annual basis, enough freshwater is available to meet such demand, but spatial and temporal variations of water demand and availability are large, leading to (physical) water scarcity in several parts of the world during specific times of the year. All causes of water scarcity are related to human interference with the water cycle. Scarcity varies over time as a result of natural hydrological variability, but varies even more so as a function of prevailing economic policy, planning and management approaches. Scarcity can be expected to intensify with most forms of economic development, but, if correctly identified, many of its causes can be predicted, avoided or mitigated.\n\nSome countries have already proven that decoupling water use from economic growth is possible. For example, in Australia, water consumption declined by 40% between 2001 and 2009 while the economy grew by more than 30%. The International Resource Panel of the UN states that governments have tended to invest heavily in largely inefficient solutions: mega-projects like dams, canals, aqueducts, pipelines and water reservoirs, which are generally neither environmentally sustainable nor economically viable. The most cost-effective way of decoupling water use from economic growth, according to the scientific panel, is for governments to create holistic water management plans that take into account the entire water cycle: from source to distribution, economic use, treatment, recycling, reuse and return to the environment.\n\nThe total amount of easily accessible freshwater on Earth, in the form of surface water (rivers and lakes) or groundwater (in aquifers, for example), is 14.000 cubic kilometres (nearly 3359 cubic miles). Of this total amount, 'just' 5.000 cubic kilometres are being used and reused by humanity. Hence, in theory, there is more than enough freshwater available to meet the demands of the current world population of 7 billion people, and even support population growth to 9 billion or more. Due to the unequal geographical distribution and especially the unequal consumption of water, however, it is a scarce resource in some parts of the world and for some parts of the population.\n\nScarcity as a result of consumption is caused primarily by the extensive use of water in agriculture/livestock breeding and industry. People in developed countries generally use about 10 times more water daily than those in developing countries. A large part of this is \"indirect use\" in water-intensive agricultural and industrial production processes of consumer goods, such as fruit, oil seed crops and cotton. Because many of these production chains have been globalised, a lot of water in developing countries is being used and polluted in order to produce goods destined for consumption in developed countries.\n\nWater scarcity can result from two mechanisms:\n\n\nPhysical water scarcity results from inadequate natural water resources to supply a region's demand, and economic water scarcity results from poor management of the sufficient available water resources. According to the United Nations Development Programme, the latter is found more often to be the cause of countries or regions experiencing water scarcity, as most countries or regions have enough water to meet household, industrial, agricultural, and environmental needs, but lack the means to provide it in an accessible manner.\nAround one fifth of the world's population currently live in regions affected by Physical water scarcity, where there is inadequate water resources to meet a country's or regional demand, including the water needed to fulfill the demand of ecosystems to function effectively. Arid regions frequently suffer from physical water scarcity. It also occurs where water seems abundant but where resources are over-committed, such as when there is over development of hydraulic infrastructure for irrigation. Symptoms of physical water scarcity include environmental degradation and declining groundwater as well as other forms of exploitation or overuse.\n\nEconomic water scarcity is caused by a lack of investment in infrastructure or technology to draw water from rivers, aquifers or other water sources, or insufficient human capacity to satisfy the demand for water. One quarter of the world's population is affected by economic water scarcity. Economic water scarcity includes a lack of infrastructure, causing the people without reliable access to water to have to travel long distances to fetch water, that is often contaminated from rivers for domestic and agricultural uses. Large parts of Africa suffer from economic water scarcity; developing water infrastructure in those areas could therefore help to reduce poverty. Critical conditions often arise for economically poor and politically weak communities living in already dry environment. Consumption increases with GDP per capita in most developed countries the average amount is around 200–300 litres daily. In underdeveloped countries (e.g. African countries such as Mozambique), average daily water consumption per capita was below 10 L. This is against the backdrop of international organisations, which recommend a minimum of 20 L of water (not including the water needed for washing clothes), available at most 1 km from the household. Increased water consumption is correlated with increasing income, as measured by GDP per capita. In countries suffering from water shortages water is the subject of speculation.\n\nThe United Nations Committee on Economic, Social and Cultural Rights established a foundation of five core attributes for water security. They declare that the human right to water entitles everyone to sufficient, safe, acceptable, physically accessible, and affordable water for personal and domestic use.\n\nAt the 2000 Millennium Summit, the United Nations addressed the effects of economic water scarcity by making increased access to safe drinking water an international development goal. During this time, they drafted the Millennium Development Goals and all 189 UN members agreed on eight goals. MDG 7 sets a target for reducing the proportion of the population without sustainable safe drinking water access by half by 2015. This would mean that more than 600 million people would gain access to a safe source of drinking water. In 2016, the Sustainable Development Goals replaced the Millennium Development Goals\n\nWater scarcity has many negative impacts on the environment, including lakes, rivers, wetlands and other fresh water resources. The resulting water overuse that is related to water scarcity, often located in areas of irrigation agriculture, harms the environment in several ways including increased salinity, nutrient pollution, and the loss of floodplains and wetlands. Furthermore, water scarcity makes flow management in the rehabilitation of urban streams problematic.\n\nThrough the last hundred years, more than half of the Earth's wetlands have been destroyed and have disappeared. These wetlands are important not only because they are the habitats of numerous inhabitants such as mammals, birds, fish, amphibians, and invertebrates, but they support the growing of rice and other food crops as well as provide water filtration and protection from storms and flooding. Freshwater lakes such as the Aral Sea in central Asia have also suffered. Once the fourth largest freshwater lake, it has lost more than 58,000 square km of area and vastly increased in salt concentration over the span of three decades.\n\nSubsidence, or the gradual sinking of landforms, is another result of water scarcity. The U.S. Geological Survey estimates that subsidence has affected more than 17,000 square miles in 45 U.S. states, 80 percent of it due to groundwater usage. In some areas east of Houston, Texas the land has dropped by more than nine feet due to subsidence. Brownwood, a subdivision near Baytown, Texas, was abandoned due to frequent flooding caused by subsidence and has since become part of the Baytown Nature Center.\n\nAquifer drawdown or overdrafting and the pumping of fossil water increases the total amount of water within the hydrosphere subject to transpiration and evaporation processes, thereby causing accretion in water vapour and cloud cover, the primary absorbers of infrared radiation in the earth's atmosphere. Adding water to the system has a forcing effect on the whole earth system, an accurate estimate of which hydrogeological fact is yet to be quantified.\n\nApart from the conventional surface water sources of freshwater such as rivers and lakes, other resources of freshwater such as groundwater and glaciers have become more developed sources of freshwater, becoming the main source of clean water. Groundwater is water that has pooled below the surface of the Earth and can provide a usable quantity of water through springs or wells. These areas where groundwater is collected are also known as aquifers. Glaciers provide freshwater in the form meltwater, or freshwater melted from snow or ice, that supply streams or springs as temperatures rise. More and more of these sources are being drawn upon as conventional sources' usability decreases due to factors such as pollution or disappearance due to climate changes. The exponential growth rate of the human population is a main contributing factor in the increasing use of these types of water resources.\n\nUntil recent 2015, groundwater was not a highly utilized resource. In the 1960s, more and more groundwater aquifers developed. Changes in knowledge, technology and funding have allowed for focused development into abstracting water from groundwater resources away from surface water resources. These changes allowed for progress in society such as the \"agricultural groundwater revolution\", expanding the irrigation sector allowing for increased food production and development in rural areas. Groundwater supplies nearly half of all drinking water in the world. The large volumes of water stored underground in most aquifers have a considerable buffer capacity allowing for water to be withdrawn during periods of drought or little rainfall. This is crucial for people that live in regions that cannot depend on precipitation or surface water as a supply alone, instead providing reliable access to water all year round. As of 2010, the world's aggregated groundwater abstraction is estimated at approximately 1,000 kmper year, with 67% used for irrigation, 22% used for domestic purposes and 11% used for industrial purposes. The top ten major consumers of abstracted water (India, China, United States of America, Pakistan, Iran, Bangladesh, Mexico, Saudi Arabia, Indonesia, and Italy) make up 72% of all abstracted water use worldwide. Groundwater has become crucial for the livelihoods and food security of 1.2 to 1.5 billion rural households in the poorer regions of Africa and Asia.\n\nAlthough groundwater sources are quite prevalent, one major area of concern is the renewal rate or recharge rate of some groundwater sources. Abstracting from groundwater sources that are non-renewable could lead to exhaustion if not properly monitored and managed. Another concern of increased groundwater usage is the diminished water quality of the source over time. Reduction of natural outflows, decreasing stored volumes, declining water levels and water degradation are commonly observed in groundwater systems. Groundwater depletion may result in many negative effects such as increased cost of groundwater pumping, induced salinity and other water quality changes, land subsidence, degraded springs and reduced baseflows. Human pollution is also harmful to this important resource.\n\nTo set up a big plant near a water abundant area, bottled water companies need to extract groundwater from a source at a rate more than the replenishment rate leading to the persistent decline in the groundwater levels. The groundwater is taken out, bottled, and then shipped all over the country or world and this water never goes back. When the water table depletes beyond a critical limit, bottling companies just move from that area leaving a grave water scarcity. Groundwater depletion impacts everyone and everything in the area who uses water: farmers, businesses, animals, ecosystems, tourism, and the regular guy getting his water from a well. Millions of gallons of water out of the ground leaves the water table depleted uniformly and not just in that area because the water table is connected across the landmass. Bottling Plants generate water scarcity and impact ecological balance. They lead to water stressed areas which bring in droughts.\n\nGlaciers are noted as a vital water source due to their contribution to stream flow. Rising global temperatures have noticeable effects on the rate at which glaciers melt, causing glaciers in general to shrink worldwide. Although the meltwater from these glaciers are increasing the total water supply for the present, the disappearance of glaciers in the long term will diminish available water resources. Increased meltwater due to rising global temperatures can also have negative effects such as flooding of lakes and dams and catastrophic results.\n\nHydrologists today typically assess water scarcity by looking at the population-water equation. This is done by comparing the amount of total available water resources per year to the population of a country or region. A popular approach to measuring water scarcity has been to rank countries according to the amount of annual water resources available per person. For example, according to the Falkenmark Water Stress Indicator, a country or region is said to experience \"water stress\" when annual water supplies drop below 1,700 cubic metres per person per year. At levels between 1,700 and 1,000 cubic metres per person per year, periodic or limited water shortages can be expected. When water supplies drop below 1,000 cubic metres per person per year, the country faces \"water scarcity\". The United Nations' FAO states that by 2025, 1.9 billion people will live in countries or regions with absolute water scarcity, and two-thirds of the world population could be under stress conditions. The World Bank adds that climate change could profoundly alter future patterns of both water availability and use, thereby increasing levels of water stress and insecurity, both at the global scale and in sectors that depend on water.\n\nOther ways of measuring water scarcity include examining the physical existence of water in nature, comparing nations with lower or higher volumes of water available for use. This method often fails to capture the accessibility of the water resource to the population that may need it. Others have related water availability to population.\n\nAnother measurement, calculated as part of a wider assessment of water management in 2007, aimed to relate water availability to how the resource was actually used. It therefore divided water scarcity into 'physical' and 'economic'. Physical water scarcity is where there is not enough water to meet all demands, including that needed for ecosystems to function effectively. Arid regions frequently suffer from physical water scarcity. It also occurs where water seems abundant but where resources are over-committed, such as when there is overdevelopment of hydraulic infrastructure for irrigation. Symptoms of physical water scarcity include environmental degradation and declining groundwater. Water stress harms living things because every organism needs water to live.\n\nRenewable freshwater supply is a metric often used in conjunction when evaluating water scarcity. This metric is informative because it can describe the total available water resource each country contains. By knowing the total available water source, an idea can be gained about whether a country is prone to experiencing physical water scarcity. This metric has its faults in that it is an average; precipitation delivers water unevenly across the planet each year and annual renewable water resources vary from year to year. This metric also does not describe the accessibility of water to individuals, households, industries, or the government. Lastly, as this metric is a description of a whole country, it does not accurately portray whether a country is experiencing water scarcity. Canada and Brazil both have very high levels of available water supply, but still experience various water related problems.\n\nIt can be observed that tropical countries in Asia and Africa have low availability of freshwater resources.\n\nThe following table displays the average annual renewable freshwater supply by country including both surface-water and groundwater supplies. This table represents data from the UN FAO AQUASTAT, much of which are produced by modeling or estimation as opposed to actual measurements.\n\nThe United Nations (UN) estimates that, of 1.4 billion cubic kilometers (1 quadrillion acre-feet) of water on Earth, just 200,000 cubic kilometers (162.1 billion acre-feet) represent fresh water available for human consumption.\n\nMore than one in every six people in the world is water stressed, meaning that they do not have sufficient access to potable water. Those that are water stressed make up 1.1 billion people in the world and are living in developing countries. According to the Falkenmark Water Stress Indicator, a country or region is said to experience \"water stress\" when annual water supplies drop below 1,700 cubic metres per person per year. At levels between 1,700 and 1,000 cubic meters per person per year, periodic or limited water shortages can be expected. When a country is below 1,000 cubic meters per person per year, the country then faces water scarcity . In 2006, about 700 million people in 43 countries were living below the 1,700 cubic metres per person threshold. Water stress is ever intensifying in regions such as China, India, and Sub-Saharan Africa, which contains the largest number of water stressed countries of any region with almost one fourth of the population living in a water stressed country. The world's most water stressed region is the Middle East with averages of 1,200 cubic metres of water per person. In China, more than 538 million people are living in a water-stressed region. Much of the water stressed population currently live in river basins where the usage of water resources greatly exceed the renewal of the water source.\n\nAnother popular opinion is that the amount of available freshwater is decreasing because of climate change. Climate change has caused receding glaciers, reduced stream and river flow, and shrinking lakes and ponds. Many aquifers have been over-pumped and are not recharging quickly. Although the total fresh water supply is not used up, much has become polluted, salted, unsuitable or otherwise unavailable for drinking, industry and agriculture. To avoid a global water crisis, farmers will have to strive to increase productivity to meet growing demands for food, while industry and cities find ways to use water more efficiently.\n\nA New York Times article, \"Southeast Drought Study Ties Water Shortage to Population, Not Global Warming\", summarizes the findings of Columbia University researcher on the subject of the droughts in the American Southeast between 2005 and 2007. The findings published in the \"Journal of Climate\" say that the water shortages resulted from population size more than rainfall. Census figures show that Georgia’s population rose from 6.48 to 9.54 million between 1990 and 2007. After studying data from weather instruments, computer models, and tree ring measurements, they found that the droughts were not unprecedented and result from normal climate patterns and random weather events. \"Similar droughts unfolded over the last thousand years\", the researchers wrote, \"Regardless of climate change, they added, similar weather patterns can be expected regularly in the future, with similar results.\" As the temperature increases, rainfall in the Southeast will increase but because of evaporation the area may get even drier. The researchers concluded with a statement saying that any rainfall comes from complicated internal processes in the atmosphere and are very hard to predict because of the large amount of variables.\n\nWhen there is not enough potable water for a given population, the threat of a \"water crisis\" is realized.\nThe United Nations and other world organizations consider a variety of regions to have water crises of global concern. Other organizations, such as the Food and Agriculture Organization, argue that there are no water crises in such places, but steps must still be taken to avoid one.\n\nThere are several principal manifestations of the water crisis.\n\nWaterborne diseases caused by lack of sanitation and hygiene are one of the leading causes of death worldwide. For children under age five, waterborne diseases are a leading cause of death. According to the World Bank, 88 percent of all waterborne diseases are caused by unsafe drinking water, inadequate sanitation and poor hygiene.\n\nWater is the underlying tenuous balance of safe water supply, but controllable factors such as the management and distribution of the water supply itself contribute to further scarcity.\n\nA 2006 United Nations report focuses on issues of governance as the core of the water crisis, saying \"There is enough water for everyone\" and \"Water insufficiency is often due to mismanagement, corruption, lack of appropriate institutions, bureaucratic inertia and a shortage of investment in both human capacity and physical infrastructure\". Official data also shows a clear correlation between access to safe water and GDP per capita.\n\nIt has also been claimed, primarily by economists, that the water situation has occurred because of a lack of property rights, government regulations and subsidies in the water sector, causing prices to be too low and consumption too high.\n\nVegetation and wildlife are fundamentally dependent upon adequate freshwater resources. Marshes, bogs and riparian zones are more obviously dependent upon sustainable water supply, but forests and other upland ecosystems are equally at risk of significant productivity changes as water availability is diminished. In the case of wetlands, considerable area has been simply taken from wildlife use to feed and house the expanding human population. But other areas have suffered reduced productivity from gradual diminishing of freshwater inflow, as upstream sources are diverted for human use. In seven states of the U.S. over 80 percent of all historic wetlands were filled by the 1980s, when Congress acted to create a \"no net loss\" of wetlands.\n\nIn Europe extensive loss of wetlands has also occurred with resulting loss of biodiversity. For example, many bogs in Scotland have been developed or diminished through human population expansion. One example is the Portlethen Moss in Aberdeenshire.\n\nOn Madagascar's highland plateau, a massive transformation occurred that eliminated virtually all the heavily forested vegetation in the period 1970 to 2000. The slash and burn agriculture eliminated about ten percent of the total country's native biomass and converted it to a barren wasteland. These effects were from overpopulation and the necessity to feed poor indigenous peoples, but the adverse effects included widespread gully erosion that in turn produced heavily silted rivers that \"run red\" decades after the deforestation. This eliminated a large amount of usable fresh water and also destroyed much of the riverine ecosystems of several large west-flowing rivers. Several fish species have been driven to the edge of extinction and some, such as the disturbed Tokios coral reef formations in the Indian Ocean, are effectively lost.\nIn October 2008, Peter Brabeck-Letmathe, chairman and former chief executive of Nestlé, warned that the production of biofuels will further deplete the world's water supply.\n\nThere are many other countries of the world that are severely impacted with regard to human health and inadequate drinking water. The following is a partial list of some of the countries with significant populations (numerical population of affected population listed) whose only consumption is of contaminated water:\n\nSeveral world maps showing various aspects of the problem can be found in this graph article.\n\nWater deficits, which are already spurring heavy grain imports in numerous smaller countries, may soon do the same in larger countries, such as China and India. The water tables are falling in scores of countries (including Northern China, the US, and India) due to widespread overpumping using powerful diesel and electric pumps. Other countries affected include Pakistan, Iran, and Mexico. This will eventually lead to water scarcity and cutbacks in grain harvest. Even with the overpumping of its aquifers, China is developing a grain deficit. When this happens, it will almost certainly drive grain prices upward. Most of the 3 billion people projected to be added worldwide by mid-century will be born in countries already experiencing water shortages. Unless population growth can be slowed quickly, it is feared that there may not be a practical non-violent or humane solution to the emerging world water shortage.\n\nAfter China and India, there is a second tier of smaller countries with large water deficits — Algeria, Egypt, Iran, Mexico, and Pakistan.\n\nAccording to a UN climate report, the Himalayan glaciers that are the sources of Asia's biggest rivers – Ganges, Indus, Brahmaputra, Yangtze, Mekong, Salween and Yellow – could disappear by 2035 as temperatures rise. It was later revealed that the source used by the UN climate report actually stated 2350, not 2035. Approximately 2.4 billion people live in the drainage basin of the Himalayan rivers. India, China, Pakistan, Bangladesh, Nepal and Myanmar could experience floods followed by droughts in coming decades. In India alone, the Ganges provides water for drinking and farming for more than 500 million people. The west coast of North America, which gets much of its water from glaciers in mountain ranges such as the Rocky Mountains and Sierra Nevada, also would be affected.\n\nBy far the largest part of Australia is desert or semi-arid lands commonly known as the outback. In June 2008 it became known that an expert panel had warned of long term, possibly irreversible, severe ecological damage for the whole Murray-Darling basin if it does not receive sufficient water by October. Water restrictions are currently in place in many regions and cities of Australia in response to chronic shortages resulting from drought. The Australian of the year 2007, environmentalist Tim Flannery, predicted that unless it made drastic changes, Perth in Western Australia could become the world’s first ghost metropolis, an abandoned city with no more water to sustain its population. However, Western Australia's dams reached 50% capacity for the first time since 2000 as of September 2009. As a result, heavy rains brought forth positive results for the region. Nonetheless, the following year, 2010, Perth suffered its second-driest winter on record and the water corporation tightened water restrictions for spring.\n\nConstruction of wastewater treatment plants and reduction of groundwater overdrafting appear to be obvious solutions to the worldwide problem; however, a deeper look reveals more fundamental issues in play. Wastewater treatment is highly capital intensive, restricting access to this technology in some regions; furthermore the rapid increase in population of many countries makes this a race that is difficult to win. As if those factors are not daunting enough, one must consider the enormous costs and skill sets involved to maintain wastewater treatment plants even if they are successfully developed.\n\nReducing groundwater overdrafting is usually politically unpopular, and can have major economic impacts on farmers. Moreover, this strategy necessarily reduces crop output, something the world can ill-afford given the current population.\n\nAt more realistic levels, developing countries can strive to achieve primary wastewater treatment or secure septic systems, and carefully analyse wastewater outfall design to minimize impacts to drinking water and to ecosystems. Developed countries can not only share technology better, including cost-effective wastewater and water treatment systems but also in hydrological transport modeling. At the individual level, people in developed countries can look inward and reduce over consumption, which further strains worldwide water consumption. Both developed and developing countries can increase protection of ecosystems, especially wetlands and riparian zones. There measures will not only conserve biota, but also render more effective the natural water cycle flushing and transport that make water systems more healthy for humans.\n\nA range of local, low-tech solutions are being pursued by a number of companies. These efforts center around the use of solar power to distill water at temperatures slightly beneath that at which water boils. By developing the capability to purify any available water source, local business models could be built around the new technologies, accelerating their uptake. For example, Bedouins from the town of Dahab in Egypt have installed Aqua Danial's Water Stellar, which uses a solar thermal collector measuring two square meters to distill from 40 to 60 liters per day from any local water source. This is five times more efficient than conventional stills and eliminates the need for polluting plastic PET bottles or transportation of water supply.\n\nIt is alleged that the likelihood of conflict rises if the rate of change within the basin exceeds the capacity of institution to absorb that change. Although water crisis is closely related to regional tensions, history showed that acute conflicts over water are far less than the record of cooperation.\n\nThe key lies in strong institutions and cooperation. The Indus River Commission and the Indus Water Treaty survived two wars between India and Pakistan despite their hostility, proving to be a successful mechanism in resolving conflicts by providing a framework for consultation inspection and exchange of data. The Mekong Committee has also functioned since 1957 and survived the Vietnam War. In contrast, regional instability results when there is an absence of institutions to co-operate in regional collaboration, like Egypt's plan for a high dam on the Nile. However, there is currently no global institution in place for the management and management of trans-boundary water sources, and international co-operation has happened through ad hoc collaborations between agencies, like the Mekong Committee which was formed due to an alliance between UNICEF and the US Bureau of Reclamation. Formation of strong international institutions seems to be a way forward – they fuel early intervention and management, preventing the costly dispute resolution process.\n\nOne common feature of almost all resolved disputes is that the negotiations had a \"need-based\" instead of a \"right–based\" paradigm. Irrigable lands, population, technicalities of projects define \"needs\". The success of a need-based paradigm is reflected in the only water agreement ever negotiated in the Jordan River Basin, which focuses in needs not on rights of riparians. In the Indian subcontinent, irrigation requirements of Bangladesh determine water allocations of the Ganges River. A need-based, regional approach focuses on satisfying individuals with their need of water, ensuring that minimum quantitative needs are being met. It removes the conflict that arises when countries view the treaty from a national interest point of view, move away from the zero-sum approach to a positive sum, integrative approach that equitably allocated the water and its benefits.\n\nThe Blue Peace framework developed by Strategic Foresight Group in partnership with the Governments of Switzerland and Sweden offers a unique policy structure which promotes sustainable management of water resources combined with cooperation for peace. By making the most of shared water resources through cooperation rather than mere allocation between countries, the chances for peace can be increased. The Blue Peace approach has proven to be effective in cases like the Middle East and the Nile basin. NGOs like Water.org, There Is No Limit Foundation, and are leading the way in providing access to clean water.\n\n\n\n"}
{"id": "7827316", "url": "https://en.wikipedia.org/wiki?curid=7827316", "title": "World Administrative Radio Conference", "text": "World Administrative Radio Conference\n\nThe World Administrative Radio Conference (WARC) was a technical conference of the International Telecommunication Union (ITU) where delegates from member nations of the ITU met to revise or amend the entire international Radio Regulations pertaining to all telecommunication services throughout the world. The conference was held in Geneva, Switzerland, with preparatory conferences held in Panama City, Panama.\n\nIn 1992 at an \"Additional Plenipotentiary Conference\" in Geneva the ITU was restructured and as a result from 1993 the conference became known as the World Radiocommunication Conference or WRC. \n\n\n"}
{"id": "1838622", "url": "https://en.wikipedia.org/wiki?curid=1838622", "title": "World Environment Day", "text": "World Environment Day\n\nWorld Environment Day (WED) is celebrated on the 5th of June every year, and is the United Nation's principal vehicle for encouraging awareness and action for the protection of our environment. First held in 1974, it has been a flagship campaign for raising awareness on emerging environmental issues from marine pollution, human overpopulation, and global warming, to sustainable consumption and wildlife crime. WED has grown to become a global platform for public outreach, with participation from over 143 countries annually. Each year, WED has a new theme that major corporations, NGOs, communities, governments and celebrities worldwide adopt to advocate environmental causes.\n\nWorld Environment Day [WED] was established by the UN General Assembly in 1972 on the first day of the Stockholm Conference on the Human Environment, resulting from discussions on the integration of human interactions and the environment. Two years later, in 1974 the first WED was held with the theme \"Only One Earth\". Even though WED celebration have been held annually since 1974, in 1987 the idea for rotating the center of these activities through selecting different host countries began.\n\nFor almost five decades, World Environment Day has been raising awareness, supporting action, and driving change. Here is a timeline of key accomplishments in WEDs’ history:\nThe theme for 2018 is \"Beat Plastic Pollution\". The host nation is India. By choosing this Theme, it is aimed that people may strive to change their everyday lives to reduce the heavy burden of plastic pollution. People should be free from the over-reliance on single-use or disposables, as they have severe environmental consequences. We should liberate our natural places, our wildlife - and our own health from plastics. Indian government pledged to eliminate all single use of plastic in India by 2022.\n\nThe theme for 2017 was 'Connecting People to Nature – in the city and on the land, from the poles to the equator'. The host nation was Canada.\n\nThe 2016 WED was organized under the theme \"Go wild for life\". This edition of the WED aims to reduce and prevent the illegal trade in wildlife. Angola was chosen as the host country of the 2016 WED during the COP21 in Paris.\n\nThe Slogan of the 2015 edition of the World Environment Day is \"Seven Billion Dreams. One Planet. Consume with Care\". The slogan was picked through a voting process on social media. In Saudi Arabia, 15 women recycled 2000 plastic bags to crochet a mural in support of the WED 2015. In India, Narendra Modi planted a Kadamb sapling to celebrate the World Environment Day and raise awareness for Environment. Italy is the host country of the 43rd edition of the WED. The celebrations took place as part of Milan Expo around the theme: Feeding the Planet - Energy for Life. \n\nThe Theme of the 2014 WED is : International Year of Small Islands Developing States (SIDS). By choosing this Theme the UN General Assembly aimed to highlight the development Challenges and successes of the SIDS. In 2014, the World Environment Day focused on global warming and its impact on ocean levels. The Slogan of the WED 2014 is \"Raise your voice not the sea level\", as Barbados hosted the global celebrations of the 42nd edition of the World Environment Day. The UN Environement Programme named actor Ian Somerhalder as the official Goodwill ambassador of the WED 2014.\n\nThe 2013 theme for World Environment Day was \"Think.Eat.Save.\"\n\nThe campaign addressed the huge annual wastage and losses in food, which, if conserved, would release a large quantity of food as well as reduce the overall carbon footprint. The campaign aimed to bring about awareness in countries with lifestyles resulting in food wastage. It also aimed to empower people to make informed choices about the food they eat so as to reduce the overall ecological impact due to the worldwide production of food..The host country for the year's celebrations was Mongolia.\n\nThe theme for the 2012 World Environment Day was \"Green Economy: Does it include you?\"\n\nThe theme aimed to invite people to examine their activities and lifestyle and see how the concept of a \"Green Economy\" fits into it. The host country for the year's celebrations was Brazil.\n\nThe theme for 2011 was Forests-Nature At Your Service. Thousands of activities were organized worldwide, with beach clean-ups, concerts, exhibits, film festivals, community events and much more. That year's global host, India, is a country of wide biodiversity.\n\n'Many Species. One Planet. One Future', was the theme of 2010.\n\nIt celebrated the diversity of life on Earth as part of the 2010 International Year of Biodiversity. It was hosted in Rwanda. Thousands of activities were organized worldwide, with beach clean-ups, concerts, exhibits, film festivals, community events and much more. Each continent (except Antarctica) had a \"regional host city\", the U.N. chose Pittsburgh, Pennsylvania as the host for all North.\n\nThe theme for WED 2009 was 'Your Planet Needs You – UNite to Combat Climate Change', and Michael Jackson's 'Earth Song' was declared 'World Environment Day Song'. It was hosted in Mexico.\n\nThe host for World Environment Day 2008 was New Zealand, with the main international celebrations scheduled for Wellington. The slogan for 2008 was \"CO, Kick the Habit! Towards a Low Carbon Economy.\" New Zealand was one of the first countries to pledge to achieve carbon neutrality, and will also focus on forest management as a tool for reducing greenhouse gases. \n\nThe Chicago Botanic Garden served as the North American host for World Environment Day on 5 June 2008.\n\nThe topic for World Environment Day for 2007 was \"Melting Ice – a Hot Topic?\" During International Polar Year, WED 2007 focused on the effects that climate change is having on polar ecosystems and communities, on other ice- and snow-covered areas of the world, and the resulting global impacts.\n\nThe main international celebrations of the WED 2007 were held in the city of Tromsø, Norway, a city north of the Arctic Circle.\n\nThe topic for WED 2006 was Deserts and Desertification and the slogan was \"Don't desert drylands\".\n\nThe slogan emphasised the importance of protecting drylands. The main international celebrations of the World Environment Day 2006 were held in Algeria.\n\nThe theme for the 2005 World Environment Day was \"Green Cities\" and the slogan was \"Plan for the Planet!\".\n\nWorld Environment Day celebrations have been (and will be) hosted in the following cities:\nAn Earth Anthem penned by poet Abhay K is sung to celebrate World Environment Day.\n\n<poem>\nOur cosmic oasis, cosmic blue pearl\nthe most beautiful planet in the universe\nall the continents and the oceans \nunited we stand as flora and fauna\nunited we stand as species of one earth\ndifferent cultures, beliefs and ways\nwe are humans, the earth is our home\nall the people and the nations of the world\nall for one and one for all\nunited we unfurl the blue flag.\n</poem>\n\nIt was launched in June 2013 on the occasion of the World Environment Day by Kapil Sibal and Shashi Tharoor, then Union Ministers of India, at a function organized by the Indian Council of Cultural Relations in New Delhi. It is supported by the global organization Habitat For Humanity.\n\n\n"}
{"id": "44940", "url": "https://en.wikipedia.org/wiki?curid=44940", "title": "World Heritage site", "text": "World Heritage site\n\nA World Heritage site is a landmark or area which is selected by the United Nations Educational, Scientific and Cultural Organization (UNESCO) as having cultural, historical, scientific or other form of significance, and is legally protected by international treaties. The sites are judged important to the collective interests of humanity.\n\nTo be selected, a World Heritage site must be an already classified landmark, unique in some respect as a geographically and historically identifiable place having special cultural or physical significance (such as an ancient ruin or historical structure, building, city, complex, desert, forest, island, lake, monument, mountain, or wilderness area). It may signify a remarkable accomplishment of humanity, and serve as evidence of our intellectual history on the planet.\n\nThe sites are intended for practical conservation for posterity, which otherwise would be subject to risk from human or animal trespassing, unmonitored/uncontrolled/unrestricted access, or threat from local administrative negligence. Sites are demarcated by UNESCO as protected zones. The list is maintained by the international World Heritage Program administered by the UNESCO World Heritage Committee, composed of 21 states parties which are elected by their General Assembly.\n\nThe programme catalogues, names, and conserves sites of outstanding cultural or natural importance to the common culture and heritage of humanity. Under certain conditions, listed sites can obtain funds from the World Heritage Fund. The program began with the \"Convention Concerning the Protection of the World's Cultural and Natural Heritage\", which was adopted by the General Conference of UNESCO on 16 November 1972. Since then, 193 state parties have ratified the convention, making it one of the most widely recognized international agreements and the world's most popular cultural program.\n\nAs of July 2018, a total of 1,092 World Heritage sites (845 cultural, 209 natural, and 38 mixed properties) exist across 167 countries. Italy, with 54 sites, has the most of any country, followed by China (53), Spain (47), France (44), Germany (44), India (37), and Mexico (35).\n\nIn 1954, the government of Egypt decided to build the new Aswan High Dam, whose resulting future reservoir would eventually inundate a large stretch of the Nile valley containing cultural treasures of ancient Egypt and ancient Nubia. In 1959, the governments of Egypt and Sudan requested UNESCO to assist their countries to protect and rescue the endangered monuments and sites. In 1960, the Director-General of UNESCO launched an appeal to the member states for an International Campaign to Save the Monuments of Nubia. This appeal resulted in the excavation and recording of hundreds of sites, the recovery of thousands of objects, as well as the salvage and relocation to higher ground of a number of important temples, the most famous of which are the temple complexes of Abu Simbel and Philae. The campaign, which ended in 1980, was considered a success. As tokens of its gratitude to countries which especially contributed to the campaign's success, Egypt donated four temples: the Temple of Dendur was moved to the Metropolitan Museum of Art in New York City, the Temple of Debod was moved to the Parque del Oeste in Madrid, the Temple of Taffeh was moved to the Rijksmuseum van Oudheden in the Netherlands, and the Temple of Ellesyia to Museo Egizio in Turin.\n\nThe project cost $80 million, about $40 million of which was collected from 50 countries. The project's success led to other safeguarding campaigns: saving Venice and its lagoon in Italy, the ruins of Mohenjo-daro in Pakistan, and the Borobodur Temple Compounds in Indonesia. UNESCO then initiated, with the International Council on Monuments and Sites, a draft convention to protect the common cultural heritage of humanity. \n\nThe United States initiated the idea of cultural conservation with nature conservation. The White House conference in 1965 called for a \"World Heritage Trust\" to preserve \"the world's superb natural and scenic areas and historic sites for the present and the future of the entire world citizenry\". The International Union for Conservation of Nature developed similar proposals in 1968, and they were presented in 1972 to the United Nations Conference on the Human Environment in Stockholm. Under the World Heritage Committee, signatory countries are required to produce and submit periodic data reporting providing the World Heritage Committee with an overview of each participating nation's implementation of the World Heritage Convention and a \"snapshot\" of current conditions at World Heritage properties.\n\nA single text was agreed on by all parties, and the \"Convention Concerning the Protection of the World Cultural and Natural Heritage\" was adopted by the General Conference of UNESCO on 16 November 1972.\n\nThe Convention came into force on 17 December 1975. As of May 2017, it has been ratified by 193 states parties, including 189 UN member states plus the Cook Islands, the Holy See, Niue, and the State of Palestine. Only four UN member states have not ratified the Convention: Liechtenstein, Nauru, Somalia and Tuvalu.\n\nA country must first list its significant cultural and natural sites; the result is called the Tentative List. A country may not nominate sites that have not been first included on the Tentative List. Next, it can place sites selected from that list into a Nomination File.\n\nThe Nomination File is evaluated by the International Council on Monuments and Sites and the World Conservation Union. These bodies then make their recommendations to the World Heritage Committee. The Committee meets once per year to determine whether or not to inscribe each nominated property on the World Heritage List and sometimes defers or refers the decision to request more information from the country which nominated the site. There are ten selection criteria – a site must meet at least one of them to be included on the list.\n\nUp to 2004, there were six criteria for cultural heritage and four criteria for natural heritage. In 2005, this was modified so that there is now only one set of ten criteria. Nominated sites must be of \"outstanding universal value\" and meet at least one of the ten criteria. These criteria have been modified or/amended several times since their creation.\n\nUNESCO designation as a World Heritage site provides \"prima facie\" evidence that such culturally sensitive sites are legally protected pursuant to the Law of War, under the Geneva Convention, its articles, protocols and customs, together with other treaties including the Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict and international law.\n\nThus, the Geneva Convention treaty promulgates:\n\n\"Article 53. PROTECTION OF CULTURAL OBJECTS AND OF PLACES OF WORSHIP. Without prejudice to the provisions of the Hague Convention for the Protection of Cultural Property in the Event of Armed Conflict of 14 May 1954,' and of other relevant international instruments, it is prohibited:\n\nA country may request to extend or reduce the boundaries, modify the official name, or change the selection criteria of one of its already listed sites. Any proposal for a significant boundary change or modify the site's selection criteria must be submitted as if it were a new nomination, including first placing it on the Tentative List and then onto the Nomination File.\n\nA request for a minor boundary change, one that does not have a significantly impact on the extent of the property or affect its \"outstanding universal value\", is also evaluated by the advisory bodies before being sent to the Committee. Such proposals can be rejected by either the advisory bodies or the Committee if they judge it to be a significant change instead of a minor one.\n\nProposals to change the site's official name is sent directly to the Committee.\n\nA site may be added to the List of World Heritage in Danger if there are conditions that threaten the characteristics for which the landmark or area was inscribed on the World Heritage List. Such problems may involve armed conflict and war, natural disasters, pollution, poaching, or uncontrolled urbanization or human development. This danger list is intended to increase international awareness of the threats and to encourage counteractive measures. Threats to a site can be either proven imminent threats or potential dangers that could have adverse effects on a site.\n\nThe state of conservation for each site on the danger list is reviewed on a yearly basis, after which the committee may request additional measures, delete the property from the list if the threats have ceased or consider deletion from both the List of World Heritage in Danger and the World Heritage List.\n\nOnly two sites have ever been delisted: the Arabian Oryx Sanctuary in Oman and the Dresden Elbe Valley in Germany. The Arabian Oryx Sanctuary was directly delisted in 2007, instead of first being put on the danger list, after the Omani government decided to reduce the protected area's size by 90 percent. The Dresden Elbe Valley was first placed on the danger list in 2006 when the World Heritage Committee decided that plans to construct the Waldschlösschen Bridge would significantly alter the valley's landscape. In response, the Dresden City Council attempted to stop the bridge's construction, but after several court decisions allowed the building of the bridge to proceed, the valley was removed from the World Heritage List in 2009.\n\nThe first global assessment to quantitatively measure threats to Natural World Heritage sites found that 63 percent of sites have been damaged by increasing human pressures including encroaching roads, agriculture infrastructure and settlements over the last two decades. These activities endanger Natural World Heritage sites and could compromise their unique values. Of the Natural World Heritage sites that contain forest, 91 percent of those experienced some loss since the year 2000. Many Natural World Heritage sites are more threatened than previously thought and require immediate conservation action.\n\nThere are 1092 World Heritage sites located in 167 states. Of these, 845 are cultural, 209 are natural and 38 are mixed properties. The World Heritage Committee has divided the world into five geographic zones which it calls regions: Africa, Arab states, Asia and the Pacific, Europe and North America, and Latin America and the Caribbean.\n\nRussia and the Caucasus states are classified as European, while Mexico and the Caribbean are classified as belonging to the Latin America & Caribbean zone, despite their location in North America. The UNESCO geographic zones also give greater emphasis on administrative, rather than geographic associations. Hence, Gough Island, located in the South Atlantic, is part of the Europe & North America region because the government of the United Kingdom nominated the site.\n\nThe table below includes a breakdown of the sites according to these zones and their classification:\n<nowiki>*</nowiki>The properties \"Uvs Nuur Basin\" and \"Landscapes of Dauria\" (Mongolia, Russian Federation) are trans-regional properties located in Europe and Asia and the Pacific region. They are counted here in the Asia and the Pacific region.\n\n<nowiki>*</nowiki>The property \"The Architectural Work of Le Corbusier, an Outstanding Contribution to the Modern Movement\" (Argentina, Belgium, France, Germany, India, Japan, Switzerland) is a trans-regional property with component sites located in three regions - Europe and North America, Asia and the Pacific, and Latin America and the Caribbean. It is counted here in Europe and North America.\n\nCountries with fifteen or more World Heritage sites, as of July 2018.\nDespite the successes of World Heritage listing in promoting conservation, the UNESCO administered project has attracted criticism from some for perceived under-representation of heritage sites outside Europe, disputed decisions on site selection and adverse impact of mass tourism on sites unable to manage rapid growth in visitor numbers.\n\nA sizable lobbying industry has grown around the awards because World Heritage listing has the potential to significantly increase tourism revenue from sites selected. Site listing bids are often lengthy and costly, putting poorer countries at a disadvantage. Eritrea's efforts to promote Asmara are one example.\n\nIn 2016, the Australian government was reported to have successfully lobbied for Great Barrier Reef conservation efforts to be removed from a UNESCO report titled 'World Heritage and Tourism in a Changing Climate'. The Australian government's actions were in response to their concern about the negative impact that an 'at risk' label could have on tourism revenue at a previously designated UNESCO World Heritage site.\n\nA number of listed World Heritage locations such as George Town, Penang, and Casco Viejo, Panama, have struggled to strike the balance between the economic benefits of catering to greatly increased visitor numbers and preserving the original culture and local communities that drew the recognition.\n\n\n"}
{"id": "3262732", "url": "https://en.wikipedia.org/wiki?curid=3262732", "title": "World Map at Lake Klejtrup", "text": "World Map at Lake Klejtrup\n\nThe World Map at Lake Klejtrup () is a miniature world map built of stones and grass in Klejtrup Sø near the village of Klejtrup, Viborg Municipality, Denmark.\n\nIn 1943, Søren Poulsen, a local farmer, was working on the drainage of the surrounding meadows when he found a stone shaped like the Jutland peninsula. This inspired him to create a small world of his own. During the winter months, with the use of primitive tools, he placed big stones carefully on the ice. When spring arrived, the stones could easily be tilted into place, and in this way the World Map took shape. Some of the stones used weighted more than 2 tonnes. \n\nPoulsen created the World Map between 1944 and 1969. It measures 45 by 90 metres (49 by 98 yards), covering an area of over 4000 square meters (1 acre). One 111-kilometre (69 mi) degree of latitude corresponds to 27 centimetres (11 inches) on the map. On Poulsen's map, Antarctica is not present and the Northern Hemisphere is marked in two places, ensuring a better impression of the correct distances between the countries to avoid the difficulties of spreading out our planet's globular shape. Red poles mark the equator, and each country is represented by miniature flags, which are updated yearly. State borders in the United States of America are marked with yellow bricks; Poulsen lived 20 years in America.\n\nThe map is the epicenter of a park, which has, among other attractions, a picnic area, a coffee shop and a playground. \n\nThe World Map is an important attraction in the area, and attracts about 40,000 visitors per year, most of them Danish.\n\n\n"}
{"id": "5656862", "url": "https://en.wikipedia.org/wiki?curid=5656862", "title": "World Rugby Rankings", "text": "World Rugby Rankings\n\nThe World Rugby Rankings (formerly the IRB Rankings) is a ranking system for men's national teams in rugby union, managed by World Rugby, the sport's governing body. The teams of World Rugby's member nations are ranked based on their game results, with the most successful teams being ranked highest. A point system is used, with points being awarded on the basis of the results of World Rugby-recognized international matches. Rankings are based on a team's performance, with more recent results and more significant matches being more heavily weighted to help reflect the current competitive state of a team. The ranking system was introduced the month before the 2003 Rugby World Cup, with the first new rankings issued on 8 September 2003.\n\nWhen the system was introduced England were the top team and maintained that position following victory in the 2003 Rugby World Cup. New Zealand took the lead from 7 June 2004. After winning the 2007 Rugby World Cup final, South Africa became the third team to achieve first place. The first two fixtures of the 2008 Tri Nations resulted in the top two teams switching places: the All Blacks regained the top spot after defeating South Africa in the Tri-Nations opener on 5 July 2008 in Wellington; a week later the Springboks returned the favour in Dunedin, scoring their first win over the All Blacks in New Zealand since 1998, reclaiming the top spot, only for the All Blacks to defeat both Australia and South Africa in August 2008 to regain the top spot by a considerable margin. South Africa regained the lead in July 2009 after beating New Zealand in Bloemfontein and kept the lead until losing to France in November of that year, allowing the All Blacks to regain the top spot.\n\nNew Zealand have been the most consistently ranked #1 team since the introduction of IRB World Rankings, having held the #1 ranking for more than 85 percent of the time during this period. South Africa and England make up the remainder.\n\nBelow is a list of the best and worst ranking positions for nations that have appeared in the Rugby World Cup:\nAll World Rugby member countries have been given a rating that is in the range of 0 to 100 with the top side achieving a rating of about 90 points. The point system is calculated using a 'Points Exchange' system, in which sides receive points from each other on the basis of the match result – whatever one side gains, the other loses. The exchanges are based on the match result, the ranking of each team, and the margin of victory, with an allowance for home advantage. As the system aims to depict current team strengths, past successes or losses will fade and be superseded by more recent results. Thus, it is thought that it will produce an accurate picture depicting the actual current strength and thus rank of the nations. The rankings are responsive to results and it is possible to climb to the top from the bottom (and vice versa) in fewer than 20 matches. As all matches are worth a net of 0 points for the two teams combined, there is no particular advantage to playing more matches. A rating stays the same until the team plays again. Although matches often result in points exchanges, 'predictable' results lead to very minor changes, and may result in no change to either side's rating.\n\nThe system ensures that it is representative of the teams' performance despite playing differing numbers of matches per annum, and the differing strength of opposition that teams have to face. The factors taken into account are as follows:\n\nFor each match played points exchanges are awarded for the following five outcomes and was developed using results of international matches from 1871 to the present day: \n\nDifferent matches have different importance to teams, and World Rugby has tried to respect this by using a weighting system, where the most significant matches are in the World Cup Finals. Points exchanges are doubled during the World Cup Finals to recognise the unique importance of this event. All other full international matches are treated the same, to be as fair as possible to countries playing a different mix of friendly and competitive matches. Matches that do not have full international status do not count.\n\nA win against a very highly ranked opponent is a considerably greater achievement than a win against a low-rated opponent, so the strength of the opposing team is a factor. Thus match results are more important than margins of victory in producing accurate rankings. This is because when a highly ranked team plays a lowly ranked team and manages to beat them by over 50 points, it does not necessarily indicate how either team will perform in the future.\n\nWhen calculating points exchanges, the home side is handicapped by treating them as though they are three rating points better than their current rating. This results in the home side gaining fewer points for winning and losing more points for losing. Because of this, ideally, any advantage that a side may have by playing in front of their home crowd is cancelled out.\n\nAll new member nations start with 30.00 points, which is provisional until they have completed ten test matches. When countries merge, the new country inherits the higher rating of the two countries but when they split e.g., the 2010 breakup of the Arabian Gulf rugby union team into separate teams representing its current member countries, the new countries will inherit a rating at a fixed level below the rating of the original country.\n\nBefore 1 December 2012 new member nations were given 40.00 points.\n\nCountries that have not played a test for two years are removed from the ranking system and the list. If they become active again, they resume their previous rating.\n\n<nowiki>***</nowiki>For a full explanation of how rankings are calculated, see the World Rugby rankings website.\n\n"}
{"id": "33692", "url": "https://en.wikipedia.org/wiki?curid=33692", "title": "World history", "text": "World history\n\nWorld history or global history (not to be confused with diplomatic, transnational or international history) is a field of historical study that emerged as a distinct academic field in the 1980s. It examines history from a global perspective. It is not to be confused with comparative history, which, like world history, deals with the history of multiple cultures and nations, but does not do so on a global scale. World history looks for common patterns that emerge across all cultures. World historians use a thematic approach, with two major focal points: integration (how processes of world history have drawn people of the world together) and difference (how patterns of world history reveal the diversity of the human experiences).\n\nJerry H. Bentley has observed that 'the term \"world history\" has never been a clear signifier with a stable referent', and that usage of the term overlaps with universal history, comparative history, global history, big history, macro history, and transnational history, amongst others.\n\nThe advent of world history as a distinct academic field of study can be traced to the 1960s, but the pace quickened in the 1980s. A key step was the creation of the World History Association and graduate programs at a handful of universities. Over the next decades scholarly publications, professional and academic organizations, and graduate programs in World History proliferated. World History has often displaced Western Civilization in the required curriculum of American high schools and universities, and is supported by new textbooks with a world history approach.\n\nWorld History attempts to recognise and address two structures that have profoundly shaped professional history-writing:\nThus World History tends to study networks, connections, and systems that cross traditional boundaries of historical study like linguistic, cultural, and national borders. World History is often concerned to explore social dynamics that have led to large-scale changes in human society, such as industrialisation and the spread of capitalism, and to analyse how large-scale changes like these have affected different parts of the world. Like other branches of history-writing in the second half of the twentieth century, World History has a scope far beyond historians' traditional focus on politics, wars, and diplomacy, taking in a panoply of subjects like gender history, social history, cultural history, and environmental history.\n\n\nThe study of world history, as distinct from national history, has existed in many world cultures. However, early forms of world history were not truly global, and were limited to only the regions known by the historian.\n\nIn Ancient China, Chinese world history, that of China and the surrounding people of East Asia, was based on the dynastic cycle articulated by Sima Qian in circa 100 BC. Sima Qian's model is based on the Mandate of Heaven. Rulers rise when they united China, then are overthrown when a ruling dynasty became corrupt. Each new dynasty begins virtuous and strong, but then decays, provoking the transfer of Heaven's mandate to a new ruler. The test of virtue in a new dynasty is success in being obeyed by China and neighboring barbarians. After 2000 years Sima Qian's model still dominates scholarship, although the dynastic cycle is no longer used for modern Chinese history.\n\nIn Ancient Greece, Herodotus (5th century BC), as founder of Greek historiography, presents insightful and lively discussions of the customs, geography, and history of Mediterranean peoples, particularly the Egyptians. However, his great rival Thucydides promptly discarded Herodotus's all-embracing approach to history, offering instead a more precise, sharply focused monograph, dealing not with vast empires over the centuries but with 27 years of war between Athens and Sparta. In Rome, the vast, patriotic history of Rome by Livy (59 BC-17 AD) approximated Herodotean inclusiveness; Polybius (c.200-c.118 BC) aspired to combine the logical rigor of Thucydides with the scope of Herodotus.\n\nIn Central Asia, The Secret History of Mongols is regarded as the single significant native Mongolian account of Genghis Khan. The Secret History is regarded as a piece of classic literature in both Mongolia and the rest of the world.\n\nIn the Middle East, Ala'iddin Ata-Malik Juvayni (1226–1283) was a Persian historian who wrote an account of the Mongol Empire entitled Ta' rīkh-i jahān-gushā (History of the World Conqueror). The standard edition of Juvayni is published under the title Ta' rīkh-i jahān-gushā, ed. Mirza Muhammad Qazwini, 3 vol, Gibb Memorial Series 16 (Leiden and London, 1912–37). An English translation by John Andrew Boyle \"The History of the World-Conqueror\" was republished in 1997.\n\nRashīd al-Dīn Fadhl-allāh Hamadānī (1247–1318), was a Persian physician of Jewish origin, polymathic writer and historian, who wrote an enormous Islamic history, the Jami al-Tawarikh, in the Persian language, often considered a landmark in intercultural historiography and a key document on the Ilkhanids (13th and 14th century). His encyclopedic knowledge of a wide range of cultures from Mongolia to China to the Steppes of Central Eurasia to Persia, the Arabic-speaking lands, and Europe, provide the most direct access to information on the late Mongol era. His descriptions also highlight the manner in which the Mongol Empire and its emphasis on trade resulted in an atmosphere of cultural and religious exchange and intellectual ferment, resulting in the transmission of a host of ideas from East to West and vice versa.\n\nOne Muslim scholar, Ibn Khaldun (1332-1409) broke with traditionalism and offered a model of historical change in \"Muqaddimah,\" an exposition of the methodology of scientific history. Ibn Khaldun focused on the reasons for the rise and fall of civilization, arguing that the causes of change are to be sought in the economic and social structure of society. His work was largely ignored in the Muslim world. Otherwise the Muslim, Chinese and Indian intellectuals held fast to a religious traditionalism, leaving them unprepared to advise national leaders on how to confront the European intrusion into Asia after 1500 AD.\n\nDuring the Renaissance in Europe, history was written about states or nations. The study of history changed during the Enlightenment and Romanticism. Voltaire described the history of certain ages that he considered important, rather than describing events in chronological order. History became an independent discipline. It was not called \"philosophia historiae\" anymore, but merely history (\"historia\").Voltaire, in the 18th century, attempted to revolutionize the study of world history. First, Voltaire concluded that the traditional study of history was flawed. The Christian Church, one of the most powerful entities in his time, had presented a framework for studying history. Voltaire, when writing \"History of Charles XII\" (1731) and \"The Age of Louis XIV\" (1751), instead choose to focus economics, politics and culture. These aspects of history were mostly unexplored by his contemporaries and would each develop into their own sections of world history. Above all else, Voltaire regarded truth as the most essential part of recording world history. Nationalism and religion only subtracted from objective truth, so Voltaire freed himself for their influence when he recorded history.\n\nGiambattista Vico (1668–1744) in Italy wrote \"Scienza nuva seconda\" (The New Science) in 1725, which argued history as the expression of human will and deeds. He thought that men are historical entities and that human nature changes over time. Each epoch should be seen as a whole in which all aspects of culture—art, religion, philosophy, politics, and economics—are interrelated (a point developed later by Oswald Spengler). Vico showed that myth, poetry, and art are entry points to discovering the true spirit of a culture. Vico outlined a conception of historical development in which great cultures, like Rome, undergo cycles of growth and decline. His ideas were out of fashion during the Enlightenment, but influenced the Romantic historians after 1800.\n\nA major theoretical foundation for world history was given by German philosopher G. W. F. Hegel, who saw the modern Prussian state as the latest (though often confused with the highest) stage of world development.\n\nG.W.F. Hegel developed three lenses through which he believed world history could be viewed. Documents produced during a historical period, such as journal entries and contractual agreements, were considered by Hegel to be part of Original History. These documents are produced by a person enveloped within a culture, making them conduits of vital information but also limited in their contextual knowledge. Documents which pertain to Hegel’s Original History are classified by modern historians as primary sources.\n\nReflective History, Hegel’s second lens, are documents written with some temporal distance separating the event which is discussed in the academic writing. What limited this lens, according to Hegel, was the imposition of the writers own cultural values and views on the historical event. This criticism of Reflective History was later formalized by Anthropologists Franz Boa and coined as Cultural relativism by Alain Locke. Both of these lenses were considered to be partially flawed by Hegel.\n\nHegel termed the lens which he advocated to view world history through as Philosophical History. In order to view history through this lens, one must analyze events, civilizations, and periods objectively. When done in this fashion, the historian can then extract the prevailing theme from their studies. This lens differs from the rest because it is void of any cultural biases and takes a more analytical approach to history. World History can be a broad topic, so focusing on extracting the most valuable information from certain periods may be the most beneficial approach. This third lens, as did Hegel’s definitions of the other two, affected the study of history in the early modern period and our contemporary period.\n\nAnother early modern historian was Adam Ferguson. Ferguson’s main contribution to the study of world history was his \"An Essay on the History of Civil Society\" (1767). According to Ferguson, world history was a combination of two forms of history. One was natural history; the aspects of our world which god created. The other, which was more revolutionary, was social history. For him, social history was the progress humans made towards fulfilling God’s plan for humanity. He believed that progress, which could be achieved through individuals pursuing commercial success, would bring us closer to a perfect society; but we would never reach one. However, he also theorized that a complete dedication to commercial success could lead to societal collapse—like what happened in Rome—because people would lose morality. Through this lens, Ferguson viewed world history as humanities struggle to reach an ideal society.\n\nHenry Home, Lord Kames was a philosopher during the Enlightenment and contributed to the study or world history. In his major historical work, \"Sketches on the History of Man\", Home’s outlined the four stages of human history which he observed. The first and most primitive stage was small hunter-gatherer groups. Then, in order to form larger groups, humans transitioned into the second stage when they began to domesticate animals. The third stage was the development of agriculture. This new technology established trade and higher levels of cooperation amongst sizable groups of people. With the gathering of people into agricultural villages, laws and social obligations needed to be developed so a form of order could be maintained. The fourth, and final stage, involved humans moving into market towns and seaports where agriculture was not the focus. Instead, commerce and other forms of labor arouse in a society. By defining the stages of human history, Homes influenced his successors. He also contributed to the development of other studies such as sociology and anthropology.\n\nWorld history became a popular genre in the 20th century with universal history. In the 1920s, several best-sellers dealt with the history of the world, including surveys \"The Story of Mankind\" (1921) by Hendrik Willem van Loon and \"The Outline of History\" (1918) by H.G. Wells. Influential writers who have reached wide audiences include H. G. Wells, Oswald Spengler, Arnold J. Toynbee, Pitirim Sorokin, Carroll Quigley, Christopher Dawson, and Lewis Mumford. Scholars working the field include Eric Voegelin, William Hardy McNeill and Michael Mann. With evolving technologies such as dating methods and surveying laser technology called LiDAR, contemporary historians have access to knew information which changes how past civilizations are studied.\n\nSpengler's \"Decline of the West\" (2 vol 1919–1922) compared nine organic cultures: Egyptian (3400 BC-1200 BC), Indian (1500 BC-1100 BC), Chinese (1300 BC-AD 200), Classical (1100 BC-400 BC), Byzantine (AD 300–1100), Aztec (AD 1300–1500), Arabian (AD 300–1250), Mayan (AD 600–960), and Western (AD 900–1900). His book was a smashing success among intellectuals worldwide as it predicted the disintegration of European and American civilization after a violent \"age of Caesarism,\" arguing by detailed analogies with other civilizations. It deepened the post-World War I pessimism in Europe, and was warmly received by intellectuals in China, India, and Latin America who hoped his predictions of the collapse of European empires would soon come true.\n\nIn 1936–1954, Toynbee's ten-volume \"A Study of History\" came out in three separate installments. He followed Spengler in taking a comparative topical approach to independent civilizations. Toynbee said they displayed striking parallels in their origin, growth, and decay. Toynbee rejected Spengler's biological model of civilizations as organisms with a typical life span of 1,000 years. Like Sima Qian, Toynbee explained decline as due to their moral failure. Many readers rejoiced in his implication (in vols. 1–6) that only a return to some form of Catholicism could halt the breakdown of western civilization which began with the Reformation. Volumes 7–10, published in 1954, abandoned the religious message, and his popular audience slipped away, while scholars picked apart his mistakes.,\n\nMcNeill wrote \"The Rise of the West\" (1963) to improve upon Toynbee by showing how the separate civilizations of Eurasia interacted from the very beginning of their history, borrowing critical skills from one another, and thus precipitating still further change as adjustment between traditional old and borrowed new knowledge and practice became necessary. McNeill took a broad approach organized around the interactions of peoples across the Earth. Such interactions have become both more numerous and more continual and substantial in recent times. Before about 1500, the network of communication between cultures was that of Eurasia. The term for these areas of interaction differ from one world historian to another and include \"world-system\" and \"ecumene.\" Whatever it is called, the importance of these intercultural contacts has begun to be recognized by many scholars.\n\nT. Walter Wallbank and Alastair M. Taylor co-authored \"Civilization Past & Present\", the first world-history textbook published in the United States (1942). With additional authors, this very successful work went through numerous editions up to the first decade of the twenty-first century. According to the Golden Anniversary edition of 1992, the ongoing objective of \"Civilization Past & Present\" \"was to present a survey of world cultural history, treating the development and growth of civilization not as a unique European experience but as a global one through which all the great culture systems have interacted to produce the present-day world. It attempted to include all the elements of history – social, economic, political, religious, aesthetic, legal, and technological.\" In college curricula of the United States, world history became a popular replacement for courses on Western Civilization. Professors Patrick Manning, previously of Northeastern University and now at the University of Pittsburgh's World History Center; and Ross E. Dunn at San Diego State are leaders in promoting innovative teaching methods.\n\nIn schools of architecture in the U.S., the National Architectural Accrediting Board now requires that schools teach history that includes a non-west or global perspective. This reflects a decade-long effort to move past the standard Euro-centric approach that had dominated the field.\n\nIn recent years, the relationship between African and world history has shifted rapidly from one of antipathy to one of engagement and synthesis. Reynolds (2007) surveys the relationship between African and world histories, with an emphasis on the tension between the area studies paradigm and the growing world-history emphasis on connections and exchange across regional boundaries. A closer examination of recent exchanges and debates over the merits of this exchange is also featured. Reynolds sees the relationship between African and world history as a measure of the changing nature of historical inquiry over the past century.\n\nThe Marxist theory of historical materialism claims the history of the world is fundamentally determined by the \"material conditions\" at any given time – in other words, the relationships which people have with each other in order to fulfil basic needs such as feeding, clothing and housing themselves and their families. Overall, Marx and Engels claimed to have identified five successive stages of the development of these material conditions in Western Europe.\n\nThe theory divides the history of the world into the following periods: Primitive communism; Slave society; Feudalism; Capitalism; and Socialism.\n\nRegna Darnell and Frederic Gleach argue that, in the Soviet Union, the Marxian theory of history was the only accepted orthodoxy, and stifled research into other schools of thought on history. However, adherents of Marx's theories argue that Stalin distorted Marxism.\n\n\n\n\n\n\n\n\n"}
{"id": "19620245", "url": "https://en.wikipedia.org/wiki?curid=19620245", "title": "Zealandia", "text": "Zealandia\n\nZealandia (), also known as the New Zealand continent or Tasmantis is an almost entirely submerged mass of continental crust that sank after breaking away from Australia 60–85 million years ago, having separated from Antarctica between 85 and 130 million years ago. It has variously been described as a continental fragment, a microcontinent, a submerged continent, and a continent. The name and concept for Zealandia was proposed by Bruce Luyendyk in 1995. Zealandia's status as a continent is not universally accepted, but New Zealand geologist Nick Mortimer has commented that \"if it wasn't for the ocean\" it would have been recognized as such long ago.\n\nThe land mass may have been completely submerged about 23 million years ago, and most of it (93%) remains submerged beneath the Pacific Ocean. With a total area of approximately , it is the world's largest current microcontinent, more than twice the size of the next-largest microcontinent and more than half the size of the Australian continent. As such, and due to other geological considerations, such as crustal thickness and density, it is arguably a continent in its own right. This was the argument which made news in 2017, when geologists from New Zealand, New Caledonia, and Australia concluded that Zealandia fulfills all the requirements to be considered a continent, rather than a microcontinent or continental fragment.\n\nZealandia supports substantial inshore fisheries and contains gas fields, of which the largest known is New Zealand's Maui gas field, near Taranaki. Permits for oil exploration in the Great South Basin were issued in 2007. Offshore mineral resources include iron sands, volcanic massive sulfides and ferromanganese nodule deposits.\n\nZealandia is largely made up of two nearly parallel ridges, separated by a failed rift, where the rift breakup of the continent stops and becomes a filled graben. The ridges rise above the sea floor to heights of , with few rocky islands rising above sea level. The ridges are continental rock, but are lower in elevation than normal continents because their crust is thinner than usual, approximately thick, and consequently they do not float as high above the Earth's mantle.\n\nAbout 25 million years ago, the southern part of Zealandia (on the Pacific Plate) began to shift relative to the northern part (on the Indo-Australian Plate). The resulting displacement by approximately along the Alpine Fault is evident in geological maps. Movement along this plate boundary has also offset the New Caledonia Basin from its previous continuation through the Bounty Trough.\n\nCompression across the boundary has uplifted the Southern Alps, although due to rapid erosion their height reflects only a small fraction of the uplift. Farther north, subduction of the Pacific Plate has led to extensive volcanism, including the Coromandel and Taupo Volcanic Zones. Associated rifting and subsidence has produced the Hauraki Graben and more recently the Whakatane Graben and Wanganui Basin.\n\nVolcanism on Zealandia has also taken place repeatedly in various parts of the continental fragment before, during and after it rifted away from the supercontinent Gondwana. Although Zealandia has shifted approximately to the northwest with respect to the underlying mantle from the time when it rifted from Antarctica, recurring intracontinental volcanism exhibits magma composition similar to that of volcanoes in previously adjacent parts of Antarctica and Australia.\n\nThis volcanism is widespread across Zealandia but generally of low volume apart from the huge mid to late Miocene shield volcanoes that developed the Banks and Otago Peninsulas. In addition, it took place continually in numerous limited regions all through the Late Cretaceous and the Cenozoic. However, its causes are still in dispute. During the Miocene, the northern section of Zealandia (Lord Howe Rise) might have slid over a stationary hotspot, forming the Lord Howe Seamount Chain.\n\nZealandia is occasionally divided by scientists into two regions, North Zealandia (or Western Province) and South Zealandia (or Eastern Province), the latter of which contains most of the Median Batholith crust. These two features are separated by the Alpine Fault and Kermadec Trench and by the wedge-shaped Hikurangi Plateau, and are moving separately to each other.\n\nThe case for Zealandia being a continent in its own right was argued by Nick Mortimer and Hamish Campbell in their book \"Zealandia: Our continent revealed\" in 2014, citing geological and ecological evidence to support the proposal.\n\nIn 2017, a team of eleven geologists from New Zealand, New Caledonia, and Australia concluded that Zealandia fulfills all the requirements to be considered a drowned continent, rather than a microcontinent or continental fragment. This was widely covered by news media.\n\nNew Caledonia lies at the northern end of the ancient continent, while New Zealand rises at the plate boundary that bisects it. These land masses are two outposts of the Antarctic Flora, including Araucarias and Podocarps. At Curio Bay, logs of a fossilized forest closely related to modern Kauri and Norfolk Pine can be seen that grew on Zealandia about 180 million years ago during the Jurassic period, before it split from Gondwana. These were buried by volcanic mud flows and gradually replaced by silica to produce the fossils now exposed by the sea.\n\nDuring glacial periods, more of Zealandia becomes a terrestrial rather than a marine environment. Zealandia was originally thought to have no native land mammal fauna, but the discovery in 2006 of a fossil mammal jaw from the Miocene in the Otago region shows otherwise.\n\nThe total land area (including inland water bodies) of Zealandia is . Of this, New Zealand comprises the majority, at , or 93%) which includes the mainland, nearby islands, and most outlying islands including the Antipodes Islands, Auckland Islands, Bounty Islands, Campbell Islands, and Chatham Islands (but not the Kermadec Islands or Macquarie Island (Australia), which are part of the rift).\n\nNew Caledonia and the islands surrounding it comprise some or 7%) and the remainder is made up of various territories of Australia including Lord Howe Island Group (New South Wales) at or 0.02%), Norfolk Island at or 0.01%), as well as Elizabeth and Middleton Reefs (Coral Sea Islands Territory) with .\n\nThe total human population of Zealandia today is about 5 million people.\n"}
