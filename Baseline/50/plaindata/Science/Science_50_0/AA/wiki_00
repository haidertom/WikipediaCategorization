{"id": "57980271", "url": "https://en.wikipedia.org/wiki?curid=57980271", "title": "ADRON-RM", "text": "ADRON-RM\n\nADRON-RM (Autonomous Detector of Radiation of Neutrons Onboard Rover at Mars) is a neutron spectrometer to search for subsurface water ice and hydrated minerals. This analyser is part of the science payload on board the European Space Agency ExoMars rover, tasked to search for biosignatures and biomarkers on Mars. The rover is planned to be launched in July 2020 and land on Mars in March 2021.\n\nADRON-RM is a near copy of ADRO-EM on the stationary ExoMars 2020 surface platform and the Dynamic Albedo of Neutrons (DAN) neutron detector on board NASA's \"Curiosity\" rover, all designed by Igor Mitrofanov from the Russian Space Research Institute (IKI). \n\nADRON-RM is a neutron spectrometer that will search for hydrogen in the form of bound water or water ice, and hydrogen-bearing compounds. It will be used in combination with WISDOM instrument (a ground-penetrating radar) to study the subsurface beneath the rover and to search for optimal sites for drilling and sample collection. It can also detect trace elements such as Gd and major elements that constitute soil, such as Cl, Fe, Ti. It will also monitor the neutron component of the radiation background on Mars' surface.\n\nThe Principal Investigator is Igor Mitrofanov from the Russian Space Research Institute (IKI). The instrument is almost a reproduction of the Dynamic Albedo of Neutrons (DAN) neutron detector on board NASA's \"Curiosity rover\" also developed in Russia. Mitrofanov is also developing the active gamma and neutron spectrometer, ADRON-EM (Active Detection of Radiation of Nuclei-ExoMars) for the stationary ExoMars 2020 surface platform —the primary goal of which will be to measure water distribution in the Martian subsurface. Measurements by ADRON-RM and ADRON-EM will work in synergy with other ExoMars instruments.\n\nADRON-RM uses two He proportional counters with a cylindrical shape of about 25  mm in diameter and 55  mm in total length. Each counter is filled with He gas under 4 atmospheres of pressure. Each neutron detector will measure two 32-channel spectra. The data will be obtained as routine and passive measurements, which will not be saved but will be immediately transmitted from the instrument to the rover computer. This means that all ADRON-RM measurements will be performed only when the 'Rover Compute Element' is active (daytime).\n\nADRON-RM is installed inside the ExoMars rover body at the rear balcony. The height above the surface is 0.8 m (2.6 ft)\n\nThe stated objectives of the ADRON-RM scientific investigation include:\n\n"}
{"id": "12851541", "url": "https://en.wikipedia.org/wiki?curid=12851541", "title": "Artificial seawater", "text": "Artificial seawater\n\nArtificial seawater (abbreviated ASW) is a mixture of dissolved mineral salts (and sometimes vitamins) that simulates seawater. Artificial seawater is primarily used in marine biology and in marine and reef aquaria, and allows the easy preparation of media appropriate for marine organisms (including algae, bacteria, plants and animals). From a scientific perspective, artificial seawater has the advantage of reproducibility over natural seawater since it is a standardized formula. Synthetic seawater is also known as artificial seawater and substitute ocean water. \n\nThe tables below present an example of an artificial seawater (35.00‰ of salinity) preparation devised by Kester, Duedall, Connors and Pytkowicz (1967). The recipe consists of two lists of mineral salts, the first of anhydrous salts that can be weighed out, the second of hydrous salts that should be added to the artificial seawater as a solution.\n\nWhile all of the compounds listed in the recipe above are inorganic, mineral salts, some artificial seawater recipes, such as Goldman and McCarthy (1978), make use of trace solutions of vitamins or organic compounds.\n\nThe International Standard for making Artificial Seawater can be found at ASTM International. The current standard is named ASTM D1141-98 (the original standard was ASTM D1141-52) and describes the standard practice for the preparation of substitute ocean water. \n\nThere are various applications for ASTM D1141-98 synthetic seawater including corrosion studies, ocean instrument calibration and chemical processing. Typically, laboratory grade water is used when making synthetic salts \n\n\n\n"}
{"id": "10037743", "url": "https://en.wikipedia.org/wiki?curid=10037743", "title": "Association theory", "text": "Association theory\n\nAssociation theory (also aggregate theory) is a discredited theory first advanced by chemist Thomas Graham in 1861 to describe the molecular structure of substances such as cellulose and starch, now understood to be polymers. Association theory postulates that such materials are composed of a collection of smaller molecules bound together by an unknown force. Graham termed these materials colloids. Prior to the development of macromolecular theory by Hermann Staudinger in the 1920s, association theory remained the most prevalent model of polymer structure in the scientific community.\n\n"}
{"id": "2060912", "url": "https://en.wikipedia.org/wiki?curid=2060912", "title": "Backward induction", "text": "Backward induction\n\nBackward induction is the process of reasoning backwards in time, from the end of a problem or situation, to determine a sequence of optimal actions. It proceeds by first considering the last time a decision might be made and choosing what to do in any situation at that time. Using this information, one can then determine what to do at the second-to-last time of decision. This process continues backwards until one has determined the best action for every possible situation (i.e. for every possible information set) at every point in time.\n\nIn the mathematical optimization method of dynamic programming, backward induction is one of the main methods for solving the Bellman equation. In game theory, backward induction is a method used to compute subgame perfect equilibria in sequential games. The only difference is that optimization involves just one decision maker, who chooses what to do at each point of time, whereas game theory analyzes how the decisions of several players interact. That is, by anticipating what the last player will do in each situation, it is possible to determine what the second-to-last player will do, and so on. In the related fields of automated planning and scheduling and automated theorem proving, the method is called backward search or backward chaining. In chess it is called retrograde analysis.\n\nBackward induction has been used to solve games as long as the field of game theory has existed. John von Neumann and Oskar Morgenstern suggested solving zero-sum, two-person games by backward induction in their \"Theory of Games and Economic Behavior\" (1944), the book which established game theory as a field of study.\n\nConsider an unemployed person who will be able to work for ten more years \"t\" = 1,2...,10. Suppose that each year in which he remains unemployed, he may be offered a 'good' job that pays $100, or a 'bad' job that pays $44, with equal probability (50/50). Once he accepts a job, he will remain in that job for the rest of the ten years. (Assume for simplicity that he cares only about his monetary earnings, and that he values earnings at different times equally, i.e., the discount rate is zero.)\n\nShould this person accept bad jobs? To answer this question, we can reason backwards from time \"t\" = 10.\n\nIt can be verified by continuing to work backwards that bad offers should only be accepted if one is still unemployed at times 9 or 10; they should be rejected at all times up to \"t\" = 8. The intuition is that if one expects to work in a job for a long time, this makes it more valuable to be picky about what job to accept.\n\nA dynamic optimization problem of this kind is called an optimal stopping problem, because the issue at hand is when to stop waiting for a better offer. Search theory is the field of microeconomics that applies problems of this type to contexts like shopping, job search, and marriage.\n\nConsider the ultimatum game, where one player proposes to split a dollar with another. The first player (the proposer) suggests a division of the dollar between the two players. The second player is then given the option to either accept the split or reject it. If the second player accepts, both get the amount suggested by the proposer. If rejected, neither receives anything.\n\nConsider the actions of the second player given any arbitrary proposal by the first player (that gives the second player more than zero). Since the only choice the second player has at each of these points in the game is to choose between something and nothing, one can expect that the second will accept. Given that the second will accept all proposals offered by the first (that give the second anything at all), the first ought to propose giving the second as little as possible. This is the unique subgame perfect equilibrium of the Ultimatum Game. (However, the Ultimatum Game does have several other Nash equilibria which are not subgame perfect.)\n\nSee also centipede game.\n\nConsider a dynamic game in which the players are an incumbent firm in an industry and a potential entrant to that industry. As it stands, the incumbent has a monopoly over the industry and does not want to lose some of its market share to the entrant. If the entrant chooses not to enter, the payoff to the incumbent is high (it maintains its monopoly) and the entrant neither loses nor gains (its payoff is zero). If the entrant enters, the incumbent can \"fight\" or \"accommodate\" the entrant. It will fight by lowering its price, running the entrant out of business (and incurring exit costs — a negative payoff) and damaging its own profits. If it accommodates the entrant it will lose some of its sales, but a high price will be maintained and it will receive greater profits than by lowering its price (but lower than monopoly profits).\n\nConsider if the best response of the incumbent is to accommodate if the entrant enters. If the incumbent accommodates, the best response of the entrant is to enter (and gain profit). Hence the strategy profile in which the entrant enters and the incumbent accommodates if the entrant enters is a Nash equilibrium consistent with backward induction. However, if the incumbent is going to fight, the best response of the entrant is to not enter, and if the entrant does not enter, it does not matter what the incumbent chooses to do in the hypothetical case that the entrant does enter. Hence the strategy profile in which the incumbent fights if the entrant enters, but the entrant does not enter is also a Nash equilibrium. However, were the entrant to deviate and enter, the incumbent's best response is to accommodate—the threat of fighting is not credible. This second Nash equilibrium can therefore be eliminated by backward induction.\n\nThe unexpected hanging paradox is a paradox related to backward induction. Suppose a prisoner is told that she will be hanged sometime between Monday and Friday of next week. However, the exact day will be a surprise (i.e. she will not know the night before that she will be executed the next day). The prisoner, interested in outsmarting her executioner, attempts to determine which day the execution will occur.\n\nShe reasons that it cannot occur on Friday, since if it had not occurred by the end of Thursday, she would know the execution would be on Friday. Therefore, she can eliminate Friday as a possibility. With Friday eliminated, she decides that it cannot occur on Thursday, since if it had not occurred on Wednesday, she would know that it had to be on Thursday. Therefore, she can eliminate Thursday. This reasoning proceeds until she has eliminated all possibilities. She concludes that she will not be hanged next week.\n\nTo her surprise, she is hanged on Wednesday. She made the mistake of assuming that she knew definitively whether the unknown future factor that would cause her execution was one that she could reason about.\n\nHere the prisoner reasons by backward induction, but seems to come to a false conclusion. Note, however, that the description of the problem assumes it is possible to surprise someone who is performing backward induction. The mathematical theory of backward induction does not make this assumption, so the paradox does not call into question the results of this theory. Nonetheless, this paradox has received some substantial discussion by philosophers.\n"}
{"id": "576120", "url": "https://en.wikipedia.org/wiki?curid=576120", "title": "Balzan Prize", "text": "Balzan Prize\n\nThe International Balzan Prize Foundation awards four annual monetary prizes to people or organizations who have made outstanding achievements in the fields of humanities, natural sciences, culture, as well as for endeavours for peace and the brotherhood of man.\n\nEach year the foundation chooses the fields eligible for the next year's prizes, and determines the prize amount. These are generally announced in May, with the winners announced the September the following year. Since 2001 the prize money has increased to 1 million Swiss Francs per prize, on condition that half the money is used for projects involving young researchers.\n\nThe Balzan Prize committee comprises twenty members of the prestigious learned societies of Europe.\n\nThe assets behind the foundation were established by the Italian Eugenio Balzan (1874–1953), a part-owner of \"Corriere della Sera\" who had invested his assets in Switzerland and in 1933 had left Italy in protest against fascism. He left a substantial inheritance to his daughter Angela Lina Balzan (1892–1956), who at the time was suffering an incurable disease. Before her death, she left instructions for the foundation and since then it has two headquarters, the Prize administered from Milan, the Fund from Zurich.\n\nThe first award was in fact 1 million Swiss francs to the Nobel foundation in 1961. After 1962 a gap of 16 years followed when prizes recommenced with an award of half a million Swiss francs to Mother Teresa. Award ceremonies alternate between Bern and the Accademia dei Lincei in Rome, and frequently winners have later won a Nobel Prize.\n\nThe amount of each of the four Balzan Prizes is now 750,000 Swiss francs (approx. €700,000; $790,000; £500,000).\n\nAll awards are decided by a single committee.\n\nFour prizes have been awarded annually since 1978. The award fields vary each year and can be related to either a specific or an interdisciplinary field. The prizes go beyond the traditional subjects both in the humanities (literature, the moral sciences and the arts) and in the sciences (medicine and the physical, mathematical and natural sciences), with an emphasis on innovative research.\n\nEvery 3 to 7 years the foundation also awards the \"Prize for humanity, peace and brotherhood among peoples\". It was last awarded in 2014 to Vivre en Famille.\n\n\n"}
{"id": "24759981", "url": "https://en.wikipedia.org/wiki?curid=24759981", "title": "Captive white tigers", "text": "Captive white tigers\n\nCaptive white tigers are of little known lineage. They are held captive around the world, usually for financial purposes. The Tiger Species Survival Plan devised by the Association of Zoos and Aquariums has condemned the breeding of white tigers. The genes responsible for white colour are represented by 0.001% of the tiger population. However, in 2008–2009, a closing stock of 264 Bengal tigers and 100 white Bengal tigers were accounted for in Indian zoos. The disproportionate growth in numbers of the latter points to the relentless inbreeding resorted to among homozygous recessive individuals for selectively multiplying the white animals. This progressively increasing process will eventually lead to inbreeding depression and loss of genetic variability.\n\nMohan was the founding father of the white tigers of Rewa. He was captured as a cub in 1951 by Maharaja of Rewa, whose hunting party in Bandhavgarh found a tigress with four 9-month-old cubs, one of which was white. All of them were shot except for the white cub. After shooting a white tiger in 1948 the Maharaja of Rewa had resolved to capture one, as his father had done in 1915, at his next opportunity. Water was used to lure the thirsty cub into a cage, after he returned to a kill made by his mother. The white cub mauled a man during the capture process and was clubbed on the head and knocked unconscious. He was not necessarily expected to wake up, and this was his second brush with death. He recovered though, and was housed in the unused palace at Govindgarh in the erstwhile harem courtyard. The Maharaja named him Mohan, which roughly translates as \"Enchanter\", one of the many names of the Hindu deity Krishna.The name of the Maharaja was Raja Martand Singh.\n\nThe white tiger the previous Maharaja had kept in captivity from 1915 to 1920 was also a male, unusually large like most white tigers (Mohan was no exception in this regard), and had a white male sibling still living in the wild. After the captive white tiger's death in 1920 he was mounted and presented to the Emperor King George V, as a token of loyalty. This specimen is now in the British Museum. The first live white tiger reached England in 1820, and was exhibited at London's Exeter Change menagerie where it was examined by the famous French anatomist Georges Cuvier, who described it in his \"Animal Kingdom\" as having faint stripes only visible from certain angles of refraction. In 1960 there was a mounted white tiger, with faint reddish brown stripes, in the throne room of the Maharaja of Rewa.\n\nIn 1953, Mohan was bred to a normal-coloured wild tigress called Begum (\"royal consort\"), which produced two male orange cubs on September 7, one of which went to Bombay Zoo. In 1955 they had a litter of two males and two females on April 10 (which included a male named Sampson and a female named Radha), all normal-coloured. On July 10, 1956 they again had a litter of two males and two females, which included a male named Sultan who went to Ahmedabad Zoo, and a female named Vindhya who went to the Delhi Zoo and was later bred to an unrelated male named Suraj. Once again, the breeding experiments failed to yield a single white cub.\n\nMohan was then bred to his daughter Radha (who carried the white gene inherited from her father) with success. The initial litter of four cubs—a male named Raja, and three females named Rani, Mohini, and Sukeshi—were the first white tigers born in captivity, on October 30, 1958. Raja and Rani went to the New Delhi Zoo, and Mohini was bought by the German-American billionaire John Kluge for $10,000, for the National Zoo in Washington D.C., as a gift to the children of America, in 1960.\n\nThe Government of India made a deal with the Maharaja, under the terms of which Raja and Rani would go to the New Delhi Zoo for free. In exchange the Maharaja's white tiger breeding would be subsidized and he would receive a share of their cubs. He wanted Rs 100,000 for them. Technically Sukeshi was also the property of the New Delhi Zoo, and in a sense India had nationalized the captive white tigers of Rewa. The Parliament of India would hear reports on the progress of the white tigers, and Prime Minister Indira Gandhi and U Nu of Burma participated in public christening ceremonies for white cubs at New Delhi Zoo. Sukeshi remained at Govindgarh Palace, in the harem courtyard where she was born, as a mate for Mohan.\n\nThat same year, India imposed a ban on the export of white tigers, in an effort to preserve a monopoly (as a tourist attraction), possibly because Anglo-Indian naturalist Edward Pritchard Gee recommended that Govindgarh Palace, and its white tiger inhabitants, be made a \"national trust\", which did not happen. Mohini was only allowed to leave India because US President Dwight D. Eisenhower intervened personally with Prime Minister Jawaharlal Nehru, to ask for the release of the United States government's white tiger. A white sister of Mohini's had been brought to New Delhi the year before to show the President, who was no stranger to white tigers. After the export ban was imposed the Maharaja threatened to release all of his white tigers into the Rewa forest, and so he was given dispensation to sell two more pairs abroad, to offset his costs.\n\nSix zoos acquired white tigers from the Maharaja of Rewa including the Bristol Zoo in England (a brother and sister pair named Champak and Chameli on June 22, 1963 for the equivalent of $10,000 each.) and the Crandon Park Zoo (which closed around 1983, and moved out of Crandon Park to the site of the Miami MetroZoo) in Miami acquired a white tigress in 1968. Bristol Zoo's pair, born in 1962, came from another litter of four, all white, but two (one female and one male) did not survive. Years later the Bristol Zoo needed a new breeding male and traded a white female to New Delhi Zoo for a white tiger named Roop, who had been named by U Nu, the Prime Minister of Burma. He was the son of Raja by his own mother and half sister- Radha, born in New Delhi. Radha, and many other tigers from Govindgarh including Sukeshi, were later transferred to New Delhi. Begum went to live at Ahmedabad Zoo and was bred to her son Sultan. They produced twelve cubs in four litters between 1958 and 1961. Bristol Zoo later transferred two male white tigers to Dudley Zoo.\n\nThe government of West Bengal bought two white males, named Niladari and Himadri, from the Maharaja for the Alipore Zoological Gardens (Calcutta Zoo), and an orange female named Malini, from the same litter of three born in 1960, accompanied them there. The Alipore Zoo in Kolkata, recovered the purchase price of its white tigers within six months by charging extra to see them. By 1966 the Bombay Zoo had a white tigress named Lakshmi, born in 1964, from the Maharaja. The Calcutta Zoo sold a white tigress named Sefali to Gauhati Zoo and sent a second white tiger there on loan. Circus owner Clyde Beatty also bought a white tiger from the Maharaja in 1960, for $10,000 in a deal facilitated by the Smithsonian National Zoo director T.H. Reed, who had traveled to India to escort Mohini to Washington, which had to be canceled because of the export ban, which made Mohini even more valuable. She was estimated to be worth $28,000. President Tito of Yugoslavia visited New Delhi Zoo and asked for white tigers for Belgrade Zoo, but was refused. A white tiger named Dalip from New Delhi Zoo represented India in two international expositions in Budapest and Osaka. A white tigress named Nandni, who was born in New Delhi Zoo in 1971, went to Hyderabad Zoo. By 1976 the Lucknow Zoo also had a white tiger which was a gift from New Delhi Zoo. Zoos with white tigers constituted a most exclusive club and the white tigers themselves represented a single extended family. In 1965 or 1966 Terence Walton, a member of the Maharaja of Rewa's staff, was attending a performance of the Ringling Bros. Circus in Madison Square Garden and had a note passed to tiger trainer Charles Baumann, on the Maharaja's stationary, requesting an opportunity to discuss white tigers. He may have hoped to make a sale. Baumann was invited to Rewa, but was not able to go.\n\nMohan was featured in the National Geographic documentary \"Great Zoos Of The World\" in 1970.\nHe died later that year, aged almost 20, and was laid to rest with Hindu rites as the palace staff observed official mourning. He was the last recorded white tiger born in the wild. The last white tiger seen in the wild was shot in 1958 in the Hazaribagh forests of Bihar. There have been rumors of white tigers in Hazaribagh, the Tora forsts of Rewa, and Kanha National Park since 1958, but these were not considered credible by K.S. Sankhala. A photograph of Mohan's stuffed head, in a display case in the private museum of the Maharaja of Rewa in Govindgarh Lake Palace, appears in the National Geographic book \"The Year Of The Tiger.\" Another picture of Mohan's head appears on the official website of the Maharaja of Rewa (MP).\n\nThe Maharaja of Rewa turned Mohan's native forest into the Bandhavgarh National Park, because he could not control the poaching. The Maharaja was negotiating the sale of a white male, named Virat, as late as 1976, when he died of enteritis. Virat was a son of Mohan and Sukeshi.\n\nAt Bandhavgarh visitors can stay at the White Tiger Lodge, which is the local version of Tiger Tops in Royal Chitwan in Nepal. Pushpraj Singh, the reigning Maharaja of Rewa, has asked students to sign a petition to ask the President of India to return at least two white tigers to Govindgarh Lake Palace, as a tourist attraction.\n\nMohini, a daughter of Mohan, was officially presented to President Eisenhower by John W. Kluge, in a ceremony at the White House on December 5, 1960, and went to live at the Lion House, in the National Zoo, in Rock Creek Park. A reporter for The New York Times described the meeting of Mohini and President Eisenhower: \"The President shied noticeably when the beast roared and leaped in his direction inside the traveling cage drawn up on the White House south driveway. An eloquent \"Well!\" was the President's only comment for the next few seconds.\" T.H. Reed, the director of the National Zoo, gave this description of Mohini: \"Her stripes were black, shading into brown, but her main coat was eggshell white instead of the normal rufous orange. Exotic coloring and magnificent physique made her a tiger without peer. For a two-year-old kitten she had tremendous growth-almost 190 pounds, three feet tall at the shoulders, and eight feet from nose to tail.\" White tigers are larger and heavier than regular orange tigers. The average length of a white tiger at birth is 53 cm, compared to 50 cm for a normal orange cub. Shoulder height is 17 cm (normal 12 cm), weight 1.37 kg (normal 1.25 kg). Dalip and Krishna, two white tigers at New Delhi Zoo, weighed 139 kg and 120 kg respectively, at two years of age. Ram and Jim, two normal colored tigers at the same zoo, weighed 106 kg and 119 kg, at the same age. Raja, the white tiger, had a shoulder height of 100 cm, at ten years of age, while Suraj, an orange tiger, had a shoulder height of only 90 cm, at 12 years of age, according to New Delhi Zoo director K.S. Sankhala. Ratna and Vindhya, orange tigresses \"from the white race\", who carried the white gene as a recessive (both were fathered by Mohan), were higher at the shoulder than average, measuring 87 and 88 cm, compared to a normal orange tigress named Asharfi, who measured 82 cm at the shoulder. White tigers also grow faster than orange tigers. This would have given them an advantage in the wild.\n\nFollowing Mohini's arrival in New York City from India, with National Zoo director T.H. Reed, she spent one night in the Bronx Zoo A reception was scheduled at the Explorer's Club, and Mohini was to appear on the children's television show \"Wonderama\", with big game hunter Ralph S. Scott, who had been instrumental in bringing her to America. Mohini was also scheduled to appear on television in Philadelphia and Washington D.C. On Dec. 7, 1960 a television special was aired titled \"White Tiger\", which was a film about Mohini's trip from India. (The birth of Mohini's first litter in 1964 was televised in a national special.) Mohini was exhibited for three days in the Philadelphia Zoo, before traveling on to Washington. Her name is the feminine of Mohan, and translates as \"Enchantress\". She was her father's namesake. She was a great attraction, and the zoo wanted to breed more white tigers. At the time, no more white tigers were being allowed out of India, so Mohini was mated to Sampson, her uncle and half brother, who was sent from Ahmedabad Zoo in 1963. (It seems probable that financial considerations may have also precluded Washington from acquiring a second white tiger as a mate for Mohini.) Sampson was donated to the National Zoo by Ralph S. Scott. Mohini was originally betrothed to an orange Bengal tiger named \"Mighty Mo\", who was captured in Central India in the forests of the Maharaja of Panna by Ralph S. Scott, and donated to the National Zoo on June 19, 1959. Today there is a Panna National Park. Unfortunately Mohini used to push Mighty Mo around. The original plan was to breed Mohini with an unrelated orange tiger, and then breed her to one or more of her male offspring, in the hope of producing white cubs. That was before Sampson arrived. Sampson fathered the first two of Mohini's four litters, which were born in 1964 and 1966. Mighty Mo and another tiger named \"Foa\" were given to the Pittsburgh Zoo in August 1966.\n\nAfter Sampson's death in 1966, at age 11 of kidney failure, Mohini was bred to her son Ramana, who was then the only male white gene carrier available. This resulted in the birth of a white daughter named Rewati on April 13, 1969 and a white son named Moni on Feb. 8, 1970. Moni died of a neurological disorder in 1971 at 16 months. Moni was to have undertaken a fund raising tour for Project Tiger. He was born in a litter of five, which included two white males and three orange females. One was stillborn and the mother crushed the others after three days. When Moni was a cub he was photographed with Mrs. Suharto, the wife of Indonesian President Suharto, when she visited the National Zoo. Rewati had an orange male littermate which died after two days. Ramana was born on July 1, 1964 and had two litter mates-a white male named Rajkumar, who was the first white tiger born in a zoo, and an orange female named Ramani. Both died of feline distemper despite having been vaccinated, at ten months of age. Rajkumar had a particularly nasty disposition. All of Mohini's cubs were named by the Indian Ambassador. At the time of his death, at only ten months of age, Rajkumar already weighed 175 pounds, and could hardly be called a cub. He was first named \"Charlie\" by one of his keepers, before the Indian Ambassador gave him his official name. The National Zoo planned to trade Rajkumar for a number of other animals. He was equal to ten zebras in value. The Smithsonian Institution stepped in and vetoed the plan, insisting that Rajkumar would remain a permanent resident of Washington D.C. Rajkumar was the only white tiger fathered by Sampson.\n\nThe birth of Mohini's first litter was televised in a national special. Mohini's orange daughter Kesari was born in 1966 with an orange female who was stillborn. It was even suggested, although probably not too seriously, that Indian Prime Minister Indira Gandhi be asked to bring a white tiger cub for the zoo, when she was scheduled to visit Washington in 1966. After Moni died in 1971 the National Zoo tried to acquire an orange tiger named Ram from Trivandrum Zoo, in southern India, as a mate for Mohini. Ram was her first cousin, a grandson of Mohan, and there was a 50% chance that he carried white genes. 25% of Ram's genes came from Mohan and 25% from Begum. 25% of Mohini's genes were from Begum and 75% from Mohan. Ram was a son of Vindhya and Suraj born on 23 IV 1965 at New Delhi Zoo, the same Ram discussed earlier. Two sisters of Ram, born on 22 Feb. 1967, went to the Romanshorn Zoo in Switzerland. In 1973 an Indochinese tiger (\"Panthera tigris corbetti\") named Poona, who was born at the Woodland Park Zoo in Seattle in 1962, was sent to Washington on a six-month breeding loan from the Brookfield Zoo and bred to Mohini and Kesari. (Poona would have been regarded as a Bengal tiger for the first two years of his life because the Indo-Chinese subspecies was not recognized until 1968.) Mohini did not conceive. Kesari produced six orange cubs, an extraordinary number, especially for a first litter, but only one survived, a female named Marvina. Kesari handed Marvina over to her keepers and kept the other five. Marvina was mistaken for male, and named Marvin which was changed to Marvina when it was discovered that he was a she. Washington Zoo keeper Art Cooper, who hand reared Marvina, observed that white tigers were the most obstinate cats in the zoo, and said that Marvina had a typical white tiger personality. (Poona also fathered litters by two other tigresses in Brookfield.) In 1974 Marvina, Ramana, and Kesari were sent to the Cincinnati Zoo and Botanical Garden, and Rewati and Mohini went to the Brookfield Zoo, to be boarded during renovations in Washington, until 1976.\nAs a fringe benefit of inbreeding the four cubs were pure-Bengal tigers, and they were the last registered Bengal tigers born in the United States. Ranjit, Bharat, Priya, Peela, and Rewati had inbreeding coefficients of 0.406. Ramana died in 1974 of a kidney infection and became a father for the last time posthumously.\n\nA white half sister of Mohini's, bred from Mohan and Sukishi, born on March 26, 1966, named Gomti and later renamed Princess, lived in the Crandon Park Zoo in Miami for almost three years, before she died of a viral infection at age five in December 1970. She arrived in Miami on January 13, 1968.\nMiami mayor Chuck Hall met the 22-month-old 350 lbs. white tigress at the airport and rode with her to the zoo. He wanted to call her Maya, the name suggested by the Maharaja, which translates as Princess. Ralph S. Scott, who paid $35,000 for her and gave her to the Zoological Society of Florida, preferred the name Princess. The Zoological Society of Florida loaned Princess to the Crandon Park Zoo. It was Ralph S. Scott, a famous big game hunter, who suggested to John W. Kluge that he buy a white tiger for the children of America. He had seen the white tigers in Govindgarh Palace while tiger hunting in India. The government of India wanted Princess to be the last white tiger exported from the country. A male white tiger, named Ravi, acquired by Ralph S. Scott for the Crandon Park Zoo died at Kanpur railway station en route from India in 1967. He was a son of Raja and Rani born in New Delhi Zoo, and sold by the Maharaja of Rewa. In 1970 Jimmy Stewart was on \"The Tonight Show Starring Johnny Carson\" and said that his wife was going to buy a white tiger from the Maharaja of Rewa for the Los Angeles Zoo. Ralph S. Scott was watching and felt as though he was being robbed. He had been trying to get a mate for Princess for years. A bidding war erupted between Scott, Jimmy Stewart, a major league baseball team, a Hollywood producer, and a major European zoo. Scott said of Princess \"It is cruel to expect an animal like that to live alone. And you can't mate her with an ordinary tiger-she's so superior...I appealed to the Maharaja from a conservation standpoint and it hit home.\" Princess and Rajah were to be a \"royal couple.\" The Los Angeles Zoo had already spent $20,000 building a white tiger exhibit. Scott said that he would try and send them a pair of cubs from Princess and Rajah, but Princess died a week before Rajah was scheduled to arrive. Scott hired an Indian taxidermist to stuff Princess, and she was presented to the Museum of Science in Miami in 1972, but she is now in the reception area of the Miami MetroZoo's administration building. Scott paid around $45,000 for Raja, who he thought might still be mated to Mohini, but Rajah never arrived in Crandon Park. Scott was so respected as a tiger hunter that he was asked to deal with man eaters which were terrorizing villages. He was a hunter turned conservationist, and a cat-lover. Mohini died in 1979. The skins and skulls of Mohini and Moni are in the Smithsonian, but are not on display.\n\nAn orange brother of Mohini's named Ramesh lived in the Ménagerie du Jardin des Plantes (Paris Zoo), and was bred to an unrelated tigress, but none of the offspring survived to reproduce. Ramesh was born in Govindgarh Palace and had an orange female littermate, named Ratna who went to New Delhi Zoo, and a white male littermate named Ramu. They were the fourth and last litter of Mohan and Radha. Ratna was paired with a wild caught male named Jim, at New Delhi Zoo, and produced three litters. Each cub would have had a 50% chance of inheriting the white gene from Ratna. Jim was captured in the Rewa forest, so they thought there was a chance he carried white genes. He had been somebody's pet, but after he ate a cat he was given to New Delhi Zoo. Jim used to appear leaping into his pond, at New Delhi Zoo, in the opening of one of Gerald Durrell's TV shows. Edward Pritchard Gee mentioned, in his book \"The Wildlife Of India\" (which has a foreword by Jawaharlal Nehru), that Bristol Zoo wanted to acquire one of the cubs of Mohan and Begum, as a mate for one of its white tigers, Champak or Chameli, to lessen the degree of inbreeding, as the US National Zoo had done with Sampson. The Bristol Zoo did acquire one of the daughters of Mohan and Begum. In 1987 Ranjit, Bharat, Priya, and Peela were sold to the International Animal Exchange. Ranjit, Priya, and Peela went to the IAE's facility in Grand Prairie, Texas. The phenomenon of spontaneous ovulation in a tiger was first observed by Devra Kleiman, in one of the white tigresses at the National Zoo, which meant that it was possible to breed tigers by artificial insemination.\nMohini died in 1979 at 20 years of age. Edwards Park wrote in Smithsonian magazine that National Zoo director Ted Reed was \"mourning his queen the late Mohini Rewa.\" Ted Reed said \"It's impossible to say how much the zoo owes that cat and her cubs. They drew attention to the facility and made all of our recent improvements so much easier. If she had been human she would have been a movie star.\"\n\nTony, born in July 1972 in the Circus Winter Quarters of the Cole Bros. Circus (the Terrell Jacobs farm) in Peru, Indiana, was the founder of many American white tiger lines, especially those used in circuses. His grandfather was a registered Siberian tiger, named Kubla, who was born at the Como Park, Zoo, and Conservatory in Saint Paul, Minnesota. Kubla's parents were born in the wild and believed to be brother and sister. Kubla was bred to a Bengal tigress named Susie, from a west coast zoo, at the Great Plains Zoo in Sioux Falls in South Dakota. She was once co-owned by Clyde Beatty. Between April 10, 1966 and August 3, 1969 Kubla and Susie produced 13 or 14 cubs in 5 or 6 litters. The cubs were widely distributed. One eventually reached Paris, and another went from the Utica Zoo in New York State to Japan. Two of their cubs (Rajah and Sheba II) were bred by Baron Julius Von Uhl, who lived in Peru, Indiana. Julius Von Uhl was born in Budapest and came to America in 1956 from Hungary after the revolution. One of the results of his tiger breeding was Tony, who therefore carried mixed blood He may have been the source of a gene for stripelessness. Kubla was also bred to an Amur tigress named Katrina, who was born at the Rotterdam Zoo, and passed through the hands of two American zoos before joining Kubla and Susie at the Great Plains Zoo. Kubla and Katrina have living pure-Amur descendants which may include a line of white tigers, that are claimed as pure-Amurs, which originated out of Center Hill, Florida. These white tigers are not registered Amur tigers. A tiger trainer named Alan Gold owned a pair of Amur tigers which once produced a stillborn white cub.\n\nIn 1972 there were four white tigers in the United States: Mohini and her daughter Rewati in Washington D.C., Tony, and his first cousin named Bagheera, a female born on July 8, 1972 in a litter of two white cubs, including a male which did not survive, in the Hawthorn Circus of John F. Cuneo Jr. Bagheera's mother, Sheba III, was a sister of Tony's mother, Sheba II. Bagheera's father was either an Amur tiger, named Ural, who was her preferred mate, and may have been her uncle and a littermate, or younger sibling, of Kubla, born at the Como Zoo; or one of two of her brothers, named Prince and Saber, who were also brothers to Tony's parents.\n\nMost of Sheba III's litters did not include white cubs, but at least 50% of her orange cubs would have been white gene carriers, since they could have inherited the gene from their mother, and if both parents were heterozygotes 66%, or two out of three, of their orange cubs are likely to have been carriers. She had 27 cubs in 9 litters between July 8, 1972 and July 26, 1975, of which only 3 were white, or 11%, not 25% as would be expected if both parents in each mating were heterozygotes. Prince was castrated before Sheba III conceived another white cub, a male named Frosty, born on Feb. 25, 1975, in a litter which included two orange females and one orange male. It seems odd that a tiger which may have been fathering such valuable cubs (Prince) would have been neutered. Saber was never observed trying to mate, so perhaps Ural did sire one or more of Sheba III's white cubs, which would have been three quarters Siberian had this been the case. It is possible for tigers from the same litter to have different fathers. It's also possible that any or all three tigers-Ural, Prince, and Saber, carried the white gene. Ural was a sad specimen. He was cross eyed, although he was not white. Bagheera and Frosty were both severely cross eyed.\n\nTony was purchased by John F. Cuneo Jr., owner of the Hawthorn Circus Corp. of Grayslake, Illinois, in February 1975 for $20,000 in Detroit. Tony's parents, Raja and Sheba, produced two more white cubs at the Baltimore County Fair on June 27, 1976. The cubs were a white male, named \"Baltimore County Fair\", a white female named \"Snowball\", and an orange male. National Zoo spokeswoman Sybille Hamlem said: \"This could be a real bonus for the breed if the two stay in the United States. The white tigers are no longer found in the wild and there have been genetic problems because of inbreeding. But that's apparently not the case here.\" Snowball's name was later changed to \"Maharani\", and all three cubs were sold to the Ringling Bros. Barnum & Bailey Circus in Washington D.C.. Maharani died in 1984. Baron Julius Von Uhl had another three white cubs born between June 18 and 19, 1977 at Kingdom's 3 (formerly Lion Country Safari) at Stockbridge, Georgia off I-75 south of Atlanta. Two lived only a short time. The other, named Scarlett O'Hara, died at the Grady Memorial Hospital's animal research clinic in Atlanta, on Jan. 30, 1978, of cardiac arrest resulting from anaesthesia. She was there to undergo surgery to correct crossed eyes. (She was only cross eyed in the right eye, which turned inward toward the nose.) She was still owned by Julius Von Uhl at the time. Tony was sent on breeding loan to the Cincinnati Zoo in 1976, to be bred to Rewati from the US National Zoo. However, Tony and Rewati did not breed, so he was bred to Mohini's orange daughter Kesari instead, resulting in a litter of four white and one orange cub June 27, 1976, the same day that eight-year-old Sheba had her white cubs in Baltimore, Maryland. It is an astounding coincidence that both tigresses gave birth to white cubs on exactly the same day. On that one day America's white tiger population nearly doubled from 8 to 14. Kesari's 1976 litter represented a mixture of the two unrelated strains.\n\nAll of the white cubs from Kesari's 1976 litter by Tony were cross-eyed, as were Rewati, Bagheera, and Frosty. The Cincinnati Zoo retained a brother and sister pair from the litter, named Bhim and Sumita, and their orange sister Kamala. Two white males returned to the Hawthorn Circus with Tony as John Cuneo's share from the breeding loan. John Cuneo also asked the Bristol Zoo to trade some white tigers, to diversify the gene pool, but the Bristol Zoo declined, perhaps not wishing to exchange pure-Bengals for mongrels. Tony, Bagheera, and Frosty lived for years with a troop of Hawthorn Circus tigers stationed at Marineland and Game Farm, in Niagara Falls, Ontario, Canada. Because of selective breeding only a few of the oldest white tigers in the Hawthorn Circus today are cross eyed. Bhim and Sumita became the world record parents of white cubs. In 1976 there were 39 white tigers-7 in New Delhi, 7 in Kolkata, one in Guwahati, one in Lucknow, one in Hyderabad, 8 in Bristol, Cincinnati Zoo had 2, Washington had 5, John Cuneo had 5, and Julius Von Uhl had 2. The Maharaja of Rewa retired from the white tiger business in 1976. He later abdicated in favor of his son so that he could run for the family seat in parliament and became an MP. There is a white tiger cub on the shield of the coat of arms of the Maharajas of Rewa.\n\nOver 70 white tigers have been born at the Cincinnati Zoo, which is no longer in the white tiger business. The Cincinnati Zoo sold white tigers for $60,000 each. Siegfried & Roy bought a litter of three white cubs from the Cincinnati Zoo, which were offspring of Bhim and Sumita, for around $125,000. Prior to 1974 the Cincinnati Zoo wanted to acquire a white tiger, but no zoo would sell at any price. By the 1980s the Cincinnati Zoo was the world's leading purveyor of white tigers. It was a cousin of the Maharaja of Rewa, Lt. Col. Fatesinghrao \"Jackie\" Gaekwad, the Maharaja of Baroda, who was also the Commissioner of Indian Wildlife and an MP, who suggested to Siegfried and Roy that they acquire white tigers from the Cincinnati Zoo and include them in their act.\"Jackie\" was also the President of the World Wildlife Fund India. In the mid 1980s Siegfried & Roy owned 10% of the world's white tigers, and they were escorting two big white tiger cubs, with dark stripes, to their new home in Phantasialand in Brühl, Germany, when the white tigers were briefly stolen with their truck in New York City. The driver stopped for coffee. The white tigers made their debut in Germany at a ceremony attended by the United States Ambassador.\n\nThe Henry Doorly Zoo in Omaha, Nebraska bought Tony's parents and orange sister Obie (born in 1975) in 1978, and bred more white tigers. Kesari also went to live at Omaha Zoo, but did not have any more cubs. Some of Tony's white siblings born in Omaha proved to be sterile. Obie was paired with Ranjit from the National Zoo, and their cubs like those of Tony and Kesari, included non inbred white tigers. A white tiger named Chester, who was a son of Ranjit and Obie, born at the Omaha Zoo, fathered the first test tube tigers, and then became the first white tiger in Australia when he was sent to the Taronga Zoo in Sydney. His brother, Panghur Ban, was the National Zoo's last white tiger. A white tiger named Rajiv, a son of Bhim, became the first white tiger in Africa, when he was sent to Pretoria Zoo in exchange for a king cheetah.\n\nIn 1983 Rewati was paired with Ika, from Kesari's 1976 litter, at the Columbus Zoo. By this time he was a three legged amputee retired from circus performance, put out to pasture to breed. Ika killed Rewati in the act of mating. Ika was then mated with a white tigress named Taj, who was a grand daughter of his brothers Ranjit and Bhim. Ika was also bred to Taj's orange mother Dolly, a daughter of Bhim and an unrelated orange tigress named Kimanthi, in Columbus. Taj's father, Duke, was a son of Ranjit from an outcross to an unrelated orange tigress. Isson, a white grandson of Kesari and Tony, was also dispatched to Columbus on breeding loan from the Hawthorn Circus, of Grayslake, Illinois, which eventually had 80 white tigers, the largest collection in the world at the time. In 1984 five white tiger cubs were stolen from the Hawthorn Circus in Portland, Oregon, and two died. The tigers were touring with the Ringling Bros. Barnum & Bailey Circus. The culprit was a veterinarian who was sentenced to one year in prison and six months in a halfway house. Cincinnati Zoo director Ed Maruska testified in the case that the five white cubs had a dollar value in excess of $5000.\n\nIn 1974 a white cub was born in the Racine Zoological Gardens in Wisconsin, from a father-daughter mating. The father, named Bucky, killed the white cub. The mother, named Bonnie, was later bred with an orange littermate of Tony named \"Chequila\", who belonged to James Witchey of Ravenna, Ohio, who bought him from Dick Hartman of South Lebanon, Ohio, when he was four or five years of age. Chequila proved to be a white gene carrier and fathered at least one white cub in the Racine Zoo in 1980. It is not known whether Bucky, who came from the Fort Wayne Children's Zoo in Indiana, and his daughter Bonnie, were related to any of the established strains of white tigers, but it is possible that Bucky was another one of the cubs of Kubla and Susie, born in Sioux Falls. By 1987 10% of North American zoo tigers were white.\n\nThree white tigers were also born in the Nandankanan Zoo in Bhubaneswar, Orissa, India in 1980. Their parents were an orange father–daughter pair called Deepak and Ganga, who were not related to Mohan or any other captive white tiger – one of their wild-caught ancestors would have carried the recessive white gene, and it showed up when Deepak was mated to his daughter. Deepak's sister also turned out to be a white gene carrier. These white tigers are therefore referred to as the Orissa strain, as opposed to the Rewa strain, of white tigers founded by Mohan.\n\nWhen the surprise birth of three white cubs occurred there was a white tigress already living at the zoo, named Diana, from New Delhi Zoo. One of the three was later bred to her creating another blend of two unrelated strains of white tigers. This lineage resulted in several white tigers in Nandan Kanan Zoo. Today the Nandankanan Zoo has the largest collection of white tigers in India. The Cincinnati Zoo acquired two female white tigers from the Nandan Kanan Zoo, in the hopes of establishing a line of pure-Bengal white tigers in America, but they never got a male, and did not receive authorization from the Association of Zoos and Aquariums (AZA)'s Species Survival Plan (SSP) to breed them. The Zoo Outreach Organisation used to publish studbooks for white tigers, which were compiled by A.K. Roychoudhury of the Bose Institute in Calcutta, and subsidized by the Humane Society of India. The Columbus Zoo had also hoped to breed pure-Bengal white tigers, but were unable to obtain a white registered Bengal mate for Rewati from India.\n\nThere were also surprise births of white tigers in the Asian Circus, in India, to parents not known to have been white gene carriers, or heterozygotes, and not known to have any relationship to any other white tiger strains. There was a female white cub born at Mysore Zoo in 1984, from orange parents, descended from Deepak's sister. The white cub's grandmother Thara came from the Nandankanan Zoo in 1972. Mysore Zoo had a second female white tiger cub from New Delhi Zoo in 1984. On August 29, 1979 a white tigress named Seema was dispatched to Kanpur Zoo to be bred to Badal, a tiger who was a fourth generation descendant of Mohan and Begum. The pair did not breed so it was decided to pair Seema with one of two wild caught, notorious man eaters, either Sheru or Titu, from the Jim Corbett National Park. Seema and Sheru produced a white cub, and for a while it was thought there might be white genes in Corbett's population of tigers, but the cub did not stay white.\n\nThere have been other cases of white tiger, white lion, and white panther cubs being born, and then changing to normal color. White tigers which were a mixture of the Rewa and Orissa strains, born at the Nandan Kanan Zoo, were non inbred. A white tiger from out of the Orissa strain found its way to the Western Plains Zoo in Australia. Australia's Dreamworld, on the Gold Coast, wanted to breed this tiger to one of their white tigers from the United States.\n\n"}
{"id": "24239277", "url": "https://en.wikipedia.org/wiki?curid=24239277", "title": "Cosmos (Humboldt)", "text": "Cosmos (Humboldt)\n\nCosmos (in German \"Kosmos – Entwurf einer physischen Weltbeschreibung\") is an influential treatise on science and nature written by the German scientist and explorer Alexander von Humboldt. \"Cosmos\" began as a lecture series delivered by Humboldt at the University of Berlin, and was published in five volumes between 1845 and 1862 (the fifth was posthumous and completed based on Humboldt's notes). In the first volume of \"Cosmos\", Humboldt paints a general “portrait of nature”, describing the physical nature of outer space and the earth. In the second volume he describes the history of science. Widely read by academics and laymen alike, it applied the ancient Greek view of the orderliness of the cosmos (the harmony of the universe) to the Earth, suggesting that universal laws applied as well to the apparent chaos of the terrestrial world. Humboldt goes on to suggest that when one contemplates the beauty of the cosmos, one can obtain personal inspiration and a beneficial, if subjective, awareness about life.\n\n\"Cosmos\" was influenced by Humboldt’s various travels and studies, but mainly by his journey throughout the Americas. As he wrote, “it was the discovery of America that planted the seed of the Cosmos.” Due to all of his experience in the field, Humboldt was preeminently qualified for the task to represent the universe in a single work. He had extensive knowledge of many fields of learning, varied experiences as a traveler, and the resources of the scientific and literary world at his disposal.\n\n\"Cosmos\" was highly popular when it was released, with the first volume selling out in two months, and the work translated into most European languages. Although the natural sciences have diverged from the romantic perspective Humboldt presented in \"Cosmos\", the work is still considered to be a substantial scientific and literary achievement, having influenced subsequent scientific progress and imparted a unifying perspective to the studies of science, nature, and mankind.\n\nSince the early years of the nineteenth century, Humboldt had been a world-famous figure, second in renown only to Napoleon. As the son of an aristocratic family in Prussia, he received the best education available at the time in Europe, studying under famous thinkers at the universities of Frankfurt and Göttingen. By the time he wrote \"Cosmos\", Humboldt was an esteemed explorer, cosmographer, biologist, diplomat, engineer, and citizen of the world. While considered a geographer, he is accredited with contributing to most of the sciences of the natural world environment found today.\n\nProbably more than any other factor, Humboldt's career was shaped by his travels in South and Central America in the five years from 1799 to 1804. Humboldt said that his Cosmos was born on the slopes of the Andes. Beginning in Venezuela, he explored the Orinoco and upper Amazon valleys, climbed Mount Chimborazo — then believed to be the world's highest mountain — investigated changing vegetation from the tropical jungles to the top of the Andes, collected thousands of plant specimens, and accumulated a vast collection of animals, insects, and geological fragments. From the notes he gathered on this journey, Humboldt was able to produce at least thirty volumes based on his observations. His studies related to many scientific fields, including botany, zoology, geology, and geography, as well as narratives of popular travel and discussions of political, economic, and social conditions.\n\nTwenty-five years after his exploration of the Americas, at the age of sixty, Humboldt undertook an extended tour, subsidized by the Tsar of Russia, into the interior of Asia. Between May and November 1829, Humboldt and his two subordinates, C.G. Ehrenberg and Gustav Rose, traveled across the vast expanse of the Russian empire. Upon his return, Humboldt left the publication of the scientific results to Ehrenberg and Rose, while his own work — a three-volume descriptive geography entitles \"Asie Centrale\" — did not appear until many years later. This work was very modest in comparison to Humboldt's South American publications. \"Asie Centrale\" focused on the facts and figures of Central Asian geography, along with data to complete his isothermal world map. It was during his South American and Asian explorations that Humboldt made the observations crucial to forming his physical description of the universe in \"Cosmos\".\n\nIn 1827, having spent himself into poverty publishing his scientific works, his king, Friedrich Wilhelm III, reminded Humboldt of his debt and recalled him to Berlin. When he arrived in Berlin, Humboldt announced that he would give a course of lectures on physical geography. From November 1827 to April 1828, he delivered a series of sixty-one lectures at the University of Berlin. The lectures were so well-attended, that Humboldt soon announced a second series, which was held in a music hall before an audience of thousands, free to all comers. Beginning in 1828, Humboldt finally gave expression to his concept in his Berlin lectures, and from then on he labored to produce his physical description of the universe in book form. Collaborators pledged to his assistance included the greatest scientists of his generation, including leaders in chemistry, astronomy, anatomy, mathematics, mineralogy, botany, and other areas of study.\n\nIn 1828 after the Berlin lectures, Humboldt began formulating his vision in writing. His factual text, heavily loaded with footnotes and references, was sent in proof sheets to all the various specialists for comments and corrections before publication. In this way, he aimed to ensure that what he wrote was both accurate and up-to-date. He continually looked to his friend and literary advisor Varnhagen von Ense for advice in the matter of his style of writing. In total \"Cosmos\" took twenty-five years to write.\n\nHumboldt viewed the world as what the ancient Greeks called a kosmos – “a beautifully ordered and harmonious system” – and coined the modern word “cosmos” to use as the title of his final work. This title allowed him to encompass heaven and earth together. He reintroduced Cosmos as “the assemblage of all things in heaven and earth, the universality of created things constituting the perceptible world.” His basic purpose is outlined in the introduction to the first volume:\n\nHumboldt soon adds that Cosmos signifies both the “order of the world, and adornment of this universal order.” Thus, there are two aspects of the Cosmos, the “order” and the “adornment.” The first refers to the observed fact that the physical universe, independently of humans, demonstrates regularities and patterns that we can define as laws. Adornment, however, is up to human interpretation. To Humboldt, Cosmos is both ordered and beautiful, through the human mind. He created a dynamic picture of the universe that would continually grow and change as human conceptions of nature and the depth of human feeling about nature enlarge and deepen.\n\nTo represent this double-sided aspect of Cosmos, Humboldt divided his book into two parts, with the first painting a general “portrait of nature.” Humboldt first examines outer space – the Milky Way, cosmic nebulae, and planets – and then proceeds to the earth and its physical geography; climate; volcanoes; relationships among plants, animals, and mankind; evolution; and the beauty of nature. In the second part, on the history of science, Humboldt aims to take the reader on an inner or “subjective” journey through the mind. Humboldt is concerned with “the difference of feeling excited by the contemplation of nature at different epochs,” that is, the attitudes toward natural phenomena among poets, painters, and students of nature through the ages. The final three volumes are devoted to a more detailed account of scientific studies in astronomy, the earth’s physical properties, and geological formations. On the whole, the final work followed the scheme of the Berlin lectures reasonably faithfully.\n\nIn the book Humboldt provided observations supporting the elevation crater theory of his friend Leopold von Buch. The theory in question intended to explain the origin of mountains and retained some popularity among geologists into the 1870s.\n\n\"Cosmos\" was considered to be both a scientific and literary achievement, immensely popular among nineteenth-century readers. Although the book bore the daunting subtitle of \"A Sketch of a Physical Description of the Universe\", and had an index that ran to more than 1,000 pages, the first volume sold out in two months, the work was translated into all major languages and sold hundreds of thousands of copies. Humboldt's publisher claimed: \"The demand is epoch-making. Book parcels destined for London and St. Petersburg were torn out of our hands by agents who wanted their orders filled for the bookstores in Vienna and Hamburg.\" \n\n\"Cosmos\" largely enhanced Humboldt's reputation in his own lifetime, not only in his own country but throughout Europe and America as well. Its enthusiastic reception in England, where it came out in the Bohn Scientific Library in a translation by Elizabeth Leeves, particularly surprised him. The reviews were gushing in praise of both the author and his work.\n\nHowever, some felt he had not done justice to the contribution of modern British scientists and many were quick to point out that Humboldt, who had written so exhaustively about the creation of the universe, failed to ever mention God the Creator.\n\nHumboldt's \"Cosmos\" had a significant impact on scientific progress, as well as various scientists and authors throughout Europe and America. Humboldt's work gave a strong impetus to scientific exploration throughout the nineteenth century, inspiring many, including Charles Darwin, who brought some of Humboldt's earlier writings with him on his voyage as the naturalist aboard the \"Beagle\" in the 1830s. Darwin called Humboldt \"the greatest scientific traveler who ever lived.\"\n\n\"Cosmos\" influenced several American authors, including Edgar Allan Poe, Walt Whitman, and Ralph Waldo Emerson. Emerson read Humboldt's work throughout his life, and for him, \"Cosmos\" capped Humboldt's role as a scientific revolutionary. Edgar Allan Poe was also an admirer of Humboldt, even dedicating his last major work, \"Eureka: A Prose Poem\", to Humboldt. Humboldt's attempt to unify the sciences was a major inspiration for Poe's work. Walt Whitman was said to have kept a copy of \"Cosmos\" on his desk for inspiration as he wrote \"Leaves of Grass\", and Henry David Thoreau's \"Walden\", like \"Eureka\", was a response to Humboldt's ideas.\n\nAlthough \"Cosmos\" and Humboldt's work in general had such a lasting impact on scientific thought, interest in Humboldt's life, work, and ideas over the past two centuries has dwindled in comparison to what it once was. However, starting in the 1990s and continuing to date, an upswing in scholarly interest in Humboldt has occurred. A new edition of \"Cosmos\" released in Germany in 2004 received avid reviews, renewing Humboldt's prominence in German society. German media outlets hailed the largely forgotten Humboldt as a new avatar figure for German national renewal and a model cosmopolitan ambassador of German culture and civilization for the twenty-first century.\n\nHumboldt is also credited with laying the foundations of physical geography, meteorology, and especially biogeography. His account in \"Cosmos\" of the propagation of seismic waves also became the basis of modern seismology. His most enduring contribution to scientific progress, however, in his conception of the unity of science, nature, and mankind. \"Cosmos\" showed nature as a whole, not as unconnected parts.\n\n\n\n"}
{"id": "3609312", "url": "https://en.wikipedia.org/wiki?curid=3609312", "title": "Crash Test Danny", "text": "Crash Test Danny\n\nCrash Test Danny was a series of 13 educational science sketch television shows for the Discovery Kids channel in the UK.\n\nDanny, played by Ben Langley, is a crash test dummy who goes the extra mile to put the fizz into physics. He is both motivated and hindered by the Professor, played by Gary Carpenter (who also co-wrote the program).\n\nThe shows were directed by Justin Rhodes, narrated by Jon Holmes, and series produced by Mark Robson at Initial Television.\n"}
{"id": "1751466", "url": "https://en.wikipedia.org/wiki?curid=1751466", "title": "Differential thermal analysis", "text": "Differential thermal analysis\n\nDifferential thermal analysis (or DTA) is a thermoanalytic technique that is similar to differential scanning calorimetry. In DTA, the material under study and an inert reference are made to undergo identical thermal cycles, (i.e., same cooling or heating programme) while recording any temperature difference between sample and reference. This differential temperature is then plotted against time, or against temperature (DTA curve, or thermogram). Changes in the sample, either exothermic or endothermic, can be detected relative to the inert reference. Thus, a DTA curve provides data on the transformations that have occurred, such as glass transitions, crystallization, melting and sublimation. The area under a DTA peak is the enthalpy change and is not affected by the heat capacity of the sample.\n\nA DTA consists of a sample holder, thermocouples, sample containers and a ceramic or metallic block; a furnace; a temperature programmer; and a recording system. The key feature is the existence of two thermocouples connected to a voltmeter. One thermocouple is placed in an inert material such as AlO, while the other is placed in a sample of the material under study. As the temperature is increased, there will be a brief deflection of the voltmeter if the sample is undergoing a phase transition. This occurs because the input of heat will raise the temperature of the inert substance, but be incorporated as latent heat in the material changing phase.\n\nIn today's market most manufacturers no longer make true DTA systems but rather have incorporated this technology into thermogravimetric analysis (TGA) systems, which provide both mass loss and thermal information. With today's advancements in software, even these instruments are being replaced by true TGA-DSC instruments that can provide the temperature and heat flow of the sample, simultaneously with mass loss. \n\nA DTA curve can be used only as a \"finger print\" for identification purposes but usually the applications of this method are the determination of phase diagrams, heat change measurements and decomposition in various atmospheres.\n\nDTA is widely used in the pharmaceutical and food industries.\n\nDTA may be used in cement chemistry, mineralogical research and in environmental studies.\n\nDTA curves may also be used to date bone remains or to study archaeological materials.\nUsing DTA one can obtain liquidus & solidus lines of phase diagrams.\n"}
{"id": "42022855", "url": "https://en.wikipedia.org/wiki?curid=42022855", "title": "Dissernet", "text": "Dissernet\n\nDissernet () is a volunteer community network working to clean Russian science of plagiarism. The core activity of the community is conducting examinations of doctoral and habilitation (higher doctorate) theses defended in Russian scientific and educational institutions since the end of the 1990s, and making the results of such examinations known to as many people as possible. The community is composed of professional scientists working in various fields of science both in Russia and abroad, and also journalists, civil activists and volunteers.\n\nLaunched in early 2013, the project had by 2016 identified around 5,600 suspected plagiarists — focusing on officials in government and academia, and other member the country's elite — and released reports on around 1,300 of them. Russian media regularly report on Dissernet's findings, and the site has been credited with raising attention for the issue of academic fraud in the country. In a 2016 exposé, Dissernet showed that 1 in 9 members of the State Duma had obtained academic degrees with theses that were substantially plagiarized and likely ghostwritten.\n\nThe objective of Dissernet’s examinations is to detect gross and deliberate violations of the legally established rules for certification of scientific workers’, as well as violations of the regulations for awarding academic degrees. Dissernet deems theses containing such violations to be fraudulent, and diplomas certifying the doctoral and higher doctoral degrees conferred after defending such theses to be illegal and subject to cancellation.\n\nThe key elements of the dissertation fraud detected in the course of Dissernet’s analyses are as follows:\n\n\nWithin the active phase of Dissernet’s activity, which started in January 2013, its members have published results of examinations of thesis defended by scores of well-known and powerful Russian politicians and public figures. As of the middle of the year 2014, more than 2000 completed expertises of Doctoral and higher doctoral theses in various directions and branches of science were contained in Dissernet database. Among the people attracting Dissernet’s attention were deputies of the State Duma, members of the Federation Council, governors of constituent entities of the Russian Federation, high-ranking officials of executive authorities, heads of security and military services, etc.\n\nThe work of the Dissernet community has gained much publicity and has been broadly covered in Russian and foreign mass media.\nThe result of the research performed by the Monitoring Center Public.Ru (more than 7500 federal and regional social and political and business printed matters, Internet mass media, news feeds of Press agencies, programs of central TV- and radio-channels have been analysed) showed that at 2013 year-end the word \"Dissernet\" was accepted as third by popularity neologism of the year, after the words \"Euromaidan\" and \"titushky\". Also the magazine \"Russian Reporter\" mentioned Dissernet’s work among the most notable milestones of the year 2013, and the Internet edition \"The Village\" published an article about Dissernet in the dictionary \"Summaries of 2013: main words and phrases of the outgoing year\".\n\nThe magazine \"Kommersant\", while listing in its concluding issue the major cultural and social events of the 2013, devoted a separate article to Dissernet’s struggle with falsifications and plagiarism.\n\nThe \"Dissernet Manifest\", which was developed and agreed upon in June–July 2013, contains the declaration of the organizers and members about the basic objectives, tasks and principles of joint activity of the community members:\n\n“Dissernet” is a networking community of experts, researchers and reporters seeking to unmask swindlers, forgers and liars. With their joint efforts based on the use of modern computer technologies and principles of networking division of labor, the community members oppose abusive practices, machinations and falsifications in the fields of scientific research and education, in particular in the process of defending theses and awarding academic degrees in Russia. The analytical work and disclosures of Dissernet equally cover various categories of Russian citizens:\n\n\nThe Dissernet members act on a voluntary basis, on their own initiative and without any outside pressure. The examinations, studies and publications of Dissernet are in strict compliance with the laws of the Russian Federation and do not violate any copyright or other rights. Dissernet is under no obligations to any state authorities, governmental\nor administrative institutions, political associations or movements, commercial corporations or entities. Its members work in the community irrespective of their political, corporate or any other views, they do not pursue any commercial purposes, their efforts are not aimed at advertising or promotion of any product or trademark, and they do not set any other goals except the above-mentioned ones. Dissernet’s expenses in connection with the fulfillment of its tasks are covered by means of voluntary donations by people sympathizing with its mission and sharing its views.\n\nThe community was established in January 2013. The full Dissernet site, dissernet.org, as well as its reduced version, dissernet.ru, were opened on 23 September 2013.\n\nOn 28 February 2014, Sergey Parkhomenko received the \"Golden Pen of Russia\" award from the Russian Union of Journalists for the year 2013, recognizing his \"Dissernet\" activity in press and in Internet.\n\nOn 24 April 2014, the jury of the PolitProsvet Award 2014 conferred the Dissernet project with two awards, \"For Honor and Dignity\", and \"People's Vote\".\n\nA number of prominent doctoral theses examinations published by the Dissernet community which dealt with well-known and powerful figures on the Russian political and scientific scene:\n\n\n\nAmong examinations performed by Dissernet, there is a number of monographs. In particular, examinations of the following books have been published:\n\n\nThe noted community activists, in particular, include:\n\n\n\n"}
{"id": "7991854", "url": "https://en.wikipedia.org/wiki?curid=7991854", "title": "Economic and Social Research Council", "text": "Economic and Social Research Council\n\nThe Economic and Social Research Council (ESRC) is one of the seven Research Councils in the United Kingdom. It receives most of its funding from the Department for Business, Energy and Industrial Strategy, and provides funding and support for research and training work in the social sciences and economics, such as postgraduate degrees.\n\nThe ESRC is based at Polaris House in Swindon, UK which is also the location of the head offices of several other UK Research Councils and Innovate UK. ESRC is part of United Kingdom Research and Innovation (UKRI), an organisation that brings together the UK’s research councils, Innovate UK and Research England. UKRI is a non-departmental public body funded by a grant-in-aid from the UK government. \n\nThe ESRC's mission, according to its website, is to:\n\n\nThe ESRC was founded in 1965 as the \"Social Science Research Council\" (SSRC - not to be confused with the Social Science Research Council in the United States). The establishment of a state funding body for the social sciences in the United Kingdom, had been under discussion since the Second World War; however, it was not until the 1964 election of Prime Minister Harold Wilson that the political climate for the creation of the SSRC became sufficiently favourable.\n\nThe first chief executive of the SSRC was Michael Young (later Baron Young of Dartington). Subsequent holders of the post have included Michael Posner, later Secretary General of the European Science Foundation. The current Chief Executive of the ESRC is Professor Jennifer Rubin who took over from Professor Jane Elliott in October 2017.\nFollowing the election of Prime Minister Margaret Thatcher in the 1979 general election, the Government expressed reservations about the value of research in the social sciences, and the extent to which it should be publicly funded. In 1981, the Education Secretary Sir Keith Joseph asked Lord Rothschild to lead a review into the future of the SSRC.\n\nIt was ultimately decided (due in no small part to the efforts of Michael Posner, chief executive of the SSRC at the time) that the Council should remain, but that its remit should be expanded beyond the social sciences, to include more 'empirical' research and research of 'more public concern'. To reflect this, in 1983 the SSRC was renamed the Economic and Social Research Council.\n\nChairman:\n\nChief Executive:\n\nExecutive Chair:\n\n"}
{"id": "940046", "url": "https://en.wikipedia.org/wiki?curid=940046", "title": "Electroacupuncture", "text": "Electroacupuncture\n\nElectroacupuncture is a form of acupuncture where a small electric current is passed between pairs of acupuncture needles. \nAccording to some acupuncturists, this practice augments the use of regular acupuncture, can restore health and well-being, and is particularly good for treating pain . There is evidence for some efficacy (when used in addition to antiemetics) in treating moderate post-chemotherapy vomiting, but not for acute vomiting or delayed nausea severity .\n\nAccording to \"Acupuncture Today\", a trade journal for acupuncturists:\n\nThat article adds:\n\nElectroacupuncture is also variously termed EA, electro-acupuncture or incorporated under the generic term electrotherapy.\n\n\"Electroacupuncture according to Voll\" (EAV) claims to measure \"energy\" in acupuncture points and to diagnose ailments. Some devices are registered in FDA as galvanic skin response measuring devices; they may not be used in diagnosis and treatment. Units reportedly sell for around $15,000 and are promoted for diagnosis of conditions including \"parasites, food and environmental sensitivities, candida, nutritional deficiencies and much more.\" It is promoted for diagnosis of allergies.\n\nThe Cochrane Collaboration, a group of evidence-based medicine (EBM) reviewers, reviewed eleven randomized controlled trials on the use of electroacupuncture at the P6 acupuncture point to control chemotherapy-induced nausea or vomiting. The reviewers found that electroacupuncture applied along with anti-vomiting drugs reduced first-day vomiting after chemotherapy more effectively than anti-vomiting drugs alone. However, the drugs given were not the most modern drugs available, so the reviewers stated that further research with state-of-the-art drugs was needed to determine clinical relevance. The reviewers concluded:\n\nThe Cochrane Collaboration also reviewed acupuncture and electroacupuncture for the treatment of rheumatoid arthritis. Because of the small number and poor quality of studies, they found no evidence to recommend its use for this condition. The reviewers concluded:\n\nA 2016 systematic review and meta-analysis found inconclusive evidence that electroacupuncture was effective for nausea and vomiting and hyperemesis gravidarum during pregnancy.\n\nRegarding EAV devices, \"results are not reproducible when subject to rigorous testing and do not correlate with clinical evidence of allergy\". There is no credible evidence of diagnostic capability. The American Cancer Society has concluded that the evidence does not support the use of EAV \"as a method that can diagnose, cure, or otherwise help people with cancer\" or \"as a reliable aid in diagnosis or treatment of .. other illness\" In double-blind trials, \"A wide variability of the measurements was found in most patients irrespective of their allergy status and of the substance tested. Allergic patients showed more negative skin electrical response at the second trial, compared to normal controls, independent of the tested substance. No significant difference in skin electrical response between allergens and negative controls could be detected.\"\n\nResearchers at the U.S. Food and Drug Administration (FDA) Center for Devices and Radiological Health (Rockville, Maryland) evaluated three representative devices intended for electrostimulation of acupuncture needles. The abstract at PubMed summarizes their findings:\n\n\n"}
{"id": "17375183", "url": "https://en.wikipedia.org/wiki?curid=17375183", "title": "Floodplain restoration", "text": "Floodplain restoration\n\nFloodplain restoration is the process of fully or partially restoring a river's floodplain to its original conditions before having been affected by the construction of levees (dikes) and the draining of wetlands and marshes. \n\nThe objectives of restoring floodplains include the reduction of the incidence of floods, the provision of habitats for aquatic species, the improvement of water quality and the increased recharge of groundwater.\n\nIn Europe very few schemes for restoring functional floodplains have been put in practice so far, despite a surge of interest in the topic among policy and research circles. One of the drivers for floodplain restoration is the EU Water Framework Directive. Early floodplain restoration schemes were undertaken in the mid-1990s in the Rheinvorland-Süd on the Upper Rhine, the Bourret on the Garonne, and as part of the Long Eau project in England. Ongoing schemes in 2007 include Lenzen on the Elbe, La Basse on the Seine and the Parrett Catchment Project in England. On the Elbe River near Lenzen (Brandenburg) 420 hectares of floodplain were restored in order to prevent a recurrence of the Elbe floods of 2002. A total of 20 floodplain restoration projects on the Elbe River were envisaged after the 2002 floods, but only two have been implemented as of 2009 according to the environmental group .\n\nIn the United States examples of floodplain restoration can be found in the catchment area of the Chesapeake Bay in Maryland, in the Emiquon Preserve on the Illinois River, in Charlotte, North Carolina and along the Baraboo River in Wisconsin.\n\n"}
{"id": "1950766", "url": "https://en.wikipedia.org/wiki?curid=1950766", "title": "Graph isomorphism problem", "text": "Graph isomorphism problem\n\nThe graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic.\n\nThe problem is not known to be solvable in polynomial time nor to be NP-complete, and therefore may be in the computational complexity class NP-intermediate. It is known that the graph isomorphism problem is in the low hierarchy of class NP, which implies that it is not NP-complete unless the polynomial time hierarchy collapses to its second level. At the same time, isomorphism for many special classes of graphs can be solved in polynomial time, and in practice graph isomorphism can often be solved efficiently.\n\nThis problem is a special case of the subgraph isomorphism problem, which asks whether a given graph \"G\" contains a subgraph that is isomorphic to another given graph \"H\" and which is known to be NP-complete. It is also known to be a special case of the non-abelian hidden subgroup problem over the symmetric group.\n\nIn the area of image recognition it is known as the exact graph matching.\n\nThe best currently accepted theoretical algorithm is due to , and is based on the earlier work by combined with a \"subfactorial\" algorithm of V. N. Zemlyachenko . The algorithm has run time 2 for graphs with \"n\" vertices and relies on the classification of finite simple groups. Without CFSG, a slightly weaker bound \n\nIn November 2015, Babai announced a quasipolynomial time algorithm for all graphs, that is, one with running time formula_1 for some fixed formula_2. On January 4, 2017, Babai retracted the quasi-polynomial claim and stated a sub-exponential time bound instead after Harald Helfgott discovered a flaw in the proof. On January 9, 2017, Babai announced a correction (published in full on January 19) and restored the quasi-polynomial claim, with Helfgott confirming the fix. Helfgott further claims that one can take , so the running time is . The new proof has not been fully peer-reviewed yet.\n\nThere are several competing practical algorithms for graph isomorphism, such as those due to , , and . While they seem to perform well on random graphs, a major drawback of these algorithms is their exponential time performance in the worst case.\n\nThe graph isomorphism problem is computationally equivalent to the problem of computing the automorphism group of a graph, and is weaker than the permutation group isomorphism problem and the permutation group intersection problem. For the latter two problems, obtained complexity bounds similar to that for graph isomorphism.\n\nA number of important special cases of the graph isomorphism problem have efficient, polynomial-time solutions:\n\nSince the graph isomorphism problem is neither known to be NP-complete nor known to be tractable, researchers have sought to gain insight into the problem by defining a new class GI, the set of problems with a polynomial-time Turing reduction to the graph isomorphism problem. If in fact the graph isomorphism problem is solvable in polynomial time, GI would equal P.\n\nAs is common for complexity classes within the polynomial time hierarchy, a problem is called GI-hard if there is a polynomial-time Turing reduction from any problem in GI to that problem, i.e., a polynomial-time solution to a GI-hard problem would yield a polynomial-time solution to the graph isomorphism problem (and so all problems in GI). A problem formula_3 is called complete for GI, or GI-complete, if it is both GI-hard and a polynomial-time solution to the GI problem would yield a polynomial-time solution to formula_3.\n\nThe graph isomorphism problem is contained in both NP and co-AM. GI is contained in and low for Parity P, as well as contained in the potentially much smaller class SPP. That it lies in Parity P means that the graph isomorphism problem is no harder than determining whether a polynomial-time nondeterministic Turing machine has an even or odd number of accepting paths. GI is also contained in and low for ZPP. This essentially means that an efficient Las Vegas algorithm with access to an NP oracle can solve graph isomorphism so easily that it gains no power from being given the ability to do so in constant time.\n\nThere are a number of classes of mathematical objects for which the problem of isomorphism is a GI-complete problem. A number of them are graphs endowed with additional properties or restrictions:\nA class of graphs is called GI-complete if recognition of isomorphism for graphs from this subclass is a GI-complete problem. The following classes are GI-complete:\n\nMany classes of digraphs are also GI-complete.\n\nThere are other nontrivial GI-complete problems in addition to isomorphism problems.\n\n\n have shown a probabilistic checker for programs for graph isomorphism. Suppose \"P\" is a claimed polynomial-time procedure that checks if two graphs are isomorphic, but it is not trusted. To check if \"G\" and \"H\" are isomorphic:\n\n\nThis procedure is polynomial-time and gives the correct answer if \"P\" is a correct program for graph isomorphism. If \"P\" is not a correct program, but answers correctly on \"G\" and \"H\", the checker will either give the correct answer, or detect invalid behaviour of \"P\".\nIf \"P\" is not a correct program, and answers incorrectly on \"G\" and \"H\", the checker will detect invalid behaviour of \"P\" with high probability, or answer wrong with probability 2.\n\nNotably, \"P\" is used only as a blackbox.\n\nGraphs are commonly used to encode structural information in many fields, including computer vision and pattern recognition, and graph matching, i.e., identification of similarities between graphs, is an important tools in these areas. In these areas graph isomorphism problem is known as the exact graph matching.\n\nIn cheminformatics and in mathematical chemistry, graph isomorphism testing is used to identify a chemical compound within a chemical database. Also, in organic mathematical chemistry graph isomorphism testing is useful for generation of molecular graphs and for computer synthesis.\n\nChemical database search is an example of graphical data mining, where the graph canonization approach is often used. In particular, a number of identifiers for chemical substances, such as SMILES and InChI, designed to provide a standard and human-readable way to encode molecular information and to facilitate the search for such information in databases and on the web, use canonization step in their computation, which is essentially the canonization of the graph which represents the molecule.\n\nIn electronic design automation graph isomorphism is the basis of the Layout Versus Schematic (LVS) circuit design step, which is a verification whether the electric circuits represented by a circuit schematic and an integrated circuit layout are the same.\n\n\n\n\n"}
{"id": "13945403", "url": "https://en.wikipedia.org/wiki?curid=13945403", "title": "Horizons: Exploring the Universe", "text": "Horizons: Exploring the Universe\n\nHorizons: Exploring the Universe is an astronomy textbook that was written by Michael A. Seeds and Dana E. Backman. It is in its 13th edition (), and is used in some colleges as a guide book for introductory astronomy classes. It covers all major ideas in astronomy, from the apparent magnitude scale, to the Cosmic Microwave Background Radiation, to gamma ray bursts.\n"}
{"id": "386398", "url": "https://en.wikipedia.org/wiki?curid=386398", "title": "Hypothetico-deductive model", "text": "Hypothetico-deductive model\n\nThe hypothetico-deductive model or method is a proposed description of scientific method. According to it, scientific inquiry proceeds by formulating a hypothesis in a form that can be falsifiable, using a test on observable data where the outcome is not yet known. A test outcome that could have and does run contrary to predictions of the hypothesis is taken as a falsification of the hypothesis. A test outcome that could have, but does not run contrary to the hypothesis corroborates the theory. It is then proposed to compare the explanatory value of competing hypotheses by testing how stringently they are corroborated by their predictions.\n\nOne example of an algorithmic statement of the hypothetico-deductive method is as follows:\n\nOne possible sequence in this model would be 1, 2, 3, 4. If the outcome of 4 holds, and 3 is not yet disproven, you may continue with 3, 4, 1, and so forth; but if the outcome of 4 shows 3 to be false, you will have to go back to 2 and try to invent a new 2, deduce a new 3, look for 4, and so forth.\n\nNote that this method can never absolutely verify (prove the truth of) 2. It can only falsify 2. (This is what Einstein meant when he said, \"No amount of experimentation can ever prove me right; a single experiment can prove me wrong.\")\n\nAdditionally, as pointed out by Carl Hempel (1905–1997), this simple view of the scientific method is incomplete; a conjecture can also incorporate probabilities, e.g., the drug is effective about 70% of the time. Tests, in this case, must be repeated to substantiate the conjecture (in particular, the probabilities). In this and other cases, we can quantify a probability for our confidence in the conjecture itself and then apply a Bayesian analysis, with each experimental result shifting the probability either up or down. Bayes' theorem shows that the probability will never reach exactly 0 or 100% (no absolute certainty in either direction), but it can still get very close to either extreme. See also confirmation holism.\n\nQualification of corroborating evidence is sometimes raised as philosophically problematic. The raven paradox is a famous example. The hypothesis that 'all ravens are black' would appear to be corroborated by observations of only black ravens. However, 'all ravens are black' is logically equivalent to 'all non-black things are non-ravens' (this is the contraposition form of the original implication). 'This is a green tree' is an observation of a non-black thing that is a non-raven and therefore corroborates 'all non-black things are non-ravens'. It appears to follow that the observation 'this is a green tree' is corroborating evidence for the hypothesis 'all ravens are black'. Attempted resolutions may distinguish:\n\nEvidence contrary to a hypothesis is itself philosophically problematic. Such evidence is called a falsification of the hypothesis. However, under the theory of confirmation holism it is always possible to save a given hypothesis from falsification. This is so because any falsifying observation is embedded in a theoretical background, which can be modified in order to save the hypothesis. Popper acknowledged this but maintained that a critical approach respecting methodological rules that avoided such \"immunizing stratagems\" is conducive to the progress of science.\n\nPhysicist Sean Carroll claims the model ignores underdetermination.\nThe hypothetico-deductive model (or approach) versus other research models\n\nThe hypothetico-deductive approach contrasts with other research models such as the inductive approach or grounded theory. In the data percolation methodology, \nthe hypothetico-deductive approach is included in a paradigm of pragmatism by which four types of relations between the variables can exist: descriptive, of influence, longitudinal or causal. The variables are classified in two groups, structural and functional, a classification that drives the formulation of hypotheses and the statistical tests to be performed on the data so as to increase the efficiency of the research. \n\n"}
{"id": "3813250", "url": "https://en.wikipedia.org/wiki?curid=3813250", "title": "Introduction to Automata Theory, Languages, and Computation", "text": "Introduction to Automata Theory, Languages, and Computation\n\nIntroduction to Automata Theory, Languages, and Computation is an influential computer science textbook by John Hopcroft and Jeffrey Ullman on formal languages and the theory of computation. Rajeev Motwani contributed to the 2000, and later, edition.\n\nAmong experts also known as the Cinderella Book. This nickname is derived from a girl (putatively Cinderella) on the cover with a Rube Goldberg machine.\n\nThe forerunner of this book appeared under the title Formal Languages and Their Relation to Automata in 1968. Forming a basis both for the creation of courses on the topic, as well as for further research, that book shaped the field of automata theory for over a decade, cf. (Hopcroft 1989).\n\nThe first edition of \"Introduction to Automata Theory, Languages, and Computation\" was published in 1979, the second edition in November 2000, and the third edition appeared in February 2006. Since the second edition, Rajeev Motwani has joined Hopcroft and Ullman as third author. \nStarting with the second edition, the book features extended coverage of examples where automata theory is applied, whereas large parts of more advanced theory were taken out. While this makes the second and third editions more accessible to beginners, it makes it less suited for more advanced courses. The new bias away from theory is not seen positive by all: As Shallit quotes one professor, \"they have removed all good parts.\" (Shallit 2008).\n\nThe first edition in turn constituted a major revision of a previous textbook also written by Hopcroft and Ullman, entitled \"Formal Languages and Their Relation to Automata\". It was published in 1968 and is referred to in the introduction of the 1979 edition. \nIn a personal historical note regarding the 1968 book, Hopcroft states: \"Perhaps the success of the book came from our efforts to present the essence of each proof before actually giving the proof\" (Hopcroft 1989). Compared with the forerunner book, the 1979 edition was expanded, and the material was reworked to make it more accessible to students, cf. (Hopcroft 1989).\nThis gearing towards understandability at the price of succinctness was not seen positive by all. As Hopcroft reports on feedback to the overhauled 1979 edition: \"It seems that our attempts to lower the level of our presentation for the benefit of students by including more detail and explanations had an adverse effect on the faculty, who then had to sift through the added material to outline and prepare their lectures\" (Hopcroft 1989).\n\nStill, the most cited edition of the book is apparently the 1979 edition: According to the website CiteSeerX, \nover 3000 scientific papers freely available online cite this edition of the book (CiteSeerX, 2009).\n\n\n\n"}
{"id": "12984160", "url": "https://en.wikipedia.org/wiki?curid=12984160", "title": "Joseph Finnegan (cryptographer)", "text": "Joseph Finnegan (cryptographer)\n\nJoseph Finnegan (April 12, 1905 – September 8, 1980) was an American linguist and cryptanalyst with Station Hypo during the Second World War.\n\nIn 2002, Tex Biard described Finnegan as \"intuitive\" and \"brilliant\", and second only to Station Hypo chief Joseph J. Rochefort, saying that Finnegan's survival of the bombing of the \"USS Tennessee\" was a \"fatal mistake\" on the part of the Japanese, and that Finnegan's survival \"cost (the Japanese) the war.\"\n\nEdwin T. Layton, in his book \"And I Was There: Pearl Harbor and Midway -- Breaking the Secrets\" (1985) recounts a tremendous effort by Finnegan on the Hypo team concerning the exact date on which the attack on Midway would occur. This involved the date-time groups in Japanese naval messages.\n\nLayton refers to the date-time data as being “superenciphered,” meaning that this data was preencoded even before it was added to the JN-25 cipher. When Hypo made their all-out effort to crack this, they started by searching the stacks of printouts and punched cards for five-digit number sequences. Those they found were in low grade codes, a poor starting point, but a starting point.\n\nLayton describes this method as \"involving a 12 x 31 (12 rows for months, 31 columns for day) garble check. The 31 kana [Japanese syllabic scripts] of the first row were A, I, U, E, O, KA, KI …………….HA, HI, HU, HE, HO. The second row was I, U, E, O ……………HE, HO, A; the third, U, E, O ……….HO, A, I, and so on, for 12 rows. At the left, representing the 12 months, was a column of 12 kana, different from those in the table – SA, AI, SU, SE, SO, TA, TI, TU, TE, TO, NA, NI (SA for January, NI for December). To encipher, for example 27 May, one picked the 5th line (May=SO), ran across to the twenty-seventh column, HA, and recorded the kana at that intersection, HO. The encipherment, then, was SO, HA, HO, the third kana providing the garble check.\" (Layton, pp. 427–428)\n\n"}
{"id": "53913187", "url": "https://en.wikipedia.org/wiki?curid=53913187", "title": "Land change modeling", "text": "Land change modeling\n\nLand change models (LCMs) describe, project, and explain changes in and the dynamics of land use and land-cover. LCMs are a means of understanding ways that humans are changing the Earth's surface in the past, present, in forecasting land change into the future.\n\nLand change models are valuable in development policy, helping guide more appropriate decisions for resource management and the natural environment at a variety of scales ranging from a small piece of land to the entire spatial extent. Moreover, developments within land-cover, environmental and socio-economic data (as well as within technological infrastructures) have increased opportunities for land change modeling to help support and influence decisions that affect human-environment systems, as national and international attention increasingly focuses on issues of global climate change and sustainability.\n\nThese are key concepts and various other terminologies necessary to understand the topic of land change modeling. \n\nChanges in land systems have consequences for climate and environmental change on every scale. Therefore, decisions and policies in relation to land systems are very important for reacting these changes and working towards a more sustainable society and planet.\n\nLand change models are significant in their ability to help guide the land systems to positive societal and environmental outcomes at a time when attention to changes across land systems is increasing.\n\nA plethora of science and practitioner communities have been able to advance the amount and quality of data in land change modeling in the past few decades. That has influenced the development of methods and technologies in model land change. The multitudes of land change models that have been developed are significant in their ability to address land system change and useful in various science and practitioner communities.\n\nFor the science community, land change models are important in their ability to test theories and concepts of land change and its connections to human-environment relationships, as well as explore how these dynamics will change future land systems without real-world observation.\n\nLand change modeling is useful to explore spatial land systems, uses, and covers. Land change modeling can account for complexity within dynamics of land use and land cover by linking with climactic, ecological, biogeochemical, biogeophysical and socioeconomic models. Additionally, LCMs are able to produce spatially explicit outcomes according to the type and complexity within the land system dynamics within the spatial extent. Many biophysical and socioeconomic variables influence and produce a variety of outcomes in land change modeling.\n\nA notable property of all land change models is that they have some irreducible level of uncertainty in the model structure, parameter values, and/or input data. For instance, one uncertainty within land change models is a result from temporal non-stationarity that exists in land change processes, so the further into the future the model is applied, the more uncertain it is. Another uncertainty within land change models are data and parameter uncertainties within physical principles (i.e., surface typology), which leads to uncertainties in being able to understand and predict physical processes.\n\nFurthermore, land change model design are a product of both decision-making and physical processes. Human-induced impact on the socio-economic and ecological environment is important to take into account, as it is constantly changing land cover and sometimes model uncertainty. To avoid model uncertainty and interpret model outputs more accurately, a model diagnosis is used to understand more about the connections between land change models and the actual land system of the spatial extent. The overall importance of model diagnosis with model uncertainty issues is its ability to assess how interacting processes and the landscape are represented, as well as the uncertainty within the landscape and its processes.\n\nA machine-learning approach uses land-cover data from the past to try to assess how land will change in the future, and works best with large datasets. There are multiple types of machine-learning and statistical models - a study in western Mexico from 2011 found that results from two outwardly similar models were considerably different, as one used a neural network and the other used a simple weights-of-evidence model.\n\nA cellular land change model uses maps of suitability for various types of land use, and compares areas that are immediately adjacent to one another to project changes into the future. Variations in the scale of cells in a cellular model can have significant impacts on model outputs.\n\nEconomic models are built on principles of supply and demand. They use mathematical parameters in order to predict what land types will be desired and which will be discarded. These are frequently built for urban areas, such as a 2003 study of the highly dense Pearl River Delta in southern China.\n\nAgent-based models try to simulate the behavior of many individuals making independent choices, and then see how those choices affect the landscape as a whole. Agent-based modeling can be complex - for instance, a 2005 study combined an agent-based model with computer-based genetic programming to explore land change in the Yucatan peninsula of Mexico.\n\nMany models do not limit themselves to one of the approaches above - they may combine several in order to develop a fully comprehensive and accurate model.\n\nScientists use LCMs to build and test theories in land change modeling for a variety of human and environmental dynamics. Land change modeling has a variety of implementation opportunities in many science and practice disciplines, such as in decision-making, policy, and in real-world application in public and private domains. The science disciplines use LCMs to formalize and test land change theory, and the explore and experiment with different scenarios of land change modeling. The practical disciplines use LCMs to analyze current land change trends and explore future outcomes from policies or actions in order to set appropriate guidelines, limits and principles for policy and action. Research and practitioner communities may study land change to address topics related to land-climate interactions, water quantity and quality, food and fiber production, and urbanization, infrastructure, and the built environment.\n\nOne improvement for land change modeling can be made through better data and integration with available data and models. Improved observational data can influence modeling quality. Finer spatial and temporal resolution data that can integrate with socioeconomic and biogeophysical data can help land change modeling couple the socioeconomic and biogeological modeling types. Land change modelers should value data at finer scales. Fine data can give a better conceptual understanding of underlying constructs of the model and capture additional dimensions of land use. It is important to maintain the temporal and spatial continuity of data from airborne-based and survey-based observation through constellations of smaller satellite coverage, image processing algorithms, and other new data to link satellite-based land use information and land management information. It is also important to have better information on land change actors and their beliefs, preferences, and behaviors to improve the predictive ability of models and evaluate the consequences of alternative policies.\n\nOne important improvement for land change modeling can be made though better aligning model choices with model goals. It is important to choose the appropriate modeling approach based on the scientific and application contexts of the specific study of interest. For example, when someone needs to design a model with policy and policy actors in mind, they may choose an agent-based model. Here, structural economic or agent-based approaches are useful, but specific patterns and trends in land change as with many ecological systems may not be as useful. When one needs to grasp the early stages of problem identification, and thus needs to understand the scientific patterns and trend of land change, machine learning and cellular approaches are useful.\n\nLand Change Modeling should also better integrate positive and normative approaches to explanation and prediction based on evidence-based accounts of land systems. It should also integrate optimization approaches to explore the outcomes that are the most beneficial and the processes that might produce those outcomes.\n\nIt is important to better integrate data across scales. A models design is based on the dominant processes and data from a specific scale of application and spatial extent. Cross-scale dynamics and feedbacks between temporal and spatial scales influences the patterns and processes of the model. Process like tele-coupling, indirect land use change, and adaption to climate change at multiple scales requires better representation by cross-scale dynamics. Implementing these processes will require a better understanding of feedback mechanisms across scales.\n\nAs there is continuous reinvention of modeling environments, frameworks, and platforms, land change modeling can improve from better research infrastructure support. For example, model and software infrastructure development can help avoid duplication of initiatives by land change modeling community members, co-learn about land change modeling, and integrate models to evaluate impacts of land change. Better data infrastructure can provide more data resources to support compilation, curation, and comparison of heterogeneous data sources. Better community modeling and governance can advance decision-making and modeling capabilities within a community with specific and achievable goals. Community modeling and governance would provide a step towards reaching community agreement on specific goals to move modeling and data capabilities forward.\n\nA number of modern challenges in land change modeling can potentially be addressed through contemporary advances in cyberinfrastructure such as crowd-source, “mining” for distributed data, and improving high-performance computing. Because it is important for modelers to find more data to better construct, calibrate, and validate structural models, the ability to analyze large amount of data on individual behaviors is helpful. For example, modelers can find point-of-sales data on individual purchases by consumers and internet activities that reveal social networks. However, some issues of privacy and propriety for crowdsourcing improvements have not yet been resolved.\n\nThe land change modeling community can also benefit from Global Positioning System and Internet-enabled mobile device data distribution. Combining various structural-based data-collecting methods can improve the availability of microdata and the diversity of people that see the findings and outcomes of land change modeling projects. For example, citizen-contributed data supported the implementation of Ushahidi in Haiti after the 2010 earthquake, helping at least 4,000 disaster events. Universities, non-profit agencies, and volunteers are needed to collect information on events like this to make positive outcomes and improvements in land change modeling and land change modeling applications. Tools such as mobile devices are available to make it easier for participants to participate in collecting micro-data on agents. Google Maps uses cloud-based mapping technologies with datasets that are co-produced by the public and scientists. Examples in agriculture such as coffee farmers in Avaaj Otalo showed use of mobile phones for collecting information and as an interactive voice.\n\nCyberinfrastructure developments may also increase the ability of land change modeling to meet computational demands of various modeling approaches given increasing data volumes and certain expected model interactions. For example, improving the development of processors, data storage, network bandwidth, and coupling land change and environmental process models at high resolution.\n\nAn additional way to improve land change modeling is through improvement of model evaluation approaches. Improvement in sensitivity analysis is needed to gain a better understand of the variation in model output in response to model elements like input data, model parameters, initial conditions, boundary conditions, and model structure. Improvement in pattern validation can help land change modelers make comparisons between model outputs parameterized for some historic case, like maps, and observations for that case. Improvement in uncertainty sources is needed to improve forecasting of future states that are non-stationary in processes, input variables, and boundary conditions. One can explicitly recognize stationarity assumptions and explore data for evidence in non-stationarity to better acknowledge and understand model uncertainty to improve uncertainty sources. Improvement in structural validation can help improve acknowledgement and understanding of the processes in the model and the processes operating in the real world through a combination of qualitative and quantitative measures.\n\n"}
{"id": "27734797", "url": "https://en.wikipedia.org/wiki?curid=27734797", "title": "List of Catholic clergy scientists", "text": "List of Catholic clergy scientists\n\nThis is a list of Catholic churchmen throughout history who have made contributions to science. These churchmen-scientists include Nicolaus Copernicus, Gregor Mendel, Georges Lemaître, Albertus Magnus, Roger Bacon, Pierre Gassendi, Roger Joseph Boscovich, Marin Mersenne, Bernard Bolzano, Francesco Maria Grimaldi, Nicole Oresme, Jean Buridan, Robert Grosseteste, Christopher Clavius, Nicolas Steno, Athanasius Kircher, Giovanni Battista Riccioli, William of Ockham, and others listed below. The Catholic Church has also produced many lay scientists and mathematicians.\n\nThe Jesuits in particular have made numerous significant contributions to the development of science. For example, the Jesuits have dedicated significant study to earthquakes, and seismology has been described as \"the Jesuit science.\" The Jesuits have been described as \"the single most important contributor to experimental physics in the seventeenth century.\" According to Jonathan Wright in his book \"God's Soldiers\", by the eighteenth century the Jesuits had \"contributed to the development of pendulum clocks, pantographs, barometers, reflecting telescopes and microscopes, to scientific fields as various as magnetism, optics and electricity. They observed, in some cases before anyone else, the colored bands on Jupiter’s surface, the Andromeda nebula and Saturn’s rings. They theorized about the circulation of the blood (independently of Harvey), the theoretical possibility of flight, the way the moon effected the tides, and the wave-like nature of light.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1750893", "url": "https://en.wikipedia.org/wiki?curid=1750893", "title": "List of Commonwealth visits made by Elizabeth II", "text": "List of Commonwealth visits made by Elizabeth II\n\nQueen Elizabeth II became Head of the Commonwealth upon the death of her father, King George VI, on 6 February 1952. Since then, she has toured the Commonwealth of Nations and their territories and dependencies widely. She has visited all except Cameroon and Rwanda, two of the most recent members.\n\nTours of the British Islands are excluded from the list below.\n\n"}
{"id": "37924611", "url": "https://en.wikipedia.org/wiki?curid=37924611", "title": "List of Cuculiformes by population", "text": "List of Cuculiformes by population\n\nThis is a list of Cuculiformes species by global population. While numbers are estimates, they have been made by the experts in their fields. For more information on how these estimates were ascertained, see Wikipedia's articles on population biology and population ecology.\n\nThis list is incomprehensive, as not all Cuculiformes have had their numbers quantified.\n"}
{"id": "88231", "url": "https://en.wikipedia.org/wiki?curid=88231", "title": "List of IBM products", "text": "List of IBM products\n\nThe following is a partial list of products, services, and subsidiaries of International Business Machines (IBM) Corporation and its predecessor corporations, beginning in the 1890s. \n\nThis list is eclectic; it includes, for example, the \"AN/FSQ-7\", which was not a product in the sense of \"offered for sale\", but was a product in the sense of \"manufactured—produced by the labor of IBM\". Several machines manufactured for the Astronomical Computing Bureau at Columbia University are included, as are some machines built only as demonstrations of IBM technology. Missing are RPQs, OEM products (semiconductors, for example), and supplies (punched cards, for example). These products and others are missing simply because no one has added them.\n\nIBM sometimes uses the same number for a system and for the principal component of that system. For example, the IBM 604 Calculating Unit is a component of the IBM 604 Calculating Punch. And different IBM divisions used the same model numbers; for example \"IBM 01\" without context clues could be a reference to a keypunch or to IBM's first electric typewriter.\n\nNumber sequence may not correspond to product development sequence. For example, the 402 tabulator was an improved, modernized, 405. \n\nIBM uses two naming structures for its modern hardware products. Products are normally given a three- or four-digit machine type and a model number (can be a mix of letters and numbers). A product may also have a marketing or brand name. For instance, 2107 is the machine type for the IBM System Storage DS8000. While the majority of products are listed here by machine type, there are instances where only a marketing or brand name is used. Care should be taken when searching for a particular product as sometimes the type and model numbers overlap. For instance the IBM storage product known as the Enterprise Storage Server is machine type 2105, and the IBM printing product known as the IBM Infoprint 2105 is machine type 2705, so searching for an IBM 2105 could result in two different products—or the wrong product—being found.\n\nIBM introduced the 80-column rectangular hole punched card in 1928. Pre-1928 machine models that continued in production with the new 80-column card format had the same model number as before. Machines manufactured prior to 1928 were, in some cases, retrofitted with 80-column card readers and/or punches thus there existed machines with pre-1928 dates of manufacture that contain 1928 technology.\n\nThis list is organized by classifications of both machines and applications, rather than by product name. Thus some (few) entries will be duplicated. The 1420, for example, is listed both as a member of the 1401 family and as a machine for Bank and finance.\n\nIBM product names have varied over the years; for example these two texts both reference the same product.\nThis article uses the name, or combination of names, most descriptive of the product. Thus the entry for the above is \n\nProducts of The Tabulating Machine Company can be identified by date, before 1933 when the subsidiaries were merged into IBM. \n\n\n\n\n\n\n\n402 and known versions\n\n404\n\n405 and known versions\n\n407 and known versions\n\n\nIBM manufactured a range of clocks and other devices until 1958 when they sold the Time Equipment Division to Simplex Time Recorder Company (SimplexGrinnell, as of 2001). See:\n\n\n\n\n\"The IBM line of Copier/Duplicators, and their associated service contracts, were sold to Eastman Kodak in 1988.\"\n\n\n\nFor these computers most components were unique to a specific computer and are shown here immediately following the computer entry.\n\n\n\"Further information\": IBM mainframe, IBM minicomputer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn IBM's terminology beginning with the System/360 disk and such devices featuring short access times were collectively called DASD. The IBM 2321 Data Cell is a DASD that used tape as its storage medium. See also history of IBM magnetic disk drives.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome software listings are for software families, not products (\"Fortran\" was not a product; \"Fortran H\" was a product).\n\nSome IBM software products were distributed free (no charge for the software itself, a common practice early in the industry). The term \"Program Product\" was used by IBM to denote that it's freely available but not for free. Prior to June 1969, the majority of software packages written by IBM were available at no charge to IBM customers; with the June 1969 announcement, new software not designated as \"System Control Programming\" became Program Products, although existing non-system software remained available for free.\n\n\n\nIBM distributes its diverse collection of software products over several brands; mainly:\n\n\nThe Watson Customer Engagement (commonly known as WCE and formerly known as IBM Commerce) business unit supports marketing, commerce, and supply chain software development and product offerings for IBM. Software and solutions offered as part of these three portfolios by WCE are as follows:\n\n\n\n\n\n\n\n\n"}
{"id": "463506", "url": "https://en.wikipedia.org/wiki?curid=463506", "title": "List of International Electrotechnical Commission standards", "text": "List of International Electrotechnical Commission standards\n\nThis is an incomplete list of standards published by the International Electrotechnical Commission (IEC).\n\nThe numbers of older IEC standards were converted in 1997 by adding 60000; for example IEC 27 became IEC 60027.\nIEC standards often have multiple sub-part documents; only the main title for the standard is listed here.\n\n\n\n"}
{"id": "169589", "url": "https://en.wikipedia.org/wiki?curid=169589", "title": "List of continuity-related mathematical topics", "text": "List of continuity-related mathematical topics\n\nIn mathematics, the terms continuity, continuous, and continuum are used in a variety of related ways.\n\n\n"}
{"id": "7161512", "url": "https://en.wikipedia.org/wiki?curid=7161512", "title": "List of cryptographic file systems", "text": "List of cryptographic file systems\n\nThis is a list of filesystems with support for filesystem-level encryption. Not to be confused with full-disk encryption.\n\n\n\n\n"}
{"id": "40557499", "url": "https://en.wikipedia.org/wiki?curid=40557499", "title": "List of international presidential trips made by Hassan Rouhani", "text": "List of international presidential trips made by Hassan Rouhani\n\nThis is a list of international presidential trips made by Hassan Rouhani, the 7th and current President of Iran\n\nThe following international trips were made by President Hassan Rouhani in 2013:\nThe following international trips were made by President Hassan Rouhani in 2014:\nThe following international trips were made by President Hassan Rouhani in 2015:\nThe following international trips were made by President Hassan Rouhani in 2016:\nThe following international trips were made by President Hassan Rouhani in 2017:\n\nThe following international trips were made by President Hassan Rouhani in 2018:\n\n"}
{"id": "5971797", "url": "https://en.wikipedia.org/wiki?curid=5971797", "title": "List of mathematicians (B)", "text": "List of mathematicians (B)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "37660", "url": "https://en.wikipedia.org/wiki?curid=37660", "title": "List of parasitic organisms", "text": "List of parasitic organisms\n\nThis is an \"incomplete list\" of organisms that are true parasites upon other organisms.\n\n\nThese can be categorized into three groups; cestodes, nematodes and trematodes. Examples include:\n\n\n\n\n\nMonogeneans are flatworms, generally ectoparasites on fish.\n\n\n\n\n\n"}
{"id": "47356330", "url": "https://en.wikipedia.org/wiki?curid=47356330", "title": "List of sea stacks", "text": "List of sea stacks\n\nThe following list enumerates and expands on notable sea stacks, including former sea stacks that no longer exist.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "54390625", "url": "https://en.wikipedia.org/wiki?curid=54390625", "title": "Lucianne Walkowicz", "text": "Lucianne Walkowicz\n\nLucianne Walkowicz is an American astronomer based at the Adler Planetarium noted for her research contributions in stellar magnetic activity and its impact on planetary suitability for extraterrestrial life. Since 2008, she has been the chair of the Large Synoptic Survey Telescope (LSST) Transients and Variable Stars collaboration and is the founding director of the LSST Data Science Fellowship program. She is internationally recognized for her advocacy for conservation of dark night skies, and was named a 2011 National Academy of Science Kavli Fellow and a 2012 TED Senior Fellow.\n\nIn 2017, she was named the fifth Baruch S. Blumberg NASA/Library of Congress Chair in Astrobiology in the John W. Kluge Center at the Library of Congress. She began began her tenure October 1, 2017, working on a project titled “Fear of a Green Planet: Inclusive Systems of Thought for Human Exploration of Mars.” Her project aims to create an inclusive framework for human exploration of Mars, encompassing both cutting-edge research on Mars as a place of essential astrobiological significance, while weaving in lessons from the diverse histories of exploration on Earth.\n\nWalkowicz holds a BS in physics and astronomy from Johns Hopkins University, and an MS and PhD in astronomy from the University of Washington. She got her taste for astronomy as an undergrad at Johns Hopkins, testing detectors for the Hubble Space Telescope’s new camera.\n\nShe appeared in Werner Herzog's 2016 documentary \"Lo and Behold\".\n\nShe appeared in National Geographic's series \"MARS\".\n"}
{"id": "1271730", "url": "https://en.wikipedia.org/wiki?curid=1271730", "title": "Matrix of domination", "text": "Matrix of domination\n\nThe matrix of domination or matrix of oppression is a sociological paradigm that explains issues of oppression that deal with race, class, and gender, which, though recognized as different social classifications, are all interconnected. Other forms of classification, such as sexual orientation, religion, or age, apply to this theory as well. Patricia Hill Collins is credited with introducing the theory in her work entitled \"Black Feminist Thought: Knowledge, Consciousness, and the Politics of Empowerment\". \nAs the term implies, there are many different ways one might experience domination, facing many different challenges in which one obstacle, such as race, may overlap with other sociological features. Characteristics such as race, age, and sex, may affect an individual in extremely different ways, in such simple cases as varying geography, socioeconomic status, or simply throughout time. Other scholars such as Kimberle Crenshaw's \"Mapping the Margins: Intersectionality, Identity Politics, and Violence against Women of Color\" are credited with expanding Collins' work. The matrix of domination is a way for people to acknowledge their privileges in society. How one is able to interact, what social groups on is in, and the networks one establishes is all based on different interconnected classifications.\n\nThough Collins' main focus of the theory of the matrix of domination was applied to African-American women, there are many other examples that can be used to illustrate the theory. Other examples include Log Cabin Republicans, female criminality, and African-American Muslim Women. One of the key concepts of the matrix of domination is that the different categories, like race and gender, are separate groups, rather than a combination. This is a problem that can be seen in the law as well when it comes to discrimination because the courts fail to view discrimination as an overarching umbrella of intersectionality.\n\nA way in which the Matrix of Domination works with regards to privilege can be if two people all have the same classification, except one person has an education and one does not have as high of an education. Their gender, race, sexuality, educational attainment all intersect to identify who they are. However, compared to other people one classification can point out privilege and, in turn, open more opportunities for one individual over the other.\n\nOne of the main aspects of the matrix of domination is the fact that one may be privileged in one area, yet they can be oppressed in a different aspect of their identity. Some people believe that racial discrimination is on its way to being eradicated from the United States when they look as people like Colin Powell, a very successful, African-American, middle-aged man. Although Powell obtains the characteristics of a person that may not face oppression (upper-class, middle-aged, male), he is still discriminated against because of his race. This shows one of the key components of the matrix of domination; the idea that one cannot look at the individual facets of someone's identity, but rather that they are all interconnected.\n\nIn Collins’ \"Black Feminist Thought: Knowledge, Consciousness, and the Politics of Empowerment\", she first describes the concept of matrix thinking within the context of how black women in America encounter institutional discrimination based upon their race and gender. A prominent example of this in the 1990s was racial segregation, especially as it related to housing, education, and employment. At the time, there was very little encouraged interaction between whites and blacks in these common sectors of society. Collins argues that this demonstrates how being black and female in America continues to perpetuate certain common experiences for African-American women. As such, African-American women live in a different world than those who are not black and female. Collins notes how this shared social struggle can actually result in the formation of a group-based collective effort, citing how the high concentration of African-American women in the domestic labor sector in combination with racial segregation in housing and schooling contributed directly to the organization of the black feminist movement. The collective wisdom shared by black women that held these specific experiences constituted a distinct viewpoint for African-American women concerning correlations between their race and gender and the resulting economic consequences.\n\nMoolman points out the main issue concerning matrix thinking is how one accounts for the power dynamics between varying identifying categories that are ingrained in both oppression and domination instead of the traditional approach, reducing experiences to a single identity. For instance, black women’s experiences with society are used to illustrate how even though white scholars have attempted to use intersectionality in their research, they may still be inclined to default towards single-identity thinking that often fails to address all aspects of black women’s experiences, thus ignoring the organization the matrix objectively offers.\n\nThe matrix of domination in the colonial era and white society has also been carefully examined. The societal hierarchy determined by race and implemented under apartheid locates different racial populations in regards to their privilege, with African Americans usually at the bottom of the ladder. Dhamoon argues that on a global scale, the spot occupied by African Americans in such context is interchangeable with indigenous populations, as marginalized peoples are systematically working both within and across a matrix of interrelated axes of “penalty and privilege”. The interconnectivity of different identities in regards to power in racial structures in post-colonial era societies help illustrate what changes make a difference. The framework setup of the matrix of domination connects its origin theory and thought to existing struggles in the political and social spheres of society. A closer look at both specific and broader aspects of matrix thought will shed more light on the inner-workings and mechanisms that determine how different relationship dynamics influence matrix categorizations.\n\nMay cites that an important implication that matrix thinking inspires is that it directly goes against what is often described as the socially inclusive ‘add and stir’ approach. This is often used when describing the addition of one or more identity group to existing epistemological approaches, political strategies or research methodologies. This accounts for the proper weighing of power dynamics and their impact on different groups of people. Intersectionality centers power in a multi-pronged way as shifting across different sites and scales at the same time. Therefore, it is not neutral but evolved out of histories of struggle that pursue multidimensional forms of justice.\n\nKimberlé Crenshaw, the founder of the term intersectionality, brought national and scholarly credential to the term through the paper \"Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics\" in The University of Chicago Legal Forum. In the paper, she uses intersectionality to reveal how feminist movements and antiracist movements exclude and women of color. Focusing on the experiences of Black women, she dissects several court cases, influential pieces of literature, personal experiences, and doctrinal manifestations as evidence for the way Black women are oppressed through many different experiences, systems and groups.\n\nThough the specifics differ, the basic argument is the same: Black women are oppressed in a multitude of situations because people are unable to see how their identities intersect and influence each other. Feminism has been crafted for white middle class women, only considering problems that affect this group of people. Unfortunately, this only captures a small facet of the oppression women face. By catering to the most privileged women and addressing only the problems they face, feminism alienates women of color and lower class women by refusing to accept the way other forms of oppression feed into the sexism they face. Not only does feminism completely disregard the experiences of women of color, it also solidifies the connection between womanhood and whiteness when feminists speak for “all women”. (Crenshaw:154) Oppression cannot be detangled or separated easily in the same way identities cannot be separated easily. It is impossible to address the problem of sexism without addressing racism, as many women experience both racism and sexism. This theory can also be applied to the antiracist movement, which rarely addresses the problem of sexism, even though it is thoroughly intertwined with the problem of racism. Feminism remains white, and antiracism remains male. In essence, any theory that tries to measure the extent and manner of oppression Black women face will be wholly incorrect without using intersectionality.\n\nPatricia Hill Collins wrote a book entitled \"Black Feminist Thought: Knowledge, Consciousness and the Politics of Empowerment\", which articulated \"Black Feminist Thought\" in relation to intersectionality with a focus on the plight of Black women in face of the world, the white feminist movement, and the male antiracism movement. Collins references Crenshaw's concept of intersectionality and relates it to the matrix of domination, \"The term matrix of domination describes this overall social organization within which intersecting oppressions originate, develop, and are contained.\".\n\nBoth intersectionality and the matrix of domination help sociologists understand power relationships and systems of oppression in society. The matrix of domination looks at the overall organization of power in society while intersectionality is used to understand a specific social location of an identity using mutually constructing features of oppression.\n\nThe concept of intersectionality today is used to move away from one dimensional thinking in the matrix of domination approach by allowing for different power dynamics of different identity categories at the same time. Researchers in public health are using Intersectionality-Based Policy Analysis (IBPA) Framework to show how social categories intersect to identify health disparities that evolve from factors beyond an individual’s personal health. Ferlatte applied an IBPA framework and used structural interviews to identify barriers to the allocation of HIV prevention funding for gay men. He highlighted policy more likely to cause harm than reduce the epidemic stemming from policy makers missing the ‘intersections of oppression, sex panic, and medicalization’.\n\nIntersectionality can also be used to correct for the over-attribution of traits to groups and be used to emphasize unique experiences within a group. As a result, the field of social work is introducing intersectional approaches in their research and client interactions. At the University of Arkansas, the curriculum for a Master of Social Work (MSW) is being amended to include the Multi-Systems Life Course (MSLC) approach. Christy and Valandra apply an MSLC approach to intimate partner violence and economic abuse against poor women of color to explain that symbols of safety (such as police) in one population can be symbols of oppression in another. By teaching this approach to future social workers, the default recommendation for these women to file a police report is amended and an intervention rooted in the individual case can emerge.\n\nMany approaches have been used that consider the concepts of identity, societal structures, and representation to be mutually exclusive, but the introduction of Patricia Hill Collins’ matrix of domination addresses the interlocking patterns of privilege and marginalization along the lines of race, class, gender, and class inside social institutions as well as at the community level. With this work has come greater recognition of the various effects that each identity holds in different societal contexts, in both the micro- and macro-level structures within the systems of oppression that exist.\n\nIn April Bernard's article, “The Intersectional Alternative: Explaining Female Criminality”, Bernard applies Patricia Hill Collins’ work to the study of feminist criminology, as a means of explaining the cumulative effects of identity in a system of oppression on women’s decisions to commit a crime. Bernard employs an intersectional approach to dissect the complexities that act as determinant factors in a woman’s decision to partake in criminal activities, and more specifically, the limiting pressures of a patriarchal society. In particular, this article is framed in response to Robert Merton’s claims about deviance as a response to a lack of adequate resources to achieve cultural goals, as Bernard employs an intersectional paradigm model that explores female criminality as an expression of constraint and circumscription, rather than a “strained reality”. With this alternative framework, Bernard suggests that societal goals are not unanimous, and are instead shaped by individuals’ experiences in economic, political, and social spaces; for marginalized women, access to the means through which they build success are impacted by micro- and macro-level norms and histories that have created indicators of class (e.g. racial, economic, political, sexual) and subjugated them to limited networks. Thus, identity makes women with marginalized identities more vulnerable in the legal system, subjugates to oppressive states within multiple institutions, and creating a need for policies that move toward creating an equitable reality for them.\n\nIn the United States, especially, the matrix of domination has implications within the welfare state. Several sociological studies on the welfare state take note of state-market relations while ignoring the salient roles held by other identities such as gender, race, class, language, and age, among others. Due to the nature of the welfare state, there has not been much regard for exploring the existence of multiple axes of oppression which has led to lineation of categories of race, class, and gender. In “\"Politics, Gender, and Concepts\"”, Gary Goertz and Amy Mazur assert that literature about the welfare state should focus on the relationship between social positions and social policies, as well as provide a framework for investigations into the causal effects of class, gender, and race. As such, using the idea of a matrix of domination in these kinds of studies provides a basis for empirical research on the relationship between social positions and policies, and also, for a comparison between the outcomes of social policies on marginalized and privileged women.\n\nThe benefits that upper-class citizens receive from their employer are far different from that of working-class employees. This is due to the upper class taking jobs that give them a higher status or position, whereas the working class take jobs with lower status such as retail and blue-collar jobs. The most obvious benefit that differs between classes is the amount of money made. Upper-class workers receive significantly more pay than the working class, and while the upper class receive salaries, the lower class typically receive their pay based off hourly wages. Moreover, the chance of getting a raise is greater for the higher-ups. More benefits that the upper class enjoy over the working class are vacation time, flexible hours, retirement savings, and greater coverage of healthcare and insurance.\n\nWhen it comes to workplace benefits such as health insurance coverage, pensions, sick leave, and disability plans, there are gender differences in whether or not these benefits are offered. Women are less likely to be offered pensions, health coverage and disability plans. In fact, high poverty rates among elderly women have been linked to lack of pension coverage. Additionally, many female heads of household remain on welfare because they cannot find jobs with adequate health insurance coverage. When it comes to union contracts, men are also twice as likely to be covered. This gender gap in benefits coverage may be due to the fact that women tend to have higher medical expenditures than males of the same age. As a result, some of the observed gap in wages between males and females in the United States could be the result of employers compensating for the higher cost of employer-sponsored health insurance. This further perpetuates gender discrimination because it means that firms who offer ESI (Employer Sponsored Insurance) will prefer to hire males. Another effect of women generally having greater healthcare expenses than men is that they are likely to place a higher value on insurance and be more inclined to pass up jobs for insurance-related reasons. This lowers the probability of obtaining jobs that pay higher wages directly and decrease a woman's bargaining power with her current employer. Indeed, health insurance has a larger (negative) effect on the job mobility of women, which they attribute to women's elevated healthcare expenses.\n\nIn the United States there is an unequal distribution of income between social classes for multiple reasons. Level of education has a great influence on average salaries. The higher the socioeconomic status (SES) of an individual the more likely they are to graduate from high school and potentially obtain a college degree, which in return increases their chances of a larger salary. The average salary of an individual with a high school diploma is about $35,000, but increases to about $60,000 by obtaining a bachelor's degree after. The gap in salary increases with each additional level of education received. Those in the lower class face more obstacles and have less opportunities to pursue additional education due to their lack of resources. The wage gap is even larger for individuals affected by poverty and racial barriers. Whites have a median income of about $71,000 while blacks have a median income of about $43,000. Statistics show that blacks make up 16% of public high school graduates, 14% of those enrolling in college, and only 9% of those receiving a bachelor's degree. At the same time, whites make up 59%, 58%, and 69%, respectively. That is a 61% difference between blacks not obtaining a bachelor's degree and whites graduating with one. Individuals in poverty already face a disadvantage in obtaining the same level of income as their upper class coworkers, but when also affected by racial barriers the chances of reaching the same income are even fewer.\n\nThere is definitely intersectionality and inequality with women and men when it comes to wage gaps. Careers that pay well are often male dominated, and do not tolerate women and their personal needs. There has been a stable “pay gap” between men and women which has remained between 10-20% difference in their average earnings. (Women, careers and work life preferences). When discussing wage gaps between genders, scientists takes into account two questions, the first being “is there differential access jobs on the basis of gender?” and the second being, “ is women’s work perceived to have less value than comparable work done by men?”. When women begin to increase their numbers in certain job positions the status or value of the job decreases. Conceptualizing intersectionality through class, gender and race then identifying the barriers that create inequality in Work organizations is found in the idea of “inequality regimes”. Workplaces are prominent locations to analyze the continuous efforts of inequalities because many societal inequality issues stem in such areas. In the works of Inequality Regimes: Gender, Class, and Race in Organizations, inequality in gender, race, class are examined through intersectionality in organizations. Joane Acker discussed Inequality Regimes: Gender, Class, and Race in Organizations in Sociologists for Women in Society Feminists Lecture through studies conducted using Swedish Bank. Studies have shown in the 1980s depict that wage gaps were increasing between genders. Men were being rewarded the higher paying positions such as local managers, and believed fair wages for men should be higher than fair wages for women.\n\nSocial class plays a large role on people’s everyday life, yet their representation is not always fair. In television and popular culture, those who fall into the lower class are often portrayed differently based on if they are a woman or a man. If they are a woman, they often are portrayed as being more intelligent and responsible than their husbands, almost acting as their mothers. The male head of the household is typically portrayed as being less intelligent, with some redeeming qualities, but typically is not respected. Together they can be shown in a light that makes them seem lazy or dishonest. The upper class however, does not face these same issues with representation in the media. The man of the household takes on stereotypical male qualities, while the woman takes on stereotypical female qualities. The children in this upper class scenario are what provides entertainment value, rather than focusing on the unintelligent and unorganized adults as in the lower class model. Overall, in the upper-class family unit, they are portrayed as organized and put together, while the lower class model are portrayed as lazy and unorganized.\n\nIn American society and many other societies around the world, being a largely represented group in a position of power means that one has succeeded in life. Whether one is a manager of a fast food restaurant or the CEO of a Fortune 500 company, authority is power and power is advantage. But just like the widespread power struggle, there is a widespread equality struggle. One of the largest workplace and societal inequalities is the inequality between genders. A prime example of this is the wage gap. Women in 2016 earned, on average, 82 cents to a man’s dollar. This unequal pay is part of the reason that many women are the ones to leave the workforce when it is determined that a stay-at-home parent is required; if women are contributing less to the household income, it will make less of an impact if they quit their jobs. Women are also not granted the same opportunities for employment as men. A clear example is the U.S. military. Women were banned from all combat roles until recently. In 2011, only 14 percent of the armed forces were female, and only 14 percent of officers were female. Another example is the U.S. congress. In 2015, 80 percent of the Senate was male, and only 20 was female. This numbers were similar for the House, at 80.6 percent male and 19.4 percent female. The gender composition of the military and the government, along with the wage gap, shines a lights on the gender inequality experienced right here at home, but this inequality is more greatly felt abroad. Some countries place strict limitations on women, not allowing them to vote or work or even drive a car. While the U.S. is seen as a country of dreams and opportunity, is far easier to see this when compared to an even more unequal country. The United States has been trending toward gender equality in recent years, but it has a while to go.\n\nAn article found in the November 1998 issue of \"Social Problems\" details the conflict involving racial domination by identifying the complexity African-Americans face. In many cases, sociologists and laypersons alike are often limited in their approach to the problem. Michelle Byng, in \"Mediating Discrimination: Oppression among African-American Muslim Women\"—the 1998 article—brings to focus new approaches to understanding discrimination, but also, she writes to illustrate the many overlooked opportunities in which the discriminated are able to empower themselves in certain situations.\n\nThere are countless numbers of court cases that examine intersectionality within the workforce that did not allow individuals to have equal opportunities because of their race, gender, and social class.\n\nThe case, DeGraffenreid vs. General Motors, provides an example when the law does an injustice to those that face discrimination. Five black female production workers were laid off, and took it to court claiming that the company was violating Title VII because, \"it perpetrated past discriminatory practices of not hiring Black females.\" The court looked at each of the categories, race and gender, separately therefore they missed the discrimination of a person being both African-American and female. \"It was argued, black women can expect little protection as long as approaches, such as that in DeGraffenreid, which completely obscure problems of intersectionality prevail.”\n\nAnother case, Maivan Lam v. University of Hawai'i, where intersectionality was the core reason behind the problem that emerged. Maivan Lam was not offered a job twice when she applied to be the Director of the Law School's Pacific Asian Legal Studies Program. Two times, the university was looking for a director and when the final offer came around, and she was the best candidate available, the university simply cancelled their search. In the first search, Professor Lam made it to the final round, but was not offered the job before the whole search was simply shut down. The second time, the position was offered to another candidate and the other candidate refused to accept, the search was simply cancelled without it being offered to Professor Lam. When Lam brought this situation up to the Dean, he suggested reopening the search so that male candidates who had not applied on time could submit their applications. It is stated, \"Early in the 1989-90 academic year, the new appointments committee reviewed applications for a commercial law position. At one meeting, a male committee member stated that the Law School should not have two women teaching commercial law. This comment was reported to the Dean, who said that he recognized that the professor had difficulty dealing with women but took no action to remove him from the committee or otherwise to remedy the problem.\". There was clear intersectionality as Professor Lam was not only arguing regards to race but also how her gender affected her position.\n\nIn the case, Jefferies v. Harris County Community Action Association, April 21, 1980, Dafro M. Jefferies claimed that her former employer failed to promote her to a higher position because of her race and sex. In 1967 she was employed by Harris County Community Action Association as a Secretary to the Director of Programs. She was later promoted to Personal Interviewer in 1970. Everything seemed to be moving in a positive direction for her. However, between 1971 and April 1974, Jefferies applied for promotions in various positions and departments without any luck. She realized that her employer was discriminating against her when two Field Representative positions opened. Jefferies immediately applied. However, the positions were already staffed by a white female and black male the same day that she was told about the vacant position. The company had purposefully told her about the open positions knowing that they were already filled by other staff members. After several complaints to the company, on April 23, 1974 Jefferies was placed on probation. In June 1974, she was terminated from the job because she had called the company out for discriminating against her because of her race and sex. There was clear evidence of intersectionality in this case; Jefferies was not promoted to a higher position because she was both black and a female.\n\n\n"}
{"id": "11736014", "url": "https://en.wikipedia.org/wiki?curid=11736014", "title": "Minneapolis Domestic Violence Experiment", "text": "Minneapolis Domestic Violence Experiment\n\nThe Minneapolis Domestic Violence Experiment (MDVE) evaluated the effectiveness of various police responses to domestic violence calls in Minneapolis, Minnesota. This experiment was implemented during 1981-82 by Lawrence W. Sherman, Director of Research at the Police Foundation, and by the Minneapolis Police Department with funding support from the National Institute of Justice. Among a pool of domestic violence offenders for whom there was probable cause to make an arrest, the study design called for officers to randomly select one third of the offenders for arrest, one third would be counseled and one third would be separated from their domestic partner.\n\nThe results of the study, showing a deterrent effect for arrest, had a \"virtually unprecedented impact in changing then-current police practices.\" Subsequently, numerous states and law enforcement agencies enacted policies for mandatory arrest, without warrant, for domestic violence cases in which the responding police officer had probable cause that a crime had occurred.\n\nDomestic violence historically has been viewed as a private family matter that need not involve government or criminal justice intervention. Before the early 1970s, police in the United States favored a \"hands-off\" approach to domestic violence calls, with arrest only used as a last resort. At the time, domestic violence cases were typically classified as misdemeanor assault cases. During the 1970s, many U.S. jurisdictions did not authorize the police to make arrests in any misdemeanor assault, whether it involved a domestic partner or not, unless the assault occurred in the officer's presence. A 1978 court order in New York City mandated that arrests only be made in cases of serious violence, thus officers instead made effort to mediate family disputes.\n\nIn the early 1970s, clinical psychologists argued that police should make an effort to mediate disputes.\n\nStatistics on incidence of domestic violence, published in the late 1970s, helped raise public awareness of the problem and increase activism. A study published in 1976 by the Police Foundation found that the police had intervened at least once in the previous two years in 85 percent of spouse homicides. In the late 1970s and early 1980s, feminists and battered women's advocacy groups were calling on police to take domestic violence more seriously and change intervention strategies. In some instances, these groups took legal action against police departments, including in Oakland, California and New York City, to get them to make arrests in domestic violence cases. They claimed that police assigned low priority to domestic disturbance calls.\n\nIn 1978, the National Academy of Sciences published a report, \"Deterrence and Incapacitation: Estimating the Effects of Criminal Sanctions on Crime Rates\", which called for more rigorous assessments of policies and practices based on social control theories and use of deterrence for crime control. Based on the Academy's recommendations, the National Institute of Justice began funding studies of the deterrent effects of criminal sanctions and, in 1980, one of the sponsored studies was the Minneapolis Domestic Violence Experiment.\n\nThe Minneapolis Domestic Violence Experiment looked at effectiveness of methods used by police to reduce domestic violence. Cases used in the study were misdemeanor assault calls, which make up the bulk of domestic violence calls for service. Both the victim and offender needed to still be present when the police arrived, in order to be included in the study.\n\n51 patrol officers in the Minneapolis Police Department participated in the study. Each was asked to use one of three approaches for handling domestic violence calls, in cases where officers had probable cause to believe an assault had occurred:\n\nInterviews were conducted during a 6-month follow-up period, with both victims and offenders, as well as official records consulted to determine whether or not re-offending had occurred. The study lasted approximately 17 months and included 330 cases.\n\nArrest was found to be the most effective police response. The study found that the offenders assigned to be arrested had lower rates of re-offending than offenders assigned to counseling or temporarily sent away. (19% for arrest 37% for advise and 34% for Send)\n\nThe results of the study received a great deal of attention from the news media, including \"The New York Times\" and prime-time news coverage on television. Many U.S. police departments responded to the study, adopting a mandatory arrest policy for spousal violence cases with probable cause. New York City Police Department Commissioner Benjamin Ward quickly issued a new mandate for officers to make arrests, after reading the results of the study in a Police Foundation report. Ward stated his belief that \"arresting violent members of a household would be more effective in protecting other family members and help safeguard police officers called in to stop the highly charged quarrels. I thought it was about time to put policemen out of the counseling business and into what they really are best at, which is making arrests, then let the judge decide.\" With this mandate, Ward also included cohabitants and same-sex couples in the police definition of family. The Houston and Dallas Police Departments were also quick to change their approach to domestic disturbance calls, and make more arrests. Within a year, the number of police departments using arrest as a strategy in domestic violence cases jumped from 10 to 31%, and to 46% by 1986. Numerous other police departments had partially changed their approach to domestic violence cases.\n\nIn 1984, the U.S. Attorney General's Task Force on Family Violence report drew heavily upon the Minneapolis study, in recommending that domestic violence be handled with a criminal justice approach. Within eight years, 15 states and the District of Columbia enacted new domestic violence laws that required the arrest of violent domestic offenders. By 2005, 23 states and the District of Columbia had enacted mandatory arrest for domestic assault, without warrant, given that the officer has probable cause and regardless of whether or not the officer witnessed the crime. The Minneapolis study also influenced policy in other countries, including New Zealand, which adopted a pro-arrest policy for domestic violence cases.\n\nMandatory arrest laws were implemented in the U.S. during the 1980s and 1990s due, in great part, to the impact of the Minneapolis Experiment. The Violence Against Women Act of 1994 added to the volume of legislation in the 1990s pertaining to mandatory arrest laws, affecting those states that lacked such laws themselves. The laws \"require the police to make arrests in domestic violence cases when there was probable cause to do so, regardless of the wishes of the victim.\" Before the laws were put into effect, police officers were required to witness the abuse occurring first hand prior to making an arrest. Currently, 23 states use mandatory arrest policies. \n\nPrior to the implementation of mandatory arrest policies in the United States, police often were not able to arrest individuals suspected of domestic violence. In an article from the \"California Law Review\" titled \"Domestic Violence as a Crime Against the State,\" Machaela Hoctor explained that \"when officers did respond to a domestic violence call, they usually attempted to mediate the dispute. This \"mediation\" consisted of a variety of approaches, including attempts by officers to convince the parties to reconcile immediately at the scene or to use formal alternative dispute resolution programs.\"\nThe debate over mandatory arrest is still underway, as many people believe it has negative effects on the assailant, victim, and their family members including but not limited to the breakdown of the family, the economic deprivation of the victim, the trauma associated with separation of families, and the lack of childcare in situations of dual arrest. Sometimes when police respond, they arrest both parties involved in a domestic violence situation. As described by Margaret Martin in the \"Journal of Family Violence\", \"The practice of dual arrest, the arrest of two parties, usually a man and a woman engaged in a 'domestic dispute,' has arisen in localities which employ presumptive and mandatory arrest\". Police are more likely to arrest both parties if the primary aggressor is female However, not every domestic violence situation results in dual arrest. Police Officers are trained to deduce who the primary aggressor is in a domestic violence dispute, leading to the arrest of the assailant and not the victim.\n\nSome states will arrest simply based on probable cause to believe an act of domestic violence has been committed, while others do not allow for an arrest after a specific amount of time following the incident. For example, in Alaska the police cannot make an arrest if the abuse occurred more than 12 hours prior to notification Police are specifically trained to assess the situation and decide whether they have the required probable cause to make an arrest. For instance, Wisconsin has a list of requirements that must be met before an officer can arrest a suspect. These include the age of the suspect(s), their relationship to the victim(s), and whether the act could be considered an intentional assault. The officer must also be able to identify the \"predominant aggressor\" \n\nResearch has consistently reported an increase in the use of arrest for domestic violence in the United States. One large (but not necessarily representative) study of over 650,000 incidents drawn from 2,819 jurisdictions in 19 states during the year 2000 found that in the 197,064 incidents when victims and offenders were intimate partners, police made one or more arrests in 48.0 percent of incidents, and dual arrests in 1.9 percent of incidents \nIn regards to same-sex relationships, the arrest rates for domestic violence were the same as those for heterosexual couples. For all intimate partner relationships, offenders were more likely to be arrested if the incident of violence was a serious aggravated assault. The NIJ also reported that \"arrest occurred more frequently in cases involving intimate partners if the offender was white\" and \"cases involving intimate partners and acquaintances were more likely to result in arrest if the offender was 21 or older\". The increased use of arrest has led to concerns about increases in arrests of women or arrests of two or more persons (dual arrests) in the same incident.\n\nThe Minneapolis study was criticized for its methods and its conclusions. The follow-up period of six months may have been too short to capture the episodic and cyclical patterns that can occur with domestic violence. Also, Minneapolis may have been unusual, in that they kept arrestees overnight in jail, whereas in other jurisdictions arrestees might be sent home much quicker.\n\nWhile the Minneapolis design had many methodological strengths, randomized experiments look at the average causal effects for the group as a whole. Conclusions may be made that apply to most individuals in the group, but not all individuals, with some possibly experiencing negative effects of the intervention. In some cases, arrest may provoke the abuser and increase the possibility of more retributive violence.\n\nThe Minneapolis Experiment was based on deterrence theory, which includes the assumption that the offender is making rational decisions. In the case of domestic violence (and many other offenses), offenders \noften show little rational behavior. In addition, the Minneapolis Experiment did not measure whether being arrested increased the offenders' fear of future sanctions, a crucial element in deterrence theory.\n\nBeginning in 1986, the National Institute of Justice sponsored six replications of the Minneapolis Domestic Violence Experiment. While each site was an independent study, NIJ required that each study had to 1) use an experimental design (i.e., random assignment), 2) address domestic violence incidents that come to the attention of the police, 3) use arrest as one of the treatments, and 4) measure repeat offending using official police records and interviews with victims. The study sites included police departments in Omaha, Nebraska, Charlotte, North Carolina, Milwaukee, Wisconsin, Miami-Dade County, Florida, and Colorado Springs, Colorado. In Metro-Dade, 907 cases were used, compared to 1,200 cases in Milwaukee and over 1,600 cases in Colorado Springs. A study initiated in Atlanta was never completed. Although these five studies have been described as replications of the Minneapolis experiment, they each varied considerably from that study and from each other in methods and measures.\n\nThe initial findings from the five completed replications were reported independently beginning in 1990. The original authors' findings about the crime control effects of arrest varied depending upon the site studied, the measures of repeat offending used, and which alternative treatments were compared to arrest. Each replication reported multiple findings with some results favoring arrest, some showing no differences and some showing that arrest was associated with more repeat offending. None of the replications reported effects as strong as those reported for the Minneapolis Experiment.\n\nTwo articles synthesizing the findings from these studies report a crime control effect for the use of arrest for domestic violence. First, a meta-analysis of the published findings based on official police records from the Minneapolis and the SARP experiments reported a deterrent effect for arrest. Second, a re-analysis that applied consistent measures and methods to the archived data from the five replications reported that arrest was associated with as much as a 25% reduction in repeat offending and that those results were consistent across all five sites.\n"}
{"id": "1022111", "url": "https://en.wikipedia.org/wiki?curid=1022111", "title": "Monoclonal", "text": "Monoclonal\n\nMonoclonal cells are a group of cells produced from a single ancestral cell by repeated cellular replication. Thus they can be said to form a single clone. The term \"monoclonal\" comes from the Ancient Greek \"monos\", meaning \"alone\" or \"single\", and \"klon\", meaning \"twig\".\n\nThe process of replication can occur \"in vivo\", or may be stimulated \"in vitro\" for laboratory manipulations. The use of the term typically implies that there is some method to distinguish between the cells of the original population from which the single ancestral cell is derived, such as a random genetic alteration, which is inherited by the progeny.\n\nCommon usages of this term include:\n"}
{"id": "19645563", "url": "https://en.wikipedia.org/wiki?curid=19645563", "title": "Outline of clinical research", "text": "Outline of clinical research\n\nThe following outline is provided as an overview of and topical guide to clinical research:\n\nClinical research is the aspect of biomedical research that addresses the assessment of new pharmaceutical and biological drugs, medical devices and vaccines in humans.\n\n\n\nClinical study design\n\nHuman subject research\n\n\n\n\nContract research organization\n\nClinical data acquisition\n\nMedical classification\n\nClinical Data Interchange Standards Consortium\n\nAnalysis of clinical trials\n\n\n\n\n\n\n\nFood and Drug Administration\n\n\n\n"}
{"id": "15659224", "url": "https://en.wikipedia.org/wiki?curid=15659224", "title": "Petrus Jacobus Kipp", "text": "Petrus Jacobus Kipp\n\nPetrus Jacobus Kipp (Utrecht, 5 March 1808 – Delft, 3 February 1864) was a Dutch apothecary, chemist and instrument maker. He became known as the inventor of the Kipp apparatus, chemistry equipment for the development of gases.\n\nKipp passed his pharmacist exam in 1829 in Utrecht with a thesis on seven substances. When he found that, contrary to the academic medicine education with national value, his exam had mere local value, to start a pharmacy in faraway Delft, he had to pass a further test at the provincial medical council. At the beginning of the 19th century, in Delft as in other cities, there was a surplus of pharmacists. Many decided to combine their work with other activities. So too Kipp started in 1830 in Delft a trade in scientific instruments and chemicals. At first his chemicals business made most profit, but after publishing a catalogue in 1850 with instruments imported from Germany and France, his sales of instruments grew in importance.\n\nIn 1840, Kipp was elected into the medical council of the city of Delft. In this responsibility, he conducted various chemical investigations assigned by the authorities, e.g., of drinking water and of lamp oil used in street lighting. Many pharmacists in those times were working on professionalisation of the profession. Kipp was one of the founders in 1842 of the Nederlandsche Maatschappij ter Bevordering der Pharmacy (Dutch Company for the advancement of Pharmacy). Also in 1842, in Delft the Polytechnische Hogeschool was founded by King Willem II. The following year, Kipp's friend Carel Frederik Donnadieu was appointed Professor of Chemistry there. Between 1844 and 1850, Kipp earned extra money by translation of German chemistry books to Dutch for use at the university.\n\nIn 1842, Kipp published the results of research into the presence of arsenic in livers and kidneys in rabbits. For this research, he used an apparatus to generate hydrogen, which had been developed in 1836 by the English scientist James Marsh. Because the hydrogen production could not easily be stopped during the experiments, Kipp was dissatisfied with the design, and decided to develop his own gas generator. His first version was created by the German glassblower Heinrich Geißler; but was very fragile. The same year, Kipp make a new design, created again by Geißler. This design would be the model for all future versions of the Kipp's apparatus. In 1844, Kipp published two descriptions in the Tijdschrift voor Handel and Nijverheid (Journal for trade and industry). The oldest known copy of the apparatus is owned by the Boerhaave Museum in Leiden. It is 62 cm high and was made between 1845 and 1875.\n\nKipp was married with Hanna Petronella Regina Heijligers. Together they had ten children. After his death in 1864, his activities were continued by his wife and their sons Willem and Anton under the name of 'P.J. Kipp en Zonen.' Willem ran the pharmacy, later under his own name 'W.A. Kipp,' while Anton did the trade of chemicals and instruments. At the end of the 19th century, the own production of scientific and medical instruments grew in importance more than the import of foreign products. In 2008, the three companies still are active: the Delft pharmacy W.A. Kipp, the Delft instrument maker Kipp & Zonen, and the trade company Salm and Kipp.\n\n"}
{"id": "24269", "url": "https://en.wikipedia.org/wiki?curid=24269", "title": "Process philosophy", "text": "Process philosophy\n\nProcess philosophy — also ontology of becoming, processism, or philosophy of organism — identifies metaphysical reality with change. In opposition to the classical model of change as illusory (as argued by Parmenides) or accidental (as argued by Aristotle), process philosophy regards change as the cornerstone of reality—the cornerstone of being thought of as becoming.\n\nSince the time of Plato and Aristotle, philosophers have posited true reality as \"timeless\", based on permanent substances, while processes are denied or subordinated to timeless substances. If Socrates changes, becoming sick, Socrates is still the same (the substance of Socrates being the same), and change (his sickness) only glides over his substance: change is accidental, whereas the substance is essential. Therefore, classic ontology denies any full reality to change, which is conceived as only accidental and not essential. This classical ontology is what made knowledge and a theory of knowledge possible, as it was thought that a science of something in becoming was an impossible feat to achieve.\n\nPhilosophers who appeal to process rather than substance include Heraclitus, Karl Marx, Friedrich Nietzsche, Henri Bergson, Martin Heidegger, Charles Sanders Peirce, William James, Alfred North Whitehead, Alfred Korzybski, R. G. Collingwood, Alan Watts, Robert M. Pirsig, Charles Hartshorne, Arran Gare, Nicholas Rescher, Colin Wilson, Jacques Derrida, and Gilles Deleuze. In physics, Ilya Prigogine distinguishes between the \"physics of being\" and the \"physics of becoming\". Process philosophy covers not just scientific intuitions and experiences, but can be used as a conceptual bridge to facilitate discussions among religion, philosophy, and science.\n\nProcess philosophy is sometimes classified as closer to Continental philosophy than analytic philosophy, because it is usually only taught in Continental departments. However, other sources state that process philosophy should be placed somewhere in the middle between the poles of analytic versus Continental methods in contemporary philosophy.\n\nHeraclitus proclaimed that the basic nature of all things is change.\n\nThe quotation from Heraclitus appears in Plato's \"Cratylus\" twice; in 401d as:\n\"Ta onta ienai te panta kai menein ouden\"\"All entities move and nothing remains still\"and in 402a\n\n\"Panta chōrei kai ouden menei kai dis es ton auton potamon ouk an embaies\"\n\"Everything changes and nothing remains still ... and ... you cannot step twice into the same stream\"\n\nHeraclitus considered fire as the most fundamental element.\n\n\"All things are an interchange for fire, and fire for all things, just like goods for gold and gold for goods.\"\n\nThe following is an interpretation of Heraclitus's concepts into modern terms by Nicholas Rescher.\n\n\"...reality is not a constellation of things at all, but one of processes. The fundamental \"stuff\" of the world is not material substance, but volatile flux, namely \"fire\", and all things are versions thereof (puros tropai). Process is fundamental: the river is not an object, but a continuing flow; the sun is not a thing, but an enduring fire. Everything is a matter of process, of activity, of change (panta rhei).\"\n\nAn early expression of this viewpoint is in Heraclitus's fragments. He posits strife, \"ἡ ἔρις\" (strife, conflict), as the underlying basis of all reality defined by change. The balance and opposition in strife were the foundations of change and stability in the flux of existence.\n\nIn early twentieth century, the philosophy of mathematics was undertaken to develop mathematics as an airtight, axiomatic system in which every truth could be derived logically from a set of axioms. In the foundations of mathematics, this project is variously understood as logicism or as part of the formalist program of David Hilbert. Alfred North Whitehead and Bertrand Russell attempted to complete, or at least facilitate, this program with their seminal book Principia Mathematica, which purported to build a logically consistent set theory on which to found mathematics. After this, Whitehead extended his interest to natural science, which he held needed a deeper philosophical basis. He intuited that natural science was struggling to overcome a traditional ontology of timeless material substances that does not suit natural phenomena. According to Whitehead, material is more properly understood as 'process'. In 1929, he produced the most famous work of process philosophy, \"Process and Reality\", continuing the work begun by Hegel but describing a more complex and fluid dynamic ontology.\n\nProcess thought describes truth as \"movement\" in and through substance (Hegelian truth), rather than substances as fixed concepts or \"things\" (Aristotelian truth). Since Whitehead, process thought is distinguished from Hegel in that it describes entities that arise or coalesce in \"becoming\", rather than being simply dialectically determined from prior posited determinates. These entities are referred to as \"complexes of occasions of experience\". It is also distinguished in being not necessarily conflictual or oppositional in operation. Process may be integrative, destructive or both together, allowing for aspects of interdependence, influence, and confluence, and addressing coherence in universal as well as particular developments, i.e., those aspects not befitting Hegel's system. Additionally, instances of determinate occasions of experience, while always ephemeral, are nonetheless seen as important to define the type and continuity of those occasions of experience that flow from or relate to them.\n\nAlfred North Whitehead began teaching and writing on process and metaphysics when he joined Harvard University in 1924.\n\nIn his book \"Science and the Modern World\" (1925), Whitehead noted that the human intuitions and experiences of science, aesthetics, ethics, and religion influence the worldview of a community, but that in the last several centuries science dominates Western culture. Whitehead sought a holistic, comprehensive cosmology that provides a systematic descriptive theory of the world which can be used for the diverse human intuitions gained through ethical, aesthetic, religious, and scientific experiences, and not just the scientific.\n\nWhitehead's influences were not restricted to philosophers or physicists or mathematicians. He was influenced by the French philosopher Henri Bergson (1859–1941), whom he credits along with William James and John Dewey in the preface to \"Process and Reality\".\n\nFor Whitehead, metaphysics is about logical frameworks for the conduct of discussions of the character of the world. It is not directly and immediately about facts of nature, but only indirectly so, in that its task is to explicitly formulate the language and conceptual presuppositions that are used to describe the facts of nature. Whitehead thinks that discovery of previously unknown facts of nature can in principle call for reconstruction of metaphysics.\n\nThe process metaphysics elaborated in \"Process and Reality\" posits an ontology which is based on the two kinds of existence of entity, that of actual entity and that of abstract entity or abstraction, also called 'object'.\n\nActual entity is a term coined by Whitehead to refer to the entities that really exist in the natural world. For Whitehead, actual entities are spatiotemporally extended events or processes. An actual entity is how something is happening, and how its happening is related to other actual entities. The actually existing world is a multiplicity of actual entities overlapping one another.\n\nThe ultimate abstract principle of actual existence for Whitehead is creativity. Creativity is a term coined by Whitehead to show a power in the world that allows the presence of an actual entity, a new actual entity, and multiple actual entities. Creativity is the principle of novelty. It is manifest in what can be called 'singular causality'. This term may be contrasted with the term 'nomic causality'. An example of singular causation is that I woke this morning because my alarm clock rang. An example of nomic causation is that alarm clocks generally wake people in the morning. Aristotle recognizes singular causality as efficient causality. For Whitehead, there are many contributory singular causes for an event. A further contributory singular cause of my being awoken by my alarm clock this morning was that I was lying asleep near it till it rang.\n\nAn actual entity is a general philosophical term for an utterly determinate and completely concrete individual particular of the actually existing world or universe of considered in terms of singular causality, about which categorical statements can be made. Whitehead's most far-reaching and radical contribution to metaphysics is his invention of a better way of choosing the actual entities. Whitehead chooses a way of defining the actual entities that makes them all alike, \"qua\" actual entities, with a single exception.\n\nFor example, for Aristotle, the actual entities were the substances, such as Socrates. Besides Aristotle's ontology of substances, another example of an ontology that posits actual entities is in the monads of Leibniz, which are said to be 'windowless'.\n\nFor Whitehead's ontology of processes as defining the world, the actual entities exist as the only fundamental elements of reality.\n\nThe actual entities are of two kinds, temporal and atemporal.\n\nWith one exception, all actual entities for Whitehead are temporal and are occasions of experience (which are not to be confused with consciousness). An entity that people commonly think of as a simple concrete object, or that Aristotle would think of as a substance, is, in this ontology, considered to be a temporally serial composite of indefinitely many overlapping occasions of experience. A human being is thus composed of indefinitely many occasions of experience.\n\nThe one exceptional actual entity is at once both temporal and atemporal: God. He is objectively immortal, as well as being immanent in the world. He is objectified in each temporal actual entity; but He is not an eternal object.\n\nThe occasions of experience are of four grades. The first grade comprises processes in a physical vacuum such as the propagation of an electromagnetic wave or gravitational influence across empty space. The occasions of experience of the second grade involve just inanimate matter; \"matter\" being the composite overlapping of occasions of experience from the previous grade. The occasions of experience of the third grade involve living organisms. Occasions of experience of the fourth grade involve experience in the mode of presentational immediacy, which means more or less what are often called the qualia of subjective experience. So far as we know, experience in the mode of presentational immediacy occurs in only more evolved animals. That some occasions of experience involve experience in the mode of presentational immediacy is the one and only reason why Whitehead makes the occasions of experience his actual entities; for the actual entities must be of the ultimately general kind. Consequently, it is inessential that an occasion of experience have an aspect in the mode of presentational immediacy; occasions of the grades one, two, and three, lack that aspect.\n\nThere is no mind-matter duality in this ontology, because \"mind\" is simply seen as an abstraction from an occasion of experience which has also a material aspect, which is of course simply another abstraction from it; thus the mental aspect and the material aspect are abstractions from one and the same concrete occasion of experience. The brain is part of the body, both being abstractions of a kind known as \"persistent physical objects\", neither being actual entities. Though not recognized by Aristotle, there is biological evidence, written about by Galen, that the human brain is an essential seat of human experience in the mode of presentational immediacy. We may say that the brain has a material and a mental aspect, all three being abstractions from their indefinitely many constitutive occasions of experience, which are actual entities.\n\nInherent in each actual entity is its respective dimension of time. Potentially, each Whiteheadean occasion of experience is causally consequential on every other occasion of experience that precedes it in time, and has as its causal consequences every other occasion of experience that follows it in time; thus it has been said that Whitehead's occasions of experience are 'all window', in contrast to Leibniz's 'windowless' monads. In time defined relative to it, each occasion of experience is causally influenced by prior occasions of experiences, and causally influences future occasions of experience. An occasion of experience consists of a process of prehending other occasions of experience, reacting to them. This is the process in process philosophy.\n\nSuch process is never deterministic. Consequently, free will is essential and inherent to the universe.\n\nThe causal outcomes obey the usual well-respected rule that the causes precede the effects in time. Some pairs of processes cannot be connected by cause-and-effect relations, and they are said to be spatially separated. This is in perfect agreement with the viewpoint of the Einstein theory of special relativity and with the Minkowski geometry of spacetime. It is clear that Whitehead respected these ideas, as may be seen for example in his 1919 book \"An Enquiry concerning the Principles of Natural Knowledge\" as well as in \"Process and Reality\". Time in this view is relative to an inertial reference frame, different reference frames defining different versions of time.\n\nThe actual entities, the occasions of experience, are logically atomic in the sense that an occasion of experience cannot be cut and separated into two other occasions of experience. This kind of logical atomicity is perfectly compatible with indefinitely many spatio-temporal overlaps of occasions of experience. One can explain this kind of atomicity by saying that an occasion of experience has an internal causal structure that could not be reproduced in each of the two complementary sections into which it might be cut. Nevertheless, an actual entity can completely contain each of indefinitely many other actual entities.\n\nAnother aspect of the atomicity of occasions of experience is that they do not change. An actual entity is what it is. An occasion of experience can be described as a process of change, but it is itself unchangeable.\n\nThe reader should bear in mind that the atomicity of the actual entities is of a simply logical or philosophical kind, thoroughly different in concept from the natural kind of atomicity that describes the atoms of physics and chemistry.\n\nWhitehead's theory of extension was concerned with the spatio-temporal features of his occasions of experience. Fundamental to both Newtonian and to quantum theoretical mechanics is the concept of velocity. The measurement of a velocity requires a finite spatiotemporal extent. Because it has no finite spatiotemporal extent, a single point of Minkowski space cannot be an occasion of experience, but is an abstraction from an infinite set of overlapping or contained occasions of experience, as explained in \"Process and Reality\". Though the occasions of experience are atomic, they are not necessarily separate in extension, spatiotemporally, from one another. Indefinitely many occasions of experience can overlap in Minkowski space.\n\nNexus is a term coined by Whitehead to show the network actual entity from universe. In the universe of actual entities spread actual entity. Actual entities are clashing with each other and form other actual entities. The birth of an actual entity based on an actual entity, actual entities around him referred to as nexus.\n\nAn example of a nexus of temporally overlapping occasions of experience is what Whitehead calls an enduring physical object, which corresponds closely with an Aristotelian substance. An enduring physical object has a temporally earliest and a temporally last member. Every member (apart from the earliest) of such a nexus is a causal consequence of the earliest member of the nexus, and every member (apart from the last) of such a nexus is a causal antecedent of the last member of the nexus. There are indefinitely many other causal antecedents and consequences of the enduring physical object, which overlap, but are not members, of the nexus. No member of the nexus is spatially separate from any other member. Within the nexus are indefinitely many continuous streams of overlapping nexūs, each stream including the earliest and the last member of the enduring physical object. Thus an enduring physical object, like an Aristotelian substance, undergoes changes and adventures during the course of its existence.\n\nIn some contexts, especially in the theory of relativity in physics, the word 'event' refers to a single point in Minkowski or in Riemannian space-time. A point event is not a process in the sense of Whitehead's metaphysics. Neither is a countable sequence or array of points. A Whiteheadian process is most importantly characterized by extension in space-time, marked by a continuum of uncountably many points in a Minkowski or a Riemannian space-time. The word 'event', indicating a Whiteheadian actual entity, is not being used in the sense of a point event.\n\nWhitehead's abstractions are conceptual entities that are abstracted from or derived from and founded upon his actual entities. Abstractions are themselves not actual entities. They are the only entities that can be real but are not actual entities. This statement is one form of Whitehead's 'ontological principle'.\n\nAn abstraction is a conceptual entity that refers to more than one single actual entity. Whitehead's ontology refers to importantly structured collections of actual entities as nexuses of actual entities. Collection of actual entities into a nexus emphasizes some aspect of those entities, and that emphasis is an abstraction, because it means that some aspects of the actual entities are emphasized or dragged away from their actuality, while other aspects are de-emphasized or left out or left behind.\n\n'Eternal object' is a term coined by Whitehead. It is an abstraction, a possibility, or pure potential. It can be ingredient into some actual entity. It is a principle that can give a particular form to an actual entity.\n\nWhitehead admitted indefinitely many eternal objects. An example of an eternal object is a number, such as the number 'two'. Whitehead held that eternal objects are abstractions of a very high degree of abstraction. Many abstractions, including eternal objects, are potential ingredients of processes.\n\nFor Whitehead, besides its temporal generation by the actual entities which are its contributory causes, a process may be considered as a concrescence of abstract ingredient eternal objects. God enters into every temporal actual entity.\n\nWhitehead's ontological principle is that whatever reality pertains to an abstraction is derived from the actual entities upon which it is founded or of which it is comprised.\n\nConcrescence is a term coined by Whitehead to show the process of jointly forming an actual entity that was without form, but about to manifest itself into an entity Actual full (\"satisfaction\") based on datums or for information on the universe. The process of forming an actual entity is the case based on the existing datums. Concretion process can be regarded as \"subjectification process.\"\n\nDatum is a term coined by Whitehead to show the different variants of information possessed by actual entity. In process philosophy, datum is obtained through the events of concrescence. Every actual entity has a variety of datum.\n\nWhitehead is not an idealist in the strict sense. Whitehead's thought may be regarded as related to the idea of panpsychism (also known as panexperientialism, because of Whitehead's emphasis on experience).\n\nWhitehead's philosophy is very complex, subtle and nuanced and in order to comprehend his thinking regarding what is commonly referred to by many religions as \"God\", it is recommended that one read from \"Process and Reality Corrected Edition\", wherein regarding \"God\" the authors elaborate Whitehead's conception.\n\"He is the unconditioned actuality of conceptual feeling at the base of things; so that by reason of this primordial actuality, there is an order in the relevance of eternal objects to the process of creation (343 of 413) (Location 7624 of 9706 Kindle ed.) Whitehead continues later with, \"The particularities of the actual world presuppose it ; while it merely presupposes the general metaphysical character of creative advance, of which it is the primordial exemplification (344 of 413) (Location 7634 of 9706 Kindle Edition).\" \n\nProcess philosophy, might be considered according to some theistic forms of religion to give a God a special place in the universe of occasions of experience. Regarding Whitehead's use of the term, \"occasions\" in reference to, \"God\" it is explained in \"Process and Reality Corrected Edition\" that \"'Actual entities'-also termed 'actual occasions'-are the final real things of which the world is made up. There is no going behind actual entities to find anything [28] more real. They differ among themselves: God is an actual entity, and so is the most trivial puff of existence in far-off empty space. But, though there are gradations of importance, and diversities of function, yet in the principles which actuality exemplifies all are on the same level. The final facts are, all alike, actual entities; and these actual entities are drops of experience, complex and interdependent.\n\nIt can be also be assumed within some forms of theology that a God encompasses all the other occasions of experience but also transcends them and this might lead to it being argued that Whitehead endorses some form of panentheism. Since, it is argued theologically, that \"free will\" is inherent to the nature of the universe, Whitehead's God is not omnipotent in Whitehead's metaphysics. God's role is to offer enhanced occasions of experience. God participates in the evolution of the universe by offering possibilities, which may be accepted or rejected. Whitehead's thinking here has given rise to process theology, whose prominent advocates include Charles Hartshorne, John B. Cobb, Jr., and Hans Jonas, who was also influenced by the non-theological philosopher Martin Heidegger. However, other process philosophers have questioned Whitehead's theology, seeing it as a regressive Platonism.\n\nWhitehead enumerated three essential \"natures of God\". The \"primordial\" nature of God consists of all potentialities of existence for actual occasions, which Whitehead dubbed eternal objects. God can offer possibilities by ordering the relevance of eternal objects. The \"consequent\" nature of God prehends everything that happens in reality. As such, God experiences all of reality in a sentient manner. The last nature is the \"superjective\". This is the way in which God's synthesis becomes a sense-datum for other actual entities. In some sense, God is prehended by existing actual entities.\n\nIn plant morphology, Rolf Sattler developed a process morphology (dynamic morphology) that overcomes the structure/process (or structure/function) dualism that is commonly taken for granted in biology. According to process morphology, structures such as leaves of plants do not have processes, they \"are\" processes.\n\nIn evolution and in development, the nature of the changes of biological objects are considered by many authors to be more radical than in physical systems. In biology, changes are not just changes of state in a pre-given space, instead the space and more generally the mathematical structures required to understand object change over time.\n\nWith its perspective that everything is interconnected, that all life has value, and that non-human entities are also experiencing subjects, process philosophy has played an important role in discourse on ecology and sustainability. The first book to connect process philosophy with environmental ethics was John B. Cobb, Jr.'s 1971 work, \"Is It Too Late: A Theology of Ecology\". In a more recent book (2018) edited by John B. Cobb, Jr. and Wm. Andrew Schwartz, \"Putting Philosophy to Work: Toward an Ecological Civilization\" contributors explicitly explore the ways in which process philosophy can be put to work to address the most urgent issues facing our world today, by contributing to a transition toward an ecological civilization. That book emerged out of the largest international conference held on the theme of ecological civilization (\"Seizing an Alternative: Toward an Ecological Civilization\") which was organized by the Center for Process Studies in June 2015. The conference brought together roughly 2,000 participants from around the world and featured such leaders in the environmental movement as Bill McKibben, Vandana Shiva, John B. Cobb, Jr., Wes Jackson, and Sheri Liao. The notion of ecological civilization is often affiliated with the process philosophy of Alfred North Whitehead--especially in China.\n\nIn the philosophy of mathematics, some of Whitehead's ideas re-emerged in combination with cognitivism as the cognitive science of mathematics and embodied mind theses.\n\nSomewhat earlier, exploration of mathematical practice and quasi-empiricism in mathematics from the 1950s to 1980s had sought alternatives to metamathematics in social behaviours around mathematics itself: for instance, Paul Erdős's simultaneous belief in Platonism and a single \"big book\" in which all proofs existed, combined with his personal obsessive need or decision to collaborate with the widest possible number of other mathematicians. The process, rather than the outcomes, seemed to drive his explicit behaviour and odd use of language, as if the synthesis of Erdős and collaborators in seeking proofs, creating sense-datum for other mathematicians, was itself the expression of a divine will. Certainly, Erdős behaved as if nothing else in the world mattered, including money or love, as emphasized in his biography \"The Man Who Loved Only Numbers\".\n\nSeveral fields of science and especially medicine seem to make liberal use of ideas in process philosophy, notably the theory of pain and healing of the late 20th century. The philosophy of medicine began to deviate somewhat from scientific method and an emphasis on repeatable results in the very late 20th century by embracing population thinking, and a more pragmatic approach to issues in public health, environmental health and especially mental health. In this latter field, R. D. Laing, Thomas Szasz and Michel Foucault were instrumental in moving medicine away from emphasis on \"cures\" and towards concepts of individuals in balance with their society, both of which are changing, and against which no benchmarks or finished \"cures\" were very likely to be measurable.\n\nIn psychology, the subject of imagination was again explored more extensively since Whitehead, and the question of feasibility or \"eternal objects\" of thought became central to the impaired theory of mind explorations that framed postmodern cognitive science. A biological understanding of the most eternal object, that being the emerging of similar but independent cognitive apparatus, led to an obsession with the process \"embodiment\", that being, the emergence of these cognitions. Like Whitehead's God, especially as elaborated in J. J. Gibson's perceptual psychology emphasizing affordances, by ordering the relevance of eternal objects (especially the cognitions of other such actors), the world becomes. Or, it becomes simple enough for human beings to begin to make choices, and to prehend what happens as a result. These experiences may be summed in some sense but can only approximately be shared, even among very similar cognitions with identical DNA. An early explorer of this view was Alan Turing who sought to prove the limits of expressive complexity of human genes in the late 1940s, to put bounds on the complexity of human intelligence and so assess the feasibility of artificial intelligence emerging. Since 2000, Process Psychology has progressed as an independent academic and therapeutic discipline: In 2000, Michel Weber created the Whitehead Psychology Nexus: an open forum dedicated to the cross-examination of Alfred North Whitehead's process philosophy and the various facets of the contemporary psychological field.\n\n\n\n"}
{"id": "288400", "url": "https://en.wikipedia.org/wiki?curid=288400", "title": "Provenance", "text": "Provenance\n\nProvenance (from the French \"provenir\", 'to come from/forth') is the chronology of the ownership, custody or location of a historical object. The term was originally mostly used in relation to works of art but is now used in similar senses in a wide range of fields, including archaeology, paleontology, archives, manuscripts, printed books and science and computing. The primary purpose of tracing the provenance of an object or entity is normally to provide contextual and circumstantial evidence for its original production or discovery, by establishing, as far as practicable, its later history, especially the sequences of its formal ownership, custody and places of storage. The practice has a particular value in helping authenticate objects. Comparative techniques, expert opinions and the results of scientific tests may also be used to these ends, but establishing provenance is essentially a matter of documentation. The term dates to the 1780s in English. Provenance is conceptually comparable to the legal term \"chain of custody\".\n\nIn archaeology and paleontology, the derived term provenience is used with a related but very particular meaning, to refer to the location (in modern research, recorded precisely in three dimensions) where an artifact or other ancient item was found. \"Provenance\" covers an object's complete documented history. An artifact may thus have both a provenience and a provenance.\n\nThe provenance of works of fine art, antiques and antiquities is of great importance, especially to their owner. There are a number of reasons why painting provenance is important, which mostly also apply to other types of fine art. A good provenance increases the value of a painting, and establishing provenance may help confirm the date, artist and, especially for portraits, the subject of a painting. It may confirm whether a painting is genuinely of the period it seems to date from. The provenance of paintings can help resolve ownership disputes. For example, provenance between 1933 and 1945 can determine whether a painting was looted by the Nazis. Many galleries are putting a great deal of effort into researching the provenance of paintings in their collections for which there is no firm provenance during that period. Documented evidence of provenance for an object can help to establish that it has not been altered and is not a forgery, a reproduction, stolen or looted art. Provenance helps assign the work to a known artist, and a documented history can be of use in helping to prove ownership. An example of a detailed provenance is given in the Arnolfini portrait.\n\nThe quality of provenance of an important work of art can make a considerable difference to its selling price in the market; this is affected by the degree of certainty of the provenance, the status of past owners as collectors, and in many cases by the strength of evidence that an object has not been illegally excavated or exported from another country. The provenance of a work of art may vary greatly in length, depending on context or the amount that is known, from a single name to an entry in a scholarly catalogue some thousands of words long.\n\nAn expert certification can mean the difference between an object having no value and being worth a fortune. Certifications themselves may be open to question. Jacques van Meegeren forged the work of his father Han van Meegeren (who in his turn had forged the work of Vermeer). Jacques sometimes produced a certificate with his forgeries stating that a work was created by his father.\n\nJohn Drewe was able to pass off as genuine paintings, a large number of forgeries that would have easily been recognised as such by scientific examination. He established an impressive (but false) provenance and because of this galleries and dealers accepted the paintings as genuine. He created this false provenance by forging letters and other documents, including false entries in earlier exhibition catalogues.\n\nSometimes provenance can be as simple as a photograph of the item with its original owner. Simple yet definitive documentation such as that can increase its value by an order of magnitude, but only if the owner was of high renown. Many items that were sold at auction have gone far past their estimates because of a photograph showing that item with a famous person. Some examples include antiques owned by politicians, musicians, artists, actors, etc.\n\nThe objective of provenance research is to produce a complete list of owners (together, where possible, with the supporting documentary proof) from when the painting was commissioned or in the artist's studio through to the present time. In practice, there are likely to be gaps in the list and documents that are missing or lost. The documented provenance should also list when the painting has been part of an exhibition and a bibliography of when it has been discussed (or illustrated) in print.\n\nWhere the research is proceeding backwards, to discover the previous provenance of a painting whose current ownership and location is known, it is important to record the physical details of the painting – style, subject, signature, materials, dimensions, frame, etc. The titles of paintings and the attribution to a particular artist may change over time. The size of the work and its description can be used to identify earlier references to the painting. The back of a painting can contain significant provenance information. There may be exhibition marks, dealer stamps, gallery labels and other indications of previous ownership. There may also be shipping labels. In the BBC TV programme \"Fake or Fortune?\" the provenance of the painting \"Bords de la Seine à Argenteuil\" was investigated using a gallery sticker and shipping label on the back. Early provenance can sometimes be indicated by a \"cartellino\" (a representation of an inscribed label) added to the front of a painting. However, these can be forged, or can fade or be painted over.\n\nAuction records are an important resource to assist in researching the provenance of paintings. \n\nIf a painting has been in private hands for an extended period and on display in a stately home, it may be recorded in an inventory – for example, the Lumley inventory. The painting may also have been noticed by a visitor who subsequently wrote about it. It may have been mentioned in a will or a diary. Where the painting has been bought from a dealer, or changed hands in a private transaction, there may be a bill of sale or sales receipt that provides evidence of provenance. Where the artist is known, there may be a catalogue raisonné listing all the artist's known works and their location at the time of writing. A database of catalogues raisonnés is available at the International Foundation for Art Research. Historic photos of the painting may be discussed and illustrated in a more general work on the artist, period or genre. Similarly, a photograph of a painting may show inscriptions (or a signature) that subsequently became lost as a result of overzealous restoration. Conversely, a photograph may show that an inscription was not visible at an earlier date. One of the disputed aspects of the \"Rice\" portrait of Jane Austen concerns apparent inscriptions identifying artist and sitter.\n\nProvenance – also known as \"custodial history\" – is a core concept of archival science and archival processing. The term refers to the individuals, groups, or organizations that originally created or received the items in an accumulation of records, and to the items' subsequent chain of custody. The principle of provenance (sometimes also termed the principle of \"archival integrity\" or \"respect des fonds\") stipulates that records originating from a common source (or fonds) should be kept together – where practicable, physically; but in all cases intellectually, in the way in which they are catalogued and arranged in finding aids. Conversely, records of different provenance should be preserved and documented separately. In archival practice, proof of provenance is provided by the operation of control systems that document the history of records kept in archives, including details of amendments made to them. The authority of an archival document or set of documents of which the provenance is uncertain (because of gaps in the recorded chain of custody) will be considered to be severely compromised.\n\nThe principles of archival provenance were developed in the 19th century by both French and Prussian archivists, and gained widespread acceptance on the basis of their formulation in the \"Manual for the Arrangement and Description of Archives\" by Dutch state archivists Samuel Muller, J. A. Feith, and R. Fruin, published in the Netherlands in 1898 (often referred to as the \"Dutch Manual\").\n\nSeamus Ross has argued a case for adapting established principles and theories of archival provenance to the field of modern digital preservation and curation.\n\n\"Provenance\" is also the title of the journal published by the Society of Georgia Archivists.\n\nIn the case of books, the study of provenance refers to the study of the ownership of individual copies of books. It is usually extended to include study of the circumstances in which individual copies of books have changed ownership, and of evidence left in books that shows how readers interacted with them.\n\nProvenance studies may shed light on the books themselves, providing evidence of the role particular titles have played in social, intellectual and literary history. Such studies may also add to our knowledge of particular owners of books. For instance, looking at the books owned by a writer may help to show which works influenced him or her.\n\nMany provenance studies are historically focused, and concentrated on books owned by writers, politicians and public figures. The recent ownership of books is studied, however, as is evidence of how ordinary or anonymous readers have interacted with books.\n\nProvenance can be studied both by examining the books themselves (for instance looking at inscriptions, marginalia, bookplates, book rhymes, and bindings) and by reference to external sources of information such as auction catalogues.\n\nIn transactions of old wine with the potential of improving with age, the issue of provenance has a large bearing on the assessment of the contents of a bottle, both in terms of quality and the risk of wine fraud. A documented history of wine cellar conditions is valuable in estimating the quality of an older vintage due to the fragile nature of wine.\n\nRecent technology developments have aided collectors in assessing the temperature and humidity history or the wine which are two key components in establishing perfect provenance. For example, there are devices available that rest inside the wood case and can be read through the wood by waving a smartphone equipped with a simple app. These devices track the conditions the case has been exposed to for the duration of the battery life, which can be as long as 15 years, and sends a graph and high/low readings to the smartphone user. This takes the trust issue out of the hands of the owner and gives it to a third party for verification.\n\nArchaeology and anthropology researchers use \"provenience\" to refer to the exact location or find spot of an artifact, a bone or other remains, a soil sample, or a feature within an ancient site, whereas \"provenance\" covers an object's complete documented history. Ideally, in modern excavations, the provenience is recorded in three dimensions on a site grid with great precision, and may also be recorded on video to provide additional proof and context. In older work, often undertaken by amateurs, only the general site or approximate area may be known, especially when an artifact was found outside a professional excavation and its specific position not recorded. The term \"provenience\" appeared in the 1880s, about a century after \"provenance\". Outside of academic contexts, it has been used as a synonymous variant spelling of \"provenance\", especially in American English.\n\nAny given antiquity may have both a provenience (where it was found) and a provenance (where it has been since it was found). A summary of the distinction is that \"provenience is a fixed point, while provenance can be considered an itinerary that an object follows as it moves from hand to hand.\" Another metaphor is that provenience is an artifact's \"birthplace\", while provenance is its \"résumé\", though this is imprecise (many artifacts originated as trade goods created in one region but were used and finally deposited in another).\n\nAside from scientific precision, a need for the distinction in these fields has been described thus:\nIn this context, the \"provenance\" can occasionally be the detailed history of where an object has been since its creation, as in art history contexts – not just since its modern finding. In some cases, such as where there is an inscription on the object, or an account of it in written materials from the same era, an object of study in archaeology or cultural anthropology may have an early provenance – a known history that predates modern research – then a provenience from its modern finding, and finally a continued provenance relating to its handling and storage or display after the modern acquisition.\n\nEvidence of provenance in the more general sense can be of importance in archaeology. Fakes are not unknown, and finds are sometimes removed from the context in which they were found without documentation, reducing their value to science. Even when apparently discovered \"in situ\", archaeological finds are treated with caution. The provenience of a find may not be properly represented by the context in which it was found (e.g. due to stratigraphic layers being disturbed by erosion, earthquakes, or ancient reconstruction or other disturbance at a site. Artifacts can also be moved through looting as well as trade, far from their place of origin and long before modern rediscovery. Further research is often required to establish the true provenance of a find, and what the relationship is between the exact provenience and the overall provenance.\n\nIn paleontology and paleoanthropology, it is recognized that fossils can also move from their primary context and are sometimes found, apparently \"in-situ\", in deposits to which they do not belong because they have been moved, for example, by the erosion of nearby but different outcrops. It is unclear how strictly paleontology maintains the \"provenience\" and \"provenance\" distinction. For example, a short glossary at a website (primarily aimed at young students) of the American Museum of Natural History treats the terms as synonymous, while scholarly paleontology works make frequent use of \"provenience\" in the same precise sense as used in archaeology and paleoanthropology.\n\nWhile exacting details of a find's provenience are primarily of use to scientific researchers, most natural history and archaeology museums also make strenuous efforts to record how the items in their collections were acquired. These records are often of use in helping to establish a chain of provenance.\n\nScientific research is generally held to be of good provenance when it is documented in detail sufficient to allow reproducibility. Scientific workflow systems assist scientists and programmers with tracking their data through all transformations, analyses, and interpretations. Data sets are reliable when the process used to create them are reproducible and analyzable for defects. Current initiatives to effectively manage, share, and reuse ecological data are indicative of the increasing importance of data provenance. Examples of these initiatives are National Science Foundation Datanet projects, DataONE and Data Conservancy, as well as the U.S. Global Change Research Program. Some international academic consortia, such as the Research Data Alliance, have specific group to tackle issues of provenance. In that case it is the Research Data Provenance Interest Group.\n\nWithin computer science, informatics uses the term 'provenance' to mean the lineage of data, as per data provenance, with research in the last decade extending the conceptual model of causality and relation to include processes that act on data and agents that are responsible for those processes. See, for example, the proceedings of the International Provenance Annotation Workshop (IPAW) and Theory and Practice of Provenance (TaPP). Semantic web standards bodies, including the World Wide Web Consortium in 2014, have ratified a standard data model for provenance representation known as PROV which draws from many of the better-known provenance representation systems that preceded it, such as the Proof Markup Language and the Open Provenance Model.\n\nInteroperability is a design goal of most recent computer science provenance theories and models, for example the Open Provence Model (OPM) 2008 generation workshop aimed at \"establishing inter-operability of systems\" through information exchange agreements. Data models and serialisation formats for delivering provenance information typically reuse existing metadata models where possible to enable this. Both the OPM Vocabulary and the PROV Ontology make extensive use of metadata models such as Dublin Core and Semantic Web technologies such as the Web Ontology Language (OWL). Current practice is to rely on the W3C PROV data model, OPM's successor.\n\nThere are several maintained and open-source provenance capture implementation at the operating system level such as CamFlow, Progger for Linux and MS Windows, and SPADE for Linux, MS Windows, and MacOS. Other implementations exist for specific programming and scripting languages, such as RDataTracker for R, and NoWorkflow for Python.\n\n\nIn the geologic use of the term, provenance instead refers to the origin or source area of particles within a rock, most commonly in sedimentary rocks. It does not refer to the circumstances of the collection of the rock. The provenance of sandstone, in particular, can be evaluated by determining the proportion of quartz, feldspar, and lithic fragments (see diagram).\n\nSeed provenance refers to the specified area in which plants that produced seed are located or were derived. Local provenancing is a position maintained by ecologists that suggests that only seeds of local provenance should be planted in a particular area. However, this view depends on the adaptationist program – a view that populations are universally locally adapted. It is maintained that local seed is best adapted to local conditions, and that outbreeding depression will be avoided. Evolutionary biologists suggest that strict adherence to provenance collecting is not a wise decision because:\n\nProvenance trials, where material of different provenances are planted in a single place or at different locations spanning a range of environmental conditions, is a way to reveal genetic variation among provenances. It also contributes to an understanding of how different provenances respond to various climatic and environmental conditions and can as such contribute with knowledge on how to strategically select provenances for climate change adaptation.\n\nThe term \"provenance\" is used when ascertaining the source of goods such as computer hardware to assess if they are genuine or counterfeit. Chain of custody is an equivalent term used in law, especially for evidence in criminal or commercial cases.\n\nSoftware provenance encompasses the origin of software and its licensing terms. For example, when incorporating a free, open source or proprietary software component in an application, one may wish to understand its provenance to ensure that licensing requirements are fulfilled and that other software characteristics can be understood.\n\nData provenance covers the provenance of computerized data. There are two main aspects of data provenance: ownership of the data and data usage. Ownership will tell the user who is responsible for the source of the data, ideally including information on the originator of the data. Data usage gives details regarding how the data has been used and modified and often includes information on how to cite the data source or sources. Data provenance is of particular concern with electronic data, as data sets are often modified and copied without proper citation or acknowledgement of the originating data set. Databases make it easy to select specific information from data sets and merge this data with other data sources without any documentation of how the data was obtained or how it was modified from the original data set or sets.\nThe automated analysis of data provenance graphs has been described as a mean to verify compliance with regulations regarding data usage such as introduced by the EU GDPR.\n\nSecure Provenance refers to providing integrity and confidentiality guarantees to provenance information. In other words, secure provenance means to ensure that history cannot be rewritten, and users can specify who else can look into their actions on the object.\n\nA simple method of ensuring data provenance in computing is to mark a file as read only. This allows the user to view the contents of the file, but not edit or otherwise modify it. Read only can also in some cases prevent the user from accidentally or intentionally deleting the file.\n\n\nProvenance in book studies\n\n"}
{"id": "36607703", "url": "https://en.wikipedia.org/wiki?curid=36607703", "title": "PyLadies", "text": "PyLadies\n\nPyLadies is an international mentorship group which focuses on helping more women become active participants in the Python open-source community. It is part of the Python Software Foundation. It was started in Los Angeles in 2011. The mission of the group is to create a diverse Python community through outreach, education, conferences and social gatherings. PyLadies also provides funding for women to attend open source conferences. The aim of PyLadies is increasing the participation of women in computing. PyLadies became a multi-chapter organization with the founding of the Washington (D.C.) chapter in August 2011. The group currently has more than 40 chapters around the world. \n\nThe organization was created in Los Angeles in April 2011 by seven women: Audrey Roy Greenfeld, Christine Cheung, Esther Nam, Jessica Stanton, Katharine Jarmul, Sandy Strong, and Sophia Viklund. Around 2012, the organization filed for nonprofit status.\n\nAnother PyLadies chapter opened in Dublin in 2013. PyLadies started a chapter in Tokyo in 2014. In 2018, PyLadies opened up a chapter in St. Petersburg.\n\nPyLadies has conducted outreach events for both beginners and experienced users. PyLadies has conducted hackathons, ladies' nights and workshops for Python enthusiasts.\n\nEach chapter is free to run themselves as they wish as long as they are focused on the goal of empowering women. Women make up the majority of the group, but membership is not limited to women and the group is open to helping people who identify as other gender identities as well. \n\n"}
{"id": "17808461", "url": "https://en.wikipedia.org/wiki?curid=17808461", "title": "Raman Science Centre", "text": "Raman Science Centre\n\nThe Raman Science Centre and Raman Planetarium Complex at Nagpur is an interactive science centre affiliated with Mumbai's Nehru Science Centre. The centre was developed to promote a scientific attitude, portray the growth of science and technology and their applications in industry and human welfare, and hold science exhibits. The centre is named after famous Nobel Prize winner Indian physicist Chandrasekhara Venkata Raman.\nThe Raman Science Centre was inaugurated on 7 March 1992 and the planetarium was started on 5 January 1997. The centre is located opposite Gandhi Sagar Lake in the heart of Nagpur. Between 1 April 2014 and 31 March 2015 the Centre recorded a visitor count of 582,962. The centre is part of the National Council of Science Museums (NCSM), India which is also the largest network of science centres/museums under a single administrative umbrella in the world. NCSM rates the centre as regional level and it has a total floor area of 4333 sq meters.\nThe centre carries out numerous programs to spread science and technology knowledge amongst the general public. The centre has started innovation centre from 14 February 2017, that gives opportunities to students, who are dedicated to science. The centre along with local NGO Hirwai gives the \"Green Finger Award\" to create awareness about the environment amongst school children. In August 2007, the information and communication technology gallery was opened where ISRO Chairman Madhavan Nair declared that India will send astronauts in space by 2015. The centre currently has 4 different interactive galleries, an Innovation Centre, a Science on a Sphere Show and a 133-seat planetarium, fun science, a prehistoric Animal Park and more. The centre also holds science lectures, science film shows and 3-dimensional science shows. The centre also organises activities like planet watching and other celestial phenomena for citizens.\n\nSchedule of the science centre\n\nCharges\n"}
{"id": "902940", "url": "https://en.wikipedia.org/wiki?curid=902940", "title": "Recognition heuristic", "text": "Recognition heuristic\n\nThe recognition heuristic, originally termed the recognition principle, has been used as a model in the psychology of judgment and decision making and as a heuristic in artificial intelligence. The goal is to make inferences about a criterion that is not directly accessible to the decision maker, based on recognition retrieved from memory. This is possible if recognition of alternatives has relevance to the criterion. For two alternatives, the heuristic is defined as: \n\nThe recognition heuristic is part of the \"adaptive toolbox\" of \"fast and frugal\" heuristics proposed by Gigerenzer and Goldstein. It is one of the most frugal of these, meaning it is simple or economical. In their original experiment, Daniel Goldstein and Gerd Gigerenzer quizzed students in Germany and the United States on the populations of both German and American cities. Participants received pairs of city names and had to indicate which city has more inhabitants. In this and similar experiments, the recognition heuristic typically describes about 80–90% of participants' choices, in cases where they recognize one but not the other object (see criticism of this measure below). Surprisingly, American students scored higher on German cities, while German participants scored higher on American cities, despite only recognizing a fraction of the foreign cities. This has been labeled the \"less-is-more effect\" and mathematically formalized.\n\nThe recognition heuristic is posited as a domain-specific strategy for inference. It is ecologically rational to rely on the recognition heuristic in domains where there is a correlation between the criterion and recognition. The higher the recognition validity α for a given criterion, the more ecologically rational it is to rely on this heuristic and the more likely people will rely on it. For each individual, α can be computed by\n\nwhere C is the number of correct inferences the recognition heuristic would make, computed across all pairs in which one alternative is recognized and the other is not, and W is the number of wrong inferences. Domains in which the recognition heuristic was successfully applied include the prediction of geographical properties (such as the size of cities, mountains, etc.), of sports events (such as Wimbledon and soccer championships) and elections. Research also shows that the recognition heuristic is relevant to marketing science. Recognition based heuristics help consumers choose which brands to buy in frequently purchased categories. A number of studies addressed the question of whether people rely on the recognition heuristic in an ecologically rational way. For instance, name recognition of Swiss cities is a valid predictor of their population (α = 0.86) but not their distance from the center of Switzerland (α = 0.51). Pohl reported that 89% of inferences accorded with the model in judgments of population, compared to only 54% in judgments of the distance. More generally, there is a positive correlation of r = 0.64 between the recognition validity and the proportion of judgments consistent with the recognition heuristic across 11 studies. Another study by Pachur suggested that the recognition heuristic is more likely a tool for exploring natural rather than induced recognition (i.e. not provoked in a laboratory setting) when inferences have to be made from memory. In one of his experiments, the results showed that there was a difference between participants in an experimental setting vs. a non-experimental setting.\n\nIf α > β, and α, β are independent of n, then a less-is-more effect will be observed. Here, β is the knowledge validity, measured as C/(C+W) for all pairs in which both alternatives are recognized, and n is the number of alternatives an individual recognizes. A less-is-more effect means that the function between accuracy and n is inversely U-shaped rather than monotonically increasing. Some studies reported less-is-more effects empirically among two, three, or four alternatives and in group decisions), whereas others failed to do so, possibly because the effect is predicted to be small (see Katsikopoulos).\n\nSmithson explored the \"less-is-more effect\" (LIME) with the recognition heuristic and challenges some of the original assumptions. The LIME occurs when a \"recognition-dependent agent has a greater probability of choosing the better item than a more knowledgeable agent who recognizes more items.\" A mathematical model is used in describing the LIME and Smithson’s study used it and attempted to modify it. The study was meant to mathematically provide an understanding of when the LIME occurs and explain the implications of the results. The main implication is \"that the advantage of the recognition cue depends not only on the cue validities, but also on the order in which items are learned\".\n\nThe recognition heuristic can also be depicted using neuroimaging techniques. A number of studies have shown that people do not automatically use the recognition heuristic when it can be applied, but evaluate its ecological validity. It is less clear, however, how this evaluation process can be modeled. A functional magnetic resonance imaging study tested whether the two processes, recognition and evaluation, can be separated on a neural basis. Participants were given two tasks; the first involved only a recognition judgment (\"Have you ever heard of Modena? Milan?\"), while the second involved an inference in which participants could rely on the recognition heuristic (\"Which city has the larger population: Milan or Modena?\"). For mere recognition judgments, activation in the precuneus, an area that is known from independent studies to respond to recognition confidence, was reported. In the inference task, precuneus activation was also observed, as predicted, and activation was detected in the anterior frontomedian cortex (aFMC), which has been linked in earlier studies to evaluative judgments and self-referential processing. The aFMC activation could represent the neural basis of this evaluation of ecological rationality.\n\nSome researchers have used event-related potentials (ERP) to test psychological mechanisms behind the recognition heuristic. Rosburg, Mecklinger, and Frings used a standard procedure with a city-size comparison task, similar to that used by Goldstein and Gigerenzer. They used ERP and analyzed familiarity-based recognition occurring 300-450 milliseconds after stimulus onset in order to predict the participants’ decisions. Familiarity-based recognition processes are relatively automatic and fast so these results provide evidence that simple heuristics like the recognition heuristic utilize basic cognitive processes.\n\nResearch on the recognition heuristic has sparked a number of controversies.\n\nThe recognition heuristic is a model that relies on recognition only. This leads to the testable prediction that people who rely on it will ignore strong, contradicting cues (i.e., do not make trade-offs; so-called noncompensatory inferences). In an experiment by Daniel M. Oppenheimer participants were presented with pairs of cities, which included actual cities and fictional cities. Although the recognition heuristic predicts that participants would judge the actual (recognizable) cities to be larger, participants judged the fictional (unrecognizable) cities to be larger, showing that more than recognition can play a role in such inferences.\n\nNewell & Fernandez performed two experiments to try to test the claims that the recognition heuristic is distinguished from availability and fluency through binary treatment of information and inconsequentiality of further knowledge. The results of their experiments did not support these claims. Newell & Fernandez and Richter & Späth tested the non-compensatory prediction of the recognition heuristic and stated that \"recognition information is not used in an all-or-none fashion but is integrated with other types of knowledge in judgment and decision making.\"\n\nA reanalysis of these studies at an individual level, however, showed that typically about half of the participants consistently followed the recognition heuristic in every single trial, even in the presence of up to three contradicting cues. Furthermore, in response to those criticisms, Marewski et al. pointed out that none of the studies above formulated and tested a compensatory strategy against the recognition heuristic, leaving the strategies that participants relied on unknown. They tested five compensatory models and found that none could predict judgments better than the simple model of the recognition heuristic.\n\nOne major criticism of studies on the recognition heuristic that was raised was that mere accordance with the recognition heuristic is not a good measure of its use. As an alternative, Hilbig et al. proposed to test the recognition heuristic more precisely devised a multinomial processing tree model for the recognition heuristic. A multinomial processing tree model is a simple statistical model often used in cognitive psychology for categorical data. Hilbig et al. claimed that a new model of recognition heuristic use was needed due to the confound between recognition and further knowledge. The multinomial processing tree model was shown to be effective and Hilbig et al. claimed that it provided an unbiased measure of the recognition heuristic.\n\nPachur stated that it is an imperfect model but currently it is still the best model to predict people’s recognition-based inferences. He believes that precise tests have a limited value basically because certain aspects of the recognition heuristic are often ignored and so the results could be inconsequential or misleading.\n\nHilbig et al. state that heuristics are meant to reduce effort and that the recognition heuristic reduces effort in making judgments by relying on one single cue and ignoring other information. In their study, they found that the recognition heuristic is more useful in deliberate thought than in intuitive thought. This means it is more useful when thoughts are intentional and not impulsive as opposed to intuitive thought, which is based more on impulse rather than conscious reasoning. \nIn contrast, a study by Pachur and Hertwig found that it is actually the faster responses that are more in line with the recognition heuristic. Also, judgments accorded more strongly with the recognition heuristic under time pressure. In line with these findings, neural evidence suggests that the recognition heuristic may be relied upon by default.\n\nGoldstein and Gigerenzer state that due to its simplicity, the recognition heuristic shows to what degree and in what situations behavior can be predicted. Some researchers suggest that the idea of the recognition heuristic should be retired but Pachur believes that a different approach should be taken in testing it. There are some researchers who believe that the recognition heuristic should be investigated using precise tests of the exclusive use of recognition.\n\nAnother study by Pachur suggested that the recognition heuristic is more likely a tool for exploring natural rather than induced recognition (i.e. not provoked in a laboratory setting) when inferences have to be made from memory. In one of his experiments, the results showed that there was a difference between participants in an experimental setting vs. a non-experimental setting.\n\nUsing an adversarial collaboration approach, three special issues of the open access journal \"Judgment and Decision Making\" have been devoted to unravel the support for and problems with the recognition heuristic, providing the most recent and comprehensive synopsis of the epistemic status quo. In their Editorial to Issue III, the three guest editors strive for a cumulative theory integration.\n"}
{"id": "665503", "url": "https://en.wikipedia.org/wiki?curid=665503", "title": "Science Olympiad", "text": "Science Olympiad\n\nScience Olympiad is an American team competition in which students compete in 23 events pertaining to various scientific disciplines, including earth science, biology, chemistry, physics, and engineering. Over 7,800 middle school and high school teams from 50 U.S. states compete each year. U.S. territories do not compete; however, since 2012 high school teams from Japan have competed at the national tournament as unranked guests.\n\nThere are multiple levels of competition: invitational, regional, state, and national. Invitational tournaments, run by high schools and universities, are unofficial tournaments and serve as practice for regional and state competitions. Teams that excel at regional competitions advance to the state level; the top one or two teams from each state (depending on the state) then advance the national level. Winners later receive several kinds of awards, including medals, trophies and plaques, as well as scholarships. The program for elementary-age students is less common and consistent. Schools have flexibility to implement the program to meet their needs. Some communities host competitive elementary tournaments.\n\nScience Olympiad is not associated with the International Science Olympiads, which follow a completely different format and set of rules.\n\nThe first recorded Science Olympiad was held on Saturday, November 23, 1974 at St. Andrews Presbyterian College in Laurinburg, North Carolina. Dr. Barnes and Dr. David Wetmore were the originators of this event. Fifteen schools from North and South Carolina participated in this event. It was a day-long affair, with competitions and demonstrations for high school students in the areas of biology, chemistry, and physics. There were four event periods during this day and each event period had one fun event (like beaker race or paper airplane), one demonstration (like glassblowing and holography), and one serious event (like periodic table quiz or Science Bowl). An article by David Wetmore was published in the Journal of Chemical Education in January 1978 documenting the success of recruiting students through Science Olympiad. St. Andrews Presbyterian College continues to host a Science Olympiad tournament to this day. Mr. John C. \"Jack\" Cairns was a teacher at Dover High School in Delaware when he learned about the Science Olympiad tournament in North Carolina. He shared this information with Dr. Douglas R. Macbeth, the Delaware State Science Supervisor. Mr. Cairns was appointed to a steering committee to organize the first Science Olympiad in Delaware which took place at Delaware State University in the Spring of 1977. A write-up in \"The Science Teacher\" of December 1977 caught the attention of Dr. Gerard Putz, who proposed that the program be expanded throughout the United States. After competition tests in Michigan at the Lawrence Institute of Technology and Oakland University in 1983 and 1984, Putz and Delaware director John Cairns took their plan for a national competition to the National Science Teachers Conference in Boston. The first National Tournament was attended by representatives of 17 states, held at Michigan State University in 1985. Since then, the program has expanded greatly, with 60 teams present in each division at the National Tournament. In 2012, a Global Ambassador Team from Japan was invited to attend the national tournament at the University of Central Florida. Japan continues to send a team, as of the 2017 National Tournament.\n\nThere are three divisions in the hierarchy of Science Olympiad:\n\n\nHowever, the national tournament and generally state and regional tournaments are only for divisions B and C. Division A teams usually have separate interscholastic tournaments, apart from the more common intra-school competitions. Note that 6th and 9th graders have the option of competing in either of the two divisions in which they meet the grade requirements and are part of the competing school. A middle school may, however, only use up to 5 members who have graduated to the next school if they are in 9th grade or lower. Students in grades lower than the division in which the school competes in may also be on the team. Teams are restricted to five 9th graders for division B and seven 12th graders for division C. Students may not participate on multiple teams, e.g. a 9th grader on both a high school and middle school team would not be allowed.\n\nIn Divisions B and C, teams may compete in up to twenty-three main events, which usually occur over a single day (some tournaments, such as the Texas State tournament, run competitive events over multiple days); done by a team of no more than 15 members. Events fall into five main categories: Life, Personal, and Social Science, Earth & Space Science, Physical Science & Chemistry, Technology & Engineering, Inquiry & Nature of Science. They are either knowledge-based (for example, written tests on earth science, physics, astronomy, or biology), hands-on (for example, chemistry lab practicals or events involving both device testing and an exam), or engineering-based (participants construct a device to do specified tasks).\n\nKnowledge-based events generally have two participants taking a test and/or mathematically analyzing data. Examples of such events are Anatomy and Physiology, Meteorology, and Remote Sensing.\n\nHands-on events generally consist of two participants performing experiments or interacting with physical objects to achieve a certain goal. Some examples are Forensics, Experimental Design, and Hovercraft.\n\nEngineering-based events have a team of two to three participants. They are to construct a device following a specific event's parameters and test the device against others. Examples include Battery Buggy, Towers, and Mission Possible.\n\nThe majority of events allow two team members, though a few allow more. If one member is unable to attend an event, the other is able to continue, depending on the event, with the competition, though at an obvious disadvantage. If the team has one available, a backup team member may be placed with the member as opposed to their former partner.\n\nThe list and rules for events change and are updated every year to input dynamism and to limit the advantages of more experienced teams.\n\nStates have substantial leeway in how they run their organization; several states, notably North Carolina and Texas, run altered slates of events; in the case of Texas, teams can choose to replace National events with state-exclusive events.\n\nIncludes the Division B and Division C events designated by the national committee. For information on state-specific events, please check your state website.\nTrial/Pilot events are, at Regional and State tournaments, events that are specific to that state that are being considered as events for the next year. At Regionals and States, these events may count towards the team's score. At Nationals, however, there is a completely different set of Trial/Pilot events, sometimes known as \"alternate events\" because the people entering them do not have to be on the official team. These do not count towards the team's score, but ribbons and medals are usually awarded.\n\nThe terms \"trial event\" and \"pilot event\" (also called \"exploratory event\") are sometimes interchangeable, both pertaining to an event that is not an official, national event for the year. However, at the National Tournament, there are often two differences. First, in 2010, it was announced that medals would only be awarded to the top 3 in the Trial events, but not at all in the Pilot events. Also, the Trial event are often much closer to becoming official events for following years than pilot events. Almost all of the Trial events from recent National tournaments have become official events within a few years of the tournament, while the same is not true for almost any of the pilot events.\n\nTeams are hosted by the school from which the participants attend. Science Olympiad is most often run as an after-school extracurricular activity, but some schools offer Science Olympiad classes that allow students to receive academic credit for participation. A teacher, parent, or student (usually a volunteer) coordinates the team in practice and preparation for the competition. Often there are others who coach individual events as well. A team can consist of up to 15 students and any number of alternates; some states allow more students per team. At the middle school level at nationals, only five ninth graders are allowed to compete on one team; at the high school level, only seven twelfth graders are allowed per team. However, state organizations occasionally amend these rules. Homeschool groups may also form teams to compete.\n\nAlthough teams may have an unlimited number of alternates, it is implicitly stated within the rules that competitors present at the event must have completed all of the work on their event. This is specifically aimed at building events. It is illegal for teams to have their alternates as \"builders\" and their formal team members as \"thinkers\". Judges at the event are allowed to ask any question of the machine or contraption in an effort to keep the scenario above from occurring. Nonetheless, competitors, coaches, and entire teams are expected to have integrity and to abide by this rule.\n\nA great deal of strategy usually goes into forming a team. Since events go on at the same time as other events during a competition and conflicts may occur, the coach or coordinator must make decisions based on the competitor's specialty and ability in order to correctly place him/her. Sometimes, usually during the reformation of competitors when a team advances a level, a competitor who wasn't originally planned to compete in a certain event may have to compete in it to fill the certain event slot.\n\nThe of the competition is determined by each team's overall score. Each school is ranked in every event based on that event's rules. For each type of event, the ranking differs. Knowledge events are scored by the correct number of answers; the team with the highest score will receive 1st place, the second highest will receive 2nd place, and so on. If two teams are tied, there are usually tiebreaker questions that apply only to those teams that are tied. The non-testing events are scored based off the individual requirements listed in the Science Olympiad rule book, released each year to reflect new events, requirements, and clarifications. Some events, such as the knowledge/testing-based will rank teams by using the highest scores. However others may use the lowest score. The team's overall score is then calculated by adding together the rank of the school in all events (e.g. 1st place receives 1 point, 2nd place 2 points, etc.). The team with the lowest overall score is declared the winner. However, it should be noted that some state competitions choose to score the competition by awarding more points per place (e.g. 13 points for 1st place, 12 points for 2nd place, etc.) and having the team with the most points being declared the winner.\n\nThere are several ways to break a tie (draw):\n\n\nScience Olympiad competitions occur at the regional, state and national level. Normally, the top few teams advance from the regional level to state competition, the exact number depending on how many regions there are and how many teams compete. For example, in Ohio, the number of teams qualifying for the state tournament from each regional depends solely on the number of teams participating at that regional, whereas in New York the allocation system involved determining whether or not the winning team in a regional tournament had won the previous year (this method has since been discontinued). In most states, the top team advances from the state to the national competition. Some states with a larger number of teams are allotted a second spot at the national competition to represent their larger participation. Currently, 120 teams compete at the national level each year (60 from Division B and 60 from Division C); the number has changed over the years to accommodate growing participation.\n\nMany states also hold invitational tournaments. These competitions serve as \"practice rounds\" for qualifying tournaments, and are hosted by individual middle schools, high schools and/or colleges. Invitationals occur most commonly in January or February, although there have been some as early as October or as late as April. Teams can participate in invitationals from multiple states depending on availability. At some invitationals, only a few events are held. However, many invitational tournaments mimic regional and state competitions in their competitive intensity. For example, MIT hosts an invitational tournament each year with around 70 teams from over a dozen states, including 20 or more past national qualifiers. Other tournaments, especially in the midwest, are well known for their quality and competitiveness. In this way, teams can gain extra practice before competing in regional, state, or national tournaments. In 2014, Yale University became one of the first institutions of higher education to host a tournament run by Science Olympiad alumni, with several more following over the next few years.\n\nThe National Science Olympiad competition is held in late May at a different university every year. Teams compete at the state competition with the top two schools in Division B and Division C each earning a spot at the national competition. Some states are given a second slot, based on the membership within the division. The total number of invited teams in each division is equal to 60 and the national tournament hosts 120 teams. In 2012, at the University of Central Florida, a team from Japan was invited as a Global Ambassador Team. Although they competed in several events, their scores were not tallied against the state teams.\n\nThe competition officially begins with opening ceremonies on Friday night that usually include a notable speaker, such as a Nobel Laureate. A traditional Swap Meet follows the opening ceremonies which is an opportunity for teams to meet and greet. They bring state memorabilia to trade with other teams. The most popular items include hats, license plates, T-shirts, and key chains.\n\nSaturday includes several time blocks. Each block includes a 60-minute section for each study event, plus a 15-minute break time for competitors to get from one event to another.\n\nThat night, a formal Awards Ceremony is held. It opens with a short speech followed by awarding medals for the top six teams in each event. Points for all the events are added together to determine an overall national team winner. The trial events are not included in this tally. The top ten teams in each division are recognized with trophies and plaques.\n\nIn some national tournaments, scholarships are awarded to the top teams in each event. For the 2005 and 2010 competitions, held at the University of Illinois at Urbana-Champaign, first-place event winners received full four-year tuition waivers to the university. At the 2006 National Tournament, host Indiana University awarded $7,000 annual scholarships to those who finished first place in Division C and who attend the university in their freshman year. The George Washington University offered Division C gold medalists at its 2008 National Tournament a $20,000 stipend for those who were accepted and attended GWU. In 2012, the University of Central Florida offered $30,000 scholarships to the university for first place medalists in Division C. Additional awards may also provided by sponsors and industry leaders for specific events. For example, the Center for Disease Control and Prevention provided first place medalists in Disease Detectives (Divisions B & C) with a trip for the two competitors and their coach to tour the CDC facility in Atlanta, Georgia.\n\nThis is a list of past national champions and locations.\nList of National Championships by SchoolDivision B\nDivision C\nList of States by Number of National Tournaments Hosted\nSix universities have hosted the National Tournament twice: Michigan State University, Ohio State University, University of Illinois-Urbana-Champaign, Indiana University-Bloomington, University of Central Florida, and Wright State University.\n\nDivision A generally covers elementary school students (through 6th grade). Schools which wish to start a Science Olympiad program at their school can take advantage of the resources offered on the National Science Olympiad website. There is no National membership fee required to participate in Elementary Division activities. An appropriate program will depend upon the objectives and resources of the local school or community. Programs can range from a Fun Night to a large competitive tournament. Some Elementary programs have existed as long as the National program, and have developed additional resources that schools may find helpful.\n\nMacomb Science Olympiad / Southeast Michigan Region 7 Division B&C, Macomb and St. Clair counties Division A, Website http://MacombSO.org\n"}
{"id": "11026307", "url": "https://en.wikipedia.org/wiki?curid=11026307", "title": "Simulation theory of empathy", "text": "Simulation theory of empathy\n\nSimulation theory of empathy is a theory that holds that humans anticipate and make sense of the behavior of others by activating mental processes that, if carried into action, would produce similar behavior. This includes intentional behavior as well as the expression of emotions. The theory states that children use their own emotions to predict what others will do. Therefore, we project our own mental states onto others.\nSimulation theory is not primarily a theory \"of\" empathy, but rather a theory of how people understand others—that they do so \"by way of\" a kind of empathetic response. This theory uses more biological evidence than other theories of mind, such as the theory-theory.\n\nSimulation theory is based in philosophy of mind, a branch of philosophy that studies the nature of the mind and its relationship to the brain, especially the work of Alvin Goldman and Robert Gordon. The discovery of mirror neurons in macaque monkeys has provided a physiological mechanism for the common coding between perception and action (see Wolfgang Prinz) and the hypothesis of a similar mirror neuron system in the human brain.\nSince the discovery of the mirror neuron system, many studies have been carried out to examine the role of this system in action understanding, emotion and other social functions.\n\nMirror neurons are activated both when actions are executed and the actions are observed. This unique function of mirror neurons may explain how people recognize and understand the states of others; mirroring observed action in the brain as if they conducted the observed action.\n\nTwo sets of evidence suggest that mirror neurons in the monkey have a role in action understanding. First, the activation of mirror neurons requires biological effectors such as hand or mouth. Mirror neurons do not respond to the action with tools like pliers. Mirror neurons respond to neither the sight of an object alone nor an action without an object (intransitive action). Umilta and colleagues demonstrated that a subset of mirror neurons fired when final critical part of the action was not visible to the observer. The experimenter showed his hand moving toward a cube and grasping it, and later showed the same action without showing later part grasping the cube (placing the cube behind the occluder). Mirror neurons fired on both visible and invisible conditions. On the other hand, mirror neurons did not discharge when the observer knew that there was not a cube behind the occluder.\n\nSecond, responses of mirror neurons to same actions are different depending on context of the action. A single cell recording experiment with monkeys demonstrated the different level of activation of mouth mirror neurons when monkey observed mouth movement depending on context (ingestive actions such as sucking juice vs. communicative actions such as lip-smacking or tongue protrusions). An fMRI study also showed that mirror neurons respond to the action of grasping a cup differently depending on context (to drink a cup of coffee vs. to clean a table on which a cup was placed).\n\nOne criticism of mirror neurons is that since they show that same muscle groups is used for someone watching an action as someone completing an action, they only predict actions, not beliefs or desires. While someone acts a certain way, they may not believe that what they are doing is the right thing.\n\nShared neural representation for a motor behavior and its observation has been extended into the domains of feelings and emotions. Not only movements but also facial expressions activate the same brain regions that are activated by direct experiences. In an fMRI study, same brain regions on action representation found to be activated when people both imitated and observed emotional facial expressions such as happy, sad, angry, surprise, disgust, and afraid.\n\nObserving video clips that displayed facial expression of feeling disgust activated the neural networks typical of direct experience of disgust. Similar results have been found in the case of touch. Watching movies that someone touched legs or faces activated the somatosensory cortex for direct feeling of the touch. A similar mirror system exists in perceiving pain. When people see other people feel pain, people feel pain not only affectively, but also sensorially.\n\nThese results suggest that understanding other's feelings and emotions is driven not by cognitive deduction of what the stimuli means but by automatic activation of somatosensory neurons. A recent study on pupil size directly demonstrated emotion perception was automatic process modulated by mirror systems. When people saw sad faces, pupil sizes influenced viewers in perceiving and judging emotional states without explicit awareness of differences of pupil size. When pupil size was 180% of original size, people perceived a sad face as less negative and less intense than when pupil was smaller than or equal to original pupil size. This mechanism was correlated with brain regions that implicated in emotion process, the amygdala. Furthermore, viewers mimic the size of their own pupils to those of sad faces they watched. Considering that pupil size is beyond voluntary control, the change of pupil size upon emotion judgment is a good indication that understanding emotions is automatic process. However, the study could not find other emotional faces such as happiness and anger influence pupil size as sadness did.\n\nUnderstanding other's actions and emotions is believed to facilitate efficient human communication. Based on findings from neuroimaging studies, de Vignemont and Singer proposed empathy as a crucial factor in human communication arguing its epistemological role; \"Empathy might enable us to make faster and more accurate predictions of other people's needs and actions and discover salient aspects of our environment.\" Mental mirroring of actions and emotions may enable humans to understand other's actions and their related environment quickly, and thus help humans communicate efficiently.\n\nIn an fMRI study, a mirror system has been proposed as common neural substrates to mediate the experiences of basic emotions. Participants watched video clips of happy, sad, angry and disgust facial expressions, and measured their empathy quotient (EQ). Specific brain regions relevant to the four emotions were found to be correlated with the EQ while the mirror system (i.e., the left dorsal inferior frontal gyrus/premotor cortex) was correlated to the EQ across all emotions. The authors interpreted this result as an evidence that action perception mediates face perception to emotion perception.\n\nA paper published in Science (Singer et al., 2005) challenges the idea that pain sensations and mirror neurons play a role in empathy for pain. Specifically, the authors found that activity in the anterior insula and the anterior cingulate cortex was present both when one's self and another person were presented with a painful stimulus, two regions known to be responsible for the affective experience of pain, but the rest of the pain matrix, responsible for sensation, was not active. Furthermore, participants merely saw the hand of another person with the electrode on it, making it unlikely that 'mirroring' could have caused the empathic response. However, a number of other studies, using magnetoencephalography and functional MRI have since demonstrated that empathy for pain does involve the somatosensory cortex, which supports the simulation theory.\n\nSupport for anterior insula and anterior cingulate cortex being the neural substrates of empathy include Wicker et al., 2003 who report that their \"core finding is that the anterior insula is activated both during observation of disgusted facial expressions and during the emotion of disgust evoked by unpleasant odorants\" (p. 655).\n\nFurthermore, one study demonstrated that \"for actions, emotions, and sensations both animate and inanimate touch activates our inner representation of touch.\" They note, however that \"it is important at this point to clarify the fact that we do not believe that the activation we observe evolved in order to empathize with other objects or human beings\" (p. 343).\n\nThis model states that empathy activates only one interpersonal motivation: altruism. Theoretically, this model makes sense, because empathy is an other-focused emotion. There is an impressive history of research suggesting that empathy, when activated, causes people to act in ways to benefit the other, such as receiving electric shocks for the other. These findings have often been interpreted in terms of empathy causing increased altruistic motivation, which in turn causes helping behavior.\n"}
{"id": "150135", "url": "https://en.wikipedia.org/wiki?curid=150135", "title": "Spontaneous generation", "text": "Spontaneous generation\n\nSpontaneous generation refers to an obsolete body of thought on the ordinary formation of living organisms without descent from similar organisms. The theory of spontaneous generation held that living creatures could arise from nonliving matter and that such processes were commonplace and regular. For instance, it was hypothesized that certain forms such as fleas could arise from inanimate matter such as dust, or that maggots could arise from dead flesh. A variant idea was that of equivocal generation, in which species such as tapeworms arose from unrelated living organisms, now understood to be their hosts. The idea of univocal generation, by contrast, refers to effectively exclusive reproduction from genetically related parent(s), generally of the same species.\n\nThe doctrine of spontaneous generation was coherently synthesized by Aristotle, who compiled and expanded the work of earlier natural philosophers and the various ancient explanations for the appearance of organisms, and was taken as scientific fact for two millennia. Though challenged in the 17th and 18th centuries by the experiments of Francesco Redi and Lazzaro Spallanzani, spontaneous generation was not disproved until the work of Louis Pasteur and John Tyndall in the mid-19th century.\n\nRejection of spontaneous generation is no longer controversial among biologists. By the middle of the 19th century, experiments of Louis Pasteur and others refuted the traditional theory of spontaneous generation and supported biogenesis.\n\nSpontaneous generation refers both to the supposed processes by which different types of life might repeatedly emerge from specific sources other than seeds, eggs, or parents, and also to theoretical principles presented in support of any such phenomena. Crucial to this doctrine are the ideas that life comes from non-life and that no causal agent, such as a parent, is needed. The hypothetical processes by which life routinely emerges from nonliving matter on a time scale of minutes, weeks, or years (e.g. in the supposed seasonal generation of mice and other animals from the mud of the Nile) are sometimes referred to as \"abiogenesis\". Such ideas have no operative principles in common with the modern hypothesis of abiogenesis, which asserts that life emerged in the early ages of the planet, over a time span of at least millions of years, and subsequently diversified, and that there is no evidence of any subsequent repetition of the event.\n\nThe term \"equivocal generation\", sometimes known as \"heterogenesis\" or \"xenogenesis\", describes the supposed process by which one form of life arises from a different, unrelated form, such as tapeworms from the bodies of their hosts.\n\nIn the years following Louis Pasteur's 1859 experiment, the term \"spontaneous generation\" fell increasingly out of favor. Experimentalists used a variety of terms for the study of the origin of life from nonliving materials. \"Heterogenesis\" was applied to the generation of living things from once-living organic matter (such as boiled broths), and Henry Charlton Bastian proposed the term \"archebiosis\" for life originating from inorganic materials. Disliking the randomness and unpredictability implied by the term \"'spontaneous' generation,\" in 1870 Bastian coined the term \"biogenesis\" to refer to the formation of life from nonliving matter. Soon thereafter, however, English biologist Thomas Henry Huxley proposed the term \"abiogenesis\" to refer to this same process and adopted \"biogenesis\" for the process by which life arises from existing life; it is this latter set of definitions that became dominant.\n\nActive in the 6th and 5th centuries BCE, early Greek philosophers, called \"physiologoi\" in antiquity (Greek: φυσιολόγοι; in English, physical or natural philosophers), attempted to give natural explanations of phenomena that had previously been ascribed to the agency of the gods. The \"physiologoi\" sought the material principle or \"arche\" (Greek: ἀρχή) of things, emphasizing the rational unity of the external world and rejecting theological or mythological explanations. \n\nAnaximander, who believed that all things arose from the elemental nature of the universe, the \"apeiron\" (ἄπειρον) or the \"unbounded\" or \"infinite,\" was likely the first western thinker to propose that life developed spontaneously from nonliving matter. The primal chaos of the \"apeiron,\" eternally in motion, served as a substratum in which elemental opposites (e.g., wet and dry, hot and cold) generated and shaped the many and varied things in the world. According to Hippolytus of Rome in the third century CE, Anaximander claimed that fish or fish-like creatures were first formed in the \"wet\" when acted on by the heat of the sun and that these aquatic creatures gave rise to human beings. Censorinus, writing in the 3rd century, reports:\n\nAnaximenes, a pupil of Anaximander, thought that air was the element that imparted life and endowed creatures with motion and thought. He proposed that plants and animals, including human beings, arose from a primordial terrestrial slime, a mixture of earth and water, combined with the sun's heat. Anaxagoras, too, believed that life emerged from a terrestrial slime. However, he held that the seeds of plants existed in the air from the beginning, and those of animals in the aether. Xenophanes traced the origin of man back to the transitional period between the fluid stage of the earth and the formation of land, under the influence of the sun.\n\nIn what has occasionally been seen as a prefiguration of a concept of natural selection, Empedocles accepted the spontaneous generation of life but held that different forms, made up of differing combinations of parts, spontaneously arose as though by trial and error: successful combinations formed the species we now see, whereas unsuccessful forms failed to reproduce.\n\nIn his biological works, the natural philosopher Aristotle theorized extensively the reproduction of various animals, whether by sexual, parthenogenetic, or spontaneous generation. In accordance with his fundamental theory of hylomorphism, which held that every physical entity was a compound of matter and form, Aristotle's basic theory of sexual reproduction contended that the male's seed imposed form, the set of characteristics passed down to offspring on the \"matter\" (menstrual blood) supplied by the female. Thus female matter is the material cause of generation—it supplies the matter that will constitute the offspring—while the male semen is the efficient cause, the factor that instigates and delineates the thing's existence. Yet, as proposed in the \"History of Animals\", many creatures form not through sexual processes but by spontaneous generation:\n\nAccording to this theory, living things may come forth from nonliving things in a manner roughly analogous to the \"enformation of the female matter by the agency of the male seed\" seen in sexual reproduction. Nonliving materials, like the seminal fluid present in sexual generation, contain \"pneuma\" (πνεῦμα, \"breath\"), or \"vital heat\". According to Aristotle, pneuma had more \"heat\" than regular air did, and this heat endowed the substance with certain vital properties:\nAristotle drew an analogy between the \"foamy matter\" (τὸ ἀφρῶδες) found in nature and the \"seed\" of an animal, which he viewed as being a kind of foam itself (composed, as it was, from a mixture of water and pneuma). For Aristotle, the generative materials of male and female animals (semen and menstrual blood) were essentially refinements, made by male and female bodies according to their respective proportions of heat, of ingested food, which was, in turn, a byproduct of the elements earth and water. Thus any creature, whether generated sexually from parents or spontaneously through the interaction of vital heat and elemental matter, was dependent on the proportions of pneuma and the various elements which Aristotle believed comprised all things. While Aristotle recognized that many living things emerged from putrefying matter, he pointed out that the putrefaction was not the source of life, but the byproduct of the action of the \"sweet\" element of water.\n\nWith varying degrees of observational confidence, Aristotle theorized the spontaneous generation of a range of creatures from different sorts of inanimate matter. The testaceans (a genus which for Aristotle included bivalves and snails), for instance, were characterized by spontaneous generation from mud, but differed based upon the precise material they grew in—for example, clams and scallops in sand, oysters in slime, and the barnacle and the limpet in the hollows of rocks.\n\nVitruvius, a Roman architect and writer of the 1st century BCE, advised that libraries be placed facing eastwards to benefit from morning light, but not towards the south or the west as those winds generate bookworms.\n\nAristotle claimed that eels were lacking in sex and lacking milt, spawn and the passages for either. Rather, he asserted eels emerged from earthworms. Later authors dissented. Pliny the Elder did not argue against the anatomic limits of eels, but stated that eels reproduce by budding, scraping themselves against rocks, liberating particles that become eels. Athenaeus described eels as entwining and discharging a fluid which would settle on mud and generate life. On the other hand, Athenaeus also dissented towards spontaneous generation, claiming that a variety of anchovy did not generate from roe, as Aristotle stated, but rather, from sea foam.\n\nAs the dominant view of philosophers and thinkers continued to be in favour of spontaneous generation, some Christian theologians accepted the view. Augustine of Hippo discussed spontaneous generation in \"The City of God\" and \"The Literal Meaning of Genesis\", citing Biblical passages such as \"Let the waters bring forth abundantly the moving creature that hath life\" () as decrees that would enable ongoing creation.\n\nFrom the fall of the Roman Empire in 5th century to the East-West Schism in 1054, the influence of Greek science declined, although spontaneous generation generally went unchallenged. New descriptions were made. Of the numerous beliefs, some had doctrinal implications outside of the Book of Genesis. For example, the idea that a variety of bird known as the \"barnacle goose\" emerged from a crustacean known as the \"goose barnacle\", had implications on the practice of fasting during Lent. In 1188, Gerald of Wales, after having traveled in Ireland, argued that the \"unnatural\" generation of barnacle geese was evidence for the virgin birth. Where the practice of fasting during Lent allowed fish, but prohibited fowl, the idea that the goose was in fact a fish suggested that its consumption be permitted during Lent. The practice was eventually prohibited by decree of Pope Innocent III in 1215.\n\nAristotle, in Arabic translation, was reintroduced to Western Europe. During the 13th century, Aristotle reached his greatest acceptance. With the availability of Latin translations Saint Albertus Magnus and his student, Saint Thomas Aquinas, raised Aristotelianism to its greatest prominence. Albert wrote a paraphrase of Aristotle, \"De causis et processu universitatis\", in which he removed some and incorporated other commentaries by Arabic scholars. The influential writings of Aquinas, on both the physical and metaphysical, are predominantly Aristotelian, but show numerous other influences.\n\nSpontaneous generation is discussed as a fact in literature well into the Renaissance. Where, in passing, Shakespeare discusses snakes and crocodiles forming from the mud of the Nile (), Izaak Walton again raises the question of the origin of eels \"as rats and mice, and many other living creatures, are bred in Egypt, by the sun's heat when it shines upon the overflowing of the river...\". While the ancient question of the origin of eels remained unanswered and the additional idea that eels reproduced from corruption of age was mentioned, the spontaneous generation of rats and mice engendered no debate.\n\nThe Dutch biologist and microscopist Jan Swammerdam (1637 - 1680) rejected the concept that one animal could arise from another or from putrification by chance because it was impious and like others found the concept of spontaneous generation irreligious, and he associated it with atheism and Godless opinion.\n\nJan Baptist van Helmont (1580–1644) used experimental techniques, such as growing a willow for five years and showing it increased mass while the soil showed a trivial decrease in comparison. As the process of photosynthesis was not understood, he attributed the increase of mass to the absorption of water. His notes also describe a recipe for mice (a piece of soiled cloth plus wheat for 21 days) and scorpions (basil, placed between two bricks and left in sunlight). His notes suggest he may even have done these things.\n\nWhere Aristotle held that the embryo was formed by a coagulation in the uterus, William Harvey (1578 – 1657) by way of dissection of deer, showed that there was no visible embryo during the first month. Although his work predated the microscope, this led him to suggest that life came from invisible eggs. In the frontispiece of his book \"Exercitationes de Generatione Animalium\" (\"Essays on the Generation of Animals\"), he made an expression of biogenesis: \"omnia ex ovo\" (everything from eggs).\n\nThe ancient beliefs were subjected to testing. In 1668, Francesco Redi challenged the idea that maggots arose spontaneously from rotting meat. In the first major experiment to challenge spontaneous generation, he placed meat in a variety of sealed, open, and partially covered containers. Realizing that the sealed containers were deprived of air, he used \"fine Naples veil\", and observed no worm on the meat, but they appeared on the cloth. Redi used his experiments to support the preexistence theory put forth by the Church at that time, which maintained that living things originated from parents. In scientific circles Redi's work very soon had great influence, as evidenced in a letter from John Ray in 1671 to members of the Royal Society of London:\n\nPier Antonio Micheli, around 1729, observed that when fungal spores were placed on slices of melon the same type of fungi were produced that the spores came from, and from this observation he noted that fungi did not arise from spontaneous generation.\n\nIn 1745, John Needham performed a series of experiments on boiled broths. Believing that boiling would kill all living things, he showed that when sealed right after boiling, the broths would cloud, allowing the belief in spontaneous generation to persist. His studies were rigorously scrutinized by his peers and many of them agreed.\n\nLazzaro Spallanzani modified the Needham experiment in 1768, attempting to exclude the possibility of introducing a contaminating factor between boiling and sealing. His technique involved boiling the broth in a sealed container with the air partially evacuated to prevent explosions. Although he did not see growth, the exclusion of air left the question of whether air was an essential factor in spontaneous generation. However, by that time there was already widespread scepticism among major scientists, to the principle of spontaneous generation. Observation was increasingly demonstrating that whenever there was sufficiently careful investigation of mechanisms of biological reproduction, it was plain that processes involved basing of new structures on existing complex structures, rather from chaotic muds or dead materials. Joseph Priestley, after he had fled to America and not long before his death, wrote a letter that was read to the American Philosophical Society in 1803. It said in part:\n\nIn 1837, Charles Cagniard de la Tour, a physicist, and Theodor Schwann, one of the founders of cell theory, published their independent discovery of yeast in alcoholic fermentation. They used the microscope to examine foam left over from the process of brewing beer. Where Leeuwenhoek described \"small spheroid globules\", they observed yeast cells undergo cell division. Fermentation would not occur when sterile air or pure oxygen was introduced if yeast were not present. This suggested that airborne microorganisms, not spontaneous generation, was responsible.\n\nHowever, although the idea of spontaneous generation had been in decline for nearly a century, its supporters did not abandon it all at once. As James Rennie wrote:\n\nLouis Pasteur's 1859 experiment is widely seen as having settled the question of spontaneous generation. He boiled a meat broth in a flask that had a long neck that curved downward, like that of a goose or swan. The idea was that the bend in the neck prevented falling particles from reaching the broth, while still allowing the free flow of air. The flask remained free of growth for an extended period. When the flask was turned so that particles could fall down the bends, the broth quickly became clouded. However, minority objections were persistent and not always unreasonable, given that the experimental difficulties were far more challenging than the popular accounts suggest. The investigations of John Tyndall, a correspondent of Pasteur and a great admirer of Pasteur's work, were decisive in disproving spontaneous generation and dealing with lingering issues. Still, even Tyndall encountered difficulties in dealing with the effects of microbial spores, which were not well understood in his day. Like Pasteur, he boiled his cultures to sterilize them, and some types of bacterial spores can survive boiling. The autoclave, which eventually came into universal application in medical practice and microbiology to sterilise equipment, was not an instrument that had come into use at the time of Tyndall's experiments, let alone those of Pasteur.\n\nIn 1862, the French Academy of Sciences paid a special attention to the issue and established a prize \"to him who by well-conducted experiments throws new light on the question of the so-called spontaneous generation\" and appointed a commission to judge the winner.\n\n"}
{"id": "4790944", "url": "https://en.wikipedia.org/wiki?curid=4790944", "title": "Student orientation", "text": "Student orientation\n\nStudent orientation or new student orientation (often encapsulated into an Orientation week, Frosh Week, Welcome Week (or Freshers' Week) is a period before the start of an academic year at a university or tertiary institutions. A variety of events are held to orient and welcome new students during this period. The name of the period varies by country.\n\nAlthough usually described as a \"week\", the length of this period varies widely from university to university and country to country, ranging from about three days to a month or even more (e.g. four or five weeks, depending on program, at Chalmers). The length of the week is often affected by each university's tradition as well as financial and physical constraints. During this period, students participate in a wide range of social activities.\n\nThe week before the term starts is known as: \"Frosh\" (or \"frosh week\") in some colleges and universities in Canada. In the US, most call it by the acronym SOAR for Student Orientation And Registration; \"Freshers' week\" in the majority of the United Kingdom and Ireland and \"Orientation week\" or \"O-week\" in countries such as Australia, South Africa and New Zealand, and also in many Canadian universities. In Sweden, it is known as \"nollning\" (from \"nolla\", \"zero\", in this case meaning the students have not earned any credit points yet) or \"inspark\" (being \"kicked in\" to university life). Orientation week is the coming phrase in the United States. Some schools use the acronym WOW for Week of Welcome.\n\nIn Canada, first-year students are called \"Frosh\" or \"first-years\". The terms \"freshies\" and \"freshers\" are also emerging. In the United States, first-year university students are typically referred to as freshmen. In Australia and New Zealand, first-year students are known simply as \"first-years\", although in some the colleges of the University of Melbourne and the University of Sydney they are also called \"Freshers\". In the U.K. and Ireland first-year students are known as \"freshers\" or \"first-years\". \"Freshies\" is also an emerging term in New Zealand. In Sweden, the student is a \"nolla\" (a \"zero\") during the orientation period and usually upgraded to the status of an \"etta\" (student who is in her/his first college term) at a ceremony involving a fancy three-course dinner and a lots of singing.\n\nIn Australia, some universities require students to arrive at university a week before classes start in order to gain course approval. This also allows students a chance to orient themselves to student life without the pressure of lectures—hence the term \"Orientation week\" is used to describe this week of induction into university life.\n\nIn Australian universities, such as the University of Melbourne, University of New South Wales and University of Sydney, the last or second last night is usually celebrated with a large-scale event such as a famous band playing at an entertainment venue on campus. This is generally followed by continued partying and drinking, especially among students living in residential colleges such as Janet Clarke Hall and Ormond College.\n\nThe Adelaide University O-Week runs from Monday to Thursday in the week before lectures begin. During O-Week sporting clubs and societies set up a variety of tented areas where clubs display their activities. The Adelaide University Union coordinates a variety of events centering around beer, bands and barbecues on the lawns near the Union complex. A major event for the week is the O-Ball (live entertainment and licensed areas) which takes place in the Cloisters (Union House). The O-Ball attracts many thousands of revellers, not all of whom are Adelaide University students. In recent times Sports and Clubs have sought to distance themselves from the student union and student association controlled activities and have set themselves up on the Maths lawns.\n\nThe Australian National University has a full week (Sunday to Sunday) of events, parties and social activities open to all students of the university, organised by the Australian National University Students Association. The residential colleges often have their own \"O-week\" activities catered primarily for residents as well as the annual \"Burgmann Toga Party\" held at Burgmann College open to students from all residential colleges. \"Burgmann Toga\" is the largest party held at a university residence in the Southern Hemisphere.\n\nIn Canada, the nature and length of orientation week varies considerably between Universities. For instance, Ottawa, has two universities within its urban centre; the University of Ottawa and Carleton University, both with orientations spanning approximately 7 days. At The University of Ottawa, Frosh Week is Called 101 week. At Carleton University there are multiple orientations, SPROSH (Sprott Frosh), ENG Frosh, Radical Frosh, and the largest, CUSA/RRRA/SEO Frosh.\nIn the province of Quebec, because of the CEGEP system, \"froshies\" are of legal drinking age and Frosh activities may include the option to drink alcohol. Moreover, the proximity of the two Ottawa universities also allows them to take advantage of the drinking age in neighbouring Gatineau, Quebec. The University of British Columbia cancels the first day of class for all students, and hosts an orientation day for new students, called Imagine Day. As of 2007, the Faculty of Science also holds an annual, day-long Science Frosh event for approximately 300 first-year students, while the commerce faculty holds a 3-day-long frosh weekend before classes begin. The University of Toronto has a number of different \"Frosh Weeks\" organized concurrently by different student groups within the university; including college societies, professional faculties (perhaps the best known being organized by Engineering Society, Skule (engineering society), in which 'F!ROSH' and 'F!ROSH Leedurs' dye their bodies purple) and the University of Toronto Students' Union. Similarly, Ryerson University also has a number of \"Frosh Weeks\" organized by different student groups, although it also has a central frosh team known as the 'Ryerson Orientation Crew'. At the Friday of frosh week, the Ryerson Students' Union holds a concert that is free for all Ryerson students; the headliners for the 2015 concert included Drake and Future. McMaster University also organizes many events during what they term \"Welcome Week\". The week strongly encourages solidarity, first with members of one's own residence or for off-campus students, and later the members of a student's faculty. University of Guelph holds many orientation activities for its incoming students. The main event is the pep rally in which students from each residence perform a dance on the football field. The Guelph Engineering Society also hosts a series of special events for Engineering Frosh including frosh olympics, beach day, and a scavenger hunt. Western University hosts the largest orientation program in Canada, involving 1200 student volunteers and an entire week of activities. St Thomas University, in Fredericton, New Brunswick, hosts a week-long event including activities for each residence and activities for new students. As a rule, Frosh week at Queen's University is so secretive and confidential that no one knows what happens during the week long adventure except Queen's Students and Alumni. Wilfrid Laurier University has by definition a Lit orientation week.\n\nIn Finnish universities, the student organizations for each department independently organize orientation activities for the new students in their respective departments. New students are often assigned in groups to an upperclassman tutor and participate in many activities with their tutoring group. New students may be referred to as \"piltti\" (child), \"fuksi\" (freshman), \"fetus\" or other names according to their major subject. Activities for new students may include \"orienteering\", pub crawls, sporting events, swimming in fountains or other forms of \"baptism\", sitsit parties and saunas, often done wearing homemade fancy-dress costumes. It is also considered important for the new students to participate in the regular activities of the student department organizations.\n\nIn past years a typical orientation may consist of verbal harassment as well as initiation leading to humiliation. An orientation of freshers in Indonesia is usually called OSPEK for some universities and MOS in middle and high school. Orientations in Indonesia has event organizers that consists of seniors and the presidium of universities. The most basic form of orientation in Indonesia consist of an educational board run and introduction of campus cultural behavior. What makes orientation in Indonesia (for some universities and schools) distinctive to other countries would arguably be the freshmens' requirement to wear unusual accessories or hairstyles (i.e. Freshmens were asked to wear hats made of bird's nest, necktie made of folded paper, military hairstyle for male students or intricate braids for females, and the usage of a sack instead of a rucksack). Harsh physical punishments were not uncommon during the Suharto era, and mass media continues to report inhumane activities during those orientation that led to a few cases of death.\n\nNowadays, however, orientation is more tolerable as physical abuse is now forbidden by the law, however it is still criticized by many psychologists and people as 'too much' because of excessive verbal harassment like dissing and insulting the juniors, and the usage of unusual and humiliating attributes typically found in orientations on Junior High and High Schools. As well, it is also criticized by many parents for being economically inconvenient. The reason cited by psychologists is that orientation is often used as a tool of revenge done by the board of organizers for what the seniors did to them during their freshman year. And because of this there are so many people who believes that \"MOS\" or \"OSPEK\" is a useless traditions that needs to be erased. The 'cruelty' of MOS and OSPEK varies between universities and schools in Indonesia, although in (most) major universities and institutes that kind of humiliation and harassment doesn't exist anymore, or greatly limited to pending applicants or pledges for certain campus organizations.\n\nAs in Australia, in New Zealand students have a week to orient themselves to university life before the start of formal classes. This \"orientation week\" is a time for many social events, and is often a reason for alcohol fests. Flat warmings are often held within the time limit to couple the alcohol oriented event with the general party week.\n\nIn New Zealand's main university towns such as Dunedin and Palmerston North (where students make up around one fifth of the population) orientation week leads a wide range of events. Many top overseas and local bands tour the country at this time, and the orientation tour is one of the highlights of the year's music calendar. The University of Otago in the Scottish-settled city of Dunedin traditionally holds a parody of the Highland Games called the \"Lowland Games\", including such esoteric events as porridge wrestling.\n\nStudent pranks were once common during orientation week, but have fallen out of favour in recent years. Until recent years, many halls of residence also inducted new residents with \"Initiation\" (a form of hazing, though considerably milder than the rituals found among American college fraternities).\n\nAlthough officially designated as a week, in several New Zealand universities and polytechnics orientation week stretches to over ten days.\n\nMost Swedish universities have some kind of \"nollning\" (\"zeroing\") or \"inspark\" (\"kicking-in\"). This is most extensive at the technical faculties and at the student nation communities of Uppsala and Lund. Since student union membership was mandatory in Sweden (until July 2010), the nollning is usually centrally organized from the student union with support from the universities.\n\nAt the old universities, these traditions have often turned civilized after a dark history of hazing. Today, many student unions have strict rules against inappropriate drunkenness, sexual harassment and other problematic behaviour.\n\nAt the technical faculties, the people who organize the nollning play roles in a theatrical manner and often wear sunglasses and some form of weird clothes. Most senior students who are mentors during the nollning wear their student boilersuits or the b-frack (a worn tailcoat). This kind of organized nollning developed at KTH and Chalmers and spread to the rest of the country.\n\nIn Thailand, the activity is commonly called \"rapnong\" (รับน้อง), translated as \"welcoming of freshmen\". It takes place in the first week or month of the academic year at universities and some high schools. The purpose is to adapt new students to university culture. Activities include games, entertainment and recreation. These let the newcomers get to know other members of the university and reduce tension in the changing environment. It sometimes includes alcohol. The main object is to let juniors carry on the universities' tradition and identity and to bind together the new generation into one. Long-term activity often includes seniors taking freshman or older years to meals and meetings, usually the most senior pays for it all. Hazing is a concern in this activity, as many students have been humiliated, abused, and dehumanized by their upperclassmen.\n\nFor over 50 years, SOTUS – a Hazing based system used for college initiation in Thailand – has been involved in Thai universities. It stands for Seniority, Order, Tradition, Unity, and Spirit. It is the system for freshmen to bring harmony to their friends and to show their pride through their institute. By seniors, freshmen have to do activities such as singing university songs. Moreover, freshmen are required to do a lot of things; for example, wear a nametag, and show respect to seniors. These requirements lead seniors to try to make their juniors do what they desire and punish them if they don't do seniors' orders.\n\nPresently, there are adolescents and adults opposing those who had committed unethical or deadly actions to juniors. This group of adolescents has distributed \"Anti-SOTUS\" group and it becomes one of the main issues in Thailand recently. They consider the SOTUS system to be \"old-fashioned and source of brutality\". Since it was established, this has become the group of people who share their opinions about SOTUS system based on how they have encountered it.\n\nOn the other hand, some seniors that support this system resisting the anti SOTUS attitude for many years. They tend to say that SOTUS makes them get along together and feel proud of themselves by becoming part of their institute. Some seniors, however, coerce their freshmen to attend every activity held by them as parts of preparing them to be able to live happily in university. These become worse when some freshmen suffer from what their senior have done to them.\n\nIn Thai society, news related to this system has been reported almost every year. For example, recent news about a male freshman who died in this tradition. This news has resulted in people thinking that rapnong should end or, at least, be controlled.\n\nAs well as providing a chance to learn about the university, \"Freshers' week\" allows students to become familiar with the representatives of their Student Union and to get to know the city or town which is home to the university, often through some form of pub crawl (the legal drinking age is 18 in the UK and in Ireland).\n\nLive music is also common, as are a number of organized social gatherings especially designed to allow freshers to make new friends and to get to know their course colleagues. Because of the intensity of activities, there are often many new friendships made, especially in group accommodation, some not lasting past Freshers' Week and others lasting for the whole University career and longer.\n\nTypically a \"Freshers' Fair\" for student clubs and societies is included as part of the activities to introduce new students to facilities on offer, typically outside their course of study, such as societies, clubs and sports. The various societies and clubs available within the University have stalls and aim to entice freshers to join. Most campuses take the opportunity to promote safe sex to their students and sometimes offer leaflets on the subject and free condoms, as well as promoting the Drinksafe campaign. The aim is to lower the rate of sexually transmitted disease and to reduce the level of intoxication commonly witnessed in Freshers' Week.\n\nFreshers' Flu is a predominately British term which describes the increased rates of illness during the first few weeks of university. Although called \"Freshers' Flu\", it is often not a flu at all.\n\n\"Freshmen\" is the traditional term for first-year students arriving at school in the United States, but the slang term 'frosh' is also used. Due to the perceived gender exclusiveness of the term, some institutions including the University of North Carolina have adopted \"first-year student\" as the preferred nomenclature. Lasting between a few days and a week, the orientation is these students' informal introduction and inauguration to the institution. Typically, the first-year students are led by fellow students from upper years over the course of the week through various events ranging from campus tours, games, competitions, and field trips. At smaller liberal arts colleges, the faculty may also play a central role in orientation.\n\nIn many colleges, incoming freshmen are made to perform activities such as singing of songs, engaging in group physical activities, and playing games. These activities are often done to help freshmen make friends at their new establishment, and also to bond with each other and the upperclassmen.\n\nDespite the fact that most first-year students are below the legal drinking age (currently 21 years in all states), heavy drinking and binge drinking may occur outside the orientation curriculum. Some programs require their organizers to sign waivers stating they will not be under the influence of any substances over the course of the week as they are responsible for the well-being of the students. Most programs have one final party on the final night to finish off the week of celebrating, in which the organizers join in.\n\nAlthough it has been officially banned at many schools, hazing is not uncommon during the week. This can be anywhere from the organizers treating the first-year students in a playfully discouraging manner to forcing them to endure rigorous trials.\n\nThe attitude of the events also depends on the school. Many colleges encourage parents to come to the first day to help new students move into their dormitory, fill out paper work, and get situated. Some schools view their week as an initiation or rite of passage while others view it as a time to build school spirit and pride. In towns with more than one university, there may be a school rivalry that is reflected in the events throughout the week.\n\nAt most schools, incoming freshmen arrive at the school for a couple of days during the summer and are put into orientation groups led by an upperclassman trained for the position. Their Orientation Leader will take them around campus, do activities with them, have discussions with them, help them register for the next semester's classes and make them feel comfortable about coming to school in the fall.\n\nFreshmen orientation is usually mandatory for all new students, especially international students which is one way to activate the status of their visa.\n\nAfter first-year students have completed some time at their university, they may find that they did not make the right choice, miss being close to home, or simply want to attend a different institution. When this occurs, they may transfer to another university, usually after their first year.\n\nMany universities will hold another student orientation similar to freshman orientation for these transfer students. Freshman orientation lasts a few days or a week, on the other hand, transfer student orientation will typically last between one and three days. Transfer orientation's purpose is to acquaint transfer students with their new university. This usually includes campus tours, introducing transfer students to their adviser or perhaps a few of their teachers, and filling out paperwork for proper enrollment. At some colleges, transfer orientation is mandatory for all transfer students.\n\nUnlike freshmen, transfer students are already familiar with the independence of college life. Therefore, their orientation focuses mostly on becoming familiar with the layout and policies of their new institution, providing information about essential campus resources, and getting acquainted with other transfer students so they may make friends at their new university. Transfer students may engage in games, conversations with University faculty, and discussions with current students to make acquaintances and learn more about the university.\n\nAt \"Roskilde University\" in Denmark, orientation week (in Danish \"rusvejledning\") normally lasts from 1 week and a half to two whole weeks. During the period, approximately 14 teams consisting of 10–16 tutors each takes care of an individual \"house\" in which the new students have been allocated. There's normally 1 house of Natural Sciences, 4 of Social Studies and Economics, 4 houses of Arts and Language and 2 of technology and design. Each of the first 3 houses described has an International version as well, where the courses are taught in English instead of Danish.\n\nEach tutor group spends roughly 14 days (and 3–5 days of preeducation in the spring semester) living on campus before the arrival of the new students (also called \"ruslings\"). These periods usually involve heavy amounts of drinking, partying and sexual activity among the tutors themselves. However most festive activities including alcohol only occurs until after 4 pm, due to the alcohol policies of the university. Because of this policy, most of the daily activity is spent on planning and preparing activities for the new students.\n\nWhen the students arrive all tutor groups welcomes the \"ruslings\" with the infamous \"Marbjergmark show- \"usually a display of wacky sketches such as naked people playing chess, smashing rotten eggs at bystanders or themselves or guys chasing midgets with a butcher's knife (to name a few examples).\n\nDuring the two-week period the tutor group teach and introduce the new students to life at campus. Both the social and educational aspects. As it is with the preparation period, festive activities take place after 4 PM, and educational activities are held during the day.\n\nThe two-week period ends in a four-day period in which the \"house\" will leave campus to varied destinations. During these days mostly social activities are held, including the more secret hazing rituals of the university.\n\nThe tutors uphold a strict set of rules to maintain a safe and pleasant tutorship to prevent harmful and humiliating hazing rituals. Examples are the presence of minimum two sober tutors at each party (In Danish \"Ædruvagter\"). Engaging in sexual relations with new students is also strongly discouraged. Also it is generally not seen as appropriate to force people to drink alcohol through various games and activities. Furthermore, the university dictates that each tutor must be taught basic first aid, as well as a couple of courses in conflict management and basic education psychology.\n\nAt \"DTU \"(\"Danish Faculty of Technology and Engineering\"), \"Copenhagen Business School\" and \"Copenhagen University\" similar periods are held. They however vary, and are significantly shorter than the overall orientation period spent on \"Roskilde University\".\n\n\n"}
{"id": "185901", "url": "https://en.wikipedia.org/wiki?curid=185901", "title": "Subspecies", "text": "Subspecies\n\nIn biological classification, the term subspecies refers to a unity of populations of a species living in a subdivision of the species' global range and varies from other populations of the same species by morphological characteristics.\nA subspecies cannot be recognized independently. A species is either recognized as having no subspecies at all or at least two, including any that are extinct. The term is abbreviated subsp. in botany and bacteriology, or ssp. in zoology. The plural is the same as the singular: \"subspecies\".\n\nIn zoology, under the International Code of Zoological Nomenclature, the subspecies is the only taxonomic rank below that of species that can receive a name. In botany and mycology, under the International Code of Nomenclature for algae, fungi, and plants, other infraspecific ranks, such as variety, may be named. In bacteriology and virology, under standard bacterial nomenclature and virus nomenclature, there are recommendations but not strict requirements for recognizing other important infraspecific ranks.\n\nA taxonomist decides whether to recognize a subspecies or not. A common criterion for a subspecies is its ability of interbreeding with a different subspecies of the same species and producing fertile offspring. In the wild, subspecies do not interbreed due to their geographic isolation and sexual selection. The differences between subspecies are usually less distinct than the differences between species.\n\nIn zoology, the \"International Code of Zoological Nomenclature\" (4th edition, 1999) accepts only one rank below that of species, namely the rank of subspecies. Other groupings, \"infrasubspecific entities\" do not have names regulated by the \"ICZN\". Such forms have no official \"ICZN\" status, though they may be useful in describing altitudinal or geographical clines, pet breeds, transgenic animals, etc. While the scientific name of a species is a binomen, the scientific name of a subspecies is a trinomen - a binomen followed by a subspecific name. A tiger's binomen is \"Panthera tigris\", so for a Sumatran tiger the trinomen is, for example, \"Panthera tigris sumatrae\". \"Subspecies\" is generally abbreviated as \"ssp.\" in zoology, but is not used in scientific name.\n\nIn botany, subspecies is one of many ranks below that of species, such as variety, subvariety, form, and subform. The subspecific name is preceded by \"subsp.\", as in \"Schoenoplectus californicus\" subsp. \"tatora\". A botanical name consists of at most three parts. An infraspecific name includes the species binomial, and one infraspecific epithet, such as subspecies or variety.\n\nIn bacteriology, the only rank below species that is regulated explicitly by the code of nomenclature is \"subspecies\", but infrasubspecific taxa are extremely important in bacteriology; Appendix 10 of the code lays out some recommendations that are intended to encourage uniformity in describing such taxa. Names published before 1992 in the rank of \"variety\" are taken to be names of subspecies (see \"International Code of Nomenclature of Prokaryotes\"). As in botany, \"subspecies\" is conventionally abbreviated as \"subsp.\", and is used in the scientific name: \"Bacillus subtilis\" subsp. \"spizizenii\".\n\nIn zoological nomenclature, when a species is split into subspecies, the originally described population is retained as the \"nominotypical subspecies\" or \"nominate subspecies\", which repeats the same name as the species. For example, \"Motacilla alba alba\" (often abbreviated \"M. a. alba\") is the nominotypical subspecies of the white wagtail (\"Motacilla alba\").\n\nThe subspecies name that repeats the species name is referred to in botanical nomenclature as the subspecies \"autonym\", and the subspecific taxon as the \"autonymous subspecies\".\n\nWhen zoologists disagree over whether a certain population is a subspecies or a full species, the species name may be written in parentheses. Thus \"Larus (argentatus) smithsonianus\" means the American herring gull; the notation within the parentheses means that some consider it a subspecies of a larger herring gull species and therefore call it \"Larus argentatus smithsonianus\", while others consider it a full species and therefore call it \"Larus smithsonianus\" (and the user of the notation is not taking a position).\n\nA subspecies is a taxonomic rank below species – the only recognized rank in the zoological code, and one of three main ranks below species in the botanical code. When geographically separate populations of a species exhibit recognizable phenotypic differences, biologists may identify these as separate subspecies; a subspecies is a recognized local variant of a species. Botanists and mycologists have the choice of ranks lower than subspecies, such as variety (varietas) or form (forma), to recognize smaller differences between populations.\n\nIn biological terms, rather than in relation to nomenclature, a \"polytypic\" species has two or more genetically and phenotypically divergent subspecies, races, or more generally speaking, populations that need a separate description. These are separate groups that are clearly distinct from one another and do not generally interbreed, although there may be a relatively narrow hybridization zone, but which may interbreed if given the chance to do so. These subspecies, races, or populations, can be named as subspecies by zoologists, or in more varied ways by botanists and microbiologists.\n\nA \"monotypic\" species has no distinct population or races, or rather one race comprising the whole species. A taxonomist would not name a subspecies within such a species. Monotypic species can occur in several ways:\n\n\n"}
{"id": "2949345", "url": "https://en.wikipedia.org/wiki?curid=2949345", "title": "Sutton's law", "text": "Sutton's law\n\nSutton's law states that when diagnosing, one should first consider the obvious. It suggests that one should first conduct those tests which could confirm (or rule out) the most likely diagnosis. It is taught in medical schools to suggest to medical students that they might best order tests in that sequence which is most likely to result in a quick diagnosis, hence treatment, while minimizing unnecessary costs. It is also applied in pharmacology, when choosing a drug to treat a specific disease you want the drug to reach the disease. It is applicable to any process of diagnosis, e.g. debugging computer programs. Computer-aided diagnosis provides a statistical and quantitative approach. \n\nA more thorough analysis will consider the false positive rate of the test and the possibility that a less likely diagnosis might have more serious consequences. A competing principle is the idea of performing simple tests before more complex and expensive tests, moving from bedside tests to blood results and simple imaging such as ultrasound and then more complex such as MRI then specialty imaging. The law can also be applied in prioritizing tests when resources are limited, so a test for a treatable condition should be performed before an equally probable but less treatable condition.\n\nThe law is named after the bank robber Willie Sutton, who reputedly replied to a reporter's inquiry as to why he robbed banks by saying \"because that's where the money is.\" In Sutton's 1976 book \"Where the Money Was\", Sutton denies having said this. \n\nA similar idea is contained in the physician's adage, \"When you hear hoofbeats, think horses, not zebras.\"\n\n\n"}
{"id": "43516985", "url": "https://en.wikipedia.org/wiki?curid=43516985", "title": "The Rational Optimist", "text": "The Rational Optimist\n\nThe Rational Optimist is a 2010 popular science book by Matt Ridley, author of \"\". The book primarily focuses on the benefits of the innate human tendency to trade goods and services. Ridley argues that this trait, together with the specialization linked to it, is the source of modern human civilization, and that, as people increasingly specialize in their skill sets, we will have increased trade and more prosperity.\n\nBill Gates praised the book for critiquing opposition to international aid, but criticised the book for under-representing global catastrophic risks. Ricardo Salinas Pliego praised the book as a defence of free trade and globalisation. Michael Shermer gave the book positive reviews in \"Nature\" and \"Scientific American\" before going on to present similar ideas in conference talks, and writing \"The Moral Arc\" partly in response. David Papineau praised the book for refuting \"doomsayers who insist that everything is going from bad to worse\".\n\nGeorge Monbiot criticised the book in his \"Guardian\" column.\n\n\n"}
{"id": "35937072", "url": "https://en.wikipedia.org/wiki?curid=35937072", "title": "Woese's dogma", "text": "Woese's dogma\n\nWoese's dogma is a principle of evolutionary biology first put forth by biophysicist Carl Woese in 1977. It states that the evolution of ribosomal RNA was a necessary precursor to the evolution of modern life forms. This led to the advancement of the phylogenetic tree of life consisting of three domains rather than the previously accepted two.While the existence of Eukarya and Prokarya were already accepted, Woese was responsible for the distinction between Bacteria and Archaea. Despite initial criticism and controversy surrounding his claims, Woese's three domain system, based on his work regarding the role of rRNA in the evolution of modern life, has become widely accepted.\n\nEvidence for Woese's dogma is well established through comparisons of RNA homology. Modern research allows more liberal use of RNA sequencing, allowing for a better comparative analysis between distant RNA. When analyzing multiple strains of \"E. coli\", Root-Bernstein et. al. have compared tRNA encodings found within rRNA with tRNA found in \"E. coli\" to see if the secondary structure was the same as more “modern” tRNA present in \"E. coli\". Comparisons between the tRNA encodings found in the rRNAs and mRNAs of the control sequences found that “sortings” for these sequences were extremely similar, and comparisons of translated protein structure indicated that homology was likely. Additionally, sequences homologous to all tRNAs necessary for translation were present in 16s and 23s rRNAs, and synthetases to load these tRNAs were also found, indicating that many of the functions of transcription and translation present in more modern life exist in rRNA, if vestigially.\nWhen comparing homologies of rRNA structures, it is necessary to analyze substructures. This is because models that study RNA structure on the whole do not currently exist. Generally, phylogenies of rRNA subunits are created to understand each component, and how they function and evolve. Through phylogenies created that depict rRNA structural elements that are present in all three domains of life, the oldest structural components can be determined through relative dating. These phylogenies were used in a study by Harish et. al., to show that a helical stem labeled h44 in small subunit rRNA can be described as the oldest structural component of rRNA, which holds particular significance, as this structure responsible for linking processes in the small subunit, which is responsible for decoding, with the large subunit, which is responsible for the formation of peptide bonds and the releasing of elongation factors. This essentially shows that the functional origin of the ribosome, responsible for protein synthesis, is common in all modern life throughout each of the three domains.\n\nEvidence has also been obtained in studying eukaryotic organelles, such as the chloroplast. Zablen et al.’s phylogenetic analysis conducted electrophoresis on chloroplast ribosomal RNA, specifically on the 16S rRNA of \"Euglena gracilis\"\".\" In conducting this experiment, researchers compared the electrophoretic fingerprint of this RNA to other chloroplasts and prokarya. In comparing these results, it was found that generally, these chloroplasts show a close genomic relationship, while a more distant one is seen for algae, and subsequently prokaryotic organisms. This experiment shows that the rRNA of distantly related organisms has a similar origin of that in eukaryotic organelles, supporting the idea that the evolution of rRNA was a necessary precursor of modern life.\n\nOne of the reasons that Woese’s Dogma holds significance is because of the potential that RNA was the first primordial self-replicating molecule (see: RNA World), meaning it would be key in the progression of modern life. In particular, it has been proposed that ribosomes exist as a missing link in prebiotic evolution, with rRNA being a vestige of an ancient genome. Some evidence exists for the proposal that rRNA functioned in the past to encode proteins that are key to ribosome function. One notable example is the fact that rRNA proteins are commonly known to bind with their own mRNA. In addition, some ribosomal proteins not only regulate their own expression, but the expression of other proteins as well. These are both indications of self-replication, and indicate the possibility that the mRNA that encodes ribosomal proteins evolved from rRNA.\n\nRNA existing as a primordial self replicating entity is an idea that faces criticism. The idea of rRNA in particular being sufficient on its own to explain the progression of modern life struggles due to the fact that it lacks certain key pieces of evidence. In particular RNA cannot be shown to be prebiotic, as there is no way for the nucleotides or nucleosides that compose it to be non-enzymatically replicated. Additionally, other criticisms exist, such as the fact that RNA is not stable enough to have arisen prebiotically, and that it is too complex to have arisen prebiotically. This has led to the development of other hypotheses, such as 'proteins first', which states that proteins arose prior to RNA, or coevolved with RNA. This has also led to the proposal of other primordial molecules that may have developed into RNA and DNA, such as peptide nucleic acids, which also show evidence of self replication. Despite the fact that criticisms might exist on the primordial or prebiotic nature of rRNA, these criticisms are not aimed at Woese's Dogma on the whole, as Woese's Dogma only claims that the evolution of rRNA was a necessary precursor to modern life, not that rRNA arose prebiotically.\n"}
{"id": "23848777", "url": "https://en.wikipedia.org/wiki?curid=23848777", "title": "Working hypothesis", "text": "Working hypothesis\n\nA working hypothesis is a hypothesis that is provisionally accepted as a basis for further research in the hope that a tenable theory will be produced, even if the hypothesis ultimately fails. Like all hypotheses, a working hypothesis is constructed as a statement of expectations, which can be linked to the exploratory research purpose in empirical investigation and is often used as a conceptual framework in qualitative research.\n\nUse of the phrase \"working hypothesis\" goes back at least two centuries.\n\nCharles Sanders Peirce came to hold that an explanatory hypothesis is not only justifiable as a tentative conclusion by its plausibility (by which he meant its naturalness and economy of explanation), but also justifiable as a starting point by the broader promise that the hypothesis holds for research. This idea of justifying a hypothesis as potentially fruitful (at the level of research method), not merely as plausible (at the level of logical conclusions), is essential for the idea of a working hypothesis, as later elaborated by Peirce's fellow pragmatist John Dewey.\n\nPeirce held that, as a matter of research method, an explanatory hypothesis is judged and selected for research because it offers to economize and expedite the process of inquiry, by being testable and by further factors in the economy of hypotheses: low cost, intrinsic value (instinctive naturalness and reasoned likelihood), and relations (caution, breadth, and incomplexity) among hypotheses, inquiries, etc. (as in the game of Twenty Questions). The \"Century Dictionary Supplement\" definition of \"working hypothesis\" reflects that perspective; Peirce may or may not have written it. Peirce seldom used the phrase \"working hypothesis,\" but he once commented about a related kind of a hypothesis that it was \"a hypothesis, which like the working hypothesis of a scientific inquiry, we may not believe to be altogether true, but which is useful in enabling us to conceive of what takes place.\" For Peirce the pragmatist, conceiving pragmatically of something meant conceiving of its effects in their conceivable implications as to informed practice in general including research.\n\nJohn Dewey used the concept of the working hypothesis as a pivotal feature in his theory of inquiry. Contrary to the principles of verification and falsifiability, used in formal hypothesis testing found within dominant paradigms of 'normal' science, working hypotheses were conceived by Dewey as neither true nor false but \"provisional, working means of advancing investigation,\" which lead to the discovery of other unforeseen but \"relevant\" facts. Dewey's development of the concept of the working hypothesis emerged from his contextualist epistemology in which absolute truth is unobtainable and replaced by \"warranted assertability\". Thus, Dewey noted:\nThe history of science also shows that when hypotheses have been taken to be finally \"true\" and hence unquestionable, they have obstructed inquiry and kept science committed to doctrines that later turned out to be invalid.\n\nIn Dewey's view, the working hypothesis is generated, not directly as a testable statement of, but instead in order to \"direct inquiry into channels in which new material, factual and conceptual, is disclosed, material which is more relevant, more weighted and confirmed, more fruitful, than were the initial facts and conceptions which served as the point of departure\".\n\nAbraham Kaplan later described the working hypothesis as \"provisional or loosely formatted\" theory or constructs.\n\nWorking hypotheses are constructed to facilitate inquiry; however, formal hypotheses can often be constructed based on the results of the inquiry, which in turn allows for the design of specific experiments whose data will either support or fail to support the formal hypotheses. In \"Unity of Science as a Working Hypothesis\" Oppenheim and Putnam argued that unitary science, in which laws from one branch could be equally useful by others, could only be accepted tentatively without further empirical testing. Thus they argued:\nWe therefore think the assumption that unitary science can be attained through cumulative micro-reduction recommends itself as a working hypothesis. That is, we believe that it is in accord with the standards of reasonable scientific judgment to tentatively accept this hypothesis and to work on the assumption that further progress can be made in this direction.\n\nFor Putnam, the working hypothesis, therefore, represents a practical starting point in the design of an empirical research exploration. A contrasting example of this conception of the working hypothesis is illustrated by the brain-in-a-vat thought experiment. This experiment involves confronting the global skeptic position that we, in fact, are all just brains in vats being stimulated by a mad scientist to believe that our reality is real. Putnam argued that this proposition, however, rests on a \"magical theory of reference\" in which the existential evidence necessary to validate it is assumed. Thus, the brain-in-a-vat proposition does not make for much of a hypothesis at all since there is no means to verify its truth. It does, however, provide a contrast for what a good working hypothesis would look like: one suited to culling potential existential evidence of the subject at hand.\n\nA more concrete example would be that of conjectures in mathematics – propositions which appear to be true but which are formally unproven. Very often, conjectures will be provisionally accepted as working hypotheses in order to investigate its consequences and formulate conditional proofs.\n\nIn the field of public administration working hypotheses are used as a conceptual framework for exploratory, applied, empirical research Research projects that use working hypotheses use a deductive reasoning or logic of inquiry. In other words, the problem and preliminary theory are developed ahead of time and tested using evidence. Working hypotheses (statements of expectation) are flexible and incorporate relational or non-relational statements. They are often used as ways to investigate a problem in a particular city or public agency. \nThese projects are a type of case study and use multiple methods of evidence collection. The working hypotheses are used as a device to direct evidence collection. As a result, working hypotheses are generally organized using sub-hypotheses, which specify in more detail the kinds of data or evidence needed to support the hypothesis.\n\n"}
