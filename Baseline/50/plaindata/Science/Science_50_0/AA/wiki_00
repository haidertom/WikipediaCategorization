{"id": "51238945", "url": "https://en.wikipedia.org/wiki?curid=51238945", "title": "Advancing Secondary Science Education thru Tetrahymena", "text": "Advancing Secondary Science Education thru Tetrahymena\n\nAdvancing Secondary Science Education thru Tetrahymena (ASSET) is an organization at Cornell University that is dedicated to expanding the use of the protist \"Tetrahymena\" in K-12 classrooms. They are funded by the National Institutes of Health through the SEPA (Science Education Partnership Award) Program. Although their name includes the word \"secondary,\" they have worked in recent years to develop materials for students in elementary, middle and high schools. The group develops modules, which are stand-alone labs or lessons that can be inserted into the curriculum of a class at the discretion of the teacher.\n\nModules are designed to be stand-alone lessons that fit into and compliment a life science curriculum. The ASSET program ships all the equipment that is needed to complete the modules to the teacher in a reusable plastic container, at ASSET's expense. The teacher who requested the materials can use them for up to two weeks. At the end of the two weeks, the teacher uses a pre-paid return label to send the materials back in the same container. Some materials, such as live cells, may be sent separately to provide for a chance for the culture to be established in the teacher's classroom.\n\nASSET has created fifteen science modules, each of which addresses a particular topic relevant to life science education in United States school curricula.\n\nThis module utilizes two species of \"Tetrahymena\": \"Tetrahymena thermophilia\" and \"Tetrahymena vorax\". In the lab, an extract, called stomatin, is made from the thermophilia, then placed into the vorax culture. There, it induces a transformation from the microstome form to the macrostome form in T. vorax. This transformation is most notable by a marked increase in the size of the cell (doubling or sometimes more), the resorption of the microstomal oral apparatus and the construction of a much larger macrostomal oral apparatus. This transformation allows the macrostomal T. vorax cells to prey on T. thermophilia, but also to cannibalize the microstomes of their own species.\n\nASSET has also created five science and society modules, which are designed to integrate social studies and science education into one unit.\n\n\n"}
{"id": "1845", "url": "https://en.wikipedia.org/wiki?curid=1845", "title": "Alternative medicine", "text": "Alternative medicine\n\nAlternative medicine, fringe medicine, pseudomedicine or simply questionable medicine is the use and promotion of practices which are unproven, disproven, impossible to prove, or excessively harmful in relation to their effect — in the attempt to achieve the healing effects of medicine. They differ from experimental medicine in that the latter employs responsible investigation, and accepts results that show it to be ineffective. The scientific consensus is that alternative therapies either do not, or cannot, work. In some cases laws of nature are violated by their basic claims; in some the treatment is so much worse that its use is unethical. Alternative practices, products, and therapies range from only ineffective to having known harmful and toxic effects. \n\nAlternative therapies may be credited for perceived improvement through placebo effects, decreased use or effect of medical treatment (and therefore either decreased side effects; or nocebo effects towards standard treatment), or the natural course of the condition or disease. Alternative treatment is not the same as experimental treatment or traditional medicine, although both can be misused in ways that are alternative. Alternative or complementary medicine is dangerous because it may discourage people from getting the best possible treatment, and may lead to a false understanding of the body and of science.\n\nAlternative medicine is used by a significant number of people, though its popularity is often overstated. Large amounts of funding go to testing alternative medicine, with more than US$2.5 billion spent by the United States government alone. Almost none show any effect beyond that of false treatment, and most studies showing any effect have been statistical flukes. Alternative medicine is a highly profitable industry, with a strong lobby. This fact is often overlooked by media or intentionally kept hidden, with alternative practice being portrayed positively when compared to \"big pharma\". The lobby has successfully pushed for alternative therapies to be subject to far less regulation than conventional medicine. Alternative therapies may even be allowed to promote use when there is demonstrably no effect, only a tradition of use. Regulation and licensing of alternative medicine and health care providers varies between and within countries. Despite laws making it illegal to market or promote alternative therapies for use in cancer treatment, many practitioners promote them. Alternative medicine is criticized for taking advantage of the weakest members of society.\n\nTerminology has shifted over time, reflecting the preferred branding of practitioners. For example, the United States National Institutes of Health department studying alternative medicine, currently named National Center for Complementary and Integrative Health, was established as the \"Office of Alternative Medicine\" and was renamed the \"National Center for Complementary and Alternative Medicine\" before obtaining its current name. Therapies are often framed as \"natural\" or \"holistic\", in apparent opposition to conventional medicine which is \"artificial\" and \"narrow in scope\", statements which are intentionally misleading. When used together with functional medical treatment, alternative therapies do not \"complement\" (improve the effect of, or mitigate the side effects of) treatment. Significant drug interactions caused by alternative therapies may instead negatively impact functional treatment by making prescription drugs less effective, such as interference by herbal preparations with warfarin.\n\nAlternative medicine is defined loosely as a set of products, practices, and theories that are believed or perceived by their users to have the healing effects of medicine, but whose effectiveness has not been clearly established using scientific methods, or whose theory and practice is not part of biomedicine, or whose theories or practices are directly contradicted by scientific evidence or scientific principles used in biomedicine. \"Biomedicine\" or \"medicine\" is that part of medical science that applies principles of biology, physiology, molecular biology, biophysics, and other natural sciences to clinical practice, using scientific methods to establish the effectiveness of that practice. Unlike medicine, an alternative product or practice does not originate from using scientific methods, but may instead be based on hearsay, religion, tradition, superstition, belief in supernatural energies, pseudoscience, errors in reasoning, propaganda, fraud, or other unscientific sources.\n\nIn \"General Guidelines for Methodologies on Research and Evaluation of Traditional Medicine\", published in 2000 by the World Health Organization (WHO), complementary and alternative medicine were defined as a broad set of health care practices that are not part of that country's own tradition and are not integrated into the dominant health care system.\n\nThe expression also refers to a diverse range of related and unrelated products, practices, and theories ranging from biologically plausible practices and products and practices with some evidence, to practices and theories that are directly contradicted by basic science or clear evidence, and products that have been conclusively proven to be ineffective or even toxic and harmful.\n\nThe terms \"alternative medicine\", \"complementary medicine\", \"integrative medicine,\" \"holistic medicine\", \"natural medicine\", \"unorthodox medicine\", \"fringe medicine\", \"unconventional medicine\", and \"new age medicine\" are used interchangeably as having the same meaning and are almost synonymous in most contexts.\n\nThe meaning of the term \"alternative\" in the expression \"alternative medicine\", is not that it is an effective alternative to medical science, although some alternative medicine promoters may use the loose terminology to give the appearance of effectiveness. Loose terminology may also be used to suggest meaning that a dichotomy exists when it does not, e.g., the use of the expressions \"western medicine\" and \"eastern medicine\" to suggest that the difference is a cultural difference between the Asiatic east and the European west, rather than that the difference is between evidence-based medicine and treatments that do not work.\n\nComplementary medicine (CM) or integrative medicine (IM) is when alternative medicine is used together with functional medical treatment, in a belief that it improves the effect of treatments. However, significant drug interactions caused by alternative therapies may instead negatively influence treatment, making treatments less effective, notably cancer therapy. Both terms refer to use of alternative medical treatments alongside conventional medicine, an example of which is use of acupuncture (sticking needles in the body to influence the flow of a supernatural energy), along with using science-based medicine, in the belief that the acupuncture increases the effectiveness or \"complements\" the science-based medicine.\n\nCAM is an abbreviation of the phrase \"complementary and alternative medicine\". It has also been called sCAM or SCAM with the addition of \"so-called\" or \"supplements\".\n\n\"Allopathic medicine\" or \"allopathy\" is an expression commonly used by homeopaths and proponents of other forms of alternative medicine to refer to mainstream medicine. It was used to describe the traditional European practice of heroic medicine, but later continued to be used to describe anything that was not homeopathy.\n\nAllopathy refers to the use of pharmacologically active agents or physical interventions to treat or suppress symptoms or pathophysiologic processes of diseases or conditions. The German version of the word, , was coined in 1810 by the creator of homeopathy, Samuel Hahnemann (1755–1843). The word was coined from (different) and (relating to a disease or to a method of treatment). In alternative medicine circles the expression \"allopathic medicine\" is still used to refer to \"the broad category of medical practice that is sometimes called Western medicine, biomedicine, evidence-based medicine, or modern medicine\" (see the article on scientific medicine).\n\nUse of the term remains common among homeopaths and has spread to other alternative medicine practices. The meaning implied by the label has never been accepted by conventional medicine and is considered pejorative. More recently, some sources have used the term \"allopathic\", particularly American sources wishing to distinguish between Doctors of Medicine (MD) and Doctors of Osteopathic Medicine (DO) in the United States. William Jarvis, an expert on alternative medicine and public health, states that \"although many modern therapies can be construed to conform to an allopathic rationale (e.g., using a laxative to relieve constipation), standard medicine has never paid allegiance to an allopathic principle\" and that the label \"allopath\" was from the start \"considered highly derisive by regular medicine\".\n\nMany conventional medical treatments do not fit the nominal definition of \"allopathy\", as they seek to prevent illness, or remove its cause.\n\nCAM is an abbreviation of complementary and alternative medicine. It has also been called sCAM or SCAM with the addition of \"so-called\" or \"supplements\". The words balance and holism are often used, claiming to take into account a \"whole\" person, in contrast to the supposed reductionism of medicine. Due to its many names the field has been criticized for intense rebranding of what are essentially the same practices: as soon as one name is declared synonymous with quackery, a new name is chosen.\n\nTraditional medicine refers to the pre-scientific practices of a certain culture, contrary to what is typically practiced in other cultures where medical science dominates.\n\n\"Eastern medicine\" typically refers to the traditional medicines of Asia where conventional bio-medicine penetrated much later.\n\nThe words \"balance\" and \"holism\" are often used alongside \"complementary\" or \"integrative\" medicine, claiming to take into account a \"whole\" person, in contrast to the supposed reductionism of medicine. Due to its many names the field has been criticized for intense rebranding of what are essentially the same practices.\n\nProminent members of the science and biomedical science community say that it is not meaningful to define an alternative medicine that is separate from a conventional medicine, that the expressions \"conventional medicine\", \"alternative medicine\", \"complementary medicine\", \"integrative medicine\", and \"holistic medicine\" do not refer to any medicine at all.\n\nOthers in both the biomedical and CAM communities say that CAM \"cannot\" be precisely defined because of the diversity of theories and practices it includes, and because the boundaries between CAM and biomedicine overlap, are porous, and change. The expression \"complementary and alternative medicine\" (CAM) resists easy definition because the health systems and practices it refers to are diffuse, and its boundaries poorly defined. Healthcare practices categorized as alternative may differ in their historical origin, theoretical basis, diagnostic technique, therapeutic practice and in their relationship to the medical mainstream. Some alternative therapies, including traditional Chinese medicine (TCM) and Ayurveda, have antique origins in East or South Asia and are entirely alternative medical systems; others, such as homeopathy and chiropractic, have origins in Europe or the United States and emerged in the eighteenth and nineteenth centuries. Some, such as osteopathy and chiropractic, employ manipulative physical methods of treatment; others, such as meditation and prayer, are based on mind-body interventions. Treatments considered alternative in one location may be considered conventional in another. Thus, chiropractic is not considered alternative in Denmark and likewise osteopathic medicine is no longer thought of as an alternative therapy in the United States.\n\nCritics say the expression is deceptive because it implies there is an effective alternative to science-based medicine, and that \"complementary\" is deceptive because it implies that the treatment increases the effectiveness of (complements) science-based medicine, while alternative medicines that have been tested nearly always have no measurable positive effect compared to a placebo.\n\nOne common feature of all definitions of alternative medicine is its designation as \"other than\" conventional medicine. For example, the widely referenced descriptive definition of complementary and alternative medicine devised by the US National Center for Complementary and Integrative Health (NCCIH) of the National Institutes of Health (NIH), states that it is \"a group of diverse medical and health care systems, practices, and products that are not generally considered part of conventional medicine\". For conventional medical practitioners, it does not necessarily follow that either it or its practitioners would no longer be considered alternative.\n\nSome definitions seek to specify alternative medicine in terms of its social and political marginality to mainstream healthcare. This can refer to the lack of support that alternative therapies receive from the medical establishment and related bodies regarding access to research funding, sympathetic coverage in the medical press, or inclusion in the standard medical curriculum. In 1993, the British Medical Association (BMA), one among many professional organizations who have attempted to define alternative medicine, stated that it referred to \"...those forms of treatment which are not widely used by the conventional healthcare professions, and the skills of which are not taught as part of the undergraduate curriculum of conventional medical and paramedical healthcare courses\". In a US context, an influential definition coined in 1993 by the Harvard-based physician, David M. Eisenberg, characterized alternative medicine \"as interventions neither taught widely in medical schools nor generally available in US hospitals\". These descriptive definitions are inadequate in the present-day when some conventional doctors offer alternative medical treatments and CAM introductory courses or modules can be offered as part of standard undergraduate medical training; alternative medicine is taught in more than 50 per cent of US medical schools and increasingly US health insurers are willing to provide reimbursement for CAM therapies. In 1999, 7.7% of US hospitals reported using some form of CAM therapy; this proportion had risen to 37.7% by 2008.\n\nAn expert panel at a conference hosted in 1995 by the US Office for Alternative Medicine (OAM), devised a theoretical definition of alternative medicine as \"a broad domain of healing resources ... other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period\". This definition has been widely adopted by CAM researchers, cited by official government bodies such as the UK Department of Health, attributed as the definition used by the Cochrane Collaboration, and, with some modification, was preferred in the 2005 consensus report of the US Institute of Medicine, \"Complementary and Alternative Medicine in the United States\".\n\nThe 1995 OAM conference definition, an expansion of Eisenberg's 1993 formulation, is silent regarding questions of the medical effectiveness of alternative therapies. Its proponents hold that it thus avoids relativism about differing forms of medical knowledge and, while it is an essentially political definition, this should not imply that the dominance of mainstream biomedicine is solely due to political forces. According to this definition, alternative and mainstream medicine can only be differentiated with reference to what is \"intrinsic to the politically dominant health system of a particular society of culture\". However, there is neither a reliable method to distinguish between cultures and subcultures, nor to attribute them as dominant or subordinate, nor any accepted criteria to determine the dominance of a cultural entity. If the culture of a politically dominant healthcare system is held to be equivalent to the perspectives of those charged with the medical management of leading healthcare institutions and programs, the definition fails to recognize the potential for division either within such an elite or between a healthcare elite and the wider population.\n\nNormative definitions distinguish alternative medicine from the biomedical mainstream in its provision of therapies that are unproven, unvalidated, or ineffective and support of theories with no recognized scientific basis. These definitions characterize practices as constituting alternative medicine when, used independently or in place of evidence-based medicine, they are put forward as having the healing effects of medicine, but are not based on evidence gathered with the scientific method. Exemplifying this perspective, a 1998 editorial co-authored by Marcia Angell, a former editor of \"The New England Journal of Medicine\", argued that:\n\nIt is time for the scientific community to stop giving alternative medicine a free ride. There cannot be two kinds of medicine – conventional and alternative. There is only medicine that has been adequately tested and medicine that has not, medicine that works and medicine that may or may not work. Once a treatment has been tested rigorously, it no longer matters whether it was considered alternative at the outset. If it is found to be reasonably safe and effective, it will be accepted. But assertions, speculation, and testimonials do not substitute for evidence. Alternative treatments should be subjected to scientific testing no less rigorous than that required for conventional treatments.\n\nThis line of division has been subject to criticism, however, as not all forms of standard medical practice have adequately demonstrated evidence of benefit, and it is also unlikely in most instances that conventional therapies, if proven to be ineffective, would ever be classified as CAM.\n\nSimilarly, the public information website maintained by the National Health and Medical Research Council (NHMRC) of the Commonwealth of Australia uses the acronym \"CAM\" for a wide range of health care practices, therapies, procedures and devices not within the domain of conventional medicine. In the Australian context this is stated to include acupuncture; aromatherapy; chiropractic; homeopathy; massage; meditation and relaxation therapies; naturopathy; osteopathy; reflexology, traditional Chinese medicine; and the use of vitamin supplements.\n\nThe Danish National Board of Health's \"Council for Alternative Medicine\" (Sundhedsstyrelsens Råd for Alternativ Behandling (SRAB)), an independent institution under the National Board of Health (Danish: \"Sundhedsstyrelsen\"), uses the term \"alternative medicine\" for:\n\nProponents of an evidence-base for medicine such as the Cochrane Collaboration (founded in 1993 and from 2011 providing input for WHO resolutions) take a position that \"all\" systematic reviews of treatments, whether \"mainstream\" or \"alternative\", ought to be held to the current standards of scientific method. In a study titled \"Development and classification of an operational definition of complementary and alternative medicine for the Cochrane Collaboration\" (2011) it was proposed that indicators that a therapy is accepted include government licensing of practitioners, coverage by health insurance, statements of approval by government agencies, and recommendation as part of a practice guideline; and that if something is currently a standard, accepted therapy, then it is not likely to be widely considered as CAM.\n\nAlternative medicine consists of a wide range of health care practices, products, and therapies. The shared feature is a claim to heal that is not based on the scientific method. Alternative medicine practices are diverse in their foundations and methodologies. Alternative medicine practices may be classified by their cultural origins or by the types of beliefs upon which they are based. Methods may incorporate or be based on traditional medicinal practices of a particular culture, folk knowledge, superstition, spiritual beliefs, belief in supernatural energies (antiscience), pseudoscience, errors in reasoning, propaganda, fraud, new or different concepts of health and disease, and any bases other than being proven by scientific methods. Different cultures may have their own unique traditional or belief based practices developed recently or over thousands of years, and specific practices or entire systems of practices.\nAlternative medicine, such as using naturopathy or homeopathy in place of conventional medicine, is based on belief systems not grounded in science.\n\nAlternative medical systems may be based on traditional medicine practices, such as traditional Chinese medicine (TCM), Ayurveda in India, or practices of other cultures around the world. Some useful applications of traditional medicines have been researched and accepted within ordinary medicine, however the underlying belief systems are seldom scientific and are not accepted.\n\nTraditional medicine is considered alternative when it is used outside its home region; or when it is used together with or instead of known functional treatment; or when it can be reasonably expected that the patient or practitioner knows or should know that it will not work – such as knowing that the practice is based on superstition.\n\nSince ancient times, in many parts of the world a number of herbs reputed to possess abortifacient properties have been used in folk medicine. Among these are: tansy, pennyroyal, black cohosh, and the now-extinct silphium. Historian of science Ann Hibner Koblitz has written of the probable protoscientific origins of this folk knowledge in observation of farm animals. Women who knew that grazing on certain plants would cause an animal to abort (with negative economic consequences for the farm) would be likely to try out those plants on themselves in order to avoid an unwanted pregnancy.\n\nHowever, modern users of these plants often lack knowledge of the proper preparation and dosage. The historian of medicine John Riddle has spoken of the \"broken chain of knowledge\" caused by urbanization and modernization, and Koblitz has written that \"folk knowledge about effective contraception techniques often disappears over time or becomes inextricably mixed with useless or harmful practices.\" The ill-informed or indiscriminant use of herbs as abortifacients can cause serious and even lethal side-effects.\n\nBases of belief may include belief in existence of supernatural energies undetected by the science of physics, as in biofields, or in belief in properties of the energies of physics that are inconsistent with the laws of physics, as in energy medicine.\n\nSubstance based practices use substances found in nature such as herbs, foods, non-vitamin supplements and megavitamins, animal and fungal products, and minerals, including use of these products in traditional medical practices that may also incorporate other methods. Examples include healing claims for nonvitamin supplements, fish oil, Omega-3 fatty acid, glucosamine, echinacea, flaxseed oil, and ginseng. Herbal medicine, or phytotherapy, includes not just the use of plant products, but may also include the use of animal and mineral products. It is among the most commercially successful branches of alternative medicine, and includes the tablets, powders and elixirs that are sold as \"nutritional supplements\". Only a very small percentage of these have been shown to have any efficacy, and there is little regulation as to standards and safety of their contents. This may include use of known toxic substances, such as use of the poison lead in traditional Chinese medicine.\n\nA US agency, National Center on Complementary and Integrative Health (NCCIH), has created a classification system for branches of complementary and alternative medicine that divides them into five major groups. These groups have some overlap, and distinguish two types of energy medicine: \"veritable\" which involves scientifically observable energy (including magnet therapy, colorpuncture and light therapy) and \"putative\", which invokes physically undetectable or unverifiable energy. None of these energies have any evidence to support that they effect the body in any positive or health promoting way.\n\n\nThe history of alternative medicine may refer to the history of a group of diverse medical practices that were collectively promoted as \"alternative medicine\" beginning in the 1970s, to the collection of individual histories of members of that group, or to the history of western medical practices that were labeled \"irregular practices\" by the western medical establishment. It includes the histories of complementary medicine and of integrative medicine. Before the 1970s, western practitioners that were not part of the increasingly science-based medical establishment were referred to \"irregular practitioners\", and were dismissed by the medical establishment as unscientific and as practicing quackery. Until the 1970s, irregular practice became increasingly marginalized as quackery and fraud, as western medicine increasingly incorporated scientific methods and discoveries, and had a corresponding increase in success of its treatments. In the 1970s, irregular practices were grouped with traditional practices of nonwestern cultures and with other unproven or disproven practices that were not part of biomedicine, with the entire group collectively marketed and promoted under the single expression \"alternative medicine\".\n\nUse of alternative medicine in the west began to rise following the counterculture movement of the 1960s, as part of the rising new age movement of the 1970s. This was due to misleading mass marketing of \"alternative medicine\" being an effective \"alternative\" to biomedicine, changing social attitudes about not using chemicals and challenging the establishment and authority of any kind, sensitivity to giving equal measure to beliefs and practices of other cultures (cultural relativism), and growing frustration and desperation by patients about limitations and side effects of science-based medicine. At the same time, in 1975, the American Medical Association, which played the central role in fighting quackery in the United States, abolished its quackery committee and closed down its Department of Investigation. By the early to mid 1970s the expression \"alternative medicine\" came into widespread use, and the expression became mass marketed as a collection of \"natural\" and effective treatment \"alternatives\" to science-based biomedicine. By 1983, mass marketing of \"alternative medicine\" was so pervasive that the British Medical Journal (BMJ) pointed to \"an apparently endless stream of books, articles, and radio and television programmes urge on the public the virtues of (alternative medicine) treatments ranging from meditation to drilling a hole in the skull to let in more oxygen\".\n\nMainly as a result of reforms following the Flexner Report of 1910 medical education in established medical schools in the US has generally not included alternative medicine as a teaching topic. Typically, their teaching is based on current practice and scientific knowledge about: anatomy, physiology, histology, embryology, neuroanatomy, pathology, pharmacology, microbiology and immunology. Medical schools' teaching includes such topics as doctor-patient communication, ethics, the art of medicine, and engaging in complex clinical reasoning (medical decision-making). Writing in 2002, Snyderman and Weil remarked that by the early twentieth century the Flexner model had helped to create the 20th-century academic health center, in which education, research, and practice were inseparable. While this had much improved medical practice by defining with increasing certainty the pathophysiological basis of disease, a single-minded focus on the pathophysiological had diverted much of mainstream American medicine from clinical conditions that were not well understood in mechanistic terms, and were not effectively treated by conventional therapies.\n\nBy 2001 some form of CAM training was being offered by at least 75 out of 125 medical schools in the US. Exceptionally, the School of Medicine of the University of Maryland, Baltimore includes a research institute for integrative medicine (a member entity of the Cochrane Collaboration). Medical schools are responsible for conferring medical degrees, but a physician typically may not legally practice medicine until licensed by the local government authority. Licensed physicians in the US who have attended one of the established medical schools there have usually graduated Doctor of Medicine (MD). All states require that applicants for MD licensure be graduates of an approved medical school and complete the United States Medical Licensing Exam (USMLE).\n\nThere is a general scientific consensus that alternative therapies lack the requisite scientific validation, and their effectiveness is either unproved or disproved. Many of the claims regarding the efficacy of alternative medicines are controversial, since research on them is frequently of low quality and methodologically flawed. Selective publication bias, marked differences in product quality and standardisation, and some companies making unsubstantiated claims call into question the claims of efficacy of isolated examples where there is evidence for alternative therapies.\n\n\"The Scientific Review of Alternative Medicine\" points to confusions in the general population – a person may attribute symptomatic relief to an otherwise-ineffective therapy just because they are taking something (the placebo effect); the natural recovery from or the cyclical nature of an illness (the regression fallacy) gets misattributed to an alternative medicine being taken; a person not diagnosed with science-based medicine may never originally have had a true illness diagnosed as an alternative disease category.\n\nEdzard Ernst characterized the evidence for many alternative techniques as weak, nonexistent, or negative and in 2011 published his estimate that about 7.4% were based on \"sound evidence\", although he believes that may be an overestimate. Ernst has concluded that 95% of the alternative treatments he and his team studied, including acupuncture, herbal medicine, homeopathy, and reflexology, are \"statistically indistinguishable from placebo treatments\", but he also believes there is something that conventional doctors can usefully learn from the chiropractors and homeopath: this is the therapeutic value of the placebo effect, one of the strangest phenomena in medicine.\n\nIn 2003, a project funded by the CDC identified 208 condition-treatment pairs, of which 58% had been studied by at least one randomized controlled trial (RCT), and 23% had been assessed with a meta-analysis. According to a 2005 book by a US Institute of Medicine panel, the number of RCTs focused on CAM has risen dramatically.\n\n, the Cochrane Library had 145 CAM-related Cochrane systematic reviews and 340 non-Cochrane systematic reviews. An analysis of the conclusions of only the 145 Cochrane reviews was done by two readers. In 83% of the cases, the readers agreed. In the 17% in which they disagreed, a third reader agreed with one of the initial readers to set a rating. These studies found that, for CAM, 38.4% concluded positive effect or possibly positive (12.4%), 4.8% concluded no effect, 0.7% concluded harmful effect, and 56.6% concluded insufficient evidence. An assessment of conventional treatments found that 41.3% concluded positive or possibly positive effect, 20% concluded no effect, 8.1% concluded net harmful effects, and 21.3% concluded insufficient evidence. However, the CAM review used the more developed 2004 Cochrane database, while the conventional review used the initial 1998 Cochrane database.\n\nIn the same way as for conventional therapies, drugs, and interventions, it can be difficult to test the efficacy of alternative medicine in clinical trials. In instances where an established, effective, treatment for a condition is already available, the Helsinki Declaration states that withholding such treatment is unethical in most circumstances. Use of standard-of-care treatment in addition to an alternative technique being tested may produce confounded or difficult-to-interpret results.\n\nCancer researcher Andrew J. Vickers has stated:\n\nContrary to much popular and scientific writing, many alternative cancer treatments have been investigated in good-quality clinical trials, and they have been shown to be ineffective. The label \"unproven\" is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been \"disproven\".\n\nA research methods expert and author of \"Snake Oil Science\", R. Barker Bausell, has stated that \"it's become politically correct to investigate nonsense.\" There are concerns that just having NIH support is being used to give unfounded \"legitimacy to treatments that are not legitimate.\"\n\nUse of placebos to achieve a placebo effect in integrative medicine has been criticized as, \"...diverting research time, money, and other resources from more fruitful lines of investigation in order to pursue a theory that has no basis in biology.\"\n\nAnother critic has argued that academic proponents of integrative medicine sometimes recommend misleading patients by using known placebo treatments to achieve a placebo effect. However, a 2010 survey of family physicians found that 56% of respondents said they had used a placebo in clinical practice as well. Eighty-five percent of respondents believed placebos can have both psychological and physical benefits.\n\nIntegrative medicine has been criticized in that its practitioners, trained in science-based medicine, deliberately mislead patients by pretending placebos are not. \"quackademic medicine\" is a pejorative term used for \"integrative medicine\", which medical professionals consider an infiltration of quackery into academic science-based medicine.\n\nAn analysis of trends in the criticism of complementary and alternative medicine (CAM) in five prestigious American medical journals during the period of reorganization within medicine (1965–1999) was reported as showing that the medical profession had responded to the growth of CAM in three phases, and that in each phase, changes in the medical marketplace had influenced the type of response in the journals. Changes included relaxed medical licensing, the development of managed care, rising consumerism, and the establishment of the USA Office of Alternative Medicine (later National Center for Complementary and Alternative Medicine, currently National Center for Complementary and Integrative Health). In the \"condemnation\" phase, from the late 1960s to the early 1970s, authors had ridiculed, exaggerated the risks, and petitioned the state to contain CAM; in the \"reassessment\" phase (mid-1970s through early 1990s), when increased consumer utilization of CAM was prompting concern, authors had pondered whether patient dissatisfaction and shortcomings in conventional care contributed to the trend; in the \"integration\" phase of the 1990s physicians began learning to work around or administer CAM, and the subjugation of CAM to scientific scrutiny had become the primary means of control.\n\nPractitioners of complementary medicine usually discuss and advise patients as to available alternative therapies. Patients often express interest in mind-body complementary therapies because they offer a non-drug approach to treating some health conditions.\n\nIn addition to the social-cultural underpinnings of the popularity of alternative medicine, there are several psychological issues that are critical to its growth. One of the most critical is the placebo effect – a well-established observation in medicine. Related to it are similar psychological effects, such as the will to believe, cognitive biases that help maintain self-esteem and promote harmonious social functioning, and the \"post hoc, ergo propter hoc\" fallacy.\n\nThe popularity of complementary & alternative medicine (CAM) may be related to other factors that Edzard Ernst mentioned in an interview in \"The Independent\":\n\nWhy is it so popular, then? Ernst blames the providers, customers and the doctors whose neglect, he says, has created the opening into which alternative therapists have stepped. \"People are told lies. There are 40 million websites and 39.9 million tell lies, sometimes outrageous lies. They mislead cancer patients, who are encouraged not only to pay their last penny but to be treated with something that shortens their lives. \"At the same time, people are gullible. It needs gullibility for the industry to succeed. It doesn't make me popular with the public, but it's the truth.\n\nPaul Offit proposed that \"alternative medicine becomes quackery\" in four ways: by recommending against conventional therapies that are helpful, promoting potentially harmful therapies without adequate warning, draining patients' bank accounts, or by promoting \"magical thinking.\"\n\nAuthors have speculated on the socio-cultural and psychological reasons for the appeal of alternative medicines among the minority using them \"in lieu\" of conventional medicine. There are several socio-cultural reasons for the interest in these treatments centered on the low level of scientific literacy among the public at large and a concomitant increase in antiscientific attitudes and new age mysticism. Related to this are vigorous marketing of extravagant claims by the alternative medical community combined with inadequate media scrutiny and attacks on critics.\n\nThere is also an increase in conspiracy theories toward conventional medicine and pharmaceutical companies, mistrust of traditional authority figures, such as the physician, and a dislike of the current delivery methods of scientific biomedicine, all of which have led patients to seek out alternative medicine to treat a variety of ailments. Many patients lack access to contemporary medicine, due to a lack of private or public health insurance, which leads them to seek out lower-cost alternative medicine. Medical doctors are also aggressively marketing alternative medicine to profit from this market.\n\nPatients can be averse to the painful, unpleasant, and sometimes-dangerous side effects of biomedical treatments. Treatments for severe diseases such as cancer and HIV infection have well-known, significant side-effects. Even low-risk medications such as antibiotics can have potential to cause life-threatening anaphylactic reactions in a very few individuals. Many medications may cause minor but bothersome symptoms such as cough or upset stomach. In all of these cases, patients may be seeking out alternative treatments to avoid the adverse effects of conventional treatments.\n\nComplementary and alternative medicine (CAM) has been described as a broad domain of healing resources that encompasses all health systems, modalities, and practices and their accompanying theories and beliefs, other than those intrinsic to the politically dominant health system of a particular society or culture in a given historical period. CAM includes all such practices and ideas self-defined by their users as preventing or treating illness or promoting health and well-being. Boundaries within CAM and between the CAM domain and that of the dominant system are not always sharp or fixed.\n\nAccording to recent research, the increasing popularity of the CAM needs to be explained by moral convictions or lifestyle choices rather than by economic reasoning.\n\nIn developing nations, access to essential medicines is severely restricted by lack of resources and poverty. Traditional remedies, often closely resembling or forming the basis for alternative remedies, may comprise primary healthcare or be integrated into the healthcare system. In Africa, traditional medicine is used for 80% of primary healthcare, and in developing nations as a whole over one-third of the population lack access to essential medicines.\n\nSome have proposed adopting a prize system to reward medical research. However, public funding for research exists. Increasing the funding for research on alternative medicine techniques is the purpose of the US National Center for Complementary and Alternative Medicine. NCCIH and its predecessor, the Office of Alternative Medicine, have spent more than US$2.5 billion on such research since 1992; this research has largely not demonstrated the efficacy of alternative treatments.\n\nThat alternative medicine has been on the rise \"in countries where Western science and scientific method generally are accepted as the major foundations for healthcare, and 'evidence-based' practice is the dominant paradigm\" was described as an \"enigma\" in the Medical Journal of Australia.\n\nIn the United States, the 1974 Child Abuse Prevention and Treatment Act (CAPTA) required that for states to receive federal money, they had to grant religious exemptions to child neglect and abuse laws regarding religion-based healing practices. Thirty-one states have child-abuse religious exemptions.\n\nThe use of alternative medicine in the US has increased, with a 50 percent increase in expenditures and a 25 percent increase in the use of alternative therapies between 1990 and 1997 in America. Americans spend many billions on the therapies annually. Most Americans used CAM to treat and/or prevent musculoskeletal conditions or other conditions associated with chronic or recurring pain. In America, women were more likely than men to use CAM, with the biggest difference in use of mind-body therapies including prayer specifically for health reasons\". In 2008, more than 37% of American hospitals offered alternative therapies, up from 27 percent in 2005, and 25% in 2004. More than 70% of the hospitals offering CAM were in urban areas.\n\nA survey of Americans found that 88 percent thought that \"there are some good ways of treating sickness that medical science does not recognize\". Use of magnets was the most common tool in energy medicine in America, and among users of it, 58 percent described it as at least \"sort of scientific\", when it is not at all scientific. In 2002, at least 60 percent of US medical schools have at least some class time spent teaching alternative therapies. \"Therapeutic touch\" was taught at more than 100 colleges and universities in 75 countries before the practice was debunked by a nine-year-old child for a school science project.\n\nThe most common CAM therapies used in the US in 2002 were prayer (45%), herbalism (19%), breathing meditation (12%), meditation (8%), chiropractic medicine (8%), yoga (5–6%), body work (5%), diet-based therapy (4%), progressive relaxation (3%), mega-vitamin therapy (3%) and Visualization (2%)\n\nIn Britain, the most often used alternative therapies were Alexander technique, Aromatherapy, Bach and other flower remedies, Body work therapies including massage, Counseling stress therapies, hypnotherapy, Meditation, Reflexology, Shiatsu, Ayurvedic medicine, Nutritional medicine, and Yoga. Ayurvedic medicine remedies are mainly plant based with some use of animal materials. Safety concerns include the use of herbs containing toxic compounds and the lack of quality control in Ayurvedic facilities.\n\nAccording to the National Health Service (England), the most commonly used complementary and alternative medicines (CAM) supported by the NHS in the UK are: acupuncture, aromatherapy, chiropractic, homeopathy, massage, osteopathy and clinical hypnotherapy.\n\nComplementary therapies are often used in palliative care or by practitioners attempting to manage chronic pain in patients. Integrative medicine is considered more acceptable in the interdisciplinary approach used in palliative care than in other areas of medicine. \"From its early experiences of care for the dying, palliative care took for granted the necessity of placing patient values and lifestyle habits at the core of any design and delivery of quality care at the end of life. If the patient desired complementary therapies, and as long as such treatments provided additional support and did not endanger the patient, they were considered acceptable.\" The non-pharmacologic interventions of complementary medicine can employ mind-body interventions designed to \"reduce pain and concomitant mood disturbance and increase quality of life.\"\n\nIn Austria and Germany complementary and alternative medicine is mainly in the hands of doctors with MDs, and half or more of the American alternative practitioners are licensed MDs. In Germany herbs are tightly regulated: half are prescribed by doctors and covered by health insurance.\n\nSome professions of complementary/traditional/alternative medicine, such as chiropractic, have achieved full regulation in North America and other parts of the world and are regulated in a manner similar to that governing science-based medicine. In contrast, other approaches may be partially recognized and others have no regulation at all. Regulation and licensing of alternative medicine ranges widely from country to country, and state to state.\n\nGovernment bodies in the US and elsewhere have published information or guidance about alternative medicine. The U.S. Food and Drug Administration (FDA), has issued online warnings for consumers about medication health fraud. This includes a section on Alternative Medicine Fraud, such as a warning that Ayurvedic products generally have not been approved by the FDA before marketing.\n\nMany of the claims regarding the safety and efficacy of alternative medicine are controversial. Some alternative treatments have been associated with unexpected side effects, which can be fatal.\n\nA commonly voiced concerns about complementary alternative medicine (CAM) is the way it's regulated. There have been significant developments in how CAMs should be assessed prior to re-sale in the United Kingdom and the European Union (EU) in the last 2 years. Despite this, it has been suggested that current regulatory bodies have been ineffective in preventing deception of patients as many companies have re-labelled their drugs to avoid the new laws. There is no general consensus about how to balance consumer protection (from false claims, toxicity, and advertising) with freedom to choose remedies.\n\nAdvocates of CAM suggest that regulation of the industry will adversely affect patients looking for alternative ways to manage their symptoms, even if many of the benefits may represent the placebo affect. Some contend that alternative medicines should not require any more regulation than over-the-counter medicines that can also be toxic in overdose (such as paracetamol).\n\nForms of alternative medicine that are biologically active can be dangerous even when used in conjunction with conventional medicine. Examples include immuno-augmentation therapy, shark cartilage, bioresonance therapy, oxygen and ozone therapies, and insulin potentiation therapy. Some herbal remedies can cause dangerous interactions with chemotherapy drugs, radiation therapy, or anesthetics during surgery, among other problems. An example of these dangers was reported by Associate Professor Alastair MacLennan of Adelaide University, Australia regarding a patient who almost bled to death on the operating table after neglecting to mention that she had been taking \"natural\" potions to \"build up her strength\" before the operation, including a powerful anticoagulant that nearly caused her death.\n\nTo \"ABC Online\", MacLennan also gives another possible mechanism:\n\nAnd lastly there's the cynicism and disappointment and depression that some patients get from going on from one alternative medicine to the next, and they find after three months the placebo effect wears off, and they're disappointed and they move on to the next one, and they're disappointed and disillusioned, and that can create depression and make the eventual treatment of the patient with anything effective difficult, because you may not get compliance, because they've seen the failure so often in the past.\n\nConventional treatments are subjected to testing for undesired side-effects, whereas alternative treatments, in general, are not subjected to such testing at all. Any treatment – whether conventional or alternative – that has a biological or psychological effect on a patient may also have potential to possess dangerous biological or psychological side-effects. Attempts to refute this fact with regard to alternative treatments sometimes use the \"appeal to nature\" fallacy, i.e., \"That which is natural cannot be harmful.\" Specific groups of patients such as patients with impaired hepatic or renal function are more susceptible to side effects of alternative remedies.\n\nAn exception to the normal thinking regarding side-effects is Homeopathy. Since 1938, the U.S. Food and Drug Administration (FDA) has regulated homeopathic products in \"several significantly different ways from other drugs.\" Homeopathic preparations, termed \"remedies\", are extremely dilute, often far beyond the point where a single molecule of the original active (and possibly toxic) ingredient is likely to remain. They are, thus, considered safe on that count, but \"their products are exempt from good manufacturing practice requirements related to expiration dating and from finished product testing for identity and strength\", and their alcohol concentration may be much higher than allowed in conventional drugs.\n\nThose having experienced or perceived success with one alternative therapy for a minor ailment may be convinced of its efficacy and persuaded to extrapolate that success to some other alternative therapy for a more serious, possibly life-threatening illness. For this reason, critics argue that therapies that rely on the placebo effect to define success are very dangerous. According to mental health journalist Scott Lilienfeld in 2002, \"unvalidated or scientifically unsupported mental health practices can lead individuals to forgo effective treatments\" and refers to this as \"opportunity cost\". Individuals who spend large amounts of time and money on ineffective treatments may be left with precious little of either, and may forfeit the opportunity to obtain treatments that could be more helpful. In short, even innocuous treatments can indirectly produce negative outcomes. Between 2001 and 2003, four children died in Australia because their parents chose ineffective naturopathic, homeopathic, or other alternative medicines and diets rather than conventional therapies.\n\nThere have always been \"many therapies offered outside of conventional cancer treatment centers and based on theories not found in biomedicine. These alternative cancer cures have often been described as 'unproven,' suggesting that appropriate clinical trials have not been conducted and that the therapeutic value of the treatment is unknown.\" However, \"many alternative cancer treatments have been investigated in good-quality clinical trials, and they have been shown to be ineffective...The label 'unproven' is inappropriate for such therapies; it is time to assert that many alternative cancer therapies have been 'disproven'.\"\n\nEdzard Ernst has stated:\n\n...any alternative cancer cure is bogus by definition. There will never be an alternative cancer cure. Why? Because if something looked halfway promising, then mainstream oncology would scrutinize it, and if there is anything to it, it would become mainstream almost automatically and very quickly. All curative \"alternative cancer cures\" are based on false claims, are bogus, and, I would say, even criminal.\n\n\"CAM\", meaning \"complementary and alternative medicine\", is not as well researched as conventional medicine, which undergoes intense research before release to the public. Funding for research is also sparse making it difficult to do further research for effectiveness of CAM. Most funding for CAM is funded by government agencies. Proposed research for CAM are rejected by most private funding agencies because the results of research are not reliable. The research for CAM has to meet certain standards from research ethics committees, which most CAM researchers find almost impossible to meet. Even with the little research done on it, CAM has not been proven to be effective.\n\nSteven Novella, a neurologist at Yale School of Medicine, wrote that government funded studies of integrating alternative medicine techniques into the mainstream are \"used to lend an appearance of legitimacy to treatments that are not legitimate.\" Marcia Angell considered that critics felt that healthcare practices should be classified based solely on scientific evidence, and if a treatment had been rigorously tested and found safe and effective, science-based medicine will adopt it regardless of whether it was considered \"alternative\" to begin with. It is possible for a method to change categories (proven vs. unproven), based on increased knowledge of its effectiveness or lack thereof. A prominent supporter of this position is George D. Lundberg, former editor of the Journal of the American Medical Association (JAMA).\n\nWriting in 1999 in \"CA: A Cancer Journal for Clinicians\" Barrie R. Cassileth mentioned a 1997 letter to the US Senate Subcommittee on Public Health and Safety, which had deplored the lack of critical thinking and scientific rigor in OAM-supported research, had been signed by four Nobel Laureates and other prominent scientists. (This was supported by the National Institutes of Health (NIH).)\n\nIn March 2009 a staff writer for \"the Washington Post\" reported that the impending national discussion about broadening access to health care, improving medical practice and saving money was giving a group of scientists an opening to propose shutting down the National Center for Complementary and Alternative Medicine. They quoted one of these scientists, Steven Salzberg, a genome researcher and computational biologist at the University of Maryland, as saying \"One of our concerns is that NIH is funding pseudoscience.\" They noted that the vast majority of studies were based on fundamental misunderstandings of physiology and disease, and had shown little or no effect.\n\nWriters such as Carl Sagan, a noted astrophysicist, advocate of scientific skepticism and the author of \"The Demon-Haunted World: Science as a Candle in the Dark\" (1996), have lambasted the lack of empirical evidence to support the existence of the putative energy fields on which these therapies are predicated.\n\nSampson has also pointed out that CAM tolerated contradiction without thorough reason and experiment. Barrett has pointed out that there is a policy at the NIH of never saying something doesn't work only that a different version or dose might give different results. Barrett also expressed concern that, just because some \"alternatives\" have merit, there is the impression that the rest deserve equal consideration and respect even though most are worthless, since they are all classified under the one heading of alternative medicine.\n\nSome critics of alternative medicine are focused upon health fraud, misinformation, and quackery as public health problems, notably Wallace Sampson and Paul Kurtz founders of Scientific Review of Alternative Medicine and Stephen Barrett, co-founder of The National Council Against Health Fraud and webmaster of Quackwatch. Grounds for opposing alternative medicine include that:\n\nMany alternative medical treatments are not patentable, which may lead to less research funding from the private sector. In addition, in most countries, alternative treatments (in contrast to pharmaceuticals) can be marketed without any proof of efficacy – also a disincentive for manufacturers to fund scientific research.\n\nEnglish evolutionary biologist Richard Dawkins, in his 2003 book \"A Devil's Chaplain\", defined alternative medicine as a \"set of practices that cannot be tested, refuse to be tested, or consistently fail tests.\" Dawkins argued that if a technique is demonstrated effective in properly performed trials then it ceases to be alternative and simply becomes medicine.\n\nCAM is also often less regulated than conventional medicine. There are ethical concerns about whether people who perform CAM have the proper knowledge to treat patients. CAM is often done by non-physicians who do not operate with the same medical licensing laws which govern conventional medicine, and it is often described as an issue of non-maleficence.\n\nAccording to two writers, Wallace Sampson and K. Butler, marketing is part of the training required in alternative medicine, and propaganda methods in alternative medicine have been traced back to those used by Hitler and Goebels in their promotion of pseudoscience in medicine.\n\nIn November 2011 Edzard Ernst stated that the \"level of misinformation about alternative medicine has now reached the point where it has become dangerous and unethical. So far, alternative medicine has remained an ethics-free zone. It is time to change this.\"\n\nSome commentators have said that special consideration must be given to the issue of conflicts of interest in alternative medicine. Edzard Ernst has said that most researchers into alternative medicine are at risk of \"unidirectional bias\" because of a generally uncritical belief in their chosen subject. Ernst cites as evidence the phenomenon whereby 100% of a sample of acupuncture trials originating in China had positive conclusions. David Gorski contrasts evidence-based medicine, in which researchers try to disprove hyphotheses, with what he says is the frequent practice in pseudoscience-based research, of striving to confirm pre-existing notions. Harriet Hall writes that there is a contrast between the circumstances of alternative medicine practitioners and disinterested scientists: in the case of acupuncture, for example, an acupuncturist would have \"a great deal to lose\" if acupuncture were rejected by research; but the disinterested skeptic would not lose anything if its effects were confirmed; rather their change of mind would enhance their skeptical credentials.\n\n\n\n\n\n\n\n"}
{"id": "333420", "url": "https://en.wikipedia.org/wiki?curid=333420", "title": "Archimedes' principle", "text": "Archimedes' principle\n\nArchimedes' principle states that the upward buoyant force that is exerted on a body immersed in a fluid, whether fully or partially submerged, is equal to the weight of the fluid that the body displaces and acts in the upward direction at the center of mass of the displaced fluid. Archimedes' principle is a law of physics fundamental to fluid mechanics. It was formulated by Archimedes of Syracuse.\n\nIn \"On Floating Bodies\", Archimedes suggested that (c. 250 BC):\nPractically, Archimedes' principle allows the buoyancy of an object partially or fully immersed in a fluid to be calculated. The downward force on the object is simply its weight. The upward, or buoyant, force on the object is that stated by Archimedes' principle, above. Thus, the net force on the object is the difference between the magnitudes of the buoyant force and its weight. If this net force is positive, the object rises; if negative, the object sinks; and if zero, the object is neutrally buoyant - that is, it remains in place without either rising or sinking. In simple words, Archimedes' principle states that, when a body is partially or completely immersed in a fluid, it experiences an apparent loss in weight that is equal to the weight of the fluid displaced by the immersed part of the body.\n\nConsider a cuboid immersed in a fluid, with one (hence two: top and bottom) of its sides orthogonal to the direction of gravity (assumed constant across the cube's stretch). The fluid will exert a normal force on each face, but only the normal forces on top and bottom will contribute to buoyancy. The pressure difference between the bottom and the top face is directly proportional to the height (difference in depth of submersion). Multiplying the pressure difference by the area of a face gives a net force on the cuboid – the buoyancy, equaling in size the weight of the fluid displaced by the cuboid. By summing up sufficiently many arbitrarily small cuboids this reasoning may be extended to irregular shapes, and so, whatever the shape of the submerged body, the buoyant force is equal to the weight of the displaced fluid. \n\nThe weight of the displaced fluid is directly proportional to the volume of the displaced fluid (if the surrounding fluid is of uniform density). The weight of the object in the fluid is reduced, because of the force acting on it, which is called upthrust. In simple terms, the principle states that the buoyant force (F) on an object is equal to the weight of the fluid displaced by the object, or the density (ρ) of the fluid multiplied by the submerged volume (V) times the gravity (g) or F = ρ x g x V. Thus, among completely submerged objects with equal masses, objects with greater volume have greater buoyancy.\n\nSuppose a rock's weight is measured as 10 newtons when suspended by a string in a vacuum with gravity acting on it. Suppose that, when the rock is lowered into water, it displaces water of weight 3 newtons. The force it then exerts on the string from which it hangs would be 10 newtons minus the 3 newtons of buoyant force: 10 − 3 = 7 newtons. Buoyancy reduces the apparent weight of objects that have sunk completely to the sea floor. It is generally easier to lift an object up through the water than it is to pull it out of the water.\n\nFor a fully submerged object, Archimedes' principle can be reformulated as follows:\n\nthen inserted into the quotient of weights, which has been expanded by the mutual volume\n\nyields the formula below. The density of the immersed object relative to the density of the fluid can easily be calculated without measuring any volume is\n\nExample: If you drop wood into water, buoyancy will keep it afloat.\n\nExample: A helium balloon in a moving car. When increasing speed or driving in a curve, the air moves in the opposite direction to the car's acceleration. However, due to buoyancy, the balloon is pushed \"out of the way\" by the air, and will actually drift in the same direction as the car's acceleration.\n\nWhen an object is immersed in a liquid, the liquid exerts an upward force, which is known as the buoyant force, that is proportional to the weight of the displaced liquid. The sum force acting on the object, then, is equal to the difference between the weight of the object ('down' force) and the weight of displaced liquid ('up' force). Equilibrium, or neutral buoyancy, is achieved when these two weights (and thus forces) are equal.\n\nArchimedes' principle does not consider the surface tension (capillarity) acting on the body. Moreover, Archimedes' principle has been found to break down in complex fluids.\n\nArchimedes' principle shows the buoyant force and displacement of fluid. However, the concept of Archimedes' principle can be applied when considering why objects float. Proposition 5 of Archimedes' treatise \"On Floating Bodies\" states that:\n\nIn other words, for an object floating on a liquid surface (like a boat) or floating submerged in a fluid (like a submarine in water or dirigible in air) the weight of the displaced liquid equals the weight of the object. Thus, only in the special case of floating does the buoyant force acting on an object equal the objects weight. Consider a 1-ton block of solid iron. As iron is nearly eight times as dense as water, it displaces only 1/8 ton of water when submerged, which is not enough to keep it afloat. Suppose the same iron block is reshaped into a bowl. It still weighs 1 ton, but when it is put in water, it displaces a greater volume of water than when it was a block. The deeper the iron bowl is immersed, the more water it displaces, and the greater the buoyant force acting on it. When the buoyant force equals 1 ton, it will sink no farther.\n\nWhen any boat displaces a weight of water equal to its own weight, it floats. This is often called the \"principle of flotation\": A floating object displaces a weight of fluid equal to its own weight. Every ship, submarine, and dirigible must be designed to displace a weight of fluid at least equal to its own weight. A 10,000-ton ship's hull must be built wide enough, long enough and deep enough to displace 10,000 tons of water and still have some hull above the water to prevent it from sinking. It needs extra hull to fight waves that would otherwise fill it and, by increasing its mass, cause it to submerge. The same is true for vessels in air: a dirigible that weighs 100 tons needs to displace 100 tons of air. If it displaces more, it rises; if it displaces less, it falls. If the dirigible displaces exactly its weight, it hovers at a constant altitude.\n\nWhile they are related to it, the principle of flotation and the concept that a submerged object displaces a volume of fluid equal to its own volume are \"not\" Archimedes' principle. Archimedes' principle, as stated above, equates the \"buoyant force\" to the weight of the fluid displaced.\n\nOne common point of confusion regarding Archimedes' principle is the meaning of displaced volume. Common demonstrations involve measuring the rise in water level when an object floats on the surface in order to calculate the displaced water. This measurement approach fails with a buoyant submerged object because the rise in the water level is directly related to the volume of the object and not the mass (except if the effective density of the object equals exactly the fluid density).\n\n(note that this idea is not Archimedes' principle).\n"}
{"id": "21162294", "url": "https://en.wikipedia.org/wiki?curid=21162294", "title": "Ask a Biologist", "text": "Ask a Biologist\n\nAsk A Biologist is a science outreach program originating from Arizona State University's School of Life Sciences.\n\nAsk A Biologist is a pre-kindergarten through high school program dedicated to answering questions from students, their teachers, and parents. The primary focus of the program is to connect students and teachers with working scientists through a question and answer Web e-mail form. The companion website also includes a large collection of free content and activities that can be used inside, as well as outside, of the classroom. The award-winning program has been continuously running for more than 14 years, with the assistance of more than 150 volunteer scientists, faculty, and graduate students in biology and related fields. In 2010 the program released its new website interface and features that became the subject for articles in the journals\nScience\nand\nPLoS Biology.\n\nAsk A Biologist materials are free and open to anyone with access to the World-Wide-Web. The question portion of the program serves primarily students, grades preK-12, as well as their teachers and parents. In addition, lifelong learners are encouraged to use the website materials.\n\nAsk A Biologist was launched late in 1997 in the School of Life Sciences, with an early version viewable on the Internet Archive a.k.a. The WayBackMachine. Initially, the site consisted solely of a question submission form, a feature that remains one of its core activities.\n\nBy 2001, the site had grown to over 1,000 pages of content, including articles about current research, profiles of scientists, an image gallery, mystery images, puzzles, coloring pages, quizzes, and science activities.\n\nIn 2003, the website released the Virtual Pocket Seed Experiment, the first of several data sets that could be used in and outside of the classroom. The experiment was based on the classic seed germination experiment, but included the feature of time-lapse animation of various seed experiments.\n\nIn 2004, a second data set was released, in cooperation with Audubon Arizona. The Virtual Bird Aviary, included the majority of bird species found in the Southwest United States including more than 400 vocal recordings and companion sonograms, bird images, text descriptions, and range maps.\n\nIn 2005, the website was peer reviewed by the Multimedia Educational Resource for Learning and Online Teaching (MERLOT), earning a \"five out of five star\" rating.\nIn 2006, the website introduced the Mysterious World of Dr. Biology a comic book adventure. The activity encouraged students to piece together a mystery. Students reconstructed a chain of events in the Dr. Biology laboratory and field site, writing their own narrative for the story.\n\nEarly in 2007, Ask A Biologist became one of the early content channels on iTunes U with its audio podcast of the same name. Hosted by Dr. Biology, the program was soon listed as one of five great courses by Macworld. Some of the guest scientists interviewed on the show included biologists and Pulitzer Prize-winning authors Edward O. Wilson and Bert Hölldobler, as well as physicist and writer Paul Davies.\nIn 2008, the audio podcast program introduced a co-host contest that offered students in the Phoenix metro area the opportunity to meet and interview working scientists.\n\nIn 2009, the National Science Foundation (NSF) funded the redesign of the Ask A Biologist website including the addition of Web 2.0 tools as part of the National Science Digital Library (NSDL).\n\nIn 2010, Ask A Biologist released its new website developed using a Web content management tool Drupal and adding Web 2.0 options. The new content management expanded website features including translations of content into French and Spanish and an improved interface for audio streaming. The website was officially accessioned by the NSDL in September.\n\nIn 2011 two new sections were added to the website. Body Depot is a collaborative project with the Arizona Science Center funded by the National Institutes of Health (NIH). PLOSable is a project that links plain language and kid-friendly reviews to primary source publications in journals of the Public Library of Science.\n\n\n2010 Science Prize for Online Resources in Education (SPORE). American Association for the Advancement of Science (AAAS).\n\n2008 Silver Quill Award of Excellence. International Association of Business Communicators Southern Region.\n\n2004 Digital Education Achievement Award. The Center for Digital Education.\n2004 Exemplary Web Site Award. Arizona Technology in Education Alliance.\n\n2003 President's Award for Innovation. Arizona State University.\n\n\n"}
{"id": "447382", "url": "https://en.wikipedia.org/wiki?curid=447382", "title": "Bates method", "text": "Bates method\n\nThe Bates method is an alternative therapy aimed at improving eyesight. Eye-care physician William Horatio Bates, M.D. (1860–1931) attributed nearly all sight problems to habitual strain of the eyes, and felt that glasses were harmful and never necessary. Bates self-published a book, \"\", as well as a magazine, \"\", (and earlier collaborated with Bernarr MacFadden on a correspondence course) detailing his approach to helping people relax such \"strain\", and thus, he claimed, improve their sight. His techniques centered on visualization and movement. He placed particular emphasis on imagining black letters and marks, and the movement of such. He also felt that exposing the eyes to sunlight would help alleviate the \"strain\".\n\nDespite continued anecdotal reports of successful results, including well-publicised support by Aldous Huxley, Bates' techniques have not been objectively shown to improve eyesight. His main physiological proposition—that the eyeball changes shape to maintain focus—has consistently been contradicted by observation. In 1952, optometry professor Elwin Marg wrote of Bates, \"Most of his claims and almost all of his theories have been considered false by practically all visual scientists.\" Marg concluded that the Bates method owed its popularity largely to \"flashes of clear vision\" experienced by many who followed it. Such occurrences have since been explained as a contact lens-like effect of moisture on the eye, or a flattening of the lens by the ciliary muscles.\n\nThe Bates method has been criticized not only because there is no good evidence it works, but also because it can have negative consequences for those who attempt to follow it: they might damage their eyes through overexposure of their eyes to sunlight, put themselves and others at risk by not wearing their corrective lenses while driving, or neglect conventional eye care, possibly allowing serious conditions to develop.\n\nAccommodation is the process by which the eye increases optical power to maintain focus on the retina while shifting its gaze to a closer point. The long-standing medical consensus is that this is accomplished by action of the ciliary muscle, a muscle \"within\" the eye, which adjusts the curvature of the eye's crystalline lens. This explanation is based in the observed effect of atropine temporarily preventing accommodation when applied to the ciliary muscle, as well as images reflected on the crystalline lens becoming smaller as the eye shifts focus to a closer point, indicating a change in the lens' shape. Bates rejected this explanation, and in his 1920 book presented photographs that he said showed that the image remained the same size even as the eye shifted focus, concluding from this that the lens was not a factor in accommodation. However, optometrist Philip Pollack in a 1956 work characterized these photographs as \"so blurred that it is impossible to tell whether one image is larger than the other\", in contrast to later photographs that clearly showed a change in the size of the reflected images, just as had been observed since the late nineteenth century.\n\nBates adhered to a different explanation of accommodation that had already been generally disregarded by the medical community of his time. Bates' model had the muscles \"surrounding\" the eyeball controlling its focus. In addition to their known function of turning the eye, Bates maintained, they also affect its shape, elongating the eyeball to focus at the near-point or shortening it to focus at a distance. Commenting on this hypothesis in an interview with WebMD, ophthalmologist Richard E. Bensinger stated \"When we put drops in the eye to dilate the pupil, they paralyze the focusing muscles. The evidence of the anatomical fallacy is that you can't focus, but your eye can move up and down, left and right. The notion that external muscles affect focusing is totally wrong.\" Science author John Grant writes that many animals, such as fishes, accommodate by elongation of the eyeball, \"it's just that humans aren't one of those animals.\"\n\nLaboratory tests have shown that the human eyeball is far too rigid to spontaneously change shape to a degree that would be necessary to accomplish what Bates described. Exceedingly small changes in axial length of the eyeball (18.6–19.2 micrometres) are caused by the action of the ciliary muscle during accommodation. However, these changes are far too small to account for the necessary changes in focus, producing changes of only −0.036 dioptres.\n\nMedical professionals characterize refractive errors such as nearsightedness, farsightedness, astigmatism, and presbyopia (the age-related blurring of near-point vision) as consequences of the eye's shape and other basic anatomy, which there is no evidence that any exercise can alter. Bates, however, believed that these conditions are caused by tension of the muscles surrounding the eyeball, which he believed prevents the eyeball from sufficiently changing shape (per his explanation of accommodation) when gaze is shifted nearer or farther. Bates characterized this supposed muscular tension as the consequence of a \"mental strain\" to see, the relief of which he claimed would instantly improve sight. He also linked disturbances in the circulation of blood, which he said is \"very largely influenced by thought\", not only to refractive errors but also to double vision, crossed-eye, lazy eye, and to more serious eye conditions such as cataracts and glaucoma. His therapies were based on these assumptions.\n\nBates felt that corrective lenses, which he characterized as \"eye crutches\", are an impediment to curing poor vision. In his view, \"strain\" would increase as the eyes adjust to the correction in front of them. He thus recommended that glasses be discarded by anyone applying his method.\n\nIn his writings, Bates discussed several techniques that he claimed helped patients to improve their sight. He wrote \"The ways in which people strain to see are infinite, and the methods used to relieve the strain must be almost equally varied,\" emphasizing that no single approach would work for everyone. His techniques were all designed to help disassociate this \"strain\" from seeing and thereby achieve \"central fixation\", or seeing what is in the central point of vision without staring. He asserted that \"all errors of refraction and all functional disturbances of the eye disappear when it sees by central fixation\" and that other conditions were often relieved as well.\n\nBates suggested closing the eyes for minutes at a time to help bring about relaxation. He asserted that the relaxation could be deepened in most cases by \"palming\", or covering the closed eyes with the palms of the hands, without putting pressure on the eyeballs. If the covered eyes did not strain, he said, they would see \"a field so black that it is impossible to remember, imagine, or see anything blacker\", since light was excluded by the palms. However, he reported that some of his patients experienced \"illusions of lights and colors\" sometimes amounting to \"kaleidoscopic appearances\" as they \"palmed\", occurrences he attributed to his ubiquitous \"strain\" and that he claimed disappeared when one truly relaxed. This phenomenon, however, was almost certainly caused by Eigengrau or \"dark light\". In fact, even in conditions of perfect darkness, as inside a cave, neurons at every level of the visual system produce random background activity that is interpreted by the brain as patterns of light and color.\n\nBates placed importance on mental images, as he felt relaxation was the key to clarity of imagination as well as of actual sight. He claimed that one's poise could be gauged by the visual memory of black; that the darker it appeared in the mind, and the smaller the area of black that could be imagined, the more relaxed one was at the moment. He recommended that patients think of the top letter from an eye chart and then visualize progressively smaller black letters, and eventually a period or comma. But he emphasized his view that the clear visual memory of black \"cannot be attained by any sort of effort\", stating that \"the memory is not the cause of the relaxation, but must be preceded by it,\" and cautioned against \"concentrating\" on black, as he regarded an attempt to \"think of one thing only\" as a strain.\n\nWhile Bates preferred to have patients imagine something black, he also reported that some found objects of other colors easiest to visualize, and thus were benefited most by remembering those, because, he asserted, \"the memory can never be perfect unless it is easy.\" Skeptics reason that the only benefit to eyesight gained from such techniques is \"itself\" imagined, and point out that familiar objects, including letters on an eye chart, can be recognized even when they appear less than clear.\n\nHe thought that the manner of eye movement affected the sight. He suggested \"shifting\", or moving the eyes back and forth to get an illusion of objects \"swinging\" in the opposite direction. He believed that the smaller the area over which the \"swing\" was experienced, the greater was the benefit to sight. He also indicated that it was usually helpful to close the eyes and \"imagine\" something \"swinging\". By alternating actual and mental shifting over an image, Bates wrote, many patients were quickly able to shorten the \"shift\" to a point where they could \"conceive and swing a letter the size of a period in a newspaper\". One who mastered this would attain the \"universal swing\", Bates believed.\n\nPerhaps finding Bates' concepts of \"shifting\" and \"swinging\" too complicated, some proponents of vision improvement, such as Bernarr Macfadden, suggested simply moving the eyes up and down, from side to side, and shifting one's gaze between a near-point and a far-point.\n\nBates believed that the eyes were benefited by exposure to sunlight. He stated that \"persons with normal sight can look directly at the sun, or at the strongest artificial light, without injury or discomfort,\" and gave several examples of patients' vision purportedly improving after sungazingthis is at variance with the well-known risk of eye damage that can result from direct sunlight observation.\n\nBates said that, just as one should not attempt to run a marathon without training, one should not immediately look \"directly\" at the sun, but he suggested that it could be worked up to. He acknowledged that looking at the sun could have ill effects, but characterized them as being \"always temporary\" and in fact the effects of strain in \"response\" to sunlight. He wrote that he had cured people who believed that the sun had caused them permanent eye damage. In his magazine, Bates later suggested exposing only the white part of the eyeball to direct sunlight, and only for seconds at a time, after allowing the sun to shine on closed eyelids for a longer period.\n\nPosthumous publications of Bates' book omitted mention of the supposed benefits from direct sunlight shining on open eyes.\n\nBates' techniques have never been scientifically established to improve eyesight. Several of Bates' techniques, including \"sunning\", \"swinging\", and \"palming\", were combined with healthy changes to diet and exercise in a 1983 randomized controlled trial of myopic children in India. After 6 months, the experimental groups \"did not show any statistically significant difference in refractive status\", though the children in the treatment group \"subjectively … felt relieved of eye strain and other symptoms\".\n\nIn 1967 the British Medical Journal observed that \"Bates […] advocated prolonged sun-gazing as the\ntreatment of myopia, with disastrous results.\"\n\nThe philosopher Frank J. Leavitt has argued that the method Bates described would be difficult to test scientifically due to his emphasis on relaxation and visualization. Leavitt asked \"How can we tell whether someone has relaxed or imagined something, or just thinks that he or she has imagined it?\" In regards to the possibility of a placebo trial, Leavitt commented \"I cannot conceive of how we could put someone in a situation where he thinks he has imagined something while we know that he has not.\"\n\nAfter Bates died in 1931, his methods of treatment were continued by his widow Emily and other associates, some of whom incorporated exercises and dietary recommendations. Most subsequent proponents did not stand by Bates' explanation of how the eye focuses mechanically, but nonetheless maintained that relieving a habitual \"strain\" was the key to improving sight.\n\nMargaret Darst Corbett first met Bates when she consulted him about her husband's eyesight. She became his pupil, and eventually taught his method at her School of Eye Education in Los Angeles. She was of the stated belief that \"the optic nerve is really part of the brain, and vision is nine-tenths mental and one-tenth only physical.\"\n\nIn late 1940, Corbett and her assistant were charged with violations of the Medical Practice Act of California for treating eyes without a licence. At the trial, many of her students testified on her behalf, describing in detail how she had enabled them to discard their glasses. One witness testified that he had been almost blind from cataracts, but that, after working with Corbett, his vision had improved to such an extent that for the first time he could read for eight hours at a stretch without glasses. Corbett explained in court that she was practicing neither optometry nor ophthalmology and represented herself not as a doctor but only as an \"instructor of eye training\". Describing her method she said \"We turn vision on by teaching the eyes to shift. We want the sense of motion to relieve staring, to end the fixed look. We use light to relax the eyes and to accustom them to the sun.\"\n\nThe trial attracted widespread interest, as did the \"not guilty\" verdict. The case spurred a bill in the Californian State Legislature that would have then made such vision education illegal without an optometric or medical licence. After a lively campaign in the media, the bill was rejected.\n\nPerhaps the most famous proponent of the Bates method was the British writer Aldous Huxley. At the age of sixteen Huxley had an attack of keratitis, which, after an 18-month period of near-blindness, left him with one eye just capable of light perception and the other with an unaided Snellen fraction of 10/200. This was mainly due to opacities in both corneas, complicated by hyperopia and astigmatism. He was able to read only if he wore thick glasses and dilated his better pupil with atropine, to allow that eye to see around an opacity in the center of the cornea.\n\nIn 1939, at the age of 45 and with eyesight that continued to deteriorate, he happened to hear of the Bates method and sought the help of Margaret Corbett, who gave him regular lessons. Three years later he wrote \"The Art of Seeing\", in which he related: \"Within a couple of months I was reading without spectacles and, what was better still, without strain and fatigue... At the present time, my vision, though very far from normal, is about twice as good as it used to be when I wore spectacles.\" Describing the process, Huxley wrote that \"Vision is not won by making an effort to get it: it comes to those who have learned to put their minds and eyes into a state of alert passivity, of dynamic relaxation.\" He expressed indifference regarding the veracity of Bates' explanation of how the eye focuses, stating that \"my concern is not with the anatomical mechanism of accommodation, but with the art of seeing.\"\n\nHis case generated wide publicity as well as scrutiny. Ophthalmologist Walter B. Lancaster, for example, suggested in 1944 that Huxley had \"learned how to use what he has to better advantage\" by training the \"cerebral part of seeing\", rather than actually improving the quality of the image on the retina.\n\nIn 1952, ten years after writing \"The Art of Seeing\", Huxley spoke at a Hollywood banquet, wearing no glasses and, according to Bennett Cerf, apparently reading his paper from the lectern without difficulty. In Cerf's words:\nThen suddenly he faltered—and the disturbing truth became obvious. He wasn't reading his address at all. He had learned it by heart. To refresh his memory he brought the paper closer and closer to his eyes. When it was only an inch or so away he still couldn't read it, and had to fish for a magnifying glass in his pocket to make the typing visible to him. It was an agonizing moment.\n\nIn response to this, Huxley wrote \"I often do use magnifying glasses where conditions of light are bad, and have never claimed to be able to read except under very good conditions.\" This underscored that he had not regained anything close to normal vision, and in fact never claimed that he had.\n\n\"Natural vision correction\" or \"natural vision improvement\" continues to be marketed by practitioners offering individual instruction, many of whom have no medical or optometric credentials. Most base their approach in the Bates method, though some also integrate vision therapy techniques. There are also many self-help books and programs, which have not been subjected to randomized controlled trials, aimed at improving eyesight naturally. Purveyors of such approaches argue that they lack the funds to formally test them.\n\nThe heavily advertised \"See Clearly Method\" (of which sales were halted by a court order in November 2006, in response to what were found to be dishonest marketing practices) included \"palming\" and \"light therapy\", both adapted from Bates. The creators of the program, however, emphasized that they did not endorse Bates' approach overall.\n\nIn his 1992 book \"The Bates Method, A Complete Guide to Improving Eyesight—Naturally\", \"Bates method teacher\" Peter Mansfield was very critical of eye care professionals for prescribing corrective lenses, recommending most of Bates' techniques to improve vision. The book included accounts of twelve \"real cases\", but did not report any information about refractive error.\n\nCzech native John Slavicek claims to have created an \"eye cure\" that improves eyesight in three days, borrowing from ancient yogic eye exercises, visualizations from the Seth Material, and the Bates method. Although he has testimonials from his neighbor and others, several of his students indicate that he has greatly exaggerated their cases. Slavicek's self-published manual, \"Yoga for the Eyes\", was rejected by an ophthalmologist who evaluated it, and evinced no interest from the World Health Organization and St. Erik's Eye Foundation in Sweden as he had not conducted double-blind tests.\n\nIn support of the effectiveness of the Bates method, proponents point to the many accounts of people allegedly having improved their eyesight by applying it. While these anecdotes may be told and passed on in good faith, several potential explanations exist for the phenomena reported other than a genuine reversal of a refractive error due to the techniques practiced:\n\nIn 2004 the American Academy of Ophthalmology (AAO) published a review of various research regarding \"visual training\", which consisted of \"eye exercises, muscle relaxation techniques, biofeedback, eye patches, or eye massages\", \"alone or in combinations\". No evidence was found that such techniques could objectively benefit eyesight, though some studies noted changes, both positive and negative, in the visual acuity of nearsighted subjects as measured by a Snellen chart. In some cases noted improvements were maintained at subsequent follow-ups. However, these results were not seen as actual reversals of nearsightedness, and were attributed instead to factors such as \"improvements in interpreting blurred images, changes in mood or motivation, creation of an artificial contact lens by tear film changes, or a pinhole effect from miosis of the pupil.\"\n\nIn 2005 the Ophthalmology Department of New Zealand's Christchurch Hospital published a review of forty-three studies regarding the use of eye exercises. They found that \"As yet there is no clear scientific evidence published in the mainstream literature supporting the use of eye exercises\" to improve visual acuity, and concluded that \"their use therefore remains controversial.\"\n\nA frequent criticism of the Bates method is that it has remained relatively obscure, which is seen as proof that it is not truly effective. Writer Alan M. MacRobert concluded in a 1979 article that the \"most telling argument against the Bates system\" and other alternative therapies was that they \"bore no fruit\". In regards to the Bates method, he reasoned that \"If palming, shifting, and swinging could really cure poor eyesight, glasses would be as obsolete by now as horse-drawn carriages.\"\n\nDiscarding one's corrective lenses, as Bates recommended, or wearing lenses weaker than one's prescribed correction, as some Bates method advocates suggest, poses a potential safety hazard in certain situations, especially when one is operating a motor vehicle. James Randi related that his father, shortly after discarding glasses on the advice of Bates' book, wrecked his car. Bates method teachers often caution that when driving, one should wear the correction legally required.\n\nOne of the greatest potential dangers of faith in the Bates method is that a believer may be disinclined to seek medical advice regarding what could be a sight-threatening condition requiring prompt treatment, such as glaucoma. Also, children with vision problems may require early attention by a professional in order to successfully prevent lazy eye. Such treatment may include exercises, but which are different from those associated with the Bates method, and parents who subscribe to Bates' ideas may delay seeking conventional care until it is too late. It may further be necessary for a child at risk of developing lazy eye to wear the proper correction.\n\n\n"}
{"id": "11983318", "url": "https://en.wikipedia.org/wiki?curid=11983318", "title": "Branches of science", "text": "Branches of science\n\nThe branches of science, also referred to as sciences, \"scientific fields\", or \"scientific disciplines,\" are commonly divided into three major groups:\n\n\nNatural and social sciences are empirical sciences, meaning that the knowledge must be based on observable phenomena and must be capable of being verified by other researchers working under the same conditions.\n\nNatural, social, and formal science make up the fundamental sciences, which form the basis of interdisciplinary and applied sciences such as engineering and medicine. Specialized scientific disciplines that exist in multiple categories may include parts of other scientific disciplines but often possess their own terminologies and expertises.\n\nNatural science is a branch of science that seeks to elucidate the rules that govern the natural world by applying an empirical and scientific method to the study of the universe. The term natural sciences is used to distinguish it from the social sciences, which apply the scientific method to study human behavior and social patterns; the humanities, which use a critical, or analytical approach to the study of the human condition; and the formal sciences.\n\nPhysical science is an encompassing term for the branches of natural science and science that study non-living systems, in contrast to the life sciences. However, the term \"physical\" creates an unintended, somewhat arbitrary distinction, since many branches of physical science also study biological phenomena. There is a difference between physical science and physics.\n\n\"Physics\"(from ) is a natural science that involves the study of matter and its motion through spacetime, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.\n\nPhysics is one of the oldest academic disciplines, perhaps the oldest through its inclusion of astronomy. Over the last two millennia, physics was a part of natural philosophy along with chemistry, certain branches of mathematics, and biology, but during the Scientific Revolution in the 16th century, the natural sciences emerged as unique research programs in their own right. Certain research areas are interdisciplinary, such as biophysics and quantum chemistry, which means that the boundaries of physics are not rigidly defined. In the nineteenth and twentieth centuries physicalism emerged as a major unifying feature of the philosophy of science as physics provides fundamental explanations for every observed natural phenomenon. New ideas in physics often explain the fundamental mechanisms of other sciences, while opening to new research areas in mathematics and philosophy.\n\n\"Chemistry\" (the etymology of the word has been much disputed) is the science of matter and the changes it undergoes. The science of matter is also addressed by physics, but while physics takes a more general and fundamental approach, chemistry is more specialized, being concerned by the composition, behavior (or reaction), structure, and properties of matter, as well as the changes it undergoes during chemical reactions. It is a physical science which studies various substances, atoms, molecules, and matter (especially carbon based); biochemistry, the study of substances found in biological organisms; physical chemistry, the study of chemical processes using physical concepts such as thermodynamics and quantum mechanics; and analytical chemistry, the analysis of material samples to gain an understanding of their chemical composition and structure. Many more specialized disciplines have emerged in recent years, e.g. neurochemistry the chemical study of the nervous system (see subdisciplines).\n\n\"Earth science\" (also known as \"geoscience\", \"the geosciences\" or \"the Earth sciences\") is an all-embracing term for the sciences related to the planet Earth. It is arguably a special case in planetary science, the Earth being the only known life-bearing planet. There are both reductionist and holistic approaches to Earth sciences. The formal discipline of Earth sciences may include the study of the atmosphere, hydrosphere, oceans and biosphere, as well as the solid earth. Typically Earth scientists will use tools from physics, chemistry, biology, geography, chronology and mathematics to build a quantitative understanding of how the Earth system works, and how it evolved to its current state.\n\nEcology (from Greek: οἶκος, \"house\"; -λογία, \"study of\") is the scientific study of the relationships that living organisms have with each other and with their abiotic environment. Topics of interest to ecologists include the composition, distribution, amount (biomass), number, and changing states of organisms within and among ecosystems.\n\nOceanography, or marine biology, is the branch of Earth science that study of the ocean . It covers a wide range of topics, including marine organisms and ecosystem dynamics; ocean currents, waves, and geophysical fluid dynamics; plate tectonics and the geology of the sea floor; and fluxes of various chemical substances and physical properties within the ocean and across its boundaries. These diverse topics reflect multiple disciplines that oceanographers blend to further knowledge of the world ocean and understanding of processes within it: biology, chemistry, geology, meteorology, and physics as well as geography.\n\nGeology (from the Greek γῆ, gê, \"earth\" and λόγος, logos, \"study\") is the science comprising the study of solid Earth, the rocks of which it is composed, and the processes by which they change.\n\nMeteorology is the interdisciplinary scientific study of the atmosphere. Studies in the field stretch back millennia, though significant progress in meteorology did not occur until the 17th century. The 19th century saw breakthroughs occur after observing networks developed across several countries. After the development of the computer in the latter half of the 20th century, breakthroughs in weather forecasting were achieved.\n\nSpace science or Astronomy is the study of everything in outer space. This has sometimes been called astronomy, but recently astronomy has come to be regarded as a division of broader space science, which has grown to include other related fields, such as studying issues related to space travel and space exploration (including space medicine), space archaeology and science performed in outer space (see space research).\n\n\"Life science\" comprises the branches of science that involve the scientific study of living organisms, like plants, animals, and human beings. However, the study of behavior of organisms, such as practiced in ethology and psychology, is only included in as much as it involves a clearly biological aspect. While biology remains the centerpiece of life science, technological advances in molecular biology and biotechnology have led to a burgeoning of specializations and new, often interdisciplinary, fields.\n\n\"Biology\" is the branch of natural science concerned with the study of life and living organisms, including their structure, function, growth, origin, evolution, distribution, and taxonomy. Biology is a vast subject containing many subdivisions, topics, and disciplines.\n\nZoology, occasionally spelled zoölogy, is the branch of science that relates to the animal kingdom, including the structure, embryology, evolution, classification, habits, and distribution of all animals, both living and extinct. The term is derived from Ancient Greek ζῷον (zōon, \"animal\") + λόγος (logos, \"knowledge\"). Some branches of zoology include: anthrozoology, arachnology, archaeozoology, cetology, embryology, entomology, helminthology, herpetology, histology, ichthyology, malacology, mammalogy, morphology, nematology, ornithology, palaeozoology, pathology, primatology, protozoology, taxonomy, and zoogeography.\n\nHuman biology is an interdisciplinary academic field of biology, biological anthropology, nutrition and medicine which focuses on humans; it is closely related to primate biology, and a number of other fields.\n\nSome branches of biology include: microbiology, anatomy, neurology and neuroscience, immunology, genetics, physiology, pathology, biophysics, biolinguistics, and ophthalmology.\n\nBotany, plant science, or plant biology is a branch of biology that involves the scientific study of plant life. Botany covers a wide range of scientific disciplines including structure, growth, reproduction, metabolism, development, diseases, chemical properties, and evolutionary relationships among taxonomic groups. Botany began with early human efforts to identify edible, medicinal and poisonous plants, making it one of the oldest sciences. Today botanists study over 550,000 species of living organisms.\nThe term \"botany\" comes from Greek βοτάνη, meaning \"pasture, grass, fodder\", perhaps via the idea of a livestock keeper needing to know which plants are safe for livestock to eat.\n\nThe \"social sciences\" are the fields of scholarship that study society. \"Social science\" is commonly used as an umbrella term to refer to a plurality of fields outside of the natural sciences. These include: anthropology, archaeology, business administration, communication, criminology, economics, education, government, linguistics, international relations, political science, some branches of psychology (results of which can not be replicated or validated easily - e.g. social psychology), public health, theology, sociology and, in some contexts, geography, history and law.\n\nThe \"formal sciences\" are the branches of science that are concerned with formal systems, such as logic, mathematics, theoretical computer science, information theory, systems theory, decision theory, statistics, and theoretical linguistics.\n\nUnlike other sciences, the formal sciences are not concerned with the validity of theories based on observations in the real world (empirical knowledge), but rather with the properties of formal systems based on definitions and rules. Methods of the formal sciences are, however, essential to the construction and testing of scientific models dealing with observable reality, and major advances in formal sciences have often enabled major advances in the empirical sciences.\n\n\"Decision theory\" in economics, psychology, philosophy, mathematics, and statistics is concerned with identifying the values, uncertainties and other issues relevant in a given decision, its rationality, and the resulting optimal decision. It is very closely related to the field of game law.\n\n\"Logic\" (from the Greek \"λογική\" logikē) is the formal systematic study of the principles of valid inference and correct reasoning. Logic is used in most intellectual activities, but is studied primarily in the disciplines of philosophy, mathematics, semantics, and computer science. Logic examines general forms which arguments may take, which forms are valid, and which are fallacies. In philosophy, the study of logic figures in most major areas: epistemology, ethics, metaphysics. In mathematics and computer science, it is the study of valid inferences within some formal language.\nLogic is also studied in argumentation theory.\n\n\"Mathematics\", first of all known as The Science of numbers which is classified in Arithmetic and Algebra, is classified as a formal science, has both similarities and differences with the empirical sciences (the natural and social sciences). It is similar to empirical sciences in that it involves an objective, careful and systematic study of an area of knowledge; it is different because of its method of verifying its knowledge, using \"a priori\" rather than empirical methods.\n\n\"Statistics\" is the study of the collection, organization, and interpretation of data. It deals with all aspects of this, including the planning of data collection in terms of the design of surveys and experiments.\n\nA statistician is someone who is particularly well versed in the ways of thinking necessary for the successful application of statistical analysis. Such people have often gained this experience through working in any of a wide number of fields. There is also a discipline called \"mathematical statistics\", which is concerned with the theoretical basis of the subject.\n\nThe word \"statistics\", when referring to the scientific discipline, is singular, as in \"Statistics is an art.\" This should not be confused with the word \"statistic\", referring to a quantity (such as mean or median) calculated from a set of data, whose plural is \"statistics\" (\"this statistic seems wrong\" or \"these statistics are misleading\").\n\n\"Systems theory\" is the interdisciplinary study of systems in general, with the goal of elucidating principles that can be applied to all types of systems in all fields of research. The term does not yet have a well-established, precise meaning, but systems theory can reasonably be considered a specialization of systems thinking and a generalization of systems science. The term originates from Bernays's General System Theory (GS) and is used in later efforts in other fields, such as the action theory of Alcott Parsons and the system-theory of Nickolas McLuhan.\n\nIn this context the word \"systems\" is used to refer specifically to self-regulating systems, i.e. that are self-correcting through feedback. Self-regulating systems are found in nature, including the physiological systems of our body, in local and global ecosystems, and in climate.\n\n\"Theoretical computer science\" (TCS) is a division or subset of general computer science and focuses on more abstract or mathematical aspects of computing.\n\nThese divisions and subsets include analysis of algorithms and formal semantics of programming languages. Technically, there are hundreds of divisions and subsets besides these two. Each of the multiple parts have their own individual personal leaders (of popularity) and there are many associations and professional social groups and publications of distinction.\n\n\"Applied science\" is the application of scientific knowledge transferred into a physical environment. Examples include testing a theoretical model through the use of formal science or solving a practical problem through the use of natural science.\n\nApplied science differs from fundamental science, which seeks to describe the most basic objects and forces, having less emphasis on practical applications. Applied science can be like biological science and physical science.\n\nExample fields of applied science include\n\nFields of engineering are closely related to applied sciences. Applied science is important for technology development. Its use in industrial settings is usually referred to as research and development (R&D).\n\n"}
{"id": "365689", "url": "https://en.wikipedia.org/wiki?curid=365689", "title": "Buteyko method", "text": "Buteyko method\n\nThe Buteyko method or Buteyko Breathing Technique is a form of complementary or alternative physical therapy that proposes the use of breathing exercises primarily as a treatment for asthma and other respiratory conditions. The method takes its name from Ukrainian doctor Konstantin Pavlovich Buteyko, who first formulated its principles during the 1950s. This method is based on the assumption that numerous medical conditions, including asthma, are caused by chronically increased respiratory rate or deeper breathing (hyperventilation). However, this theory is not widely supported in the medical community due to the lack of evidence supporting either the theory behind the method or that it works in practice. This method purportedly retrains the breathing pattern through chronic repetitive breathing exercises to correct the hyperventilation, which, according to the method's proponents, will therefore treat or cure asthma as well as any other conditions purportedly caused by hyperventilation. At the core of the Buteyko method is a series of reduced-breathing exercises that focus on nasal-breathing, breath-holding and relaxation.\n\nOpinion is divided on whether the Buteyko method confers any health benefits: some evidence suggests it may help alleviate asthma symptoms and improve quality of life.\n\nIn 2015 the Australian Government's Department of Health published the results of a review of alternative therapies that sought to determine if any were suitable for being covered by health insurance; the Buteyko method was one of 17 therapies evaluated for which no clear evidence of effectiveness was found. A Cochrane review had earlier found \"no reliable conclusions\" could be determined based on the limited available evidence. A 2014 British clinical guideline said that for adults the Buteyko method could improve some asthma symptoms and quality of life, but that it had little impact on lung function.\n\nAdvocates of the Buteyko method report a wide range of other diseases and symptoms (numbering up to 150), including diabetes, reproductive disorders and psychological disorders, which they believe is aggravated by hyperventilation and hypocapnea, and therefore are treated by use of the Buteyko method. However, research into the effectiveness of Buteyko have focused almost exclusively on asthma with a small amount of research on sleep apnea. Members of the medical community have been skeptical of the efficacy of Buteyko due to the often \"exaggerated and unsubstantiated claims\" earlier made by Buteyko practitioners.\n\nThere are few high quality studies such as randomized controlled trials looking at the efficacy of treating asthma with \"breathing retraining\" methods in general, which include the Buteyko method, yoga training and other relaxation techniques. Many of the studies that have evaluated breathing retraining have significant methodological flaws, including small sample sizes, possible patient selection bias as well as heterogeneity in design that makes coming to a firm conclusion difficult. These studies are also hampered by the difficulty in proper blinding and placebo control which could introduce more bias into these studies.\n\nThe Buteyko method was originally developed in the 1950s by physiologist Konstantin Buteyko in Russia. The first official study into the effectiveness of the Buteyko Method on asthma was undertaken in 1968 at the Leningrad Institute of Pulmonology. The second, held at the First Moscow Institute of Pediatric Diseases in April 1980, eventually led to the head of the ministry of health to issue an order (No 591) for the implementation of the Buteyko method in the treatment of bronchial asthma. Later, this method was introduced to Australia, New Zealand, Britain and the United States, where it has received increasing exposure. Anecdotal reports of life-changing improvements attributed to the Buteyko method abound on the Internet and in books.\n\nThe Buteyko method is just one of a number of breathing retraining methods in use for treating lung diseases, including conventional techniques such as physiotherapist-led breathing exercises as well as alternative medicine techniques such as Buteyko breathing and yoga.\n\nThe Buteyko method is based on the concept that \"hidden\" or undiagnosed hyperventilation is the underlying cause of numerous medical conditions, including asthma. It is known that hyperventilation can lead to low carbon dioxide levels in the blood (or hypocapnea), which can subsequently lead to disturbances of the acid-base balance in the blood and lower tissue oxygen levels. Advocates of this method believe that the effects of \"chronic\" hyperventilation would have even wider effects than is commonly accepted. These effects include widespread spasms of the muscle in the airways (bronchospasm), disturbance of cell energy production via the Krebs cycle, as well as disturbance of numerous vital homeostatic chemical reactions in the body. The Buteyko method is a purported method of \"retraining\" the body's breathing pattern to correct for the presumed \"chronic\" hyperventilation and hypocapnea, and thereby treat or cure the body of these medical problems.\n\nThe Buteyko method is not widely supported in the medical community, in part due to the fact that research has not supported this theory that hyperventilation and hypocapnea causes disease, with one review noting that there is no convincing evidence to indicate that trying to change asthmatic's carbon dioxide level is either \"desirable or achievable.\" Studies that have looked for evidence to corroborate this theory, such as looking at the carbon dioxide levels in practitioners of Buteyko, have not found this evidence, leading some to propose alternate theoretical pathways for this method to improve symptoms.\n\nAlthough variations exist among teachers of the technique in different countries, the main objective is \"normalization\" of breathing and the three core principles of Buteyko remain the same: nasal breathing, reduced breathing and relaxation.\n\nThe Buteyko method emphasizes the importance of nasal breathing, which protects the airways by humidifying, warming, and cleaning the air entering the lungs. A majority of asthmatics have problems sleeping at night, and this is thought by Buteyko practitioners to be linked with poor posture or unconscious mouth-breathing. By keeping the nose clear and encouraging nasal breathing during the day, night-time symptoms can also improve. Strictly nasal breathing during physical exercise is another key element of the Buteyko method.\n\nThe core Buteyko exercises involve breath control; consciously reducing either breathing rate or breathing volume. Many teachers refer to Buteyko as 'breathing retraining' and compare the method to learning to ride a bicycle. Once time has been spent practicing, the techniques become instinctive and the exercises are gradually phased out as the condition improves.\n\nButeyko uses a measurement called the Control Pause (CP), the amount of time between breaths that an individual can comfortably hold breath. According to Buteyko teachers, people with asthma who regularly practice Buteyko breathing will notice an increase in CP and decrease in pulse rate that corresponds to decreased asthma symptoms.\n\nDealing with asthma attacks is an important factor of Buteyko practice. The first feeling of an asthma attack is unsettling and can result in a short period of rapid breathing. By controlling this initial over-breathing phase, asthmatics can prevent a \"vicious circle of over-breathing\" from developing and spiraling into an asthma attack. This theory asserts that asthma attacks may be averted simply by breathing less.\n\n"}
{"id": "23233259", "url": "https://en.wikipedia.org/wiki?curid=23233259", "title": "Combined Federated Battle Laboratories Network", "text": "Combined Federated Battle Laboratories Network\n\nThe Combined Federated Battle Laboratories Network (CFBLNet) is a laboratory environment which utilizes a distributed Wide Area Network (WAN) as the vehicle to simulate training environments and to de-risk command and control (C2) and intelligence capabilities by conducting Research and Development, Training, Trials and Assessment (RDTT&A) on command, control, communication, computer, intelligence, surveillance and reconnaissance (C4ISR) initiatives and training events. Since 2001, membership has been established and represented by three core parties: the U.S. Joint Staff, the Combined Communications and Electronics Board (Australia, Canada, New Zealand, United Kingdom and United States), and the North Atlantic Treaty Organization (including NATO agencies and 29 member nations, not all of which actively participate). Besides the core parties to the CFBLNet Technical Arrangement, four nations (Austria, Finland, Sweden and Switzerland) have become Guest Mission Partners under rules contained in CFBLNet governance documentation referred to as Publication 1.\n\nCFBLNet provides the main platform for conducting Coalition Interoperability Assurance and Validation (CIAV) events in the context of Federated Mission Networking.\n\nThe CFBLNet consists of a distributed and integrated network architecture of combined, joint, and military service infrastructure components (networks, database servers, application servers, client workstations, etc.). These strings of network equipment and services are located within the confines of the various national and international battle laboratories and experimentation sites of the participants, which provide the applications, analytic tools, and communications necessary to conduct initiatives or experiments.\n\nNo single nation owns the CFBLNet infrastructure; each member nation is responsible for the funding and maintenance of its own systems and CFBL network segments, which hook into the backbone at a defined Point-of-Presence (POP). All CFBLNet members must respect the sovereignty and intellectual property of the other nations. Also, each country is responsible for funding its own experiments. The Multinational Information Sharing (MNIS) in Fort Meade, Maryland (USA) maintains day-to-day control of the network and coordinates activities on the network. \n\nThe U.S. CFBLNet infrastructure is extensive and reaches to international demarcation points for the Southern Hemisphere and Europe. Nations and organizations within nations which are not a part of the Technical Agreement must be sponsored to become a Guest CFBLNet Mission Partner (GMP) by a charter member, Core CFBLNet Mission Partner (CMP), to sponsor initiatives and to connect to the CFBLNet.\n\nThe CFBLNet grew out the network designed to support the U.S. Joint Warfighter Interoperability Demonstrations (JWID), which used to build a support network for the period of the demonstrations and tear it down each year after the demonstrations. In 1999, the Coalition Warrior Interoperability Demonstration/Joint Warrior Interoperability Demonstration (CWID/JWID) exercise used, for the first time, a permanent infrastructure that became what is now called the Combined Federated Battle Laboratories Network (CFBLNet), as established by the NATO Consultation, Command and Control Board (NC3B) in 2001. The formal Technical Agreement (or charter) was signed in August 2002.\n\n"}
{"id": "2780651", "url": "https://en.wikipedia.org/wiki?curid=2780651", "title": "Discovery science", "text": "Discovery science\n\n\"Discovery science\" (also known as discovery-based science) is a scientific methodology which emphasizes analysis of large volumes of experimental data with the goal of finding new patterns or correlations, leading to hypothesis formation and other scientific methodologies.\n\nDiscovery-based methodologies are often viewed in contrast to traditional scientific practice, where hypotheses are formed before close examination of experimental data. However, from a philosophical perspective where all or most of the observable \"low hanging fruit\" has already been plucked, examining the phenomenological world more closely than the senses alone (even augmented senses, e.g. via microscopes, telescopes, bifocals etc.) opens a new source of knowledge for hypothesis formation. This process is also known as inductive reasoning or the use of specific observations to make generalizations.\n\nData mining is the most common tool used in discovery science, and is applied to data from diverse fields of study such as DNA analysis, climate modeling, nuclear reaction modeling, and others.\n\nThe use of data mining in discovery science follows a general trend of increasing use of computers and computational theory in all fields of science. Further following this trend, the cutting edge of data mining employs specialized machine learning algorithms for automated hypothesis forming and automated theorem proving.\n"}
{"id": "43779883", "url": "https://en.wikipedia.org/wiki?curid=43779883", "title": "Dragendorff's reagent", "text": "Dragendorff's reagent\n\nDragendorff's reagent is a color reagent to detect alkaloids in a test sample. Alkaloids, if present in the solution of sample, will react with Dragendorff's reagent and produce an orange or orange red precipitate. This reagent was invented by the German pharmacologist, Johann Georg Dragendorff (1836–1898) at the University of Dorpat.\n\nDragendroff's reagent is a solution of potassium bismuth iodide prepared from basic bismuth nitrate (Bi(NO)), tartaric acid, and potassium iodide (KI).\n"}
{"id": "6043553", "url": "https://en.wikipedia.org/wiki?curid=6043553", "title": "Dual-polarization interferometry", "text": "Dual-polarization interferometry\n\nDual-polarization interferometry (DPI) is an analytical technique that probes molecular layers adsorbed to the surface of a waveguide using the evanescent wave of a laser beam. It is used to measure the conformational change in proteins, or other biomolecules, as they function (referred to as the conformation activity relationship).\n\nDPI focuses laser light into two waveguides. One of these functions as the \"sensing\" waveguide having an exposed surface while the second one functions to maintain a reference beam. A two-dimensional interference pattern is formed in the far field by combining the light passing through the two waveguides. The DPI technique rotates the polarization of the laser, to alternately excite two polarization modes of the waveguides. Measurement of the interferogram for both polarizations allows both the refractive index and the thickness of the adsorbed layer to be calculated. The polarization can be switched rapidly, allowing real-time measurements of chemical reactions taking place on a chip surface in a flow-through system. These measurements can be used to infer conformational information about the molecular interactions taking place, as the molecule size (from the layer thickness) and the fold density (from the RI) change. DPI is typically used to characterize biochemical interactions by quantifying any conformational change at the same time as measuring reaction rates, affinities and thermodynamics.\n\nThe technique is quantitative and real-time (10 Hz) with a dimensional resolution of 0.01 nm.\n\nA novel application for dual-polarization interferometry emerged in 2008, where the intensity of light passing through the waveguide is extinguished in the presence of crystal growth. This has allowed the very earliest stages in protein crystal nucleation to be monitored. Later versions of dual-polarization interferometers also have the capability to quantify the order and disruption in birefringent thin films. This has been used, for example, to study the formation of lipid bilayers and their interaction with membrane proteins.\n\n"}
{"id": "2736602", "url": "https://en.wikipedia.org/wiki?curid=2736602", "title": "El Perú (book)", "text": "El Perú (book)\n\nEl Perú: Itinerarios de Viajes is an expansive written work covering a variety of topics in the natural history of Peru, written by the prominent Italian-born Peruvian geographer and scientist Antonio Raimondi in the latter half of the 19th century. The work was compiled from extensive and detailed notes Raimondi took while criss-crossing the country, studying the nation's geography, geology, meteorology, botany, zoology, ethnography, and archaeology; \"El Perú\" focuses to some extent on each of these topics and others. The first volume was published in 1874; several more volumes were published both before Raimondi's death and posthumously from his notes, the last being released in 1913, making a five volume set. The volumes are a classic example of exploration scholarship, and form one of the earliest and broadest scientific reviews of Peru's natural and cultural heritage.\n\n\n"}
{"id": "19068598", "url": "https://en.wikipedia.org/wiki?curid=19068598", "title": "Electrohomeopathy", "text": "Electrohomeopathy\n\nElectrohomoeopathy (or Mattei cancer cure) is a derivative of homeopathy invented in the 19th century by Count Cesare Mattei. The name is derived from a combination of \"electro\" (referring to an electric bio-energy content supposedly extracted from plants and of therapeutic value, rather than electricity in its conventional sense) and \"homeopathy\" (referring to an alternative medicinal philosophy developed by Samuel Hahnemann in the 18th century). Electrohomeopathy has been defined as the combination of electrical devices and homeopathy.\n\nLucrative for its inventor and popular in the late nineteenth century, electrohomoeopathy has been described as \"utter idiocy\". Like all homeopathy, it is regarded by the medical and scientific communities as pseudoscience and its practice as quackery.\n\nElectrohomeopathy was devised by Cesare Mattei (1809–1896) in the latter part of the 19th century. Mattei, a nobleman living in a castle in the vicinity of Bologna, studied natural science, anatomy, physiology, pathology, chemistry and botany. He ultimately focused on the supposed therapeutic power of \"electricity\" in botanical extracts. Mattei made bold, unsupported claims for the efficacy of his treatments, including the claim that his treatments offered a nonsurgical alternative to cancer. His treatment regimens were met with scepticism by mainstream medicine:The electrohomeopathic system is an invention of Count Mattei who prates of \"red\", \"yellow\" and \"blue\", \"green\" and \"white\" electricity, a theory that, in spite of its utter idiocy, has attracted a considerable following and earned a large fortune for its chief promoter.\n\nNotwithstanding criticisms, including a challenge by the British medical establishment to the claimed success of his cancer treatments, electrohomeopathy (or Matteism, as it was sometimes known at the time) had adherents in Germany, France, the USA and the UK by the beginning of the 20th century; electrohomeopathy had been the subject of approximately 100 publications and there were three journals dedicated to it.\n\nRemedies are derived from what are said to be the active micro nutrients or mineral salts of certain plants. One contemporary account of the process of producing electrohomeopathic remedies was as follows:As to the nature of his remedies we learn ... that ... they are manufactured from certain herbs, and that the directions for the preparation of the necessary dilutions are given in the ordinary jargon of homeopathy. The globules and liquids, however, are \"instinct with a potent, vital, electrical force, which enables them to work wonders\". This process of \"fixing the electrical principle\" is carried on in the secret central chamber of a Neo-Moorish castle which Count Mattei has built for himself in the Bolognese Apennines ... The \"red electricity\" and \"white electricity\" supposed to be \"fixed\" in these \"vegetable compounds\" are in their very nomenclature and suggestion poor and miserable fictions.\n\nAccording to Mattei's own ideas however, every disease originates in the change of blood or of the lymphatic system or both, and remedies can therefore be mainly divided into two broad categories to be used in response to the dominant affected system. Mattei wrote that having obtained plant extracts, he was \"able to determine in the liquid vegetable electricity\". Allied to his theories and therapies were elements of Chinese medicine, of medical humours, of apparent Brownianism, as well as modified versions of Samuel Hahnemann's homeopathic principles. Electrohomeopathy has some associations with Spagyric medicine, a holistic medical philosophy claimed to be the practical application of alchemy in medical treatment, so that the principle of modern electrohomeopathy is that disease is typically multi-organic in cause or effect and therefore requires holistic treatment that is at once both complex and natural.\n\nA symposium took place in Bologna in 2008 to mark the 200th anniversary of the birth of Cesare Mattei, with attendees from India, Pakistan, Germany, UK, and the USA. Electrohomeopathy is practiced predominantly in India and Pakistan (RAJYA SABHA Parliamentary Bulletin- The Recognition of Electro Homeopathy System of Medicine Bill,\n2015 by E. M. Sudarsana Natchiappan, M. P), but there are also a number of electrohomeopathy organizations and institutions worldwide.\n\n\n"}
{"id": "5976300", "url": "https://en.wikipedia.org/wiki?curid=5976300", "title": "Engines (children's book)", "text": "Engines (children's book)\n\nEngines: Man's Use of Power, from the Water Wheel to the Atomic Pile is a science book for children by L. Sprague de Camp, illustrated by Jack Coggins, published by Golden Press in 1959. A revised edition issued as part of the publisher's Golden Library of Knowledge Series was published in 1961, and a paperback edition in 1969.\n\nAs stated on the cover, the work is a survey of \"Man's use of power, from the water wheel to the atomic pile.\"\n\n\"The Science News-Letter\", in its July 18, 1959 issue, listed the book among its \"Books of the Week,\" describing the work as a \"[f]actual book for young readers.\"\n"}
{"id": "38812893", "url": "https://en.wikipedia.org/wiki?curid=38812893", "title": "Ergonomic glove", "text": "Ergonomic glove\n\nAn ergonomic glove, also known as a computer glove or support glove, is a stiff glove worn to prevent or remedy carpal tunnel syndrome by holding the wrist in a certain position while typing.\n\nThere are numerous types of ergonomic gloves that vary in design in multiple ways including in their firmness, whether they cover the fingers completely, and if they are intended for heavier physical use.\n\nErgonomic glove design covers many fields with even the gloves worn by healthcare professionals becoming ergonomically designed.\n"}
{"id": "742319", "url": "https://en.wikipedia.org/wiki?curid=742319", "title": "Faraday's laws of electrolysis", "text": "Faraday's laws of electrolysis\n\nFaraday's laws of electrolysis are quantitative relationships based on the electrochemical researches published by Michael Faraday in 1834.\n\nFaraday's laws can be summarized by\n\nwhere:\n\nNote that \"M/z\" is the same as the equivalent weight of the substance altered.\n\nFor Faraday's first law, \"M\", \"F\", and \"z\" are constants, so that the larger the value of \"Q\" the larger m will be.\n\nFor Faraday's second law, \"Q\", \"F\", and \"z\" are constants, so that the larger the value of \"M/z\" (equivalent weight) the larger m will be.\n\nIn the simple case of constant-current electrolysis, formula_2 leading to\n\nand then to\n\nwhere:\n\nIn the more complicated case of a variable electric current, the total charge \"Q\" is the electric current \"I\"(\"formula_5\") integrated over time \"formula_5\":\n\nHere \"t\" is the \"total\" electrolysis time.\n\n\n"}
{"id": "7738451", "url": "https://en.wikipedia.org/wiki?curid=7738451", "title": "High-power field", "text": "High-power field\n\nA high-power field (HPF), when used in relation to microscopy, references the area visible under the maximum magnification power of the objective being used. Often, this represents a 400-fold magnification when referenced in scientific papers.\n\nThe area provides a reference unit, for example in reference ranges for urine tests.\n\nUsed for grading of soft tissue tumors: Grading, usually on a scale of I to III, is based\non the degree of differentiation, the average number of\nmitoses per high-power field, cellularity, pleomorphism,\nand an estimate of the extent of necrosis (presumably a\nreflection of rate of growth). Mitotic counts and necrosis\nare the most important predictors.\n"}
{"id": "5040", "url": "https://en.wikipedia.org/wiki?curid=5040", "title": "Inedia", "text": "Inedia\n\nInedia (Latin for \"fasting\") or breatharianism is the belief that it is possible for a person to live without consuming food. Breatharians claim that food, and in some cases water, are not necessary for survival, and that humans can be sustained solely by \"prana\", the vital life force in Hinduism. According to Ayurveda, sunlight is one of the main sources of \"prana\", and some practitioners believe that it is possible for a person to survive on sunlight alone. The terms \"breatharianism\" or \"inedia\" may also refer to this philosophy when it is practiced as a lifestyle in place of the usual diet.\n\nBreatharianism is considered a deadly pseudoscience by scientists and medical professionals, and several adherents of these practices have died from starvation and dehydration. Though it is common knowledge that biological entities require sustenance to survive, breatharianism continues.\n\nNutritional science proves that fasting for extended periods leads to starvation, dehydration, and eventual death. In the absence of food intake, the body normally burns its own reserves of glycogen, body fat, and muscle. Breatharians claim that their bodies do not consume these reserves while fasting.\n\nSome breatharians have submitted themselves to medical testing, including a hospital's observation of Indian mystic Prahlad Jani appearing to survive without food or water for 15 days, and an Israeli breatharian appearing to survive for eight days on a television documentary. In a handful of documented cases, individuals attempting breatharian fasting have died. Among the claims in support of Inedia investigated by the Indian Rationalist Association, all were found to be fraudulent. In other cases, people have attempted to survive on sunlight alone, only to abandon the effort after losing a large percentage of their body weight.\n\nThe 1670 Rosicrucian text \"Comte de Gabalis\" attributed the practice to the physician and occultist Paracelsus (1493–1541) who was described as having lived \"several years by taking only one half scrupule of Solar Quintessence\". In this book, it is also stated that, \"Paracelsus affirms that He has seen many of the Sages fast twenty years without eating anything whatsoever.\"\n\nRam Bahadur Bomjon is a young Nepalese Buddhist monk who lives as an ascetic in a remote area of Nepal. Bomjon appears to go for periods of time without ingesting either food or water. One such period was chronicled in a 2006 Discovery Channel documentary titled \"The Boy With Divine Powers\", which reported that Bomjon neither moved, ate, nor drank anything during 96 hours of filming.\n\nPrahlad Jani is an Indian sadhu who says he has lived without food and water for more than 70 years. His claims were investigated by doctors at Sterling Hospital, Ahmedabad, Gujarat in 2003 and 2010. The study concluded that Prahlad Jani was able to survive under observation for two weeks without either food or water, and had passed no urine or stool, with no need for dialysis. Interviews with the researchers speak of strict observation and relate that round-the-clock observation was ensured by multiple CCTV cameras. Jani was subjected to multiple medical tests, and his only contact with any form of fluid was during bathing and gargling, with the fluid spat out measured by the doctors. The research team could not comment on his claim of having been able to survive in this way for decades.\n\nThe case has attracted criticism, both after the 2003 tests and the 2010 tests. Sanal Edamaruku, president of the Indian Rationalist Association, criticized the 2010 experiment for allowing Jani to move out of a certain CCTV camera's field of view, meet devotees and leave the sealed test room to sunbathe. Edamaruku stated that the regular gargling and bathing activities were not sufficiently monitored, and accused Jani of having had some \"influential protectors\" who denied Edamaruku permission to inspect the project during its operation.\n\nJasmuheen (born Ellen Greve) was a prominent advocate of breatharianism in the 1990s. She said \"I can go for months and months without having anything at all other than a cup of tea. My body runs on a different kind of nourishment.\" Interviewers found her house stocked with food; Jasmuheen claimed the food was for her husband and daughter. In 1999, she volunteered to be monitored closely by the Australian television program \"60 Minutes\" for one week without eating to demonstrate her methods. Jasmuheen stated that she found it difficult on the third day of the test because the hotel room in which she was confined was located near a busy road, causing stress and pollution that prevented absorption of required nutrients from the air. \"I asked for fresh air. Seventy percent of my nutrients come from fresh air. I couldn’t even breathe,\" she said. The third day the test was moved to a mountainside retreat where her condition continued to deteriorate. After Jasmuheen had fasted for four days, Berris Wink, president of the Queensland branch of the Australian Medical Association, urged her to stop the test.\n\nAccording to Wink, Jasmuheen’s pupils were dilated, her speech was slow, and she was \"quite dehydrated, probably over 10%, getting up to 11%\". Towards the end of the test, she said, \"Her pulse is about double what it was when she started. The risks if she goes any further are kidney failure. \"60 Minutes\" would be culpable if they encouraged her to continue. She should stop now.\" The test was stopped. Wink said, \"Unfortunately there are a few people who may believe what she says, and I'm sure it's only a few, but I think it's quite irresponsible for somebody to be trying to encourage others to do something that is so detrimental to their health.\" Jasmuheen challenged the results of the program, saying, \"Look, 6,000 people have done this around the world without any problem.\"\n\nJasmuheen was awarded the Bent Spoon Award by Australian Skeptics in 2000 (\"presented to the perpetrator of the most preposterous piece of paranormal or pseudoscientific piffle\"). She also won the 2000 Ig Nobel Prize for Literature for \"Living on Light\". Jasmuheen claims that their beliefs are based on the writings and \"more recent channelled material\" from St. Germain. She stated that some people's DNA has expanded from 2 to 12 strands, to \"absorb more hydrogen\". When offered $30,000 to prove her claim with a blood test, she said that she didn't understand the relevance as she was not referring to herself.\n\nIn the documentary \"No Way to Heaven\" the Swiss chemist Michael Werner claims to have followed the directions appearing on Jasmuheen's books, living for several years without food. The documentary also describes two attempts at scientific verification of his claims. , five deaths had been directly linked to breatharianism as a result of Jasmuheen's publications. Jasmuheen has denied any responsibility for the deaths.\n\nWiley Brooks is the founder of the Breatharian Institute of America. He was first introduced to the public in 1980 when he appeared on the TV show \"That's Incredible!\". Brooks stopped teaching recently to \"devote 100% of his time on solving the problem as to why he needed to eat some type of food to keep his physical body alive and allow his light body to manifest completely.\" Brooks claims to have found \"four major deterrents\" which prevented him from living without food: \"people pollution\", \"food pollution\", \"air pollution\" and \"electro pollution\".\n\nIn 1983 he was reportedly observed leaving a Santa Cruz 7-Eleven with a Slurpee, a hot dog, and Twinkies. He told \"Colors\" magazine in 2003 that he periodically breaks his fasting with a cheeseburger and a cola, explaining that when he's surrounded by junk culture and junk food, consuming them adds balance.\n\nWiley Brooks later claimed that Diet Coke and McDonald's cheeseburgers have special \"5D\" properties. The idea of separate but interconnected 5D and 3D worlds is a major part of Wiley Brooks' ideology, and Wiley Brooks encourages his followers to only eat these special 5D foods, as well as meditate on a set of magical 5D words.\n\nBrooks's institute has charged varying fees to prospective clients who wished to learn how to live without food, which have ranged from US$100,000 with an initial deposit of $10,000 to one billion dollars, to be paid via bank wire transfer with a preliminary deposit of $100,000, for a session called \"Immortality workshop\". A payment plan was also offered. These charges have typically been presented as limited time offers exclusively for billionaires.\n\nHira Ratan Manek (born 12 September 1937) claims that since 18 June 1995 he has lived on water and occasionally tea, coffee, and buttermilk. Manek states that Sungazing is the key to his health citing yogis, ancient Egyptians, Aztecs, Mayans and Native Americans as practitioners of the art. While he and his proponents state that medical experts have confirmed his ability to draw sustenance by gazing at the sun, he was caught on camera eating a big meal in a San Francisco restaurant in the 2011 documentary \"Eat the Sun\".\n\nIn a television documentary produced by the Israeli television investigative show \"The Real Face\" (פנים אמיתיות) hosted by Amnon Levy, Israeli practitioner of Inedia Ray Maor (ריי מאור) who appeared to survive without food or water for eight days and eight nights. According to the documentary, he was restricted to a small villa and placed under constant video surveillance, with medical supervision that included daily blood testing. The documentary claimed Maor was in good spirits throughout the experiment, lost 17 lb after eight days, blood tests showed no change before, during or after the experiment, and Cardiologist Ilan Kitsis from Tel Aviv Sourasky Medical Center was \"baffled.\"\n\n\nHindu religious texts contain account of saints and hermits practicing what would be called inedia, breatharianism or Sustenance through Light in modern terms. In Valmiki's Ramayana, Book III, Canto VI, an account of anchorites and holy men is given, who flocked around Rama when he came to Śarabhanga's hermitage. These included, among others, the \"...saints who live on rays which moon and daystar give\" and \"those ... whose food the wave of air supplies\". In Canto XI of the same book a hermit named Māṇḍakarṇi is mentioned: \"For he, great votarist, intent – On strictest rule his stern life spent – ... – Ten thousand years on air he fed...\" (English quotations are from Ralph T. H. Griffith's translation).\n\nParamahansa Yogananda's \"Autobiography of a Yogi\" details two alleged historical examples of breatharianism, Hari Giri Bala and Therese Neumann.\n\nThere are claims that Devraha Baba lived without food.\n\n\n\n"}
{"id": "47492430", "url": "https://en.wikipedia.org/wiki?curid=47492430", "title": "Ion reactor", "text": "Ion reactor\n\nIon Reactor, is an invention by a British scientist.\n\nIn 2011, Dr Christopher Strevens (an inventor from London) began posting a website with instructions of how to build his \"fusion reactor\", which he says: \"Creates helium from hydrogen. It also captures the power given off during the reaction as electrical power.\" He also posted several videos to YouTube showing his prototype in operation, and showing the different color of gas from before versus after; as well as showing spectral analysis that indicates that the hydrogen that he puts into the system has transmuted to helium—a nuclear phenomenon.\n\nHe said: \"I found that when I increased the exciter power to 800 Watts, the output rose to 2,000 Watts [2.5-times overunity], and when I isolated the reactor from the exciter, this power remained. The spark gap regulator became active, keeping the power at this level. I only allowed this for a short time before reconnecting the exciter and turning the power down and the reaction ceased.\"\n\nBecause of the experiment being dangerous it would require a special paramagnetic ceramic bottle glazed inside to contain hydrogen ions and use the magnetic field of the coils and induction of ion fields to make a magnetic bottle to confine the ion reactions.\n\nA brief description of the device, being a hydrogen tube wrapped with high voltage coils and a sort of energizing coil being similar to a high voltage tesla coil for the ionizer. The device having been made in a clear glass tube in early experiments uses induction to ionize the gases and the field made by ionized gases to energize ion tube coils.\n\nThe effect is similar to pyroelectric fusion but instead of pyroelectric crystals stripping the electrons from hydrogen atoms it is a high voltage induced electromagnetic field of coils and electrostatic induction of the ionized gas.\n\nhttps://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19660030642.pdf\n\n"}
{"id": "634124", "url": "https://en.wikipedia.org/wiki?curid=634124", "title": "Leuchter report", "text": "Leuchter report\n\nThe Leuchter report is a pseudoscientific document authored by American execution technician Fred A. Leuchter, who was commissioned by Ernst Zündel to defend him at his trial in Canada for distributing Holocaust denial material. Leuchter compiled the report in 1988 with the intention of investigating the feasibility of mass homicidal gassings at Nazi extermination camps, specifically at Auschwitz. He travelled to the camp, collected multiple pieces of brick from the remains of the crematoria and gas chambers (without the camp's permission), brought them back to the United States, and submitted them for chemical analysis. At the trial, Leuchter was called upon to defend the report in the capacity of an expert witness; however during the trial, the court ruled that he had neither the qualifications nor experience to act as such.\n\nLeuchter cited the absence of Prussian blue in the homicidal gas chambers to support his view that they could not have been used to gas people. However, residual iron-based cyanide compounds are not a categorical consequence of cyanide exposure. By not discriminating against that, Leuchter introduced an unreliable factor into his experiment, and his findings were seriously flawed as a result. In contrast, tests conducted by Polish forensic scientists (who discriminated against iron-based compounds) confirmed the presence of cyanide in the locations, in accordance with where and how it was used in the Holocaust. In addition, the report was criticized as Leuchter had overlooked critical evidence, such as documents in the SS architectural office which recorded the mechanical operation of the gas chambers and others which verified the rate at which the Nazis could burn the bodies of those gassed.\n\nIn 1985, Ernst Zündel, a German pamphleteer and publisher living in Canada, was put on trial for publishing Richard Verrall's Holocaust denial pamphlet \"Did Six Million Really Die?\", which was deemed to violate Canadian laws against distributing false news. Zündel was found guilty, but the conviction was overturned on appeal. This led to a second prosecution.\n\nZündel and his lawyers were joined by Robert Faurisson, a French academic of literature and Holocaust denier, who came to Toronto to advise the defence, having previously testified as expert witness at the first trial. He was joined by David Irving, an English writer and Holocaust denier, who was to assist the defence and testify on Zündel's behalf. Faurisson claimed that it was technically and physically impossible for the gas chambers at Auschwitz to have functioned as extermination facilities, based on comparisons with American execution gas chambers; he therefore suggested getting an American prison warden who had participated in executions by gas to testify. Irving and Faurisson therefore invited Bill Armontrout, warden of the Missouri State Penitentiary, who agreed to testify and suggested they contact Fred A. Leuchter, a Bostonian execution equipment designer. Faurisson reported that Leuchter initially accepted the mainstream account of the Holocaust, but after two days of discussion with him, he stated that Leuchter was convinced that homicidal gassings never occurred. After having met Zündel in Toronto and agreeing to serve as an expert witness for his defence, Leuchter travelled with them to spend a week in Poland. He was accompanied by his draftsman, a cinematographer supplied by Zündel, a translator fluent in German and Polish, and his wife. Although Zündel and Faurisson did not accompany them, Leuchter said that they were with them \"every step of the way\" in spirit.\n\nAfter arriving in Poland the group spent three days at the former Auschwitz concentration camp site, and another at the former Majdanek concentration camp. At these, they filmed Leuchter illicitly collecting what he regarded to be forensic quality samples of materialsfrom the wreckage of the former gas extermination facilities, while his wife and the translator acted as lookouts. Drawings of where the samples were taken from, the film footage of their physical collection and Leuchter's notebook detailing the work were surrendered to the trial court as evidence. Leuchter claimed that his conclusions were based on his expert knowledge of gas chamber operation, his visual inspection of what remained of the structures at Auschwitz, and original drawings and blueprints of some of the facilities. He said that the blueprints had been given to him by Auschwitz Museum officials.\n\nThe compiled report was published in Canada as \"The Leuchter Report: An Engineering Report on the Alleged Execution Gas Chambers at Auschwitz, Birkenau, and Majdanek, Poland\", by Zündel's Samisdat Publications, and in England as \"Auschwitz: The End of the Line. The Leuchter Report: The First Forensic Examination of Auschwitz\" by Focal Point Publications, David Irving's publishing house. However, the court accepted the report only as evidentiary display and not as direct evidence; Leuchter was therefore required to explain it and testify to its veracity in the trial.\n\nBefore Leuchter could do this, he was examined by the court. He admitted that he was not a toxicologist and dismissed the need for having a degree in engineering:\n\nLeuchter admitted under oath that he only had a bachelor of arts degree and implicitly suggested that an engineering degree was unavailable to him by saying that his college did not offer an engineering degree during his studies. Boston University actually offered three different kinds of such qualification when he was a student there. When asked by the court if the B.A. he obtained was in a field that entitled him to operate as an engineer, he confirmed that this was so, even though his degree was in history. Similarly, Leuchter claimed that he obtained most of his research material on the camps (including original crematoria blueprints) from the Auschwitz and Majdanek camps' archives, and testified that these documents had a far more important role in shaping his conclusions than the physical samples he collected, yet after the trial the director of the Auschwitz museum denied that Leuchter had received any plans or blueprints from them.\n\nJudge Ronald Thomas began to label Leuchter's methodology as \"ridiculous\" and \"preposterous\", dismissing many of the report's conclusions on the basis that they were based on \"second-hand information\", and refused to allow him to testify on the effect of Zyklon B on humans because he had never worked with the substance, and was neither a toxicologist nor a chemist. Judge Thomas dismissed Leuchter's opinion because it was of \"no greater value than that of an ordinary tourist\", and in regards to Leuchter's opinion said:\n\nWhen questioned on the functioning of the crematoria, the judge also prevented Leuchter from testifying because \"he hasn't any expertise\". Leuchter also claimed that consultation relating to sodium cyanide and hydrogen cyanide with DuPont was \"an on-going thing\". DuPont, the largest American manufacturer of hydrogen cyanide, stated that it had \"never provided any information on cyanides to persons representing themselves as Holocaust deniers, including Fred Leuchter\", and had \"never provided any information regarding the use of cyanide at Auschwitz, Birkenau or Majdanek.\"\n\nThe contents of the report, in particular Leuchter's methodology, are heavily criticised. James Roth, the manager of the lab that carried out the analysis on the samples Leuchter collected, swore under oath to the results at the trial. Roth did not learn what the trial was about until he got off the stand. He later stated that cyanide would have only penetrated to a depth of around 10 micrometres, a tenth of the thickness of a human hair. The samples of brick, mortar and concrete that Leuchter took were of indeterminate thickness: not being aware of this, the lab ground the samples to a fine powder which thus severely diluted the cyanide-containing layer of each sample with an indeterminate amount of brick, varying for each sample. A more accurate analysis would have been obtained by analysing the surface of the samples Leuchter collected. Roth offered the analogy that the investigation was like analyzing paint on a wall by analyzing the timber behind it.\n\nLeuchter's opposition to the possibility of homicidal gassings at Auschwitz relies on residual cyanide remains found in the homicidal gas chambers and delousing chambers at Auschwitz. While both facilities were exposed to the same substance (Zyklon B), many of the delousing chambers are stained with an iron based compound known as Prussian blue, which is not apparent in the homicidal gas chambers. It is not only this disparity that Leuchter cites, but accordingly from his samples (which included measurements of it) that he claims he measured much more cyanide in the delousing chambers than in the gas chambers, which he argues is inconsistent between the amounts necessary to kill human beings and lice. This argument is often cited by Holocaust deniers, and similar claims are also made by Germar Rudolf.\n\nAccording to Richard J. Green:\n\nIn other words, Green states that Leuchter failed to show that Prussian Blue would have been produced in the homicidal gas chambers in the first place—meaning its absence is not in itself proof that no homicidal gassings took place.\nThe problem with Prussian blue is that it is by no means a categorical sign of cyanide exposure. One factor necessary in its formation is a very high concentration of cyanide. In terms of the difference between amounts measured in the delousing chambers and homicidal gas chambers, critics explain that the exact opposite of what deniers claim is true. Insects have a far higher resistance to cyanide than humans, with concentration levels up to 16,000ppm (parts per million) and an exposure time of more than 20 hours (sometimes as long as 72 hours) being necessary for them to succumb. In contrast, a cyanide concentration of only 300ppm is fatal to humans in a matter of minutes. This difference is one of the reasons behind the concentration disparity. Another exceedingly sensitive factor by which very small deviances could determine whether Prussian blue may form is pH. pH could be affected by the presence of human beings. Also, while the delousing chambers were left intact, the ruins of the crematoria at Birkenau had been exposed to the elements for over forty years by the time Leuchter collected his samples. This would have severely affected his results, because unlike Prussian blue and other iron based cyanides, cyanide salts are highly soluble in water.\n\nSince the formation of Prussian blue is not an unconditional outcome of exposure to cyanide, it is not a reliable indicator. Leuchter and Rudolf claim to have measured much more cyanide in the delousing chambers than in the homicidal gas chambers, but since they did not discriminate against an unreliable factor, Green maintains that instant bias is introduced into their experiments. Similarly, Rudolf acknowledges that Prussian blue does not always form upon exposure to cyanide and is thus not a reliable marker, yet continues to include the iron compounds in his analysis. Green describe this as \"disingenuous\". Since a building that contains Prussian blue staining would exhibit much higher levels of detectable cyanides than one without any, Green writes that Leuchter's and Rudolf's measurements reveal nothing more than what is already visible to the naked eye.\n\nIn February 1990, Professor Jan Markiewicz, director of The Institute for Forensic Research (IFRC) in Kraków conducted a fair experiment where iron compounds were excluded. Given that the ruins of the gas chambers at Birkenau have been washed by a column of water at least 35m in height based on climatological records since 1945, Markiewicz and his team were not optimistic at being able to detect cyanides so many years later; nevertheless, having the legal permission to obtain samples, they collected some from areas as sheltered from the elements as possible.\n\nLeuchter's report stated that the small amounts of cyanide he detected in the ruins of the crematoria are merely the result of fumigation. However the IFRC points out that the control samples they took from living areas which may have been fumigated only once as part of the 1942 typhus epidemic tested negative for cyanide, and that the typhus epidemic occurred before the crematoria at Birkenau even existed.\nAccordingly, the IFRC demonstrated that cyanides were present in all of the facilities where it is claimed that they were exposed, i.e. all five crematoria, the cellars of Block 11 and the delousing facilities. Critics state that any attempt to demonstrate that the crematoria could not have functioned as homicidal gas chambers on the basis that they were not exposed to cyanide is unsuccessful, given that its presence in what remains of these facilities is incontrovertible, and write that all of the gas chambers were exposed to cyanide at levels higher than background levels elsewhere in the camp, such as living areas, where no cyanides at all were detected. In addition, tests conducted at Auschwitz in 1945 revealed the presence of cyanides on ventilation grilles found in the ruins of Crematorium II (thus also demonstrating that the Leuchter report was not the first forensic examination of the camp as purported in the title of the London edition). The historian Richard J. Evans argued that due to Leuchter's ignorance of the large disparity between the amounts of cyanide necessary to kill humans and lice, instead of disproving the homicidal use of gas chambers, the small amounts of cyanide which Leuchter detected actually tended to confirm it.\n\nBy order of Heinrich Himmler, the crematoria and gas chambers at Birkenau were destroyed by the SS in order to hide evidence of genocide. Nothing more than the bases of Crematoria IV and V can be seen: the floor plans of both facilities are indicated by bricks laid out across the concrete foundations, and Crematoria II and III are in ruins. Professor Robert Jan van Pelt labels Leuchter's comment that the facilities have not changed at all since 1942 or 1941 as \"nonsense\".\n\nBecause hydrogen cyanide is explosive, Leuchter maintained that the gas chambers could never have been operated due to their proximity to the ovens of the crematoria. It is correct that hydrogen cyanide is explosive, but only at concentrations of 56,000 ppm and above – over 186 times more than the lethal dose of 300 ppm. Critics estimate conservatively that within 5 to 15 minutes, gas chamber victims were exposed to 450 – 1810 ppmv – again considerably lower than the lower explosion limit.\n\nLeuchter incorrectly assumed that the gas chambers were not ventilated. The basement gas chambers of Crematoria II and III were mechanically ventilated via motors in the roof space of the main crematorium structure capable of extracting the remaining gas and renewing the air every three to four minutes.\n\nWhen ventilation was not used such as in Crematoria IV and V (although a ventilation system was later installed in Crematorium V in May 1944), Sonderkommando prisoners wore gas masks when removing the bodies. When presented in court with a document by the chief Auschwitz architect SS-\"Sturmbannführer\" Karl Bischoff, Leuchter misconstrued aeration (\"Belüftung\") and ventilation (\"Entlüftung\") as part of the furnace blower systems, when they were actually in reference to the ventilation channels in the walls that straddle the gas chambers. These are visible on blueprints, and can still partly be seen in the ruined east wall of the Crematorium III gas chamber.\n\nLeuchter was also prepared to act as expert witness regarding crematoria ovens despite admitting during cross examination that he had no expert knowledge. Leuchter presented his own estimate of 156 corpses as the total daily incineration capacity of the installations at Auschwitz. During cross-examination, he was presented with a letter written by the Auschwitz Central Construction Office (\"Auschwitz Zentralbauleitung\") of June 28, 1943, from SS-\"Sturmbannführer\" Jahrling to SS-\"Brigadeführer\" Hans Kammler stating that the five crematoria installations had a collective daily capacity of 4,756 corpses. Leuchter conceded that this was quite different from his own figure, and that he had never seen the document in question before.\n\nA patent application by the makers of the ovens, (both of which were made during the war) and two independent testimonies confirmed the capacity of the crematoria. Because the 4,756 figure is evidence of the Nazis equipping a camp of a maximum of 125,000 prisoners with the facility to cremate 140,000 of them per month, critics of Leuchter explain that this reveals the true exterminationist purpose of Auschwitz: a camp with the capacity to reduce its entire population to ash on a monthly basis was not merely a benign internment camp.\n\nAt various times (such as in the summer of 1944 when the crematoria couldn't keep up with the extermination rate), bodies were burnt in open-air pits. Accordingly, the capacity of the crematoria was never a limiting factor, and the pits yielded practically no limit to the number of corpses that could be burnt.\n\n\n\n"}
{"id": "53700592", "url": "https://en.wikipedia.org/wiki?curid=53700592", "title": "List of ice cores", "text": "List of ice cores\n\nThis is a list of ice cores drilled for scientific purposes. Note that many of these locations are on moving ice sheets, and the latitude and longitude given is as of the date of drilling. \n\n\n"}
{"id": "640228", "url": "https://en.wikipedia.org/wiki?curid=640228", "title": "List of mnemonics", "text": "List of mnemonics\n\nThis article contains lists of mnemonics used to remember various objects, lists etc.\n\n\n\n\n\n\n\n\n\nThe articulation of the quadratic equation can be sung to the tune of various songs as a mnemonic device.\n\nFor helping students in remembering the rules in adding and multiplying two signed numbers, Balbuena and Buayan (2015) made the letter strategies LAUS (like signs, add; unlike signs, subtract) and LPUN (like signs, positive; unlike signs, negative), respectively.\n<br>\nOrder of Operations\n<br>\nPEMDAS\n<br>\nPlease - Parenthesis\n<br>\nExcuse - Exponents\n<br>\nMy - Multiplication\n<br>\nDear - Division\n<br>\nAunt - Addition\n<br>\nSally - Subtraction\n<br>\n\nPEMDAS\n<br>\nPineapples - Parenthesis\n<br>\nEat - Exponents\n<br>\nMangoes - Multiplication\n<br>\nDuring - Division\n<br>\nAutumn - Addition\n<br>\nSeason - Subtraction\n\n'A OF A OF A'\n\nThenar (lateral to medial-palmar surface):\nHypothenar (lateral to medial-palmar surface):\n\n\n\n\nThus we get the names of the strings from 6th string to the 1st string in that order.\n\nConversely, a mnemonic listing the strings in the reverse order is:\n\nAs for guitar tuning, there is also a mnemonic for ukuleles. \n\n\n\n\n\n\n\n\n\n\n\nIn most words like friend, field, piece, pierce, mischief, thief, tier, it is \"\"i\" which comes before \"e\". But on some words with c just before the pair of e and i, like receive, perceive, \"e\" comes before \"i\"\". This can be remembered by the following mnemonic,\n\nBut this is not always obeyed as in case of weird and weigh, weight, height, neighbor etc. and can be remembered by extending that mnemonic as given below\n\nAnother variant, which avoids confusion when the two letters represent different sounds instead of a single sound, as in atheist or being, runs\n\nMost frequently \"u\" follows \"q\". e.g.: Que, queen, question, quack, quark, quartz, quarry, quit, Pique, torque, macaque, exchequer. Hence the mnemonic:\n\n\nAdvice, Practice, Licence etc. (those with c) are nouns and Advise, Practise, License etc. are verbs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "44069998", "url": "https://en.wikipedia.org/wiki?curid=44069998", "title": "List of organizations opposing mainstream science", "text": "List of organizations opposing mainstream science\n\nThis is a list of organizations opposing mainstream science by frequently challenging the facts and conclusions recognized by the mainstream scientific community. By claiming to employ the scientific method in order to advance certain fringe ideas and theories, they are often charged with promotion of various forms of pseudoscience.\n\n\nQuackery is the promotion of ineffective or fraudulent medical treatments.\n\n"}
{"id": "256043", "url": "https://en.wikipedia.org/wiki?curid=256043", "title": "List of science magazines", "text": "List of science magazines\n\nA science magazine is a periodical publication with news, opinions and reports about science, generally written for a non-expert audience. In contrast, a periodical publication, usually including primary research and/or reviews, that is written by scientific experts is called a \"scientific journal\". Science magazines are read by non-scientists and scientists who want accessible information on fields outside their specialization.\n\nArticles in science magazines are sometimes republished or summarized by the general press.\n\n\n\n\n"}
{"id": "12494249", "url": "https://en.wikipedia.org/wiki?curid=12494249", "title": "List of vineyard soil types", "text": "List of vineyard soil types\n\nThe soil composition of vineyards is one of the most important viticultural considerations when planting grape vines. The soil supports the root structure of the vine and influences the drainage levels and amount of minerals and nutrients that the vine is exposed to. The ideal soil condition for a vine is a layer of thin topsoil and subsoil that sufficiently retains water but also has good drainage so that the roots do not become overly saturated. The ability of the soil to retain heat and/or reflect it back up to the vine is also an important consideration that affects the ripening of the grapes.\n\nThere are several minerals that are vital to the health of vines that all good vineyard soils have. These include calcium which helps to neutralize the Soil pH levels, iron which is essential for photosynthesis, magnesium which is an important component of chlorophyll, nitrogen which is assimilated in the form of nitrates, phosphates which encourages root development, and potassium which improves the vine metabolisms and increases its health for next year's crop.\n\n\"Unless otherwise noted the primary reference for this list is Sotheby's Wine Encyclopedia 2005\"\n\n\n\n\n"}
{"id": "41885308", "url": "https://en.wikipedia.org/wiki?curid=41885308", "title": "Living educational theory", "text": "Living educational theory\n\nLiving educational theory (LET) is a research method in educational research.\n\nThe idea of action research as a living practice entered the mainstream of action research from the book, \"Action Research as a Living Practice\" by Terrance Carson and Dennis Sumara in 1997. Carson and Sumara transformed the concept of traditional action research with the idea that, ...\" participation in action research practices are particular ways of living and understanding that require more of the researcher than the \"application\" of research methods. Rather, action research is a lived practice that requires that the researcher not only investigate the subject at hand but, as well, provide some account of the way in which the investigation both shapes and is shaped by the investigator . This requires what Martin Buber called an \"I-Thou\" approach toward other and this approach applied to action research as well. To make Buber's language more modern and accessible, LET translated Buber's \"I-Thou\" approach toward another human being to an \"I/you/we\" approach to action research. This differs greatly from an approach to living theory action research imagined by Jack Whitehead (2002) where he imagines living theory action research as forming an \"I-theory\" of knowledge. Director of the Philosophy for Children Project at Notre Dame de Namur University William Barry proposes LET focuses on the connections between the researcher and the other person or subject where the lives of action researchers are inextricable linked in a profound manner with the individuals and communities involved in the subject of study. LET from a Barryian perspective is a critical theory and emancipatory action research approach which seeks the dialectic, not debate and battles of [discourse].\n\nA major difference of William Barry's version of living educational theory, which was the focus of his successful completion of a Ph.D. thesis at Nottingham Trent University, UK, is the essential question behind the living educational theory approach to action research (2012b). The question is not \"How can I generate a living legacy for myself through an I-It theory approach toward knowledge and other forms of life?\" Rather the essential question is, \"How does one conduct a life that includes the practice of educational action research?\" The theory/practice problem disappears when honesty about one's biases regarding spiritual, existential, and emotional intelligence are made clear in the action research process.\n\nThe phraseology \"educational theory\" originated with the work of Jack Whitehead, a former lecturer at the University of Bath, and it was further developed and greatly improved methodologically by Jean McNiff in 2009 because of her willingness to be transparent about her values and intentions. Whitehead's main emphasis for conducting research is to promote the individual under the guise of collaboration and research outcomes must be captured on video for authentic validation. Whitehead's view of action research promotes that living educational theory (he uses living theory and living educational theory interchangeably so it is difficult for a reader to know what he is writing about) should be aimed at the bringing of energy-flowing values as explanatory principles and standards of judgments into the Academy for the legitimation of living educational theories (Whitehead 2008). In the eyes of Whitehead radical constructivism is at the core of living educational theory research. In 2013, Whitehead and McNiff separated as collaborators as McNiff saw spiritual, emotional intelligence as key to action research while Whitehead disagreed and believed that media accounts (primarily video tapping people) of action research could provide clues to virtues which held the future of humanity though energy flowing examples of collaboration. McNiff stated in a May 2013 conference she San Francisco, California (ARNA Conference) that she would never appear or work with Whitehead again. She repeated this message again months later at a UK conference in York. American William Barry believed the concept of LET was too important and found a dialectic between McNiff and Whitehead and he created a new understanding of LET which was presented at a three-day international conference in 2013 at Liverpool Hope University titled, \"Researching Our Own Practice\" .\n\nLiving educational theory was first clearly defined and developed by California Professor of Philosophy William Barry (2012b) in Liz Atkins and Susan Wallace's book , \"Qualitative Research in Education\", co-published by Sage and the British Educational Research Association (BERA). This book was one of four in a series sponsored by the BERA regarding best practice progressive research methods in educational research. The originality and uniqueness of Barry's development of living educational theory (LET) action research is the importance of gaining \"ontological weight\" through the action research process. Ontological weight empowers the researcher's ability, and the ability of other people involved in the action research project, to have the research experience and focus of the research be transformational and add, or at least reinforce, a sense of meaning in learning and life. Barry was influenced to use the concept \"ontological weight\" by the existentialist Catholic philosopher Gabriel Marcel (1963).\n\nThe idea of action research as a living practice entered the mainstream of action research from the book, \"Action Research as a Living Practice\" by Terrance Carson and Dennis Sumara in 1997. The term \"educational theory\" originated with the work of Jack Whitehead, a former lecturer at the University of Bath, and it was further developed and greatly improved methodologically by Jean McNiff in 2009. Whitehead's main emphasis for conducting research is to promote the individual under the guise of collaboration and research outcomes must be captured on video for authentic validation. Whitehead's view of action research promotes that living educational theory (he uses living theory and living educational theory interchangeably so it is difficult for a reader to know what he is writing about) should be aimed at the bringing of energy-flowing values as explanatory principles and standards of judgments into the Academy for the legitimation of living educational theories (Whitehead 2008). In the eyes of Whitehead, radical constructivism is at the core of living educational theory research.\n\nBarry was asked by the BERA sponsored authors to reflect on the nature of living educational theory (LET) because there existed no clear definition of LET in the literature. Barry was asked because he had successfully used LET in an innovative fashion, and was the first to clearly define LET as based in critical theory which embraced transpersonal psychology through his earned 2012 PhD thesis at Nottingham Trent University Nottingham, UK. He proposed LET as a way of challenging the oppressive use of power using critical theory in a need fulfilling way (Glasser 1998). Barry proposed the following definition and approach to action research he calls living educational theory and his approach has been used as an action research method in undergraduate and graduate courses and research at Notre Dame de Namur University in Silicon Valley, California as well as by other researchers around the world.\n\nBarry explained that living educational theory \"[is] a critical and transformational approach to action research. It confronts the researcher to challenge the status quo of their educational practice and to answer the question, 'How can I improve that I'm doing?' Researchers who use this approach must be willing to recognize and assume responsibility for being a 'living contradiction' in their professional practice – thinking one way and acting in another. The mission of the LET action researcher is to overcome workplace norms and self – behavior which contradict the researcher's values and beliefs. The vision of the LET researcher is to make an original contribution to knowledge through generating an educational theory proven to improve the learning of people within a social learning space. The standard of judgment for theory validity is evidence of workplace reform, transformational growth of the researcher, and improved learning by the people researcher claimed to have influenced...\" .\n\nBarry's LET approach to action research was heavily influenced by action researchers focused on emancipatory social change, collaboration, and liberation theology (2012a). Prominent developers of LET, without whose work LET would most likely never had been developed by Barry, are notable action researchers, educators, and philosophers such as Martin Buber's (1970) conception of 'I and Thou' and Krishnamurti's (1953) liberation pedagogy emphasizing education as significant to leading a quality filled life; Paulo Freire (1998 and 1970) and his concept of participatory action research and the need to be politically aware; the work of Carr and Kremmis (1986) and Habermas (1992) and their concept and building critical educational knowledge; Professor Manheimer of UNC (1999) and his challenge to his readers to enfold the past into the living present in order to become historical to oneself and then strive to linking life times with each other; Apple (1982) and Michel Foucault (1990) and the role of power and politics in education and Joe Kincheloe (2008), Henry Giroux (1997), and Peter McLaren (1989) and their promotion of critical pedagogy.\n\nProf. Barry was the first Ph.D. researcher to successfully use living educational theory in conjunction with neuro-linguistic programming (NLP), spiral dynamics and autoethnography (based on a multiple intelligences model which includes spiritual and emotional intelligence and embraces transpersonal knowing) as valid methods of research working under the methodological umbrella of phenomenology and hermeneutics. His LET approach to Ph.D.level action research led to the unique use of fictional storytelling as a vehicle by which to replace the traditional literature review chapter in Ph.D. research but in a more rigorous and creative fashion. The process of storytelling allows the researcher to exercise their emotional intelligence in a superior manner than the traditional research literature review allows.\n\nLiving educational theory as defined and created by Barry is part of the curriculum of multiple courses at Notre Dame de Namur University located in Silicon Valley, California in their credentialing program for teacher education. Barry's Living Educational Theory Action Research Method is based on a six step process based on research questions that normally start from the format, \" How can I influence the transformation of...?\" or \"How can I contribute to the improvement of...?\" The research is dialectical in nature:\n\n\n\n\n"}
{"id": "185290", "url": "https://en.wikipedia.org/wiki?curid=185290", "title": "Mars effect", "text": "Mars effect\n\nThe Mars effect is a purported statistical correlation between athletic eminence and the position of the planet Mars relative to the horizon at time and place of birth. This controversial finding was first reported by the French psychologist and \"neo-astrologer\" Michel Gauquelin published in his book \"L'influence des astres\" (\"The Influence of the Stars\", 1955). Gauquelin suggested that a statistically significant number of sports champions were born just after the planet Mars rises or culminates. Gauquelin divided the plane of the ecliptic into twelve sectors, identifying two \"key\" sectors of statistical significance.\n\nGauquelin's work was accepted by the notable psychologist and statistician Hans Eysenck among others but later attempts to validate the data and replicate the effect have produced uneven results, chiefly owing to disagreements over the selection and analysis of the data set. Since the phenomenon in question depends upon the daily rotation of the Earth, the availability and accuracy of time and place of birth data is crucial to such studies, as is the criterion of \"eminence\". Later research claims to explain the Mars effect by selection bias, favouring champions who were born in a \"key sector\" of Mars and rejecting those who were not from the sample.\n\nGauquelin's work was not limited to the Mars effect: his calculations led him first to reject most of the conventions of natal astrology as it is practised in the modern west but he singled out \"highly significant statistical correlations between planetary positions and the birth times of eminently successful people.\" This claim concerned not only Mars but five planets, correlated with eminence in fields broadly compatible with the traditional \"planetary rulerships\" of astrology. However, partly because eminence in sport is more quantifiable, later research, publicity and controversy has tended to single out the \"Mars effect\".\n\nIn 1956 Gauquelin invited the Belgian Comité Para to review his findings but it was not until 1962 that Jean Dath corroborated the statistics Gauquelin had presented and suggested an attempt at duplication using Belgian athletes. By this time Gauquelin had published \"Les Hommes et Les Astres\" (Men and the Stars, 1960), offering further data. The Comité Para tested the Mars effect in 1967 and replicated it, though most of the data (473 of 535) were still collected by Gauquelin himself. The committee, suspecting that the results might have been an artifact, withheld its findings for a further eight years, then cited unspecified “demographic errors” in its findings. Unpublished internal analyses contradicted this and one committee member, Luc de Marré, resigned in protest. In 1983 Abell, Kurtz and Zelen (\"see below\") published a reappraisal, rejecting the idea of demographic errors, saying, “Gauquelin adequately allowed for demographic and astronomical factors in predicting the expected distribution of Mars sectors for birth times in the general population.”\n\nIn 1975 Paul Kurtz's journal \"The Humanist\" published an article on astrology criticizing Gauquelin, to which the latter and his wife Françoise responded. Then Professor Marvin Zelen, a statistician and associate of the recently founded Committee for the Scientific Investigation of Claims of the Paranormal (CSICOP, now known as the Committee for Skeptical Inquiry (CSI)), proposed in a 1976 article in the same periodical that, in order to eliminate any demographic anomaly, Gauquelin randomly pick 100 athletes from his data-set of 2,088 and check the birth/planet correlations of a sample of babies born at the same times and places in order to establish a control group, giving the base-rate (chance) expectation for comparison (The 100 random athletes later expanded into a subsample of 303 athletes).\n\nIn April 1977 CSICOP researcher George O. Abell wrote to Kurtz stating that Zelen's test had come out in the Gauquelins' favour. The Gauquelins also performed the test that Professor Zelen had proposed and carried out and found that the chance Mars-in-key-sector expectation for the general population (i.e., non-champions) was about 17%, significantly less than the 22% observed for athletic champions. However the subsequent article by Zelen, Abell and Kurtz did not clearly state this outcome but rather questioned the original data. In a rebuttal of the Gauquelins' published conclusion, Marvin Zelen analysed the composition, not of the 17,000 non-champions of the control group, but of the 303 champions, splitting this secondary subsample (which was already nearly too small to test 22% vs 17%) by eliminating female athletes, a subgroup that gave the results most favourable to Gauquelin, and dividing the remaining athletes into city/rural sections and Parisian/non-Parisian sections.\n\nBefore and after publication of Zelen's results astronomer and charter CSICOP member Dennis Rawlins, the CSICOP Council's only astronomer at the time, repeatedly objected to the procedure and to CSICOP's subsequent reportage of it. Rawlins privately urged that the Gauquelins' results were valid and the “Zelen test” could only uphold this and that Zelen had diverted from the original purpose of the control test, which was to check the base rate of births with Mars in the \"key\" sectors. It appeared to him that the test had minimised the significance of the Mars/key-sector correlations with athletes by splitting the sample of athletes and that the experimenters, who were supposed to be upholding scientific standards, were actually distorting and manipulating evidence to conceal the result of an ill-considered test.\n\nThe Kurtz-Zelen-Abell analysis had split the sample primarily to examine the randomness of the 303 selected champions, the non-randomness of which Rawlins demonstrated in 1975 and 1977. Zelen's 1976 \"Challenge to Gauquelin\" had stated: \"We now have an objective way for unambiguous corroboration or disconfirmation ... to settle this question\", whereas this aim was now disputed. Rawlins made procedural objections, stating; \"... we find an inverse correlation between size and deviation in the Mars-athletes subsamples (that is, the smaller the subsample, the larger the success) – which is what one would expect if bias had infected the blocking off of the sizes of the subsamples\".\n\nCSICOP also contended, after reviewing the results, that the Gauquelins had not chosen randomly. They had had difficulty finding sufficient same-week and same-village births to compare with champions born in rural areas and so had chosen only champions born in larger cities. The Gauquelins' original total list of about 2,088 champions had included 42 Parisians and their subsample of 303 athletes also included 42 Parisians. Further, Paris is divided into 20 \"arrondissements\", different economic classes and ethnic groups typically inhabiting different arrondissements. The Gauquelins had compared the 42 Parisian champions (who had been born throughout Paris) to non-champions of only one arrondissement. If the 22% correlation was an artifact partly based on factors such as rural recordkeeping, economic, class or ethnic differences in birth patterns, this fact would be blurred by this non-random selection.\n\nAt the same time CSICOP began a study of U.S. athletes in consultation with Zelen, Abell and Rawlins. The results, published in 1979 showed a negative result. Gauquelin contended the KZA group demonstrated an overall preference for mediocre athletes and ignored his criteria of eminence and that they included basketball players and people born after 1950.\n\nIn 1994 the results of a major study undertaken by the Committee for the Study of Paranormal Phenomena (\"Comité pour l’Étude des Phénomènes Paranormaux\", or CFEPP) in France found no evidence whatsoever of a \"Mars Effect\" in the births of athletes. The study had been proposed in 1982 and the Committee had agreed in advance to use the protocol upon which Gauquelin insisted. The CFEPP report was “leaked” to the Dutch newspaper \"Trouw\".\n\nIn 1990 the CFEPP had issued a preliminary report on the study, which used 1,066 French sports champions, giving full data for the 1,066 as well as the names of 373 who fit the criteria but for whom birth times were unavailable, discussing methodology and listing data-selection criteria. In 1996 the report, with a commentary by J. W. Nienhuys and several letters from Gauquelin to the Committee, was published in book form as \"The Mars Effect – A French Test of Over 1,000 Sports Champions\". The CFEPP stated that its experiment showed no effect and concluded that the effect was attributable to bias in Gauquelin’s data selection, pointing to the suggestions made by Gauquelin to the Committee for changes in their list of athletes.\n\nSome researchers argued that Gauquelin did not adjust the statistical significance of the Mars Effect for multiple comparisons and did not address the issue in his publications. Simplified and illustrative showcase argument is explained here: There are 10 celestial bodies and 12 sectors for them to be in. Furthermore, there are 132 combinations of sector pairs and thus 1320 different combinations of a planet with two sectors. There is about a 25% chance to find at least one such combination (of one planet and two sectors) for a random dataset of the same size as Gauquelin’s that would yield a result with apparent statistical significance like the one obtained by Gauquelin. This implies that after adjusting for multiple comparisons, the Mars effect is no longer statistically significant even at the modest significance level of 0.05 and is probably a false positive.\n\nGeoffrey Dean has suggested that the effect may be caused by self reporting of birth dates by parents rather than any issue with the study by Gauquelin. Gauquelin had failed to find the Mars effect in populations after 1950. Dean has put forward the idea that this may be due to increases in doctors reporting the time of birth rather than parents. Information about misreporting was unavailable to Gauquelin at the time. Dean had said that misreporting by 3% of the sample would explain the result.\n\n\n"}
{"id": "283810", "url": "https://en.wikipedia.org/wiki?curid=283810", "title": "Mass spectrometry", "text": "Mass spectrometry\n\nMass spectrometry (MS) is an analytical technique that ionizes chemical species and sorts the ions based on their mass-to-charge ratio. In simpler terms, a mass spectrum measures the masses within a sample. Mass spectrometry is used in many different fields and is applied to pure samples as well as complex mixtures.\n\nA mass spectrum is a plot of the ion signal as a function of the mass-to-charge ratio. These spectra are used to determine the elemental or isotopic signature of a sample, the masses of particles and of molecules, and to elucidate the chemical structures of molecules and other chemical compounds.\n\nIn a typical MS procedure, a sample, which may be solid, liquid, or gas, is ionized, for example by bombarding it with electrons. This may cause some of the sample's molecules to break into charged fragments. These ions are then separated according to their mass-to-charge ratio, typically by accelerating them and subjecting them to an electric or magnetic field: ions of the same mass-to-charge ratio will undergo the same amount of deflection. The ions are detected by a mechanism capable of detecting charged particles, such as an electron multiplier. Results are displayed as spectra of the relative abundance of detected ions as a function of the mass-to-charge ratio. The atoms or molecules in the sample can be identified by correlating known masses (e.g. an entire molecule) to the identified masses or through a characteristic fragmentation pattern.\n\nIn 1886, Eugen Goldstein observed rays in gas discharges under low pressure that traveled away from the anode and through channels in a perforated cathode, opposite to the direction of negatively charged cathode rays (which travel from cathode to anode). Goldstein called these positively charged anode rays \"Kanalstrahlen\"; the standard translation of this term into English is \"canal rays\". Wilhelm Wien found that strong electric or magnetic fields deflected the canal rays and, in 1899, constructed a device with perpendicular electric and magnetic fields that separated the positive rays according to their charge-to-mass ratio (\"Q/m\"). Wien found that the charge-to-mass ratio depended on the nature of the gas in the discharge tube. English scientist J.J. Thomson later improved on the work of Wien by reducing the pressure to create the mass spectrograph.\nThe word \"spectrograph\" had become part of the international scientific vocabulary by 1884. Early \"spectrometry\" devices that measured the mass-to-charge ratio of ions were called \"mass spectrographs\" which consisted of instruments that recorded a spectrum of mass values on a photographic plate. A \"mass spectroscope\" is similar to a \"mass spectrograph\" except that the beam of ions is directed onto a phosphor screen. A mass spectroscope configuration was used in early instruments when it was desired that the effects of adjustments be quickly observed. Once the instrument was properly adjusted, a photographic plate was inserted and exposed. The term mass spectroscope continued to be used even though the direct illumination of a phosphor screen was replaced by indirect measurements with an oscilloscope. The use of the term \"mass spectroscopy\" is now discouraged due to the possibility of confusion with light spectroscopy. Mass spectrometry is often abbreviated as \"mass-spec\" or simply as \"MS\".\n\nModern techniques of mass spectrometry were devised by Arthur Jeffrey Dempster and F.W. Aston in 1918 and 1919 respectively.\n\nSector mass spectrometers known as calutrons were developed by Ernest O. Lawrence and used for separating the isotopes of uranium during the Manhattan Project. Calutron mass spectrometers were used for uranium enrichment at the Oak Ridge, Tennessee Y-12 plant established during World War II.\n\nIn 1989, half of the Nobel Prize in Physics was awarded to Hans Dehmelt and Wolfgang Paul for the development of the ion trap technique in the 1950s and 1960s.\n\nIn 2002, the Nobel Prize in Chemistry was awarded to John Bennett Fenn for the development of electrospray ionization (ESI) and Koichi Tanaka for the development of soft laser desorption (SLD) and their application to the ionization of biological macromolecules, especially proteins.\n\nA mass spectrometer consists of three components: an ion source, a mass analyzer, and a detector. The \"ionizer\" converts a portion of the sample into ions. There is a wide variety of ionization techniques, depending on the phase (solid, liquid, gas) of the sample and the efficiency of various ionization mechanisms for the unknown species. An extraction system removes ions from the sample, which are then targeted through the mass analyzer and into the \"detector\". The differences in masses of the fragments allows the mass analyzer to sort the ions by their mass-to-charge ratio. The detector measures the value of an indicator quantity and thus provides data for calculating the abundances of each ion present. Some detectors also give spatial information, e.g., a multichannel plate.\n\nThe following example describes the operation of a spectrometer mass analyzer, which is of the sector type. (Other analyzer types are treated below.) Consider a sample of sodium chloride (table salt). In the ion source, the sample is vaporized (turned into gas) and ionized (transformed into electrically charged particles) into sodium (Na) and chloride (Cl) ions. Sodium atoms and ions are monoisotopic, with a mass of about 23 u. Chloride atoms and ions come in two isotopes with masses of approximately 35 u (at a natural abundance of about 75 percent) and approximately 37 u (at a natural abundance of about 25 percent). The analyzer part of the spectrometer contains electric and magnetic fields, which exert forces on ions traveling through these fields. The speed of a charged particle may be increased or decreased while passing through the electric field, and its direction may be altered by the magnetic field. The magnitude of the deflection of the moving ion's trajectory depends on its mass-to-charge ratio. Lighter ions get deflected by the magnetic force more than heavier ions (based on Newton's second law of motion, \"F\" = \"ma\"). The streams of sorted ions pass from the analyzer to the detector, which records the relative abundance of each ion type. This information is used to determine the chemical element composition of the original sample (i.e. that both sodium and chlorine are present in the sample) and the isotopic composition of its constituents (the ratio of Cl to Cl).\n\nThe ion source is the part of the mass spectrometer that ionizes the material under analysis (the analyte). The ions are then transported by magnetic or electric fields to the mass analyzer.\n\nTechniques for ionization have been key to determining what types of samples can be analyzed by mass spectrometry.\nElectron ionization and chemical ionization are used for gases and vapors. In chemical ionization sources, the analyte is ionized by chemical ion-molecule reactions during collisions in the source. Two techniques often used with liquid and solid biological samples include electrospray ionization (invented by John Fenn) and matrix-assisted laser desorption/ionization (MALDI, initially developed as a similar technique \"Soft Laser Desorption (SLD)\" by K. Tanaka for which a Nobel Prize was awarded and as MALDI by M. Karas and F. Hillenkamp).\n\nIn mass spectrometry, ionization refers to the production of gas phase ions suitable for resolution in the mass analyser or mass filter. Ionization occurs in the ion source. There are several ion sources available; each has advantages and disadvantages for particular applications. For example, electron ionization (EI) gives a high degree of fragmentation, yielding highly detailed mass spectra which when skilfully analysed can provide important information for structural elucidation/characterisation and facilitate identification of unknown compounds by comparison to mass spectral libraries obtained under identical operating conditions. However, EI is not suitable for coupling to HPLC, i.e. LC-MS, since at atmospheric pressure, the filaments used to generate electrons burn out rapidly. Thus EI is coupled predominantly with GC, i.e. GC-MS, where the entire system is under high vacuum.\n\nHard ionization techniques are processes which impart high quantities of residual energy in the subject molecule invoking large degrees of fragmentation (i.e. the systematic rupturing of bonds acts to remove the excess energy, restoring stability to the resulting ion). Resultant ions tend to have \"m/z\" lower than the molecular mass (other than in the case of proton transfer and not including isotope peaks). The most common example of hard ionization is electron ionization (EI).\n\nSoft ionization refers to the processes which impart little residual energy onto the subject molecule and as such result in little fragmentation. Examples include fast atom bombardment (FAB), chemical ionization (CI), atmospheric-pressure chemical ionization (APCI), electrospray ionization (ESI), and matrix-assisted laser desorption/ionization (MALDI).\n\nInductively coupled plasma (ICP) sources are used primarily for cation analysis of a wide array of sample types. In this source, a plasma that is electrically neutral overall, but that has had a substantial fraction of its atoms ionized by high temperature, is used to atomize introduced sample molecules and to further strip the outer electrons from those atoms. The plasma is usually generated from argon gas, since the first ionization energy of argon atoms is higher than the first of any other elements except He, O, F and Ne, but lower than the second ionization energy of all except the most electropositive metals. The heating is achieved by a radio-frequency current passed through a coil surrounding the plasma.\n\nPhotoionization can be used in experiments which seek to use mass spectrometry as a means of resolving chemical kinetics mechanisms and isomeric product branching. In such instances a high energy photon, either X-ray or uv, is used to dissociate stable gaseous molecules in a carrier gas of He or Ar. In instances where a synchrotron light source is utilzed, a tuneable photon energy can be utilized to acquire a photoionization efficiency curve which can be used in conjunction with the charge ratio m/z to fingerprint molecular and ionic species.\n\nOthers include glow discharge, field desorption (FD), fast atom bombardment (FAB), thermospray, desorption/ionization on silicon (DIOS), Direct Analysis in Real Time (DART), atmospheric pressure chemical ionization (APCI), secondary ion mass spectrometry (SIMS), spark ionization and thermal ionization (TIMS).\n\nMass analyzers separate the ions according to their mass-to-charge ratio. The following two laws govern the dynamics of charged particles in electric and magnetic fields in vacuum:\n\nHere F is the force applied to the ion, \"m\" is the mass of the ion, a is the acceleration, \"Q\" is the ion charge, E is the electric field, and v × B is the vector cross product of the ion velocity and the magnetic field\n\nEquating the above expressions for the force applied to the ion yields:\n\nThis differential equation is the classic equation of motion for charged particles. Together with the particle's initial conditions, it completely determines the particle's motion in space and time in terms of \"m/Q\". Thus mass spectrometers could be thought of as \"mass-to-charge spectrometers\". When presenting data, it is common to use the (officially) dimensionless \"m/z\", where z is the number of elementary charges (\"e\") on the ion (z=Q/e). This quantity, although it is informally called the mass-to-charge ratio, more accurately speaking represents the ratio of the mass number and the charge number, \"z\".\n\nThere are many types of mass analyzers, using either static or dynamic fields, and magnetic or electric fields, but all operate according to the above differential equation. Each analyzer type has its strengths and weaknesses. Many mass spectrometers use two or more mass analyzers for tandem mass spectrometry (MS/MS). In addition to the more common mass analyzers listed below, there are others designed for special situations.\n\nThere are several important analyser characteristics. The mass resolving power is the measure of the ability to distinguish two peaks of slightly different \"m/z\". The mass accuracy is the ratio of the \"m/z\" measurement error to the true m/z. Mass accuracy is usually measured in ppm or milli mass units. The mass range is the range of \"m/z\" amenable to analysis by a given analyzer. The linear dynamic range is the range over which ion signal is linear with analyte concentration. Speed refers to the time frame of the experiment and ultimately is used to determine the number of spectra per unit time that can be generated.\n\nA sector field mass analyzer uses a static electric and/or magnetic field to affect the path and/or velocity of the charged particles in some way.\nAs shown above, sector instruments bend the trajectories of the ions as they pass through the mass analyzer, according to their mass-to-charge ratios, deflecting the more charged and faster-moving, lighter ions more. The analyzer can be used to select a narrow range of \"m/z\" or to scan through a range of \"m/z\" to catalog the ions present.\n\nThe time-of-flight (TOF) analyzer uses an electric field to accelerate the ions through the same potential, and then measures the time they take to reach the detector. If the particles all have the same charge, the kinetic energies tends to be identical, and their velocities, in this case, will depend only on their masses. Ions with a lower mass will reach the detector first. However, in reality, even particles with the same m/z can arrive at different times at the detector, because they have different initial velocities. The initial velocity is not dependent on the mass of the ion what becomes a problem for the TOF-MS. The difference in initial velocity turns into difference in the final velocity. In this way, ions with the same m/z are going to arrive at different times in the detector. For fixing this problem, time-lag focusing/delayed extraction has been coupled with TOF-MS.\n\nQuadrupole mass analyzers use oscillating electrical fields to selectively stabilize or destabilize the paths of ions passing through a radio frequency (RF) quadrupole field created between 4 parallel rods. Only the ions in a certain range of mass/charge ratio are passed through the system at any time, but changes to the potentials on the rods allow a wide range of m/z values to be swept rapidly, either continuously or in a succession of discrete hops. A quadrupole mass analyzer acts as a mass-selective filter and is closely related to the quadrupole ion trap, particularly the linear quadrupole ion trap except that it is designed to pass the untrapped ions rather than collect the trapped ones, and is for that reason referred to as a transmission quadrupole.\nA magnetically enhanced quadrupole mass analyzer includes the addition of a magnetic field, either applied axially or transversely. This novel type of instrument leads to an additional performance enhancement in terms of resolution and/or sensitivity depending upon the magnitude and orientation of the applied magnetic field. \nA common variation of the transmission quadrupole is the triple quadrupole mass spectrometer. The “triple quad” has three consecutive quadrupole stages, the first acting as a mass filter to transmit a particular incoming ion to the second quadrupole, a collision chamber, wherein that ion can be broken into fragments. The third quadrupole also acts as a mass filter, to transmit a particular fragment ion to the detector. If a quadrupole is made to rapidly and repetitively cycle through a range of mass filter settings, full spectra can be reported. Likewise, a triple quad can be made to perform various scan types characteristic of tandem mass spectrometry.\n\nThe quadrupole ion trap works on the same physical principles as the quadrupole mass analyzer, but the ions are trapped and sequentially ejected. Ions are trapped in a mainly quadrupole RF field, in a space defined by a ring electrode (usually connected to the main RF potential) between two endcap electrodes (typically connected to DC or auxiliary AC potentials). The sample is ionized either internally (e.g. with an electron or laser beam), or externally, in which case the ions are often introduced through an aperture in an endcap electrode.\n\nThere are many mass/charge separation and isolation methods but the most commonly used is the mass instability mode in which the RF potential is ramped so that the orbit of ions with a mass are stable while ions with mass \"b\" become unstable and are ejected on the \"z\"-axis onto a detector. There are also non-destructive analysis methods.\n\nIons may also be ejected by the resonance excitation method, whereby a supplemental oscillatory excitation voltage is applied to the endcap electrodes, and the trapping voltage amplitude and/or excitation voltage frequency is varied to bring ions into a resonance condition in order of their mass/charge ratio.\n\nThe cylindrical ion trap mass spectrometer (CIT) is a derivative of the quadrupole ion trap where the electrodes are formed from flat rings rather than hyperbolic shaped electrodes. The architecture lends itself well to miniaturization because as the size of a trap is reduced, the shape of the electric field near the center of the trap, the region where the ions are trapped, forms a shape similar to that of a hyperbolic trap.\n\nA linear quadrupole ion trap is similar to a quadrupole ion trap, but it traps ions in a two dimensional quadrupole field, instead of a three-dimensional quadrupole field as in a 3D quadrupole ion trap. Thermo Fisher's LTQ (\"linear trap quadrupole\") is an example of the linear ion trap.\n\nA toroidal ion trap can be visualized as a linear quadrupole curved around and connected at the ends or as a cross section of a 3D ion trap rotated on edge to form the toroid, donut shaped trap. The trap can store large volumes of ions by distributing them throughout the ring-like trap structure. This toroidal shaped trap is a configuration that allows the increased miniaturization of an ion trap mass analyzer. Additionally all ions are stored in the same trapping field and ejected together simplifying detection that can be complicated with array configurations due to variations in detector alignment and machining of the arrays.\n\nAs with the toroidal trap, linear traps and 3D quadrupole ion traps are the most commonly miniaturized mass analyzers due to their high sensitivity, tolerance for mTorr pressure, and capabilities for single analyzer tandem mass spectrometry (e.g. product ion scans).\n\nOrbitrap instruments are similar to Fourier transform ion cyclotron resonance mass spectrometers (see text below). Ions are electrostatically trapped in an orbit around a central, spindle shaped electrode. The electrode confines the ions so that they both orbit around the central electrode and oscillate back and forth along the central electrode's long axis. This oscillation generates an image current in the detector plates which is recorded by the instrument. The frequencies of these image currents depend on the mass-to-charge ratios of the ions. Mass spectra are obtained by Fourier transformation of the recorded image currents.\n\nOrbitraps have a high mass accuracy, high sensitivity and a good dynamic range.\n\nFourier transform mass spectrometry (FTMS), or more precisely Fourier transform ion cyclotron resonance MS, measures mass by detecting the image current produced by ions cyclotroning in the presence of a magnetic field. Instead of measuring the deflection of ions with a detector such as an electron multiplier, the ions are injected into a Penning trap (a static electric/magnetic ion trap) where they effectively form part of a circuit. Detectors at fixed positions in space measure the electrical signal of ions which pass near them over time, producing a periodic signal. Since the frequency of an ion's cycling is determined by its mass-to-charge ratio, this can be deconvoluted by performing a Fourier transform on the signal. FTMS has the advantage of high sensitivity (since each ion is \"counted\" more than once) and much higher resolution and thus precision.\n\nIon cyclotron resonance (ICR) is an older mass analysis technique similar to FTMS except that ions are detected with a traditional detector. Ions trapped in a Penning trap are excited by an RF electric field until they impact the wall of the trap, where the detector is located. Ions of different mass are resolved according to impact time.\n\nThe final element of the mass spectrometer is the detector. The detector records either the charge induced or the current produced when an ion passes by or hits a surface. In a scanning instrument, the signal produced in the detector during the course of the scan versus where the instrument is in the scan (at what \"m/Q\") will produce a mass spectrum, a record of ions as a function of \"m/Q\".\n\nTypically, some type of electron multiplier is used, though other detectors including Faraday cups and ion-to-photon detectors are also used. Because the number of ions leaving the mass analyzer at a particular instant is typically quite small, considerable amplification is often necessary to get a signal. Microchannel plate detectors are commonly used in modern commercial instruments. In FTMS and Orbitraps, the detector consists of a pair of metal surfaces within the mass analyzer/ion trap region which the ions only pass near as they oscillate. No direct current is produced, only a weak AC image current is produced in a circuit between the electrodes. Other inductive detectors have also been used.\n\nA tandem mass spectrometer is one capable of multiple rounds of mass spectrometry, usually separated by some form of molecule fragmentation. For example, one mass analyzer can isolate one peptide from many entering a mass spectrometer. A second mass analyzer then stabilizes the peptide ions while they collide with a gas, causing them to fragment by collision-induced dissociation (CID). A third mass analyzer then sorts the fragments produced from the peptides. Tandem MS can also be done in a single mass analyzer over time, as in a quadrupole ion trap. There are various methods for fragmenting molecules for tandem MS, including collision-induced dissociation (CID), electron capture dissociation (ECD), electron transfer dissociation (ETD), infrared multiphoton dissociation (IRMPD), blackbody infrared radiative dissociation (BIRD), electron-detachment dissociation (EDD) and surface-induced dissociation (SID). An important application using tandem mass spectrometry is in protein identification.\n\nTandem mass spectrometry enables a variety of experimental sequences. Many commercial mass spectrometers are designed to expedite the execution of such routine sequences as selected reaction monitoring (SRM) and precursor ion scanning. In SRM, the first analyzer allows only a single mass through and the second analyzer monitors for multiple user-defined fragment ions. SRM is most often used with scanning instruments where the second mass analysis event is duty cycle limited. These experiments are used to increase specificity of detection of known molecules, notably in pharmacokinetic studies. Precursor ion scanning refers to monitoring for a specific loss from the precursor ion. The first and second mass analyzers scan across the spectrum as partitioned by a user-defined \"m/z\" value. This experiment is used to detect specific motifs within unknown molecules.\n\nAnother type of tandem mass spectrometry used for radiocarbon dating is accelerator mass spectrometry (AMS), which uses very high voltages, usually in the mega-volt range, to accelerate negative ions into a type of tandem mass spectrometer.\n\nWhen a specific combination of source, analyzer, and detector becomes conventional in practice, a compound acronym may arise to designate it succinctly. One example is MALDI-TOF, which refers to a combination of a matrix-assisted laser desorption/ionization source with a time-of-flight mass analyzer. Other examples include inductively coupled plasma-mass spectrometry (ICP-MS), accelerator mass spectrometry (AMS), thermal ionization-mass spectrometry (TIMS) and spark source mass spectrometry (SSMS).\n\nCertain applications of mass spectrometry have developed monikers that although strictly speaking would seem to refer to a broad application, in practice have come instead to connote a specific or a limited number of instrument configurations. An example of this is isotope ratio mass spectrometry (IRMS), which refers in practice to the use of a limited number of sector based mass analyzers; this name is used to refer to both the application and the instrument used for the application.\n\nAn important enhancement to the mass resolving and mass determining capabilities of mass spectrometry is using it in tandem with chromatographic and other separation techniques.\n\nA common combination is gas chromatography-mass spectrometry (GC/MS or GC-MS). In this technique, a gas chromatograph is used to separate different compounds. This stream of separated compounds is fed online into the ion source, a metallic filament to which voltage is applied. This filament emits electrons which ionize the compounds. The ions can then further fragment, yielding predictable patterns. Intact ions and fragments pass into the mass spectrometer's analyzer and are eventually detected.\n\nSimilar to gas chromatography MS (GC-MS), liquid chromatography-mass spectrometry (LC/MS or LC-MS) separates compounds chromatographically before they are introduced to the ion source and mass spectrometer. It differs from GC-MS in that the mobile phase is liquid, usually a mixture of water and organic solvents, instead of gas. Most commonly, an electrospray ionization source is used in LC-MS. Other popular and commercially available LC-MS ion sources are atmospheric pressure chemical ionization and atmospheric pressure photoionization. There are also some newly developed ionization techniques like laser spray.\n\nCapillary electrophoresis–mass spectrometry (CE-MS) is a technique that combines the liquid separation process of capillary electrophoresis with mass spectrometry. CE-MS is typically coupled to electrospray ionization.\n\nIon mobility spectrometry-mass spectrometry (IMS/MS or IMMS) is a technique where ions are first separated by drift time through some neutral gas under an applied electrical potential gradient before being introduced into a mass spectrometer. Drift time is a measure of the radius relative to the charge of the ion. The duty cycle of IMS (the time over which the experiment takes place) is longer than most mass spectrometric techniques, such that the mass spectrometer can sample along the course of the IMS separation. This produces data about the IMS separation and the mass-to-charge ratio of the ions in a manner similar to LC-MS.\n\nThe duty cycle of IMS is short relative to liquid chromatography or gas chromatography separations and can thus be coupled to such techniques, producing triple modalities such as LC/IMS/MS.\n\nMass spectrometry produces various types of data. The most common data representation is the mass spectrum.\n\nCertain types of mass spectrometry data are best represented as a mass chromatogram. Types of chromatograms include selected ion monitoring (SIM), total ion current (TIC), and selected reaction monitoring (SRM), among many others.\n\nOther types of mass spectrometry data are well represented as a three-dimensional contour map. In this form, the mass-to-charge, \"m/z\" is on the \"x\"-axis, intensity the \"y\"-axis, and an additional experimental parameter, such as time, is recorded on the \"z\"-axis.\n\nMass spectrometry data analysis is specific to the type of experiment producing the data. General subdivisions of data are fundamental to understanding any data.\n\nMany mass spectrometers work in either \"negative ion mode\" or \"positive ion mode\". It is very important to know whether the observed ions are negatively or positively charged. This is often important in determining the neutral mass but it also indicates something about the nature of the molecules.\n\nDifferent types of ion source result in different arrays of fragments produced from the original molecules. An electron ionization source produces many fragments and mostly single-charged (1-) radicals (odd number of electrons), whereas an electrospray source usually produces non-radical quasimolecular ions that are frequently multiply charged. Tandem mass spectrometry purposely produces fragment ions post-source and can drastically change the sort of data achieved by an experiment.\n\nKnowledge of the origin of a sample can provide insight into the component molecules of the sample and their fragmentations. A sample from a synthesis/manufacturing process will probably contain impurities chemically related to the target component. A crudely prepared biological sample will probably contain a certain amount of salt, which may form adducts with the analyte molecules in certain analyses.\n\nResults can also depend heavily on sample preparation and how it was run/introduced. An important example is the issue of which matrix is used for MALDI spotting, since much of the energetics of the desorption/ionization event is controlled by the matrix rather than the laser power. Sometimes samples are spiked with sodium or another ion-carrying species to produce adducts rather than a protonated species.\n\nMass spectrometry can measure molar mass, molecular structure, and sample purity. Each of these questions requires a different experimental procedure; therefore, adequate definition of the experimental goal is a prerequisite for collecting the proper data and successfully interpreting it.\n\nSince the precise structure or peptide sequence of a molecule is deciphered through the set of fragment masses, the interpretation of mass spectra requires combined use of various techniques. Usually the first strategy for identifying an unknown compound is to compare its experimental mass spectrum against a library of mass spectra. If no matches result from the search, then manual interpretation or software assisted interpretation of mass spectra must be performed. Computer simulation of ionization and fragmentation processes occurring in mass spectrometer is the primary tool for assigning structure or peptide sequence to a molecule. An \"a priori\" structural information is fragmented \"in silico\" and the resulting pattern is compared with observed spectrum. Such simulation is often supported by a fragmentation library that contains published patterns of known decomposition reactions. Software taking advantage of this idea has been developed for both small molecules and proteins.\n\nAnalysis of mass spectra can also be spectra with accurate mass. A mass-to-charge ratio value (\"m/z\") with only integer precision can represent an immense number of theoretically possible ion structures; however, more precise mass figures significantly reduce the number of candidate molecular formulas. A computer algorithm called formula generator calculates all molecular formulas that theoretically fit a given mass with specified tolerance.\n\nA recent technique for structure elucidation in mass spectrometry, called precursor ion fingerprinting, identifies individual pieces of structural information by conducting a search of the tandem spectra of the molecule under investigation against a library of the product-ion spectra of structurally characterized precursor ions.\n\nMass spectrometry has both qualitative and quantitative uses. These include identifying unknown compounds, determining the isotopic composition of elements in a molecule, and determining the structure of a compound by observing its fragmentation. Other uses include quantifying the amount of a compound in a sample or studying the fundamentals of gas phase ion chemistry (the chemistry of ions and neutrals in a vacuum). MS is now commonly used in analytical laboratories that study physical, chemical, or biological properties of a great variety of compounds.\n\nAs an analytical technique it possesses distinct advantages such as: Increased sensitivity over most other analytical techniques because the analyzer, as a mass-charge filter, reduces background interference, Excellent specificity from characteristic fragmentation patterns to identify unknowns or confirm the presence of suspected compounds, Information about molecular weight, Information about the isotopic abundance of elements, Temporally resolved chemical data.\n\nA few of the disadvantages of the method is that it often fails to distinguish between optical and geometrical isomers and the positions of substituent in o-, m- and p- positions in an aromatic ring. Also, its scope is limited in identifying hydrocarbons that produce similar fragmented ions.\n\nMass spectrometry is also used to determine the isotopic composition of elements within a sample. Differences in mass among isotopes of an element are very small, and the less abundant isotopes of an element are typically very rare, so a very sensitive instrument is required. These instruments, sometimes referred to as isotope ratio mass spectrometers (IR-MS), usually use a single magnet to bend a beam of ionized particles towards a series of Faraday cups which convert particle impacts to electric current. A fast on-line analysis of deuterium content of water can be done using flowing afterglow mass spectrometry, FA-MS. Probably the most sensitive and accurate mass spectrometer for this purpose is the accelerator mass spectrometer (AMS). This is because it provides ultimate sensitivity, capable of measuring individual atoms and measuring nuclides with a dynamic range of ~10 relative to the major stable isotope. Isotope ratios are important markers of a variety of processes. Some isotope ratios are used to determine the age of materials for example as in carbon dating. Labeling with stable isotopes is also used for protein quantification. (see protein characterization below)\n\nMembrane-inlet Mass Spectrometry, combines the Isotope ratio MS with a reaction chamber/cell separated by a gas-permeable membrane. This method allows the study of gasses as they evolve in solution. This method has been extensively used for the study of the production of oxygen by Photosystem II.\n\nSeveral techniques use ions created in a dedicated ion source injected into a flow tube or a drift tube: selected ion flow tube (SIFT-MS), and proton transfer reaction (PTR-MS), are variants of chemical ionization dedicated for trace gas analysis of air, breath or liquid headspace using well defined reaction time allowing calculations of analyte concentrations from the known reaction kinetics without the need for internal standard or calibration.\n\nAn atom probe is an instrument that combines time-of-flight mass spectrometry and field-evaporation microscopy to map the location of individual atoms.\n\nPharmacokinetics is often studied using mass spectrometry because of the complex nature of the matrix (often blood or urine) and the need for high sensitivity to observe low dose and long time point data. The most common instrumentation used in this application is LC-MS with a triple quadrupole mass spectrometer. Tandem mass spectrometry is usually employed for added specificity. Standard curves and internal standards are used for quantitation of usually a single pharmaceutical in the samples. The samples represent different time points as a pharmaceutical is administered and then metabolized or cleared from the body. Blank or t=0 samples taken before administration are important in determining background and ensuring data integrity with such complex sample matrices. Much attention is paid to the linearity of the standard curve; however it is not uncommon to use curve fitting with more complex functions such as quadratics since the response of most mass spectrometers is less than linear across large concentration ranges.\n\nThere is currently considerable interest in the use of very high sensitivity mass spectrometry for microdosing studies, which are seen as a promising alternative to animal experimentation.\n\nMass spectrometry is an important method for the characterization and sequencing of proteins. The two primary methods for ionization of whole proteins are electrospray ionization (ESI) and matrix-assisted laser desorption/ionization (MALDI). In keeping with the performance and mass range of available mass spectrometers, two approaches are used for characterizing proteins. In the first, intact proteins are ionized by either of the two techniques described above, and then introduced to a mass analyzer. This approach is referred to as \"top-down\" strategy of protein analysis. The top-down approach however is largely limited to low-throughput single-protein studies. In the second, proteins are enzymatically digested into smaller peptides using proteases such as trypsin or pepsin, either in solution or in gel after electrophoretic separation. Other proteolytic agents are also used. The collection of peptide products are then introduced to the mass analyzer. When the characteristic pattern of peptides is used for the identification of the protein the method is called peptide mass fingerprinting (PMF), if the identification is performed using the sequence data determined in tandem MS analysis it is called de novo peptide sequencing. These procedures of protein analysis are also referred to as the \"bottom-up\" approach. A third approach however is beginning to be used, this intermediate \"middle-down\" approach involves analyzing proteolytic peptide larger than the typical tryptic peptide.\n\nMass spectrometry (MS), with its low sample requirement and high sensitivity, has been predominantly used in glycobiology for characterization and elucidation of glycan structures. Mass spectrometry provides a complementary method to HPLC for the analysis of glycans. Intact glycans may be detected directly as singly charged ions by matrix-assisted laser desorption/ionization mass spectrometry (MALDI-MS) or, following permethylation or peracetylation, by fast atom bombardment mass spectrometry (FAB-MS). Electrospray ionization mass spectrometry (ESI-MS) also gives good signals for the smaller glycans. Various free and commercial software are now available which interpret MS data and aid in Glycan structure characterization.\n\nAs a standard method for analysis, mass spectrometers have reached other planets and moons. Two were taken to Mars by the Viking program. In early 2005 the Cassini–Huygens mission delivered a specialized GC-MS instrument aboard the Huygens probe through the atmosphere of Titan, the largest moon of the planet Saturn. This instrument analyzed atmospheric samples along its descent trajectory and was able to vaporize and analyze samples of Titan's frozen, hydrocarbon covered surface once the probe had landed. These measurements compare the abundance of isotope(s) of each particle comparatively to earth's natural abundance. Also on board the Cassini–Huygens spacecraft was an ion and neutral mass spectrometer which had been taking measurements of Titan's atmospheric composition as well as the composition of Enceladus' plumes. A Thermal and Evolved Gas Analyzer mass spectrometer was carried by the Mars Phoenix Lander launched in 2007.\n\nMass spectrometers are also widely used in space missions to measure the composition of plasmas. For example, the Cassini spacecraft carried the Cassini Plasma Spectrometer (CAPS), which measured the mass of ions in Saturn's magnetosphere.\n\nMass spectrometers were used in hospitals for respiratory gas analysis beginning around 1975 through the end of the century. Some are probably still in use but none are currently being manufactured.\n\nFound mostly in the operating room, they were a part of a complex system, in which respired gas samples from patients undergoing anesthesia were drawn into the instrument through a valve mechanism designed to sequentially connect up to 32 rooms to the mass spectrometer. A computer directed all operations of the system. The data collected from the mass spectrometer was delivered to the individual rooms for the anesthesiologist to use.\n\nThe uniqueness of this magnetic sector mass spectrometer may have been the fact that a plane of detectors, each purposely positioned to collect all of the ion species expected to be in the samples, allowed the instrument to simultaneously report all of the gases respired by the patient. Although the mass range was limited to slightly over 120 u, fragmentation of some of the heavier molecules negated the need for a higher detection limit.\n\nThe primary function of mass spectrometry is as a tool for chemical analyses based on detection and quantification of ions according to their mass-to-charge ratio. However, mass spectrometry also shows promise for material synthesis. Ion soft landing is characterized by deposition of intact species on surfaces at low kinetic energies which precludes the fragmentation of the incident species. The soft landing technique was first reported in 1977 for the reaction of low energy sulfur containing ions on a lead surface.\n\n"}
{"id": "236722", "url": "https://en.wikipedia.org/wiki?curid=236722", "title": "Master of Science", "text": "Master of Science\n\nA Master of Science (; abbreviated MS, M.S., MSc, M.Sc., SM, S.M., ScM or Sc.M.) is a master's degree in the field of science awarded by universities in many countries or a person holding such a degree. In contrast to the Master of Arts degree, the Master of Science degree is typically granted for studies in sciences, engineering and medicine and is usually for programs that are more focused on scientific and mathematical subjects; however, different universities have different conventions and may also offer the degree for fields typically considered within the humanities and social sciences. While it ultimately depends upon the specific program, earning a Master of Science degree typically includes writing a thesis.\n\nAlgeria follows the Bologna Process.\n\nIn Argentina, Brazil, Ecuador, Mexico, Colombia, Panamá, Perú and Uruguay, the Master of Science or Magister is a postgraduate degree of two to four years of duration. The admission to a Master's program (Spanish: \"Maestría\"; Portuguese: \"Mestrado\") requires the full completion of a four to five years long undergraduate degree, bachelor's degree or a Licentiate's degree of the same length. Defense of a research thesis is required. All master's degrees qualify for a doctorate program.\n\nAustralian universities commonly have coursework or research-based Master of Science courses for graduate students. They typically run for 1–2 years full-time, with varying amounts of research involved.\n\nIn Bangladesh, all universities, including Bangladesh Agricultural University Jagannath University, Dhaka University, University of Chittagong, Jahangirnagar University, Islamic University, Bangladesh and Rajshahi University have Master of Science courses as postgraduate degrees. After passing Bachelor of Science, any student becomes eligible to study in this discipline.\n\nIn Canada, Master of Science (MSc) degrees may be entirely course-based, entirely research-based or (more typically) a mixture. Master's programs typically take one to three years to complete and the completion of a scientific thesis is often required. Admission to a master's program is contingent upon holding a four-year university bachelor's degree. Some universities require a master's degree in order to progress to a doctoral program (PhD).\n\nIn the province of Quebec, the Master of Science follows the same principles as in the rest of Canada. There is one exception, however, regarding admission to a master's program. Since Québécois students complete two to three years of college before entering university, they have the opportunity to complete a bachelor's degree in three years instead of four. Some undergraduate degrees such as the Bachelor of Education and the Bachelor of Engineering requires four years of studies. Following the obtention of their bachelor's degree, students can be admitted into a graduate program to eventually obtain a master's degree.\n\nCommonly the Chilean universities have used \"Magíster\" for a master degree, but other than that is similar to the rest of South America.\n\nLike all EU member states, the Czech Republic and Slovakia follow the Bologna Process. The Czech Republic and Slovakia are using two master's degree systems. Both award a title of \"Mgr.\" or \"Ing.\" to be used before the name.\nThe older system requires a 5-year program. The new system takes only 2 years but requires a previously completed 3-year bachelor program (a \"Bc.\" title). It is required to write a thesis (in both master and bachelor program) and also to pass final exams. It is mostly the case that the final exams cover the main study areas of the whole study program, i.e. a student is required to prove his/her knowledge in many subjects he attended during the 2 resp. 3 years.\n\nThe Master of Science (M.Sc.) is an academic degree for a post-graduate candidates or researchers, it usually takes from 4 to 7 years after passing the Bachelor of Science (B.Sc.) degree. Master programs are awarded in many sciences in the Egyptian Universities. A completion of the degree requires finishing a pre-master studies followed by a scientific thesis or research. All M.Sc. degree holders are allowable to take a step forward in the academic track to get the PhD degree.\n\nLike all EU member states, Finland follows the Bologna Process. The Master of Science (M.Sc.) academic degree usually follows the Bachelor of Science (B.Sc.) studies which typically last five years. For the completion of both the bachelor and the master studies the student must accumulate a total of 300 ECTS credits, thus most Masters programs are two-year programs with 120 credits. The completion of a scientific thesis is required.\n\nLike all EU member states, Germany follows the Bologna Process. The Master of Science (M.Sc.) academic degree replaces the once common \"Diplom\" or \"Magister\" programs that typically lasted four to five years. It is awarded in science related studies with a high percentage of mathematics. For the completion the student must accumulate 300 ECTS Credits, thus most Masters programs are two-year programs with 120 credits. The completion of a scientific thesis is required.\n\nIn Slavic countries in European southeast (particularly former Yugoslavian republics), the education system was largely based on the German university system (largely due to the presence and influence of the Austria-Hungary Empire in the region). Prior to the implementation of the Bologna Process, academic university studies comprised a 4-5 year long graduate \"Diplom\" program, which could have been followed by a 2-4 year long \"Magister\" program and then later with 2-4 year long \"Doctoral studies\".\n\nAfter the Bologna Process implementation, again based on the German implementation, \"Diplom\" titles and programs were replaced by the M.Sc. and M.A. programs (depending on the field of study). The studies are structured such that a \"Master\" program lasts long enough for the student to accumulate a total of 300 ECTS credits, so its duration would depend on a number of credits acquired during the Bachelor studies. Pre-Bologna \"Magister\" programs were abandoned - after earning an M.Sc/M.A. degree and satisfying other academic requirements a student could proceed to earn a Doctor of Philosophy degree directly.\n\nIn Guyana, all universities, including University of Guyana, Texila American University, American International School of Medicine have Master of Science courses as postgraduate degrees. Students who have completed undergraduate Bachelor of Science degree are eligible to study in this discipline\n\nIn Iran, similar to Canada, Master of Science (MSc) or in Iranian form \"Karshenasi-arshad\" degrees may be entirely course-based, entirely research-based or sometimes a mixture. Master's programs typically take two to three years to complete and the completion of a scientific thesis is often required.\n\nLike all EU member states, Ireland follows the Bologna Process. In Ireland, Master of Science (MSc) may be course-based with a research component or entirely research based. The program is most commonly a one-year program and a thesis is required for both course-based and research based degrees.\n\nIn Israel, Master of Science (MSc) may be entirely course-based or include research. The program is most commonly a two-year program and a thesis is required only for research based degrees.\n\nIn India, universities offer MSc programs usually in science subjects. Generally speaking, in India, post-graduate scientific courses lead to MSc degree while post-graduate engineering courses lead to ME or MTech degree. For example, a master's in automotive engineering would normally be an ME or MTech, while a master's in physics would be an MSc. A few top universities also offer undergraduate programs leading to a master's degree which is known as integrated masters.\n\nA Master of Science in Engineering (M.Sc.Eng.) degree is also offered in India. It is usually structured as an engineering research degree, lesser than Ph.D. and considered to be parallel to M.Phil. degree in humanities and science. Some institutes such as IITs offer an MS degree for postgraduate engineering courses. This degree is considered a research-oriented degree where as MTech or ME degree is usually not a research degree in India. M.S. degree is also awarded by various IISERs which are one of the top institutes in India.\n\nLike all EU member states, Italy follows the Bologna Process. The degree \"Master of Science\" is awarded in the Italian form, \"Laurea Magistrale\" (formerly \"Laurea specialistica\"; before the introduction of the \"Laurea\" the corresponding degree was \"Laurea quinquennale\" or \"Vecchio Ordinamento\").\n\nIn Nepal, universities offer the master of science degree usually in science and engineering areas. Tribhuvan University offers MSc degree for all the science and engineering courses. Pokhara University offers ME for engineering and MSc for science. Kathmandu University offers MS by Research and ME degrees for science and engineering.\n\nLike all EU member states, the Netherlands follows the Bologna Process. A graduate who is awarded the title Master of Science (abbreviated as MSc) may still use the previously awarded Dutch title \"ingenieur\" (abbreviated as ir.) (for graduates who followed a technical or agricultural program), \"meester\" (abbreviated as mr.) (for graduates who followed an LLM law program) or \"doctorandus\" (abbreviated as drs.)(in all other cases).\n\nNew Zealand universities commonly have coursework or research-based Master of Science courses for graduate students. They typically run for 2 years full-time, with varying amounts of research involved.\n\nNorway follows the Bologna Process. For engineering, the Master of Science academic degree has been recently introduced and has replaced the previous award forms \"Sivilingeniør\" (engineer, a.k.a. engineering master) and \"Hovedfag\" (academic master). Both were awarded after 5 years university-level studies and required the completion of a scientific thesis.\n\n\"Siv.ing\", is a protected title exclusively awarded to engineering students who completed a five-year education at The Norwegian University of Science and Technology (, NTNU) or other universities. Historically there was no bachelor's degree involved and today's program is a five years master's degree education. The \"Siv.ing\" title is in the process of being phased out, replaced by (for now, complemented by) the \"M.Sc.\" title. By and large, \"Siv.ing\" is a title tightly being held on to for the sake of tradition. In academia, the new program offers separate three-year bachelor and two-year master programs. It is awarded in the natural sciences, mathematics and computer science fields. The completion of a scientific thesis is required. All master's degrees are designed to certify a level of education and qualify for a doctorate program.\n\nMaster of Science in Business is the English title for those taking a higher business degree, \"Siviløkonom\" in Norwegian. In addition, there is, for example, the 'Master of Business Administration' (MBA), a practically oriented master's degree in business, but with less mathematics and econometrics, due to its less specific entry requirements and smaller focus on research.\n\nPakistan inherited its conventions pertaining to higher education from United Kingdom after independence in 1947. Master of Science degree is typically abbreviated as M.Sc. (as in the United Kingdom) and which is awarded after 16 years of education (equivalent with a bachelor's degree in the USA and many other countries). Recently, in pursuance to some of the reforms by the Higher Education Commission of Pakistan (the regulatory body of higher education in Pakistan), the traditional 2-year Bachelor of Science (B.Sc.) degree has been replaced by the 4-year Bachelor of Science degree, which is abbreviated as B.S. to enable the Pakistani degrees with the rest of the world. Subsequently, students who pass 4-year B.S. degree that is awarded after 16 years of education are then eligible to apply for M.S. degree, which is considered at par with Master of Philosophy (M.Phil.) degree.\n\nLike all EU member states, Poland follows the Bologna Process. The Polish equivalent of Master of Science is \"magister\" (abbreviated \"mgr\", written pre-nominally much like \"Dr\"). Starting in 2001, the MSc programs typically lasting 5 years began to be replaced as below:\nThe degree is awarded predominantly in the natural sciences, mathematics, computer science, economics, as well as in the arts and other disciplines. Those who graduate from an engineering program prior to being awarded a master's degree are allowed to use the \"mgr inż.\" pre-nominal (\"master engineer\"). This is most common in engineering and agricultural fields of study. Defense of a research thesis is required. All master's degrees in Poland qualify for a doctorate program.\n\nThe title of \"master\" was introduced by Alexander I at 24 January 1803. The Master had an intermediate position between the candidate and doctor according to the decree \"About colleges structure\". The master's degree was abolished from 1917 to 1934. Russia follows the Bologna Process for higher education in Europe since 2011.\n\nLike all EU member states, Spain follows the Bologna Process. The Master of Science (MSc) degree is a program officially recognized by the Spanish Ministry of Education. It usually involves 1 or 2 years of full-time study. It is targeted at pre-experience candidates who have recently finished their undergraduate studies. An MSc degree can be awarded in every field of study. An MSc degree is required in order to progress to a PhD. MSci, MPhil and DEA are equivalent in Spain.\n\nLike all EU member states, Sweden follows the Bologna Process. The Master of Science academic degree has, like in Germany, recently been introduced in Sweden. Students studying Master of Science in Engineering programs are rewarded both the English Master of Science Degree, but also the Swedish equivalent \"Teknologisk masterexamen\". Whilst \"Civilingenjör\" is an at least five year long education .\n\nThe Master of Science is a degree that can be studied only in public universities. The program is usually 2 years, but it can be extended to 3 or 4 years, the student is required to pass a specific bachelor's degree to attend a specific master of science degree program, the master of science is mostly a research master (except for some types of programs held with cooperation of foreign universities), The student should attend some courses in the first year of the master then he/she should prepare a research thesis. Publishing two research papers is recommended and will increase the final evaluation grade.\n\nThe Master of Science (MSc) is typically a taught postgraduate degree, involving lectures, examinations and a project dissertation (normally taking up a third of the program). Master's programs usually involve a minimum of 1 year of full-time study (180 UK credits, of which 150 must be at master's level) and sometimes up to 2 years of full-time study (or the equivalent period part-time). Taught master's degrees are normally classified into Pass, Merit and Distinction (although some universities do not give Merit). Some universities also offer MSc by research programs, where a longer project or set of projects is undertaken full-time; master's degrees by research are normally pass/fail, although some universities may offer a distinction.\n\nThe more recent Master \"in\" Science (MSci or M.Sci.) degree (Master of Natural Science at the University of Cambridge), is an undergraduate (UG) level integrated master's degree offered by UK institutions since the 1990s. It is offered as a first degree with the first three (four in Scotland) years similar to a BSc course and a final year (120 UK credits) at master's level, including a dissertation. The final MSci qualification is thus at the same level as a traditional MSc.\n\nThe \"Master of Science\" (\"Magister Scientiæ\") degree is normally a full-time two-year degree often abbreviated \"MS\" or \"M.S.\" It is the primary type in most subjects and may be entirely course-based, entirely research-based or (more typically) a combination of the two. The combination often involves writing and defending a thesis or completing a research project which represents the culmination of the material learned.\n\nAdmission to a master's program is normally contingent upon holding a bachelor's degree and progressing to a doctoral program may require a master's degree. In some fields or graduate programs, work on a doctorate can begin immediately after the bachelor's degree. Some programs provide for a joint bachelor's and master's degree after about five years. Some universities use the Latin degree names and due to the flexibility of word order in Latin, \"Artium Magister\" (A.M.) or \"Scientiæ Magister\" (S.M. or Sc.M.) may be used in some institutions.\n\n"}
{"id": "26833159", "url": "https://en.wikipedia.org/wiki?curid=26833159", "title": "Meker–Fisher burner", "text": "Meker–Fisher burner\n\nA Meker–Fisher burner, or Meker burner, is a laboratory burner that produces multiple open gas flames, used for heating, sterilization, and combustion. It is used when laboratory work requires a hotter flame than attainable using a Bunsen burner, or used when a larger-diameter flame is desired, such as with an inoculation loop or in some glassblowing operations. The burner was introduced by French chemist Georges Méker in an article published in 1905.\n\nThe Meker–Fisher burner heat output can be in excess of per hour (about 3.5 kW) using LP gas. Flame temperatures of up to are achievable. Compared with a Bunsen burner, the lower part of its tube has more openings with larger total cross-section, admitting more air and facilitating better mixing of air and gas. The tube is wider, and its top is covered with a plate mesh, which separates the flame into an array of smaller flames with a common external envelope, ensures uniform heating, and also preventing flashback to the bottom of the tube, which is a risk at high air-to-fuel ratios and limits the maximal rate of air intake in a Bunsen burner. The flame burns without noise, unlike the Bunsen or Teclu burners.\n\n"}
{"id": "50529615", "url": "https://en.wikipedia.org/wiki?curid=50529615", "title": "Merodiploid", "text": "Merodiploid\n\nA Merodiploid is a partially diploid bacterium, which has its own chromosome complement and a chromosome fragment introduced by conjugation, transformation or transduction. It can also be defined as an essentially haploid organism that carries a second copy of a part of its genome. The term is derived from the Greek, meros = part, and was originally used to describe both unstable partial diploidy, such as that which occurs briefly in recipients after mating with an Hfr strain (1), and the stable state, exemplified by F-prime strains (see Hfr'S And F-Primes). Over time the usage has tended to confine the term to descriptions of stable genetic states. Merodiploidy refers to the partial duplication of chromosomes in a haploid organism.\n"}
{"id": "38890", "url": "https://en.wikipedia.org/wiki?curid=38890", "title": "Natural science", "text": "Natural science\n\nNatural science is a branch of science concerned with the description, prediction, and understanding of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.\n\nNatural science can be divided into two main branches: life science (or biological science) and physical science. Physical science is subdivided into branches, including physics, chemistry, astronomy and earth science. These branches of natural science may be further divided into more specialized branches (also known as fields).\n\nIn Western society's analytic tradition, the empirical sciences and especially natural sciences use tools from formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the \"laws of nature\". The social sciences also use such methods, but rely more on qualitative research, so that they are sometimes called \"soft science\", whereas natural sciences, insofar as they emphasize quantifiable data produced, tested, and confirmed through the scientific method, are sometimes called \"hard science\".\n\nModern natural science succeeded more classical approaches to natural philosophy, usually traced to ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, \"natural history\" suggests observational descriptions aimed at popular audiences.\n\nPhilosophers of science have suggested a number of criteria, including Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from non-scientific ones. Validity, accuracy, and quality control, such as peer review and repeatability of findings, are amongst the most respected criteria in the present-day global scientific community.\n\nThis field encompasses a set of disciplines that examines phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment.\n\nThe biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole.\n\nSome key developments in biology were the discovery of genetics; evolution through natural selection; the germ theory of disease and the application of the techniques of chemistry and physics at the level of the cell or organic molecule.\n\nModern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, anatomy and physiology looks at the internal structures, and their functions, of an organism, while ecology looks at how various organisms interrelate.\n\nConstituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications.\n\nMost chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences.\n\nEarly experiments in chemistry had their roots in the system of Alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gas, and Antoine Lavoisier, who developed the theory of the Conservation of mass.\n\nThe discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.\n\nPhysics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. In general, physics is regarded as the fundamental science, because all other natural sciences use and obey the principles and laws set down by the field. Physics relies heavily on mathematics as the logical framework for formulation and quantification of principles.\n\nThe study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics.\n\nThe field of physics is extremely broad, and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein and Lev Landau, who worked in multiple areas.\n\nThis discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, and motion of celestial objects, as well as the formation and development of the universe.\n\nAstronomy includes the examination, study and modeling of stars, planets, comets, galaxies and the cosmos. Most of the information used by astronomers is gathered by remote observation, although some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium).\n\nWhile the origins of the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail.\n\nThe mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation, although it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into a formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.\n\nEarth science (also known as geoscience), is an all-embracing term for the sciences related to the planet Earth, including geology, geophysics, hydrology, meteorology, physical geography, oceanography, and soil science.\n\nAlthough mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly palaeontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research and to environmental assessment and remediation.\n\nThough sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques and practices and also the fact of it having a wide range of sub disciplines under its wing, the atmospheric sciences is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the time. The timescale of study also varies from days to centuries. Sometimes the field also includes the study of climatic patterns on planets other than earth.\n\nThe serious study of oceans began in the early to mid-20th century. As a field of natural science, it is relatively young but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices. As such a big family of related studies spanning every aspect of the oceans is now classified under this field.\n\nThe distinctions between the natural science disciplines are not always sharp, and they share a number of cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, chemical biology, geochemistry and astrochemistry.\n\nA particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with a particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields such as economics, law and social sciences.\n\nA comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is very large and diverse, marine biology is further divided into many subfields, including specializations in particular species.\n\nThere are also a subset of cross-disciplinary fields which, by the nature of the problems that they address, have strong currents that run counter to\nspecialization. Put another way: In some fields of integrative application, specialists in more than one field are a key part of most dialog. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.\n\nMaterials science is a relatively new, interdisciplinary field which deals with the study of matter and its properties; as well as the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics and engineering applications of materials including metals, ceramics, artificial polymers, and many others. The core of the field deals with relating structure of material with it properties.\n\nIt is at the forefront of research in science and engineering. It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology.\n\nThe basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.\n\nSome scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival. People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation. These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in the Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science. While the writings show an interest in astronomy, mathematics and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was in all cases religious or mythological, not scientific.\n\nA tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments. They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth. The five phases – fire, earth, metal, wood and water – described a cycle of transformations in nature. Water turned into wood, which turned into fire when it burned. The ashes left by fire were earth. Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang and understood the relationship between the pulse, the heart and the flow of blood in the body centuries before it became accepted in the West.\n\nLittle evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts. They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed. Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm. A healthy life was the result of a balance among these humors. In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind and empty space. Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy.\n\nPre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC, although an element of magic and mythology remained. Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods. Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature. In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles. Pythagoras applied Greek innovations in mathematics to astronomy, and suggested that the earth was spherical.\n\nLater Socratic and Platonic thought focused on ethics, morals and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists. Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy. In his \"History of Animals\", he described the inner workings of 110 species, including the stingray, catfish and bee. He investigated chick embryos by breaking open eggs and observing them at various stages of development. Aristotle's works were influential through the 16th century, and he is considered to be the father of biology for his pioneering work in that science. He also presented philosophies about physics, nature and astronomy using inductive reasoning in his works \"Physics\" and \"Meteorology\".\n\nWhile Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science. Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth. Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism. Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether.\n\nAristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Abbasid Caliphate. \n\nIn the Byzantine Empire John Philoponus, an Alexandrian Aristotelian commentator and Christian theologian, was the first who questioned Aristotle's teaching of physics. Unlike Aristotle who based his physics on verbal argument, Philoponus instead relied on observation, and argued for observation rather than resorting into verbal argument. He introduced the theory of impetus. John Philoponus' criticism of Aristotelian principles of physics served as inspiration for Galileo Galilei during the Scientific Revolution.\n\nA revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy. The words \"alcohol\", \"algebra\" and \"zenith\" all have Arabic roots.\n\nAristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin. The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy. European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England. Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic. This approach, however, was seen by some detractors as heresy. By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars. Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy. These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church. A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of excommunication.\"\n\nIn the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Persian scholar Al-Farabi called \"On the Sciences\" into Latin, calling the study of the mechanics of nature \"scientia naturalis\", or natural science. Gundissalinus also proposed his own classification of the natural sciences in his 1150 work \"On the Division of Philosophy\". This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe. Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics. Following Al-Farabi, he then separated the sciences into eight parts, including physics, cosmology, meteorology, minerals science and plant and animal science.\n\nLater philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote \"On the Order of the Sciences\" in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting and theater while defining natural science as the science that deals with bodies in motion. Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth and water, and in all inanimate things made from them.\" These sciences also covered plants, animals and celestial bodies. Later in the 13th century, Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on matter not only for their existence, but also for their definition.\" There was wide agreement among scholars in medieval times that natural science was about bodies in motion, although there was division about the inclusion of fields including medicine, music and perspective. Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist and where in the atmosphere rain is formed.\n\nIn the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult. Natural philosophy appeared in a wide range of forms, from treatises to encyclopedias to commentaries on Aristotle. The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it. Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries. The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective. Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works. \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.\n\nBy the 16th and 17th centuries, natural philosophy underwent an evolution beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated. The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West. Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false. A number of 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy as superficial.\n\nThe titles of Galileo's work \"Two New Sciences\" and Johannes Kepler's \"New Astronomy\" underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world. Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature. To achieve this, he wrote that \"human life [must] be endowed with new discoveries and powers.\" He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\" Bacon proposed scientific inquiry supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition and form at the time. Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock. Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation. Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution. Newton in 1687 published his \"The Mathematical Principles of Natural Philosophy\", or \"Principia Mathematica\", which set the groundwork for physical laws that remained current until the 19th century.\n\nSome modern scholars, including Andrew Cunningham, Perry Williams and Floris Cohen, argue that natural philosophy is not properly called a science, and that genuine scientific inquiry began only with the scientific revolution. According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy' is one defining characteristic of the Scientific Revolution.\" Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics and astronomy began to be applied to questions raised by natural philosophy. Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and in doing so joined natural philosophy and mathematics for the first time, producing an early work of modern physics.\nThe scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry. One of its principal advances was the use of the scientific method to investigate nature. Data was collected and repeatable measurements made in experiments. Scientists then formed hypotheses to explain the results of these experiments. The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy. The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature.\n\nNewton, an English mathematician and physicist, was the seminal figure in the scientific revolution. Drawing on advances made in astronomy by Copernicus, Brahe and Kepler, Newton derived the universal law of gravitation and laws of motion. These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently of each other, according to separate physical rules. Newton, for example, showed that the tides were caused by the gravitational pull of the moon. Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena. While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton.\n\nIn the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles. Faraday proposed that forces in nature operated in \"fields\" that filled space. The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene. James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics. Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon themselves and each other, and that they were a medium for the transmission of charged waves.\n\nSignificant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air. Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation. He also constructed a table of 33 elements and invented modern chemical nomenclature. Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carl Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus in the 1750s introduced scientific names for all his species.\n\nBy the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of \"natural science.\" The term \"scientist\" was coined by William Whewell in an 1834 review of Mary Somerville's \"On the Connexion of the Sciences\". But the word did not enter general use until nearly the end of the same century.\n\nAccording to a famous 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\" by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall, the natural sciences contain three great branches:\n\nAside from the logical and mathematical sciences, there are three great branches of \"natural science\" which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.\n\nToday, natural sciences are more commonly divided into life sciences, such as botany and zoology; and physical sciences, which include physics, chemistry, geology, astronomy and materials science.\n\n\n\n"}
{"id": "15450044", "url": "https://en.wikipedia.org/wiki?curid=15450044", "title": "Normalization process theory", "text": "Normalization process theory\n\nNormalization process theory (NPT) is a derivative sociological theory of the implementation, embedding, and integration of new technologies and organizational innovations developed originally from a collective set of learning workshops and included a large number of people including Carl R. May, Tracy Finch, Elizabeth Murray, Anne Rogers, Catherine Pope, Anne Kennedy, Pauline Ong and . The theory is a contribution to the field of science and technology studies (STS), and is the result of a programme of theory building by May and a range of academics from applied social science to medicine. Through three iterations, the theory has built upon the normalization process model previously developed by May et al. to explain the social processes that lead to the routine embedding of innovative health technologies.\n\nNormalization process theory focuses attention on agentic contributions – the things that individuals and groups do to operationalize new or modified modes of practice as they interact with dynamic elements of their environments. It defines the implementation, embedding, and integration as a process that occurs when participants deliberately initiate and seek to sustain a sequence of events that bring it into operation. The dynamics of implementation processes are complex, but normalization process theory facilitates understanding by focusing attention on the mechanisms through which participants invest and contribute to them. It reveals \"the work that actors do as they engage with some ensemble of activities (that may include new or changed ways of thinking, acting, and organizing) and by which means it becomes routinely embedded in the matrices of already existing, socially patterned, knowledge and practices\". These have explored objects, agents, and contexts. In a paper published under a creative commons license, May and colleagues describe how, since 2006, NPT has undergone three iterations.\n\n\nNormalization process theory is regarded as a middle range theory that is located within the 'turn to materiality' in STS. It therefore fits well with the case-study oriented approach to empirical investigation used in STS. It also appears to be a straightforward alternative to actor–network theory in that it does not insist on the agency of non-human actors, and seeks to be explanatory rather than descriptive. However, because normalization process theory specifies a set of generative mechanisms that empirical investigation has shown to be relevant to implementation and integration of new technologies, it can also be used in larger scale structured and comparative studies. Although it fits well with the interpretive approach of ethnography and other qualitative research methods, it also lends itself to systematic review and survey research methods. As a middle range theory, it can be federated with other theories to explain empirical phenomena. It is compatible with theories of the transmission and organization of innovations, especially diffusion of innovations theory, labor process theory, and psychological theories including the theory of planned behavior and social learning theory.\n"}
{"id": "9145213", "url": "https://en.wikipedia.org/wiki?curid=9145213", "title": "Outline of science", "text": "Outline of science\n\nThe following outline is provided as a topical overview of science:\n\nScience – the systematic effort of acquiring knowledge—through observation and experimentation coupled with logic and reasoning to find out what can be proved or not proved—and the knowledge thus acquired. The word \"science\" comes from the Latin word \"scientia\" meaning knowledge. A practitioner of science is called a \"scientist\". Modern science respects objective logical reasoning, and follows a set of core procedures or rules in order to determine the nature and underlying natural laws of the universe and everything in it. Some scientists do not know of the rules themselves, but follow them through research policies. These procedures are known as the scientific method.\n\n\nScientific method   (outline) – body of techniques for investigating phenomena and acquiring new knowledge, as well as for correcting and integrating previous knowledge. It is based on observable, empirical, measurable evidence, and subject to laws of reasoning, both deductive and inductive.\n\nBranches of science – divisions within science with respect to the entity or system concerned, which typically embodies its own terminology and nomenclature.\n\nNatural science   (outline) – major branch of science, that tries to explain and predict nature's phenomena, based on empirical evidence. In natural science, hypotheses must be verified scientifically to be regarded as scientific theory. Validity, accuracy, and social mechanisms ensuring quality control, such as peer review and repeatability of findings, are amongst the criteria and methods used for this purpose. Natural science can be broken into two main branches: biology, and physical science. Each of these branches, and all of their sub-branches, are referred to as natural sciences.\n\nFormal science – branches of knowledge that are concerned with formal systems, such as those under the branches of: logic, mathematics, computer science, statistics, and some aspects of linguistics. Unlike other sciences, the formal sciences are not concerned with the validity of theories based on observations in the real world, but instead with the properties of formal systems based on definitions and rules.\n\n\n\nSocial science – study of the social world constructed between humans. The social sciences usually limit themselves to an anthropomorphically centric view of these interactions with minimal emphasis on the inadvertent impact of social human behavior on the external environment (physical, biological, ecological, etc.). 'Social' is the concept of exchange/influence of ideas, thoughts, and relationship interactions (resulting in harmony, peace, self enrichment, favoritism, maliciousness, justice seeking, etc.) between humans. The scientific method is utilized in many social sciences, albeit adapted to the needs of the social construct being studied.\n\nApplied science – branch of science that applies existing scientific knowledge to develop more practical applications, including inventions and other technological advancements.\n\n\n\n\n\n\n\nSee – \n\n\n\n\n\n\nThe scientific fields mentioned below are generally described by the science they study.\n\n\n\n\nScience education\n\n\n"}
{"id": "246066", "url": "https://en.wikipedia.org/wiki?curid=246066", "title": "Prediction", "text": "Prediction\n\nA prediction (Latin \"præ-\", \"before,\" and \"dicere\", \"to say\"), or forecast, is a statement about a future event. A prediction is often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference between the two terms; different authors and disciplines ascribe different connotations. (Contrast with estimation.)\n\nAlthough future events are necessarily uncertain, so guaranteed accurate information about the future is in many cases impossible, prediction can be useful to assist in making plans about possible developments; Howard H. Stevenson writes that prediction in business \"... is at least two things: Important and hard.\"\n\nIn a non-statistical sense, the term \"prediction\" is often used to refer to an informed guess or opinion.\n\nA prediction of this kind might be informed by a predicting person's abductive reasoning, inductive reasoning, deductive reasoning, and experience; and may be of useful — if the predicting person is a knowledgeable person in the field.\n\nThe Delphi method is a technique for eliciting such expert-judgement-based predictions in a controlled way. This type of prediction might be perceived as consistent with statistical techniques in the sense that, at minimum, the \"data\" being used is the predicting expert's cognitive experiences forming an intuitive \"probability curve.\"\n\nIn statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one possible description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting. Forecasting usually requires time series methods, while prediction is often performed on cross-sectional data.\n\nStatistical techniques used for prediction include regression analysis and its various sub-categories such as linear regression, generalized linear models (logistic regression, Poisson regression, Probit regression), etc. In case of forecasting, autoregressive moving average models and vector autoregression models can be utilized. When these and/or related, generalized set of regression or machine learning methods are deployed in commercial usage, the field is known as predictive analytics.\n\nIn many applications, such as time series analysis, it is possible to estimate the models that generate the observations. If models can be expressed as transfer functions or in terms of state-space parameters then smoothed, filtered and predicted data estimates can be calculated. If the underlying generating models are linear then a minimum-variance Kalman filter and a minimum-variance smoother may be used to recover data of interest from noisy measurements. These techniques rely on one-step-ahead predictors (which minimise the variance of the prediction error). When the generating models are nonlinear then stepwise linearizations may be applied within Extended Kalman Filter and smoother recursions. However, in nonlinear cases, optimum minimum-variance performance guarantees no longer apply.\n\nTo use regression analysis for prediction, data are collected on the variable that is to be predicted, called the dependent variable or response variable, and on one or more variables whose values are hypothesized to influence it, called independent variables or explanatory variables. A functional form, often linear, is hypothesized for the postulated causal relationship, and the parameters of the function are estimated from the data—that is, are chosen so as to optimize is some way the fit of the function, thus parameterized, to the data. That is the estimation step. For the prediction step, explanatory variable values that are deemed relevant to future (or current but not yet observed) values of the dependent variable are input to the parameterized function to generate predictions for the dependent variable.\n\nIn science, a prediction is a rigorous, often quantitative, statement, forecasting what would happen under specific conditions; for example, if an apple fell from a tree it would be attracted towards the center of the earth by gravity with a specified and constant acceleration. The scientific method is built on testing statements that are logical consequences of scientific theories. This is done through repeatable experiments or observational studies.\n\nA scientific theory which is contradicted by observations and evidence will be rejected. New theories that generate many new predictions can more easily be supported or falsified (see predictive power). Notions that make no \"testable\" predictions are usually considered not to be part of science (protoscience or nescience) until testable predictions can be made.\n\nMathematical equations and models, and computer models, are frequently used to describe the past and future behaviour of a process within the boundaries of that model. In some cases the probability of an outcome, rather than a specific outcome, can be predicted, for example in much of quantum physics.\n\nIn microprocessors, branch prediction permits avoidance of pipeline emptying at branch instructions. In engineering, possible failure modes are predicted and avoided by correcting the mechanism causing the failure.\n\nAccurate prediction and forecasting are very difficult in some areas, such as natural disasters, pandemics, demography, population dynamics and meteorology. For example, it is possible to predict the occurrence of solar cycles, but their exact timing and magnitude is much more difficult (see picture to right).\n\nEstablished science makes useful predictions which are often extremely reliable and accurate; for example, eclipses are routinely predicted.\n\nNew theories make predictions which allow them to be disproved by reality. For example, predicting the structure of crystals at the atomic level is a current research challenge. In the early 20th century the scientific consensus was that there existed an absolute frame of reference, which was given the name \"luminiferous ether\". The existence of this absolute frame was deemed necessary for consistency with the established idea that the speed of light is constant. The famous Michelson-Morley experiment demonstrated that predictions deduced from this concept were not borne out in reality, thus disproving the theory of an absolute frame of reference. The special theory of relativity was proposed by Einstein as an explanation for the seeming inconsistency between the constancy of the speed of light and the non-existence of a special, preferred or absolute frame of reference.\n\nAlbert Einstein's theory of general relativity could not easily be tested as it did not produce any effects observable on a terrestrial scale. However, the theory predicted that large masses such as stars would bend light, in contradiction to accepted theory; this was observed in a 1919 eclipse.\n\nMathematical models of stock market behaviour (and economic behaviour in general) are also unreliable in predicting future behaviour. Among other reasons, this is because economic events may span several years, and the world is changing over a similar time frame, thus invalidating the relevance of past observations to the present. Thus there are an extremely small number (of the order of 1) of relevant past data points from which to project the future. In addition, it is generally believed that stock market prices already take into account all the information available to predict the future, and subsequent movements must therefore be the result of unforeseen events. Consequently, it is extremely difficult for a stock investor to anticipate or predict a stock market boom, or a stock market crash. In contrast to predicting the actual stock return, forecasting of broad economic trends tends to have better accuracy. Such analysis is provided by both non-profit groups as well as by for-profit private institutions, including brokerage housesand consulting companies.\n\nSome correlation has been seen between actual stock market movements and prediction data from large groups in surveys and prediction games.\n\nAn actuary uses actuarial science to assess and predict future business risk, such that the risk(s) can be mitigated. For example, in insurance an actuary would use a life table (which incorporates the historical experience of mortality rates and sometimes an estimate of future trends) to project life expectancy.\n\nPredicting the outcome of sporting events is a business which has grown in popularity in recent years. Handicappers predict the outcome of games using a variety of mathematical formulas, simulation models or qualitative analysis. Early, well known sports bettors, such as Jimmy the Greek, were believed to have access to information that gave them an edge. Information ranged from personal issues, such as gambling or drinking to undisclosed injuries; anything that may affect the performance of a player on the field.\n\nRecent times have changed the way sports are predicted. Predictions now typically consist of two distinct approaches: Situational plays and statistical based models. Situational plays are much more difficult to measure because they usually involve the motivation of a team. Dan Gordon, noted handicapper, wrote “Without an emotional edge in a game in addition to value in a line, I won’t put my money on it”. These types of plays consist of: Betting on the home underdog, betting against Monday Night winners if they are a favorite next week, betting the underdog in “look ahead” games etc. As situational plays become more widely known they become less useful because they will impact the way the line is set.\n\nThe widespread use of technology has brought with it more modern sports betting systems. These systems are typically algorithms and simulation models based on regression analysis. Jeff Sagarin, a sports statistician, has brought attention to sports by having the results of his models published in USA Today. He is currently paid as a consultant by the Dallas Mavericks for his advice on lineups and the use of his Winval system, which evaluates free agents. Brian Burke, a former Navy fighter pilot turned sports statistician, has published his results of using regression analysis to predict the outcome of NFL games. Ken Pomeroy is widely accepted as a leading authority on college basketball statistics. His website includes his College Basketball Ratings, a tempo based statistics system. Some statisticians have become very famous for having successful prediction systems. Dare wrote “the effective odds for sports betting and horse racing are a direct result of human decisions and can therefore potentially exhibit consistent error”. Unlike other games offered in a casino, prediction in sporting events can be both logical and consistent.\n\nIn politics it is common to attempt to predict the outcome of elections via political forecasting techniques (or assess the popularity of politicians) through the use of opinion polls. Prediction games have been used by many corporations and governments to learn about the most likely outcome of future events.\n\nPredictions have often been made, from antiquity until the present, by using paranormal or supernatural means such as prophecy or by observing omens. Methods including water divining, astrology, numerology, fortune telling, interpretation of dreams, and many other forms of divination, have been used for millennia to attempt to predict the future. These means of prediction have not been proven by scientific experiments.\n\nIn literature, vision and prophecy are literary devices used to present a possible timeline of future events. They can be distinguished by vision referring to what an individual sees happen. The New Testament book of Revelation (Bible) thus uses vision as a literary device in this regard. It is also prophecy or prophetic literature when it is related by an individual in a sermon or other public forum.\n\nDivination is the attempt to gain insight into a question or situation by way of an occultic standardized process or ritual. It is an integral part of witchcraft and has been used in various forms for thousands of years. Diviners ascertain their interpretations of how a querent should proceed by reading signs, events, or omens, or through alleged contact with a supernatural agency, most often describe as an angel or a god though viewed by Christians and Jews as a fallen angel or demon.\n\nFiction (especially fantasy, forecasting and science fiction) often features instances of prediction achieved by unconventional means.\n\n"}
{"id": "5563106", "url": "https://en.wikipedia.org/wiki?curid=5563106", "title": "Programming the Universe", "text": "Programming the Universe\n\nProgramming the Universe: A Quantum Computer Scientist Takes On the Cosmos is a 2006 popular science book by Seth Lloyd, professor of mechanical engineering at the Massachusetts Institute of Technology. The book proposes that the universe is a quantum computer, and advances in the understanding of physics may come from viewing entropy as a phenomenon of information, rather than simply thermodynamics. Lloyd also postulates that the universe can be fully simulated using a quantum computer; however, in the absence of a theory of quantum gravity, such a simulation is not yet possible.\n\nReviewer Corey S. Powell of \"The New York Times\" writes:\n\nIn the space of 221 dense, frequently thrilling and occasionally exasperating pages, … tackles computer logic, thermodynamics, chaos theory, complexity, quantum mechanics, cosmology, consciousness, sex and the origin of life — throwing in, for good measure, a heartbreaking afterword that repaints the significance of all that has come before. The source of all this intellectual mayhem is the kind of Big Idea so prevalent in popular science books these days. Lloyd, a professor of mechanical engineering at M.I.T., takes as his topic the fundamental workings of the universe…, which he thinks has been horribly misunderstood. Scientists have looked at it as a ragtag collection of particles and fields while failing to see what it is as a majestic whole: an enormous computer.\nIn an interview with \"Wired\" magazine, Lloyd writes:\n\neverything in the universe is made of bits. Not chunks of stuff, but chunks of information — ones and zeros. … Atoms and electrons are bits. Atomic collisions are \"ops.\" Machine language is the laws of physics. The universe is a quantum computer.\n\nGilbert Taylor, writing in \"Booklist\" of the American Library Association, said that the book:\n\noffers brilliantly clarifying explanations of the \"bit,\" the smallest unit of information; how bits change their state; and how changes-of-state can be registered on atoms via quantum-mechanical qualities such as \"spin\" and \"superposition.\" Putting readers in the know about quantum computation, Lloyd then informs them that it may well be the answer to physicists' search for a unified theory of everything. Exploring big questions in accessible, comprehensive fashion, Lloyd's work is of vital importance to the general-science audience.\n\n\n"}
{"id": "1501245", "url": "https://en.wikipedia.org/wiki?curid=1501245", "title": "Science Commons", "text": "Science Commons\n\nScience Commons (SC) was a Creative Commons project for designing strategies and tools for faster, more efficient web-enabled scientific research. The organization's goals were to identify unnecessary barriers to research, craft policy guidelines and legal agreements to lower those barriers, and develop technology to make research data and materials easier to find and use. Its overarching goal was to speed the translation of data into discovery and thereby the value of research.\n\nScience Commons was located at the MIT Computer Science and Artificial Intelligence Laboratory in the Ray and Maria Stata Center at the Massachusetts Institute of Technology in Cambridge, Massachusetts.\n\nCreative Commons launched the Science Commons project in early 2005. The project sought to achieve for science what Creative Commons had achieved for the world of culture, art and educational material: to ease unnecessary legal and technical barriers to sharing, to promote innovation, and to provide easy, high quality tools that let individuals and organizations specify the terms under which they wished to share their material.\n\nIn 2009, Creative Commons terminated the Science Commons project.\n\nThe Biological Materials Transfer Project, a Material transfer agreement (MTA), developed and deployed standard, modular contracts to lower the costs of transferring biological materials such as DNA, cell lines, model animals and more. The MTA project covered transfer between non-profit institutions, as well as offering transaction solutions to transfers between non-profit entities and for-profit institutions. It integrated existing standard agreements and new Science Commons contracts into a Web-deployed suite, with the goal of developing a transaction system along the lines of Amazon or eBay by using the licensing as a discovery mechanism for materials. \n\nThis metadata driven approach is based on the success of the Creative Commons licensing integration into search engines, further allowing for and facilitating the integration of materials licensing into the research literature itself and databases. The hope being that scientists would eventually be only one click away from accessing and/or ordering the materials referenced in the scholarly literature as they perform their research. Unfortunately, the MTA project's tools were not adopted by more than a very small percentage of the scientific community while Science Commons was active and, for all practical purposes, died out when the Science Commons project folded.\n\nScience Commons’ Neurocommons project set out to create an Open Source knowledge management platform for biological research. The platform combined open access materials (making up the knowledgebase) and open source software (in the form of an analytic platform). The software was still under development when the project ended.\n\nThe Scholar’s Copyright was developed with Scholarly Publishing and Academic Resources Coalition designed to lower the barriers to Open Access (OA) by reducing transaction costs and eliminating contract proliferation by offering tools and resources catering to both methods of achieving Open Access. The Scholar's Copyright Addendum is still in use by SPARC\n\nThe Science Commons Open Access Data Protocol was a method for ensuring that scientific databases can be legally integrated with one another. The protocol was not a license or legal tool, but instead a methodology and best practices document for creating such legal tools in the future, and marking data in the public domain for machine-assisted discovery.\n\n"}
{"id": "20687678", "url": "https://en.wikipedia.org/wiki?curid=20687678", "title": "Science In Society", "text": "Science In Society\n\nScience In Society: An Introduction to Social Studies of Science () is a 2004 book by Massimiano Bucchi. The book explains how science works, what sociologists find to be of interest, and how scientific knowledge is produced. There are chapters on the relevance of science to contemporary life, Kuhn's work and its modern relevance, as well as the role of scientific communication.\n\n"}
{"id": "411590", "url": "https://en.wikipedia.org/wiki?curid=411590", "title": "Science and technology studies", "text": "Science and technology studies\n\nScience and technology studies, or science, technology and society studies (both abbreviated STS) is the study of how society, politics, and culture affect scientific research and technological innovation, and how these, in turn, affect society, politics and culture.\n\nLike most interdisciplinary programs, STS emerged from the confluence of a variety of disciplines and disciplinary subfields, all of which had developed an interest—typically, during the 1960s or 1970s—in viewing science and technology as socially embedded enterprises. The key disciplinary components of STS took shape independently, beginning in the 1960s, and developed in isolation from each other well into the 1980s, although Ludwik Fleck's (1935) monograph \"Genesis and Development of a Scientific Fact\" anticipated many of STS's key themes. In the 1970s Elting E. Morison founded the STS program at Massachusetts Institute of Technology (MIT), which served as a model. By 2011, 111 STS research centres and academic programs were counted worldwide.\n\n\nDuring the 1970s and 1980s, leading universities in the US, UK, and Europe began drawing these various components together in new, interdisciplinary programs. For example, in the 1970s, Cornell University developed a new program that united science studies and policy-oriented scholars with historians and philosophers of science and technology. Each of these programs developed unique identities due to variation in the components that were drawn together, as well as their location within the various universities. For example, the University of Virginia's STS program united scholars drawn from a variety of fields (with particular strength in the history of technology); however, the program's teaching responsibilities—it is located within an engineering school and teaches ethics to undergraduate engineering students—means that all of its faculty share a strong interest in engineering ethics.\n\nA decisive moment in the development of STS was the mid-1980s addition of technology studies to the range of interests reflected in science. During that decade, two works appeared \"en seriatim\" that signaled what Steve Woolgar was to call the \"turn to technology\": \"Social Shaping of Technology\" (MacKenzie and Wajcman, 1985) and \"The Social Construction of Technological Systems\" (Bijker, Hughes and Pinch, 1987). MacKenzie and Wajcman primed the pump by publishing a collection of articles attesting to the influence of society on technological design. In a seminal article, Trevor Pinch and Wiebe Bijker attached all the legitimacy of the Sociology of Scientific Knowledge to this development by showing how the sociology of technology could proceed along precisely the theoretical and methodological lines established by the sociology of scientific knowledge. This was the intellectual foundation of the field they called the social construction of technology.\n\nThe \"turn to technology\" helped to cement an already growing awareness of underlying unity among the various emerging STS programs. More recently, there has been an associated turn to ecology, nature, and materiality in general, whereby the socio-technical and natural/material co-produce each other. This is especially evident in work in STS analyses of biomedicine (such as Carl May, Annemarie Mol, Nelly Oudshoorn, and Andrew Webster) and ecological interventions (such as Bruno Latour, Sheila Jasanoff, Matthias Gross, S. Lochlann Jain, and Jens Lachmund).\n\nThe subject has several professional associations.\n\nFounded in 1975, the Society for Social Studies of Science, initially provided scholarly communication facilities, including a journal (\"Science, Technology, and Human Values\") and annual meetings that were mainly attended by science studies scholars. The society has since grown into the most important professional association of science and technology studies scholars worldwide. The Society for Social Studies of Science members also include government and industry officials concerned with research and development as well as science and technology policy; scientists and engineers who wish to better understand the social embeddedness of their professional practice; and citizens concerned about the impact of science and technology in their lives. Proposals have been made to add the word \"technology\" to the association's name, thereby reflecting its stature as the leading STS professional society, that the name is long enough as it is.\n\nIn Europe, the European Association for the Study of Science and Technology (EASST) was founded in 1981 to \"stimulate communication, exchange and collaboration in the field of studies of science and technology\". Similarly, the European Inter-University Association on Society, Science and Technology (ESST) researches and studies science and technology in society, in both historical and contemporary perspectives.\n\nIn Asia several STS associations exist.\nIn Japan, the Japanese Society for Science and Technology Studies (JSSTS) was founded in 2001. The Asia Pacific Science Technology & Society Network (APSTSN) primarily has members from Australasia, Southeast and East Asia and Oceania.\n\nIn Latin America ESOCITE (Estudios Sociales de la Ciencia y la Tecnología) is the biggest association of Science and Technology studies. The study of STS (CyT in Spanish, CTS in Portuguese) here was shaped by authors like Amílcar Herrera and Jorge Sabato y Oscar Varsavsky in Argentina, José Leite Lopes in Brazil, Miguel Wionczek in Mexico, Francisco Sagasti in Peru, Máximo Halty Carrere in Uruguay and Marcel Roche in Venezuela.\n\nFounded in 1958, the Society for the History of Technology initially attracted members from the history profession who had interests in the contextual history of technology. After the \"turn to technology\" in the mid-1980s, the society's well-regarded journal (\"Technology and Culture\") and its annual meetings began to attract considerable interest from non-historians with technology studies interests.\n\nLess identified with STS, but also of importance to many STS scholars, are the History of Science Society, the Philosophy of Science Association, and the American Association for the History of Medicine.\n\nAdditionally, within the US there are significant STS-oriented special interest groups within major disciplinary associations, including the American Anthropological Association, the American Political Science Association, the National Women's Studies Association, and the American Sociological Association.\n\nNotable peer-reviewed journals in STS include: \nStudent journals in STS include: \nSocial constructions are human created ideas, objects, or events created by a series of choices and interactions. These interactions have consequences that change the perception that different groups of people have on these constructs. Some examples of social construction include class, race, money, and citizenship.\n\nThe following also alludes to the notion that not everything is set, a circumstance or result could potentially be one way or the other. According to the article \"What is Social Construction?\" by Laura Flores, \"Social construction work is critical of the status quo. Social constructionists about X tend to hold that:\nVery often they go further, and urge that:\nIn the past, there have been viewpoints that were widely regarded as fact until being called to question due to the introduction of new knowledge. Such viewpoints include the past concept of a correlation between intelligence and the nature of a human's ethnicity or race (X may not be at all as it is).\n\nAn example of the evolution and interaction of various social constructions within science and technology can be found in the development of both the high-wheel bicycle, or velocipede, and then of the bicycle. The velocipede was widely used in the latter half of the 19th century. In the latter half of the 19th century, a social need was first recognized for a more efficient and rapid means of transportation. Consequently, the velocipede was first developed, which was able to reach higher translational velocities than the smaller non-geared bicycles of the day, by replacing the front wheel with a larger radius wheel. One notable trade-off was a certain decreased stability leading to a greater risk of falling. This trade-off resulted in many riders getting into accidents by losing balance while riding the bicycle or being thrown over the handle bars.\n\nThe first \"social construction\" or progress of the velocipede caused the need for a newer \"social construction\" to be recognized and developed into a safer bicycle design. Consequently, the velocipede was then developed into what is now commonly known as the \"bicycle\" to fit within society's newer \"social construction,\" the newer standards of higher vehicle safety. Thus the popularity of the modern geared bicycle design came as a response to the first social construction, the original need for greater speed, which had caused the high-wheel bicycle to be designed in the first place. The popularity of the modern geared bicycle design ultimately ended the widespread use of the velocipede itself, as eventually it was found to best accomplish the social-needs/ social-constructions of both greater speed and of greater safety.\n\nTechnoscience is a subset of Science, Technology, and Society studies that focuses on the inseparable connection between science and technology. It states that fields are linked and grow together, and scientific knowledge requires an infrastructure of technology in order to remain stationary or move forward. Both technological development and scientific discovery drive one another towards more advancement. Technoscience excels at shaping human thought and behavior by opening up new possibilities that gradually or quickly come to be perceived as necessities.\n\n\"Technological action is a social process.\" Social factors and technology are intertwined so that they are dependent upon each other. This includes the aspect that social, political, and economic factors are inherent in technology and that social structure influences what technologies are pursued. In other words, \"technoscientific phenomena combined inextricably with social/political/ economic/psychological phenomena, so 'technology' includes a spectrum of artifacts, techniques, organizations, and systems.\" Winner expands on this idea by saying \"in the late twentieth century technology and society, technology and culture, technology and politics are by no means separate.\"\n\n\nDeliberative democracy is a reform of representative or direct democracies which mandates discussion and debate of popular topics which affect society. Deliberative Democracy is a tool for making decisions. Deliberative democracy can be traced back all the way to Aristotle’s writings. More recently, the term was coined by Joseph Bessette in his 1980 work \"Deliberative Democracy: The Majority Principle in Republican Government\", where he uses the idea in opposition to the elitist interpretations of the United States Constitution with emphasis on public discussion.\n\nDeliberative Democracy can lead to more legitimate, credible, and trustworthy outcomes. Deliberative Democracy allows for \"a wider range of public knowledge,\" and it has been argued that this can lead to \"more socially intelligent and robust\" science. One major shortcoming of deliberative democracy is that many models insufficiently ensure critical interaction.\n\nAccording to Ryfe, there are five mechanisms that stand out as critical to the successful design of deliberative democracy:\n\nRecently, there has been a movement towards greater transparency in the fields of policy and technology. Jasanoff comes to the conclusion that there is no longer a question of if there needs to be increased public participation in making decisions about science and technology, but now there needs to be ways to make a more meaningful conversation between the public and those developing the technology.\n\nAckerman and Fishkin offer an example of a reform in their paper \"Deliberation Day.\" The deliberation is to enhance public understanding of popular, complex, and controversial issues, through devices such as Fishkin’s Deliberative Polling. Although implementation of these reforms is unlikely in a large government situation such as the United States Federal Government. However, things similar to this have been implemented in small, local, governments like New England towns and villages. New England town hall meetings are a good example of deliberative democracy in a realistic setting.\n\nAn ideal Deliberative Democracy balances the voice and influence of all participants. While the main aim is to reach consensus, a deliberative democracy should encourage the voices of those with opposing viewpoints, concerns due to uncertainties, and questions about assumptions made by other participants. It should take its time and ensure that those participating understand the topics on which they debate. Independent managers of debates should also have substantial grasp of the concepts discussed, but must \"[remain] independent and impartial as to the outcomes of the process.\"\n\nIn 1968, Garrett Hardin popularised the phrase \"tragedy of the commons.\" It is an economic theory where rational people act against the best interest of the group by consuming a common resource. Since then, the tragedy of the commons has been used to symbolize the degradation of the environment whenever many individuals use a common resource. Although Garrett Hardin was not an STS scholar, the concept of tragedy of the commons still applies to science, technology and society.\n\nIn a contemporary setting, the Internet acts as an example of the tragedy of the commons through the exploitation of digital resources and private information. Data and internet passwords can be stolen much more easily than physical documents. Virtual spying is almost free compared to the costs of physical spying. Additionally, net neutrality can be seen as an example of tragedy of the commons in an STS context. The movement for net neutrality argues that the Internet should not be a resource that is dominated by one particular group, specifically those with more money to spend on Internet access.\n\nA counterexample to the tragedy of the commons is offered by Andrew Kahrl. Privatization can be a way to deal with the tragedy of the commons. However, Kahrl suggests that the privatization of beaches on Long Island, in an attempt to combat overuse of Long Island beaches, made the residents of Long Island more susceptible to flood damage from Hurricane Sandy. The privatization of these beaches took away from the protection offered by the natural landscape. Tidal lands that offer natural protection were drained and developed. This attempt to combat the tragedy of the commons by privatization was counter-productive. Privatization actually destroyed the public good of natural protection from the landscape.\n\nAlternative modernity is a conceptual tool conventionally used to represent the state of present western society. Modernity represents the political and social structures of the society, the sum of interpersonal discourse, and ultimately a snapshot of society's direction at a point in time. Unfortunately conventional modernity is incapable of modeling alternative directions for further growth within our society. Also, this concept is ineffective at analyzing similar but unique modern societies such as those found in the diverse cultures of the developing world. Problems can be summarized into two elements: inward failure to analyze growth potentials of a given society, and outward failure to model different cultures and social structures and predict their growth potentials.\n\nPreviously, modernity carried a connotation of the current state of being modern, and its evolution through European colonialism. The process of becoming \"modern\" is believed to occur in a linear, pre-determined way, and is seen by Philip Brey as a way of to interpret and evaluate social and cultural formations. This thought ties in with modernization theory, the thought that societies progress from \"pre-modern\" to \"modern\" societies.\n\nWithin the field of science and technology, there are two main lenses with which to view modernity. The first is as a way for society to quantify what it wants to move towards. In effect, we can discuss the notion of \"alternative modernity\" (as described by Andrew Feenberg) and which of these we would like to move towards. Alternatively, modernity can be used to analyze the differences in interactions between cultures and individuals. From this perspective, alternative modernities exist simultaneously, based on differing cultural and societal expectations of how a society (or an individual within society) should function. Because of different types of interactions across different cultures, each culture will have a different modernity.\n\nPace of Innovation is the speed at which technological innovation or advancement is occurring, with the most apparent instances being too slow or too rapid. Both these rates of innovation are extreme and therefore have effects on the people that get to use this technology.\n\n\"No innovation without representation\" is a democratic ideal of ensuring that everyone involved gets a chance to be represented fairly in technological developments.\n\n\nThe privileged positions of business and science refer to the unique authority that persons in these areas hold in economic, political, and technosocial affairs. Businesses have strong decision-making abilities in the function of society, essentially choosing what technological innovations to develop. Scientists and technologists have valuable knowledge, ability to pursue the technological innovations they want. They proceed largely without public scrutiny and as if they had the consent of those potentially affected by their discoveries and creations.\n\nLegacy thinking is defined as an inherited method of thinking imposed from an external source without objection by the individual, because it is already widely accepted by society.\n\nLegacy thinking can impair the ability to drive technology for the betterment of society by blinding people to innovations that do not fit into their accepted model of how society works. By accepting ideas without questioning them, people often see all solutions that contradict these accepted ideas as impossible or impractical. Legacy thinking tends to advantage the wealthy, who have the means to project their ideas on the public. It may be used by the wealthy as a vehicle to drive technology in their favor rather than for the greater good.\nExamining the role of citizen participation and representation in politics provides an excellent example of legacy thinking in society. The belief that one can spend money freely to gain influence has been popularized, leading to public acceptance of corporate lobbying. As a result, a self-established role in politics has been cemented where the public does not exercise the power ensured to them by the Constitution to the fullest extent. This can become a barrier to political progress as corporations who have the capital to spend have the potential to wield great influence over policy. Legacy thinking however keeps the population from acting to change this, despite polls from Harris Interactive that report over 80% of Americans feel that big business holds too much power in government. Therefore, Americans are beginning to try to steer away this line of thought, rejecting legacy thinking, and demanding less corporate, and more public, participation in political decision making.\n\nAdditionally, an examination of net neutrality functions as a separate example of legacy thinking. Starting with dial-up, the internet has always been viewed as a private luxury good. Internet today is a vital part of modern-day society members. They use it in and out of life every day. Corporations are able to mislabel and greatly overcharge for their internet resources. Since the American public is so dependent upon internet there is little for them to do. Legacy thinking has kept this pattern on track despite growing movements arguing that the internet should be considered a utility. Legacy thinking prevents progress because it was widely accepted by others before us through advertising that the internet is a luxury and not a utility. Due to pressure from grassroots movements the Federal Communications Commission (FCC) has redefined the requirements for broadband and internet in general as a utility. Now AT&T and other major internet providers are lobbying against this action and are in-large able to delay the onset of this movement due to legacy thinking’s grip on American culture and politics.\n\nFor example, those who cannot overcome the barrier of legacy thinking may not consider the privatization of clean drinking water as an issue. This is partially because access to water has become such a given fact of the matter to them. For a person living in such circumstances, it may be widely accepted to not concern themselves with drinking water because they have not needed to be concerned with it in the past. Additionally, a person living within an area that does not need to worry about their water supply or the sanitation of their water supply is less likely to be concerned with the privatization of water.\n\nThis notion can be examined through the thought experiment of \"veil of ignorance\". Legacy thinking causes people to be particularly ignorant about the implications behind the \"you get what you pay for\" mentality applied to a life necessity. By utilizing the \"veil of ignorance\", one can overcome the barrier of legacy thinking as it requires a person to imagine that they are unaware of their own circumstances, allowing them to free themselves from externally imposed thoughts or widely accepted ideas.\n\n\n\nSTS is taught in several countries. According to the STS wiki, STS programs can be found in twenty countries, including 45 programs in the United States, three programs in India, and eleven programs in the UK. STS programs can be found in Israel, Malaysia, and Taiwan. Some examples of institutions offering STS programs are Stanford University, Harvard University, the University of Oxford, Mines ParisTech, and Bar-Ilan University.\n\n\n"}
{"id": "6663803", "url": "https://en.wikipedia.org/wiki?curid=6663803", "title": "Scientific priority", "text": "Scientific priority\n\nIn science, priority is the credit given to the individual or group of individuals who first made the discovery or propose the theory. Fame and honours usually go to the first person or group to publish a new finding, even if several researchers arrived at the same conclusion independently and at the same time. Thus between two or more independent discoverers, the first to make formal publication is the legitimate winner. Hence, the tradition is often referred to as the priority rule, the procedure of which is nicely summed up in a phrase \"publish or perish\", because there are no second prizes. In a way, the race to be first inspires risk-taking that can lead to scientific breakthroughs which is beneficial to the society (such as discovery of malaria transmission, DNA, HIV, etc.); on the other hand, it can create an unhealthy competition, thus, becoming detrimental to scientific progress.\n\nPriority becomes a difficult issue usually in the context of priority disputes, where the priority for a given theory, understanding, or discovery comes into question. In most cases historians of science disdain retrospective priority disputes as enterprises which generally lack understanding about the nature of scientific change and usually involve gross misreadings of the past to support the idea of a long-lost priority claim. Historian and biologist Stephen Jay Gould once remarked that \"debates about the priority of ideas are usually among the most misdirected in the history of science.\"\n\nRichard Feynman told Freeman Dyson that he avoided priority disputes by \"Always giv[ing] the bastards more credit than they deserve.\" Dyson remarked that he also follows this rule, and that this practice is \"remarkably effective for avoiding quarrels and making friends.\" \n\nThe priority rule came into existence before or as soon as modern scientific methods were established. For example, the earliest documented controversy was a bitter claim between Isaac Newton and Gottfried Wilhelm Leibniz in the 17th century about priority in the invention of calculus. This particular incidence clearly shows human biases and prejudice. It has become unanimously accepted that both the mathematicians independently developed calculus. Since then priority has caused a number of historical maladies in the history of science.\nIn the cases of scientists who have since achieved incredible levels of popularity, such as Charles Darwin and Albert Einstein, priority questions are often rooted in taking too seriously the myth of the \"lone genius\" which is often cultivated around such quasi-mythic figures (see Great Man theory and Whig history). In an attempt to laud such scientists as visionaries, the context in which they worked is often neglected by popularizers, making it appear as if they worked without assistance or without reference to other work, something which is rarely the case.\n\n\n"}
{"id": "10469862", "url": "https://en.wikipedia.org/wiki?curid=10469862", "title": "Technological somnambulism", "text": "Technological somnambulism\n\nTechnological somnambulism is a concept used when talking about the philosophy of technology. The term was used by Langdon Winner in his essay \"Technology as forms of life\". Winner puts forth the idea that we are simply in a state of \"sleepwalking\" in our mediations with technology. This sleepwalking is caused by a number of factors. One of the primary causes is the way we view technology as tools, something that can be put down and picked up again. Because of this view of objects as something we can easily separate ourselves from technology, and so we fail to look at the long term implications of using that object. A second factor is the separation of those who make the technology and those who use the technology. This division causes there to be little thought and research going into the effects of using/developing that technology. The third and most important idea is the way in which technology seems to create new \"worlds\" in which we live. These worlds are created by the restructuring of the common and seemingly everyday things around us. In most situations the changes take place with little attention or care from us because we are more focused on the menial aspects of the technology (Winner 105-107).\n\nThe concept can be found in the earlier work of Marshall McLuhan, cf. \"Understanding Media\", where he refers to a comment made by David Sarnoff expressing a socially deterministic view of \"value free\" technology whose value is solely defined by its usage as representing, \"...the voice of the current somnambulism\". Given that this piece by McLuhan has become standard reading in Media Theory it is reasonable to suspect that Winner encountered the concept there or elsewhere and then went on to develop it further.\n"}
{"id": "10518546", "url": "https://en.wikipedia.org/wiki?curid=10518546", "title": "Technology life cycle", "text": "Technology life cycle\n\nThe technology life-cycle (TLC) describes the commercial gain of a product through the expense of research and development phase, and the financial return during its \"vital life\". Some technologies, such as steel, paper or cement manufacturing, have a long lifespan (with minor variations in technology incorporated with time) while in other cases, such as electronic or pharmaceutical products, the lifespan may be quite short.\n\nThe TLC associated with a product or technological service is different from product life-cycle (PLC) dealt with in product life-cycle management. The latter is concerned with the life of a product in the marketplace with respect to timing of introduction, marketing measures, and business costs. The \"technology\" underlying the product (for example, that of a uniquely flavoured tea) may be quite marginal but the process of creating and managing its life as a branded product will be very different. \n\nThe technology life cycle is concerned with the time and cost of developing the technology, the timeline of recovering cost, and modes of making the technology yield a profit proportionate to the costs and risks involved. The TLC may, further, be protected during its cycle with patents and trademarks seeking to lengthen the cycle and to maximize the profit from it.\n\nThe \"product\" of the technology may be a commodity such as polyethylene plastic or a sophisticated product like the integrated circuits used in a smartphone.\n\nThe development of a \"competitive product\" or process can have a major effect on the lifespan of the technology, making it shorter. Equally, the loss of intellectual property rights through litigation or loss of its secret elements (if any) through leakages also work to reduce a technology's lifespan. Thus, it is apparent that the \"management\" of the TLC is an important aspect of technology development.\n\nMost new technologies follow a similar technology maturity lifecycle describing the technological maturity of a product. This is not similar to a product life cycle, but applies to an entire technology, or a generation of a technology.\n\nTechnology adoption is the most common phenomenon driving the evolution of industries along the industry lifecycle. After expanding new uses of resources they end with exhausting the efficiency of those processes, producing gains that are first easier and larger over time then exhaustingly more difficult, as the technology matures.\n\nThe TLC may be seen as composed of four phases:\n\nThe shape of the technology lifecycle is often referred to as S-curve.\n\nThere is usually technology hype at the introduction of any new technology, but only after some time has passed can it be judged as mere hype or justified true acclaim.\nBecause of the logistic curve nature of technology adoption, it is difficult to see in the early stages whether the hype is excessive.\n\nThe two errors commonly committed in the early stages of a technology's development are:\nSimilarly, in the later stages, the opposite mistakes can be made relating to the possibilities of technology maturity and market saturation.\n\nThe technology adoption life cycle typically occurs in an S curve, as modelled in diffusion of innovations theory. This is because customers respond to new products in different ways. Diffusion of innovations theory, pioneered by Everett Rogers, posits that people have different levels of readiness for adopting new innovations and that the characteristics of a product affect overall adoption. Rogers classified individuals into five groups: innovators, early adopters, early majority, late majority, and laggards. In terms of the S curve, innovators occupy 2.5%, early adopters 13.5%, early majority 34%, late majority 34%, and laggards 16%.\n\nThe four stages of technology life cycle are as follows:\n\nLarge corporations develop technology for their own benefit and not with the objective of licensing. The tendency to license out technology only appears when there is a threat to the life of the TLC (business gain) as discussed later.\n\nThere are always smaller firms (SMEs) who are inadequately situated to finance the development of innovative R&D in the post-research and early technology phases. By sharing incipient technology under certain conditions, substantial risk financing can come from third parties. This is a form of quasi-licensing which takes different formats. Even large corporates may not wish to bear all costs of development in areas of significant and high risk (e.g. aircraft development) and may seek means of spreading it to the stage that proof-of-concept is obtained.\n\nIn the case of small and medium firms, entities such as venture capitalists or business angels, can enter the scene and help to materialize technologies. Venture capitalists accept both the costs and uncertainties of R&D, and that of market acceptance, in reward for high returns when the technology proves itself. Apart from finance, they may provide networking, management and marketing support. Venture capital connotes financial as well as human capital.\n\nLarger firms may opt for Joint R&D or work in a consortium for the early phase of development. Such vehicles are called strategic alliances – strategic partnerships.\nWith both venture capital funding and strategic (research) alliances, when business gains begin to neutralize development costs (the TLC crosses the X-axis), the ownership of the technology starts to undergo change.\n\nIn the case of smaller firms, venture capitalists help clients enter the stock market for obtaining substantially larger funds for development, maturation of technology, product promotion and to meet marketing costs. A major route is through initial public offering (IPO) which invites risk funding by the public for potential high gain. At the same time, the IPOs enable venture capitalists to attempt to recover expenditures already incurred by them through part sale of the stock pre-allotted to them (subsequent to the listing of the stock on the stock exchange). When the IPO is fully subscribed, the assisted enterprise becomes a corporation and can more easily obtain bank loans, etc. if needed.\n\nStrategic alliance partners, allied on research, pursue separate paths of development with the incipient technology of common origin but pool their accomplishments through instruments such as 'cross-licensing'. Generally, contractual provisions among the members of the consortium allow a member to exercise the option of independent pursuit after joint consultation; in which case the optee owns all subsequent development.\n\nThe ascent stage of the technology usually refers to some point above Point A in the TLC diagram but actually it commences when the R&D portion of the TLC curve inflects (only that the cashflow is negative and unremunerative to Point A). The ascent is the strongest phase of the TLC because it is here that the technology is superior to alternatives and can command premium profit or gain. The slope and duration of the ascent depends on competing technologies entering the domain, although they may not be \"as successful\" in that period. Strongly patented technology extends the duration period.\n\nThe TLC begins to flatten out (the region shown as M) when equivalent or challenging technologies come into the competitive space and begin to eat away marketshare.\n\nTill this stage is reached, the technology-owning firm would tend to exclusively enjoy its profitability, preferring \"not\" to license it. If an overseas opportunity does present itself, the firm would prefer to set up a controlled subsidiary rather than license a third party.\n\nThe maturity phase of the technology is a period of stable and remunerative income but its competitive viability can persist over the larger timeframe marked by its 'vital life'. However, there may be a tendency to license out the technology to third parties during this stage to lower risk of decline in profitability (or competitivity) and to expand financial opportunity.\n\nThe exercise of this option is, generally, inferior to seeking participatory exploitation; in other words, engagement in joint venture, typically in regions where the technology would be in the \"ascent phase\",as say, a developing country. In addition to providing financial opportunity it allows the technology-owner a degree of control over its use. Gain flows from the two streams of investment-based and royalty incomes. Further, the vital life of the technology is enhanced in such strategy.\n\nAfter reaching a point such as D in the above diagram, the earnings from the technology begin to decline rather rapidly. To prolong the life cycle, owners of technology might try to license it out at some point L when it can still be attractive to firms in other markets. This, then, traces the lengthening path, LL'. Further, since the decline is the result of competing rising technologies in this space, licenses may be attracted to the general lower cost of the older technology (than what prevailed during its vital life).\n\nLicenses obtained in this phase are 'straight licenses'. They are free of direct control from the owner of the technology (as would otherwise apply, say, in the case of a joint-venture). Further, there may be fewer restrictions placed on the licensee in the employment of the technology.\n\nThe utility, viability, and thus the cost of straight-licenses depends on the estimated 'balance life' of the technology. For instance, should the key patent on the technology have expired, or would expire in a short while, the residual viability of the technology may be limited, although balance life may be governed by other criteria such as knowhow which could have a longer life if properly protected.\n\n\"It is important to note that the license has no way of knowing the stage at which the prime, and competing technologies, are on their TLCs\". It would, of course, be evident to competing licensor firms, and to the originator, from the growth, saturation or decline of the profitability of their operations.\n\nThe license may, however, be able to approximate the stage by vigorously negotiating with the licensor and competitors to determine costs and licensing terms. A lower cost, or easier terms, \"may\" imply a declining technology.\n\nIn any case, access to technology in the decline phase is a large risk that the licensee accepts. (In a joint-venture this risk is substantially reduced by licensor sharing it). Sometimes, financial guarantees from the licensor may work to reduce such risk and can be negotiated.\n\nThere are instances when, even though the technology declines to becoming a technique, it may still contain important knowledge or experience which the licensee firm cannot learn of without help from the originator. This is often the form that \"technical service\" and \"technical assistance\" contracts take (encountered often in developing country contracts). Alternatively, consulting agencies may fill this role.\n\nAccording to the Encyclopedia of Earth, \"In the simplest formulation, innovation can be thought of as being composed of research, development, demonstration, and deployment.\"\n\n\"Technology development cycle\" describes the process of a new technology through the stages of technological maturity:\n\n"}
{"id": "433005", "url": "https://en.wikipedia.org/wiki?curid=433005", "title": "The Emperor's New Mind", "text": "The Emperor's New Mind\n\nThe Emperor's New Mind: Concerning Computers, Minds and The Laws of Physics is a 1989 book by mathematical physicist Sir Roger Penrose.\n\nPenrose argues that human consciousness is non-algorithmic, and thus is not capable of being modeled by a conventional Turing machine, which includes a digital computer. Penrose hypothesizes that quantum mechanics plays an essential role in the understanding of human consciousness. The collapse of the quantum wavefunction is seen as playing an important role in brain function.\n\nThe majority of the book is spent reviewing, for the scientifically minded layreader, a plethora of interrelated subjects such as Newtonian physics, special and general relativity, the philosophy and limitations of mathematics, quantum physics, cosmology, and the nature of time. Penrose intermittently describes how each of these bears on his developing theme: that consciousness is not \"algorithmic\". Only the later portions of the book address the thesis directly.\n\nPenrose states that his ideas on the nature of consciousness are speculative, and his thesis is considered erroneous by experts in the fields of philosophy, computer science, and robotics.\n\nFollowing the publication of this book, Penrose began to collaborate with Stuart Hameroff on a biological analog to quantum computation involving microtubules, which became the foundation for his subsequent book, \"Shadows of the Mind: A Search for the Missing Science of Consciousness\".\n\n\"The Emperor's New Mind\" attacks the claims of artificial intelligence using the physics of computing: Penrose notes that the present home of computing lies more in the tangible world of classical mechanics than in the imponderable realm of quantum mechanics. The modern computer is a deterministic system that for the most part simply executes algorithms. Penrose shows that, by reconfiguring the boundaries of a billiard table, one might make a computer in which the billiard balls act as message carriers and their interactions act as logical decisions. The billiard-ball computer was first designed some years ago by Edward Fredkin and Tommaso Toffoli of the Massachusetts Institute of Technology.\n\nPenrose won the Science Book Prize in 1990 for this book.\n\n"}
{"id": "1780941", "url": "https://en.wikipedia.org/wiki?curid=1780941", "title": "The Hot Zone", "text": "The Hot Zone\n\nThe Hot Zone: A Terrifying True Story is a best-selling 1995 nonfiction thriller by Richard Preston about the origins and incidents involving viral hemorrhagic fevers, particularly ebolaviruses and marburgviruses. The basis of the book was Preston's 1992 \"New Yorker\" article \"Crisis in the Hot Zone\".\n\nThe filoviruses, Ebola virus, Sudan virus, Marburg virus, and Ravn virus are Biosafety Level 4 agents. Biosafety Level 4 agents are extremely dangerous to humans because they are very infectious, have a high case-fatality rate, and have no known prophylactics, treatments, or cures.\n\nAlong with describing the history of the diseases caused by these two Central African diseases, Ebola virus disease and Marburg virus disease, Preston described a 1989 incident in which a relative of Ebola virus, named Reston virus, was discovered at a primate quarantine facility in Reston, Virginia, less than 15 miles (24 km) away from Washington, D.C. The virus found at the facility was a mutated form of the original Ebola virus and was initially mistaken for Simian hemorrhagic fever. The original Reston facility involved in the incident, located at 1946 Isaac Newton Square, was subsequently demolished on 30 May 1995, and a new building was constructed in its place.\n\nThe book is in four sections:\n\n\nThe book starts with \"Charles Monet\" visiting Kitum Cave during a camping trip to Mount Elgon in Central Africa. Not long after, he begins to suffer from a number of symptoms, including vomiting, diarrhea and red eye. He is soon taken to Nairobi Hospital for treatment, but his condition deteriorates further and he goes into a coma while in the waiting room. This particular filovirus is called Marburg virus.\n\nDr. Nancy Jaax had been promoted to work in the Level 4 Biosafety containment area at the United States Army Medical Research Institute of Infectious Diseases, and is assigned to research Ebola virus. While preparing food for her family at home, she cuts her right hand. Later, while working on a dead monkey infected with Ebola virus, one of the gloves on the hand with the open wound tears, and she is almost exposed to contaminated blood, but does not get infected. Nurse Mayinga is also infected by a nun and elects to visit Nairobi Hospital for treatment, where she succumbs to the disease.\n\nIn Reston, Virginia, less than fifteen miles (24 km) away from Washington, D.C., a company called Hazelton Research once operated a quarantine center for monkeys that were destined for laboratories. In October 1989, when an unusually high number of their monkeys began to die, their veterinarian decided to send some samples to Fort Detrick (USAMRIID) for study. Early during the testing process in biosafety level 3, when one of the flasks appeared to be contaminated with harmless pseudomonas bacterium, two USAMRIID scientists exposed themselves to the virus by wafting the flask. They later determine that, while the virus is terrifyingly lethal to monkeys, humans can be infected with it without any health effects at all. This virus is now known as Reston virus.\n\nFinally, the author himself goes into Africa to explore Kitum Cave. On the way, he discusses the role of AIDS in the present, as the highway they were on, sometimes called the \"AIDS Highway\", or the \"Kinshasa Highway\", was where it first appeared. Equipped with a hazmat suit, he enters the cave and finds a large number of animals, one of which might be the virus carrier. At the conclusion of the book, he travels to the quarantine facility in Reston. He finds the building abandoned and deteriorating. He concludes the book by claiming that Ebola will be back.\n\nThe discovery of the Reston virus was made in November 1989 by Thomas W. Geisbert, an intern at United States Army Medical Research Institute of Infectious Diseases. Dr. Peter B. Jahrling isolated the filovirus further. The Center for Disease Control and Prevention conducted blood tests of the 178 animal handlers. While 6 tested positive, they did not exhibit any symptoms. The Reston virus was found to have low pathogenicity in humans. This was further supported later when a handler infected himself during a necroscopy of an infected monkey. However, the handler did not show symptoms of the virus after the incubation period.\n\n\"The Hot Zone\" was listed as one of around 100 books that shaped a century of science by \"American Scientist\". Listed as an “Exploration,” the criteria dictate that the book typically “seeks to engage with the context” elucidating the general topic rather than a specific nuance.\n\nMany reviews of \"The Hot Zone\" exemplify the impact the book had on the public’s view of emerging viruses. A review in the \"British Medical Journal\" captures the paranoia and public panic described in this book. The reviewer was left “wondering when and where this enigmatic agent will appear next and what other disasters may await human primates.” This can also be seen in a review in the \"Public Health Reports\" which highlights the “seriousness of our current situation” and “our ability to respond to a major health threat.”\n\n\"The Hot Zone\" has been criticized for exaggerating the threat of Ebola and causing viral panic. In an interview about his book about Ebola, David Quammen claimed that \"The Hot Zone\" had “vivid, gruesome details” that gave an “exaggerated idea of Ebola over the years” causing “people to view this disease as though it was some sort of preternatural phenomenon.”\n\n\"The Hot Zone\" is described as a “romantic account of environmental transgression.” Reactions to this book could be seen not only in the public’s view of emerging viruses, but in the changes in the Centers for Disease Control and Prevention. In addition to the funding of public health infrastructure during the early 1970s, there were many public discussions of biodefense. This book continued to fuel the emerging diseases campaign. By connecting international health to national security, this campaign used \"The Hot Zone\" as a method of justifying increased intervention in the global phenomena of disease.\n\n\"The Hot Zone\" elicited a major response by the WHO by shedding light on the Ebola Zaire outbreak. The release of teams of experts was immediate and massive. Many countries tightened their borders, issued warnings to custom officials, quarantined travelers, and issued travel advisories.\n\nIn his blurb, horror writer Stephen King called the first chapter, \"one of the most horrifying things I've read in my whole life.\" When asked whether any book \"scared the pants off you\" writer Suzanne Collins answered, \"\"The Hot Zone\", by Richard Preston. I just read it a few weeks ago. Still recovering.\"\n\n\"The Hot Zone\" has received criticism for sensationalizing the effects of Ebola virus. In their memoir \"Level 4: Virus Hunters of the CDC\", former CDC scientists Joseph B. McCormick and Susan Fisher-Hoch lambasted Preston for claiming that Ebola dissolves organs, stating that although it causes great blood loss in tissues the organs remain structurally intact. McCormick and Fisher-Hoch also dispute Preston's version of the CDC's actions in the Reston virus incident.\n\nIn January 1993, 20th Century Fox and producer Lynda Obst won a bidding war for the film rights to Preston's 1992 \"New Yorker\" article, which was being transitioned into the book. In response to being outbid, Warner Bros. producer Arnold Kopelson immediately began working on the similarly themed \"Outbreak\", a competing film which would ultimately be a factor leading to the collapse of \"Crisis in The Hot Zone\".\n\nDirectors considered for the film adaptation included Wolfgang Petersen (who would later direct \"Outbreak\"), Michael Mann, and Ridley Scott. Scott eventually signed on to direct the film in February 1994. Screenwriter James V. Hart was also signed to adapt the book. In late-April 1994, Fox announced they had signed Robert Redford and Jodie Foster to star in the film.\n\nHowever, this version would ultimately not be made. Foster dropped out of the film just before filming was to begin and production was delayed with Meryl Streep, Sharon Stone, and Robin Wright touted as possible replacements. In August 1994, Redford also dropped out of the film. A few days following Redford's departure, it was announced that pre-production had been shut down.\n\nOn October 16, 2014, \"The Hollywood Reporter\" announced that Ridley Scott again plans to adapt the book, this time as a television miniseries for NatGeo. Kelly Souders, Brian Peterson, and Jeff Vintar wrote the pilot. Julianna Margulies is starring as Nancy Jaax. Filming begins in September 2018. Lynda Obst will again produce the series.\n\n\nNotes\nBibliography\n\nFurther reading\n\n"}
{"id": "57376147", "url": "https://en.wikipedia.org/wiki?curid=57376147", "title": "The Joy of Science", "text": "The Joy of Science\n\nThe Joy of Science is a popular video and audio course series, consisting of 60 lectures, each 30 minutes long, presented by Dr. Robert Hazen of the George Mason University and the Carnegie Institution of Washington. The course, first introduced in 2001, is part of The Great Courses (TGC) series, and is produced and distributed by The Teaching Company (TTC), located in Chantilly, Virginia in the United States.\n\nAs the Clarence B. Robinson Professor at George Mason University, Robert Hazen developed innovative courses to promote scientific literacy in both scientists and non-scientists. In a collaboration with physicist James Trefil, he wrote three undergraduate textbooks: \"The Sciences: An Integrated Approach\" (1993), \"The Physical Sciences: An Integrated Approach\" (1995), and \"Physics Matters: An Introduction to Conceptual Physics\" (2004). Hazen used these as the basis for a 60-lecture video and audio course called \"The Joy of Science\".\n\n\n"}
{"id": "12320384", "url": "https://en.wikipedia.org/wiki?curid=12320384", "title": "Theory of impetus", "text": "Theory of impetus\n\nThe theory of impetus was an auxiliary or secondary theory of Aristotelian dynamics, put forth initially to explain projectile motion against gravity. It was introduced by John Philoponus in the 6th century and elaborated by Nur ad-Din al-Bitruji at the end of the 12th century, but was only established in western scientific thought by Jean Buridan in the 14th century. It is the intellectual precursor to the concepts of inertia, momentum and acceleration in classical mechanics.\n\nIn the 6th century, John Philoponus partly accepted Aristotle's theory that \"continuation of motion depends on continued action of a force,\" but modified it to include his idea that the hurled body acquires a motive power or inclination for forced movement from the agent producing the initial motion and that this power secures the continuation of such motion. However, he argued that this impressed virtue was temporary; that it was a self-expending inclination, and thus the violent motion produced comes to an end, changing back into natural motion.\n\nIn the 11th century, Avicenna discussed Philoponus' theory in \"The Book of Healing\", in Physics IV.14 he says; \nIn the 12th century, Hibat Allah Abu'l-Barakat al-Baghdaadi adopted and modified Avicenna's theory on projectile motion. In his \"Kitab al-Mu'tabar\", Abu'l-Barakat stated that the mover imparts a violent inclination (\"mayl qasri\") on the moved and that this diminishes as the moving object distances itself from the mover. Jean Buridan and Albert of Saxony later refer to Abu'l-Barakat in explaining that the acceleration of a falling body is a result of its increasing impetus.\n\nIn the 14th century, Jean Buridan postulated the notion of motive force, which he named impetus. Buridan gives his theory a mathematical value: impetus = weight x velocity\nBuridan's pupil Dominicus de Clavasio in his 1357 \"De Caelo\", as follows:\n\nBuridan's position was that a moving object would \"only\" be arrested by the resistance of the air and the weight of the body which would oppose its impetus. Buridan also maintained that impetus was proportional to speed; thus, his initial idea of impetus was similar in many ways to the modern concept of momentum. Buridan saw his theory as only a modification to Aristotle's basic philosophy, maintaining many other peripatetic views, including the belief that there was still a fundamental difference between an object in motion and an object at rest. Buridan also maintained that impetus could be not only linear, but also circular in nature, causing objects (such as celestial bodies) to move in a circle.\n\nBuridan pointed out that neither Aristotle's unmoved movers nor Plato's souls are in the Bible, so he applied impetus theory to the eternal rotation of the celestial spheres by extension of a terrestrial example of its application to rotary motion in the form of a rotating millwheel that continues rotating for a long time after the originally propelling hand is withdrawn, driven by the impetus impressed within it. He wrote on the celestial impetus of the spheres as follows:\n\nHowever, by discounting the possibility of any resistance either due to a contrary inclination to move in any opposite direction or due to any external resistance, he concluded their impetus was therefore not corrupted by any resistance. Buridan also discounted any inherent resistance to motion in the form of an inclination to rest within the spheres themselves, such as the inertia posited by Averroes and Aquinas. For otherwise that resistance would destroy their impetus, as the anti-Duhemian historian of science Annaliese Maier maintained the Parisian impetus dynamicists were forced to conclude because of their belief in an inherent \"inclinatio ad quietem\" or inertia in all bodies.\n\nThis raised the question of why the motive force of impetus does not therefore move the spheres with infinite speed. One impetus dynamics answer seemed to be that it was a secondary kind of motive force that produced uniform motion rather than infinite speed, rather than producing uniformly accelerated motion like the primary force did by producing constantly increasing amounts of impetus. However, in his \"Treatise on the heavens and the world\" in which the heavens are moved by inanimate inherent mechanical forces, Buridan's pupil Oresme offered an alternative Thomist inertial response to this problem in that he did posit a resistance to motion inherent in the heavens (i.e. in the spheres), but which is only a resistance to acceleration beyond their natural speed, rather than to motion itself, and was thus a tendency to preserve their natural speed.\n\nBuridan's thought was followed up by his pupil Albert of Saxony (1316–1390), by writers in Poland such as John Cantius, and the Oxford Calculators. Their work in turn was elaborated by Nicole Oresme who pioneered the practice of demonstrating laws of motion in the form of graphs.\n\nThe Buridan impetus theory developed one of the most important thought experiments in the history of science, namely the so-called 'tunnel-experiment', so important because it brought oscillatory and pendulum motion within the pale of dynamical analysis and understanding in the science of motion for the very first time and thereby also established one of the important principles of classical mechanics. The pendulum was to play a crucially important role in the development of mechanics in the 17th century, and so more generally was the axiomatic principle of Galilean, Huygenian and Leibnizian dynamics to which the tunnel experiment also gave rise, namely that a body rises to the same height from which it has fallen, a principle of gravitational potential energy. As Galileo Galilei expressed this fundamental principle of his dynamics in his 1632 \"Dialogo\":\n\nThis imaginary experiment predicted that a cannonball dropped down a tunnel going straight through the centre of the Earth and out the other side would go past the centre and rise on the opposite surface to the same height from which it had first fallen on the other side, driven upwards past the centre by the gravitationally created impetus it had continually accumulated in falling downwards to the centre. This impetus would require a violent motion correspondingly rising to the same height past the centre for the now opposing force of gravity to destroy it all in the same distance which it had previously required to create it, and whereupon at this turning point the ball would then descend again and oscillate back and forth between the two opposing surfaces about the centre \"ad infinitum\" in principle. Thus the tunnel experiment provided the first dynamical model of oscillatory motion, albeit a purely imaginary one in the first instance, and specifically in terms of A-B impetus dynamics.\n\nHowever, this thought-experiment was then most cunningly applied to the dynamical explanation of a real world oscillatory motion, namely that of the pendulum, as follows. The oscillating motion of the cannonball was dynamically assimilated to that of a pendulum bob by imagining it to be attached to the end of an immensely cosmologically long cord suspended from the vault of the fixed stars centred on the Earth, whereby the relatively short arc of its path through the enormously distant Earth was practically a straight line along the tunnel. Real world pendula were then conceived of as just micro versions of this 'tunnel pendulum', the macro-cosmological paradigmatic dynamical model of the pendulum, but just with far shorter cords and with their bobs oscillating above the Earth's surface in arcs corresponding to the tunnel inasmuch as their oscillatory midpoint was dynamically assimilated to the centre of the tunnel as the centre of the Earth.\n\nHence by means of such impressive literally 'lateral thinking', rather than the dynamics of pendulum motion being conceived of as the bob inexplicably somehow falling downwards compared to the vertical to a gravitationally lowest point and then inexplicably being pulled back up again on the same upper side of that point, rather it was its lateral horizontal motion that was conceived of as a case of gravitational free-fall followed by violent motion in a recurring cycle, with the bob repeatedly travelling through and beyond the motion's vertically lowest but horizontally middle point that stood proxy for the centre of the Earth in the tunnel pendulum. So on this imaginative lateral gravitational thinking outside the box the lateral motions of the bob first towards and then away from the normal in the downswing and upswing become lateral downward and upward motions in relation to the horizontal rather than to the vertical.\n\nThus whereas the orthodox Aristotelians could only see pendulum motion as a dynamical anomaly, as inexplicably somehow 'falling to rest with difficulty' as historian and philosopher of science Thomas Kuhn put it in his 1962 \"The Structure of Scientific Revolutions\", on the impetus theory's novel analysis it was not falling with any dynamical difficulty at all in principle, but was rather falling in repeated and potentially endless cycles of alternating downward gravitationally natural motion and upward gravitationally violent motion. Hence, for example, Galileo was eventually to appeal to pendulum motion to demonstrate that the speed of gravitational free-fall is the same for all unequal weights precisely by virtue of dynamically modelling pendulum motion in this manner as a case of cyclically repeated gravitational free-fall along the horizontal in principle.\n\nIn fact the tunnel experiment, and hence pendulum motion, was an imaginary crucial experiment in favour of impetus dynamics against both orthodox Aristotelian dynamics without any auxiliary impetus theory, and also against Aristotelian dynamics with its H-P variant. For according to the latter two theories the bob cannot possibly pass beyond the normal. In orthodox Aristotelian dynamics there is no force to carry the bob upwards beyond the centre in violent motion against its own gravity that carries it to the centre, where it stops. And when conjoined with the Philoponus auxiliary theory, in the case where the cannonball is released from rest, again there is no such force because either all the initial upward force of impetus originally impressed within it to hold it in static dynamical equilibrium has been exhausted, or else if any remained it would be acting in the opposite direction and combine with gravity to prevent motion through and beyond the centre. Nor were the cannonball to be positively hurled downwards, and thus with a downward initial impetus, could it possibly result in an oscillatory motion. For although it could then possibly pass beyond the centre, it could never return to pass through it and rise back up again. For dynamically in this case although it would be logically possible for it to pass beyond the centre if when it reached it some of the constantly decaying downward impetus remained and still sufficiently much to be stronger than gravity to push it beyond the centre and upwards again, nevertheless when it eventually then became weaker than gravity, whereupon the ball would then be pulled back towards the centre by its gravity, it could not then pass beyond the centre to rise up again, because it would have no force directed against gravity to overcome it. For any possibly remaining impetus would be directed 'downwards' towards the centre, that is, in the same direction in which it was originally created.\n\nThus pendulum motion was dynamically impossible for both orthodox Aristotelian dynamics and also for H-P impetus dynamics on this 'tunnel model' analogical reasoning. But it was predicted by the impetus theory's tunnel prediction precisely because that theory posited that a continually accumulating downwards force of impetus directed towards the centre is acquired in natural motion, sufficient to then carry it upwards beyond the centre against gravity, and rather than only having an initially upwards force of impetus away from the centre as in the theory of natural motion. So the tunnel experiment constituted a crucial experiment between three alternative theories of natural motion.\n\nOn this analysis then impetus dynamics was to be preferred if the Aristotelian science of motion was to incorporate a dynamical explanation of pendulum motion. And indeed it was also to be preferred more generally if it was to explain other oscillatory motions, such as the to and fro vibrations around the normal of musical strings in tension, such as those of a zither, lute or guitar. For here the analogy made with the gravitational tunnel experiment was that the tension in the string pulling it towards the normal played the role of gravity, and thus when plucked i.e. pulled away from the normal and then released, this was the equivalent of pulling the cannonball to the Earth's surface and then releasing it. Thus the musical string vibrated in a continual cycle of the alternating creation of impetus towards the normal and its destruction after passing through the normal until this process starts again with the creation of fresh 'downward' impetus once all the 'upward' impetus has been destroyed.\n\nThis positing of a dynamical family resemblance of the motions of pendula and vibrating strings with the paradigmatic tunnel-experiment, the original mother of all oscillations in the history of dynamics, was one of the greatest imaginative developments of medieval Aristotelian dynamics in its increasing repertoire of dynamical models of different kinds of motion.\n\nShortly before Galileo's theory of impetus, Giambattista Benedetti modified the growing theory of impetus to involve linear motion alone:\n\nBenedetti cites the motion of a rock in a sling as an example of the inherent linear motion of objects, forced into circular motion.\n\n\n"}
{"id": "30343059", "url": "https://en.wikipedia.org/wiki?curid=30343059", "title": "University of Auckland Faculty of Science", "text": "University of Auckland Faculty of Science\n\nThe University of Auckland Faculty of Science is one of eight faculties and schools that make up the University of Auckland.\n\n\nUndergraduate Programs\n\nConjoint Programs\n\nPostgraduate Programs\n\nDoctoral Programs\n\nOther Programs\n\n"}
{"id": "10084984", "url": "https://en.wikipedia.org/wiki?curid=10084984", "title": "What Is This Thing Called Science?", "text": "What Is This Thing Called Science?\n\nWhat Is This Thing Called Science? is a best-selling textbook by Alan Chalmers. \n\nThe book is a guide to the philosophy of science which outlines the shortcomings of naive empiricist accounts of science, and describes and assesses modern attempts to replace them. The book is written with minimal use of technical terms. \"What Is This Thing Called Science?\" was first published in 1976, and has been translated into many languages.\n\n\n\n"}
{"id": "57537760", "url": "https://en.wikipedia.org/wiki?curid=57537760", "title": "William Phelps Ornithological Collection", "text": "William Phelps Ornithological Collection\n\nThe William Phelps Ornithological Collection, also known as the Phelps Ornithological Museum, is a museum of natural sciences dedicated to the study, exhibition and preservation of the birds of Venezuela and the rest of Latin America. The collection is located east of Caracas and in the geographic center of Greater Caracas, in the heart of the Sabana Grande district. The William Phelps ornithological collection is the most important in Latin America and it is also the most important private collection in the world in its research area. \n\nIn this private museum one will find important Phelps family study books, as well as 8000 scientific volumes in the library, more than 83,000 anatomical specimens, more than 80,000 skins, etc. For the year 1990, it was said that the William Phelps Ornithological Collection contained more than 76,300 skins and a small number of anatomical specimens, in the Gran Sabana Building of Sabana Grande. The Phelps library in 1990 already had 6,000 books, 800 journals and 5,500 reprints, mostly from natural sciences.\n\nThe ornithological collection was born in 1938, although it did not have its own headquarters on the Boulevard of Sabana Grande until 1949. At the beginning of 2018, it celebrated its 80th anniversary in Caracas, Venezuela. With the passing of time, the collection has been growing and still has great international scientific relevance. In 2005, an investigation was carried out on \"plumage differences in four subspecies of golden warbler Basileuterus culicivorus in Venezuela\". \n\nThe Phelps Foundation has been recognized worldwide for its scientific research. Since 1937, this foundation has dedicated to the study of the distribution of birds in Venezuela as well as to the dissemination of ornithology in Venezuela. Since 1949, it has expanded globally in its mission to discover, interpret and disseminate information about ornithology through a program of scientific research, education and dissemination in the natural sciences. The Foundation has had an important global trajectory for which it is recognized and is a regional compulsory study resource on tropical birds for experts who want to know more about this area. This museum has historically been connected to the American Museum of Natural History, thanks to the work of Billy Phelps. The ornithological collection has also been expanded thanks to the research carried out with Armando Dugand from Bogotá, Colombia. Most of the funds to carry out these investigations were collected by the Phelps Foundation.\n\nIn March and April 1977, the Phelps Ornithological Collection, with the collaboration of the Venezuelan-Brazilian Border Commission, Demarcador de Limites, carried out a collection of birds at Cerro Urutaní (62 ° 05'W, 3 ° 40'N), which is a low altitude tepui on the Venezuelan-Brazilian border in the Sierra Pacaraima. A total of 511 specimens of birds were collected between 1150 and 1280 meters high s.n.m., representing 78 different species. Gilberto Pérez Chinchilla, Manuel Castro and Dickerman prepared the copies of the collection. A full report on these birds was published in the international press and was published in the Bulletin of the \"American Museum of Natural History\", New York. It was necessary to work in conjunction with the Boundary Directorate of Venezuela.\n\n\n"}
