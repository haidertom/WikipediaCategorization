{"id": "509995", "url": "https://en.wikipedia.org/wiki?curid=509995", "title": "Abstract and concrete", "text": "Abstract and concrete\n\nAbstract and concrete are classifications that denote whether the object that a term describes has physical referents. Abstract objects have no physical referents, whereas concrete objects do. They are most commonly used in philosophy and semantics. Abstract objects are sometimes called abstracta (sing. abstractum) and concrete objects are sometimes called \"concreta\" (sing. \"concretum\"). An abstract object is an object that does not exist at any particular time or place, but rather exists as a type of thing—i.e., an idea, or abstraction. The term \"abstract object\" is said to have been coined by Willard Van Orman Quine. The study of abstract objects is called abstract object theory.\n\nThe type–token distinction identifies physical objects that are tokens of a particular type of thing. The \"type\" of which it is a part is in itself an abstract object. The abstract-concrete distinction is often introduced and initially understood in terms of paradigmatic examples of objects of each kind:\n\nAbstract objects have often garnered the interest of philosophers because they raise problems for popular theories. In ontology, abstract objects are considered problematic for physicalism and some forms of naturalism. Historically, the most important ontological dispute about abstract objects has been the problem of universals. In epistemology, abstract objects are considered problematic for empiricism. If abstracta lack causal powers or spatial location, how do we know about them? It is hard to say how they can affect our sensory experiences, and yet we seem to agree on a wide range of claims about them. \n\nSome, such as Edward Zalta and arguably, Plato in his Theory of Forms, have held that abstract objects constitute the defining subject matter of metaphysics or philosophical inquiry more broadly. To the extent that philosophy is independent of empirical research, and to the extent that empirical questions do not inform questions about abstracta, philosophy would seem especially suited to answering these latter questions. \n\nIn modern philosophy, the distinction between abstract and concrete was explored by Immanuel Kant and G. W. F. Hegel.\n\nGottlob Frege said that abstract objects, such as numbers, were members of a third realm, different from the external world or from internal consciousness. \n\nAnother popular proposal for drawing the abstract-concrete distinction contends that an object is abstract if it lacks any causal powers. A causal power has the ability to affect something causally. Thus, the empty set is abstract because it cannot act on other objects. One problem for this view is that it is not clear exactly what it is to have a causal power. For a more detailed exploration of the abstract-concrete distinction, follow the link below to the \"Stanford Encyclopedia\" article.\n\nJean Piaget uses the terms \"concrete\" and \"formal\" to describe the different types of learning. Concrete thinking involves facts and descriptions about everyday, tangible objects, while abstract (formal operational) thinking involves a mental process.\nRecently, there has been some philosophical interest in the development of a third category of objects known as the quasi-abstract. Quasi-abstract objects have drawn particular attention in the area of social ontology and documentality. Some argue that the over-adherence to the platonist duality of the concrete and the abstract has led to a large category of social objects having been overlooked or rejected as nonexisting because they exhibit characteristics that the traditional duality between concrete and abstract regards as incompatible. Specially, the ability to have temporal location, but not spatial location, and have causal agency (if only by acting through representatives). These characteristics are exhibited by a number of social objects, including states of the international legal system.\n\n"}
{"id": "48624755", "url": "https://en.wikipedia.org/wiki?curid=48624755", "title": "Acceptability", "text": "Acceptability\n\nAcceptability is the characteristic of a thing being subject to acceptance for some purpose. A thing is acceptable if it is sufficient to serve the purpose for which it is provided, even if it is far less usable for this purpose than the ideal example. A thing is unacceptable (or has the characteristic of unacceptability) if it deviates so far from the ideal that it is no longer sufficient to serve the desired purpose, or if it goes against that purpose. From a logical perspective, a thing can be said to be acceptable if it has no characteristics that make it unacceptable:\n\nHungarian mathematician Imre Lakatos developed a concept of acceptability \"taken as \"a measure of the approximation to the truth\"\". This concept was criticized in its applicability to philosophy as requiring that better theories first be eliminated. Acceptability is also a key premise of negotiation, wherein opposing sides each begin from a point of seeking their ideal solution, and compromise until they reach a solution that both sides find acceptable:\n\nWhere an unacceptable proposal has been made, \"a counterproposal is generated if there are any acceptable ones that have had already been explored\". Since the acceptability of proposition to a participant in a negotiation is only known to that participant, the participant may act as though a proposal that is actually acceptable to them is not, in order to obtain a more favorable proposal. \n\nOne concept of acceptability that has been widely studied is acceptable risk in situations affecting human health. The idea of not increasing lifetime risk by more than one in a million has become commonplace in public health discourse and policy. It is a heuristic measure. It provides a numerical basis for establishing a negligible increase in risk.\n\nEnvironmental decision making allows some discretion for deeming individual risks potentially \"acceptable\" if less than one in ten thousand chance of increased lifetime risk. Low risk criteria such as these provide some protection for a case where individuals may be exposed to multiple chemicals e.g. pollutants, food additives or other chemicals. In practice, a true zero-risk is possible only with the suppression of the risk-causing activity.\n\nStringent requirements of 1 in a million may not be technologically feasible or may be so prohibitively expensive as to render the risk-causing activity unsustainable, resulting in the optimal degree of intervention being a balance between risks vs. benefit. For example, emissions from hospital incinerators result in a certain number of deaths per year. However, this risk must be balanced against the alternatives. There are public health risks, as well as economic costs, associated with all options. The risk associated with no incineration is potential spread of infectious diseases, or even no hospitals. Further investigation identifies options such as separating noninfectious from infectious wastes, or air pollution controls on a medical incinerator.\n\nAcceptable variance is the range of variance in any direction from the ideal value that remains acceptable. In project management, variance can be defined as \"the difference between what is planned and what is actually achieved\". Degrees of variance \"can be classified into negative variance, zero variance, acceptable variance, and unacceptable variance\". In software testing, for example, \"[g]enerally 0-5% is considered as acceptable variance\" from an ideal value.\n\nAcceptance testing is a practice used in chemical and engineering fields, intended to check ahead of time whether or not a thing will be acceptable.\n"}
{"id": "2792", "url": "https://en.wikipedia.org/wiki?curid=2792", "title": "Anthropic principle", "text": "Anthropic principle\n\nThe anthropic principle is a philosophical consideration that observations of the universe must be compatible with the conscious and sapient life that observes it. Some proponents of the anthropic principle reason that it explains why this universe has the age and the fundamental physical constants necessary to accommodate conscious life. As a result, they believe it is unremarkable that this universe has fundamental constants that happen to fall within the narrow range thought to be compatible with life.\nThe strong anthropic principle (SAP) as explained by John D. Barrow and Frank Tipler states that this is all the case because the universe is in some sense compelled to eventually have conscious and sapient life emerge within it. Some critics of the SAP argue in favor of a weak anthropic principle (WAP) similar to the one defined by Brandon Carter, which states that the universe's ostensible fine tuning is the result of selection bias (specifically survivor bias): i.e., only in a universe capable of eventually supporting life will there be living beings capable of observing and reflecting on the matter. Most often such arguments draw upon some notion of the multiverse for there to be a statistical population of universes to select from and from which selection bias (our observance of \"only\" this universe, compatible with \"our\" life) could occur.\n\nThe principle was formulated as a response to a series of observations that the laws of nature and parameters of the universe take on values that are consistent with conditions for life as we know it rather than a set of values that would not be consistent with life on Earth. The anthropic principle states that this is a necessity, because if life were impossible, no living entity would be there to observe it, and thus would not be known. That is, it must be possible to observe \"some\" universe, and hence, the laws and constants of any such universe must accommodate that possibility.\n\nThe term \"anthropic\" in \"anthropic principle\" has been argued to be a misnomer. While singling out our kind of carbon-based life, none of the finely tuned phenomena require human life or some kind of carbon chauvinism. Any form of life or any form of heavy atom, stone, star or galaxy would do; nothing specifically human or anthropic is involved.\n\nThe anthropic principle has given rise to some confusion and controversy, partly because the phrase has been applied to several distinct ideas. All versions of the principle have been accused of discouraging the search for a deeper physical understanding of the universe. The anthropic principle is often criticized for lacking falsifiability and therefore critics of the anthropic principle may point out that the anthropic principle is a non-scientific concept, even though the weak anthropic principle, \"conditions that are observed in the universe must allow the observer to exist\", is \"easy\" to support in mathematics and philosophy, i.e. it is a tautology or truism. However, building a substantive argument based on a tautological foundation is problematic. Stronger variants of the anthropic principle are not tautologies and thus make claims considered controversial by some and that are contingent upon empirical verification.\n\nIn 1961, Robert Dicke noted that the age of the universe, as seen by living observers, cannot be random. Instead, biological factors constrain the universe to be more or less in a \"golden age\", neither too young nor too old. If the universe were one tenth as old as its present age, there would not have been sufficient time to build up appreciable levels of\nmetallicity (levels of elements besides hydrogen and helium) especially carbon, by nucleosynthesis. Small rocky planets did not yet exist. If the universe were 10 times older than it actually is, most stars would be too old to remain on the main sequence and would have turned into white dwarfs, aside from the dimmest red dwarfs, and stable planetary systems would have already come to an end. Thus, Dicke explained the coincidence between large dimensionless numbers constructed from the constants of physics and the age of the universe, a coincidence which had inspired Dirac's varying-G theory.\n\nDicke later reasoned that the density of matter in the universe must be almost exactly the critical density needed to prevent the Big Crunch (the \"Dicke coincidences\" argument). The most recent measurements may suggest that the observed density of baryonic matter, and some theoretical predictions of the amount of dark matter account for about 30% of this critical density, with the rest contributed by a cosmological constant. Steven Weinberg gave an anthropic explanation for this fact: he noted that the cosmological constant has a remarkably low value, some 120 orders of magnitude smaller than the value particle physics predicts (this has been described as the \"worst prediction in physics\"). However, if the cosmological constant were only several orders of magnitude larger than its observed value, the universe would suffer catastrophic inflation, which would preclude the formation of stars, and hence life.\n\nThe observed values of the dimensionless physical constants (such as the fine-structure constant) governing the four fundamental interactions are balanced as if fine-tuned to permit the formation of commonly found matter and subsequently the emergence of life. A slight increase in the strong interaction would bind the dineutron and the diproton, and nuclear fusion would have converted all hydrogen in the early universe to helium. Water, as well as sufficiently long-lived stable stars, both essential for the emergence of life as we know it, would not exist. More generally, small changes in the relative strengths of the four fundamental interactions can greatly affect the universe's age, structure, and capacity for life.\n\nArthur Schopenhauer was among the first atheist proponents of arguments along similar lines to the anthropic principle.\n\nThe phrase \"anthropic principle\" first appeared in Brandon Carter's contribution to a 1973 Kraków symposium honouring Copernicus's 500th birthday. Carter, a theoretical astrophysicist, articulated the Anthropic Principle in reaction to the Copernican Principle, which states that humans do not occupy a privileged position in the Universe. As Carter said: \"Although our situation is not necessarily \"central\", it is inevitably privileged to some extent.\" Specifically, Carter disagreed with using the Copernican principle to justify the Perfect Cosmological Principle, which states that all large regions \"and times\" in the universe must be statistically identical. The latter principle underlay the steady-state theory, which had recently been falsified by the 1965 discovery of the cosmic microwave background radiation. This discovery was unequivocal evidence that the universe has changed radically over time (for example, via the Big Bang).\n\nCarter defined two forms of the anthropic principle, a \"weak\" one which referred only to anthropic selection of privileged spacetime locations in the universe, and a more controversial \"strong\" form which addressed the values of the fundamental constants of physics.\n\nRoger Penrose explained the weak form as follows:\n\nOne reason this is plausible is that there are many other places and times in which we can imagine finding ourselves. But when applying the strong principle, we only have one universe, with one set of fundamental parameters, so what exactly is the point being made? Carter offers two possibilities: First, we can use our own existence to make \"predictions\" about the parameters. But second, \"as a last resort\", we can convert these predictions into \"explanations\" by assuming that there \"is\" more than one universe, in fact a large and possibly infinite collection of universes, something that is now called the multiverse (\"world ensemble\" was Carter's term), in which the parameters (and perhaps the laws of physics) vary across universes. The strong principle then becomes an example of a selection effect, exactly analogous to the weak principle. Postulating a multiverse is certainly a radical step, but taking it could provide at least a partial answer to a question which had seemed to be out of the reach of normal science: \"why do the fundamental laws of physics take the particular form we observe and not another?\"\n\nSince Carter's 1973 paper, the term \"anthropic principle\" has been extended to cover a number of ideas which differ in important ways from those he espoused. Particular confusion was caused in 1986 by the book \"The Anthropic Cosmological Principle\" by John D. Barrow and Frank Tipler, published that year which distinguished between \"weak\" and \"strong\" anthropic principle in a way very different from Carter's, as discussed in the next section.\n\nCarter was not the first to invoke some form of the anthropic principle. In fact, the evolutionary biologist Alfred Russel Wallace anticipated the anthropic principle as long ago as 1904: \"Such a vast and complex universe as that which we know exists around us, may have been absolutely required [...] in order to produce a world that should be precisely adapted in every detail for the orderly development of life culminating in man.\" In 1957, Robert Dicke wrote: \"The age of the Universe 'now' is not random but conditioned by biological factors [...] [changes in the values of the fundamental constants of physics] would preclude the existence of man to consider the problem.\"\n\nWeak anthropic principle (WAP) (Carter): \"[W]e must be prepared to take account of the fact that our location in the universe is \"necessarily\" privileged to the extent of being compatible with our existence as observers.\" Note that for Carter, \"location\" refers to our location in time as well as space.\n\nStrong anthropic principle (SAP) (Carter): \"[T]he universe (and hence the fundamental parameters on which it depends) must be such as to admit the creation of observers within it at some stage. To paraphrase Descartes, \"cogito ergo mundus talis est\".\"<br>The Latin tag (\"I think, therefore the world is such [as it is]\") makes it clear that \"must\" indicates a deduction from the fact of our existence; the statement is thus a truism.\n\nIn their 1986 book, \"The Anthropic Cosmological Principle\", John Barrow and Frank Tipler depart from Carter and define the WAP and SAP as follows:\n\nWeak anthropic principle (WAP) (Barrow and Tipler): \"The observed values of all physical and cosmological quantities are not equally probable but they take on values restricted by the requirement that there exist sites where carbon-based life can evolve and by the requirements that the universe be old enough for it to have already done so.\"<br>Unlike Carter they restrict the principle to carbon-based life, rather than just \"observers\". A more important difference is that they apply the WAP to the fundamental physical constants, such as the fine structure constant, the number of spacetime dimensions, and the cosmological constant—topics that fall under Carter's SAP.\n\nStrong anthropic principle (SAP) (Barrow and Tipler): \"The Universe must have those properties which allow life to develop within it at some stage in its history.\"<br>This looks very similar to Carter's SAP, but unlike the case with Carter's SAP, the \"must\" is an imperative, as shown by the following three possible elaborations of the SAP, each proposed by Barrow and Tipler:\n\nThe philosophers John Leslie and Nick Bostrom reject the Barrow and Tipler SAP as a fundamental misreading of Carter. For Bostrom, Carter's anthropic principle just warns us to make allowance for \"anthropic bias\"—that is, the bias created by anthropic selection effects (which Bostrom calls \"observation\" selection effects)—the necessity for observers to exist in order to get a result. He writes:\n\nStrong self-sampling assumption (SSSA) (Bostrom): \"Each observer-moment should reason as if it were randomly selected from the class of all observer-moments in its reference class.\"<br> Analysing an observer's experience into a sequence of \"observer-moments\" helps avoid certain paradoxes; but the main ambiguity is the selection of the appropriate \"reference class\": for Carter's WAP this might correspond to all real or potential observer-moments in our universe; for the SAP, to all in the multiverse. Bostrom's mathematical development shows that choosing either too broad or too narrow a reference class leads to counter-intuitive results, but he is not able to prescribe an ideal choice.\n\nAccording to Jürgen Schmidhuber, the anthropic principle essentially just says that the conditional probability of finding yourself in a universe compatible with your existence is always 1. It does not allow for any additional nontrivial predictions such as \"gravity won't change tomorrow\". To gain more predictive power, additional assumptions on the prior distribution of alternative universes are necessary.\n\nPlaywright and novelist Michael Frayn describes a form of the Strong Anthropic Principle in his 2006 book \"The Human Touch\", which explores what he characterises as \"the central oddity of the Universe\":\n\nCarter chose to focus on a tautological aspect of his ideas, which has resulted in much confusion. In fact, anthropic reasoning interests scientists because of something that is only implicit in the above formal definitions, namely that we should give serious consideration to there being other universes with different values of the \"fundamental parameters\"—that is, the dimensionless physical constants and initial conditions for the Big Bang. Carter and others have argued that life as we know it would not be possible in most such universes. In other words, the universe we are in is fine tuned to permit life. Collins & Hawking (1973) characterized Carter's then-unpublished big idea as the postulate that \"there is not one universe but a whole infinite ensemble of universes with all possible initial conditions\". If this is granted, the anthropic principle provides a plausible explanation for the fine tuning of our universe: the \"typical\" universe is not fine-tuned, but given enough universes, a small fraction thereof will be capable of supporting intelligent life. Ours must be one of these, and so the observed fine tuning should be no cause for wonder.\n\nAlthough philosophers have discussed related concepts for centuries, in the early 1970s the only genuine physical theory yielding a multiverse of sorts was the many-worlds interpretation of quantum mechanics. This would allow variation in initial conditions, but not in the truly fundamental constants. Since that time a number of mechanisms for producing a multiverse have been suggested: see the review by Max Tegmark. An important development in the 1980s was the combination of inflation theory with the hypothesis that some parameters are determined by symmetry breaking in the early universe, which allows parameters previously thought of as \"fundamental constants\" to vary over very large distances, thus eroding the distinction between Carter's weak and strong principles. At the beginning of the 21st century, the string landscape emerged as a mechanism for varying essentially all the constants, including the number of spatial dimensions.\n\nThe anthropic idea that fundamental parameters are selected from a multitude of different possibilities (each actual in some universe or other) contrasts with the traditional hope of physicists for a theory of everything having no free parameters. As Einstein said: \"What really interests me is whether God had any choice in the creation of the world.\" In 2002, proponents of the leading candidate for a \"theory of everything\", string theory, proclaimed \"the end of the anthropic principle\" since there would be no free parameters to select. String theory now seems to offer no hope of predicting fundamental parameters, and now some who advocate it invoke the anthropic principle as well (see below).\n\nThe modern form of a design argument is put forth by intelligent design. Proponents of intelligent design often cite the fine-tuning observations that (in part) preceded the formulation of the anthropic principle by Carter as a proof of an intelligent designer. Opponents of intelligent design are not limited to those who hypothesize that other universes exist; they may also argue, anti-anthropically, that the universe is less fine-tuned than often claimed, or that accepting fine tuning as a brute fact is less astonishing than the idea of an intelligent creator. Furthermore, even accepting fine tuning, Sober (2005) and Ikeda and Jefferys, argue that the Anthropic Principle as conventionally stated actually undermines intelligent design; see fine-tuned universe.\n\nPaul Davies's book \"The Goldilocks Enigma\" (2006) reviews the current state of the fine tuning debate in detail, and concludes by enumerating the following responses to that debate:\n\nOmitted here is Lee Smolin's model of cosmological natural selection, also known as \"fecund universes\", which proposes that universes have \"offspring\" which are more plentiful if they resemble our universe. Also see Gardner (2005).\n\nClearly each of these hypotheses resolve some aspects of the puzzle, while leaving others unanswered. Followers of Carter would admit only option 3 as an anthropic explanation, whereas 3 through 6 are covered by different versions of Barrow and Tipler's SAP (which would also include 7 if it is considered a variant of 4, as in Tipler 1994).\n\nThe anthropic principle, at least as Carter conceived it, can be applied on scales much smaller than the whole universe. For example, Carter (1983) inverted the usual line of reasoning and pointed out that when interpreting the evolutionary record, one must take into account cosmological and astrophysical considerations. With this in mind, Carter concluded that given the best estimates of the age of the universe, the evolutionary chain culminating in \"Homo sapiens\" probably admits only one or two low probability links.\n\nNo possible observational evidence bears on Carter's WAP, as it is merely advice to the scientist and asserts nothing debatable. The obvious test of Barrow's SAP, which says that the universe is \"required\" to support life, is to find evidence of life in universes other than ours. Any other universe is, by most definitions, unobservable (otherwise it would be included in \"our\" portion of \"this\" universe). Thus, in principle Barrow's SAP cannot be falsified by observing a universe in which an observer cannot exist.\n\nPhilosopher John Leslie states that the Carter SAP (with multiverse) predicts the following:\n\nHogan has emphasised that it would be very strange if all fundamental constants were strictly determined, since this would leave us with no ready explanation for apparent fine tuning. In fact we might have to resort to something akin to Barrow and Tipler's SAP: there would be no option for such a universe \"not\" to support life.\n\nProbabilistic predictions of parameter values can be made given:\nThe probability of observing value \"X\" is then proportional to \"N\"(\"X\") \"P\"(\"X\"). A generic feature of an analysis of this nature is that the expected values of the fundamental physical constants should not be \"over-tuned\", i.e. if there is some perfectly tuned predicted value (e.g. zero), the observed value need be no closer to that predicted value than what is required to make life possible. The small but finite value of the cosmological constant can be regarded as a successful prediction in this sense.\n\nOne thing that would \"not\" count as evidence for the Anthropic Principle is evidence that the Earth or the solar system occupied a privileged position in the universe, in violation of the Copernican principle (for possible counterevidence to this principle, see Copernican principle), unless there was some reason to think that that position was a necessary condition for our existence as observers.\n\nFred Hoyle may have invoked anthropic reasoning to predict an astrophysical phenomenon. He is said to have reasoned from the prevalence on Earth of life forms whose chemistry was based on carbon-12 atoms, that there must be an undiscovered resonance in the carbon-12 nucleus facilitating its synthesis in stellar interiors via the triple-alpha process. He then calculated the energy of this undiscovered resonance to be 7.6 million electronvolts. Willie Fowler's research group soon found this resonance, and its measured energy was close to Hoyle's prediction.\n\nHowever, a recently released paper argues that Hoyle did not use anthropic reasoning to make this prediction.\n\nDon Page criticized the entire theory of cosmic inflation as follows. He emphasized that initial conditions which made possible a thermodynamic arrow of time in a universe with a Big Bang origin, must include the assumption that at the initial singularity, the entropy of the universe was low and therefore extremely improbable. Paul Davies rebutted this criticism by invoking an inflationary version of the anthropic principle. While Davies accepted the premise that the initial state of the visible universe (which filled a microscopic amount of space before inflating) had to possess a very low entropy value—due to random quantum fluctuations—to account for the observed thermodynamic arrow of time, he deemed this fact an advantage for the theory. That the tiny patch of space from which our observable universe grew had to be extremely orderly, to allow the post-inflation universe to have an arrow of time, makes it unnecessary to adopt any \"ad hoc\" hypotheses about the initial entropy state, hypotheses other Big Bang theories require.\n\nString theory predicts a large number of possible universes, called the \"backgrounds\" or \"vacua\". The set of these vacua is often called the \"multiverse\" or \"anthropic landscape\" or \"string landscape\". Leonard Susskind has argued that the existence of a large number of vacua puts anthropic reasoning on firm ground: only universes whose properties are such as to allow observers to exist are observed, while a possibly much larger set of universes lacking such properties go unnoticed.\n\nSteven Weinberg believes the Anthropic Principle may be appropriated by cosmologists committed to nontheism, and refers to that Principle as a \"turning point\" in modern science because applying it to the string landscape \" [...] may explain how the constants of nature that we observe can take values suitable for life without being fine-tuned by a benevolent creator\". Others—most notably David Gross but also Lubos Motl, Peter Woit, and Lee Smolin—argue that this is not predictive. Max Tegmark, Mario Livio, and Martin Rees argue that only some aspects of a physical theory need be observable and/or testable for the theory to be accepted, and that many well-accepted theories are far from completely testable at present.\n\nJürgen Schmidhuber (2000–2002) points out that Ray Solomonoff's theory of universal inductive inference and its extensions already provide a framework for maximizing our confidence in any theory, given a limited sequence of physical observations, and some prior distribution on the set of possible explanations of the universe.\n\nThere are two kinds of dimensions, spatial (bidirectional) and temporal (unidirectional). Let the number of spatial dimensions be \"N\" and the number of temporal dimensions be \"T\". That \"N\" = 3 and \"T\" = 1, setting aside the compactified dimensions invoked by string theory and undetectable to date, can be explained by appealing to the physical consequences of letting \"N\" differ from 3 and \"T\" differ from 1. The argument is often of an anthropic character and possibly the first of its kind, albeit before the complete concept came into vogue. \n\nThe implicit notion that the dimensionality of the universe is special is first attributed to Gottfried Wilhelm Leibniz, who in the Discourse on Metaphysics suggested that the world is Immanuel Kant argued that 3-dimensional space was a consequence of the inverse square law of universal gravitation. While Kant's argument is historically important, John D. Barrow says that it \"[...] gets the punch-line back to front: it is the three-dimensionality of space that explains why we see inverse-square force laws in Nature, not vice-versa\" (Barrow 2002: 204).\n\nIn 1920, Paul Ehrenfest showed that if there is only one time dimension and greater than three spatial dimensions, the orbit of a planet about its Sun cannot remain stable. The same is true of a star's orbit around the center of its galaxy. Ehrenfest also showed that if there are an even number of spatial dimensions, then the different parts of a wave impulse will travel at different speeds. If there are formula_1 spatial dimensions, where \"k\" is a whole number, then wave impulses become distorted. In 1922, Hermann Weyl showed that Maxwell's theory of electromagnetism works only with three dimensions of space and one of time. Finally, Tangherlini showed in 1963 that when there are more than three spatial dimensions, electron orbitals around nuclei cannot be stable; electrons would either fall into the nucleus or disperse.\n\nMax Tegmark expands on the preceding argument in the following anthropic manner. If \"T\" differs from 1, the behavior of physical systems could not be predicted reliably from knowledge of the relevant partial differential equations. In such a universe, intelligent life capable of manipulating technology could not emerge. Moreover, if \"T\" > 1, Tegmark maintains that protons and electrons would be unstable and could decay into particles having greater mass than themselves. (This is not a problem if the particles have a sufficiently low temperature.) If \"N\" < 3, gravitation of any kind becomes problematic, and the universe is probably too simple to contain observers. For example, when \"N\" < 3, nerves cannot cross without intersecting.\n\nIn general, it is not clear how physical law could function if \"T\" differed from 1. If \"T\" > 1, subatomic particles which decay after a fixed period would not behave predictably, because time-like geodesics would not be necessarily maximal. \"N\" = 1 and \"T\" = 3 has the peculiar property that the speed of light in a vacuum is a \"lower bound\" on the velocity of matter; all matter consists of tachyons. \n\nHence anthropic and other arguments rule out all cases except \"N\" = 3 and \"T\" = 1, which happens to describe the world around us.\n\nSome of the metaphysical disputes and speculations include, for example, attempts to back Teilhard de Chardin's earlier interpretation of the universe as being Christ centered (compare Omega Point), expressing a \"creatio evolutiva\" instead the elder notion of \"creatio continua\". From a strictly secular, humanist perspective, it allows as well to put human beings back in the center, an anthropogenic shift in cosmology. Karl W. Giberson has been sort of laconic in stating that\n\nA thorough extant study of the anthropic principle is the book \"The Anthropic Cosmological Principle\" by John D. Barrow, a cosmologist, and Frank J. Tipler, a cosmologist and mathematical physicist. This book sets out in detail the many known anthropic coincidences and constraints, including many found by its authors. While the book is primarily a work of theoretical astrophysics, it also touches on quantum physics, chemistry, and earth science. An entire chapter argues that \"Homo sapiens\" is, with high probability, the only intelligent species in the Milky Way.\n\nThe book begins with an extensive review of many topics in the history of ideas the authors deem relevant to the anthropic principle, because the authors believe that principle has important antecedents in the notions of teleology and intelligent design. They discuss the writings of Fichte, Hegel, Bergson, and Alfred North Whitehead, and the Omega Point cosmology of Teilhard de Chardin. Barrow and Tipler carefully distinguish teleological reasoning from \"eutaxiological\" reasoning; the former asserts that order must have a consequent purpose; the latter asserts more modestly that order must have a planned cause. They attribute this important but nearly always overlooked distinction to an obscure 1883 book by L. E. Hicks.\n\nSeeing little sense in a principle requiring intelligent life to emerge while remaining indifferent to the possibility of its eventual extinction, Barrow and Tipler propose the final anthropic principle (FAP): Intelligent information-processing must come into existence in the universe, and, once it comes into existence, it will never die out.\n\nBarrow and Tipler submit that the FAP is both a valid physical statement and \"closely connected with moral values\". FAP places strong constraints on the structure of the universe, constraints developed further in Tipler's \"The Physics of Immortality\". One such constraint is that the universe must end in a big crunch, which seems unlikely in view of the tentative conclusions drawn since 1998 about dark energy, based on observations of very distant supernovas.\n\nIn his review of Barrow and Tipler, Martin Gardner ridiculed the FAP by quoting the last two sentences of their book as defining a Completely Ridiculous Anthropic Principle (CRAP):\n\nCarter has frequently regretted his own choice of the word \"anthropic\", because it conveys the misleading impression that the principle involves humans specifically, rather than intelligent observers in general. Others have criticised the word \"principle\" as being too grandiose to describe straightforward applications of selection effects.\n\nA common criticism of Carter's SAP is that it is an easy deus ex machina which discourages searches for physical explanations. To quote Penrose again: \"[I]t tends to be invoked by theorists whenever they do not have a good enough theory to explain the observed facts.\"\n\nCarter's SAP and Barrow and Tipler's WAP have been dismissed as truisms or trivial tautologies—that is, statements true solely by virtue of their logical form (the conclusion is identical to the premise) and not because a substantive claim is made and supported by observation of reality. As such, they are criticized as an elaborate way of saying \"if things were different, they would be different\", which is a valid statement, but does not make a claim of some factual alternative over another.\n\nCritics of the Barrow and Tipler SAP claim that it is neither testable nor falsifiable, and thus is not a scientific statement but rather a philosophical one. The same criticism has been leveled against the hypothesis of a multiverse, although some argue that it does make falsifiable predictions. A modified version of this criticism is that we understand so little about the emergence of life, especially intelligent life, that it is effectively impossible to calculate the number of observers in each universe. Also, the prior distribution of universes as a function of the fundamental constants is easily modified to get any desired result.\n\nMany criticisms focus on versions of the strong anthropic principle, such as Barrow and Tipler's \"anthropic cosmological principle\", which are teleological notions that tend to describe the existence of life as a \"necessary prerequisite\" for the observable constants of physics. Similarly, Stephen Jay Gould, Michael Shermer, and others claim that the stronger versions of the anthropic principle seem to reverse known causes and effects. Gould compared the claim that the universe is fine-tuned for the benefit of our kind of life to saying that sausages were made long and narrow so that they could fit into modern hotdog buns, or saying that ships had been invented to house barnacles. These critics cite the vast physical, fossil, genetic, and other biological evidence consistent with life having been fine-tuned through natural selection to adapt to the physical and geophysical environment in which life exists. Life appears to have adapted to the universe, and not vice versa.\n\nSome applications of the anthropic principle have been criticized as an argument by lack of imagination, for tacitly assuming that carbon compounds and water are the only possible chemistry of life (sometimes called \"carbon chauvinism\", see also alternative biochemistry). The range of fundamental physical constants consistent with the evolution of carbon-based life may also be wider than those who advocate a fine tuned universe have argued. For instance, Harnik et al. propose a weakless universe in which the weak nuclear force is eliminated. They show that this has no significant effect on the other fundamental interactions, provided some adjustments are made in how those interactions work. However, if some of the fine-tuned details of our universe were violated, that would rule out complex structures of any kind—stars, planets, galaxies, etc.\n\nLee Smolin has offered a theory designed to improve on the lack of imagination that anthropic principles have been accused of. He puts forth his fecund universes theory, which assumes universes have \"offspring\" through the creation of black holes whose offspring universes have values of physical constants that depend on those of the mother universe.\n\nThe philosophers of cosmology John Earman, Ernan McMullin, and Jesús Mosterín contend that \"in its weak version, the anthropic principle is a mere tautology, which does not allow us to explain anything or to predict anything that we did not already know. In its strong version, it is a gratuitous speculation\". A further criticism by Mosterín concerns the flawed \"anthropic\" inference from the assumption of an infinity of worlds to the existence of one like ours:\n\n\n"}
{"id": "8934226", "url": "https://en.wikipedia.org/wiki?curid=8934226", "title": "Basic limiting principle", "text": "Basic limiting principle\n\nA Basic Limiting Principle (B.L.P.) is a general principle that limits our explanations metaphysically or epistemologically, and which normally goes unquestioned or even unnoticed in our everyday or scientific thinking. The term was introduced by the philosopher C. D. Broad in his 1949 paper \"The Relevance of Psychical research to Philosophy\":\n\n\"There are certain limiting principles which we unhesitatingly take for granted as the framework within which all our practical activities and our scientific theories are confined. Some of these seem to be self-evident. Others are so overwhelmingly supported by all the empirical facts which fall within the range of ordinary experience and the scientific elaborations of it (including under this heading orthodox psychology) that it hardly enters our heads to question them. Let us call these Basic Limiting Principles.\"\n\nBroad offers nine examples of B.L.P.s, including the principle that there can be no backward causation, that there can be no action at a distance, and that one cannot perceive physical events or material things directly, unmediated by sensations.\n\n"}
{"id": "714069", "url": "https://en.wikipedia.org/wiki?curid=714069", "title": "Church–Turing–Deutsch principle", "text": "Church–Turing–Deutsch principle\n\nIn computer science and quantum physics, the Church–Turing–Deutsch principle (CTD principle) is a stronger, physical form of the Church–Turing thesis formulated by David Deutsch in 1985.\n\nThe principle states that a universal computing device can simulate every physical process.\n\nThe principle was stated by Deutsch in 1985 with respect to finitary machines and processes. He observed that classical physics, which makes use of the concept of real numbers, cannot be simulated by a Turing machine, which can only represent computable reals. Deutsch proposed that quantum computers may actually obey the CTD principle, assuming that the laws of quantum physics can completely describe every physical process.\n\nAn earlier version of this thesis for classical computers was stated by Alan Turing's friend and student Robin Gandy in 1980.\n\n\n"}
{"id": "178942", "url": "https://en.wikipedia.org/wiki?curid=178942", "title": "Conceptual art", "text": "Conceptual art\n\nConceptual art, sometimes simply called conceptualism, is art in which the concept(s) or idea(s) involved in the work take precedence over traditional aesthetic, technical, and material concerns. Some works of conceptual art, sometimes called installations, may be constructed by anyone simply by following a set of written instructions. This method was fundamental to American artist Sol LeWitt's definition of Conceptual art, one of the first to appear in print:\n\nTony Godfrey, author of \"Conceptual Art (Art & Ideas)\" (1998), asserts that conceptual art questions the nature of art, a notion that Joseph Kosuth elevated to a definition of art itself in his seminal, early manifesto of conceptual art, \"Art after Philosophy\" (1969). The notion that art should examine its own nature was already a potent aspect of the influential art critic Clement Greenberg's vision of Modern art during the 1950s. With the emergence of an exclusively language-based art in the 1960s, however, conceptual artists such as Art & Language, Joseph Kosuth (who became the american editor of Art-Language), and Lawrence Weiner began a far more radical interrogation of art than was previously possible (see below). One of the first and most important things they questioned was the common assumption that the role of the artist was to create special kinds of material objects.\nThrough its association with the Young British Artists and the Turner Prize during the 1990s, in popular usage, particularly in the UK, \"conceptual art\" came to denote all contemporary art that does not practice the traditional skills of painting and sculpture. It could be said that one of the reasons why the term \"conceptual art\" has come to be associated with various contemporary practices far removed from its original aims and forms lies in the problem of defining the term itself. As the artist Mel Bochner suggested as early as 1970, in explaining why he does not like the epithet \"conceptual\", it is not always entirely clear what \"concept\" refers to, and it runs the risk of being confused with \"intention\". Thus, in describing or defining a work of art as conceptual it is important not to confuse what is referred to as \"conceptual\" with an artist's \"intention\".\n\nThe French artist Marcel Duchamp paved the way for the conceptualists, providing them with examples of prototypically conceptual works — the readymades, for instance. The most famous of Duchamp's readymades was \"Fountain\" (1917), a standard urinal-basin signed by the artist with the pseudonym \"R.Mutt\", and submitted for inclusion in the annual, un-juried exhibition of the Society of Independent Artists in New York (which rejected it). The artistic tradition does not see a commonplace object (such as a urinal) as art because it is not made by an artist or with any intention of being art, nor is it unique or hand-crafted. Duchamp's relevance and theoretical importance for future \"conceptualists\" was later acknowledged by US artist Joseph Kosuth in his 1969 essay, \"Art after Philosophy\", when he wrote: \"All art (after Duchamp) is conceptual (in nature) because art only exists conceptually\".\n\nIn 1956 the founder of Lettrism, Isidore Isou, developed the notion of a work of art which, by its very nature, could never be created in reality, but which could nevertheless provide aesthetic rewards by being contemplated intellectually. This concept, also called \"Art esthapériste\" (or \"infinite-aesthetics\"), derived from the infinitesimals of Gottfried Wilhelm Leibniz – quantities which could not actually exist except conceptually. The current incarnation () of the Isouian movement, Excoördism, self-defines as the art of the infinitely large and the infinitely small.\n\nIn 1961 the term \"concept art\", coined by the artist Henry Flynt in his article bearing the term as its title, appeared in a proto-Fluxus publication \"An Anthology of Chance Operations\". \nHowever, it assumed a different meaning when employed by Joseph Kosuth and by the English Art and Language group, who discarded the conventional art object in favour of a documented critical inquiry, that began in Art-Language The Journal of conceptual art in 1969, into the artist's social , philosophical and psychological status. By the mid-1970s they had produced publications, indices, performances, texts and paintings to this end. In 1970 \"Conceptual Art and Conceptual Aspects\", the first dedicated conceptual-art exhibition, took place at the New York Cultural Center.\n\nConceptual art emerged as a movement during the 1960s – in part as a reaction against formalism as then articulated by the influential New York art critic Clement Greenberg. According to Greenberg Modern art followed a process of progressive reduction and refinement toward the goal of defining the essential, formal nature of each medium. Those elements that ran counter to this nature were to be reduced. The task of painting, for example, was to define precisely what kind of object a painting truly is: what makes it a painting and nothing else. As it is of the nature of paintings to be flat objects with canvas surfaces onto which colored pigment is applied, such things as figuration, 3-D perspective illusion and references to external subject matter were all found to be extraneous to the essence of painting, and ought to be removed.\n\nSome have argued that conceptual art continued this \"dematerialization\" of art by removing the need for objects altogether,\nwhile others, including many of the artists themselves, saw conceptual art as a radical break with Greenberg's kind of formalist Modernism. Later artists continued to share a preference for art to be self-critical, as well as a distaste for illusion. However, by the end of the 1960s it was certainly clear that Greenberg's stipulations for art to continue within the confines of each medium and to exclude external subject matter no longer held traction.\nConceptual art also reacted against the commodification of art; it attempted a subversion of the gallery or museum as the location and determiner of art, and the art market as the owner and distributor of art. Lawrence Weiner said: \"Once you know about a work of mine you own it. There's no way I can climb inside somebody's head and remove it.\" Many conceptual artists' work can therefore only be known about through documentation which is manifested by it, e.g. photographs, written texts or displayed objects, which some might argue are not in themselves the art. It is sometimes (as in the work of Robert Barry, Yoko Ono, and Weiner himself) reduced to a set of written instructions describing a work, but stopping short of actually making it—emphasising the idea as more important than the artifact. This reveals an explicit preference for the \"art\" side of the ostensible dichotomy between art and craft, where art, unlike craft, takes place within and engages historical discourse: for example, Ono's \"written instructions\" make more sense alongside other conceptual art of the time.\n\nLanguage was a central concern for the first wave of conceptual artists of the 1960s and early 1970s. Although the utilisation of text in art was in no way novel, only in the 1960s did the artists Lawrence Weiner, Edward Ruscha, Joseph Kosuth, Robert Barry, and Art & Language begin to produce art by exclusively linguistic means. Where previously language was presented as one kind of visual element alongside others, and subordinate to an overarching composition (e.g. Synthetic Cubism), the conceptual artists used language in place of brush and canvas, and allowed it to signify in its own right. Of Lawrence Weiner's works Anne Rorimer writes, \"The thematic content of individual works derives solely from the import of the language employed, while presentational means and contextual placement play crucial, yet separate, roles.\"\n\nThe British philosopher and theorist of conceptual art Peter Osborne suggests that among the many factors that influenced the gravitation toward language-based art, a central role for conceptualism came from the turn to linguistic theories of meaning in both Anglo-American analytic philosophy, and structuralist and post structuralist Continental philosophy during the middle of the twentieth century. This linguistic turn \"reinforced and legitimized\" the direction the conceptual artists took. Osborne also notes that the early conceptualists were the first generation of artists to complete degree-based university training in art. Osborne later made the observation that contemporary art is \"post-conceptual\" in a public lecture delivered at the Fondazione Antonio Ratti, Villa Sucota in Como on July 9, 2010. It is a claim made at the level of the ontology of the work of art (rather than say at the descriptive level of style or movement).\n\nThe American art historian Edward A. Shanken points to the example of Roy Ascott who \"powerfully demonstrates the significant intersections between conceptual art and art-and-technology, exploding the conventional autonomy of these art-historical categories.\" Ascott, the British artist most closely associated with cybernetic art in England, was not included in Cybernetic Serendipity because his use of cybernetics was primarily conceptual and did not explicitly utilize technology. Conversely, although his essay on the application of cybernetics to art and art pedagogy, \"The Construction of Change\" (1964), was quoted on the dedication page (to Sol Lewitt) of Lucy R. Lippard's seminal \"Six Years: The Dematerialization of the Art Object from 1966 to 1972\", Ascott's anticipation of and contribution to the formation of conceptual art in Britain has received scant recognition, perhaps (and ironically) because his work was too closely allied with art-and-technology. Another vital intersection was explored in Ascott's use of the thesaurus in 1963 , which drew an explicit parallel between the taxonomic qualities of verbal and visual languages – a concept would be taken up in Joseph Kosuth's \"Second Investigation, Proposition 1\" (1968) and Mel Ramsden's \"Elements of an Incomplete Map\" (1968).\n\n\"By adopting language as their exclusive medium, Weiner, Barry, Wilson, Kosuth and Art & Language were able to sweep aside the vestiges of authorial presence manifested by formal invention and the handling of materials.\"\nAn important difference between conceptual art and more \"traditional\" forms of art-making goes to the question of artistic skill. Although skill in the handling of traditional media often plays little role in conceptual art, it is difficult to argue that no skill is required to make conceptual works, or that skill is always absent from them. John Baldessari, for instance, has presented realist pictures that he commissioned professional sign-writers to paint; and many conceptual performance artists (e.g. Stelarc, Marina Abramović) are technically accomplished performers and skilled manipulators of their own bodies. It is thus not so much an absence of skill or hostility toward tradition that defines conceptual art as an evident disregard for conventional, modern notions of authorial presence and of individual artistic expression.\n\nThe first wave of the \"conceptual art\" movement extended from approximately 1967 to 1978. Early \"concept\" artists like Henry Flynt, Robert Morris, and Ray Johnson influenced the later, widely accepted movement of conceptual art. Conceptual artists like Dan Graham, Hans Haacke, and Lawrence Weiner have proven very influential on subsequent artists, and well known contemporary artists such as Mike Kelley or Tracey Emin are sometimes labeled \"second- or third-generation\" conceptualists, or \"post-conceptual\" artists.\n\nMany of the concerns of the conceptual art movement have been taken up by contemporary artists. While they may or may not term themselves \"conceptual artists\", ideas such as anti-commodification, social and/or political critique, and ideas/information as medium continue to be aspects of contemporary art, especially among artists working with installation art, performance art, net.art and electronic/digital art.\n\n\n\nBooks\n\n\nEssays\n\n\nExhibition catalogues\n\n\n"}
{"id": "33346439", "url": "https://en.wikipedia.org/wiki?curid=33346439", "title": "Conceptual design", "text": "Conceptual design\n\nConceptual Design is an early phase of the design process, in which the broad outlines of function and form of something are articulated. It includes the design of interactions, experiences, processes and strategies. It involves an understanding of people's needs - and how to meet them with products, services, & processes. Common artifacts of conceptual design are concept sketches and models.\n\n"}
{"id": "58267", "url": "https://en.wikipedia.org/wiki?curid=58267", "title": "Conceptual schema", "text": "Conceptual schema\n\nA 'conceptual schema' is a high-level description of a business's informational needs. It typically includes only the main concepts and the main relationships among them. Typically this is a first-cut model, with insufficient detail to build an actual database. This level describes the structure of the whole database for a group of users. The conceptual model is also known as the data model that can be used to describe the conceptual schema when a database system is implemented. It hides the internal details of physical storage and targets on describing entities, datatype, relationships and constraints.\n\nA conceptual schema or conceptual data model is a map of concepts and their relationships used for databases. This describes the semantics of an organization and represents a series of assertions about its nature. Specifically, it describes the things of significance to an organization (\"entity classes\"), about which it is inclined to collect information, and characteristics of (\"attributes\") and associations between pairs of those things of significance (\"relationships\").\n\nBecause a conceptual schema represents the semantics of an organization, and not a database design, it may exist on various levels of abstraction. The original ANSI four-schema architecture began with the set of \"external schema\" that each represent one person's view of the world around him or her. These are consolidated into a single \"conceptual schema\" that is the superset of all of those external views. A data model can be as concrete as each person's perspective, but this tends to make it inflexible. If that person's world changes, the model must change. Conceptual data models take a more abstract perspective, identifying the fundamental things, of which the things an individual deals with are just examples.\n\nThe model does allow for what is called inheritance in object oriented terms. The set of instances of an entity class may be subdivided into entity classes in their own right. Thus, each instance of a \"sub-type\" entity class is also an instance of the entity class's \"super-type\". Each instance of the super-type entity class, then is also an instance of one of the sub-type entity classes.\n\nSuper-type/sub-type relationships may be \"exclusive\" or not. A methodology may require that each instance of a super-type may \"only\" be an instance of \"one\" sub-type. Similarly, a super-type/sub-type relationship may be \"exhaustive\" or not. It is exhaustive if the methodology requires that each instance of a super-type \"must be\" an instance of a sub-type. A sub-type named other is often necessary.\n\n\nA data structure diagram (DSD) is a data model or diagram used to describe conceptual data models by providing graphical notations which document entities and their relationships, and the constraints that bind them.\n\n\n\n"}
{"id": "1845675", "url": "https://en.wikipedia.org/wiki?curid=1845675", "title": "Deductive-nomological model", "text": "Deductive-nomological model\n\nThe deductive-nomological model (DN model), also known as Hempel's model, the Hempel–Oppenheim model, the Popper–Hempel model, or the covering law model, is a formal view of scientifically answering questions asking, \"Why...?\". The DN model poses scientific explanation as a deductive structure—that is, one where truth of its premises entails truth of its conclusion—hinged on accurate prediction or postdiction of the phenomenon to be explained.\n\nBecause of problems concerning humans' ability to define, discover, and know causality, it was omitted in initial formulations of the DN model. Causality was thought to be incidentally approximated by realistic selection of premises that \"derive\" the phenomenon of interest from observed starting conditions plus general laws. Still, DN model formally permitted causally irrelevant factors. Also, derivability from observations and laws sometimes yielded absurd answers.\n\nWhen logical empiricism fell out of favor in the 1960s, the DN model was widely seen as a flawed or greatly incomplete model of scientific explanation. Nonetheless, it remained an idealized version of scientific explanation, and one that was rather accurate when applied to modern physics. In the early 1980s, revision to DN model emphasized \"maximal specificity\" for relevance of the conditions and axioms stated. Together with Hempel's inductive-statistical model, the DN model forms scientific explanation's covering law model, which is also termed, from critical angle, subsumption theory.\n\nThe term \"deductive\" distinguishes the DN model's intended determinism from the probabilism of inductive inferences. The term \"nomological\" is derived from the Greek word \"νόμος\" or \"nomos\", meaning \"law\". The DN model holds to a view of scientific explanation whose \"conditions of adequacy\" (CA)—semiformal but stated classically—are \"derivability\" (CA1), \"lawlikeness\" (CA2), \"empirical content\" (CA3), and \"truth\" (CA4).\n\nIn the DN model, a law axiomatizes an unrestricted generalization from antecedent \"A\" to consequent \"B\" by conditional proposition—\"If A, then B\"—and has empirical content testable. A law differs from mere true regularity—for instance, \"George always carries only $1 bills in his wallet\"—by supporting counterfactual claims and thus suggesting what \"must\" be true, while following from a scientific theory's axiomatic structure.\n\nThe phenomenon to be explained is the explanandum—an event, law, or theory—whereas the premises to explain it are explanans, true or highly confirmed, containing at least one universal law, and entailing the explanandum. Thus, given the explanans as initial, specific conditions \"C, C . . . C\" plus general laws \"L, L . . . L\", the phenomenon \"E\" as explanandum is a deductive consequence, thereby scientifically explained.\n\nAristotle's scientific explanation in \"Physics\" resembles the DN model, an idealized form of scientific explanation. The framework of Aristotelian physics—Aristotelian metaphysics—reflected the perspective of this principally biologist, who, amid living entities' undeniable purposiveness, formalized vitalism and teleology, an intrinsic morality in nature. With emergence of Copernicanism, however, Descartes introduced mechanical philosophy, then Newton rigorously posed lawlike explanation, both Descartes and especially Newton shunning teleology within natural philosophy. At 1740, David Hume staked Hume's fork, highlighted the problem of induction, and found humans ignorant of either necessary or sufficient causality. Hume also highlighted the fact/value gap, as what \"is\" does not itself reveal what \"ought\".\n\nNear 1780, countering Hume's ostensibly radical empiricism, Immanuel Kant highlighted extreme rationalism—as by Descartes or Spinoza—and sought middle ground. Inferring the mind to arrange experience of the world into \"substance\", \"space\", and \"time\", Kant placed the mind as part of the causal constellation of experience and thereby found Newton's theory of motion universally true, yet knowledge of things in themselves impossible. Safeguarding science, then, Kant paradoxically stripped it of scientific realism. Aborting Francis Bacon's inductivist mission to dissolve the veil of appearance to uncover the \"noumena\"—metaphysical view of nature's ultimate truths—Kant's transcendental idealism tasked science with simply modeling patterns of \"phenomena\". Safeguarding metaphysics, too, it found the mind's constants holding also universal moral truths, and launched German idealism, increasingly speculative.\n\nAuguste Comte found the problem of induction rather irrelevant since enumerative induction is grounded on the empiricism available, while science's point is not metaphysical truth. Comte found human knowledge had evolved from theological to metaphysical to scientific—the ultimate stage—rejecting both theology and metaphysics as asking questions unanswerable and posing answers unverifiable. Comte in the 1830s expounded positivism—the first modern philosophy of science and simultaneously a political philosophy—rejecting conjectures about unobservables, thus rejecting search for \"causes\". Positivism predicts observations, confirms the predictions, and states a \"law\", thereupon applied to benefit human society. From late 19th century into the early 20th century, the influence of positivism spanned the globe. Meanwhile, evolutionary theory's natural selection brought the Copernican Revolution into biology and eventuated in the first conceptual alternative to vitalism and teleology.\n\nWhereas Comtean positivism posed science as \"description\", logical positivism emerged in the late 1920s and posed science as \"explanation\", perhaps to better unify empirical sciences by covering not only fundamental science—that is, fundamental physics—but special sciences, too, such as biology, psychology, economics, and anthropology. After defeat of National Socialism with World War II's close in 1945, logical positivism shifted to a milder variant, \"logical empiricism\". All variants of the movement, which lasted until 1965, are neopositivism, sharing the quest of verificationism.\n\nNeopositivists led emergence of the philosophy subdiscipline philosophy of science, researching such questions and aspects of scientific theory and knowledge. Scientific realism takes scientific theory's statements at face value, thus accorded either falsity or truth—probable or approximate or actual. Neopositivists held scientific antirealism as instrumentalism, holding scientific theory as simply a device to predict observations and their course, while statements on nature's unobservable aspects are elliptical at or metaphorical of its observable aspects, rather.\n\nDN model received its most detailed, influential statement by Carl G Hempel, first in his 1942 article \"The function of general laws in history\", and more explicitly with Paul Oppenheim in their 1948 article \"Studies in the logic of explanation\". Leading logical empiricist, Hempel embraced the Humean empiricist view that humans observe sequence of sensory events, not cause and effect, as causal relations and casual mechanisms are unobservables. DN model bypasses causality beyond mere constant conjunction: first an event like \"A\", then always an event like \"B\".\n\nHempel held natural laws—empirically confirmed regularities—as satisfactory, and if included realistically to approximate causality. In later articles, Hempel defended DN model and proposed probabilistic explanation by \"inductive-statistical model\" (IS model). DN model and IS model—whereby the probability must be high, such as at least 50%—together form \"covering law model\", as named by a critic, William Dray. Derivation of statistical laws from other statistical laws goes to the \"deductive-statistical model\" (DS model). Georg Henrik von Wright, another critic, named the totality \"subsumption theory\".\n\nAmid failure of neopositivism's fundamental tenets, Hempel in 1965 abandoned verificationism, signaling neopositivism's demise. From 1930 onward, Karl Popper had refuted any positivism by asserting falsificationism, which Popper claimed had killed positivism, although, paradoxically, Popper was commonly mistaken for a positivist. Even Popper's 1934 book embraces DN model, widely accepted as the model of scientific explanation for as long as physics remained the model of science examined by philosophers of science.\n\nIn the 1940s, filling the vast observational gap between cytology and biochemistry, cell biology arose and established existence of cell organelles besides the nucleus. Launched in the late 1930s, the molecular biology research program cracked a genetic code in the early 1960s and then converged with cell biology as \"cell and molecular biology\", its breakthroughs and discoveries defying DN model by arriving in quest not of lawlike explanation but of causal mechanisms. Biology became a new model of science, while special sciences were no longer thought defective by lacking universal laws, as borne by physics.\n\nIn 1948, when explicating DN model and stating scientific explanation's semiformal \"conditions of adequacy\", Hempel and Oppenheim acknowledged redundancy of the third, \"empirical content\", implied by the other three—\"derivability\", \"lawlikeness\", and \"truth\". In the early 1980s, upon widespread view that causality ensures the explanans' relevance, Wesley Salmon called for returning \"cause\" to \"because\", and along with James Fetzer helped replace CA3 \"empirical content\" with CA3' \"strict maximal specificity\".\n\nSalmon introduced \"causal mechanical\" explanation, never clarifying how it proceeds, yet reviving philosophers' interest in such. Via shortcomings of Hempel's inductive-statistical model (IS model), Salmon introduced \"statistical-relevance model\" (SR model). Although DN model remained an idealized form of scientific explanation, especially in applied sciences, most philosophers of science consider DN model flawed by excluding many types of explanations generally accepted as scientific.\n\nAs theory of knowledge, epistemology differs from ontology, which is a subbranch of metaphysics, theory of reality. Ontology poses which categories of being—what sorts of things exist—and so, although a scientific theory's ontological commitment can be modified in light of experience, an ontological commitment inevitably precedes empirical inquiry.\n\nNatural laws, so called, are statements of humans' observations, thus are epistemological—concerning human knowledge—the \"epistemic\". Causal mechanisms and structures existing putatively independently of minds exist, or would exist, in the natural world's structure itself, and thus are ontological, the \"ontic\". Blurring epistemic with ontic—as by incautiously presuming a natural law to refer to a causal mechanism, or to trace structures realistically during unobserved transitions, or to be true regularities always unvarying—tends to generate a \"category mistake\".\n\nDiscarding ontic commitments, including causality \"per se\", DN model permits a theory's laws to be reduced to—that is, subsumed by—a more fundamental theory's laws. The higher theory's laws are explained in DN model by the lower theory's laws. Thus, the epistemic success of Newtonian theory's law of universal gravitation is reduced to—thus explained by—Einstein's general theory of relativity, although Einstein's discards Newton's ontic claim that universal gravitation's epistemic success predicting Kepler's laws of planetary motion is through a causal mechanism of a straightly attractive force instantly traversing absolute space despite absolute time.\n\nCovering law model reflects neopositivism's vision of empirical science, a vision interpreting or presuming unity of science, whereby all empirical sciences are either fundamental science—that is, fundamental physics—or are special sciences, whether astrophysics, chemistry, biology, geology, psychology, economics, and so on. All special sciences would network via covering law model. And by stating \"boundary conditions\" while supplying \"bridge laws\", any special law would reduce to a lower special law, ultimately reducing—theoretically although generally not practically—to fundamental science. (\"Boundary conditions\" are specified conditions whereby the phenomena of interest occur. \"Bridge laws\" translate terms in one science to terms in another science.)\n\nBy DN model, if one asks, \"Why is that shadow 20 feet long?\", another can answer, \"Because that flagpole is 15 feet tall, the Sun is at \"x\" angle, and laws of electromagnetism\". Yet by problem of symmetry, if one instead asked, \"Why is that flagpole 15 feet tall?\", another could answer, \"Because that shadow is 20 feet long, the Sun is at \"x\" angle, and laws of electromagnetism\", likewise a deduction from observed conditions and scientific laws, but an answer clearly incorrect. By the problem of irrelevance, if one asks, \"Why did that man not get pregnant?\", one could in part answer, among the explanans, \"Because he took birth control pills\"—if he factually took them, and the law of their preventing pregnancy—as covering law model poses no restriction to bar that observation from the explanans.\n\nMany philosophers have concluded that causality is integral to scientific explanation. DN model offers a necessary condition of a causal explanation—successful prediction—but not sufficient conditions of causal explanation, as a universal regularity can include spurious relations or simple correlations, for instance \"Z\" always following \"Y\", but not \"Z\" because of \"Y\", instead \"Y\" and then \"Z\" as an effect of \"X\". By relating temperature, pressure, and volume of gas within a container, Boyle's law permits prediction of an unknown variable—volume, pressure, or temperature—but does not explain \"why\" to expect that unless one adds, perhaps, the kinetic theory of gases.\n\nScientific explanations increasingly pose not determinism's universal laws, but probabilism's chance, \"ceteris paribus\" laws. Smoking's contribution to lung cancer fails even the inductive-statistical model (IS model), requiring probability over 0.5 (50%). (Probability standardly ranges from 0 (0%) to 1 (100%).) An applied science that applies statistics seeking associations between events, epidemiology cannot show causality, but consistently found higher incidence of lung cancer in smokers versus otherwise similar nonsmokers, although the proportion of smokers who develop lung cancer is modest. Versus nonsmokers, however, smokers as a group showed over 20 times the risk of lung cancer, and in conjunction with basic research, consensus followed that smoking had been scientifically explained as \"a\" cause of lung cancer, responsible for some cases that without smoking would not have occurred, a probabilistic counterfactual causality.\n\nThrough lawlike explanation, fundamental physics—often perceived as fundamental science—has proceeded through intertheory relation and theory reduction, thereby resolving experimental paradoxes to great historical success, resembling covering law model. In early 20th century, Ernst Mach as well as Wilhelm Ostwald had resisted Ludwig Boltzmann's reduction of thermodynamics—and thereby Boyle's law—to statistical mechanics partly \"because\" it rested on kinetic theory of gas, hinging on atomic/molecular theory of matter. Mach as well as Ostwald viewed matter as a variant of energy, and molecules as mathematical illusions, as even Boltzmann thought possible.\n\nIn 1905, via statistical mechanics, Albert Einstein predicted the phenomenon Brownian motion—unexplained since reported in 1827 by botanist Robert Brown. Soon, most physicists accepted that atoms and molecules were unobservable yet real. Also in 1905, Einstein explained the electromagnetic field's energy as distributed in \"particles\", doubted until this helped resolve atomic theory in the 1910s and 1920s. Meanwhile, all known physical phenomena were gravitational or electromagnetic, whose two theories misaligned. Yet belief in aether as the source of all physical phenomena was virtually unanimous. At experimental paradoxes, physicists modified the aether's hypothetical properties.\n\nFinding the luminiferous aether a useless hypothesis, Einstein in 1905 \"a priori\" unified all inertial reference frames to state special \"principle\" of relativity, which, by omitting aether, converted space and time into \"relative\" phenomena whose relativity aligned electrodynamics with the Newtonian principle Galilean relativity or invariance. Originally epistemic or instrumental, this was interpreted as ontic or realist—that is, a causal mechanical explanation—and the \"principle\" became a \"theory\", refuting Newtonian gravitation. By predictive success in 1919, general relativity apparently overthrew Newton's theory, a revolution in science resisted by many yet fulfilled around 1930.\n\nIn 1925, Werner Heisenberg as well as Erwin Schrödinger independently formalized quantum mechanics (QM). Despite clashing explanations, the two theories made identical predictions. Paul Dirac's 1928 model of the electron was set to special relativity, launching QM into the first quantum field theory (QFT), quantum electrodynamics (QED). From it, Dirac interpreted and predicted the electron's antiparticle, soon discovered and termed \"positron\", but the QED failed electrodynamics at high energies. Elsewhere and otherwise, strong nuclear force and weak nuclear force were discovered.\n\nIn 1941, Richard Feynman introduced QM's path integral formalism, which if taken toward \"interpretation\" as a causal mechanical model clashes with Heisenberg's matrix formalism and with Schrödinger's wave formalism, although all three are empirically identical, sharing predictions. Next, working on QED, Feynman sought to model particles without fields and find the vacuum truly empty. As each known fundamental force is apparently an effect of a field, Feynman failed. Louis de Broglie's waveparticle duality had rendered atomism—indivisible particles in a void—untenable, and highlighted the very notion of discontinuous particles as selfcontradictory.\n\nMeeting in 1947, Freeman Dyson, Richard Feynman, Julian Schwinger, and Sin-Itiro Tomonaga soon introduced \"renormalization\", a procedure converting QED to physics' most predictively precise theory, subsuming chemistry, optics, and statistical mechanics. QED thus won physicists' general acceptance. Paul Dirac criticized its need for renormalization as showing its unnaturalness, and called for an aether. In 1947, Willis Lamb had found unexpected motion of electron orbitals, shifted since the vacuum is not truly empty. Yet \"emptiness\" was catchy, abolishing aether conceptually, and physics proceeded ostensibly without it, even suppressing it. Meanwhile, \"sickened by untidy math, most philosophers of physics tend to neglect QED\".\n\nPhysicists have feared even mentioning \"aether\", renamed \"vacuum\", which—as such—is nonexistent. General philosophers of science commonly believe that aether, rather, is fictitious, \"relegated to the dustbin of scientific history ever since\" 1905 brought special relativity. Einstein was noncommittal to aether's nonexistence, simply said it superfluous. Abolishing Newtonian motion for electrodynamic primacy, however, Einstein inadvertently reinforced aether, and to explain motion was led back to aether in general relativity. Yet resistance to relativity theory became associated with earlier theories of aether, whose word and concept became taboo. Einstein explained special relativity's compatibility with an aether, but Einstein aether, too, was opposed. Objects became conceived as pinned directly on space and time by abstract geometric relations lacking ghostly or fluid medium.\n\nBy 1970, QED along with weak nuclear field was reduced to electroweak theory (EWT), and the strong nuclear field was modeled as quantum chromodynamics (QCD). Comprised by EWT, QCD, and Higgs field, this Standard Model of particle physics is an \"effective theory\", not truly fundamental. As QCD's particles are considered nonexistent in the everyday world, QCD especially suggests an aether, routinely found by physics experiments to exist and to exhibit relativistic symmetry. Confirmation of the Higgs particle, modeled as a condensation within the Higgs field, corroborates aether, although physics need not state or even include aether. Organizing regularities of \"observations\"—as in the covering law model—physicists find superfluous the quest to discover \"aether\".\n\nIn 1905, from special relativity, Einstein deduced mass–energy equivalence, particles being variant forms of distributed energy, how particles colliding at vast speed experience that energy's transformation into mass, producing heavier particles, although physicists' talk promotes confusion. As \"the contemporary locus of metaphysical research\", QFTs pose particles not as existing individually, yet as \"excitation modes\" of fields, the particles and their masses being states of aether, apparently unifying all physical phenomena as the more fundamental causal reality, as long ago foreseen. Yet a \"quantum\" field is an intricate abstraction—a \"mathematical\" field—virtually inconceivable as a \"classical\" field's physical properties. Nature's deeper aspects, still unknown, might elude any possible field theory.\n\nThough discovery of causality is popularly thought science's aim, search for it was shunned by the Newtonian research program, even more Newtonian than was Isaac Newton. By now, most theoretical physicists infer that the four, known fundamental interactions would reduce to superstring theory, whereby atoms and molecules, after all, are energy vibrations holding mathematical, geometric forms. Given uncertainties of scientific realism, some conclude that the concept \"causality\" raises comprehensibility of scientific explanation and thus is key folk science, but compromises precision of scientific explanation and is dropped as a science matures. Even epidemiology is maturing to heed the severe difficulties with presumptions about causality. Covering law model is among Carl G Hempel's admired contributions to philosophy of science.\n\nTypes of inference\n\nRelated subjects\n\n\n"}
{"id": "33548913", "url": "https://en.wikipedia.org/wiki?curid=33548913", "title": "Dehaene–Changeux model", "text": "Dehaene–Changeux model\n\nThe Dehaene–Changeux model (DCM), also known as the global neuronal workspace or the global cognitive workspace model is a part of Bernard Baars's \"global workspace model\" for consciousness.\n\nIt is a computer model of the neural correlates of consciousness programmed as a neural network. It attempts to reproduce the swarm behaviour of the brain's \"higher cognitive functions\" such as consciousness, decision-making and the central executive functions. It was developed by cognitive neuroscientists Stanislas Dehaene and Jean-Pierre Changeux beginning in 1986. It has been used to provide a predictive framework to the study of inattentional blindness and the solving of the Tower of London test.\n\nThe Dehaene–Changeux model was initially established as a spin glass neural network attempting to represent learning and to then provide a stepping stone towards artificial learning among other objectives. It would later be used to predict observable reaction times within the priming paradigm and in inattentional blindness.\n\nThe Dehaene–Changeux model is a meta neural network (i.e. a network of neural networks) composed of a very large number of integrate-and-fire neurons programmed in either a stochastic or deterministic way. The neurons are organised in complex thalamo-cortical columns with long-range connexions and a critical role played by the interaction between von Economo's areas. Each thalamo-cortical column is composed of pyramidal cells and inhibitory interneurons receiving a long-distance excitatory neuromodulation which could represent noradrenergic input.\n\nAmong others Cohen & Hudson (2002) had already used \"\"Meta neural networks as intelligent agents for diagnosis \" Similarly to Cohen & Hudson, Dehaene & Changeux have established their model as an interaction of meta-neural networks (thalamocortical columns) themselves programmed in the manner of a \"hierarchy of neural networks that together act as an intelligent agent\"\", in order to use them as a system composed of a large scale of inter-connected intelligent agents for predicting the self-organized behaviour of the neural correlates of consciousness. It may also be noted that Jain et al. (2002) had already clearly identified spiking neurons as intelligent agents since the lower bound for computational power of networks of spiking neurons is the capacity to simulate in real-time for boolean-valued inputs any Turing machine. The DCM being composed of a very large number of interacting sub-networks which are themselves intelligent agents, it is formally a Multi-agent system programmed as a Swarm or neural networks and \"a fortiori\" of spiking neurons.\n\nThe DCM exhibits several surcritical emergent behaviors such as multistability and a Hopf bifurcation between two very different regimes which may represent either sleep or arousal with a various all-or-none behaviors which Dehaene et al. use to determine a testable taxonomy between different states of consciousness. \n\nThe Dehaene-Changeux Model contributed to the study of nonlinearity and self-organized criticality in particular as an explanatory model of the brain's emergent behaviors, including consciousness. Studying the brain's phase-locking and large-scale synchronization, Kitzbichler et al. (2011a) confirmed that criticality is a property of human brain functional network organization at all frequency intervals in the brain's physiological bandwidth.\n\nFurthermore, exploring the neural dynamics of cognitive efforts after, \"inter alia\", the Dehaene-Changeux Model, Kitzbichler et al. (2011b) demonstrated how cognitive effort breaks the modularity of mind to make human brain functional networks transiently adopt a more efficient but less economical configuration. Werner (2007a) used the Dehaene-Changeux Global Neuronal Workspace to defend the use of statistical physics approaches for exploring phase transitions, scaling and universality properties of the so-called \"Dynamic Core\" of the brain, with relevance to the macroscopic electrical activity in EEG and EMG. Furthermore, building from the Dehaene-Changeux Model, Werner (2007b) proposed that the application of the twin concepts of scaling and universality of the theory of non-equilibrium phase transitions can serve as an informative approach for elucidating the nature of underlying neural-mechanisms, with emphasis on the dynamics of recursively reentrant activity flow in intracortical and cortico-subcortical neuronal loops. Friston (2000) also claimed that \"the nonlinear nature of asynchronous coupling enables the rich, context-sensitive interactions that characterize real brain dynamics, suggesting that it plays a role in functional integration that may be as important as synchronous interactions\".\n\nIt contributed to the study of phase transition in the brain under sedation, and notably GABA-ergic sedation such as that induced by propofol (Murphy et al. 2011, Stamatakis et al. 2010). The Dehaene-Changeux Model was contrasted and cited in the study of collective consciousness and its pathologies (Wallace et al. 2007). Boly et al. (2007) used the model for a reverse somatotopic study, demonstrating a correlation between baseline brain activity and somatosensory perception in humans. Boly et al. (2008) also used the DCM in a study of the baseline state of consciousness of the human brain's default network.\n\n\n"}
{"id": "17910574", "url": "https://en.wikipedia.org/wiki?curid=17910574", "title": "Digital ecosystem", "text": "Digital ecosystem\n\nA digital ecosystem is a distributed, adaptive, open socio-technical system with properties of self-organisation, scalability and sustainability inspired from natural ecosystems. Digital ecosystem models are informed by knowledge of natural ecosystems, especially for aspects related to competition and collaboration among diverse entities. The term is used in the computer industry, the entertainment industry, and the World Economic Forum.\n\nThe concept of Digital Business Ecosystem was put forward in 2002 by a group of European researchers and practitioners, including Francesco Nachira, Paolo Dini and Andrea Nicolai, who applied the general notion of digital ecosystems to model the process of adoption and development of ICT-based products and services in competitive, highly fragmented markets like the European one\n. Elizabeth Chang, Ernesto Damiani and Tharam Dillon started in 2007 the IEEE Digital EcoSystems and Technologies Conference (IEEE DEST). Richard Chbeir, Youakim Badr, Dominique Laurent, and Hiroshi Ishikawa started in 2009 the ACM Conference on Management of Digital EcoSystems (MEDES)\n\nThe digital ecosystem metaphor and models have been applied to a number of business areas related to the production and distribution of knowledge-intensive products and services, including higher education. The perspective of this research is providing methods and tools to achieve a set of objectives of the ecosystem (e.g. sustainability, fairness, bounded information asymmetry, risk control and gracious failure). These objectives are seen as desirable properties whose emergence should be fostered by the digital ecosystem self-organization, rather than as explicit design goals like in conventional IT.\n\n\n"}
{"id": "2286327", "url": "https://en.wikipedia.org/wiki?curid=2286327", "title": "Distinction without a difference", "text": "Distinction without a difference\n\nA distinction without a difference is a type of logical fallacy where an author or speaker attempts to describe a distinction between two things where no discernible difference exists. It is particularly used when a word or phrase has connotations associated with it that one party to an argument prefers to avoid.\n\n\n"}
{"id": "152902", "url": "https://en.wikipedia.org/wiki?curid=152902", "title": "Dormant Commerce Clause", "text": "Dormant Commerce Clause\n\nThe Dormant Commerce Clause, or Negative Commerce Clause, in American constitutional law, is a legal doctrine that courts in the United States have inferred from the Commerce Clause in Article I of the US Constitution. The Dormant Commerce Clause is used to prohibit state legislation that discriminates against interstate or international commerce.\n\nFor example, it is lawful for Michigan to require food labels that specifically identify certain animal parts, if they are present in the product, because the state law applies to food produced in Michigan as well as food imported from other states and foreign countries; the state law would violate the Commerce Clause if it applied only to imported food or if it was otherwise found to favor domestic over imported products. Likewise, California law requires milk sold to contain a certain percentage of milk solids that federal law does not require, which is allowed under the Dormant Commerce Clause doctrine because California's stricter requirements apply equally to California-produced milk and imported milk and so does not discriminate against or inappropriately burden interstate commerce.\n\nThe idea that regulation of interstate commerce may to some extent be an exclusive Federal power was discussed even before adoption of the Constitution, but the framers did not use the word \"dormant\". On September 15, 1787, the Framers of the Constitution debated in Philadelphia whether to guarantee states the ability to lay duties of tonnage without Congressional interference so that the states could finance the clearing of harbors and the building of lighthouses. James Madison believed that the mere existence of the Commerce Clause would bar states from imposing any duty of tonnage: \"He was more and more convinced that the regulation of Commerce was in its nature indivisible and ought to be wholly under one authority.\"\n\nRoger Sherman disagreed: \"The power of the United States to regulate trade being supreme can control interferences of the State regulations when such interferences happen; so that there is no danger to be apprehended from a concurrent jurisdiction.\" Sherman saw the commerce power as similar to the tax power, the latter being one of the concurrent powers shared by the federal and state governments. Ultimately, the Philadelphia Convention decided upon the present language about duties of tonnage in , which says: \"No state shall, without the consent of Congress, lay any duty of tonnage ...\"\n\nThe word \"dormant,\" in connection with the Commerce Clause, originated in dicta of Chief Justice John Marshall. For example, in the case of \"Gibbons v. Ogden\", , he wrote that the power to regulate interstate commerce \"can never be exercised by the people themselves, but must be placed in the hands of agents, or lie dormant.\" Concurring Justice William Johnson was even more emphatic that the Constitution is \"altogether in favor of the exclusive grants to Congress of power over commerce.\"\n\nLater, in the case of \"Willson v. Black-Bird Creek Marsh Co.\", , Marshall wrote: \"We do not think that the [state] act empowering the Black Bird Creek Marsh Company to place a dam across the creek, can, under all the circumstances of the case, be considered as repugnant to the power to regulate commerce in its dormant state, or as being in conflict with any law passed on the subject.\"\n\nIf Marshall was suggesting that the power over interstate commerce is an exclusive federal power, the Dormant Commerce Clause doctrine eventually developed very differently: it treats regulation that does not discriminate against or unduly burden interstate commerce as a concurrent power, rather than an exclusive federal power, and it treats regulation that does so as an exclusive federal power. Thus, the modern doctrine says that congressional power over interstate commerce is somewhat exclusive but \"not absolutely exclusive\". The approach began in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court: \"Either absolutely to affirm, or deny that the nature of this [commerce] power requires exclusive legislation by Congress, is to lose sight of the nature of the subjects of this power, and to assert concerning all of them, what is really applicable but to a part.\" The first clear holding of the Supreme Court striking down a state law under the Dormant Commerce Clause came in 1873.\n\nJustice Anthony Kennedy has written that: \"The central rationale for the rule against discrimination is to prohibit state or municipal laws whose object is local economic protectionism, laws that would excite those jealousies and retaliatory measures the Constitution was designed to prevent.\" In order to determine whether a law violates a so-called \"dormant\" aspect of the Commerce Clause, the court first asks whether it discriminates on its face against interstate commerce. In this context, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter.\n\nThus, in a dormant Commerce Clause case, a court is initially concerned with whether the law facially discriminates against out-of-state actors or has the effect of favoring in-state economic interests over out-of-state interests. Discriminatory laws motivated by \"simple economic protectionism\" are subject to a \"virtually per se rule of invalidity\", \"City of Philadelphia v. New Jersey\" 437 U.S. 617 (1978), \"Dean Milk Co. v. City of Madison, Wisconsin\", 340 U.S. 349 (1951), \"Hunt v. Washington State Apple Advertising Comm.\", 432 U.S. 333 (1977) which can only be overcome by a showing that the State has no other means to advance a legitimate local purpose, \"Maine v. Taylor\", 477 U.S. 131(1986). See also \"Brown-Forman Distillers v. New York State Liquor Authority\", .\n\nOn the other hand, when a law is \"directed to legitimate local concerns, with effects upon interstate commerce that are only incidental\" (United Haulers Association, Inc.), that is, where other legislative objectives are credibly advanced and there is no patent discrimination against interstate trade, the Court has adopted a much more flexible approach, the general contours of which were outlined in \"Pike v. Bruce Church, Inc.\", 397 U.S. 137, 142 (1970) and \"City of Philadelphia v. New Jersey\", 437 U.S. at 624. If the law is not outright or intentionally discriminatory or protectionist, but still has some impact on interstate commerce, the court will evaluate the law using a balancing test. The Court determines whether the interstate burden imposed by a law outweighs the local benefits. If such is the case, the law is usually deemed unconstitutional. See \"Pike v. Bruce Church, Inc.\", . In the Pike case, the Court explained that a state regulation having only \"incidental\" effects on interstate commerce \"will be upheld unless the burden imposed on such commerce is clearly excessive in relation to the putative local benefits\". 397 U.S. at 142, 90 S.Ct. at 847. When weighing burdens against benefits, a court should consider both \"the nature of the local interest involved, and ... whether it could be promoted as well with a lesser impact on interstate activities\". Id. Thus regulation designed to implement public health and safety, or serve other legitimate state interests, but impact interstate commerce as an incident to that purpose, are subject to a test akin to the rational basis test, a minimum level of scrutiny. See \"Bibb v. Navajo Freight Lines, Inc.\" In USA Recycling, Inc. v. Town of Babylon, 66 F.3d 1272, 1281 (C.A.2 (N.Y.), 1995), the court explained:\n\nIf the state activity constitutes \"regulation\" of interstate commerce, then the court must proceed to a second inquiry: whether the activity regulates evenhandedly with only \"incidental\" effects on interstate commerce, or discriminates against interstate commerce. As we use the term here, \"discrimination\" simply means differential treatment of in-state and out-of-state economic interests that benefits the former and burdens the latter. The party challenging the validity of a state statute or municipal ordinance bears the burden of showing that it discriminates against, or places some burden on, interstate commerce. \"Hughes v. Oklahoma\", 441 U.S. 322, 336, 99 S.Ct. 1727, 1736, 60 L.Ed.2d 250 (1979). If discrimination is established, the burden shifts to the state or local government to show that the local benefits of the statute outweigh its discriminatory effects, and that the state or municipality lacked a nondiscriminatory alternative that could have adequately protected the relevant local interests. If the challenging party cannot show that the statute is discriminatory, then it must demonstrate that the statute places a burden on interstate commerce that \"is clearly excessive in relation to the putative local benefits.\" \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456, 471(1981) (quoting Pike, 397 U.S. at 142, 90 S.Ct. at 847).\n\nOver the years, the Supreme Court has consistently held that the language of the Commerce Clause contains a further, negative command prohibiting certain state taxation even when Congress has failed to legislate on the subject. Examples of such cases are \"Quill Corp. v. North Dakota\", 504 U.S. 298 (1992); \"Northwestern States Portland Cement Co. v. Minnesota\", 358 U.S. 450, 458 (1959) and \"H.P. Hood & Sons, Inc. v. Du Mond\", 336 U.S. 525 (1949).\n\nMore recently, in the 2015 case of \"Comptroller of Treasury of MD. v. Wynne\", the Court addressed Maryland's unusual practice of taxing personal income earned in Maryland, and taxing personal income of its citizens earned outside Maryland, \"without\" any tax credit for income tax paid to other states. The Court held this sort of double-taxation to be a violation of the dormant Commerce Clause. The Court faulted Justice Antonin Scalia's criticism of the dormant Commerce Clause doctrine by saying that he failed to \"explain why, under his interpretation of the Constitution, the Import-Export Clause \nwould not lead to the same result that we reach under the dormant Commerce Clause\".\n\nApplication of the dormant commerce clause to state taxation is another manifestation of the Court's holdings that the Commerce Clause prevents a State from retreating into economic isolation or jeopardizing the welfare of the Nation as a whole, as it would do if it were free to place burdens on the flow of commerce across its borders that commerce wholly within those borders would not bear. The Court's taxation decisions thus \"reflected a central concern of the Framers that was an immediate reason for calling the Constitutional Convention: the conviction that in order to succeed, the new Union would have to avoid the tendencies toward economic Balkanization that had plagued relations among the Colonies and later among the States under the Articles of Confederation.\" \"Wardair Canada, Inc. v. Florida Dept. of Revenue\", 477 U.S. 1 (1986); \"Hughes v. Oklahoma\", 441 U.S. 322 (1979); \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995).\n\nAs with the Court's application of the dormant commerce clause to discriminatory regulation, the pre-New Deal Court attempted to apply a formalistic approach to state taxation alleged to interfere with interstate commerce. The history is described in \"Oklahoma Tax Commission v. Jefferson Lines, Inc.\", 514 U.S. 175 (1995):\n\nThe command has been stated more easily than its object has been attained, however, and the Court's understanding of the dormant Commerce Clause has taken some turns. In its early stages, the Court held the view that interstate commerce was wholly immune from state taxation \"in any form\", \"even though the same amount of tax should be laid on (intrastate) commerce\". This position gave way in time to a less uncompromising but formal approach, according to which, for example, the Court would invalidate a state tax levied on gross receipts from interstate commerce, or upon the \"freight carried\" in interstate commerce, but would allow a tax merely measured by gross receipts from interstate commerce as long as the tax was formally imposed upon franchises, or \"'in lieu of all taxes upon (the taxpayer's) property,'\" Dissenting from this formal approach in 1927, Justice Stone remarked that it was \"too mechanical, too uncertain in its application, and too remote from actualities, to be of value.\" \n\nAccompanying the revolution in approach in the Court's Congressional powers jurisprudence, the New Deal Court began to change its approach to state taxation as well. The Jefferson Lines decision continues:\n\nIn 1938, the old formalism began to give way with Justice Stone's opinion in \"Western Live Stock v. Bureau of Revenue\", 303 U.S. 250, which examined New Mexico's franchise tax, measured by gross receipts, as applied to receipts from out-of-state advertisers in a journal produced by taxpayers in New Mexico but circulated both inside and outside the State. Although the assessment could have been sustained solely on prior precedent, Justice Stone added a dash of the pragmatism that, with a brief interlude, has since become our aspiration in this quarter of the law. ... The Court explained that \"[i]t was not the purpose of the commerce clause to relieve those engaged in interstate commerce from their just share of state tax burden even though it increases the cost of doing the business.\" \n\nDuring the transition period, some taxes were upheld based on a careful review of the actual economic impact of the tax, and other taxes were reviewed based on the kind of tax involved, whether the tax had a nefarious impact on commerce or not. Under this formalistic approach, a tax might be struck down, and then re-passed with exactly the same economic incidence, but under another name, and then withstand review.\n\nThe absurdity of this approach was made manifest in the two Railway Express cases. In the first, a tax imposed by the state of Virginia on American business concerns operating within the state was struck down because it was a business privilege tax imposed on the privilege of doing business in interstate commerce. But then, in the second, Virginia revised the wording of its statute to impose a \"franchise tax\" on \"intangible property\" in the form of \"going concern\" value as measured by gross receipts.\n\nThe Court upheld the reworded statute as not violative of the prohibition on privilege taxes, even though the impact of the old tax and new were essentially identical. There was no real economic difference between the statutes in Railway Express I and Railway Express II. The Court long since had recognized that interstate commerce may be made to pay its way. Yet under the Spector rule, the economic realities in Railway Express I became irrelevant. The Spector rule (against privilege taxes) had come to operate only as a rule of draftsmanship, and served only to distract the courts and parties from their inquiry into whether the challenged tax produced results forbidden by the Commerce Clause.\n\nThe death knell of formalism occurred in \"Complete Auto Transit, Inc v. Brady\", 430 U.S. 274 (1977), which approved a Mississippi privilege tax upon a Michigan company engaged in the business of shipping automobiles to Mississippi dealers. The Court there explained:\n\nAppellant's attack is based solely on decisions of this Court holding that a tax on the \"privilege\" of engaging in an activity in the State may not be applied to an activity that is part of interstate commerce. See, e. g., \"Spector Motor Service v. O'Connor\", 340 U.S. 602 (1951); \"Freeman v. Hewit\", 329 U.S. 249 (1946). This rule looks only to the fact that the incidence of the tax is the \"privilege of doing business\"; it deems irrelevant any consideration of the practical effect of the tax. The rule reflects an underlying philosophy that interstate commerce should enjoy a sort of \"free trade\" immunity from state taxation. \n\nComplete Auto Transit is the last in a line of cases that gradually rejected a per se approach to state taxation challenges under the commerce clause. In overruling prior decisions which struck down privilege taxes per se, the Court noted the following, in what has become a central component of commerce clause state taxation jurisprudence:\n\nWe note again that no claim is made that the activity is not sufficiently connected to the State to justify a tax, or that the tax is not fairly related to benefits provided the taxpayer, or that the tax discriminates against interstate commerce, or that the tax is not fairly apportioned.\n\nThese four factors, nexus, relationship to benefits, discrimination, and apportionment, have come to be regarded as the four Complete Auto Transit factors applied repeatedly in subsequent cases. Complete Auto Transit must be recognized as the culmination of the Court's emerging commerce clause approach, not just in taxation, but in all of its aspects. Application of Complete Auto Transit to State taxation remains a highly technical and specialized venture, requiring the application of commerce clause principles to an understanding of specialized tax law.\n\nIn addition to satisfying the four-prong test in \"Complete Auto Transit\", the Supreme Court has held state taxes which burden international commerce cannot create a substantial risk of multiple taxations and must not prevent the federal government from \"speaking with one voice when regulating commercial relations with foreign governments\". \"Japan Lines, Ltd. v. County of Los Angeles\", 441 U.S. 434 (1979).\n\nIn \"Kraft Gen. Foods, Inc. v. Iowa Dept. of Revenue and Finance\", 505 U.S. 71 (1992), the Supreme Court considered a case in which Iowa taxed dividends from foreign subsidiaries, without allowing a credit for taxes paid to foreign governments, but not dividends from domestic subsidiaries operating outside Iowa. This differential treatment arose from Iowa's adoption of the definition of \"net income\" used by the Internal Revenue Service. For federal income tax purposes, dividends from domestic subsidiaries are allowed to be exempted from the parent corporations income to avoid double taxation. The Iowa Supreme Court rejected a Commerce Clause claim because Kraft failed to show \"that Iowa businesses receive a commercial advantage over foreign commerce due to Iowa's taxing scheme.\" Considering an Equal Protection Clause challenge, the Iowa Supreme Court held that the use of the federal government's definitions of income were convenient for the state and was \"rationally related to the goal of administrative efficiency\". The Supreme Court rejected the notion that administrative convenience was a sufficient defense for subjecting foreign commerce to a higher tax burden than interstate commerce. The Supreme Court held that \"a State's preference for domestic commerce over foreign commerce is inconsistent with the Commerce Clause even if the State's own economy is not a direct beneficiary of the discrimination.\"\n\nDiscrimination in the flow of interstate commerce has arisen in a variety of contexts. A line of important cases has dealt with local processing requirements. Under the local processing requirement, a municipality seeks to force the local processing of raw materials before they are shipped in interstate commerce.\n\nThe basic idea of the local processing ordinance was to provide favored access to local processors of locally produced raw materials. Examples of Supreme Court decisions in this vein are set out in its Carbone decision. They include \"Minnesota v. Barber\", 136 U.S. 313, (1890) (striking down a Minnesota statute that required any meat sold within the State, whether originating within or without the State, to be examined by an inspector within the State); \"Foster-Fountain Packing Co. v. Haydel\", 278 U.S. 1 (1928) (striking down a Louisiana statute that forbade shrimp to be exported unless the heads and hulls had first been removed within the State); \"Johnson v. Haydel\", 278 U.S. 16 (1928) (striking down analogous Louisiana statute for oysters); \"Toomer v. Witsell\", 334 U.S. 385 (1948) (striking down South Carolina statute that required shrimp fishermen to unload, pack, and stamp their catch before shipping it to another State); \"Pike v. Bruce Church, Inc.\", supra (striking down Arizona statute that required all Arizona-grown cantaloupes to be packaged within the State prior to export); \"South-Central Timber Development, Inc. v. Wunnicke\", 467 U.S. 82 (1984) (striking down an Alaska regulation that required all Alaska timber to be processed within the State prior to export). The Court has defined \"protectionist\" state legislation as \"regulatory measures designed to benefit in-state economic interests by burdening out-of-state competitors\". \"New Energy Co. of Indiana v. Limbach\", 486 U.S. 269, 273–74 (1988).\n\nIn the 1980s, spurred by RCRA's emphasis on comprehensive local planning, many states and municipalities sought to promote investment in more costly disposal technologies, such as waste-to-energy incinerators, state-of-the-art landfills, composting and recycling. Some states and localities sought to promote private investment in these costly technologies by guaranteeing a longterm supply of customers. See Phillip Weinberg, Congress, the Courts, and Solid Waste Transport: Good Fences Don't Always Make Good Neighbors, 25 Envtl. L. 57 (1995); Atlantic Coast Demolition & Recycling, Inc., 112 F.3d 652, 657 (3d Cir. 1997). For about a decade, the use of regulation to channel private commerce to designated private disposal sites was greatly restricted as the result of the Carbone decision discussed below.\n\nFlow control laws typically came in various designs. One common theme was the decision to fund local infrastructure by guaranteeing a minimum volume of business for privately constructed landfills, incinerators, composters or other costly disposal sites. In some locales, choice of the flow control device was driven by state bonding laws, or municipal finance concerns. If a county or other municipality issued general obligation bonds for construction of a costly incinerator, for example, state laws might require a special approval process. If approval could be obtained, the bonds themselves would be counted against governmental credit limitations, or might impact the governmental body's credit rating: in either instance the ability to bond for other purposes might be impaired. But by guaranteeing customers for a privately constructed and financed facility, a private entity could issue its own bonds, privately, on the strength of the public's waste assurance.\n\nThe private character of flow control regimens can thus be explained in part by the desire to utilize particular kinds of public financing devices. It can also be explained by significant encouragement at the national level, in national legislation as well as in federal executive policy to achieve environmental objectives utilizing private resources. Ironically, these public-private efforts often took the form of local processing requirements which ultimately ran afoul of the commerce clause.\n\nThe Town of Clarkstown had decided that it wanted to promote waste assurance through a local private transfer station. The transfer station would process waste and then forward the waste to the disposal site designated by the Town. The ordinance had the following features:\n\nWaste hauling in the Town of Clarkstown was accomplished by private haulers, subject to local regulation. The scheme had the following aspects: (A) The Town promoted the financing of a privately owned transfer station through a waste assurance agreement with the private company. Thus the designated facility was a private company. (B) The Town of Clarkstown forced private haulers to bring their solid waste for local processing at the designated transfer station, even if the ultimate destination of solid waste was an out-of-state disposal site. (C) The primary rationale for forcing in-state waste into the designated private transfer station was financial; it was seen as a device to raise revenue to finance the transfer station.\n\nThe Town of Clarkstown's ordinance was designed and written right in the teeth of the long line of Supreme Court cases which had historically struck down local processing requirements. In short, it was as if the authors of the ordinance had gone to a treatise on the commerce clause and intentionally chosen a device which had been traditionally prohibited. A long line of Supreme Court case law had struck down local processing requirements when applied to goods or services in interstate commerce. As the Court in Carbone wrote:\n\nWe consider a so-called flow control ordinance, which requires all solid waste to be processed at a designated transfer station before leaving the municipality. The avowed purpose of the ordinance is to retain the processing fees charged at the transfer station to amortize the cost of the facility. Because it attains this goal by depriving competitors, including out-of-state firms, of access to a local market, we hold that the flow control ordinance violates the Commerce Clause.\n\nThe Court plainly regarded the decision as a relatively unremarkable decision, not a bold stroke. As the Court wrote: \"The case decided today, while perhaps a small new chapter in that course of decisions, rests nevertheless upon well-settled principles of our Commerce Clause jurisprudence.\" And, the Court made it plain, that the problem with Clarkstown's ordinance was that it created a local processing requirement protective of a local private processing company:\n\nIn this light, the flow control ordinance is just one more instance of local processing requirements that we long have held invalid ... The essential vice in laws of this sort is that they bar the import of the processing service. Out-of-state meat inspectors, or shrimp hullers, or milk pasteurizers, are deprived of access to local demand for their services. Put another way, the offending local laws hoard a local resource—be it meat, shrimp, or milk—for the benefit of local businesses that treat it. 511 U.S. at 392–393.\n\nThe Court's 2007 decision in \"United Haulers Association v. Oneida-Herkimer Solid Waste Management Authority\" starkly illustrates the difference in result when the Court finds that local regulation is not discriminatory. The Court dealt with a flow control regimen quite similar to that considered in Carbone. The \"only salient difference is that the laws at issue here require haulers to bring waste to facilities owned and operated by a state-created public benefit corporation.\" The Court decided that the balancing test should apply, because the regulatory scheme favored the government owned facility, but treated all private facilities equally.\n\nCompelling reasons justify treating these laws differently from laws favoring particular private businesses over their competitors. \"Conceptually, of course, any notion of discrimination assumes a comparison of substantially similar entities.\" \"General Motors Corp. v. Tracy\", 519 U.S. 278 (1997). But States and municipalities are not private businesses—far from it. Unlike private enterprise, government is vested with the responsibility of protecting the health, safety, and welfare of its citizens. See \"Metropolitan Life Ins. Co. v. Massachusetts\", 471 U.S. 724 (1985) ... These important responsibilities set state and local government apart from a typical private business.\n\nThe Court's United Haulers decision demonstrates an understanding of the regulatory justifications for flow control starkly missing in the Carbone decision:\n\nBy the 1980s, the Counties confronted what they could credibly call a solid waste \" 'crisis.' \"... Many local landfills were operating without permits and in violation of state regulations. Sixteen were ordered to close and remediate the surrounding environment, costing the public tens of millions of dollars. These environmental problems culminated in a federal clean-up action against a landfill in Oneida County; the defendants in that case named over local businesses and several municipalities and school districts as third-party defendants The \"crisis\" extended beyond health and safety concerns. The Counties had an uneasy relationship with local waste management companies, enduring price fixing, pervasive overcharging, and the influence of organized crime. Dramatic price hikes were not uncommon: In 1986, for example, a county contractor doubled its waste disposal rate on six weeks' notice\n\nThe Court would not interfere with local government's efforts to solve an important public and safety problem.\n\nThe contrary approach of treating public and private entities the same under the dormant Commerce Clause would lead to unprecedented and unbounded interference by the courts with state and local government. The dormant Commerce Clause is not a roving license for federal courts to decide what activities are appropriate for state and local government to undertake, and what activities must be the province of private market competition. In this case, the citizens of Oneida and Herkimer Counties have chosen the government to provide waste management services, with a limited role for the private sector in arranging for transport of waste from the curb to the public facilities. The citizens could have left the entire matter for the private sector, in which case any regulation they undertook could not discriminate against interstate commerce. But it was also open to them to vest responsibility for the matter with their government, and to adopt flow control ordinances to support the government effort. It is not the office of the Commerce Clause to control the decision of the voters on whether government or the private sector should provide waste management services. \"The Commerce Clause significantly limits the ability of States and localities to regulate or otherwise burden the flow of interstate commerce, but it does not elevate free trade above all other values.\"\n\nThe history of commerce clause jurisprudence evidences a distinct difference in approach where the state is seeking to exercise its public health and safety powers, on the one hand, as opposed to attempting to regulate the flow of commerce. The exact dividing line between the two interests, the right of states to exercise regulatory control over their public health and safety, and the interest of the national government in unfettered interstate commerce is not always easy to discern. One Court has written as follows:\n\nNot surprisingly, the Court's effort to preserve a national market has, on numerous occasions, come into conflict with the states' traditional power to \"legislat[e] on all subjects relating to the health, life, and safety of their citizens.\" \"Huron Portland Cement Co. v. City of Detroit\", 362 U.S. 440, 443 (1960). On these occasions, the Supreme Court has \"struggled (to put it nicely) to develop a set of rules by which we may preserve a national market without needlessly intruding upon the States' police powers, each exercise of which no doubt has some effect on the commerce of the Nation.\" \"Camps Newfound/Owatonna v. Town of Harrison\", 520 U.S. 564, 596 (1997) (Scalia, J., dissenting) (citing \"Okla. Tax Comm'n v. Jefferson Lines\", 514 U.S. 175, 180–83 (1995)); see generally Boris I. Bittker, Regulation of Interstate and Foreign Commerce § 6.01[A], at 6–5 (\"[T]he boundaries of the [State's] off-limits area are, and always have been, enveloped in a haze.\"). Those rules are \"simply stated, if not simply applied.\" Camps Newfound/Owatonna, 520 U.S. at 596 (Scalia, J., dissenting).\n\nA frequently cited example of the deference afforded to the powers of state and local government may be found in \"Exxon Corp. v. Maryland\", 437 U.S. 117 (1978), where the State of Maryland barred producers of petroleum products from operating retail service stations in the state. It is difficult to imagine a regimen which might have greater impact on the way in which markets are organized. Yet, the Court found the legislation constitutionally permitted: \"The fact that the burden of a state regulation falls on some interstate companies does not, by itself establish a claim of discrimination against interstate commerce,\" the Court wrote. The \"Clause protects interstate market, not particular interstate firms, from prohibitive or burdensome regulations.\"\n\nSimilarly, in \"Minnesota v. Clover Leaf Creamery Co.\", 449 U.S. 456 (1981) the Court upheld a state law that banned nonreturnable milk containers made of plastic but permitted other nonreturnable milk containers. The Court found that the existence of a burden on out-of-state plastic industry was not 'clearly excessive' in comparison to the state's interest in promoting conservation. And the court continued:\n\nIn Exxon, the Court stressed that the Commerce Clause protects the interstate market, not particular interstate firms, from prohibitive or burdensome regulations. A nondiscriminatory regulation serving substantial state purpose is not invalid simply because it causes some business to shift from a predominantly out-of-state industry to a predominantly in-state industry. Only if the burden on interstate commerce clearly outweighs the State's legitimate purpose does such a regulation violate the commerce clause. When a state statute regarding safety matters applies equally to interstate and intrastate commerce, the courts are generally reluctant to invalidate it even if it may have some impact on interstate commerce. In \"Bibb v. Navajo Freight Lines\" 359 U.S. 520, 524 (1959), the United States Supreme Court stated: 'These safety measures carry a strong presumption of validity when challenged in court. If there are alternative ways of solving a problem, we do not sit to determine which of them is best suited to achieve a valid state objective. Policy decisions are for the state legislature, absent federal entry into the field. Unless we can conclude on the whole record that \"the total effect of the law as a safety measure in reducing accidents and casualties is so slight or problematical as not to outweigh the national interest in keeping interstate commerce free from interferences which seriously impede it\" we must uphold the statute.\n\nThere are two notable exceptions to the dormant Commerce Clause doctrine that can permit state laws or actions that otherwise violate the Dormant Commerce Clause to survive court challenges.\n\nThe first exception occurs when Congress has legislated on the matter. See \"Western & Southern Life Ins. v. State Board of California\", . In this case the Dormant Commerce Clause is no longer \"dormant\" and the issue is a Commerce Clause issue, requiring a determination of whether Congress has approved, preempted, or left untouched the state law at issue.\n\nThe second exception is \"market participation exception\". This occurs when the state is acting \"in the market\", like a business or customer, rather than as a \"market regulator\". For example, when a state is contracting for the construction of a building or selling maps to state parks, rather than passing laws governing construction or dictating the price of state park maps, it is acting \"in the market\". Like any other business in such cases, a state may favor or shun certain customers or suppliers.\n\nThe Supreme Court introduced the market participant doctrine in \"Hughes v. Alexandria Scrap Corp.\", 426 U.S. 794 (1976), which upheld a Maryland program that offered bounties to scrap processors to destroy abandoned automobile hulks. See also \"Wisconsin Dep't of Indus., Labor & Human Relations v. Gould Inc.\", 475 U.S. 282, 289 (1986); \"Reeves, Inc. v. Stake\", 447 U.S. 429, 437 (1980). Because Maryland required out-of-state processors, but not in-state processors, to submit burdensome documentation to claim their bounties, the state effectively favored in-state processors over out-of-state processors. The Court held that because the state was merely attaching conditions to its expenditure of state funds, the Maryland program affected the market no differently than if Maryland were a private company bidding up the price of auto hulks. Because the state was not \"regulating\" the market, its economic activity was not subject to the anti-discrimination principles underlying the dormant Commerce Clause—and the state could impose different paperwork burdens on out-of-state processors. \"Nothing in the purposes animating the Commerce Clause prohibits a State, in the absence of congressional action, from participating in the market and exercising the right to favor its own citizens over others.\"\n\nAnother important case is \"White v. Massachusetts Council of Constr. Employers, Inc.\", in which the Supreme Court held that the City of Boston could require its building contractors to hire at least fifty percent of their workforce from among Boston residents. 460 U.S. at 214–15. Because all of the employees covered by that mandate were \"in a substantial if informal sense, 'working for the city,' \" Boston was considered to be simply favoring its own residents through the expenditures of municipal funds. The Supreme Court stated, \"when a state or local government enters the market as a participant it is not subject to the restraints of the Commerce Clause.\" Id. at 208. Nothing in the Constitution precludes a local government from hiring a local company precisely because it is local.\n\nOther important cases enunciating the market participation exception principle are \"Reeves, Inc. v. Stake\", and \"South-Central Timber Development, Inc. v. Wunnicke\", . The \"Reeves\" case outlines the market participation exception test. In this case state-run cement co-ops were allowed to make restrictive rules (e.g. rules not to sell out-of-state). Here, this government-sponsored business was acting restrictively like an individually owned business and this action was held to be constitutional. \"South-Central Timber\" is important because it limits the market exception. \"South-Central Timber\" holds that the market-participant doctrine is limited in allowing a State to impose burdens on commerce within the market in which it is a participant, but allows it to go no further. The State may not impose conditions that have a substantial regulatory effect outside of that particular market.\n\nThe \"market participation exception\" to the dormant Commerce Clause does not give states unlimited authority to favor local interests, because limits from other laws and Constitutional limits still apply. In \"United Building & Construction Trades Council v. Camden\", , the city of Camden, New Jersey had passed an ordinance requiring that at least forty percent of the employees of contractors and subcontractors on city projects be Camden residents. The Supreme Court found that while the law was not infirm because of the Dormant Commerce Clause, it violated the Privileges and Immunities Clause of Article IV of the Constitution. Justice Rehnquist's opinion distinguishes the market-participant doctrine from the privileges and immunities doctrine. Similarly, Congress has the power itself under the Commerce Clause to regulate and sanction states acting as \"market participants\", but it lacks power to legislate in ways that violate Article IV.\n\nIn the 21st century, the dormant Commerce Clause has been a frequent legal issue in cases arising under state laws regulating some aspects of Internet activity. Because of the interstate, and often international, nature of Internet communications, state laws addressing internet-related subjects such as spam, online sales or online pornography can often trigger Dormant Commerce Clause issues.\n\nA \"negative\" or \"dormant\" component to the Commerce Clause has been the subject of scholarly discussion for many decades. Both Supreme Court Justices Antonin Scalia and Clarence Thomas have rejected the notion of a Dormant Commerce Clause. They believe that such a doctrine is inconsistent with an originalist interpretation of the Constitution—so much so that they believe the doctrine is a \"judicial fraud\".\n\nA number of earlier Supreme Court justices also expressed dissatisfaction with the dormant Commerce Clause doctrine. For example, Chief Justice Taney said this in 1847:\n\nIf it was intended to forbid the States from making any regulations of commerce, it is difficult to account for the omission to prohibit it, when that prohibition has been so carefully and distinctly inserted in relation to other powers ... [T]he legislation of Congress and the States has conformed to this construction from the foundation of the government ... The decisions of this court will also, in my opinion, when carefully examined, be found to sanction the construction I am maintaining.\n\nHowever, that statement by Taney in 1847 was before the doctrine morphed in the 1851 case of \"Cooley v. Board of Wardens\", in which Justice Benjamin R. Curtis wrote for the Court that the Commerce Clause does not always require \"exclusive legislation by Congress\".\n\nIn \"Trailer Marine Transport Corp. v. Rivera Vázquez\", 977 F.2d 1, 7-8 (1st Cir. 1992), the First Circuit held that the dormant Commerce Clause applies to Puerto Rico.\n\n\n"}
{"id": "194143", "url": "https://en.wikipedia.org/wiki?curid=194143", "title": "Double negative", "text": "Double negative\n\nA double negative is a grammatical construction occurring when two forms of negation are used in the same sentence. Multiple negation is the more general term referring to the occurrence of more than one negative in a clause. In some languages, double negatives cancel one another and produce an affirmative; in other languages, doubled negatives intensify the negation. Languages where multiple negatives affirm each other are said to have negative concord or emphatic negation. Portuguese, Persian, Russian, Spanish, Neapolitan, Italian, Japanese, Bulgarian, Czech, Polish, Afrikaans, Hebrew, and some dialects of English, such as African-American Vernacular English, are examples of negative-concord languages, while Latin and German do not have negative concord. It is cross-linguistically observed that negative-concord languages are more common than those without.\n\nLanguages without negative concord typically have negative polarity items that are used in place of additional negatives when another negating word already occurs. Examples are \"ever\", \"anything\" and \"anyone\" in the sentence \"I haven't ever owed anything to anyone\" (cf. \"I have\"n't\" \"never\" owed \"nothing\" to \"no one\"\" in negative-concord dialects of English, and \"\"Nunca\" devi \"nada\" a \"ninguém\"\" in Portuguese, lit. \"Never have I owed nothing to no one\", or \"\"Non\" ho \"mai\" dovuto \"nulla\" a \"nessuno\"\" in Italian). Note that negative polarity can be triggered not only by direct negatives such as \"not\" or \"never\", but also by words such as \"doubt\" or \"hardly\" (\"I doubt he has ever owed anything to anyone\" or \"He has hardly ever owed anything to anyone\").\n\nStylistically, in English, double negatives can sometimes be used for affirmation (e.g. \"I'm not feeling not good\"), an understatement of the positive (\"I'm feeling good\"). The rhetorical term for this is litotes.\n\nWhen two negatives are used in one independent clause, in standard English the negatives are understood to cancel one another and produce a weakened affirmative: this is known as litotes. However, depending on how such a sentence is constructed, in some dialects if a verb or adverb is in between two negatives then the latter negative is assumed to be intensifying the former thus adding weight or feeling to the negative clause of the sentence. For this reason, it is difficult to portray double negatives in writing as the level of intonation to add weight in one's speech is lost. A double negative intensifier does not necessarily require the prescribed steps, and can easily be ascertained by the mood or intonation of the speaker.\n\nvs.\n\nThese two sentences would be different in how they are communicated by speech. Any assumption would be correct, and the first sentence can be just as right or wrong in intensifying a negative as it is in cancelling it out; thereby rendering the sentence's meaning ambiguous. Since there is no adverb or verb to support the latter negative, the usage here is ambiguous and lies totally on the context behind the sentence. In light of punctuation, the second sentence can be viewed as the intensifier; and the former being a statement thus an admonishment.\n\nIn Standard English, two negatives are understood to resolve to a positive. This rule was observed as early as 1762, when Bishop Robert Lowth wrote \"A Short Introduction to English Grammar with Critical Notes\". For instance, \"I do not disagree\" could mean \"I certainly agree\", \"I agree\", \"I sort of agree\", \"I don't understand your point of view\", \"I have no opinion\", and so on; it is a form of \"weasel words\". Further statements are necessary to resolve which particular meaning was intended.\n\nThis is opposed to the single negative \"I do not agree\", which typically means \"I disagree\". However, the statement \"I do not completely disagree\" is a similar double negative to \"I do not disagree\" but needs little or no clarification.\n\nWith the meaning \"I completely agree\", Lowth would have been referring to litotes wherein two negatives simply cancel each other out. However, the usage of intensifying negatives and examples are presented in his work, which could also imply he wanted either usage of double negatives abolished. Because of this ambiguity, double negatives are frequently employed when making back-handed compliments. The phrase \"Mr. Jones was not incompetent.\" will seldom mean \"Mr. Jones was very competent\" since the speaker would have found a more flattering way to say so. Instead, some kind of problem is implied, though Mr. Jones possesses basic competence at his tasks.\n\nDiscussing English grammar, the term \"double negative\" is often though not universally applied to the non-standard use of a second negative as an intensifier to a negation.\n\nDouble negatives are usually associated with regional and ethnical dialects such as Southern American English, African American Vernacular English, and various British regional dialects. Indeed, they were used in Middle English. Historically, Chaucer made extensive use of double, triple, and even quadruple negatives in his \"Canterbury Tales\". About the Friar, he writes \"Ther nas no man no wher so vertuous\" (\"There never was no man nowhere so virtuous\"). About the Knight, \"He nevere yet no vileynye ne sayde / In all his lyf unto no maner wight\" (\"He never yet no vileness didn't say / In all his life to no manner of man\").\n\nFollowing the battle of Marston Moor, Oliver Cromwell quoted his nephew's dying words in a letter to the boy's father Valentine Walton: \"A little after, he said one thing lay upon his spirit. I asked him what it was. He told me it was that God had not suffered him to be no more the executioner of His enemies.\" Although this particular letter has often been reprinted, it is frequently changed to read \"not ... to be any more\" instead.\n\nWhereas some double negatives may resolve to a positive, in some dialects others resolve to intensify the negative clause within a sentence. For example:\n\nIn contrast, some double negatives become positives:\n\nThe key to understanding the former examples and knowing whether a double negative is intensive or negative is finding a verb between the two negatives. If a verb is present between the two, the latter negative becomes an intensifier which does not negate the former. In the first example, the verb \"to go\" separates the two negatives; therefore the latter negative does not negate the already negated verb. Indeed, the word 'nowhere' is thus being used as an adverb and does not negate the argument of the sentence. One interesting thing to note is that double negatives such as \"I don't want to know no more\" contrasts with Romance languages such as French in \"Je ne veux pas savoir.\" \n\nAn exception is when the second negative is stressed, as in \"I'm not doing ; I'm thinking.\" A sentence can otherwise usually only become positive through consecutive uses of negatives, such as those prescribed in the later examples, where a clause is void of a verb and lacks an adverb to intensify it. Two of them also use emphasis to make the meaning clearer. The last example is a popular example of a double negative that resolves to a positive. This is because the verb 'to doubt' has no intensifier which effectively resolves a sentence to a positive. Had we added an adverb thus:\n\nThen what happens is that the verb \"to doubt\" becomes intensified, which indeed deduces that the sentence is indeed false since nothing was resolved to a positive. The same applies to the third example, where the adverb 'more' merges with the prefix \"no-\" to become a negative word, which when combined with the sentence's former negative only acts as an intensifier to the verb \"hungry\". Where people think that the sentence \"I'm not hungry no more\" resolves to a positive is where the latter negative \"no\" becomes an adjective which only describes its suffix counterpart \"more\" which effectively becomes a noun, instead of an adverb. This is a valid argument since adjectives do indeed describe the nature of a noun; yet some fail to take into account that the phrase \"no more\" is only an adverb and simply serves as an intensifier. Another argument used to support the position double negatives aren't acceptable is a mathematical analogy: negating a negative number results in a positive one; e.g., ; therefore, it is argued, \"I did not go nowhere\" resolves to \"I went somewhere\".\n\nOther forms of double negatives, which are popular to this day and do strictly enhance the negative rather than destroying it, are described thus:\n\nPhilosophies aside, this form of double negative is still in use whereby the use of 'nor' enhances the negative clause by emphasizing what isn't to be. Opponents of double negatives would have preferred \"I'm not entirely familiar with Nihilism or Existentialism\"; however this renders the sentence somewhat empty of the negative clause being advanced in the sentence. This form of double negative along with others described are standard ways of intensifying as well as enhancing a negative. The use of 'nor' to emphasise the negative clause is still popular today, and has been popular in the past through works of Shakespeare and Milton:\n\nTo the common reader the negatives herein do not cancel each other out but simply emphasizes the negative clause.\nUp to the 18th century, double negatives were used to emphasize negation. \"Prescriptive grammarians\" recorded and codified a shift away from the double negative in the 1700s. Double negatives continue to be spoken by those of Vernacular English, such as those of Appalachian English and African American Vernacular English. To such speakers, they view double negatives as emphasizing the negative rather than cancelling out the negatives. Researchers have studied African American Vernacular English (AAVE) and trace its origins back to colonial English. This shows that double negatives were present in colonial English, and thus presumably English as a whole, and were acceptable at that time. English after the 18th century was changed to become more logical and double negatives became seen as canceling each other as in mathematics. The use of double negatives became associated with being uneducated and illogical.\n\nIn his \"Essay towards a practical English Grammar\" of 1711, James Greenwood first recorded the rule: \"Two Negatives, or two Adverbs of Denying do in English affirm\". Robert Lowth stated in his grammar textbook \"A Short Introduction to English Grammar\" (1762) that \"two negatives in English destroy one another, or are equivalent to an affirmative\". Grammarians have assumed that Latin was the model for Lowth and other early grammarians in prescribing against negative concord, as Latin does not feature it. Data indicates, however, that negative concord had already fallen into disuse in Standard English by the time of Lowth's grammar, and no evidence exists that the loss was driven by prescriptivism, which was well established by the time it appeared.\n\nDouble negatives have been employed in various films and television shows. In the film \"Mary Poppins\", the chimney sweep Bert employs a double negative when he says, \"If you don't want to go nowhere...\" Another is used by the bandits in the \"Stinking Badges\" scene of John Huston's \"The Treasure of the Sierra Madre\": \"Badges? We ain't got no badges. We don't need no badges!\".\n\nMore recently, the British television show \"EastEnders\" has received some publicity over the Estuary accent of character Dot Branning, who speaks with double and triple negatives (\"I ain't never heard of no licence.\").. In the Harry Enfield sketch \"Mr Cholmondley-Warner's Guide to the Working-Class\", a stereotypical Cockney employs a septuple-negative: \"Inside toilet? I ain't never not heard of one of them nor I ain't nor nothing.\"\n\nIn music, double negatives can be employed to similar effect (as in Pink Floyd's \"Another Brick in the Wall\", in which schoolchildren chant \"We don't need no education / We don't need no thought control\") or used to establish a frank and informal tone (as in The Rolling Stones' \"(I Can't Get No) Satisfaction\").\n\nDouble negation is uncommon in other West Germanic languages. A notable exception is Afrikaans, where it is mandatory (for example, \"He cannot speak Afrikaans\" becomes \"Hy kan nie Afrikaans praat nie\", \"He cannot Afrikaans speak not\"). Dialectal Dutch, French and San have been suggested as possible origins for this trait. Its proper use follows a set of fairly complex rules as in these examples provided by Bruce Donaldson:\n\nAnother point of view is that this construction is not really an example of a \"double negative\" but simply a grammatical template for negation. The second \"nie\" cannot be understood as a noun or adverb (as can, e.g., \"pas\" in French), and cannot be substituted by any part of speech other than itself with the sentence remaining grammatical. It is a grammatical particle with no independent meaning that happens to be spelled and pronounced the same as the embedded \"nie\", meaning \"not\", through historical accident.\n\nThe second \"nie\" is used if and only if the sentence or phrase doesn't already end with \"nie\" or another negating adverb.\n\nAfrikaans shares with English the property that two negatives make a positive. For example,\n\nWhile double negation is still found in the Low Franconian dialects of west Flanders (e.g., \"Ik ne willen da nie doen\", \"I do not want to do that\") and in some villages in the central Netherlands such as Garderen, it takes a different form than that found in Afrikaans. In Belgian Dutch dialects, however, there are still some widely used expressions like \"nooit niet\" (\"never not\") for \"never\".\n\nSimilar to some dialectal English, Bavarian employs both single and double negation, with the latter denoting special emphasis. For example, compare the Bavarian \"Des hob i no nia ned g'hört\" (\"This have I yet never not heard\") with the standard German \"Das habe ich noch nie gehört\". The German emphatic \"niemals!\" (roughly \"never ever\") corresponds to Bavarian \"(går) nia ned\" or even \"nie nicht\" in Standard German pronunciation.\n\nAnother exception is Yiddish. Due to Slavic influence, the double (and sometimes even triple) negative is quite common.\n\nA few examples would be:\n\nWhile in Latin a second negative word appearing along with \"non\" turns the meaning into a positive one: \"ullus\" means \"any\", \"nullus\" means \"no\", \"non...nullus\" (\"nonnullus\") means \"some\". In the same way, \"umquam\" means \"ever\", \"numquam\" means \"never\", \"non...numquam\" (\"nonnumquam\") means \"sometimes\", in many Romance languages a second term indicated a negative is required.\n\nIn French, the usual way to express negation is to employ two negatives, e.g. \"ne [verb] pas\", \"ne [verb] plus\", or \"ne [verb] jamais\", as in the sentences \"Je ne sais pas\" (\"I do not know\"), \"Il n'y a plus de baguettes\" (\"There aren't any more baguettes\"), and \"On ne sait jamais\" (\"one never knows\"). The second term was originally an emphatic; \"pas\", for example, derives from the Latin \"passus\", meaning \"step\", so that French \"Je ne marche pas\" and Catalan \"No camino pas\" originally meant \"I will not walk a single step.\" This initial usage spread so thoroughly that it became a necessary element of any negation in the modern French language and that, in fact, in contemporary French, the original actual negative \"ne\" is mostly left away in favour of \"pas\", as in \"Je sais pas\" \"I don't know\". In Northern Catalan, \"no\" may be omitted in colloquial language, and Occitan, which uses \"non\" only as a short answer to questions. In Venetian, the double negation \"no ... mìa\" can likewise lose the first particle and rely only on the second: \"magno mìa\" (\"I eat not\") and \"vegno mìa\" (\"I come not\"). These exemplify Jespersen's cycle.\n\nItalian, Portuguese and Romanian languages usually employ doubled negative correlatives. Portuguese \"Não vejo nada\", Romanian \"Nu văd nimic\" and Italian \"Non vedo niente\" (\"I do not see nothing\") are used to express \"No, I do not see anything\". In Italian, a second following negative particle \"non\" turns the phrase into a positive one, but with a slightly different meaning. For instance, while both \"Voglio mangiare\" (\"I want to eat\") and \"Non voglio non mangiare\" (\"I don't want not to eat\") mean \"I want to eat\", the latter phrase more precisely means \"I'd prefer to eat\".\n\nOther Romance languages employ double negatives less regularly. In Asturian, an extra negative particle is used with negative adverbs: \"Yo nunca nun lu viera\" (\"I had not never seen him\") means \"I have never seen him\" and \"A mi tampoco nun me presta\" (\"I neither do not like it\") means \"I do not like it either\". Standard Catalan and Galician also used to possess a tendency to double \"no\" with other negatives, so \"Jo tampoc no l'he vista\" or \"Eu tampouco non a vira\", respectively (\"I neither have not seen her\") meant \"I have not seen her either\". That practice is dying out.\n\nIn 1974, Italy held a referendum on whether to repeal a recent law that allowed divorce. Voters were said to have been confused in that in order to support divorce, they needed to vote 'no' on the referendum which was worded so that 'yes' would support repeal. And to reframe the fundamental underlying issue as being support/non-support of the continuation of marriage, then the vote was structured as a triple negative (with divorce as the negation of the continuation of marriage being the first negative). This referendum was defeated, and without this confusion, it was said that it would have been defeated more decisively.\n\nIn spoken Welsh, the word ddim (not) often occurs with a prefixed or mutated verb form that is negative in meaning: \"Dydy hi ddim yma\" (word-for-word, \"Not-is she not here\") expresses \"She is not here\" and \"Chaiff Aled ddim mynd\" (word-for-word, \"Not-will-get Aled not go\") expresses \"Aled is not allowed to go\".\n\nNegative correlatives can also occur with already negative verb forms. In literary Welsh, the mutated verb form is caused by an initial negative particle, ni or nid. The particle is usually omitted in speech but the mutation remains: \"[Ni] wyddai neb\" (word-for-word, \"[Not] not-knew nobody\") means \"Nobody knew\" and \"[Ni] chaiff Aled fawr o bres\" (word-for-word, \"[Not] not-will-get Aled lots of money\") means \"Aled will not get much money\". This is not usually regarded as three negative markers, however, because the negative mutation is really just an effect of the initial particle on the following word.\n\nDoubled negatives are perfectly correct in Ancient Greek. With few exceptions, a simple negative (οὐ or μή) following another negative (for example, οὐδείς, \"no one\") results in an affirmation: οὐδείς οὐκ ἔπασχε τι (\"No one was not suffering\") means more simply \"Everyone was suffering\". Meanwhile, a compound negative following a negative strengthens the negation: μὴ θορυβήσῃ μηδείς (\"Do not permit no one to raise an uproar\") means \"Let not a single one among them raise an uproar\".\n\nThose constructions apply only when the negatives all refer to the same word or expression. Otherwise, the negatives simply work independently of one another: οὐ διὰ τὸ μὴ ἀκοντίζειν οὐκ ἔβαλον αὐτόν means \"It was not on account of their not throwing that they did not hit him\", and one should not blame them for not trying.\n\nIn Modern Greek, negative concord is standard and more commonly used. For example, the sentence 'You (pl.) will not find anything' can be said in two ways: 'Δε θα βρείτε τίποτα' ('Not will find nothing') is more common than 'Δε θα βρείτε κάτι' ('Not will find something'). It depends simply on the mood of the speaker, and the latter being is considered slightly more polite. An exception to that rule is the (archaic) pronoun ουδείς, also meaning \"no one\", which does not allow negation of the verb that it governs.\n\nIn Slavic languages other than Slavonic, multiple negatives are grammatically correct ways to express negation, and a single negative is often incorrect. In complex sentences, every part that could be grammatically negated should be negative. For example, in the Serbo-Croatian, \"Ni(t)ko nikad(a) nigd(j)e ništa nije uradio\" (\"Nobody never did not do nothing nowhere\") means \"Nobody has ever done anything, anywhere\", and \"Nikad nisam tamo išao/išla\" (\"Never I did not go there\") means \"I have never been there\". In Czech it is also common to use three or more negations. For example, \"Nikdy jsem nikde nikoho neviděl\" (\"I have not never seen no one nowhere\"). In Russian, \"I know nothing\" is я ничего не знаю (\"ya nichevo nye znayu\"), lit. \"I nothing don't know.\"\n\nA single negation, while syntactically correct, may result in a very unusual meaning or make no sense at all. Saying \"I saw nobody\" in Polish (\"Widziałem nikogo\") instead of the more usual \"I did not see nobody\" (\"Nikogo nie widziałem\") might mean \"I saw an instance of nobody\" or \"I saw Mr. Nobody\" but it would not have its plain English meaning. Likewise, in Slovenian, saying \"I do not know anyone\" (') in place of \"I do not know no one\" (') has the connotation \"I do not know just \"anyone\"\": I know someone important or special.\n\nAs with most synthetic \"satem\" languages double negative is mandatory in Latvian and Lithuanian. Furthermore, all verbs and indefinite pronouns in a given statement must be negated, so it could be said that multiple negative is mandatory in Latvian.\n\nFor instance, a statement \"I have not ever owed anything to anyone\" would be rendered as \"es nekad nevienam neko neesmu bijis parādā\". The only alternative would be using a negating subordinate clause and subjunctive in the main clause, which could be approximated in English as \"there has not ever been an instance that I would have owed anything to anyone\" (\"nav bijis tā, ka es kādreiz būtu kādam bijis kaut ko parādā\"), where negative pronouns (\"nekad, neviens, nekas\") are replaced by indefinite pronouns (\"kādreiz, kāds, kaut kas\") more in line with the English \"ever, any\" indefinite pronoun structures.\n\nDouble or multiple negatives are grammatically required in Hungarian with negative pronouns: \"Nincs semmim\" (word for word: \"[doesn't-exists] [nothing-of-mine]\", and translates literally as \"I do not have nothing\") means \"I do not have anything\". Negative pronouns are constructed by means of adding the prefixes \"se-,\" \"sem-,\" and \"sen-\" to interrogative pronouns.\n\nSomething superficially resembling double negation is required also in Finnish, which uses the auxiliary verb \"ei\" to express negation. Negative pronouns are constructed by adding one of the suffixes \"-an,\" \"-än,\" \"-kaan,\" or \"-kään\" to interrogative pronouns: \"Kukaan ei soittanut minulle\" means \"No one called me\". These suffices are, however, never used alone, but always in connection with \"ei\". This phenomenon is commonplace in Finnish, where many words have alternatives that are required in negative expressions, for example \"edes\" for \"jopa\" (\"even\"), as in \"jopa niin paljon\" meaning \"even so much\", and \"ei edes niin paljoa\" meaning \"not even so much\".\n\nNegative verb forms are grammatically required in Turkish phrases with negative pronouns or adverbs that impart a negative meaning on the whole phrase. For example, \"Hiçbir şeyim yok\" (literally, word for word, \"Not-one thing-of-mine exists-not\") means \"I don't have anything\". Likewise, \"Asla memnun değilim\" (literally, \"Never satisfied not-I-am\") means \"I'm never satisfied\".\n\nJapanese employs litotes to phrase ideas in a more indirect and polite manner. Thus, one can indicate necessity by emphasizing that not doing something would not be proper. For instance, しなければならない (\"shinakereba naranai\", \"must\") literally means \"not doing [it] would not be proper\". しなければいけません (\"shinakereba ikemasen\", also \"must\") similarly means \"not doing [it] cannot go forward\".\n\nOf course, indirectness can also be employed to put an edge on one's rudeness as well. \"He has studied Japanese, so he should be able to write kanji\" can be phrased 彼は日本語を勉強したから漢字で書けないわけがありません (\"kare wa nihongo o benkyō shita kara kanji de kakenai wake ga arimasen\"), there is a rather harsher idea: \"As he has studied Japanese, the reasoning that he cannot write Kanji does not exist\".\n\nMandarin Chinese also employs litotes in a like manner. One common construction is 不得不 (Pinyin: \"bùdébù\", \"cannot not\"), which is used to express (or feign) a necessity more regretful and polite than that expressed by 必须 (\"bìxū\", \"must\"). Compared with \"我必须走\" (\"Wǒ bìxū zǒu\", \"I must go\"), \"我不得不走\" (\"Wǒ bùdébù zǒu\", \"I cannot not go\") tries to emphasize that the situation is out of the speaker's hands and that the speaker has no choice in the matter: \"Unfortunately, I have got to go\". Similarly, \"没有人不知道\" (\"Méiyǒu rén bù zhīdào\", \"There is not a person who does not know\") is a more emphatic way to express \"Everyone knows\".\n\nDouble negatives nearly always resolve to a positive meaning even in colloquial speech, while triple negatives resolve to a negative meaning. For example, \"我不相信没人不来\" (\"Wǒ bù xiāngxìn méi rén bù lái\", \"I do not believe no one will not come\") means \"I do not think everyone will come\". However, triple or multiple negatives are considered obscure and are typically avoided.\n\nMany languages, including all living Germanic languages, French, Welsh and some Berber and Arabic dialects, have gone through a process known as Jespersen's cycle, where an original negative particle is replaced by another, passing through an intermediate stage employing two particles (e.g. Old French \"jeo ne dis\" → Modern Standard French \"je ne dis pas\" → Modern Colloquial French \"je dis pas\" \"I don't say\").\n\nIn many cases, the original sense of the new negative particle is not negative \"per se\" (thus in French \"pas\" \"step\", originally \"not a step\" = \"not a bit\"), but in Germanic languages, such as English and German the intermediate stage was a case of double negation, as the current negatives \"not\" and \"nicht\" in these languages originally meant \"nothing\": e.g. Old English \"ic ne seah\" \"I didn't see\" » Middle English \"I ne saugh nawiht\", lit. \"I didn't see nothing\" » Early Modern English \"I saw not\".\n\nA similar development to a circumfix from double negation can be seen in non-Indo-European languages, too: for example, in Maltese, \"kiel\" \"he ate\" is negated as \"ma kielx\" \"he did not eat\", where the verb is preceded by a negative particle \"ma\"- \"not\" and followed by the particle -\"x\", which was originally a shortened form of \"xejn\" \"nothing\" - thus, \"he didn't eat nothing\".\n\n"}
{"id": "335910", "url": "https://en.wikipedia.org/wiki?curid=335910", "title": "Eight-circuit model of consciousness", "text": "Eight-circuit model of consciousness\n\nThe Eight-Circuit Model of Consciousness is a hypothesis by Timothy Leary, and later expanded on by Robert Anton Wilson and Antero Alli, that \"suggested eight periods [circuits] and twenty-four stages of neurological evolution\". The eight circuits, or eight \"brains\" as referred by other authors, operate within the human nervous system, each corresponding to its own imprint and direct experience of reality. Leary and Alli include three stages for each circuit that details developmental points for each level of consciousness.\nThe first four circuits deal with life on earth, and survival of the species. The last four circuits are post-terrestrial, and deal with the evolution of the species, altered states of consciousness, enlightenment, mystical experiences, psychedelic states of mind, and psychic abilities. The proposal suggests that these altered states of consciousness are recently realized, but not widely utilized. Leary describes the first four as \"larval circuits\", necessary for surviving and functioning in a terrestrial human society, and proposed that the post terrestrial circuits will be useful for future humans who, through a predetermined script, continue to act on their urge to migrate to outer space and live extra-terrestrially. Leary, Wilson, and Alli have written about the idea in depth, and have explored and attempted to define how each circuit operates, both in the lives of individual people and in societies and civilization.\n\nThe term \"circuit\" is equated to a metaphor of the brain being computer hardware, and that the wiring of the brain as circuitry.\n\nLeary uses the eight circuits along with recapitulation theory to explain the evolution of the human species, the personal development of an individual, and the biological evolution of all life.\n\nEach circuit listed has each name from Leary's book \"Exo-Psychology\" after the preface, and Wilson's book \"Quantum Psychology\" pgs.196-201. \"Note:In other books from Leary, Wilson, and Alli, the eight circuits have different names due to different interpretations and findings of each author. Please reference bibliography section for other works on labeling of each circuit.\"\n\nThis circuit is concerned with nourishment, physical safety, comfort and survival, suckling, cuddling, etc. It begins with one spatial dimension, forward/back.\n\nThis circuit is imprinted early in infancy. The imprint will normally last for life unless it is re-imprinted by a powerful experience. Depending on the nature of the imprint, the organism will tend towards one of two basic attitudes:\n\nThis circuit is said to have appeared in the earliest evolution of the invertebrate brain and corresponds to the reptilian brain of triune brain theory. This circuit operates in essentially the same way across mammals, reptiles, fish, primates and humans. \n\nRobert Anton Wilson equated this circuit with the oral stage in the Freudian theory of psychosexual development, and proposed that this circuit is activated in adults by strong opioids.\n\nThe emotional-territorial circuit is imprinted in the toddler stage. It is concerned with domination and submission, territoriality, etc.\n\nThe imprint on this circuit will trigger one of two states:\n\nThis circuit is activated by depressant drugs such as alcohol, barbiturates, and benzodiazepines. This circuit appeared first in territorial vertebrate animals and is preserved across all mammals. It corresponds to the mammalian brain of triune brain theory. Robert Anton Wilson equated this circuit with the anal stage in the Freudian theory of psycho-sexual development. This circuit introduces a 2nd spatial dimension; up/down.\n\nThe first and second circuits both imprint in a binary fashion: trust/suspicion and dominance/submission. Thus there are four possible ways of imprinting the first two circuits:\n\n\nThis circuit is imprinted by human symbol systems. It is concerned with language, handling the environment, invention, calculation, prediction, building a mental \"map\" of the universe, physical dexterity, etc.\n\nThis circuit is activated by stimulant drugs such as amphetamines, cathinones, cocaine, and caffeine. This circuit supposedly appeared first when hominids started differentiating from the rest of the primates.\n\nRobert Anton Wilson, being heavily influenced by General Semantics, writes of this circuit as the 'time-binding circuit'. This means that this circuit's contents – including human know-how, technology, science etc. - are preserved memetically and passed on from generation to generation, constantly mutating and increasing in sophistication.\n\nThis fourth circuit is imprinted by the first orgasm-mating experiences and tribal \"morals\". It is concerned with sexual pleasure (instead of sexual reproduction), local definitions of \"moral\" and \"immoral\", reproduction, rearing of the young, etc. The fourth circuit concerns itself with cultural values and operating within social networks. This circuit is said to have first appeared with the development of tribes. Some have pointed out that entactogens such as MDMA seem to meet some of the requirements needed to activate this circuit.\n\nThis is concerned with neurological-somatic feedbacks, feeling high and blissful, somatic reprogramming, etc. It may be called the rapture circuit.\n\nWhen this circuit is activated, a non-conceptual feeling of well-being arises. This has a beneficial effect on the health of the physical body.\n\nThe fifth circuit is consciousness of the body. There is a marked shift from linear visual space to an all-encompassing aesthetic sensory space. Perceptions are judged not so much for their meaning and utility, but for their aesthetic qualities. Experience of this circuit often accompanies an hedonistic turn-on, a rapturous amusement, a detachment from the previously compulsive mechanism of the first four circuits.\n\nThis circuit is activated by ecstatic experiences via physiological effects of cannabis, Hatha Yoga, tantra and Zen meditation. Robert Anton Wilson writes, \"Tantra yoga is concerned with shifting consciousness entirely into this circuit\" and that \"Prolonged sexual play without orgasm always triggers some Circuit V consciousness\".\n\nLeary describes that this circuit first appeared in the upper classes, with the development of leisure-class civilizations around 2000 BC.\n\n\"Note: Timothy Leary lists this circuit as the sixth, and the neurogenetic circuit as the seventh. In \"Prometheus Rising\", Robert Anton Wilson reversed the order of these two circuits, describing the neurogenetic circuit as the sixth circuit, and the metaprogramming circuit as the seventh. In the subsequently published \"Quantum Psychology\", he reverted this back to the order proposed by Leary.\"\n\nThis circuit is concerned with re-imprinting and re-programming all earlier circuits and the relativity of \"realities\" perceived. The sixth circuit consists of the nervous system becoming aware of itself. Leary says this circuit enables telepathic communication and is activated by low-to-moderate doses of LSD (50-150 µg), moderate doses of peyote, psilocybin mushrooms and meditation/chanting especially when used in a group or ritual setting. This circuit is traced by Leary back to 500 BC.\n\nThis circuit is the connection of the individual's mind to the whole sweep of evolution and life as a whole. It is the part of consciousness that echoes the experiences of the previous generations that have brought the individual's brain-mind to its present level.\n\nIt deals with ancestral, societal and scientific DNA-RNA-brain feedbacks. Those who achieve this mutation may speak of past lives, reincarnation, immortality etc. It corresponds to the collective unconscious in the models of Carl Jung where archetypes reside.\n\nActivation of this circuit may be equated with consciousness of the Great God Pan in his aspect as Life as a whole, or with consciousness of Gaia, the biosphere considered as a single organism.\n\nThis circuit is activated by higher doses of LSD (200-500 µg), higher doses of peyote, higher doses of psilocybin mushrooms, yoga and meditation.\n\nThe circuit first appeared among the Hindus in the early first millennium and later reappeared among the Sufi sects.\n\nThe eighth circuit is concerned with quantum consciousness, non-local awareness (information from beyond ordinary space-time awareness which is limited by the speed of light), illumination. Some of the ways this circuit can get activated are: the awakening of kundalini, shock, a near-death experience, DMT, high doses of LSD and according to Robert Anton Wilson almost any dose of ketamine. This circuit has even been compared to the Buddhist concept of Indra's net from the Avatamsaka Sutra.\n\nLeary stated \"They[The theories presented in \"Info-Psychology\"] are scientific in that they are based on empirical findings from physics, physiology, pharmacology, genetics, astronomy, behavioral psychology, information science, and most importantly, neurology.\" \n\nLeary called his book \"science faction\" or \"psi-phy\" and noted he had written it \"in various prisons to which the author had been sentenced for dangerous ideology and violations of Newtonian and religious laws\".\n\nAlthough Leary propounded the basic premise of eight \"brains\" or brain circuits, he was inspired by sources such as the Hindu \"chakra\" system.\n\nLeary claimed that among other things this model explained the social conflict in the 1960s, where the mainstream was said to be those with four circuits active and characterized by Leary as tribal moralists and clashed with the counter-culturists, who were then said to be those with the fifth circuit active and characterized as individualists and hedonists. \n\nLeary's first book on the subject, \"Neurologic\", only included seven circuits when it was published in 1973. \"Exo-Psychology\", published in 1977, expanded the number of circuits to eight and clarified the subject. In it, he puts forward the theory that the later four circuits are \"post terrestrial;\" intended to develop as we migrate off this planet and colonize others. Once we begin space migration, according to Leary, we will have more ready access to these higher circuits. \"Exo-Psychology\" was re-published as revised by Timothy Leary with additional material in 1989 under the title \"Info-Psychology\" (New Falcon Publishing).\n\nLeary's ideas heavily influenced the work of Robert Anton Wilson. Wilson's book \"Prometheus Rising\" is an in-depth work documenting Leary's eight-circuit model of consciousness. Wilson's published screenplay \"Reality Is What You Can Get Away With\" uses and explains the model. Wilson, like Leary, wrote about the distinction between terrestrial and post-terrestrial life.\n\n\"Angel Tech\" by Antero Alli, is structured around the Eight-circuit model of consciousness. Alli defines the word angel as \"a being of light\" and tech from the word \"techne\" meaning \"art\". The title is defined as \"the art of being light\". It includes suggested activities such as meditations and construction of tarot-card collages associated with each circuit and imprint.\n\nThe model is fairly prominent in chaos magic. This concept has been detailed in \"Chaotopia!\" by Dave Lee, a leading member of the magic society Illuminates of Thanateros. Leary and Wilson were also members of the society. \n\nRolf Von Eckartsberg also appears to have been influenced by the model.\n\n\n\n"}
{"id": "14162696", "url": "https://en.wikipedia.org/wiki?curid=14162696", "title": "Fluid Concepts and Creative Analogies", "text": "Fluid Concepts and Creative Analogies\n\nFluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought is a 1995 book by Douglas Hofstadter and other members of the Fluid Analogies Research Group exploring the mechanisms of intelligence through computer modeling. It contends that the notions of analogy and fluidity are fundamental to explain how the human mind solves problems and to create computer programs that show intelligent behavior. It analyzes several computer programs that members of the group have created over the years to solve problems that require intelligence.\n\nIt was the first book ever sold by Amazon.com.\n\nThe book is a collection of revised articles that appeared in precedence, each preceded by an introduction by Hofstadter.\nThey describe the scientific work by him and his collaborators in the 1980s and 1990s.\nThe project started in the late 1970s at Indiana University.\nIn 1983 he took a sabbatical year at MIT, working in Marvin Minsky's Artificial Intelligence Lab.\nThere he met and collaborated with Melanie Mitchell, who then became his doctoral student.\nSubsequently, Hofstadter moved to the University of Michigan, where the FARG (Fluid Analogies Research Group) was founded.\nEventually he returned to Indiana University in 1988, continuing the FARG research there.\nThe book was written during a sabbatical year at the Istituto per la Ricerca Scientifica e Tecnologica in Trento, Italy.\n\nUpon publication, Jon Udell, a BYTE senior technical editor-at-large said:\nFifteen years ago, \"Gödel, Escher, Bach: An Eternal Golden Braid\" exploded on the literary scene, earning its author a Pulitzer prize and a monthly column in \"Scientific American\". Douglas Hofstadter's exuberant synthesis of math, music, and art, and his inspired thought experiments with \"tangled hierarchy,\" recursion, pattern recognition, figure/ground reversal, and self-reference, delighted armchair philosophers and AI theorists. But in the end, many people believed that these intellectual games yielded no useful model of cognition on which to base future AI research. Now \"Fluid Concepts and Creative Analogies\" presents that model, along with the computer programs Hofstadter and his associates have designed to test it. These programs work in stripped-down yet surprisingly rich microdomains.\n\nOn April 3, 1995, \"Fluid Concepts and Creative Analogies\" became the first book ordered online by an Amazon.com customer.\n\n\nThe first AI project by Hofstadter stemmed from his teenage fascination with number sequences.\nWhen he was 17, he studied the way that triangular and square numbers interleave, and eventually found a recursive relation describing it.\nIn his first course on AI, he set to the students and to himself the task of writing a program that could extrapolate the rule by which a numeric sequence is generated.\nHe discusses breadth-first and depth-first techniques, but eventually concludes that the results represent expert systems that incarnate a lot of technical knowledge but don't shine much light on the mental processes that humans use to solve such puzzles.\n\nInstead he devised a simplified version of the problem, called SeekWhence, where sequences are based on very simple basic rules not requiring advanced mathematical knowledge.\nHe argues that pattern recognition, analogy, and fluid working hypotheses are fundamental to understand how humans tackle such problems.\n\nJumbo is a program to solve jumbles, word puzzles consisting in five or six scrambled letters that need to be anagrammed to form an English word.\nThe resulting word does not need to be a real one but just to a plausible, that is, to consists of a sequence of letters that is normal in English.\n\nThe constituent elements of Jumbo are the following:\nA \"temperature\" is associated to the present state of the cytoplasm; it determines how probable it is that a destructive codelet is executed.\nThere is a \"freezing\" temperature at which no destruction can occur anymore: a solution has been found.\n\nNumbo is a program by Daniel Defays that tries to solve numerical problems similar to those used in the French game \"Le compte est bon\". The game consists in combining some numbers called \"bricks\", using the operations of multiplication, addition, and subtraction, to obtain a given result.\n\nThe program is modeled on Jumbo and Copycat and uses a permanent network of known mathematical facts, a working memory in the form of a cytoplasm, and a coderack containing codelets to produce free associations of bricks in order to arrive at the result.\n\nThe chapter subtitle \"A Critique of Artificial-intelligence Methodology\" indicates that this is a polemical article, in which David Chalmers, Robert French, and Hofstadter criticize most of the research going on at that time (the early '80s) as exaggerating results and missing the central features of human intelligence.\n\nSome of these AI projects, like the structure mapping engine (SME), claimed to model high faculties of the human mind and to be able to understand literary analogies and to rediscover important scientific breakthroughs.\nIn the introduction, Hofstadter warns about the Eliza effect that leads people to attribute understanding to a computer program that only uses a few stock phrases.\nThe authors claim that the input data for such impressive results are already heavily structured in the direction of the intended discovery and only a simple matching task is left to the computer.\n\nTheir main claim is that it is impossible to model high-level cognition without at the same time modeling low-level perception.\nWhile cognition is necessarily based on perception, they argue that it in turn influences perception itself.\nTherefore, a sound AI project should try to model the two together.\nIn a slogan repeated several times throughout the book: \"cognition is recognition\".\n\nSince human perception is too complex to be modelled by available technology, they favor the restriction of AI projects to limited domains like the one used for the Copycat project.\n\nThis chapter presents, as stated in the full title, \"A Model of Mental Fluidity and Analogy-making\".\nIt is a description of the architecture of the Copycat program, developed by Hofstadter and Melanie Mitchell.\nThe field of application of the program is a domain of short alphabetic sequences.\nA typical puzzle is: \"If abc were changed to abd, how would you change ijk in the same way?\".\nThe program tries to find an answer using a strategy supposedly similar to the way the human mind tackles the question.\n\nCopycat has three major components:\nThe resulting software displays emergent properties.\nIt works according to a \"parallel terraced scan\" that runs several possible processes at the same time.\nIt shows mental fluidity in that concepts may \"slip\" into similar ones.\nIt emulates human behavior in tending to find the most obvious solutions most of the time but being more satisfied (as witnessed by low temperature) by more clever and deep answers that it finds more rarely.\n\nThis chapter compares Copycat with other recent (at the time) work in artificial intelligence.\nSpecifically, it matches it with the claimed results from the structure mapping engine SME and the Analogical Constraint Mapping Engine (ACME).\nThe authors' judgment is that those programs suffer from two defects: Their input is pre-structured by the developers to highlight the analogies that the software is supposed to find; and the general architecture of the programs is serial and deterministic rather than parallel and stochastic like Copycat's, which they consider psychologically more plausible.\n\nSevere criticism is put on the claim that these tools can solve \"real-life\" problems.\nIn fact, only the terms used in the example suggest that the input to the programs comes from a concrete situation.\nThe logical structures don't actually imply any meaning for the term.\n\nFinally a more positive assessment is given to two other projects: Indurkhya' PAN model and Kokinov's AMBR system.\n\nThis chapter looks at those aspects of human creativity that are not yet modeled by Copycat and lays down a research plan for a future extension of the software.\nThe main missing element is the mind's ability to observe itself and reflect on its own thinking process.\nAlso important is the ability to learn and to remember the results of the mental activity.\n\nThe creativity displayed in finding analogies should be applicable at ever higher levels: making analogies between analogies (expression inspired by the title of a book by Stanislaw Ulam), analogies between these second-order analogies, and so on.\n\nAnother of Hofstadter's students, Robert French, was assigned the task of applying the architecture of Copycat to a different domain, consisting in analogies between objects lying on a table in a coffeehouse.\nThe resulting program was named Tabletop.\n\nThe authors present a different and vaster domain to justify the relevance of attacking such a trivial-seeming project.\nThe alternative domain is called Ob-Platte and consists in discovering analogies between geographical locations in different regions or countries.\n\nOnce again arguments are offered against a brute-force approach, which would work on the small Tabletop domain but would become unfeasible on the larger Ob-Platte domain.\nInstead a parallel non-deterministic architecture is used, similar to the one adopted by the Copycat project.\n\nIn the premise to the chapter, title \"The Knotty Problem of Evaluating Research\", Hofstadter considers the question of how research in AI should be assessed.\nHe argues against a strict adherence to a match between the results of an AI program with the average answer of human test subjects.\nHe gives two reasons for his rejection: the AI program is supposed to emulate creativity, while an average of human responses will delete any original insight by any of the single subjects; and the architecture of the program should be more important that its mere functional description.\n\nIn the main article, the architecture of Tabletop is described: it is strongly inspired by that of Copycat and consists of a Slipnet, a Workspace, and a Corerack.\n\nThis last chapter is about a more ambitious project that Hofstadter started with student Gary McGraw.\nThe microdomain used is that of grid fonts: typographic alphabets constructed using a rigid system of small rigid components.\nThe goal is to construct a program that, given only a few or just one letter from the grid font, can generate the whole alphabet \"in the same style\".\nThe difficulty lies in the ambiguity and undefinability of \"style\".\nThe projected program would have a structure very similar to that of Jumble, Numble, Copycat, and Tabletop.\n\nIn the concluding part of the book, Hofstadter analyses some AI projects with a critical eye.\nHe finds that today's AI is missing the gist of human creativity and is making exaggerated claims.\nThe project under scrutiny are the following.\n\nAARON, a computer artist that can draw images of people in outdoor settings in a distinctive style reminiscent of that of a human artist; criticism: the program doesn't have any understanding of the objects it draws, it just uses some graphical algorithms with some randomness thrown in to generate different scenes at every run and to give the style a more natural feel.\n\nRacter, a computer author that wrote a book entitled \"The Policeman's Beard Is Half Constructed\".\nAlthough some of the prose generated by the program is quite impressive, due in part to the Eliza effect, the computer does not have any notion of plot or of the meaning of the words it uses. Furthermore, the book is made up of selected texts from thousands produced by the computer over several years.\n\nAM, a computer mathematician that generates new mathematical concepts. It managed to produce by itself the notion of prime number and the Goldbach conjecture. As with Racter, the question is how much the programmer filtered the output of the program, keeping only the occasional interesting output.\nAlso, mathematics being a very specialized domain, it is doubtful whether the techniques used can be abstracted to general cognition.\n\nAnother mathematical program, called Geometry, was celebrated for making an insightful discovery of an original proof that an isosceles triangle has equal base angles. The proof is based on seeing the triangle in two different ways. However, the program generates all possible ways of seeing the triangle, not even knowing that it is the same triangle.\n\nHofstadter concludes with some methodological remarks on the Turing Test.\nIn his opinion it is still a good definition and he argues that by interacting with a program, a human may be able to have insight not just on its behaviour but also on its structure.\nHowever, he criticises the use that is made of it at present: it encourages the development of fancy natural-language interfaces instead of the investigation of deep cognitive faculties.\n"}
{"id": "161999", "url": "https://en.wikipedia.org/wiki?curid=161999", "title": "Idea", "text": "Idea\n\nIn philosophy, ideas are usually taken as mental representational images of some object. Ideas can also be abstract concepts that do not present as mental images. Many philosophers have considered ideas to be a fundamental ontological category of being. The capacity to create and understand the meaning of ideas is considered to be an essential and defining feature of human beings. In a popular sense, an idea arises in a reflexive, spontaneous manner, even without thinking or serious reflection, for example, when we talk about the \"idea\" of a person or a place. A new or original idea can often lead to innovation.\n\nThe word \"idea\" comes from Greek ἰδέα \"idea\" \"form, pattern,\" from the root of ἰδεῖν \"idein\", \"to see.\" \n\nOne view on the nature of ideas is that there exist some ideas (called \"innate ideas\") which are so general and abstract that they could not have arisen as a representation of an object of our perception but rather were in some sense always present. These are distinguished from \"adventitious ideas\" which are images or concepts which are accompanied by the judgment that they are caused or occasioned by an external object.\n\nAnother view holds that we only discover ideas in the same way that we discover the real world, from personal experiences. The view that humans acquire all or almost all their behavioral traits from nurture (life experiences) is known as \"tabula rasa\" (\"blank slate\"). Most of the confusions in the way ideas arise is at least in part due to the use of the term \"idea\" to cover both the representation perceptics and the object of conceptual thought. This can be always illustrated in terms of the scientific doctrines of innate ideas, \"concrete ideas versus abstract ideas\", as well as \"simple ideas versus complex ideas\".\n\nPlato in Ancient Greece was one of the earliest philosophers to provide a detailed discussion of ideas and of the thinking process (it must be noted that in Plato's Greek the word \"idea\" carries a rather different sense from our modern English term). Plato argued in dialogues such as the \"Phaedo\", \"Symposium\", \"Republic\", and \"Timaeus\" that there is a realm of ideas or forms (\"eidei\"), which exist independently of anyone who may have thoughts on these ideas, and it is the ideas which distinguish mere opinion from knowledge, for unlike material things which are transient and liable to contrary properties, ideas are unchanging and nothing but just what they are. Consequently, Plato seems to assert forcefully that material things can only be the objects of opinion; real knowledge can only be had of unchanging ideas. Furthermore, ideas for Plato appear to serve as universals; consider the following passage from the \"Republic\":\nDescartes often wrote of the meaning of \"idea\" as an image or representation, often but not necessarily \"in the mind\", which was well known in the vernacular. Despite that Descartes is usually credited with the invention of the non-Platonic use of the term, he at first followed this vernacular use. In his \"Meditations on First Philosophy\" he says, \"Some of my thoughts are like images of things, and it is to these alone that the name 'idea' properly belongs.\" He sometimes maintained that ideas were innate and uses of the term \"idea\" diverge from the original primary scholastic use. He provides multiple non-equivalent definitions of the term, uses it to refer to as many as six distinct kinds of entities, and divides \"ideas\" inconsistently into various genetic categories. For him knowledge took the form of ideas and philosophical investigation is the deep consideration of these entities.\n\nIn striking contrast to Plato's use of idea is that of John Locke. In his Introduction to An Essay Concerning Human Understanding, Locke defines \"idea\" as \"that term which, I think, serves best to stand for whatsoever is the object of the understanding when a man thinks, I have used it to express whatever is meant by phantasm, notion, species, or whatever it is which the mind can be employed about in thinking; and I could not avoid frequently using it.\" He said he regarded the book necessary to examine our own abilities and see what objects our understandings were, or were not, fitted to deal with. In his philosophy other outstanding figures followed in his footsteps — Hume and Kant in the 18th century, Arthur Schopenhauer in the 19th century, and Bertrand Russell, Ludwig Wittgenstein, and Karl Popper in the 20th century. Locke always believed in \"good sense\" — not pushing things to extremes and on taking fully into account the plain facts of the matter. He considered his common-sense ideas \"good-tempered, moderate, and down-to-earth.\"\n\nAs John Locke studied humans in his work “An Essay Concerning Human Understanding” he continually referenced Descartes for ideas as he asked this fundamental question: “When we are concerned with something about which we have no certain knowledge, what rules or standards should guide how confident we allow ourselves to be that our opinions are right?” A simpler way of putting it is how do humans know ideas, and what are the different types of ideas. An idea to Locke “can simply mean some sort of brute experience.” He shows that there are “No innate principles in the mind.”. Thus, he concludes that “our ideas are all experiential in nature.” An experience can either be a sensation or a reflection: “consider whether there are any innate ideas in the mind before any are brought in by the impression from sensation or reflection.” Therefore, an idea was an experience in which the human mind apprehended something.\n\nIn a Lockean view, there are really two types of ideas: complex and simple. Simple ideas are the building blocks for much more complex ideas, and “While the mind is wholly passive in the reception of simple ideas, it is very active in the building of complex ideas…” Complex ideas, therefore, can either be modes, substances, or relations. Modes are when ideas are combined in order to convey new information. For instance, David Banach gives the example of beauty as a mode. He says that it is the combination of color and form. Substances, however, is different. Substances are certain objects, that can either be dogs, cats, or tables. And relations represent the relationship between two or more ideas. In this way, Locke did, in fact, answer his own questions about ideas and humans.\n\nHume differs from Locke by limiting \"idea\" to the more or less vague mental reconstructions of perceptions, the perceptual process being described as an \"impression.\" Hume shared with Locke the basic empiricist premise that it is only from life experiences (whether their own or others') that humans' knowledge of the existence of anything outside of themselves can be ultimately derived, that they shall carry on doing what they are prompted to do by their emotional drives of varying kinds. In choosing the means to those ends, they shall follow their accustomed associations of ideas. Hume has contended and defended the notion that \"reason alone is merely the 'slave of the passions'.\" \n\nImmanuel Kant defines an \"idea\" as opposed to a \"concept\". \"Regulative ideas\" are ideals that one must tend towards, but by definition may not be completely realized. Liberty, according to Kant, is an idea. The autonomy of the rational and universal subject is opposed to the determinism of the empirical subject. Kant felt that it is precisely in knowing its limits that philosophy exists. The business of philosophy he thought was not to give rules, but to analyze the private judgements of good common sense.\n\nWhereas Kant declares limits to knowledge (\"we can never know the thing in itself\"), in his epistemological work, Rudolf Steiner sees \"ideas\" as \"objects of experience\" which the mind apprehends, much as the eye apprehends light. In \"Goethean Science\" (1883), he declares, \"Thinking ... is no more and no less an organ of perception than the eye or ear. Just as the eye perceives colors and the ear sounds, so thinking perceives ideas.\" He holds this to be the premise upon which Goethe made his natural-scientific observations.\n\nWundt widens the term from Kant's usage to include \"conscious representation of some object or process of the external world\". In so doing, he includes not only ideas of memory and imagination, but also perceptual processes, whereas other psychologists confine the term to the first two groups. One of Wundt's main concerns was to investigate conscious processes in their own context by experiment and introspection. He regarded both of these as \"exact methods\", interrelated in that experimentation created optimal conditions for introspection. Where the experimental method failed, he turned to other \"objectively valuable aids\", specifically to \"those products of cultural communal life which lead one to infer particular mental motives. Outstanding among these are speech, myth, and social custom.\" Wundt designed the basic mental activity apperception — a unifying function which should be understood as an activity of the will. Many aspects of his empirical physiological psychology are used today. One is his principles of mutually enhanced contrasts and of assimilation and dissimilation (i.e. in color and form perception and his advocacy of \"objective\" methods of expression and of recording results, especially in language. Another is the principle of heterogony of ends — that multiply motivated acts lead to unintended side effects which in turn become motives for new actions.\n\nC. S. Peirce published the first full statement of pragmatism in his important works \"\" (1878) and \"\" (1877). In \"How to Make Our Ideas Clear\" he proposed that a \"clear idea\" (in his study he uses concept and \"idea\" as synonymic) is defined as one, when it is apprehended such as it will be recognized wherever it is met, and no other will be mistaken for it. If it fails of this clearness, it is said to be obscure. He argued that to understand an idea clearly we should ask ourselves what difference its application would make to our evaluation of a proposed solution to the problem at hand. Pragmatism (a term he appropriated for use in this context), he defended, was a method for ascertaining the meaning of terms (as a theory of meaning). The originality of his ideas is in their rejection of what was accepted as a view and understanding of knowledge by scientists for some 250 years, i.e. that, he pointed, knowledge was an impersonal fact. Peirce contended that we acquire knowledge as \"participants\", not as \"spectators\". He felt \"the real\", sooner or later, is information acquired through ideas and knowledge with the application of logical reasoning would finally result in. He also published many papers on logic in relation to \"ideas\".\n\nG. F. Stout and J. M. Baldwin, in the \"Dictionary of Philosophy and Psychology\", define \"idea\" as \"the reproduction with a more or less adequate image, of an object not actually present to the senses.\" They point out that an idea and a perception are by various authorities contrasted in various ways. \"Difference in degree of intensity\", \"comparative absence of bodily movement on the part of the subject\", \"comparative dependence on mental activity\", are suggested by psychologists as characteristic of an idea as compared with a perception.\n\nIt should be observed that an idea, in the narrower and generally accepted sense of a mental reproduction, is frequently composite. That is, as in the example given above of the idea of a chair, a great many objects, differing materially in detail, all call a single idea. When a man, for example, has obtained an idea of chairs in general by comparison with which he can say \"This is a chair, that is a stool\", he has what is known as an \"abstract idea\" distinct from the reproduction in his mind of any particular chair (see abstraction). Furthermore, a complex idea may not have any corresponding physical object, though its particular constituent elements may severally be the reproductions of actual perceptions. Thus the idea of a centaur is a complex mental picture composed of the ideas of man and horse, that of a mermaid of a woman and a fish.\n\nDiffusion studies explore the spread of ideas from culture to culture. Some anthropological theories hold that all cultures imitate ideas from one or a few original cultures, the Adam of the Bible, or several cultural circles that overlap. Evolutionary diffusion theory holds that cultures are influenced by one another but that similar ideas can be developed in isolation.\n\nIn the mid-20th century, social scientists began to study how and why ideas spread from one person or culture to another. Everett Rogers pioneered diffusion of innovations studies, using research to prove factors in adoption and profiles of adopters of ideas. In 1976, in his book \"The Selfish Gene\", Richard Dawkins suggested applying biological evolutionary theories to the spread of ideas. He coined the term \"meme\" to describe an abstract unit of selection, equivalent to the gene in evolutionary biology.\n\nJames Boswell recorded Samuel Johnson's opinion about ideas. Johnson claimed that they are mental images or internal visual pictures. As such, they have no relation to words or the concepts which are designated by verbal names.\n\nTo protect the cause of invention and innovation, the legal constructions of Copyrights and Patents were established. Patent law regulates various aspects related to the functional manifestation of inventions based on new ideas or incremental improvements to existing ones. Thus, patents have a direct relationship to ideas.\n\nIn some cases, authors can be granted limited legal monopolies on the manner in which certain works are expressed. This is known colloquially as copyright, although the term intellectual property is used mistakenly in place of \"copyright\". Copyright law regulating the aforementioned monopolies generally does not cover the actual ideas. The law does not bestow the legal status of property upon ideas per se. Instead, laws purport to regulate events related to the usage, copying, production, sale and other forms of exploitation of the fundamental expression of a work, that may or may not carry ideas. Copyright law is fundamentally different from patent law in this respect: patents do grant monopolies on ideas (more on this below).\n\nA copyright is meant to regulate some aspects of the usage of expressions of a work, \"not\" an idea. Thus, copyrights have a negative relationship to ideas.\n\nWork means a tangible medium of expression. It may be an original or derivative work of art, be it literary, dramatic, musical recitation, artistic, related to sound recording, etc. In (at least) countries adhering to the Berne Convention, copyright automatically starts covering the work upon the original creation and fixation thereof, without any extra steps. While creation usually involves an idea, the idea in itself does not suffice for the purposes of claiming copyright. \nConfidentiality and nondisclosure agreements are legal instruments that assist corporations and individuals in keeping ideas from escaping to the general public. Generally, these instruments are covered by contract law.\n\n\n"}
{"id": "2892645", "url": "https://en.wikipedia.org/wiki?curid=2892645", "title": "Instantiation principle", "text": "Instantiation principle\n\nThe instantiation principle or principle of instantiation or principle of exemplification is the concept in metaphysics and logic (first put forward by David Malet Armstrong) that there can be no uninstantiated or unexemplified properties (or universals). In other words, it is impossible for a property to exist which is not had by some object.\n\nConsider a chair. Presumably chairs did not exist 150,000 years ago. Thus, according to the principle of instantiation, the property of being a chair did not exist 150,000 years ago either. Similarly, if all red objects were to suddenly go out of existence, then the property of being red would likewise go out of existence.\n\nTo make the principle more plausible in the light of these examples, the existence of properties or universals is not tied to their actual existence now, but to their existence in space-time considered as a whole. Thus, any property which \"is\", \"has been\", or \"will be\" instantiated exists. The property of being red would exist even if all red things were to be destroyed, because it has been instantiated. This broadens the range of properties which exist if the principle is true. \n\nThose who endorse the principle of instantiation are known as \"in re\" (in thing or in reality) realists or 'immanent realists'.\n\n"}
{"id": "182727", "url": "https://en.wikipedia.org/wiki?curid=182727", "title": "Mach's principle", "text": "Mach's principle\n\nIn theoretical physics, particularly in discussions of , Mach's principle (or Mach's conjecture) is the name given by Einstein to an imprecise hypothesis often credited to the physicist and philosopher Ernst Mach. The idea is that the existence of absolute rotation (the distinction of local inertial frames vs. rotating reference frames) is determined by the large-scale distribution of matter, as exemplified by this anecdote:\n\nYou are standing in a field looking at the stars. Your arms are resting freely at your side, and you see that the distant stars are not moving. Now start spinning. The stars are whirling around you and your arms are pulled away from your body. Why should your arms be pulled away when the stars are whirling? Why should they be dangling freely when the stars don't move?\nMach's principle says that this is not a coincidence—that there is a physical law that relates the motion of the distant stars to the local inertial frame. If you see all the stars whirling around you, Mach suggests that there is some physical law which would make it so you would feel a centrifugal force. There are a number of rival formulations of the principle. It is often stated in vague ways, like \"mass out there influences inertia here\". A very general statement of Mach's principle is \"local physical laws are determined by the large-scale structure of the universe\".\n\nThis concept was a guiding factor in Einstein's development of the general theory of relativity. Einstein realized that the overall distribution of matter would determine the metric tensor, which tells you which frame is rotationally stationary. Frame-dragging and conservation of gravitational angular momentum makes this into a true statement in the general theory in certain solutions. But because the principle is so vague, many distinct statements can be (and have been) made that would qualify as a Mach principle, and some of these are false. The Gödel rotating universe is a solution of the field equations that is designed to disobey Mach's principle in the worst possible way. In this example, the distant stars seem to be revolving faster and faster as one moves further away. This example doesn't completely settle the question, because it has closed timelike curves.\n\nThe basic idea also appears before Mach's time, in the writings of George Berkeley. The book \"Absolute or Relative Motion?\" (1896) by Benedict Friedländer and his brother Immanuel contained ideas similar to Mach's principle.\n\nThere is a fundamental issue in relativity theory. If all motion is relative, how can we measure the inertia of a body? We must measure the inertia with respect to something else. But what if we imagine a particle completely on its own in the universe? We might hope to still have some notion of its state of motion. Mach's principle is sometimes interpreted as the statement that such a particle's state of motion has no meaning in that case.\n\nIn Mach's words, the principle is embodied as follows:\n\nAlbert Einstein seemed to view Mach's principle as something along the lines of:\n\nIn this sense, at least some of Mach's principles are related to philosophical holism. Mach's suggestion can be taken as the injunction that gravitation theories should be relational theories. Einstein brought the principle into mainstream physics while working on general relativity. Indeed, it was Einstein who first coined the phrase \"Mach's principle\". There is much debate as to whether Mach really intended to suggest a new physical law since he never states it explicitly.\n\nThe writing in which Einstein found inspiration from Mach was \"The Science of Mechanics\", where the philosopher criticized Newton's idea of absolute space, in particular the argument that Newton gave sustaining the existence of an advantaged reference system: what is commonly called \"Newton's bucket argument\".\n\nIn his \"Philosophiae Naturalis Principia Mathematica\", Newton tried to demonstrate that:\n\nMach, in his book, says that:\n\nThis same thought had been expressed by the philosopher George Berkeley in his \"De Motu\". It is then not clear, in the passages from Mach just mentioned, if the philosopher intended to formulate a new kind of physical action between heavy bodies. This physical mechanism should determine the inertia of bodies, in a way that the heavy and distant bodies of our universe should contribute the most to the inertial forces. More likely, Mach only suggested a mere \"redescription of motion in space as experiences that do not invoke the term \"space\"\". What is certain is that Einstein interpreted Mach's passage in the former way, originating a long-lasting debate.\n\nMost physicists believe Mach's principle was never developed into a quantitative physical theory that would explain a mechanism by which the stars can have such an effect. It was never made clear by Mach himself exactly what his principle was. Although Einstein was intrigued and inspired by Mach's principle, Einstein's formulation of the principle is not a fundamental assumption of general relativity.\n\nBecause intuitive notions of distance and time no longer apply, what exactly is meant by \"Mach's principle\" in general relativity is even less clear than in Newtonian physics and at least 21 formulations of Mach's principle are possible, some being considered more strongly Machian than others. A relatively weak formulation is the assertion that the motion of matter in one place should affect which frames are inertial in another.\n\nEinstein, before completing his development of the general theory of relativity, found an effect which he interpreted as being evidence of Mach's principle. We assume a fixed background for conceptual simplicity, construct a large spherical shell of mass, and set it spinning in that background. The reference frame in the interior of this shell will precess with respect to the fixed background. This effect is known as the Lense–Thirring effect. Einstein was so satisfied with this manifestation of Mach's principle that he wrote a letter to Mach expressing this:\nThe Lense–Thirring effect certainly satisfies the very basic and broad notion that \"matter there influences inertia here\". The plane of the pendulum would not be dragged around if the shell of matter were not present, or if it were not spinning. As for the statement that \"inertia originates in a kind of interaction between bodies\", this too could be interpreted as true in the context of the effect.\n\nMore fundamental to the problem, however, is the very existence of a fixed background, which Einstein describes as \"the fixed stars\". Modern relativists see the imprints of Mach's principle in the initial-value problem. Essentially, we humans seem to wish to separate spacetime into slices of constant time. When we do this, Einstein's equations can be decomposed into one set of equations, which must be satisfied on each slice, and another set, which describe how to move between slices. The equations for an individual slice are elliptic partial differential equations. In general, this means that only part of the geometry of the slice can be given by the scientist, while the geometry everywhere else will then be dictated by Einstein's equations on the slice.\n\nIn the context of an asymptotically flat spacetime, the boundary conditions are given at infinity. Heuristically, the boundary conditions for an asymptotically flat universe define a frame with respect to which inertia has meaning. By performing a Lorentz transformation on the distant universe, of course, this inertia can also be transformed.\n\nA stronger form of Mach's principle applies in Wheeler–Mach–Einstein spacetimes, which require spacetime to be spatially compact and globally hyperbolic. In such universes Mach's principle can be stated as \"the distribution of matter and field energy-momentum (and possibly other information) at a particular moment in the universe determines the inertial frame at each point in the universe\" (where \"a particular moment in the universe\" refers to a chosen Cauchy surface).\n\nThere have been other attempts to formulate a theory that is more fully Machian, such as the Brans–Dicke theory and the Hoyle–Narlikar theory of gravity, but most physicists argue that none have been fully successful. At an exit poll of experts, held in Tübingen in 1993, when asked the question \"Is general relativity perfectly Machian?\", 3 respondents replied \"yes\", and 22 replied \"no\". To the question \"Is general relativity with appropriate boundary conditions of closure of some kind very Machian?\" the result was 14 \"yes\" and 7 \"no\".\n\nHowever, Einstein was convinced that a valid theory of gravity would necessarily have to include the relativity of inertia:\nThe broad notion that \"mass there influences inertia here\" has been expressed in several forms.\nHermann Bondi and Joseph Samuel have listed eleven distinct statements that can be called Mach principles, labelled by \"Mach0\" through \"Mach10\". Though their list is not necessarily exhaustive, it does give a flavor for the variety possible.\n\n\n\n"}
{"id": "26127533", "url": "https://en.wikipedia.org/wiki?curid=26127533", "title": "Marginal abatement cost", "text": "Marginal abatement cost\n\nAbatement cost is the cost of reducing environmental negatives such as pollution. Marginal cost is an economic concept that measures the cost of an additional unit. The marginal abatement cost (MAC), in general, measures the cost of reducing one more unit of pollution.\n\nAlthough marginal abatement costs can be negative, such as when the low carbon option is cheaper than the business-as-usual option, MACs often rise steeply as more pollution is reduced.\n\nMarginal abatement costs are typically used on a marginal abatement cost curve (MACC) or MAC curve, which shows the marginal cost of additional reductions in pollution.\n\nCarbon traders use MAC curves to derive the supply function for modelling carbon price fundamentals. Power companies may employ MAC curves to guide their decisions about long-term capital investment strategies to select among a variety of efficiency and generation options. Economists have used MAC curves to explain the economics of interregional carbon trading. Policy-makers use MAC curves as merit order curves, to analyze how much abatement can be done in an economy at what cost, and where policy should be directed to achieve the emission reductions.\n\nHowever, MAC curves should not be used as abatement supply curves (or merit order curves) to decide which measures to implement in order to achieve a given emission-reduction target. Indeed, the options they list would take decades to implement, and it may be optimal to implement expensive but high-potential measures before introducing cheaper measures.\n\nThe way that MAC curves are usually built has been criticized for lack of transparency and the poor treatment it makes of uncertainty, inter-temporal dynamics, interactions between sectors and ancillary benefits. There is also concern regarding the biased ranking that occurs if some included options have negative costs. \n\nVarious economists, research organizations, and consultancies have produced MAC curves. Bloomberg New Energy Finance and McKinsey & Company have produced economy wide analyses on greenhouse gas emissions reductions for the United States. ICF International produced a California specific curve following AB-32 legislation as have Sweeney and Weyant.\n\nThe Wuppertal Institute for Climate, Environment and Energy produced several marginal abatement cost curves for Germany (also called Cost Potential Curves), depending on the perspective (end-user, utilities, society).\n\nThe US Environmental Protection Agency has done work on a MAC curve for non carbon dioxide emissions such as methane, NO, and HFCs. Enerdata and LEPII-CNRS (France) produce MAC curves with the Prospective Outlook on Long-term Energy Systems (POLES) model for the 6 Kyoto Protocol gases. These curves have been used for various public and private actors either to assess carbon policies or through the use of a carbon market analysis tool.\n\nThe World Bank 2013 low-carbon energy development plan for Nigeria, prepared jointly with the World Bank, ulitizes MAC curves created in Analytica.\n\n"}
{"id": "21582679", "url": "https://en.wikipedia.org/wiki?curid=21582679", "title": "Marginal product of labor", "text": "Marginal product of labor\n\nIn economics, the marginal product of labor (MP) is the change in output that results from employing an added unit of labor.\n\nThe marginal product of a factor of production is generally defined as the change in output associated with a change in that factor, holding other inputs into production constant.\n\nThe marginal product of labor is then the change in output (\"Y\") per unit change in labor (\"L\"). In discrete terms the marginal product of labor is:\n\nIn continuous terms, the \"MP\" is the first derivative of the production function:\n\nGraphically, the \"MP\" is the slope of the production function.\n\nThere is a factory which produces toys. When there are no workers in the factory, no toys are produced. When there is one worker in the factory, six toys are produced per hour. When there are two workers in the factory, eleven toys are produced per hour. There is a marginal product of labor of five when there are two workers in the factory compared to one. When the marginal product of labor is increasing, this is called increasing marginal returns. However, as the number of workers increases, the marginal product of labor may not increase indefinitely. When not scaled properly, the marginal product of labor may go down when the number of employees goes up, creating a situation known as diminishing marginal returns. When the marginal product of labor becomes negative, it is known as negative marginal returns.\n\nThe marginal product of labor is directly related to costs of production. Costs are divided between fixed and variable costs. Fixed costs are costs that relate to the fixed input, capital, or \"rK\", where \"r\" is the rental cost of capital and \"K\" is the quantity of capital. Variable costs (VC) are the costs of the variable input, labor, or \"wL\", where \"w\" is the wage rate and \"L\" is the amount of labor employed. Thus, VC = wL . Marginal cost (MC) is the change in total cost per unit change in output or ∆C/∆Q. In the short run, production can be varied only by changing the variable input. Thus only variable costs change as output increases: ∆C = ∆VC = ∆(wL). Marginal cost is ∆(Lw)/∆Q. Now, ∆L/∆Q is the reciprocal of the marginal product of labor (∆Q/∆L). Therefore, marginal cost is simply the wage rate w divided by the marginal product of labor\n\nThus if the marginal product of labor is rising then marginal costs will be falling and if the marginal product of labor is falling marginal costs will be rising (assuming a constant wage rate).\n\nThe average product of labor is the total product of labor divided by the number of units of labor employed, or \"Q/L\". The average product of labor is a common measure of labor productivity. The AP curve is shaped like an inverted “u”. At low production levels the AP tends to increase as additional labor is added. The primary reason for the increase is specialization and division of labor. At the point the AP reaches its maximum value AP equals the MP. Beyond this point the AP falls.\n\nDuring the early stages of production MP is greater than AP. When the MP is above the AP the AP will increase. Eventually the \"MP\" reaches it maximum value at the point of diminishing returns. Beyond this point MP will decrease. However, at the point of diminishing returns the MP is still above the AP and AP will continue to increase until MP equals AP. When MP is below AP, AP will decrease.\n\nGraphically, the \"AP\" curve can be derived from the total product curve by drawing secants from the origin that intersect (cut) the total product curve. The slope of the secant line equals the average product of labor, where the slope = dQ/dL. The slope of the curve at each intersection marks a point on the average product curve. The slope increases until the line reaches a point of tangency with the total product curve. This point marks the maximum average product of labor. It also marks the point where MP (which is the slope of the total product curve) equals the AP (the slope of the secant). Beyond this point the slope of the secants become progressively smaller as \"AP\" declines. The MP curve intersects the AP curve from above at the maximum point of the AP curve. Thereafter, the MP curve is below the AP curve.\n\nThe falling MP is due to the law of diminishing marginal returns. The law states, \"as units of one input are added (with all other inputs held constant) a point will be reached where the resulting additions to output will begin to decrease; that is marginal product will decline.\" The law of diminishing marginal returns applies regardless of whether the production function exhibits increasing, decreasing or constant returns to scale. The key factor is that the variable input is being changed while all other factors of production are being held constant. Under such circumstances diminishing marginal returns are inevitable at some level of production.\n\nDiminishing marginal returns differs from diminishing returns. Diminishing marginal returns means that the marginal product of the variable input is falling. Diminishing returns occur when the marginal product of the variable input is negative. That is when a unit increase in the variable input causes total product to fall. At the point that diminishing returns begin the MP is zero.\n\nThe general rule is that a firm maximizes profit by producing that quantity of output where marginal revenue equals marginal costs. The profit maximization issue can also be approached from the input side. That is, what is the profit maximizing usage of the variable input? To maximize profits the firm should increase usage \"up to the point where the input’s marginal revenue product equals its marginal costs\". So, mathematically the profit maximizing rule is MRP = MC. The marginal profit per unit of labor equals the marginal revenue product of labor minus the marginal cost of labor or Mπ = MRP − MCA firm maximizes profits where Mπ = 0.\n\nThe marginal revenue product is the change in total revenue per unit change in the variable input assume labor. That is, MRP = ∆TR/∆L. MRP is the product of marginal revenue and the marginal product of labor or MRP = MR × MP.\n\n\n • formula_4\n\n • Output price is $40 per unit.\n\n\nIn the aftermath of the marginal revolution in economics, a number of economists including John Bates Clark and Thomas Nixon Carver sought to derive an ethical theory of income distribution based on the idea that workers were morally entitled to receive a wage exactly equal to their marginal product. In the 20th century, marginal productivity ethics found few supporters among economists, being criticised not only by egalitarians but by economists associated with the Chicago school such as Frank Knight (in \"The Ethics of Competition\") and the Austrian School, such as Leland Yeager. However, marginal productivity ethics were defended by George Stigler.\n\n\n"}
{"id": "570963", "url": "https://en.wikipedia.org/wiki?curid=570963", "title": "Marginal propensity to save", "text": "Marginal propensity to save\n\nThe marginal propensity to save (MPS) is the fraction of an increase in income that is not spent on an increase in consumption. That is, the marginal propensity to save is the proportion of each additional dollar of household income that is used for saving. It is the slope of the line plotting saving against income. For example, if a household earns one extra dollar, and the marginal propensity to save is 0.35, then of that dollar, the household will spend 65 cents and save 35 cents. Likewise, it is the fractional decrease in saving that results from a decrease in income.\n\nThe MPS plays a central role in Keynesian economics as it quantifies the saving-income relation, which is the flip side of the consumption-income relation, and according to Keynes it reflects the fundamental psychological law. The marginal propensity to save is also a key variable in determining the value of the multiplier.\n\nMPS can be calculated as the change in savings divided by the change in income.\n\nOr mathematically, the marginal propensity to save (MPS) function is expressed as the derivative of the savings (S) function with respect to disposable income (Y).\n\nNow, MPS can be calculated as follows:\n\nMPS = (Change in savings) / (Change in income)\n\nThis implies that for each additional one unit of income, the savings increase by 0.4.\n\nThere are different implications of this above-mentioned formula.\n\n\nSince MPS is measured as ratio of change in savings to change in income, its value lies between 0 and 1.\nAlso, marginal propensity to save is opposite of marginal propensity to consume.\n\nMathematically, in a closed economy, MPS + MPC = 1, since an increase in one unit of income will be either consumed or saved.\n\nIn the above example, If MPS = 0.4, then MPC = 1 - 0.4 = 0.6.\n\nGenerally, it is assumed that value of marginal propensity to save for the richer is more than the marginal propensity to save for the poorer. If income increases for both parties by $1, then the propensity to save for a richer person would be more than that for the poorer person.\n\nMarginal propensity to save is also used as an alternative term for slope of saving line.\nThe slope of a saving line is given by the equation S = -a + (1-b)Y, where -a refers to autonomous savings and (1-b) refers to marginal propensity to save (here b refers to marginal propensity to consume but as MPC + MPS = 1, so (1-b) refers to MPS).\n\nIn this diagram, the savings function is an increasing function of disposable income i.e. savings increase as income increases.\n\nAn important implication of marginal propensity to save is measurement of the multiplier. A multiplier measures the magnified change in aggregate product i.e. the gross domestic product, resulting from a change in an autonomous variable (for example, government expenditure, investment expenditures, etc.).\n\nThe effect of a change in production creates a multiplied impact because it creates income which further creates consumption. However, the resulting consumption is also an expenditure which thus, generates more income, which creates more consumption. This next round of consumption leads to a further change in production, which generates even more income, and which induces even more consumption.\n\nAnd thus, as it goes on and on, it results in a magnified, multiplied change in aggregate production initially triggered by a change in autonomous variable, but amplified by the creation of more income and increase in consumption.\n\nMathematically, the above effect can be stated as:\n\nAnd it goes on and on.\nWe can express this as:\n\nThe end result is a magnified, multiplied change in aggregate production initially triggered by the change in investment, but amplified by the change in consumption i.e. the initial investment multiplied by the consumption coefficient (Marginal Propensity to consume).\n\nThe MPS enters into the process because it indicates the division of extra income between consumption and saving. It determines how much saving is induced with each change in production and income, and thus how much consumption is induced. If the MPS is smaller, then the multiplier process is also greater as less saving is induced, but more consumption is induced, with each round of activity.\n\nThus, in this highly simplified model, total magnified change in production due to change in an autonomous variable by $1\n\nThe effect of a multiplier effect can be measured as:\n\nIf the MPS is smaller, then the multiplier process is also greater as less saving is induced, and more consumption is induced with each round of activity.\n\nFor example, if MPS = 0.2, then multiplier effect is 5, and if MPS = 0.4, then the multiplier effect is 2.5. Thus, we can see that a lower propensity to save implies a higher multiplier effect.\n\n\n"}
{"id": "1280458", "url": "https://en.wikipedia.org/wiki?curid=1280458", "title": "Marginal revenue", "text": "Marginal revenue\n\nIn microeconomics, marginal revenue (R') is the additional revenue that will be generated by increasing product sales by one unit. It can also be described as the unit revenue the last item sold has generated for the firm. In a perfectly competitive market, the additional revenue generated by selling an additional unit of a good is equal to the price the firm is able to charge the buyer of the good. This is because a firm in a competitive market will always get the same price for every unit it sells regardless of the number of units the firm sells since the firm's sales can never impact the industry's price. However, a monopoly determines the entire industry's sales. As a result, it will have to lower the price of all units sold to increase sales by 1 unit. Therefore, the marginal revenue generated is always lower than the price the firm is able to charge for the unit sold, since each reduction in price causes unit revenue to decline on every good the firm sells. The marginal revenue (the increase in total revenue) is the price the firm gets on the additional unit sold, less the revenue lost by reducing the price on all other units that were sold prior to the decrease in price. \n\nA firms profits will be maximized when marginal revenue (MR) equals marginal cost (MC). If formula_1 then a firm should increase output for more profits, if formula_2 then a firm should decrease output for additional profits. A firm should choose the output level which is profit maximizing under perfect competition theory formula_3.\n\nMarginal revenue is equal to the ratio of the change in revenue for some change in quantity sold to that change in quantity sold. This can also be represented as a derivative when the change in quantity sold becomes arbitrarily small. More formally, define the revenue function to be the following\n\nBy the product rule, marginal revenue is then given by\n\nFor a firm facing perfect competition, price does not change with quantity sold (formula_6), so marginal revenue is equal to price. For a monopoly, the price decreases with quantity sold (formula_7), so marginal revenue is less than price (for positive formula_8).\n\nThe marginal revenue curve is affected by the same factors as the demand curve - changes in income, change in the prices of complements and substitutes, change in populations. These factors can cause the R curve to shift and rotate.\n\nThe relationship between marginal revenue and the elasticity of demand by the firm's customers can be derived as follows:\n\nwhere e is the price elasticity of demand. If demand is inelastic (e < 1) then R' will be negative, because to sell a marginal (infinitesimal) unit the firm would have to lower the selling price so much that it would lose more revenue on the pre-existing units than it would gain on the incremental unit. If demand is elastic (e > 1) R' will be positive, because the additional unit would not drive down the price by so much. If the firm is a perfect competitor, so that it is so small in the market that its quantity produced and sold has no effect on the price, then the price elasticity of demand is negative infinity, and marginal revenue simply equals the (market-determined) price.\n\nProfit maximization requires that a firm produces where marginal revenue equals marginal costs. Firm managers are unlikely to have complete information concerning their marginal revenue function or their marginal costs. Fortunately, the profit maximization conditions can be expressed in a “more easily applicable form” or rule of thumb.\n\nMarkup is the difference between price and marginal cost. The formula states that markup as a percentage of price equals the negative of the inverse of elasticity of demand. Alternatively, the relationship can be expressed as:\n\nThus if e is - 2 and mc is $5.00 then price is $10.00.\n\n(<R> - C')/ <R> = - 1/e is called the Lerner index after economist Abba Lerner. The Lerner index is a measure of market power - the ability of a firm to charge a price that exceeds marginal cost. The index varies from zero to 1. The greater the difference between price and marginal cost the closer the index value is to 1. The Lerner index increases as demand becomes less elastic.\n\nExample\nIf a company can sell 10 units at $20 each or 11 units at $19 each, then the marginal revenue from the eleventh unit is (11 × 19) - (10 × 20) = $9.\n\n\n"}
{"id": "27079770", "url": "https://en.wikipedia.org/wiki?curid=27079770", "title": "Mental model theory of reasoning", "text": "Mental model theory of reasoning\n\nThe mental model theory of reasoning was developed by Philip Johnson-Laird and Ruth M.J. Byrne (Johnson-Laird and Byrne, 1991). It has been applied to the main domains of deductive inference including relational inferences such as spatial and temporal deductions; propositional inferences, such as conditional, disjunctive and negation deductions; quantified inferences such as syllogisms; and meta-deductive inferences.\n\nOngoing research on mental models and reasoning has led the theory to be extended to account for probabilistic inference (e.g., Johnson-Laird, 2006) and counterfactual thinking (Byrne, 2005).\n\n"}
{"id": "795103", "url": "https://en.wikipedia.org/wiki?curid=795103", "title": "Multiple drafts model", "text": "Multiple drafts model\n\nDaniel Dennett's multiple drafts model of consciousness is a physicalist theory of consciousness based upon cognitivism, which views the mind in terms of information processing. The theory is described in depth in his book, \"Consciousness Explained\", published in 1991. As the title states, the book proposes a high-level explanation of consciousness which is consistent with support for the possibility of strong AI.\n\nDennett describes the theory as \"first-person operationalism\". As he states it:\nDennett's thesis is that our modern understanding of consciousness is unduly influenced by the ideas of René Descartes. To show why, he starts with a description of the phi illusion. In this experiment, two different coloured lights, with an angular separation of a few degrees at the eye, are flashed in succession. If the interval between the flashes is less than a second or so, the first light that is flashed appears to move across to the position of the second light. Furthermore, the light seems to change colour as it moves across the visual field. A green light will appear to turn red as it seems to move across to the position of a red light. Dennett asks how we could see the light change colour \"before\" the second light is observed.\n\nDennett claims that conventional explanations of the colour change boil down to either \"Orwellian\" or \"Stalinesque\" hypotheses, which he says are the result of Descartes' continued influence on our vision of the mind. In an Orwellian hypothesis, the subject comes to one conclusion, then goes back and changes that memory in light of subsequent events. This is akin to George Orwell's \"Nineteen Eighty-Four\", where records of the past are routinely altered. In a Stalinesque hypothesis, the two events would be reconciled prior to entering the subject's consciousness, with the final result presented as fully resolved. This is akin to Joseph Stalin's show trials, where the verdict has been decided in advance and the trial is just a rote presentation.\nDennett argues that there is no principled basis for picking one of these theories over the other, because they share a common error in supposing that there is a special time and place where unconscious processing becomes consciously experienced, entering into what Dennett calls the \"Cartesian theatre\". Both theories require us to cleanly divide a sequence of perceptions and reactions into before and after the instant that they reach the seat of consciousness, but he denies that there is any such moment, as it would lead to infinite regress. Instead, he asserts that there is no privileged place in the brain where consciousness happens. Dennett states that, \"[t]here does not exist ... a process such as 'recruitment of consciousness' (into what?), nor any place where the 'vehicle's arrival' is recognized (by whom?)\"\n\nWith no theatre, there is no screen, hence no reason to re-present data after it has already been analysed. Dennett says that, \"the Multiple Drafts model goes on to claim that the brain does not bother 'constructing' any representations that go to the trouble of 'filling in' the blanks. That would be a waste of time and (shall we say?) paint. The judgement is already in so we can get on with other tasks!\"\n\nAccording to the model, there are a variety of sensory inputs from a given event and also a variety of interpretations of these inputs. The sensory inputs arrive in the brain and are interpreted at different times, so a given event can give rise to a succession of discriminations, constituting the equivalent of multiple drafts of a story. As soon as each discrimination is accomplished, it becomes available for eliciting a behaviour; it does not have to wait to be presented at the theatre.\n\nLike a number of other theories, the Multiple Drafts model understands conscious experience as taking time to occur, such that percepts do not instantaneously arise in the mind in their full richness. The distinction is that Dennett's theory denies any clear and unambiguous boundary separating conscious experiences from all other processing. According to Dennett, consciousness is to be found in the actions and flows of information from place to place, rather than some singular view containing our experience. There is no central experiencer who confers a durable stamp of approval on any particular draft.\n\nDifferent parts of the neural processing assert more or less control at different times. For something to reach consciousness is akin to becoming famous, in that it must leave behind consequences by which it is remembered. To put it another way, consciousness is the property of having enough influence to affect what the mouth will say and the hands will do. Which inputs are \"edited\" into our drafts is not an exogenous act of supervision, but part of the self-organizing functioning of the network, and at the same level as the circuitry that conveys information bottom-up.\n\nThe conscious self is taken to exist as an abstraction visible at the level of the intentional stance, akin to a body of mass having a \"centre of gravity\". Analogously, Dennett refers to the self as the \"centre of narrative gravity\", a story we tell ourselves about our experiences. Consciousness exists, but not independently of behaviour and behavioural disposition, which can be studied through heterophenomenology.\n\nThe origin of this operationalist approach can be found in Dennett's immediately preceding work. Dennett (1988) explains consciousness in terms of \"access consciousness\" alone, denying the independent existence of what Ned Block has labeled \"phenomenal consciousness\". He argues that \"Everything real has properties, and since I don't deny the reality of conscious experience, I grant that conscious experience has properties\". Having related all consciousness to properties, he concludes that they cannot be meaningfully distinguished from our judgements about them. He writes: \nIn other words, once we've explained a perception fully in terms of how it affects us, there is nothing left to explain. In particular, there is no such thing as a perception which may be considered in and of itself (a quale). Instead, the subject's honest reports of how things seem to them are inherently authoritative on how things seem to them, but not on the matter of how things actually are.\nThe key to the multiple drafts model is that, after removing qualia, explaining consciousness boils down to explaining the behaviour we recognise as conscious. Consciousness is as consciousness does.\n\nSome of the criticism of Dennett's theory is due to the perceived tone of his presentation. As one grudging supporter admits, \"there is much in this book that is disputable. And Dennett is at times aggravatingly smug and confident about the merits of his arguments ... All in all Dennett's book is annoying, frustrating, insightful, provocative and above all annoying\" (Korb 1993).\n\nBogen (1992) points out that the brain is bilaterally symmetrical. That being the case, if Cartesian materialism is true, there might be \"two\" Cartesian theatres, so arguments against only one are flawed. Velmans (1992) argues that the phi effect and the cutaneous rabbit illusion demonstrate that there is a delay whilst modelling occurs and that this delay was discovered by Libet.\n\nIt has also been claimed that the argument in the multiple drafts model does not support its conclusion.\n\nMuch of the criticism asserts that Dennett's theory attacks the wrong target, failing to explain what it claims to. Chalmers (1996) maintains that Dennett has produced no more than a theory of how subjects report events. Some even parody the title of the book as \"Consciousness Explained Away\", accusing him of greedy reductionism. Another line of criticism disputes the accuracy of Dennett's characterisations of existing theories:\nMultiple drafts is also attacked for making a claim to novelty. It may be the case, however, that such attacks mistake which features Dennett is claiming as novel. Korb states that, \"I believe that the central thesis will be relatively uncontentious for most cognitive scientists, but that its use as a cleaning solvent for messy puzzles will be viewed less happily in most quarters.\" (Korb 1993) In this way, Dennett uses uncontroversial ideas towards more controversial ends, leaving him open to claims of unoriginality when uncontroversial parts are focused upon.\n\nEven the notion of consciousness as drafts is not unique to Dennett. According to Hankins, Dieter Teichert suggests that Paul Ricoeur's theories agree with Dennett's on the notion that \"the self is basically a narrative entity, and that any attempt to give it a free-floating independent status is misguided.\" [Hankins] Others see Derrida's (1982) representationalism as consistent with the notion of a mind that has perceptually changing content without a definitive present instant.\n\nTo those who believe that consciousness entails something more than behaving in all ways conscious, Dennett's view is seen as eliminativist, since it denies the existence of qualia and the possibility of philosophical zombies. However, Dennett is not denying the existence of the mind or of consciousness, only what he considers a naive view of them. The point of contention is whether Dennett's own definitions are indeed more accurate: whether what we think of when we speak of perceptions and consciousness can be understood in terms of nothing more than their effect on behaviour.\n\nThe role of information processing in consciousness has been criticised by John Searle who, in his Chinese room argument, states that he cannot find anything that could be recognised as conscious experience in a system that relies solely on motions of things from place to place. Dennett sees this argument as misleading, arguing that consciousness is not to be found in a specific part of the system, but in the actions of the whole. In essence, he denies that consciousness requires something in addition to capacity for behaviour, saying that philosophers such as Searle, \"just can't imagine how understanding could be a property that emerges from lots of distributed quasi-understanding in a large system\" (p. 439).\n\n\n\n\n"}
{"id": "47077309", "url": "https://en.wikipedia.org/wiki?curid=47077309", "title": "Negative-dimensional space", "text": "Negative-dimensional space\n\nIn topology, a discipline within mathematics, a negative-dimensional space is an extension of the usual notion of space, allowing for negative dimensions.\n\nSuppose that is a compact space of Hausdorff dimension , which is an element of a scale of compact spaces embedded in each other and parametrized by (). Such scales are considered \"equivalent\" with respect to if the compact spaces constituting them coincide for . It is said that the compact space is the \"hole\" in this equivalent set of scales, and is the negative dimension of the corresponding equivalence class.\n\nBy the 1940s, the science of topology had developed and studied a thorough basic theory of topological spaces of positive dimension. Motivated by computations, and to some extent aesthetics, topologists searched\nfor mathematical frameworks that extended our notion of space to allow for negative dimensions. Such dimensions, as well as the fourth and higher dimensions, are hard to imagine since we are not able to directly observe them. It wasn’t until the 1960s that a special topological framework was constructed—the category of spectra. A spectrum is a generalization of space that allows for negative dimensions. The concept of negative-dimensional spaces is applied, for example, to analyze linguistic statistics.\n\n\n"}
{"id": "407044", "url": "https://en.wikipedia.org/wiki?curid=407044", "title": "Negative (photography)", "text": "Negative (photography)\n\nIn photography, a negative is an image, usually on a strip or sheet of transparent plastic film, in which the lightest areas of the photographed subject appear darkest and the darkest areas appear lightest. This reversed order occurs because the extremely light-sensitive chemicals a camera film must use to capture an image quickly enough for ordinary picture-taking are darkened, rather than bleached, by exposure to light and subsequent photographic processing.\n\nIn the case of color negatives, the colors are also reversed into their respective complementary colors. Typical color negatives have an overall dull orange tint due to an automatic color-masking feature that ultimately results in improved color reproduction.\n\nNegatives are normally used to make positive prints on photographic paper by projecting the negative onto the paper with a photographic enlarger or making a contact print. The paper is also darkened in proportion to its exposure to light, so a second reversal results which restores light and dark to their normal order.\n\nNegatives were once commonly made on a thin sheet of glass rather than a plastic film, and some of the earliest negatives were made on paper.\n\nIt is incorrect to call an image a negative solely because it is on a transparent material. Transparent prints can be made by printing a negative onto special positive film, as is done to make traditional motion picture film prints for use in theaters. Some films used in cameras are designed to be developed by reversal processing, which produces the final positive, instead of a negative, on the original film. Positives on film or glass are known as transparencies or diapositives, and if mounted in small frames designed for use in a slide projector or magnifying viewer they are commonly called slides.\n\nA positive image is a normal image. A negative image is a total inversion, in which light areas appear dark and vice versa. A negative color image is additionally color-reversed, with red areas appearing cyan, greens appearing magenta, and blues appearing yellow, and vice versa.\n\nFilm negatives usually have less contrast, but a wider dynamic range, than the final printed positive images. The contrast typically increases when they are printed onto photographic paper. When negative film images are brought into the digital realm, their contrast may be adjusted at the time of scanning or, more usually, during subsequent post-processing.\n\nFilm for cameras that use the 35 mm still format is sold as a long strip of emulsion-coated and perforated plastic spooled in a light-tight cassette. Before each exposure, a mechanism inside the camera is used to pull an unexposed area of the strip out of the cassette and into position behind the camera lens. When all exposures have been made the strip is rewound into the cassette. After the film is chemically developed, the strip shows a series of small negative images. It is usually then cut into sections for easier handling. Medium format cameras use 120 film, which yields a strip of negatives 60 mm wide, and large format cameras capture each image on a single sheet of film which may be as large as 20 x 25 cm (8 x 10 inches) or even larger. Each of these photographed images may be referred to as a negative and an entire strip or set of images may be collectively referred to as \"the negatives\". They are the master images, from which all positive prints will derive, so they are handled and stored with special care.\n\nMany photographic processes create negative images: the chemicals involved react when exposed to light, so that during development they produce deposits of microscopic dark silver particles or colored dyes in proportion to the amount of exposure. However, when a negative image is created from a negative image (just like multiplying two negative numbers in mathematics) a positive image results. This makes most chemical-based photography a two-step process, which uses negative film and ordinary processing. Special films and development processes have been devised so that positive images can be created directly on the film; these are called positive, or slide, or (perhaps confusingly) reversal films and reversal processing.\n\nDespite the market's evolution away from film, there is still a desire and market for products which allow fine art photographers to produce negatives from digital images for their use in alternative processes such as cyanotypes, gum bichromate, platinum prints, and many others.\n\n"}
{"id": "19471895", "url": "https://en.wikipedia.org/wiki?curid=19471895", "title": "Negative affectivity", "text": "Negative affectivity\n\nNegative affectivity (NA), or negative affect, is a personality variable that involves the experience of negative emotions and poor self-concept. Negative affectivity subsumes a variety of negative emotions, including anger, contempt, disgust, guilt, fear, and nervousness. Low negative affectivity is characterized by frequent states of calmness and serenity, along with states of confidence, activeness, and great enthusiasm.\n\nIndividuals differ in negative emotional reactivity. Trait negative affectivity roughly corresponds to the dominant personality factor of anxiety/neuroticism that is found within the Big Five personality traits as emotional stability. The Big Five are characterized as openness, conscientiousness, extraversion, agreeableness, and neuroticism. Neuroticism can plague an individual with severe mood swings, frequent sadness, worry, and being easily disturbed, and predicts the development and onset of all \"common\" mental disorders. Research shows that negative affectivity relates to different classes of variables: Self-reported stress and (poor) coping skills, health complaints, and frequency of unpleasant events. Weight gain and mental health complaints are often experienced as well.\n\nPeople who express high negative affectivity view themselves and a variety of aspects of the world around them in generally negative terms. Negative affectivity is strongly related to life satisfaction. Individuals high in negative affect will exhibit, on average, higher levels of distress, anxiety, and dissatisfaction, and tend to focus on the unpleasant aspects of themselves, the world, the future, and other people, and also evoke more negative life events. The similarities between these affective traits and life satisfaction have led some researchers to view both positive and negative affect with life satisfaction as specific indicators of the broader construct of subjective well-being.\n\nNegative affect arousal mechanisms can induce negative affective states as evidenced by a study conducted by Stanley S. Seidner on negative arousal and white noise. The study quantified reactions from Mexican and Puerto Rican participants in response to the devaluation of speakers from other ethnic origins.\n\nThere are many instruments that can be used to measure negative affectivity, including measures of related concepts, such as neuroticism and trait anxiety. Two frequently used are:\n\nPANAS – The Positive and Negative Affect Schedule incorporates a 10-item negative affect scale. The PANAS-X is an expanded version of PANAS that incorporates negative affect subscales for Fear, Sadness, Guilt, Hostility, and Shyness.\n\nI-PANAS-SF – The International Positive and Negative Affect Schedule Short Form is an extensively validated brief, cross-culturally reliable 10-item version of the PANAS. Negative Affect items are Afraid, Ashamed, Hostile, Nervous and Upset. Internal consistency reliabilities between .72 and .76 are reported. The I-PANAS-SF was developed to eliminate redundant and ambiguous items and thereby derive an efficient measure for general use in research situations where either time or space are limited, or where international populations are of interest but where English may not be the mother tongue.\n\nRecent studies indicate that negative affect has important, beneficial impacts on cognition and behavior. These developments are a remarkable departure from past psychological research, which is characterized by a unilateral emphasis on the benefits of positive affect. Both states of affect influence mental processes and behavior. Negative affect is regularly recognized as a \"stable, heritable trait tendency to experience a broad range of negative feelings, such as worry, anxiety, self-criticisms, and a negative self-view\". This allows one to feel every type of emotion, which is regarded as a normal part of life and human nature. So, while the emotions themselves are viewed as negative, the individual experiencing them should not be classified as a negative person or depressed. They are going through a normal process and are feeling something that many individuals may not be able to feel or process due to differing problems.\n\nThese findings complement evolutionary psychology theories that affective states serve adaptive functions in promoting suitable cognitive strategies to deal with environmental challenges. Positive affect is associated with assimilative, top-down processing used in response to familiar, benign environments. Negative affect is connected with accommodative, bottom-up processing in response to unfamiliar, or problematic environments. Thus, positive affectivity promotes simplistic heuristic approaches that rely on preexisting knowledge and assumptions. Conversely, negative affectivity promotes controlled, analytic approaches that rely on externally drawn information.\n\nBenefits of negative affect are present in areas of cognition including perception, judgment, memory and interpersonal personal relations. Since negative affect relies more on cautious processing than preexisting knowledge, people with negative affect tend to perform better in instances involving deception, manipulation, impression formation, and stereotyping. Negative affectivity's analytical and detailed processing of information leads to fewer reconstructive-memory errors, whereas positive mood relies on broader schematic to thematic information that ignores detail. Thus, information processing in negative moods reduces the misinformation effect and increases overall accuracy of details. People also exhibit less interfering responses to stimuli when given descriptions or performing any cognitive task.\n\nPeople are notoriously susceptible to forming inaccurate judgments based on biases and limited information. Evolutionary theories propose that negative affective states tend to increase skepticism and decrease reliance on preexisting knowledge. Consequently, judgmental accuracy is improved in areas such as impression formation, reducing fundamental attribution error, stereotyping, and gullibility. While sadness is normally associated with the hippocampus, it does not produce the same side effects that would be associated with feelings of pleasure or excitement. Sadness correlates with feeling blue or the creation of tears, while excitement may cause a spike in blood pressure and one's pulse. As far as judgment goes, most people think about how they themselves feel about a certain situation. They will jump right to their current mood when asked a question. However, some mistake this process when using their current mood to justify a reaction to a stimulus. If you're sad, yet only a little bit, chances are your reactions and input will be negative as a whole.\n\nFirst impressions are one of the most basic forms of judgments people make on a daily basis; yet judgment formation is a complex and fallible process. Negative affect is shown to decrease errors in forming impressions based on presuppositions. One common judgment error is the halo effect, or the tendency to form unfounded impressions of people based on known but irrelevant information. For instance, more attractive people are often attributed with more positive qualities. Research demonstrates that positive affect tends to increase the halo effect, whereas negative affect decreases it.\n\nA study involving undergraduate students demonstrated a halo effect in identifying a middle-aged man as more likely to be a philosopher than an unconventional, young woman. These halo effects were nearly eliminated when participants were in a negative affective state. In the study, researchers sorted participants into either happy or sad groups using an autobiographical mood induction task in which participants reminisced on sad or happy memories. Then, participants read a philosophical essay by a fake academic who was identified as either a middle-aged, bespectacled man or as a young, unorthodox-looking woman. The fake writer was evaluated on intelligence and competence. The positive affect group exhibited a strong halo effect, rating the male writer significantly higher than the female writer in competence. The negative affect group exhibited almost no halo effects rating the two equally. Researchers concluded that impression formation is improved by negative affect. Their findings support theories that negative affect results in more elaborate processing based upon external, available information.\n\nThe systematic, attentive approach caused by negative affect reduces fundamental attribution error, the tendency to inaccurately attribute behavior to a person's internal character without taking external, situational factors into account. The fundamental attribution error (FAE) is connected with positive affect since it occurs when people use top-down cognitive processing based on inferences. Negative affect stimulates bottom-up, systematic analysis that reduces fundamental attribution error.\n\nThis effect is documented in FAE research in which students evaluated a fake debater on attitude and likability based on an essay the \"debater\" wrote. After being sorted into positive or negative affect groups, participants read one of two possible essays arguing for one side or another on a highly controversial topic. Participants were informed that the debater was assigned a stance to take in the essay that did not necessarily reflect his views. Still, the positive affect groups rated debaters who argued unpopular views as holding the same attitude expressed in the essay. They were also rated as unlikeable compared to debaters with popular stances, thus, demonstrating FAE. In contrast, the data for the negative affect group displayed no significant difference in ratings for debaters with popular stance and debaters with unpopular stances. These results indicate that positive affect assimilation styles promote fundamental attribution error, and negative affect accommodation styles minimize the error in respect to judging people.\n\nNegative affect benefits judgment in diminishing the implicit use of stereotypes by promoting closer attention to stimuli. In one study, participants were less likely to discriminate against targets that appeared Muslim when in a negative affective state. After organizing participants into positive and negative affect groups, researchers had them play a computer game. Participants had to make rapid decisions to shoot only at targets carrying a gun. Some of the targets wore turbans making them appear Muslim. As expected, there was a significant bias against Muslim targets resulting in a tendency to shoot at them. However, this tendency decreased with subjects in negative affective states. Positive affect groups developed more aggressive tendencies toward Muslims. Researchers concluded that negative affect leads to less reliance on internal stereotypes, thus decreasing judgmental bias.\n\nMultiple studies have shown that negative affectivity has a beneficial role in increasing skepticism and decreasing gullibility. Because negative affective states increase external analysis and attention to details, people in negative states are better able to detect deception.\n\nResearchers have presented findings in which students in negative affective states had improved lie detection compared to students in positive affective states. In a study, students watched video clips of everyday people either lying or telling the truth. First, music was used to induce positive, negative, or neutral affect in participants. Then, experimenters played 14 video messages that had to be identified by participants as true or false. As expected, the negative affect group performed better in veracity judgments than the positive affect group who performed no better than chance. Researchers believe that the negative affect groups detected deception more successfully because they attended to stimulus details and systematically built inferences from those details.\n\nMemory has been found to have many failures that effect the accuracy of recalled memories. This has been especially pragmatic in criminal settings as eyewitness memories have been found to be less reliable than one would hope. However, the externally focused and accommodative processing of negative affect has a positive effect on the overall improvement of memory. This evidenced by reduction of the misinformation effect and the number of false memories reported. The knowledge implies that negative affect can be used to enhance eyewitness memory; however, additional research suggests that the extent to which memory is improved by negative affect does not sufficiently improve eyewitness testimonies to significantly reduce its error.\n\nNegative affect has been shown to decrease susceptibility of incorporating misleading information, which is related to the misinformation effect. The misinformation effect refers to the finding that misleading information presented between the encoding of an event and its subsequent recall influences a witness's memory. This corresponds to two types of memory failure:\n\nNegative mood is shown to decrease suggestibility error. This is seen through reduced amounts of incorporation of false memories when misleading information is present. On the other hand, positive affect has shown to increase susceptibility to misleading information. An experiment with undergraduate students supported these results. Participants began the study in a lecture hall and witnessed what they thought was an unexpected five-minute belligerent encounter between an intruder and the lecturer. A week later, these participants watched a 10-minute-long video that generated either a positive, negative or neutral mood. They then completed a brief questionnaire about the previous incident between the intruder and lecturer that they witnessed the week earlier. In this questionnaire half of the participants received questions with misleading information and the other half received questions without any misleading information. This manipulation was used to determine if participants were susceptible to suggestibility failure. After 45 minutes of unrelated distractors participants were given a set of true or false questions which tested for false memories. Participants experiencing negative moods reported fewer numbers of false memories, whereas those experiencing positive moods reported a greater amount of false memories. This implies that positive affect promotes integration of misleading details and negative affect reduces the misinformation effect.\n\nPeople who experience negative affectivity following an event report fewer reconstructive false memories. This was evidenced by two studies conducted around public events. The first surrounded the events of the televised O.J. Simpson trial. Participants were asked to fill out questionnaires three times: one week, two months and a year after the televised verdict. These questionnaires measured participant emotion towards the verdict and the accuracy of their recalled memory of what occurred during the trial. Overall the study found that although participant response to the event outcome did not affect the quantity of remembered information, it did influence the likelihood of false memory. Participants who were pleased with the verdict of the O.J. Simpson trial were more likely to falsely believe something occurred during the trial than those who were displeased with the verdict. Another experiment found the same findings with Red Sox fans and Yankees fans in their overall memory of events that occurred in the final game of a 2004 playoff series in which the Red Sox defeated the Yankees. The study found that the Yankees fans had better memory of events that occurred than the Red Sox fans. The results from both of these experiments are consistent with the findings that negative emotion can lead to fewer memory errors and thus increased memory accuracy of events.\n\nAlthough negative affect has been shown to decrease the misinformation effect, the degree to which memory is improved is not enough to make a significant effect on witness testimony. In fact, emotions, including negative affect, are shown to reduce accuracy in identifying perpetrators from photographic lineups. Researchers demonstrated this effect in an experiment in which participants watched a video that induced either negative emotion or a neutral mood. The two videos were deliberately similar except for the action of interest, which was either a mugging (negative emotion) or a conversation (neutral emotion). After watching one of the two videos participants are shown perpetrator lineups, which either contained the target perpetrator from the video or a foil, a person that looked similar to the target. The results revealed that the participants who watched the emotion-induced video were more likely to incorrectly identify the innocent foil than to correctly identify the perpetrator. Neutral participants were more likely to correctly identify the perpetrator in comparison to their emotional counterparts. This demonstrates that emotional affect in forensic settings decreases accuracy of eyewitness memory. These findings are consistent with prior knowledge that stress and emotion greatly impair eyewitness ability to recognitive perpetrators.\n\nNegative affectivity can produce several interpersonal benefits. It can cause subjects to be more polite and considerate with others. Unlike positive mood, which causes less assertive approaches, negative affectivity can, in many ways, cause a person to be more polite and elaborate when making requests.\n\nNegative affectivity increases the accuracy of social perceptions and inferences. Specifically, high negative-affectivity people have more negative, but accurate, perceptions of the impression they make to others. People with low negative affectivity form overly-positive, potentially inaccurate impression of others that can lead to misplaced trust.\n\nA research conducted by Forgas J.P studied how affectivity can influence intergroup discrimination. He measured affectivity by how people allocate rewards to in-group and out-group members. In the procedure, participants had to describe their interpretations after looking at patterns of judgments about people. Afterwards, participants were exposed to a mood induction process, where they had to watch videotapes designed to elicit negative or positive affectivity. Results showed that participants with positive affectivity were more negative and discriminated more than participants with negative affectivity. Also, happy participants were more likely to discriminate between in-group and out-group members than sad participants. Negative affect is often associated with team selection. It is viewed as a trait that could make selecting individuals for a team irrelevant, thus preventing knowledge from becoming known or predicted for current issues that may arise.\n\nNegative affectivity subconsciously signals a challenging social environment.\nNegative mood may increase a tendency to conform to social norms.\n\nIn a study, college students where exposed to a mood induction process. After the mood induction process, participants were required to watch a show with positive and negative elements. After watching the show, they were asked to engage on a hypothetical conversation in which they \"describe the episode (they) just observed to a friend\". Their speech was recorded and transcribed during this task. Results showed that speakers in a negative mood had a better quality descriptions and greater amount of information and details. These results show that negative mood can improve people's communication skills.\n\nA negative mood is closely linked to better conversation because it makes use of the hippocampus and different regions of the brain. When someone is upset, that individual may see or hear things differently than an individual who is very upbeat and happy all the time. The small details the negative individual picks up may be something completely overlooked before. Anxiety disorders are often associated with over-thinking and ruminating on topics that would seem irrelevant and pointless to an individual without a disorder. OCD is one common anxiety trait that allows the affected individual a different insight on how things may appear to be. A person that makes use of his or her negative affect has a different view of the world and what goes on in it, thus making their conversations different and interesting to others.\n\nResults of one study show that participants with negative affectivity were more careful with the information they shared with others, being more cautious with who they could trust or not. Researchers found that negative mood not only decreases intimacy levels but also increases caution in placing trust in others.\n\n\n"}
{"id": "677516", "url": "https://en.wikipedia.org/wiki?curid=677516", "title": "Negative campaigning", "text": "Negative campaigning\n\nNegative campaigning or mudslinging is the process of deliberate spreading negative information about someone or something to worsen the public image of the described.\n\nDeliberate spreading of such information can be motivated either by honest desire of the campaigner to warn others against real dangers or deficiencies of the described, or by the campaigner's dishonest ideas on methods of winning in political, business or other spheres of competition against an honest rival.\n\nThe public image of an entity can be defined as reputation, esteem, respect, acceptance of the entity's appearance, values and behaviour by the general public of a given territory and/or a social group, possibly within time limits. As target groups of public and their values differ, so negativity or positivity of a public image is relative: e.g. while in most societies having an honest source of income is a positive value and stealing is discouraged, in the world of professional thieves honest work is frowned upon and stealing is encouraged. In polygamous societies monogamy is not viewed in the way it is valued in monogamous societies. Values of a society also change with time: e.g. homosexuality in Western culture was considered immoral and was criminally prosecuted until the sexual revolution of the second half of the 20 century.\nThus negative campaigning to be successful has to take into account current values of the group it addresses. The degree of strictness in practicing the group's values as opposed to its tolerance for violating the norms has also to be taken into consideration: e.g. while in the Old Testament and other traditional religious societies adultery and prostitution were outlawed and supposed to be punished by death, modern Western societies show much greater tolerance to these.\n\nIn United States politics, negative campaigning has been called \"as American as Mississippi mud\" and \"as American as apple pie\". Some research suggests negative campaigning is the norm in all political venues, mitigated only by the dynamics of a particular contest.\n\nThere are a number of techniques used in negative campaigning. Among the most effective is running advertisements attacking an opponent's personality, record, or opinion. There are two main types of ads used in negative campaigning: attack and contrast.\n\nAttack ads focus exclusively on the negative aspects of the opponent. There is no positive content in an attack ad, whether it is about the candidate or the opponent. Attack ads usually identify the risks associated with the opponent, often exploiting people’s fears to manipulate and lower the impression voters have of the opponent. Because attack ads have no positive content, they have the potential to be more influential than contrast ads in shaping voters’ views of the sponsoring candidate’s opponent.\n\nUnlike attack ads, contrast ads contain information about both the candidate and the opponent. The information about the candidate is positive, while the information about the opponent is negative. Contrast ads compare and contrast the candidate with the opponent, juxtaposing the positive information about the candidate with the negative information of the opponent. Because contrast ads must contain positive information, contrast ads are seen as less damaging to the political process than attack ads.\n\nOne of the most famous such ads was \"Daisy Girl\" by the campaign of Lyndon B. Johnson that successfully portrayed Republican Barry Goldwater as threatening nuclear war. Common negative campaign techniques include painting an opponent as soft on criminals, dishonest, corrupt, or a danger to the nation. One common negative campaigning tactic is attacking the other side for running a negative campaign.\n\nDirty tricks are also common in negative political campaigns. These generally involve secretly leaking damaging information to the media. This isolates a candidate from backlash and also does not cost any money. The material must be substantive enough to attract media interest, however, and if the truth is discovered it could severely damage a campaign. Other dirty tricks include trying to feed an opponent's team false information hoping they will use it and embarrass themselves.\n\nOften a campaign will use outside organizations, such as lobby groups, to launch attacks. These can be claimed to be coming from a neutral source and if the allegations turn out not to be true the attacking candidate will not be damaged if the links cannot be proven. Negative campaigning can be conducted by proxy. For instance, highly partisan ads were placed in the 2004 U.S. presidential election by allegedly independent bodies like MoveOn.org and Swift Boat Veterans for Truth.\n\nPush polls are attacks disguised as telephone polls. They might ask a question like \"How would you react if Candidate A was revealed to beat his wife?\", giving the impression that Candidate A might beat his wife. Members of the media and of the opposing party are deliberately not called making these tactics all but invisible and unprovable.\n\nG. Gordon Liddy played a major role in developing these tactics during the Nixon campaign playing an important advisory of rules that led to the campaign of 1972. James Carville, campaign manager of Bill Clinton's 1992 election, is also a major proponent of negative tactics. Lee Atwater, best known for being an advisor to presidents Ronald Reagan and George H.W. Bush, also pioneered many negative campaign techniques seen in political campaigns today.\n\nSponsors of overt negative campaigns often cite reasons to support mass communication of negative ideas. The Office of National Drug Control Policy uses negative campaigns to steer the public away from health risks. Similar negative campaigns have been used to rebut mass marketing by tobacco companies, or to discourage drunk driving. Those who conduct negative political campaigns sometimes say the public needs to know about the person he or she is voting for, even if it is bad. In other words, if a candidate’s opponent is a crook or a bad person, then he or she should be able to tell the public about it.\n\nMartin Wattenberg and Craig Brians, of the University of California, Irvine, considered in their study whether negative campaigning mobilizes or alienates voters. They concluded that data used by Stephen Ansolabehere in a 1994 American Political Science Review article to advance the hypothesis that negative campaigning demobilizes voters was flawed.\n\nA subsequent study done by Ansolabehere and Shanto Iyengar in 1995 corrected some of the previous study's flaws. This study concluded that negative advertising suppressed voter turnout, particularly for Independent voters. They speculated that campaigns tend to go negative only if the Independent vote is leaning toward the opponent. In doing so, they insure that the swing voters stay home, leaving the election up to base voters. They also found that negative ads have a greater impact on Democrats than on Republicans. According to them, base Republicans will vote no matter what (and will vote only for a Republican), but Democrats can be influenced to either stay home and not vote at all or to switch sides and vote for a Republican. This, combined with the effect negativity has on Independents, led them to conclude that Republicans benefit more from going negative than Democrats.\n\nOther researchers have found different, more positive outcomes from negative campaigns. Rick Farmer, PhD, an assistant professor of political science at the University of Akron found that negative ads are more memorable than positive ads when they reinforce a preexisting belief and are relevant to the central issues of a marketing campaign. Researchers at the University of Georgia found the impact of negative ads increases over time, while positive ads used to counteract negative ads lack the power of negative ads . Research also suggests negative campaigning introduces controversy and raises public awareness through additional news coverage .\n\nMost recently, Kyle Mattes and David P. Redlawsk in \"The Positive Case for Negative Campaigning\" show through surveys and experiments that negative campaigning provides informational benefits for voters. Without negativity, voters would not have full information about all of their choices, since no candidate will say anything bad about herself. They argue that candidates have to point out the flaws in their opponents for voters to be fully informed.\n\nSome strategists say that an effect of negative campaigning is that while it motivates the base of support it can alienate centrist and undecided voters from the political process, reducing voter turnout and radicalizing politics. \nIn a study done by Gina Garramone about how negative advertising affects the political process, it was found that a consequence of negative campaigning is greater image discrimination of the candidates and greater attitude polarization. While positive ads also contributed to the image discrimination and attitude polarization, Garramone found that negative campaigning played a more influential role in the discrimination and polarization than positive campaigning.\n\nNegative ads can produce a backlash. A disastrous ad was run by the Progressive Conservative Party of Canada in the 1993 Canadian federal election, apparently emphasizing Liberal Party of Canada leader Jean Chrétien's Bell's Palsy partial facial paralysis in a number of unflattering photos, with the subtext of criticizing his platforms. Chrétien took maximum advantage of the opportunity to gain the public's sympathy as a man who struggled with a physical disability and his party's subsequent overwhelming victory in the election helped reduce the governing Conservatives to two seats.\n\nA similar backlash happened to the Liberal Party in the 2006 federal election for running an attack ad that suggested that Conservative leader Stephen Harper would use Canadian soldiers to patrol Canadian cities, and impose some kind of martial law. The ad was only available from the Liberal Party's web site for a few hours prior to the release of the attack ads on television; nevertheless, it was picked up by the media and widely criticized for its absurdity, in particular the sentence \"we're not making this up; we're not allowed to make this stuff up\". Liberal MP Keith Martin expressed his disapproval of \"whoever the idiot who approved that ad was,\" shortly before Liberal leader Paul Martin (no relation) stated that he had personally approved them. The effect of the ads was to diminish the credibility of the party's other attack ads. It offended many Canadians, particularly those in the military, some of whom were fighting in Afghanistan at the time. (See Canadian federal election, 2006)\n\nMore recently, in the 2008 US Senate race in North Carolina, Republican incumbent Elizabeth Dole attempted an attack ad on Democratic challenger Kay Hagan, who had taken a small lead in polls, by tying her to atheists. Dole's campaign released an ad questioning Hagan's religion and it included a voice saying \"There is no God!\" over a picture of Kay Hagan's face. The voice was not Hagan's but it is believed the ad implied that it was. Initially, it was thought the ad would work as religion has historically been a very important issue to voters in the American south, but the ad produced a backlash across the state and Hagan responded forcefully with an ad saying that she was a Sunday school teacher and was a religious person. Hagan also claimed Dole was trying to change the subject from the economy (the ad appeared around the same time as the 2008 financial crisis). Hagan's lead in polls doubled and she won the race by a nine-point margin.\n\nBecause of the possible harm that can come from being seen as a negative campaigner, candidates often pledge to refrain from negative attacks. This pledge is usually abandoned when an opponent is perceived to be \"going negative,\" with the first retaliatory attack being, ironically, an accusation that the opponent is a negative campaigner.\n\nWhile some research has found advantages and other has found disadvantages, some studies find no difference between negative and positive approaches .\n\nResearch published in the Journal of Advertising found that negative political advertising makes the body want to turn away physically, but the mind remembers negative messages. The findings are based on research conducted by James Angelini, professor of communication at the University of Delaware, in collaboration with Samuel Bradley, assistant professor of advertising at Texas Tech University, and Sungkyoung Lee of Indiana University, which used ads that aired during the 2000 presidential election. During the study, the researchers placed electrodes under the eyes of willing participants and showed them a series of 30-second ads from both the George W. Bush and Al Gore campaigns. The electrodes picked up on the \"startle response,\" the automatic eye movement typically seen in response to snakes, spiders and other threats. Compared to positive or neutral messages, negative advertising prompted greater reflex reactions and a desire to move away.\n\n\n\n\n"}
{"id": "17235432", "url": "https://en.wikipedia.org/wiki?curid=17235432", "title": "Negative elongation factor", "text": "Negative elongation factor\n\nIn molecular biology, NELF (negative elongation factor) is a four-subunit protein (NELF-A, NELF-B, NELF-C/NELF-D, and NELF-E) that negatively impacts transcription by RNA polymerase II (Pol II) by pausing about 20-60 nucleotides downstream from the transcription start site (TSS).\n\nThe NELF-A subunit is encoded by the gene WHSC2 (Wolf-Hirschhorn syndrome candidate 2). Microsequencing analysis demonstrated that NELF-B was the protein previously identified as the protein encoded by the gene COBRA1, and was shown to interact with BRCA1. It is unknown whether or not NELF-C and NELF-D are peptides resulting from the same mRNA with different translation initiation sites, possibly differing only in an extra 9 amino acids for NELF-C at the N-terminus, or peptides from different mRNAs entirely. A single NELF complex consists of either NELF-C or NELF-D but not both. NELF-E is also known as RDBP.\n\nNELF binds in a stable complex with DSIF and RNA polymerase II together, but not with either alone. P-TEFb (positive transcription elongation factor b) inhibits the effect of NELF and DSIF on Pol II elongation, via its phosphorylation of serine-2 of the C-terminal domain of Pol II, and the SPT5 subunit of DSIF, causing dissociation of NELF. NELF homologues exist in some metazoans (e.g. insects and vertebrates) but have not been found in plants, yeast, or nematode (worms).\n"}
{"id": "1697560", "url": "https://en.wikipedia.org/wiki?curid=1697560", "title": "Negative equity", "text": "Negative equity\n\nNegative equity occurs when the value of an asset used to secure a loan is less than the outstanding balance on the loan. In the United States, assets (particularly real estate, whose loans are mortgages) with negative equity are often referred to as being \"underwater\", and loans and borrowers with negative equity are said to be \"upside down\". \n\nPeople and companies alike may have negative equity, as reflected on their balance sheets.\n\nIn the owner-occupied housing market, a fall in the market value of a mortgaged house or apartment/flat is the usual cause of negative equity. It may occur when the property owner obtains second-mortgage home-equity loans, causing the combined loans to exceed the home value, or simply because the original mortgage was too generous. If the borrower defaults, repossession and sale of the property by the lender will not raise enough cash to repay the amount outstanding, and the borrower will still be in debt as well as having lost the property. Some US states like California require lenders to choose between going after the borrower or taking repossession, but not both.\n\nThe term negative equity was widely used in the United Kingdom during the economic recession between 1991 and 1996, and in Hong Kong between 1998 and 2003. These recessions led to increased unemployment and a decline in property prices, which in turn led to an increase in repossessions by banks and building societies of properties worth less than the outstanding debt.\n\nIt is also common for negative equity to occur when the value of a property drops shortly after its purchase. This occurs frequently in automobile loans, where the market value of a car might drop by 20-30% as soon as the car is driven off the lot.\n\nWhile typically a result of fluctuating asset prices, negative equity can occur when the value of the asset stays fixed and the loan balance increases because loan payments are less than the interest, a situation known as negative amortization. The typical assets securing such loans are real property – commercial, office or residential. When the loan is nonrecourse, the lender can only look to the security, that is, the value of the property, when the borrower fails to repay the loan.\n\nSince 2007, those most exposed to negative equity are borrowers who obtained loans of a high percentage of the property value (such as 90% or even 100%). These were commonly available before the credit crunch. Such cases are of course the most at risk from falls in property value.\n\nA person who has negative equity can be said to have \"negative net worth\", where the person's liabilities exceed their assets. One might come to have negative equity as a result of taking out a substantial, unsecured loan. For example, one might use a student loan to pursue higher education. Although education increases the likelihood of higher future earnings, potential alone is not a financial asset. \n\nIn the United States, student loans are rarely dischargeable in bankruptcy, and typically lenders provide student loans without requiring security. This stands in contrast to lenders requiring borrowers to have an equity stake in a comparably-sized real estate loan, as described above, secured by both a down payment and a mortgage. An explanation for the willingness of creditors to provide unsecured student loans is that, in a practical sense, American student loans are secured by the borrower's future earnings. This is so since creditors may legally garnish wages when a borrower defaults.\n\nA home owner who is under water might be financially incapable of selling their current house and buying another one.\n\n\n"}
{"id": "5205483", "url": "https://en.wikipedia.org/wiki?curid=5205483", "title": "Negative free bid", "text": "Negative free bid\n\nNegative free bid is a contract bridge treatment whereby a free bid by responder over an opponent's overcall shows a long suit in a weak hand and is not forcing. This is in contrast with standard treatment, where a free bid can show unlimited values and is unconditionally forcing. The treatment is a relatively recent invention, and has become quite popular, especially in expert circles.\n\nNegative free bids resolve relatively frequent situations where the responder holds a long suit with which he would like to compete for a partscore, but is deprived from bidding it by opponent's overcall.\n\nFor example, if South holds: , partner opens 1 and East overcalls 1, he couldn't bid 2 in standard methods, as it would show 10+ high-card points, and a negative double would be too off-shape. With NFB treatment in effect though, he can bid 2 which the partner may pass (unless he has extra values and support, or an excellent suit of its own without tolerance for hearts).\n\nHowever, as a corollary, negative free bids affect the scope of negative double; if the hand is suitable for \"standard\" forcing free bid (10-11+ points), a negative double has to be made first and the suit bid only in the next round. Thus, the negative double can be made with the following types of hand:\nThis can sometimes allow the opponents to preempt effectively. \nFor example, West, holding: , after this auction is in an awkward situation — he doesn't know whether partner has spades or not; whether South was bidding to make or to sacrifice — is it correct to double, bid 4 or pass?\n\n"}
{"id": "548558", "url": "https://en.wikipedia.org/wiki?curid=548558", "title": "Negative pressure", "text": "Negative pressure\n\nNegative pressure may refer to:\n\n"}
{"id": "3921784", "url": "https://en.wikipedia.org/wiki?curid=3921784", "title": "Original camera negative", "text": "Original camera negative\n\nThe original camera negative (OCN) is the film in a traditional film-based movie camera which captures the original image. This is the film from which all other copies will be made. It is known as raw stock prior to exposure.\n\nThe size of a roll varies depending on the film gauge and whether or not a new roll, re-can, or short end was used. One hundred or 400 foot rolls are common in 16mm, while 400 or 1,000 foot (ft) rolls are used in 35mm work. While these are the most common sizes, other lengths such as 200, 800, or 1,200 ft may be commercially available from film stock manufacturers, usually by special order. Rolls of 100 and 200 ft are generally wound on spools for daylight-loading, while longer lengths are only wound around a plastic core. Core-wound stock has no exposure protection outside its packaging, and therefore must be loaded into a camera magazine within a darkroom or changing bag/tent in order to prevent the film being fogged.\n\nOriginal camera negative is of great value, as if lost or damaged it cannot be re-created without re-shooting the scene, something which is often impossible. It also contains the highest-quality version of the original image available, before any analog resolution and dynamic range loss from copying. For these reasons, original camera negative is handled with great care, and only by specialized trained people in dedicated film laboratories.\n\nAfter the film is processed by the film lab, camera rolls are assembled into lab rolls of 1,200 to 1,500 ft. Work prints may be made for viewing dailies or editing the picture on film.\n\nOnce film editing is finalized, a negative cutter will conform the negative using the Keykode on the edge of the film as a reference, cutting the original camera negative and incorporating any opticals (titles, dissolves, fades, and special effects), and cementing it together into several rolls. \n\nThe edited original negative is then copied to create a safety positive which can be used as a backup to create a usable negative. At this point, an answer print will be created from the original camera negative, and upon its approval, interpositives (IPs) and internegatives (INs) are created, from which the release prints are made. Generally speaking, the original camera negative is considered too important and delicate to be used for any processes more than necessary, as each pass through a lab process carries the risk of further degrading the quality of the negative by scratching the emulsion. Once an answer print is approved, the interpositives and internegatives are regarded as the earliest generation of the finished and graded film, and are almost always used for transfers to video or new film restorations. The original camera negatives is usually regarded as a last resort in the event that all of the intermediate elements have been compromised or lost.\n\nThe more popular a film is, the higher the likelihood that the original negative is in a worse shape, due to the need to return to the original camera negative to strike new interpositives to replace the exhausted ones, and thus create more internegatives and release prints. Before 1969, 35mm prints were struck directly from the original negative, often running into hundreds of copies, and causing further wear on the original.\n\nPhysical film stock is still occasionally used in film-making, particularly in prestige productions where the director and cinematographer have the power to require the extra cost, but as of 2016, it is becoming increasingly rare.\n\nIn modern cinematography, the camera is usually a digital camera, and no physical negative exists. However, the concept of \"camera original material\" is still used to describe camera image data. Camera original material that has not yet been ingested, duplicated, and archived is in a similar precarious state to original camera negative in a film process. One of the jobs of the digital imaging technician is to ensure that digital camera original material is backed up as soon as possible.\n"}
{"id": "6880483", "url": "https://en.wikipedia.org/wiki?curid=6880483", "title": "Philosophy of mind", "text": "Philosophy of mind\n\nPhilosophy of mind is a branch of philosophy that studies the nature of the mind. The mind–body problem is a paradigm issue in philosophy of mind, although other issues are addressed, such as the hard problem of consciousness, and the nature of particular mental states. Aspects of the mind that are studied include mental events, mental functions, mental properties, consciousness, the ontology of the mind, the nature of thought, and the relationship of the mind to the body.\n\nDualism and monism are the two central schools of thought on the mind–body problem, although nuanced views have arisen that do not fit one or the other category neatly. Dualism is found in both Eastern and Western traditions (in the Sankhya and Yoga schools of Hindu philosophy as well as Plato) but its entry into Western philosophy was thanks to René Descartes in the 17th century. Substance dualists like Descartes argue that the mind is an independently existing substance, whereas property dualists maintain that the mind is a group of independent properties that emerge from and cannot be reduced to the brain, but that it is not a distinct substance.\n\nMonism is the position that mind and body are not ontologically distinct entities (independent substances). This view was first advocated in Western philosophy by Parmenides in the 5th century BCE and was later espoused by the 17th century rationalist Baruch Spinoza. Physicalists argue that only entities postulated by physical theory exist, and that mental processes will eventually be explained in terms of these entities as physical theory continues to evolve. Physicalists maintain various positions on the prospects of reducing mental properties to physical properties (many of whom adopt compatible forms of property dualism), and the ontological status of such mental properties remains unclear. Idealists maintain that the mind is all that exists and that the external world is either mental itself, or an illusion created by the mind. Neutral monists such as Ernst Mach and William James argue that events in the world can be thought of as either mental (psychological) or physical depending on the network of relationships into which they enter, and dual-aspect monists such as Spinoza adhere to the position that there is some other, neutral substance, and that both matter and mind are properties of this unknown substance. The most common monisms in the 20th and 21st centuries have all been variations of physicalism; these positions include behaviorism, the type identity theory, anomalous monism and functionalism.\n\nMost modern philosophers of mind adopt either a reductive or non-reductive physicalist position, maintaining in their different ways that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, especially in the fields of sociobiology, computer science (specifically, artificial intelligence), evolutionary psychology and the various neurosciences. Reductive physicalists assert that all mental states and properties will eventually be explained by scientific accounts of physiological processes and states. Non-reductive physicalists argue that although the mind is not a separate substance, mental properties supervene on physical properties, or that the predicates and vocabulary used in mental descriptions and explanations are indispensable, and cannot be reduced to the language and lower-level explanations of physical science. Continued neuroscientific progress has helped to clarify some of these issues; however, they are far from being resolved. Modern philosophers of mind continue to ask how the subjective qualities and the intentionality of mental states and properties can be explained in naturalistic terms.\n\nThe mind–body problem concerns the explanation of the relationship that exists between minds, or mental processes, and bodily states or processes. The main aim of philosophers working in this area is to determine the nature of the mind and mental states/processes, and how—or even if—minds are affected by and can affect the body.\n\nOur perceptual experiences depend on stimuli that arrive at our various sensory organs from the external world, and these stimuli cause changes in our mental states, ultimately causing us to feel a sensation, which may be pleasant or unpleasant. Someone's desire for a slice of pizza, for example, will tend to cause that person to move his or her body in a specific manner and in a specific direction to obtain what he or she wants. The question, then, is how it can be possible for conscious experiences to arise out of a lump of gray matter endowed with nothing but electrochemical properties.\n\nA related problem is how someone's propositional attitudes (e.g. beliefs and desires) cause that individual's neurons to fire and muscles to contract. These comprise some of the puzzles that have confronted epistemologists and philosophers of mind from at least the time of René Descartes.\n\nDualism is a set of views about the relationship between mind and matter (or body). It begins with the claim that mental phenomena are, in some respects, non-physical. One of the earliest known formulations of mind–body dualism was expressed in the eastern Sankhya and Yoga schools of Hindu philosophy (c. 650 BCE), which divided the world into purusha (mind/spirit) and prakriti (material substance). Specifically, the Yoga Sutra of Patanjali presents an analytical approach to the nature of the mind.\n\nIn Western Philosophy, the earliest discussions of dualist ideas are in the writings of Plato who maintained that humans' \"intelligence\" (a faculty of the mind or soul) could not be identified with, or explained in terms of, their physical body. However, the best-known version of dualism is due to René Descartes (1641), and holds that the mind is a non-extended, non-physical substance, a \"res cogitans\". Descartes was the first to clearly identify the mind with consciousness and self-awareness, and to distinguish this from the brain, which was the seat of intelligence. He was therefore the first to formulate the mind–body problem in the form in which it still exists today.\n\nThe most frequently used argument in favor of dualism appeals to the common-sense intuition that conscious experience is distinct from inanimate matter. If asked what the mind is, the average person would usually respond by identifying it with their self, their personality, their soul, or some other such entity. They would almost certainly deny that the mind simply is the brain, or vice versa, finding the idea that there is just one ontological entity at play to be too mechanistic, or simply unintelligible. Many modern philosophers of mind think that these intuitions are misleading and that we should use our critical faculties, along with empirical evidence from the sciences, to examine these assumptions to determine whether there is any real basis to them.\n\nAnother important argument in favor of dualism is that the mental and the physical seem to have quite different, and perhaps irreconcilable, properties. Mental events have a subjective quality, whereas physical events do not. So, for example, one can reasonably ask what a burnt finger feels like, or what a blue sky looks like, or what nice music sounds like to a person. But it is meaningless, or at least odd, to ask what a surge in the uptake of glutamate in the dorsolateral portion of the prefrontal cortex feels like.\n\nPhilosophers of mind call the subjective aspects of mental events \"qualia\" or \"raw feels\". There is something that it is like to feel pain, to see a familiar shade of blue, and so on. There are qualia involved in these mental events that seem particularly difficult to reduce to anything physical. David Chalmers explains this argument by stating that we could conceivably know all the objective information about something, such as the brain states and wavelengths of light involved with seeing the color red, but still not know something fundamental about the situation – what it is like to see the color red.\n\nIf consciousness (the mind) can exist independently of physical reality (the brain), one must explain how physical memories are created concerning consciousness. Dualism must therefore explain how consciousness affects physical reality. One possible explanation is that of a miracle, proposed by Arnold Geulincx and Nicolas Malebranche, where all mind–body interactions require the direct intervention of God.\n\nAnother possible argument that has been proposed by C. S. Lewis is the Argument from Reason: if, as monism implies, all of our thoughts are the effects of physical causes, then we have no reason for assuming that they are also the consequent of a reasonable ground. Knowledge, however, is apprehended by reasoning from ground to consequent. Therefore, if monism is correct, there would be no way of knowing this—or anything else—we could not even suppose it, except by a fluke.\n\nThe zombie argument is based on a thought experiment proposed by Todd Moody, and developed by David Chalmers in his book \"The Conscious Mind\". The basic idea is that one can imagine one's body, and therefore conceive the existence of one's body, without any conscious states being associated with this body. Chalmers' argument is that it seems possible that such a being could exist because all that is needed is that all and only the things that the physical sciences describe about a zombie must be true of it. Since none of the concepts involved in these sciences make reference to consciousness or other mental phenomena, and any physical entity can be by definition described scientifically via physics, the move from conceivability to possibility is not such a large one. Others such as Dennett have argued that the notion of a philosophical zombie is an incoherent, or unlikely, concept. It has been argued under physicalism that one must either believe that anyone including oneself might be a zombie, or that no one can be a zombie—following from the assertion that one's own conviction about being (or not being) a zombie is a product of the physical world and is therefore no different from anyone else's. This argument has been expressed by Dennett who argues that \"Zombies think they are conscious, think they have qualia, think they suffer pains—they are just 'wrong' (according to this lamentable tradition) in ways that neither they nor we could ever discover!\"\nSee also the problem of other minds.\n\nInteractionist dualism, or simply interactionism, is the particular form of dualism first espoused by Descartes in the \"Meditations\". In the 20th century, its major defenders have been Karl Popper and John Carew Eccles. It is the view that mental states, such as beliefs and desires, causally interact with physical states.\n\nDescartes' famous argument for this position can be summarized as follows: Seth has a clear and distinct idea of his mind as a thinking thing that has no spatial extension (i.e., it cannot be measured in terms of length, weight, height, and so on). He also has a clear and distinct idea of his body as something that is spatially extended, subject to quantification and not able to think. It follows that mind and body are not identical because they have radically different properties.\n\nAt the same time, however, it is clear that Seth's mental states (desires, beliefs, etc.) have causal effects on his body and vice versa: A child touches a hot stove (physical event) which causes pain (mental event) and makes her yell (physical event), this in turn provokes a sense of fear and protectiveness in the caregiver (mental event), and so on.\n\nDescartes' argument crucially depends on the premise that what Seth believes to be \"clear and distinct\" ideas in his mind are necessarily true. Many contemporary philosophers doubt this. For example, Joseph Agassi suggests that several scientific discoveries made since the early 20th century have undermined the idea of privileged access to one's own ideas. Freud claimed that a psychologically-trained observer can understand a person's unconscious motivations better than the person himself does. Duhem has shown that a philosopher of science can know a person's methods of discovery better than that person herself does, while Malinowski has shown that an anthropologist can know a person's customs and habits better than the person whose customs and habits they are. He also asserts that modern psychological experiments that cause people to see things that are not there provide grounds for rejecting Descartes' argument, because scientists can describe a person's perceptions better than the person herself can.\n\nPsychophysical parallelism, or simply parallelism, is the view that mind and body, while having distinct ontological statuses, do not causally influence one another. Instead, they run along parallel paths (mind events causally interact with mind events and brain events causally interact with brain events) and only seem to influence each other. This view was most prominently defended by Gottfried Leibniz. Although Leibniz was an ontological monist who believed that only one type of substance, the monad, exists in the universe, and that everything is reducible to it, he nonetheless maintained that there was an important distinction between \"the mental\" and \"the physical\" in terms of causation. He held that God had arranged things in advance so that minds and bodies would be in harmony with each other. This is known as the doctrine of pre-established harmony.\n\nOccasionalism is the view espoused by Nicholas Malebranche as well as Islamic philosophers such as Abu Hamid Muhammad ibn Muhammad al-Ghazali that asserts that all supposedly causal relations between physical events, or between physical and mental events, are not really causal at all. While body and mind are different substances, causes (whether mental or physical) are related to their effects by an act of God's intervention on each specific occasion.\n\nProperty dualism is the view that the world is constituted of just one kind of substance – the physical kind – and there exist two distinct kinds of properties: physical properties and mental properties. In other words, it is the view that non-physical, mental properties (such as beliefs, desires and emotions) inhere in some physical bodies (at least, brains). How mental and physical properties relate causally depends on the variety of property dualism in question, and is not always a clear issue. Sub-varieties of property dualism include:\n\n\nDual aspect theory or dual-aspect monism is the view that the mental and the physical are two aspects of, or perspectives on, the same substance. (Thus it is a mixed position, which is monistic in some respects). In modern philosophical writings, the theory's relationship to neutral monism has become somewhat ill-defined, but one proffered distinction says that whereas neutral monism allows the context of a given group of neutral elements and the relationships into which they enter to determine whether the group can be thought of as mental, physical, both, or neither, dual-aspect theory suggests that the mental and the physical are manifestations (or aspects) of some underlying substance, entity or process that is itself neither mental nor physical as normally understood. Various formulations of dual-aspect monism also require the mental and the physical to be complementary, mutually irreducible and perhaps inseparable (though distinct).\n\nThis is a philosophy of mind that regards the degrees of freedom between mental and physical well-being as not necessarily synonymous thus implying an experiential dualism between body and mind. An example of these disparate degrees of freedom is given by Allan Wallace who notes that it is \"experientially apparent that one may be physically uncomfortable—for instance, while engaging in a strenuous physical workout—while mentally cheerful; conversely, one may be mentally distraught while experiencing physical comfort\". Experiential dualism notes that our subjective experience of merely seeing something in the physical world seems qualitatively different than mental processes like grief that comes from losing a loved one. This philosophy also is a proponent of causal dualism which is defined as the dual ability for mental states and physical states to affect one another. Mental states can cause changes in physical states and vice versa.\n\nHowever, unlike cartesian dualism or some other systems, experiential dualism does not posit two fundamental substances in reality: mind and matter. Rather, experiential dualism is to be understood as a conceptual framework that gives credence to the qualitative difference between the experience of mental and physical states. Experiential dualism is accepted as the conceptual framework of Madhyamaka Buddhism.\n\nMadhayamaka Buddhism goes even further, finding fault with the monist view of physicalist philosophies of mind as well in that these generally posit matter and energy as the fundamental substance of reality. Nonetheless, this does not imply that the cartesian dualist view is correct, rather Madhyamaka regards as error any affirming view of a fundamental substance to reality.In denying the independent self-existence of all the phenomena that make up the world of our experience, the Madhyamaka view departs from both the substance dualism of Descartes and the substance monism—namely, physicalism—that is characteristic of modern science. The physicalism propounded by many contemporary scientists seems to assert that the real world is composed of physical things-in-themselves, while all mental phenomena are regarded as mere appearances, devoid of any reality in and of themselves. Much is made of this difference between appearances and reality.\nIndeed, physicalism, or the idea that matter is the only fundamental substance of reality, is explicitly rejected by Buddhism.In the Madhyamaka view, mental events are no more or less real than physical events. In terms of our common-sense experience, differences of kind do exist between physical and mental phenomena. While the former commonly have mass, location, velocity, shape, size, and numerous other physical attributes, these are not generally characteristic of mental phenomena. For example, we do not commonly conceive of the feeling of affection for another person as having mass or location. These physical attributes are no more appropriate to other mental events such as sadness, a recalled image from one's childhood, the visual perception of a rose, or consciousness of any sort. Mental phenomena are, therefore, not regarded as being physical, for the simple reason that they lack many of the attributes that are uniquely characteristic of physical phenomena. Thus, Buddhism has never adopted the physicalist principle that regards only physical things as real.\n\nHylomorphism is a theory that originates with Aristotelian philosophy, which conceives being as a compound of matter and form. \"Hylomorphism\" is a 19th-century term formed from the Greek words ὕλη \"hyle\", \"wood, matter\", and μορφή, \"morphē\", \"form\".\n\nIn contrast to dualism, monism does not accept any fundamental divisions. The fundamentally disparate nature of reality has been central to forms of eastern philosophies for over two millennia. In Indian and Chinese philosophy, monism is integral to how experience is understood. Today, the most common forms of monism in Western philosophy are physicalist. Physicalistic monism asserts that the only existing substance is physical, in some sense of that term to be clarified by our best science. However, a variety of formulations (see below) are possible. Another form of monism, idealism, states that the only existing substance is mental. Although pure idealism, such as that of George Berkeley, is uncommon in contemporary Western philosophy, a more sophisticated variant called panpsychism, according to which mental experience and properties may be at the foundation of physical experience and properties, has been espoused by some philosophers such as Alfred North Whitehead and David Ray Griffin.\n\nPhenomenalism is the theory that representations (or sense data) of external objects are all that exist. Such a view was briefly adopted by Bertrand Russell and many of the logical positivists during the early 20th century. A third possibility is to accept the existence of a basic substance that is neither physical nor mental. The mental and physical would then both be properties of this neutral substance. Such a position was adopted by Baruch Spinoza and was popularized by Ernst Mach in the 19th century. This neutral monism, as it is called, resembles property dualism.\n\nBehaviorism dominated philosophy of mind for much of the 20th century, especially the first half. In psychology, behaviorism developed as a reaction to the inadequacies of introspectionism. Introspective reports on one's own interior mental life are not subject to careful examination for accuracy and cannot be used to form predictive generalizations. Without generalizability and the possibility of third-person examination, the behaviorists argued, psychology cannot be scientific. The way out, therefore, was to eliminate the idea of an interior mental life (and hence an ontologically independent mind) altogether and focus instead on the description of observable behavior.\n\nParallel to these developments in psychology, a philosophical behaviorism (sometimes called logical behaviorism) was developed. This is characterized by a strong verificationism, which generally considers unverifiable statements about interior mental life pointless. For the behaviorist, mental states are not interior states on which one can make introspective reports. They are just descriptions of behavior or dispositions to behave in certain ways, made by third parties to explain and predict another's behavior.\n\nPhilosophical behaviorism has fallen out of favor since the latter half of the 20th century, coinciding with the rise of cognitivism. Cognitivists reject behaviorism due to several perceived problems. For example, behaviorism could be said to be counterintuitive when it maintains that someone is talking about behavior in the event that a person is experiencing a painful headache.\n\nType physicalism (or type-identity theory) was developed by John Smart and Ullin Place as a direct reaction to the failure of behaviorism. These philosophers reasoned that, if mental states are something material, but not behavioral, then mental states are probably identical to internal states of the brain. In very simplified terms: a mental state \"M\" is nothing other than brain state \"B\". The mental state \"desire for a cup of coffee\" would thus be nothing more than the \"firing of certain neurons in certain brain regions\".\nDespite its initial plausibility, the identity theory faces a strong challenge in the form of the thesis of multiple realizability, first formulated by Hilary Putnam. For example, not only humans, but many different species of animals can experience pain. However, it seems highly unlikely that all of these diverse organisms with the same pain experience are in the identical brain state. And if this is the case, then pain cannot be identical to a specific brain state. The identity theory is thus empirically unfounded.\n\nOn the other hand, even granted the above, it does not follow that identity theories of all types must be abandoned. According to token identity theories, the fact that a certain brain state is connected with only one mental state of a person does not have to mean that there is an absolute correlation between types of mental state and types of brain state. The type–token distinction can be illustrated by a simple example: the word \"green\" contains four types of letters (g, r, e, n) with two tokens (occurrences) of the letter \"e\" along with one each of the others.\nThe idea of token identity is that only particular occurrences of mental events are identical with particular occurrences or tokenings of physical events. Anomalous monism (see below) and most other non-reductive physicalisms are token-identity theories. Despite these problems, there is a renewed interest in the type identity theory today, primarily due to the influence of Jaegwon Kim.\n\nFunctionalism was formulated by Hilary Putnam and Jerry Fodor as a reaction to the inadequacies of the identity theory. Putnam and Fodor saw mental states in terms of an empirical computational theory of the mind. At about the same time or slightly after, D.M. Armstrong and David Kellogg Lewis formulated a version of functionalism that analyzed the mental concepts of folk psychology in terms of functional roles. Finally, Wittgenstein's idea of meaning as use led to a version of functionalism as a theory of meaning, further developed by Wilfrid Sellars and Gilbert Harman. Another one, psychofunctionalism, is an approach adopted by the naturalistic philosophy of mind associated with Jerry Fodor and Zenon Pylyshyn.\n\nWhat all these different varieties of functionalism share in common is the thesis that mental states are characterized by their causal relations with other mental states and with sensory inputs and behavioral outputs. That is, functionalism abstracts away from the details of the physical implementation of a mental state by characterizing it in terms of non-mental functional properties. For example, a kidney is characterized scientifically by its functional role in filtering blood and maintaining certain chemical balances. From this point of view, it does not really matter whether the kidney be made up of organic tissue, plastic nanotubes or silicon chips: it is the role that it plays and its relations to other organs that define it as a kidney.\n\nNon-reductionist philosophers hold firmly to two essential convictions with regard to mind–body relations: 1) Physicalism is true and mental states must be physical states, but 2) All reductionist proposals are unsatisfactory: mental states cannot be reduced to behavior, brain states or functional states. Hence, the question arises whether there can still be a non-reductive physicalism. Donald Davidson's anomalous monism is an attempt to formulate such a physicalism. He \"thinks that when one runs across what are traditionally seen as absurdities of Reason, such as akrasia or self-deception, the personal psychology framework is not to be given up in favor of the subpersonal one, but rather must be enlarged or extended so that the rationality set out by the principle of charity can be found elsewhere.\"\n\nDavidson uses the thesis of supervenience: mental states supervene on physical states, but are not reducible to them. \"Supervenience\" therefore describes a functional dependence: there can be no change in the mental without some change in the physical–causal reducibility between the mental and physical without ontological reducibility.\n\nBecause non-reductive physicalist theories attempt to both retain the ontological distinction between mind and body and try to solve the \"surfeit of explanations puzzle\" in some way; critics often see this as a paradox and point out the similarities to epiphenomenalism, in that it is the brain that is seen as the root \"cause\" not the mind, and the mind seems to be rendered inert.\n\nEpiphenomenalism regards one or more mental states as the byproduct of physical brain states, having no influence on physical states. The interaction is one-way (solving the \"surfeit of explanations puzzle\") but leaving us with non-reducible mental states (as a byproduct of brain states) – causally reducible, but ontologically irreducible to physical states. Pain would be seen by epiphenomenaliasts as being caused by the brain state but as not having effects on other brain states, though it might have effects on other mental states (i.e. cause distress).\n\nWeak emergentism is a form of \"non-reductive physicalism\" that involves a layered view of nature, with the layers arranged in terms of increasing complexity and each corresponding to its own special science. Some philosophers hold that emergent properties causally interact with more fundamental levels, while others maintain that higher-order properties simply supervene over lower levels without direct causal interaction. The latter group therefore holds a less strict, or \"weaker\", definition of emergentism, which can be rigorously stated as follows: a property P of composite object O is emergent if it is metaphysically impossible for another object to lack property P if that object is composed of parts with intrinsic properties identical to those in O and has those parts in an identical configuration.\n\nSometimes emergentists use the example of water having a new property when Hydrogen H and Oxygen O combine to form HO (water). In this example there \"emerges\" a new property of a transparent liquid that would not have been predicted by understanding hydrogen and oxygen as gases. This is analogous to physical properties of the brain giving rise to a mental state. Emergentists try to solve the notorious mind–body gap this way. One problem for emergentism is the idea of \"causal closure\" in the world that does not allow for a mind-to-body causation.\n\nIf one is a materialist and believes that all aspects of our common-sense psychology will find reduction to a mature cognitive neuroscience, and that non-reductive materialism is mistaken, then one can adopt a final, more radical position: eliminative materialism.\n\nThere are several varieties of eliminative materialism, but all maintain that our common-sense \"folk psychology\" badly misrepresents the nature of some aspect of cognition. Eliminativists such as Patricia and Paul Churchland argue that while folk psychology treats cognition as fundamentally sentence-like, the non-linguistic vector/matrix model of neural network theory or connectionism will prove to be a much more accurate account of how the brain works.\n\nThe Churchlands often invoke the fate of other, erroneous popular theories and ontologies that have arisen in the course of history. For example, Ptolemaic astronomy served to explain and roughly predict the motions of the planets for centuries, but eventually this model of the solar system was eliminated in favor of the Copernican model. The Churchlands believe the same eliminative fate awaits the \"sentence-cruncher\" model of the mind in which thought and behavior are the result of manipulating sentence-like states called \"propositional attitudes\".\n\nIdealism is the form of monism that sees the world as consisting of minds, mental contents and or consciousness.\nIdealists are not faced with explaining how minds arise from bodies: rather, the world, bodies and objects are regarded as mere appearances held by minds. However, accounting for the mind–body problem is not usually the main motivation for idealism; rather, idealists tend to be motivated by skepticism, intentionality, and the unique nature of ideas.\nIdealism is prominent in Eastern religious and philosophical thought. It has gone through several cycles of popularity and neglect in the history of Western philosophy.\n\nDifferent varieties of idealism may hold that there are\n\nNeutral monism, in philosophy, is the metaphysical view that the mental and the physical are two ways of organizing or describing the same elements, which are themselves \"neutral\", that is, neither physical nor mental. This view denies that the mental and the physical are two fundamentally different things. Rather, neutral monism claims the universe consists of only one kind of stuff, in the form of neutral elements that are in themselves neither mental nor physical. These neutral elements might have the properties of color and shape, just as we experience those properties. But these shaped and colored elements do not exist in a mind (considered as a substantial entity, whether dualistically or physicalistically); they exist on their own.\n\nSome philosophers take an epistemic approach and argue that the mind–body problem is currently unsolvable, and perhaps will always remain unsolvable to human beings. This is usually termed New mysterianism. Colin McGinn holds that human beings are cognitively closed in regards to their own minds. According to McGinn human minds lack the concept-forming procedures to fully grasp how mental properties such as consciousness arise from their causal basis. An example would be how an elephant is cognitively closed in regards to particle physics.\n\nA more moderate conception has been expounded by Thomas Nagel, which holds that the mind–body problem is currently unsolvable at the present stage of scientific development and that it might take a future scientific paradigm shift or revolution to bridge the explanatory gap. Nagel posits that in the future a sort of \"objective phenomenology\" might be able to bridge the gap between subjective conscious experience and its physical basis.\n\nEach attempt to answer the mind–body problem encounters substantial problems. Some philosophers argue that this is because there is an underlying conceptual confusion. These philosophers, such as Ludwig Wittgenstein and his followers in the tradition of linguistic criticism, therefore reject the problem as illusory. They argue that it is an error to ask how mental and biological states fit together. Rather it should simply be accepted that human experience can be described in different ways—for instance, in a mental and in a biological vocabulary. Illusory problems arise if one tries to describe the one in terms of the other's vocabulary or if the mental vocabulary is used in the wrong contexts. This is the case, for instance, if one searches for mental states of the brain. The brain is simply the wrong context for the use of mental vocabulary—the search for mental states of the brain is therefore a category error or a sort of fallacy of reasoning.\n\nToday, such a position is often adopted by interpreters of Wittgenstein such as Peter Hacker. However, Hilary Putnam, the originator of functionalism, has also adopted the position that the mind–body problem is an illusory problem which should be dissolved according to the manner of Wittgenstein.\n\nWhere is the mind located? If the mind is a physical phenomenon of some kind, it has to be located somewhere. According to some, there are two possible options: either the mind is internal to the body (internalism) or the mind is external to it (externalism). More generally, either the mind depends only on events and properties taking place inside the subject's body or it depends also on factors external to it.\n\nProponents of internalism are committed to the view that neural activity is sufficient to produce the mind.\nProponents of externalism maintain that the surrounding world is in some sense constitutive of the mind.\n\nExternalism differentiates into several versions. The main ones are semantic externalism, cognitive externalism and phenomenal externalism. Each of these versions of externalism can further be divided into whether they refer only to the content or to the vehicles of the mind.\n\nSemantic externalism holds that the semantic content of the mind is totally or partially defined by a state of affairs external to the body of the subject. Hilary Putnam's Twin Earth thought experiment is a good example.\n\nCognitive externalism is a very broad collection of views that suggests the role of the environment, of tools, of development, and of the body in fleshing out cognition. Embodied cognition, the extended mind, and enactivism are good examples.\n\nPhenomenal externalism suggests that the phenomenal aspects of the mind are external to the body. Authors who addressed this possibility are Ted Honderich, Edwin Holt, Francois Tonneau, Kevin O'Regan, Riccardo Manzotti, Teed Rockwell and Max Velmans.\n\nThe thesis of physicalism is that the mind is part of the material (or physical) world. Such a position faces the problem that the mind has certain properties that no other material thing seems to possess. Physicalism must therefore explain how it is possible that these properties can nonetheless emerge from a material thing. The project of providing such an explanation is often referred to as the \"naturalization of the mental\". Some of the crucial problems that this project attempts to resolve include the existence of qualia and the nature of intentionality.\n\nMany mental states seem to be experienced subjectively in different ways by different individuals. And it is characteristic of a mental state that it has some experiential \"quality\", e.g. of pain, that it hurts. However, the sensation of pain between two individuals may not be identical, since no one has a perfect way to measure how much something hurts or of describing exactly how it feels to hurt. Philosophers and scientists therefore ask where these experiences come from. The existence of cerebral events, in and of themselves, cannot explain why they are accompanied by these corresponding qualitative experiences. The puzzle of why many cerebral processes occur with an accompanying experiential aspect in consciousness seems impossible to explain.\n\nYet it also seems to many that science will eventually have to explain such experiences. This follows from an assumption about the possibility of reductive explanations. According to this view, if an attempt can be successfully made to explain a phenomenon reductively (e.g., water), then it can be explained why the phenomenon has all of its properties (e.g., fluidity, transparency). In the case of mental states, this means that there needs to be an explanation of why they have the property of being experienced in a certain way.\n\nThe 20th-century German philosopher Martin Heidegger criticized the ontological assumptions underpinning such a reductive model, and claimed that it was impossible to make sense of experience in these terms. This is because, according to Heidegger, the nature of our subjective experience and its \"qualities\" is impossible to understand in terms of Cartesian \"substances\" that bear \"properties\". Another way to put this is that the very concept of qualitative experience is incoherent in terms of—or is semantically incommensurable with the concept of—substances that bear properties.\n\nThis problem of explaining introspective first-person aspects of mental states and consciousness in general in terms of third-person quantitative neuroscience is called the explanatory gap. There are several different views of the nature of this gap among contemporary philosophers of mind. David Chalmers and the early Frank Jackson interpret the gap as ontological in nature; that is, they maintain that qualia can never be explained by science because physicalism is false. There are two separate categories involved and one cannot be reduced to the other. An alternative view is taken by philosophers such as Thomas Nagel and Colin McGinn. According to them, the gap is epistemological in nature. For Nagel, science is not yet able to explain subjective experience because it has not yet arrived at the level or kind of knowledge that is required. We are not even able to formulate the problem coherently. For McGinn, on other hand, the problem is one of permanent and inherent biological limitations. We are not able to resolve the explanatory gap because the realm of subjective experiences is cognitively closed to us in the same manner that quantum physics is cognitively closed to elephants. Other philosophers liquidate the gap as purely a semantic problem. This semantic problem, of course, led to the famous \"Qualia Question\", which is: \"Does Red cause Redness\"?\n\nIntentionality is the capacity of mental states to be directed towards (\"about\") or be in relation with something in the external world. This property of mental states entails that they have contents and semantic referents and can therefore be assigned truth values. When one tries to reduce these states to natural processes there arises a problem: natural processes are not true or false, they simply happen. It would not make any sense to say that a natural process is true or false. But mental ideas or judgments are true or false, so how then can mental states (ideas or judgments) be natural processes? The possibility of assigning semantic value to ideas must mean that such ideas are about facts. Thus, for example, the idea that Herodotus was a historian refers to Herodotus and to the fact that he was a historian. If the fact is true, then the idea is true; otherwise, it is false. But where does this relation come from? In the brain, there are only electrochemical processes and these seem not to have anything to do with Herodotus.\n\nPhilosophy of perception is concerned with the nature of perceptual experience and the status of perceptual objects, in particular how perceptual experience relates to appearances and beliefs about the world. The main contemporary views within philosophy of perception include naive realism, enactivism and representational views.\n\nHumans are corporeal beings and, as such, they are subject to examination and description by the natural sciences. Since mental processes are intimately related to bodily processes, the descriptions that the natural sciences furnish of human beings play an important role in the philosophy of mind. There are many scientific disciplines that study processes related to the mental. The list of such sciences includes: biology, computer science, cognitive science, cybernetics, linguistics, medicine, pharmacology, and psychology.\n\nThe theoretical background of biology, as is the case with modern natural sciences in general, is fundamentally materialistic. The objects of study are, in the first place, physical processes, which are considered to be the foundations of mental activity and behavior. The increasing success of biology in the explanation of mental phenomena can be seen by the absence of any empirical refutation of its fundamental presupposition: \"there can be no change in the mental states of a person without a change in brain states.\"\n\nWithin the field of neurobiology, there are many subdisciplines that are concerned with the relations between mental and physical states and processes: Sensory neurophysiology investigates the relation between the processes of perception and stimulation. Cognitive neuroscience studies the correlations between mental processes and neural processes. Neuropsychology describes the dependence of mental faculties on specific anatomical regions of the brain. Lastly, evolutionary biology studies the origins and development of the human nervous system and, in as much as this is the basis of the mind, also describes the ontogenetic and phylogenetic development of mental phenomena beginning from their most primitive stages. Evolutionary biology furthermore places tight constraints on any philosophical theory of the mind, as the gene-based mechanism of natural selection does not allow any giant leaps in the development of neural complexity or neural software but only incremental steps over long time periods.\n\nThe methodological breakthroughs of the neurosciences, in particular the introduction of high-tech neuroimaging procedures, has propelled scientists toward the elaboration of increasingly ambitious research programs: one of the main goals is to describe and comprehend the neural processes which correspond to mental functions (see: neural correlate). Several groups are inspired by these advances.\n\nComputer science concerns itself with the automatic processing of information (or at least with physical systems of symbols to which information is assigned) by means of such things as computers. From the beginning, computer programmers have been able to develop programs that permit computers to carry out tasks for which organic beings need a mind. A simple example is multiplication. It is not clear whether computers could be said to have a mind. Could they, someday, come to have what we call a mind? This question has been propelled into the forefront of much philosophical debate because of investigations in the field of artificial intelligence (AI).\n\nWithin AI, it is common to distinguish between a modest research program and a more ambitious one: this distinction was coined by John Searle in terms of a weak AI and strong AI. The exclusive objective of \"weak AI\", according to Searle, is the successful simulation of mental states, with no attempt to make computers become conscious or aware, etc. The objective of strong AI, on the contrary, is a computer with consciousness similar to that of human beings. The program of strong AI goes back to one of the pioneers of computation Alan Turing. As an answer to the question \"Can computers think?\", he formulated the famous Turing test. Turing believed that a computer could be said to \"think\" when, if placed in a room by itself next to another room that contained a human being and with the same questions being asked of both the computer and the human being by a third party human being, the computer's responses turned out to be indistinguishable from those of the human. Essentially, Turing's view of machine intelligence followed the behaviourist model of the mind—intelligence is as intelligence does. The Turing test has received many criticisms, among which the most famous is probably the Chinese room thought experiment formulated by Searle.\n\nThe question about the possible sensitivity (qualia) of computers or robots still remains open. Some computer scientists believe that the specialty of AI can still make new contributions to the resolution of the \"mind–body problem\". They suggest that based on the reciprocal influences between software and hardware that takes place in all computers, it is possible that someday theories can be discovered that help us to understand the reciprocal influences between the human mind and the brain (wetware).\n\nPsychology is the science that investigates mental states directly. It uses generally empirical methods to investigate concrete mental states like joy, fear or obsessions. Psychology investigates the laws that bind these mental states to each other or with inputs and outputs to the human organism.\n\nAn example of this is the psychology of perception. Scientists working in this field have discovered general principles of the perception of forms. A law of the psychology of forms says that objects that move in the same direction are perceived as related to each other. This law describes a relation between visual input and mental perceptual states. However, it does not suggest anything about the nature of perceptual states. The laws discovered by psychology are compatible with all the answers to the mind–body problem already described.\n\nCognitive science is the interdisciplinary scientific study of the mind and its processes. It examines what cognition is, what it does, and how it works. It includes research on intelligence and behavior, especially focusing on how information is represented, processed, and transformed (in faculties such as perception, language, memory, reasoning, and emotion) within nervous systems (human or other animal) and machines (e.g. computers). Cognitive science consists of multiple research disciplines, including psychology, artificial intelligence, philosophy, neuroscience, linguistics, anthropology, sociology, and education. It spans many levels of analysis, from low-level learning and decision mechanisms to high-level logic and planning; from neural circuitry to modular brain organisation. Rowlands argues that cognition is enactive, embodied, embedded, affective and (potentially) extended. The position is taken that the \"classical sandwich\" of cognition sandwiched between perception and action is artificial; cognition has to be seen as a product of a strongly coupled interaction that cannot be divided this way.\n\nMost of the discussion in this article has focused on one style or tradition of philosophy in modern Western culture, usually called analytic philosophy (sometimes described as Anglo-American philosophy). Many other schools of thought exist, however, which are sometimes subsumed under the broad (and vague) label of continental philosophy. In any case, though topics and methods here are numerous, in relation to the philosophy of mind the various schools that fall under this label (phenomenology, existentialism, etc.) can globally be seen to differ from the analytic school in that they focus less on language and logical analysis alone but also take in other forms of understanding human existence and experience. With reference specifically to the discussion of the mind, this tends to translate into attempts to grasp the concepts of thought and perceptual experience in some sense that does not merely involve the analysis of linguistic forms.\n\nImmanuel Kant's \"Critique of Pure Reason\", first published in 1781 and presented again with major revisions in 1787, represents a significant intervention into what will later become known as the philosophy of mind. Kant's first critique is generally recognized as among the most significant works of modern philosophy in the West. Kant is a figure whose influence is marked in both continental and analytic/Anglo-American philosophy. Kant's work develops an in-depth study of transcendental consciousness, or the life of the mind as conceived through universal categories of consciousness.\n\nIn Georg Wilhelm Friedrich Hegel's \"Philosophy of Mind\" (frequently translated as \"Philosophy of Spirit\" or Geist), the third part of his \"Encyclopedia of the Philosophical Sciences\", Hegel discusses three distinct types of mind: the \"subjective mind/spirit\", the mind of an individual; the \"objective mind/spirit\", the mind of society and of the State; and the \"Absolute mind/spirit\", the position of religion, art, and philosophy. See also Hegel's \"The Phenomenology of Spirit\". Nonetheless, Hegel's work differs radically from the style of Anglo-American philosophy of mind.\n\nIn 1896, Henri Bergson made in \"Matter and Memory\" \"Essay on the relation of body and spirit\" a forceful case for the ontological difference of body and mind by reducing the problem to the more definite one of memory, thus allowing for a solution built on the \"empirical test case\" of aphasia.\n\nIn modern times, the two main schools that have developed in response or opposition to this Hegelian tradition are phenomenology and existentialism. Phenomenology, founded by Edmund Husserl, focuses on the contents of the human mind (see noema) and how processes shape our experiences. Existentialism, a school of thought founded upon the work of Søren Kierkegaard, focuses on Human predicament and how people deal with the situation of being alive. Existential-phenomenology represents a major branch of continental philosophy (they are not contradictory), rooted in the work of Husserl but expressed in its fullest forms in the work of Martin Heidegger, Jean-Paul Sartre, Simone de Beauvoir and Maurice Merleau-Ponty. See Heidegger's \"Being and Time\", Merleau-Ponty's \"Phenomenology of Perception\", Sartre's \"Being and Nothingness\", and Simone de Beauvoir's \"The Second Sex\".\n\nSubstance Dualism is a common feature of several orthodox Hindu schools including the Sāṅkhya, Nyāya, Yoga and Dvaita Vedanta. In these schools a clear difference is drawn between matter and a non-material soul, which is eternal and undergoes samsara, a cycle of death and rebirth. The Nyāya school argued that qualities such as cognition and desire are inherent qualities which are not possessed by anything solely material, and therefore by process of elimination must belong to a non-material self, the atman. Many of these schools see their spiritual goal as moksha, liberation from the cycle of reincarnation.\n\nIn the Advaita Vedanta of the 8th century Indian philosopher Śaṅkara, the mind, body and world are all held to be the same unchanging eternal conscious entity called Brahman. Advaita, which means non-dualism, holds the view that all that exists is pure absolute consciousness. The fact that the world seems to be made up of changing entities is an illusion, or Maya. The only thing that exists is Brahman, which is described as Satchitananda (Being, consciousness and bliss). Advaita Vedanta is best described by a verse which states \"Brahman is alone True, and this world of plurality is an error; the individual self is not different from Brahman.\"\n\nAnother form of monistic Vedanta is Vishishtadvaita (qualified non-dualism) as posited by the eleventh century philosopher Ramanuja. Ramanuja criticized Advaita Vedanta by arguing that consciousness is always intentional and that it is also always a property of something. Ramanuja's Brahman is defined by a multiplicity of qualities and properties in a single monistic entity. This doctrine is called \"samanadhikaranya\" (several things in a common substrate).\n\nArguably the first exposition of empirical materialism in the history of philosophy is in the Cārvāka school (also called Lokāyata). The Cārvāka school rejected the existence of anything but matter (which they defined as being made up of the four elements), including God and the soul. Therefore, they held that even consciousness was nothing but a construct made up of atoms. A section of the Cārvāka school believed in a material soul made up of air or breath, but since this also was a form of matter, it was not said to survive death.\n\nBuddhist teachings describe that the mind manifests moment-to-moment as sense impressions and mental phenomena that are continuously changing. The moment-by-moment manifestation of the mind-stream has been described as happening in every person all the time, even in a scientist who analyses various phenomena in the world, or analyses the material body including the organ brain. The manifestation of the mind-stream is also described as being influenced by physical laws, biological laws, psychological laws, volitional laws, and universal laws.\n\nA salient feature of Buddhist philosophy which sets it apart from Indian orthodoxy is the centrality of the doctrine of not-self (Pāli. anatta, Skt. anātman). The Buddha's not-self doctrine sees humans as an impermanent composite of five psychological and physical aspects instead of a single fixed self. In this sense, what is called ego or the self is merely a convenient fiction, an illusion that does not apply to anything real but to an erroneous way of looking at the ever-changing stream of five interconnected aggregate factors. The relationship between these aggregates is said to be one of dependent-arising (pratītyasamutpāda). This means that all things, including mental events, arise co-dependently from a plurality of other causes and conditions. This seems to reject both causal determinist and epiphenomenalist conceptions of mind.\n\nThree centuries after the death of the Buddha (c. 150 BCE) saw the growth of a large body of literature called the Abhidharma in several contending Buddhist schools. In the Abhidharmic analysis of mind, the ordinary thought is defined as prapañca ('conceptual proliferation'). According to this theory, perceptual experience is bound up in multiple conceptualizations (expectations, judgments and desires). This proliferation of conceptualizations form our illusory superimposition of concepts like self and other upon an ever-changing stream of aggregate phenomena.\nIn this conception of mind no strict distinction is made between the conscious faculty and the actual sense perception of various phenomena. Consciousness is instead said to be divided into six sense modalities, five for the five senses and sixth for perception of mental phenomena. The arising of cognitive awareness is said to depend on sense perception, awareness of the mental faculty itself which is termed mental or 'introspective awareness' (\"manovijñāna\") and attention (\"āvartana\"), the picking out of objects out of the constantly changing stream of sensory impressions.\n\nRejection of a permanent agent eventually led to the philosophical problems of the seeming continuity of mind and also of explaining how rebirth and karma continue to be relevant doctrines without an eternal mind. This challenge was met by the Theravāda school by introducing the concept of mind as a factor of existence. This \"life-stream\" (Bhavanga-sota) is an undercurrent forming the condition of being. The continuity of a karmic \"person\" is therefore assured in the form of a mindstream (citta-santana), a series of flowing mental moments arising from the subliminal life-continuum mind (Bhavanga-citta), mental content, and attention.\n\nThe Sautrāntika school held a form of phenomenalism that saw the world as imperceptible. It held that external objects exist only as a support for cognition, which can only apprehend mental representations. This influenced the later Yogācāra school of Mahayana Buddhism. The Yogācāra school is often called the mind-only school because of its internalist stance that consciousness is the ultimate existing reality. The works of Vasubandhu have often been interpreted as arguing for some form of Idealism. Vasubandhu uses the dream argument and a mereological refutation of atomism to attack the reality of external objects as anything other than mental entities. Scholarly interpretations of Vasubandhu's philosophy vary widely, and include phenomenalism, neutral monism and realist phenomenology.\n\nThe Indian Mahayana schools were divided on the issue of the possibility of reflexive awareness (\"svasaṃvedana\"). Dharmakīrti accepted the idea of reflexive awareness as expounded by the Yogācāra school, comparing it to a lamp that illuminates itself while also illuminating other objects. This was strictly rejected by Mādhyamika scholars like Candrakīrti. Since in the philosophy of the Mādhyamika all things and mental events are characterized by emptiness, they argued that consciousness could not be an inherently reflexive ultimate reality since that would mean it was self-validating and therefore not characterized by emptiness. These views were ultimately reconciled by the 8th century thinker Śāntarakṣita. In Śāntarakṣita's synthesis he adopts the idealist Yogācāra views of reflexive awareness as a conventional truth into the structure of the two truths doctrine. Thus he states: \"By relying on the Mind-Only system, know that external entities do not exist. And by relying on this Middle Way system, know that no self exists at all, even in that [mind].\" \n\nThe Yogācāra school also developed the theory of the repository consciousness (\"ālayavijñāna\") to explain continuity of mind in rebirth and accumulation of karma. This repository consciousness acts as a storehouse for karmic seeds (bija) when all other senses are absent during the process of death and rebirth as well as being the causal potentiality of dharmic phenomena. Thus according to B. Alan Wallace: \nNo constituents of the body—in the brain or elsewhere—transform into mental states and processes. Such subjective experiences do not emerge from the body, but neither do they emerge from nothing. Rather, all objective mental appearances arise from the substrate, and all subjective mental states and processes arise from the substrate consciousness.\n\nTibetan Buddhist theories of mind evolved directly from the Indian Mahayana views. Thus the founder of the Gelug school, Je Tsongkhapa discusses the Yogācāra system of the Eight Consciousnesses in his \"Explanation of the Difficult Points\". He would later come to repudiate Śāntarakṣita's pragmatic idealism. \nAccording to the 14th Dalai Lama the mind can be defined \"as an entity that has the nature of mere experience, that is, 'clarity and knowing'. It is the knowing nature, or agency, that is called mind, and this is non-material.\" The simultaneously dual nature of mind is as follows:\nThe 14th Dalai Lama has also explicitly laid out his theory of mind as experiential dualism which is described above under the different types of dualism.\n\nBecause Tibetan philosophy of mind is ultimately soteriological, it focuses on meditative practices such as Dzogchen and Mahamudra that allow a practitioner to experience the true reflexive nature of their mind directly. This unobstructed knowledge of one's primordial, empty and non-dual Buddha nature is called rigpa. The mind's innermost nature is described among various schools as pure luminosity or \"clear light\" ('od gsal) and is often compared to a crystal ball or a mirror. Sogyal Rinpoche speaks of mind thus:\n\"Imagine a sky, empty, spacious, and pure from the beginning; its essence is like this. Imagine a sun, luminous, clear, unobstructed, and spontaneously present; its nature is like this.\"\n\nThe central issue in Chinese Zen philosophy of mind is in the difference between the pure and awakened mind and the defiled mind. Chinese Chan master Huangpo described the mind as without beginning and without form or limit while the defiled mind was that which was obscured by attachment to form and concepts. The pure Buddha-mind is thus able to see things \"as they truly are\", as absolute and non-dual \"thusness\" (Tathatā). This non-conceptual seeing also includes the paradoxical fact that there is no difference between a defiled and a pure mind, as well as no difference between samsara and nirvana.\n\nIn the Shobogenzo, the Japanese philosopher Dogen argued that body and mind are neither ontologically nor phenomenologically distinct but are characterized by a oneness called \"shin jin\" (bodymind). According to Dogen, \"casting off body and mind\" (\"Shinjin datsuraku\") in zazen will allow one to experience things-as-they-are (\"genjokoan\") which is the nature of original enlightenment (\"hongaku\").\n\nThere are countless subjects that are affected by the ideas developed in the philosophy of mind. Clear examples of this are the nature of death and its definitive character, the nature of emotion, of perception and of memory. Questions about what a person is and what his or her identity consists of also have much to do with the philosophy of mind. There are two subjects that, in connection with the philosophy of the mind, have aroused special attention: free will and the self.\n\nIn the context of philosophy of mind, the problem of free will takes on renewed intensity. This is certainly the case, at least, for materialistic determinists. According to this position, natural laws completely determine the course of the material world. Mental states, and therefore the will as well, would be material states, which means human behavior and decisions would be completely determined by natural laws. Some take this reasoning a step further: people cannot determine by themselves what they want and what they do. Consequently, they are not free.\n\nThis argumentation is rejected, on the one hand, by the compatibilists. Those who adopt this position suggest that the question \"Are we free?\" can only be answered once we have determined what the term \"free\" means. The opposite of \"free\" is not \"caused\" but \"compelled\" or \"coerced\". It is not appropriate to identify freedom with indetermination. A free act is one where the agent could have done otherwise if it had chosen otherwise. In this sense a person can be free even though determinism is true. The most important compatibilist in the history of the philosophy was David Hume. More recently, this position is defended, for example, by Daniel Dennett.\n\nOn the other hand, there are also many incompatibilists who reject the argument because they believe that the will is free in a stronger sense called libertarianism. These philosophers affirm the course of the world is either a) not completely determined by natural law where natural law is intercepted by physically independent agency, b) determined by indeterministic natural law only, or c) determined by indeterministic natural law in line with the subjective effort of physically non-reducible agency. Under Libertarianism, the will does not have to be deterministic and, therefore, it is potentially free. Critics of the second proposition (b) accuse the incompatibilists of using an incoherent concept of freedom. They argue as follows: if our will is not determined by anything, then we desire what we desire by pure chance. And if what we desire is purely accidental, we are not free. So if our will is not determined by anything, we are not free.\n\nThe philosophy of mind also has important consequences for the concept of \"self\". If by \"self\" or \"I\" one refers to an essential, immutable nucleus of the \"person\", some modern philosophers of mind, such as Daniel Dennett believe that no such thing exists. According to Dennett and other contemporaries, the self is considered an illusion. The idea of a self as an immutable essential nucleus derives from the idea of an immaterial soul. Such an idea is unacceptable to modern philosophers with physicalist orientations and their general skepticism of the concept of \"self\" as postulated by David Hume, who could never catch himself \"not\" doing, thinking or feeling anything. However, in the light of empirical results from developmental psychology, developmental biology and neuroscience, the idea of an essential inconstant, material nucleus—an integrated representational system distributed over changing patterns of synaptic connections—seems reasonable.\n\n"}
{"id": "249438", "url": "https://en.wikipedia.org/wiki?curid=249438", "title": "Principle of least action", "text": "Principle of least action\n\nThe principle of least action – or, more accurately, the principle of stationary action – is a variational principle that, when applied to the action of a mechanical system, can be used to obtain the equations of motion for that system. In relativity, a different action must be minimized or maximized. The principle can be used to derive Newtonian, Lagrangian and Hamiltonian equations of motion, and even general relativity (see Einstein–Hilbert action). The physicist Paul Dirac, and after him Julian Schwinger and Richard Feynman, demonstrated how this principle can also be used in quantum calculations.\nIt was historically called \"least\" because its solution requires finding the path that has the least value. Its classical mechanics and electromagnetic expressions are a consequence of quantum mechanics, but the stationary action method helped in the development of quantum mechanics.\n\nThe principle remains central in modern physics and mathematics, being applied in thermodynamics, fluid mechanics, the theory of relativity, quantum mechanics, particle physics, and string theory and is a focus of modern mathematical investigation in Morse theory. Maupertuis' principle and Hamilton's principle exemplify the principle of stationary action.\n\nThe action principle is preceded by earlier ideas in optics. In ancient Greece, Euclid wrote in his \"Catoptrica\" that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection. Hero of Alexandria later showed that this path was the shortest length and least time.\n\nScholars often credit Pierre Louis Maupertuis for formulating the principle of least action because he wrote about it in 1744 and 1746. However, Leonhard Euler discussed the principle in 1744, and evidence shows that Gottfried Leibniz preceded both by 39 years.\n\nIn 1933, Paul Dirac discerned the quantum mechanical underpinning of the principle in the quantum interference of amplitudes.\n\nThe starting point is the \"action\", denoted formula_1 (calligraphic S), of a physical system. It is defined as the integral of the Lagrangian \"L\" between two instants of time \"t\" and \"t\" - technically a functional of the \"N\" generalized coordinates q = (\"q\", \"q\", ... , \"q\") which define the configuration of the system:\n\nwhere the dot denotes the time derivative, and \"t\" is time.\n\nMathematically the principle is\n\nwhere \"δ\" (lowercase Greek delta) means a \"small\" change. In words this reads:\n\nIn applications the statement and definition of action are taken together:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nIn the 1600s, Pierre de Fermat postulated that \"\"light travels between two given points along the path of shortest time\",\" which is known as the principle of least time or Fermat's principle.\n\nCredit for the formulation of the principle of least action is commonly given to Pierre Louis Maupertuis, who felt that \"Nature is thrifty in all its actions\", and applied the principle broadly:\n\nThis notion of Maupertuis, although somewhat deterministic today, does capture much of the essence of mechanics.\n\nIn application to physics, Maupertuis suggested that the quantity to be minimized was the product of the duration (time) of movement within a system by the \"vis viva\",\n\nwhich is the integral of twice what we now call the kinetic energy \"T\" of the system.\n\nLeonhard Euler gave a formulation of the action principle in 1744, in very recognizable terms, in the \"Additamentum 2\" to his \"Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes\". Beginning with the second paragraph:\n\nAs Euler states, ∫\"Mv\"d\"s\" is the integral of the momentum over distance travelled, which, in modern notation, equals the abbreviated or reduced action\n\nThus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. Curiously, Euler did not claim any priority, as the following episode shows.\n\nMaupertuis' priority was disputed in 1751 by the mathematician Samuel König, who claimed that it had been invented by Gottfried Leibniz in 1707. Although similar to many of Leibniz's arguments, the principle itself has not been documented in Leibniz's works. König himself showed a \"copy\" of a 1707 letter from Leibniz to Jacob Hermann with the principle, but the \"original\" letter has been lost. In contentious proceedings, König was accused of forgery, and even the King of Prussia entered the debate, defending Maupertuis (the head of his Academy), while Voltaire defended König.\n\nEuler, rather than claiming priority, was a staunch defender of Maupertuis, and Euler himself prosecuted König for forgery before the Berlin Academy on 13 April 1752. The claims of forgery were re-examined 150 years later, and archival work by C.I. Gerhardt in 1898 and W. Kabitz in 1913 uncovered other copies of the letter, and three others cited by König, in the Bernoulli archives.\n\nEuler continued to write on the topic; in his \"Reflexions sur quelques loix generales de la nature\" (1748), he called the quantity \"effort\". His expression corresponds to what we would now call potential energy, so that his statement of least action in statics is equivalent to the principle that a system of bodies at rest will adopt a configuration that minimizes total potential energy.\n\nMuch of the calculus of variations was stated by Joseph-Louis Lagrange in 1760 and he proceeded to apply this to problems in dynamics. In \"Méchanique Analytique\" (1788) Lagrange derived the general equations of motion of a mechanical body. William Rowan Hamilton in 1834 and 1835 applied the variational principle to the classical Lagrangian function\n\nto obtain the Euler–Lagrange equations in their present form.\n\nIn 1842, Carl Gustav Jacobi tackled the problem of whether the variational principle always found minima as opposed to other stationary points (maxima or stationary saddle points); most of his work focused on geodesics on two-dimensional surfaces. The first clear general statements were given by Marston Morse in the 1920s and 1930s, leading to what is now known as Morse theory. For example, Morse showed that the number of conjugate points in a trajectory equalled the number of negative eigenvalues in the second variation of the Lagrangian.\n\nOther extremal principles of classical mechanics have been formulated, such as Gauss's principle of least constraint and its corollary, Hertz's principle of least curvature.\n\nThe mathematical equivalence of the differential equations of motion and their integral\ncounterpart has important philosophical implications. The differential equations are statements about quantities localized to a single point in space or single moment of time. For example, Newton's second law\n\nstates that the \"instantaneous\" force F applied to a mass \"m\" produces an acceleration a at the same \"instant\". By contrast, the action principle is not localized to a point; rather, it involves integrals over an interval of time and (for fields) an extended region of space. Moreover, in the usual formulation of classical action principles, the initial and final states of the system are fixed, e.g.,\n\nIn particular, the fixing of the \"final\" state has been interpreted as giving the action principle a teleological character which has been controversial historically. However, according to W. Yourgrau and S. Mandelstam, \"the teleological approach... presupposes that the variational principles themselves have mathematical characteristics which they \"de facto\" do not possess\" In addition, some critics maintain this apparent teleology occurs because of the way in which the question was asked. By specifying some but not all aspects of both the initial and final conditions (the positions but not the velocities) we are making some inferences about the initial conditions from the final conditions, and it is this \"backward\" inference that can be seen as a teleological explanation. Teleology can also be overcome if we consider the classical description as a limiting case of the quantum formalism of path integration, in which stationary paths are obtained as a result of interference of amplitudes along all possible paths.\n\nThe short story \"Story of Your Life\" by the speculative fiction writer Ted Chiang contains visual depictions of Fermat's Principle along with a discussion of its teleological dimension. Keith Devlin's \"The Math Instinct\" contains a chapter, \"Elvis the Welsh Corgi Who Can Do Calculus\" that discusses the calculus \"embedded\" in some animals as they solve the \"least time\" problem in actual situations.\n\n"}
{"id": "1788944", "url": "https://en.wikipedia.org/wiki?curid=1788944", "title": "Principle of plenitude", "text": "Principle of plenitude\n\nThe principle of plenitude asserts that the universe contains all possible forms of existence. The historian of ideas Arthur Lovejoy was the first to trace the history of this philosophically important principle explicitly. Lovejoy distinguishes two versions of the principle: a static version, in which the universe displays a constant fullness and diversity, and a temporalized version, in which fullness and diversity gradually increase over time.\n\nLovejoy traces the principle of plenitude to the writings of Plato, finding in the \"Timaeus\" an insistence on \"the necessarily complete translation of all the ideal possibilities into actuality\". By contrast, he takes Aristotle to reject the principle in his \"Metaphysics\", when he writes that \"it is not necessary that everything that is possible should exist in actuality\".\n\nSince Plato, the principle of plenitude has had the following adherents:\n\n\n\n"}
{"id": "1875925", "url": "https://en.wikipedia.org/wiki?curid=1875925", "title": "Pythagorean Method of Memorization", "text": "Pythagorean Method of Memorization\n\nPythagorean Method of Memorization (PYMOM), also known as Triangular Movement Cycle (TMC), is a game-based, educational methodology or associative-learning technique that primarily uses corresponding information, such as terms and definitions on opposing sides, displayed on cue cards, to exploit psychological retention of information for academic study and language acquisition. PYMOM is named such because of the shape the cue-cards form during the progression of the game, a right-angled or Pythagorean triangle.\n\nIt is a theoretical educational method that is made up of several established and tested educational methods that have been in use for decades.\n\nPYMOM is a composite body of techniques that claims, in its digital form, to incorporate (to a greater or lesser degree): spaced repetition, non-failure redundant subroutine, chromatics, positive reinforcement, the Von Restorff effect, picture association, selective musical tonality, kinesthetics, the serial-position effect and meditation. There are two branches of this methodology:\n\nAs with both branches, there is only one variable in the game or learning method: a correct or incorrect answer. The initial movement cycle also remains largely unchanged.\n\nThe movement cycle which is most crucial to the methodology and reinforces the spaced repetition, begins with either 3, 4 or 5 cards; 3 cards for a 6-card session, 4 cards for a 10-card session and 5 cards for the most advanced 15-card session. Because two-sided associative cue-cards are being used, all cards are presented with a congruent side up, either all \"terms\" or \"definitions,\" not mixed.\n\nOnce cards have been answered correctly, the predominant row has reached its maximum and a card must be graduated out of this row to continue the game. Thus the card to the far right comes into play. Routines are repeated as each row reaches its maximum. A cue-card is finally eliminated from the game session by being answered correctly once more, after it has graduated to the top tier or row.\n\nThe first manifestation, referred to as the \"Triangular Movement Cycle\" or TMC, was a simple paper-based learning technique that was primarily a manual movement cycle using physical cue-cards, which allowed for manual-spaced repetition to elicit psychological retention of information. Its origins, however, are not very clear. Using TMC, teachers would move the cards for the student in a one-on-one setting according to either the correct or incorrect feedback from the student.This presented challenges for the teacher or tutor using this method. The first challenge lies in the fact that, although TMC lent itself well to a two-party learning group (i.e. teacher & student), it could also be done by the student themselves on their own. It was a very easy system to utilize once learned, however, it was found exceptionally difficult to teach the complex movement cycle and principles behind such to students, especially where a linguistic barrier was present. The second challenge lay in that the educator needed to create and remember innumerable cue-cards or create custom master lists in order to know the correct answers — and properly guide the student, thus progressing or digressing the card in play. TMC often failed to keep the attention of many students owing to the fact that cards were not very visually appealing. To make them so required tremendous effort — and was very time consuming.\n\nThe term \"Pythagorean Method of Memorization\" was coined in 2013 and officially copyrighted in October 2014 by a Canadian company named You Learn Educational Solutions & Linguistics Inc. PYMOM takes the movement cycle from TMC and remedied the challenge of teaching the movement cycle itself to students by providing a software-based solution to handling cycles by means of sub-routines prompted by the user’s input.\n\nPYMOM wove established educational theory into the fabric of TMC to create a viable educational platform for academic and linguistic study by several means. Because spaced repetition is intrinsically part of the movement-cycle subroutines, it adds to the content and surrounding experience making it into a platform. The developers of PYMOM describe it as an “organic learning experience.” The tenets that truly allow a learning system to be a PYMOM-based system are enumerated thusly: The Von Restorff effect: for example, where it features a language, this method is employed to further aid in memory retention of the highlighted word in the phrase.\n\n"}
{"id": "48000439", "url": "https://en.wikipedia.org/wiki?curid=48000439", "title": "Records Continuum Model", "text": "Records Continuum Model\n\nThe Records Continuum Model (RCM) was created in the 1990s by Monash University academic Frank Upward with input from colleagues Sue McKemmish and Livia Iacovino as a response to evolving discussions about the challenges of managing digital records and archives in the discipline of Archival Science. The RCM was first published in Upward’s 1996 paper \"Structuring the Records Continuum – Part One: Postcustodial principles and properties\". Upward describes the RCM within the broad context of a continuum where activities and interactions transform documents into records, evidence and memory that are used for multiple purposes over time. Upward places the RCM within a post-custodial, postmodern and structuration conceptual framework. Australian academics and practitioners continue to explore, develop and extend the RCM and records continuum theory, along with international collaborators, via the Records Continuum Research Group (RCRG) at Monash University.\n\nThe RCM is an abstract conceptual model that helps to understand and explore recordkeeping activities (as interaction) in relation to multiple contexts over space and time (spacetime). Recordkeeping activities take place from before the records are created by identifying recordkeeping requirements in policies, systems, organizations, processes, laws, social mandates that impact on what is created and how it is managed over spacetime. In a continuum, recordkeeping processes, such as adding metadata, fix documents so that they can be managed as evidence. Those records deemed as having continuing value are retained and managed as an archive. The implication of an RCM approach to records and archives is that systems and processes can be designed and put in place before records are even created. A continuum approach therefore highlights that records are both current and archival at the point of creation.\n\nThe RCM is represented as a series of concentric rings (dimensions of \"Create\", \"Capture\", \"Organize\" and \"Pluralize\") and crossed axes (transactionality, evidentiality, recordkeeping and identity) with each axis labelled with a description of the activity or interaction that occurs at that intersection. \"Create\", \"Capture\", \"Organize\" and \"Pluralize\" represent recordkeeping activities that occur within spacetime. Activities that occur in these dimensions across the axes are explained in the table below:\n\nThe value of the RCM is that it can help to map where on a continuum recordkeeping activities are or can be placed. The RCM can then be used to explore the conceptual and practical assumptions that underpin the practice, in particular the dualisms inherent in the usage and practice of the terms \"records\" and \"archives\". This definition lends itself to a linear reading of the RCM – starting at \"Create\" as the initiating phase and working outwards towards \"Pluralization\" of recorded information. Another linear reading is to consider design first – the role that systems of \"Pluralization\" and \"Organization\" play in designing, planning and implementing recordkeeping and then considering the implications for \"Create\" and \"Capture\". However, these are just two of many ways to interpret the model as the dimensions and axes represent multiple realities that occur within spacetime, any of which can occur simultaneously, concurrently and sequentially in electronic or digital environments, and/or physical spaces.\n\nBy representing multiple realities, the RCM articulates the numerous and diverse perspectives that contribute to records and archives including individual, group, community, organizational, institutional and societal. These contexts reveal the need to take into account various stakeholders and co-contributors in relation to use, access and appraisal of records and archives. Over the lifespan of a record multiple decisions are made by various stakeholders of the records that include, but are not limited to records managers and archivists. Other stakeholders can be identified at various dimensions of interaction, including those involved in providing information (not only the person or organization who produced or captured it), as well as their family and community. Records are therefore not simply physical or digital representations of physical objects held and managed in an archive or repository, but are evidence of multiple perspectives, narratives and contexts that contributed to their formation.\n\nThe RCM is often described as being in contrast or at odds with the lifecycle records model. While the RCM is inclusive of multiple ways of conceptualizing and performing recordkeeping, including a lifecycle approach, there are some significant differences. Firstly, where the lifecycle approach shows clearly demarcated phases in the management of records, a continuum approach conceptualizes elements as continuous with no discernable parts. Secondly, the lifecycle approach identifies clear conceptual and procedural boundaries between active or current records and inactive or historical records, but a continuum approach sees records processes as more integrated across spacetime. In the continuum it is recordkeeping processes that carry records forward through spacetime to enable their use for multiple purposes. What this means is that records are always \"in a state of always becoming...\", and able to contribute new contexts via the recordkeeping processes that occur with them. Archival records are therefore not just historical, but are able to be re-interpreted, re-created, and re-contextualized according to their place and use in spacetime. In this way, archival institutions are nodes in the network of recorded information and its contexts, rather than the end point in a lifecycle stage for records that are managed as \"relics\".\n\nThe RCM is a representation of what is commonly referred to as records continuum theory, as well as Australian continuum thinking and/or approaches. These ideas were evolved as part of an Australian approach to archival management espoused by Ian Maclean, Chief Archivist of the Commonwealth Archives Office in Australia in the 1950s and 1960s. Maclean, whose ideas and practices were the subject of the first RCRG publication in 1994, referred in a 1959 \"American Archivist\" article to a \"continuum of (public) records administration\" from administrative efficiency through recordkeeping to the safe keeping of a \"cultural end-product\". Maclean’s vision challenged the divide between current recordkeeping and archival practice. Fellow contemporary at the Commonwealth Archives Office Peter Scott is also included as a core influence on Australian records continuum theory with his development of the Australian Series System, a registry system that helped identify and document the complex and multiple \"social, functional, provenancial, and documentary relationships\" involved in managing records and recordkeeping processes over spacetime.\n\nFurther influences on the RCRG group include archival professionals and researchers like David Bearman and his work on transactionality and systems thinking, and Terry Cook's ideas about postcustodialism and macroappraisal. Wider influencing ideas include those from philosophers and social theorists Jacques Lacan, Michel Foucault, Jacques Derrida, and Jean-François Lyotard, as well as sociologist Anthony Giddens, with structuration theory being a core component of understanding social interaction over spacetime. Canadian archivist Jay Atherton's critique of the division between records managers and archivists in the 1980s and use of the term \"records continuum\" re-commenced the conversation MacLean began during his career and helped to bring his ideas and this term to Australian records continuum thinking. Atherton's use of the term records continuum has several significant differences in conception, application and heritage when compared to Australian records continuum thinking.\n\nPost-custodiality as an archival concept plays a major role in how the RCM was conceived. This term was born from an identified and urgent need to address the complexities of computer technologies on records creation and management over time and space. Post-custodiality is discussed by Frank Upward and Sue McKemmish in 1994 as part of an exploration of changes in archival discourse commencing in the 1980s by Gerald Ham and expanded on by Terry Cook as part of a \"post-custodial paradigm shift\". Post-custodiality in relation to the RCM is explored by Upward and McKemmish as an entry point into a wider conversation about records and recordkeeping being part of a process in which archival institutions have a part to play beyond that of the archival authority handling, appraisal, describing and arranging physical objects in their custody.\n\nDrawing from the above theoretical foundations, the RCM as a framework acknowledges the central role that recordkeeping activities have on the creation, capture, organization and ongoing management of records over time and throughout spaces such as organizations and institutional archives. Recordkeeping is a practice and a concept clearly defined in the archival and records literature by continuum writers as \"a broad and inclusive concept of integrated recordkeeping and archiving processes for current, regulatory, and historical recordkeeping purposes\". Recordkeeping refers to the activities performed on records that add new contexts such as capturing a record into a system, adding metadata, or selecting it for an archive. In the RCM records are therefore not defined according to their status as objects. Rather, records are understood as being part of a continuum of activity related to known (as well as potentially unknown) contexts. A record (as well as records, collections and archives) are therefore part of larger social, cultural, political, legal and archival processes. It is these contexts that are vital to understanding the role, value and evidential qualities of records in and across spacetime (past, present and potential future).\n\nThe RCM is the most well-known of all the continuum models created, but does not exist in isolation. Several other complementary models have been created by RCM creator Frank Upward, and there are others created by continuum researchers that offer enhanced or alternative ways of understanding the continuum.\n\nThe series of continuum models created by Frank Upward include:\n\nModels created in collaboration:\n\nOther models:\n\n"}
{"id": "1241988", "url": "https://en.wikipedia.org/wiki?curid=1241988", "title": "System of measurement", "text": "System of measurement\n\nA system of measurement is a collection of units of measurement and rules relating them to each other. Systems of measurement have historically been important, regulated and defined for the purposes of science and commerce. Systems of measurement in modern use include the metric system, the imperial system, and United States customary units.\n\nThe French Revolution gave rise to the metric system, and this has spread around the world, replacing most customary units of measure. In most systems, length (distance), mass, and time are \"base quantities\".\n\nLater science developments showed that either electric charge or electric current could be added to extend the set of base quantities by which many other metrological units could be easily defined. (However, electrical units are not necessary for such a set. Gaussian units, for example, have only length, mass, and time as base quantities, and the ampere is defined in terms of other units.) Other quantities, such as power and speed, are derived from the base set: for example, speed is distance per unit time. Historically a wide range of units was used for the same type of quantity: in different contexts, length was measured in inches, feet, yards, fathoms, rods, chains, furlongs, miles, nautical miles, stadia, leagues, with conversion factors which were not powers of ten. Such arrangements were satisfactory in their own contexts.\n\nThe preference for a more universal and consistent system (based on more rational base units) only gradually spread with the growth of science. Changing a measurement system has substantial financial and cultural costs which must be offset against the advantages to be obtained from using a more rational system. However pressure built up, including from scientists and engineers for conversion to a more rational, and also internationally consistent, basis of measurement.\n\nIn antiquity, \"systems of measurement\" were defined locally: the different units might be defined independently according to the length of a king's thumb or the size of his foot, the length of stride, the length of arm, or maybe the weight of water in a keg of specific size, perhaps itself defined in \"hands\" and \"knuckles\". The unifying characteristic is that there was some definition based on some standard. Eventually \"cubits\" and \"strides\" gave way to \"customary units\" to meet the needs of merchants and scientists.\n\nIn the metric system and other recent systems, a single basic unit is used for each base quantity. Often secondary units (multiples and submultiples) are derived from the basic units by multiplying by powers of ten, i.e. by simply moving the decimal point. Thus the basic metric unit of length is the metre; a distance of 1.234 m is 1,234 millimetres, or 0.001234 kilometres.\n\nMetrication is complete or nearly complete in almost all countries. US customary units are heavily used in the United States and to some degree in Liberia. Traditional Burmese units of measurement are used in Burma. U.S. units are used in limited contexts in Canada due to the large volume of trade; there is also considerable use of Imperial weights and measures, despite \"de jure\" Canadian conversion to metric.\n\nA number of other jurisdictions have laws mandating or permitting other systems of measurement in some or all contexts, such as the United Kingdom – whose road signage legislation, for instance, only allows distance signs displaying imperial units (miles or yards) – or Hong Kong.\n\nIn the United States, metric units are used almost universally in science, widely in the military, and partially in industry, but customary units predominate in household use. At retail stores, the liter is a commonly used unit for volume, especially on bottles of beverages, and milligrams, rather than grains, are used for medications. Some other standard non-SI units are still in international use, such as nautical miles and knots in aviation and shipping.\n\nMetric systems of units have evolved since the adoption of the first well-defined system in France in 1795. During this evolution the use of these systems has spread throughout the world, first to non-English-speaking countries, and then to English speaking countries.\n\nMultiples and submultiples of metric units are related by powers of ten and their names are formed with prefixes. This relationship is compatible with the decimal system of numbers and it contributes greatly to the convenience of metric units.\n\nIn the early metric system there were two base units, the metre for length and the gram for mass. The other units of length and mass, and all units of area, volume, and derived units such as density were derived from these two base units.\n\nMesures usuelles (French for \"customary measurements\") were a system of measurement introduced as a compromise between the metric system and traditional measurements. It was used in France from 1812 to 1839.\n\nA number of variations on the metric system have been in use. These include gravitational systems, the centimetre–gram–second systems (cgs) useful in science, the metre–tonne–second system (mts) once used in the USSR and the metre–kilogram–second system (mks).\n\nThe current international standard metric system is the International System of Units (\"Système international d'unités\" or SI) It is an mks system based on the metre, kilogram and second as well as the kelvin, ampere, candela, and mole.\n\nThe SI includes two classes of units which are defined and agreed internationally. The first of these classes includes the seven SI base units for length, mass, time, temperature, electric current, luminous intensity and amount of substance. The second class consists of the SI derived units. These derived units are defined in terms of the seven base units. All other quantities (e.g. work, force, power) are expressed in terms of SI derived units.\n\nBoth imperial units and US customary units derive from earlier English units. Imperial units were mostly used in the former British Empire and the British Commonwealth, but in all these countries they have been largely supplanted by the metric system. They are still used for some applications in the United Kingdom but have been mostly replaced by the metric system in commercial, scientific, and industrial applications.\nUS customary units, however, are still the main system of measurement in the United States. While some steps towards metrication have been made (mainly in the late 1960s and early 1970s), the customary units have a strong hold due to the vast industrial infrastructure and commercial development.\n\nWhile imperial and US customary systems are closely related, there are a number of differences between them. Units of length and area (the inch, foot, yard, mile etc.) are identical except for surveying purposes. The Avoirdupois units of mass and weight differ for units larger than a pound (lb). The imperial system uses a stone of 14 lb, a long hundredweight of 112 lb and a long ton of 2240 lb. The stone is not used in the US and the hundredweights and tons are short: 100 lb and 2000 lb respectively.\n\nWhere these systems most notably differ is in their units of volume. A US fluid ounce (fl oz), about 29.6 millilitres (ml), is slightly larger than the imperial fluid ounce (about 28.4 ml). However, as there are 16 US fl oz to a US pint and 20 imp fl oz per imperial pint, the imperial pint is about 20% larger. The same is true of quarts, gallons, etc. Six US gallons are a little less than five imperial gallons.\n\nThe Avoirdupois system served as the general system of mass and weight. In addition to this there are the Troy and the Apothecaries' systems. Troy weight was customarily used for precious metals, black powder and gemstones. The troy ounce is the only unit of the system in current use; it is used for precious metals. Although the troy ounce is larger than its Avoirdupois equivalent, the pound is smaller. The obsolete troy pound was divided into 12 ounces, rather than the 16 ounces per pound of the Avoirdupois system. The Apothecaries' system was traditionally used in pharmacology, but has now been replaced by the metric system; it shared the same pound and ounce as the troy system but with different further subdivisions.\n\nNatural units are physical units of measurement defined in terms of universal physical constants in such a manner that selected physical constants take on the numerical value of one when expressed in terms of those units. Natural units are so named because their definition relies on only properties of nature and not on any human construct. Various systems of natural units are possible.\n\nSome other examples are as follows:\n\nNon-standard measurement units, sometimes found in books, newspapers etc., include:\n\n\n\nA unit of measurement that applies to money is called a unit of account in economics and unit of measure in accounting. This is normally a currency issued by a country or a fraction thereof; for instance, the US dollar and US cent ( of a dollar), or the euro and euro cent.\n\nISO 4217 is the international standard describing three letter codes (also known as the currency code) to define the names of currencies established by the International Organization for Standardization (ISO).\n\nThroughout history, many official systems of measurement have been used. While no longer in official use, some of these customary systems are occasionally used in day-to-day life, for instance in cooking.\n\n\n\n"}
{"id": "49535", "url": "https://en.wikipedia.org/wiki?curid=49535", "title": "Thought experiment", "text": "Thought experiment\n\nA thought experiment (, \"Gedanken-Experiment\", or \"Gedankenerfahrung\",) considers some hypothesis, theory, or principle for the purpose of thinking through its consequences. Given the structure of the experiment, it may not be possible to perform it, and even if it could be performed, there need not be an intention to perform it.\n\nThe common goal of a thought experiment is to explore the potential consequences of the principle in question:\n\nExamples of thought experiments include Schrödinger's cat, illustrating quantum indeterminacy through the manipulation of a perfectly sealed environment and a tiny bit of radioactive substance, and Maxwell's demon, which attempts to demonstrate the ability of a hypothetical finite being to violate the 2nd law of thermodynamics.\n\nThe ancient Greek δείκνυμι \"(transl.: deiknymi)\", or thought experiment, \"was the most ancient pattern of mathematical proof\", and existed before Euclidean mathematics, where the emphasis was on the conceptual, rather than on the experimental part of a thought-experiment. Perhaps the key experiment in the history of modern science is Galileo's demonstration that falling objects must fall at the same rate regardless of their masses. This is widely thought to have been a straightforward physical demonstration, involving climbing up the Leaning Tower of Pisa and dropping two heavy weights off it, whereas in fact, it was a logical demonstration, using the 'thought experiment' technique. The 'experiment' is described by Galileo in \"Discorsi e dimostrazioni matematiche\" (1638) (literally, 'Discourses and Mathematical Demonstrations') thus:\n\nAlthough the extract does not convey the elegance and power of the 'demonstration' terribly well, it is clear that it is a 'thought' experiment, rather than a practical one. Strange then, as Cohen says, that philosophers and scientists alike refuse to acknowledge either Galileo in particular, or the thought experiment technique in general for its pivotal role in both science and philosophy. (The exception proves the rule — the iconoclastic philosopher of science, Paul Feyerabend, has also observed this methodological prejudice.)\n\nInstead, many philosophers prefer to consider 'Thought Experiments' to be merely the use of a hypothetical scenario to help understand the way things are.\n\nThought experiments have been used in a variety of fields, including philosophy, law, physics, and mathematics. In philosophy, they have been used at least since classical antiquity, some pre-dating Socrates. In law, they were well-known to Roman lawyers quoted in the Digest. In physics and other sciences, notable thought experiments date from the 19th and especially the 20th century, but examples can be found at least as early as Galileo.\n\nJohann Witt-Hansen established that Hans Christian Ørsted was the first to use the Latin-German mixed term \"Gedankenexperiment\" (lit. thought experiment) circa 1812. Ørsted was also the first to use its entirely German equivalent, \"Gedankenversuch\", in 1820.\n\nMuch later, Ernst Mach used the term \"Gedankenexperiment\" in a different way, to denote exclusively the \"imaginary\" conduct of a \"real\" experiment that would be subsequently performed as a \"real physical experiment\" by his students. Physical and mental experimentation could then be contrasted: Mach asked his students to provide him with explanations whenever the results from their subsequent, real, physical experiment differed from those of their prior, imaginary experiment.\n\nThe English term \"thought experiment\" was coined (as a calque) from Mach's \"Gedankenexperiment\", and it first appeared in the 1897 English translation of one of Mach’s papers. Prior to its emergence, the activity of posing hypothetical questions that employed subjunctive reasoning had existed for a very long time (for both scientists and philosophers). However, people had no way of categorizing it or speaking about it. This helps to explain the extremely wide and diverse range of the application of the term \"thought experiment\" once it had been introduced into English.\n\nThought experiments, which are well-structured, well-defined hypothetical questions that employ subjunctive reasoning (irrealis moods) – \"What might happen (or, what might have happened) if . . . \" – have been used to pose questions in philosophy at least since Greek antiquity, some pre-dating Socrates. In physics and other sciences many thought experiments date from the 19th and especially the 20th Century, but examples can be found at least as early as Galileo.\n\nIn thought experiments we gain new information by rearranging or reorganizing already known empirical data in a new way and drawing new (a priori) inferences from them or by looking at these data from a different and unusual perspective. In Galileo’s thought experiment, for example, the rearrangement of empirical experience consists in the original idea of combining bodies of different weight.\n\nThought experiments have been used in philosophy (especially ethics), physics, and other fields (such as cognitive psychology, history, political science, economics, social psychology, law, organizational studies, marketing, and epidemiology). In law, the synonym \"hypothetical\" is frequently used for such experiments.\n\nRegardless of their intended goal, all thought experiments display a patterned way of thinking that is designed to allow us to explain, predict and control events in a better and more productive way.\n\nIn terms of their theoretical consequences, thought experiments generally:\n\nThought experiments can produce some very important and different outlooks on previously unknown or unaccepted theories. However, they may make those theories themselves irrelevant, and could possibly create new problems that are just as difficult, or possibly more difficult to resolve.\n\nIn terms of their practical application, thought experiments are generally created to:\n\nScientists tend to use thought experiments as imaginary, \"proxy\" experiments prior to a real, \"physical\" experiment (Ernst Mach always argued that these gedankenexperiments were \"a necessary precondition for physical experiment\"). In these cases, the result of the \"proxy\" experiment will often be so clear that there will be no need to conduct a physical experiment at all.\n\nScientists also use thought experiments when particular physical experiments are impossible to conduct (Carl Gustav Hempel labeled these sorts of experiment \"theoretical experiments-in-imagination\"), such as Einstein's thought experiment of chasing a light beam, leading to special relativity. This is a unique use of a scientific thought experiment, in that it was never carried out, but led to a successful theory, proven by other empirical means.\n\nThe relation to real experiments can be quite complex, as can be seen again from an example going back to Albert Einstein. In 1935, with two coworkers, he published a paper on a newly created subject called later the EPR effect (EPR paradox). In this paper, starting from certain philosophical assumptions, on the basis of a rigorous analysis of a certain, complicated, but in the meantime assertedly realizable model, he came to the conclusion that \"quantum mechanics should be described as \"incomplete\"\". Niels Bohr asserted a refutation of Einstein's analysis immediately, and his view prevailed. After some decades, it was asserted that feasible experiments could prove the error of the EPR paper. These experiments tested the Bell inequalities published in 1964 in a purely theoretical paper. The above-mentioned EPR philosophical starting assumptions were considered to be falsified by empirical fact (e.g. by the optical \"real experiments\" of Alain Aspect).\n\nThus \"thought experiments\" belong to a theoretical discipline, usually to theoretical physics, but often to theoretical philosophy. In any case, it must be distinguished from a real experiment, which belongs naturally to the experimental discipline and has \"the final decision on \"true\" or \"not true\"\", at least in physics.\n\nThe first characteristic pattern that thought experiments display is their orientation\nin time. They are either:\n\nThe second characteristic pattern is their movement in time in relation to “the present\nmoment standpoint” of the individual performing the experiment; namely, in terms of:\n\nGenerally speaking, there are seven types of thought experiments in which one reasons from causes to effects, or effects to causes:\n\n\"Prefactual (before the fact) thought experiments\" — the term prefactual was coined by Lawrence J. Sanna in 1998 — speculate on possible future outcomes, given the present, and ask \"What will be the outcome if event E occurs?\"\n\n\"Counterfactual (contrary to established fact) thought experiments\" — the term \"counterfactual\" was coined by Nelson Goodman in 1947, extending Roderick Chisholm's (1946) notion of a \"contrary-to-fact conditional\" — speculate on the possible outcomes of a different past; and ask \"What might have happened if A had happened instead of B?\" (e.g., \"If Isaac Newton and Gottfried Leibniz had cooperated with each other, what would mathematics look like today?\").\n\nThe study of counterfactual speculation has increasingly engaged the interest of scholars in a wide range of domains such as philosophy, psychology, cognitive psychology, history, political science, economics, social psychology, law, organizational theory, marketing, and epidemiology.\n\n\"Semifactual thought experiments\" — the term \"semifactual\" was coined by Nelson Goodman in 1947 — speculate on the extent to which things might have remained the same, despite there being a different past; and asks the question Even though X happened instead of E, would Y have still occurred? (e.g., Even if the goalie had moved left, rather than right, could he have intercepted a ball that was traveling at such a speed?).\n\nSemifactual speculations are an important part of clinical medicine.\n\nThe activity of prediction attempts to project the circumstances of the present into the future. According to David Sarewitz and Roger Pielke (1999, p123), scientific prediction takes two forms:\n\nAlthough they perform different social and scientific functions, the only difference between the qualitatively identical activities of \"predicting\", \"forecasting,\" and \"nowcasting\" is the distance of the speculated future from the present moment occupied by the user. Whilst the activity of nowcasting, defined as “a detailed description of the current weather along with forecasts obtained by extrapolation up to 2 hours ahead”, is essentially concerned with describing the current state of affairs, it is common practice to extend the term “to cover very-short-range forecasting up to 12 hours ahead” (Browning, 1982, p.ix).\n\nThe activity of hindcasting involves running a forecast model after an event has happened in order to test whether the model's simulation is valid.\n\nIn 2003, Dake Chen and his colleagues “trained” a computer using the data of the surface temperature of the oceans from the last 20 years. Then, using data that had been collected on the surface temperature of the oceans for the period 1857 to 2003, they went through a hindcasting exercise and discovered that their simulation not only accurately predicted every El Niño event for the last 148 years, it also identified the (up to 2 years) looming foreshadow of every single one of those El Niño events.\n\nThe activity of \"retrodiction\" (or \"postdiction\") involves moving backwards in time, step-by-step, in as many stages as are considered necessary, from the present into the speculated past to establish the ultimate cause of a specific event (e.g., reverse engineering and forensics).\n\nGiven that retrodiction is a process in which \"past observations, events and data are used as evidence to infer the process(es) the produced them\" and that diagnosis \"involve[s] going from visible effects such as symptoms, signs and the like to their prior causes\", the essential balance between prediction and retrodiction could be characterized as:\nregardless of whether the prognosis is of the course of the disease in the absence of treatment, or of the application of a specific treatment regimen to a specific disorder in a particular patient.\n\nThe activity of \"backcasting\" — the term \"backcasting\" was coined by John Robinson in 1982 — involves establishing the description of a very definite and very specific future situation. It then involves an imaginary moving backwards in time, step-by-step, in as many stages as are considered necessary, from the future to the present to reveal the mechanism through which that particular specified future could be attained from the present.\n\nBackcasting is not concerned with predicting the future:\n\nAccording to Jansen (1994, p. 503:\n\nIn philosophy, a thought experiment typically presents an imagined scenario with the intention of eliciting an intuitive or reasoned response about the way things are in the thought experiment. (Philosophers might also supplement their thought experiments with theoretical reasoning designed to support the desired intuitive response.) The scenario will typically be designed to target a particular philosophical notion, such as morality, or the nature of the mind or linguistic reference. The response to the imagined scenario is supposed to tell us about the nature of that notion in any scenario, real or imagined.\n\nFor example, a thought experiment might present a situation in which an agent intentionally kills an innocent for the benefit of others. Here, the relevant question is not whether the action is moral or not, but more broadly whether a moral theory is correct that says morality is determined solely by an action's consequences (See Consequentialism). John Searle imagines a man in a locked room who receives written sentences in Chinese, and returns written sentences in Chinese, according to a sophisticated instruction manual. Here, the relevant question is not whether or not the man understands Chinese, but more broadly, whether a functionalist theory of mind is correct.\n\nIt is generally hoped that there is universal agreement about the intuitions that a thought experiment elicits. (Hence, in assessing their own thought experiments, philosophers may appeal to \"what we should say,\" or some such locution.) A successful thought experiment will be one in which intuitions about it are widely shared. But often, philosophers differ in their intuitions about the scenario.\n\nOther philosophical uses of imagined scenarios arguably are thought experiments also. In one use of scenarios, philosophers might imagine persons in a particular situation (maybe ourselves), and ask what they would do.\n\nFor example, in the veil of ignorance, John Rawls asks us to imagine a group of persons in a situation where they know nothing about themselves, and are charged with devising a social or political organization. The use of the state of nature to imagine the origins of government, as by Thomas Hobbes and John Locke, may also be considered a thought experiment. Søren Kierkegaard explored the possible ethical and religious implications of Abraham's binding of Isaac in \"Fear and Trembling\" Similarly, Friedrich Nietzsche, in \"On the Genealogy of Morals\", speculated about the historical development of Judeo-Christian morality, with the intent of questioning its legitimacy.\n\nAn early written thought experiment was Plato's allegory of the cave. Another historic thought experiment was Avicenna's \"Floating Man\" thought experiment in the 11th century. He asked his readers to imagine themselves suspended in the air isolated from all in order to demonstrate human self-awareness and self-consciousness, and the substantiality of the soul.\n\nIn many thought experiments, the scenario would be nomologically possible, or possible according to the laws of nature. John Searle's Chinese room is nomologically possible.\n\nSome thought experiments present scenarios that are not nomologically possible. In his Twin Earth thought experiment, Hilary Putnam asks us to imagine a scenario in which there is a substance with all of the observable properties of water (e.g., taste, color, boiling point), but is chemically different from water. It has been argued that this thought experiment is not nomologically possible, although it may be possible in some other sense, such as metaphysical possibility. It is debatable whether the nomological impossibility of a thought experiment renders intuitions about it moot.\n\nIn some cases, the hypothetical scenario might be considered metaphysically impossible, or impossible in any sense at all. David Chalmers says that we can imagine that there are zombies, or persons who are physically identical to us in every way but who lack consciousness. This is supposed to show that physicalism is false. However, some argue that zombies are inconceivable: we can no more imagine a zombie than we can imagine that 1+1=3. Others have claimed that the conceivability of a scenario may not entail its possibility.\n\nThe philosophical work of Stefano Gualeni focuses on the use of virtual worlds to materialize thought experiments and to playfully negotiate philosophical ideas. His arguments were originally presented in his 2015 book \"Virtual Worlds as Philosophical Tools\".\n\nGualeni's argument is that the history of philosophy has, until recently, merely been the history of written thought, and digital media can complement and enrich the limited and almost exclusively linguistic approach to philosophical thought. He considers virtual worlds to be philosophically viable and advantageous in contexts like those of thought experiments, when the recipients of a certain philosophical notion or perspective are expected to objectively test and evaluate different possible courses of action, or in cases where they are confronted with interrogatives concerning non-actual or non-human phenomenologies .\n\nAmong the most visible thought experiments designed by Stefano Gualeni:\n\nOther examples of playful, interactive thought experiments:\n\n\n\n\n\n\n\n"}
{"id": "5615980", "url": "https://en.wikipedia.org/wiki?curid=5615980", "title": "Tree of knowledge system", "text": "Tree of knowledge system\n\nThe tree of knowledge (ToK) system is a theoretical approach to the unification of psychology developed by Gregg Henriques, associate professor and director of the Combined-Integrated Doctoral Program in Clinical and School Psychology at James Madison University.\n\nThe outline of the system was published in 2003 in \"Review of General Psychology\". Two special issues of the \"Journal of Clinical Psychology\" in December 2004 and January 2005 were devoted to the elaboration and evaluation of the model. The latest evaluation of this model appeared in a December 2008 special issue of \"Theory & Psychology''.\n\nThe official website on the tree of knowledge system claims that the ToK is\n\nHenriques argues that the most difficult problem in psychology as a discipline is that while there is incredible diversity offered by different approaches to psychology, there is no consensus model of what \"psychology\" actually is.\nAccording to the ToK system, the \"problem of psychology\", (as Henriques puts it), is that a clear definition, an agreed upon subject matter, and a coherent conceptual framework have eluded its students for its entire history. He further argues that the patent tendency of psychology has been toward theoretical and substantial fragmentation and increasing insularity among the \"specialties.\" In other words, the discipline has fragmented into different schools of thought and methodology, with no overall framework to interpret and integrate the research of different areas. At its best, the different approaches are a strength of psychology; different approaches lead to novel ideas, and prevent psychologists from clinging to a paradigm that fails to explain a phenomenon. At its worst, adherents of one particular school cling to their beliefs concerning the relative importance of their research and disregard or are ignorant of different approaches. In most cases, individual psychologists have to determine for themselves which elements of which perspective to apply, and how to integrate them into their overall understanding.\nThe reason for psychology's fragmentation, according to the ToK, is that there has been no meta-theoretical frame that allows scholars to agree on the basic questions that need to be addressed. As such, the different schools of thought in psychology are like the blind men who each grab a part of the elephant and proclaim they have discovered its true nature. With its novel depiction of evolving dimensions of complexity, the ToK allows scholars finally to see the elephant. In his 2003 \"Review of General Psychology\" paper, Henriques used the ToK System with the attempt to clarify and align the views of B.F. Skinner and Sigmund Freud. These luminaries were chosen because when one considers their influence and historical opposition, it can readily be argued that they represent two schools of thought that are the most difficult to integrate. Henriques used the meta-perspective offered by the ToK to argue how one can retain the key insights from each school of thought, identify errors and points of confusion, and integrate the insights into a coherent whole.\n\nCultural and personality psychologist, Michael Katzko, however critiques Henriques' position on \"the problem of psychology\":\n\nIn one way, the tree of knowledge system reflects a fairly common hierarchy of nature and of the sciences that has been represented in one way or another since the time of Auguste Comte, who in the 19th century used a hierarchical conception of nature to argue for the existence of sociology. Despite its surface agreement with a standard conception, the ToK System offers a set of ideas that have added implications for both ontology and epistemology. The ontological claim made by the ToK, (and depicted pictorially above), is that cosmic evolution consists of four separable dimensions of complexity, namely matter, life, mind, and culture. The dimension of complexity argument is arguably one of the most complicated aspects of the system. Many have argued nature is hierarchically leveled; for example, a list of such levels might be subatomic particles, atoms, molecules, cells, organ structures, multi-celled organisms, consciousness, and society is common. The ToK System embraces a view of nature as levels, but adds the notion that there are also \"dimensions of complexity\". The difference can be seen pictorially. A view of nature as solely consisting of levels would have a single \"cone\" of complexity, whereas the ToK depicts four \"cones\". The ToK posits that a separate dimension of complexity emerges when a process of selection operates on a unit of information. Thus, according to the ToK, natural selection operating on genetic combinations gives rise to the dimension of Life; behavioral selection operating on neuronal combinations gives rise to the dimension of Mind; and justification operating on symbolic combinations gives rise to Culture.\n\nThe ToK system also offers a new epistemology that Henriques believes will move toward what E.O. Wilson termed \"consilience\". Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. The ToK offers alternative perspectives on how knowledge is obtained because it depicts science itself as both emerging out of culture and as a unique type of \"justification system\" that is based on the values of accuracy and objectivity. A \"justification system\", according to Henriques, refers to any belief system that emerges that coordinates the behaviors of individual humans to human populations. The four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nThe dimension of matter refers to the set of material objects and their behaviors through time. In accordance with modern cosmology, matter is theorized to have emerged from a pure energy singularity at the Big Bang. Space and time were also born at such a point. Nonliving material objects range in complexity from subatomic particles to large organic molecules. The physical sciences (i.e., physics, chemistry, geology, astronomy) describe the behavior of material objects.\n\nThe dimension of life refers to organisms and their behaviors through time. Living objects are considered a unique subset of material objects. Just as quantum particles form the fundamental units of material complexity, genes are the fundamental units of living information. Although many questions about the emergence of life remain unanswered, in accordance with modern biology, the ToK posits that natural selection operating on genetic combinations through time is the unified theory of biology and forms the foundational understanding for the emergence of organic complexity.\n\nMind/cognition in the ToK system refers to the set of mental behaviors. \"Mental behaviors\" are behaviors of animals mediated by the nervous system that produce a functional effect on the animal-environment relationship. As such, Mind/cognition is essentially synonymous with what behavioral psychologists have meant when they use the term behavior. Thus, a fly avoiding a fly swatter, a rat pushing a bar or a human getting a drink of water are all mental behaviors. Mind is not synonymous with sentience or the capacity for mental experience, although such processes are presumed to emerge in the mental/cognitive dimension. Cognition, in the broad sense of the term is meaning bodily-neuro-social information processing, as in EEEE Cognition: Embodied, Embedded, Enactive, Extended. While cognitive science stands for naturalist study of mind, psychology is an approach grounded in the tradition of humanities, especially philosophy. Thus, by defining mind as mental behavior, Henriques argues that the ToK System provides a way to bridge the epistemological differences between cognitive and behavioral science.\n\nCulture in the ToK system refers to the set of sociolinguistic behaviors, which range from large scale nation states to individual human justifications for particular actions. Just as genetic information processing is associated with the Life dimension and neuronal information processing associated with the Mind dimension, symbolic information processing emerges with the Cultural dimension.\n\nQuantum gravity refers to the imagined merger between the twin pillars of physical science which are quantum mechanics, the study of the microscopic (e.g., electrons), and general relativity, the science of the macroscopic (e.g., galaxies). Currently, these two great domains of science cannot be effectively interwoven into a single, physical Theory of Everything. Yet progress is being made, most notably through string theory, loop quantum gravity, black hole thermodynamics and the study of the early universe. Some of the difficulties combining these two pillars of physical science are philosophical in nature and it is possible that the macro view of knowledge offered by the ToK may eventually aid in the construction of a coherent theory of quantum gravity. The reason the ToK might help is that it locates scientific knowledge in relationship to the physical universe.\n\nThe modern synthesis refers to the merger of genetics with natural selection which occurred in the 1930s and 1940s and offers a reasonably complete framework for understanding the emergence of biological complexity. Although there remain significant gaps in biological knowledge surrounding questions such as the origin of life and the emergence of sexual reproduction, the modern synthesis represents the most complete and well-substantiated joint point.\n\nBehavioral investment theory (BIT) is proposed as a merger of the selection science of behaviorism with the information science of cognitive neuroscience (notice the parallel with the modern synthesis). BIT posits that the nervous system evolved as an increasingly flexible computational control system that coordinates the behavioral expenditure of energy of the animal as a whole. Expenditure of behavioral energy is theorized to be computed on an investment value system built evolutionarily through natural selection operating on genetic combinations and ontogenetically through behavioral selection operating on neural combinations. As such, the current behavioral investments of the animal are conceptualized as the joint product of the two vectors of phylogeny and ontogeny. A unique element of BIT is that it finds a core of agreement and builds bridges between five brain-behavior paradigms: (1) cognitive science; (2) behavioral science; (3) evolutionary theory and genetics; (4) neuroscience; and (5) cybernetics/systems theory.\n\nDavid C. Geary noted the similarities between his \"motive-to-control\" hypothesis and Henriques' Behavioral Investment Theory, which were developed independently of each other. Furthermore, Geary suggested that his model \"seem[ed] to fill in many of the proximate mechanisms and evolutionary pressures that define the life-mind joint point, and provided a framework for further development of the mind-culture joint point.\"\n\nThe justification hypothesis (JH) is a novel proposal that allows for both the understanding of the evolution of culture and for identifying what makes humans distinct animals. A basic initial claim of the JH is that the process of justification is a crucial component of human mental behavior at both the individual and societal level. Unlike all other animals, humans everywhere ask for and give explanations for their actions. Arguments, debates, moral dictates, rationalizations, and excuses all involve the process of explaining why one's claims, thoughts or actions are warranted. In virtually every form of social exchange, from warfare to politics to family struggles to science, humans are constantly justifying their behavioral investments to themselves and others.\n\nThe JH can be stated succinctly as follows: The evolution of language gave rise to the problem of justification, and this evolutionary pressure ultimately resulted in the human self-consciousness system and human culture. The JH carries with it three fundamental postulates. \n\nThe problem of psychology, according to the ToK, is its conceptual incoherence, which Henriques identifies by the following: \n\nWhen the various conceptions of psychology (e.g., behavioral, humanistic, cognitive) are viewed through the lens of the ToK System, psychology spans two different dimensions of complexity: the mental and the cultural. In other words, the discipline has historically spanned two fundamentally separate problems: \nIf, as previously thought, nature simply consisted of levels of complexity, psychology would not be crisply defined in relationship to biology or the social sciences. And, indeed, it is frequently suggested that psychology exists in an amorphous space between biology and the social sciences. However, with its dimension of complexity depiction, the ToK System suggests that psychology can be crisply defined as the science of mind, which is the third dimension of complexity. Furthermore, because human behavior exists in the fourth dimension, psychology must be divided into two broad scientific domains of \n\n\"Psychological formalism\" is defined as the science of mind and corresponds to the behavior of animal objects. \"Human psychology\" is considered to be a unique subset of psychological formalism that deals with human behavior at the level of the individual. Because human behavior is immersed in the larger socio-cultural context (level four in the ToK System), human psychology is considered a hybrid discipline that merges the pure science of psychology with the social sciences. It is important to point out that there are other disciplines the ToK System would classify as “hybrids.” Molecular genetics, for example, is a hybrid between chemistry and biology and neuroscience is a hybrid between biology and psychology. As with Henriques' proposed conception of human psychology, both of these disciplines adopt an object level perspective (molecular and cellular, respectively) on phenomena that simultaneously exist as part of meta-level system processes (life and mind, respectively).\n\nThough David A. F. Haaga \"congratulate[d] Dr. Henriques' ambitious, scholarly, provocative paper\", and \"found the Tree of Knowledge taxonomy, the theoretical joint points, the evolutionary history, and the levels of emergent properties highly illuminating\", he asks the rhetorical questions, \n\nIn a similar vein, Scott O. Lilienfeld, who described Henriques' effort as \"thoughtful\", contended that psychology is \"an inherently fuzzy concept that resists precise definition\" and that \"attempts to define psychology [would be] likely to hamper rather than foster consilience across disciplines\". Lilienfield went on further to suggest that the scientist-practitioner gap in psychology lies not in definitional issues, but in different \"epistemic attitudes\" between these two groups. He stated that scientists have an epistemic attitude of empiricism, (where questions regarding human nature are settled by scientific evidence), and that practitioners have an epistemic attitude of romanticism, (where questions of human nature are settled by intuition). Lilienfeld suggested that the solution to the scientist-practitioner gulf isn't definitional, but in \"train[ing] future clinical scientists to appreciate the proper places of romanticism and empiricism within science\".\n\nA frequent question and point of confusion in the ToK System is the definition and meaning of consciousness. As mentioned above, mind is not synonymous with consciousness. And, to understand consciousness from a ToK vantage point, it is crucial to recognize that the term is often ambiguous in its meaning. Two primary meanings are sentience, which is the capacity for mental experience and self-awareness, which is the capacity to be aware of one's awareness. Sentience is conceptualized as a \"level 3\" phenomena, possessed by many animals other than humans and is defined as a \"perceived\" electro-neuro-chemical representation of animal-environment relations. The ingredient of neurological behavior that allows for the emergence of mental experience is considered the \"hard\" problem of conscious and the ToK System does not address this question explicitly. In contrast, through the Justification Hypothesis (see below), the ToK System involves a very direct analysis of the other issue of consciousness, that of self-awareness. \nAnother frequent question that is raised is \"Where does individual human behavior fall on the ToK?\" To analyze human behavior from the context of the ToK, one uses the ToK like a prism to separate the dimensions of behavior into physiochemical, biogenetic, neuropsychological and sociolinguistic. Thus if we imagine a conversation between a husband and wife as follows:\n\nThe words represent the sociolinguistic dimension and are understood as a function of justification. Justification systems are seen both at the level of individual, micro-social and societal (i.e., the context of justification in which men work and women stay at home). The actions of the husband and wife in terms of facial expression, body movement, etc. are seen as the mental dimension and are understood as a function of behavioral investment. The physiological make up of the organ systems and cells of each body is seen as the biogenetic dimension. Finally, the position, temperature, molecular make up is seen as the physiochemical dimension. Each of the more basic dimensions represent conditions of possibility that allow for the emergence of the higher dimension of process. Thus, insufficient oxygen disrupts organic processes which in turn renders neuropsychological and sociolinguistic processes impossible.\n\nAs stated above, the ToK System proposes a new epistemology with the goal of moving academic knowledge toward what E.O. Wilson termed consilience. Consilience is the interlocking of fact and theory into a coherent, holistic view of knowledge. Henriques argues that the ToK affords new perspectives on how knowledge is obtained because it depicts how science emerges from culture and that the four dimensions of complexity correspond to four broad classes of science: the physical, biological, psychological and social sciences.\n\nHenriques further argues that developing such a system for integrating knowledge is not just an academic enterprise. He suggests that in an increasingly complex world, the fragmented state of knowledge can be seen as one of the most pressing social problems of our time. Henriques also believes that history seems to attest that the absence of a collective worldview ostensibly condemns humanity to an endless series of conflicts that inevitably stem from incompatible, partially correct, locally situated justification systems. Thus, from Henriques' perspective, there are good reasons for believing that if there was a shared, general background of explanation, humanity might be able to achieve much greater levels of harmonious relations.\nIn a 2008 article on the ToK, Henriques cites Oliver Reiser's 1958 call for unifying scientific knowledge that Henriques implies is similar in theme to the ToK:\n\nWith its depiction of the dimensions of complexity and interlocking theoretical joint points, Henriques' believes that his ToK System offers new avenues that might allow scholars to meet Reiser’s call for academic synthesis. Henriques, like Reiser, believes that with a shared sense of purpose and a common background of explanation, people might yet be able to integrate bodies of knowledge into a unified interpretation of humanity, with humanity's place in nature and its potentialities for creating the good society.\n\n\n"}
{"id": "491097", "url": "https://en.wikipedia.org/wiki?curid=491097", "title": "Variational principle", "text": "Variational principle\n\nA variational principle is a scientific principle used within the calculus of variations, which develops general methods for finding functions which extremize the value of quantities that depend upon those functions. For example, to answer this question: \"What is the shape of a chain suspended at both ends?\" we can use the variational principle that the shape must minimize the gravitational potential energy.\n\nAny physical law which can be expressed as a variational principle describes a self-adjoint operator. These expressions are also called Hermitian. Such an expression describes an invariant under a Hermitian transformation.\n\nFelix Klein's Erlangen program attempted to identify such invariants under a group of transformations. In what is referred to in physics as Noether's theorem, the Poincaré group of transformations (what is now called a gauge group) for general relativity defines symmetries under a group of transformations which depend on a variational principle, or action principle.\n\n\n"}
{"id": "449568", "url": "https://en.wikipedia.org/wiki?curid=449568", "title": "−1", "text": "−1\n\nIn mathematics, −1 is the additive inverse of 1, that is, the number that when added to 1 gives the additive identity element, 0. It is the negative integer greater than negative two (−2) and less than 0.\n\nNegative one bears relation to Euler's identity since \"e\" = −1.\n\nIn software development, −1 is a common initial value for integers and is also used to show that a variable contains no useful information.\n\nIn programming languages, −1 can be used to index the last (or 2nd last) item of an array, depending on whether 0 or 1 represents the first item.\n\nNegative one has some similar but slightly different properties to positive one.\n\nMultiplying a number by −1 is equivalent to changing the sign on the number. This can be proved using the distributive law and the axiom that 1 is the multiplicative identity: for \"x\" real, we have\n\nwhere we used the fact that any real \"x\" times 0 equals 0, implied by cancellation from the equation\n\nIn other words,\n\nso (−1) · \"x\", or −\"x\", is the arithmetic inverse of \"x\".\n\nThe square of −1, i.e. −1 multiplied by −1, equals 1. As a consequence, a product of two negative real numbers is positive.\n\nFor an algebraic proof of this result, start with the equation\n\nThe first equality follows from the above result. The second follows from the definition of −1 as additive inverse of 1: it is precisely that number that when added to 1 gives 0. Now, using the distributive law, we see that\n\nThe second equality follows from the fact that 1 is a multiplicative identity. But now adding 1 to both sides of this last equation implies\n\nThe above arguments hold in any ring, a concept of abstract algebra generalizing integers and real numbers.\n\nAlthough there are no real square roots of -1, the complex number \"i\" satisfies \"i\" = −1, and as such can be considered as a square root of −1. The only other complex number who's square is 1 is −\"i\". In the algebra of quaternions, which contain the complex plane, the equation \"x\" = −1 has infinite solutions.\n\nExponentiation of a non-zero real number can be extended to negative integers. We make the definition that \"x\" = , meaning that we define raising a number to the power −1 to have the same effect as taking its reciprocal. This definition is then extended to negative integers preserves the exponential law \"x\"\"x\" = \"x\" for real numbers \"a\" and \"b\".\n\nExponentiation to negative integers can be extended to invertible elements of a ring, by defining \"x\" as the multiplicative inverse of \"x\".\n\n−1 that appears next to functions or matrices does not mean raising them to the power −1 but their inverse functions or inverse matrices. For example, \"f\"(\"x\") is the inverse of \"f\"(\"x\"), or sin(\"x\") is a notation of arcsine function.\n\nMost computer systems represent negative integers using two's complement. In such systems, −1 is represented using a bit pattern of all ones. For example, an 8-bit signed integer using two's complement would represent −1 as the bitstring \"11111111\", or \"FF\" in hexadecimal (base 16). If interpreted as an unsigned integer, the same bitstring of \"n\" ones represents 2 − 1, the largest possible value that \"n\" bits can hold. For example, the 8-bit string \"11111111\" above represents 2 − 1 = 255.\n\nIn some programming languages, when used to index some data types (such as an array), then -1 can be used to identify the very last (or 2nd last) item, depending on whether 0 or 1 represents the first item.\nIf the first item is indexed by 0, then -1 identifies the last item.\nIf the first item is indexed by 1, then -1 identifies the second-to-last item.\n"}
