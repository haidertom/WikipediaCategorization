{"id": "34460605", "url": "https://en.wikipedia.org/wiki?curid=34460605", "title": "Andersen healthcare utilization model", "text": "Andersen healthcare utilization model\n\nThe Andersen Healthcare Utilization Model - is a conceptual model aimed at demonstrating the factors that lead to the use of health services. According to the model, usage of health services (including inpatient care, physician visits, dental care etc.) is determined by three dynamics: predisposing factors, enabling factors, and need. Predisposing factors can be characteristics such as race, age, and health beliefs. For instance, an individual who believes health services are an effective treatment for an ailment is more likely to seek care. Examples of enabling factors could be family support, access to health insurance, one's community etc. Need represents both perceived and actual need for health care services. The original model was developed by Ronald M. Andersen, a health services professor at UCLA, in 1968. The original model was expanded through numerous iterations and its most recent form models past the use of services to end at health outcomes and includes feedback loops.\n\nA major motivation for the development of the model was to offer measures of access. Andersen discusses four concepts within access that can be viewed through the conceptual framework. Potential access is the presence of enabling resources, allowing the individual to seek care if needed. Realized access is the actual use of care, shown as the outcome of interest in the earlier models. The Andersen framework also makes a distinction between equitable and inequitable access. Equitable access is driven by demographic characteristics and need whereas inequitable access is a result of social structure, health beliefs, and enabling resources.\n\nAndersen also introduces the concept of mutability of his factors. The idea here being that if a concept has a high degree of mutability (can be easily changed) perhaps policy would be justified in using its resources to do rather than a factor with low mutability. Characteristics that fall under demographics are quite difficult to change, however, enabling resources is assigned a high degree of mutability as the individual, community, or national policy can take steps to alter the level of enabling resources for an individual. For example, if the government decides to expand the Medicaid program an individual may experience an increase in enabling resources, which in turn may beget an increase in health services usage. The RAND Health Insurance Experiment (HIE) changed a highly mutable factor, out-of-pocket costs, which greatly changed individual rates of health services usage.\n\nThe initial behavior model was an attempt to study of why a family uses health services. However, due to the heterogeneity of family members the model focused on the individual rather than the family as the unit of analysis. Andersen also states that the model functions both to predict and explain use of health services.\n\nA second model was developed in the 1970s in conjunction with Aday and colleagues at the University of Chicago. This iteration includes systematic concepts of health care such as current policy, resources, and organization. The second generation model also extends the outcome of interest beyond utilization to consumer satisfaction.\n\nThe next generation of the model builds upon this idea by including health status (both perceived and evaluated) as outcomes alongside consumer satisfaction. Furthermore, this model include personal health practices as an antecedent to outcomes, acknowledging that it not solely use of health services that drives health and satisfaction. This model emphasizes a more public health approach of prevention, as advocated by Evans and Stoddart wherein personal health practices (i.e. smoking, diet, exercise) are included as a driving force towards health outcomes.\n\nThe 6th iteration of Andersen’s conceptual framework focuses on the individual as the unit of analysis and goes beyond health care utilization, adopting health outcomes as the endpoint of interest. This model is further differentiated from its predecessors by using a feedback loop to illustrate that health outcomes may affect aspects such as health beliefs, and need. It added genetic susceptibility as a predisposing determinant and quality of life as an outcome By using the framework’s relationships we can determine the directionality of the effect following a change in an individual’s characteristics or environment. For example, if one experiences an increase in need as a result of an infection, the Andersen model predicts this will lead to an increased use of services (all else equal). One potential change for a future iteration of this model is to add genetic information under predisposing characteristics. As genetic information becomes more readily available it seems likely this could impact health services usage, as well as health outcomes, beyond what is already accounted for in the current model.\n\nThe model has been criticized for not paying enough attention to culture and social interaction but Andersen argues this social structure is included in the \"predisposing characteristics\" component. Another criticism was the overemphasis of need and at the expense of health beliefs and social structure. However, Andersen argues need itself is a social construct. This is why need is split into perceived and evaluated. Where evaluated need represents a more measurable/objective need, perceived need is partly determined by health beliefs, such as whether or not people think their condition is serious enough to seek health services. Another limitation of the model is its emphasis on health care utilization or adopting health outcomes as a dichotomous factor, present or not present. Other help-seeking models also consider the type of help source, including informal sources. More recent work has taken help-seeking behaviors further, and more real-world, by including online and other non-face-to-face sources.\n"}
{"id": "9305546", "url": "https://en.wikipedia.org/wiki?curid=9305546", "title": "Anxiety of influence", "text": "Anxiety of influence\n\nAnxiety of Influence is a type of literary criticism established by Harold Bloom in 1973, in his book, \"\". It refers to the psychological struggle of aspiring authors to overcome the anxiety posed by the influence of their literary antecedents.\n\nThe theory of anxiety of influence is a theory applied principally to early nineteenth century romantic poetry. Its author, Harold Bloom, maintains that the theory has general applicability to the study of literary tradition, ranging from Homer and the Bible to Thomas Pynchon and Anne Carson in the 20th and 21st century. It is based primarily on Bloom's belief that there is no such thing as an original poem, that every new composition is simply a misreading or misinterpretation of an earlier poem and that influence is unavoidable and inescapable; all writers inevitably, to some degree, adopt, manipulate or alter and assimilate certain aspects of the content or subject matter, literary style or form from their predecessors. To illustrate his point, Bloom quotes Oscar Wilde, noting that:\n\n\"Influence is simply transference of personality, a mode of giving away what is most precious to one's self, and its exercise produces a sense, and, it may be, a reality of loss. Every disciple takes away something from his master.\"\n\nContrary to universally-held beliefs that the literary influence of precursors can provide a purely benevolent, inspirational platform for young, aspiring writers; Bloom deems that this influence provides entirely the opposite effect. He contends that it proves detrimental; that this influence instills in young writers a type of unease, apprehension or anxiety as they psychologically struggle against past literary forebears to create something definitive and original and achieve literary recognition and success. Bloom equates this struggle to the Freudian family drama; particularly to the Oedipal complex and relationship of son to father, where the emerging writer is cast as the 'son' in a battle against the 'father'; a literary precursor. Bloom claims that this forces poets or authors into a type of 'creative misprision'; where they must distort the works of their literary masters in an attempt to create something revolutionary and innovative. The authors who triumph over this struggle are deemed 'strong', with some even receiving acclaim to the extent where the contemporary has the potential to transcend time, with some literary predecessors being read in terms of their contemporary successor. In contrast, writers who cannot prevail over this anxiety of influence are deemed 'weak', with their works regarded as markedly derivative and reminiscent of the works of earlier literary masters. Some great poets in which Bloom draws upon, who have overcome this anxiety of influence include Wordsworth, Shelley and Wallace Stevens.\n\nIn order to determine the effects of this theory on literature, Bloom has established the six \"revisionary ratios\", which are occasionally based on the model of Freud's defense mechanisms. These ratios show the developmental stages of anxiety of influence in relation to the ways a poet or author misreads and deforms the work of a literary precursor in the composition of a literary text.\n\n\n"}
{"id": "3162", "url": "https://en.wikipedia.org/wiki?curid=3162", "title": "Arbitrage", "text": "Arbitrage\n\nIn economics and finance, arbitrage (, ) is the practice of taking advantage of a price difference between two or more markets: striking a combination of matching deals that capitalize upon the imbalance, the profit being the difference between the market prices. When used by academics, an arbitrage is a (imagined, hypothetical, thought experiment) transaction that involves no negative cash flow at any probabilistic or temporal state and a positive cash flow in at least one state; in simple terms, it is the possibility of a risk-free profit after transaction costs. For example, an arbitrage opportunity is present when there is the opportunity to instantaneously buy something for a low price and sell it for a higher price.\n\nIn principle and in academic use, an arbitrage is risk-free; in common use, as in statistical arbitrage, it may refer to \"expected\" profit, though losses may occur, and in practice, there are always risks in arbitrage, some minor (such as fluctuation of prices decreasing profit margins), some major (such as devaluation of a currency or derivative). In academic use, an arbitrage involves taking advantage of differences in price of a \"single\" asset or \"identical\" cash-flows; in common use, it is also used to refer to differences between \"similar\" assets (relative value or convergence trades), as in merger arbitrage.\n\nPeople who engage in arbitrage are called arbitrageurs —such as a bank or brokerage firm. The term is mainly applied to trading in financial instruments, such as bonds, stocks, derivatives, commodities and currencies.\n\nArbitrage has the effect of causing prices of the same or very similar assets in different markets to converge.\n\n\"Arbitrage\" is a French word and denotes a decision by an arbitrator or arbitration tribunal. (In modern French, \" usually means referee or umpire.) In the sense used here, it was first defined in 1704 by Mathieu de la Porte in his treatise \" as a consideration of different exchange rates to recognise the most profitable places of issuance and settlement for a bill of exchange (\" [, in modern spelling] \".)\n\nIf the market prices do not allow for profitable arbitrage, the prices are said to constitute an arbitrage equilibrium, or an arbitrage-free market. An arbitrage equilibrium is a precondition for a general economic equilibrium. The \"no arbitrage\" assumption is used in quantitative finance to calculate a unique risk neutral price for derivatives.\n\nThis refers to the method of valuing a coupon-bearing financial instrument by discounting its future cash flows by multiple discount rates. By doing so, a more accurate price can be obtained than if the price is calculated with a present-value pricing approach. Arbitrage-free pricing is used for bond valuation and to detect arbitrage opportunities for investors.\n\nFor the purpose of valuing the price of a bond, its cash flows can each be thought of as packets of incremental cash flows with a large packet upon maturity, being the principal. Since the cash flows are dispersed throughout future periods, they must be discounted back to the present. In the present-value approach, the cash flows are discounted with one discount rate to find the price of the bond. In arbitrage-free pricing, multiple discount rates are used.\n\nThe present-value approach assumes that the yield of the bond will stay the same until maturity. This is a simplified model because interest rates may fluctuate in the future, which in turn affects the yield on the bond. The discount rate may be different for each of the cash flows for this reason. Each cash flow can be considered a zero-coupon instrument that pays one payment upon maturity. The discount rates used should be the rates of multiple zero-coupon bonds with maturity dates the same as each cash flow and similar risk as the instrument being valued. By using multiple discount rates, the arbitrage-free price is the sum of the discounted cash flows. Arbitrage-free price refers to the price at which no price arbitrage is possible.\n\nThe ideas of using multiple discount rates obtained from zero-coupon bonds and discount a similar bonds cash flow to find its price is derived from the yield curve. The yield curve is a curve of the yields of the same bond with different maturities. This curve can be used to view trends in market expectations of how interest rates will move in the future. In arbitrage-free pricing of a bond, a yield curve of similar zero-coupon bonds with different maturities is created. If the curve were to be created with Treasury securities of different maturities, they would be stripped of their coupon payments through bootstrapping. This is to transform the bonds into zero-coupon bonds. The yield of these zero-coupon bonds would then be plotted on a diagram with time on the \"x\"-axis and yield on the \"y\"-axis.\n\nSince the yield curve displays market expectations on how yields and interest rates may move, the arbitrage-free pricing approach is more realistic than using only one discount rate. Investors can use this approach to value bonds and find mismatches in prices, resulting in an arbitrage opportunity. If a bond valued with the arbitrage-free pricing approach turns out to be priced higher in the market, an investor could have such an opportunity:\n\nIf the outcome from the valuation were the reversed case, the opposite positions would be taken in the bonds. This arbitrage opportunity comes from the assumption that the prices of bonds with the same properties will converge upon maturity. This can be explained through market efficiency, which states that arbitrage opportunities will eventually be discovered and corrected accordingly. The prices of the bonds in t move closer together to finally become the same at t.\n\nArbitrage is possible when one of three conditions is met:\n\n\nArbitrage is not simply the act of buying a product in one market and selling it in another for a higher price at some later time. The transactions must occur \"simultaneously\" to avoid exposure to market risk, or the risk that prices may change on one market before both transactions are complete. In practical terms, this is generally possible only with securities and financial products that can be traded electronically, and even then, when each leg of the trade is executed the prices in the market may have moved. Missing one of the legs of the trade (and subsequently having to trade it soon after at a worse price) is called 'execution risk' or more specifically 'leg risk'.\n\nIn the simplest example, any good sold in one market should sell for the same price in another. Traders may, for example, find that the price of wheat is lower in agricultural regions than in cities, purchase the good, and transport it to another region to sell at a higher price. This type of price arbitrage is the most common, but this simple example ignores the cost of transport, storage, risk, and other factors. \"True\" arbitrage requires that there is no market risk involved. Where securities are traded on more than one exchange, arbitrage occurs by simultaneously buying in one and selling on the other.\n\nSee rational pricing, particularly § arbitrage mechanics, for further discussion.\n\nMathematically it is defined as follows:\n\nwhere formula_2 and formula_3 denotes the portfolio value at time \"t\".\n\nArbitrage has the effect of causing prices in different markets to converge. As a result of arbitrage, the currency exchange rates, the price of commodities, and the price of securities in different markets tend to converge. The speed at which they do so is a measure of market efficiency. Arbitrage tends to reduce price discrimination by encouraging people to buy an item where the price is low and resell it where the price is high (as long as the buyers are not prohibited from reselling and the transaction costs of buying, holding and reselling are small relative to the difference in prices in the different markets).\n\nArbitrage moves different currencies toward purchasing power parity. As an example, assume that a car purchased in the United States is cheaper than the same car in Canada. Canadians would buy their cars across the border to exploit the arbitrage condition. At the same time, Americans would buy US cars, transport them across the border, then sell them in Canada. Canadians would have to buy American dollars to buy the cars and Americans would have to sell the Canadian dollars they received in exchange. Both actions would increase demand for US dollars and supply of Canadian dollars. As a result, there would be an appreciation of the US currency. This would make US cars more expensive and Canadian cars less so until their prices were similar. On a larger scale, international arbitrage opportunities in commodities, goods, securities and currencies tend to change exchange rates until the purchasing power is equal.\n\nIn reality, most assets exhibit some difference between countries. These, transaction costs, taxes, and other costs provide an impediment to this kind of arbitrage. Similarly, arbitrage affects the difference in interest rates paid on government bonds issued by the various countries, given the expected depreciation in the currencies relative to each other (see interest rate parity).\n\nArbitrage transactions in modern securities markets involve fairly low day-to-day risks, but can face extremely high risk in rare situations, particularly financial crises, and can lead to bankruptcy. Formally, arbitrage transactions have negative skew – prices can get a small amount closer (but often no closer than 0), while they can get very far apart. The day-to-day risks are generally small because the transactions involve small differences in price, so an execution failure will generally cause a small loss (unless the trade is very big or the price moves rapidly). The rare case risks are extremely high because these small price differences are converted to large profits via leverage (borrowed money), and in the rare event of a large price move, this may yield a large loss.\n\nThe main day-to-day risk is that part of the transaction fails – execution risk. The main rare risks are counterparty risk and liquidity risk – that a counterparty to a large transaction or many transactions fails to pay, or that one is required to post margin and does not have the money to do so.\n\nIn the academic literature, the idea that seemingly very low risk arbitrage trades might not be fully exploited because of these risk factors and other considerations is often referred to as limits to arbitrage.\n\nGenerally it is impossible to close two or three transactions at the same instant; therefore, there is the possibility that when one part of the deal is closed, a quick shift in prices makes it impossible to close the other at a profitable price. However, this is not necessarily the case. Many exchanges and inter-dealer brokers allow multi legged trades (e.g. basis block trades on LIFFE).\n\nCompetition in the marketplace can also create risks during arbitrage transactions. As an example, if one was trying to profit from a price discrepancy between IBM on the NYSE and IBM on the London Stock Exchange, they may purchase a large number of shares on the NYSE and find that they cannot simultaneously sell on the LSE. This leaves the arbitrageur in an unhedged risk position.\n\nIn the 1980s, risk arbitrage was common. In this form of speculation, one trades a security that is clearly undervalued or overvalued, when it is seen that the wrong valuation is about to be corrected by events. The standard example is the stock of a company, undervalued in the stock market, which is about to be the object of a takeover bid; the price of the takeover will more truly reflect the value of the company, giving a large profit to those who bought at the current price—if the merger goes through as predicted. Traditionally, arbitrage transactions in the securities markets involve high speed, high volume and low risk. At some moment a price difference exists, and the problem is to execute two or three balancing transactions while the difference persists (that is, before the other arbitrageurs act). When the transaction involves a delay of weeks or months, as above, it may entail considerable risk if borrowed money is used to magnify the reward through leverage. One way of reducing this risk is through the illegal use of inside information, and in fact risk arbitrage with regard to leveraged buyouts was associated with some of the famous financial scandals of the 1980s such as those involving Michael Milken and Ivan Boesky.\n\nAnother risk occurs if the items being bought and sold are not identical and the arbitrage is conducted under the assumption that the prices of the items are correlated or predictable; this is more narrowly referred to as a convergence trade. In the extreme case this is merger arbitrage, described below. In comparison to the classical quick arbitrage transaction, such an operation can produce disastrous losses.\n\nAs arbitrages generally involve \"future\" movements of cash, they are subject to counterparty risk: if a counterparty fails to fulfill their side of a transaction. This is a serious problem if one has either a single trade or many related trades with a single counterparty, whose failure thus poses a threat, or in the event of a financial crisis when many counterparties fail. This hazard is serious because of the large quantities one must trade in order to make a profit on small price differences.\n\nFor example, if one purchases many risky bonds, then hedges them with CDSes, profiting from the difference between the bond spread and the CDS premium, in a financial crisis the bonds may default \"and\" the CDS writer/seller may itself fail, due to the stress of the crisis, causing the arbitrageur to face steep losses.\n\nArbitrage trades are necessarily synthetic, \"leveraged\" trades, as they involve a short position. If the assets used are not identical (so a price divergence makes the trade temporarily lose money), or the margin treatment is not identical, and the trader is accordingly required to post margin (faces a margin call), the trader may run out of capital (if they run out of cash and cannot borrow more) and be forced to sell these assets at a loss even though the trades may be expected to ultimately make money. In effect, arbitrage traders synthesize a put option on their ability to finance themselves.\n\nPrices may diverge during a financial crisis, often termed a \"flight to quality\"; these are precisely the times when it is hardest for leveraged investors to raise capital (due to overall capital constraints), and thus they will lack capital precisely when they need it most.\n\nAlso known as geographical arbitrage, this is the simplest form of arbitrage. In spatial arbitrage, an arbitrageur looks for price differences between geographically separate markets. For example, there may be a bond dealer in Virginia offering a bond at 100-12/23 and a dealer in Washington bidding 100-15/23 for the same bond. For whatever reason, the two dealers have not spotted the difference in the prices, but the arbitrageur does. The arbitrageur immediately buys the bond from the Virginia dealer and sells it to the Washington dealer.\n\nAlso called risk arbitrage, merger arbitrage generally consists of buying/holding the stock of a company that is the target of a takeover while shorting the stock of the acquiring company.\n\nUsually the market price of the target company is less than the price offered by the acquiring company.\nThe spread between these two prices depends mainly on the probability and the timing of the takeover being completed as well as the prevailing level of interest rates.\n\nThe bet in a merger arbitrage is that such a spread will eventually be zero, if and when the takeover is completed. The risk is that the deal \"breaks\" and the spread massively widens.\n\nAlso called \"municipal bond relative value arbitrage\", \"municipal arbitrage\", or just \"muni arb\", this hedge fund strategy involves one of two approaches. The term \"arbitrage\" is also used in the context of the Income Tax Regulations governing the investment of proceeds of municipal bonds; these regulations, aimed at the issuers or beneficiaries of tax-exempt municipal bonds, are different and, instead, attempt to remove the issuer's ability to arbitrage between the low tax-exempt rate and a taxable investment rate.\n\nGenerally, managers seek relative value opportunities by being both long and short municipal bonds with a duration-neutral book. The relative value trades may be between different issuers, different bonds issued by the same entity, or capital structure trades referencing the same asset (in the case of revenue bonds). Managers aim to capture the inefficiencies arising from the heavy participation of non-economic investors (i.e., high income \"buy and hold\" investors seeking tax-exempt income) as well as the \"crossover buying\" arising from corporations' or individuals' changing income tax situations (i.e., insurers switching their munis for corporates after a large loss as they can capture a higher after-tax yield by offsetting the taxable corporate income with underwriting losses). There are additional inefficiencies arising from the highly fragmented nature of the municipal bond market which has two million outstanding issues and 50,000 issuers, in contrast to the Treasury market which has 400 issues and a single issuer.\n\nSecond, managers construct leveraged portfolios of AAA- or AA-rated tax-exempt municipal bonds with the duration risk hedged by shorting the appropriate ratio of taxable corporate bonds. These corporate equivalents are typically interest rate swaps referencing Libor or SIFMA . The arbitrage manifests itself in the form of a relatively cheap longer maturity municipal bond, which is a municipal bond that yields significantly more than 65% of a corresponding taxable corporate bond. The steeper slope of the municipal yield curve allows participants to collect more after-tax income from the municipal bond portfolio than is spent on the interest rate swap; the carry is greater than the hedge expense. Positive, tax-free carry from muni arb can reach into the double digits. The bet in this municipal bond arbitrage is that, over a longer period of time, two similar instruments—municipal bonds and interest rate swaps—will correlate with each other; they are both very high quality credits, have the same maturity and are denominated in the same currency. Credit risk and duration risk are largely eliminated in this strategy. However, basis risk arises from use of an imperfect hedge, which results in significant, but range-bound principal volatility. The end goal is to limit this principal volatility, eliminating its relevance over time as the high, consistent, tax-free cash flow accumulates. Since the inefficiency is related to government tax policy, and hence is structural in nature, it has not been arbitraged away.\n\nNote, however, that many municipal bonds are callable, and that this imposes substantial additional risks to the strategy.\n\nA convertible bond is a bond that an investor can return to the issuing company in exchange for a predetermined number of shares in the company.\n\nA convertible bond can be thought of as a corporate bond with a stock call option attached to it.\n\nThe price of a convertible bond is sensitive to three major factors:\n\n\nGiven the complexity of the calculations involved and the convoluted structure that a convertible bond can have, an arbitrageur often relies on sophisticated quantitative models in order to identify bonds that are trading cheap versus their theoretical value.\n\nConvertible arbitrage consists of buying a convertible bond and hedging two of the three factors in order to gain exposure to the third factor at a very attractive price.\n\nFor instance an arbitrageur would first buy a convertible bond, then sell fixed income securities or interest rate futures (to hedge the interest rate exposure) and buy some credit protection (to hedge the risk of credit deterioration).\nEventually what he'd be left with is something similar to a call option on the underlying stock, acquired at a very low price.\nHe could then make money either selling some of the more expensive options that are openly traded in the market or delta hedging his exposure to the underlying shares.\n\nA depositary receipt is a security that is offered as a \"tracking stock\" on another foreign market. For instance, a Chinese company wishing to raise more money may issue a depository receipt on the New York Stock Exchange, as the amount of capital on the local exchanges is limited. These securities, known as ADRs (American depositary receipt) or GDRs (global depository receipt) depending on where they are issued, are typically considered \"foreign\" and therefore trade at a lower value when first released. Many ADR's are exchangeable into the original security (known as fungibility) and actually have the same value. In this case there is a spread between the perceived value and real value, which can be extracted. Other ADR's that are not exchangeable often have much larger spreads. Since the ADR is trading at a value lower than what it is worth, one can purchase the ADR and expect to make money as its value converges on the original. However, there is a chance that the original stock will fall in value too, so by shorting it one can hedge that risk.\n\nCross-border arbitrage exploits different prices of the same stock in different countries:\n\nExample: Apple is trading on NASDAQ at US$108.84. The stock is also traded on the German electronic exchange, XETRA. If 1 euro costs US$1.11, a cross-border trader could enter a buy order on the XETRA at €98.03 per Apple share and a sell order at €98.07 per share.\n\nSome brokers in Germany do not offer access to the U.S. exchanges. Hence if a German retail investor wants to buy Apple stock, he needs to buy it on the XETRA. The cross-border trader would sell the Apple shares on XETRA to the investor and buy the shares in the same second on NASDAQ. Afterwards, the cross-border trader would need to transfer the shares bought on NASDAQ to the German XETRA exchange, where he is obliged to deliver the stock.\n\nIn most cases, the quotation on the local exchanges is done electronically by high-frequency traders, taking into consideration the home price of the stock and the exchange rate. This kind of high-frequency trading benefits the public as it reduces the cost to the German investor and enables him to buy U.S. shares.\n\nA dual-listed company (DLC) structure involves two companies incorporated in different countries contractually agreeing to operate their businesses as if they were a single enterprise, while retaining their separate legal identity and existing stock exchange listings. In integrated and efficient financial markets, stock prices of the twin pair should move in lockstep. In practice, DLC share prices exhibit large deviations from theoretical parity. Arbitrage positions in DLCs can be set up by obtaining a long position in the relatively underpriced part of the DLC and a short position in the relatively overpriced part. Such arbitrage strategies start paying off as soon as the relative prices of the two DLC stocks converge toward theoretical parity. However, since there is no identifiable date at which DLC prices will converge, arbitrage positions sometimes have to be kept open for considerable periods of time. In the meantime, the price gap might widen. In these situations, arbitrageurs may receive margin calls, after which they would most likely be forced to liquidate part of the position at a highly unfavorable moment and suffer a loss. Arbitrage in DLCs may be profitable, but is also very risky.\n\nA good illustration of the risk of DLC arbitrage is the position in Royal Dutch Shell—which had a DLC structure until 2005—by the hedge fund Long-Term Capital Management (LTCM, see also the discussion below). Lowenstein (2000) describes that LTCM established an arbitrage position in Royal Dutch Shell in the summer of 1997, when Royal Dutch traded at an 8 to 10 percent premium. In total, $2.3 billion was invested, half of which was long in Shell and the other half was short in Royal Dutch (Lowenstein, p. 99). In the autumn of 1998, large defaults on Russian debt created significant losses for the hedge fund and LTCM had to unwind several positions. Lowenstein reports that the premium of Royal Dutch had increased to about 22 percent and LTCM had to close the position and incur a loss. According to Lowenstein (p. 234), LTCM lost $286 million in equity pairs trading and more than half of this loss is accounted for by the Royal Dutch Shell trade.\n\nThe market prices for privately held companies are typically viewed from a return on investment perspective (such as 25%), whilst publicly held and or exchange listed companies trade on a Price to earnings ratio (P/E) (such as a P/E of 10, which equates to a 10% ROI). Thus, if a publicly traded company specialises in the acquisition of privately held companies, from a per-share perspective there is a gain with every acquisition that falls within these guidelines. E.g., Berkshire Hathaway and Halydean Corporation. Private to public equities arbitrage is a term which can arguably be applied to investment banking in general. Private markets to public markets differences may also help explain the overnight windfall gains enjoyed by principals of companies that just did an initial public offering (IPO).\n\nRegulatory arbitrage is where a regulated institution takes advantage of the difference between its real (or economic) risk and the regulatory position. For example, if a bank, operating under the Basel I accord, has to hold 8% capital against default risk, but the real risk of default is lower, it is profitable to securitise the loan, removing the low risk loan from its portfolio. On the other hand, if the real risk is higher than the regulatory risk then it is profitable to make that loan and hold on to it, provided it is priced appropriately. Regulatory arbitrage can result in parts of entire businesses being unregulated as a result of the arbitrage.\n\nThis process can increase the overall riskiness of institutions under a risk insensitive regulatory regime, as described by Alan Greenspan in his October 1998 speech on The Role of Capital in Optimal Banking Supervision and Regulation.\n\nThe term \"Regulatory Arbitrage\" was used for the first time in 2005 when it was applied by Scott V. Simpson, a partner at law firm Skadden, Arps, to refer to a new defence tactic in hostile mergers and acquisitions where differing takeover regimes in deals involving multi-jurisdictions are exploited to the advantage of a target company under threat.\n\nIn economics, regulatory arbitrage (sometimes, tax arbitrage) may be used to refer to situations when a company can choose a nominal place of business with a regulatory, legal or tax regime with lower costs. For example, an insurance company may choose to locate in Bermuda due to preferential tax rates and policies for insurance companies. This can occur particularly where the business transaction has no obvious physical location. In the case of many financial products, it may be unclear \"where\" the transaction occurs.\n\nRegulatory arbitrage can include restructuring a bank by outsourcing services such as IT. The outsourcing company takes over the installations, buying out the bank's assets and charges a periodic service fee back to the bank. This frees up cashflow usable for new lending by the bank. The bank will have higher IT costs, but counts on the multiplier effect of money creation and the interest rate spread to make it a profitable exercise.\n\nExample:\nSuppose the bank sells its IT installations for 40 million USD. With a reserve ratio of 10%, the bank can create 400 million USD in additional loans (there is a time lag, and the bank has to expect to recover the loaned money back into its books). The bank can often lend (and securitize the loan) to the IT services company to cover the acquisition cost of the IT installations. This can be at preferential rates, as the sole client using the IT installation is the bank. If the bank can generate 5% interest margin on the 400 million of new loans, the bank will increase interest revenues by 20 million. The IT services company is free to leverage their balance sheet as aggressively as they and their banker agree to. This is the reason behind the trend towards outsourcing in the financial sector. Without this money creation benefit, it is actually more expensive to outsource the IT operations as the outsourcing adds a layer of management and increases overhead.\n\nAccording to PBS Frontline's 2012 four-part documentary, \"Money, Power, and Wall Street,\" regulatory arbitrage, along with asymmetric bank lobbying in Washington and abroad, allowed investment banks in the pre- and post-2008 period to continue to skirt laws and engage in the risky proprietary trading of opaque derivatives, swaps, and other credit-based instruments invented to circumvent legal restrictions at the expense of clients, government, and publics.\n\nDue to the Affordable Care Act’s expansion of Medicaid coverage, one form of Regulatory Arbitrage can now be found when businesses engage in “Medicaid Migration”, a maneuver by which qualifying employees who would typically be enrolled in company health plans elect to enroll in Medicaid instead. These programs that have similar characteristics as insurance products to the employee, but have radically different cost structures, resulting in significant expense reductions for employers.\n\nTelecom arbitrage companies allow phone users to make international calls for free through certain access numbers. Such services are offered in the United Kingdom; the telecommunication arbitrage companies get paid an interconnect charge by the UK mobile networks and then buy international routes at a lower cost. The calls are seen as free by the UK contract mobile phone customers since they are using up their allocated monthly minutes rather than paying for additional calls.\n\nSuch services were previously offered in the United States by companies such as FuturePhone.com. These services would operate in rural telephone exchanges, primarily in small towns in the state of Iowa. In these areas, the local telephone carriers are allowed to charge a high \"termination fee\" to the caller's carrier in order to fund the cost of providing service to the small and sparsely populated areas that they serve. However, FuturePhone (as well as other similar services) ceased operations upon legal challenges from AT&T and other service providers.\n\nStatistical arbitrage is an imbalance in expected nominal values. A casino has a statistical arbitrage in every game of chance that it offers—referred to as the house advantage, house edge, vigorish or house vigorish.\n\nLong-Term Capital Management (LTCM) lost 4.6 billion U.S. dollars in fixed income arbitrage in September 1998. LTCM had attempted to make money on the price difference between different bonds. For example, it would sell U.S. Treasury securities and buy Italian bond futures. The concept was that because Italian bond futures had a less liquid market, in the short term Italian bond futures would have a higher return than U.S. bonds, but in the long term, the prices would converge. Because the difference was small, a large amount of money had to be borrowed to make the buying and selling profitable.\n\nThe downfall in this system began on August 17, 1998, when Russia defaulted on its ruble debt and domestic dollar debt. Because the markets were already nervous due to the 1997 Asian financial crisis, investors began selling non-U.S. treasury debt and buying U.S. treasuries, which were considered a safe investment. As a result, the price on US treasuries began to increase and the return began decreasing because there were many buyers, and the return (yield) on other bonds began to increase because there were many sellers (i.e. the price of those bonds fell). This caused the difference between the prices of U.S. treasuries and other bonds to increase, rather than to decrease as LTCM was expecting. Eventually this caused LTCM to fold, and their creditors had to arrange a bail-out. More controversially, officials of the Federal Reserve assisted in the negotiations that led to this bail-out, on the grounds that so many companies and deals were intertwined with LTCM that if LTCM actually failed, they would as well, causing a collapse in confidence in the economic system. Thus LTCM failed as a fixed income arbitrage fund, although it is unclear what sort of profit was realized by the banks that bailed LTCM out.\n\n\n\n"}
{"id": "14475526", "url": "https://en.wikipedia.org/wiki?curid=14475526", "title": "Bathroom privileges", "text": "Bathroom privileges\n\nBathroom privileges refers to the rules or the possibility of the use of a toilet. Most commonly, the term is used in the following settings:\n\n"}
{"id": "5502984", "url": "https://en.wikipedia.org/wiki?curid=5502984", "title": "Carroll diagram", "text": "Carroll diagram\n\nA Carroll diagram, Lewis Carroll's square, biliteral diagram or a \"two-way table\" is a diagram used for grouping things in a yes/no fashion. Numbers or objects are either categorised as 'x' (having an attribute x) or 'not x' (not having an attribute 'x'). They are named after Lewis Carroll, the pseudonym of Charles Lutwidge Dodgson.\n\nAlthough Carroll diagrams can be as simple as the first one above, the most well known types are those similar to the second one, where two attributes are shown. The <nowiki>'</nowiki>universe<nowiki>'</nowiki> of a Carroll diagram is contained within the boxes in the diagram, as any number or object has to either have an attribute or not have it.\n\nCarroll diagrams are often learnt by schoolchildren, but they can also be used outside the field of education, since they are a tidy way of categorising and displaying information.\n\n\n"}
{"id": "6449", "url": "https://en.wikipedia.org/wiki?curid=6449", "title": "Clock", "text": "Clock\n\nA clock is an instrument used to measure, keep, and indicate time. The clock is one of the oldest human inventions, meeting the need to measure intervals of time shorter than the natural units: the day, the lunar month, and the year. Devices operating on several physical processes have been used over the millennia.\n\nSome predecessors to the modern clock may be considered as \"clocks\" that are based on movement in nature: A sundial shows the time by displaying the position of a shadow on a flat surface. There is a range of duration timers, a well-known example being the hourglass. Water clocks, along with the sundials, are possibly the oldest time-measuring instruments. A major advance occurred with the invention of the verge escapement, which made possible the first mechanical clocks around 1300 in Europe, which kept time with oscillating timekeepers like balance wheels.\n\nTraditionally in horology, the term \"clock\" was used for a striking clock, while a clock that did not strike the hours audibly was called a timepiece. In general usage today, a \"clock\" refers to any device for measuring and displaying the time. Watches and other timepieces that can be carried on one's person are often distinguished from clocks.\nSpring-driven clocks appeared during the 15th century. During the 15th and 16th centuries, clockmaking flourished. The next development in accuracy occurred after 1656 with the invention of the pendulum clock. A major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The electric clock was patented in 1840. The development of electronics in the 20th century led to clocks with no clockwork parts at all.\n\nThe timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates at a particular frequency.\nThis object can be a pendulum, a tuning fork, a quartz crystal, or the vibration of electrons in atoms as they emit microwaves.\n\nClocks have different ways of displaying the time. Analog clocks indicate time with a traditional clock face, with moving hands. Digital clocks display a numeric representation of time. Two numbering systems are in use; 24-hour time notation and 12-hour notation. Most digital clocks use electronic mechanisms and LCD, LED, or VFD displays. For the blind and use over telephones, speaking clocks state the time audibly in words. There are also clocks for the blind that have displays that can be read by touch. The study of timekeeping is known as horology.\n\nThe word \"clock\" is derived (via Dutch, Northern French, and Medieval Latin) from the Celtic words \"clagan\" and \"clocca\" meaning \"bell\".\n\nThe apparent position of the Sun in the sky moves over the course of each day, reflecting the rotation of the Earth. Shadows cast by stationary objects move correspondingly, so their positions can be used to indicate the time of day. A sundial shows the time by displaying the position of a shadow on a (usually) flat surface, which has markings that correspond to the hours. Sundials can be horizontal, vertical, or in other orientations. Sundials were widely used in ancient times. With the knowledge of latitude, a well-constructed sundial can measure local solar time with reasonable accuracy, within a minute or two. Sundials continued to be used to monitor the performance of clocks until the modern era.\n\nMany devices can be used to mark passage of time without respect to reference time (time of day, minutes, etc.) and can be useful for measuring duration or intervals. Examples of such duration timers are candle clocks, incense clocks and the hourglass. Both the candle clock and the incense clock work on the same principle wherein the consumption of resources is more or less constant allowing reasonably precise and repeatable estimates of time passages. In the hourglass, fine sand pouring through a tiny hole at a constant rate indicates an arbitrary, predetermined, passage of time. The resource is not consumed but re-used.\n\nWater clocks, also known as clepsydrae (sg: \"clepsydra\"), along with the sundials, are possibly the oldest time-measuring instruments, with the only exceptions being the vertical gnomon and the day counting tally stick. Given their great antiquity, where and when they first existed is not known and perhaps unknowable. The bowl-shaped outflow is the simplest form of a water clock and is known to have existed in Babylon and in Egypt around the 16th century BC. Other regions of the world, including India and China, also have early evidence of water clocks, but the earliest dates are less certain. Some authors, however, write about water clocks appearing as early as 4000 BC in these regions of the world.\n\nGreek astronomer Andronicus of Cyrrhus supervised the construction of the Tower of the Winds in Athens in the 1st century B.C. The Greek and Roman civilizations are credited for initially advancing water clock design to include complex gearing, which was connected to fanciful automata and also resulted in improved accuracy. These advances were passed on through Byzantium and Islamic times, eventually making their way back to Europe. Independently, the Chinese developed their own advanced water clocks（水鐘）in 725 AD, passing their ideas on to Korea and Japan.\n\nSome water clock designs were developed independently and some knowledge was transferred through the spread of trade. Pre-modern societies do not have the same precise timekeeping requirements that exist in modern industrial societies, where every hour of work or rest is monitored, and work may start or finish at any time regardless of external conditions. Instead, water clocks in ancient societies were used mainly for astrological reasons. These early water clocks were calibrated with a sundial. While never reaching the level of accuracy of a modern timepiece, the water clock was the most accurate and commonly used timekeeping device for millennia, until it was replaced by the more accurate pendulum clock in 17th-century Europe.\n\nIslamic civilization is credited with further advancing the accuracy of clocks with elaborate engineering. In 797 (or possibly 801), the Abbasid caliph of Baghdad, Harun al-Rashid, presented Charlemagne with an Asian Elephant named Abul-Abbas together with a \"particularly elaborate example\" of a water clock.\nPope Sylvester II introduced clocks to northern and western Europe around 1000AD\n\nIn the 13th century, Al-Jazari, an engineer from Mesopotamia (lived 1136–1206) who worked for Artuqid king of Diyar-Bakr, Nasir al-Din, made numerous clocks of all shapes and sizes. A book on his work described 50 mechanical devices in 6 categories, including water clocks. The most reputed clocks included the Elephant, Scribe and Castle clocks, all of which have been successfully reconstructed. As well as telling the time, these grand clocks were symbols of status, grandeur and wealth of the Urtuq State.\n\nThe word \"horologia\" (from the Greek ὥρα, hour, and λέγειν, to tell) was used to describe early mechanical clocks, but the use of this word (still used in several Romance languages) for all timekeepers conceals the true nature of the mechanisms. For example, there is a record that in 1176 Sens Cathedral installed a ‘horologe’ but the mechanism used is unknown. According to Jocelin of Brakelond, in 1198 during a fire at the abbey of St Edmundsbury (now Bury St Edmunds), the monks 'ran to the clock' to fetch water, indicating that their water clock had a reservoir large enough to help extinguish the occasional fire. The word \"clock\" (from the Celtic words \"clocca\" and \"clogan\", both meaning \"bell\"), which gradually supersedes \"horologe\", suggests that it was the sound of bells which also characterized the prototype mechanical clocks that appeared during the 13th century in Europe.\n\nA water-powered cogwheel clock was created in China in AD 725 by Yi Xing and Liang Lingzan. This is not considered an escapement mechanism clock as it was unidirectional, the Song dynasty polymath and genius Su Song (1020–1101) incorporated it into his monumental innovation of the astronomical clock-tower of Kaifeng in 1088. His astronomical clock and rotating armillary sphere still relied on the use of either flowing water during the spring, summer, autumn seasons and liquid mercury during the freezing temperature of winter (i.e. hydraulics). A mercury clock, described in the \"Libros del saber\", a Spanish work from 1277 consisting of translations and paraphrases of Arabic works, is sometimes quoted as evidence for Muslim knowledge of a mechanical clock. A mercury-powered cogwheel clock was created by Ibn Khalaf al-Muradi\n\nIn Europe, between 1280 and 1320, there is an increase in the number of references to clocks and horologes in church records, and this probably indicates that a new type of clock mechanism had been devised. Existing clock mechanisms that used water power were being adapted to take their driving power from falling weights. This power was controlled by some form of oscillating mechanism, probably derived from existing bell-ringing or alarm devices. This controlled release of power—the escapement—marks the beginning of the true mechanical clock, which differed from the previously mentioned cogwheel clocks. Verge escapement mechanism derived in the surge of true mechanical clocks, which didn't need any kind of fluid power, like water or mercury, to work.\n\nThese mechanical clocks were intended for two main purposes: for signalling and notification (e.g. the timing of services and public events), and for modeling the solar system. The former purpose is administrative, the latter arises naturally given the scholarly interests in astronomy, science, astrology, and how these subjects integrated with the religious philosophy of the time. The astrolabe was used both by astronomers and astrologers, and it was natural to apply a clockwork drive to the rotating plate to produce a working model of the solar system.\n\nSimple clocks intended mainly for notification were installed in towers, and did not always require faces or hands. They would have announced the canonical hours or intervals between set times of prayer. Canonical hours varied in length as the times of sunrise and sunset shifted. The more sophisticated astronomical clocks would have had moving dials or hands, and would have shown the time in various time systems, including Italian hours, canonical hours, and time as measured by astronomers at the time. Both styles of clock started acquiring extravagant features such as automata.\n\nIn 1283, a large clock was installed at Dunstable Priory; its location above the rood screen suggests that it was not a water clock. In 1292, Canterbury Cathedral installed a 'great horloge'. Over the next 30 years there are mentions of clocks at a number of ecclesiastical institutions in England, Italy, and France. In 1322, a new clock was installed in Norwich, an expensive replacement for an earlier clock installed in 1273. This had a large (2 metre) astronomical dial with automata and bells. The costs of the installation included the full-time employment of two clockkeepers for two years.\n\nBesides the Chinese astronomical clock of Su Song in 1088 mentioned above, in Europe there were the clocks constructed by Richard of Wallingford in St Albans by 1336, and by Giovanni de Dondi in Padua from 1348 to 1364. They no longer exist, but detailed descriptions of their design and construction survive, and modern reproductions have been made. They illustrate how quickly the theory of the mechanical clock had been translated into practical constructions, and also that one of the many impulses to their development had been the desire of astronomers to investigate celestial phenomena.\n\nWallingford's clock had a large astrolabe-type dial, showing the sun, the moon's age, phase, and node, a star map, and possibly the planets. In addition, it had a wheel of fortune and an indicator of the state of the tide at London Bridge. Bells rang every hour, the number of strokes indicating the time. Dondi's clock was a seven-sided construction, 1 metre high, with dials showing the time of day, including minutes, the motions of all the known planets, an automatic calendar of fixed and movable feasts, and an eclipse prediction hand rotating once every 18 years. It is not known how accurate or reliable these clocks would have been. They were probably adjusted manually every day to compensate for errors caused by wear and imprecise manufacture. Water clocks are sometimes still used today, and can be examined in places such as ancient castles and museums. The Salisbury Cathedral clock, built in 1386, is considered to be the world's oldest surviving mechanical clock that strikes the hours.\n\nClockmakers developed their art in various ways. Building smaller clocks was a technical challenge, as was improving accuracy and reliability. Clocks could be impressive showpieces to demonstrate skilled craftsmanship, or less expensive, mass-produced items for domestic use. The escapement in particular was an important factor affecting the clock's accuracy, so many different mechanisms were tried.\n\nSpring-driven clocks appeared during the 15th century, although they are often erroneously credited to Nuremberg watchmaker Peter Henlein (or Henle, or Hele) around 1511. The earliest existing spring driven clock is the chamber clock given to Phillip the Good, Duke of Burgundy, around 1430, now in the Germanisches Nationalmuseum. Spring power presented clockmakers with a new problem: how to keep the clock movement running at a constant rate as the spring ran down. This resulted in the invention of the \"stackfreed\" and the fusee in the 15th century, and many other innovations, down to the invention of the modern \"going barrel\" in 1760.\n\nEarly clock dials did not indicate minutes and seconds. A clock with a dial indicating minutes was illustrated in a 1475 manuscript by Paulus Almanus, and some 15th-century clocks in Germany indicated minutes and seconds.\nAn early record of a seconds hand on a clock dates back to about 1560 on a clock now in the Fremersdorf collection.\n\nDuring the 15th and 16th centuries, clockmaking flourished, particularly in the metalworking towns of Nuremberg and Augsburg, and in Blois, France. Some of the more basic table clocks have only one time-keeping hand, with the dial between the hour markers being divided into four equal parts making the clocks readable to the nearest 15 minutes. Other clocks were exhibitions of craftsmanship and skill, incorporating astronomical indicators and musical movements. The cross-beat escapement was invented in 1584 by Jost Bürgi, who also developed the remontoire. Bürgi's clocks were a great improvement in accuracy as they were correct to within a minute a day. These clocks helped the 16th-century astronomer Tycho Brahe to observe astronomical events with much greater precision than before.\n\nThe next development in accuracy occurred after 1656 with the invention of the pendulum clock. Galileo had the idea to use a swinging bob to regulate the motion of a time-telling device earlier in the 17th century. Christiaan Huygens, however, is usually credited as the inventor. He determined the mathematical formula that related pendulum length to time (about 99.4 cm or 39.1 inches for the one second movement) and had the first pendulum-driven clock made. The first model clock was built in 1657 in the Hague, but it was in England that the idea was taken up. The longcase clock (also known as the \"grandfather clock\") was created to house the pendulum and works by the English clockmaker William Clement in 1670 or 1671. It was also at this time that clock cases began to be made of wood and clock faces to utilize enamel as well as hand-painted ceramics.\n\nIn 1670, William Clement created the anchor escapement, an improvement over Huygens' crown escapement. Clement also introduced the pendulum suspension spring in 1671. The concentric minute hand was added to the clock by Daniel Quare, a London clockmaker and others, and the second hand was first introduced.\n\nIn 1675, Huygens and Robert Hooke invented the spiral balance spring, or the hairspring, designed to control the oscillating speed of the balance wheel. This crucial advance finally made accurate pocket watches possible. The great English clockmaker, Thomas Tompion, was one of the first to use this mechanism successfully in his pocket watches, and he adopted the minute hand which, after a variety of designs were trialled, eventually stabilised into the modern-day configuration. The rack and snail striking mechanism for striking clocks, was introduced during the 17th century and had distinct advantages over the 'countwheel' (or 'locking plate') mechanism. During the 20th century there was a common misconception that Edward Barlow invented \"rack and snail\" striking. In fact, his invention was connected with a repeating mechanism employing the rack and snail. The repeating clock, that chimes the number of hours (or even minutes) was invented by either Quare or Barlow in 1676. George Graham invented the deadbeat escapement for clocks in 1720.\n\nA major stimulus to improving the accuracy and reliability of clocks was the importance of precise time-keeping for navigation. The position of a ship at sea could be determined with reasonable accuracy if a navigator could refer to a clock that lost or gained less than about 10 seconds per day. This clock could not contain a pendulum, which would be virtually useless on a rocking ship. In 1714, the British government offered large financial rewards to the value of 20,000 pounds, for anyone who could determine longitude accurately. John Harrison, who dedicated his life to improving the accuracy of his clocks, later received considerable sums under the Longitude Act.\n\nIn 1735, Harrison built his first chronometer, which he steadily improved on over the next thirty years before submitting it for examination. The clock had many innovations, including the use of bearings to reduce friction, weighted balances to compensate for the ship's pitch and roll in the sea and the use of two different metals to reduce the problem of expansion from heat. The chronometer was tested in 1761 by Harrison's son and by the end of 10 weeks the clock was in error by less than 5 seconds.\n\nThe British had predominated in watch manufacture for much of the 17th and 18th centuries, but maintained a system of production that was geared towards high quality products for the elite. Although there was an attempt to modernise clock manufacture with mass production techniques and the application of duplicating tools and machinery by the British Watch Company in 1843, it was in the United States that this system took off. In 1816, Eli Terry and some other Connecticut clockmakers developed a way of mass-producing clocks by using interchangeable parts. Aaron Lufkin Dennison started a factory in 1851 in Massachusetts that also used interchangeable parts, and by 1861 was running a successful enterprise incorporated as the Waltham Watch Company.\n\nIn 1815, Francis Ronalds published the first electric clock powered by dry pile batteries. Alexander Bain, Scottish clockmaker, patented the electric clock in 1840. The electric clock's mainspring is wound either with an electric motor or with an electromagnet and armature. In 1841, he first patented the electromagnetic pendulum. By the end of the nineteenth century, the advent of the dry cell battery made it feasible to use electric power in clocks. Spring or weight driven clocks that use electricity, either alternating current (AC) or direct current (DC), to rewind the spring or raise the weight of a mechanical clock would be classified as an electromechanical clock. This classification would also apply to clocks that employ an electrical impulse to propel the pendulum. In electromechanical clocks the electricity serves no time keeping function. These types of clocks were made as individual timepieces but more commonly used in synchronized time installations in schools, businesses, factories, railroads and government facilities as a master clock and slave clocks.\n\nElectric clocks that are powered from the AC supply often use synchronous motors. The supply current alternates with a frequency of 50 hertz in many countries, and 60 hertz in others. The rotor of the motor rotates at a speed that is related to the alternation frequency. Appropriate gearing converts this rotation speed to the correct ones for the hands of the analog clock. The development of electronics in the 20th century led to clocks with no clockwork parts at all. Time in these cases is measured in several ways, such as by the alternation of the AC supply, vibration of a tuning fork, the behaviour of quartz crystals, or the quantum vibrations of atoms. Electronic circuits divide these high-frequency oscillations to slower ones that drive the time display. Even mechanical clocks have since come to be largely powered by batteries, removing the need for winding.\n\nThe piezoelectric properties of crystalline quartz were discovered by Jacques and Pierre Curie in 1880. The first crystal oscillator was invented in 1917 by Alexander M. Nicholson after which, the first quartz crystal oscillator was built by Walter G. Cady in 1921. In 1927 the first quartz clock was built by Warren Marrison and J.W. Horton at Bell Telephone Laboratories in Canada. The following decades saw the development of quartz clocks as precision time measurement devices in laboratory settings—the bulky and delicate counting electronics, built with vacuum tubes, limited their practical use elsewhere. The National Bureau of Standards (now NIST) based the time standard of the United States on quartz clocks from late 1929 until the 1960s, when it changed to atomic clocks. In 1969, Seiko produced the world's first quartz wristwatch, the Astron. Their inherent accuracy and low cost of production resulted in the subsequent proliferation of quartz clocks and watches.\n\nAs of the 2010s, atomic clocks are the most accurate clocks in existence. They are considerably more accurate than quartz clocks as they can be accurate to within a few seconds over thousands of years. Atomic clocks were first theorized by Lord Kelvin in 1879. In the 1930s the development of Magnetic resonance created practical method for doing this. A prototype ammonia maser device was built in 1949 at the U.S. National Bureau of Standards (NBS, now NIST). Although it was less accurate than existing quartz clocks, it served to demonstrate the concept. The first accurate atomic clock, a caesium standard based on a certain transition of the caesium-133 atom, was built by Louis Essen in 1955 at the National Physical Laboratory in the UK. Calibration of the caesium standard atomic clock was carried out by the use of the astronomical time scale \"ephemeris time\" (ET). As of 2013, the most stable atomic clocks are ytterbium clocks, which are stable to within less than two parts in 1 quintillion ().\n\nThe invention of the mechanical clock in the 13th century initiated a change in timekeeping methods from continuous processes, such as the motion of the gnomon's shadow on a sundial or the flow of liquid in a water clock, to periodic oscillatory processes, such as the swing of a pendulum or the vibration of a quartz crystal, which had the potential for more accuracy. All modern clocks use oscillation.\n\nAlthough the mechanisms they use vary, all oscillating clocks, mechanical, digital and atomic, work similarly and can be divided into analogous parts. They consist of an object that repeats the same motion over and over again, an \"oscillator\", with a precisely constant time interval between each repetition, or 'beat'. Attached to the oscillator is a \"controller\" device, which sustains the oscillator's motion by replacing the energy it loses to friction, and converts its oscillations into a series of pulses. The pulses are then counted by some type of \"counter\", and the number of counts is converted into convenient units, usually seconds, minutes, hours, etc. Finally some kind of \"indicator\" displays the result in human readable form.\n\nThe timekeeping element in every modern clock is a harmonic oscillator, a physical object (resonator) that vibrates or oscillates repetitively at a precisely constant frequency.\nThe advantage of a harmonic oscillator over other forms of oscillator is that it employs resonance to vibrate at a precise natural resonant frequency or 'beat' dependent only on its physical characteristics, and resists vibrating at other rates. The possible precision achievable by a harmonic oscillator is measured by a parameter called its Q, or quality factor, which increases (other things being equal) with its resonant frequency. This is why there has been a long term trend toward higher frequency oscillators in clocks. Balance wheels and pendulums always include a means of adjusting the rate of the timepiece. Quartz timepieces sometimes include a rate screw that adjusts a capacitor for that purpose. Atomic clocks are primary standards, and their rate cannot be adjusted.\n\nSome clocks rely for their accuracy on an external oscillator; that is, they are automatically synchronized to a more accurate clock:\n\nThis has the dual function of keeping the oscillator running by giving it 'pushes' to replace the energy lost to friction, and converting its vibrations into a series of pulses that serve to measure the time.\nIn mechanical clocks, the low Q of the balance wheel or pendulum oscillator made them very sensitive to the disturbing effect of the impulses of the escapement, so the escapement had a great effect on the accuracy of the clock, and many escapement designs were tried. The higher Q of resonators in electronic clocks makes them relatively insensitive to the disturbing effects of the drive power, so the driving oscillator circuit is a much less critical component.\n\nThis counts the pulses and adds them up to get traditional time units of seconds, minutes, hours, etc. It usually has a provision for \"setting\" the clock by manually entering the correct time into the counter.\n\nThis displays the count of seconds, minutes, hours, etc. in a human readable form.\n\nClocks can be classified by the type of time display, as well as by the method of timekeeping.\n\nAnalog clocks usually use a clock face which indicates time using rotating pointers called \"hands\" on a fixed numbered dial or dials. The standard clock face, known universally throughout the world, has a short \"hour hand\" which indicates the hour on a circular dial of 12 hours, making two revolutions per day, and a longer \"minute hand\" which indicates the minutes in the current hour on the same dial, which is also divided into 60 minutes. It may also have a \"second hand\" which indicates the seconds in the current minute. The only other widely used clock face today is the 24 hour analog dial, because of the use of 24 hour time in military organizations and timetables. Before the modern clock face was standardized during the Industrial Revolution, many other face designs were used throughout the years, including dials divided into 6, 8, 10, and 24 hours. During the French Revolution the French government tried to introduce a 10-hour clock, as part of their decimal-based metric system of measurement, but it didn't catch on. An Italian 6 hour clock was developed in the 18th century, presumably to save power (a clock or watch striking 24 times uses more power).\nAnother type of analog clock is the sundial, which tracks the sun continuously, registering the time by the shadow position of its gnomon. Because the sun does not adjust to daylight saving time, users must add an hour during that time. Corrections must also be made for the equation of time, and for the difference between the longitudes of the sundial and of the central meridian of the time zone that is being used (i.e. 15 degrees east of the prime meridian for each hour that the time zone is ahead of GMT). Sundials use some or part of the 24 hour analog dial. There also exist clocks which use a digital display despite having an analog mechanism—these are commonly referred to as flip clocks. Alternative systems have been proposed. For example, the \"Twelv\" clock indicates the current hour using one of twelve colors, and indicates the minute by showing a proportion of a circular disk, similar to a moon phase.\n\nDigital clocks display a numeric representation of time. Two numeric display formats are commonly used on digital clocks:\n\nMost digital clocks use electronic mechanisms and LCD, LED, or VFD displays; many other display technologies are used as well (cathode ray tubes, nixie tubes, etc.). After a reset, battery change or power failure, these clocks without a backup battery or capacitor either start counting from 12:00, or stay at 12:00, often with blinking digits indicating that the time needs to be set. Some newer clocks will reset themselves based on radio or Internet time servers that are tuned to national atomic clocks. Since the advent of digital clocks in the 1960s, the use of analog clocks has declined significantly.\n\nSome clocks, called 'flip clocks', have digital displays that work mechanically. The digits are painted on sheets of material which are mounted like the pages of a book. Once a minute, a page is turned over to reveal the next digit. These displays are usually easier to read in brightly lit conditions than LCDs or LEDs. Also, they do not go back to 12:00 after a power interruption. Flip clocks generally do not have electronic mechanisms. Usually, they are driven by AC-synchronous motors.\n\nClocks with analog quadrants, with a digital component, usually minutes and hours displayed analogously and seconds displayed in digital mode.\n\nFor convenience, distance, telephony or blindness, auditory clocks present the time as sounds. The sound is either spoken natural language, (e.g. \"The time is twelve thirty-five\"), or as auditory codes (e.g. number of sequential bell rings on the hour represents the number of the hour like the bell, Big Ben). Most telecommunication companies also provide a speaking clock service as well.\n\nWord clocks are clocks that display the time visually using sentences. E.g.: \"It’s about three o’clock.\" These clocks can be implemented in hardware or software.\n\nSome clocks, usually digital ones, include an optical projector that shines a magnified image of the time display onto a screen or onto a surface such as an indoor ceiling or wall. The digits are large enough to be easily read, without using glasses, by persons with moderately imperfect vision, so the clocks are convenient for use in their bedrooms. Usually, the timekeeping circuitry has a battery as a backup source for an uninterrupted power supply to keep the clock on time, while the projection light only works when the unit is connected to an A.C. supply. Completely battery-powered portable versions resembling flashlights are also available.\n\nAuditory and projection clocks can be used by people who are blind or have limited vision. There are also clocks for the blind that have displays that can be read by using the sense of touch. Some of these are similar to normal analog displays, but are constructed so the hands can be felt without damaging them. Another type is essentially digital, and uses devices that use a code such as Braille to show the digits so that they can be felt with the fingertips.\n\nSome clocks have several displays driven by a single mechanism, and some others have several completely separate mechanisms in a single case. Clocks in public places often have several faces visible from different directions, so that the clock can be read from anywhere in the vicinity; all the faces show the same time. Other clocks show the current time in several time-zones. Watches that are intended to be carried by travellers often have two displays, one for the local time and the other for the time at home, which is useful for making pre-arranged phone calls. Some equation clocks have two displays, one showing mean time and the other solar time, as would be shown by a sundial. Some clocks have both analog and digital displays. Clocks with Braille displays usually also have conventional digits so they can be read by sighted people.\n\nClocks are in homes, offices and many other places; smaller ones (watches) are carried on the wrist or in a pocket; larger ones are in public places, e.g. a railway station or church. A small clock is often shown in a corner of computer displays, mobile phones and many MP3 players.\n\nThe primary purpose of a clock is to \"display\" the time. Clocks may also have the facility to make a loud alert signal at a specified time, typically to waken a sleeper at a preset time; they are referred to as \"alarm clocks\". The alarm may start at a low volume and become louder, or have the facility to be switched off for a few minutes then resume. Alarm clocks with visible indicators are sometimes used to indicate to children too young to read the time that the time for sleep has finished; they are sometimes called \"training clocks\".\n\nA clock mechanism may be used to \"control\" a device according to time, e.g. a central heating system, a VCR, or a time bomb (see: digital counter). Such mechanisms are usually called timers. Clock mechanisms are also used to drive devices such as solar trackers and astronomical telescopes, which have to turn at accurately controlled speeds to counteract the rotation of the Earth.\n\nMost digital computers depend on an internal signal at constant frequency to synchronize processing; this is referred to as a clock signal. (A few research projects are developing CPUs based on asynchronous circuits.) Some equipment, including computers, also maintains time and date for use as required; this is referred to as time-of-day clock, and is distinct from the system clock signal, although possibly based on counting its cycles.\n\nIn Chinese culture, giving a clock (送鍾/送钟, sòng zhōng) is often taboo, especially to the elderly as the term for this act is a homophone with the term for the act of attending another's funeral (送終/送终, sòngzhōng). A UK government official Susan Kramer gave a watch to Taipei mayor Ko Wen-je unaware of such a taboo which resulted in some professional embarrassment and a pursuant apology.\n\nIt is undesirable to give someone a clock or (depending on the region) other timepiece as a gift. Traditional superstitions regard this as counting the seconds to the recipient's death. Another common interpretation of this is that the phrase \"to give a clock\" () in Chinese is pronounced \"sòng zhōng\" in Mandarin, which is a homophone of a phrase for \"terminating\" or \"attending a funeral\" (both can be written as (traditional) or (simplified)). Cantonese people consider such a gift as a curse.\n\nThis homonymic pair works in both Mandarin and Cantonese, although in most parts of China only clocks and large bells, and not watches, are called \"zhong\", and watches are commonly given as gifts in China.\n\nHowever, should such a gift be given, the \"unluckiness\" of the gift can be countered by exacting a small monetary payment so the recipient is buying the clock and thereby counteracting the (\"give\") expression of the phrase.\n\nFor some scientific work timing of the utmost accuracy is essential. It is also necessary to have a standard of the maximum accuracy against which working clocks can be calibrated. An ideal clock would give the time to unlimited accuracy, but this is not realisable. Many physical processes, in particular including some transitions between atomic energy levels, occur at exceedingly stable frequency; counting cycles of such a process can give a very accurate and consistent time—clocks which work this way are usually called atomic clocks. Such clocks are typically large, very expensive, require a controlled environment, and are far more accurate than required for most purposes; they are typically used in a standards laboratory.\n\nUntil advances in the late twentieth century, navigation depended on the ability to measure latitude and longitude. Latitude can be determined through celestial navigation; the measurement of longitude requires accurate knowledge of time. This need was a major motivation for the development of accurate mechanical clocks. John Harrison created the first highly accurate marine chronometer in the mid-18th century. The Noon gun in Cape Town still fires an accurate signal to allow ships to check their chronometers. Many buildings near major ports used to have (some still do) a large ball mounted on a tower or mast arranged to drop at a pre-determined time, for the same purpose. While satellite navigation systems such as the Global Positioning System (GPS) require unprecedentedly accurate knowledge of time, this is supplied by equipment on the satellites; vehicles no longer need timekeeping equipment.\n\n\n\n"}
{"id": "5938019", "url": "https://en.wikipedia.org/wiki?curid=5938019", "title": "Complexity theory and organizations", "text": "Complexity theory and organizations\n\nComplexity theory and organizations, also called complexity strategy or complex adaptive organizations, is the use of the study of complexity systems in the field of strategic management and organizational studies.\n\nComplexity theory is an interdisciplinary theory that grew out of systems theory in the 1960s. It draws from research in the natural sciences that examines uncertainty and non-linearity. Complexity theory emphasizes interactions and the accompanying feedback loops that constantly change systems. While it proposes that systems are unpredictable, they are also constrained by order-generating rules.\n\nComplexity theory has been used in the fields of strategic management and organizational studies. Application areas include understanding how organizations or firms adapt to their environments and how they cope with conditions of uncertainty. Organisations have complex structures in that they are dynamic networks of interactions, and their relationships are not aggregations of the individual static entities. They are adaptive; in that the individual and collective behavior mutate and self-organize corresponding to a change-initiating micro-event or collection of events.\n\nOrganizations can be treated as complex adaptive systems (CAS) as they exhibit fundamental CAS principles like self-organization, complexity, emergence, interdependence, space of possibilities, co-evolution, chaos, and self-similarity.\n\nCAS are contrasted with ordered and chaotic systems by the relationship that exists between the system and the agents which act within it. In an ordered system the level of constraint means that all agent behaviour is limited to the rules of the system. In a chaotic system the agents are unconstrained and susceptible to statistical and other analysis. In a CAS, the system and the agents co-evolve; the system lightly constrains agent behaviour, but the agents modify the system by their interaction with it. This self-organizing nature is an important characteristic of CAS; and its ability to learn to adapt, differentiate it from other self organizing systems.\n\nCAS approaches to strategy seek to understand the nature of system constraints and agent interaction and generally takes an evolutionary or naturalistic approach to strategy. Some research integrates computer simulation and organizational studies.\n\nComplexity theory also relates to knowledge management (KM) and organizational learning (OL). \"Complex systems are, by any other definition, learning organizations.\" Complexity Theory, KM, and OL are all complimentary and co-dependent. “KM and OL each lack a theory of how cognition happens in human social systems – complexity theory offers this missing piece”.\n\nComplexity theory is also being used to better understand new ways of doing project management, as traditional models have been found lacking to current challenges. This approaches advocates forming a \"culture of trust\" that \"welcomes outsiders, embraces new ideas, and promotes cooperation.\"\n\nComplexity Theory implies approaches that focus on flatter, more flexible organizations, rather than top-down, command-and-control styles of management.\n\nA typical example for an organization behaving as CAS, is Wikipedia – collaborated and managed by a loosely organized management structure, composed of a complex mix of human–computer interactions. By managing behavior, and not only mere content, Wikipedia uses simple rules to produce a complex, evolving knowledge base which has largely replaced older sources in popular use.\n\nOther examples include the complex global macroeconomic network within a country or group of countries; stock market and complex web of cross-border holding companies; manufacturing businesses; and any human social group-based endeavour in a particular ideology and social system such as political parties, communities, geopolitical organisations, and terrorist networks of both hierarchical and leaderless nature. This new macro level state may create difficulty for an observer in explaining and describing the collective behaviour in terms of its constituent parts; as a result of the complex dynamic networks of interactions, outlined earlier.\n\n\n"}
{"id": "36211405", "url": "https://en.wikipedia.org/wiki?curid=36211405", "title": "Conspiracies against the laity", "text": "Conspiracies against the laity\n\nConspiracies against the laity is a term coined by George Bernard Shaw in his 1906 play The Doctor's Dilemma. The conspiracies refer to the methods used by professions to acquire prestige, power and wealth.\n\nOn the subject of such conspiracies, Tim Harford argued the following in his 2006 book The Undercover Economist: \"... doctors, actuaries, accountants and lawyers manage to maintain high wages through... erecting virtual 'green belts' to make it hard for competitors to set up shop. Typical virtual green belts will include very long qualification periods and professional bodies that give their approval only to a certain number of candidates per year. Many of the organisations that are put forth to protect us from 'unqualified' professionals in fact serve to maintain the high rates of the 'qualified' to whom we are directed.\"\n\nShaw's sentiments echo Adam Smith's earlier writing: \"People of the same trade seldom meet together, even for merriment and diversion, but the conversation ends in a conspiracy against the public, or in some contrivance to raise prices.\"\n"}
{"id": "40494939", "url": "https://en.wikipedia.org/wiki?curid=40494939", "title": "Ecofeminism", "text": "Ecofeminism\n\nThe term Ecofeminism is used to describe a feminist approach to understanding ecology. Ecofeminist thinkers draw on the concept of gender to theorize on the relationship between humans and the natural world. The term was coined by the French writer Françoise d'Eaubonne in her book \"Le Féminisme ou la Mort\" (1974). Today, there are many interpretations of ecofeminism and how it might be applied to social thought, including: ecofeminist art, ecofeminist theory, social justice and political philosophy, religion, contemporary feminism and poetry. As there are several different types of feminism and different beliefs held by feminists, there are different versions of ecofeminism.\n\nEcofeminism is widely referred to as the third wave of feminism, it adds to the former feminist theory that an environmental perspective is a necessary part of feminism. Ecofeminism uses the parallels between the oppression of nature and the oppression of women as a way to highlight the idea that both must be understood in order to properly recognize how they are connected. These parallels include but are not limited to seeing women and nature as property, seeing men as the curators of culture and women as the curators of nature, and how men dominate women and humans dominate nature.\n\nCharlene Spretnak has offered one way of categorizing ecofeminist work: 1) through the study of political theory as well as history; 2) through the belief and study of nature-based religions; 3) through environmentalism.\n\nAccording to Françoise d'Eaubonne in her book \"Le Féminisme ou la Mort\" (1974, Ecofeminism relates the oppression and domination of all subordinate groups (women, people of color, children, the poor) to the oppression and domination of nature (animals, land, water, air, etc.). In the book, the author argues that oppression, domination, exploitation, and colonization from the Western patriarchal society has directly caused irreversible environmental damage. Françoise d'Eaubonne was an activist and organizer, and her writing encouraged the eradication of all social injustice, not just injustice against women and the environment.\n\nThis tradition includes a number of influential texts including: \"Women and Nature\" (Susan Griffin 1978), \"The Death of Nature\" (Carolyn Merchant 1980) and \"Gyn/Ecology\" (Mary Daly 1978). These texts helped to propel the association between domination by man on women and the domination of culture on nature. From these texts feminist activism of the 1980s linked ideas of ecology and the environment. For example, conferences for women devoted to living on the earth and protests against nuclear testing and other militarism that oppresses femininity. Writing in this circle discussed ecofeminism drawing from Green Party politics, peace movements, and direct action movements.\n\nModern ecofeminism, or feminist eco-criticism, eschews such essentialism and instead focuses more on intersectional questions, such as how the nature-culture split enables the oppression of female and nonhuman bodies. It is also an activist and academic movement that sees critical connections between the exploitation of nature and the domination over women both caused by men.\n\nOne ecofeminist theory is that capitalist values reflect paternalistic and gendered values. In this interpretation effects of capitalism has led to a harmful split between nature and culture. In the 1970s, early ecofeminists discussed that the split can only be healed by the feminine instinct for nurture and holistic knowledge of nature's processes.\n\nSeveral feminists make the distinction that it is not \"because\" women are female or \"feminine\" that they relate to nature, but because of their similar states of oppression by the same male-dominant forces. The marginalization is evident in the gendered language used to describe nature and the animalized language used to describe women. Some discourses link women specifically to the environment because of their traditional social role as a nurturer and caregiver. Ecofeminists following in this line of thought believe that these connections are illustrated through the coherence of socially-labeled values associated with 'femininity' such as nurturing, which are present both among women and in nature.\n\nVandana Shiva says that women have a special connection to the environment through their daily interactions and this connection has been ignored. She says that women in subsistence economies who produce \"wealth in partnership with nature, have been experts in their own right of holistic and ecological knowledge of nature's processes\". She makes the point that \"these alternative modes of knowing, which are oriented to the social benefits and sustenance needs are not recognized by the capitalist reductionist paradigm, because it fails to perceive the interconnectedness of nature, or the connection of women's lives, work and knowledge with the creation of wealth (23)\".  Shiva blames this failure on the West's patriarchy, and the patriarchal idea of what development is. According to Shiva, patriarchy has labeled women, nature, and other groups not growing the economy as \"unproductive\".\n\nIn the 1993 essay entitled \"Ecofeminism: Toward Global Justice and Planetary Health\" authors Greta Gaard and Lori Gruen outline what they call the \"ecofeminist framework\". The essay provides a wealth of data and statistics in addition to laying out the theoretical aspects of the ecofeminist critique. The framework described is intended to establish ways of viewing and understanding our current global situations so that we are better able to understand how we arrived at this point and what may be done to ameliorate the ills.\n\nGaard and Gruen argue that there are four sides to this framework:\n\n\nThey hold that these four factors have brought us to what ecofeminists see as a \"separation between nature and culture\" that is the root source of our planetary ills.\n\nIn \"Ecofeminism\" (1993) authors Vandana Shiva, Maria Mies and Evan Bondi ponder modern science and its acceptance as a universal and value-free system. Instead, they view the dominant stream of modern science as a projection of Western men's values. The privilege of determining what is considered scientific knowledge has been controlled by men, and for the most part of history restricted to men. Bondi and Miles list examples including the medicalization of childbirth and the industrialization of plant reproduction.\n\nBondi argues that the medicalization of childbirth has marginalized midwife knowledge and changed the natural process of childbirth into a procedure dependent on specialized technologies and appropriated expertise.\nA common claim within ecofeminist literature is that patriarchal structures justify their dominance through binary opposition, these include but are not limited to: heaven/earth, mind/body, male/female, human/animal, spirit/matter, culture/nature and white/non-white. Oppression is reinforced by assuming truth in these binaries and instilling them as 'marvelous to behold' through religious and scientific constructs.\n\nThe application of ecofeminism to animal rights has established vegetarian ecofeminism, which asserts that \"omitting the oppression of animals from feminist and ecofeminist analyses […] is inconsistent with the activist and philosophical foundations of both feminism (as a \"movement to end all forms of oppression\") and ecofeminism.\" It puts into practice \"the personal is political\" for it believes that \"meat-eating is a form of patriarchal domination…that suggests a link between male violence and a meat-based diet.\" Vegetarian ecofeminism combines sympathy with the analysis of culture and politics to refine a system of ethics and action.\n\nEcofeminism as materialist is another common theme in ecofeminism. A materialist view connects some institutions such as labor, power and property as the source of domination over women and nature. There are connections made between these subjects because similarly there are varying values in production and reproduction.\n\nSpiritual ecofeminism is another branch of ecofeminism, and is popular among ecofeminist authors such as Starhawk, Riane Eisler, Carol J. Adams, and more. Starhawk calls this an earth-based spirituality, which recognizes that the Earth is alive, that we are interconnected, as well as a community. Spiritual ecofeminism is not linked to one specific religion, but is centered around values of caring, compassion, and non-violence. Often, ecofeminists refer to more ancient traditions, such as the worship of Gaia, the Goddess of nature and spirituality (also known as Mother Earth).\n\nBuddhism\n\nBuddhism and feminism simultaneously gained momentum in America during the 1960s. Some of the parallels between these movements include their experiential epistemology, the intersection of the constrained mind, intersectionality and connection, using emotional energy as a coping mechanism, relational ethics, and a communal mindset and lifestyle. Both Buddhist and ecofeminist practitioners viewed nature as a pathway to enlightenment and as a source of many jumping off points for introspection and deeper thought about the relationship of nature and humans.\n\nJudaism\n\nJewish ideology emphasizes leaving the earth as it was found (or in better condition than it was found in). This concept is also promoted through ecofeminist theories and movements. Both Judaism and ecofeminism do not always present as social justice movements, but they often contribute ideals and motivations for social change. Social justice is an important part of Judaism, and many practitioners see social justice as a form of spirituality, and some feel this form of spirituality through environmental and feminist movements.\n\nHinduism\n\nIn Hinduism, the Ganges River is personified by the Goddess Ganga. She is a paradoxical deity, as she is supposed to be independent yet guarded, pure yet polluted. Ganga is referred to as a deity to be both subjugated and protected. The underlying tone of the passages is patriarchal despite being reverent. The pollution or purity of the Ganges river is a reflection on Ganga, which represents the relationship between spirituality and nature. Hinduism emphasizes that all lives are connected and indistinguishable. In this context, every life, be it human or animal, is important.\n\nWomen participated in the environmental movements, specifically preservation and conservation beginning in the late nineteenth century and continuing into the early twentieth century.\n\nIn northern India in 1973, women took part in the Chipko movement to protect forests from deforestation. Non-violent protest tactics were used to occupy trees so that loggers could not cut them down.\n\nIn Kenya in 1977, the Green Belt Movement was initiated by Professor Wangari Maathai, environmental and political activist, and is ongoing today. It is rural tree planting program led by women, which Maathai designed to help prevent desertification in the area. The program created a 'green belt' of at least 1,000 trees around villages, and gives participants the ability to take charge in their communities. In later years, the Green Belt Movement was an advocate for informing and empowering citizens through seminars for civic and environmental education, as well as holding national leaders accountable for their actions and instilling agency in citizens.\n\nIn 1978 in New York, mother and environmentalist Lois Gibbs led her community in protest after discovering that their entire neighborhood, Love Canal, was built on top of a toxic dump site. The toxins in the ground were causing illness among children and reproductive issues among women, as well as birth defects in babies born to pregnant women exposed to the toxins. The Love Canal movement eventually led to the evacuation and relocation of nearly 800 families by the federal government.\n\nIn 1980 and 1981, members of such a conference organized a peaceful protest at the Pentagon. Women stood, hand in hand, demanding equal rights (including social, economic, and reproductive rights) as well as an end to militaristic actions taken by the government and exploitation of the community (people and the environment). This movement is known as the Women's Pentagon Actions.\n\nIn 1985, the Akwesasne Mother's Milk Project was launched by Katsi Cook. This study was funded by the government, and investigated how the higher level of contaminants in water near the Mohawk reservation impacted babies. It revealed that through breast milk, Mohawk children were being exposed to 200% more toxins than children not on the reservation. Toxins contaminate water all over the world, but to due environmental racism, certain subversive groups are exposed to a much higher amount.\n\nThe Greening of Harlem Coalition is another example of an ecofeminist movement. In 1989, Bernadette Cozart founded the coalition, which is responsible for many urban gardens around Harlem. Cozart's goal is to turn vacant lots into community gardens. This is economically beneficial, and also provides a way for very urban communities to be in touch with nature and each other. The majority of people interested in this project (as noted in 1990) were women. Through these gardens, they were able to participate in and become leaders of their communities. Urban greening exists in other places as well. Beginning in 1994, a group of African-American women in Detroit have developed city gardens, and call themselves the Gardening Angels. Similar garden movements have been occurring globally.\n\nThe development of vegetarian ecofeminism can be traced to the mid-80s and 90s, where it first appeared in writing. However, the roots of a vegetarian ecofeminist view can be traced back further by looking at sympathy for non-humans and counterculture movements of the 1960s and 1970s. At the culmination of the decade ecofeminism had spread to both coasts and articulated an intersectional analysis of women and the environment. Eventually, challenging ideas of environmental classism and racism, resisting toxic dumping and other threats to the impoverished.\n\nIn the 1980s and 1990s some began to see the advancing theories in ecofeminism as essentialist. Through analysis done by post structural and third wave feminists it was argued that ecofeminism equated women with nature. This dichotomy is dangerous because it groups all women into one category and enforces the very societal norms that feminism is trying to break. Out of this critique rose the anti-essentialist argument. Ecofeminist and author Noel Sturgeon says in an interview that what anti-essentialists are critiquing is a strategy used to mobilize large and diverse groups of both theorists and activists.\n\nComing out of the 90s, ecofeminism met a lot of criticism from anti-essentialist feminism, which heavily critiqued what they viewed as essentialism. The essentialist view saw ecofeminism as reinforcing and growing patriarchal dominance and norms. Feminist thoughts surrounding ecofeminism grew in some areas as it was criticized; vegetarian ecofeminism contributed intersectional analysis; and ecofeminisms that analyzed animal rights, labor rights and activisms as they could draw lines among oppressed groups. To some, the inclusion of non-human animals also became to be viewed as essentialist. According to ecofeminist and author Charlene Spretnak, modern ecofeminism is concerned about a variety of issues, including reproductive technology, equal pay and equal rights, toxic poisoning, Third World development, and more.\n\nEcofeminism as it propelled into the 21st century became aware of the criticisms, and in response ecofeminists with a materialist lens began doing research and renaming the topic, i.e. queer ecologies, global feminist environmental justice, and gender and the environment.\n\nBeginning in the late 20th century, women worked in efforts to protect wildlife, food, air and water. These efforts depended largely on new developments in the environmental movement from influential writers, such as Henry David Thoreau, Aldo Leopold, John Muir, and Rachel Carson. Fundamental examples of women's efforts in the 20th century are the books \"Silent Spring\" by Rachel Carson and \"Refuge\" by Terry Tempest Williams. These works truly opened American's eyes to the environmental harm they were perpetuating, and created a platform for change.\n\nEcofeminist author Karren Warren lists Aldo Leopold's essay \"Land Ethic\" (1949) as a fundamental work to the ecofeminist conception, as Leopold was the first to pen an ethic for the land which understands all non-human parts of that community (animals, plants, land, air, water) as equal to and in a relationship with humans. This inclusive understanding of the environment launched the modern preservation movement and illustrated how issues can be viewed through a framework of caring.\n\nSusan A. Mann an eco-feminist and professor of sociological and feminist theory considers the roles women played in these activisms to be the starter for ecofeminism in later centuries. Mann associates the beginning of ecofeminism not with feminists but with women of different race and class backgrounds who made connections among gender, race, class and environmental issues. This ideal is upheld through the notion that in activist and theory circles marginalized groups must be included in the discussion. In early environmental and women's movements, issues of varying races and classes were often separated.\n\nThe major criticism of ecofeminism is that it is essentialist. The ascribed essentialism appears in two main areas:\n\n\nSocial ecologist and feminist Janet Biehl has criticized ecofeminism for focusing too much on a mystical connection between women and nature and not enough on the actual conditions of women. She has also stated that rather than being a forward-moving theory, ecofeminism is an anti-progressive movement for women.\n\nRosemary Radford Ruether also critiques this focus on mysticism over work that focuses on helping women, but argues that spirituality and activism can be combined effectively in ecofeminism.\n\nA.E. Kings has criticized ecofeminism for limiting itself to focusing only on gender and the environment, and neglecting to take an intersectional approach. Kings says that ecofeminists claim to be intersectional, however have fallen short on their commitment until recently.\n\n\n\n\n\n\n\n"}
{"id": "10274", "url": "https://en.wikipedia.org/wiki?curid=10274", "title": "Enthalpy", "text": "Enthalpy\n\nEnthalpy , a property of a thermodynamic system, is equal to the system's internal energy plus the product of its pressure and volume. For processes at constant pressure, the heat absorbed or released equals the change in enthalpy.\n\nThe unit of measurement for enthalpy in the International System of Units (SI) is the joule. Other historical conventional units still in use include the British thermal unit (BTU) and the calorie.\n\nEnthalpy comprises a system's internal energy, which is the energy required to create the system, plus the amount of work required to make room for it by displacing its environment and establishing its volume and pressure.\n\nEnthalpy is defined as a state function that depends only on the prevailing equilibrium state identified by the system's internal energy, pressure, and volume. It is an extensive quantity.\n\nEnthalpy is the preferred expression of system energy changes in many chemical, biological, and physical measurements at constant pressure, because it simplifies the description of energy transfer. At constant pressure, the enthalpy change equals the energy transferred from the environment through heating or work other than expansion work.\n\nThe total enthalpy, \"H\", of a system cannot be measured directly. The same situation exists in classical mechanics: only a change or difference in energy carries physical meaning. Enthalpy itself is a thermodynamic potential, so in order to measure the enthalpy of a system, we must refer to a defined reference point; therefore what we measure is the change in enthalpy, Δ\"H\". The Δ\"H\" is a positive change in endothermic reactions, and negative in heat-releasing exothermic processes.\n\nFor processes under constant pressure, Δ\"H\" is equal to the change in the internal energy of the system, plus the pressure-volume work p ΔV done by the system on its surroundings (which is > 0 for an expansion and < 0 for a contraction). This means that the change in enthalpy under such conditions is the heat absorbed or released by the system through a chemical reaction or by external heat transfer. Enthalpies for chemical substances at constant pressure usually refer to standard state: most commonly 1 bar pressure. Standard state does not, strictly speaking, specify a temperature (see standard state), but expressions for enthalpy generally reference the standard heat of formation at 25 °C.\n\nEnthalpy of ideal gases and incompressible solids and liquids does not depend on pressure, unlike entropy and Gibbs energy. Real materials at common temperatures and pressures usually closely approximate this behavior, which greatly simplifies enthalpy calculation and use in practical designs and analyses.\n\nThe word \"enthalpy\" was coined relatively late, in the early 20th century, in analogy with the 19th-century terms \"energy\" (introduced in its modern sense by Thomas Young in 1802) and \"entropy\" (coined in analogy to \"energy\" by Rudolf Clausius in 1865).\nIt uses the root of Greek \"warmth, heat\" \nwhere \"energy\" has \"work\" and \"entropy\" \"transformation\", by analogy expressing the idea of \"heat-content\" where energy refers to \"work-content\" and entropy to \"transformation-content\".\nThe term does in fact stand in for the older term \"heat content\", \na term which is now mostly deprecated as misleading, as refers to the amount of heat absorbed in a process at constant pressure only, \nbut not in the general case (when pressure is variable). \nJosiah Willard Gibbs used the term \"a heat function for constant pressure\" for clarity.\n\nIntroduction of the concept of \"heat content\" is associated with Benoît Paul Émile Clapeyron and Rudolf Clausius (Clausius–Clapeyron relation, 1850).\n\nThe term \"enthalpy\" first appeared in print in 1909. It is attributed to Heike Kamerlingh Onnes, who most likely introduced it orally the year before, at the first meeting of the Institute of Refrigeration in Paris.\nIt gained currency only in the 1920s, notably with the \"Mollier Steam Tables and Diagrams\", published in 1927.\n\nUntil the 1920s, the symbol was used, somewhat inconsistently, for \"heat\" in general. \nThe definition of as strictly limited to enthalpy or \"heat content at constant pressure\" was formally proposed by Alfred W. Porter in 1922.\n\nThe enthalpy of a thermodynamic system is defined as\nwhere\nEnthalpy is an extensive property. This means that, for homogeneous systems, the enthalpy is proportional to the size of the system. It is convenient to introduce the specific enthalpy \"h\" = , where \"m\" is the mass of the system, or the molar enthalpy \"H\" = , where \"n\" is the number of moles (\"h\" and \"H\" are intensive properties). For inhomogeneous systems the enthalpy is the sum of the enthalpies of the composing subsystems:\n\nwhere the label \"k\" refers to the various subsystems. In case of continuously varying \"p\", \"T\" or composition, the summation becomes an integral:\n\nwhere \"ρ\" is the density.\n\nThe enthalpy of homogeneous systems can be viewed as function \"H\"(\"S\",\"p\") of the entropy \"S\" and the pressure \"p\", and a differential relation for it can be derived as follows. We start from the first law of thermodynamics for closed systems for an infinitesimal process:\n\nHere, \"δQ\" is a small amount of heat added to the system, and \"δW\" a small amount of work performed by the system. In a homogeneous system only reversible processes can take place, so the second law of thermodynamics gives , with \"T\" the absolute temperature of the system. Furthermore, if only \"pV\" work is done, . As a result,\n\nAdding \"d\"(\"pV\") to both sides of this expression gives\n\nor\n\nSo\n\nThe above expression of \"dH\" in terms of entropy and pressure may be unfamiliar to some readers. However, there are expressions in terms of more familiar variables such as temperature and pressure:\n\nHere \"C\" is the heat capacity at constant pressure and \"α\" is the coefficient of (cubic) thermal expansion:\n\nWith this expression one can, in principle, determine the enthalpy if \"C\" and \"V\" are known as functions of \"p\" and \"T\".\n\nNote that for an ideal gas, \"αT\" = 1, so that\n\nIn a more general form, the first law describes the internal energy with additional terms involving the chemical potential and the number of particles of various types. The differential statement for \"dH\" then becomes\n\nwhere \"μ\" is the chemical potential per particle for an \"i\"-type particle, and \"N\" is the number of such particles. The last term can also be written as (with \"dn\" the number of moles of component \"i\" added to the system and, in this case, \"μ\" the molar chemical potential) or as (with \"dm\" the mass of component \"i\" added to the system and, in this case, \"μ\" the specific chemical potential).\n\nThe \"U\" term can be interpreted as the energy required to create the system, and the \"pV\" term as the work that would be required to \"make room\" for the system if the pressure of the environment remained constant. When a system, for example, \"n\" moles of a gas of volume \"V\" at pressure \"p\" and temperature \"T\", is created or brought to its present state from absolute zero, energy must be supplied equal to its internal energy \"U\" plus \"pV\", where \"pV\" is the work done in pushing against the ambient (atmospheric) pressure.\n\nIn basic physics and statistical mechanics it may be more interesting to study the internal properties of the system and therefore the internal energy is used. In basic chemistry, experiments are often conducted at constant atmospheric pressure, and the pressure-volume work represents an energy exchange with the atmosphere that cannot be accessed or controlled, so that Δ\"H\" is the expression chosen for the heat of reaction.\n\nFor a heat engine a change in its internal energy is the difference between the heat input and the pressure-volume work done by the working substance while a change in its enthalpy is the difference between the heat input and the work done by the engine:\nwhere the work W done by the engine is: \n\nIn order to discuss the relation between the enthalpy increase and heat supply, we return to the first law for closed systems: . We apply it to the special case with a uniform pressure at the surface. In this case the work term can be split into two contributions, the so-called \"pV\" work, given by (where here \"p\" is the pressure at the surface, \"dV\" is the increase of the volume of the system) and all other types of work \"δW′\", such as by a shaft or by electromagnetic interaction. So we write . In this case the first law reads:\n\nor\n\nFrom this relation we see that the increase in enthalpy of a system is equal to the added heat:\n\nprovided that the system is under constant pressure (\"dp\" = 0) and that the only work done by the system is expansion work (\"δW\"' = 0).\n\nIn thermodynamics, one can calculate enthalpy by determining the requirements for creating a system from \"nothingness\"; the mechanical work required, \"pV\", differs based upon the conditions that obtain during the creation of the thermodynamic system.\n\nEnergy must be supplied to remove particles from the surroundings to make space for the creation of the system, assuming that the pressure \"p\" remains constant; this is the \"pV\" term. The supplied energy must also provide the change in internal energy, \"U\", which includes activation energies, ionization energies, mixing energies, vaporization energies, chemical bond energies, and so forth. Together, these constitute the change in the enthalpy \"U\" + \"pV\". For systems at constant pressure, with no external work done other than the \"pV\" work, the change in enthalpy is the heat received by the system.\n\nFor a simple system, with a constant number of particles, the difference in enthalpy is the maximum amount of thermal energy derivable from a thermodynamic process in which the pressure is held constant.\n\nThe total enthalpy of a system cannot be measured directly, the \"enthalpy change\" of a system is measured instead. Enthalpy change is defined by the following equation:\n\nwhere\n\nFor an exothermic reaction at constant pressure, the system's change in enthalpy equals the energy released in the reaction, including the energy retained in the system and lost through expansion against its surroundings. In a similar manner, for an endothermic reaction, the system's change in enthalpy is equal to the energy \"absorbed\" in the reaction, including the energy \"lost by\" the system and \"gained\" from compression from its surroundings. If Δ\"H\" is positive, the reaction is endothermic, that is heat is absorbed by the system due to the products of the reaction having a greater enthalpy than the reactants. On the other hand, if Δ\"H\" is negative, the reaction is exothermic, that is the overall decrease in enthalpy is achieved by the generation of heat.\n\nFrom the definition of enthalpy as formula_19 the enthalpy change at constant pressure formula_20 However for most chemical reactions, the work term formula_21 is much smaller than the internal energy change formula_22 which is approximately equal to formula_23 As an example, for the combustion of carbon monoxide 2 CO(g) + O(g) → 2 CO(g), formula_24 = –566.0 kJ and formula_22 = –563.5 kJ. Since the differences are so small, reaction enthalpies are often loosely described as reaction energies and analyzed in terms of bond energies.\n\nThe specific enthalpy of a uniform system is defined as \"h\" = where \"m\" is the mass of the system. The SI unit for specific enthalpy is joule per kilogram. It can be expressed in other specific quantities by , where \"u\" is the specific internal energy, \"p\" is the pressure, and \"v\" is specific volume, which is equal to , where \"ρ\" is the density.\n\nAn enthalpy change describes the change in enthalpy observed in the constituents of a thermodynamic system when undergoing a transformation or chemical reaction. It is the difference between the enthalpy after the process has completed, i.e. the enthalpy of the products, and the initial enthalpy of the system, i.e. the reactants. These processes are reversible and the enthalpy for the reverse process is the negative value of the forward change.\n\nA common standard enthalpy change is the enthalpy of formation, which has been determined for a large number of substances. Enthalpy changes are routinely measured and compiled in chemical and physical reference works, such as the CRC Handbook of Chemistry and Physics. The following is a selection of enthalpy changes commonly recognized in thermodynamics.\n\nWhen used in these recognized terms the qualifier \"change\" is usually dropped and the property is simply termed \"enthalpy of 'process\"'. Since these properties are often used as reference values it is very common to quote them for a standardized set of environmental parameters, or standard conditions, including: \nFor such standardized values the name of the enthalpy is commonly prefixed with the term \"standard\", e.g. \"standard enthalpy of formation\".\n\nChemical properties:\n\nPhysical properties:\n\nIn thermodynamic open systems, matter may flow in and out of the system boundaries. The first law of thermodynamics for open systems states: The increase in the internal energy of a system is equal to the amount of energy added to the system by matter flowing in and by heating, minus the amount lost by matter flowing out and in the form of work done by the system:\n\nwhere \"U\" is the average internal energy entering the system, and \"U\" is the average internal energy leaving the system.\n\nThe region of space enclosed by the boundaries of the open system is usually called a control volume, and it may or may not correspond to physical walls. If we choose the shape of the control volume such that all flow in or out occurs perpendicular to its surface, then the flow of matter into the system performs work as if it were a piston of fluid pushing mass into the system, and the system performs work on the flow of matter out as if it were driving a piston of fluid. There are then two types of work performed: \"flow work\" described above, which is performed on the fluid (this is also often called \"pV work\"), and \"shaft work\", which may be performed on some mechanical device.\n\nThese two types of work are expressed in the equation\n\nSubstitution into the equation above for the control volume (cv) yields:\n\nThe definition of enthalpy, \"H\", permits us to use this thermodynamic potential to account for both internal energy and \"pV\" work in fluids for open systems:\n\nIf we allow also the system boundary to move (e.g. due to moving pistons), we get a rather general form of the first law for open systems. In terms of time derivatives it reads:\n\nwith sums over the various places \"k\" where heat is supplied, matter flows into the system, and boundaries are moving. The \"Ḣ\" terms represent enthalpy flows, which can be written as\n\nwith \"ṁ\" the mass flow and \"ṅ\" the molar flow at position \"k\" respectively. The term represents the rate of change of the system volume at position \"k\" that results in \"pV\" power done by the system. The parameter \"P\" represents all other forms of power done by the system such as shaft power, but it can also be e.g. electric power produced by an electrical power plant.\n\nNote that the previous expression holds true only if the kinetic energy flow rate is conserved between system inlet and outlet. Otherwise, it has to be included in the enthalpy balance. During steady-state operation of a device (\"see turbine, pump, and engine\"), the average may be set equal to zero. This yields a useful expression for the average power generation for these devices in the absence of chemical reactions:\n\nwhere the angle brackets denote time averages. The technical importance of the enthalpy is directly related to its presence in the first law for open systems, as formulated above.\n\nThe enthalpy values of important substances can be obtained using commercial software. Practically all relevant material properties can be obtained either in tabular or in graphical form. There are many types of diagrams, such as \"h\"–\"T\" diagrams, which give the specific enthalpy as function of temperature for various pressures, and \"h\"–\"p\" diagrams, which give \"h\" as function of \"p\" for various \"T\". One of the most common diagrams is the temperature–specific entropy diagram (\"T\"–\"s\"-diagram). It gives the melting curve and saturated liquid and vapor values together with isobars and isenthalps. These diagrams are powerful tools in the hands of the thermal engineer.\n\nThe points a through h in the figure play a role in the discussion in this section.\n\nOne of the simple applications of the concept of enthalpy is the so-called throttling process, also known as Joule-Thomson expansion. It concerns a steady adiabatic flow of a fluid through a flow resistance (valve, porous plug, or any other type of flow resistance) as shown in the figure. This process is very important, since it is at the heart of domestic refrigerators, where it is responsible for the temperature drop between ambient temperature and the interior of the refrigerator. It is also the final stage in many types of liquefiers.\n\nFor a steady state flow regime, the enthalpy of the system (dotted rectangle) has to be constant. Hence\n\nSince the mass flow is constant, the specific enthalpies at the two sides of the flow resistance are the same:\n\nthat is, the enthalpy per unit mass does not change during the throttling. The consequences of this relation can be demonstrated using the \"T\"–\"s\" diagram above. Point c is at 200 bar and room temperature (300 K). A Joule–Thomson expansion from 200 bar to 1 bar follows a curve of constant enthalpy of roughly 425 kJ/kg (not shown in the diagram) lying between the 400 and 450 kJ/kg isenthalps and ends in point d, which is at a temperature of about 270 K. Hence the expansion from 200 bar to 1 bar cools nitrogen from 300 K to 270 K. In the valve, there is a lot of friction, and a lot of entropy is produced, but still the final temperature is below the starting value!\n\nPoint e is chosen so that it is on the saturated liquid line with . It corresponds roughly with and . Throttling from this point to a pressure of 1 bar ends in the two-phase region (point f). This means that a mixture of gas and liquid leaves the throttling valve. Since the enthalpy is an extensive parameter, the enthalpy in f (\"h\") is equal to the enthalpy in g (\"h\") multiplied by the liquid fraction in f (\"x\") plus the enthalpy in h (\"h\") multiplied by the gas fraction in f . So\n\nWith numbers: , so \"x\" = 0.64. This means that the mass fraction of the liquid in the liquid–gas mixture that leaves the throttling valve is 64%.\n\nA power \"P\" is applied e.g. as electrical power. If the compression is adiabatic, the gas temperature goes up. In the reversible case it would be at constant entropy, which corresponds with a vertical line in the \"T\"–\"s\" diagram. For example, compressing nitrogen from 1 bar (point a) to 2 bar (point b) would result in a temperature increase from 300 K to 380 K. In order to let the compressed gas exit at ambient temperature \"T\", heat exchange, e.g. by cooling water, is necessary. In the ideal case the compression is isothermal. The average heat flow to the surroundings is \"Q̇\". Since the system is in the steady state the first law gives\n\nThe minimal power needed for the compression is realized if the compression is reversible. In that case the second law of thermodynamics for open systems gives\n\nEliminating \"Q̇\" gives for the minimal power\n\nFor example, compressing 1 kg of nitrogen from 1 bar to 200 bar costs at least . With the data, obtained with the \"T\"–\"s\" diagram, we find a value of 476 kJ/kg.\n\nThe relation for the power can be further simplified by writing it as\n\nWith , this results in the final relation\n\n\n\n"}
{"id": "1058787", "url": "https://en.wikipedia.org/wiki?curid=1058787", "title": "Erikson's stages of psychosocial development", "text": "Erikson's stages of psychosocial development\n\nErikson's stages of psychosocial development, as articulated in the second half of the 20th century by Erik Erikson in collaboration with Joan Erikson, is a comprehensive psychoanalytic theory that identifies a series of eight stages that a healthy developing individual should pass through from infancy to late adulthood.\n\nErikson's stage theory characterizes an individual advancing through the eight life stages as a function of negotiating his or her biological and sociocultural forces. Each stage is characterized by a psychosocial crisis of these two conflicting forces. If an individual does indeed successfully reconcile these forces (favoring the first mentioned attribute in the crisis), he or she emerges from the stage with the corresponding virtue. For example, if an infant enters into the toddler stage (autonomy vs. shame and doubt) with more trust than mistrust, he or she carries the virtue of hope into the remaining life stages. The challenges of stages not successfully completed may be expected to return as problems in the future. However, mastery of a stage is not required to advance to the next stage. The outcome of one stage is not permanent and can be modified by later experiences.\n\n\nThe first stage of Erik Erikson's theory centers around the infant's basic needs being met by the parents and how this interaction leads to trust or mistrust. Trust as defined by Erikson is \"an essential trustfulness of others as well as a fundamental sense of one's own trustworthiness.\" The infant depends on the parents, especially the mother, for sustenance and comfort. The child's relative understanding of world and society comes from the parents and their interaction with the child. A child's first trust is always with the parent or caregiver; whoever that might be, however, the caregiver is secondary whereas the parents are primary in the eyes of the child. If the parents expose the child to warmth, regularity, and dependable affection, the infant's view of the world will be one of trust. Should parents fail to provide a secure environment and to meet the child's basic needs; a sense of mistrust will result. Development of mistrust can lead to feelings of frustration, suspicion, withdrawal, and a lack of confidence.\n\nAccording to Erik Erikson, the major developmental task in infancy is to learn whether or not other people, especially primary caregivers, regularly satisfy basic needs. If caregivers are consistent sources of food, comfort, and affection, an infant learns trust — that others are dependable and reliable. If they are neglectful, or perhaps even abusive, the infant instead learns mistrust — that the world is an undependable, unpredictable, and possibly a dangerous place. While negative, having some experience with mistrust allows the infant to gain an understanding of what constitutes dangerous situations later in life; yet being at the stage of infant or toddler, it is a good idea not to put them in prolonged situations of mistrust: the child's number one needs are to feel safe, comforted, and well cared for.\n\nAs the child gains control over eliminative functions and motor abilities, they begin to explore their surroundings. Parents still provide a strong base of security from which the child can venture out to assert their will. The parents' patience and encouragement helps foster autonomy in the child. Children at this age like to explore the world around them and they are constantly learning about their environment. Caution must be taken at this age while children may explore things that are dangerous to their health and safety. \n\nAt this age children develop their first interests. For example, a child who enjoys music may like to play with the radio. Children who enjoy the outdoors may be interested in animals and plants. Highly restrictive parents, however, are more likely to instill in the child a sense of doubt, and reluctance to attempt new challenges. As they gain increased muscular coordination and mobility, toddlers become capable of satisfying some of their own needs. They begin to feed themselves, wash and dress themselves, and use the bathroom.\n\nIf caregivers encourage self-sufficient behavior, toddlers develop a sense of autonomy—a sense of being able to handle many problems on their own. But if caregivers demand too much too soon, or refuse to let children perform tasks of which they are capable, or ridicule early attempts at self-sufficiency, children may instead develop shame and doubt about their ability to handle problems.\n\nInitiative adds to autonomy the quality of planning, undertaking and attacking a task for the sake of just being active and on the move. The child is learning to master the world around them, learning basic skills and principles of physics. Things fall down, not up. Round things roll. They learn how to zip and tie, count and speak with ease. At this stage, the child wants to begin and complete their own actions for a purpose. Guilt is a confusing new emotion. They may feel guilty over things that logically should not cause guilt. They may feel guilt when this initiative does not produce desired results.\n\nThe development of courage and independence are what set preschoolers, ages three to six years of age, apart from other age groups. Young children in this category face the challenge of initiative versus guilt. As described in Bee and Boyd (2004), the child during this stage faces the complexities of planning and developing a sense of judgment. During this stage, the child learns to take initiative and prepare for leadership and goal achievement roles. Activities sought out by a child in this stage may include risk-taking behaviors, such as crossing a street alone or riding a bike without a helmet; both these examples involve self-limits. \n\nWithin instances requiring initiative, the child may also develop negative behaviors. These negative behaviors are a result of the child developing a sense of frustration for not being able to achieve a goal as planned and may engage in negative behaviors that seem aggressive, ruthless, and overly assertive to parents. Aggressive behaviors, such as throwing objects, hitting, or yelling, are examples of observable behaviors during this stage.\n\nPreschoolers are increasingly able to accomplish tasks on their own, and can start new things. With this growing independence comes many choices about activities to be pursued. Sometimes children take on projects they can readily accomplish, but at other times they undertake projects that are beyond their capabilities or that interfere with other people's plans and activities. If parents and preschool teachers encourage and support children's efforts, while also helping them make realistic and appropriate choices, children develop initiative—independence in planning and undertaking activities. But if, instead, adults discourage the pursuit of independent activities or dismiss them as silly and bothersome, children develop guilt about their needs and desires.\n\nThe aim to bring a productive situation to completion gradually supersedes the whims and wishes of play.\nThe fundamentals of technology are developed. The failure to master trust, autonomy, and industrious skills may cause the child to doubt his or her future, leading to shame, guilt, and the experience of defeat and inferiority. \n\nThe child must deal with demands to learn new skills or risk a sense of inferiority, failure, and incompetence.\n\n\"Children at this age are becoming more aware of themselves as individuals.\" They work hard at \"being responsible, being good and doing it right.\" They are now more reasonable to share and cooperate. Allen and Marotz (2003) also list some perceptual cognitive developmental traits specific for this age group. Children grasp the concepts of space and time in more logical, practical ways. They gain a better understanding of cause and effect, and of calendar time. At this stage, children are eager to learn and accomplish more complex skills: reading, writing, telling time. They also get to form moral values, recognize cultural and individual differences and are able to manage most of their personal needs and grooming with minimal assistance. At this stage, children might express their independence by talking back and being disobedient and rebellious.\n\nErikson viewed the elementary school years as critical for the development of self-confidence. Ideally, elementary school provides many opportunities to achieve the recognition of teachers, parents and peers by producing things—drawing pictures, solving addition problems, writing sentences, and so on. If children are encouraged to make and do things and are then praised for their accomplishments, they begin to demonstrate industry by being diligent, persevering at tasks until completed, and putting work before pleasure. If children are instead ridiculed or punished for their efforts or if they find they are incapable of meeting their teachers' and parents' expectations, they develop feelings of inferiority about their capabilities.\n\nAt this age, children start recognizing their special talents and continue to discover interests as their education improves. They may begin to choose to do more \nactivities to pursue that interest, such as joining a sport if they know they have athletic ability, or joining the band if they are good at music. If not allowed to discover their own talents in their own time, they will develop a sense of lack of motivation, low self-esteem, and lethargy. They may become \"couch potatoes\" if they are not allowed to develop interests.\n\nThe adolescent is newly concerned with how they appear to others. Superego identity is the accrued confidence that the outer sameness and continuity prepared in the future are matched by the sameness and continuity of one's meaning for oneself, as evidenced in the promise of a career. The ability to settle on a school or occupational identity is pleasant. In later stages of adolescence, the child develops a sense of sexual identity. As they make the transition from childhood to adulthood, adolescents ponder the roles they will play in the adult world. Initially, they are apt to experience some role confusion—mixed ideas and feelings about the specific ways in which they will fit into society—and may experiment with a variety of behaviors and activities (e.g. tinkering with cars, baby-sitting for neighbors, affiliating with certain political or religious groups). Eventually, Erikson proposed, most adolescents achieve a sense of identity regarding who they are and where their lives are headed.\n\nThe teenager must achieve identity in occupation, gender roles, politics, and, in some cultures, religion.\n\nErikson is credited with coining the term \"identity crisis\". Each stage that came before and that follows has its own 'crisis', but even more so now, for this marks the transition from childhood to adulthood. This passage is necessary because \"Throughout infancy and childhood, a person forms many identifications. But the need for identity in youth is not met by these.\" This turning point in human development seems to be the reconciliation between 'the person one has come to be' and 'the person society expects one to become'. This emerging sense of self will be established by 'forging' past experiences with anticipations of the future. In relation to the eight life stages as a whole, the fifth stage corresponds to the crossroads:\n\nWhat is unique about the stage of Identity is that it is a special sort of synthesis of earlier stages and a special sort of anticipation of later ones. Youth has a certain unique quality in a person's life; it is a bridge between childhood and adulthood. Youth is a time of radical change—the great body changes accompanying puberty, the ability of the mind to search one's own intentions and the intentions of others, the suddenly sharpened awareness of the roles society has offered for later life.\n\nAdolescents \"are confronted by the need to re-establish boundaries for themselves and to do this in the face of an often potentially hostile world\". This is often challenging since commitments are being asked for before particular identity roles have formed. At this point, one is in a state of 'identity confusion', but society normally makes allowances for youth to \"find themselves\", and this state is called 'the moratorium':\n\nThe problem of adolescence is one of role confusion—a reluctance to commit which may haunt a person into his mature years. Given the right conditions—and Erikson believes these are essentially having enough space and time, a psychosocial moratorium, when a person can freely experiment and explore—what may emerge is a firm sense of identity, an emotional and deep awareness of who he or she is.\n\nAs in other stages, bio-psycho-social forces are at work. No matter how one has been raised, one's personal ideologies are now chosen for oneself. Often, this leads to conflict with adults over religious and political orientations. Another area where teenagers are deciding for themselves is their career choice, and often parents want to have a decisive say in that role. If society is too insistent, the teenager will acquiesce to external wishes, effectively forcing him or her to ‘foreclose' on experimentation and, therefore, true self-discovery. Once someone settles on a worldview and vocation, will he or she be able to integrate this aspect of self-definition into a diverse society? According to Erikson, when an adolescent has balanced both perspectives of \"What have I got?\" and \"What am I going to do with it?\" he or she has established their identity:\n\nDependent on this stage is the ego quality of \"fidelity—the ability to sustain loyalties freely pledged in spite of the inevitable contradictions and confusions of value systems\". (Italics in original)\nGiven that the next stage (Intimacy) is often characterized by marriage, many are tempted to cap off the fifth stage at 20 years of age. However, these age ranges are actually quite fluid, especially for the achievement of identity, since it may take many years to become grounded, to identify the object of one's fidelity, to feel that one has \"come of age\". In the biographies \"Young Man Luther\" and \"Gandhi's Truth\", Erikson determined that their crises ended at ages 25 and 30, respectively:\n\nErikson does note that the time of Identity crisis for persons of genius is frequently prolonged. He further notes that in our industrial society, identity formation tends to be long, because it takes us so long to gain the skills needed for adulthood's tasks in our technological world. So… we do not have an exact time span in which to find ourselves. It doesn't happen automatically at eighteen or at twenty-one. A \"very\" approximate rule of thumb for our society would put the end somewhere in one's twenties.\n\nThe Intimacy vs. Isolation conflict is emphasized around the age of 30. At the start of this stage, identity vs. role confusion is coming to an end, though it still lingers at the foundation of the stage (Erikson, 1950). Young adults are still eager to blend their identities with friends. They want to fit in.\nErikson believes we are sometimes isolated due to intimacy. We are afraid of rejections such as being turned down or our partners breaking up with us. We are familiar with pain and to some of us rejection is so painful that our egos cannot bear it. Erikson also argues that \"Intimacy has a counterpart: Distantiation: the readiness to isolate and if necessary, to destroy those forces and people whose essence seems dangerous to our own, and whose territory seems to encroach on the extent of one's intimate relations\" (1950).\n\nOnce people have established their identities, they are ready to make long-term commitments to others. They become capable of forming intimate, reciprocal relationships (e.g. through close friendships or marriage) and willingly make the sacrifices and compromises that such relationships require. If people cannot form these intimate relationships—perhaps because of their own needs—a sense of isolation may result; arousing feelings of darkness and angst.\n\nGenerativity is the concern of guiding the next generation. Socially-valued work and disciplines are expressions of generativity. \n\nThe adult stage of generativity has broad application to family, relationships, work, and society. \"Generativity, then is primarily the concern in establishing and guiding the next generation... the concept is meant to include... productivity and creativity.\"\n\nDuring middle age the primary developmental task is one of contributing to society and helping to guide future generations. When a person makes a contribution during this period, perhaps by raising a family or working toward the betterment of society, a sense of generativity—a sense of productivity and accomplishment—results. In contrast, a person who is self-centered and unable or unwilling to help society move forward develops a feeling of stagnation—a dissatisfaction with the relative lack of productivity.\n\n\nAs we grow older and become senior citizens we tend to slow down our productivity and explore life as a retired person. It is during this time that we contemplate our accomplishments and are able to develop integrity if we see ourselves as leading a successful life. If we see our life as unproductive, or feel that we did not accomplish our life goals, we become dissatisfied with life and develop despair, often leading to depression and hopelessness.\n\nThe final developmental task is retrospection: people look back on their lives and accomplishments. They develop feelings of contentment and integrity if they believe that they have led a happy, productive life. They may instead develop a sense of despair if they look back on a life of disappointments and unachieved goals.\n\nThis stage can occur out of the sequence when an individual feels they are near the end of their life (such as when receiving a terminal disease diagnosis).\n\nJoan M. Erikson, who married and collaborated with Erik Erikson, added a ninth stage in \"The Life Cycle Completed: Extended Version\". Living in the ninth stage, she wrote, \"old age in one's eighties and nineties brings with it new demands, reevaluations, and daily difficulties\". Addressing these new challenges requires \"designating a new ninth stage\". Erikson was ninety-three years old when she wrote about the ninth stage.\n\nJoan Erikson showed that all the eight stages \"are relevant and recurring in the ninth stage\". In the ninth stage, the psychosocial crises of the eight stages are faced again, but with the quotient order reversed. For example, in the first stage (infancy), the psychosocial crisis was \"Trust vs. Mistrust\" with Trust being the \"syntonic quotient\" and Mistrust being the \"dystonic\". Joan Erikson applies the earlier psychosocial crises to the ninth stage as follows:\n\n\"Basic Mistrust vs. Trust: Hope\"\nIn the ninth stage, \"elders are forced to mistrust their own capabilities\" because one's \"body inevitably weakens\". Yet, Joan Erikson asserts that \"while there is light, there is hope\" for a \"bright light and revelation\".\n\n\"Shame and Doubt vs. Autonomy: Will\"\nNinth stage elders face the \"shame of lost control\" and doubt \"their autonomy over their own bodies\". So it is that \"shame and doubt challenge cherished autonomy\".\n\n\"Inferiority vs. Industry: Competence\"\nIndustry as a \"driving force\" that elders once had is gone in the ninth stage. Being incompetent \"because of aging is belittling\" and makes elders \"like unhappy small children of great age\".\n\n\"Identity confusion vs. Identity: Fidelity\"\nElders experience confusion about their \"existential identity\" in the ninth stage and \"a real uncertainty about status and role\".\n\n\"Isolation vs. Intimacy: Love\"\nIn the ninth stage, the \"years of intimacy and love\" are often replaced by \"isolation and deprivation\". Relationships become \"overshadowed by new incapacities and dependencies\".\n\n\"Stagnation vs. Generativity: Care\"\nThe generativity in the seventh stage of \"work and family relationships\", if it goes satisfactorily, is \"a wonderful time to be alive\". In one's eighties and nineties, there is less energy for generativity or caretaking. Thus, \"a sense of stagnation may well take over\".\n\n\"Despair and Disgust vs. Integrity: Wisdom\"\nIntegrity imposes \"a serious demand on the senses of elders\". Wisdom requires capacities that ninth stage elders \"do not usually have\". The eighth stage includes retrospection that can evoke a \"degree of disgust and despair\". In the ninth stage, introspection is replaced by the attention demanded to one's \"loss of capacities and disintegration\".\n\nLiving in the ninth stage, Joan Erikson expressed confidence that the psychosocial crisis of the ninth stage can be met as in the first stage with the \"basic trust\" with which \"we are blessed\".\n\nErikson was a student of Anna Freud, the daughter of Sigmund Freud, whose psychoanalytic theory and psychosexual stages contributed to the basic outline of the eight stages, at least those concerned with childhood. Namely, the first four of Erikson's life stages correspond to Freud's oral, anal, phallic, and latency phases, respectively. Also, the fifth stage of adolescence is said to parallel the genital stage in psychosexual development:\n\nAlthough the first three phases are linked to those of the Freudian theory, it can be seen that they are conceived along very different lines. Emphasis is not so much on sexual modes and their consequences as on the ego qualities which emerge from each stages. There is an attempt also to link the sequence of individual development to the broader context of society.\n\nErikson saw a dynamic at work throughout life, one that did not stop at adolescence. He also viewed the life stages as a cycle: the end of one generation was the beginning of the next. Seen in its social context, the life stages were linear for an individual but circular for societal development:\n\nIn Freud's view, development is largely complete by adolescence. In contrast, one of Freud's students, Erik Erikson (1902–1994) believed that development continues throughout life. Erikson took the foundation laid by Freud and extended it through adulthood and into late life.\nErikson's theory may be questioned as to whether his stages must be regarded as sequential, and only occurring within the age ranges he suggests. There is debate as to whether people only search for identity during the adolescent years or if one stage needs to happen before other stages can be completed. However, Erikson states that each of these processes occur throughout the lifetime in one form or another, and he emphasizes these \"phases\" only because it is at these times that the conflicts become most prominent.\n\nMost empirical research into Erikson has related to his views on adolescence and attempts to establish identity. His theoretical approach was studied and supported, particularly regarding adolescence, by James E. Marcia. Marcia's work has distinguished different forms of identity, and there is some empirical evidence that those people who form the most coherent self-concept in adolescence are those who are most able to make intimate attachments in early adulthood. This supports the part of Eriksonian theory, that suggests that those best equipped to resolve the crisis of early adulthood are those who have most successfully resolved the crisis of adolescence.\n\n\n"}
{"id": "10772", "url": "https://en.wikipedia.org/wiki?curid=10772", "title": "Fair use", "text": "Fair use\n\nFair use is a doctrine in the law of the United States that permits limited use of copyrighted material without having to first acquire permission from the copyright holder. Fair use is one of the limitations to copyright intended to balance the interests of copyright holders with the public interest in the wider distribution and use of creative works by allowing as a defense to copyright infringement claims certain limited uses that might otherwise be considered infringement.\n\nThe 1710 Statute of Anne, an act of the Parliament of Great Britain, created copyright law to replace a system of private ordering enforced by the Stationers' Company. The Statute of Anne did not provide for legal unauthorized use of material protected by copyright. In \"Gyles v Wilcox\", the Court of Chancery established the doctrine of \"fair abridgement,\" which permitted unauthorized abridgement of copyrighted works under certain circumstances. Over time, this doctrine evolved into the modern concepts of fair use and fair dealing. Fair use was a common-law doctrine in the U.S. until it was incorporated into the Copyright Act of 1976, .\n\nThe term \"fair use\" originated in the United States. Although related, the limitations and exceptions to copyright for teaching and library archiving in the U.S. are located in a different section of the statute. A similar-sounding principle, fair dealing, exists in some other common law jurisdictions but in fact it is more similar in principle to the enumerated exceptions found under civil law systems. Civil law jurisdictions have other limitations and exceptions to copyright.\n\nIn response to perceived over-expansion of copyrights, several electronic civil liberties and free expression organizations began in the 1990s to add fair use cases to their dockets and concerns. These include the Electronic Frontier Foundation (\"EFF\"), the American Civil Liberties Union, the National Coalition Against Censorship, the American Library Association, numerous clinical programs at law schools, and others. The \"Chilling Effects\" archive was established in 2002 as a coalition of several law school clinics and the EFF to document the use of cease and desist letters. Most recently, in 2006, Stanford University began an initiative called \"The Fair Use Project\" (FUP) to help artists, particularly filmmakers, fight lawsuits brought against them by large corporations.\n\nExamples of fair use in United States copyright law include commentary, search engines, criticism, parody, news reporting, research, and scholarship. Fair use provides for the legal, unlicensed citation or incorporation of copyrighted material in another author's work under a four-factor test.\n\nThe U.S. Supreme Court has traditionally characterized fair use as an affirmative defense, but in \"Lenz v. Universal Music Corp.\" (2015) (the \"dancing baby\" case), the U.S. Court of Appeals for the Ninth Circuit concluded that fair use was not merely a defense to an infringement claim, but was an expressly authorized right, and an exception to the exclusive rights granted to the author of a creative work by copyright law: \"Fair use is therefore distinct from affirmative defenses where a use infringes a copyright, but there is no liability due to a valid excuse, e.g., misuse of a copyright.\"\n\nThe four factors of analysis for fair use set forth above derive from the opinion of Joseph Story in \"Folsom v. Marsh\", in which the defendant had copied 353 pages from the plaintiff's 12-volume biography of George Washington in order to produce a separate two-volume work of his own. The court rejected the defendant's fair use defense with the following explanation:\n\nThe statutory fair use factors quoted above come from the Copyright Act of 1976, which is codified at . They were intended by Congress to restate, but not replace, the prior judge-made law. As Judge Pierre N. Leval has written, the statute does not \"define or explain [fair use's] contours or objectives.\" While it \"leav[es] open the possibility that other factors may bear on the question, the statute identifies none.\" That is, courts are entitled to consider other factors in addition to the four statutory factors.\n\nThe first factor is \"the purpose and character of the use, including whether such use is of a commercial nature or is for nonprofit educational purposes.\" To justify the use as fair, one must demonstrate how it either advances knowledge or the progress of the arts through the addition of something new.\n\nIn the 1841 copyright case Folsom v. Marsh, Justice Joseph Story wrote:\nA key consideration in recent fair use cases is the extent to which the use is \"transformative\". In the 1994 decision \"Campbell v. Acuff-Rose Music Inc\", the U.S. Supreme Court held that when the purpose of the use is transformative, this makes the first factor more likely to favor fair use. Before the \"Campbell\" decision, federal Judge Pierre Leval argued that transformativeness is central to the fair use analysis in his 1990 article, Toward a Fair Use Standard. \"Blanch v. Koons\" is another example of a fair use case that focused on transformativeness. In 2006, Jeff Koons used a photograph taken by commercial photographer Andrea Blanch in a collage painting. Koons appropriated a central portion of an advertisement she had been commissioned to shoot for a magazine. Koons prevailed in part because his use was found transformative under the first fair use factor.\n\nThe \"Campbell\" case also addressed the subfactor mentioned in the quotation above, \"whether such use is of a commercial nature or is for nonprofit educational purposes.\" In an earlier case, \"Sony Corp. of America v. Universal City Studios, Inc.\", the Supreme Court had stated that \"every commercial use of copyrighted material is presumptively . . . unfair.\" In \"Campbell\", the court clarified that this is not a \"hard evidentiary presumption\" and that even the tendency that commercial purpose will \"weigh against a finding of fair use . . . will vary with the context.\" The \"Campbell\" court held that hip-hop group 2 Live Crew's parody of the song \"Oh, Pretty Woman\" was fair use, even though the parody was sold for profit. Thus, having a commercial purpose does not preclude a use from being found fair, even though it makes it less likely.\n\nLikewise, the noncommercial purpose of a use makes it more likely to be found a fair use, but it does not make it a fair use automatically. For instance, in \"L.A. Times v. Free Republic\", the court found that the noncommercial use of \"Los Angeles Times\" content by the Free Republic Web site was not fair use, since it allowed the public to obtain material at no cost that they would otherwise pay for. Richard Story similarly ruled in \"Code Revision Commission and State of Georgia v. Public.Resource.Org, Inc.\" that despite the fact that it is a non-profit and didn't sell the work, the service profited from its unauthorized publication of the Official Code of Georgia Annotated because of \"the attention, recognition, and contributions\" it received in association with the work.\n\nAnother factor is whether the use fulfills any of the preamble purposes, also mentioned in the legislation above, as these have been interpreted as \"illustrative\" of transformative use.\n\nIt is arguable, given the dominance of a rhetoric of the \"transformative\" in recent fair use determinations, that the first factor and transformativeness in general have become the most important parts of fair use.\n\nAlthough the Supreme Court has ruled that the availability of copyright protection should not depend on the artistic quality or merit of a work, fair use analyses consider certain aspects of the work to be relevant, such as whether it is fictional or non-fictional.\n\nTo prevent the private ownership of work that rightfully belongs in the public domain, facts and ideas are not protected by copyright—only their particular expression or fixation merits such protection. On the other hand, the social usefulness of freely available information can weigh against the appropriateness of copyright for certain fixations. The Zapruder film of the assassination of President Kennedy, for example, was purchased and copyrighted by \"Time\" magazine. Yet its copyright was not upheld, in the name of the public interest, when \"Time\" tried to enjoin the reproduction of stills from the film in a history book on the subject in \"Time Inc v. Bernard Geis Associates\".\n\nIn the decisions of the Second Circuit in \"Salinger v. Random House\" and in \"New Era Publications Int'l v. Henry Holt & Co\", the aspect of whether the copied work has been previously published was considered crucial, assuming the right of the original author to control the circumstances of the publication of his work or preference not to publish at all. However, Judge Pierre N. Leval views this importation of certain aspects of France's \"droit moral d'artiste\" (moral rights of the artist) into American copyright law as \"bizarre and contradictory\" because it sometimes grants greater protection to works that were created for private purposes that have little to do with the public goals of copyright law, than to those works that copyright was initially conceived to protect. This is not to claim that unpublished works, or, more specifically, works not intended for publication, do not deserve legal protection, but that any such protection should come from laws about privacy, rather than laws about copyright. The statutory fair use provision was amended in response to these concerns by adding a final sentence: \"The fact that a work is unpublished shall not itself bar a finding of fair use if such finding is made upon consideration of all the above factors.\"\n\nThe third factor assesses the amount and substantiality of the copyrighted work that has been used. In general, the less that is used in relation to the whole, the more likely the use will be considered fair.\n\nUsing most or all of a work does not bar a finding of fair use. It simply makes the third factor less favorable to the defendant. For instance, in \"Sony Corp. of America v. Universal City Studios, Inc.\" copying entire television programs for private viewing was upheld as fair use, at least when the copying is done for the purposes of time-shifting. In \"Kelly v. Arriba Soft Corporation,\" the Ninth Circuit held that copying an entire photo to use as a thumbnail in online search results did not even weigh against fair use, \"if the secondary user only copies as much as is necessary for his or her intended use\".\n\nHowever, even the use of a small percentage of a work can make the third factor unfavorable to the defendant, because the \"substantiality\" of the portion used is considered in addition to the amount used. For instance, in \"Harper & Row v. Nation Enterprises,\", the U.S. Supreme Court held that a news article's quotation of fewer than 400 words from President Ford's 200,000-word memoir was sufficient to make the third fair use factor weigh against the defendants, because the portion taken was the \"heart of the work.\" This use was ultimately found not to be fair.\n\nThe fourth factor measures the effect that the allegedly infringing use has had on the copyright owner's ability to exploit his original work. The court not only investigates whether the defendant's specific use of the work has significantly harmed the copyright owner's market, but also whether such uses in general, if widespread, would harm the potential market of the original. The burden of proof here rests on the copyright owner, who must demonstrate the impact of the infringement on commercial use of the work.\n\nFor example, in \"Sony Corp v. Universal City Studios\", the copyright owner, Universal, failed to provide any empirical evidence that the use of Betamax had either reduced their viewership or negatively impacted their business. In \"Harper & Row,\" the case regarding President Ford's memoirs, the Supreme Court labeled the fourth factor \"the single most important element of fair use\" and it has enjoyed some level of primacy in fair use analyses ever since. Yet the Supreme Court's more recent announcement in \"Campbell v. Acuff-Rose Music Inc\" that \"all [four factors] are to be explored, and the results weighed together, in light of the purposes of copyright\" has helped modulate this emphasis in interpretation.\n\nIn evaluating the fourth factor, courts often consider two kinds of harm to the potential market for the original work.\n\n\nCourts recognize that certain kinds of market harm do not negate fair use, such as when a parody or negative review impairs the market of the original work. Copyright considerations may not shield a work against adverse criticism.\n\nAs explained by Judge Leval, courts are permitted to include additional factors in their analysis.\n\nOne such factor is acknowledgement of the copyrighted source. Giving the name of the photographer or author may help, but it does not automatically make a use fair. While plagiarism and copyright infringement are related matters, they are not identical. Plagiarism (using someone's words, ideas, images, etc. without acknowledgment) is a matter of professional ethics. Copyright is a matter of law, and protects exact expression, \"not\" ideas. One can plagiarize even a work that is not protected by copyright, for example by passing off a line from Shakespeare as one's own. Conversely, attribution prevents accusations of plagiarism, but it does not prevent infringement of copyright. For example, reprinting a copyrighted book without permission, while citing the original author, would be copyright infringement but not plagiarism.\n\nThe U.S. Supreme Court described fair use as an affirmative defense in \"Campbell v. Acuff-Rose Music, Inc.\" This means that in litigation on copyright infringement, the defendant bears the burden of raising and proving that the use was fair and not an infringement. Thus, fair use need not even be raised as a defense unless the plaintiff first shows (or the defendant concedes) a \"prima facie\" case of copyright infringement. If the work was not copyrightable, the term had expired, or the defendant's work borrowed only a small amount, for instance, then the plaintiff cannot make out a \"prima facie\" case of infringement, and the defendant need not even raise the fair use defense. In addition, fair use is only one of many limitations, exceptions, and defenses to copyright infringement. Thus, a \"prima facie\" case can be defeated without relying on fair use. For instance, the Audio Home Recording Act establishes that it is legal, using certain technologies, to make copies of audio recordings for non-commercial personal use.\n\nSome copyright owners claim infringement even in circumstances where the fair use defense would likely succeed, in hopes that the user will refrain from the use rather than spending resources in their defense. Strategic lawsuit against public participation (SLAPP) cases such as these—alleging copyright infringement, patent infringement, defamation, or libel—often come into conflict with the defendant's right to freedom of speech, and has prompted some jurisdictions to pass anti-SLAPP legislation which raises the plaintiff's burdens and risk.\n\nAlthough fair use ostensibly permits certain uses without liability, many content creators and publishers try to avoid a potential court battle by seeking a legally unnecessary license from copyright owners for \"any\" use of non-public domain material, even in situations where a fair use defense would likely succeed. The simple reason is that the license terms negotiated with the copyright owner may be much less expensive than defending against a copyright suit, or having the mere possibility of a lawsuit threaten the publication of a work in which a publisher has invested significant resources.\n\nFair use rights take precedence over the author's interest. Thus the copyright holder cannot use a non-binding disclaimer, or notification, to revoke the right of fair use on works. However, binding agreements such as contracts or licence agreements may take precedence over fair use rights.\n\nThe practical effect of the fair use doctrine is that a number of conventional uses of copyrighted works are not considered infringing. For instance, quoting from a copyrighted work in order to criticize or comment upon it or teach students about it, is considered a fair use. Certain well-established uses cause few problems. A teacher who prints a few copies of a poem to illustrate a technique will have no problem on all four of the above factors (except possibly on amount and substantiality), but some cases are not so clear. All the factors are considered and balanced in each case: a book reviewer who quotes a paragraph as an example of the author's style will probably fall under fair use even though they may sell their review commercially; but a non-profit educational website that reproduces whole articles from technical magazines will probably be found to infringe if the publisher can demonstrate that the website affects the market for the magazine, even though the website itself is non-commercial.\n\nFair use is decided on a case by case basis, on the entirety of circumstances. The same act done by different means or for a different purpose can gain or lose fair use status. Even repeating an identical act at a different time can make a difference due to changing social, technological, or other surrounding circumstances.\n\nThe case \"Oracle America, Inc. v. Google, Inc.\" revolves around the use of application programming interfaces (APIs) used to define functionality of the Java programming language, created by Sun Microsystems and now owned by Oracle Corporation. Google used the APIs definition and their structure, sequence and organization (SSO) in creating the Android to support the mobile device market. Oracle had sued Google in 2010 over both patent and copyright violations, but after two cycles, the case matter was narrowed down to whether Google's use of the definition and SSO of Oracle's Java APIs (determined to be copyrightable) was within fair use. The Federal Circuit Court of Appeals has ruled against Google, stating that while Google could defend its use in the nature of the copyrighted work, its use was not transformative, and more significantly, it commercially harmed Oracle as they were also seeking entry to the mobile market. The case, should this ruling hold, could have a significant impact on developing products for interoperability using APIs, such as with many open source projects.\n\nIn April 2006, the filmmakers of the Loose Change series were served with a lawsuit by Jules and Gédéon Naudet over the film's use of their footage, specifically footage of the firefighters discussing the collapse of the World Trade Center.\nWith the help of an intellectual property lawyer, the creators of Loose Change successfully argued that a majority of the footage used was for historical purposes and was significantly transformed in the context of the film. They agreed to remove a few shots that were used as B-roll and served no purpose to the greater discussion. The case was settled and a potential multimillion-dollar lawsuit was avoided.\n\nThis Film Is Not Yet Rated also relied on fair use to feature several clips from copyrighted Hollywood productions. The director had originally planned to license these clips from their studio owners but discovered that studio licensing agreements would have prohibited him from using this material to criticize the entertainment industry. This prompted him to invoke the fair use doctrine, which permits limited use of copyrighted material to provide analysis and criticism of published works.\n\nIn 2009, fair use appeared as a defense in lawsuits against filesharing. Charles Nesson argued that file-sharing qualifies as fair use in his defense of alleged filesharer Joel Tenenbaum. Kiwi Camara, defending alleged filesharer Jammie Thomas, announced a similar defense.\nHowever, the Court in the case at bar rejected the idea that file-sharing is fair use.\nA U.S. court case from 2003, \"Kelly v. Arriba Soft Corp.,\" provides and develops the relationship between thumbnails, inline linking and fair use. In the lower District Court case on a motion for summary judgment, Arriba Soft's use of thumbnail pictures and inline linking from Kelly's website in Arriba Soft's image search engine was found not to be fair use. That decision was appealed and contested by Internet rights activists such as the Electronic Frontier Foundation, who argued that it was fair use.\n\nOn appeal, the Ninth Circuit Court of Appeals found in favour of the defendant, Arriba Soft. In reaching its decision, the court utilized the statutory four-factor analysis. First, it found the purpose of creating the thumbnail images as previews to be sufficiently transformative, noting that they were not meant to be viewed at high resolution as the original artwork was. Second, the photographs had already been published, diminishing the significance of their nature as creative works. Third, although normally making a \"full\" replication of a copyrighted work may appear to violate copyright, here it was found to be reasonable and necessary in light of the intended use. Lastly, the court found that the market for the original photographs would not be substantially diminished by the creation of the thumbnails. To the contrary, the thumbnail searches could increase the exposure of the originals. In looking at all these factors as a whole, the court found that the thumbnails were fair use and remanded the case to the lower court for trial after issuing a revised opinion on July 7, 2003. The remaining issues were resolved with a default judgment after Arriba Soft had experienced significant financial problems and failed to reach a negotiated settlement.\n\nIn August 2008, Judge Jeremy Fogel of the Northern District of California ruled in \"Lenz v. Universal Music Corp.\" that copyright holders cannot order a deletion of an online file without determining whether that posting reflected \"fair use\" of the copyrighted material. The case involved Stephanie Lenz, a writer and editor from Gallitzin, Pennsylvania, who made a home video of her thirteen-month-old son dancing to Prince's song Let's Go Crazy and posted the video on YouTube. Four months later, Universal Music, the owner of the copyright to the song, ordered YouTube to remove the video under the Digital Millennium Copyright Act. Lenz notified YouTube immediately that her video was within the scope of fair use, and she demanded that it be restored. YouTube complied after six weeks, rather than the two weeks required by the Digital Millennium Copyright Act. Lenz then sued Universal Music in California for her legal costs, claiming the music company had acted in bad faith by ordering removal of a video that represented fair use of the song. On appeal, the Court of Appeals for the Ninth Circuit ruled that a copyright owner must affirmatively consider whether the complained of conduct constituted fair use before sending a takedown notice under the Digital Millennium Copyright Act, rather than waiting for the alleged infringer to assert fair use. 801 F.3d 1126 (9th Cir. 2015). \"Even if, as Universal urges, fair use is classified as an 'affirmative defense,' we hold—for the purposes of the DMCA—fair use is uniquely situated in copyright law so as to be treated differently than traditional affirmative defenses. We conclude that because 17 U.S.C. § 107 created a type of non-infringing use, fair use is \"authorized by the law\" and a copyright holder must consider the existence of fair use before sending a takedown notification under § 512(c).\"\n\nIn June 2011, Judge Philip Pro of the District of Nevada ruled in \"Righthaven v. Hoehn\" that the posting of an entire editorial article from the Las Vegas Review Journal in a comment as part of an online discussion was unarguably fair use. Judge Pro noted that \"Noncommercial, nonprofit use is presumptively fair. ... Hoehn posted the Work as part of an online discussion. ... This purpose is consistent with comment, for which 17 U.S.C. § 107 provides fair use protection. ... It is undisputed that Hoehn posted the entire work in his comment on the Website. ... wholesale copying does not preclude a finding of fair use. ... there is no genuine issue of material fact that Hoehn's use of the Work was fair and summary judgment is appropriate.\" On appeal, the Court of Appeals for the Ninth Circuit ruled that Righthaven did not even have the standing needed to sue Hoehn for copyright infringement in the first place.\n\nIn addition to considering the four fair use factors, courts deciding fair use cases also look to the standards and practices of the professional community where the case comes from. Among the communities are documentarians, librarians, makers of Open Courseware, visual art educators, and communications professors.\n\nSuch codes of best practices have permitted communities of practice to make more informed risk assessments in employing fair use in their daily practice. For instance, broadcasters, cablecasters, and distributors typically require filmmakers to obtain errors and omissions insurance before the distributor will take on the film. Such insurance protects against errors and omissions made during the copyright clearance of material in the film. Before the \"Documentary Filmmakers' Statement of Best Practices in Fair Use\" was created in 2005, it was nearly impossible to obtain errors and omissions insurance for copyright clearance work that relied in part on fair use. This meant documentarians had either to obtain a license for the material or to cut it from their films. In many cases, it was impossible to license the material because the filmmaker sought to use it in a critical way. Soon after the best practices statement was released, all errors and omissions insurers in the U.S. shifted to begin offering routine fair use coverage.\n\nBefore 1991, sampling in certain genres of music was accepted practice and the copyright considerations were viewed as largely irrelevant. The strict decision against rapper Biz Markie's appropriation of a Gilbert O'Sullivan song in the case \"Grand Upright Music, Ltd. v. Warner Bros. Records Inc.\" changed practices and opinions overnight. Samples now had to be licensed, as long as they rose \"to a level of legally cognizable appropriation.\" This left the door open for the \"de minimis\" doctrine, for short or unrecognizable samples; such uses would not rise to the level of copyright infringement, because under the \"de minimis\" doctrine, \"the law does not care about trifles.\" However, 3 years later, the Sixth Circuit effectively eliminated the \"de minimis\" doctrine in the \"Bridgeport Music, Inc. v. Dimension Films\" case, holding that artists must \"get a license or do not sample\". The Court later clarified that its opinion did not apply to fair use, but between \"Grand Upright\" and \"Bridgeport\", practice had effectively shifted to eliminate unlicensed sampling.\n\nProducers or creators of parodies of a copyrighted work have been sued for infringement by the targets of their ridicule, even though such use may be protected as fair use. These fair use cases distinguish between parodies, which use a work in order to poke fun at or comment on the work itself and satire, or comment on something else. Courts have been more willing to grant fair use protections to parodies than to satires, but the ultimate outcome in either circumstance will turn on the application of the four fair use factors.\n\nFor example, when Tom Forsythe appropriated Barbie dolls for his photography project \"Food Chain Barbie\" (depicting several copies of the doll naked and disheveled and about to be baked in an oven, blended in a food mixer, and the like), Mattel lost its copyright infringement lawsuit against him because his work effectively parodies Barbie and the values she represents. In \"Rogers v. Koons\", Jeff Koons tried to justify his appropriation of Art Rogers' photograph \"Puppies\" in his sculpture \"String of Puppies\" with the same parody defense. Koons lost because his work was not presented as a parody of Rogers' photograph in particular, but as a satire of society at large. This was insufficient to render the use fair.\n\nIn \"Campbell v. Acuff-Rose Music Inc\" the U.S. Supreme Court recognized parody as a potential fair use, even when done for profit. Roy Orbison's publisher, Acuff-Rose Music, had sued 2 Live Crew in 1989 for their use of Orbison's \"Oh, Pretty Woman\" in a mocking rap version with altered lyrics. The Supreme Court viewed 2 Live Crew's version as a ridiculing commentary on the earlier work, and ruled that when the parody was itself the product rather than mere advertising, commercial nature did not bar the defense. The \"Campbell\" court also distinguished parodies from satire, which they described as a broader social critique not intrinsically tied to ridicule of a specific work and so not deserving of the same use exceptions as parody because the satirist's ideas are capable of expression without the use of the other particular work.\n\nA number of appellate decisions have recognized that a parody may be a protected fair use, including the Second (\"Leibovitz v. Paramount Pictures Corp.\"); the Ninth (\"Mattel v. Walking Mountain Productions\"); and the Eleventh Circuits (\"Suntrust Bank v. Houghton Mifflin Co.\"). In the 2001 \"Suntrust Bank\" case, Suntrust Bank and the Margaret Mitchell estate unsuccessfully brought suit to halt the publication of \"The Wind Done Gone\", which reused many of the characters and situations from \"Gone with the Wind\" but told the events from the point of view of the enslaved people rather than the slaveholders. The Eleventh Circuit, applying \"Campbell\", found that \"The Wind Done Gone\" was fair use and vacated the district court's injunction against its publication.\n\nCases in which a satirical use was found to be fair include \"Blanch v. Koons\" and \"Williams v. Columbia Broadcasting Systems\".\n\nThe transformative nature of computer based analytical processes such as text mining, web mining and data mining has led many to form the view that such uses would be protected under fair use. This view was substantiated by the rulings of Judge Denny Chin in \"Authors Guild, Inc. v. Google, Inc.\", a case involving mass digitisation of millions of books from research library collections. As part of the ruling that found the book digitisation project was fair use, the judge stated \"Google Books is also transformative in the sense that it has transformed book text into data for purposes of substantive research, including data mining and text mining in new areas\".\n\nText and data mining was subject to further review in \"Authors Guild v. HathiTrust\", a case derived from the same digitization project mentioned above. Judge Harold Baer, in finding that the defendant's uses were transformative, stated that 'the search capabilities of the [HathiTrust Digital Library] have already given rise to new methods of academic inquiry such as text mining.\"\n\nThere is a substantial body of fair use law regarding reverse engineering of computer software, hardware, network protocols, encryption and access control systems.\n\nIn May 2015, Richard Prince displayed his art gallery at the Frieze Art Fair. His gallery consisted of screenshots of Instagram users' pictures with Prince's commentary photoshopped below in the comments section. Not one Instagram user authorized Prince to use their pictures, but because Prince added his own commentary, the pictures were considered original artwork. One of the pieces sold for $90,000. Further, the Gagosian Gallery, where the pictures were showcased stated that \"All images are subject to copyright.\"\n\nWhile U.S. fair use law has been influential in some countries, some countries have fair use criteria drastically different from those in the U.S., and some countries do not have a fair use framework at all. Some countries have the concept of fair dealing instead of fair use, while others use different systems of limitations and exceptions to copyright. Many countries have some reference to an exemption for educational use, though the extent of this exemption varies widely.\n\nSources differ on whether fair use is fully recognized by countries other than the United States. American University's \"infojustice.org\" published a compilation of portions of over 40 nations' laws that explicitly mention fair use or fair dealing, and asserts that some of the fair dealing laws, such as Canada's, have evolved (such as through judicial precedents) to be quite close to those of the United States. This compilation includes fair use provisions from Bangladesh, Israel, South Korea, the Philippines, Sri Lanka, Taiwan, Uganda, and the United States. However, Paul Geller's 2009 \"International Copyright Law and Practice\" says that while some other countries recognize similar exceptions to copyright, only the United States and Israel fully recognize the concept of fair use.\n\nIn November 2007, the Israeli Knesset passed a new Copyright Law that included a U.S.-style fair use exception. The law, which took effect in May 2008, permits the fair use of copyrighted works for purposes such as private study, research, criticism, review, news reporting, quotation, or instruction or testing by an educational institution. The law sets up four factors, similar to the U.S. fair use factors (see above), for determining whether a use is fair.\n\nOn September 2, 2009, the Tel Aviv District court ruled in \"The Football Association Premier League Ltd. v. Ploni\" that fair use is a user right. The court also ruled that streaming of live soccer games on the Internet is fair use. In doing so, the court analyzed the four fair use factors adopted in 2007 and cited U.S. case law, including \"Kelly v. Arriba Soft Corp.\" and \"Perfect 10, Inc. v. Amazon.com, Inc.\".\n\nAn amendment in 2012 to the section 13(2)(a) of the Copyright Act 1987 created an exception called 'fair dealing' which is not restricted in its purpose. The four factors for fair use as in US law are included.\n\nFair use exists in Polish law and is covered by the Polish copyright law articles 23 to 35.\n\nCompared to the United States, Polish fair use distinguishes between private and public use. In Poland, when the use is public, its use risks fines. The defendant must also prove that his use was private when accused that it was not, or that other mitigating circumstances apply. Finally, Polish law treats all cases in which private material was made public as a potential copyright infringement, where fair use can apply, but has to be proven by reasonable circumstances.\n\nSection 35 of the Singaporean Copyright Act 1987 has been amended in 2004 to allow a 'fair dealing' exception for any purpose. The four fair use factors similar to US law are included in the new section 35.\n\nThe Korean Copyright Act was amended to include a fair use provision, Article 35-3, in 2012. The law now states that, \"the copyrighted work may be used, among other things, for reporting, criticism, education, and research.\" Then, the law outlines a four-factor test similar to that used under U.S. law:\n\nFair dealing allows specific exceptions to copyright protections. The open-ended concept of fair use is generally not observed in jurisdictions where fair dealing is in place, although this does vary. Fair dealing is established in legislation in Australia, Canada, New Zealand, Singapore, India, South Africa and the United Kingdom, among others.\n\nWhile Australian copyright exceptions are based on the Fair Dealing system, since 1998 a series of Australian government inquiries have examined, and in most cases recommended, the introduction of a \"flexible and open\" Fair Use system into Australian copyright law. From 1998 to 2017 there have been eight Australian government inquiries which have considered the question of whether fair use should be adopted in Australia. Six reviews have recommended Australia adopt a \"Fair Use\" model of copyright exceptions: two enquiries specifically into the Copyright Act (1998, 2014); and four broader reviews (both 2004, 2013, 2016). One review (2000) recommended against the introduction of fair use and another (2005) issued no final report. Two of the recommendations were specifically in response to the stricter copyright rules introduced as part of the Australia–United States Free Trade Agreement (AUSFTA), while the most recent two, by the Australian Law Reform Commission (ALRC) and the Productivity Commission (PC) were with reference to strengthening Australia's \"digital economy\".\n\nThe \"Copyright Act of Canada\" establishes fair dealing in Canada, which allows specific exceptions to copyright protection. In 1985, the Sub-Committee on the Revision of Copyright rejected replacing fair dealing with an open-ended system, and in 1986 the Canadian government agreed that \"the present fair dealing provisions should not be replaced by the substantially wider 'fair use' concept\". Since then, the Canadian fair dealing exception has broadened. It is now similar in effect to U.S. fair use, even though the frameworks are different.\n\nCCH Canadian Ltd v. Law Society of Upper Canada [2004] 1 S.C.R. 339, is a landmark Supreme Court of Canada case that establishes the bounds of fair dealing in Canadian copyright law. The Law Society of Upper Canada was sued for copyright infringement for providing photocopy services to researchers. The Court unanimously held that the Law Society's practice fell within the bounds of fair dealing.\n\nWithin the United Kingdom, fair dealing is a legal doctrine that provides an exception to the nation's copyright law in cases where the copyright infringement is for the purposes of non-commercial research or study, criticism or review, or for the reporting of current events.\n\nA balanced copyright law provides an economic benefit to many high-tech businesses such as search engines and software developers. Fair use is also crucial to non-technology industries such as insurance, legal services, and newspaper publishers.\n\nOn September 12, 2007, the Computer and Communications Industry Association (CCIA), a group representing companies including Google Inc., Microsoft Inc., Oracle Corporation, Sun Microsystems, Yahoo! and other high-tech companies, released a study that found that fair use exceptions to US copyright laws were responsible for more than $4.5 trillion in annual revenue for the United States economy representing one-sixth of the total US GDP. The study was conducted using a methodology developed by the World Intellectual Property Organization.\n\nThe study found that fair use dependent industries are directly responsible for more than eighteen percent of US economic growth and nearly eleven million American jobs. \"As the United States economy becomes increasingly knowledge-based, the concept of fair use can no longer be discussed and legislated in the abstract. It is the very foundation of the digital age and a cornerstone of our economy,\" said Ed Black, President and CEO of CCIA. \"Much of the unprecedented economic growth of the past ten years can actually be credited to the doctrine of fair use, as the Internet itself depends on the ability to use content in a limited and unlicensed manner.\"\n\nFair Use Week is an international event that celebrates fair use and fair dealing. Fair Use Week was first proposed on a Fair Use Allies listserv, which was an outgrowth of the Library Code of Best Practices Capstone Event, celebrating the development and promulgation of ARL's \"Code of Best Practices in Fair Use for Academic and Research Libraries\". While the idea was not taken up nationally, Copyright Advisor at Harvard University, launched the first ever Fair Use Week at Harvard University in February 2014, with a full week of activities celebrating fair use. The first Fair Use Week included blog posts from national and international fair use experts, live fair use panels, fair use workshops, and a Fair Use Stories Tumblr blog, where people from the world of art, music, film, and academia shared stories about the importance of fair use to their community. The first Fair Use Week was so successful that in 2015 ARL teamed up with Courtney and helped organize the Second Annual Fair Use Week, with participation from many more institutions. ARL also launched an official Fair Use Week website, which was transferred from Pia Hunter, who attended the Library Code of Best Practices Capstone Event and had originally purchased the domain name fairuseweek.org.\n\n\n"}
{"id": "11529", "url": "https://en.wikipedia.org/wiki?curid=11529", "title": "Fermion", "text": "Fermion\n\nIn particle physics, a fermion is a particle that follows Fermi–Dirac statistics. These particles obey the Pauli exclusion principle. Fermions include all quarks and leptons, as well as all composite particles made of an odd number of these, such as all baryons and many atoms and nuclei. Fermions differ from bosons, which obey Bose–Einstein statistics.\n\nA fermion can be an elementary particle, such as the electron, or it can be a composite particle, such as the proton. According to the spin-statistics theorem in any reasonable relativistic quantum field theory, particles with integer spin are bosons, while particles with half-integer spin are fermions.\n\nIn addition to the spin characteristic, fermions have another specific property: they possess conserved baryon or lepton quantum numbers. Therefore, what is usually referred to as the spin statistics relation is in fact a spin statistics-quantum number relation.\n\nAs a consequence of the Pauli exclusion principle, only one fermion can occupy a particular quantum state at any given time. If multiple fermions have the same spatial probability distribution, then at least one property of each fermion, such as its spin, must be different. Fermions are usually associated with matter, whereas bosons are generally force carrier particles, although in the current state of particle physics the distinction between the two concepts is unclear. Weakly interacting fermions can also display bosonic behavior under extreme conditions. At low temperature fermions show superfluidity for uncharged particles and superconductivity for charged particles.\n\nComposite fermions, such as protons and neutrons, are the key building blocks of everyday matter.\n\nThe name fermion was coined by English theoretical physicist Paul Dirac from the surname of Italian physicist Enrico Fermi.\n\nThe Standard Model recognizes two types of elementary fermions: quarks and leptons. In all, the model distinguishes 24 different fermions. There are six quarks (up, down, strange, charm, bottom and top quarks), and six leptons (electron, electron neutrino, muon, muon neutrino, tau particle and tau neutrino), along with the corresponding antiparticle of each of these.\n\nMathematically, fermions come in three types:\nMost Standard Model fermions are believed to be Dirac fermions, although it is unknown at this time whether the neutrinos are Dirac or Majorana fermions (or both). Dirac fermions can be treated as a combination of two Weyl fermions. In July 2015, Weyl fermions have been experimentally realized in Weyl semimetals.\n\nComposite particles (such as hadrons, nuclei, and atoms) can be bosons or fermions depending on their constituents. More precisely, because of the relation between spin and statistics, a particle containing an odd number of fermions is itself a fermion. It will have half-integer spin.\n\nExamples include the following:\n\nThe number of bosons within a composite particle made up of simple particles bound with a potential has no effect on whether it is a boson or a fermion.\n\nFermionic or bosonic behavior of a composite particle (or system) is only seen at large (compared to size of the system) distances. At proximity, where spatial structure begins to be important, a composite particle (or system) behaves according to its constituent makeup.\n\nFermions can exhibit bosonic behavior when they become loosely bound in pairs. This is the origin of superconductivity and the superfluidity of helium-3: in superconducting materials, electrons interact through the exchange of phonons, forming Cooper pairs, while in helium-3, Cooper pairs are formed via spin fluctuations.\n\nThe quasiparticles of the fractional quantum Hall effect are also known as composite fermions, which are electrons with an even number of quantized vortices attached to them.\n\nIn a quantum field theory, there can be field configurations of bosons which are topologically twisted. These are coherent states (or solitons) which behave like a particle, and they can be fermionic even if all the constituent particles are bosons. This was discovered by Tony Skyrme in the early 1960s, so fermions made of bosons are named skyrmions after him.\n\nSkyrme's original example involved fields which take values on a three-dimensional sphere, the original nonlinear sigma model which describes the large distance behavior of pions. In Skyrme's model, reproduced in the large N or string approximation to quantum chromodynamics (QCD), the proton and neutron are fermionic topological solitons of the pion field.\n\nWhereas Skyrme's example involved pion physics, there is a much more familiar example in quantum electrodynamics with a magnetic monopole. A bosonic monopole with the smallest possible magnetic charge and a bosonic version of the electron will form a fermionic dyon.\n\nThe analogy between the Skyrme field and the Higgs field of the electroweak sector has been used to postulate that all fermions are skyrmions. This could explain why all known fermions have baryon or lepton quantum numbers and provide a physical mechanism for the Pauli exclusion principle.\n\n"}
{"id": "452577", "url": "https://en.wikipedia.org/wiki?curid=452577", "title": "Free body diagram", "text": "Free body diagram\n\nIn physics and engineering, a free body diagram (force diagram, or FBD) is a graphical illustration used to visualize the applied forces, movements, and resulting reactions on a body in a given condition. They depict a body or connected bodies with all of the applied forces and moments, as well as reactions, that act on that/those body(ies). The body may consist of multiple internal members, for example, a truss, or be a compact body such as a beam. A series of free bodies and other diagrams may be necessary to solve complex problems.\n\nFree body diagrams are used to visualize the forces and moments applied to a body and calculate the resulting reactions, in many types of mechanics problems. Most free body diagrams are used both to determine the loading of individual structural components as well as calculating internal forces within the structure in almost all engineering disciplines from Biomechanics to Structural.\nIn the educational environment, learning to draw a free body diagram is an important step in understanding certain topics in physics, such as statics, dynamics and other forms of classical mechanics.\n\nA free body diagram is not meant to be a scaled drawing. It is a diagram that is modified as the problem is solved. There is an art and flexibility to the process. The iconography of a free body diagram, not only how it is drawn but also how it is interpreted, depends upon how a body is modeled.\n\nFree body diagrams consist of:\n\nThe number of forces and moments shown in a free body diagram depends on the specific problem and the assumptions made; common assumptions are neglecting air resistance, friction and assuming rigid bodies. In statics all forces and moments must balance to zero; the physical interpretation of this is that if the forces and moments do not sum to zero the body is accelerating and the principles of statics do not apply. In dynamics the resultant forces and moments can be non-zero.\n\nFree body diagrams may not represent an entire physical body. Using what is known as a \"cut\" only portions of a body are selected for modeling. This technique exposes internal forces, making them external, therefore allowing analysis. This technique is often used several times, iteratively to peel back forces acting on a physical body. For example, a gymnast performing the iron cross: analyzing the ropes and the person lets you know the total force (body weight, neglecting rope weight, breezes, buoyancy, electrostatics, relativity, rotation of the earth, etc..). Then cut the person out and only show one rope. You get force direction. Then only look at the person, now you can get hand forces. Now only look at the arm to get the shoulder forces and moments, and on and on until the component you intend to analyze is exposed.\n\nA body may be modeled \nin three ways:\n\n\nConsider a body in free fall in a uniform gravitational field. The body may be\n\n\nAn FBD represents the body of interest and the external forces on it.\n\n\nTypically, however, a provisional free body sketch is drawn before all these things are known. After all, the whole point of the diagram is to help to determine magnitude, direction, and point of application of the external loads! Thus when a force arrow is originally drawn its length may not be meant to indicate the unknown magnitude. Its line may not correspond to the exact line of action. Even its direction may turn out to be wrong. Very often the original direction of the arrow may be directly opposite to the true direction. External forces known to be small that are known to have negligible effect on the result of the analysis, are sometimes omitted, but only after careful consideration or after other analysis proving it (e.g. buoyancy forces of the air in the analysis of a chair, or atmospheric pressure on the analysis of a frying pan).\n\nThe external; forces acting on the object include friction, gravity, normal force, drag, tension, or a human force due to pushing or pulling. When in a non-inertial reference frame (see coordinate system, below), fictitious forces, such as centrifugal pseudoforce are appropriate.\n\nA coordinate system is sometimes included, and is chosen according to convenience (or advantage). Savvy selection of coordinate frame may make defining the vectors simpler when writing the equations of motion. The \"x\" direction might be chosen to point down the ramp in an inclined plane problem, for example. In that case the friction force only has an \"x\" component, and the normal force only has a \"y\" component. The force of gravity will still have components in both the \"x\" and \"y\" direction: \"mg\"sin(\"θ\") in the \"x\" and \"mg\"cos(\"θ\") in the \"y\", where \"θ\" is the angle between the ramp and the horizontal.\n\nThere are some things that a free body diagram explicitly excludes. Although other sketches that include these things may be helpful in visualizing a problem, a proper free body diagram should \"not\" show:\n\nA free body diagram is analyzed by summing all of the forces, often accomplished by summing the forces in each of the axis directions. When the net force is zero, the body must be at rest or must be moving at a constant velocity (constant speed and direction), by Newton's first law. If the net force is not zero, then the body is accelerating in that direction according to Newton's second law.\n\nDetermining the sum of the forces is straightforward if all they are aligned with the coordinate frame's axes, but it is somewhat more complex if some forces are not aligned. It is often convenient to analyze the components of the forces, in which case the symbols ΣF and ΣF are used instead of ΣF. Forces that point at an angle to the diagram's coordinate axis can be broken down into two parts (or three, for three dimensional problems)—each part being directed along one of the axes—horizontally (\"F\") and vertically (\"F\").\n\nA simple free body diagram, shown above, of a block on a ramp illustrates this.\n\nSome care is needed in interpreting the diagram.\n\n"}
{"id": "6415715", "url": "https://en.wikipedia.org/wiki?curid=6415715", "title": "Group tournament ranking system", "text": "Group tournament ranking system\n\nIn a group tournament, unlike a knockout tournament, there is no scheduled decisive final match. Instead, all the competitors are ranked by examining the results of all the matches played in the tournament. Typically, points are awarded for each match, with competitors ranked based either on total number of points or average points per match. Usually each competitor finishes with an equal number of matches, in which case rankings by total points and by average points are equivalent at the end of the tournament, though not necessarily while it is in progress. Examples with unequal numbers of matches include the 1895 County Championship in English cricket, and the U.S. National Football League prior to 1972, when tie games were excluded from the winning percentage used for regular-season standings.\n\nIn two-competitor games where ties are rare or impossible, competitors are typically ranked by number of wins, with ties counting half; each competitor's listings are usually ordered wins–losses(–ties). Giving a half-point for a draw in chess was introduced in 1868 by the British Chess Association; previously, drawn games in chess tournaments were replayed. Where draws are more common, the award may be 2 points for a win and 1 for a draw, which is mathematically equivalent but avoids having half-points in the listings. These are usually ordered wins–draws–losses. If there are more than 2 competitors per match, points may be ordinal—for example, 3 for first, 2 for second, 1 for third. An extreme example of this is Formula One, where the top ten racers in each Grand Prix are given 25, 18, 15, 12, 10, 8, 6, 4, 2 and 1 respectively. \n\nSome games may have more complex ranking criteria. For example, in rugby union, bonus points may be awarded for scoring a certain number of tries in a match, usually four, or for losing by a relatively small margin, usually 7 (the value of a converted try) or less.\n\nAdditionally in many leagues, the governing body is able to penalize a competitor who has broken the league's rules (for instance by allowing an ineligible player to play) by deducting points from that competitor's total. Sometimes this deduction may be carried over to a following season, particularly if the infraction occurs during the off-season, meaning that the competitor will start the following season with a negative points total rather than zero.\n\nOfficial listings while a tournament is in progress may need to take account of competitors having played differing fractions of their schedules. Some use average points (such as the \"points percentage\" of the National Hockey League) and others total points (such as the English Premier League, although comparisons between teams typically mention where one has \"games in hand\" on the other). The games behind figure in Major League Baseball gives the same rank order as average points.\n\nIn association football, where draws are relatively common, many leagues give 3 points for a win and 1 for a draw in an attempt to encourage attacking play. Besides the traditional 2-1-0 points and newer 3-1-0 points systems for win-draw-loss, various other systems have been used to try to encourage attractive play. Some examples:\n\nSome leagues have used penalty shootouts after drawn games, in which case points will vary for regulation win — penalties win — penalties loss — regulation loss:\n\nIn FIBA (basketball)-sanctioned tournaments, where ties are impossible (a game goes into as many extra periods — or overtimes — as necessary to determine a winner), the following method is used:\nFor an example, see 2006 FIBA World Championship.\n\nIn the National Hockey League (and various other minor hockey leagues), where regular season games tied after three periods go into a five-minute sudden-death overtime period and then a shootout if needed, the following method is used:\n\n\nMost European ice hockey leagues including the KHL use an alteration to the NHL method that does not encourage regulation draws by awarding more combined points than regulation decisions. This system was also used at the 2010 Winter Olympics in the preliminary round-robin games:\n\n\nWhen competitors are level on points, there is usually some tiebreaker criterion. \n\nSometimes, however, ranking ties may stand: prior to 1994, the Five Nations Championship in rugby union could result in joint champions; likewise for the British Home Championship in association football until 1978. In college football in the United States, many conferences permit joint champions. However, if ranking within the conference determines eligibility for a postseason bowl game, tiebreak criteria will be required to separate the joint champions. Similarly, U.S. college conferences in other sports, notably basketball, use tiebreak criteria as needed to determine seeding in postseason conference tournaments.\n\nA tiebreaker may be a play-off, with extra matches between the tied competitors. This may be a full match or a reduced format such as a penalty shootout or speed chess. If there are more than two tied competitors in a 2-competitor game, the play-off may be a round-robin or knockout tournament. \n\nInstead of a playoff, the original matches may provide the tie-breaker criteria: \nSwiss system tournaments and variants thereof use a variety of tie-breaking criteria not found in other types of tournament which exploit features specific to the Swiss system: see tie-breaking in Swiss system tournaments. Chess and some Go tournaments use Swiss pairing.\n\n"}
{"id": "18259273", "url": "https://en.wikipedia.org/wiki?curid=18259273", "title": "Last injurious exposure rule", "text": "Last injurious exposure rule\n\nIn law, the last injurious exposure rule is the principle that when an occupational disease was caused by a succession of jobs, or could have been caused by any one of a succession of jobs, the most recent employer with the risk exposure is liable.\n"}
{"id": "27940157", "url": "https://en.wikipedia.org/wiki?curid=27940157", "title": "Marginal factor cost", "text": "Marginal factor cost\n\nIn microeconomics, the marginal factor cost (MFC) is the increment to total costs paid for a factor of production resulting from a one-unit increase in the amount of the factor employed. It is expressed in currency units per incremental unit of a factor of production (input), such as labor, per unit of time. In the case of the labor input, for example, if the wage rate paid is unaffected by the number of units of labor hired, the marginal factor cost is identical to the wage rate. However, if hiring another unit of labor drives up the wage rate that must be paid to all existing units of labor employed, then the marginal cost of the labor factor is higher than the wage rate paid to the last unit because it also includes the increment to the rates paid to the other units.\n\nThus for any factor the MFC is the change in total amount paid for all units of that factor divided by the change in the quantity of that factor employed.\n\nA firm that wants to optimize its profits hires each factor up to the point at which its marginal factor cost equals its marginal revenue product (MFC=MRP).\n"}
{"id": "4111503", "url": "https://en.wikipedia.org/wiki?curid=4111503", "title": "Mining simulator", "text": "Mining simulator\n\nA mining simulator is a system used to replicate elements of mining operations, for training or efficiency analysis. Mining simulation application can range from pure statistical analysis, to scale models, all the way to replica cabins of mining machinery mounted on pneumatic actuators surrounded by screens displaying three-dimensional imagery. These simulators rely on physics engines and geodata to accurately simulate the dynamics of the environment.\n\n"}
{"id": "21056", "url": "https://en.wikipedia.org/wiki?curid=21056", "title": "Moral equivalence", "text": "Moral equivalence\n\nMoral equivalence is a term used in political debate, usually to deny that a moral comparison can be made of two sides in a conflict, or in the actions or tactics of two sides.\n\nThe term had some currency in polemic debates about the Cold War, and currently the Arab–Israeli conflict. \"Moral equivalence\" began to be used as a polemic \"term-of-retort\" to \"moral relativism\", which had been gaining use as an indictment against political foreign policy that appeared to use only a situation-based application of widely held ethical standards.\n\nInternational conflicts are sometimes viewed similarly, and interested parties periodically urge both sides to conduct a ceasefire and negotiate their differences. However these negotiations may prove difficult in that both parties in a conflict believe that they are morally superior to the other, and are unwilling to negotiate on basis of moral equivalence.\n\nIn the Cold War context, the term was and is most commonly used by anti-Communists as an accusation of formal fallacy for leftist criticisms of United States foreign policy and military conduct.\n\nMany such people believed in the idea that the United States was intrinsically benevolent, that the extension of its power, influence and hegemony was an extension of benevolence and would bring freedom to those people subject to that hegemony. Therefore, those who opposed the United States were by definition evil, trying to deny its benevolence to people. The USSR and its allies, in contrast, practiced a totalitarian ideology. A territory under US hegemony thus would be freed from possibly being in the camp of the totalitarian power and would help to weaken it. Thus, all means were justified in keeping territories away from Soviet influence in this way. This extended to countries not under Soviet influence but instead said to be sympathetic at all in any way with it. Therefore, Chile under Salvador Allende was not under Soviet domination, but removing him would help weaken the USSR by removing a government ruled with the help of a Communist Party. The big picture, they would say, justified the tortures carried out by the Augusto Pinochet dictatorship as it served to weaken the totalitarian Communist camp and in time bring about the freedom of those under its domination.\n\nSome of those who criticized US foreign policy at the time contended that US power in the Cold War was used only to pursue an economically-driven agenda. They claim that the underlying economic motivation eroded any claims of moral superiority, leaving the hostile acts in (Korea, Hungary, Cuban Missile Crisis, Vietnam, Afghanistan, Nicaragua) to stand on their own. In contrast, those who justified US interventions in the Cold War period always cast these as being motivated by the need to contain totalitarianism and thus fulfilled a higher moral imperative.\n\nAn early popularizer of the expression was Jeane Kirkpatrick, who was United States ambassador to the United Nations in the Reagan administration. Kirkpatrick published an article called \"The Myth of Moral Equivalence\" in 1986, in which sharply criticized those who she alleged were claiming that there was \"no moral difference\" between the Soviet Union and democratic states. In fact, very few critics of United States policies in the Cold War era argued that there was a moral equivalence between the two sides. Communists, for instance, argued that the Soviet Union was morally superior to its adversaries. Kirkpatrick herself was one of the most outspoken voices calling for the US to support authoritarian military regimes in Central America that were responsible for major human rights violations. When four US churchwomen were raped and murdered by government soldiers in El Salvador, Kirkpatrick downplayed the gravity of the crime, remarking that 'the nuns were not just nuns, they were political activists'. According to Congressman Robert Torricelli, Reagan administration officials, including Kirkpatrick, deliberately suppressed information about government abuses in El Salvador: \"While the Reagan Administration was certifying human rights progress in El Salvador they knew the terrible truth that the Salvadoran military was engaged in a widespread campaign of terror and torture.\"\n\nLeftist critics usually argued that the United States itself created a \"moral equivalence\" when some of its actions, such as President Ronald Reagan's support for the \"Contra\" insurgency against the Sandinista government in Nicaragua, put it on the same level of immorality as the Soviet Union.\n\nMoral equivalence has featured in debates over NATO expansion, the overthrow of rogue states, the invasion of Iraq, and the War on Terror. Concepts of moral hierarchy have been applied to foreign policy challenges such as Islamic fundamentalists, anti-Israel powers, Russia, China, drug traffickers, and Serbian nationalists, among others.\n\n"}
{"id": "142142", "url": "https://en.wikipedia.org/wiki?curid=142142", "title": "Motion picture content rating system", "text": "Motion picture content rating system\n\nA motion picture content rating system is designated to classify films with regard to suitability for audiences in terms of issues such as sex, violence, substance abuse, profanity, impudence or other types of mature content. A particular issued rating can be called a certification, classification, certificate or rating. Ratings typically carry age recommendations in an advisory or restrictive capacity, and are often given in lieu of censorship. In some jurisdictions the legal obligation of administering the rating may be imposed on movie theaters.\n\nIn countries such as Australia and Singapore, an official government body decides on ratings; in other countries, such as the United States, it is done by industry committees with little if any official government status. In most countries, however, films that are considered morally offensive have been censored, restricted, or banned. Even if the film rating system has no legal consequences, and a film has not explicitly been restricted or banned, there are usually laws forbidding certain films, or forbidding minors to view them.\n\nThe influence of specific factors in deciding a rating varies from country to country. In countries such as the United States, films with strong sexual content tend to be restricted to older viewers, though those same films are very often considered suitable for all ages in countries such as France and Germany. In contrast, films with violent content which would be rated leniently in the United States and Australia are often subject to high ratings and sometimes even censorship in countries such as Germany and Finland.\n\nOther factors may or may not influence the classification process, such as being set within a non-fictional historical context, whether the film glorifies violence or drug use, whether said violence or drug use is carried out by the protagonist, with whom the viewer should empathize, or by the antagonist. In Germany, for example, films depicting explicit war violence in a real war context (such as the Second World War) are handled more leniently than films with purely fictional settings.\n\nA film may be produced with a particular rating in mind. It may be re-edited if the desired rating is not obtained, especially to avoid a higher rating than intended. A film may also be re-edited to produce an alternate version for other countries.\n\nA comparison of current film rating systems, showing age on the horizontal axis. Note however that the specific \"criteria\" used in assigning a classification can vary widely from one country to another. Thus a color code or age range cannot be directly compared from one country to another.\n\nKey:\n\n\nThrough its Advisory Commission of Cinematographic Exhibition (\"Comisión Asesora de Exhibición Cinematográfica\") the National Institute of Cinema and Audiovisual Arts (INCAA) issues ratings for films based on the following categories:\n\n\nThe Classification Board and Classification Review Board are government-funded organizations which classify all films that are released for public exhibition.\n\nFilms intended to inform, educate or instruct or concerned with sport, religion or music are exempt from classification provided they do not contain material that would result in an \"M\" rating or higher if submitted for classification.\n\nMotion pictures are rated by the Austrian Board of Media Classification (ABMC) for the Federal Ministry of Education, Arts and Culture (Bundesministerium für Unterricht, Kunst und Kultur). The recommendations made by the ABMC are generally not legally binding and there are nine sets of state laws on the cinema sector with different age provisions. The only exception is in the case of \"16\" rated films, since under Austrian law there is a legal age restriction on certain types of content i.e. discrimination, sexual abuse, glorification of violence etc. In addition to the ABMC's age recommendations, in the state of Vienna children under the age of 6 are only permitted to attend public film performances if they are accompanied.\n\nThe AMBC issues age recommendation from the following categories:\n\nThere are only two classifications for films publicly exhibited in Belgium issued by the Inter-Community Commission for Film Rating (; ). Films are prohibited to minors under the age of 16 unless passed by the commission. There is no mandatory rating system for video formats but 90% of video distribution abides by the voluntary Belgium Video Federation. It is basically the same as the system for theatrical exhibition, but also provides a \"12\" rating.\n\n\nAll films that are exhibited in public or released on a home video format in Brazil must be submitted for classification to the advisory rating (\"Classificação Indicativa\", abbreviated ClassInd), which is run by the Brazilian Ministry of Justice (\"Ministério da Justiça\"). Anyone below the film's minimum age can watch it if accompanied by the parent or guardian who is at least 18 years old, except for those rated \"Not recommended for ages under 18\", which, by law, are strictly prohibited from viewing by people under 18. Unlike many countries, the ClassInd does not have any legal right to ban, demand cuts or refuse to rate any movie.\n\nThe ClassInd uses the following system:\n\nThere are also operational descriptions of attenuating and aggravating elements that can interfere on the final rating.\n\nThe Bulgarian film rating system is defined in the Film Industry Act of 2003 and administered by the National Film Rating Committee.\n\n\nFilm ratings in Canada are a provincial responsibility, and each province has its own legislation, rules and regulations regarding rating, exhibition and admission. Ratings are required for theatrical exhibition, but not all provinces require classification for home video. In the past there was a wide range of rating categories and practices in the various provinces; however, the seven rating systems—with the exception of Quebec—now all use categories and logos derived from the Canadian Home Video Rating System (CHVRS).\n\nThe categories are mostly identical to the CHVRS with a few minor variations. In the provinces that require classification of video formats, supply of 14A and 18A films is restricted to customers above those ages. In the case of theater exhibition, children are admitted to 14A and 18A films in the Manitoba and Maritime provinces if accompanied by an adult, although admittance is restricted to children over the age of 14 in the case of 18A films. Likewise, British Columbia, Saskatchewan (administered by the British Columbia Film Classification Office), Alberta and Ontario also admit children to 14A and 18A films if accompanied, but do not impose an age restriction on 18A films. The Maritimes and British Columbia (along with Saskatchewan) also provide an \"A\" classification for adult content. Some provinces, such as Nova Scotia, reserve the right to prohibit films altogether.\n\nIn general, the categories are:\n\nIn Quebec, the Régie du cinéma rates all films and videos. The Régie is a governmental agency overseen by the Quebec Ministry of Culture and Communications; its purview devolves from the \"Cinema Act\" (chapter C-18.1). In some cases the Régie du cinéma may refuse to provide a classification, effectively banning the film. Educational and sports films are exempt from classification.\n\n\nFilms are classified by the Council of Cinematographic Classification (\"Consejo de Calificación Cinematográfica\") which is a central agency under the Ministry of Education. In 2002 legislation was enacted which reversed the ban on all 1,090 films that had previously been banned in Chile.\n\nThe age ratings are:\n\n\nThe age ratings may also be supplemented by the following content categories:\n\n\nPornographic films may only be exhibited at venues licensed for that purpose. Minors are not admitted to films with pornographic or excessively violent content.\n\nChina does not have a rating system. Only films that are passed as \"suitable for all ages\" are released although some exhibitors have introduced informal ratings. A March 2017 effective law on film does require non-violations of the lawful rights and interests of minors (Chinese: 未成年人) or harming the physical and psychological health of minors. However, in an interview with China Central Television in the same month, the State Administration of Press, Publication, Radio, Film and Television's film chief Mr. Zhang Hongsen said it was inaccurate for the media to label the guideline for minors as manual/euphemistic classification and it was a misinterpretation or over-interpretation of the new law.\n\nAs of June 22, 2005, the Ministry of Culture issued its new rating system. The classifications are:\n\n\nIn Denmark, the Media Council for Children and Young People currently rates films. Films do not have to be submitted for a rating and in such instances must be labelled a \"15\" (restricted to people aged 15 and above). Children aged 7 and above may attend any performance—including those restricted to older audiences—if they are accompanied by an adult.\n\nFilm classification in Estonia is regulated by the Child Welfare Act.\n\n\nFilms in Finland are classified by the National Audiovisual Institute. A minor up to 3 years younger than the age limit is permitted to see a film in a cinema when accompanied by an adult, except for 18-rated films. Films with an age rating may contain an additional marker for violence, sex, fear, or substance abuse. The ratings are as follows:\n\n\nPrior to showing in theaters, a distribution certificate must be obtained from the Ministry of Culture. The Minister will decide which certificate to issue based on a recommendation by the National Center of Cinematography and the moving image (CNC) classification. In some cases films may be classified as \"pornographic films or those containing an incitement to violence\" or completely prohibited from screening. A certificate will be granted from the following:\n\n\nThe Freiwillige Selbstkontrolle der Filmwirtschaft (Voluntary Self-Regulation of the Film Industry, FSK) has a film ratings system under which films are classified. All the ratings contain the phrase \"gemäß §14 JuSchG\" (in accordance with §14 of the Youth Protection Law), signifying that they are legally binding for minors. Cinemas may legally exhibit films without a classification but minors are prohibited from such screenings.\n\n\nThe FSK rating also limits the time of the day in which the movie may be aired on free-to-air TV stations to a time frame between 22:00 (FSK 16) or 23:00 (FSK 18) and 6:00. Stations are permitted to broadcast films not approved for audiences under 12 at their own discretion.\n\nAll publicly released films must be submitted to the Youth Committee for classification. There are four categories:\n\n\nFilms intended for public exhibition have to be submitted to the Director of Film, Newspaper and Article Administration, who is the Film Censorship Authority (FCA) under the Ordinance, for approval. Films approved for public exhibition are then either classified or exempted from classification.\n\n\nOf the four levels, Levels I, II, and II are unrestricted. Only Level III is a restricted category and regulated by the Government.\n\nHungarian ratings are decided by the National Media and Infocommunications Authority (NMHH):\n\n\nThe current one is the third motion picture rating system in Hungary. The first system existed between 1965 and 2004, and was administered by the Ministry for National Cultural Heritage and its predecessors. Its categories were \"Without age restriction\", \"Not recommended below age of 14\", \"Above age of 16 only\", and \"Above age of 18 only\". A second system was introduced in 2004 which was overhauled in 2011 in favour of the current system. Its categories—given by the National Film Office—were \"Without age restriction\", \"Parental guidance suggested below age of 12\", \"Not recommended below age of 16\", \"Not recommended below age of 18\", and \"For adults only\".\n\nSince July 1, 2006, FRÍSK (short for Félag rétthafa í sjónvarps- og kvikmyndaiðnaði) has replaced the Kvikmyndaskoðun system in Iceland. In October 2013, FRÍSK announced that it was adopting a new system similar to the Netherlands' Kijkwijzer at least through 2016. The Icelandic ratings system also provides an \"18\" rating in addition to the Kijkwijzer ratings. Under Icelandic law, minors aged 14-years-old and over may be admitted to a film carrying a higher age rating if accompanied by an adult.\n\n\nIn India, Central Board of Film Certification (CBFC) is responsible for certifying films meant for public exhibition.\n\n\nMotion pictures shown in Indonesia must undergo reviewing by the Indonesian Film Censorship Board. Other than issuing certificates, the LSF/IFCB also reviews and issues permits for film-related advertising, such as movie trailers and posters. LSF has the authority to cut scenes from films. Films passed for exhibition are awarded one of the following classifications:\n\nAll films that are exhibited in public or released on a home video format must be submitted for classification to the Irish Film Classification Office (IFCO).\n\n\nAll films aimed to be shown in Italy are classified by the Committee for the Theatrical Review of the Italian Ministry of Cultural Heritage and Activities into one of the following categories:\n\n\nFilm classification in Jamaica is a requirement of the Cinematograph Act of 1913, which also established the Cinematograph Authority.\n\n\nA Japanese film rating regulator known as [full-name: ] has a film classification system under which films are classified into one of four categories. The categories have been in use since 1 May 1998.\n\nIn Kazakhstan, films are rated by the Committee for Culture of the Ministry for Culture and Information.\n\n\nIn Latvia it is the duty of the producer of a film or distributor to assign a rating according to a pre-determined set of criteria. All publicly exhibited films, visual recordings and films broadcast over television and electronic networks must be classified.\n\n\nHistorically, film censorship in Malaysia was carried out by police under the Theatre Ordinance 1908. In 1954 the Film Censorship Board (LPF) was created to censor films distributed across Malaysia in accordance with the Cinematograph Films Act 1952, and later the Film Censorship Act 2002. Malaysia's motion picture rating system was introduced in 1953, initially classifying films either for General Audiences (\"Tontonan Umum\") or For Adults Only (\"Untuk Orang Dewasa Sahaja\"), and in 1996 these classifications were changed to U and four different 18 categories. In mid-April 2010, the four 18 categories were deprecated, and was simplified to just 18. In late 2008, the PG13 classification was introduced, which was changed to P13 in 2012.\n\nUpon viewing the board will assign one of three categories to the film:\n\n\nShould a film be approved, the Board then assigns the film a classification. As of 2012 the ratings are:\n\n\nFilm in the Maldives are classified by the National Bureau of Classification. Certificates issued are based on the following categories:\n\nAs of 2012, films in Malta are classified by the Film Board in accordance with the Malta Council for Culture and the Arts Act. As part of an overhaul in 2013 the \"14\" and \"16\" age classifications were replaced by \"12A\" and \"15\"; the \"PG\" rating was redefined while \"U\", \"12\" and \"18\" were retained in their existing form.\n\nIf the film is deemed \"fit for exhibition\" it will be awarded one of the following classifications:\n\n\nThe General Directorate of Radio, Television and Cinematography (in Spanish, \"Dirección General de Radio, Televisión y Cinematografía\") is the issuer of ratings for motion pictures. The RTC is an agency of the Department of State (\"Secretaría de Gobernación\"). It has its own classification system, as follows:\n\n\nIn the Netherlands, the Kijkwijzer system is used, which is executed by the Netherlands Institute for the Classification of Audiovisual Media (NICAM). Under Dutch law children are admitted to films carrying an age rating if accompanied by an adult except in the case of \"16\" rated films.\n\n\nMostly, these icons are used along with other symbols, displaying if a movie contains violence, sexual content, frightening scenes, drug or alcohol abuse, discrimination, or coarse language. These symbols are also used for TV-programs in the Netherlands.\n\nThe \"Films, Videos, and Publications Classification Act 1993\" gives the Office of Film and Literature Classification the power to classify publications into three categories: unrestricted, restricted, or \"objectionable\" (banned). With a few exceptions, films, videos, DVDs and restricted computer games must carry a label before being offered for supply or exhibited to the public.\n\nIn 2017 the Office of Film and Literature Classification created a special RP18 rating for online content in response to the Netflix television series, \"13 Reasons Why\". The new classification reflects concerns raised with 17 and 18 year olds in New Zealand being at a higher risk of suicide. The current ratings are:\n\n\nThe National Film and Video Censors Board classifies films, videos, DVDs, and VCDs. Classifications carrying an age rating are legally restricted, although the \"15\" and \"18\" classifications do not apply to people below 2 years of age. The categories are:\n\nThe Norwegian Media Authority (\"Medietilsynet\") sets the age limits on films to be exhibited in Norway. Films not submitted to the Media Authority for classification carry a mandatory age rating of \"18\".\n\nThe following age limits apply to films to be shown in cinemas:\n\n\nThe Media Authority has no power to ban films but must not classify films which they consider contravene Norwegian criminal law.\n\nIn the Philippines, motion pictures, along with television programs, are rated by the Movie and Television Review and Classification Board, a special agency of the Office of the President. As of 2012, the Board uses six classification ratings.\n\n\nRatings in Poland are not set by any board or advisory body. Prior to 1989 the applicable age ratings were \"no age limit\", \"over 7\", \"over 12\", \"over 15\" and \"over 18\" and were set by The General Committee of Cinematography. Since 1989 there is no official classification system, with age ratings being self-prescriptive and set by the distributors. In case of television, the supervisory body – Krajowa Rada Radiofonii i Telewizji (KRRiT, The National Council of Radio Broadcasting and Television) can impose fines upon those responsible for improper rating of a broadcast, or lack of it.\n\nMovies are rated in Portugal by the Comissão de Classificação de Espectáculos of the Ministry of Culture. In cinemas the ratings are mandatory (subject to parental guidance) whereas for video releases they are merely advisory, except in the case of pornographic content. Children under the age of 3 were previously prohibited from public film performances, but a special category was introduced for this age group when the classification system was overhauled in 2014. A category for 14-year-olds was also introduced, and the lowest age rating was dropped from 4 years of age to 3. The categories are the following:\n\n\nRatings in Romania are set by the National Center of Cinematography () (CNC).\n\n\nSince 2012 the rating appears inside circles, which indicate age restrictions followed by a plus(+), and appears in most shows, including TV and Internet shows in Russian.\n\nThe indication shown:\n\nFilm classification in Singapore was introduced on 1 July 1991 and comes under the jurisdiction of the Board of Film Censors (BFC), currently part of the Info-communications Media Development Authority (IMDA). There were three ratings originally: G (General), PG (Parental Guidance) and R (Restricted to 18 years and above). Prior to then films were either approved or effectively banned. Since then, there have been several alterations to the ratings over the years. In September 1991, a Restricted (Artistic) (R(A)) rating was introduced to replaced the previous R-rating so as to allow the screening of certain art-house films which would otherwise have been banned without said rating, with an increased age restriction set at 21 years of age. The R(A) rating has since been replaced by NC16 (No Children under 16), M18 (Mature 18) and R21 (Restricted 21). A PG13 (Parental Guidance 13) rating, introduced in 2011, is the latest rating to be introduced. The G, PG and PG13 ratings are advisory while NC16, M18 and R21 carry age restrictions. Video ratings are mostly the same as the cinema ratings, except only go up to M18. Some titles, such as documentaries, children's programmes and sports programmes may be exempt from classification on video, but all titles must be classified for public theatrical exhibition. \n\nThe categories are:\n\nIn South Africa, films are classified by the Film and Publication Board. Distributors and exhibitors are legally compelled to comply with the age ratings. All broadcasters, cinemas and distributors of DVD/video and computer games must comply with the following:\n\n\nThere are also sub-descriptors used with some of the ratings:\n\n\nThe Korea Media Rating Board (영상물등급위원회) in Seoul divides licensed films into the following categories:\n\nAll films to be commercially released in Spain in any medium must be submitted to the ICAA (Instituto de Cinematografía y Artes Audiovisuales - Cinematography and Audiovisual Arts Institute). Classifications are advisory except for X-rated films, which are restricted to specially licensed venues. A supplementary classification, \"Especialmente Recomendada para la Infancia\" (Especially recommended for children), is sometimes appended to the lowest two classifications. Another supplementary classification, \"Especialmente recomendada para el fomento de la igualdad de género\" (Especially recommended for the promotion of gender equality), is sometimes appended to any of the classifications except the last one.\n\n\nThe Swedish Media Council (\"Statens medieråd\") is a government agency with the aims to reduce the risk of harmful media influences among minors and to empower minors as conscious media users. The classification bestowed on a film should not be viewed as recommendations on the suitability for children, as the law the council operates under (SFS 2010:1882) only mandates them to assess the relative risk to children's well-being. It is not a legal requirement to submit a film to the Media Council. The councils classification only applies to public exhibition, and the law does not require classification of home media.\n\nThe following categories are used:\n\n\nSwitzerland has adopted Germany's Freiwillige Selbstkontrolle der Filmwirtschaft (Voluntary Self-Regulation of the Film Industry, FSK). Under Swiss law, however, children up to two years younger than the age recommendations will be admitted if accompanied by a person invested with parental authority.\n\nFrom 1994 until 2015, the Government Information Office (GIO) classified films into four categories (General Audience/Protected/Parental Guidance/Restricted) pursuant to its issued ( in traditional Chinese): The \"Parental Guidance\" rating previously prohibited viewing by children under the age of 12 and required adolescents aged 12–17 to be accompanied by an adult. In 2015, the \"Parental Guidance\" rating was further divided into two categories: one that prohibits children under the age of 12 and one that prohibits adolescents under the age of 15.\n\nA motion picture rating system was proposed in the Film and Video Act of 2007, and was passed on December 20, 2007 by the Thai military-appointed National Legislative Assembly, replacing laws which had been in place since 1930. The draft law was met with resistance from the film industry and independent filmmakers. Activists had hoped for a less-restrictive approach; however, films are still subject to censorship, or can be banned from release altogether if the film is deemed to \"undermine or disrupt social order and moral decency, or might impact national security or the pride of the nation\".\n\nThe ratings were put into effect in August 2009. They are as follows:\n\n\nIn Turkey, movies to be shown in cinemas are rated by the Evaluation and Classification Board of the Ministry of Culture and Tourism. All films to be made commercially available must be classified, except in the case of educational films which are labeled as \"for educational purposes\" instead. The board also has the power to refuse classification in extreme cases (producers and distributors can submit an edited version of a movie to the board but edited versions may also be rejected if still deemed inappropriate); in this case, the movie will be banned with the exception of special artistic activities like fairs, festivals, feasts and carnivals.\n\n\nThe Ministry of Information of the United Arab Emirates classifies all films, which cinemas must abide by.\n\n\nThe British Board of Film Classification (BBFC) classifies films to be publicly exhibited in the United Kingdom, although statutory powers remain with local councils which can overrule any of the BBFC's decisions. Since 1984, the BBFC also classifies films made commercially available though a home video format. If the BBFC refuses a classification this effectively amounts to a ban (although local councils retain the legal right to overturn it in the case of cinema exhibition). The BBFC's regulatory powers do not extend to the Internet, so a film they have banned on physical media can still be made available via streaming media/video on demand. Videos designed to inform, educate or instruct or concerned with sport, religion or music are exempt from classification; exempt films may be marked as \"E\", but this is not an official label.\n\nThe current BBFC system is:\n\nIn the United States of America, film classification is a voluntary process with the ratings issued by the Motion Picture Association of America (MPAA) via the Classification and Rating Administration (CARA). The system was established in 1968, but the version listed below is the most recent revision, having been in effect since 1990. An unrated film is often informally denoted by \"NR\" in newspapers and so forth.\n\n\nAge ratings are divided into several categories. The age that corresponds to the category and the level of enforcement is defined by municipality ordinances.\n\nIn the San Cristóbal municipality the following ratings apply:\n\n\nIn the Baruta municipality the following ratings apply:\n\n\nIn the Maracaibo municipality children under the age of two are not admitted to performances and the ratings are enforced:\n\n\nAll theatrical releases are screened by the Cinema Department of the Ministry of Culture, Sport and Travel of Vietnam to ensure suitability for public viewing. Regardless of the rating, some scenes may be altered or removed to comply with regulations. The classification was revised in January 2017, replacing the previous rating system.\n\n\nUnlike the previous rating system, the current rating system does not have parental guidance and ratings other than P are considered to be restricted.\n\n\n"}
{"id": "25466062", "url": "https://en.wikipedia.org/wiki?curid=25466062", "title": "Natural Capital Initiative", "text": "Natural Capital Initiative\n\nThe Natural Capital Initiative (NCI) is a partnership of UK organisations which promotes and supports decision-making across government, business and the private sector that results in the sustainable management of our natural capital. Formed in 2009, it is a partnership between two of the UK's learned societies and two of its scientific research institutes, namely: \n\nIts stated mission is \"to support decision-making that results in the sustainable management of our natural capital based on sound science\". It aims to provide decision-makers from UK government, academia, business, and civil society with a high-level forum for cross-sectoral dialogue on how to realise the UK government's stated objective of embedding natural capital thinking in policy and practice, and it does this based upon sound evidence from across the natural and social sciences.\n\nThe four main objectives of the Natural Capital Initiative are to:\n\n\nThe NCI was established in 2009 in response to the increasing global awareness of the need for more sustainable management of the world's limited natural resources, as highlighted by the 2005 Millennium Ecosystem Assessment. It was launched in April 2009 at a three-day symposium held in London, entitled \"Valuing our Life Support Systems\". Its conclusions were published in August 2009.\n\nIn 2012 the James Hutton Institute announced it was joining the partnership.\n\nIn November 2014 the NCI held a second major symposium/summit, entitled: ‘Valuing our Life Support Systems’ with the aim of pushing forward the shared natural capital agenda.\n\nThe Chair of the NCI (as at 2016) is Professor Alison Hester of the James Hutton Institute\n\nThe Natural Capital Initiative has been recognised as one of the key groups or organisations in the UK now examining the main issues surrounding the depletion of natural resources and in encouraging the preservation of natural capital. \n\nIts 2009 symposium recommendations are included in the UK government own recommendations and resource documents to senior policy-makers.\n\nIn recent years the NCI's support for the development of high-level policy based on sound science, and in line with an ecosystems approach to decision-making, has transferred into reality in a number of ways. In particular, the Natural Capital Committee, supported by DEFRA, evolved out of the UK Government's 2011 'Natural Environment White Paper', as did the 48 Local Nature Partnerships now established across England which focus on sustainable land management, greening economic growth and human health and wellbeing.\n\nIn 2014 the NCI called on the UK government to introduce a natural capital accounting element into its national accounting framework, and supported the Natural Capital Committee's own proposal for a 25-year Government plan for the UK Natural Environment, which is now under development.\n\nIn 2015 the NCI published a symposium report, entitled 'Valuing our Life Support Systems', which was launched at a parliamentary event on 25 June. The launch was chaired by Barry Gardiner MP. The report spelled out the need for a methodology to be established for valuing natural capital. It also raised concerns there was a danger that natural capital could be treated as a commodity unless an ethical framework was in place to ensure the process of natural capital valuation was applied fairly and evenly. It urged that all natural capital concepts needed to be clear, evidence-based and robust, and it welcomed the Government's proposal to extend the life of the Natural Capital Committee until at least the end of that current parliament.\n\n"}
{"id": "2795942", "url": "https://en.wikipedia.org/wiki?curid=2795942", "title": "Negative cutting", "text": "Negative cutting\n\nNegative cutting (also known as negative matching and negative conforming) is the process of cutting motion picture negative to match precisely the final edit as specified by the film editor. Original camera negative (OCN) is cut with scissors and joined using a film splicer and film cement. Negative cutting is part of the post-production process and occurs after editing and prior to striking internegatives and release prints. The process of negative cutting has changed little since the beginning of cinema in the early 20th century. In the early 1980s computer software was first used to aid the cutting process. Kodak introduced barcode on motion picture negative in the mid-1990s. This enabled negative cutters to more easily track shots and identify film sections based on keykode.\n\nToward the late 1990s and early 2000s negative cutting changed due to the advent of digital cinema technologies such as digital intermediate (DI), digital projection and high-definition television. In some countries, due to the high cost of online suites, negative cutting is still used for commercials by reducing footage. Increasingly feature films are bypassing the negative cutting process altogether and are being scanned directly from the uncut rushes.\n\nThe existence of digital intermediates (DI) has created a new demand for negative cutters to extract selected takes which are cut from the rushes and re-spliced into new rolls (in edit order) to reduce the volume of footage for scanning.\n\nAfter a film shoot, the original camera negative (OCN) is sent to a film laboratory for processing. Two or three camera rolls are spliced together to create a lab roll approximately long. After developing the lab roll, it is put through a telecine to create a rushes transfer tape. This rushes transfer tape is of lower quality than film and is used for editing purposes only.\n\nThe rushes tape is sent to the Editor who loads it into an offline edit suite. The lab rolls are sent to the negative cutter for logging and storage.\n\nAfter the Editor finishes the Edit it is exported to an offline EDL list and the EDL list is sent to the negative cutter. The negative cutter will translate the Timecode in the EDL list to edge numbers (keykode) using specially designed negative cutting software to find which shot is needed from the rushes negative.\n\nTraditionally a negative cutter would then fine cut the negative to match the Editor's final edit frame accurately. Negative would be spliced together to create rolls less than which would then be sent to the film laboratory to print release prints.\n\nToday most feature films are extracted full takes (as selected takes) and scanned digitally as a digital intermediate. Television series and commercials shot on film follow the same extraction process but are sent for telecine. Each required shot is extracted from the lab roll as a full take and respliced together to create a new selected roll of negative. This reduces the negative required by up to 1/10 of the footage shot, saving considerable time during scanning or telecine. The negative cutter will create a new Online EDL list replacing the rushes roll timecode with the new selected roll timecode.\n\nIn the case of feature films the selected roll and Online EDL are sent to a post production facility for scanning as a digital intermediate. For television commercials or series the selected takes and EDL are sent to a post production facility for re-telecine and compiled in an Online Suite for final grading.\n\nThere have been a number of dedicated software systems that have been developed for and by negative cutters to manage the process of cutting motion picture negative. A number of individual proprietary software systems have been developed starting in the early 1980s. Stan Sztaba developed a system for World Cinevision Services Inc (New York) in 1983 using Apple II DOS and then ProDOS, this system is still used today. Elliott Gamson of Immaculate Matching (New York) developed a system using MS-DOS. Computamatch was one of the first MS-DOS-based systems developed and is still in use today in several countries.\n\nThe first commercially available software product was OSC/R (pronounced \"Oscar\"), a DOS-based application developed in Toronto, Canada by The Adelaide Works. OSC/R was very widely used and at the time was the only negative cutting software on the market until Adelaide Works ceased operation in 1993. OSC/R is still used today in some negative cutting facilities but has been mostly replaced by newer and more advanced systems. Excalibur was a later Windows 98 based product developed by FilmLab Engineering in Britain. Film Fusion is one of the most recent developments and is a Windows XP and Vista based system developed in Sydney, Australia by Popsoft IT.\n\nNegative cutters use various hardware tools such as film synchronizers, re-winders, film splicers, scissors, film cement and film keykode readers. DigiSync, a purpose built keykode reader is used by most negative cutters in conjunction with software for logging the keykode from film. DigiSync was developed by Research In Motion and in 1998 it won a Technical Achievement Academy Award for the design and development of the DigiSync Film Keykode Reader. Research In Motion later moved on to bigger things and invented the BlackBerry Wireless Email Phone and is now a publicly listed company. Other brands of barcode scanners are also in use.\n\n"}
{"id": "1762868", "url": "https://en.wikipedia.org/wiki?curid=1762868", "title": "Negative luminescence", "text": "Negative luminescence\n\nNegative luminescence is a physical phenomenon by which an electronic device emits less thermal radiation when an electric current is passed through it than it does in thermal equilibrium (current off). When viewed by a thermal camera, an operating negative luminescent device looks colder than its environment.\n\nNegative luminescence is most readily observed in semiconductors. Incoming infrared radiation is absorbed in the material by the creation of an electron–hole pair. An electric field is used to remove the electrons and holes from the region before they have a chance to recombine and re-emit thermal radiation. This effect occurs most efficiently in regions of low charge carrier density.\n\nNegative luminescence has also been observed in semiconductors in orthogonal electric and magnetic fields. In this case, the junction of a diode is not necessary and the effect can be observed in bulk material. A term that has been applied to this type of negative luminescence is \"galvanomagnetic luminescence\".\n\nNegative luminescence might appear to be a violation of Kirchhoff's law of thermal radiation. This is not true, as the law only applies in thermal equilibrium.\n\nAnother term that has been used to describe negative luminescent devices is \"Emissivity switch\", as an electric current changes the effective emissivity.\n\nThis effect was first seen by Russian physicists in the 1960s in A.F.Ioffe Physicotechnical Institute, Leningrad, Russia. Subsequently, it was studied in semiconductors such as indium antimonide (InSb), germanium (Ge) and indium arsenide (InAs) by workers in West Germany, Ukraine (Institute of Semiconductor Physics, Kiev), Japan (Chiba University) and the United States. It was first observed in the mid-infrared (3-5 µm wavelength) in the more convenient diode structures in InSb heterostructure diodes by workers at the Defence Research Agency, Great Malvern, UK (now QinetiQ). These British workers later demonstrated LWIR band (8-12 µm) negative luminescence using mercury cadmium telluride diodes.\n\nLater the Naval Research Laboratory, Washington DC, started work on negative luminescence in mercury cadmium telluride (HgCdTe). The phenomenon has since been observed by several university groups around the world.\n\n\n\n"}
{"id": "2471086", "url": "https://en.wikipedia.org/wiki?curid=2471086", "title": "Nippon Decimal Classification", "text": "Nippon Decimal Classification\n\nThe Nippon Decimal Classification (NDC, also called the Nippon Decimal System) is a system of library classification developed for mainly Japanese language books maintained by the Japan Library Association since 1956. It is based on the Dewey Decimal System. The system is based upon using each successive digit to divide into nine divisions with the digit zero used for those not belonging to any of the divisions.\n\nThe system is made up of ten categories:\n\n\n\n"}
{"id": "34082849", "url": "https://en.wikipedia.org/wiki?curid=34082849", "title": "Operating deflection shape", "text": "Operating deflection shape\n\nOperating deflection shape (ODS), is a term often used in the structural vibration analysis, known as ODS analysis. ODS analysis is a method used for visualisation of the vibration pattern of a machine or structure as influenced by its own operating forces. This is as opposed to the study of the vibration pattern of a machine under an (known) external force analysis, which is called modal analysis.\n"}
{"id": "4227727", "url": "https://en.wikipedia.org/wiki?curid=4227727", "title": "Panopticism", "text": "Panopticism\n\nPanopticism is a social theory named after the Panopticon, originally developed by French philosopher Michel Foucault in his book \"Discipline and Punish.\" The \"panopticon\" refers to an experimental laboratory of power in which behaviour could be modified, and Foucault viewed the panopticon as a symbol of the disciplinary society of surveillance.\n\nJeremy Bentham proposed the panopticon as a circular building with an observation tower in the centre of an open space surrounded by an outer wall. This wall would contain cells for occupants. This design would increase security by facilitating more effective surveillance. Residing within cells flooded with light, occupants would be readily distinguishable and visible to an official invisibly positioned in the central tower. Conversely, occupants would be invisible to each other, with concrete walls dividing their cells. Due to the bright lighting emitted from the watch tower, occupants would not be able to tell if and when they are being watched, making discipline a passive rather than an active action. Strangely, the cell-mates act in matters as if they are being watched, though they cannot be certain eyes are actually on them. There is a type of invisible discipline that reigns through the prison, for each prisoner self-regulates, in fear that someone is watching their every move. Although usually associated with prisons, the panoptic style of architecture might be used in other institutions with surveillance needs, such as schools, factories, or hospitals.\n\nIn \"Discipline and Punish\", Michel Foucault builds on Bentham's conceptualization of the panopticon as he elaborates upon the function of disciplinary mechanisms in such a prison and illustrates the function of discipline as an apparatus of power. The ever-visible inmate, Foucault suggests, is always \"the object of information, never a subject in communication\". He adds that,\n\n\"He who is subjected to a field of visibility, and who knows it, assumes responsibility for the constraints of power; he makes them play spontaneously upon himself; he inscribes in himself the power relation in which he simultaneously plays both roles; he becomes the principle of his own subjection\" (202-203).\n\nFoucault offers still another explanation for the type of \"anonymous power\" held by the operator of the central tower, suggesting that, \"We have seen that anyone may come and exercise in the central tower the functions of surveillance, and that this being the case, he can gain a clear idea of the way the surveillance is practiced\". By including the anonymous \"public servant,\" as part of the built-in \"architecture\" of surveillance, the disciplinary mechanism of observation is decentered and its efficacy improved.\n\nAs hinted at by the architecture, this panoptic design can be used for any \"population\" that needs to be kept under observation or control, such as: prisoners, schoolchildren, medical patients, or workers: \"If the inmates are convicts, there is no danger of a plot, an attempt at collective escape, the planning of new crimes for the future, bad reciprocal influences; if they are patients, there is no danger of contagion; if they are madmen there is no risk of their committing violence upon one another; if they are schoolchildren, there is no copying, no noise, no chatter, no waste of time; if they are workers, there are no disorders, no theft, no coalitions, none of those distractions that slow down the rate of work, make it less perfect or cause accidents\". By individualizing the subjects and placing them in a state of constant visibility, the efficiency of the institution is maximized. Furthermore, it guarantees the function of power, even when there is no one actually asserting it. It is in this respect that the Panopticon functions automatically. Foucault goes on to explain that this design is also applicable for a laboratory. Its mechanisms of individualization and observation give it the capacity to run many experiments simultaneously. These qualities also give an authoritative figure the \"ability to penetrate men’s behavior\" without difficulty. This is all made possible through the ingenuity of the geometric architecture. In light of this fact Foucault compares jails, schools, and factories in their structural similarities.\n\nA central idea of Foucault’s panopticism concerns the systematic ordering and controlling of human populations through subtle and often unseen forces. Such ordering is apparent in many parts of the modernized and now, increasingly digitalized, world of information. Contemporary advancements in technology and surveillance techniques have perhaps made Foucault’s theories more pertinent to any scrutiny of the relationship between the state and its population.\n\nHowever, while on one hand, new technologies, such as CCTV or other surveillance cameras, have shown the continued utility of panoptic mechanisms in liberal democracies, it could also be argued that electronic surveillance technologies are unnecessary in the original \"organic\" or \"geometric\" disciplinary mechanisms as illustrated by Foucault. Foucault argues, for instance, that Jeremy Bentham's Panopticon provides us with a model in which a self-disciplined society has been able to develop. These apparatuses of behavior control are essential if we are to govern ourselves, without the constant surveillance and intervention by an \"agency\" in every aspect of our lives. The Canadian historian Robert Gellately has observed, for instance, that because of the widespread willingness of Germans to inform on each other to the Gestapo that Germany between 1933-45 was a prime example of Panopticism.\n\nPanoptic theory has other wide-ranging impacts for surveillance in the digital era as well. Kevin Haggerty and Richard Ericson, for instance, have hinted that technological surveillance \"solutions\" have a particularly \"strong cultural allure\" in the West. Increasingly visible data, made accessible to organizations and individuals from new data-mining technologies, has led to the proliferation of “dataveillance,” which may be described as a mode of surveillance that aims to single out particular transactions through routine algorithmic production. In some cases, however, particularly in the case of mined credit card information, dataveillance has been documented to have led to a greater incidence of errors than past surveillance techniques.\n\nAccording to the tenets of Foucault's panopticism, if discursive mechanisms can be effectively employed to control and/or modify the body of discussion within a particular space (usually to the benefit of a particular governing class or organization), then there is no longer any need for an \"active agent\" to display a more overtly coercive power (i.e., the threat of violence). Since the beginning of the Information Age, there exists a debate over whether these mechanisms are being refined or accelerated, or on the other hand, becoming increasingly redundant, due to new and rapid technological advancements.\n\nFoucault also relates panopticism to capitalism:\n\"[The] peculiarity of the disciplines [elements of Panopticism] is that they try to define in relation to the multiplicities a tactics of power that fulfils three criteria: firstly, to obtain the exercise of power at the lowest possible cost (economically, by the low expenditure it involves; politically, by its discretion, its low exteriorization, its relative invisibility, the little resistance it arouses); secondly, to bring the effects of this social power to their maximum intensity and to extend them as far as possible, without either failure or interval; thirdly, to link this 'economic' growth of power with the output of the apparatuses (educational, military, industrial or medical) within which it is exercised; in short, to increase both the docility and the utility of all elements of the system\" (218).\n\n\"If the economic take-off of the West began with the techniques that made possible the accumulation of capital, it might perhaps be said that the methods for administering the accumulation of men made possible a political take-off in relation to the traditional, ritual, costly, violent forms of power [i.e. torture, public executions, corporal punishment, etc. of the middle ages], which soon fell into disuse and were superseded by a subtle, calculated technology of subjection. In fact, the two processes - the accumulation of men and the accumulation of capital - cannot be separated; it would not be possible to solve the problem of the accumulation of men without the growth of an apparatus of production capable of both sustaining them and using them; conversely, the techniques that made the cumulative multiplicity of men useful accelerated the accumulation of capital ... The growth of the capitalist economy gave rise to the specific modality of disciplinary power, whose general formulas, techniques of submitting forces and bodies, in short, 'political anatomy', could be operated in the most diverse political régimes, apparatuses or institutions\" (220-221).\n\nBuilding onto Foucault's Panopticism and Bentham's original Panopticon, Shoshana Zuboff applies the Panoptical theory in a technological context in her book, \"In the Age of the Smart Machine.\" In chapter nine, Zuboff provides a very vivid portrayal of the Information Panopticon as a means of surveillance, discipline and, in some cases, punishment in a work environment. The Information Panopticon embodies Bentham's idea in a very different way. Information Panopticons do not rely on physical arrangements, such as building structures and direct human supervision. Instead, a computer keeps track of a worker’s every move by assigning him or her specific tasks to perform during their shift. Everything, from the time a task is started to the time it is completed, is recorded. Workers are given a certain amount of time to complete the task based on its complexity. All this is monitored by supervision from a computer. Based on the data, the supervisor can monitor a worker’s performance and take any necessary action when needed.\n\nThe Information Panopticon can be defined as a form of centralized power that uses information and communication technology as observational tools and control mechanisms. Unlike the Panopticon envisioned by Bentham and Foucault, in which those under surveillance were unwilling subjects, Zuboff’s work suggests that the Information Panopticon is facilitated by the benefits it offers to willing participants.\n\nIn chapter ten of “In the Age of the Smart Machine,” Zuboff provides the example of DIALOG, a computer conferencing system used at a pharmaceutical corporation in the 1970s. The conferencing system, originally intended to facilitate communication among the corporation’s many branches, quickly became popular with employees. Users of DIALOG found that the system facilitated not only innovation and collaboration, but also relaxation, as many employees began to use the system to joke with one another and discuss non-work related topics. Employees widely reported that using the system was a positive experience because it created a culture of shared information and discussion, which transcended the corporation’s norms of formality and hierarchy that limited the spread of information between divisions and employees of different ranks. This positive culture was enabled by the privacy seemingly offered by the conferencing system, as discussion boards could be made to allow access only to those who were invited to participate. The Panoptic function of the conferencing system was revealed, however, when managers were able to gain access to the informal discussion boards where employees posted off-color jokes. Messages from the discussion were posted around the office to shame contributors, and many of DIALOG’s users, now knowing there was a possibility that their contributions could be read by managers and fearing they would face disciplinary action, stopped using the system. Some users, however, kept using the system, raising the question of whether remaining users modified their behavior under the threat of surveillance, as prisoners in Bentham’s Panopticon would, or whether they believed that the benefits offered by the system outweighed the possibility of punishment.\n\nZuboff’s work shows the dual nature of the Information Panopticon – participants may be under surveillance, but they may also use the system to conduct surveillance of others by monitoring or reporting other users’ contributions. This is true of many other information and communication technologies with Panoptic functions – cellphone owners may be tracked without their knowledge through the phones’ GPS capabilities, but they may also use the device to conduct surveillance of others. Thus, compared to Bentham’s Panopticon, the Information Panopticon is one in which everyone has the potential to be both a prisoner and a guard.\n\nIt is argued by Foucault that industrial management has paved the way for a very disciplinary society. A society that values objectivity over everything else. The point of this is to get as much productivity from the workers as possible. Contrasting with Bentham's model prison, workers within the Information Panopticon know they are being monitored at all times. Even if a supervisor is not physically there, the computer records their every move and all this data is at the supervisor's finger tips at all times. The system's objectivity can have a psychological impact on the workers. Workers feel the need to conform and satisfy the system rather than doing their best work or expressing concerns they might have.\n\nThe Information Panopticon diverts from Jeremy Bentham's model prison by adding more levels of control. While the Bentham's model prison system is made up of inmates at the lowest level monitored by a guard, the Information Panopticon can have various levels. A company or firm can have various satellite locations, each monitored by a supervisor, and then a regional supervisor monitoring the supervisors below him or her. Depending on the structure and size of a firm, information Panopticons can have several levels, each monitoring all the levels beneath it.\n\nNow, the efficiency of the Information Panopticon is in question. Does it really lead to a better work place and higher productivity, or does it simply put unnecessary stress on the people being monitored? A major criticism of the system is its objectivity. It is solely based on numbers, therefore not allowing for human error. According to Zuboff, some people find the system to be highly advantageous, while others think it is very flawed because it does not account for the effort a worker puts into a task or things outside of a worker's control. Furthermore, the lack of direct supervision only adds to a potentially precarious situation.\n\nTheoretical arguments in favor of rejecting the Foucauldian model of Panopticism may be considered under five general headings:\n\n\nThe first point concerns Zygmunt Bauman’s argument that the leading principle of social order has moved from Panopticism to seduction. This argument is elaborated in his 1998 essay ‘On postmodern uses of sex’.\n\nThe second argument concerns surveillance redundance, and it is increasingly relevant in the age of Facebook and online self-disclosure. Is the metaphor of a panopticon appropriate for voluntary surrender of privacy?\n\nThe third argument for post-Panopticism, concerning action before the fact, is articulated by William Bogard:\n\nThe figure of the Panopticon is already haunted by a parallel figure of simulation. Surveillance, we are told, is discreet, unobtrusive, camouflaged, unverifiable – all elements of artifice designed into an architectural arrangement of spaces to produce real effects of discipline. Eventually this will lead, by its means of perfection, to the elimination of the Panopticon itself . . . surveillance as its own simulation. Now it is no longer a matter of the speed at which information is gained to defeat an enemy. . . . Now, one can simulate a space of control, project an indefinite number of courses of action, train for each possibility, and react immediately with pre-programmed responses to the actual course of events . . . with simulation, sight and foresight, actual and virtual begin to merge. . . . Increasingly the technological enlargement of the field of perceptual control, the erasure of distance in the speed of electronic information has pushed surveillance beyond the very limits of speed toward the purest forms of anticipation.\n\nThis kind of anticipation is particularly evident in emergent surveillance technologies such as social network analysis.\n\nThe ‘Synopticon’ concerns the surveillance of the few by the many. Examples of this kind of surveillance may include the theatre, the Coliseum, and celebrity tabloid reporting. This “reversal of the Panoptical polarity may have become so marked that it finally deconstructs the Panoptical metaphor altogether”.\n\nFinally, the fifth point concerns the self-defeating nature of Panoptical regimes. The failure of surveillance states is illustrated by examples such as “prison riots, asylum sub-cultures, ego survival in Gulag or concentration camp, [and] retribalization in the Balkans.”\n\nIn their 2007 article, Dobson and Fisher lay out an alternative model of post-panopticism as they identify three panoptic models. Panopticism I refers to Jeremy Bentham’s original conceptualization of the panopticon, and is it the model of panopticism that Foucault responds to in his 1975 Discipline and Punish. Panopticism II refers to an Orwellian ‘Big Brother’ ideal of surveillance. Panopticism III, the final model of panopticism, refers to the high-technology human tracking systems that are emergent in this 21st century. These geographical information systems (GIS) include technologies such as cellphone GPS, RFIDs (radio-frequency identification tags), and geo-fences. Panopticism III is also distinguished by its costs:\n\nPanopticon III is affordable, effective, and available to anyone who wants to use it. Initial purchase prices and monthly service fees are equivalent to cell-phone costs. In less than five years, the cost of continuous surveillance of a single individual has dropped from several hundred thousand dollars per year to less than $500 per year. Surveillance formerly justified solely for national security and high-stakes commerce is readily available to track a spouse, child, parent, employee, neighbor, or stranger.\n\nThe Cornell University professor and information theorist Branden Hookway introduced the concept of a Panspectrons in 2000: an evolution of the panopticon to the effect that it does not define an object of surveillance more, but everyone and everything is monitored. The object is defined only in relation to a specific issue.\n\nParis School academic Didier Bigo coined the term 'Banopticon' to describe a situation where profiling technologies are used to determine who to place under surveillance.\n\n\n"}
{"id": "5007494", "url": "https://en.wikipedia.org/wiki?curid=5007494", "title": "Parabolic cylindrical coordinates", "text": "Parabolic cylindrical coordinates\n\nIn mathematics, parabolic cylindrical coordinates are a three-dimensional orthogonal coordinate system that results from projecting the two-dimensional parabolic coordinate system in the\nperpendicular formula_1-direction. Hence, the coordinate surfaces are confocal parabolic cylinders. Parabolic cylindrical coordinates have found many applications, e.g., the potential theory of edges.\n\nThe parabolic cylindrical coordinates are defined in terms of the Cartesian coordinates by:\n\nThe surfaces of constant form confocal parabolic cylinders\n\nthat open towards , whereas the surfaces of constant form confocal parabolic cylinders\n\nthat open in the opposite direction, i.e., towards . The foci of all these parabolic cylinders are located along the line defined by . The radius has a simple formula as well\n\nthat proves useful in solving the Hamilton–Jacobi equation in parabolic coordinates for the inverse-square central force problem of mechanics; for further details, see the Laplace–Runge–Lenz vector article.\n\nThe scale factors for the parabolic cylindrical coordinates and are:\n\nThe infinitesimal element of volume is\n\nThe differential displacement is given by:\n\nThe differential normal area is given by:\n\nLet be a scalar field. The gradient is given by\n\nThe Laplacian is given by\n\nLet be a vector field of the form:\n\nThe divergence is given by\n\nThe curl is given by\n\nOther differential operators can be expressed in the coordinates by substituting the scale factors into the general formulae found in orthogonal coordinates.\n\nRelationship to cylindrical coordinates :\n\nParabolic unit vectors expressed in terms of Cartesian unit vectors:\n\nSince all of the surfaces of constant , and are conicoids, Laplace's equation is separable in parabolic cylindrical coordinates. Using the technique of the separation of variables, a separated solution to Laplace's equation may be written:\n\nand Laplace's equation, divided by , is written:\n\nSince the equation is separate from the rest, we may write\n\nwhere is constant. has the solution:\n\nSubstituting for formula_21, Laplace's equation may now be written:\n\nWe may now separate the and functions and introduce another constant to obtain:\n\nThe solutions to these equations are the parabolic cylinder functions\n\nThe parabolic cylinder harmonics for are now the product of the solutions. The combination will reduce the number of constants and the general solution to Laplace's equation may be written:\n\nThe classic applications of parabolic cylindrical coordinates are in solving partial differential equations, e.g., Laplace's equation or the Helmholtz equation, for which such coordinates allow a separation of variables. A typical example would be the electric field surrounding a flat semi-infinite conducting plate.\n\n\n\n"}
{"id": "44903985", "url": "https://en.wikipedia.org/wiki?curid=44903985", "title": "Phenomenological model", "text": "Phenomenological model\n\nA phenomenological model is a scientific model that describes the empirical relationship of phenomena to each other, in a way which is consistent with fundamental theory, but is not directly derived from theory. In other words, a phenomenological model is not derived from first principles. A phenomenological model foregoes any attempt to explain why the variables interact the way they do, and simply attempts to describe the relationship, with the assumption that the relationship extends past the measured values. Regression analysis is sometimes used to create statistical models that serve as phenomenological models.\n\nPhenomenological models have been characterized as being completely independent of theories, though many phenomenological models, while failing to be derivable from a theory, incorporate principles and laws associated with theories. The liquid drop model of the atomic nucleus, for instance, portrays the nucleus as a liquid drop and describes it as having several properties (surface tension and charge, among others) originating in different theories (hydrodynamics and electrodynamics, respectively). Certain aspects of these theories—though usually not the complete theory—are then used to determine both the static and dynamical properties of the nucleus.\n"}
{"id": "233833", "url": "https://en.wikipedia.org/wiki?curid=233833", "title": "Pontifex (project)", "text": "Pontifex (project)\n\nPONTIFEX (Planning Of Non-specific Transportation by an Intelligent Fleet EXpert) was a mid-1980s project that introduced a novel approach to complex aircraft fleet scheduling, partially funded by the European Commission's Strategic Programme for R&D in Information Technology.\n\nSince the mathematical problems stemming from nontrivial fleet scheduling easily become computationally unsolvable, the PONTIFEX idea consisted in a seamless merge of algorithms and heuristic knowledge embedded in rules. The system, based on domain knowledge collected from airliners Alitalia, KLM, Swissair, and TAP Portugal, was first adopted by Swissair and Alitalia in the late 1980s, then also by the Italian railroad national operator, for their cargo division. It is still in use today (2008).\n"}
{"id": "248256", "url": "https://en.wikipedia.org/wiki?curid=248256", "title": "Principle of double effect", "text": "Principle of double effect\n\nThe principle of double effect—also known as the rule of double effect; the doctrine of double effect, often abbreviated as DDE or PDE, double-effect reasoning; or simply double effect—is a set of ethical criteria which Christian philosophers, and some others, have advocated for evaluating the permissibility of acting when one's otherwise legitimate act (for example, relieving a terminally ill patient's pain) may also cause an effect one would otherwise be obliged to avoid (sedation and a slightly shortened life). The first known example of double-effect reasoning is Thomas Aquinas' treatment of homicidal self-defense, in his work \"Summa Theologica\".\n\nThis set of criteria states that an action having foreseen harmful effects practically inseparable from the good effect is justifiable if the following are true:\n\nThe principle of double effect is based on the idea that there is a morally relevant difference between an \"intended\" consequence of an act and one that is foreseen by the actor but not calculated to achieve his motive. So, for example, the principle is invoked to consider the terror bombing of non-combatants having as its goal victory in a legitimate war morally out of bounds, while holding as ethically in bounds an act of strategic bombing that similarly harms non-combatants with foresight as a side effect of destroying a legitimate military target. Because advocates of double effect propose that consequentially similar acts can be morally different, double effect is most often criticized by consequentialists who consider the consequences of actions entirely determinative of the action's morality.\n\nIn their use of the distinction between intent and foresight without intent, advocates of double effect make three arguments. First, that intent differs from foresight, even in cases in which one foresees an effect as inevitable. Second, that one can apply the distinction to specific sets of cases found in military ethics (terror bombing/strategic bombing), medical ethics (craniotomy/hysterectomy), and social ethics (euthanasia). Third, that the distinction has moral relevance, importance, or significance.\n\nThe doctrine consists of four conditions that must be satisfied before an act is morally permissible:\n\n\nThe second of these four conditions is an application of the more general principle that good ends do not justify evil means.\n\nThe principle of double effect is frequently cited in cases of pregnancy and abortion. A doctor who believes abortion is always morally wrong may still remove the uterus or fallopian tubes of a pregnant woman, knowing the procedure will cause the death of the embryo or fetus, in cases in which the woman is certain to die without the procedure (examples cited include aggressive uterine cancer and ectopic pregnancy). In these cases, the intended effect is to save the woman's life, not to terminate the pregnancy, and the effect of not performing the procedure would result in the greater evil of the death of both the mother and the foetus.\n\nIn cases of terminally ill patients who would hasten their deaths because of unbearable pain, or whose caregivers would do so for them (euthanasia, medical aid in dying, etc.), a principle of \"double effect death\" could be applied to justify the deliberate administration of a pain-killer in potentially unsafe doses—not in an attempt to end life but the pain suffered is considered harmful to the patient. The U.S. Supreme Court has voiced support for this principle in its deliberations over the constitutionality of medical aid in dying.\n\nConsequentialists, in particular, reject the notion that two acts can differ in their moral permissibility if both have exactly the same consequences, or expected consequences. John Stuart Mill, a nineteenth-century advocate of the utilitarian version of consequentialism, argues that it is a mistake to confuse the standards for right action with a consideration of our motives to perform a right action: \"He who saves a fellow creature from drowning does what is morally right, whether his motive be duty, or the hope of being paid for his trouble; he who betrays the friend that trusts him, is guilty of a crime, even if his object be to serve another friend to whom he is under greater obligations.\" According to Mill, scrutiny of motives will show that almost all good behavior proceeds from questionable intentions. Therefore, Mill argues, our moral analysis should ignore matters of motivation, and so we should reject DDE, which appeals to a distinction between intended and unintended consequences. Mill further claims that scrutiny of motives will reveal a man's character, but utilitarianism does not judge character, only the rightness or wrongness of actions.\n\n\n"}
{"id": "1137736", "url": "https://en.wikipedia.org/wiki?curid=1137736", "title": "Principle of sufficient reason", "text": "Principle of sufficient reason\n\nThe principle of sufficient reason states that everything must have a reason or a cause. The modern formulation of the principle is usually attributed to Gottfried Leibniz, although the idea was conceived of and utilized by various philosophers who preceded him, including Anaximander, Parmenides, Archimedes, Plato and Aristotle, Cicero, Avicenna, Thomas Aquinas, and Spinoza. Some philosophers have associated the principle of sufficient reason with \"ex nihilo nihil fit\". Hamilton identified the laws of inference modus ponens with the \"law of Sufficient Reason, or of Reason and Consequent\" and modus tollens with its contrapositive expression.\n\nThe principle has a variety of expressions, all of which are perhaps best summarized by the following:\n\n\nA sufficient explanation may be understood either in terms of \"reasons\" or \"causes,\" for like many philosophers of the period, Leibniz did not carefully distinguish between the two. The resulting principle is very different, however, depending on which interpretation is given.\n\nIt is an open question whether the principle of sufficient reason can be applied to axioms within a logic construction like a mathematical or a physical theory, because axioms are propositions accepted as having no justification possible within the system\nThe principle declares that all propositions considered to be true within a system should be deducible from the set axioms at the base of the construction (with some theoretical exceptions: see Gödel's theorem).\n\nLeibniz identified two kinds of truth, necessary and contingent truths. He believed necessary mathematical truths to be derived from the law of identity (and the principle of contradiction): \"Necessary truths are those that can be demonstrated through an analysis of terms, so that in the end they become identities, just as in Algebra an equation expressing an identity ultimately results from the substitution of values [for variables]. That is, necessary truths depend upon the principle of contradiction.\" Leibniz states that the sufficient reason for necessary truths is that their negation is a contradiction.\n\nLeibniz admitted contingent truths on the basis of infinitary reasons, to which God had access but humans did not:\nIn contingent truths, even though the predicate is in the subject, this can never be demonstrated, nor can a proposition ever be reduced to an equality or to an identity, but the resolution proceeds to infinity, God alone seeing, not the end of the resolution, of course, which does not exist, but the connection of the terms or the containment of the predicate in the subject, since he sees whatever is in the series.Without this qualification, the principle can be seen as a description of a certain notion of closed system, in which there is no 'outside' to provide unexplained events with causes. It is also in tension with the paradox of Buridan's ass. Leibniz denied that the paradox of Buridan's ass could ever occur, saying:\n\nLeibniz also used the principle of sufficient reason to refute the idea of absolute space:\n\nI say then, that if space is an absolute being, there would be something for which it would be impossible there should be a sufficient reason. Which is against my axiom. And I prove it thus. Space is something absolutely uniform; and without the things placed in it, one point in space does not absolutely differ in any respect whatsoever from another point of space. Now from hence it follows, (supposing space to be something in itself, beside the order of bodies among themselves,) that 'tis impossible that there should be a reason why God, preserving the same situation of bodies among themselves, should have placed them in space after one particular manner, and not otherwise; why everything was not placed the quite contrary way, for instance, by changing East into West.\n\nThe principle was one of the four recognised laws of thought, that held a place in European pedagogy of logic and reasoning (and, to some extent, philosophy in general) in the 18th and 19th centuries. It was influential in the thinking of Leo Tolstoy, amongst others, in the elevated form that history could not be accepted as random.\n\nA sufficient reason is sometimes described as the coincidence of every single thing that is needed for the occurrence of an effect (i.e. of the so-called \"necessary conditions\"). Such view could perhaps be also applied to indeterministic systems, as long as randomness is in a way incorporated in the preconditions.\n\nHere is how Hamilton, circa 1837–1838, expressed his \"fourth law\" in his LECT. V. LOGIC. 60–61:\n\nAccording to Schopenhauer's \"On the Fourfold Root of the Principle of Sufficient Reason\", there are four distinct forms of the principle.\n\nFirst Form: The Principle of Sufficient Reason of Becoming (principium rationis sufficientis fiendi); appears as the law of causality in the understanding.\n\nSecond Form: The Principle of Sufficient Reason of Knowing (principium rationis sufficientis cognoscendi); asserts that if a judgment is to express a piece of knowledge, it must have a sufficient ground or reason, in which case it receives the predicate true.\n\nThird Form: The Principle of Sufficient Reason of Being (principium rationis sufficientis essendi); the law whereby the parts of space and time determine one another as regards those relations. Example in arithmetic: Each number presupposes the preceding numbers as grounds or reasons of its being; \"I can reach ten only by going through all the preceding numbers; and only by virtue of this insight into the ground of being, do I know that where there are ten, so are there eight, six, four.\"\n\n\"Now just as the subjective correlative to the first class of representations is the understanding, that to the second the faculty of reason, and that to the third pure sensibility, so is the subjective correlative to this fourth class found to be the inner sense, or generally self-consciousness.\" \n\nFourth Form: The Principle of Sufficient Reason of Acting (principium rationis sufficientis agendi); briefly known as the law of motivation. \"Any judgment that does not follow its previously existing ground or reason\" or any state that cannot be explained away as falling under the three previous headings \"must be produced by an act of will which has a motive.\" As his proposition in 43 states, \"Motivation is causality seen from within.\"\n\nSeveral proofs have been prepared in order to demonstrate that the universe is at bottom causal, i.e. works in accord with the principle in question; perhaps not in every single case (randomness might still play a part here and there), but that causality must be the way it works at least \"in general\", in most of what we see; and that our minds are aware of the principle even before any experience. The two famous arguments or proofs were proposed by Immanuel Kant (from the form of Time, temporal ordering of events and \"directionality\" of time) and by Arthur Schopenhauer (by demonstrating how all perception depends on causality and the intellect).\n\nOnce it is agreed (e.g. from a kind of an \"arrow of time\") that causal interconnections, as a form of principle of sufficient reason, indeed must in general exist everywhere in the universe (at least in the large scale), \"backwards\" causality in general might then be precluded using a form of the paradox of free will (i.e. an event that has a future source might cause us to remove that source quick enough and thus causality would not work).\n\n\n"}
{"id": "1875925", "url": "https://en.wikipedia.org/wiki?curid=1875925", "title": "Pythagorean Method of Memorization", "text": "Pythagorean Method of Memorization\n\nPythagorean Method of Memorization (PYMOM), also known as Triangular Movement Cycle (TMC), is a game-based, educational methodology or associative-learning technique that primarily uses corresponding information, such as terms and definitions on opposing sides, displayed on cue cards, to exploit psychological retention of information for academic study and language acquisition. PYMOM is named such because of the shape the cue-cards form during the progression of the game, a right-angled or Pythagorean triangle.\n\nIt is a theoretical educational method that is made up of several established and tested educational methods that have been in use for decades.\n\nPYMOM is a composite body of techniques that claims, in its digital form, to incorporate (to a greater or lesser degree): spaced repetition, non-failure redundant subroutine, chromatics, positive reinforcement, the Von Restorff effect, picture association, selective musical tonality, kinesthetics, the serial-position effect and meditation. There are two branches of this methodology:\n\nAs with both branches, there is only one variable in the game or learning method: a correct or incorrect answer. The initial movement cycle also remains largely unchanged.\n\nThe movement cycle which is most crucial to the methodology and reinforces the spaced repetition, begins with either 3, 4 or 5 cards; 3 cards for a 6-card session, 4 cards for a 10-card session and 5 cards for the most advanced 15-card session. Because two-sided associative cue-cards are being used, all cards are presented with a congruent side up, either all \"terms\" or \"definitions,\" not mixed.\n\nOnce cards have been answered correctly, the predominant row has reached its maximum and a card must be graduated out of this row to continue the game. Thus the card to the far right comes into play. Routines are repeated as each row reaches its maximum. A cue-card is finally eliminated from the game session by being answered correctly once more, after it has graduated to the top tier or row.\n\nThe first manifestation, referred to as the \"Triangular Movement Cycle\" or TMC, was a simple paper-based learning technique that was primarily a manual movement cycle using physical cue-cards, which allowed for manual-spaced repetition to elicit psychological retention of information. Its origins, however, are not very clear. Using TMC, teachers would move the cards for the student in a one-on-one setting according to either the correct or incorrect feedback from the student.This presented challenges for the teacher or tutor using this method. The first challenge lies in the fact that, although TMC lent itself well to a two-party learning group (i.e. teacher & student), it could also be done by the student themselves on their own. It was a very easy system to utilize once learned, however, it was found exceptionally difficult to teach the complex movement cycle and principles behind such to students, especially where a linguistic barrier was present. The second challenge lay in that the educator needed to create and remember innumerable cue-cards or create custom master lists in order to know the correct answers — and properly guide the student, thus progressing or digressing the card in play. TMC often failed to keep the attention of many students owing to the fact that cards were not very visually appealing. To make them so required tremendous effort — and was very time consuming.\n\nThe term \"Pythagorean Method of Memorization\" was coined in 2013 and officially copyrighted in October 2014 by a Canadian company named You Learn Educational Solutions & Linguistics Inc. PYMOM takes the movement cycle from TMC and remedied the challenge of teaching the movement cycle itself to students by providing a software-based solution to handling cycles by means of sub-routines prompted by the user’s input.\n\nPYMOM wove established educational theory into the fabric of TMC to create a viable educational platform for academic and linguistic study by several means. Because spaced repetition is intrinsically part of the movement-cycle subroutines, it adds to the content and surrounding experience making it into a platform. The developers of PYMOM describe it as an “organic learning experience.” The tenets that truly allow a learning system to be a PYMOM-based system are enumerated thusly: The Von Restorff effect: for example, where it features a language, this method is employed to further aid in memory retention of the highlighted word in the phrase.\n\n"}
{"id": "35139871", "url": "https://en.wikipedia.org/wiki?curid=35139871", "title": "Q-Vectors", "text": "Q-Vectors\n\nQ-vectors are used in atmospheric dynamics to understand physical processes such as vertical motion and frontogenesis. Q-vectors are not physical quantities that can be measured in the atmosphere but are derived from the quasi-geostrophic equations and can be used in the previous diagnostic situations. On meteorological charts, Q-vectors point toward upward motion and away from downward motion. Q-vectors are an alternative to the omega equation for diagnosing vertical motion in the quasi-geostrophic equations.\n\nFirst derived in 1978, Q-vector derivation can be simplified for the midlatitudes, using the midlatitude β-plane quasi-geostrophic prediction equations:\n\n\nAnd the thermal wind equations:\n\nformula_4 (x component of thermal wind equation)\n\nformula_5 (y component of thermal wind equation)\n\nwhere formula_6 is the Coriolis parameter, approximated by the constant 1e s; formula_7 is the atmospheric ideal gas constant; formula_8 is the latitudinal change in the Coriolis parameter formula_9; formula_10 is a static stability parameter; formula_11 is the specific heat at constant pressure; formula_12 is pressure; formula_13 is temperature; anything with a subscript formula_14 indicates geostrophic; anything with a subscript formula_15 indicates ageostrophic; formula_16 is a diabatic heating rate; and formula_17 is the Lagrangian rate change of pressure with time. formula_18. Note that because pressure decreases with height in the atmosphere, a formula_19 is upward vertical motion, analogous to formula_20.\n\nFrom these equations we can get expressions for the Q-vector:\n\nformula_21\n\nformula_22\n\nAnd in vector form:\n\nformula_23\n\nformula_24\n\nPlugging these Q-vector equations into the quasi-geostrophic omega equation gives:\n\nformula_25\n\nWhich in an adiabatic setting gives:\n\nformula_26\n\nExpanding the left-hand side of the quasi-geostrophic omega equation in a Fourier Series gives the formula_27 above, implying that a formula_27 relationship with the right-hand side of the quasi-geostrophic omega equation can be assumed.\n\nThis expression shows that the divergence of the Q-vector (formula_29) is associated with downward motion. Therefore, convergent formula_30 forces ascend and divergent formula_30 forces descend. Q-vectors and all ageostrophic flow exist to preserve thermal wind balance. Therefore, low level Q-vectors tend to point in the direction of low-level ageostrophic winds.\n\nQ-vectors can be determined wholly with: geopotential height (formula_32) and temperature on a constant pressure surface. Q-vectors always point in the direction of ascending air. For an idealized cyclone and anticyclone in the Northern Hemisphere (where formula_33), cyclones have Q-vectors which point parallel to the thermal wind and anticyclones have Q-vectors that point antiparallel to the thermal wind. This means upward motion in the area of warm air advection and downward motion in the area of cold air advection.\n\nIn frontogenesis, temperature gradients need to tighten for initiation. For those situations Q-vectors point toward ascending air and the tightening thermal gradients. In areas of convergent Q-vectors, cyclonic vorticity is created, and in divergent areas, anticyclonic vorticity is created.\n"}
{"id": "49069006", "url": "https://en.wikipedia.org/wiki?curid=49069006", "title": "RPC54", "text": "RPC54\n\nThe RPC54 system is a role-playing game system that uses poker cards to determine the effects of actions. The cards are numbered 1 through 13 and have four customs suits that reflect the different environments where the characters might find themselves. This system is used in Burning Games LTD's game .\n\nEach player, including the gamemaster, draws a hand of seven cards from a poker deck. They will use those cards to try to prevail whenever they want to stop someone from doing something or someone tries to stop them. This mechanic allows for luck management, as the players can choose which cards they play and when, and is an equaliser in the sense that all players must play through the same deck eventually, thus giving them equal or very similar chances of success or failure.\n"}
{"id": "25269475", "url": "https://en.wikipedia.org/wiki?curid=25269475", "title": "Sense data", "text": "Sense data\n\nIn the philosophy of perception, the theory of sense data was a popular view held in the early 20th century by philosophers such as Bertrand Russell, C. D. Broad, H. H. Price, A. J. Ayer, and G. E. Moore. Sense data are taken to be mind-dependent objects whose existence and properties are known directly to us in perception. These objects are unanalyzed experiences inside the mind, which appear to subsequent more advanced mental operations exactly as they are.\n\nSense data are often placed in a time and/or causality series, such that they occur after the potential unreliability of our perceptual systems yet before the possibility of error during higher-level conceptual analysis and are thus incorrigible. They are thus distinct from the 'real' objects in the world outside the mind, about whose existence and properties we often \"can\" be mistaken.\n\nTalk of sense-data has since been largely replaced by talk of the closely related qualia. The formulation \"the given\" is also closely related. None of these terms has a single coherent and widely agreed-upon definition, so their exact relationships are unclear. One of the greatest troubling aspects to 20th century theories of sense data is its unclear rubric nature.\n\nBertrand Russell heard the sound of his knuckles rapping his writing table, felt the table's hardness and saw its apparent colour (which he knew 'really' to be the brown of wood) change significantly under shifting lighting conditions.\n\nH. H. Price found that although he was able to doubt the presence of a tomato before him, he was unable to doubt the existence of his red, round and 'somewhat bulgy' sense-datum and his consciousness of this sense-datum.\n\nWhen we twist a coin it 'appears' to us as elliptical. This elliptical 'appearance' cannot be identical with the coin (for the coin is perfectly round), and is therefore a sense datum, which somehow represents the round coin to us.\n\nConsider a reflection which appears to us in a mirror. There is nothing corresponding to the reflection in the world external to the mind (for our reflection appears to us as the image of a human being apparently located inside a wall, or a wardrobe). The appearance is therefore a mental object, a sense datum.\n\nThe idea that our perceptions are based on sense data is supported by a number of arguments. The first is popularly known as the argument from illusion. From a subjective experience of perceiving something, it is theoretically impossible to distinguish perceiving something which exists independently of oneself from an hallucination or mirage. Thus, we do not have any direct access to the outside world that would allow us to reliably distinguish it from an illusion that caused identical experiences. Since (the argument claims) we must have direct access to some specific experiential entity in order to have the percepts that we do, and since this entity is not identical to the real object itself, there must be some sort of internal mental entity somehow correlated to the real world, about which we afterwards have perceptions, make judgments, etc. This entity is a sense-datum.\n\nAbstract sense data is sense data without human judgement, sense data without human conception and yet evident to the senses, found in aesthetic experience. As opposed to; imaginary sense data which is more like a quasi substance and does not really exist; Imaginary sense data is abstract sense data as presented from the aestheticized senses to consciousness; i.e. imagination, power of reason and inner subjective states of self-awareness including: emotion, self-reflection, ego, and theory. The theory of abstract and imaginary sense data operates on the tacit definition of imagination as \"a power mediating between the senses and the reason by virtue of representing perceptual objects without their presence\". Imaginary sense data are 'imaginary' per Immanuel Kant's analysis that imagination is the primary faculty of mind capable of synthesizing input from the senses into a world of objects. Abstract and imaginary sense data are key to understanding abstract art's relationship with the conscious and unconscious mind.\n\nSense data theories have been criticised by philosophers such as J. L. Austin and Wilfrid Sellars (Sellars diagnosing in them The Myth of the Given), and more recently by Kevin O'Regan, Alva Noë and Daniel Dennett. Much of the early criticism may arise from a claim about sense data that was held by philosophers such as A. J. Ayer. This was that sense data really do have the properties they appear to have. Thus, in this account of sense data, the sense data that are responsible for the experience of a red tomato really \"are red\".\n\nIn one sense this is ridiculous, since there is nothing red in a brain to act as a sense datum. However, in another sense it is perfectly consistent—in the sense that the data \"are red\" when experienced directly, even though the physical processes of perception may not appear red if they were experienced in a contrived and inappropriately indirect way, such as by examining the brain of the experiencer with scientific instruments.\n\nOn some theories, the tomato itself is not red except in the eyes of a red-seeing being. Thus when one says that a neural state is or is not 'red' without referring the judgement of redness to the owner of the neurons concerned, there is an assumption that things can have innate appearances without reference to perceivers—which is implicitly denied by the sense data theory. Thus the criticism that sense data cannot really be red is made from a position of presupposition inconsistent with a theory of sense data—so it is bound to seem to make the theory seem wrong. More recent opposition to the existence of sense data appears to be simply regression to naïve realism.\n\nBy objectifying and partially externalising a Subject's basic experiences of the world as 'sense-data', positing their necessity for perception and higher order thinking and installing them permanently between the perceiving Subject and the 'real world', sense-data theories tend towards Solipsism. Attempts to repair this must avoid both obscurantism and over-dependence on psychology (and therefore empiricism, and potentially circularity).\n\n"}
{"id": "39033720", "url": "https://en.wikipedia.org/wiki?curid=39033720", "title": "Stochastic cellular automaton", "text": "Stochastic cellular automaton\n\nStochastic cellular automata or 'probabilistic cellular automata' (PCA) or 'random cellular automata' or locally interacting Markov chains are an important extension of cellular automaton. Cellular automata are a discrete-time dynamical system of interacting entities, whose state is discrete.\n\nThe state of the collection of entities is updated at each discrete time according to some simple homogeneous rule. All entities' states are updated in parallel or synchronously. Stochastic Cellular Automata are CA whose updating rule is a stochastic one, which means the new entities' states are chosen according to some probability distributions. It is a discrete-time random dynamical system. From the spatial interaction between the entities, despite the simplicity of the updating rules, complex behaviour may emerge like self-organization. As mathematical object, it may be considered in the framework of stochastic processes as an interacting particle system in discrete-time.\nSee \nfor a more detailed introduction.\n\nAs discrete-time Markov process, PCA are defined on a product space formula_1 (cartesian product) where formula_2\nis a finite or infinite graph, like formula_3 and where formula_4 is a finite space, like for instance\nformula_5 or formula_6. The transition probability has a product form\nformula_7 where \nformula_8 and formula_9 is a probability distribution on formula_10.\nIn general some locality is required formula_11 where \nformula_12 with formula_13 a finite neighbourhood of k. See for a more detailed introduction following the probability theory's point of view.\n\nThere is a version of the majority cellular automaton with probabilistic updating rules. See the Toom's rule.\n\nPCA may be used to simulate the Ising model of ferromagnetism in statistical mechanics.\nSome categories of models were studied from a statistical mechanics point of view.\n\nThere is a strong connection\nbetween probabilistic cellular automata and the cellular Potts model in particular when it is implemented in parallel.\n\nThe Galves-Locherbach model is an example of a generalized PCA with a non Markovian aspect.\n\n"}
{"id": "140841", "url": "https://en.wikipedia.org/wiki?curid=140841", "title": "Sufficient statistic", "text": "Sufficient statistic\n\nIn statistics, a statistic is \"sufficient\" with respect to a statistical model and its associated unknown parameter if \"no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter\". In particular, a statistic is sufficient for a family of probability distributions if the sample from which it is calculated gives no additional information than does the statistic, as to which of those probability distributions is that of the population from which the sample was taken.\n\nA related concept is that of linear sufficiency, which is weaker than \"sufficiency\" but can be applied in some cases where there is no sufficient statistic, although it is restricted to linear estimators. The Kolmogorov structure function deals with individual finite data; the related notion there is the algorithmic sufficient statistic.\n\nThe concept is due to Sir Ronald Fisher in 1920. Stephen Stigler noted 1973 that the concept of sufficiency had fallen out of favor in descriptive statistics because of the strong dependence on an assumption of the distributional form (see Pitman–Koopman–Darmois theorem below), but remained very important in theoretical work.\n\nRoughly, given a set formula_1 of independent identically distributed data conditioned on an unknown parameter formula_2, a sufficient statistic is a function formula_3 whose value contains all the information needed to compute any estimate of the parameter (e.g. a maximum likelihood estimate). Due to the factorization theorem (see below), for a sufficient statistic formula_3, the probabilty density can be written as formula_5. From this factorization, it can easily be seen that the maximum likelihood estimate of formula_2 will interact with formula_7 only through formula_3. Typically, the sufficient statistic is a simple function of the data, e.g. the sum of all the data points.\n\nMore generally, the \"unknown parameter\" may represent a vector of unknown quantities or may represent everything about the model that is unknown or not fully specified. In such a case, the sufficient statistic may be a set of functions, called a \"jointly sufficient statistic\". Typically, there are as many functions as there are parameters. For example, for a Gaussian distribution with unknown mean and variance, the jointly sufficient statistic, from which maximum likelihood estimates of both parameters can be estimated, consists of two functions, the sum of all data points and the sum of all squared data points (or equivalently, the sample mean and sample variance).\n\nThe concept is equivalent to the statement that, conditional on the value of a sufficient statistic for a parameter, the joint probability distribution of the data does not depend on that parameter. Both the statistic and the underlying parameter can be vectors.\n\nA statistic \"t\" = \"T\"(\"X\") is sufficient for underlying parameter \"θ\" precisely if the conditional probability distribution of the data \"X\", given the statistic \"t\" = \"T\"(\"X\"), does not depend on the parameter \"θ\".\n\nAs an example, the sample mean is sufficient for the mean (\"μ\") of a normal distribution with known variance. Once the sample mean is known, no further information about \"μ\" can be obtained from the sample itself. On the other hand, for an arbitrary distribution the median is not sufficient for the mean: even if the median of the sample is known, knowing the sample itself would provide further information about the population mean. For example, if the observations that are less than the median are only slightly less, but observations exceeding the median exceed it by a large amount, then this would have a bearing on one's inference about the population mean.\n\n\"Fisher's factorization theorem\" or \"factorization criterion\" provides a convenient characterization of a sufficient statistic. If the probability density function is ƒ(\"x\"), then \"T\" is sufficient for \"θ\" if and only if nonnegative functions \"g\" and \"h\" can be found such that\n\ni.e. the density ƒ can be factored into a product such that one factor, \"h\", does not depend on \"θ\" and the other factor, which does depend on \"θ\", depends on \"x\" only through \"T\"(\"x\").\n\nIt is easy to see that if \"F\"(\"t\") is a one-to-one function and \"T\" is a sufficient\nstatistic, then \"F\"(\"T\") is a sufficient statistic. In particular we can multiply a\nsufficient statistic by a nonzero constant and get another sufficient statistic.\n\nAn implication of the theorem is that when using likelihood-based inference, two sets of data yielding the same value for the sufficient statistic \"T\"(\"X\") will always yield the same inferences about \"θ\". By the factorization criterion, the likelihood's dependence on \"θ\" is only in conjunction with \"T\"(\"X\"). As this is the same in both cases, the dependence on \"θ\" will be the same as well, leading to identical inferences.\n\nDue to Hogg and Craig. Let formula_10, denote a random sample from a distribution having the pdf \"f\"(\"x\", \"θ\") for \"ι\" < \"θ\" < \"δ\". Let \"Y\" = \"u\"(\"X\", \"X\", ..., \"X\") be a statistic whose pdf is \"g\"(\"y\"; \"θ\"). Then \"Y\" = \"u\"(\"X\", \"X\", ..., \"X\") is a sufficient statistic for \"θ\" if and only if, for some function \"H\",\n\nFirst, suppose that\n\nWe shall make the transformation \"y\" = \"u\"(\"x\", \"x\", ..., \"x\"), for \"i\" = 1, ..., \"n\", having inverse functions \"x\" = \"w\"(\"y\", \"y\", ..., \"y\"), for \"i\" = 1, ..., \"n\", and Jacobian formula_13. Thus,\n\nThe left-hand member is the joint pdf \"g\"(\"y\", \"y\", ..., \"y\"; θ) of \"Y\" = \"u\"(\"X\", ..., \"X\"), ..., \"Y\" = \"u\"(\"X\", ..., \"X\"). In the right-hand member, formula_15 is the pdf of formula_16, so that formula_17 is the quotient of formula_18 and formula_15; that is, it is the conditional pdf formula_20 of formula_21 given formula_22.\n\nBut formula_23, and thus formula_24, was given not to depend upon formula_2. Since formula_2 was not introduced in the transformation and accordingly not in the Jacobian formula_27, it follows that formula_28 does not depend upon formula_2 and that formula_16 is a sufficient statistics for formula_2.\n\nThe converse is proven by taking:\n\nwhere formula_33 does not depend upon formula_2 because formula_35 depend only upon formula_36, which are independent on formula_37 when conditioned by formula_16, a sufficient statistics by hypothesis. Now divide both members by the absolute value of the non-vanishing Jacobian formula_27, and replace formula_40 by the functions formula_41 in formula_42. This yields\n\nwhere formula_44 is the Jacobian with formula_45 replaced by their value in terms formula_46. The left-hand member is necessarily the joint pdf formula_47 of formula_48. Since formula_49, and thus formula_50, does not depend upon formula_2, then\n\nis a function that does not depend upon formula_2.\n\nA simpler more illustrative proof is as follows, although it applies only in the discrete case.\n\nWe use the shorthand notation to denote the joint probability density of formula_54 by formula_55. Since formula_56 is a function of formula_57, we have formula_58, as long as formula_59 and zero otherwise. Therefore:\n\nwith the last equality being true by the definition of sufficient statistics. Thus formula_61 with formula_62 and formula_63.\n\nConversely, if formula_61, we have\n\nWith the first equality by the definition of pdf for multiple variables, the second by the remark above, the third by hypothesis, and the fourth because the summation is not over formula_66.\n\nLet formula_67 denote the conditional probability density of formula_57 given formula_69. Then we can derive an explicit expression for this:\n\nWith the first equality by definition of conditional probability density, the second by the remark above, the third by the equality proven above, and the fourth by simplification. This expression does not depend on formula_2 and thus formula_56 is a sufficient statistic.\n\nA sufficient statistic is minimal sufficient if it can be represented as a function of any other sufficient statistic. In other words, \"S\"(\"X\") is minimal sufficient if and only if\n\nIntuitively, a minimal sufficient statistic \"most efficiently\" captures all possible information about the parameter \"θ\".\n\nA useful characterization of minimal sufficiency is that when the density \"f\" exists, \"S\"(\"X\") is minimal sufficient if and only if\n\nThis follows as a direct consequence from Fisher's factorization theorem stated above.\n\nA case in which there is no minimal sufficient statistic was shown by Bahadur, 1954. However, under mild conditions, a minimal sufficient statistic does always exist. In particular, in Euclidean space, these conditions always hold if the random variables (associated with formula_75 ) are all discrete or are all continuous.\n\nIf there exists a minimal sufficient statistic, and this is usually the case, then every complete sufficient statistic is necessarily minimal sufficient(note that this statement does not exclude the option of a pathological case in which a complete sufficient exists while there is no minimal sufficient statistic). While it is hard to find cases in which a minimal sufficient statistic does not exist, it is not so hard to find cases in which there is no complete statistic.\n\nThe collection of likelihood ratios formula_76 is a minimal sufficient statistic if formula_77 is discrete or has a density function.\n\nIf \"X\", ..., \"X\" are independent Bernoulli-distributed random variables with expected value \"p\", then the sum \"T\"(\"X\") = \"X\" + ... + \"X\" is a sufficient statistic for \"p\" (here 'success' corresponds to \"X\" = 1 and 'failure' to \"X\" = 0; so \"T\" is the total number of successes)\n\nThis is seen by considering the joint probability distribution:\n\nBecause the observations are independent, this can be written as\n\nand, collecting powers of \"p\" and 1 − \"p\", gives\n\nwhich satisfies the factorization criterion, with \"h\"(\"x\") = 1 being just a constant.\n\nNote the crucial feature: the unknown parameter \"p\" interacts with the data \"x\" only via the statistic \"T\"(\"x\") = Σ \"x\".\n\nAs a concrete application, this gives a procedure for creating a fair coin from a biased coin.\n\nIf \"X\", ..., \"X\" are independent and uniformly distributed on the interval [0,\"θ\"], then \"T\"(\"X\") = max(\"X\", ..., \"X\") is sufficient for θ — the sample maximum is a sufficient statistic for the population maximum.\n\nTo see this, consider the joint probability density function of \"X\"=(\"X\"...,\"X\"). Because the observations are independent, the pdf can be written as a product of individual densities\n\nwhere 1 is the indicator function. Thus the density takes form required by the Fisher–Neyman factorization theorem, where \"h\"(\"x\") = 1, and the rest of the expression is a function of only \"θ\" and \"T\"(\"x\") = max{\"x\"}.\n\nIn fact, the minimum-variance unbiased estimator (MVUE) for \"θ\" is\n\nThis is the sample maximum, scaled to correct for the bias, and is MVUE by the Lehmann–Scheffé theorem. Unscaled sample maximum \"T\"(\"X\") is the maximum likelihood estimator for \"θ\".\n\nIf formula_83 are independent and uniformly distributed on the interval formula_84 (where formula_85 and formula_86 are unknown parameters), then formula_87 is a two-dimensional sufficient statistic for formula_88.\n\nTo see this, consider the joint probability density function of formula_89. Because the observations are independent, the pdf can be written as a product of individual densities, i.e.\n\nThe joint density of the sample takes the form required by the Fisher–Neyman factorization theorem, by letting\n\nSince formula_92 does not depend on the parameter formula_93 and formula_94 depends only on formula_95 through the function formula_96\n\nthe Fisher–Neyman factorization theorem implies formula_97 is a sufficient statistic for formula_88.\n\nIf \"X\", ..., \"X\" are independent and have a Poisson distribution with parameter \"λ\", then the sum \"T\"(\"X\") = \"X\" + ... + \"X\" is a sufficient statistic for \"λ\".\n\nTo see this, consider the joint probability distribution:\n\nBecause the observations are independent, this can be written as\n\nwhich may be written as\n\nwhich shows that the factorization criterion is satisfied, where \"h\"(\"x\") is the reciprocal of the product of the factorials. Note the parameter λ interacts with the data only through its sum \"T\"(\"X\").\n\nIf formula_102 are independent and normally distributed with expected value formula_2 (a parameter) and known finite variance formula_104 then \n\nis a sufficient statistic for formula_106\n\nTo see this, consider the joint probability density function of formula_107. Because the observations are independent, the pdf can be written as a product of individual densities, i.e. -\n\nThe joint density of the sample takes the form required by the Fisher–Neyman factorization theorem, by letting\n\nSince formula_92 does not depend on the parameter formula_2 and formula_112 depends only on formula_95 through the function \n\nthe Fisher–Neyman factorization theorem implies formula_115 is a sufficient statistic for formula_2.\n\n(If formula_117 is unknown: since formula_118, the above likelihood can be rewritten as:\n\nthe Fisher–Neyman factorization theorem still holds and implies that formula_120 is a joint sufficient statistic for formula_121.\n\nIf formula_48 are independent and exponentially distributed with expected value \"θ\" (an unknown real-valued positive parameter), then formula_123 is a sufficient statistic for θ.\n\nTo see this, consider the joint probability density function of formula_107. Because the observations are independent, the pdf can be written as a product of individual densities, i.e. -\n\nThe joint density of the sample takes the form required by the Fisher–Neyman factorization theorem, by letting\n\nSince formula_92 does not depend on the parameter formula_2 and formula_112 depends only on formula_95 through the function formula_123\n\nthe Fisher–Neyman factorization theorem implies formula_123 is a sufficient statistic for formula_2.\n\nIf formula_48 are independent and distributed as a formula_135, where formula_85 and formula_86 are unknown parameters of a Gamma distribution, then formula_138 is a two-dimensional sufficient statistic for formula_93.\n\nTo see this, consider the joint probability density function of formula_107. Because the observations are independent, the pdf can be written as a product of individual densities, i.e. -\n\nThe joint density of the sample takes the form required by the Fisher–Neyman factorization theorem, by letting\n\nSince formula_92 does not depend on the parameter formula_88 and formula_94 depends only on formula_95 through the function formula_147\n\nthe Fisher–Neyman factorization theorem implies formula_148 is a sufficient statistic for formula_149\n\nSufficiency finds a useful application in the Rao–Blackwell theorem, which states that if \"g\"(\"X\") is any kind of estimator of \"θ\", then typically the conditional expectation of \"g\"(\"X\") given sufficient statistic \"T\"(\"X\") is a better estimator of \"θ\", and is never worse. Sometimes one can very easily construct a very crude estimator \"g\"(\"X\"), and then evaluate that conditional expected value to get an estimator that is in various senses optimal.\n\nAccording to the Pitman–Koopman–Darmois theorem, among families of probability distributions whose domain does not vary with the parameter being estimated, only in exponential families is there a sufficient statistic whose dimension remains bounded as sample size increases. Less tersely, suppose formula_150 are independent identically distributed random variables whose distribution is known to be in some family of probability distributions with fixed support. Only if that family is an exponential family there is a (possibly vector-valued) sufficient statistic formula_151 whose number of scalar components does not increase as the sample size \"n\" increases.\n\nThis theorem shows that sufficiency (or rather, the existence of a scalar or vector-valued of bounded dimension sufficient statistic) sharply restricts the possible forms of the distribution.\n\nAn alternative formulation of the condition that a statistic be sufficient, set in a Bayesian context, involves the posterior distributions obtained by using the full data-set and by using only a statistic. Thus the requirement is that, for almost every \"x\",\n\nMore generally, without assuming a parametric model, we can say that the statistics \"T\" is \"predictive sufficient\" if\n\nIt turns out that this \"Bayesian sufficiency\" is a consequence of the formulation above, however they are not directly equivalent in the infinite-dimensional case. A range of theoretical results for sufficiency in a Bayesian context is available.\n\nA concept called \"linear sufficiency\" can be formulated in a Bayesian context, and more generally. First define the best linear predictor of a vector \"Y\" based on \"X\" as formula_154. Then a linear statistic \"T\"(\"x\") is linear sufficient if\n\n\n"}
{"id": "48511100", "url": "https://en.wikipedia.org/wiki?curid=48511100", "title": "Tape diagram", "text": "Tape diagram\n\nA tape diagram, also known as a bar model, is a pictorial representation of ratios. In mathematics education, it is used to solve word problems. So lets do this problem that has been given to us by teachers all around. \"A boy has won 15 games, for every 3 games that he wins, he loses 2, how many games has the little boy lost?\"\n\nA math problem reads: A boy has won 15 games. His ratio for him is 3:2. The tape diagram would look something like this:\nUsing the above diagram, it can be concluded that the boy has lost 10 playing times.\n\nThe tape diagram works by showing different ratios. In the example before it shows how a boy has won 15 games. So if his win to loss ratio is 3:2 it would look like this. How many games has he lost?(3x5)\n\nNow how did you get those three 5s you ask.\nSo the answer to how many games did he lose is 10. Because 5+5=10\n"}
{"id": "1448018", "url": "https://en.wikipedia.org/wiki?curid=1448018", "title": "Teleonomy", "text": "Teleonomy\n\nTeleonomy is the quality of apparent purposefulness and goal-directedness of structures and functions in living organisms brought about by the exercise, augmentation, and, improvement of reasoning. The term derives from two Greek words, τέλος \"telos\" (\"end, purpose\") and νόμος \"nomos\" (\"law\"), and means \"end-directed\" (literally \"purpose-law\"). Teleonomy is sometimes contrasted with teleology, where the latter is understood as a purposeful goal-directedness brought about through human or divine intention. Teleonomy is thought to derive from evolutionary history, adaptation for reproductive success, and/or the operation of a program. Teleonomy is related to programmatic or computational aspects of purpose.\n\nColin Pittendrigh, who coined the term in 1958, applied it to biological phenomena that appear to be end-directed, hoping to limit the much older term teleology to actions planned by an agent who can internally model alternative futures with intention, purpose and foresight:\nIn 1965 Ernst Mayr cited Pittendrigh and criticized him for not making a \"clear distinction between the two teleologies of Aristotle\"; evolution involves Aristotle's material causes and formal causes rather than efficient causes. Mayr adopted Pittendrigh's term, but supplied his own definition:\n\nRichard Dawkins described the properties of \"archeo-purpose\" (by natural selection) and \"neo-purpose\" (by evolved adaptation) in his talk on the \"Purpose of Purpose\". Dawkins attributes the brain's flexibility as an evolutionary feature in adapting or subverting goals to making neo-purpose goals on an overarching evolutionary archeo-purpose. Language allows groups to share neo-purposes, and cultural evolution - occurring much faster than natural evolution - can lead to conflict or collaborations.\n\nIn behavior analysis, Hayne Reese made the adverbial distinction between purposefulness (having an internal determination) and purposiveness (serving or effecting a useful function). Reese implies that non-teleological statements are called teleonomic when they represent an \"if A then C\" phenomenon's antecedent; where, teleology is a consequent representation. The concept of purpose, as only being the teleology final cause, requires supposedly impossible time reversal; because, the future consequent determines the present antecedent. Purpose, as being both in the beginning and the end, simply rejects teleology, and addresses the time reversal problem. In this, Reese sees no value for teleology and teleonomic concepts in behavior analysis; however, the concept of purpose preserved in process can be useful, if not reified. A theoretical time-dimensional tunneling and teleological functioning of temporal paradox would also fit this description without the necessity of a localized intelligence. Whereas the concept of a teleonomic process, such as evolution, can simply refer to a system capable of producing complex products without the benefit of a guiding foresight.\n\nIn 1966 George C. Williams approved of the term in the last chapter of his \"Adaptation and Natural Selection; a critique of some current evolutionary thought\". In 1970, Jacques Monod, in \"Chance and Necessity, an Essay on the Natural Philosophy of Modern Biology\", suggested teleonomy as a key feature that defines life:\n\nIn 1974 Ernst Mayr illustrated the difference in the statements:\n\nSubsequently, philosophers like Ernest Nagel further analysed the concept of goal-directedness in biology and by 1982, philosopher and historian of science David Hull joked about the use of teleology and teleonomy by biologists:\n\nThe concept of teleonomy was largely developed by Mayr and Pittendrigh to separate biological evolution from teleology. Pittendrigh's purpose was to enable biologists who had become overly cautious about goal-oriented language to have a way of discussing the goals and orientations of an organism's behaviors without inadvertently invoking teleology. Mayr was even more explicit, saying that while teleonomy certainly operates on the level of organisms, the process of evolution itself is necessarily non-teleonomic.\n\nEvolution largely hoards hindsight, as variations unwittingly make \"predictions\" about structures and functions which could successfully cope with the future, and which participate in a process of natural selection that culls the unfit, leaving the fit to the next generation. Information accumulates about functions and structures that are successful, exploiting feedback from the environment via the selection of fitter coalitions of structures and functions. Robert Rosen has described these features as an anticipatory system which builds an internal model based on past and possible future states.\n\nIn 1962, Grace A. de Laguna's \"The Role of Teleonomy in Evolution\" attempted to show how different stages of evolution were characterized by different types of teleonomy. de Laguna points out that humans have oriented teleonomy so that the teleonomic goal is not restricted to the reproduction of humans, but also to cultural ideals.\n\nIn recent years, a few biologists believe that the separation of teleonomy from the process of evolution has gone too far. Peter Corning notes that behavior, which is a teleonomic trait, is responsible for the construction of biological niches, which is an agent of selection. Therefore, it would be inaccurate to say that there was no role for teleonomy in the process of evolution, since teleonomy dictates the fitness landscape according to which organisms are selected. Corning calls this phenomenon \"teleonomic selection\".\n\nIn teleology, Kant's positions as expressed in Critique of Judgment, were neglected for many years because in the minds of many scientists they were associated with vitalist views of evolution. Their recent rehabilitation is evident in teleonomy, which bears a number of features, such as the description of organisms, that are reminiscent of the Aristotelian conception of final causes as essentially recursive in nature. Kant's position is that, even though we cannot know whether there are final causes in nature, we are constrained by the peculiar nature of the human understanding to view organisms teleologically. Thus the Kantian view sees teleology as a necessary principle for the study of organisms, but only as a regulative principle, and with no ontological implications.\n\nTalcott Parsons, in the later part of his working with a theory of social evolution and a related theory of world-history, adopted the concept of teleonomy as the fundamental organizing principle for directional processes and his theory of societal development in general. In this way, Parsons tried to find a theoretical compromise between voluntarism as a principle of action and the idea of a certain directionality in history.\n\nTeleonomy is closely related to concepts of emergence, complexity theory, and self-organizing systems. It has extended beneath biology to be applied in the context of chemistry. Some philosophers of biology resist the term and still employ \"teleology\" when analyzing biological function and the language used to describe it, while others endorse it.\n\n\n\n"}
{"id": "49535", "url": "https://en.wikipedia.org/wiki?curid=49535", "title": "Thought experiment", "text": "Thought experiment\n\nA thought experiment (, \"Gedanken-Experiment\", or \"Gedankenerfahrung\",) considers some hypothesis, theory, or principle for the purpose of thinking through its consequences. Given the structure of the experiment, it may not be possible to perform it, and even if it could be performed, there need not be an intention to perform it.\n\nThe common goal of a thought experiment is to explore the potential consequences of the principle in question:\n\nExamples of thought experiments include Schrödinger's cat, illustrating quantum indeterminacy through the manipulation of a perfectly sealed environment and a tiny bit of radioactive substance, and Maxwell's demon, which attempts to demonstrate the ability of a hypothetical finite being to violate the 2nd law of thermodynamics.\n\nThe ancient Greek δείκνυμι \"(transl.: deiknymi)\", or thought experiment, \"was the most ancient pattern of mathematical proof\", and existed before Euclidean mathematics, where the emphasis was on the conceptual, rather than on the experimental part of a thought-experiment. Perhaps the key experiment in the history of modern science is Galileo's demonstration that falling objects must fall at the same rate regardless of their masses. This is widely thought to have been a straightforward physical demonstration, involving climbing up the Leaning Tower of Pisa and dropping two heavy weights off it, whereas in fact, it was a logical demonstration, using the 'thought experiment' technique. The 'experiment' is described by Galileo in \"Discorsi e dimostrazioni matematiche\" (1638) (literally, 'Discourses and Mathematical Demonstrations') thus:\n\nAlthough the extract does not convey the elegance and power of the 'demonstration' terribly well, it is clear that it is a 'thought' experiment, rather than a practical one. Strange then, as Cohen says, that philosophers and scientists alike refuse to acknowledge either Galileo in particular, or the thought experiment technique in general for its pivotal role in both science and philosophy. (The exception proves the rule — the iconoclastic philosopher of science, Paul Feyerabend, has also observed this methodological prejudice.)\n\nInstead, many philosophers prefer to consider 'Thought Experiments' to be merely the use of a hypothetical scenario to help understand the way things are.\n\nThought experiments have been used in a variety of fields, including philosophy, law, physics, and mathematics. In philosophy, they have been used at least since classical antiquity, some pre-dating Socrates. In law, they were well-known to Roman lawyers quoted in the Digest. In physics and other sciences, notable thought experiments date from the 19th and especially the 20th century, but examples can be found at least as early as Galileo.\n\nJohann Witt-Hansen established that Hans Christian Ørsted was the first to use the Latin-German mixed term \"Gedankenexperiment\" (lit. thought experiment) circa 1812. Ørsted was also the first to use its entirely German equivalent, \"Gedankenversuch\", in 1820.\n\nMuch later, Ernst Mach used the term \"Gedankenexperiment\" in a different way, to denote exclusively the \"imaginary\" conduct of a \"real\" experiment that would be subsequently performed as a \"real physical experiment\" by his students. Physical and mental experimentation could then be contrasted: Mach asked his students to provide him with explanations whenever the results from their subsequent, real, physical experiment differed from those of their prior, imaginary experiment.\n\nThe English term \"thought experiment\" was coined (as a calque) from Mach's \"Gedankenexperiment\", and it first appeared in the 1897 English translation of one of Mach’s papers. Prior to its emergence, the activity of posing hypothetical questions that employed subjunctive reasoning had existed for a very long time (for both scientists and philosophers). However, people had no way of categorizing it or speaking about it. This helps to explain the extremely wide and diverse range of the application of the term \"thought experiment\" once it had been introduced into English.\n\nThought experiments, which are well-structured, well-defined hypothetical questions that employ subjunctive reasoning (irrealis moods) – \"What might happen (or, what might have happened) if . . . \" – have been used to pose questions in philosophy at least since Greek antiquity, some pre-dating Socrates. In physics and other sciences many thought experiments date from the 19th and especially the 20th Century, but examples can be found at least as early as Galileo.\n\nIn thought experiments we gain new information by rearranging or reorganizing already known empirical data in a new way and drawing new (a priori) inferences from them or by looking at these data from a different and unusual perspective. In Galileo’s thought experiment, for example, the rearrangement of empirical experience consists in the original idea of combining bodies of different weight.\n\nThought experiments have been used in philosophy (especially ethics), physics, and other fields (such as cognitive psychology, history, political science, economics, social psychology, law, organizational studies, marketing, and epidemiology). In law, the synonym \"hypothetical\" is frequently used for such experiments.\n\nRegardless of their intended goal, all thought experiments display a patterned way of thinking that is designed to allow us to explain, predict and control events in a better and more productive way.\n\nIn terms of their theoretical consequences, thought experiments generally:\n\nThought experiments can produce some very important and different outlooks on previously unknown or unaccepted theories. However, they may make those theories themselves irrelevant, and could possibly create new problems that are just as difficult, or possibly more difficult to resolve.\n\nIn terms of their practical application, thought experiments are generally created to:\n\nScientists tend to use thought experiments as imaginary, \"proxy\" experiments prior to a real, \"physical\" experiment (Ernst Mach always argued that these gedankenexperiments were \"a necessary precondition for physical experiment\"). In these cases, the result of the \"proxy\" experiment will often be so clear that there will be no need to conduct a physical experiment at all.\n\nScientists also use thought experiments when particular physical experiments are impossible to conduct (Carl Gustav Hempel labeled these sorts of experiment \"theoretical experiments-in-imagination\"), such as Einstein's thought experiment of chasing a light beam, leading to special relativity. This is a unique use of a scientific thought experiment, in that it was never carried out, but led to a successful theory, proven by other empirical means.\n\nThe relation to real experiments can be quite complex, as can be seen again from an example going back to Albert Einstein. In 1935, with two coworkers, he published a paper on a newly created subject called later the EPR effect (EPR paradox). In this paper, starting from certain philosophical assumptions, on the basis of a rigorous analysis of a certain, complicated, but in the meantime assertedly realizable model, he came to the conclusion that \"quantum mechanics should be described as \"incomplete\"\". Niels Bohr asserted a refutation of Einstein's analysis immediately, and his view prevailed. After some decades, it was asserted that feasible experiments could prove the error of the EPR paper. These experiments tested the Bell inequalities published in 1964 in a purely theoretical paper. The above-mentioned EPR philosophical starting assumptions were considered to be falsified by empirical fact (e.g. by the optical \"real experiments\" of Alain Aspect).\n\nThus \"thought experiments\" belong to a theoretical discipline, usually to theoretical physics, but often to theoretical philosophy. In any case, it must be distinguished from a real experiment, which belongs naturally to the experimental discipline and has \"the final decision on \"true\" or \"not true\"\", at least in physics.\n\nThe first characteristic pattern that thought experiments display is their orientation\nin time. They are either:\n\nThe second characteristic pattern is their movement in time in relation to “the present\nmoment standpoint” of the individual performing the experiment; namely, in terms of:\n\nGenerally speaking, there are seven types of thought experiments in which one reasons from causes to effects, or effects to causes:\n\n\"Prefactual (before the fact) thought experiments\" — the term prefactual was coined by Lawrence J. Sanna in 1998 — speculate on possible future outcomes, given the present, and ask \"What will be the outcome if event E occurs?\"\n\n\"Counterfactual (contrary to established fact) thought experiments\" — the term \"counterfactual\" was coined by Nelson Goodman in 1947, extending Roderick Chisholm's (1946) notion of a \"contrary-to-fact conditional\" — speculate on the possible outcomes of a different past; and ask \"What might have happened if A had happened instead of B?\" (e.g., \"If Isaac Newton and Gottfried Leibniz had cooperated with each other, what would mathematics look like today?\").\n\nThe study of counterfactual speculation has increasingly engaged the interest of scholars in a wide range of domains such as philosophy, psychology, cognitive psychology, history, political science, economics, social psychology, law, organizational theory, marketing, and epidemiology.\n\n\"Semifactual thought experiments\" — the term \"semifactual\" was coined by Nelson Goodman in 1947 — speculate on the extent to which things might have remained the same, despite there being a different past; and asks the question Even though X happened instead of E, would Y have still occurred? (e.g., Even if the goalie had moved left, rather than right, could he have intercepted a ball that was traveling at such a speed?).\n\nSemifactual speculations are an important part of clinical medicine.\n\nThe activity of prediction attempts to project the circumstances of the present into the future. According to David Sarewitz and Roger Pielke (1999, p123), scientific prediction takes two forms:\n\nAlthough they perform different social and scientific functions, the only difference between the qualitatively identical activities of \"predicting\", \"forecasting,\" and \"nowcasting\" is the distance of the speculated future from the present moment occupied by the user. Whilst the activity of nowcasting, defined as “a detailed description of the current weather along with forecasts obtained by extrapolation up to 2 hours ahead”, is essentially concerned with describing the current state of affairs, it is common practice to extend the term “to cover very-short-range forecasting up to 12 hours ahead” (Browning, 1982, p.ix).\n\nThe activity of hindcasting involves running a forecast model after an event has happened in order to test whether the model's simulation is valid.\n\nIn 2003, Dake Chen and his colleagues “trained” a computer using the data of the surface temperature of the oceans from the last 20 years. Then, using data that had been collected on the surface temperature of the oceans for the period 1857 to 2003, they went through a hindcasting exercise and discovered that their simulation not only accurately predicted every El Niño event for the last 148 years, it also identified the (up to 2 years) looming foreshadow of every single one of those El Niño events.\n\nThe activity of \"retrodiction\" (or \"postdiction\") involves moving backwards in time, step-by-step, in as many stages as are considered necessary, from the present into the speculated past to establish the ultimate cause of a specific event (e.g., reverse engineering and forensics).\n\nGiven that retrodiction is a process in which \"past observations, events and data are used as evidence to infer the process(es) the produced them\" and that diagnosis \"involve[s] going from visible effects such as symptoms, signs and the like to their prior causes\", the essential balance between prediction and retrodiction could be characterized as:\nregardless of whether the prognosis is of the course of the disease in the absence of treatment, or of the application of a specific treatment regimen to a specific disorder in a particular patient.\n\nThe activity of \"backcasting\" — the term \"backcasting\" was coined by John Robinson in 1982 — involves establishing the description of a very definite and very specific future situation. It then involves an imaginary moving backwards in time, step-by-step, in as many stages as are considered necessary, from the future to the present to reveal the mechanism through which that particular specified future could be attained from the present.\n\nBackcasting is not concerned with predicting the future:\n\nAccording to Jansen (1994, p. 503:\n\nIn philosophy, a thought experiment typically presents an imagined scenario with the intention of eliciting an intuitive or reasoned response about the way things are in the thought experiment. (Philosophers might also supplement their thought experiments with theoretical reasoning designed to support the desired intuitive response.) The scenario will typically be designed to target a particular philosophical notion, such as morality, or the nature of the mind or linguistic reference. The response to the imagined scenario is supposed to tell us about the nature of that notion in any scenario, real or imagined.\n\nFor example, a thought experiment might present a situation in which an agent intentionally kills an innocent for the benefit of others. Here, the relevant question is not whether the action is moral or not, but more broadly whether a moral theory is correct that says morality is determined solely by an action's consequences (See Consequentialism). John Searle imagines a man in a locked room who receives written sentences in Chinese, and returns written sentences in Chinese, according to a sophisticated instruction manual. Here, the relevant question is not whether or not the man understands Chinese, but more broadly, whether a functionalist theory of mind is correct.\n\nIt is generally hoped that there is universal agreement about the intuitions that a thought experiment elicits. (Hence, in assessing their own thought experiments, philosophers may appeal to \"what we should say,\" or some such locution.) A successful thought experiment will be one in which intuitions about it are widely shared. But often, philosophers differ in their intuitions about the scenario.\n\nOther philosophical uses of imagined scenarios arguably are thought experiments also. In one use of scenarios, philosophers might imagine persons in a particular situation (maybe ourselves), and ask what they would do.\n\nFor example, in the veil of ignorance, John Rawls asks us to imagine a group of persons in a situation where they know nothing about themselves, and are charged with devising a social or political organization. The use of the state of nature to imagine the origins of government, as by Thomas Hobbes and John Locke, may also be considered a thought experiment. Søren Kierkegaard explored the possible ethical and religious implications of Abraham's binding of Isaac in \"Fear and Trembling\" Similarly, Friedrich Nietzsche, in \"On the Genealogy of Morals\", speculated about the historical development of Judeo-Christian morality, with the intent of questioning its legitimacy.\n\nAn early written thought experiment was Plato's allegory of the cave. Another historic thought experiment was Avicenna's \"Floating Man\" thought experiment in the 11th century. He asked his readers to imagine themselves suspended in the air isolated from all in order to demonstrate human self-awareness and self-consciousness, and the substantiality of the soul.\n\nIn many thought experiments, the scenario would be nomologically possible, or possible according to the laws of nature. John Searle's Chinese room is nomologically possible.\n\nSome thought experiments present scenarios that are not nomologically possible. In his Twin Earth thought experiment, Hilary Putnam asks us to imagine a scenario in which there is a substance with all of the observable properties of water (e.g., taste, color, boiling point), but is chemically different from water. It has been argued that this thought experiment is not nomologically possible, although it may be possible in some other sense, such as metaphysical possibility. It is debatable whether the nomological impossibility of a thought experiment renders intuitions about it moot.\n\nIn some cases, the hypothetical scenario might be considered metaphysically impossible, or impossible in any sense at all. David Chalmers says that we can imagine that there are zombies, or persons who are physically identical to us in every way but who lack consciousness. This is supposed to show that physicalism is false. However, some argue that zombies are inconceivable: we can no more imagine a zombie than we can imagine that 1+1=3. Others have claimed that the conceivability of a scenario may not entail its possibility.\n\nThe philosophical work of Stefano Gualeni focuses on the use of virtual worlds to materialize thought experiments and to playfully negotiate philosophical ideas. His arguments were originally presented in his 2015 book \"Virtual Worlds as Philosophical Tools\".\n\nGualeni's argument is that the history of philosophy has, until recently, merely been the history of written thought, and digital media can complement and enrich the limited and almost exclusively linguistic approach to philosophical thought. He considers virtual worlds to be philosophically viable and advantageous in contexts like those of thought experiments, when the recipients of a certain philosophical notion or perspective are expected to objectively test and evaluate different possible courses of action, or in cases where they are confronted with interrogatives concerning non-actual or non-human phenomenologies .\n\nAmong the most visible thought experiments designed by Stefano Gualeni:\n\nOther examples of playful, interactive thought experiments:\n\n\n\n\n\n\n\n"}
{"id": "17213240", "url": "https://en.wikipedia.org/wiki?curid=17213240", "title": "Treaty rights", "text": "Treaty rights\n\nTreaty rights are certain rights that were reserved by indigenous peoples when they signed treaties with settler societies in the wake of European colonization. This applies to the rights of Alaska Natives and Native Americans in the United States and First Nations in Canada, as well as to a smaller number of Inuit and Metis in Canada who have entered into treaties.\n\nTreaty rights are not the only rights claimed by indigenous peoples. Indigenous people claim inherent rights to self-determination, which implies that they be recognized as rights-bearing groups (called \"tribes\", \"bands\", or \"nations\" depending on the place and time) capable of self-determination and cultural survival. Once the state recognizes that there is another body corporate with legal personality capable of making binding agreements on behalf of its members, then negotiations can begin for mutual exchange and aid: a treaty. The earliest of these agreements, between colonial powers such the French, British, and the Dutch and various indigenous peoples of the Atlantic coastal regions had the character of military alliances, as between peers. Later treaties, however, were generally about the cession of land from weakened Aboriginal peoples to expanding settler states. By the Royal Proclamation of 1763 the British Crown (i.e. the state) declared that individual British subjects could not buy land from native nations; only the Crown could obtain land from native nations through treaty, which it could then redistribute to individuals. This principle, which was adopted by both Canada and the United States upon gaining independence from Britain, became the legal impetus for all subsequent treaties in North America.\n\nBy signing treaties, indigenous peoples traded vast amounts of their land and resources in exchange for reserved areas of land (Indian reservations [US terminology] and Indian reserves [Canadian terminology]) and things like protection (from attacks on their lands), health care, , and religious freedom, protection of hunting and fishing rights, and sometimes some monies as well. Because Article Six of the United States Constitution declares treaties to be the supreme law of the land, treaties are just as valid today as they were the day they were signed, and treaty rights are still legally binding as well. Likewise treaty rights were enshrined in Canada under section 35 by the package of constitutional reforms of 1982.\n\nA common critique of the treaty relationship is that treaty rights are \"special\" rights given to indigenous people by the state because of their racial status. Defenders of the treaty system argue however, that the government does not \"give\" treaty rights to anyone – native people reserved them when they signed treaties in a government-to-government relationship.\n\nTreaty rights are frequently subject to public debate, particularly hunting and fishing rights. Many Native nations have reserved rights to hunt and fish in their accustomed places, which are often lands that was given up at the treaty signing, or \"ceded land\". This leads to conflict with sports and commercial hunters and fishers, who are competing for the same limited resource in the same place.\n\nAnother common source of conflict is management decisions about the land or rivers on which Native people have rights. Things like dams and logging have huge effects on fish and wildlife populations. In Canadian law, government how have a court-mandated \"duty to consult\" indigenous peoples regarding the management process of these lands and rivers. In the United States, no such mandate exists.\n\nBeginning in the 1980s and extending into the early 1990s, Northern Wisconsin was rife in protests against Ojibwe spearfishing. The Voigt decision in 1983 had reaffirmed that the treaties made in 1837 and 1842 still stood. These treaties gave the Ojibwe the rights to hunt, fish, and gather off-reservation, which was not subject to state regulation. This heralded a backlash of non-Natives, who believed the Ojibwe had been granted special rights. Spearheaded by groups like Stop Treaty Abuse (STA), often violent and racially discriminatory protests against spearfishing covered boat landings across northern Wisconsin. This led to the case \"Lac du Flambeau Band of Lake Superior Chippewa Indians v. Stop Treaty Abuse-Wisconsin\", which culminated with Judge Barbara Crabb upholding the Voigt decision and many members, donors, and politicians distancing themselves from the STA, which many believed was racist.\n\nThe right to hunt North Pacific Gray Whales has been a contentious issue for the Makah people in Washington state. The Makah people ceded much of their traditional lands in the Treaty of Neah Bay in 1855, but retained the right to whale. The tribe voluntarily gave up this practice in 1915 because of decimated Grey Whale populations, but once the species was taken off the Federal Endangered Species List in 1993, the tribe sought to continue whaling. In 1999, they killed one whale, but faced immediate backlash from environmental groups and animal rights groups. The International Whaling Commission (IWC) believed that the Makah tribe’s quota of harvesting up to five whales a year would not hurt the recovering population. Because of a number of new studies garnishing evidence for and against this practice, the issue has been tied up in court since 1999, with the tribe being unable to exercise the right given to them in the Treaty of Neah Bay.\n\nThroughout the nineteenth century, the United States made a number of treaties with the then Kingdom of Hawaii, the final being in 1887. These treaties recognized the Kingdom of Hawaii as being sovereign and independent. In 1893, John L. Stevens, US minister assigned to the Kingdom of Hawaii, led a group of non-indigenous people to overthrow Queen Lili‘uokalani, which was backed by United States naval forces. They established a Provisional government, which then declared itself the Republic of Hawaii. In 1899, the US annexed Hawaii. Many Hawaiian sovereignty activists feel that because of the aforementioned treaties, Hawaii should today be its own Nation instead of part of the United States.\n\nThe Indigenous people of Standing Rock reservation in North and South Dakota believe that the Dakota Access Pipeline, which runs near their main source of water, could contaminate that source of water should it leak. The also cite the Fort Laramie Treaties of 1851 and 1868, which promised the land that DAPL runs through to the Sioux Tribe. Lands were seized in 1877 (8) and 1887 with the Dawes Allotment Act that broke up reservations (9). Some call for these treaties to be reinstated and enforced today, which would put the course of the DAPT straight through Sioux lands.\n"}
{"id": "5090138", "url": "https://en.wikipedia.org/wiki?curid=5090138", "title": "Unisystem", "text": "Unisystem\n\nUnisystem is a generic role-playing game system produced by Eden Studios, Inc. It is used in \"All Flesh Must Be Eaten\", the Buffyverse role-playing games, \"CJ Carella's WitchCraft\", \"Conspiracy X (2nd Ed.)\", and several other games. Games designed using \"Unisystem\" have been nominated for, and won, Origins Awards.\n\n\"Unisystem\" was developed by C.J. Carella for the first editions of his \"WitchCraft\" and \"\" role-playing games, \npublished by Myrmidion Press. Eden Studios uses \"Unisystem\" under an exclusive license, but has allowed other game companies to license the system. Eden Studios published \"WitchCraft 2nd Edition\" and their original \"All Flesh Must Be Eaten\" games as \"Unisystem\" games. \"Armageddon 2nd Edition\" and a new 2nd Edition of Eden's own \"Conspiracy X\" were later published using this gaming system.\n\n\"Unisystem\" games use a point-buy system to generate characters. Points are spent on Attributes, Skills, Qualities and Metaphysics. Drawbacks can be used to gain extra points.\n\nAttributes represent the character's main six abilities, which in this case are three physical attributes: Strength, Dexterity, Constitution; and three mental attributes: Intelligence, Perception, Willpower. Characters also have a variety of Skills, Qualities and Drawbacks to give the character added perks or faults (respectively). Other Qualities and Drawbacks are of a Supernatural sort and are used to designate a character as a particular type of character (be it a Gifted human, a Vampyre, a Spirit, and so on) or detail what sort of magic they can do.\n\nActions are resolved by adding together the character's relevant Attribute, the applicable Skill, and a ten-sided die roll, plus any modifiers for difficulty; if the total is nine or greater, the action succeeds. Gamemasters (called \"Chroniclers\" in \"Classic\", and \"Directors\" in \"Cinematic Unisystem\") are also given tables of successes: characters that get a higher roll than the required nine have bonuses on their final outcome; this table describes how successful they turn out to be, and gives guidelines to figure out the resulting bonuses.\n\n\"Unisystem\" can be divided into two sub-systems: \"Classic\" and \"Cinematic\".\n\"Classic Unisystem\" was developed first and is featured in \"All Flesh Must Be Eaten\", \"\", \"Conspiracy X 2.0\", \"Terra Primate\" and \"WitchCraft\". It is typified by grittier, more realistic play, greater attention to detail in skills and combat, and the use of Secondary Attributes that are typically derived from the Primary Attributes. Essence, or the measure of life-force and magic, is also central to \"Classic Unisystem\" play.\n\n\"Cinematic Unisystem\" was designed to mimic the \"cinematic\" exploits of characters. First developed for the \"Buffy the Vampire Slayer Roleplaying Game\", it was later adopted for use in \"Angel\",\n\"Army of Darkness\" and \"Ghosts of Albion\". \"Cinematic\" features a limited set of skills that cover most situations, no Secondary Attributes or Essence, simplified combat and flat damage resolution. \"Cinematic\" games also feature the use of Drama Points which allow the character to do things above and beyond what they could do under the system alone.\n\nThe chief difference between the systems are Drama Points (used only in the Cinematic system), and differences in magic system mechanics.\n\nDespite the differences the two are still largely compatible, though every \"Cinematic Unisystem\" game offers an appendix to convert values from the \"Classic\" system, and the \"Buffy RPG\"s Magic Box supplement offers an appendix on converting between the \"Buffy\" magic system and the \"WitchCraft\" magic system.\n"}
{"id": "434818", "url": "https://en.wikipedia.org/wiki?curid=434818", "title": "Veil of ignorance", "text": "Veil of ignorance\n\nThe \"veil of ignorance\" is a method of determining the morality of political issues proposed in 1971 by American philosopher John Rawls in his \"original position\" political philosophy. It is based upon the following thought experiment: people making political decisions imagine that they know nothing about the particular talents, abilities, tastes, social class, and positions they will have within a social order. When such parties are selecting the principles for distribution of rights, positions, and resources in the society in which they will live, this \"veil of ignorance\" prevents them from knowing who will receive a given distribution of rights, positions, and resources in that society. For example, for a proposed society in which 50% of the population is kept in slavery, it follows that on entering the new society there is a 50% likelihood that the participant would be a slave. The idea is that parties subject to the veil of ignorance will make choices based upon moral considerations, since they will not be able to make choices based on their own self- or class-interest.\n\nAs Rawls put it, \"no one knows his place in society, his class position or social status; nor does he know his fortune in the distribution of natural assets and abilities, his intelligence and strength, and the like\". The idea of the thought experiment is to render obsolete those personal considerations that are morally irrelevant to the justice or injustice of principles meant to allocate the benefits of social cooperation. The veil of ignorance is part of a long tradition of thinking in terms of a social contract that includes the writings of Immanuel Kant, Thomas Hobbes, John Locke, Jean Jacques Rousseau, and Thomas Jefferson.\n\nSpencer J. Maxcy outlines the concept as follows:Imagine that you have set for yourself the task of developing a totally new social contract for today's society. How could you do so fairly? Although you could never actually eliminate all of your personal biases and prejudices, you would need to take steps at least to minimize them. Rawls suggests that you imagine yourself in an original position behind a veil of ignorance. Behind this veil, you know nothing of yourself and your natural abilities, or your position in society. You know nothing of your sex, race, nationality, or individual tastes. Behind such a veil of ignorance all individuals are simply specified as rational, free, and morally equal beings. You do know that in the \"real world\", however, there will be a wide variety in the natural distribution of natural assets and abilities, and that there will be differences of sex, race, and culture that will distinguish groups of people from each other.\n\nIt has been argued that such a concept can have grand effects if it were to be practiced both in the present and in the past. Referring again to the example of slavery, if the slave-owners were forced through the veil of ignorance to imagine that they themselves may be slaves, then suddenly slavery may no longer seem justifiable. A grander example would be if each individual in society were to base their practices off the fact that they could be the least advantaged member of society. In this scenario, freedom and equality could possibly coexist in a way that has been the ideal of many philosophers. For example, in the imaginary society, one might or might not be intelligent, rich, or born into a preferred class. Since one may occupy any position in the society once the veil is lifted, the device forces the parties to consider society from the perspective of all members, including the worst-off and best-off members.\n\nThe concept of the veil of ignorance has been in use by other names for centuries by philosophers such as John Stuart Mill and Immanuel Kant whose work discussed the concept of the social contract. John Harsanyi helped to formalize the concept in economics, and argued that it provides an argument in favor of utilitarianism rather than an argument for a social contract. The modern usage was developed by John Rawls in his 1971 book \"A Theory of Justice\"\n\n\n"}
{"id": "1519001", "url": "https://en.wikipedia.org/wiki?curid=1519001", "title": "Verstehen", "text": "Verstehen\n\nVerstehen (, literally: \"to understand\"), in the context of German philosophy and social sciences in general, has been used since the late 19th century – in English as in German – with the particular sense of the \"interpretive or participatory\" examination of social phenomena. The term is closely associated with the work of the German sociologist, Max Weber, whose antipositivism established an alternative to prior sociological positivism and economic determinism, rooted in the analysis of social action. In anthropology, \"Verstehen\" has come to mean a systematic interpretive process in which an outside observer of a culture attempts to relate to it and understand others. \n\n\"Verstehen\" is now seen as a concept and a method central to a rejection of positivistic social science (although Weber appeared to think that the two could be united). \"Verstehen\" refers to understanding the meaning of action from the actor's point of view. It is entering into the shoes of the other, and adopting this research stance requires treating the actor as a subject, rather than an object of your observations. It also implies that unlike objects in the natural world human actors are not simply the product of the pulls and pushes of external forces. Individuals are seen to create the world by organizing their own understanding of it and giving it meaning. To do research on actors without taking into account the meanings they attribute to their actions or environment is to treat them like objects.\n\nInterpretative sociology (\"verstehende Soziologie\") is the study of society that concentrates on the meanings people associate to their social world. Interpretative sociology strives to show that reality is constructed by people themselves in their daily lives.\n\n\"Verstehen\" roughly translates to \"meaningful understanding\" or \"putting yourself in the shoes of others to see things from their perspective\". Interpretive sociology differs from positivist sociology in three ways:\n\n\"Verstehen\" was introduced into philosophy and the human sciences (\"Geisteswissenschaften\") by the German historist philosopher Johann Gustav Droysen. He first made a distinction between nature and history in terms of the categories of space and time. The method of the natural sciences (\"Naturwissenschaften\") is explanation (\"erklären\"), while that of history is understanding (\"verstehen\").\n\nThe concept of \"Verstehen\" was later used by the German philosopher Wilhelm Dilthey to describe the first-person participatory perspective that agents have on their individual experience as well as their culture, history, and society. In this sense, it is developed in the context of the theory and practice of \"interpretation\" (as understood in the context of hermeneutics) and contrasted with the external objectivating third-person perspective of \"explanation\" (\"das Erklären\") in which human agency, subjectivity, and its products are analyzed as effects of impersonal natural forces in the natural sciences and social structures in sociology.\n\nTwentieth-century philosophers such as Martin Heidegger and Hans-Georg Gadamer have been critical of what they considered to be the romantic and subjective character of \"Verstehen\" in Dilthey, although both Dilthey and the early Heidegger were interested in the \"facticity\" and \"life-context\" of understanding, and sought to universalize it as the way humans exist through language on the basis of ontology. \"Verstehen\" also played a role in Edmund Husserl and Alfred Schutz's analysis of the \"lifeworld.\" Jürgen Habermas and Karl-Otto Apel further transformed the concept of \"Verstehen\", reformulating it on the basis of a transcendental-pragmatic philosophy of language and the theory of communicative action.\n\nMax Weber and Georg Simmel introduced interpretive understanding (\"Verstehen\") into sociology, where it has come to mean a systematic interpretive process in which an outside observer of a culture (such as an anthropologist or sociologist) relates to an indigenous people or sub-cultural group on their own terms and from their own point of view, rather than interpreting them in terms of his or her own culture. \"Verstehen\" can mean either a kind of empathic or participatory understanding of social phenomena. In anthropological terms this is sometimes described as cultural relativism, especially by those that have a tendency to argue toward universal ideals. In sociology it is an aspect of the comparative-historical approach, where the context of a society like twelfth century \"France\" can be potentially better understood (\"Besserverstehen\") by the sociologist than it could have been by people living in a village in Burgundy. It relates to how people in life give meaning to the social world around them and how the social scientist accesses and evaluates this \"first-person perspective\". This concept has been both expanded and criticized by later social scientists. Proponents laud this concept as the only means by which researchers from one culture can examine and explain behaviors in another. While the exercise of \"Verstehen\" has been more popular among social scientists in Europe, such as Habermas, \"Verstehen\" was introduced into the practice of sociology in the United States by Talcott Parsons, an American follower of Max Weber. Parsons used his structural functionalism to incorporate this concept into his 1937 work, \"The Structure of Social Action\".\n\nWeber had more specific beliefs than Marx where he put value to understanding and meaning of key elements—not just with intuition or sympathy with the individual but also the product of \"systematic and rigorous research\". The goal is to identify human actions and interpreting them as observable events leading us to believe that it not only provides for a good explanation for individual actions but also for group interactions. The meaning attached needs to include constraints and limitations and analyze the motivation for action. Weber believed that this gives the sociologist an advantage over a natural scientist because \"We can accomplish something which is never attainable in the natural sciences, namely the subjective understanding of the action of the component individuals\" (Weber, Economy and Society, p. 15).\n\nCritics of the social scientific concept of \"Verstehen\" such as Mikhail Bakhtin and Dean MacCannell counter that it is simply impossible for a person born of one culture to ever completely understand another culture, and that it is arrogant and conceited to attempt to interpret the significance of one culture's symbols through the terms of another (supposedly superior) culture. Such criticisms do not necessarily allow for the possibility that \"Verstehen\" does not involve \"complete\" understanding. Just as in physical science all knowledge is asymptotic to the full explanation, a high degree of cross-cultural understanding is very valuable. The opposite of \"Verstehen\" would seem to be ignorance of all but that which is immediately observable, meaning that we would not be able to understand any time and place but our own. A certain level of interpretive understanding is necessary for our own cultural setting, however, and it can easily be argued that even the full participant in a culture does not fully understand it in every regard.\n\nCritics also believe that it is the sociologist's job to not just observe people and what people do but also share in their world of meaning and come to appreciate why they act as they do. Subjective thoughts and feelings regarded as bias in the sciences is an important aspect to be controlled for while doing sociological research.\n\n"}
{"id": "191445", "url": "https://en.wikipedia.org/wiki?curid=191445", "title": "Vocabulary", "text": "Vocabulary\n\nA vocabulary is a set of familiar words within a person's language. A vocabulary, usually developed with age, serves as a useful and fundamental tool for communication and acquiring knowledge. Acquiring an extensive vocabulary is one of the largest challenges in learning a second language.\n\nVocabulary is commonly defined as \"all the words known and used by a particular person\". \"Knowing\" a word, however, is not as simple as merely being able to recognize or use it. There are several aspects of word knowledge that are used to measure word knowledge.\n\nThe first major distinction that must be made when evaluating word knowledge is whether the knowledge is productive (also called achieve) or receptive (also called receive); even within those opposing categories, there is often no clear distinction. Words that are generally understood when heard or read or seen constitute a person's receptive vocabulary. These words may range from well-known to barely known (see degree of knowledge below). A person's receptive vocabulary is the larger of the two. For example, although a young child may not yet be able to speak, write, or sign, he or she may be able to follow simple commands and appear to understand a good portion of the language to which they are exposed. In this case, the child's receptive vocabulary is likely tens, if not hundreds of words, but his or her active vocabulary is zero. When that child learns to speak or sign, however, the child's active vocabulary begins to increase. It is also possible for the productive vocabulary to be larger than the receptive vocabulary, for example in a second-language learner who has learned words through study rather than exposure, and can produce them, but has difficulty recognizing them in conversation.\n\nProductive vocabulary, therefore, generally refers to words that can be produced within an appropriate context and match the intended meaning of the speaker or signer. As with receptive vocabulary, however, there are many degrees at which a particular word may be considered part of an active vocabulary. Knowing how to pronounce, sign, or write a word does not necessarily mean that the word that has been used correctly or accurately reflects the intended message; but it does reflect a minimal amount of productive knowledge.\n\nWithin the receptive–productive distinction lies a range of abilities that are often referred to as \"degree of knowledge\". This simply indicates that a word gradually enters a person's vocabulary over a period of time as more aspects of word knowledge are learnt. Roughly, these stages could be described as:\n\n\nThe differing degrees of word knowledge imply a greater \"depth of knowledge\", but the process is more complex than that. There are many facets to knowing a word, some of which are not hierarchical so their acquisition does not necessarily follow a linear progression suggested by \"degree of knowledge\". Several frameworks of word knowledge have been proposed to better operationalise this concept. One such framework includes nine facets:\n\n\nWords can be defined in various ways, and estimates of vocabulary size differ depending on the definition used. The most common definition is that of a lemma (the uninflected or dictionary form; this includes \"walk\", but not \"walks, walked or walking\"). Most of the time lemmas do not include proper nouns (names of people, places, companies, etc). Another definition often used in research of vocabulary size is that of word family. These are all the words that can be derived from a ground word (e.g., the words \"effortless, effortlessly, effortful, effortfully\" are all part of the word family \"effort\"). Estimates of vocabulary size range from as high as 200 thousand to as low as 10 thousand, depending on the definition used. \n\n\"Listed in order of most ample to most limited:\"\n\nA literate person's vocabulary is all the words they can recognize when reading. This is generally the largest type of vocabulary simply because a reader tends to be exposed to more words by reading than by listening.\n\nA person's listening vocabulary is all the words they can recognize when listening to speech. People may still understand words they were not exposed to before using cues such as tone, gestures, the topic of discussion and the social context of the conversation.\n\nA person's speaking vocabulary is all the words they use in speech. It is likely to be a subset of the listening vocabulary. Due to the spontaneous nature of speech, words are often misused. This misuse, though slight and unintentional, may be compensated by facial expressions and tone of voice.\n\nWords are used in various forms of writing from formal essays to social media feeds. Many written words do not commonly appear in speech. Writers generally use a limited set of words when communicating. For example, if there are a number of synonyms, a writer may have a preference as to which of them to use, and they are unlikely to use technical vocabulary relating to a subject in which they have no knowledge or interest.\n\nFocal vocabulary is a specialized set of terms and distinctions that is particularly important to a certain group: those with a particular focus of experience or activity. A lexicon, or vocabulary, is a language's dictionary: its set of names for things, events, and ideas. Some linguists believe that lexicon influences people's perception of things, the Sapir–Whorf hypothesis. For example, the Nuer of Sudan have an elaborate vocabulary to describe cattle. The Nuer have dozens of names for cattle because of the cattle's particular histories, economies, and environments. This kind of comparison has elicited some linguistic controversy, as with the number of \"Eskimo words for snow\". English speakers with relevant specialised knowledge can also display elaborate and precise vocabularies for snow and cattle when the need arises.\n\nDuring its infancy, a child instinctively builds a vocabulary. Infants imitate words that they hear and then associate those words with objects and actions. This is the listening vocabulary. The speaking vocabulary follows, as a child's thoughts become more reliant on his/her ability to self-express without relying on gestures or babbling. Once the reading and writing vocabularies start to develop, through questions and education, the child starts to discover the anomalies and irregularities of language.\n\nIn first grade, a child who can read learns about twice as many words as one who cannot. Generally, this gap does not narrow later. This results in a wide range of vocabulary by age five or six, when an English-speaking child will have learned about 1500 words.\n\nVocabulary grows throughout our entire life. Between the ages of 20 and 60, people learn some 6,000 more lemmas, or one every other day. An average 20-year-old knows 42,000 words coming from 11,100 word families; an average 60-year-old knows 48,200 lemmas coming from 13,400 word families. People expand their vocabularies by e.g. reading, playing word games, and participating in vocabulary-related programs. Exposure to traditional print media teaches correct spelling and vocabulary, while exposure to text messaging leads to more relaxed word acceptability constraints.\n\n\nEstimating average vocabulary size poses various difficulties and limitations due to the different definitions and methods employed such as what is the word, what is to know a word, what sample dictionaries were used, how tests were conducted, and so on. Native speakers' vocabularies also vary widely within a language, and are dependent on the level of the speaker's education.\n\nAs a result estimates vary from as little as 10,000 to as many as over 50,000 for young adult native speakers of English.\n\nOne most recent 2016 study shows that 20-year-old English native speakers recognize on average 42,000 lemmas, ranging from 27,100 for the lowest 5% of the population to 51,700 lemmas for the highest 5%. These lemmas come from 6,100 word families in the lowest 5% of the population and 14,900 word families in the highest 5%. 60-year-olds know on average 6,000 lemmas more.\nAccording to another, earlier 1995 study junior-high students would be able to recognize the meanings of about 10,000–12,000 words, whereas for college students this number grows up to about 12,000–17,000 and for elderly adults up to about 17,000 or more.\n\nFor native speakers of German average absolute vocabulary sizes range from 5,900 lemmas in first grade to 73,000 for adults.\n\nThe knowledge of the 3000 most frequent English word families or the 5000 most frequent words provides 95% vocabulary coverage of spoken discourse.\nFor minimal reading comprehension a threshold of 3,000 word families (5,000 lexical items) was suggested and for reading for pleasure 5,000 word families (8,000 lexical items) are required. An \"optimal\" threshold of 8,000 word families yields the coverage of 98% (including proper nouns).\n\nLearning vocabulary is one of the first steps in learning a second language, but a learner never finishes vocabulary acquisition. Whether in one's native language or a second language, the acquisition of new vocabulary is an ongoing process. There are many techniques that help one acquire new vocabulary.\n\nAlthough memorization can be seen as tedious or boring, associating one word in the native language with the corresponding word in the second language until memorized is considered one of the best methods of vocabulary acquisition. By the time students reach adulthood, they generally have gathered a number of personalized memorization methods. Although many argue that memorization does not typically require the complex cognitive processing that increases retention (Sagarra and Alba, 2006), it does typically require a large amount of repetition, and spaced repetition with flashcards is an established method for memorization, particularly used for vocabulary acquisition in computer-assisted language learning. Other methods typically require more time and longer to recall.\n\nSome words cannot be easily linked through association or other methods. When a word in the second language is phonologically or visually similar to a word in the native language, one often assumes they also share similar meanings. Though this is frequently the case, it is not always true. When faced with a false friend, memorization and repetition are the keys to mastery. If a second language learner relies solely on word associations to learn new vocabulary, that person will have a very difficult time mastering false friends. When large amounts of vocabulary must be acquired in a limited amount of time, when the learner needs to recall information quickly, when words represent abstract concepts or are difficult to picture in a mental image, or when discriminating between false friends, rote memorization is the method to use. A neural network model of novel word learning across orthographies, accounting for L1-specific memorization abilities of L2-learners has recently been introduced (Hadzibeganovic and Cannas, 2009).\n\nOne useful method of building vocabulary in a second language is the keyword method. If time is available or one wants to emphasize a few key words, one can create mnemonic devices or word associations. Although these strategies tend to take longer to implement and may take longer in recollection, they create new or unusual connections that can increase retention. The keyword method requires deeper cognitive processing, thus increasing the likelihood of retention (Sagarra and Alba, 2006). This method uses fits within Paivio's (1986) dual coding theory because it uses both verbal and image memory systems. However, this method is best for words that represent concrete and imageable things. Abstract concepts or words that do not bring a distinct image to mind are difficult to associate. In addition, studies have shown that associative vocabulary learning is more successful with younger students (Sagarra and Alba, 2006). Older students tend to rely less on creating word associations to remember vocabulary.\n\nSeveral word lists have been developed to provide people with a limited vocabulary either for the purpose of rapid language proficiency or for effective communication. These include Basic English (850 words), Special English (1,500 words), General Service List (2,000 words), and Academic Word List. Some learner's dictionaries have developed defining vocabularies which contain only most common and basic words. As a result word definitions in such dictionaries can be understood even by learners with a limited vocabulary. Some publishers produce dictionaries based on word frequency or thematic groups.\n\nThe Swadesh list was made for investigation in linguistics.\n\n\n\n"}
{"id": "164631", "url": "https://en.wikipedia.org/wiki?curid=164631", "title": "Wavenumber–frequency diagram", "text": "Wavenumber–frequency diagram\n\nA wavenumber–frequency diagram is a plot displaying the relationship between the wavenumber (spatial frequency) and the frequency (temporal frequency) of certain phenomena. Usually frequencies are placed on the vertical axis, while wavenumbers are placed on the horizontal axis.\n\nIn the atmospheric sciences, these plots are a common way to visualize atmospheric waves.\n\nIn the geosciences, especially seismic data analysis, these plots also called \"f\"–\"k\" plot, in which energy density within a given time interval is contoured on a frequency-versus-wavenumber basis. They are used to examine the direction and apparent velocity of seismic waves and in velocity filter design.\n\nIn general, the relationship between wavelength formula_1, frequency formula_2, and the phase velocity formula_3 of a sinusoidal wave is:\n\nUsing the wavenumber (formula_5) and angular frequency (formula_6) notation, the previous equation can be rewritten as\n\nOn the other hand, the group velocity is equal to the slope of the wavenumber–frequency diagram:\n\nAnalyzing such relationships in detail often yields information on the physical properties of the medium, such as density, composition, etc.\n\n"}
{"id": "273831", "url": "https://en.wikipedia.org/wiki?curid=273831", "title": "Weber–Fechner law", "text": "Weber–Fechner law\n\nThe Weber–Fechner law refers to two related laws in the field of psychophysics, known as Weber's law and Fechner's law. Both laws relate to human perception, more specifically the relation between the \"actual\" change in a physical stimulus and the \"perceived\" change. This includes stimuli to all senses: vision, hearing, taste, touch, and smell.\n\nBoth Weber's law and Fechner's law were formulated by Gustav Theodor Fechner (1801–1887). They were first published in 1860 in the work \"Elemente der Psychophysik\" (Elements of psychophysics). This publication was the first work ever in this field, and where Fechner coined the term \"psychophysics\" to describe the interdisciplinary study of how humans perceive physical magnitudes.\n\nErnst Heinrich Weber (1795–1878) was one of the first people to approach the study of the human response to a physical stimulus in a quantitative fashion. Fechner was a student of Weber and named his first law in honor of his mentor, since it was Weber who had conducted the experiments needed to formulate the law.\n\nFechner formulated several versions of the law, all stating the same thing. One formulation states:\n\n\"Simple differential sensitivity is inversely proportional to the size of the components of the difference; relative differential sensitivity remains the same regardless of size.\"\n\nWhat this means is that the perceived change in stimuli is proportional to the initial stimuli.\n\nWeber's law also incorporates the Just Noticeable Difference (JND). This is the smallest change in stimuli that can be perceived. As stated above, the JND is proportional to the initial stimuli. Fechner found that the JND is constant for any sense.\n\nAlthough Weber's law includes a statement of the proportionality of a perceived change to initial stimuli, Weber only refers to this as a rule of thumb regarding human perception. It was Fechner who formulated this statement as a mathematical expression referred to as Weber contrast. \n\nWeber contrast is not part of Weber's law.\n\nFechner noticed in his own studies that different individuals have different sensitivity to certain stimuli. For example, the ability to perceive differences in light intensity could be related to how good that individual's vision is. He also noted that the human sensitivity to stimuli changes depends on which sense is affected. He used this to formulate another version of Weber's law that he named \"the Massformel\", the \"measurement formula\". Fechner's law states that the subjective sensation is proportional to the logarithm of the stimulus intensity. According to this law, human perceptions of sight and sound work as follows: Perceived loudness/brightness is proportional to logarithm of the actual intensity measured with an accurate nonhuman instrument.\n\nThe relationship between stimulus and perception is logarithmic. This logarithmic relationship means that if a stimulus varies as a geometric progression (i.e., multiplied by a fixed factor), the corresponding perception is altered in an arithmetic progression (i.e., in additive constant amounts). For example, if a stimulus is tripled in strength (i.e., 3 x 1), the corresponding perception may be two times as strong as its original value (i.e., 1 + 1). If the stimulus is again tripled in strength (i.e., 3 x 3 x 1), the corresponding perception will be three times as strong as its original value (i.e., 1 + 1 + 1). Hence, for multiplications in stimulus strength, the strength of perception only adds. The mathematical derivations of the torques on a simple beam balance produce a description that is strictly compatible with Weber's law.\n\nFechner's law is a mathematical derivation of Weber's law.\n\nIntegrating the mathematical expression for Weber's law gives:\n\nwhere formula_6 is the constant of integration and \"ln\" is the natural logarithm.\n\nTo solve for formula_6, assume that the perceived stimuli becomes zero at some threshold stimuli formula_8. Using this as a constraint, set formula_9 and formula_10. This gives:\n\nSubstituting formula_6 in the integrated expression for Weber's law, the expression can be written as:\n\nThe constant k is sense-specific and must be determined depending on the sense and type of stimuli.\n\nWeber and Fechner conducted research on differences in light intensity and the perceived difference in weight. Other sense modalities provide only mixed support for either Weber's law or Fechner's law.\n\nWeber found that the just noticeable difference (JND) between two weights was approximately proportional to the weights. Thus, if the weight of 105 g can (only just) be distinguished from that of 100 g, the JND (or differential threshold) is 5 g. If the mass is doubled, the differential threshold also doubles to 10 g, so that 210 g can be distinguished from 200 g. In this example, a weight (any weight) seems to have to increase by 5% for someone to be able to reliably detect the increase, and this minimum required fractional increase (of 5/100 of the original weight) is referred to as the \"Weber fraction\" for detecting changes in weight. Other discrimination tasks, such as detecting changes in brightness, or in tone height (pure tone frequency), or in the length of a line shown on a screen, may have different Weber fractions, but they all obey Weber's law in that observed values need to change by at least some small but constant proportion of the current value to ensure human observers will reliably be able to detect that change.\n\nFechner did not conduct any experiments on how perceived heaviness increased with the mass of the stimulus. Instead, he assumed that all JNDs are subjectively equal, and argued mathematically that this would produce a logarithmic relation between the stimulus intensity and the sensation. These assumptions have both been questioned. \nFollowing the work of S. S. Stevens, many researchers came to believe in the 1960s that the power law was a more general psychophysical principle than Fechner's logarithmic law. But in 1963 Donald Mackay showed and in 1978 John Staddon demonstrated with Stevens' own data, that the power law is the result of logarithmic input and output processes.\n\nWeber's law does not quite hold for loudness. It is a fair approximation for higher intensities, but not for lower amplitudes.\n\nWeber's law does not hold at perception of higher intensities. Intensity discrimination improves at higher intensities. The first demonstration of the phenomena was presented by Riesz in 1928, in Physical Review. This deviation of the Weber's law is known as the \"near miss\" of the Weber's law. This term was coined by McGill and Goldberg in their paper of 1968 in Perception & Psychophysics. Their study consisted of intensity discrimination in pure tones. Further studies have shown that the near miss is observed in noise stimuli as well. Jesteadt et al. (1977) demonstrated that the near miss holds across all the frequencies, and that the intensity discrimination is not a function of frequency, and that the change in discrimination with level can be represented by a single function across all frequencies.\n\nThe eye senses brightness approximately logarithmically over a moderate range (but more like a power law over a wider range), and stellar magnitude is measured on a logarithmic scale.\nThis magnitude scale was invented by the ancient Greek astronomer Hipparchus in about 150 B.C. He ranked the stars he could see in terms of their brightness, with 1 representing the brightest down to 6 representing the faintest, though now the scale has been extended beyond these limits; an increase in 5 magnitudes corresponds to a decrease in brightness by a factor of 100.\nModern researchers have attempted to incorporate such perceptual effects into mathematical models of vision.\n\nPerception of Glass patterns and mirror symmetries in the presence of noise follows Weber's law in the middle range of regularity-to-noise ratios (\"S\"), but in both outer ranges, sensitivity to variations is disproportionally lower. As Maloney, Mitchison, & Barlow (1987) showed for Glass patterns, and as van der Helm (2010) showed for mirror symmetries, perception of these visual regularities in the whole range of regularity-to-noise ratios follows the law \"p\" = \"g\"/(2+1/\"S\") with parameter \"g\" to be estimated using experimental data.\n\nActivation of neurons by sensory stimuli in many parts of the brain is by a proportional law: neurons change their spike rate by about 10–30%, when a stimulus (e.g. a natural scene for vision) has been applied. However, as Scheler (2017) showed,\nthe population distribution of the intrinsic excitability or gain of a neuron is a heavy tail distribution, more precisely a lognormal shape, which is equivalent to a logarithmic coding scheme. Neurons may therefore spike with 5–10 fold different mean rates. Obviously, this increases the dynamic range of a neuronal population, while stimulus-derived changes remain small and linear proportional.\n\nThe Weber–Fechner law has been applied in other fields of research than just the human senses.\n\nPsychological studies show that it becomes increasingly difficult to discriminate between two numbers as the difference between them decreases. This is called the \"distance effect\". This is important in areas of magnitude estimation, such as dealing with large scales and estimating distances. It may also play a role in explaining why consumers neglect to shop around to save a small percentage on a large purchase, but will shop around to save a large percentage on a small purchase which represents a much smaller absolute dollar amount.\n\nIt has been hypothesized that dose–response relationships can follow Weber's Law which suggests this law – which is often applied at the sensory level – originates from underlying chemoreceptor responses to cellular signaling dose relationships within the body. Dose response can be related to the Hill equation, which is closer to a power law.\n\nThere is a new branch of the literature on public finance hypothesizing that the Weber–Fechner law can explain the increasing levels of public expenditures in mature democracies. Election after election, voters demand more public goods to be effectively impressed; therefore, politicians try to increase the magnitude of this \"signal\" of competence – the size and composition of public expenditures – in order to collect more votes.\n\n\n\n"}
{"id": "957160", "url": "https://en.wikipedia.org/wiki?curid=957160", "title": "Will to power", "text": "Will to power\n\nThe will to power () is a prominent concept in the philosophy of Friedrich Nietzsche. The will to power describes what Nietzsche may have believed to be the main driving force in humans – achievement, ambition, and the striving to reach the highest possible position in life. These are all manifestations of the will to power; however, the concept was never systematically defined in Nietzsche's work, leaving its interpretation open to debate.\n\nAlfred Adler incorporated the will to power into his individual psychology. This can be contrasted to the other Viennese schools of psychotherapy: Sigmund Freud's pleasure principle (will to pleasure) and Viktor Frankl's logotherapy (will to meaning). Each of these schools advocates and teaches a very different essential driving force in human beings.\n\nSome of the misconceptions of the will to power, including Nazi appropriation of Nietzsche's philosophy, arise from overlooking Nietzsche's distinction between \"Kraft\" (force) and \"Macht\" (power). \"Kraft\" is primordial strength that may be exercised by anything possessing it, while \"Macht\" is, within Nietzsche's philosophy, closely tied to sublimation and \"self-overcoming\", the conscious channeling of \"Kraft\" for creative purposes.\n\nNietzsche's early thinking was influenced by that of Arthur Schopenhauer, whom he first discovered in 1865. Schopenhauer puts a central emphasis on will and in particular has a concept of the \"will to live\". Writing a generation before Nietzsche, he explained that the universe and everything in it is driven by a primordial will to live, which results in a desire in all living creatures to avoid death and to procreate. For Schopenhauer, this will is the most fundamental aspect of reality – more fundamental even than being.\n\nAnother important influence was Roger Joseph Boscovich, whom Nietzsche discovered and learned about through his reading, in 1866, of Friedrich Albert Lange's 1865 \"Geschichte des Materialismus\" (\"History of Materialism\"). As early as 1872, Nietzsche went on to study Boscovich’s book \"Theoria Philosophia Naturalis\" for himself. Nietzsche makes his only reference in his published works to Boscovich in \"Beyond Good and Evil\", where he declares war on \"soul-atomism\". Boscovich had rejected the idea of \"materialistic atomism\", which Nietzsche calls \"one of the best refuted theories there is\". The idea of centers of force would become central to Nietzsche's later theories of \"will to power\".\n\nAs the 1880s began, Nietzsche began to speak of the \"Desire for Power\" (\"Machtgelüst\"); this appeared in \"The Wanderer and his Shadow\" (1880) and \"Daybreak\" (1881). \"Machtgelüst\", in these works, is the pleasure of the feeling of power and the hunger to overpower.\n\nWilhelm Roux published his \"The Struggle of Parts in the Organism\" (\"Der Kampf der Teile im Organismus\") in 1881, and Nietzsche first read it that year. The book was a response to Darwinian theory, proposing an alternative mode of evolution. Roux was a disciple of and influenced by Ernst Haeckel who believed the struggle for existence occurred at the cellular level. The various cells and tissues struggle for finite resources, so that only the strongest survive. Through this mechanism, the body grows stronger and better adapted. Lacking modern genetic theory, Roux's model assumed a Lamarckian or pangenetic model of inheritance.\n\nNietzsche began to expand on the concept of \"Machtgelüst\" in \"The Gay Science\" (1882), where in a section titled \"On the doctrine of the feeling of power\", he connects the desire for cruelty with the pleasure in the feeling of power. Elsewhere in \"The Gay Science\" he notes that it is only \"in intellectual beings that pleasure, displeasure, and will are to be found\", excluding the vast majority of organisms from the desire for power.\n\nLéon Dumont (1837–77), whose 1875 book \"Théorie Scientifique de La Sensibilité, le Plaisir et la Peine\" Nietzsche read in 1883, seems to have exerted some influence on this concept. Dumont believed that pleasure is related to increases in force. In \"The Wanderer\" and \"Daybreak\", Nietzsche had speculated that pleasures such as cruelty are pleasurable because of exercise of power. But Dumont provided a physiological basis for Nietzsche’s speculation. Dumont’s theory also would have seemed to confirm Nietzsche’s claim that pleasure and pain are reserved for intellectual beings, since, according to Dumont, pain and pleasure require a coming to consciousness and not just a sensing.\n\nIn 1883 Nietzsche coined the phrase \"Wille zur Macht\" in \"Thus Spoke Zarathustra\". The concept, at this point, was no longer limited to only those intellectual beings that can actually experience the feeling of power; it now applied to all life. The phrase \"Wille zur Macht\" first appears in part 1, \"1001 Goals\" (1883), then in part 2, in two sections, \"Self-Overcoming\" and \"Redemption\" (later in 1883). \"Self-Overcoming\" describes it in most detail, saying it is an \"unexhausted procreative will of life\". There is will to power where there is life and even the strongest living things will risk their lives for more power. This suggests that the will to power is stronger than the will to survive.\n\nSchopenhauer's \"Will to life\" thus became a subsidiary to the will to power, which is the stronger will. Nietzsche thinks his notion of the will to power is far more useful than Schopenhauer's will to live for explaining various events, especially human behavior—for example, Nietzsche uses the will to power to explain both ascetic, life-denying impulses and strong, life-affirming impulses in the European tradition, as well as both master and slave morality. He also finds the will to power to offer much richer explanations than utilitarianism's notion that all people really want to be happy, or the Platonist's notion that people want to be unified with the Good.\n\nNietzsche read William Rolph’s \"Biologische Probleme\" around mid-1884, and it clearly interested him, for his copy is heavily annotated. He made many notes concerning Rolph. Rolph was another evolutionary anti-Darwinist like Roux, who wished to argue for evolution by a different mechanism than the struggle for existence. Rolph argued that all life seeks primarily to expand itself. Organisms fulfill this need through assimilation, trying to make as much of what is found around them into part of themselves, for example by seeking to increase intake and nutriment. Life forms are naturally insatiable in this way.\n\nNietzsche's next published work was \"Beyond Good and Evil\" (1886), where the influence of Rolph seems apparent. Nietzsche writes,\n\n\"Beyond Good and Evil\" has the most references to \"will to power\" in his published works, appearing in 11 aphorisms. The influence of Rolph and its connection to \"will to power\" also continues in book 5 of \"Gay Science\" (1887) where Nietzsche describes \"will to power\" as the instinct for \"expansion of power\" fundamental to all life.\n\nKarl Wilhelm von Nägeli's 1884 book \"Mechanisch-physiologische Theorie der Abstammungslehre\", which Nietzsche acquired around 1886 and subsequently read closely, also had considerable influence on his theory of will to power. Nietzsche wrote a letter to Franz Overbeck about it, noting that it has \"been sheepishly put aside by Darwinists\". Nägeli believed in a \"perfection principle\", which led to greater complexity. He called the seat of heritability the idioplasma, and argued, with a military metaphor, that a more complex, complicatedly ordered idioplasma would usually defeat a simpler rival. In other words, he is also arguing for internal evolution, similar to Roux, except emphasizing complexity as the main factor instead of strength.\n\nThus, Dumont’s pleasure in the expansion of power, Roux’s internal struggle, Nägeli’s drive towards complexity, and Rolph’s principle of insatiability and assimilation are fused together into the biological side of Nietzsche’s theory of will to power, which is developed in a number of places in his published writings. Having derived the \"will to power\" from three anti-Darwin evolutionists, as well as Dumont, it seems appropriate that he should use his \"will to power\" as an anti-Darwinian explanation of evolution. He expresses a number of times the idea that adaptation and the struggle to survive is a secondary drive in the evolution of animals, behind the desire to expand one’s power – the \"will to power\".\n\nNonetheless, in his notebooks he continues to expand the theory of the will to power. Influenced by his earlier readings of Boscovich, he began to develop a physics of the will to power. The idea of matter as centers of force is translated into matter as centers of will to power. Nietzsche wanted to slough off the theory of matter, which he viewed as a relic of the metaphysics of substance.\n\nThese ideas of an all inclusive physics or metaphysics built upon the will to power do not appear to arise anywhere in his published works or in any of the final books published posthumously, except in the above-mentioned aphorism from \"Beyond Good & Evil\", where he references Boscovich (section 12). It does recur in his notebooks, but not all scholars treat these ideas as part of his thought.\n\nThroughout the 1880s, in his notebooks, Nietzsche developed a theory of the \"eternal recurrence of the same\" and much speculation on the physical possibility of this idea and the mechanics of its actualization occur in his later notebooks. Here, the will to power as a potential physics is integrated with the postulated eternal recurrence. Taken literally as a theory for how things are, Nietzsche appears to imagine a physical universe of perpetual struggle and force that repeatedly completes its cycle and returns to the beginning.\n\nSome scholars believe that Nietzsche used the concept of eternal recurrence metaphorically. But others, such as Paul Loeb, have argued that \"Nietzsche did indeed believe in the truth of cosmological eternal recurrence.\" By either interpretation the acceptance of eternal recurrence raises the question of whether it could justify a trans-valuation of one's life, and be a necessary precursor to the overman in her/his perfect acceptance of all that is, for the love of life itself and amor fati.\n\nIn contemporary Nietzschean scholarship, some interpreters have emphasized the will to power as a psychological principle because Nietzsche applies it most frequently to human behavior. However, in Nietzsche's unpublished notes (later published by his sister as \"The Will to Power\"), Nietzsche sometimes seemed to view the will to power as a more (metaphysical) general force underlying \"all\" reality, not just human behavior—thus making it more directly analogous to Schopenhauer's will to live. For example, Nietzsche claims the \"world is the will to power—and nothing besides!\". Nevertheless, in relation to the entire body of Nietzsche's published works, many scholars have insisted that Nietzsche's principle of the will to power is less metaphysical and more pragmatic than Schopenhauer's will to live: while Schopenhauer thought the will to live was what was most real in the universe, Nietzsche can be understood as claiming only that the will to power is a particularly useful principle for his purposes.\n\nSome interpreters also upheld a biological interpretation of the \"Wille zur Macht\", making it equivalent with some kind of social Darwinism. For example, the concept was appropriated by some Nazis such as Alfred Bäumler, who may have drawn influence from it or used it to justify their expansive quest for power.\n\nThis reading was criticized by Martin Heidegger in his 1930s courses on Nietzsche—suggesting that raw physical or political power was not what Nietzsche had in mind. This is reflected in the following passage from Nietzsche's notebooks:\n\nOpposed to a biological and voluntary conception of the \"Wille zur Macht\", Heidegger also argued that the will to power must be considered in relation to the \"Übermensch\" and the \"thought of eternal recurrence\"—although this reading itself has been criticized by Mazzino Montinari as a \"macroscopic Nietzsche\". Gilles Deleuze also emphasized the connection between the will to power and eternal return. Both Jacques Derrida and Gilles Deleuze were careful to point out that the primary nature of will to power is unconscious. This means that the drive to power is always already at work unconsciously, perpetually advancing the will of the one over the other. This thus creates the state of things in the observable or conscious world still operating through the same tension. Derrida is careful not to confine the will to power to human behavior, the mind, metaphysics, nor physical reality individually. It is the underlying life principle inaugurating all aspects of life and behavior, a self-preserving force. A sense of entropy and the eternal return, which are related, is always indissociable from the will to power. The eternal return of all memory initiated by the will to power is an entropic force again inherent to all life.\n\nOpposed to this interpretation, the \"will to power\" can be understood (or misunderstood) to mean a struggle against one's surroundings that culminates in personal growth, self-overcoming, and self-perfection, and assert that the power held over others as a result of this is coincidental. Thus Nietzsche wrote:\n\nIt would be possible to claim that rather than an attempt to 'dominate over others', the \"will to power\" is better understood as the tenuous equilibrium in a system of forces' relations to each other. While a rock, for instance, does not have a conscious (or unconscious) \"will\", it nevertheless acts as a site of resistance within the \"will to power\" dynamic. Moreover, rather than 'dominating over others', \"will to power\" is more accurately positioned in relation to the subject (a mere synecdoche, both fictitious and necessary, for there is \"no doer behind the deed,\" (see \"On the Genealogy of Morals\")) and is an idea behind the statement that words are \"seductions\" within the process of self-mastery and self-overcoming. The \"will to power\" is thus a \"cosmic\" inner force acting in and through both animate and inanimate objects. Not just instincts but also higher level behaviors (even in humans) were to be reduced to the \"will to power\". This includes both such apparently harmful acts as physical violence, lying, and domination, on one hand, and such apparently non-harmful acts as gift-giving, love, and praise on the other—though its manifestations can be altered significantly, such as through art and aesthetic experience. In \"Beyond Good and Evil\", he claims that philosophers' \"will to truth\" (i.e., their apparent desire to dispassionately seek objective, absolute truth) is actually nothing more than a manifestation of their will to power; this will can be life-affirming or a manifestation of nihilism, but it is the will to power all the same.\n\nOther Nietzschean interpreters dispute the suggestion that Nietzsche's concept of the will to power is merely and only a matter of narrow, harmless, humanistic self-perfection. They suggest that, for Nietzsche, power means self-perfection \"as well as\" outward, political, elitist, aristocratic domination. Nietzsche, in fact, explicitly and specifically defined the egalitarian state-idea as the embodiment of the will to power in decline:\n\nAlfred Adler borrowed heavily from Nietzsche's work to develop his second Viennese school of psychotherapy called individual psychology. Adler (1912) wrote in his important book \"Über den nervösen Charakter (The Neurotic Constitution)\":\n\nAdler's adaptation of the will to power was and still is in contrast to Sigmund Freud's pleasure principle or the \"will to pleasure\", and to Viktor Frankl's logotherapy or the \"will to meaning\". Adler's intent was to build a movement that would rival, even supplant, others in psychology by arguing for the holistic integrity of psychological well-being with that of social equality. His interpretation of Nietzsche's will to power was concerned with the individual patient's overcoming of the superiority-inferiority dynamic.\n\nIn \"Man's Search for Meaning\", Frankl compared his third Viennese school of psychotherapy with Adler's psychoanalytic interpretation of the will to power:\n\nThe 1999 4x strategy game Sid Meier's Alpha Centauri refers to the will to power by naming one of its available technologies by that name. A quote from \"Thus Spoke Zarathustra\" is given when the technology is discovered by the player.\n\nThe character of 'The Jackal' in the 2008 Ubisoft game Far Cry 2 quotes from Beyond Good and Evil and the Will to Power. \n\nThe 2016 4x strategy game Stellaris also includes a technology with this name.\n\nBob Rosenberg, founder of freestyle music group Will to Power chose the name for the group as an homage to German philosopher Friedrich Nietzsche's theory of an individual's fundamental \"will to power\".\n\nThe first title in the \"Xenosaga\" trilogy is \"Xenosaga Episode I: Der Wille zur Macht\".\n\nOn September 8th, 2017, melodic death metal band Arch Enemy released an album entitled \"Will to Power\". \n\nThe book makes an appearance in the 1933 Barbara Stanwyck movie \"Baby Face\".\n\nIn \"Smallville\" (Season 1, Episode 17), the Lex Luthor character reveals that his father gave him a copy of the book for his tenth birthday.\n\n\n"}
{"id": "2113849", "url": "https://en.wikipedia.org/wiki?curid=2113849", "title": "Wilson cycle", "text": "Wilson cycle\n\nThe Wilson cycle is a model where a continent rifts, forms an ocean basin in-between, and then begins a process of convergence that leads to the collision of the two plates and closure of the ocean. The model is named after its originator John Tuzo Wilson. It has been suggested that Wilson cycles on Earth started about 3 Ga (3 billion years) ago in the Archean Eon of Earth's history.\n\nA Wilson cycle is not the same as a supercontinent cycle, which is the break-up of one supercontinent and the development of another and takes place on a global scale. The Wilson cycle rarely synchronizes with the timing of a supercontinent cycle. However, supercontinent cycles and Wilson cycles were both involved in the creation of Pangaea and Rodinia.\n"}
