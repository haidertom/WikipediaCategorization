{"id": "11114793", "url": "https://en.wikipedia.org/wiki?curid=11114793", "title": "AN/ALE-55 Fiber-Optic Towed Decoy", "text": "AN/ALE-55 Fiber-Optic Towed Decoy\n\nThe AN/ALE-55 Fiber-Optic Towed Decoy, or ALE-55, is an RF (Radio Frequency) countermeasure under development by BAE Systems Electronic Solutions for the F/A-18E/F Super Hornet.\n\nThe ALE-55 is an RF countermeasure designed to protect an aircraft from radar guided missiles. It consists of an aircraft-towed decoy and onboard electronics. It works together with the aircraft's electronic warfare system to provide radar jamming. In addition, it can also be used in a backup mode as a signal repeater, which allows it to lure incoming missiles away from their actual target. It is currently in use with the F/A-18E/F Super hornet, but can be adapted to a wide variety of platforms with minimal modification.\n\nThe ALE-55 provides three layers of defensive jamming against a radar-based threat: preventing radars from tracking, breaking radar locks, and acting as a target for incoming missiles.\n\nThe system detects a threat radar in its acquisition mode and tries to prevent it from locking by using jamming techniques. The onboard electronic warfare package analyzes the threat, while the towed decoy emits the jamming signals to confuse the tracking radar.\n\nIn the case that a radar obtains a lock on the aircraft or decoy system, the electronics on board the aircraft analyze the emissions and then determine the optimum jamming technique to break the radar lock. The jamming is done by the decoy. The ALE-55 also possesses the useful ability to send out multiple jamming frequencies if more than one radar is locked on to the decoy or aircraft.\n\nWhen a missile launch is detected, indicated by the difference in radar signal and type, the ALE-55 runs a last resort attempt to protect the aircraft towing it. This last resort is becoming the target, rather than the aircraft, by trying to jam the missile or simulating the aircraft's radar signature.\n\nThe ALE-55 consists of two components. An onboard signal conditioning assembly and fiber optic towed decoys.\n\nThe onboard electric frequency converter analyzes radar signals detected by the plane's electronic warfare system and calculates an appropriate jamming and spoofing signal, which is then transmitted to the FOTD through a fiber optic cable.\n\nThe towed decoy has dual high power traveling-wave tubes (TWTs) to allow for enough power to protect large aircraft. It is launched with the Raytheon Integrated Multi-Platform Launch Controller (IMPLC), which it shares with the towed ALE-50. An active braking system allows for fast deployment.\n\n\n\n\nAN/ALE-50\n\n"}
{"id": "45339245", "url": "https://en.wikipedia.org/wiki?curid=45339245", "title": "Alice Perry", "text": "Alice Perry\n\nAlice Jacqueline Perry (24 October 1885 – 21 April 1969) was a poet and the first woman in Ireland to graduate with a degree in engineering.\n\nBorn in Wellpark, Galway in 1885, Alice was one of five daughters of James and Martha Perry (née Park). Her father was the County Surveyor in Galway West and co-founded the Galway Electric Light Company. Her uncle, John Perry, was a Fellow of the Royal Society and invented the navigational gyroscope.\n\nAfter graduating from the High School in Galway, she won a scholarship to study in Royal University, Galway in 1902. Having excelled in mathematics, she changed from studying for a degree in arts to an engineering degree. She graduated with first class honours in 1906. The family appear to have been academically gifted. Her sisters Molly and Nettie also went on to third level education; a third sister Agnes earned BA (1903) and MA (1905) in mathematics from Queen's College Galway (later UCG then NUIG), taught there in 1903–1904, was a Royal University of Ireland examiner in mathematics in 1906, and later became assistant headmistress at a secondary school in London.\n\nFollowing her graduation she was offered a senior postgraduate scholarship but owing to her father's death the following month, she did not take up this position. In December 1906 she succeeded her father temporarily as county surveyor for Galway County Council. She remained in this position for five or six months until a permanent appointment was made. She was an unsuccessful candidate for the permanent position and for a similar opportunity to be a surveyor in Galway East. She remains the only woman to have been a County Surveyor (County Engineer) in Ireland.\n\nIn 1908 she moved to London with her sisters, where she worked as a Lady Factory Inspector for the Home Office. From there she moved to Glasgow, at which point she converted from Presbyterianism to Christian Science in 1915. She met and married John (Bob) Shaw on 30 September 1916. Shaw was a soldier who died in 1917 on the Western Front.\n\nPerry retired from her inspector's position in 1921 and became interested in poetry, first publishing in 1922. In 1923 she moved to Boston, the headquarters of Christian Science. Until her death in 1969, Perry worked within the Christian Science movement as a poetry editor and practitioner, publishing seven books of poetry.\n\nAn All-Ireland medal has been named in her honour, The Alice Perry Medal, with the first prizes awarded in 2014.\n\nOn Monday 6 March 2017, NUI Galway held an official ceremony to mark the naming of the Alice Perry Engineering Building.\n\n\n\n\n"}
{"id": "51421991", "url": "https://en.wikipedia.org/wiki?curid=51421991", "title": "Alison Van Eenennaam", "text": "Alison Van Eenennaam\n\nAlison L. Van Eenennaam is a Cooperative Extension Specialist in the Department of Animal Science at the University of California, Davis and runs the Animal Genomics and Biotechnology Laboratory. She has served on national committees such as the USDA National Advisory Committee on Biotechnology in the 21st Century (AC21) and was awarded the 2014 Borlaug CAST Communication Award. Van Eenennaam writes the Biobeef Blog.\n\nVan Eenennaam began her university career at the University of Melbourne, Victoria, Australia in 1987, receiving a BS (Honors) degree in Agricultural Science. She received a Master of Science degree in Animal Science in 1990, and a Ph.D in Genetics in 1997, both from University of California, Davis.\n\nVan Eenennaam began her work in animal science as an intern at Genetic Resources Inc.'s Bovine Reproduction Facility in San Marcos, Texas in 1984. From 1991 to 1993 she worked as a livestock and dairy farm advisor for the UC Cooperative Extension in the San Joaquin and Sacramento Counties of California. From 1998 to 2002, following the completion of her Ph.D degree, Van Eenennaam worked for Calgene (purchased by Monsanto Corporation in 1997) in Davis, California, first as a research scientist, and then as a project leader. Since 2002, Van Eenennaam has been a Cooperative Extension Specialist in the field of Animal Genomics and Biotechnology in the Department of Animal Science at University of California, Davis.\n\nShe has served on several national committees including the USDA National Advisory Committee on Biotechnology and 21st Century Agriculture, (2005–2009), and was a temporary voting member of the 2010 FDA Veterinary Medicine Advisory Committee meeting on the AquAdvantage salmon, the first genetically engineered animal to be evaluated for entry into the food supply.\n\nThe mission of Van Eenennaam's animal biotechnology lab is to \"provide research and education on the use of animal genomics and biotechnology in livestock production systems\", with a focus on beef cattle production.\n\nVan Eenennaam's biotechnology lab at UC Davis is working on a collaborative project focused on the production of hornless dairy cattle through gene editing on a USDA National Institute of Food and Agriculture grant. This project involves using a gene sequence from Angus cattle in the genome of dairy cattle to prevent horns from growing. Van Eenennaam stated that the use of genetics rather than chemicals to solve problems can address some of the animal welfare concerns and environmental impacts of animal production. In October, 2016, this project was featured on Science Friday. Funding sources for this research and extension program are found on Van Eenennaam's public website.\n\nVan Eenennaam was appointed to the Food and Drug Administration Veterinary Medicine Advisory Committee evaluating the AquAdvantage salmon, a genetically engineered fish. A paper, authored in 2006 by Van Eenennaam with Paul Olin of University of California Cooperative Extension Sea Grant, discussed transgenic fish. The paper cites a number of benefits of genetically engineered fish, including a larger number of eggs laid per female, a low probability of carrying human pathogens, strong markets for aquaculture, and increased feed-conversion efficiency. This paper also describes the risk factor that these fish could escape breeding locations and mix with wild fish populations. \n\nIn 2014 Van Eenennaam co-authored a review article on the use of genetically modified feed for cattle. The data represented more than 100 billion animals in 29 studies and found \"GMO feed is safe and nutritionally equivalent to non-GMO feed\".\n\nVan Eenennaam won two awards from American Society of Animal Science. One was for the 2013 video \"Gene Shop\", a five-minute parody of Macklemore’s “Thrift Shop” in which Dr. Van Eenennaam and UC Davis students engagingly emphasize the importance of funding for agricultural research. The second award was for the 2012 video \"Were Those the Days My friend?\", a take on a ballad from the 1960s that highlights the importance of genetic advances for improved production efficiency and food security. This competition was designed to further the \"goal of sharing the importance of animal science with the public\".\n\nAdditional YouTube videos on biotechnology topics are linked to the BioBeef Blog written by Van Eenennaam in order \"to try to interject scientific nuance into these controversial and often politicized scientific topics\". Van Eenennaam participated in the 2014 Intelligence Squared debate on the topic of genetically modify food.\n\nIn 2014, Van Eenennaam was awarded the Borlaug CAST Communication Award by the Council for Agricultural Science and Technology (CAST), named after agricultural biologist and 1970 Nobel Peace Prize winner Norman Borlaug. CAST indicated that Van Eenennaam is known for her communication skills and praised for her understanding of biotechnology, her enthusiasm for agricultural education, and her abilities to use novel ideas to get important messages to policymakers and the public alike.\n\nVan Eenennaam appeared in the 2016 documentary production, \"Food Evolution\", written and produced by Trace Sheehan and Scott Hamilton Kennedy. The film, narrated by Neil deGrasse Tyson, features scientific experts in the areas of genetics, biology, biotechnology, and nutrition, as well as farmers and activists discussing the problems of feeding a growing global population.\n\n\nVan Eenennaam has authored or co-authored more than 80 academic articles. The following are selected articles in which Van Eenennaam is listed as the lead author.\n"}
{"id": "15581418", "url": "https://en.wikipedia.org/wiki?curid=15581418", "title": "Aviation transponder interrogation modes", "text": "Aviation transponder interrogation modes\n\nThe aviation transponder interrogation modes are the standard formats of pulsed sequences from an interrogating Secondary Surveillance Radar (SSR) or similar Automatic Dependent Surveillance-Broadcast (ADS-B) system. The reply format is usually referred to as a \"code\" from a transponder, which is used to determine detailed information from a suitably equipped aircraft.\n\nIn its simplest form, a \"Mode\" or interrogation type, is generally determined by pulse spacing between two or more interrogation pulses. Various modes exist from Mode 1 to 5 for military use, to Mode A, B, C and D and Mode S for civilian use.\n\nSeveral different RF communication protocols have been standardized for aviation transponders:\n\nWhen the transponder receives an interrogation request, it broadcasts the configured transponder code (or \"squawk code\"). This is referred to as \"Mode 3A\" or more commonly, Mode A. A separate type of response called \"Ident\" can be initiated from the airplane by pressing a button on the transponder control panel.\n\nA Mode A transponder code response can be augmented by a pressure altitude response, which is then referred to as Mode C operation. Pressure altitude is obtained from an altitude encoder, either a separate self-contained unit mounted in the aircraft or an integral part of the transponder. The altitude information is passed to the transponder using a modified form of the modified Gray code called a Gillham code.\n\nMode A and C responses are used to help air traffic controllers identify a particular aircraft's position and altitude on a radar screen, in order to maintain separation.\n\nAnother mode called Mode S (Select) is designed to help avoiding overinterrogation of the transponder (having many radars in busy areas) and to allow automatic collision avoidance. Mode S transponders are compatible with Mode A and Mode C Secondary Surveillance Radar (SSR) systems. This is the type of transponder that is used for TCAS or ACAS II (Airborne Collision Avoidance System) functions, and is required to implement the extended squitter broadcast, one means of participating in ADS-B systems. A TCAS-equipped aircraft must have a Mode S transponder, but not all Mode S transponders include TCAS. Likewise, a Mode S transponder is required to implement 1090ES extended squitter ADS-B Out, but there are other ways to implement ADS-B Out (in the U.S. and China.) The format of Mode S messages is documented in ICAO Doc 9688, \"Manual on Mode S Specific Services\".\n\nUpon interrogation, Mode S transponders transmit information about the aircraft to the SSR system, to TCAS receivers on board aircraft and to the ADS-B SSR system. This information includes the call sign of the aircraft and/or the aircraft's permanent ICAO 24-bit address (which is represented for human interface purposes as six hexadecimal characters. One of the hidden features of Mode S transponders is that they are backwards compatible; an aircraft equipped with a Mode S transponder can still be used to send replies to Mode A or C interrogations. This feature can be activated by a specific type of interrogation sequence called inter-mode. \n\nMode S equipped aircraft are assigned a unique ICAO 24-bit address or (informally) Mode-S \"hex code\" upon national registration and this address becomes a part of the aircraft's Certificate of Registration. Normally, the address is never changed, however, the transponders are reprogrammable and, occasionally, are moved from one aircraft to another (presumably for operational or cost purposes), either by maintenance or by changing the appropriate entry in the aircraft's Flight management system.\n\nThere are 16,777,214 (2-2) unique ICAO 24-bit addresses (hex codes) available. The ICAO 24-bit address can be represented in three digital formats: hexadecimal, octal, and binary. These addresses are used to provide a unique identity normally allocated to an individual aircraft or registration.\n\nAs an example, following is the ICAO 24-bit address assigned to the Shuttle Carrier Aircraft with the registration N905NA:\nThese are all the same 24-bit address of the Shuttle Carrier Aircraft, represented in different numeral systems (see above).\n\nAn issue with Mode S transponders arises, when pilots enter the wrong flight identity code into the Mode S transponder. In this case, the capabilities of ACAS II and Mode S SSR can be degraded.\n\nIn 2009 the ICAO published an \"extended\" form of Mode S with more message formats to use with ADS-B; it was further refined in 2012. Countries implementing ADS-B can require the use of either the extended squitter mode of a suitably-equipped Mode S transponder, or the UAT transponder on 978 MHz.\n"}
{"id": "33933664", "url": "https://en.wikipedia.org/wiki?curid=33933664", "title": "Batoning", "text": "Batoning\n\nBatoning is the technique of cutting or splitting wood by using a baton-sized stick or mallet to repeatedly strike the spine of a sturdy knife, chisel or blade in order to drive it through wood, similar to how a froe is used. The batoning method can be used to make kindling or desired forms such as boards, slats or notches. The practice is most useful for obtaining dry wood from the inside of logs for the purpose of fire making.\n\nTools used in batoning are: a strong, fixed-blade, preferably full tang knife or machete with a thick spine, and a club-sized length of dense or green wood for striking the knife's spine and tip.\n\nThe basic method involves repeatedly striking the spine of the knife to force the middle of the blade into the wood. The tip is then struck, to continue forcing the blade deeper, until a split is achieved.\n\nThis technique is useful for the simple splitting of wood for kindling, to access dry wood within a wet log, and for the production of shingles, slats, or boards. It is also useful for cutting notches, or making clean crosscuts against the grain of the wood. The technique is also especially useful when a chopping tool is not available.\n\nCare must be taken to avoid damage to the knife. Breakage of the blade is a common result of striking the spine of the knife at an angle. If this happens the broken blade can become irretrievably embedded within the split. In a survival situation, this can be catastrophic.\n\n"}
{"id": "4243241", "url": "https://en.wikipedia.org/wiki?curid=4243241", "title": "Cellular architecture", "text": "Cellular architecture\n\nA cellular architecture is a type of computer architecture prominent in parallel computing. Cellular architectures are relatively new, with IBM's Cell microprocessor being the first one to reach the market. Cellular architecture takes multi-core architecture design to its logical conclusion, by giving the programmer the ability to run large numbers of concurrent threads within a single processor. Each 'cell' is a compute node containing thread units, memory, and communication. Speed-up is achieved by exploiting thread-level parallelism inherent in many applications.\n\nCell, a cellular architecture containing 9 cores, is the processor used in the PlayStation 3. Another prominent cellular architecture is Cyclops64, a massively parallel architecture currently under development by IBM. \n\nCellular architectures follow the low-level programming paradigm, which exposes the programmer to much of the underlying hardware. This allows the programmer to greatly optimize their code for the platform, but at the same time makes it more difficult to develop software. \n"}
{"id": "51623139", "url": "https://en.wikipedia.org/wiki?curid=51623139", "title": "Clinatec", "text": "Clinatec\n\nClinatec is a biomedical research center based at the Polygone Scientifique in Grenoble. Doctors, biologists and experts work side-by-side at the 6,000 m² facility. Around a hundred researchers and employees work at the center. When it opened at the end of 2011, it was hailed as the first center of its kind in the world. With six hospital rooms, cutting-edge medical imaging equipment and an operating suite, Clinatec was developed by the Research Division of the CEA (French Alternative Energies and Atomic Energy Commission), Grenoble-Alpes University Hospital (CHU), Inserm and the Université Grenoble Alpes. The primary focus is on cancer, neurodegenerative diseases and disability.\n\nProfessor Alim Louis Benabid and Jean Therme first met back in 2006.\n\nAlim Louis Benabid is a neurosurgeon. Together with Professor Pierre Pollak, he developed a new treatment for Parkinson's disease, deep brain stimulation. His work received recognition in 2014 with the Lasker Award and in 2016 with the European Inventor Award.\n\nJean Therme is Director of Technological Research at CEA Grenoble. He has worked tirelessly to make Grenoble a key center of expertise in electronics and , and to encourage close collaboration between the worlds of research and industry.\n\nBoth men were convinced that millions of lives could be transformed by merging medical research and technology R&D. They agreed that the way to achieve this would be to bring together doctors, researchers, biologists, engineers, robotics engineers, mathematicians and knowledge engineers to work at a single site.\n\nIn 2010–11, the 6,000 m² building was built and equipped.\n\nIn 2013, the first patient was admitted to the center, in relation to the \"Protool\" clinical trial coordinated by Professor Berger. For the first time, it was possible to use non-invasive procedures to explore regions of the brain that had hitherto been inaccessible. The exciting prospects opened up by this technology, the result of the coupling of technology developed at with the clinical approach taken by researchers at Grenoble-Alpes University Hospital (CHU), Inserm and the Université Grenoble Alpes, led to setting up the MedPrint start-up, which won an I-lab award in 2015.\n\nIn 2014, Clinatec launched a campaign to raise 30 million euros in order to develop a number of projects, including: \n\nClinatec is the outcome of a solid partnership between the CEA (French Alternative Energies and Atomic Energy Commission), Grenoble-Alpes University Hospital (CHU), Inserm and the Université Grenoble Alpes.\n\nThe Clinatec endowment fund was set up in 2014. A sponsorship campaign has been launched, aiming to raise 30 million euros by 2018.\n\nProfessor Alim Louis Benabid, Clinatec's founder and Chairman of the Board, member of the French Academy of Sciences, is one of the joint winners of the prestigious Albert-Lasker Award for clinical research awarded by the Albert and Mary Lasker Foundation in New York. Professors Alim-Louis Benabid and Mahlon R. Delong of Emory University received awards for their contributions to the development of deep brain stimulation. The technique consists in stimulating the subthalamic nucleus to inhibit tremors and restore motor function in patients suffering from Parkinson's disease and from complications caused by Levodopa, a drug widely used in treating the disease.\n\nSet up in 2013 by Mark Zuckerberg and Priscilla Chan of Facebook, Sergey Brin, founder of Google, and Yuri Milner and Anne Wojcicki, the founders of 23andMe, the Breakthrough Prize in Life Sciences is awarded to researchers whose work extends human life expectancy.\nProfessor Alim Louis Benabid was awarded the prize in 2015 for the development of deep brain stimulation, a technique which has revolutionized the treatment of Parkinson’s disease.\n\nOn June 9, 2016, Professor Alim Louis Benabid received the European Inventor Award 2016 for his work on deep brain stimulation. The technique, now used all over the world, has radically transformed the lives of more than 150,000 people suffering from Parkinson's disease, and significantly improved their quality of life.\n\nIn July 2012, Philippe Pozzo di Borgo, rendered quadriplegic following an accident and the man whose story was the inspiration for the French film Intouchables, visited Clinatec with a group of local journalists. He expressed his confidence in and admiration for this research program in a report aired on regional news channel, France 3 Alpes\n\n\n"}
{"id": "856308", "url": "https://en.wikipedia.org/wiki?curid=856308", "title": "Cloaca Maxima", "text": "Cloaca Maxima\n\nThe Cloaca Maxima (, lit. \"Greatest Sewer\", i.e. \"Main\") has constituted one of the world's earliest sewage systems. Constructed in Ancient Rome in order to drain local marshes and remove the waste of one of the world's most populous cities, it carried effluent to the River Tiber, which ran beside the city.\n\nAccording to tradition, it may have been initially constructed around 600 BC under the orders of the king of Rome, Tarquinius Priscus.\n\nThe Cloaca Maxima originally was built by the Etruscans as an open-air canal. Over time, the Romans covered over the canal and expanded it into a sewer system for the city.\n\nThis public work was largely achieved through the use of Etruscan engineers and large amounts of semi-forced labour from the poorer classes of Roman citizens. Underground work is said to have been carried out on the sewer by Tarquinius Superbus, Rome's seventh and last king.\n\nAlthough Livy describes it as being tunnelled out beneath Rome, he was writing centuries after the event. From other writings and from the path that it takes, it seems more likely that it was originally an open drain, formed from streams from three of the neighbouring hills, that were channelled through the main Forum and then on to the Tiber. This open drain would then have been gradually built over, as building space within the city became more valuable. It is possible that both theories are correct, and certainly some of the main lower parts of the system suggest that they would have been below ground level even at the time of the supposed construction.\n\nThe eleven aqueducts which supplied water to Rome by the 1st century AD were finally channelled into the sewers after having supplied the many public baths such as the Baths of Diocletian and the Baths of Trajan, the public fountains, imperial palaces and private houses. The continuous supply of running water helped to remove wastes and keep the sewers clear of obstructions. The best waters were reserved for potable drinking supplies, and the second quality waters would be used by the baths, the outfalls of which connected to the sewer network under the streets of the city. The aqueduct system was investigated by the general Frontinus at the end of the 1st century AD, who published his report on its state directly to the emperor Nerva.\n\nThere were many branches off the main sewer, but all seemed to be 'official' drains that would have served public toilets, bath-houses and other public buildings. Private residences in Rome, even of the rich, would have relied on some sort of cess-pit arrangement for sewage.\n\nThe Cloaca Maxima was well maintained throughout the life of the Roman Empire and even today drains rainwater and debris from the center of town, below the ancient Forum, Velabro and Foro Boario. In 33 BC it is known to have received an inspection and overhaul from Agrippa, and archaeology reveals several building styles and material from various ages, suggesting that the systems received regular attention. In more recent times, the remaining passages have been connected to the modern-day sewage system, mainly to cope with problems of backwash from the river.\n\nThe Cloaca Maxima was thought to be presided over by the goddess Cloacina.\n\nThe Romans are recorded – the veracity of the accounts depending on the case – to have dragged the bodies of a number of people to the sewers rather than give them proper burial, among them the emperor Elagabalus and Saint Sebastian: the latter scene is the subject of a well-known artwork by Lodovico Carracci.\n\nThe outfall of the Cloaca Maxima into the River Tiber is still visible today near the bridge Ponte Rotto, and near Ponte Palatino. There is a stairway going down to it visible next to the Basilica Julia at the Forum. Some of it is also visible from the surface opposite the church of San Giorgio al Velabro.\n\nThe system of Roman sewers was much imitated throughout the Roman Empire, especially when combined with copious supplies of water from Roman aqueducts. The sewer system in Eboracum—the modern-day English city of York—was especially impressive and part of it still survives.\n\n"}
{"id": "35669840", "url": "https://en.wikipedia.org/wiki?curid=35669840", "title": "ClockworkMod", "text": "ClockworkMod\n\nClockworkMod is a software company, owned by Koushik \"Koush\" Dutta, which develops various software products for Android smartphones and tablets. The company is primarily known for its custom recovery image, known as ClockworkMod Recovery, which is used in many custom made ROMs.\n\nClockworkMod Recovery is an Android custom recovery image. Once installed, this recovery image replaces your Android device's stock recovery image. Using this recovery image, various system-level operations can be performed. For example, one can create and restore partition backups, root, install / repair / upgrade system software and/or custom ROMs, and use other developer tools.\n\nClockworkMod Recovery is free and open-source software, released under the terms of the Apache License 2.0 software license. The ClockworkMod Recovery source code is included in the CyanogenMod source code tree.\n\n\nThe company also provides the following apps:\n\n\n"}
{"id": "44204924", "url": "https://en.wikipedia.org/wiki?curid=44204924", "title": "Crypto Wars", "text": "Crypto Wars\n\nThe Crypto Wars is an unofficial name for the U.S. and allied governments' attempts to limit the public's and foreign nations' access to cryptography strong enough to resist decryption by national intelligence agencies (especially USA's NSA).\n\nIn the early days of the Cold War, the U.S. and its allies developed an elaborate series of export control regulations designed to prevent a wide range of Western technology from falling into the hands of others, particularly the Eastern bloc. All export of technology classed as 'critical' required a license. CoCom was organized to coordinate Western export controls.\n\nTwo types of technology were protected: technology associated only with weapons of war (\"munitions\") and dual use technology, which also had commercial applications. In the U.S., dual use technology export was controlled by the Department of Commerce, while munitions were controlled by the State Department. Since in the immediate post WWII period the market for cryptography was almost entirely military, the encryption technology (techniques as well as equipment and, after computers became important, crypto software) was included as a Category XIII item into the United States Munitions List. The multinational control of the export of cryptography on the Western side of the cold war divide was done via the mechanisms of CoCom.\n\nBy the 1960s, however, financial organizations were beginning to require strong commercial encryption on the rapidly growing field of wired money transfer. The U.S. Government's introduction of the Data Encryption Standard in 1975 meant that commercial uses of high quality encryption would become common, and serious problems of export control began to arise. Generally these were dealt with through case-by-case export license request proceedings brought by computer manufacturers, such as IBM, and by their large corporate customers.\n\nEncryption export controls became a matter of public concern with the introduction of the personal computer. Phil Zimmermann's PGP cryptosystem and its distribution on the Internet in 1991 was the first major 'individual level' challenge to controls on export of cryptography. The growth of electronic commerce in the 1990s created additional pressure for reduced restrictions. Shortly afterward, Netscape's SSL technology was widely adopted as a method for protecting credit card transactions using public key cryptography.\n\nSSL-encrypted messages used the RC4 cipher, and used 128-bit keys. U.S. government export regulations would not permit crypto systems using 128-bit keys to be exported. At this stage Western governments had, in practice, a split personality when it came to encryption; policy was made by the military cryptanalysts, who were solely concerned with preventing their 'enemies' acquiring secrets, but that policy was then communicated to commerce by officials whose job was to support industry.\n\nThe longest key size allowed for export without individual license proceedings was 40 bits, so Netscape developed two versions of its web browser. The \"U.S. edition\" had the full 128-bit strength. The \"International Edition\" had its effective key length reduced to 40 bits by revealing 88 bits of the key in the SSL protocol. Acquiring the 'U.S. domestic' version turned out to be sufficient hassle that most computer users, even in the U.S., ended up with the 'International' version, whose weak 40-bit encryption could be broken in a matter of days using a single personal computer. A similar situation occurred with Lotus Notes for the same reasons.\n\nLegal challenges by Peter Junger and other civil libertarians and privacy advocates, the widespread availability of encryption software outside the U.S., and the perception by many companies that adverse publicity about weak encryption was limiting their sales and the growth of e-commerce, led to a series of relaxations in US export controls, culminating in 1996 in President Bill Clinton signing the Executive order 13026 transferring the commercial encryption from the Munition List to the Commerce Control List. Furthermore, the order stated that, \"the software shall not be considered or treated as 'technology'\" in the sense of Export Administration Regulations. This order permitted the United States Department of Commerce to implement rules that greatly simplified the export of proprietary and open source software containing cryptography, which they did in 2000.\n\nAs of 2009, non-military cryptography exports from the U.S. are controlled by the Department of Commerce's Bureau of Industry and Security. Some restrictions still exist, even for mass market products, particularly with regard to export to \"rogue states\" and terrorist organizations. Militarized encryption equipment, TEMPEST-approved electronics, custom cryptographic software, and even cryptographic consulting services still require an export license (pp. 6–7). Furthermore, encryption registration with the BIS is required for the export of \"mass market encryption commodities, software and components with encryption exceeding 64 bits\" (). In addition, other items require a one-time review by or notification to BIS prior to export to most countries. For instance, the BIS must be notified before open-source cryptographic software is made publicly available on the Internet, though no review is required. Export regulations have been relaxed from pre-1996 standards, but are still complex. Other countries, notably those participating in the Wassenaar Arrangement, have similar restrictions.\n\nUntil 1996, the UK government withheld export licenses from exporters unless they used weak ciphers or short keys, and generally discouraged practical public cryptography. A debate about cryptography for the NHS brought this out in the open.\n\nThe Clipper chip was a chipset for mobile phones made by the NSA in the 1990s, which implemented encryption with a backdoor for the US government. The US government tried to get phone manufacturers to adopt the chipset, but without success, and the program was finally defunct by 1996.\n\nA5/1 is a stream cipher used to provide over-the-air communication privacy in the GSM cellular telephone standard.\n\nSecurity researcher Ross Anderson reported in 1994 that \"there was a terrific row between the NATO signal intelligence agencies in the mid-1980s over whether GSM encryption should be strong or not. The Germans said it should be, as they shared a long border with the Warsaw Pact; but the other countries didn't feel this way, and the algorithm as now fielded is a French design.\"\n\nAccording to professor Jan Arild Audestad, at the standardization process which started in 1982, A5/1 was originally proposed to have a key length of 128 bits. At that time, 128 bits was projected to be secure for at least 15 years. It is now estimated that 128 bits would in fact also still be secure as of 2014. Audestad, Peter van der Arend, and Thomas Haug say that the British insisted on weaker encryption, with Haug saying he was told by the British delegate that this was to allow the British secret service to eavesdrop more easily. The British proposed a key length of 48 bits, while the West Germans wanted stronger encryption to protect against East German spying, so the compromise became a key length of 56 bits.\n\nThe widely used DES encryption algorithm was originally planned by IBM to have a key size of 128 bits; NSA lobbied for a key size of 48 bits. The end compromise were a key size of 64 bits, 8 of which were parity bits, to make an effective key security parameter of 56 bits. DES was considered insecure as early as 1977, and documents leaked in the 2013 Snowden leak shows that it was in fact easily crackable by the NSA, but was still recommended by NIST. The DES Challenges were a series of brute force attack contests created by RSA Security to highlight the lack of security provided by the Data Encryption Standard. As part of the successful cracking of the DES-encoded messages, EFF constructed a specialized DES cracking computer nicknamed Deep Crack.\n\nThe successful cracking of DES likely helped gather both political and technical support for more advanced encryption in the hands of ordinary citizens. In 1997, NIST began a competition to select a replacement for DES, resulting in the publication in 2000 of the Advanced Encryption Standard (AES). NSA considers AES strong enough to protect information classified at the Top Secret level.\n\nFearing widespread adoption of encryption, the NSA set out to stealthily influence and weaken encryption standards and obtain master keys—either by agreement, by force of law, or by computer network exploitation (hacking).\n\nAccording to the \"New York Times\": \"But by 2006, an N.S.A. document notes, the agency had broken into communications for three foreign airlines, one travel reservation system, one foreign government’s nuclear department and another’s Internet service by cracking the virtual private networks that protected them. By 2010, the Edgehill program, the British counterencryption effort, was unscrambling VPN traffic for 30 targets and had set a goal of an additional 300.\"\n\nAs part of Bullrun, NSA has also been actively working to \"Insert vulnerabilities into commercial encryption systems, IT systems, networks, and endpoint communications devices used by targets\". \"The New York Times\" has reported that the random number generator Dual_EC_DRBG contains a back door from the NSA, which would allow the NSA to break encryption relying on that random number generator. Even though Dual_EC_DRBG was known to be an insecure and slow random number generator soon after the standard was published, and the potential NSA backdoor was found in 2007, and alternative random number generators without these flaws were certified and widely available, RSA Security continued using Dual_EC_DRBG in the company's BSAFE toolkit and Data Protection Manager until September 2013. While RSA Security has denied knowingly inserting a backdoor into BSAFE, it has not yet given an explanation for the continued usage of Dual_EC_DRBG after its flaws became apparent in 2006 and 2007, however it was reported on December 20, 2013 that RSA had accepted a payment of $10 million from the NSA to set the random number generator as the default. Leaked NSA documents state that their effort was “a challenge in finesse” and that “Eventually, N.S.A. became the sole editor” of the standard.\n\nBy 2010, the NSA had developed “groundbreaking capabilities” against encrypted Internet traffic. A GCHQ document warned however “These capabilities are among the Sigint community’s most fragile, and the inadvertent disclosure of the simple ‘fact of’ could alert the adversary and result in immediate loss of the capability.” Another internal document stated that “there will be NO ‘need to know.’” Several experts, including Bruce Schneier and Christopher Soghoian, have speculated that a successful attack against RC4, a 1987 encryption algorithm still used in at least 50 per cent of all SSL/TLS traffic is a plausible avenue, given several publicly known weaknesses of RC4. Others have speculated that NSA has gained ability to crack 1024-bit RSA and Diffie–Hellman public keys. A team of researchers have pointed out that there is wide reuse of a few non-ephemeral 1024 bit primes in Diffie–Hellman implementations, and that NSA having done precomputation against those primes in order to break encryption using them in real time is very plausibly what NSA's \"groundbreaking capabilities\" refer to.\n\nThe Bullrun program is controversial, in that it is believed that NSA deliberately inserts or keeps secret vulnerabilities which affect both law-abiding US citizens as well as NSA's targets, under its NOBUS policy. In theory, NSA has two jobs: prevent vulnerabilities that affect the US, and find vulnerabilities that can be used against US targets; but as argued by Bruce Schneier, NSA seems to prioritize finding (or even creating) and keeping vulnerabilities secret. Bruce Schneier has called for the NSA to be broken up so that the group charged with strengthening cryptography is not subservient to the groups that want to break the cryptography of its targets.\n\nAs part of the Snowden leaks, it became widely known that intelligence agencies could bypass encryption of data stored on Android and iOS smartphones by legally ordering Google and Apple to bypass the encryption on specific phones. Around 2014, as a reaction to this, Google and Apple redesigned their encryption so that they did not have the technical ability to bypass it, and it could only be unlocked by knowing the user's password.\n\nVarious law enforcements officials, including the Obama administration's Attorney General Eric Holder responded with strong condemnation, calling it unacceptable that the state could not access alleged criminals' data even with a warrant. One of the more iconic responses being the chief of detectives for Chicago’s police department stating that \"Apple will become the phone of choice for the pedophile\". Washington Post posted an editorial insisting that \"smartphone users must accept that they cannot be above the law if there is a valid search warrant\", and after agreeing that backdoors would be undesirable, suggested implementing a \"golden key\" backdoor which would unlock the data with a warrant.\n\nFBI Director James Comey cited a number of cases to support the need to decrypt smartphones. Interestingly, in none of the presumably carefully handpicked cases did the smartphone have anything to do with the identification or capture of the culprits, and FBI seems to have been unable to find any strong cases supporting the need for smartphone decryption.\n\nBruce Schneier has labelled the right to smartphone encryption debate \"Crypto Wars II\", while Cory Doctorow called it \"Crypto Wars redux\".\n\nLegislators in the US states of California and New York have proposed bills to outlaw the sale of smartphones with unbreakable encryption. As of February 2016, no bills have been passed.\n\nIn February 2016 the FBI obtained a court order demanding that Apple create and electronically sign new software which would enable the FBI to unlock an iPhone 5c it recovered from one of the shooters in the 2015 terrorist attack in San Bernardino, California. Apple has challenged the order. In the end the FBI hired a third party to crack the phone. \"See\" FBI–Apple encryption dispute.\n\nIn April 2016, Dianne Feinstein and Richard Burr sponsored an overly vague bill that would be likely to criminalise all forms of strong encryption.\n\nIn October 2017, Deputy Attorney General Rod Rosenstein called for responsible encryption as a solution to the ongoing problem of \"going dark\". This refers to wiretapping court orders and police measures increasingly becoming ineffective as strong end-to-end encryption are increasingly added to widespread messenger products. Responsible encryption means that companies need to introduce key escrow that allows them to provide their customers with a way to recover their encrypted data if they forget their password, so that it is not lost forever. According to Rosenstein's reasoning, it would be irresponsible to leave the user helpless in such a case. As a pleasant side effect, this would allow a judge to issue a search warrant instructing the company to decrypt the data, which the company would then be able to comply with. In contrast to previous proposals, the decentral storage of key recovery material by companies instead of government agencies would be an additional safeguard.\n\nIn 2015 the head of the NSA, Admiral Michael S. Rogers suggested to further decentralize the key escrow by introducing \"front doors\" instead of back doors into encryption. This way, the key would be split into two halves, with one half being kept by the government authorities and the other by the company responsible for the encryption product. Thus, the government would still have to get a search warrant to obtain the other half of the key from the company, and the company would be unable to abuse the key escrow to access the user's data, since it would lack the other half of the key kept by the government. Experts were not impressed.\n\nIn 2018, the NSA promoted the use of \"lightweight encryption\", in particular its ciphers Simon and Speck, for Internet of Things devices. However, the attempt to have those ciphers standardized by ISO failed because of severe criticism raised by the board of cryptography experts which provoked fears that the NSA had non-public knowledge of how to break them.\n\nFollowing the 2015 \"Charlie Hebdo\" shooting, a terrorism attack, former UK Prime Minister David Cameron called for outlawing non-backdoored cryptography, saying that there should be no \"means of communication\" which \"we cannot read\". US president Barack Obama sided with Cameron on this.\n\n"}
{"id": "4992395", "url": "https://en.wikipedia.org/wiki?curid=4992395", "title": "DAVIC", "text": "DAVIC\n\nDAVIC, Digital Audio Video Council, was founded in 1994 with the aim of promoting the success of interactive digital audio-visual applications and services by promulgating specifications of open interfaces and protocols that maximise interoperability, not only across geographical boundaries but also across diverse applications, services and industries. It was a non-profit international organization based in Switzerland.\n\nDAVIC was closed, according to its statutes, after 5 years of activity.\n\nAt the most DAVIC had 222 companies from more than 25 countries as members, although over its life 295 organisations were members at some stage. It represented all sectors of the audio-visual industry: manufacturing (computer, consumer electronics and telecommunications equipment) and service (broadcasting, telecommunications and CATV), as well as a number of government agencies and research organisations. \n\nThe four major sets of specifications culminated in 1999 with the 1.4 version. The 1.5 version of DAVIC was a set of additional tools and service definitions opening the road towards the setting of IP based audiovisual services such as the TV Anytime and TV Anywhere services. Version 1.3.1 was re-structured and was approved as an ISO/IEC standard and technical report (ISO/IEC 16500 and ISO/IEC TR 16501).\n\nThe management of DAVIC had proposed the continuation beyond the 5-year date with a program of work centred on two projects: \"TV Anytime\" and \"TV Anywhere\". The membership did not support the continuation of DAVIC but the idea of \"TV-Anytime\" continued as a new organisation. \n\n\n\n"}
{"id": "31417867", "url": "https://en.wikipedia.org/wiki?curid=31417867", "title": "Dimefox", "text": "Dimefox\n\nDimefox was an organophosphate pesticide. In its pure form it is a colourless liquid with a fishy odour. Dimefox was first produced in 1940 by the group of Gerhard Schrader in Germany. It was historically used as a pesticide, but has been deemed obsolete or discontinued for use by the World Health Organization. However, they do not guarantee that all commercial use of this compound ceased. But in most countries it is no longer registered for use as a pesticide. It is considered an extremely hazardous substance as defined by the United States Emergency Planning and Community Right-to-Know Act.\n"}
{"id": "196141", "url": "https://en.wikipedia.org/wiki?curid=196141", "title": "Electrician", "text": "Electrician\n\nAn electrician is a tradesman specializing in electrical wiring of buildings, transmission lines, stationary machines, and related equipment. Electricians may be employed in the installation of new electrical components or the maintenance and repair of existing electrical infrastructure. Electricians may also specialize in wiring ships, airplanes, and other mobile platforms, as well as data and cable lines.\n\n\"Electricians\" were originally people who demonstrated or studied the principles of electricity, often electrostatic generators of one form or another. \nIn the United States, electricians are divided into two primary categories: linemen, who work on electric utility company distribution systems at higher voltages, and wiremen, who work with the lower voltages utilized inside buildings. Wiremen are generally trained in one of five primary specialties: commercial, residential, light industrial, industrial, and low-voltage wiring, more commonly known as Voice-Data-Video, or VDV. Other sub-specialties such as control wiring and fire-alarm may be performed by specialists trained in the devices being installed, or by inside wiremen.\nElectricians are trained to one of three levels: Apprentice, Journeyman, and Master Electrician. In the US and Canada, apprentices work and receive a reduced compensation while learning their trade. They generally take several hundred hours of classroom instruction and are contracted to follow apprenticeship standards for a period of between three and six years, during which time they are paid as a percentage of the Journeyman's pay. Journeymen are electricians who have completed their Apprenticeship and who have been found by the local, State, or National licensing body to be competent in the electrical trade. Master Electricians have performed well in the trade for a period of time, often seven to ten years, and have passed an exam to demonstrate superior knowledge of the National Electrical Code, or NEC.\nService electricians are tasked to respond to requests for isolated repairs and upgrades. They have skills troubleshooting wiring problems, installing wiring in existing buildings, and making repairs. Construction electricians primarily focus on larger projects, such as installing all new electrical system for an entire building, or upgrading an entire floor of an office building as part of a remodeling process. Other specialty areas are marine electricians, research electricians and hospital electricians. \"Electrician\" is also used as the name of a role in stagecraft, where electricians are tasked primarily with hanging, focusing, and operating stage lighting. In this context, the Master Electrician is the show's chief electrician. Although theater electricians routinely perform electrical work on stage lighting instruments and equipment, they are not part of the electrical trade and have a different set of skills and qualifications from the electricians that work on building wiring.\nIn the film industry and on a television crew the head electrician is referred to as a \"Gaffer.\"\nElectrical contractors are businesses that employ electricians to design, install, and maintain electrical systems. Contractors are responsible for generating bids for new jobs, hiring tradespeople for the job, providing material to electricians in a timely manner, and communicating with architects, electrical and building engineers, and the customer to plan and complete the finished product.\n\nMany jurisdictions have regulatory restrictions concerning electrical work for safety reasons due to the many hazards of working with electricity. Such requirements may be testing, registration or licensing. Licensing requirements vary between jurisdictions.\n\nAn electrician's license entitles the holder to carry out all types of electrical installation work in Australia without supervision. However, to contract, or offer to contract, to carry out electrical installation work, a licensed electrician must also be registered as an electrical contractor. Under Australian law, electrical work that involves fixed wiring is strictly regulated and must almost always be performed by a licensed electrician or electrical contractor. A local electrician can handle a range of work including air conditioning, light fittings and installation, safety switches, smoke alarm installation, inspection and certification and testing and tagging of electrical appliances. \n\nTo provide data, structured cabling systems, home automation & theatre, LAN, WAN and VPN data solutions or phone points, an installer must be licensed as a Telecommunications Cable Provider under a scheme controlled by Australian Communications and Media Authority\n\nElectrical licensing in Australia is regulated by the individual states. In Western Australia, the Department of Commerce tracks licensee's and allows the public to search for individually named/licensed Electricians.\n\nCurrently in Victoria the apprenticeship last for four years, during three of those years the apprentice attends trade school in either a block release of one week each month or one day each week. At the end of the apprenticeship the apprentice is required to pass three examinations, one of which is theory based with the other two practically based. Upon successful completion of these exams, providing all other components of the apprenticeship are satisfactory, the apprentice is granted an A Class licence on application to Energy Safe Victoria (ESV).\n\nAn A Class electrician may perform work unsupervised but is unable to work for profit or gain without having the further qualifications necessary to become a Registered Electrical Contractor (REC) or being in the employment of a person holding REC status. However, some exemptions do exist.\n\nIn most cases a certificate of electrical safety must be submitted to the relevant body after any electrical works are performed.\n\nSafety equipment used and worn by electricians in Australia (including insulated rubber gloves and mats) needs to be tested regularly to ensure it is still protecting the worker. Because of the high risk involved in this trade, this testing needs performed regularly and regulations vary according to state. Industry best practice is the Queensland Electrical Safety Act 2002, and requires six-monthly testing.\n\nTraining of electricians follows an apprenticeship model, taking four or five years to progress to fully qualified journeyman level. Typical apprenticeship programs consists of 80-90% hands-on work under the supervision of journeymen and 10-20% classroom training. Training and licensing of electricians is regulated by each province, however professional licenses are valid throughout Canada under Agreement on Internal Trade. An endorsement under the Red Seal Program provides additional competency assurance to industry standards.\nIn order for individuals to become a licensed electricians, they need to have 9000 hours of practical, on the job training. They also need to attend school for 4 terms and pass a provincial exam. This training enables them to become journeyman electricians. Furthermore, in British Columbia, an individual can go a step beyond that and become a “FSR”, or field safety representative. This credential gives the ability to become a licensed electrical contractor and to pull permits. Notwithstanding this, some Canadian provinces only grant \"permit pulling privileges\" to current Master Electricians, that is, a journeyman who has been engaged in the industry for three (3) years AND has passed the Master's examination (i.e. Alberta). The various levels of field safety representatives are A,B and C. The only difference between each class is that they are able to do increasingly higher voltage and current work.\n\nCompetency standards in the UK are defined by the Sector Skills council Summit Skills. Qualifications certificated by awarding organisations such as City and Guilds and EAL are based on these National Occupational Standards. Once qualified and demonstrating the required level of competence an Electrician can register with the JIB (Joint industry Board) for an Electrotechnical Certification Scheme (ECS) card. Electrical competence is required at Level 3 to practice as an electrician in the UK. The electrical industry is one of the few that require a trade test to be achieved prior to being fully qualified. This is known as the AM2.\n\nElectricians can demonstrate further competence by studying further qualifications in Design and Verification of Electrical Installations or in the Test and Inspection of Electrical Installations. These qualifications can be listed on the reverse of the JIB card.\n\nThe Electricity at Work Regulations are the statutory document that covers electrical installations. Further information is provided in the non-statutory document BS7671 - Requirements for Electrical Installations otherwise known as the Wiring Regulations currently (2013) in their 17th Edition. Installations that comply with BS7671 are deemed to have met the EAWR. Electrical Installation in domestic properties is governed by Part P of the Building Regulations and electricians have to register certain aspects of their work in domestic properties with the local building control authority.\n\nWith the exception of the work described in Part P of the Building Regulations there are no laws that prevent anyone from carrying out electrical work in the UK. A possible result of this is that during 2010/11 and in 2011/12 there were 3,822 domestic electrical fires in Great Britain, resulting in 14 deaths. Organisations such as the Electrical Safety Council are working hard to educate the public not to use electricians who are not fully qualified or competent and to check the ElectricSafe register to ensure an Electrician has been deemed competent.\n\nThe United States does not offer nationwide licensing and electrical licenses are issued by individual states. There are variations in licensing requirements, however, all states recognize three basic skill categories: level electricians. Journeyman electricians can work unsupervised provided that they work according to a master's direction. Generally, states do not offer journeyman permits, and journeyman electricians and other apprentices can only work under permits issued to a master electrician. Apprentices may not work without direct supervision. \n\nBefore electricians can work unsupervised, they are usually required to serve an apprenticeship lasting from 3 to 5 years under the general supervision of a Master Electrician and usually the direct supervision of a Journeyman Electrician. Schooling in electrical theory and electrical building codes is required to complete the apprenticeship program. Many apprenticeship programs provide a salary to the apprentice during training. A Journeyman electrician is a classification of licensing granted to those who have met the experience requirements for on the job training (usually 4000 to 6000 hours) and classroom hours (about 144 hours). Requirements include completion of two to six years of apprenticeship training, and passing a licensing exam.}.\n\nAn electrician's license is valid for work in the state where the license was issued. In addition, many states recognize licenses from other states, sometimes called interstate reciprocity participation, although there can be conditions imposed. For example, California reciprocates with Arizona, Nevada, and Utah on the condition that licenses are in good standing and have been held at the other state for five years. Nevada reciprocates with Arizona, California, and Utah. Maine reciprocates with New Hampshire and Vermont at the master level, and the state reciprocates with New Hampshire, North Dakota, Idaho, Oregon, Vermont, and Wyoming at the journeyman level.\n\nElectricians use a range of hand and power tools and instruments.\n\nSome of the more common tools are:\n\nIn addition to the workplace hazards generally faced by industrial workers, electricians are also particularly exposed to injury by electricity. An electrician may experience electric shock due to direct contact with energized circuit conductors or due to stray voltage caused by faults in a system. An electric arc exposes eyes and skin to hazardous amounts of heat and light. Faulty switchgear may cause an arc flash incident with a resultant blast. Electricians are trained to work safely and take many measures to minimize the danger of injury. Lockout and tagout procedures are used to make sure that circuits are proven to be de-energized before work is done. Limits of approach to energized equipment protect against arc flash exposure; specially designed flash-resistant clothing provides additional protection; grounding (earthing) clamps and chains are used on line conductors to provide a visible assurance that a conductor is de-energized. Personal protective equipment provides electrical insulation as well as protection from mechanical impact; gloves have insulating rubber liners, and work boots and hard hats are specially rated to provide protection from shock. If a system cannot be de-energized, insulated tools are used; even high-voltage transmission lines can be repaired while energized, when necessary.\n\nElectrical workers, which includes electricians, accounted for 34% of total electrocutions of construction trades workers in the United States between 1992–2003.\n\nWorking conditions for electricians vary by specialization. Generally an electrician's work is physically demanding such as climbing ladders and lifting tools and supplies. Occasionally an electrician must work in a cramped space or on scaffolding, and may frequently be bending, squatting or kneeling, to make connections in awkward locations. Construction electricians may spend much of their days in outdoor or semi-outdoor loud and dirty work sites. Industrial electricians may be exposed to the heat, dust, and noise of an industrial plant. Power systems electricians may be called to work in all kinds of adverse weather to make emergency repairs.\n\nSome electricians are union members and work under their union's policies.\n\nElectricians can choose to be represented by the Electrical Trade Union (ETU). Electrical Contractors can be represented by the National Electrical & Communications Association or Master Electricians Australia.\n\nSome electricians are union members. Some examples of electricians' unions include the International Brotherhood of Electrical Workers, Canadian Union of Public Employees, and the International Association of Machinists and Aerospace Workers.<br>The International Brotherhood of Electrical Workers provides its own apprenticeships through its National Joint Apprenticeship and Training Committee and the National Electrical Contractors Association. Many merit shop training and apprenticeship programs also exist, including those offered by such as trade associations as Associated Builders and Contractors and Independent Electrical Contractors. These organizations provide comprehensive training, in accordance with U.S. Department of Labor regulations.\n\nIn the United Kingdom, electricians are represented by several unions including Unite the Union\n\nIn the Republic of Ireland there are two self-regulation/self certification bodies RECI Register of Electrical Contractors of Ireland and ECSSA.\n\nAn auto electrician is a tradesman specializing in electrical wiring of motor vehicles. Auto electricians may be employed in the installation of new electrical components or the maintenance and repair of existing electrical components. Auto electricians specialize in cars and commercial vehicles. The auto electrical trade is generally more difficult than the electrical trade due to the confined spaces, engineering complexity of modern automotive electrical systems, and working conditions (often roadside breakdowns or on construction sites, mines, quarries to repair machinery etc.) Also the presence of high-current DC electricity makes injury from burns and arc-flash injury possible.\n\n\n"}
{"id": "58302950", "url": "https://en.wikipedia.org/wiki?curid=58302950", "title": "Elizabeth Bragg", "text": "Elizabeth Bragg\n\nElizabeth Bragg (1854 - 1929) was the first woman to receive a civil engineering degree from an American university. \n\nElizabeth Bragg receiving her degree in civil engineering from the University of California at Berkley in 1876. Bragg married George Cumming in 1888, a civil engineer with the Southern Pacific Railroad Company. Elizabeth Bragg Cumming died on the 10 November 1929, and is buried in California. \n"}
{"id": "727012", "url": "https://en.wikipedia.org/wiki?curid=727012", "title": "End-of-train device", "text": "End-of-train device\n\nThe end of train device (ETD), sometimes referred to as an EOT, flashing rear-end device (FRED) or sense and braking unit (SBU) is an electronic device mounted on the end of freight trains in lieu of a caboose. They are divided into three categories: \"dumb\" units, which only provide a visible indication of the rear of the train with a flashing red taillight; \"average intelligence\" units with a brake pipe pressure gauge; and \"smart\" units, which send back data to the crew in the locomotive via radio-based telemetry. They originated in North America, and are also used elsewhere in the world, where they may include complete End of Train Air System (ETAS) or Sense and Brake Unit (SBU) devices.\n\nA \"dumb\" ETD can be as simple as a red flag attached to the coupler on the last car of the train, whereas \"smart\" devices monitor functions such as brake line pressure and accidental separation of the train using a motion sensor, functions that were previously monitored by a crew in the caboose. The ETD transmits data via a telemetry link to the Head-of-Train Device (HTD) in the locomotive, known colloquially among railroaders as a \"Wilma,\" a play on the first name of the wife of cartoon character Fred Flintstone. In Canada, this device is known as a sense and braking unit (SBU).\n\nA typical HTD contains several lights indicating telemetry status and rear end movement, along with a digital readout of the brake line pressure from the ETD. It also contains a toggle switch used to initiate an emergency brake application from the rear end. In modern locomotives, the HTD is built into the locomotive's computer system, and the information is displayed on the engineer's computer screen.\n\nRailroads have strict government-approved air brake testing procedures for various circumstances when assembling trains or switching out cars en route. After a cut is made between cars in a train and the train is rejoined, in addition to other tests, the crew must verify that the brakes apply and release on the rear car (to ensure that all of the brake hoses are connected and the angle cocks, or valves, are opened). In most cases, the engineer is able to use information from the ETD to verify that the air pressure reduces and increases at the rear of the train accordingly, indicating proper brake pipe continuity. This device is said to constitute a fail-safe condition.\n\nThe ETD reduced labor costs, as well as the costs of the purchase and upkeep of cabooses. The Brotherhood of Conductors, and Brotherhood of Railroad Brakemen were also greatly affected by ETD, as this electronic unit replaced two crewmen per train. The widespread use of ETD's has made the caboose nearly obsolete. Some roads still use cabooses where the train must be backed up, on short local runs, as rolling offices, or railroad police stations and as transportation for right-of-way maintenance crews. In some cases (see photo) instead of hitching a caboose, an employee stands on the last car when the train is backing up.\n\nThe first ETD use is attributed to Florida East Coast Railway in 1969, soon after which other Class I railroads began using ETD's as well. By the mid-1980s they were common equipment. Early models were little more than a brake line connection / termination, a battery and flashing tail light. As their use became more widespread through the 1980s, ETD's were equipped with radio telemetry transmitters to send brake pressure data to a receiver in the locomotive. To reduce the cost of battery replacements, ambient light sensors were added so the flashing light on the ETD would illuminate only during dusk and after dark. Later models have a small turbine-powered electrical generator using air pressure from the brake line to power the ETD's radio and sensors.\n\nThe one-way communication of brake data from the ETD to the locomotive evolved into two-way communication that enables the engineer to apply the brakes from both ends of the train simultaneously in an emergency. This is useful in the event that a blockage (or an unopened valve) in the train's brake line is preventing dumping the air pressure and causing all of the brakes in the train going into an emergency application. Such a situation could be dangerous, as stopping distance increases with fewer functioning brakes. Dumping the brake line pressure from both the front and rear of the train simultaneously ensures that the entire train applies all of its brakes in emergency. Other electronics within the ETD were also enhanced, and many now include GPS receivers as well as the two-way radio communications.\n"}
{"id": "699052", "url": "https://en.wikipedia.org/wiki?curid=699052", "title": "Ethics of technology", "text": "Ethics of technology\n\nEthics in technology is a sub-field of ethics addressing the ethical questions specific to the Technology Age. Some prominent works of philosopher Hans Jonas are devoted to ethics of technology. The subject has also been explored, following the work of Mario Bunge, under the term technoethics.\n\nIt is often held that technology itself is incapable of possessing moral or ethical qualities, since \"technology\" is merely tool making. But many now believe that each piece of technology is endowed with and radiating ethical commitments all the time, given to it by those that made it, and those that decided how it must be made and used. Whether merely a lifeless amoral 'tool' or a solidified embodiment of human values \"ethics of technology\" refers to two basic subdivisions:-\n\nIn the former case, ethics of such things as computer security and computer viruses asks whether the very act of innovation is an ethically right or wrong act. Similarly, does a scientist have an ethical obligation to produce or fail to produce a nuclear weapon? What are the ethical questions surrounding the production of technologies that waste or conserve energy and resources? What are the ethical questions surrounding the production of new manufacturing processes that might inhibit employment, or might inflict suffering in the third world?\n\nIn the latter case, the ethics of technology quickly break down into the ethics of various human endeavors as they are altered by new technologies. For example, bioethics is now largely consumed with questions that have been exacerbated by the new life-preserving technologies, new cloning technologies, and new technologies for implantation. In law, the right of privacy is being continually attenuated by the emergence of new forms of surveillance and anonymity. The old ethical questions of privacy and free speech are given new shape and urgency in an Internet age. Such tracing devices as RFID, biometric analysis and identification, genetic screening, all take old ethical questions and amplify their significance.\n\nTechnoethics (TE) is an interdisciplinary research area that draws on theories and methods from multiple knowledge domains (such as communications, social sciences information studies, technology studies, applied ethics, and philosophy) to provide insights on ethical dimensions of technological systems and practices for advancing a technological society.\n\nTechnoethics views technology and ethics as socially embedded enterprises and focuses on discovering the ethical use of technology, protecting against the misuse of technology, and devising common principles to guide new advances in technological development and application to benefit society. Typically, scholars in technoethics have a tendency to conceptualize technology and ethics as interconnected and embedded in life and society. Technoethics denotes a broad range of ethical issues revolving around technology – from specific areas of focus affecting professionals working with technology to broader social, ethical, and legal issues concerning the role of technology in society and everyday life.\n\nTechnoethical perspectives are constantly in transition as technology advances in areas unseen by creators, as users change the intended uses of new technologies. Humans cannot be separated from these technologies because it is an inherent part of consciousness and meaning in life therefore, requiring an ethical model. The short term and longer term ethical considerations for technologies do not just engage the creator and producer but makes the user question their beliefs in correspondence with this technology and how governments must allow, react to, change, and/or deny technologies.\n\nUsing theories and methods from multiple domains, technoethics provides insights on ethical aspects of technological systems and practices, examines technology-related social policies and interventions, and provides guidelines for how to ethically use new advancements in technology. Technoethics provides a systems theory and methodology to guide a variety of separate areas of inquiry into human-technological activity and ethics. Moreover, the field unites both technocentric and bio-centric philosophies, providing \"conceptual grounding to clarify the role of technology to those affected by it and to help guide ethical problem solving and decision making in areas of activity that rely on technology.\" As a bio-techno-centric field, technoethics \"has a relational orientation to both technology and human activity\"; it provides \"a system of ethical reference that justifies that profound dimension of technology as a central element in the attainment of a 'finalized' perfection of man.'\n\n\nThough the ethical consequences of new technologies have existed since Socrates' attack on writing in Plato's dialogue, \"Phaedru\"s, the formal field of technoethics had only existed for a few decades. The first traces of TE can be seen in Dewey and Peirce's pragmatism. With the advent of the industrial revolution, it was easy to see that technological advances were going to influence human activity. This is why they put emphasis on the responsible use of technology.\n\nThe term \"technoethics\" was coined in 1977 by the philosopher Mario Bunge to describe the responsibilities of technologists and scientists to develop ethics as a branch of technology. Bunge argued that the current state of technological progress was guided by ungrounded practices based on limited empirical evidence and trial-and-error learning. He recognized that \"the technologist must be held not only technically but also morally responsible for whatever he designs or executes: not only should his artifacts be optimally efficient but, far from being harmful, they should be beneficial, and not only in the short run but also in the long term.\" He recognized a pressing need in society to create a new field called 'Technoethics' to discover rationally grounded rules for guiding science and technological progress.\n\nWith the spurt in technological advances came technological inquiry. Societal views of technology were changing; people were becoming more critical of the developments that were occurring and scholars were emphasizing the need to understand and to take a deeper look and study the innovations. Associations were uniting scholars from different disciplines to study the various aspects of technology. The main disciplines being philosophy, social sciences and science and technology studies (STS). Though many technologies were already focused on ethics, each technology discipline was separated from each other, despite the potential for the information to intertwine and reinforce itself. As technologies became increasingly developed in each discipline, their ethical implications paralleled their development, and became increasingly complex. Each branch eventually became united, under the term technoethics, so that all areas of technology could be studied and researched based on existing, real-world examples and a variety of knowledge, rather than just discipline-specific knowledge.\n\nTechnoethics involves the ethical aspects of technology within a society that is shaped by technology. This brings up a series of social and ethical questions regarding new technological advancements and new boundary crossing opportunities. Before moving forward and attempting to address any ethical questions and concerns, it is important to review the three major ethical theories to develop a perspective foundation :\n\n\n\nMany advancements within the past decades have added to the field of technoethics. There are multiple concrete examples that have illustrated the need to consider ethical dilemmas in relation to technological innovations. Beginning in the 1940s influenced by the British eugenic movement, the Nazis conduct \"racial hygiene\" experiments causing widespread, global anti-eugenic sentiment. In the 1950s the first satellite Sputnik 1 orbited the earth, the Obninsk Nuclear Power Plant was the first nuclear power plant to be opened, the American nuclear tests take place. The 1960s brought about the first manned moon landing, ARPANET created which leads to the later creation of the Internet, first heart transplantation completed, and the Telstar communications satellite is launched. The 70s, 80s, 90s, 2000s and 2010s also brought multiple developments.\n\nTechnological consciousness is the relationship between humans and technology. Technology is seen as an integral component of human consciousness and development. Technology, consciousness and society are intertwined in a relational process of creation that is key to human evolution. Technology is rooted in the human mind, and is made manifest in the world in the form of new understandings and artifacts. The process of technological consciousness frames the inquiry into ethical responsibility concerning technology by grounding technology in human life.\n\nThe structure of technological consciousness is relational but also situational, organizational, aspectual and integrative. Technological consciousness situates new understandings by creating a context of time and space. As well, technological consciousness organizes disjointed sequences of experience under a sense of unity that allows for a continuity of experience. The aspectual component of technological consciousness recognizes that individuals can only be conscious of aspects of an experience, not the whole thing. For this reason, technology manifests itself in processes that can be shared with others. The integrative characteristics of technological consciousness are assimilation, substitution and conversation. Assimilation allows for unfamiliar experiences to be integrated with familiar ones. Substitution is a metaphorical process allowing for complex experiences to be codified and shared with others — for example, language. Conversation is the sense of an observer within an individual's consciousness, providing stability and a standpoint from which to interact with the process.\n\nThe common misunderstandings about consciousness and technology are listed as follows. The first misunderstanding is that consciousness is only in the head when in fact, consciousness is not only in the head meaning that \"[c]onsciousness is responsible for the creation of new conscious relations wherever imagined, be it in the head, on the street or in the past.\" The second misunderstanding is technology is not a part of consciousness. The truth is that technology is a part of consciousness as \"the conceptualization of technology has gone through drastic changes.\" The third misunderstanding is that technology controls society and consciousness, when really technology does not control society and consciousness; meaning \"that technology is rooted in consciousness as an integral part of mental life for everyone. This understanding will most likely alter how both patients and psychologists deal with the trials and tribunes of living with technology.\" The last misunderstanding is society controls technology and consciousness which is not true, society does not control technology and consciousness. \"…(other) accounts fail to acknowledge the complex relational nature of technology as an operation within mind and society. This realization shifts the focus on technology to its origins within the human mind as explained through the theory of technological consciousness.\"\n\n\nEthical challenges arise in many different situations,\n\n\nDigital copyrights are a heated issue because there are so many sides to the discussion. There are ethical considerations surrounding the artist, producer, end user, and the country are intertwined. Not to mention the relationships with other countries and the impact on the use (or no use) of content housed in their countries. In Canada, national laws such as the Copyright Act and the history behind Bill C-32 are just the beginning of the government's attempt to shape the \"wild west\" of Canadian Internet activities. The ethical considerations behind Internet activities such a peer-to-peer file sharing involve every layer of the discussion – the consumer, artist, producer, music/movie/software industry, national government, and international relations. Overall, technoethics forces the \"big picture\" approach to all discussions on technology in society. Although time consuming, this \"big picture\" approach offers some level of reassurance when considering that any law put in place could drastically alter the way we interact with our technology and thus the direction of work and innovation in the country.\n\nThe use of copyrighted material to create new content is a hotly debated topic. The emergence of the musical \"mashup\" genre has compounded the issue of creative licensing. A moral conflict is created between those who believe that copyright protects any unauthorized use of content, and those who maintain that sampling and mash-ups are acceptable musical styles and, though they use portions of copyrighted material, the end result is a new creative piece which is the property of the creator, and not of the original copyright holder. Whether or not the mashup genre should be allowed to use portions of copyrighted material to create new content is one which is currently under debate.\n\nFor many years , new technologies took an important place in social, cultural, political, and economic life. Thanks to the democratization of informatics access and the network's globalization, the number of exchanges and transaction is in perpetual progress.\n\nMany people are exploiting the facilities and anonymity that modern technologies offer in order to commit multiple criminal activities. Cybercrime is one of the fastest growing areas of crime. The problem is that some laws that profess to protect people from those who would do wrong things via digital means also threaten to take away people's freedom.\n\nSince the introduction of full body X-ray scanners to airports in 2007, many concerns over traveler privacy have arisen. Individuals are asked to step inside a rectangular machine that takes an alternate wavelength image of the person's naked body for the purpose of detecting metal and non-metal objects being carried under the clothes of the traveler. This screening technology comes in two forms, millimeter wave technology (MM-wave technology) or backscatter X-rays (similar to x-rays used by dentists). Full-body scanners were introduced into airports to increase security and improve the quality of screening for objects such as weapons or explosives due to an increase of terrorist attacks involving airplanes occurring in the early 2000s.\n\nEthical concerns of both travelers and academic groups include fear of humiliation due to the disclosure of anatomic or medical details, exposure to a low level of radiation (in the case of backscatter X-ray technology), violation of modesty and personal privacy, clarity of operating procedures, the use of this technology to discriminate against groups, and potential misuse of this technology for reasons other than detecting concealed objects. Also people with religious beliefs that require them to remain physically covered (arms, legs, face etc.) at all times will be unable and morally opposed to stepping inside of this virtually intrusive scanning technology. The Centre for Society, Science and Citizenship have discussed their ethical concerns including the ones mentioned above and suggest recommendations for the use of this technology in their report titled \"Whole Body Imaging at airport checkpoints: the ethical and policy context\" (2010).\n\nThe discourse around GPS tracking devices and geolocation technologies and this contemporary technology's ethical ramifications on privacy is growing as the technology becomes more prevalent in society. As discussed in the \"New York Times\"'s Sunday Review on September 22, 2012, the editorial focused on the ethical ramifications that imprisoned a drug offender because of the GPS technology in his cellphone was able to locate the criminal's position. Now that most people carry on the person a cell, the authorities have the ability to constantly know the location of a large majority of citizens. The ethical discussion now can be framed from a legal perspective. As raised in the editorial, there are stark infractions that these geolocation devices on citizens' Fourth Amendment and their protection against unreasonable searches. This reach of this issue is not just limited to the United States but affects more democratic state that uphold similar citizens' rights and freedoms against unreasonable searches.\n\nThese geolocation technologies are not only affecting how citizens interact with their state but also how employees interact with their workplaces. As discussed in article by the Canadian Broadcasting Company, \"GPS and privacy\", that a growing number of employers are installing geolocation technologies in \"company vehicles, equipment and cellphones\" (Hein, 2007). Both academia and unions are finding these new powers of employers to be indirect contradiction with civil liberties. This changing relationship between employee and employer because of the integration of GPS technology into popular society is demonstrating a larger ethical discussion on what are appropriate privacy levels. This discussion will only become more prevalent as the technology becomes more popular.\n\nGenetically modified foods have become quite common in developed countries around the world, boasting greater yields, higher nutritional value, and greater resistance to pests, but there are still many ethical concerns regarding their use. Even commonplace genetically modified crops like corn raise questions of the ecological consequences of unintended cross pollination, potential horizontal gene transfer, and other unforeseen health concerns for humans and animals.\n\nTrademarked organisms like the \"Glofish\" are a relatively new occurrence. These zebrafish, genetically modified to appear in several fluorescent colours and sold as pets in the United States, could have unforeseen effects on freshwater environments were they ever to breed in the wild.\n\nProviding they receive approval from the U.S. Food and Drug Administration (FDA), another new type of fish may be arriving soon. The \"AquAdvantage salmon\", engineered to reach maturity within roughly 18 months (as opposed to three years in the wild), could help meet growing global demand. There are health and environmental concerns associated with the introduction any new GMO, but more importantly this scenario highlights the potential economic impact a new product may have. The FDA does perform an economic impact analysis to weigh, for example, the consequences these new genetically modified fish may have on the traditional salmon fishing industry against the long term gain of a cheaper, more plentiful source of salmon. These technoethical assessments, which regulatory organizations like the FDA are increasingly faced with worldwide, are vitally important in determining how GMOs—with all of their potential beneficial and harmful effects—will be handled moving forward.\n\nFor over 40 years, newborn screening has been a triumph of the 20th century public health system. Through this technology, millions of parents are given the opportunity to screen for and test a number of disorders, sparing the death of their children or complications such as mental retardation. However, this technology is growing at a fast pace, disallowing researchers and practitioners from being able to fully understand how to treat diseases and provide families in need with the resources to cope.\nA version of pre-natal testing, called tandem mass spectrometry, is a procedure that \"measures levels and patterns of numerous metabolites in a single drop of blood, which are then used to identify potential diseases. Using this same drop of blood, tandem mass spectrometry enables the detection of at least four times the number of disorders than was possible with previous technologies.\" This allows for a cost-effective and fast method of pre-natal testing.\n\nHowever, critics of tandem mass spectrometry and technologies like it are concerned about the adverse consequences of expanding newborn screen technology and the lack of appropriate research and infrastructure needed to provide optimum medical services to patients. Further concerns include \"diagnostic odysseys\", a situation in which the patient aimlessly continues to search for diagnoses where none exists.\n\nAmong other consequences, this technology raises the issue of whether individuals other than newborn will benefit from newborn screening practices. A reconceptualization of the purpose of this screening will have far reaching economic, health and legal impact. This discussion is only just beginning and requires informed citizenry to reach legal if not moral consensus on how far we as a society are comfortable with taking this technology.\n\nCitizen journalism is a concept describing citizens who wish to act as a professional journalist or media person by \"collecting, reporting, analyzing, and disseminating news and information\" According to Jay Rosen, citizen journalists are \"the people formerly known as the audience,\" who \"were on the receiving end of a media system that ran one way, in a broadcasting pattern, with high entry fees and a few firms competing to speak very loudly while the rest of the population listened in isolation from one another— and who today are not in a situation like that at all. ... The people formerly known as the audience are simply the public made realer, less fictional, more able, less predictable\". \n\nThe internet has provided society with a modern and accessible public space. Due to the openness of the internet, there are discernible effects on the traditional profession of journalism. Although the concept of citizen journalism is a seasoned one, \"the presence of online citizen journalism content in the marketplace may add to the diversity of information that citizens have access to when making decisions related to the betterment of their community or their life\". The emergence of online citizen journalism is fueled by the growing use of social media websites to share information about current events and issues locally, nationally and internationally.\n\nThe open and instantaneous nature of the internet affects the criteria of information quality on the web. A journalistic code of ethics is not instilled for those who are practicing citizen journalism. Journalists, whether professional or citizen, have needed to adapt to new priorities of current audiences: accessibility, quantity of information, quick delivery and aesthetic appeal. Thus, technology has affected the ethical code of the profession of journalism with the popular free and instant sharing qualities of the internet. Professional journalists have had to adapt to these new practices to ensure that truthful and quality reporting is being distributed. The concept can be seen as a great advancement in how society communicates freely and openly or can be seen as contributing to the decay of traditional journalistic practices and codes of ethics.\n\nOther issues to consider:\n\nDespite the amassing body of scholarly work related to technoethics beginning in the 1970s, only recently has it become institutionalized and recognized as an important interdisciplinary research area and field of study. In 1998, the Epson Foundation founded the Instituto de Tecnoética in Spain under the direction of Josep Esquirol. This institute has actively promoted technoethical scholarship through awards, conferences, and publications. This helped encourage scholarly work for a largely European audience. The major driver for the emergence of technoethics can be attributed to the publication of major reference works available in English and circulated globally. The \"Encyclopedia of Science, Technology, and Ethics\" included a section on technoethics which helped bring it into mainstream philosophy. This helped to raise further interest leading to the publication of the first reference volume in the English language dedicated to the emerging field of Technoethics. The two volume \"Handbook of Research on Technoethics\" explores the complex connections between ethics and the rise of new technologies (e.g., life-preserving technologies, stem cell research, cloning technologies, new forms of surveillance and anonymity, computer networks, Internet advancement, etc.) This recent major collection provides the first comprehensive examination of technoethics and its various branches from over 50 scholars around the globe. The emergence of technoethics can be juxtaposed with a number of other innovative interdisciplinary areas of scholarship which have surfaced in recent years such as technoscience and technocriticism.\n\nWith all the developments we've had in technology it has created a lot advancement for the music industry both positive and negative. A main concern is piracy and illegal downloading; with all that is available through the internet a lot of music (TV shows and movies as well) have become easily accessible to download and upload for free. This does create new challenges for artist, producers, and copyright laws. The advances it has positively made for the industry is a whole new genre of music. Computers are being used to create electronic music, as well as synthesizers (computerized/electronic piano). This type of music is becoming rapidly more common and listened to. These advances have allowed the industry to try new things and make new explorations.\n\nThe future of technoethics is a promising, yet evolving field. The studies of e-technology in workplace environments are an evolving trend in technoethics. With the constant evolution of technology, and innovations coming out daily, technoethics is looking to be a rather promising guiding framework for the ethical assessments of new technologies. Some of the questions regarding technoethics and the workplace environment that have yet to be examined and treated are listed below:\n\nUNESCO – a specialized intergovernmental agency of the United Nations, focusing on promotion of education, culture social and natural sciences and communication and information.\nIn the future, the use of principles as expressed in the UNESCO Universal Declaration on Bioethics and Human Rights (2005) will also be analyzed to broaden the description of bioethical reasoning (Adell & Luppicini, 2009).\n\nBiotech ethics concerned with ethical dilemmas surrounding the use of biotechnologies in fields including medical research, health care, and industrial applications. Topics such as cloning ethics, e-health ethics, telemedicine ethics, genetics ethics, neuroethics, and sport and nutrition ethics fall into this category; examples of specific issues include the debates surrounding euthanasia and reproductive rights.\n\nThis area of technoethical inquiry is concerned with technology's relation to the human mind, artificial agents, and society. Topics of study that would fit into this category would be artificial morality and moral agents, technoethical systems and techno-addiction.\n\nThis field is concerned with the uses of technology to ethically regulate aspects of a society. For example: digital property ethics, social theory, law, science, organizational ethics and global ethics.\n\nTechnoethics has concerned itself with society as a general group and made no distinctions between the genders, but considers technological effects and influences on each gender individually. This is an important consideration as some technologies are created for use by a specific gender, including birth control, abortion, fertility treatments, and Viagra. Feminists have had a significant influence on the prominence and development of reproductive technologies. Technoethical inquiry must examine these technologies' effects on the intended gender while also considering their influence on the other gender. Another dimension of technofeminism concerns female involvement in technological development: women's participation in the field of technology has broadened society's understanding of how technology affects the female experience in society.\n\nInformation and communication technoethics is \"concerned with ethical issues and responsibilities arising when dealing with information and communication technology in the realm of communication.\" This field is related to internet ethics, rational and ethical decision making models, and information ethics. A major area of interest is the convergence of technologies: as technologies become more interdependent and provide people with multiple ways of accessing the same information, they transform society and create new ethical dilemmas. This is particularly evident in the realms of the internet. In recent years, users have had the unprecedented position of power in creating and disseminating news and other information globally via social networking; the concept of \"citizen journalism\" primarily relates to this. With developments in the media, has led to open media ethics as Ward writes, leading to citizen journalism.\n\nIn cases such as the 2004 Indian Ocean Tsunami or the 2011 Arab Spring movements, citizen journalists were seen to have been significant sources of facts and information in relation to the events. These were re-broadcast by news outlets, and more importantly, re-circulated by and to other internet users. As Jay David Bolter and Richard Grusin state in their book Remediation: Understanding New Media (1999): \"The liveness of the Web is a refashioned version of the liveness of broadcast television\" However, it is commonly political events (such as 'Occupy' movements or the Iran Elections of 2009) that tend to raise ethical questions and concerns. In the latter example, there had been efforts made by the Iranian government in censoring and prohibiting the spread of internal happenings to the outside by its citizen journalists. This occurrence questioned the importance of the spread of crucial information regarding the issue, and the source from which it came from (citizen journalists, government authorities, etc.). This goes to prove how the internet \"enables new forms of human action and expression [but] at the same time it disables [it]\" Information and Communication Technoethics also identifies ways to develop ethical frameworks of research structures in order to capture the essence of new technologies.\n\nTechnoethical inquiry in the field of education examines how technology impacts the roles and values of education in society. This field considers changes in student values and behavior related to technology, including access to inappropriate material in schools, online plagiarism using material copied directly from the internet, or purchasing papers from online resources and passing them off as the student's own work. Educational technoethics also examines the digital divide that exists between educational institutions in developed and developing countries or between unequally-funded institutions within the same country: for instance, some schools offer students access to online material, while others do not. Professional technoethics focuses on the issue of ethical responsibility for those who work with technology within a professional setting, including engineers, medical professionals, and so on. Efforts have been made to delineate ethical principles in professions such as computer programming (see programming ethics).\n\nEnvironmental technoethics originate from the 1960s and 1970s' interest in environment and nature. The field focuses on the human use of technologies that may impact the environment; areas of concern include transport, mining, and sanitation. Engineering technoethics emerged in the late 19th century. As the Industrial Revolution triggered a demand for expertise in engineering and a need to improve engineering standards, societies began to develop codes of professional ethics and associations to enforce these codes. Ethical inquiry into engineering examines the \"responsibilities of engineers combining insights from both philosophy and the social sciences.\"\n\nA technoethical assessment (TEA) is an interdisciplinary, systems-based approach to assessing ethical dilemmas related to technology. TEAs aim to guide actions related to technology in an ethical direction by advancing knowledge of technologies and their effects; successful TEAs thus produce a shared understanding of knowledge, values, priorities, and other ethical aspects associated with technology. TEAs involve five key steps:\n\n\nTechnoethical design (TED) refers to the process of designing technologies in an ethical manner, involving stakeholders in participatory design efforts, revealing hidden or tacit technological relations, and investigating what technologies make possible and how people will use them. TED involves the following four steps:\n\n\nBoth TEA and TED rely on systems theory, a perspective that conceptualizes society in terms of events and occurrences resulting from investigating system operations. Systems theory assumes that complex ideas can be studied as systems with common designs and properties which can be further explained using systems methodology. The field of technoethics regards technologies as self-producing systems that draw upon external resources and maintain themselves through knowledge creation; these systems, of which humans are a part, are constantly in flux as relations between technology, nature, and society change. TEA attempts to elicit the knowledge, goals, inputs, and outputs that comprise technological systems. Similarly, TED enables designers to recognize technology's complexity and power, to include facts and values in their designs, and to contextualize technology in terms of what it makes possible and what makes it possible.\n\nRecent advances in technology and their ability to transmit vast amounts of information in a short amount of time has changed the way information is being shared amongst co-workers and managers throughout organizations across the globe. Starting in the 1980s with information and communications technologies (ICTs), organizations have seen an increase in the amount of technology that they rely on to communicate within and outside of the workplace. However, these implementations of technology in the workplace create various ethical concerns and in turn a need for further analysis of technology in organizations. As a result of this growing trend, a subsection of technoethics known as organizational technoethics has emerged to address these issues.\n\nKey scholarly contributions linking ethics, technology, and society can be found in a number of seminal works:\n\nThis resulting scholarly attention to ethical issues arising from technological transformations of work and life has helped given rise to a number of key areas (or branches) of technoethical inquiry under various research programs (i.e., computer ethics, engineering ethics, environmental technoethics, biotech ethics, Nanoethics, educational technoethics, information and communication ethics, media ethics, and Internet ethics).\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21713048", "url": "https://en.wikipedia.org/wiki?curid=21713048", "title": "Finacle", "text": "Finacle\n\nFinacle is a core banking product developed by the Indian corporation Infosys that provides universal digital banking functionality to banks. In August 2015, Finacle became part of EdgeVerve Systems Limited, a wholly owned product subsidiary of Infosys.\n\nFinacle is used by banks across 100 countries to serve over 1 billion customers. Finacle has a strong focus on product strategy and a global market presence.\n\nIn 2003, India’s largest bank, State Bank of India (SBI), deployed Infosys’s Finacle core banking, e-banking and treasury solutions across its foreign branches outside India. SBI implemented Finacle in 25 countries. The bank's global business operations were standardised on the Finacle platform, on three data centres in the US, UK, and India. These three data centres were later downsized to a single one in India, while upgrading the entire system. \nThis achieved a reduction in costs as well as allowing the bank to offer more products, roll them out faster.\n\nThe next implementation was at DBS Bank, Singapore, who selected Finacle Universal Banking Solution from Infosys as the core banking platform for the programme. The bank ventured on the implementation of Finacle in Singapore and its key markets across the region with core banking product offerings, and strengthen its technology platform. DBS Bank implemented Finacle in 13 countries.\n\nIn 2008, Finacle was implemented at Punjab National Bank, India’s second largest public sector bank at all its 4,604 branches and extension counters on Sun infrastructure and the Oracle database. The bank said that the solution was rolled out across 100 per cent of PNB’s branches in record time. The bank had 22,500 concurrent users on the Finacle platform, with the number of peak transactions at 3.5 million.\n\nIn 2009, Emirates NBD, created from the merger of two leading banks in Dubai (Emirates Bank and National Bank of Dubai) implemented Finacle to be the solution \"core to the merger\" for facilitating the bank's expansion plans into the region. \n\nIn 2009, Kashi Gomti Samyut Gramin Bank, among all Gramin Banks of India, become first to shift at Core Banking Solutions through Finacle. \n\nIn 2011, Finacle was set to enter the United States market via a deal with Union Bank N.A. of California, reported at a value of 20 million. The deal was however called off by the bank in the same year.\n\nFinacle was chosen by Standard Bank, South Africa as the preferred solution for Africa (excluding South Africa). Its flexibility, scalability, cost effectiveness and compatibility with other systems in the bank made Finacle the ideal choice. In July 2011, Nigeria was the first country to start operations on Finacle. Namibia was the second country to go live on Finacle in 2012. Malawi,Uganda, Tanzania, Zambia and Botswana are in the pipe line to launch operations on Finacle.\n\nIn May 2012 Kotak Mahindra Bank implemented the Core Banking Solution platform Finacle. The entire implementation, which replaced the bank's legacy core banking system with no disruption to business, was completed in 18 months. Finacle 10 was found to be the best fit, both functionally and technologically by the bank. Kotak Mahindra Bank is the fourth largest Indian private sector bank by market capitalization, headquartered in Mumbai, Maharashtra.\n\nIn October 2012, Rizal Commercial Banking Corporation (RCBC), a private domestic commercial bank in the Philippines, implemented Finacle on the IBM System Z server to transform its business and leverage new growth opportunities in an increasingly competitive consumer banking market. The transformation was completed smoothly with no disruption in services to RCBC customers.\n\nIn May 2013, (T-Bank), one of Turkey’s private banks completed the installation of the Infosys Finacle core banking solution, which includes customer relationship management (CRM) and treasury modules. The Turkish bank says that the live deployment of the Infosys Finacle software will enable T-bank to roll-out products faster and improve customer services helping to meet the growing needs of retail and corporate customers in a fast expanding economy.\n\nIn September 2013, Infosys announced the launch of Finacle 11E, an advanced universal banking solution that simplifies banking transformation. The solution, built on the component model, helps banks of all sizes efficiently modernize their operations in a phased manner, reducing the risks in the journey. The enterprise class components are expected to improve both the bank's efficiency and the customer experience across all channels. Some of the new enterprise components added to the Finacle suite include - Payments, Multichannel Framework, Offers and Catalog, Liquidity Management, and Dashboards.\n\nIn November 2014, it has successfully implemented its Finacle™ core banking solution for Discover Financial Services (NYSE: DFS), a leading U.S. direct bank and payment services company.\n\nIn early 2015, First Merchant Bank of Malawi (FMB) successfully rolled out Finacle operations across all its branches within Malawi. Standard Bank Malawi has also unveiled plans to roll out Finacle operations by mid-2018.\n\nFinacle also won a deal at Qantas Credit Union for deployment of the solution on a private cloud.\n\nIn September 2017, Finacle partnered with ToneTag for sound-based contactless payments solutions.\n\nFinacle offers a suite of products, which are periodically evaluated by independent research firms like Forrester.\n\nMajor customers of Finacle include Dena Bank, Union Bank of India, Bank of India, Oriental Bank of Commerce, Punjab National Bank, Jammu & Kashmir Bank, Bank of Baroda, IDBI Bank, Karnataka Bank Andhra Bank, Federal Bank, Punjab & Sind Bank, South Indian Bank, United Bank of India, UCO Bank, Qantas Credit Union in Australia, ICICI Bank in India, DBS Bank in India, Indian Overseas Bank in India, Discover Financial Services in the US, Emirates NBD, Al Hilal Bank in the United Arab Emirates. and Corporation Bank.\n\nFinacle was identified as a long term leader by Forrester for offering comprehensive banking functionalities. With capabilities to support customer focused banking, customer oriented product configuration, multi-channel capability and social media support, Finacle topped the Forrester Wave: Customer-Centric Global Banking Platforms.\n\nFinacle from Infosys was positioned as the leader in Gartner Magic Quadrant for International Retail Core Banking in November 2014, ahead of SAP and Temenos, demonstrating the highest Ability to Execute, and Completeness of Vision.\n\nIDC Financial Insights released in its new IDC MarketScape report, and noted Infosys-Finacle as a \"leader\". The solution is considered to exhibit high levels of strategic vision, demonstrable an ability to deliver functionality and support to their clients. The recent signing of new contracts in Europe indicated that the vendor was able to overcome the hurdles of regulatory requirements and budgeting constraints for core banking systems in Europe.\n\nFinacle Mobile Banking and e-Banking solutions were rated as ‘Best-in-Class’ for security and authentication capabilities and enterprise support, in a report published by CEB TowerGroup analysts\n\nFinacle was recognized as a leader among Omnichannel Banking Solutions. Six top omnichannel banking platforms were evaluated on a comprehensive set of 41 criteria focusing on strategy, current offering, and market presence. Finacle also achieved the highest score amongst all six vendors on the strategy and market presence criteria.\n\n"}
{"id": "41076137", "url": "https://en.wikipedia.org/wiki?curid=41076137", "title": "Frank W. Weston", "text": "Frank W. Weston\n\nFrank W. Weston (1843–1911) was an English-born and trained architect who practiced in Portland, Maine and Boston, Massachusetts. He also invested in the bicycle industry and promoted cycling as a sport. He was the co-founder of the Boston Bicycling Club is known as the \"father of American bicycling.\"\n\nFrank Weston was born July 13, 1843, in the Oxford Terrace, England. He studied at private schools before training as an architect. He emigrated from England to the United States, arriving in Boston on June 1, 1866.\n\nWeston went to work in Boston for William Ralph Emerson in the Studio building. He relocated to Portland, Maine after a fire in that city on July 4, 1866. He assisted with the rebuilding of the city. He remained in Portland for two years before returning to Boston, where he worked for N.J. Bradlee. He started his own firm in 1869, and in 1870, formed a partnership with George Rand. The firm of Weston and Rand designed the Hotel Agassiz at 191 Commonwealth Avenue (1872) and a building at 270 Clarendon Street (1873). Weston worked on many homes on the Boston Back Bay, and he built a house for himself at Savin Hill. He lived there with his wife, whom he married in 1873.\n\nWeston worked as an architect out of Malden and Boston. He designed the Essex Town Hall and TOHP Burnham Library at 30 Martin Street in Essex, Massachusetts, built in the Shingle style of architecture. It is listed on the National Register of Historic Places. He also designed the Massachusetts Insane Hospital in Worcester, the New England Telephone & Telegraph Company exchange building on Oxford Street in Lynn, Massachusetts, and a \"boxy Queen Anne style house.\"\n\nIn 1877, Frank Weston formed a business partnership with Arthur Cunningham, Sidney Heath, and Harold Williams to import bicycles to the United States. The new firm, Cunningham, Heath, and Company, started a bicycle riding school when it received its first order of high-wheelers from England. Weston withdrew from active management of the firm and founded the \"American Bicycling Journal\", which he used to promote the sport of cycling and the interests of the firm. First issued Dec. 22, 1877, it was the first periodical of its kind in the United States. Weston sold the magazine in 1879 to Charles Pratt and Edward C. \"Ned\" Hodges.\n\nIn February 1878, Weston and thirteen fellow cyclists launched the Boston Bicycle Club, the first club of its kind in the United States. Weston was also the first and only American of the Cyclists' Touring Club of England, originally the Bicycle Touring Club. He organized races, one hundred-mile bicycle club runs and \"the first hundred mile tricycle club run\" in the United States. Weston served as first vice-president of the League of American Wheelmen and president of the Pioneers of the League of American Wheelmen.\n\nWeston died at the age of 67 in 1911. His wishes were for his body to be cremated and his ashes mixed with those of his wife. In 1926 it was discovered that this had not happened, and that his ashes were still in the undertaker's storage room. The members of the Boston Bicycling Club could not located his wife's ashes, but decided to scatter his at the Fairbanks House in Dedham, Massachusetts, a favorite spot of the club's, under a tree that had been planted there on the occasion of the club's 40th anniversary.\n"}
{"id": "17557473", "url": "https://en.wikipedia.org/wiki?curid=17557473", "title": "Georg Friedrich Strass", "text": "Georg Friedrich Strass\n\nGeorg Friedrich Strass (; 29 May 1701, Wolfisheim near Strasbourg - 22 December 1773) was an Alsatian jeweler and inventor of imitation gemstones. He is best known as the inventor of the rhinestone, called \"strass\" in many European languages, from a particular type of crystal he found in the river Rhine.\n\nHe used mixtures of bismuth and thallium to improve the refractive quality of his imitations, and altered their colors with metal salts. The imitations were, in his view, so similar to real gems that he invented the concept of the \"simulated gemstone\" to describe them. He considerably improved his gems' brilliance by gluing metal foil behind them. This foil was later replaced with a vapor-deposited mirror coating.\n\nStrass opened his own business in 1730, and devoted himself wholly to the development of imitation diamonds. Due to his great achievements, he was awarded the title \"King's Jeweler\" in 1734.\n\nHe was a partner in the jewellery business of Madame Prévot. He continued improving his artificial gemstones during this time. His work was in great demand at the court of King Louis XV of France, and he controlled a large market for artificial gems.\n\nWealthy through his businesses, he was able to retire comfortably at age 52.\n"}
{"id": "13305402", "url": "https://en.wikipedia.org/wiki?curid=13305402", "title": "Global brain", "text": "Global brain\n\nThe global brain is a neuroscience-inspired and futurological vision of the planetary information and communications technology network that interconnects all humans and their technological artifacts. As this network stores ever more information, takes over ever more functions of coordination and communication from traditional organizations, and becomes increasingly intelligent, it increasingly plays the role of a brain for the planet Earth.\n\nProponents of the global brain hypothesis claim that the Internet increasingly ties its users together into a single information processing system that functions as part of the collective nervous system of the planet. The intelligence of this network is collective or distributed: it is not centralized or localized in any particular individual, organization or computer system. Therefore, no one can command or control it. Rather, it self-organizes or emerges from the dynamic networks of interactions between its components. This is a property typical of complex adaptive systems.\n\nThe World-wide web in particular resembles the organization of a brain with its webpages (playing a role similar to neurons) connected by hyperlinks (playing a role similar to synapses), together forming an associative network along which information propagates. This analogy becomes stronger with the rise of social media, such as Facebook, where links between personal pages represent relationships in a social network along which information propagates from person to person.\nSuch propagation is similar to the spreading activation that neural networks in the brain use to process information in a parallel, distributed manner.\n\nAlthough some of the underlying ideas were already expressed by Nikola Tesla in the late 19th century and were written about by many others before him, the term “global brain” was coined in 1982 by Peter Russell in his book \"The Global Brain\". How the Internet might be developed to achieve this was set out in 1986. The first peer-reviewed article on the subject was published by Gottfried Mayer-Kress in 1995, while the first algorithms that could turn the world-wide web into a collectively intelligent network were proposed by Francis Heylighen and Johan Bollen in 1996.\n\nReviewing the strands of intellectual history that contributed to the global brain hypothesis, Francis Heylighen distinguishes four perspectives: \"“organicism”\", \"“encyclopedism”\", \"“emergentism”\" and \"“evolutionary cybernetics”\". He asserts that these developed in relative independence but now are converging in his own scientific re-formulation.\n\nIn the 19th century, the sociologist Herbert Spencer saw society as a social organism and reflected about its need for a nervous system. Entomologist William Wheeler developed the concept of the ant colony as a spatially extended organism, and in the 1930s he coined the term superorganism to describe such an entity. This concept was later adopted by thinkers such as Gregory Stock in his book Metaman and Joel de Rosnay to describe planetary society as a superorganism.\n\nThe mental aspects of such an organic system at the planetary level were perhaps first broadly elaborated by palaeontologist and Jesuit priest Pierre Teilhard de Chardin. In 1945, he described a coming “planetisation” of humanity, which he saw as the next phase of accelerating human “socialisation”. Teilhard described both socialization and planetization as irreversible, irresistible processes of \"macrobiological development\" culminating in the emergence of a noosphere, or global mind (see Emergentism below).\n\nThe more recent living systems theory describes both organisms and social systems in terms of the \"critical subsystems\" (\"organs\") they need to contain in order to survive, such as an internal transport system, a resource reserve, and a decision-making system. This theory has inspired several thinkers, including Peter Russell and Francis Heylighen to define the global brain as the network of information processing subsystems for the planetary social system.\n\nIn the perspective of encyclopedism, the emphasis is on developing a universal knowledge network. The first systematic attempt to create such an integrated system of the world's knowledge was the 18th century \"Encyclopédie\" of Denis Diderot and Jean le Rond d'Alembert. However, by the end of the 19th century, the amount of knowledge had become too large to be published in a single synthetic volume. To tackle this problem, Paul Otlet founded the science of documentation, now called information science. In the 1930s he envisaged a World Wide Web-like system of associations between documents and telecommunication links that would make all the world's knowledge available immediately to anybody. H. G. Wells proposed a similar vision of a collaboratively developed world encyclopedia that would be constantly updated by a global university-like institution. He called this a World Brain, as it would function as a continuously updated memory for the planet, although the image of humanity acting informally as a more organic global brain is a recurring motif in other of his works.\n\nTim Berners-Lee, the inventor of the World Wide Web, too, was inspired by the free-associative possibilities of the brain for his invention. The brain can link different kinds of information without any apparent link otherwise; Berners-Lee thought that computers could become much more powerful if they could imitate this functioning, i.e. make links between any arbitrary piece of information. The most powerful implementation of encyclopedism to date is Wikipedia, which integrates the associative powers of the world-wide-web with the collective intelligence of its millions of contributors, approaching the ideal of a global memory. The Semantic web, also first proposed by Berners-Lee, is a system of protocols to make the pieces of knowledge and their links readable by machines, so that they could be used to make automatic inferences, thus providing this brain-like network with some capacity for autonomous \"thinking\" or reflection.\n\nThis approach focuses on the emergent aspects of the evolution and development of complexity, including the spiritual, psychological, and moral-ethical aspects of the global brain, and is at present the most speculative approach. The global brain is here seen as a natural and emergent process of planetary evolutionary development. Here again Pierre Teilhard de Chardin attempted a synthesis of science, social values, and religion in his The Phenomenon of Man, which argues that the \"telos\" (drive, purpose) of universal evolutionary process is the development of greater levels of both complexity and consciousness. Teilhard proposed that if life persists then planetization, as a biological process producing a global brain, would necessarily also produce a global mind, a new level of planetary consciousness and a technologically supported network of thoughts which he called the \"noosphere\". Teilhard's proposed technological layer for the noosphere can be interpreted as an early anticipation of the Internet and the Web.\n\nPhysicist and philosopher Peter Russell elaborates a similar view, and stresses the importance of personal spiritual growth, in order to build and to achieve synergy with the spiritual dimension of the emerging superorganism. This approach is most popular in New Age circles, which emphasize growth in consciousness rather than scientific modelling or the implementation of technological and social systems.\n\nSystems theorists and cyberneticists commonly describe the emergence of a higher order system in evolutionary development as a “metasystem transition” (a concept introduced by Valentin Turchin) or a “major evolutionary transition”. Such a metasystem consists of a group of subsystems that work together in a coordinated, goal-directed manner. It is as such much more powerful and intelligent than its constituent systems. Francis Heylighen has argued that the global brain is an emerging metasystem with respect to the level of individual human intelligence, and investigated the specific evolutionary mechanisms that promote this transition\n\nIn this scenario, the Internet fulfils the role of the network of “nerves” that interconnect the subsystems and thus coordinates their activity. The cybernetic approach makes it possible to develop mathematical models and simulations of the processes of self-organization through which such coordination and collective intelligence emerges.\n\nIn 1994 Kevin Kelly, in his popular book \"\", posited the emergence of a \"hive mind\" from a discussion of cybernetics and evolutionary biology.\n\nIn 1996, Francis Heylighen and Ben Goertzel founded the Global Brain group, a discussion forum grouping most of the researchers that had been working on the subject of the global brain to further investigate this phenomenon. The group organized the first international conference on the topic in 2001 at the Vrije Universiteit Brussel.\n\nAfter a period of relative neglect, the Global Brain idea has recently seen a resurgence in interest, in part due to talks given on the topic by Tim O'Reilly, the Internet forecaster who popularized the term Web 2.0, and Yuri Milner, the social media investor. In January 2012, the Global Brain Institute (GBI) was founded at the Vrije Universiteit Brussel to develop a mathematical theory of the “brainlike” propagation of information across the Internet. In the same year, Thomas W. Malone and collaborators from the MIT Center for Collective Intelligence have started to explore how the global brain could be “programmed” to work more effectively, using mechanisms of collective intelligence. The complexity scientist Dirk Helbing and his NervousNet group have recently started developing a \"Planetary Nervous System\", which includes a \"Global Participatory Platform\", as part of the large-scale FuturICT project, thus preparing some of the groundwork for a Global Brain.\n\nIn July 2017 Elon Musk founded the company Neuralink, which aims to create a Neural Lace, which is a concept invented by the novelist Iain M. Banks and basically refers to a machine interface woven into the brain, to allow the user to access all available human information. A core driver behind this business idea is Mr Musk’s argument, that human beings soon have to embrace brain implants to stay relevant in a world which, he believes, will soon be dominated by artificial intelligence. The firm raised $27m from 12 Investors in 2017 .\n\nA common criticism of the idea that humanity would become directed by a global brain is that this would reduce individual diversity and freedom, and lead to mass surveillance. This criticism is inspired by totalitarian forms of government, as exemplified by George Orwell's character of \"Big Brother\". It is also inspired by the analogy between collective intelligence or swarm intelligence and insect societies, such as beehives and ant colonies, in which individuals are essentially interchangeable. In a more extreme view, the global brain has been compared with the Borg, the race of collectively thinking cyborgs conceived by the Star Trek science fiction franchise.\n\nGlobal brain theorists reply that the emergence of distributed intelligence would lead to the exact opposite of this vision. The reason is that effective collective intelligence requires diversity of opinion, decentralization and individual independence, as demonstrated by James Surowiecki in his book The Wisdom of Crowds. Moreover, a more distributed form of decision-making would decrease the power of governments, corporations or political leaders, thus increasing democratic participation and reducing the dangers of totalitarian control.\n\n\n\n\nFor more references, check the GBI bibliography:\n\n"}
{"id": "42438345", "url": "https://en.wikipedia.org/wiki?curid=42438345", "title": "Goražde printing house", "text": "Goražde printing house\n\nThe Goražde printing house ( or ) was one of the earliest printing houses among the Serbs, and the first in the territory of present-day Bosnia and Herzegovina (then part of the Ottoman Empire). Established in 1519 in Venice, it was soon relocated to the Serbian Orthodox Church of Saint George in the village of Sopotnica near Goražde, in the Ottoman Sanjak of Herzegovina. It was founded and run by Božidar Ljubavić, also known as Božidar Goraždanin, who was a prominent merchant from Goražde. His son Teodor Ljubavić, a hieromonk of the Mileševa Monastery, managed the work of the printing house. It worked until 1523, producing three books, which are counted among the better accomplishments of early Serb printers.\n\nAfter the printing press was invented around 1450 by Johannes Gutenberg in Mainz, Germany, the art of book printing was soon introduced in other parts of Europe. By the end of the 15th century, Venice had become a major centre of printing. In 1493, Đurađ Crnojević, the ruler of the Principality of Zeta (in present-day Montenegro), sent Hieromonk Makarije to Venice to buy a press and learn the art of printing. At Cetinje, the capital of Zeta, Makarije printed in 1494 the Cetinje Octoechos, the first incunable written in the Serbian recension of Church Slavonic. The Crnojević printing house worked until 1496, when Zeta fell to the Ottomans. In 1518, Božidar Ljubavić resided at the Mileševa Monastery, the see of a Serbian Orthodox diocese which had been part of the Kingdom of Bosnia since 1373. Mileševa and other parts of its diocese, including the town of Goražde, were located in the region of Herzegovina, which was gradually conquered by the Ottomans between 1465 and 1481.\n\nIn the second half of 1518, Božidar Ljubavić sent his sons, Đurađ and hieromonk Teodor, to Venice to buy a printing press and to learn the art of printing. The Ljubavić brothers procured a press and began printing a hieratikon (priest's service book), copies of which had been completed by 1 July 1519 either in Venice or at the Church of Saint George near Goražde. After Đurađ Ljubavić died in Venice on 2 March 1519, it is unclear whether his brother transported the press to Goražde before or after finishing the work on the hieratikon. At the Church of Saint George, Teodor organised the Goražde printing house, which produced, beside the hieratikon, two more books in Church Slavonic of the Serbian recension: a psalter in 1521, and a small euchologion in 1523. The Goražde Psalter, containing 352 leaves, is the biggest of the three books. They were not bound at the printing house, as this job was a responsibility of book vendors. Trade was well developed in Goražde, as the town was built at the junction of three important roads, which connected it with Dubrovnik, Vrhbosna (Sarajevo), and Kosovo.\n\nThe next printing house would not appear in Bosnia and Herzegovina until 1866, when Sopron's Printing House began its work. In 1544, the printing press was transported from Goražde to Târgoviște, the capital of Wallachia, thus becoming the second such facility in the territory of present-day Romania. Its relocation and reactivation was accomplished by Dimitrije Ljubavić, Božidar's grandson. In Târgoviște, Dimitrije printed a euchologion at the beginning of 1545, and an apostolarium in 1547. Božidar Vuković founded his printing house in Venice in 1519 or 1520, contemporaneously with the Ljubavić brothers. It worked, with interruptions, until the end of the 16th century. There were other early Serbian printing works, established in the territory of the Ottoman Empire: at the Rujan Monastery near Užice in 1529, at the Gračanica Monastery near Priština in 1539, at the Mileševa Monastery in 1546, in Belgrade in 1552, again at Mileševa in 1557, at the Mrkšina Crkva Monastery near Valjevo in 1562, and in Skadar in 1563. They were active for one to four years and produced one to three books each.\n\n\n"}
{"id": "9465204", "url": "https://en.wikipedia.org/wiki?curid=9465204", "title": "Guard tour patrol system", "text": "Guard tour patrol system\n\nA guard tour patrol system is a system for logging the rounds of employees in a variety of situations such as security guards patrolling property, technicians monitoring climate-controlled environments, and correctional officers checking prisoner living areas. It helps ensure that the employee makes his or her appointed rounds at the correct intervals and can offer a record for legal or insurance reasons. Such systems have existed for many years using mechanical watchclock-based systems (watchman clocks/guard tour clocks/patrol clocks). Computerized systems were first introduced in Europe in the early 1980s, and in North America in 1986. Modern systems are based on handheld data loggers and RFID sensors.\nThe system provides a means to record the time when the employee reaches certain points on their tour. Checkpoints or watchstations are commonly placed at the extreme ends of the tour route and at critical points such as vaults, specimen refrigerators, vital equipment, and access points. Some systems are set so that the interval between stations is timed so if the employee fails to reach each point within a set time, other staff are dispatched to ensure the employee's well-being.\nAn example of a modern set-up might work as follows: the employee carries a portable electronic sensor (PES) or electronic data collector which is activated at each checkpoint. Checkpoints can consist of iButton semiconductors, magnetic strips, proximity microchips such as RFIDs or NFC- or optical barcodes. The data collector stores the serial number of the checkpoint with the date and time. Later, the information is downloaded from the collector into a computer where the checkpoint's serial number will have an assigned location (i.e. North Perimeter Fence, Cell Number 1, etc.). Data collectors can also be programmed to ignore duplicate checkpoint activations that occur sequentially or within a certain time period. Computer software used to compile the data from the collector can print out summaries that pinpoint missed checkpoints or patrols without the operator having to review all the data collected. Because devices can be subject to misuse, some have built-in microwave, g-force, and voltage detection.\n\nIt combines readers, tags and software.\n\nThe first Guard tour system were the touch readers with software. Upon further development, more working modes for the readers became available. Such as RFID and GPS. And the communication of readers and software was connected with USB cables or download stations. For USB connection, the \"Pogo Pin\" connection is very popular. Because the contacts with gold-plating are very stable and waterproof.\n\nNewer, light-weight guard touring systems utilize QR codes or barcodes rather than expensive electronic components. A mobile phone app is used to scan (take a photo) of the QR code which creates a time stamp in the system.\n\nThe reader needs to read the tags to record the information, such as the time and tag's ID. Then upload the information to software to get the report.\n\nThere are three types of guard patrol software. They are desktop, local network client-server, and web-based versions. \nThe desktop version can only work on one computer.\nThe local network client server type can work using the local area network.\nThe web-based version can work everywhere with internet access.\nIn the analog age, the device used for this purpose was the watchclock. Watchclocks often had a paper or light cardboard disk placed inside for each 24-hour period. The user would carry the clock to each checkpoint, where a numbered key could be found (typically chained in place). The key would be inserted into the clock where it would imprint the disk. At the end of the shift or 24-hour period an authorized person (usually a supervisor) would unlock the watchclock and retrieve the disk.\nAs development of guard tour system, the device can work with more functions. Such as send data real-time by GPRS to software and GPS location and tracking mode.\nIn software, we set up the Patrol Department, Patrol Route, Guard, Checkpoint, Event and Patrol Plan in general, depending on the software purchased. The software will then have specific tours set for officers to complete, being able to indicate whether the inspection was completed properly or not, with the ability to note a specific temperature of an inspection, or make any kind of notes necessary. Guard Tour software systems seem to be becoming the norm in tracking tours for officers. Examples of this software would be the Guard Tour System by Silvertrac Software, SequriX, Trackforce, QR-Patrol, Kugadi, GigaTrak TrackTik, PatrolLIVE, Orna, Guarnic. GuardMetrics and Youtility\n\nNew touring solutions rely on cloud-based Software as a Service (SaaS) combined with mobile or fixed on-site devices. These offer the advantages of lower installation and maintenance costs, forgoing the need for hardware, software upgrades, data backups and computer maintenance. On-site systems need all the usual software patches, backups and periodic hardware replacement. In operation, the role of the watchclock system, described above, has largely been replaced by some combination of GPS, RFID/NFC, or QR coded labels. Users prove that they have visited particular locations or performed tasks by scanning these tags or via GPS generated maps. These technologies result in lower costs, while increasing the flexibility of the systems to handle changes or new uses. This is important when routes change, or if a solution is needed on short notice. Tag-based touring systems typically utilize a mobile phone or tablet app to scan the tags and then upload that information along with a time stamp, phone's location information, and optionally other information the guard enters into the app on the phone. These systems provide instant access to tour information as it is uploaded by the application or device carried by the user, rather than requiring the officer to return to an upload station.\n\nAlthough this technology was initially developed for the security market, there are other uses. Some include:\n\n\nFor routes which have significant outdoor exposure GPS units have proven to be an effective means of tracking security and law enforcement patrol behavior. GPS systems do not function in the most vulnerable areas such as indoors or underground. Accordingly, systems using assisted GPS have been developed.\n\n"}
{"id": "33758261", "url": "https://en.wikipedia.org/wiki?curid=33758261", "title": "HP Performance Optimized Datacenter", "text": "HP Performance Optimized Datacenter\n\nThe HP Performance Optimized Datacenter (POD) is a range of three modular data centers manufactured by HP.\n\nHoused in purpose-built modules of standard shipping container form-factor of either 20 feet or 40 feet in length the data centers are shipped preconfigured with racks, cabling and equipment for power and cooling. They can support technologies from HP or third parties. The claimed capacity is the equivalent of up to 10,000 square feet of typical data center capacity depending on the model. \nDepending on the model, they use either chilled water cooling or a combination of direct expansion air cooling.\n\nThe POD 40c was launched in 2008. This 40-foot modular data center has a maximum power capacity up to 27 kW per rack. The POD 40c supports 3,500 compute nodes or 12,000 LFF hard drives. HP has claimed this offers the computing equivalent of 4,000 square foot of traditional data center space.\n\nThe POD 20c was launched in 2010. This modular data center is housed in a 20-foot container. This version houses 10 industry-standard 50U racks of hardware. The POD uses an efficient cooling design of variable speed fans, hot and cold aisle containment and close coupled cooling to maximize capacity and efficiency. The POD 20c can operate at a Power Usage Effectiveness of 1.25. PODs can maintain cold aisle temperatures higher than typical brick and mortar data centers. The temperature of the cold aisle in traditional data centers is typically 68 to 72 degrees, whereas the POD can efficiently operate at temperatures in this range up to 90 degrees.\n\nBoth the 20c and the 40c are water-cooled. The benefit of water cooling is higher capacity and less power usage than traditional air-cooled systems.\n\nThe HP POD 240a was launched in June 2011. It can be configured with two rows of 44 extra height 50U racks that could house 4,400 server nodes of typical size, or 7,040 server nodes of the densest size. HP claimed that the typical brick and mortar data center required to house this equipment would be 10,000 square feet of floor space.\n\nHP claims \"near-perfect\" Energy efficiency for the POD 240a, which it nicknames the \"EcoPOD\". HP says it has recorded estimated Power Usage Effectiveness (PUE) ratios of 1.05 to 1.4, depending on IT load and location. The perfect efficiency would be a Power usage effectiveness (PUE) of 1.0.\n\nThe POD 240a has a refrigerant-based air cooled HVAC system with air-side economization When the ambient air conditions are cool enough, the 240a uses economizer or free air mode—where outside air can be taken in and circulated inside the modular data center to cool the IT equipment.\n\nIn September 2013, Ebay announced that they were \"deploying the world’s largest modular data center, with 44 rack positions and 1.4Megawatts of power\" using HP EcoPODs.\n\n\n"}
{"id": "56358777", "url": "https://en.wikipedia.org/wiki?curid=56358777", "title": "Hardware-based encryption", "text": "Hardware-based encryption\n\nHardware-based encryption is the use of computer hardware to assist software, or sometimes replace software, in the process of data encryption. Typically, this is implemented as part of the processor's instruction set. For example, the AES encryption algorithm (a modern cipher) can be implemented using the AES instruction set on the ubiquitous x86 architecture. Such instructions also exist on the ARM architecture. However, more unusual systems exist where the cryptography module is separate from the central processor, instead being implemented as a coprocessor, in particular a secure cryptoprocessor or cryptographic accelerator, of which an example is the IBM 4758, or its successor, the IBM 4764. Hardware implementations can be faster and less prone to exploitation than traditional software implementations, and furthermore can be protected against tampering. \nPrior to the use of computer hardware, cryptography could be performed through various mechanical or electro-mechanical means. An early example is the Scytale used by the Spartans. The Enigma machine was an electro-mechanical system cipher machine notably used by the Germans in World War II. After World War II, purely electronic systems were developed. In 1987 the ABYSS (A Basic Yorktown Security System) project was initiated. The aim of this project was to protect against software piracy. However, the application of computers to cryptography in general dates back to the 1940s and Bletchley Park, where the Colossus computer was used to break the encryption used by German High Command during World War II. The use of computers to \"encrypt\", however, came later. In particular, until the development of the integrated circuit, of which the first was produced in 1960, computers were impractical for encryption, since, in comparison to the portable form factor of the Enigma machine, computers of the era took the space of an entire building. It was only with the development of the microcomputer that computer encryption became feasible, outside of niche applications. The development of the World Wide Web lead to the need for consumers to have access to encryption, as online shopping became prevalent. The key concerns for consumers were security and speed. This led to the eventual inclusion of the key algorithms into processors as a way of both increasing speed and security.\n\nThe X86 architecture, as a CISC (Complex Instruction Set Computer) Architecture, typically implements complex algorithms in hardware. Cryptographic algorithms are no exception. The x86 architecture implements significant components of the AES (Advanced Encryption Standard) algorithm, which can be used by the NSA for Top Secret information. The architecture also includes support for the SHA Hashing Algorithms through the Intel SHA extensions. Whereas AES is a cipher, which is useful for encrypting documents, hashing is used for verification, such as of passwords (see PBKDF2).\n\nARM processors can optionally support Security Extensions. Although ARM is a RISC (Reduced Instruction Set Computer) architecture, there are several optional extensions specified by ARM Holdings.\n\nAdvanced Micro Devices (AMD) processors are also x86 devices, and have supported the AES instructions since the 2011 Bulldozer processor iteration. \nDue to the existence of encryption instructions on modern processors provided by both Intel and AMD, the instructions are present on most modern computers. They also exist on many tablets and smartphones due to their implementation in ARM processors.\n\nImplementing cryptography in hardware means that part of the processor is dedicated to the task. This can lead to a large increase in speed. In particular, modern processor architectures that support pipelining can often perform other instructions concurrently with the execution of the encryption instruction. Furthermore, hardware can have methods of protecting data from software. Consequently, even if the operating system is compromised, the data may still be secure (see Software Guard Extensions).\n\nIf, however, the hardware implementation is compromised, major issues arise. Malicious software can retrieve the data from the (supposedly) secure hardware – a large class of method used is the timing attack. This is far more problematic to solve than a software bug, even within the operating system. Microsoft regularly deals with security issues through Windows Update. Similarly, regular security updates are released for Mac OS X and Linux, as well as mobile operating systems like iOS, Android, and Windows Phone. However, hardware is a different issue. Sometimes, the issue will be fixable through updates to the processor's microcode (a low level type of software). However, other issues may only be resolvable through replacing the hardware, or a workaround in the operating system which mitigates the performance benefit of the hardware implementation, such as in the Spectre exploit.\n\n"}
{"id": "4168072", "url": "https://en.wikipedia.org/wiki?curid=4168072", "title": "Information ethics", "text": "Information ethics\n\nInformation ethics has been defined as \"the branch of ethics that focuses on the relationship between the creation, organization, dissemination, and use of information, and the ethical standards and moral codes governing human conduct in society\". The term \"information ethics\" was first coined by Robert Hauptman and used in the book \"Ethical challenges in librarianship\". It examines the morality that comes from information as a resource, a product, or as a target. It provides a critical framework for considering moral issues concerning informational privacy, moral agency (e.g. whether artificial agents may be moral), new environmental issues (especially how agents should behave in the infosphere), problems arising from the life-cycle (creation, collection, recording, distribution, processing, etc.) of information (especially ownership and copyright, digital divide, and digital rights). It is very vital to understand that librarians, archivists, information professionals among others, really understand the importance of knowing how to disseminate proper information as well as being responsible with their actions when addressing information.\n\nInformation ethics has evolved to relate to a range of fields such as computer ethics, medical ethics, journalism and the philosophy of information.\n\nDilemmas regarding the life of information are becoming increasingly important in a society that is defined as \"the information society\". The explosion of so much technology has brought information ethics to a forefront in ethical considerations. Information transmission and literacy are essential concerns in establishing an ethical foundation that promotes fair, equitable, and responsible practices. Information ethics broadly examines issues related to ownership, access, privacy, security, and community. It is also concerned with relational issues such as \"the relationship between information and the good of society, the relationship between information providers and the consumers of information\".\n\nInformation technology affects common issues such as copyright protection, intellectual freedom, accountability, privacy, and security. Many of these issues are difficult or impossible to resolve due to fundamental tensions between Western moral philosophies (based on rules, democracy, individual rights, and personal freedoms) and the traditional Eastern cultures (based on relationships, hierarchy, collective responsibilities, and social harmony). The multi-faceted dispute between Google and the government of the People's Republic of China reflects some of these fundamental tensions.\n\nProfessional codes offer a basis for making ethical decisions and applying ethical solutions to situations involving information provision and use which reflect an organization's commitment to responsible information service. Evolving information formats and needs require continual reconsideration of ethical principles and how these codes are applied. Considerations regarding information ethics influence \"personal decisions, professional practice, and public policy\". Therefore, ethical analysis must provide a framework to take into consideration \"many, diverse domains\" (ibid.) regarding how information is distributed.\n\nThe main, peer-reviewed, academic journals reporting on information ethics are the \"Journal of the Association for Information Systems\", the flagship publication of the Association for Information Systems, and \"Ethics and Information Technology\", published by Springer.\n\nThe field of information ethics has a relatively short but progressive history having been recognized in the United States for nearly 20 years. The origins of the field are in librarianship though it has now expanded to the consideration of ethical issues in other domains including computer science, the internet, media, journalism, management information systems, and business.\n\nEvidence of scholarly work on this subject can be traced to the 1980s, when an article authored by Barbara J. Kostrewski and Charles Oppenheim and published in the \"Journal of Information Science\", discussed issues relating to the field including confidentiality, information biases, and quality control. Another scholar, Robert Hauptman, has also written extensively about information ethics in the library field and founded the \"Journal of Information Ethics\" in 1992.\n\nCensorship is an issue commonly involved in the discussion of information ethics because it describes the inability to access or express opinions or information based on the belief it is bad for others to view this opinion or information. Sources that are commonly censored include books, articles, speeches, art work, data, music and photos. Censorship can be perceived both as ethical and non-ethical in the field of information ethics.\n\nThose who believe censorship is ethical say the practice prevents readers from being exposed to offensive and objectionable material. Topics such as sexism, racism, homophobia, and anti-semitism are present in public works and are widely seen as unethical in the public eye. There is concern regarding the exposure of these topics to the world, especially the young generation. The Australian Library Journal states proponents for censorship in libraries, the practice of librarians deciphering which books/ resources to keep in their libraries, argue the act of censorship is an ethical way to provide information to the public that is considered morally sound, allowing positive ethics instead of negative ethics to be dispersed. According to the same journal, librarians have an \"ethical duty\" to protect the minds, particularly young people, of those who read their books through the lens of censorship to prevent the readers from adopting the unethical ideas and behaviors portrayed in the books.\n\nHowever, others in the field of information ethics argue the practice of censorship is unethical because it fails to provide all available information to the community of readers. British philosopher John Stuart Mill argued censorship is unethical because it goes directly against the moral concept of utilitarianism. Mill believes humans are unable to have true beliefs when information is withheld from the population via censorship and acquiring true beliefs without censorship leads to greater happiness. According to this argument, true beliefs and happiness (of which both concepts are considered ethical) cannot be obtained through the practice of censorship. Librarians and others who disperse information to the public also face the dilemma of the ethics of censorship through the argument that censorship harms students and is morally wrong because they are unable to know the full extent of knowledge available to the world. The debate of information ethics in censorship was highly contested when schools removed information about evolution from libraries and curriculums due to the topic conflicting with religious beliefs. In this case, advocates against ethics in censorship argue it is more ethical to include multiple sources information on a subject, such as creation, to allow the reader to learn and decipher their beliefs.\n\nEthical concerns regarding international security, surveillance, and the right to privacy are on the rise. The issues of security and privacy commonly overlap in the field of information, due to the interconnectedness of online research and the development of Information Technology (IT). Some of the areas surrounding security and privacy are identity theft, online economic transfers, medical records, and state security. Companies, organizations, and institutions use databases to store, organize, and distribute user's information—with or without their knowledge.\n\nThe more recent trend of medical records is to digitize them. The sensitive information secured within medical records makes security measure vitally important. The ethical concern of medical records is great within the context of emergency wards, where any patient records can be accessed at all times. Within an emergency ward, patient medical records need to be available for quick access; however, this means that all medical records can be accessed at any moment within emergency wards with or without the patient present.\n\nWarfare has also changed the security of countries within the 21st Century. After the events of 9-11 and other terrorism attacks on civilians, surveillance by states raises ethical concerns of the individual privacy of citizens. The USA PATRIOT Act 2001 is a prime example of such concerns. Many other countries, especially European nations within the current climate of terrorism, is looking for a balancing between stricter security and surveillance, and not committing the same ethical concerns associated with the USA Patriot Act. International security is moving to towards the trends of cybersecurity and unmanned systems, which involve the military application of IT. Ethical concerns of political entities regarding information warfare include the unpredictability of response, difficulty differentiating civilian and military targets, and conflict between state and non-state actors.\n\n\n\n"}
{"id": "8096738", "url": "https://en.wikipedia.org/wiki?curid=8096738", "title": "Ionics EMS", "text": "Ionics EMS\n\nIonics EMS, Inc. Ionics is a full-service Electronics Manufacturing Services provider with a full spectrum of Manufacturing know how supported by in-house Fabrication of Materials, Engineering Design, Original Design Management Development, Supply Chain Management and Corporate Investments; Schematic Design, PCB Layout, Firmware Integration, New Product Introduction, Industrial Engineering capabilities.\n\nIonics is publicly listed in the Philippine Stock Exchange having 8 new facilities. Ionics is equipped with the latest high speed SMT equipment and features Smart Factory Solutions for mid and high volume assembly. With over 4 decades of experience in EMS,it is considered the oldest and most experienced electronics manufacturing services provider in the Philippines joined by its NPI house in Santa Clara, California.Ionics has worked with a range of world-class Original Equipment Manufacturers from different industries such as Telecommunication, Industrial, Automotive, Medical, Consumer Electronics, Computer and Computer Peripherals. \n\nThe only Filipino firm listed on the Philippine and Singapore Stock Exchanges\n\nIn September 2009, Ionics entered into a strategic IP licensing agreement with Norwegian fingerprint biometrics company IDEX ASA. The agreement provides Ionics the rights to manufacturing and sales of fingerprint sensor products based on IDEX’s patented SmartFinger technology.\n\nIn December 2015, a UNIT of electronics manufacturer Ionics Inc. has teamed up with global technology giant IBM to develop high-tech products that ride on the Internet of things (IoT) using cloud, mobile and big data analytics. Ionics-EMS will work with IBM to build a platform for the development of cloud-based apps for clients on Bluemix via the IBM Internet of Things Foundation.\n\n"}
{"id": "1605951", "url": "https://en.wikipedia.org/wiki?curid=1605951", "title": "Limelight Department", "text": "Limelight Department\n\nThe Limelight Department was one of the world's first film studios, beginning in 1898, operated by The Salvation Army in Melbourne, Australia. The Limelight Department produced evangelistic material for use by the Salvation Army, including lantern slides as early as 1891, as well as private and government contracts. In its 19 years of operation, the Limelight Department produced about 300 films of various lengths, making it one of largest film producers of its time.\n\nThe Salvation Army Limelight Department unofficially started in 1891, when Adjutant Joseph Perry started a photographic studio in Ballarat, Victoria, to supplement the income of the Salvation Army's Prison Gate Home. At the time, Perry was on compassionate leave from active ministry, as his wife Annie had died earlier that year, leaving Perry to raise their three children. In September 1891, Perry was temporarily reassigned to the Australasian Headquarters in Melbourne to assist Australasian commander, Commissioner Thomas Coombs, in putting together a presentation of General William Booth's \"In Darkest England\" program. At this stage, Perry was using lantern slides which projected hand coloured photographs onto a large screen. Coombs was impressed by the quality and effectiveness of presentation, making Perry's move to Melbourne permanent. The Limelight Department was officially established on 11 June 1892.\nIn 1896, when Commissioner Coombs was replaced as Australasian commander by General Booth’s youngest son, 'Commandant Herbert Booth. Booth immediately warmed to the innovation of the Limelight Department, giving Perry the freedom and the financial support to expand into the newly developing medium of film. Under Booth’s direction, Perry started work on \"Social Salvation\" in 1898, one of the first presentations of its type to integrate the traditional lantern slides with film segments. On 20 December 1899, the Limelight Department premiered a series on the \"Passion\" at the Collingwood corps. The presentation contained thirteen, ninety second sections which portrayed the life of Jesus from birth to death. The presentation was similar in style to that produced by the Lumiere Company earlier that year, however, as none of the original film remains, it can never be determined if the Limelight Department used Lumiere footage in the presentation.\n\nThe major innovation of the Limelight Department would come in 1899 when Booth and Perry began work on \"Soldiers of the Cross\", arguably the first feature-length film (see the last section, below).\n\nThe presentation contained fifteen ninety-second sections and two hundred lantern slides, and ran for nearly two and a half hours. While some Lumiere footage was used in the opening passion sequence of the film, the majority of the footage was filmed in Melbourne, either in the attic of 69 Bourke Street, on the tennis court of the Murrumbeena Girls Home, or in the pool at Richmond Baths.\n\nThe presentation itself focused of the lives and deaths of early Christian martyrs and cost £550 to produce. The scenes were considered extremely violent for their time, including such images as the stoning of Stephen, the burning of Polycarp and unnamed Christians being tortured, beheaded, killed by gladiators, drowned, or burned alive. The presentation included a cast of 150 Salvation Army officers who were stationed in Melbourne at the time. The many death scenes took their toll, with the cast suffering various injuries, including scorched hair and eyebrows from the fires used. The presentation premiered on 13 September 1901, at the Melbourne Town Hall, to a crowd somewhere between three and four thousand. One reviewer spoke of how the death scenes caused several women to faint in the aisles.\n\n\"Soldiers of the Cross\" fortified the Limelight Department as a major player in the early film industry. However, \"Soldiers of the Cross\" would be dwarfed by \"Inauguration of the Australian Commonwealth\", when the Limelight Department was commissioned to film the 1901 Federation of Australia. It was the hope of the New South Wales government that the film would prove an imperishable record of the event, though little of the footage still exists. Perry set up five cameras at various point of the procession route and had to use a fire carriage to move quickly from one camera to the next.\n\nIn order that \"Soldiers of the Cross\" could be seen by a wide audience, the Limelight Department created groups known as Biorama Companies. Teams of musicians, lecturers, and projectionists would travel throughout Australia presenting the material that the Limelight Department had produced. Screenings were generally held in local halls, but it was the Biorama Companies sometimes used the sides of buildings as screens so that passersby could see it. When Herbert left the Salvation Army (taking the original \"Soldiers of the Cross\" material with him), he was replaced by Commissioner Thomas McKie. McKie encouraged the expansion of the Limelight Department, the creation of additional Biorama Companies and even the reshooting of \"Soldiers of the Cross\" in 1909, titled \"Heroes of the Cross\". In addition to the evangelical material produced for the Biorama Companies, the Limelight produced many films for private clients and the government. Some of the most notable of these were films showing the royal visit of the Duke and Duchess of York for the opening of the first sitting of the Parliament of Australia (the session itself could not be filmed due to poor lighting), the visit of America's Great White Fleet, and the Victoria's Second Boer War Contingent leaving South Africa. Engaging in such private contracts was a way in which the Limelight Department raised capital to support its operation and the operation of other Salvation Army programs.\n\nIn 1910, McKie was replaced as the Australasian commander by a more conservative Commissioner named James Hay. Hay felt that cinema was not something that the church should be involved in and he shut down the Limelight Department at the height of its operation. In his autobiography \"Aggressive Salvationism\", Hay wrote ’the cinema, as conducted by The Salvation Army, had led to weakness and a lightness incompatible with true Salvationism and was completely ended by me.’\n\nFor many years, a question mark has hung over the question of whether \"Soldiers of the Cross\" should be counted as the first feature film. The issues of concern are basically length of the footage, length of the presentation, and the continuity of the storyline.\n\nA feature film is generally defined as running for over sixty minutes with a constant storyline running throughout. Placed end to end, the film footage in \"Soldiers of the Cross\" equals twenty-two and a half minutes, falling short of the time requirements. Furthermore, the film did not have one long story, but rather a collection of short stories which also seems to disqualify it from contention. However, it has been argued that as the entire presentation, including films strips, lantern slides and live sections, runs for over two hours, \"Soldiers of the Cross\" should be considered a feature film.\n\nIn 1902, the year after \"Soldiers of the Cross\" was made, the Limelight Department produced \"Under Southern Skies\", a film examining life in Australia from European Settlement to Federation. This film ran about one hundred minutes but as it is a documentary, not a dramatised story, it is not considered to be a feature film. \"Heroes of the Cross\" ran for about 75 minutes and contained a more defined story thread than its predecessor; however, it was produced in 1909, three years after \"The Story of the Kelly Gang\" (1906) which is considered by most people to be the actual first feature film, including the Australian Film Commission.\n\n\n\n\n"}
{"id": "13016885", "url": "https://en.wikipedia.org/wiki?curid=13016885", "title": "List of nuclear power stations", "text": "List of nuclear power stations\n\nThe following page lists all nuclear power stations that are larger than in current net capacity. Those power stations that are smaller than , and those that are only at a planning or proposal stage, may be found in regional lists at the end of the page or in the list of nuclear reactors. The list is based on figures from PRIS (Power Reactor Information System) maintained by International Atomic Energy Agency.\n\nThis table lists all currently operational power stations with current net capacity over . Some of these may have reactors under construction, but only current net capacity is listed. Capacity of permanently shut-down reactors is not included, but capacity of long-term shut-down reactors is included. Power stations with past net capacity over and current net capacity under are listed in third table.\n\nThis table lists stations under construction or operational stations with under-construction reactors and current net capacity under . Planned connection column indicate connection of first reactor, not thus whole capacity.\n\n! Power station !! # units !! Net capacity under construction (MW) !! Construction start !! Planned connection !! Country !! Location\n<!-- on-hold |-\n"}
{"id": "2238883", "url": "https://en.wikipedia.org/wiki?curid=2238883", "title": "Manifold System", "text": "Manifold System\n\nManifold System is a geographic information system (GIS) software package developed by Manifold Software Limited that runs on Microsoft Windows. Manifold System handles both vector and raster data, includes spatial SQL, a built-in Internet Map Server (IMS), and other general GIS features. Manifold System Release 8.00 was the first commercial product to include massively parallel GPGPU functionality utilizing NVIDIA GPUs, implemented within approximately 35 functions used in the raster transformation system. Manifold System has an active user community with a mailing list and online forums.\n\nThe development team for Manifold was created in 1993 to optimize mathematics libraries for a massively-parallel supercomputer created in a joint venture between Intel Corporation and the US Department of Defense. The team subsequently embarked on a plan to create and sell mathematics libraries, including the General Graph Facilities library (GGF) and the Computational Geometry Library (CGL), under the name of the Center for Digital Algorithms.\n\nA series of \"workbench\" products were created to help teach customers the operation of algorithms in the libraries using visual means. Road networks and geometric data in geographic contexts were used to provide visual familiarity and interest, in effect creating a GIS-like product. In 1997 and 1998 customers asked for a true GIS product based on the workbench products and development of Manifold System was launched. The company soon changed its name to Manifold Software Limited to match the new product's name.\n\nManifold System was first sold in January 1998 as Release 3.00. Releases 3.00 and 4.00 were heavily weighted to analytics, with many tools for abstract graph theory analysis but a very limited GIS toolset. At the request of GIS users and resellers, Release 4.50 emphasized general GIS features of broader interest and emerged as Manifold's first commercial GIS, a typical vector GIS more or less equivalent to classic vector GIS packages such as ArcView 3.x or MapInfo Professional.\n\nThe Release 5.00 series in 2001 and 2002 integrated display and editing of raster images and surfaces, including terrain elevation surfaces, and both 2D and 3D rendering. The 5.x series also introduced an integrated Internet Map Server (IMS) and the first Enterprise editions of Manifold System allowing collaboration by teams using shared components. The 5.x series also introduced a new spatial SQL and fuzzy logic using the Decision Support System.\n\nReleases since 2003 include 5.50, 6.00 (two major feature upgrades via service pack), 6.50, 7.00 and 7x. 6.50 introduced image tiling from Terraserver and OGC WMS image servers using Manifold as a client and extended IMS support to include OGC WMS when using Manifold as a server. 7.00 further extended IMS to include OGC WFS-T and image server functionality as well.\n\nRelease 7.00 was issued in May 2006 and followed up by Release 7x in the next three months. 7x was released in two flavors: 32-bit and 64-bit.\n\nRelease 7.00 introduced direct support for Oracle Spatial (vector drawings, raster images and raster surfaces) and included concurrent multiuser editing capability for Oracle and a variety of other databases, including DB2 and Microsoft SQL Server. 7.00 introduced multiprocessor support with multithreaded rendering of image libraries, multithreaded connections to DBMS providers and use of multiple processors in other areas as well.\n\n7.00 also introduced the Manifold Image Server interface API, allowing users to create modules that enable usage within Manifold of image servers such as Virtual Earth, Google Maps, Yahoo, Ask and others. Open source image server modules have been published by the user community in both 32-bit and 64-bit versions that enable automatic fetching and tiling of either satellite images or street map images from various image servers.\n\nRelease 8.00 was issued in the summer of 2007 and introduced 420 improvements. 8.00 expanded support for direct use of spatial DBMS beyond Oracle to include IBM DB2 with Spatial Extender, PostgreSQL / PostGIS and pre-releases of Microsoft's SQL Server 2008 spatial product available in 2007. 8.00 also introduced a Manifold-written spatial extender for Microsoft SQL Server 2005 as well as generic spatial DBMS capability from Manifold enabling spatial DBMS storage of vectors and rasters in any DBMS providing binary storage capability.\n\nRelease 8.00 became the first GIS product to include support for NVIDIA CUDA technology, in which massively-parallel architectures utilized in NVIDIA GPU cards, employing up to hundreds of stream processors per card, can be utilized to execute general purpose GIS code for computations on rasters. Typical speed increases when using NVIDIA CUDA reduce the time required for complex surface calculations in Manifold from minutes to seconds. For its use of NVIDIA CUDA Manifold System won the 2008 Geospatial Innovator Award at the GeoTec 2008 conference.\n\n8.00 was updated through 2008 to improve support for PostgreSQL/PostGIS, to support final production releases of Microsoft SQL Server 2008 and to support new Windows releases through Windows Server 2008 x64. As new Windows were issued Manifold verified compatibility for Release 8 through all Windows editions up to current Windows 10 and related Windows server editions, both in 32 bit and x64. \n\nIn December 2015 Manifold.net announced that Manifold's parallel database engine for OEMs, Radian, would be the foundation for future releases of Manifold System GIS, and provided a road map for Radian Studio and Manifold Release 9 implementations in 2016. Beta testing for Radian commenced in 2016 with a first release of Radian Studio in February 2017. \n\nA series of updates for 8.00 were issued in 2017 to add integration with ODBC access to Radian data sources. This gave Release 8 the ability to connect, through Radian, to any data source Radian can use, expanding Release 8 connectivity to data sources 8.00 previously could not access, such as MrSID, ESRI file geodatabases, numerous GDAL/OGR sources and similar.\n\nLater versions have benefited greatly from community involvement via online discussion and beta testing. From July 2002 to May 2017 over 4000 new items have been cited in release notes, many of which originated in the user community.\n\nThe online Georeference forum was started by David Brubacher and William Howell in 2004 and incorporated into the manifold.net site in January 2006 at forum.manifold.net\n\n"}
{"id": "30516155", "url": "https://en.wikipedia.org/wiki?curid=30516155", "title": "Mason Pearson Brushes", "text": "Mason Pearson Brushes\n\nMason Pearson Brushes is a British company specialising in the manufacture of hairbrushes. In the mid 1860s a Yorkshireman called Mason Pearson came to work at the British Steam Brush Works, in the East End of London. In 1885 he invented the \"pneumatic\" rubber-cushion hairbrush which became the company's primary product and is still on sale, little changed from the original design. The \"Junior\" model, which has a mix of boar nylon bristles, has been called \"the Ferrari of brushes.\"\n\nMason Pearson hairbrushes are sold worldwide. The family business is still passed down through generations and is currently run by the Pearson family in London today.\n"}
{"id": "907134", "url": "https://en.wikipedia.org/wiki?curid=907134", "title": "Microwave Data Systems", "text": "Microwave Data Systems\n\nMicrowave Data Systems, Inc. (MDS) is a company that makes wireless communications equipment for the industrial market. MDS was a subsidiary of Moseley Associates and was acquired, in January 2007, by General Electric Multilin, becoming known as GE MDS, LLC.\n\nProducts include \"point-to-point\" and \"point-to-multipoint\" radios that operate in the licensed and unlicensed bands. The company is known particularly for long range communications, interference rejection, industrial quality, and high reliability.\n\n\n\n"}
{"id": "271013", "url": "https://en.wikipedia.org/wiki?curid=271013", "title": "Minister of Science and Sport", "text": "Minister of Science and Sport\n\nThe Minister of Science and Sport, formerly the Minister of Science, is an office in the Cabinet of Canada that originally existing from 1990 to 1995 and was brought back in 2008.\n\nPrior to 1990, the responsibilities of the Industry, Science and Technology portfolio were divided between the now-defunct post of Minister of Regional Industrial Expansion and a Minister of State for Science and Technology.\n\nIn 1995, the portfolio was merged with that of the Minister of Consumer and Corporate Affairs to create the post of Minister of Industry. In the appointments to the Cabinet of Canada of October 30, 2008 under Stephen Harper, the portfolio was reintroduced as a Minister of State and given to Gary Goodyear.\n\nIn 2015, Kirsty Duncan was appointed a Minister of State styled as \"Minister of Science\" to assist the Minister of Industry (the senior portfolio was also renamed the Minister of Innovation, Science and Economic Development). Duncan's portfolio was expected to oversee basic research, while Navdeep Bains would oversee applied science. In July 2018, the office's portfolio was expanded, being renamed to \"Minister of Science and Sport\".\n\nThe Official Opposition Shadow Minister for Science is Matt Jeneroux Member of Parliament for Edmonton Riverbend.\n\nKey:\n\n"}
{"id": "44578211", "url": "https://en.wikipedia.org/wiki?curid=44578211", "title": "Ministry of Information Policy (Ukraine)", "text": "Ministry of Information Policy (Ukraine)\n\nThe Ministry of Information Policy () or MIP is a government ministry in Ukraine established on 2 December 2014. It was created concurrently with the formation of the Second Yatsenyuk Government, after the October 2014 Ukrainian parliamentary election. The ministry oversees information policy in Ukraine. According to the first Minister of Information, Yuriy Stets, one of the goals of its formation was to counteract \"Russian information aggression\" amidst pro-Russian unrest across Ukraine, and the ongoing Russian military intervention of 2014. Ukrainian president Petro Poroshenko mentioned that the main function of the ministry is to stop \"the spreading of biased information about Ukraine\".\n\nA proposal to establish an information ministry for Ukraine was first put forth on 30 November 2014 by Internal Affairs Ministry advisor Anton Herashchenko. He said that ministry could protect \"Ukraine's information space from Russian propaganda and counter propaganda in Russia, in the temporarily occupied territories of Crimea and eastern Ukraine\". The proposal was made amidst ongoing efforts to form a government, following the October 2014 Ukrainian parliamentary election. Ukrainian president Petro Poroshenko advocated for the establishment of such a ministry through the night on 1–2 December. It was quickly pushed through parliament with little fanfare. The formation of the Second Yatsenyuk Government was announced on 2 December, with Poroshenko ally Yuriy Stets confirmed as the first Minister for Information Policy. One day after his appointment, Stets published the ministry's regulations, which were based on a draft he wrote in 2007–09. According to these regulations, the ministry is meant to \"develop and implement professional standards in the media sphere\", \"ensure freedom of speech\", and prevent the spread of \"incomplete, outdated, or unreal information.\n\nPrior to its establishment, many Ukrainian journalists protested the creation of the ministry. They cited concerns that the ministry would \"open the way to grave excesses\" in restricting free speech, and that the ministry would inhibit journalists' work. Journalists demonstrating outside the parliament building said that the creation of the ministry was equivalent to \"a step back to the USSR\". The ministry was given the satirical appellation \"Ministry of Truth\" (), a reference to George Orwell's dystopian novel \"Nineteen Eighty-Four\". Reporters without Borders strongly opposed the creation of the ministry, and said that it was a \"retrograde step\". Petro Poroshenko Bloc politician Serhiy Leshchenko called for the ministry's immediate dissolution, whilst Poroshenko Bloc politician Svitlana Zalishchuk said that ministry's implementation should be put on hold, and that its regulations should be redrafted.\n\nNewly appointed Minister for Information Policy Yuriy Stets said that one of the primary goals of the ministry was to counteract \"Russian information aggression\" amidst the prolonged 2013–2014 crisis in Ukraine, and the ongoing war in the Donbass region. According to Stets, no other Ukrainian government institution was capable of handling this task. He stated that \"different states with different historical and cultural experiences in times of crisis came to need to create a body of executive power that would control and manage the information security of the country\". Stets also said that the ministry \"will in no way try to impose censorship or restrict freedom of speech\". President Poroshenko told journalists on 7 December 2014 that the main purpose of the ministry is to stop external \"information attacks on Ukraine\" by promoting the \"truth about Ukraine\" across the world. Poroshenko added that it was \"foolish\" to think that the ministry would become an organ of censorship.\n\nThe ministry was officially established by a resolution of the Ukrainian government on 14 January 2015. The resolution contained the duties and regulations of the ministry. According to the resolution, the primary objectives of the MIP are to \"protect the information sovereignty of Ukraine\", and to \"implement media reforms to spread socially important information\".\n\nA statement released by the ministry on 19 February 2015 announced the creation of an \"information force\" to counter misinformation on social media and across the internet. The force is targeted at Russia, which has been said to employ an \"army of trolls\" to spread false information and propaganda during the Ukrainian crisis.\n\nYuriy Stets resigned from his post of Minister of Information Policy on 8 December 2015. He withdrew his letter of resignation on 4 February 2016, and continued in the post.\n\nUnited States Ambassador to Ukraine Geoffrey Pyatt stated late January 2016 \"It's a huge mistake to create a 'Ministry of Truth' that tries to generate alternative stories. That is not the way to defeat this information warfare\".\n\nThe approved staff of the Ministry of Information Policy of Ukraine for 2015 includes 29 employees. Given the Ukrainian practice, the figure seems to be very small.However, the experience of many countries and contemporary understanding of management prove that these are the public institutions with a small staff working based on delegation principles, which are much more capable to implement global state tasks.\n\nDespite strict requirements of Ukrainian legislation, the Ministry of Information Policy is able to successfully perform all assigned tasks with minimum resources.\" to \"The approved staff of the Ministry of Information Policy of Ukraine for 2015 includes 29 employees, so we are a \"lean and mean\" department. Contemporary management strategies suggest that public institutions with a small staff working on delegated principles, can be highly effective to performing government tasks.\n\nThe Ministry of Information Policy aim is to successfully perform all assigned tasks with minimum resources, and within the strict requirements of Ukrainian legislation.\n\n\nAccording to Ukrainian legislation, social advertising - information of any kind, common in any form, whoch aims to achieve social goals, promotion of human values and the distribution of which is not intended to make a profit.\n\nMIP launched such social campaigns:\n\nThe Ministry has video and photo materials that it claims prove Russian military presence on the territory of Ukrainian Donbass.\n\nOn February 23, 2014 MIP created the Internet platform \"Information armies Ukraine\". Today the site pages and relevant social networks are used by more than 10,000 users. Monthly dozens of anti-Ukrainian users are blocked. Information attack of FSB security services is blocked, monthly almost 80,000 users get information on the fake of Russian propaganda. On each post there are dozens of false answers, which provided facts and arguments, so that the effectiveness of anti-Ukrainian propaganda in social networks plummeted.\n\nThe Ministry of Defense of Ukraine jointly with the Ministry of Information Policy of Ukraine continue to take the application process for project «Embedded journalists» - attaching media to military units in the ATO area and journalists are invited to participate. The journalist is not involved in the fighting, but he is subject to the relevant officer and lives in the same conditions as the rest of the soldiers. Today, Ukrainian and foreign journalists are working successfully with the military in the area ATO within the project Embedded journalism. Journalists from BBC, CNN, Washington Post, London Evening Standard, The Independent, Newsweek, France Press, Polsat, Daily Signal, Hanslucas, Tsenzor.NET, \"Radio Liberty\", Inter, Business Ukraine, New time have already participated. Results for the three months of the work: 18 video, 15 articles and three films in the foreign media.\n\n"}
{"id": "21610483", "url": "https://en.wikipedia.org/wiki?curid=21610483", "title": "NASA Orbital Debris Observatory", "text": "NASA Orbital Debris Observatory\n\nNASA Orbital Debris Observatory (NODO) was an astronomical observatory located in the Lincoln National Forest near Cloudcroft, New Mexico approximately northeast of Alamogordo. From 1995 to 2002 it hosted two telescopes funded and operated by NASA that were dedicated to detecting orbital debris. The facility was initially called the Cloudcroft Electro-Optical Research Facility when it was completed in 1962, and was also known as the Cloudcroft Observatory. It is now privately owned.\n\n\n\n"}
{"id": "5678122", "url": "https://en.wikipedia.org/wiki?curid=5678122", "title": "NERC Data Centres", "text": "NERC Data Centres\n\nThe Natural Environment Research Council (NERC) has seven subject-based environmental data centres (EDCs) to store and distribute data from its own research programmes and data that are of general use to the environmental research community. These data centres are sometimes called the NERC designated Data Centres.\n\nThe NERC Environmental Data Centres and their areas of responsibility are as follows: \n\nThe data centres hold data from environmental scientists working in the UK and around the world.\nThey provide access to a comprehensive data and information resource about our environment, through the NERC Data Catalog Service. CEDA hosts the NERC Data Catalog Service , data may also be cataloged from certain NERC Data Centres and also at data.gov.uk.\nAccess to these data is freely available to students, researchers and stakeholders, as well as business users and policy makers, to help them understand the environment in which we live.\n\nEach data centre works to build user confidence, using common data formats and noting sources, contexts, and degrees of accuracy. They combine expertise in the scientific collection of information, state-of-the-art data management and preservation techniques, making them an important national asset.\n\nThe NERC Data Policy is commitment to support the long-term management of environmental data and also outlines the roles and responsibilities of all those involved in collecting and managing environmental data. The NERC Data Centres provide support and guidance in data management to those funded by NERC, are responsible for the long-term management of data and provide access to NERC's holdings of environmental data.\n\nThe data policy is consistent with legal frameworks, such as the Environmental Information Regulations 2004, the INSPIRE Regulations 2009 and contractual arrangements with other bodies where, for example, NERC holds data on their behalf but does not own the intellectual property rights.\n\nTo reflect NERC's continuing commitment to openness and transparency in the research process, and in support of the government's developing agenda on open access to public data, the NERC Data Policy has been substantially revised, and this new version of NERC's Data Policy came into force in January 2011.\n\nThe NERC charging regime recognises two classes of data:\n\n\nThe licence charges that NERC can levy for the supply of data and information are governed by HM Treasury guidance and Government regulations.\nDuring 2010 NERC aimed to move to a position where it will supply its data for free for all teaching and research activities, apart from large or complex requests where they may make a nominal handling charge.\n\nThe NERC Science Information Strategy (SIS) was created to provide the framework for NERC to work more closely and effectively with its scientific communities, both internal and external, in delivering data and information management services to support its five-year science strategy, the Next Generation Science for Planet Earth 2007-2012.\n\nThe Science Information Strategy is being implemented in three phases, beginning April 2010. Each of these phases will be made up of a number of smaller projects. Project/Phase 1 will concentrate on determining user requirements, documenting existing holdings, agreeing an information architecture, updating the NERC Data Policy and improving the data discovery tools.\n\nThe benefits of the SIS implementation for science include the increased competence of data centre staff to inform the science community, policy and decision makers nationally and internationally on best practice, standards development and the establishment of new projects and initiatives (e.g. EU Framework projects):enhancing NERC's national and international reputation and increased capability to conduct multi-disciplinary, integrated science.\n\n"}
{"id": "1179553", "url": "https://en.wikipedia.org/wiki?curid=1179553", "title": "One Good Turn (book)", "text": "One Good Turn (book)\n\nOne Good Turn: A Natural History of the Screwdriver and the Screw is a book published in 2000 by Canadian architect, professor and writer Witold Rybczynski.\n\nThe idea for the book came in 1999 when an editor at \"The New York Times Magazine\" asked Rybczynski to write a short essay on the best and most useful common tool of the previous 1000 years. Rybczynski took the assignment, but as he researched the history of the items in his workshop – hammers and saws, levels and planes – he found that most dated well back into antiquity. At the point of giving up, he asked his wife for ideas. She answered: \"You always need a screwdriver for something.\"\n\nRybczynski discovered that the screwdriver is a relatively new addition to the toolbox, an invention of the Late Middle Ages in Europe and the only major mechanical device not independently invented by the Chinese. Leonardo da Vinci was there at the start, designing a number of screw-cutting machines with interchangeable gears. Nevertheless, it took generations for the screw (and with it, the screwdriver and lathe) to come into general use, and it was not until modern times that improvements such as slotted screws came into being. Rybczynski spends some time discussing the Canadian invention, the Robertson screwdriver.\n\n"}
{"id": "190260", "url": "https://en.wikipedia.org/wiki?curid=190260", "title": "OpenDoc", "text": "OpenDoc\n\nOpenDoc is a multi-platform software componentry framework standard created by Apple for compound documents, intended as an alternative to Microsoft's Object Linking and Embedding (OLE). Active development was discontinued in March 1997.\n\nThe core idea of OpenDoc is to create small, reusable components, responsible for a specific task, such as text editing, bitmap editing, or browsing an FTP server. OpenDoc provides a framework in which these components can run together, and a document format for storing the data created by each component. These documents can then be opened on other machines, where the OpenDoc frameworks substitute suitable components for each part, even if they are from different vendors. In this way users can \"build up\" their documents from parts. Since there is no main application and the only visible interface is the document itself, the system is known as \"document centered\".\n\nAt its inception, it was envisioned that OpenDoc would allow smaller, third-party developers to enter the then-competitive office software market, able to build one good editor instead of having to provide a complete suite.\n\nOpenDoc was initially created by Apple in 1992, after Microsoft approached Apple asking for input on a proposed OLE II project. Apple had been experimenting with software components internally for some time, based on the initial work done on its Publish and Subscribe linking model and the AppleScript scripting language, which in turn was based on the HyperCard programming environment. Apple reviewed the Microsoft prototype and document and returned a list of problems they saw with the design. Microsoft and Apple, who were very competitive at the time, were unable to agree on common goals and did not work together.\n\nAt about the same time, a group of third-party developers had met at the Apple Worldwide Developers Conference (WWDC '91) and tried to hammer out a standardized document format, based conceptually on the Electronic Arts Interchange File Format (IFF). Apple became interested in this work, and soon dedicated some engineers to the task of building, or at least documenting, such a system. Initial work was published on the WWDC CDs, as well as a number of follow-up versions on later developer CDs. A component document system would only work with a known document format that all the components could use, and so soon the standardized document format was pulled into the component software effort. The format quickly changed from a simple one using tags to a very complex object oriented persistence layer called Bento.\n\nInitially the effort was codenamed \"Exemplar\", then \"Jedi\", \"Amber\", and eventually \"OpenDoc\".\n\nApple was also involved in the Taligent project during some of this period, which offered somewhat similar functionality although based on very different underlying mechanisms. While OpenDoc was still being developed, Apple confused things greatly by suggesting that it should be used by people porting existing software only, and new projects should instead be based on Taligent since that would be the next OS. Taligent was considered the future of the Mac platform, and work on other tools like MacApp were considerably deprioritized. When Taligent died in 1995, Apple was left with no major ongoing efforts along these lines, and OpenDoc became the future of Mac development.\n\nStarting in 1992, Apple had also been involved in an effort to replace MacApp development framework with a cross-platform solution known as Bedrock, from Symantec. Symantec's Think C was rapidly becoming the tool of choice for development on the Mac. Apple had been working with them to port their tools to the PowerPC when they learned of Symantec's internal porting tools. Apple proposed merging existing MacApp concepts and code with Symantec's to produce an advanced cross-platform system. Bedrock began to compete with OpenDoc as \"the\" solution for future development.\n\nAs OpenDoc gained currency within Apple, the company started to push Symantec into including OpenDoc functionality in Bedrock. Symantec was uninterested in this, and eventually gave up on the effort, passing the code to Apple. Bedrock was in a very early state of development at this point, in spite of 18 months of work, as the development team at Symantec suffered continual turnover. Apple proposed that the code would be used for OpenDoc programming, but nothing was ever heard of this again, and Bedrock disappeared.\n\nAs a result of Taligent and Bedrock both being the \"next big thing\", little effort had been expended on updating MacApp. As these two projects died, this left Apple with only OpenDoc as a modern OO-based programming system.\n\nThe development team realized in mid-1992 that an industry coalition was needed to promote the system, and created the \"Component Integration Laboratories\" (\"CI Labs\") with IBM and WordPerfect. IBM introduced the System Object Model (SOM) shared library system to the project, which became a major part of Apple's future efforts, in and out of OpenDoc. In 1996 the project was adopted by the Object Management Group, in part due to SOM's use of Common Object Request Broker Architecture (CORBA), maintained by the OMG.\n\nOpenDoc was one of Apple's earliest experiments with open standards and collaborative development methods with other companies. Apple and its partners never publicly released the source code, but did make the complete source available to developers for feedback and for testing and debugging purposes.\n\nOpenDoc was initially released to run on classic Mac OS System 7.5. From IBM’s involvement in Taligent, there was an implementation of OpenDoc in OS/2 Warp 4.\n\nThe WAV word processor was a semi-successful OpenDoc word processor from Digital Harbor; the Numbers & Charts package was a spreadsheet and 3D real-time charting solution from Adrenaline Software; and the Cyberdog web browser was created by Apple as an OpenDoc application. Lexi from Soft-Linc, Inc. was a linguistic package containing a spell checker, thesaurus and a simple translation tool which WAV and other components used. The Nisus Writer software by Nisus incorporated OpenDoc, but its implementation was hopelessly buggy. Bare Bones Software tested the waters by making its BBEdit Lite freeware text editor available as an OpenDoc editor component. RagTime, a completely integrated office package with spreadsheet, publishing and image editing was ported to OpenDoc shortly before OpenDoc was cancelled. Apple's 1996 release of ClarisWorks 5.0 (the predecessor of AppleWorks) was planned to support OpenDoc components, but this was dropped.\n\nAnother OpenDoc container application, called \"Dock'Em\", was written by MetaMind Software under a grant from the National Science Foundation and commissioned by The Center for Research in Math and Science Education, headquartered at San Diego State University. The goal was to allow multimedia content to be included in documents describing curriculum.\n\nA number of physics simulations were written by MetaMind Software and by Russian software firm Physicon (OpenTeach) as OpenDoc parts. Physics curricula for high school and middle school used them as their focus. With the demise of OpenDoc, the simulations were rewritten as Java applets and are still available from the Center under the title of \"The Constructing Physics Understanding (CPU) Project\" by Dr. Fred Goldberg.\n\nComponents of the E-Slate educational microworlds' platform were originally implemented as OpenDoc parts in C++ on both MacOS and Windows, reimplemented later on (after the demise of OpenDoc) as Java applets and eventually as JavaBeans.\n\nOpenDoc's flexibility came at a cost. OpenDoc components were invariably large and slow. For instance, opening a simple text editor part would often require 2 megabytes of RAM or more, whereas the same editor written as a standalone application could be as small as 32 KB. This initial overhead became less important as the number of documents open increased, since the basic cost was for shared libraries which implemented the system, but it was large compared to entry level machines of the day. Many developers felt that the extra overhead was too large, and since the operating system did not include OpenDoc capability, the memory footprint of their OpenDoc based applications appeared unacceptably large. In absolute terms, the one-time library overhead was approximately 1 megabyte of RAM, which at the time was nearly half of a low-end desktop computer's entire RAM complement.\n\nAnother issue was that OpenDoc had little in common with most \"real world\" document formats, and so OpenDoc documents could really only be used by other OpenDoc machines. Although one would expect some effort to allow the system to export to other formats, this was often impractical because each component held its own data. For instance, it took significant effort for the system to be able to turn a text file with some pictures into a Microsoft Word document, both because the text editor had no idea what was in the embedded objects, and because the proprietary Microsoft format was undocumented and required reverse engineering.\n\nAnother problem was the fact that each part saved its data within Bento (the former name of an OpenDoc compound document file format) in its own internal binary format, and it was very common to find one component could not open a document created by another, even though the internal data represented similar objects (spreadsheet data for instance). OpenDoc attempted to solve this problem by allowing developers to store multiple formats to represent the same document object. For instance, it was both possible and encouraged to store a common format like JPEG along with editable binary format, but in practice few developers followed this recommendation. This problem was not unique to OpenDoc, and in fact was also experienced by the Microsoft equivalent, Object Linking and Embedding (OLE). Indeed, many years later, XML documents which attempt to perform embedding of other XML formats also encounter similar issues.\n\nIt also appears that OpenDoc was a victim of an oversold concept, that of compound documents. Only a few specific examples are common, for instance most word processors and page layout programs include the ability to include graphics, and spreadsheets are expected to handle charts.\n\nBut certainly the biggest problem with the project was that it was part of a very acrimonious competition between OpenDoc consortium members and Microsoft. The members of the OpenDoc alliance were all trying to obtain traction in a market rapidly being dominated by Microsoft Office. As the various partners all piled in their own pet technologies in hopes of making it an industry standard, OpenDoc grew increasingly unwieldy. At the same time, Microsoft used the synergy between the OS and applications divisions of the company to make it effectively mandatory that developers adopt the competing OLE technology. In order to obtain a Windows 95 compliance logo from Microsoft, one had to meet certain interoperability tests which were quite difficult to meet without adoption of OLE technology, even though the technology was largely only useful in integrating with Microsoft Office. OpenDoc was forced to create an interoperability layer in order to allow developers to even consider adoption, and this added a great technical burden to the project.\n\nOpenDoc had several hundred developers signed up but the timing was poor. Apple was rapidly losing money at the time and many in the industry press expected the company to fail.\n\nOpenDoc was soon discontinued, with Steve Jobs (who had been at NeXT during this development) noting that they \"put a bullet through [OpenDoc's] head\", and most of the Apple Advanced Technology Group was laid off in a big reduction in force in March 1997. Other sources noted that Microsoft hired away three ClarisWorks developers who were responsible for OpenDoc integration into ClarisWorks.\n\nAppleShare IP Manager from versions 5.0 to 6.2 relied on OpenDoc, but AppleShare IP 6.3, the first Mac OS 9 compatible version (released in 1999), eliminated the reliance on OpenDoc. Apple officially relinquished the last trademark on the name OpenDoc on June 11, 2005.\n\n\n"}
{"id": "7710805", "url": "https://en.wikipedia.org/wiki?curid=7710805", "title": "Packaging engineering", "text": "Packaging engineering\n\nPackaging engineering, also package engineering, packaging technology and packaging science, is a broad topic ranging from design conceptualization to product placement. All steps along the manufacturing process, and more, must be taken into account in the design of the package for any given product. Package engineering includes industry-specific aspects of industrial engineering, marketing, materials science, industrial design and logistics. Packaging engineers must interact with research and development, manufacturing, marketing, graphic design, regulatory, purchasing, planning and so on. The package must sell and protect the product, while maintaining an efficient, cost-effective process cycle.\n\nEngineers develop packages from a wide variety of rigid and flexible materials. Some materials have scores or creases to allow controlled folding into package shapes (sometimes resembling origami). Packaging involves extrusion, thermoforming, molding and other processing technologies. Packages are often developed for high speed fabrication, filling, processing, and shipment. Packaging engineers use principles of structural analysis and thermal analysis in their evaluations.\n\nSome packaging engineers have backgrounds in other science, engineering, or design disciplines while some have college degrees specializing in this field.\n\nFormal packaging programs might be listed as package engineering, packaging science, packaging technology, etc. BE, BS, MS, M.Tech and PhD programs are available. Students in a packaging program typically begin with generalized science, business, and engineering classes before progressing into industry-specific topics such as shelf life stability, corrugated box design, cushioning, engineering design, labeling regulations, project management, food safety, robotics, RFID tags, quality management, package testing, packaging machinery, tamper-evident methods, recycling, computer-aided design, etc.\n\n\n"}
{"id": "53307360", "url": "https://en.wikipedia.org/wiki?curid=53307360", "title": "Payability", "text": "Payability\n\nPayability is an American FinTech company based in New York City. Payability provides finance solutions to suppliers of digital marketplaces.\n\nPayability's platform integrates with digital marketplaces, most notably Amazon's. The company offers customers invoice factoring: It purchases suppliers’ accounts receivable, allowing suppliers to get paid for their sales on an expedited schedule.\n\nScott Lynn and Keith Smith founded Payability in 2014.\n\nAccording to Smith, the idea for Payability came when he saw a recurring pain point for the suppliers of online marketplaces like Amazon: Marketplaces often withhold payments from sellers for a number of weeks, creating cash flow challenges for suppliers. In a 2015 interview, Smith pointed out that being unable to access cash from their sales frustrates sellers and limits their ability to replenish inventory or fund marketing campaigns.\n\nPayability began working with mobile app developers in 2015. In 2016 the company expanded to focus on Amazon sellers and vendors on other online marketplaces.\n\nIn August 2016, Payability partnered with Amazon Launchpad to offer financing for startups through Amazon Launchpad Services Hub.\n\nSmith is the founder of BigDoor, a gamification startup that he exited in 2014. He was previously the founder of two other companies, CyberMortgage and Zango.\n\nLynn is the founder of advertising technology company AdKnowledge.\n\n"}
{"id": "15644495", "url": "https://en.wikipedia.org/wiki?curid=15644495", "title": "Perepiteia", "text": "Perepiteia\n\nPerepiteia is claimed to be a new generator developed by the Canadian inventor Thane Heins. The device is named after the Greek word for peripety, a dramatic reversal of circumstances or turning point in a story. The device was quickly attributed the term \"perpetual motion machine\" by several media outlets. Due to the long history of hoaxes and failures of perpetual motion machines and the incompatibility of such a device with accepted principles of physics, Heins' claims about Perepiteia have been treated with considerable skepticism.\n\nIn 2003, Heins filed a patent application in Canada but no patent was granted. Heins also founded Potential Difference Inc, the website of which contains a series of videos of the inventor demonstrating the machine. US patent #9,230,730 issued in 2016 pertaining to another of Thane's inventions, a bi-toroidal topology transformer.\n\nHeins has recently stated that he is unsure whether or not the machine really produces energy, but in communications with science writer David Bradley of ScienceBase, Heins made claims of up to 7000% efficiency for the bi-toroidal transformer. Heins, who reportedly works 8–12 hours a day on the Perepiteia, insists that it is viable and that \"This technology should be mainstream.\"\n\nMechanically, the device appears to be an induction motor with a magnetic material placed inside the rotor core. Heins believes that the device's potential may rest in its atypical manipulation of the back electromotive force (back EMF). A more detailed description of the device may be found in the patent application, minus supporting figures.\n\nThe apparent unique quality of the Perepiteia machine is that instead of maintaining a certain state of motion, it appears to generate acceleration. According to Heins, the Perepiteia produces magnetic friction which somehow gets turned into a magnetic boost. Using an electric motor, the drive shaft is attached to a steel rotor with small round magnets lining its outer edges. In this set-up of a simple generator, the rotor spins so that the magnets pass by a wire coil just in front of them, generating electrical energy.\n\nPerepiteia's process begins by overloading the generator to get a current, which typically causes the wire coil to build up a large electromagnetic field. Usually, this kind of electromagnetic field creates an effect called the back electromotive force (back EMF) due to Lenz's law. The effect should repel the spinning magnets on the rotor, and slow them down until the motor stops completely, in accordance with the law of conservation. However, instead of stopping, the rotor accelerates - i.e. the magnetic friction did not repel the magnets and wire coil. Heins states that the steel rotor and driveshaft had conducted the magnetic resistance away from the coil and back into the electric motor. In effect, the back EMF was boosting the magnetic fields used by the motor to generate electrical energy and cause acceleration. The faster the motor accelerated, the stronger the electromagnetic field it would create on the wire coil, which in turn would make the motor go even faster. Heins seemed to have created a positive feedback loop. To confirm the theory, Heins replaced part of the driveshaft with plastic pipe that wouldn't conduct the magnetic field. There was no acceleration.\n\nIn early 2008, Heins was given access to equipment to demonstrate it by professor Riadh Habash of the University of Ottawa, who says of it, \"It accelerates, but when it comes to an explanation, there is no backing theory for it. That's why we're consulting MIT. But at this time we can't support any claim.\"\n\nAfter examining the machine and witnessing a demonstration, Massachusetts Institute of Technology (MIT) professor Markus Zahn admitted that he could not fully explain its operation. Although he refused to call it perpetual motion, he stated that it might be an extremely efficient motor. Regarding the device, Zahn stated that \"It's an unusual phenomena I wouldn't have predicted in advance. But I saw it. It's real. Now I'm just trying to figure it out...To my mind this is unexpected and new, and it's worth exploring all the possible advantages once you're convinced it's a real effect.\" However, even if Perepiteia does not produce perpetual motion, Zahn still believes that the device could have considerable practical applications, noting that \"There are an infinite number of induction machines in people's homes and everywhere around the world. If you could make them more efficient, cumulatively, it could make a big difference.\"\n\nHowever, Zahn later stated in an interview that \"I can't understand how [Heins] can even breathe the words 'perpetual motion.' He plugs it into the wall.\" In a subsequent e-mail to Heins, Zahn wrote that: \"Any talk of perpetual motion, over unity efficiency, etc. discredits you, now me, and your ideas.\" Zahn further stated that he would not endorse Heins' device until \"the foolishness is stopped of hinting that your motor violates fundamental laws of physics.\"\n\nCritics of the system have pointed out that the system described by Heins simply demonstrates a change in the motor's hysteresis drag, increasing the speed of the rotor but not producing any energy. In other words, when the rotor exhibits acceleration following a specific electrical short-out, the device is merely more efficiently converting the input electricity to mechanical energy than in the other test configurations.\n\nOn February 29, 2008, six members of Ottawa Skeptics, met at the Colonel By building at the University of Ottawa to witness a demonstration of Perepiteia. Heins, who conducted the demonstration, later met with the members to discuss his device and answer questions. In a subsequent report released in May, Ottawa Skeptics expressed severe doubts about Heins' claims regarding Perepiteia. They noted that Perepiteia produces either observed acceleration or a slight increase in generator electrical output but that this alone does not automatically mean that \"free energy\" or perpetual motion is being produced or that there is a \"real and measurable effect.\" While acknowledging that the speed-up behaviour of the generator cannot be fully explained, they stated that there is no evidence that Perepiteia \"represents any challenge to currently known laws of physics.\"\n\nOn May 21, 2009, a skeptic writing under the name Natan Weissman wrote an explanation of Perepiteia in relation to its motor, a Ryobi bench grinder. The author states that the acceleration behavior of the machine is due to the consumption of torque from the induction motor, rather than any unconventional manipulation of Electromagnetic fields or Counter-electromotive force.\n\nOn June 3, 2013, posting in response to questions \"Pure Energy Blog\", Heins provided an explanation of his claims, stating that: \"A generator that requires a 1 Watt increase in mechanical drive shaft power to deliver 1 Watt of electrical power to a load would be 100% efficient. A generator that delivers 0.95 Watts with a 1 Watt increase in mechanical drive shaft power from no-load to on-load would be 95% efficient.\"\n\n"}
{"id": "26681707", "url": "https://en.wikipedia.org/wiki?curid=26681707", "title": "Pionen", "text": "Pionen\n\nPionen is a former civil defence center built in the White Mountains Södermalm borough of Stockholm, Sweden in 1943 to protect essential government functions from nuclear strike. The address of the Pionen data center is Renstiernas gata 35 and 37.\n\nIt was converted into a data center by the Swedish Internet service provider Bahnhof. It opened on 11 September 2008 and Bahnhof continues to use the facility today. Because of the facility being buried under the mountain, secured by a 40-centimeter thick door, and only reachable by an entrance tunnel, the data center is capable of withstanding a hydrogen bomb. The Pionen data center is also a colocation centre. In 2010 WikiLeaks used Pionen's colocation services to store their servers.\n\nPionen is a data center deep below 30 meters of granite, with three physical datalinks into the mountain. Also, Pionen is located in Central Stockholm, with 1,100 square meters of space. Pionen features fountains, greenhouses, simulated daylight and a huge salt water fish tank. Its data center has two backup power generators, which are actually submarine engines.\n\nIt previously hosted WikiLeaks.\n"}
{"id": "106162", "url": "https://en.wikipedia.org/wiki?curid=106162", "title": "Poem code", "text": "Poem code\n\nThe poem code is a simple, and insecure, cryptographic method which was used by SOE to communicate with their agents in Nazi-occupied Europe.\n\nThe method works by the sender and receiver pre-arranging a poem to use. The sender chooses a set number of words at random from the poem and gives each letter in the chosen words a number. The numbers are then used as a key for some cipher to conceal the plaintext of the message. The cipher used was often double transposition. To indicate to the receiver which words had been chosen an indicator group is sent at the start of the message.\n\nTo encrypt a message, the agent would select words from the poem as the key. Every poem code message commenced with an indicator-group of five letters, which showed which five words of an agent's poem had been used to encrypt the message.\n\nThe words would be written sequentially, and their letters numbered to create a transposition key to encrypt a message. For example, if the words are YEO THOMAS IS A PAIN IN THE ARSE, then the transposition key is: 25 5 16, 23 8 17 13 1 20, 10 21, 2, 18 3 11 14, 12 15, 24 9 6, 4 19 22 7. These are the locations of the first appearances of A's, B, etc. in the sentence.\n\nThis defines a permutation which is used for encryption (25->1, 5->2 etc.). First, the plaintext message is arranged in columns. Then the columns are permuted, and then the rows are permuted.\n\nFor example, the text \"THE OPERATION TO DEMOLISH THE BUNKER IS TOMORROW AT ELEVEN\" would be written on grid paper as:\n<poem>\nTPTTMSEKSOWLN\nHEIOOHBETRAEA\nERODLTURORTVX\nOANEIHNIMOEET\n</poem>\n(The above transposition key requires longer messages which would have at least 25 columns and 25 rows).\n\nSecurity checks: As an additional security measure, the agent would add prearranged errors into the text as security checks. For example, there might be an intentional error in every 18th letter. This was to ensure that, if the agent was captured or the poem was found, the enemy might transmit without the security checks.\n\nThe code's advantage is to provide relatively strong security while not requiring any codebook.\n\nHowever, the encryption process is error-prone when done by hand, and for security reasons, messages should be at least 200 words long.\nThe security check was usually not effective: if a code was used once intercepted and decoded, any security checks were revealed and could often be tortured out from the agent.\n\nThere are a number of other weaknesses\n\nWhen Leo Marks was appointed codes officer of the Special Operations Executive (SOE) in London during World War II, he very quickly recognized the weakness of the technique, and the consequent damage to agents and to their organizations on the Continent, and began to press for changes. Eventually, the SOE began using original compositions (thus not in any published collection of poems from any poet) to give added protection (see \"The Life That I Have\", an example). Frequently, the poems were humorous or overtly sexual to make them memorable (\"Is de Gaulle's prick//Twelve inches thick//Can it rise//To the size//Of a proud flag-pole//And does the sun shine//From his arse-hole?\"). Another improvement was to use a new poem for each message, where the poem was written on fabric rather than memorized.\n\nGradually the SOE replaced the poem code with more secure methods. Worked-out Keys (WOKs) was the first major improvement – an invention of Marks. WOKs are pre-arranged transposition keys given to the agents and which made the poem unnecessary. Each message would be encrypted on one key, which was written on special silk. The key was disposed of, by tearing a piece off the silk, when the message was sent.\n\nA project of Marks, named by him \"Operation Gift-Horse\", was a deception scheme aimed to disguise the more secure WOK code traffic as poem code traffic, so that German cryptographers would think \"Gift-Horsed\" messages were easier to break than they actually were. This was done by adding false duplicate indicator groups to WOK-keys, to give the appearance that an agent had repeated the use of certain words of their code poem. The aim of Gift Horse was to waste the enemy's time, and was deployed prior to D-Day, when code traffic increased dramatically.\n\nThe poem code was ultimately replaced with the one-time pad, specifically the letter one-time pad (LOP). In LOP, the agent was provided with a string of letters and a substitution square. The plaintext was written under the string on the pad. The pairs of letters in each column (such as P,L) indicated a unique letter on the square (Q). The pad was never reused while the substitution square could be reused without loss of security. This enabled rapid and secure encoding of messages.\n\n\n"}
{"id": "6883009", "url": "https://en.wikipedia.org/wiki?curid=6883009", "title": "RF resonant cavity thruster", "text": "RF resonant cavity thruster\n\nA radio frequency (RF) resonant cavity thruster, also known as an EmDrive, is a hypothesized type of propellant-free thruster that was proposed in 2001 by Roger Shawyer. No plausible theory of operation for such drives has been proposed; the theories that were proposed were shown to be inconsistent with known laws of physics, including conservation of momentum and conservation of energy.\n\nSeveral prototypes of this concept have been constructed and tested, including by the Advanced Propulsion Physics Laboratory at NASA. Initially, a few tests of prototype drives were reported to produce a small apparent thrust, but subsequent testing has failed to reliably reproduce these results.\n\nDue to the lack of both a physically plausible theory of operation and of reliably reproducible evidence, many theoretical physicists and commentators consider the device unworkable, explaining the observed thrust as measurement errors. Various media platforms have referred to the engine as the Impossible Drive or the Impossible Space Drive.\n\nElectromagnetic propulsion designs which operate on the principle of reaction mass have been around since the start of the 20th century. In the 1960s, extensive research was conducted on two designs which emit high-velocity ionized gases in similar ways: ion thrusters that convert propellant to ions and accelerate and eject them via electric potentials, and plasma thrusters that convert propellant to plasma ions and accelerate and eject them via plasma currents. In the latter, plasma can be generated from an intense source of microwave or other radio-frequency (RF) energy, and in combination with a resonant cavity, can be tuned to resonate at a precise frequency.\n\nA low-propellant space drive has long been a goal for space exploration, since the propellant is dead weight that must be lifted and accelerated with the ship all the way from launch until the moment it is used (see Tsiolkovsky rocket equation). Gravity assists, solar sails, and beam-powered propulsion from a spacecraft-remote location such as the ground or in orbit, are useful because they allow a ship to gain speed without propellant. However, some of these methods do not work in deep space. Shining a light out of the ship provides a small force from radiation pressure, i.e., using photons as a form of propellant, but the force is far too weak, for a given amount of input power, to be useful in practice.\n\nA true zero-propellant drive is widely believed to be impossible, but if it existed, it could potentially be used for travel in many environments including deep space. Thus, such drives are a popular concept in science fiction, and their improbability contributes to enthusiasm for exploring such designs.\n\nConventional rocket engines expel propellant, such as when ships move masses of water, aircraft move masses of air, or rockets expel exhaust. A drive which does not expel propellant in order to produce a reaction force, providing thrust while being a closed system with no external interaction, would be a reactionless drive. Such a drive would violate the conservation of momentum and Newton's third law, leading many physicists to believe it to be impossible, labelling the idea pseudoscience. On the other hand, a drive that interacts with an external field would be part of an open system, propellantless but not reactionless, like a sail catching and redirecting existing winds to move a ship.\n\nThe first proposal for an RF resonant cavity thruster came from British engineer Roger Shawyer in 2001. He invented a design with a conical cavity, calling it the EmDrive. Guido Fetta later built the Cannae Drive based on Shawyer's concept a resonant thruster with a pillbox-shaped cavity.\nSince 2008, a few physicists have tested their own models, trying to confirm results claimed by Shawyer and Fetta. Juan Yang at Xi'an's Northwestern Polytechnical University (NWPU) initially reported thrust from a model they built, but retracted her claims in 2016 after a measurement error was identified and an improved setup measured no significant thrust. In 2016, Harold White's group at NASA's Advanced Propulsion Physics Laboratory reported a test of their own model had observed 40–100 μN of thrust from inputs of 40–80 W, in the Journal of Propulsion and Power. In December 2016, Yue Chen, part of the communication satellite division of the China Academy of Space Technology (CAST), said his team had tested several prototypes using an \"experimental verification platform\", observed thrust, and was carrying out in-orbit verification. In September 2017, Chen talked about this CAST project again in an interview on CCTV.\n\nThe plausibility of thrusters that emit no propellant, such as the EmDrive, is controversial, primarily because their operation would violate the conservation of momentum.\n\nMedia coverage of experiments using these designs has been controversial and polarized. The EmDrive first drew attention, both credulous and dismissive, when \"New Scientist\" wrote about it as an \"impossible\" drive in 2006. Media outlets were later criticised for misleading claims that a resonant cavity thruster had been \"validated by NASA\" following White's first tentative test reports in 2014. Scientists have continued to note the lack of unbiased coverage, from both polarized sides.\n\nIn 2006, responding to the \"New Scientist\" piece, mathematical physicist John C. Baez at the University of California, Riverside, and Australian science-fiction writer Greg Egan, said the positive results reported by Shawyer were likely misinterpretations of experimental errors.\n\nIn 2014, White's conference paper suggested that resonant cavity thrusters could work by transferring momentum to the \"quantum vacuum virtual plasma.\" Baez and Carroll criticized this explanation, because in the standard description of vacuum fluctuations, virtual particles do not behave as a plasma; Carroll also noted that the quantum vacuum has no \"rest frame\", providing nothing to push against, so it can't be used for propulsion. In the same way, physicists James F. Woodward and Heidi Fearn published two papers showing that the amount of electron-positron virtual pairs of the quantum vacuum, used by White as a virtual plasma propellant, cannot account for thrusts in any isolated, closed electromagnetic system such as a quantum vacuum thruster.\n\nPhysicists Eric W. Davis at the Institute for Advanced Studies in Austin and Sean M. Carroll at the California Institute of Technology said in 2015 that the thrust reported in papers by both Tajmar and White were indicative of thermal effect errors.\n\nIn May 2018, researchers from the Institute of Aerospace Engineering at Technische Universität Dresden, Germany, concluded that the apparent thrust is clearly an artifact caused by Earth's magnetic field interacting with power cables in the chamber, a result that other experts agree with.\n\nIn 2001, Shawyer founded \"Satellite Propulsion Research Ltd\", in order to work on the EmDrive, a drive that he said used a resonant cavity to produce thrust without propellant. The company was backed by SMART award grants from the UK Department of Trade and Industry. In December 2002, he described a working prototype with an alleged total thrust of about 0.02 newtons powered by an 850 W cavity magnetron. The device could operate for only a few dozen seconds before the magnetron failed, due to overheating.\n\nIn October 2006, Shawyer conducted tests on a new water-cooled prototype and said that it had increased thrust. He planned to have the device ready to use in space by May 2009 and was considering making the resonant cavity a superconductor.\n\n\"New Scientist\" magazine featured the EmDrive on the cover of 8 September 2006 issue. The article portrayed the device as plausible and emphasized the arguments of those who held that point of view. Science fiction author Greg Egan distributed a public letter stating that \"a sensationalist bent and a lack of basic knowledge by its writers\" made the magazine's coverage unreliable, sufficient \"to constitute a real threat to the public understanding of science\". Especially, Egan said he was \"gobsmacked by the level of scientific illiteracy\" in the magazine's coverage, alleging that it used \"meaningless double-talk\" to obfuscate the problem of conservation of momentum. The letter was endorsed by mathematical physicist John C. Baez and posted on his blog. \"New Scientist\" editor Jeremy Webb responded to critics: \"New Scientist\" also published a letter from the former technical director of EADS Astrium: A letter from physicist Paul Friedlander: \n\nIn 2007, the UK Department of Trade and Industry granted SPR an export licence to Boeing in the US. In December 2008, Shawyer was invited to The Pentagon to present on the EmDrive, and in 2009 Boeing confirmed they wanted to license the technology. The UK Ministry of Defence agreed to a technology transfer, and SPR designed, built and tested a thruster for use on a test satellite. According to Shawyer, the 10-month contract was completed by July 2010 and the thruster, giving 18 grams of thrust, was transferred to Boeing. Boeing did not, however, license the technology and communication stopped. Questioned on the matter in 2012, a Boeing representative confirmed that\nBoeing Phantom Works used to explore exotic forms of space propulsion, including Shawyer's drive, but such work has since ceased. They confirmed that \"Phantom Works is not working with Mr. Shawyer,\" adding that the company is no longer pursuing those explorations.\n\nIn 2013 and 2014, Shawyer presented ideas for 'second-generation' EmDrive designs and applications, at the annual International Astronautical Congress. A paper based on his 2014 presentation was published in \"Acta Astronautica\" in 2015. It describes a model for a superconducting resonant cavity and three models for thrusters with multiple cavities, with hypothetical applications for launching space probes.\n\nIn October 2016, a UK patent application describing a new superconducting EmDrive was published, followed by a first international version. Shortly thereafter Shawyer unveiled the creation of \"Universal Propulsion Ltd.\", a new company aimed to develop and commercialise such thrusters, as a joint venture with \"Gilo Industries Group\", a small UK aerospace company designing and selling paramotors and the Parajet Skycar.\n\nThe Cannae Drive (formerly Q-drive), another engine designed to generate propulsion from a resonant cavity without propellant, is another implementation of this idea. Its cavity is also asymmetric, but relatively flat rather than a truncated cone. It was designed by Fetta in 2006 and has been promoted within the US through his company, Cannae LLC, since 2011. In 2016, Fetta announced plans to eventually launch a CubeSat satellite containing a version of the Cannae Drive, which they would run for 6 months to observe how it functions in space.\n\nIn China, researchers working under Yang at NWPU developed their own prototype resonant cavity thruster in 2008, publishing a report in their university's journal on the theory behind such devices. In 2012 they measured thrust from their prototype, however, in 2014 they found this had been an experimental error. A second, improved prototype did not produce any measured thrust.\n\nAt the China Academy of Space Technology, Yue Chen filed several patent applications in 2016 describing various RF resonant cavity thruster designs. These included a method for stacking several short resonant cavities to improve thrust, and a design with a cavity that was a semicylinder instead of a frustum. That December, Chen announced that CAST was conducting tests on a resonant cavity thruster in orbit, without specifying what design was used. In an interview on CCTV in September 2017, Chen Yue showed some testing of a flat cylindrical device corresponding to the patent describing stacked short cavities with internal diaphragms.\n\nThe proposed theory for how the EmDrive works violates the conservation of momentum, which states any interaction cannot have a net force; a consequence of the conservation of momentum is Newton's third law, where for every action there is an equal and opposite reaction. The conservation of momentum is a symmetry of nature.\n\nIn instances where matter appears to violate conservation laws, the apparent non-conservation is in reality an interaction with the vacuum so that overall symmetry in the system is restored. An often cited example of apparent nonconservation of momentum is the Casimir effect; in the standard case where two parallel plates are attracted to each other. However the plates move in opposite directions, so no net momentum is extracted from the vacuum and, moreover, the energy must be put into the system to take the plates apart again.\n\nAssuming homogeneous electric and magnetic fields, it is impossible for the EmDrive, or any other device, to extract a net momentum transfer from either a classical or quantum vacuum.\nExtraction of a net momentum \"from nothing\"\nhas been postulated in an inhomogeneous vacuum, but this remains highly controversial as it will violate Lorentz invariance.\n\nBoth Harold White's\nand Mike McCulloch's theories of how the EmDrive could work rely on these asymmetric or dynamical Casimir effects. However, if these vacuum forces are present, they are expected to be exceptionally tiny based on our current understanding, too small to explain the level of observed thrust.\nIn the event that observed thrust is not due to experimental error, a positive result could indicate new physics.\n\nCritics liken the EmDrive to trying to move a car by getting inside and pushing on the windshield. This violation of the fundamental principles of physics has drawn criticism from the scientific community, leading to various attempts to explain the apparent or observed thrust. However, to date, there is no acceptance or consensus on how or why these cavities produce thrust if they produce thrust at all.\n\nAttempts to explain the thrust (assuming that there is thrust) generally fall into four categories:\n\nThe simplest and most likely explanation is that any thrust detected is due to experimental error or noise. In all of the experiments set up, a very large amount of energy goes into generating a tiny amount of thrust. When attempting to measure a small signal superimposed on a large signal, the noise from the large signal can obscure the small signal and give incorrect results. The strongest early result, from Yang's group in China, was later reported to be caused by an experimental error.\n\nThe largest error source is believed to come from the thermal expansion of the thruster's heat sink; as it expands this would lead to a change in the centre of gravity causing the resonant cavity to move. White's team attempted to model the thermal effect on the overall displacement by using a superposition of the displacements caused by \"thermal effects\" and \"impulsive thrust\" with White saying \"That was the thing we worked the hardest to understand and put in a box\". Despite these efforts, White's team were unable to fully account for the thermal expansion. In an interview with \"Aerospace America\", White comments that \"although maybe we put a little bit of a pencil mark through [thermal errors]... they are certainly not black-Sharpie-crossed-out.\"\n\nTheir method of accounting for thermal effects has been criticized by Millis and Davies, who highlight that there is a lack of both mathematical and empirical detail to justify the assumptions made about those effects. For example, they do not provide data on temperature measurement over time compared to device displacement. The paper includes a graphical chart, but it is based on \"a priori\" assumptions about what the shapes of the \"impulsive thrust\" and \"thermal effects\" should be, and how those signals will superimpose. The model further assumes all noise to be thermal and does not include other effects such as interaction with the chamber wall, power lead forces, and tilting. Because the Eagleworks paper has no explicit model for thrust to compare with the observations, it is ultimately subjective, and its data can be interpreted in more than one way. The Eagleworks test, therefore, does not conclusively show a thrust effect, but cannot rule it out either.\n\nWhite suggested future experiments could run on a Cavendish balance. In such a setup, the thruster could rotate out to much larger angular displacements, letting a thrust (if present) dominate any possible thermal effects. Testing a device in space would also eliminate the center-of-gravity issue.\n\nAnother source of error could have arisen from electromagnetic interaction with the walls of the vacuum chamber. White argued that any wall interaction could only be the result of a well-formed resonance coupling between the device and wall and that the high frequency used imply the chances of this would be highly dependent on the device's geometry. As components get warmer due to thermal expansion, the device's geometry changes, shifting the resonance of the cavity. In order to counter this effect and keep the system in optimal resonance conditions, White used a phase-locked loop system (PLL). Their analysis assumed that using a PLL ruled out significant electromagnetic interaction with the wall.\n\nAnother potential source of error was a Lorentz force arising from power leads. Many previous experiments used cups with Galinstan metal alloy, which is liquid at room temperature, to supply electrical power to the device in lieu of solid wires. Martin Tajmar and his graduate student Fiedler characterized and attempted to quantify possible sources of error in their experiment at Dresden University of Technology. They ran multiple tests on their experimental setup, including measurements of the force along different axes with respect to the power supply current. While eliminating or accounting for many other sources of error in previous experiments, such as replacing a magnetic damping mechanism with an oil damper, less efficient but significantly less interacting with electromagnetic field, the study remained inconclusive as to the effects of electromagnetic interaction with the apparatus' power feed, at the same time noting it as possibly the most significant source of noise. White's power setup may have been different, but their paper does not state if the connections are all coaxially aligned with the stand's rotation axis, which would be required to minimize errors from Lorentz forces, and it gives no data from equivalent tests with power into a dummy load so these influences can be compared with those seen in the Tajmar-Fiedler run.\n\nWhite's 2016 paper went through about a year of peer review involving five referees. Peer review does not mean the results or observations are true, only that the referees looked at the experiment, results and interpretation and found it to be sound and sensible. Brice Cassenti, a professor at the University of Connecticut and an expert in advanced propulsion, spoke to one of the referees, and reported the referee did not believe the results point to any new physics, but that the results were puzzling enough to publish. Cassenti believes there is a mundane explanation for the results, but the probability of the results being valid is slim but not zero.\n\nWhite's paper was published in the \"Journal of Propulsion and Power\". Marc Millis and Eric Davies who ran NASA's previous advanced propulsion project, the Breakthrough Propulsion Physics Program have commented that while White used techniques that would be acceptable for checking the electric propulsion of Hall thrusters, the tests were not sufficient to demonstrate that any new physics effect exists.\n\nIn 2004, Shawyer reported seven independent positive reviews from experts at BAE Systems, EADS Astrium, Siemens and the IEE, but these were disputed. In a letter to \"New Scientist\", the then-technical director of EADS Astrium (Shawyer's former employer) denied this: \n\nIn 2011, Fetta tested a superconducting version of the Cannae drive. The RF resonant cavity was suspended inside a liquid helium-filled dewar. The weight of the cavity was monitored by load cells. Fetta theorized that when the device was activated and produced upward thrust, the load cells would detect the thrust as a change in weight. When the drive was energized by sending 10.5 watt power pulses of RF power into the resonant cavity, there was, as predicted, a reduction in compressive force on the load cells consistent with thrust of 8–10 mN.\n\nNone of these results have been published in the scientific literature, or replicated by independent researchers. They have been posted on their inventors' websites.\n\nIn 2015, Shawyer published an article in \"Acta Astronautica\", summarising existing tests on the EmDrive. Of seven tests, four produced a measured force in the intended direction and three produced thrust in the opposite direction. Furthermore, in one test, thrust could be produced in either direction by varying the spring constants in the measuring apparatus.\n\nIn 2008, a team of Chinese researchers led by Juan Yang (杨涓), professor of propulsion theory and engineering of aeronautics and astronautics at Northwestern Polytechnical University (NWPU) in Xi'an, China, said that they had developed a valid electro-magnetic theory behind a microwave resonant cavity thruster. A demonstration version of the drive was built and tested with different cavity shapes and at higher power levels in 2010. Using an aerospace engine test stand usually used to precisely test spacecraft engines like ion drives, they reported a maximum thrust of 720 mN at 2,500 W of input power. Yang noted that her results were tentative, and said she \"[was] not able to discuss her work until more results are published\". This positive result was over 100x more thrust per input power than any other experiment, and inspired other groups to try to replicate their work.\n\nIn a 2014 follow-up experiment (published in 2016), Yang could not reproduce the 2010 observation and suggested it was due to experimental error. In that experiment they refined their experimental setup, using a three-wire torsion pendulum to measure thrust, and tested two different power setups. In one trial, the power system was outside the cavity, and they observed a \"thrust\" of 8–10 mN. In a second trial, the power system was within the cavity, and they measured no such thrust. Instead they observed an insignificant thrust below their noise threshold of 3 mN, fluctuating between ±0.7 mN with a measurement uncertainty of 80%, with 230 W of input power. They concluded that they were unable to measure significant thrust; that \"thrust\" measured when using external power sources (as in their 2010 experiment) could be noise; and that it was important to use self-contained power systems for these experiments, and more sensitive pendulums with lower torsional stiffness.\n\nSince 2011, White has had a team at NASA known as the Advanced Propulsion Physics Laboratory, or Eagleworks Laboratories, which is devoted to studying exotic propulsion concepts. The group has investigated ideas for a wide range of untested and fringe proposals, including Alcubierre drives, drives that interact with the quantum vacuum, and RF resonant cavity thrusters.\n\nIn 2014, the group began testing resonant cavity thrusters of their own design and sharing some of their results. In November 2016, they published their first peer-reviewed paper on this work, in the \"Journal of Propulsion and Power\".\n\nIn July 2014, White reported tentative positive results for evaluating a tapered RF resonant cavity. Testing was performed using a low-thrust torsion pendulum able to detect force at the micronewton level within a sealed but unevacuated vacuum chamber (the RF power amplifier used an electrolytic capacitor unable to operate in a hard vacuum). The experimenters recorded directional thrust immediately upon application of power.\n\nTheir first tests of this tapered cavity were conducted at very low power (2% of Shawyer's 2002 experiment). A net mean thrust over five runs was measured at 91.2 µN at 17 W of input power. The experiment was criticized for its small data set and for not having been conducted in vacuum, to eliminate thermal air currents.\n\nThe group announced a plan to upgrade their equipment to higher power levels, to use vacuum-capable RF amplifiers with power ranges of up to 125 W, and to design a new tapered cavity that could be in the 0.1 N/kW range. The test article was to be subject to independent verification and validation at Glenn Research Center, the Jet Propulsion Laboratory and the Johns Hopkins University Applied Physics Laboratory. , this validation has not happened.\n\nIn 2015, Paul March from Eagleworks made new results public, measured with a torsional pendulum in a hard vacuum: about 50 µN with 50 W of input power at 5.0×10 torr. The new RF power amplifiers were said to be made for hard vacuum, but failed rapidly due to internal corona discharges. Without funding to replace or upgrade them, measurements were scarce for a time.\n\nThey conducted further experiments in vacuum, a set of 18 observations with 40-80W of input power. They published the results in the American Institute of Aeronautics and Astronautics's peer-reviewed \"Journal of Propulsion and Power\", under the title \"Measurement of Impulsive Thrust from a Closed Radio-Frequency Cavity in Vacuum\". This was released online in November 2016, with print publication in December. The study said that the system was \"consistently performing with a thrust-to-power ratio of 1.2±0.1mN/kW\", and enumerated many potential sources of error.\n\nThe paper suggested that pilot-wave theory (a controversial, non-mainstream deterministic interpretation of quantum mechanics) could explain how the device produces thrust. Commenters pointed out that just because a study reporting consistent thrust was published with peer-review does not necessarily mean that the drive functions as claimed. Physicist Ethan Siegal commented on the paper, saying that the drive most likely does not violate conservation of momentum as this would \"make physics fall apart\" but rather that there is something else going on. He said that \"Whether it's new physics [or] the effect's cause simply hasn't been determined yet, more and better experiments will be the ultimate arbiter\". Physicist Chris Lee was very critical of the work, saying that the paper had a small data set and a number of missing details he described as 'gaping holes'. Electrical Engineer George Hathaway analyzed and criticized the scientific method described in the paper.\n\nWhite's 2014 tests also evaluated two Cannae drive prototypes. One had radial slots engraved along the bottom rim of the resonant cavity interior, as required by Fetta's hypothesis to produce thrust; another \"null\" test article lacked those radial slots. Both drives were equipped with an internal dielectric. A third test article, the experimental control, had an RF load but no resonant cavity interior. These tests took place at atmospheric pressure.\n\nAbout the same net thrust was reported for both the device with radial slots and the device without slots. Thrust was not reported for the experimental control. Some considered the positive result for the non-slotted device a possible flaw in the experiment, as the null test device had been expected to produce less or no thrust based upon Fetta's hypothesis of how thrust was produced by the device. In the complete paper, however, White concluded that the test results proved that \"thrust production was not dependent upon slotting\".\n\nIn July 2015, an aerospace research group at the Dresden University of Technology (TUD) under Martin Tajmar reported results for an evaluation of an RF resonant tapered cavity similar to the EmDrive. Testing was performed first on a knife-edge beam balance able to detect force at the micronewton level, atop an antivibration granite table at ambient air pressure; then on a torsion pendulum with a force resolution of 0.1 mN, inside a vacuum chamber at ambient air pressure and in a hard vacuum at .\n\nThey used a conventional ISM band 2.45 GHz 700 W oven magnetron, and a small cavity with a low Q factor (20 in vacuum tests). They observed small positive thrusts in the positive direction and negative thrusts in the negative direction, of about 20 µN in a hard vacuum. However, when they rotated the cavity upwards as a \"null\" configuration, they observed an anomalous thrust of hundreds of micronewtons, significantly larger than the expected result of zero thrust. This indicated a strong source of noise which they could not identify. This led them to conclude that they could not confirm or refute claims about such a thruster. At the time they considered future experiments with better magnetic shielding, other vacuum tests and improved cavities with higher \"Q\" factors.\n\nIn 2018, the TU Dresden research team presented a conference paper summarizing the results from the most recent experiments on their upgraded test rig, which seemed to show that their measured thrust was a result of experimental error from insufficiently shielded components interacting with the earth's magnetic field. In their experiments, they measured thrust values consistent with previous experiments, and the thrust reversed appropriately when the thruster was rotated by 180°. However, the team also measured thrust perpendicular to the expected direction when the thruster was rotated by 90°, and did not measure a reduction in thrust when an attenuator was used to reduce the power that actually entered the resonant cavity by a factor of 10,000, which they said \"clearly indicates that the \"thrust\" is not coming from the EMDrive but from some electromagnetic interaction.\" They concluded that \"magnetic interaction from not sufficiently shielded cables or thrusters are a major factor that needs to be taken into account for proper μN thrust measurements for these type of devices,\" and they plan on conducting future tests at higher power and at different frequencies, and with improved shielding and cavity geometry.\n\nIn August 2016, Cannae announced plans to launch its thruster on a 6U cubesat which they would run for 6 months to observe how it functions in space. Cannae has formed a company called Theseus for the venture and partnered with LAI International and SpaceQuest Ltd. to launch the satellite. No launch date has yet been announced.\n\nIn November 2016, the \"International Business Times\" published an unconfirmed report that the U.S. government was testing a version of the EmDrive on the Boeing X-37B and that the Chinese government has made plans to incorporate the EmDrive on its orbital space laboratory Tiangong-2. The US Air Force has only confirmed that the X-37B mission in question did an electric propulsion system test using a Hall-effect thruster, a type of ion thruster that uses a gaseous propellant.\n\nIn December 2016, Yue Chen told a reporter at China's \"Science and Technology Daily\" that his team was testing an EmDrive in orbit, and that they had been funding research in the area for five years. Chen noted that their prototype's thrust was at the \"micronewton to millinewton level\", which would have to be scaled up to at least 100–1000 millinewtons for a chance of conclusive experimental results. Despite this, he said his goal was to complete validation of the drive, and then to make such technology available in the field of satellite engineering \"as quickly as possible\".\n\nThe drive is featured in \"Salvation\", an American Sci-Fi suspense drama. It is described as a theoretically impossible piece of technology, but the protagonists manage to make it work using an exotic space-originated crystal extracted from an asteroid.\n\n"}
{"id": "10194891", "url": "https://en.wikipedia.org/wiki?curid=10194891", "title": "Raleigh Bicycle Company", "text": "Raleigh Bicycle Company\n\nThe Raleigh Bicycle Company is a bicycle manufacturer based in Nottingham, England. Founded by Woodhead and Angois in 1885, who used Raleigh as their brand name, it is one of the oldest bicycle companies in the world. After being acquired by Frank Bowden, it became The Raleigh Cycle Company in December 1888, which was registered as a limited liability company in January 1889. By 1913, it was the biggest bicycle manufacturing company in the world. From 1921 to 1935, Raleigh also produced motorcycles and three-wheel cars, leading to the formation of the Reliant Company. The Raleigh division of bicycles is currently owned by the Dutch corporation Accell.\n\nIn 2006, the Raleigh Chopper was named in the list of British design icons in the Great British Design Quest organised by the BBC and the Design Museum.\n\nThe history of Raleigh bicycles started in 1885, when Richard Morriss Woodhead from Sherwood Forest, and Paul Eugene Louis Angois, a French citizen, set up a small bicycle workshop in Raleigh Street, Nottingham, England. In the spring of that year, they started advertising in the local press. The \"Nottinghamshire Guardian\" of 15 May 1885 printed what was possibly the first Woodhead and Angois classified advertisement.\nNearly two years later, the 11 April 1887 issue of \"The Nottingham Evening Post\" contained a display advertisement for the Raleigh ‘Safety’ model under the new banner ‘Woodhead, Angois, and Ellis. Russell Street Cycle Works.’ William Ellis had recently joined the partnership and provided much-needed financial investment. Like Woodhead and Angois, Ellis’s background was in the lace industry. He was a lace gasser, a service provider involved in the bleaching and treating of lace, with premises in nearby Clare Street and Glasshouse Street. Thanks to Ellis, the bicycle works had now expanded round the corner from Raleigh Street into former lace works on the adjoining road, Russell Street. By 1888, the company was making about three cycles a week and employed around half a dozen men. It was one of 15 bicycle manufacturers based in Nottingham at that time.\n\nFrank Bowden, a recent convert to cycling who on medical advice had toured extensively on a tricycle, first saw a Raleigh bicycle in a shop window in Queen Victoria Street, London, about the time that William Ellis’s investment in the cycle workshop was beginning to take effect. Bowden described how this led to him visiting the Raleigh works:\n\nIt is clear from Frank Bowden’s own account that, although he bought a Raleigh ‘Safety’ in 1887, he did not visit the Raleigh workshop until autumn 1888. That visit led to Bowden replacing Ellis as the partnership’s principal investor, though Bowden did not become the outright owner of the firm. He concluded that the company had a profitable future if it promoted its innovative features, increased its output, cut its overhead costs and tailored its products to the individual tastes and preferences of its customers. He bought out William Ellis’s share in the firm and was allotted 5,000 £1 shares, while Woodhead and Angois between them held another 5,000 shares.\n\nIn Frank Bowden's own lifetime, Raleigh publicity material stated that the firm was founded in 1888, which was when Bowden, as he himself confirmed, first bought into the enterprise. Thus, Raleigh's 30th anniversary was celebrated in 1918. The 1888 foundation date is confirmed by Bowden's great-grandson, Gregory Houston Bowden, who states that Frank Bowden \"began to negotiate with Woodhead and Angois and in December 1888 founded 'The Raleigh Cycle Company'.\" The December 1888 foundation date is also confirmed by Nottinghamshire Archives. In recent years, the Raleigh company has cited 1887 as a foundation date but, whilst this pre-dates Bowden's involvement, the Raleigh brand name was created by Woodhead and Angois and the enterprise can, as demonstrated above, be traced back to 1885.\n\nThe company established by Bowden in December 1888 was still privately owned with unlimited public liability. In January 1889, it became the first of a series of limited liability companies with Raleigh in its name. It had a nominal capital of £20,000, half of which was provided by Frank Bowden. Paul Angois was appointed director responsible for product design, Richard Woodhead was made director responsible for factory management, and Frank Bowden became chairman and managing director. Some shares were made available to small investors and local businessmen, but take-up was minimal, and Bowden ended up buying most of the public shares. He subsequently supplied virtually all the capital needed to expand the firm.\n\nWhen Frank Bowden got involved with the enterprise, the works comprised three small workshops and a greenhouse. As Woodhead, Angois and Ellis, the firm had expanded round the corner from Raleigh Street into Russell Street, where also stood Clarke’s five-storey former lace factory. To enable further expansion of the business, Bowden financed the renting of this property and installation of new machinery.\n\nUnder Bowden's guidance, Raleigh expanded rapidly. By 1891, the company occupied not only Clarke's factory but also Woodroffe’s Factory and Russell Street Mills. In November 1892, Raleigh signed a tenancy agreement for rooms in Butler’s factory on the other side of Russell Street. Shortly after this, the company also occupied Forest Road Mill. (Forest Road junctions with Russell Street at the opposite end from Raleigh Street.)\n\nBowden created a business which, by 1913, was the biggest bicycle manufacturing company in the world, occupying seven and a half acres in purpose-built premises completed in 1897 at Faraday Road, Lenton, Nottingham. It subsequently became very much bigger.\n\nSir Frank Bowden died in 1921 and his son Sir Harold Bowden, 2nd Baronet took over as\nchairman and chief executive, guiding the company through the next 17 years of expansion.\nThere was a resurgence in domestic and export demand for pedal bicycles and by February 1932 Raleigh had acquired all the Humber Limited trade marks. Manufacture was transferred to Raleigh's Nottingham works. Raleigh-made Humbers differed from Raleighs only in chainwheels, fork crowns and some brakework.\n\nDuring the Second World War, the Raleigh factory in Nottingham was used for the production of fuzes. Bicycle production was reduced to approximately 5% of its peacetime capacity.\n\nIn 1939, Raleigh opened a bicycle factory at 6 Hanover Quay, Dublin, Ireland and commenced bicycle production there. The Raleigh (Ireland) business expanded and moved to 8–11 Hanover Quay, Dublin in 1943. The plant produced complete bicycles and Sturmey-Archer hubs, and remained in production until 1976, when the factory burned down. Models produced there latterly were the Chopper and Triumph 20. The head badges changed in the late 1960s, possibly after the passing of the Trade Descriptions Act in the UK. Dublin-made machines no longer had \"Nottingham England\" on the Heron or Triumph head badge, the panel being left blank instead.\nIn 1899, Raleigh started to build motorcycles and in 1903, introduced the Raleighette, a belt-driven three-wheel motorcycle with the driver in the back and a wicker seat for the passenger between the two front wheels. Financial losses meant production lasted only until 1908.\n\nIn 1930, the company acquired the rights to the Ivy Karryall, a motorcycle fitted with a cabin for cargo and a hood for the driver. Raleigh's version was called the Light Delivery Van and had a chain drive. A two-passenger version was followed by Raleigh's first three-wheel car, the Safety Seven. It was a four-seat convertible with shaft drive and a maximum of . A saloon version was planned, but Raleigh shut its motor department to concentrate on bicycles again. Chief designer T. L. Williams took the equipment and remaining parts and moved to Tamworth, where his company produced three-wheelers for 65 years. The leftover parts from Raleigh carried an \"R\", so Williams chose a matching name: Reliant.\nRaleigh also made mopeds in the late 1950s and 1960s as the bicycle market declined. The most popular of which was the RM6 Runabout. This model featured unsprung front forks and a cycle type calliper front brake which made it a very affordable mode of transport. Because of its success, production continued until February 1971; 17 months after Raleigh had stopped manufacturing all other mopeds.\n\nAfter World War II, Raleigh became known for its lightweight \"sports roadster\" bicycles, often using Sturmey-Archer three and five-speed transmissions. These cycles were considerably lighter and quicker than either the old heavy English utility roadster or the American \"balloon-tire\" cruiser bikes. In 1946, Raleigh and other English bicycle manufacturers accounted for 95% of the bicycles imported into the United States.\n\nRaleigh's \"sports roadster\", or \"British racer\" bicycles were exported around the world, including the United States. The company continued to increase imports to the United States until 1955, when a rate increase in foreign bicycle tariffs caused a shift in imports in favour of bicycles from West Germany and the Netherlands. However, this proved only a temporary setback, and by 1964, Raleigh was again a major selling brand in the US bicycle market.\n\nIn 1965, Raleigh introduced the RSW 16, its long-awaited competitor to the hugely successful Moulton Bicycle. The new Raleigh shared several important features with the Moulton, including small wheels, an open frame and built-in luggage carrying capacity.\n\nHowever, the RSW lacked the Moulton's suspension, which compensated for the bumpy ride that comes with small wheels. Instead, Raleigh fitted the RSW with balloon tyres, which effectively smoothed the ride but at the cost of increased rolling resistance. Nevertheless, the RSW was pleasant to ride, and Raleigh's extensive retail network ensured its success.\n\nThe success of the RSW took sales away from the Moulton and put the maker into financial difficulties. Raleigh then bought out Moulton and produced both bikes until 1974. Raleigh also produced a sister model to the RSW, the 'Twenty', which was more successful and remained in production well into the 1980s.\n\nWhile bicycle production had steadily risen through the mid-1950s, the British market began to decline with the increasing affordability and popularity of the motor car. For much of the postwar era, British bicycle manufacturers had largely competed with each other in both the home and export markets, but 1956 saw the formation of the British Cycle Corporation by the Tube Investments Group which already owned Phillips, Hercules, Armstrong, and Norman. In 1957, Raleigh bought the BSA Cycles Ltd., BSA's bicycle division, which gave them exclusive use of the former brand names New Hudson and Sunbeam. Raleigh also already owned the Robin Hood brand, and Three Spires with Triumph (cycles) also at their disposal.\n\nBSA had itself acquired Triumph Cycle Co. Ltd. only five years previously. Ti added the Sun bicycle company to their stable in 1958, and with two \"super groups\" now controlling a large portion of the market, it was perhaps inevitable that in 1960, Tube Investments acquired Raleigh and merged the British Cycle Corporation with Raleigh to form TI–Raleigh, which now had 75% of the UK market. TI–Raleigh then acquired Carlton Cycles in Worksop, England that same year, at the time one of the largest semi-custom lightweight makers in the UK. Ti Raleigh gave total control of its cycle division to Raleigh and soon set about marketing many of the acquired names as budget ranges, though with Raleigh frames. The old British Cycle Corporation factory at Handsworth continued to produce non Raleigh branded product well into the 1970s, with Raleigh branded models built in the main plant at Nottingham. However, the Sun branded bicycles were made in the Carlton factory at Worksop, England.\n\nAs a vertically integrated manufacturer in the mid-1960s, TI–Raleigh owned Brooks (one of the oldest saddle makers in the world), Sturmey-Archer (pioneer of 3-speed hubs), and Reynolds (maker of 531 tubing). Carlton, which had been unable to make inroads in the USA market after a failed rebranding deal with Huffy, found success in the late 1960s by recasting itself as \"Raleigh-Carlton\", a Raleigh-logo'd bike with some Carlton badging, and using the US dealer network to import and distribute bikes.\n\nThe Raleigh Chopper was designed by Nottingham native Alan Oakley, though this has been disputed by Cambridge designer Tom Karen. The Chopper was patented in the UK in 1967 and patented in the US in 1968. The bike was the \"must have\" item and signifier of \"coolness\" for many children at the time. The Chopper was first available for sale in June 1969 in North America. It went on sale in the UK in 1970 and sold well, and was a key factor in reviving the company's fortunes. The \"Chopper\" featured a 3-speed Sturmey-Archer gear hub, shifted using a top-tube mounted gear lever reminiscent of the early Harley-Davidson suicide shifter — one of its \"cool\" features. Other differences were the unusual frame, long padded seat with backrest, sprung suspension at the back, high-rise handlebars, and differently sized front (16\") and rear (20\") wheels. Tyres were wider than usual for the time, with a chunky tread on the rear wheel, featuring red highlights on the sidewall. The price was from approximately £32 for a standard \"Chopper\" to £55 for the deluxe. Two smaller versions, the \"Chipper\" and \"Tomahawk\", also sold well.\n\nThe Mk 2 \"Chopper\" was an improved version from 1972. It had the option of five-speed derailleur gears in the United States, but all UK bikes had the 3 speed hub, with the exception of a model introduced in 1973 and only available in a bizarre shade of pink. This model was discontinued in 1976. The Mk 2 had a shorter seat and the frame modified to move the rear of the seat forward, this helped prevent the bike tipping up. The shorter seat also made it harder to ride '2 up' (2 people on the bike at a time). The \"Chopper\" remained in production until 1982, when the rising popularity of the BMX bicycle caused sales to drop off.\n\nRaleigh revisited the chopper design in recent times, with great success although the new version has had some changes to conform to modern safety laws. Gone is the top tube shifter and long integrated seat, but the look and feel of the bike remain.\n\nIn 1979, production of Raleigh 531 butted-tube bicycles reached 10,000 units a year. In 1980, the former Carlton factory at Worksop closed and production was moved to a Lightweights facility at Nottingham. However, all bicycles made there afterward still carried the W for Worksop frame number designation. In 1982, rights to the \"Raleigh USA\" name were purchased by the Huffy Corporation. Under the terms of the agreement, \"Raleigh of England\" licensed Huffy to design and distribute Raleigh bicycles in the USA, and Huffy was given instant access to a nationwide network of bike shops. The renamed \"Raleigh Cycle Company of America\" sold bikes in the US while the rest of the world, including Canada, received \"Raleigh of England\" bikes. At that time, production of some U.S. Raleigh models were shifted to Japan, with Bridgestone manufacturing most of these bikes. By 1984, all Raleighs for the American market, except the top-of-the range Team Professional (made in Ilkeston) and Prestige road bikes (made in Nottingham), were produced in the Far East. Meanwhile, in the home market, Raleigh had broken into the new UK BMX market with their Burner range, which was very successful.\n\nIn 1987, the leading German bicycle manufacturer Derby Cycle bought Raleigh from Ti and Raleigh USA from Huffy. In 1988, Derby opened a factory in Kent, Washington manufacturing two Raleigh lines, the bimetallic \"Technium\" road bike line, which used heat-treated aluminum main frame tubes, thermally bonded and heat-cured to internal steel lugs using a Boeing-developed proprietary epoxy — along with chromoly steel head tube and rear stays. Kent also manufactured the off-road chromoly steel \"Altimetric\" line (Tangent CX, Traverse CX, Tactic CX and Talon CX 1991-1992). The factory closed in 1994. All \"Raleigh Cycle Company of America\" parts and frames from 1995 on were then mass-produced in China and Taiwan and assembled in other plants.\n\nThe high-end framesets offered for sale in Raleigh catalogues together with the frames built for Team riders were produced in Ilkeston by the Special Bicycle Developments Unit (SBDU) from 1974 to 1989 under the guidance of Gerald V O'Donovan; this production was moved to a new \"Raleigh Special Products\" division in Nottingham on closure of the Kent factory.\n\n\"Raleigh Canada\" had a factory in Waterloo, Quebec from 1972 to 2013. Derby Cycle acquired Diamondback Bicycles in 1999. In the same year, Raleigh ceased volume production of frames in the UK and its frame-making equipment were sold by auction.\n\nIn 2000, Derby Cycle controlled Raleigh USA, Raleigh UK, Raleigh Canada, and Raleigh Ireland. In the latter three markets, Raleigh was the number-one manufacturer of bicycles. Derby Cycle began a series of divestitures, because of financial pressure and sold Sturmey-Archer's factory site to the University of Nottingham and Sturmey-Archer and saddle manufacturer Brooks to a small company called Lenark. Lenark promised to build a new factory in Calverton but failed to pay the first instalment and the company entered liquidation. It was reported that the reason for selling the business, after extracting the cash for the factory site, was to have Lenark declare it insolvent so that neither Derby nor Lenark would have to pay the redundancy costs. Sturmey-Archer's assets were acquired by SunRace of Taiwan who relocated the factory to Taiwan and sales to the Netherlands. Sister company Brooks was sold to Selle Royal of Italy.\n\nIn 2001, following continuing financial problems at Derby Cycle, there was a management buy-out of all the remaining Raleigh companies led by Alan Finden-Crofts.\n\nBy 2003, assembly of bicycles had ended in the UK with 280 assembly and factory staff made redundant, and bicycles were to come \"from Vietnam and other centres of 'low-cost, high-quality' production.\" with final assembly takes place in Cloppenburg, Germany.\n\nIn 2012, Derby was acquired by Pon, a Dutch company, as part of their new bicycle group, which also owns Gazelle and Cervélo. Pon now sell Raleigh under licence throughout Germany.\n\nIn April 2012, Raleigh UK, Canada and USA were acquired by a separate Dutch group Accell for £62m (US$100m), whose portfolio includes the Lapierre and Ghost bicycle brands.\n\nRaleigh had a long association with cycle sport. Most notable is the TI–Raleigh team of the 1970s and 1980s. In 1980 Joop Zoetemelk won the Tour de France on a Raleigh. In the mid-1980s the Raleigh team was co-sponsored by Panasonic. In 1984, riding Raleigh-badged bicycles, Team USA scored several impressive victories at the Olympic Games in Los Angeles. The company also supplied bicycles to the French Système U team in the late 1980s where Laurent Fignon lost the 1989 Tour de France to Greg LeMond by 8 seconds. The company's special products division made race frames, including those used by the Raleigh professional team of the 1970s. Presently Raleigh as a company owns the Diamondback Bike brand as well. During the 1980s Raleigh also supported British professional teams, including \"Raleigh Banana\" and \"Raleigh Weinmann\". Raleigh's most notable riders were Paul Sherwin, Malcolm Elliott, Mark Bell, Paul Watson, Jon Clay and Jeff Williams. It also sponsored a mountain bike team in the early 1990s that also raced in road events.\n\nIn 2009 it was announced that the company would be creating a new Continental-level cycling team called Team Raleigh. The Team were co-sponsored by the global shipping and logistics firm GAC in 2012 and were known as Team Raleigh-GAC. The season was notable for Team Raleigh's first victory in the Tour Series Round 6 and a succession of Premier Calendar wins, which resulted in team rider Graham Briggs finishing the season at the top of British Cycling's UK Elite Men's standings. Raleigh once again became the sole headline sponsor of the team in 2013 and the team re-paid the investment with high-profile wins in the Tour de Normandie, Tour of the Reservoir and Tour Series Rounds 1 and 2.\n\nThe Raleigh archives, including the Sturmey-Archer papers, are at Nottinghamshire Record Office.\n\n\n\"Saturday Night and Sunday Morning\", the 1958 debut novel by Alan Sillitoe, is partly set in Raleigh's Nottingham factory, Sillitoe himself being an ex-employee of the firm. Several scenes for the 1960 film adaptation starring Albert Finney were filmed on location at the factory itself. In the 1985 movie \"American Flyers\", David Sommers played by David Marshall Grant, is seen riding through St. Louis, Missouri, on a Raleigh bicycle from that same era. Later in the film, specialized bicycles are used for the race scenes in Colorado and training. In the 1986 bike messenger film \"Quicksilver\" a variety of Raleigh USA bicycles are used. 1984–85 road bikes are used throughout by notable players in the movie. Kevin Bacon's bicycle is a singlespeed '84 Raleigh Competition. While no differentiation is made in the film, at least three different configurations are seen on Bacon's bike during the movie: fixed-gear, singlespeed, and outfitted with 0-degree trick forks during various scenes in Bacon's apartment. A possible freewheel is suggested early in the film when Bacon dismounts while in motion and a distinct clicking sound is heard until the bike stops moving. A 1984/5 Raleigh Grand Prix is used for the opening chase sequence, and a 1984 or '85 Super Course makes a brief appearance in the opening credits.\n\n\nA much expanded version of the text of this book, with full academic referencing, is held by the National Cycle Archive at Warwick University for the benefit of serious researchers.\n\n"}
{"id": "26627431", "url": "https://en.wikipedia.org/wiki?curid=26627431", "title": "Reinhard Mitschek", "text": "Reinhard Mitschek\n\nReinhard Mitschek is a former managing director of the Nabucco Gas Pipeline International GmbH, a project company for the Nabucco pipeline project.\n"}
{"id": "29301", "url": "https://en.wikipedia.org/wiki?curid=29301", "title": "Semiotics", "text": "Semiotics\n\nSemiotics (also called semiotic studies) is the study of sign process (semiosis). It is not to be confused with the Saussurean tradition called semiology, which is a subset of semiotics. Semiotics includes the study of signs and sign processes, indication, designation, likeness, analogy, allegory, metonymy, metaphor, symbolism, signification, and communication.\n\nThe semiotic tradition explores the study of signs and symbols as a significant part of communications. Different from linguistics, semiotics also studies non-linguistic sign systems.\n\nSemiotics is frequently seen as having important anthropological and sociological dimensions; for example, the Italian semiotician and novelist Umberto Eco proposed that every cultural phenomenon may be studied as communication. Some semioticians focus on the logical dimensions of the science, however. They examine areas belonging also to the life sciences—such as how organisms make predictions about, and adapt to, their semiotic niche in the world (see semiosis). In general, semiotic theories take \"signs\" or sign systems as their object of study: the communication of information in living organisms is covered in biosemiotics (including zoosemiotics).\n\nThe term derives from the Greek σημειωτικός \"sēmeiōtikos\", \"observant of signs\" (from σημεῖον \"sēmeion\", \"a sign, a mark\") and it was first used in English prior to 1676 by Henry Stubbes (spelt \"semeiotics\") in a very precise sense to denote the branch of medical science relating to the interpretation of signs. John Locke used the term \"sem(e)iotike\" in book four, chapter 21 of \"An Essay Concerning Human Understanding\" (1690). Here he explains how science may be divided into three parts:\n\nLocke then elaborates on the nature of this third category, naming it Σημειωτική (\"Semeiotike\") and explaining it as \"the doctrine of signs\" in the following terms:\n\nIn the nineteenth century, Charles Sanders Peirce defined what he termed \"semiotic\" (which he sometimes spelled as \"semeiotic\") as the \"quasi-necessary, or formal doctrine of signs\", which abstracts \"what must be the characters of all signs used by ... an intelligence capable of learning by experience\", and which is philosophical logic pursued in terms of signs and sign processes. The Peirce scholar and editor Max H. Fisch claimed in 1978 that \"semeiotic\" was Peirce's own preferred rendering of Locke's σημιωτική.\n\nCharles W. Morris followed Peirce in using the term \"semiotic\" and in extending the discipline beyond human communication to animal learning and use of signals.\n\nFerdinand de Saussure, however, founded his semiotics, which he called semiology, in the social sciences:\n\nWhile the Saussurean semiotic is dyadic (sign/syntax, signal/semantics), the Peircean semiotic is triadic (sign, object, interpretant), being conceived as philosophical logic studied in terms of signs that are not always linguistic or artificial. The Peircean semiotic addresses not only the external communication mechanism, as per Saussure, but the internal representation machine, investigating not just sign processes, or modes of inference, but the whole inquiry process in general. Peircean semiotics further subdivides each of the three triadic elements into three sub-types. For example, signs can be icons, indices, and symbols.\n\nYuri Lotman introduced Eastern Europe to semiotics and adopted Locke's coinage as the name to subtitle (\"Σημειωτική\") his founding at the University of Tartu in Estonia in 1964 of the first semiotics journal, \"Sign Systems Studies\".\n\nThomas Sebeok assimilated \"semiology\" to \"semiotics\" as a part to a whole, and was involved in choosing the name \"Semiotica\" for the first international journal devoted to the study of signs.\n\nSaussurean semiotics have been challenged with serious criticism, for example by Jacques Derrida's assertion that signifier and signified are not fixed, coining the expression \"différance\", relating to the endless deferral of meaning, and to the absence of a 'transcendent signified'. For Derrida, 'il n'y a pas de hors-texte' (\"there is nothing outside the text\"). He was in obvious opposition to materialists and marxists who argued that a sign has to point towards a real meaning, and cannot be controlled by the referent's closed-loop references.\n\nThe importance of signs and signification has been recognized throughout much of the history of philosophy, and in psychology as well. Plato and Aristotle both explored the relationship between signs and the world, and Augustine considered the nature of the sign within a conventional system. These theories have had a lasting effect in Western philosophy, especially through scholastic philosophy. (More recently, Umberto Eco, in his \"Semiotics and the Philosophy of Language\", has argued that semiotic theories are implicit in the work of most, perhaps all, major thinkers.)\n\nThe general study of signs that began in Latin with Augustine culminated in Latin with the 1632 \"Tractatus de Signis\" of John Poinsot, and then began anew in late modernity with the attempt in 1867 by Charles Sanders Peirce to draw up a \"new list of categories\". Peirce aimed to base his new list directly upon experience precisely as constituted by action of signs, in contrast with the list of Aristotle's categories which aimed to articulate within experience the dimension of being that is independent of experience and knowable as such, through human understanding.\n\nThe estimative powers of animals interpret the environment as sensed to form a \"meaningful world\" of objects, but the objects of this world (or \"Umwelt\", in Jakob von Uexküll's term,) consist exclusively of objects related to the animal as desirable (+), undesirable (–), or \"safe to ignore\" (0).\n\nIn contrast to this, human understanding adds to the animal \"Umwelt\" a relation of self-identity within objects which transforms objects experienced into \"things\" as well as +, –, 0 objects. Thus, the generically animal objective world as \"Umwelt\", becomes a species-specifically human objective world or \"Lebenswelt\" (life-world), wherein linguistic communication, rooted in the biologically underdetermined \"Innenwelt\" (inner-world) of humans, makes possible the further dimension of cultural organization within the otherwise merely social organization of non-human animals whose powers of observation may deal only with directly sensible instances of objectivity. This further point, that human culture depends upon language understood first of all not as communication, but as the biologically underdetermined aspect or feature of the human animal's \"Innenwelt\", was originally clearly identified by Thomas A. Sebeok. Sebeok also played the central role in bringing Peirce's work to the center of the semiotic stage in the twentieth century, first with his expansion of the human use of signs (\"anthroposemiosis\") to include also the generically animal sign-usage (\"zoösemiosis\"), then with his further expansion of semiosis (based initially on the work of Martin Krampen, but taking advantage of Peirce's point that an interpretant, as the third item within a sign relation, \"need not be mental\") to include the vegetative world (\"phytosemiosis\").\n\nPeirce's distinguished between the interpretant and the interpreter. The interpretant is the internal, mental representation that mediates between the object and its sign. The interpreter is the human who is creating the interpretant. Peirce's \"interpretant\" notion opened the way to understanding an action of signs beyond the realm of animal life (study of \"phytosemiosis\" + \"zoösemiosis\" + \"anthroposemiosis\" = \"biosemiotics\"), which was his first advance beyond Latin Age semiotics. Other early theorists in the field of semiotics include Charles W. Morris. Max Black argued that the work of Bertrand Russell was seminal in the field.\n\nSemioticians classify signs or sign systems in relation to the way they are transmitted (see modality). This process of carrying meaning depends on the use of codes that may be the individual sounds or letters that humans use to form words, the body movements they make to show attitude or emotion, or even something as general as the clothes they wear. To coin a word to refer to a \"thing\" (see lexical words), the community must agree on a simple meaning (a denotative meaning) within their language, but that word can transmit that meaning only within the language's grammatical structures and codes (see syntax and semantics). Codes also represent the values of the culture, and are able to add new shades of connotation to every aspect of life.\n\nTo explain the relationship between semiotics and communication studies, communication is defined as the process of transferring data and-or meaning from a source to a receiver. Hence, communication theorists construct models based on codes, media, and contexts to explain the biology, psychology, and mechanics involved. Both disciplines recognize that the technical process cannot be separated from the fact that the receiver must decode the data, i.e., be able to distinguish the data as salient, and make meaning out of it. This implies that there is a necessary overlap between semiotics and communication. Indeed, many of the concepts are shared, although in each field the emphasis is different. In \"Messages and Meanings: An Introduction to Semiotics\", Marcel Danesi (1994) suggested that semioticians' priorities were to study signification first, and communication second. A more extreme view is offered by Jean-Jacques Nattiez (1987; trans. 1990: 16), who, as a musicologist, considered the theoretical study of communication irrelevant to his application of semiotics.\n\nSemiotics differs from linguistics in that it generalizes the definition of a sign to encompass signs in any medium or sensory modality. Thus it broadens the range of sign systems and sign relations, and extends the definition of language in what amounts to its widest analogical or metaphorical sense. Peirce's definition of the term \"semiotic\" as the study of necessary features of signs also has the effect of distinguishing the discipline from linguistics as the study of contingent features that the world's languages happen to have acquired in the course of their evolutions. From a subjective standpoint, perhaps more difficult is the distinction between semiotics and the philosophy of language. In a sense, the difference lies between separate traditions rather than subjects. Different authors have called themselves \"philosopher of language\" or \"semiotician\". This difference does \"not\" match the separation between analytic and continental philosophy. On a closer look, there may be found some differences regarding subjects. Philosophy of language pays more attention to natural languages or to languages in general, while semiotics is deeply concerned with non-linguistic signification. Philosophy of language also bears connections to linguistics, while semiotics might appear closer to some of the humanities (including literary theory) and to cultural anthropology.\n\nSemiosis or \"semeiosis\" is the process that forms meaning from any organism's apprehension of the world through signs. Scholars who have talked about semiosis in their subtheories of semiotics include C. S. Peirce, John Deely, and Umberto Eco. Cognitive semiotics is combining methods and theories developed in the disciplines of cognitive methods and theories developed in semiotics and the humanities, with providing new information into human signification and its manifestation in cultural practices. The research on cognitive semiotics brings together semiotics from linguistics, cognitive science, and related disciplines on a common meta-theoretical platform of concepts, methods, and shared data.\n\nCognitive semiotics may also be seen as the study of meaning-making by employing and integrating methods and theories developed in the cognitive sciences. This involves conceptual and textual analysis as well as experimental investigations. Cognitive semiotics initially was developed at the Center for Semiotics at Aarhus University (Denmark), with an important connection with the Center of Functionally Integrated Neuroscience (CFIN) at Aarhus Hospital. Amongst the prominent cognitive semioticians are Per Aage Brandt, Svend Østergaard, Peer Bundgård, Frederik Stjernfelt, Mikkel Wallentin, Kristian Tylén, Riccardo Fusaroli, and Jordan Zlatev. Zlatev later in co-operation with Göran Sonesson established CCS (Center for Cognitive Semiotics) at Lund University, Sweden.\n\nSyntactics is the Morris'ean branch of semiotics that deals with the formal properties of signs and symbols; the interrelation of the signs, without regard to meaning. Semantics deals with the relation of signs to their designata and the objects that they may or do denote; the relation between the signs and the objects to which they apply. Finally, pragmatics deals with the biotic aspects of semiosis, with all the psychological, biological, and sociological phenomena that occur in the functioning of signs; the relation between the sign system and its human (or animal) user. Unlike his mentor George Herbert Mead, Morris was a behaviorist and sympathetic to the Vienna Circle positivism of his colleague, Rudolf Carnap. Morris was accused by John Dewey of misreading Peirce.\n\n\nThe flexibility of human semiotics is well demonstrated in dreams. Sigmund Freud spelled out how meaning in dreams rests on a blend of images, affects, sounds, words, and kinesthetic sensations. In his chapter on \"The Means of Representation\" he showed how the most abstract sorts of meaning and logical relations can be represented by spatial relations. Two images in sequence may indicate \"if this, then that\" or \"despite this, that\". Freud thought the dream started with \"dream thoughts\" which were like logical, verbal sentences. He believed that the dream thought was in the nature of a taboo wish that would awaken the dreamer. In order to safeguard sleep, the mindbrain converts and disguises the verbal dream thought into an imagistic form, through processes he called the \"dream-work\".\n\nApplications of semiotics include:\n\nIn some countries, its role is limited to literary criticism and an appreciation of audio and visual media. This narrow focus may inhibit a more general study of the social and political forces shaping how different media are used and their dynamic status within modern culture. Issues of technological determinism in the choice of media and the design of communication strategies assume new importance in this age of mass media.\n\nPublication of research is both in dedicated journals such as \"Sign Systems Studies\", established by Yuri Lotman and published by Tartu University Press; \"Semiotica\", founded by Thomas A. Sebeok and published by Mouton de Gruyter; \"Zeitschrift für Semiotik\"; \"European Journal of Semiotics\"; \"Versus\" (founded and directed by Umberto Eco), et al.; \"The American Journal of Semiotics\"; and as articles accepted in periodicals of other disciplines, especially journals oriented toward philosophy and cultural criticism.\n\nThe major semiotic book series \"Semiotics, Communication, Cognition\", published by De Gruyter Mouton (series editors Paul Cobley and Kalevi Kull) replaces the former \"Approaches to Semiotics\" (more than 120 volumes) and \"Approaches to Applied Semiotics\" (series editor Thomas A. Sebeok). Since 1980 the Semiotic Society of America has produced an annual conference series: \"\".\n\nMarketing is another application of semiotics. Epure, Eisenstat and Dinu (2014) said, \"semiotics allows for the practical distinction of persuasion from manipulation in marketing communication\" (p. 592). Semiotics are used in marketing as a persuasive device to influence buyers to change their attitudes and behaviors in the market place. Two ways that Epure, Eisenstat and Dinu (2014) state that semiotics are used are:\nSemiotics analysis is used by scholars and professional researchers as a method to interpret meanings behind symbols and how the meanings are created. Below is an example of how semiotic analysis is utilized in a research paper published in an academic journal: Educational Research and Reviews.\n\nSemiotics has sprouted subfields including, but not limited to, the following:\n\nPictorial semiotics is intimately connected to art history and theory. It goes beyond them both in at least one fundamental way, however. While art history has limited its visual analysis to a small number of pictures that qualify as \"works of art\", pictorial semiotics focuses on the properties of pictures in a general sense, and on how the artistic conventions of images can be interpreted through pictorial codes. Pictorial codes are the way in which viewers of pictorial representations seem automatically to decipher the artistic conventions of images by being unconsciously familiar with them.\n\nAccording to Göran Sonesson, a Swedish semiotician, pictures can be analyzed by three models: (a) the narrative model, which concentrates on the relationship between pictures and time in a chronological manner as in a comic strip; (b) the rhetoric model, which compares pictures with different devices as in a metaphor; and (c) the laokoon (or laocoon) model, which considers the limits and constraints of pictorial expressions by comparing textual mediums that utilize time with visual mediums that utilize space.\n\nThe break from traditional art history and theory—as well as from other major streams of semiotic analysis—leaves open a wide variety of possibilities for pictorial semiotics. Some influences have been drawn from phenomenological analysis, cognitive psychology, structuralist, and cognitivist linguistics, and visual anthropology and sociology.\n\nOne of the many ways that pictorial semiotics has been changing has been through the use of emojis in email, text or other online conversations. Though not seen as works of art, these small images of happy, sad, winking faces or even a smiling poo image, have made their way into our everyday communication through digital devices. In the early advances of mobile technology and the increasing manner in which such devices are used, many in the linguistic community felt that vital communication cues, such as the importance of nonverbal cues, would be lost. Another concern is that with the high use of these symbols would begin to oversimplify our language to where the language's strength would be lost.\n\nHowever, others have said that the use of emojis in digital conversation has helped to give more clarity to a conversation. Since the ability to read another person's facial expressions, nonverbal cues or tone of voice isn’t possible in a typed message, emojis allow a communicator to convey attitudes and emotions to their message receiver. As for oversimplifying our language, some have argued that perhaps our language is not being simplified, but that new generations are revitalizing the early forms of semiotics like cave paintings or hieroglyphics. As technology advances, so will the use of emojis or possibly a more advanced form of pictorial symbols to use in digital communication.\n\nStudies have shown that semiotics may be used to make or break a brand. Culture codes strongly influence whether a population likes or dislikes a brand's marketing, especially internationally. If the company is unaware of a culture's codes, it runs the risk of failing in its marketing. Globalization has caused the development of a global consumer culture where products have similar associations, whether positive or negative, across numerous markets.\n\nMistranslations may lead to instances of \"Engrish\" or \"Chinglish\", terms for unintentionally humorous cross-cultural slogans intended to be understood in English. This may be caused by a sign that, in Peirce's terms, mistakenly indexes or symbolizes something in one culture, that it does not in another. In other words, it creates a connotation that is culturally-bound, and that violates some culture code. Theorists who have studied humor (such as Schopenhauer) suggest that contradiction or incongruity creates absurdity and therefore, humor. Violating a culture code creates this construct of ridiculousness for the culture that owns the code. Intentional humor also may fail cross-culturally because jokes are not on code for the receiving culture.\n\nA good example of branding according to cultural code is Disney's international theme park business. Disney fits well with Japan's cultural code because the Japanese value \"cuteness\", politeness, and gift giving as part of their culture code; Tokyo Disneyland sells the most souvenirs of any Disney theme park. In contrast, Disneyland Paris failed when it launched as Euro Disney because the company did not research the codes underlying European culture. Its storybook retelling of European folktales was taken as elitist and insulting, and the strict appearance standards that it had for employees resulted in discrimination lawsuits in France. Disney souvenirs were perceived as cheap trinkets. The park was a financial failure because its code violated the expectations of European culture in ways that were offensive.\n\nOn the other hand, some researchers have suggested that it is possible to successfully pass a sign perceived as a cultural icon, such as the Coca-Cola or McDonald's logos, from one culture to another. This may be accomplished if the sign is migrated from a more economically-developed to a less developed culture. The intentional association of a product with another culture has been called Foreign Consumer Culture Positioning (FCCP). Products also may be marketed using global trends or culture codes, for example, saving time in a busy world; but even these may be fine-tuned for specific cultures.\n\nResearch also found that, as airline industry brandings grow and become more international, their logos become more symbolic and less iconic. The iconicity and symbolism of a sign depends on the cultural convention and, are on that ground in relation with each other. If the cultural convention has greater influence on the sign, the signs get more symbolic value.\n\nGraffiti is used by gang members to mark their territory and to warn off rivals. Graffiti is a great example of semiotics and the use of symbols. Police task forces are now starting to use a programming system called GARI, they upload pictures of gang symbols that they find and it helps them to decipher the meaning of the symbols. Gang members use semiotics and symbols for many different reasons, for example: Government-sanctioned graffiti from the city's Department of Public Works, in red, typically indicates an abandoned building, or the stylized SS stands for South Side, a faction of the 18th Street gang based in southern Indianapolis. A rival gang sprayed red Xs over the work as a sign of disrespect.\n\nIn the book \"The Lost Boyz: A Dark Side of Graffiti\" by Justin Rollins it talks about how Rollins started writing on trains at a very early age, because it helped him find himself, he was now a graffiti writer – a somebody. This is true with a lot of symbols. People use symbols to express themselves by getting tattoos or wearing a symbol on their clothing. This helps them to feel like they belong to something or it helps them to express themselves. Gangs also use their clothing as a symbol, they do something unique for their group so that you aware of the gang you are encountering. An example could be a certain pant leg rolled up or wearing a certain color of bandana. The book also talks about how Rollins joined a graffiti gang that was called the WK which stands for Who Kares but it also had a different meaning which was Wanted Kriminals. Many symbols in semiotics theory have different meanings; these meanings can be different from country to country or even just from person to person depending on where and how a person was raised. For example, in the United States, waving is a form of \"hello\" but in other cultures this could mean something offensive.\n\nSemiotics is anything that can stand for something else. A symbol does not stand on its own, it is a part of something, a system perhaps. Symbols in gangs are used for different things. Not just to show which gang a person is associated with but to express what has happened in their gang and in their individual lives. Gangs were first created to strengthen a certain ethnic group. They are very territorial. To show which territory is theirs they will mark the streets so that other gangs are aware. There are special ways to read gang signs and their tattoos: left to right, top to bottom. They may also make the tattoo or graffiti cluttered so that it is hard to read.\n\nEach gang has special hand signals or set of signs to identify themselves. There are some gangs who add a dot or something similar to their graffiti to stand as a phrase used in their specific gang. Each gang is unique and has special symbols, and these symbols usually have some sort of meaning. For example, the gang called \"Bloods\" use the color red in their clothing to symbolize they are a part of this specific gang. They also are known to use the number 5 and have tattoos and graffiti of a five-point crown. The hand sign that they use mostly is a \"b\" which stands for bloods.\n\nSome gangs use graffiti and tattoos more than others as well as using their clothing as a symbol of their gang. Gangs will also use codes to communicate on the streets. They will oftentimes use a number that will relate to a letter of the alphabet. Depending on the gang, they may use more complex codes.\n\nStreet gangs are known for using graffiti in their neighborhoods to mark their territory. Gangs are also using their graffiti to challenge other gangs and to disrespect them. When they do this, they will somehow cross the other gang's symbol, and they will use their gang slang as well. They make sure to do it in a place that is clear and will leave a direct hit on the other gang.\n\nSemiotics and gang graffiti merge on the undercarriage of bridges, the face of billboards, on abandoned buildings, storefronts, the sides of railroad cars, and even in inconspicuous places found along dirt roads in small rural areas. The idea, is to communicate a message. A message that is part of a system.\n\nIt is not uncommon for those outside the culture of gangs to judge the meaning of graffiti and its direct connection to semiotics as completely negative. This kind of graffiti was branded \"Graffiti of Grief\" in an article by Gabrielle Luber. They are a commemoration of mourning; a \"funeral\" for those who have died on the streets. The murals are often created by\nmembers designated within the gang and the artwork is intended \"provide glimpses of their lives, possessions, friends, and surroundings, and map out for us the identities of lost friends.\" The symbols used in these murals are intentional and communicate significant meanings within the developed culture. It is an art, an expression of respect and loyalty. It is a portable headstone and is often layered (painted over) with the next memorial of death caused by street violence. In its own right, it is a historical timeline, a genealogy of grief, and it is rewritten every day with the same story and a new name. Poverty, marginalism, survival, perpetual crisis, inadequate housing, low or no job skills, poor education, and social constraints and constructs will continue to market for the consumption of lives on the streets. The symbol of grief portrayed in certain kinds of graffiti suggest a depth of meaning and a place or memorial far beyond what the eye can see. To an outsider, it may easily be interpreted without compassion.\n\nA world organisation of semioticians, the International Association for Semiotic Studies, and its journal \"Semiotica\", was established in 1969. The larger research centers together with teaching program include the semiotics departments at the University of Tartu, Aarhus University, and Bologna University.\n\nThe discipline is mentioned in an episode of The Big Bang Theory called The Hamburger Postulate.\n\n\n"}
{"id": "360869", "url": "https://en.wikipedia.org/wiki?curid=360869", "title": "Stewart platform", "text": "Stewart platform\n\nA Gough-Stewart platform is a type of parallel robot that has six prismatic actuators, commonly hydraulic jacks or electric actuators, attached in pairs to three positions on the platform's baseplate, crossing over to three mounting points on a top plate. Devices placed on the top plate can be moved in the six degrees of freedom in which it is possible for a freely-suspended body to move. These are the three linear movements x, y, z (lateral, longitudinal and vertical), and the three rotations pitch, roll, & yaw. The terms \"six-axis\" or \"6-DoF\" (Degrees of Freedom) platform are also used, also \"synergistic\" (see below).\n\nThis specialised six-jack layout was first used by V E (Eric) Gough of the UK and was operational in 1954, the design later being publicised in a 1965 paper by D Stewart to the UK Institution of Mechanical Engineers. Although the short title \"Stewart Platform\" is now used for this jack layout, it would be fairer to Eric Gough to call it a \"Gough/Stewart platform\". To be more precise, the original Stewart platform had a slightly different design. See the more detailed references at the end of this article.\n\nBecause the motions are produced by a combination of movements of several of the jacks, such a device is sometimes called a \"synergistic motion platform\", due to the synergy (mutual interaction) between the way that the jacks are programmed.\n\nBecause the device has six jacks, it is often also known as a \"hexapod\" (six legs). The trademarked name \"hexapod\" (by Geodetic Technology) was originally for Stewart platforms used in machine tools. However, the term is now used for 6-jack platforms outside of the machine tool area, since it simply means \"six legs\".\n\nStewart platforms have applications in flight simulators, machine tool technology, crane technology, underwater research, air-to-sea rescue, mechanical bulls, satellite dish positioning, telescopes and orthopedic surgery.\n\nThe Stewart platform design is extensively used in flight simulation, particularly in the so-called full flight simulator for which all 6 degrees of freedom are required. This application was developed by Redifon, whose simulators featuring it became available for the Boeing 707, Douglas DC-8, Sud Aviation Caravelle, Canadair CL-44, Boeing 727, Comet, Vickers Viscount, Vickers Vanguard,\nConvair CV 990, Lockheed C-130 Hercules, Vickers VC10, and Fokker F-27 by 1962.\n\nIn this role, the payload is a replica cockpit and a visual display system, normally of several channels, for showing the outside-world visual scene to the aircraft crew that are being trained. Payload weights in the case of a full flight simulator for a large transport aircraft can be up to about 15,000 kilogrammes.\n\nSimilar platforms are used in driving simulators, typically mounted on large x-y tables to simulate short term acceleration. Long term acceleration can be simulated by tilting the platform, and an active research area is how to mix the two.\n\nJames S. Albus of the National Institute of Standards and Technology (NIST) developed the RoboCrane, where the platform hangs from six cables instead of being supported by six jacks.\n\nThe Low impact docking system developed by NASA uses a Stewart platform to manipulate space vehicles during the docking process.\n\nThe Computer Assisted Rehabilitation Environment developed by Motek Medical uses a Stewart platform coupled with virtual reality to do advanced biomechanical and clinical research.\n\nDr. J. Charles Taylor utilized the Stewart platform to develop the Taylor Spatial Frame, an external fixator used in orthopedic surgery for the correction of bone deformities and treatment of complex fractures.\n\n\n\n\n"}
{"id": "12521936", "url": "https://en.wikipedia.org/wiki?curid=12521936", "title": "TechTarget", "text": "TechTarget\n\nTechTarget is an American company which offers data-driven marketing services to business-to-business technology vendors. It uses purchase intent data gleaned from the readership of its 140 + technology focused web sites to help tech vendors reach buyers actively researching relevant IT products and services.\n\nTechTarget, Inc. was founded in 1999 and is headquartered in Newton, Massachusetts with offices in Atlanta, London, Munich, Paris, San Francisco, Singapore and Sydney.\n\nTechTarget was founded in 1999 by Greg Strakosch and Don Hawk as a spin-off of United Communications Group (UCG). In 2001, the company was recognized by B2B Magazine on the Media Power 50 list. And in 2005, AdAge named CEO Greg Strakosch a Top 25 Newsmaker.\n\nIn 2016, TechTarget named Michael Cotoia as CEO and board member, and elected Greg Stakosch as executive chairman. The company had its initial public offering in May 2007, listing on the NASDAQ exchange with symbol TTGT.\n\nIts current board of directors includes EMC Corporation founder Roger Marino, Atlanta Hawks co-owner Bruce Levenson, former CFO of the New York Times Co. , , CEO Michael Cotoia and executive chairman Greg Strakosch.\n\nSince launch, the company has made several acquisitions aimed at building its technology content reach: In 2003, TechTarget acquired Information Security Magazine. In 2004, the company acquired publications including: Bitpipe.com, TheServerSide.com and TheServerSide.net. In 2007 (the year the company went public), TechTarget acquired KnowledgeStorm for $58 million. In 2008, TechTarget acquired BrianMadden.com and BriForum. The company acquired LeMagIT and opened local operations in France. In 2011, it made a significant acquisition of Uk-based Computer Weekly, which had been published as a weekly print magazine by Reed Business Information for more than 45 years. \n\nThe company launched its current purchase intent data services (IT Deal Alert) for technology vendors in 2014 and has since added to the portfolio with a product called Priority Engine.\n"}
{"id": "361239", "url": "https://en.wikipedia.org/wiki?curid=361239", "title": "Technological utopianism", "text": "Technological utopianism\n\nTechnological utopianism (often called techno-utopianism or technoutopianism) is any ideology based on the premise that advances in science and technology could and should bring about a utopia, or at least help to fulfill one or another utopian ideal.\n\nA techno-utopia is therefore an ideal society, in which laws, government, and social conditions are solely operating for the benefit and well-being of all its citizens, set in the near- or far-future, as advanced science and technology will allow these ideal living standards to exist; for example, post-scarcity, transformations in human nature, the avoidance or prevention of suffering and even the end of death.\n\nTechnological utopianism is often connected with other discourses presenting technologies as agents of social and cultural change, such as technological determinism or media imaginaries.\n\nDouglas Rushkoff, a leading theorist on technology and cyberculture claims that technology gives everyone a chance to voice their own opinions, fosters individualistic thinking, and dilutes hierarchy and power structures by giving the power to the people. He says that the whole world is in the middle of a new Renaissance, one that is centered on technology and self-expression. However, Rushkoff makes it clear that “people don’t live their lives behind a desk with their hands on a keyboard” \n\nA tech-utopia does not disregard any problems that technology may cause, but strongly believes that technology allows mankind to make social, economic, political, and cultural advancements. Overall, Technological Utopianism views technology’s impacts as extremely positive.\n\nIn the late 20th and early 21st centuries, several ideologies and movements, such as the cyberdelic counterculture, the Californian Ideology, transhumanism, and singularitarianism, have emerged promoting a form of techno-utopia as a reachable goal. Cultural critic Imre Szeman argues technological utopianism is an irrational social narrative because there is no evidence to support it. He concludes that it shows the extent to which modern societies place faith in narratives of progress and technology overcoming things, despite all evidence to the contrary.\n\nKarl Marx believed that science and democracy were the right and left hands of what he called the move from the realm of necessity to the realm of freedom. He argued that advances in science helped delegitimize the rule of kings and the power of the Christian Church.\n\n19th-century liberals, socialists, and republicans often embraced techno-utopianism. Radicals like Joseph Priestley pursued scientific investigation while advocating democracy. Robert Owen, Charles Fourier and Henri de Saint-Simon in the early 19th century inspired communalists with their visions of a future scientific and technological evolution of humanity using reason. Radicals seized on Darwinian evolution to validate the idea of social progress. Edward Bellamy’s socialist utopia in \"Looking Backward\", which inspired hundreds of socialist clubs in the late 19th century United States and a national political party, was as highly technological as Bellamy’s imagination. For Bellamy and the Fabian Socialists, socialism was to be brought about as a painless corollary of industrial development.\n\nMarx and Engels saw more pain and conflict involved, but agreed about the inevitable end. Marxists argued that the advance of technology laid the groundwork not only for the creation of a new society, with different property relations, but also for the emergence of new human beings reconnected to nature and themselves. At the top of the agenda for empowered proletarians was \"to increase the total productive forces as rapidly as possible\". The 19th and early 20th century Left, from social democrats to communists, were focused on industrialization, economic development and the promotion of reason, science, and the idea of progress.\n\nSome technological utopians promoted eugenics. Holding that in studies of families, such as the Jukes and Kallikaks, science had proven that many traits such as criminality and alcoholism were hereditary, many advocated the sterilization of those displaying negative traits. Forcible sterilization programs were implemented in several states in the United States.\n\nH.G. Wells in works such as \"The Shape of Things to Come\" promoted technological utopianism.\n\nThe horrors of the 20th century – namely fascist dictatorships and the world wars – caused many to abandon optimism. The Holocaust, as Theodor Adorno underlined, seemed to shatter the ideal of Condorcet and other thinkers of the Enlightenment, which commonly equated scientific progress with social progress.\n\nA movement of techno-utopianism began to flourish again in the dot-com culture of the 1990s, particularly in the West Coast of the United States, especially based around Silicon Valley. The Californian Ideology was a set of beliefs combining bohemian and anti-authoritarian attitudes from the counterculture of the 1960s with techno-utopianism and support for libertarian economic policies. It was reflected in, reported on, and even actively promoted in the pages of \"Wired\" magazine, which was founded in San Francisco in 1993 and served for a number years as the \"bible\" of its adherents.\n\nThis form of techno-utopianism reflected a belief that technological change revolutionizes human affairs, and that digital technology in particular – of which the Internet was but a modest harbinger – would increase personal freedom by freeing the individual from the rigid embrace of bureaucratic big government. \"Self-empowered knowledge workers\" would render traditional hierarchies redundant; digital communications would allow them to escape the modern city, an \"obsolete remnant of the industrial age\".\n\nSimilar forms of \"digital utopianism\" has often entered in the political messages of party and social movements that point to the Web or more broadly to new media as harbingers of political and social change. Its adherents claim it transcended conventional \"right/left\" distinctions in politics by rendering politics obsolete. However, techno-utopianism disproportionately attracted adherents from the libertarian right end of the political spectrum. Therefore, techno-utopians often have a hostility toward government regulation and a belief in the superiority of the free market system. Prominent \"oracles\" of techno-utopianism included George Gilder and Kevin Kelly, an editor of \"Wired\" who also published several books.\n\nDuring the late 1990s dot-com boom, when the speculative bubble gave rise to claims that an era of \"permanent prosperity\" had arrived, techno-utopianism flourished, typically among the small percentage of the population who were employees of Internet startups and/or owned large quantities of high-tech stocks. With the subsequent crash, many of these dot-com techno-utopians had to rein in some of their beliefs in the face of the clear return of traditional economic reality.\n\nIn the late 1990s and especially during the first decade of the 21st century, technorealism and techno-progressivism are stances that have risen among advocates of technological change as critical alternatives to techno-utopianism. However, technological utopianism persists in the 21st century as a result of new technological developments and their impact on society. For example, several technical journalists and social commentators, such as Mark Pesce, have interpreted the WikiLeaks phenomenon and the United States diplomatic cables leak in early December 2010 as a precursor to, or an incentive for, the creation of a techno-utopian transparent society. Cyber-utopianism, first coined by Evgeny Morozov, is another manifestation of this, in particular in relation to the Internet and social networking.\n\nBernard Gendron, a professor of philosophy at the University of Wisconsin–Milwaukee, defines the four principles of modern technological utopians in the late 20th and early 21st centuries as follows:\n\n\nRushkoff presents us with multiple claims that surround the basic principles of Technological Utopianism:\n\n\nCritics claim that techno-utopianism's identification of social progress with scientific progress is a form of positivism and scientism. Critics of modern libertarian techno-utopianism point out that it tends to focus on \"government interference\" while dismissing the positive effects of the regulation of business. They also point out that it has little to say about the environmental impact of technology and that its ideas have little relevance for much of the rest of the world that are still relatively quite poor (see global digital divide).\n\nIn his 2010 study \"System Failure: Oil, Futurity, and the Anticipation of Disaster\", Canada Research Chairholder in cultural studies Imre Szeman argues that technological utopianism is one of the social narratives that prevent people from acting on the knowledge they have concerning the effects of oil on the environment.\n\nIn a controversial article \"Techno-Utopians are Mugged by Reality\", Wall Street Journal explores the concept of the violation of free speech by shutting down social media to stop violence. As a result of British cities being looted consecutively, Prime British Minister David Cameron argued that the government should have the ability to shut down social media during crime sprees so that the situation could be contained. A poll was conducted to see if Twitter users would prefer to let the service be closed temporarily or keep it open so they can chat about the famous television show X-Factor. The end report showed that every Tweet opted for X-Factor. The negative social effects of technological utopia is that society is so addicted to technology that we simply can't be parted even for the greater good. While many Techno-Utopians would like to believe that digital technology is for the greater good, it can also be used negatively to bring harm to the public.\n\nOther critics of a techno-utopia include the worry of the human element. Critics suggest that a techno-utopia may lessen human contact, leading to a distant society. Another concern is the amount of reliance society may place on their technologies in these techno-utopia settings. These criticisms are sometimes referred to as a technological anti-utopian view or a techno-dystopia.\n\nEven today, the negative social effects of a technological utopia can be seen. Mediated communication such as phone calls, instant messaging and text messaging are steps towards a utopian world in which one can easily contact another regardless of time or location. However, mediated communication removes many aspects that are helpful in transferring messages. As it stands today, most text, email, and instant messages offer fewer nonverbal cues about the speaker’s feelings than do face-to-face encounters. This makes it so that mediated communication can easily be misconstrued and the intended message is not properly conveyed. With the absence of tone, body language, and environmental context, the chance of a misunderstanding is much higher, rendering the communication ineffective. In fact, mediated technology can be seen from a dystopian view because it can be detrimental to effective interpersonal communication. These criticisms would only apply to messages that are prone to misinterpretation as not every text based communication requires contextual cues. The limitations of lacking tone and body language in text based communication are likely to be mitigated by video and augmented reality versions of digital communication technologies.\n\n\n"}
