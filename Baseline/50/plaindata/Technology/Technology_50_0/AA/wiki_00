{"id": "25210220", "url": "https://en.wikipedia.org/wiki?curid=25210220", "title": "Airscript", "text": "Airscript\n\nAirScript is a hand-held device which provides theatregoers with subtitles in a variety of languages. The device was launched in November 2009 at the musical Hairspray at London's Shaftesbury Theatre. The first languages available were English, Chinese, French, German, Italian, Spanish, and Russian. \n"}
{"id": "34306532", "url": "https://en.wikipedia.org/wiki?curid=34306532", "title": "CHFS", "text": "CHFS\n\nCHFS is a file system developed at the Department of Software Engineering, University of Szeged, Hungary. It was the first open source Flash memory-specific file system written for the NetBSD operating system.\n\nSimilar to UBIFS, the CHFS file system utilizes a separate layer for handling Flash aging and bad blocks, called EBH (erase block handler). The file system itself is modelled after JFFS2, thus the internal structure is very similar.\n\nCHFS was originally called ChewieFS during development. The name was changed to avoid legal issues and to have a more neutral name.\n\n\n"}
{"id": "18010409", "url": "https://en.wikipedia.org/wiki?curid=18010409", "title": "Collaborative journalism", "text": "Collaborative journalism\n\nCollaborative journalism is a mode of journalism where multiple reporters or news organizations, without affiliation to a common parent organization, report on and contribute news items to a news story together. It is practiced by both professional and amateur reporters. It is not to be mixed up with citizen journalism.\n\nCollaborative journalism involves the aggregation of information from numerous individuals or organizations into a single news story. Information is gathered through research or reporting, or added when readers examine, comment and build upon existing stories. Stories from the mainstream media are often built upon. Depending on the system of collaboration, individuals may also provide feedback or vote on whether an article is newsworthy. A single collaborative news story, therefore, may encompass multiple authors, varying articles, and ranged perspectives.\n\nProfessional and amateur reporters may work together to develop collaborative news articles, or mainstream media sites may gather amateur blog posts to complement reporting.\n\nCollaborative journalists either contribute directly to stories, sometimes through a wiki-style collaboration platform, or build upon the story externally, often through personal blogs. Collaborative journalists develop or examine a story one piece at a time. This contrasts the deadline and completion-centered nature of traditional media. A story is built upon continually, and a popular story may receive daily updates. Through combined authorship, collaborative journalism is thought by some to offer an increased independence of thought and experience unavailable to traditional media.\n\nSuccessful collaborative journalism projects require a participatory community with respect for content. Ross Mayfield, CEO of SocialText, has commented on wiki-style collaborative journalism:\n\n\"Most user-generated content isn't content, but conversation. Cultivating community is a decided practice. It boils down to the social contract you make with your readers-turned-writers. If they trust that their effort and words will be appropriated appropriately, while providing social incentives for participation, it can very well work.\"\n\nCollaborative journalism emerged through the internet in the early 2000s, and developed gradually through various online outlets. As examples, Wikinews was founded in 2003, and NewsVine in 2005.\n\nThe Panama Papers project may be the largest example of a journalistic consortium to date. It began sometime in 2015 (date?) when Bastian Obermayer, an investigative reporter with the south German newspaper Süddeutsche Zeitung, was contacted by an anonymous source and offered the trove of 11.5 million electronic documents from Mossack Fonseca, the world’s fourth biggest offshore law firm detailing a web of secret offshore deals and loans worth billions of dollars, and details of tax avoidance designs in numerous countries. The newspaper's editors decided they could not handle the massive volume of information alone and initiated a collaborative journalistic consortium including more than 140 journalists and the International Consortium of Investigative Journalists, a project of the nonprofit Center for Public Integrity.\n\nThe European Investigative Collaborations (EIC) working with \"over 60 journalists in 14 countries\" published a \"series of articles called \"Football Leaks\"—the \"largest leak in sports history\". \"Football Leaks\" \"led to the prosecution of football superstar Cristiano Ronaldo and coach Jose Mourinho.\" EIC was established in the fall of 2015 with founding members that include \"Der Spiegel\", \"El Mundo\", \"Médiapart\", the Romanian Centre for Investigative Journalism (CRJI), and \"Le Soir\".\n\n\n\"Link journalism\", a phrase coined by Scott Karp in 2008, is \"a form of collaborative journalism in which a news story's writer provides external links within the story to reporting or other sources on the web.\" These links are meant to complement, enhance, or add context to the original reporting. Jeff Jarvis, from the Graduate School of Journalism's new media program at the City University of New York, has said that link journalism creates a \"new architecture of news.\"\n\nCollaborative journalism has been implemented in several different ways. Wikinews, the \"free-content online news source,\" lets any user edit or create a news story, similar in style to Wikipedia. Several mainstream news sites have adopted a collaborative journalism approach toward news, through use of news aggregation. The Washington Post has developed a political site which links to related content from other news sites. NBC links to local newspapers, radio broadcasts, online videos, and blogs on its local television stations' sites. The sites do not separate articles written by NBC staff and links to outside sources.\nThe New York Times has introduced a \"Times Extra\" website feature which acts posts links to outside news sites. Commenting on the launch of \"Times Extra\", Marc Frons, CTO for Digital Operations at the New York Times, said:\n\n“In the past, I think many news organizations were afraid to link to other Web sites out of fear that they might be sending people to an unreliable source or that their readers would never return. But those fears were largely misplaced and we’ve seen a much more open policy when it comes to pointing readers at useful content elsewhere on the Web.\"\nOther sites exhibit collaborative journalism through aggregation. On the site NewsVine, for example, wire stories from the Associated Press complement user-generated stories and blog posts. Reddit and other news aggregation sites may also act as collaborative journalism sites, depending on where content originates.\n\nDue to the increase in collaborative journalism, several organizations have begun to offer grants or awards for these types of projects. For example, Online Journalism Awards (launched in May 2000) added a new award category for collaborations and partnerships. Clean Energy Wire offers grants for collaborative journalism projects on the topic of energy or the climate. The annual Hostwriter Prize awards money to support pitches and published collaborative projects by journalists.\n\nCollaborative journalism has received some criticism:\n"}
{"id": "53821763", "url": "https://en.wikipedia.org/wiki?curid=53821763", "title": "Combined storage tanks", "text": "Combined storage tanks\n\nA combined storage tank combines hot water storage both for heating support and drinking water heating in a larger tank. As a result, warm water, typically from solar thermal energy, is stored temporarily for both purposes for later consumption. Breaks in sunshine can thus be bridged without any additional heating, depending on the heating requirement and storage size. This generally takes up less space than two individual buffer tanks and thermal losses are thereby reduced. These storage tanks are mostly insulated on the outside and designed for the lowest possible heat losses. Usually, it is a tank-in-tank system, that is, a two-compartment system arranged one inside the other. It is thus a combined buffer storage tank with a heat exchange plate in between. Above is a smaller interior tank for drinking water, i.e. water for showers, washing etc. and around it on the outside is a much larger storage volume for heating water for room heating.\n\nAn alternative is the layer charge storage tank, where the warmest storage layer is located at the top of the tank and colder layers below. The advantage of this is that different temperatures cannot be mixed and warm water can be used for longer. The layer storage tank is considered the most modern type of storage in this field. For drinking water heating, a fresh water station is used in the throughflow principle with a heat exchanger, which reduces the risk of legionella formation and is therefore even more hygienic.\n\nThis leads to efficiency optimisation together with an intelligent loading and unloading control. Both are especially useful if combined with what is called a low-temperature heating system and good building insulation, which makes the energy more effective.\n\n"}
{"id": "15695872", "url": "https://en.wikipedia.org/wiki?curid=15695872", "title": "Consumability", "text": "Consumability\n\nA concept recently championed by International Business Machines (IBM), consumability is a description of customers' end-to-end experience with technology solutions (although the concept could easily apply to almost anything). The tasks associated with consumability start before the consumer purchases a product and continue until the customer stops using the product. By improving the consumability of the product, the value of that product to the client can be increased.\n\nUnderstanding product consumability requires an in-depth understanding of how clients are actually trying to use the product, which is why consumability is so closely aligned with the user experience and Outside-in software development. While usability addresses a client's ability to use a product, consumability is a higher-level concept that incorporates all the other aspects of the customer’s experience with the product.\n\nKey consumability aspects of the user experience include:\n\n\nHow efficiently and effectively clients can complete these tasks affects the value they get from the product. Missteps anywhere along this path can have direct impacts on the customer's ability to complete the task they set out to do. By focusing on consumability, developers can smooth the path, allowing technology solution consumers to focus on the needs of their business, improving their perception and satisfaction with the product or solution.\n\n\n"}
{"id": "25862206", "url": "https://en.wikipedia.org/wiki?curid=25862206", "title": "Crisis camp", "text": "Crisis camp\n\nA crisis camp is a BarCamp gathering of IT professionals, software developers, and computer programmers to aid in the relief efforts of a major crisis such as those caused by earthquakes, floods, or hurricanes. Projects that crisis camps often work on include setting up social networks for people to locate missing friends and relatives, creating maps of affected areas, and creating inventories of needed items such as food and clothing.\n\nFollowing the 2010 Haiti earthquake, many crisis camps were set up around the world, often under the name \"Crisis Camp Haiti\", to help with the relief effort. \nDue to the 2011 Tōhoku earthquake and tsunami, the Crisis Commons volunteer community was mobilized and part of the effort is being coordinated by Japanese students at U.S. universities.\n\n"}
{"id": "3779965", "url": "https://en.wikipedia.org/wiki?curid=3779965", "title": "Critical technical practice", "text": "Critical technical practice\n\nCritical technical practice is critical theory based approach towards technological design proposed by Phil Agre where critical and cultural theories are brought to bear in the work of designers and engineers. One of the goals of critical technical practice is to increase awareness and critical reflection on the hidden assumptions, ideologies and values underlying technology design.\n\nA different fork of Critical Technical Practice from Agre’s root (Agre, 1997) was initiated at the former Centre for Cultural Studies between 2007-2017, Goldsmiths, University of London as a way to examine, the live techno-social aspects of contemporary digital culture. This stream of Critical Technical Practice (CTP) at its most broad use can be described as the formation of thought and action that incorporates art as a method of enquiry. This is a compacted intellectual form, that makes the space between the technical, theory and practice ambiguous. A typical digital culture class would make/explore things, attempting to explain the phenomena caught in the lens of a project or proposition. \nAn example of a class may clarify this approach. We might propose that the class create a simple (DOS) denial of service attack on a test remote server by learning to code computers for the first time. It is empowering for students to find out how quickly they can code. The class would learn how to do this from T.J. Connor’s book Violent Python. After the group had reached a self-satisfied tingle of radicalism the group would be asked to look up the author and would find that Connor is a top grade US Military expert. The group would then look at the books distribution and market penetration and be encouraged to question the affective logics, politics and culture of the book, reflect on why the workshop had been constructed the way it was, and their own learning in different registers of technicality, politics of information and personal critic and empowerment was achieved. \nCritical Technical Practice then is not necessarily a reduction of phenomena to literature or a system of logics, but can instead be thought of as knowledge incorporated into a thing that the class created, look at or pointed too, through revealing a certain type of gaze. A prerequisite of Critical Practice is that incorporates this form of gaze is thinking through the formation of oneself as a thinker, actor in the world. Enquiring into one's pre-existence helps understand the structuring of potential that has informed what one has become, what one could easily recognise, and what one could easily achieve. This is not a summation of limit but an acknowledgement of the hard work needed to escape a pre-existence, as it may relate to the four pillars of repression, class, gender, sexuality and race. The situated knowledge of family and friends, their relation to making things, to popular culture, to oral histories, to struggles with money or law, reading, writing and speaking, all inform this process. \nTo this end, CTP is partially related to a schizoanalysis of Foucault's question “What are we today?” (Foucault 1984: pp. 42ff.) The class is always encouraged to unfold what conditions, constrain, control, resist, govern, determine this moment and not another? What patterns of recognition are we privileging and why does it blind us to others? How does language restrict us at the very moment we are able to say something? How can this engagement be born anew in every instance? \n\nPeople whose work contributes to the critical technical practice agenda include \nPhil Agre,\nPaul Dourish,\nNatalie Jeremijenko,\nMichael Mateas,\nSimon Penny,\nWarren Sack,\nGarnet Hertz,\nYoHa, (Matsuko Yokokoji, Graham Harwood)\nand \nPhoebe Sengers.\n\n\n"}
{"id": "39675445", "url": "https://en.wikipedia.org/wiki?curid=39675445", "title": "Data literacy", "text": "Data literacy\n\nData literacy is the ability to read, understand, create and communicate data as information. Much like literacy as a general concept, data literacy focuses on the competencies involved in working with data. \nAs data collection and data sharing become routine and data analysis and big data become common ideas in the news, business, government and society, it becomes more and more important for students, citizens, and readers to have some data literacy. \n\nData literacy focuses on the ability to understand and build knowledge from data, and to communicate that meaning to others. It is related to other fields, including:\n\n\n\nAs guides for finding and using information, librarians lead workshops on data literacy for students and researchers, and also work on developing their own data literacy skills. \n\nResources created by librarians include MIT's Data Management and Publishing tutorial, the EDINA Research Data Management Training (MANTRA), the University of Edinburgh’s Data Library and the University of Minnesota libraries’ Data Management Course for Structural Engineers.\n"}
{"id": "27960941", "url": "https://en.wikipedia.org/wiki?curid=27960941", "title": "De-Mail", "text": "De-Mail\n\nDe-Mail is a German E-Government communications service that makes it possible to exchange legal electronic documents between citizens, agencies, and businesses over the Internet. The project called Bürgerportal is realized by the German government in cooperation with private business partners in an effort to reduce communication costs of administration and companies.\n\nThe project was announced in 2008 and has been accepted by Bundestag and Bundesrat in early 2011. The De-Mail law (which serves as the legal basis for the service) went into effect on May 2, 2011.\n\nWith the introduction of De-Mail the German legislation implements the EU Directive on services in the internal market which demands that authorities accept legally binding electronic communications by the end of 2009.\n\nThe government specifies legal and technical requirements for the services, but does not provide the services. The services are provided by private companies which are allowed to provide De-Mail services after an official certification process.\n\nBesides email communication, other services are specified that support the communication process.\n\n\nUsage should be voluntary for all users, and should only be an additional option to postal mail.\n\nSeveral aspects of the law have sparked criticism of the system.\n\n\nThe following companies are currently accredited as De-Mail providers\nIn the process of accreditation are:\n\n\n"}
{"id": "58794547", "url": "https://en.wikipedia.org/wiki?curid=58794547", "title": "Direct Autonomous Authentication", "text": "Direct Autonomous Authentication\n\nDirect Autonomous Authentication (DAA) is a cybersecurity platform developed by San Francisco-based technology company Averon.\n\nThe DAA platform enables secure authentication of a mobile user whilst simultaneously preserving privacy of the user.\n\nThe technology was developed in stealth from late 2015, first publicly introduced by Averon in 2018, and featured at the 2018 Consumer Electronics Show as a new technology that combats the increasing threats of cybercrime and consumer account hacking.\nIn contrast to legacy methods of cybersecurity, the DAA platform bypasses end user actions, and rather than focusing on the authentication of a user's device, DAA instead provides autonomous authentication of a user's mobile phone number, since the mobile phone number continues to be associated with the user even when they lose, destroy or upgrade their mobile phone. \n\nThe DAA method uses a proprietary mix of technology developed by Averon that works inside the secure mobile network data pipelines together with encrypted technology already within every smartphone. The combination of these autonomous authentication methods has been described by research analysts as a faster, more secure, and stronger method of cybersecurity than traditional methods.\n\nBlockchain technology incorporated into the DAA platform ensures the privacy of end users. No identifiable personal data is maintained on the platform, therefore public disclosure of one's authentic identity (such as for the purpose of verified social media interactions) is voluntary. DAA technology affords the end user full control over identity disclosure in any given online interaction, which can be controlled by the end user in varying degrees from fully anonymous to fully identified publicly. In cases involving the need for anonymity with regard to an end user's safety, such as in cases of whistleblowers or political activists, the DAA platform's blockchain technology provides a method for both complete anonymity with the option of voluntary verification of limited but often needed data (such as verifying an anonymous user's general location). Thus DAA technology alleviates the heretofore insurmountable challenge of protecting user privacy with the need for authentication.\n\nThe DAA technology platform was designed to be seamlessly adopted for utilization in a wide variety of industries and use cases in which mobile authentication of users is required.\n\nSince its introduction to the market in 2018, the DAA platform has been recognized by a number of industry groups for its innovation, including winning the Gold prize at the 2018 Edison Awards, a Cybersecurity Excellence Award, and a BIG Innovation 2018 Award.\n"}
{"id": "757279", "url": "https://en.wikipedia.org/wiki?curid=757279", "title": "Disc filter", "text": "Disc filter\n\nA disc filter is a type of water filter used primarily in irrigation, similar to a screen filter, except that the filter cartridge is made of a number of plastic discs stacked on top of each other like a pile of poker chips. Each disc is covered with small grooves or bumps. The discs (or rings) each have a hole in the middle, forming a hollow cylinder in the middle of the stack. The water passes through the small passages in between and the impurities are trapped behind.\nThe filtration quality is based on the quantity and size of particles that the filtering element is able to retain. Higher quality filtration simply means cleaner water. This depends on the geometry of the channels, including the size, length, angle, and number of generated intersection points. The discs are typically color coded to denote the level of filtration. Filtration quality is usually measured in microns, based on the smallest size particle filtered. The typical range is from 25 microns for the finest level of filtration to 400 microns for the coarsest. Sometimes the filtration quality is given as the equivalent mesh size of a comparable screen filter. Typical mesh sizes range from 40 to 600. When using mesh sizes, 40 is the coarsest and 600 is the finest or highest level of filtration.\n\nDisc filters range in size from small units with a 3/4\" inlet and outlet used for landscape drip irrigation systems to very large banks of multiple filters manifolded together used for filtering large volumes of water for agricultural and industrial applications.\n\nSome disc filters, especially the smaller ones, must be taken apart and cleaned by hand. Many of the larger ones can be backflushed in such a way that the discs are able to separate and spin during the cleaning cycle. In some cases, a booster pump may be required for backflushing. Disc filters can be used for many types of contaminants, including fine sand and organic matter. However, when used to filter organic matter, they will clog more quickly than a media filter and will have to be cleaned more often. One advantage that the disc filter has over the media filter is that it can backflush more quickly with less flush water.\n\nDisc filters used in agricultural irrigation are covered by the ISO 9912-2 standard.\n\nThe disc filter was originally developed in 1936 to filter hydraulic fluid in the B-17 bomber. In these filters, the discs were made of stainless steel and brass. This type of filter began to be used in Israel to filter irrigation water in the 1960s.\n"}
{"id": "17206677", "url": "https://en.wikipedia.org/wiki?curid=17206677", "title": "Dispositif", "text": "Dispositif\n\nDispositif is a term used by the French intellectual Michel Foucault, generally to refer to the various institutional, physical, and administrative mechanisms and knowledge structures which enhance and maintain the exercise of power within the social body.\n\nDispositif is translated variously, even in the same book, as 'device', 'machinery', 'apparatus', 'construction', and 'deployment'.\n\nFoucault uses the term in his 1977 \"The Confession of the Flesh\" interview, where he answers the question, \"What is the meaning or methodological function for you of this term, apparatus (dispositif)?\" as follows:\n\nThe German linguist Siegfried Jäger defines Foucault's \"dispositif\" as\n\nThe Danish philosopher Raffnsøe \"advances the 'dispositive' (le dispositif) as a key conception in Foucault's work\" and \"a resourceful approach to the study of contemporary societal problems.\" According to Raffnsøe, \"the dispositionally prescriptive level is a crucial aspect of social reality in organizational life, since it has a determining effect on what is taken for granted and considered real. Furthermore, it determines not only what is and can be considered possible but also what can even be imagined and anticipated as potentially realizable, as something one can hope for, or act to bring about\".\n\nThe Italian political philosopher Giorgio Agamben traces the trajectory of the term to Aristotle's \"oikonomia—the effective management of the household\" and the early Church Fathers' attempt to save the concept of the Trinity from the allegation of polytheism, as the triplicity of the God is his oikonomia. \n\nAgamben defines the \"apparatus\"/\"dispositif\" as\n\nThe Italian scholar Matteo Pasquinelli criticises Agamben's genealogy with these words\n\n"}
{"id": "202311", "url": "https://en.wikipedia.org/wiki?curid=202311", "title": "E-services", "text": "E-services\n\nE-services (electronic services) are services which use of information and communication technologies (ICTs). The three main components of e-services are- service provider, service receiver and the channels of service delivery (i.e., technology). For example, as concerned to public e-service, public agencies are the service provider and citizens as well as businesses are the service receiver. The channel of service delivery is the third requirement of e-service. Internet is the main channel of e-service delivery while other classic channels (e.g. telephone, call center, public kiosk, mobile phone, television) are also considered.\n\nSince its inception in the late 1980s in Europe and formal introduction in 1993 by the US Government, the term ‘E-Government’ has now become one of the recognized research domains especially in the context of public policy and now has been rapidly gaining strategic importance in public sector modernization. E-service is one of the branches of this domain and its attention has also been creeping up among the practitioners and researchers.\n\nE-service (or eservice) is a highly generic term, usually referring to ‘The provision of services via the Internet (the prefix 'e' standing for ‘electronic’, as it does in many other usages), thus e-Service may also include e-Commerce, although it may also include non-commercial services (online), which is usually provided by the government.’ (Irma Buntantan & G. David Garson, 2004: 169-170; Muhammad Rais & Nazariah, 2003: 59, 70-71).\n\n\"E-Service constitutes the online services available on the Internet, whereby a valid transaction of buying and selling (procurement) is possible, as opposed to the traditional websites, whereby only descriptive information are available, and no online transaction is made possible.\" (Jeong, 2007).\n\nLu (2001) identifies a number of benefits for e-services, some of these are:\n\n\n\nThe term ‘e-service’ has many applications and can be found in many disciplines. The two dominant application areas of e-services are\n\nE-business (or e-commerce): e-services mostly provided by businesses or [NGO|non-government organizations] (NGOs) (private sector).\n\nE-government: e-services provided by government to citizens or business (public sector is the supply side). The use and description of the e-service in this page will be limited to the context of e-government only where of the e-service is usually associated with prefix “public”: Public e-services. In some cases, we will have to describe aspects that are related to both fields like some conferences or journals which cover the concept of “e-Service” in both domains of e-government and e-business.[example: www.eserviceforyou.com]\n\nDepending on the types of services, there are certain functionalities required in the certain layers of e-service architectural framework, these include but are not limited to – Data layer (data sources), processing layers (customer service systems, management systems, data warehouse systems, integrated customer content systems), exchange layer (Enterprise Application Integration– EAI), interaction layer ( integrating e-services), and presentation layer (customer interface through which the web pages and e-services are linked).\n\nMeasuring service quality and service excellence are important in a competitive organizational environment. The SERVQUAL- service quality model is one of the widely used tools for measuring quality of the service on various aspects. The five attributes of this model are: reliability, responsiveness, assurance, tangibles, and empathy. The following table summarizes some major of these:\n\nThe LIRNEasia study on benchmarking national telecom regulator websites focuses on content than on accessibility and ease of use, unlike the other studies mentioned here. Websites are increasingly important portals to government agencies, especially in the context of information society reforms. Stakeholders, including businesses, investors and even the general public, are interested in information produced by these agencies, and websites can help to increase their transparency and accountability. The quality of its website also demonstrates how advanced a regulatory agency is.\n\nSome major cost factors are (Lu, 2001):\n\n\nInformation technology is a powerful tool for accelerating economic development. Developing countries have focused on the development of ICT during the last two decades and as a result, it has been recognized that ICT is critical to economy and is as a catalyst of economic development. So, in recent years there seems to have been efforts for providing various e-services in many developing countries since ICT is believed to offer considerable potential for the sustainable development of e-Government and as a result, e-Services.\n\nMany government agencies in developed countries have taken progressive steps toward the web and ICT use, adding coherence to all local activities on the Internet, widening local access and skills, opening up interactive services for local debates, and increasing the participation of citizens on promotion and management of the territory(Graham and Aurigi, 1997).\n\nBut the potential for e-government in developing countries remains largely unexploited, even though. ICT is believed to offer considerable potential for the sustainable development of e-government. Different human, organizational and technological factors,\nissues and problems pertain in these countries, requiring focused studies and appropriate approaches. ICT, in general, is referred to as an “enabler”, but on the other hand, it should also be regarded as a challenge and a peril in itself. The organizations, public or private, which ignore the potential value and use of ICT may suffer pivotal competitive disadvantages. Nevertheless, some e-government initiatives have flourished in developing countries too, e.g. Brazil, India, Chile, etc. What the experience in these countries shows, is that governments in the developing world can effectively exploit and appropriate the benefits of ICT, but e-government success entails the accommodation of certain unique conditions, needs and obstacles. The adaptive challenges of e-government go far beyond technology, they call for organizational structures and skills, new forms of leadership, transformation of public-private partnerships (Allen et al., 2001).\n\nFollowing are a few examples regarding e-services in some developing countries:\n\nBangladesh first e-service system is National E-Service System ([http://www.eservice.gov.bd/[ NESS]]) and 2nd e-Service For you [http://eserviceforyou.com/[eserviceforyou.com]]. \n\nOnly a decade after emerging from the fastest genocide of the 20th Century, Rwanda, a small country in Eastern Central Africa,\nhas become one of the continent’s leaders in, and model on, bridging the digital divide through e-government. Rwanda has undergone a rapid turnaround from one of the most technologically deficient countries only a decade ago to a country\nwhere legislative business is conducted online and wireless access to the Internet is available anywhere in the country. This is\npuzzling when viewed against the limited progress made in other comparable developing countries, especially those located in the\nsame region, sub-Saharan Africa, where the structural and institutional constraints to e-government diffusion are similar.\n\nIn South Africa, there continues to be high expectations of government in respect to improved delivery of service and of closer consultation with citizens. Such expectations are not unique to this country, and in this regard there is a need for governments to recognise that the implementation of e-government systems and e-services affords them the opportunity to enhance service delivery and good governance. The implementation of e-Government has been widely acclaimed in that it provides new impetus to deliver services quickly and efficiently (Evans & Yen, 2006:208). In recognition of these benefits, various arms of the South African government have embarked on a number of e-government programmes for example the Batho Pele portal, SARS e-filing, the e-Natis system, electronic processing of grant applications from remote sites, and a large number of departmental information websites. Also a number of well publicised e-government ventures such as the latter, analysts and researchers consider the state of e-government in South Africa to be at rudimentary stages. There are various factors\nwhich collectively contribute to such an assessment. Amongst these, key factors relate to a lack of a clear strategy to facilitate uptake and adoption of e-government services as well as evaluation frameworks to assess expectations of citizens who are one of the primary user groups of these services.\n\nE-Services is one of the pilot projects under the Electronic Government Flagship within the Multimedia Super Corridor (MSC) initiative. With E-Services, one can now conduct transactions with Government agencies, such as the Road Transport Department (RTD) and private utility companies such as Tenaga Nasional Berhad (TNB) and Telekom Malaysia Berhad (TM) through various convenient channels such as the eServices kiosks and internet. No more queuing, traffic jams or bureaucratic hassles and one can now conduct transaction at one’s own convenience. Also, Electronic Labour Exchange (ELX)is one stop-centre for labor market information, as supervised by the Ministry of Human Resource (MOHR), to enable employers and job seekers to communicate on the same platform.\n\ne-Syariah is the seventh project under the Electronic Government flagship application of the Multimedia Super Corridor (MSC). A case management system that integrates the processes related to management of cases for the Syariah Courts.\n\nIn America, citizens have many options and opportunities to follow and understand government actions through e-government. Government 2.0 (Gov. 2.0) is currently in place to bring the people and governments together to learn new information, increase government transparency, and better means for communicating to one another. Gov. 2.0 offers increased citizen participation through on-line applications such as social media and other apps. Through the internet and websites such as USA.gov, an individual can perform actions such as contacting elected officials, find information about the work force such as retirement plans and labor laws, learn about money and consumer issues such as taxes, loans, and welfare, learn about citizenship and obtaining a visa or passport, and other topics such as health and welfare, education, and environmental issues.\n\nE-commerce is another growing E-service in the United States for both big and small businesses. E-commerce sales are projected to grow 10 to 12 percent annually. Amazon.com is the largest on-line marketplace in the country with annual sales of $79 billion. Wal-Mart is also a widely popular retailer. They have grown their business by having electronic services. Wal-Mart’s sales for E-commerce in 2015 was roughly $13 billion. Apple develops and sells a wide variety of technological goods and services such as cell phones, music players, and computers. Apple’s sales for E-commerce in 2015 was $12 billion. E-services allows businesses to reach new clientele and offer new services. Companies such as eBay and Etsy have achieved great success, with eBay posting a net income in 2016 of nearly $9 billion and Esty claiming roughly $200 million in profits from nearly $2 billion sales. The majority or eBay's business is conducted in the United States but it does a great deal of international business including the United Kingdom and Germany. The global reach of Etsy is seen in nearly every country in the world with 31% of gross merchandise sales occurring outside of the United States.\n\nChina’s recent realization of the continuing growth of internet usage has caused the government to recognize the need to expand their E-government services. Some steps the government wants to take in order to increase their E-government services are to develop more online functions, use government sites to integrate on-line services, have supplementary open data available to citizens to further government transparency, and to combine services from local and country-wide governments for convenience. China’s plan of action to incorporate the internet into everyday business and grow the economy is known as “Internet Plus.” The government plans to have this plan in full effect by 2025 to be the main driving force for economic and social improvements. Internet Plus will help to grow the job market as the government plans to use local citizens for development, and to generate more areas dedicated to technological growth such as Zhongguancun.\n\nBecause of the large population, China has the most internet and cell phone users in the world.(consider rewording) This causes a need for technological growth and a demand for increased E-services. In 2016, Chinese consumers spent more money for on-line goods and services than the United States and United Kingdom combined. There is(are) a wide variety of reasons as to why E-commerce flourishes in China including easy access to mobile internet, low cost of shipping, and a vast selection of cheap, unbranded products. Alibaba is China’s largest on-line marketplace with an annual revenue stream of $16 billion. Its services are globally available in Russia and Brazil through AliExpress. Tencent is another internet company with an annual revenue income of $16 billion. Tencent is used mainly for instant messaging but has other applications as well including mobile games and other digital content. By the end of 2015, Tencent’s WeChat messaging app reached around 700 million users. The biggest competitor for Tencent is Facebook’s WhatsApp. Baidu Is the most visited website in the country and it is used as a search engine and has an annual revenue of $10 billion. In March 2016, there were roughly 663 million users. Google challenges Baidu as the major internet search engines in the world. Huawei is a tech company that produces phones, tablets, and develops the equipment used in fixed-line networks. Huawei has an annual revenue income of $61 billion. It is currently located throughout 100 countries worldwide and in 2015, it filed 3,898 patent applications, more than any other country in the world. The biggest competitors to Huawei is Apple and Samsung.\n\nThe future of e-service is bright but some challenges remain. There are some challenges in e-service, as Sheth & Sharma (2007) identify, are:\n\n\nThe first challenge and primary obstacle to the e-service platform will be penetration of the internet. In some developing countries, the access to the internet is limited and speeds are also limited. In these cases firms and customers will continue to use traditional platforms. The second issue of concern is fraud on the internet. It is anticipated that the fraud on the e-commerce internet space costs $2.8 billion. Possibility of fraud will continue to reduce the utilization of the internet. The third issue is of privacy. Due to both spyware and security holes in operating systems, there is concern that the transactions that consumers undertake have privacy limitations. For example, by stealthily following online activities, firms can develop fairly accurate descriptions of customer profiles. Possibility of privacy violations will reduce the utilizations of the internet. The final issue is that e-service can also become intrusive as they reduce time and location barriers of other forms of contract. For example, firms can contact people through mobile devices at any time and at any place. Customers do not take like the intrusive behavior and may not use the e-service platform. (Heiner and lyer, 2007)\n\nA considerable amount of research efforts already exists on the subject matter exploring different aspects of e-service and e-service delivery ; one worth noting effort is Rowley’s study (2006) who did a review study on the e-service literature. The key finding of his study is that there is need to explore dimensions of e-service delivery not focusing only on service quality “In order to understand e-service experiences it is necessary to go beyond studies of e-service quality dimensions and to also take into account the inherent characteristics of e-service delivery and the factors that differentiate one service experience from another.”\n\nSome of the major keywords of e-service as found in the e-government research are as follows:\n\nUser acceptance of technology is defined according to Morris (1996, referred by Wu 2005, p. 1) as “the demonstrable willingness within a user group to employ information technology for the tasks it is designed to support”. This definition can be brought into the context of e-service where acceptance can be defined as the users’ willingness to use e-service or the willingness to decide when and how to use the e-service.\n\nUsers’ ability to access to the e-service is important theme in the previous literature. For example, Huang (2003) finds that most of the websites in general fail to serve users with disabilities. Recommendation to improve accessibility is evident in previous literature including Jaeger (2006) who suggests the following to improve e-services’ accessibility like: design for accessibility from the outset of website development, Involve users with disabilities in the testing of the site …Focus on the benefits of an accessible Web site to all users.\n\nAccording to Grönlund et al. (2007), for a simple e-service, the needs for knowledge and skills, content and procedures are considerably less. However, in complicated services there are needed to change some prevailed skills, such as replacing verbal skills with skill in searching for information online.\n\nThis theme is concerned with establishing standards for measuring e-services or the best practices within the field. This theme also includes the international benchmarking of e-government services (UN reports, EU reports); much critic has been targeting these reports being incomprehensive and useless. According to Bannister (2007) “… benchmarks are not a reliable tool for measuring real e-government progress. Furthermore, if they are poorly designed, they risk distorting government policies as countries may chase the benchmark rather than looking at real local and national needs”\n\nDigital divide is considered one of the main barriers to implementing e-services; some people do not have means to access the e-services and some others do not know how to use the technology (or the e-service). According to Helbig et al. (2009), “we suggest E-Government and the digital divide should be seen as complementary social phenomena (i.e., demand and supply). Moreover, a serious e-government digital divide is that services mostly used by social elites.\"\n\nMost of the reports and the established criteria focus on assessing the services in terms of infrastructure and public policies ignoring the citizen participation or e-readiness. According to by Shalini (2009), “the results of the research project reveal that a high index may be only indicating that a country is e-ready in terms of ICT infrastructure and info-structure, institutions, policies, and political commitment, but it is a very poor measure of the e-readiness of citizens. To summarize the findings, it can be said that Mauritius is ready but the Mauritians are not”\n\n``E-readiness, as the Economist Intelligence Unit defines, is the measure of a country’s ability to leverage digital channels for communication, commerce and government in order to further economic and social development. Implied in this measure is the extent to which the usage of communications devices and Internet services creates efficiencies for business and citizens, and the extent to which this usage is leveraged in the development of information and communications technology (ICT) industries. In general terms, the definition of e-readiness is relative, for instance depending on a country in question's priorities and perspective.\n\nAs opposed to effectiveness, efficiency is focused on the internal competence within the government departments when delivering e-services. There is a complaint that researchers focus more on effectiveness “There is an emerging trend seemingly moving away from the efficiency target and focusing on users and governance outcome. While the latter is worthwhile, efficiency must still remain a key priority for eGovernment given the budget constraints compounded in the future by the costs of an ageing population. Moreover, efficiency gains are those that can be most likely proven empirically through robust methodologies”\n\nSecurity is the most important challenge that faces the implementation of e-services because without a guarantee of privacy and security citizens will not be willing to take up e-government services. These security concerns, such as hacker attacks and the theft of credit card information, make governments hesitant to provide public online services. According to the GAO report of 2002 “security concerns present one of the toughest challenges to extending the reach of e-government.The rash of hacker attacks, Web page defacing, and credit card information being posted on electronic bulletin boards can make many federal agency officials—as well as the general public—reluctant to conduct sensitive government transactions involving personal or financial data over the Internet.” By and Large, Security is one of the major challenges that faces the implementation and development of electronic services. people want to be assured that they are safe when they are conducting online services and that their information will remain secure and confidential\n\nAxelsson et al. (2009) argue that the stakeholder concept-which was originally used in private firms-, can be used in public setting and in the context of e-government. According to them, several scholars have discussed the use of the stakeholder theory in public settings. The stakeholder theory suggests that need to focus on all the involved stakeholder s when designing the e-service; not only on the government and citizens.\n\nCompared to Accessibility, There is sufficient literature that addresses the issue of usability; researchers have developed different models and methods to measure the usability and effectiveness of eGovernment websites. However, But still there is call to improve these measures and make it more compressive\n\n``The word usability has cropped up a few times already in this unit. In the context of biometric identification, usability referred to the smoothness of enrollment and other tasks associated with setting up an identification system. A system that produced few false matches during enrollment of applicants was described as usable. Another meaning of usability is related to the ease of use of an interface. Although this meaning of the term is often used in the context of computer interfaces, there is no reason to confine it to computers.´´\n\nThe perceived effectiveness of e-service can be influenced by public’s view of the social and cultural implications of e-technologies and e-service.\n\nImpacts on individuals’ rights and privacy – as more and more companies and government agencies use technology to collect, store, and make accessible data on individuals, privacy concerns have grown. Some companies monitor their employees' computer usage patterns in order to assess individual or workgroup performance. Technological advancements are also making it much easier for businesses, government and other individuals to obtain a great deal of information about an individual without their knowledge. There is a growing concern that access to a wide range of information can be dangerous within politically corrupt government agencies.\n\nImpact on Jobs and Workplaces - in the early days of computers, management scientists anticipated that computers would replace human decision-makers. However, despite significant technological advances, this prediction is no longer a mainstream concern. At the current time, one of the concerns associated with computer usage in any organization (including governments) is the health risk – such as injuries related to working continuously on a computer keyboard. Government agencies are expected to work with regulatory groups in order to avoid these problems.\n\nPotential Impacts on Society – despite some economic benefits of ICT to individuals, there is evidence that the computer literacy and access gap between the haves and have-nots may be increasing. Education and information access are more than ever the keys to economic prosperity, yet access by individuals in different countries is not equal - this social inequity has become known as the digital divide.\n\nImpact on Social Interaction – advancements in ICT and e-Technology solutions have enabled many government functions to become automated and information to be made available online. This is a concern to those who place a high value on social interaction.\n\nInformation Security - technological advancements allow government agencies to collect, store and make data available online to individuals and organizations. Citizens and businesses expect to be allowed to access data in a flexible manner (at any time and from any location). Meeting these expectations comes at a price to government agencies where it concerns managing information – more specifically, ease of access; data integrity and accuracy; capacity planning to ensure the timely delivery of data to remote (possibly mobile) sites; and managing the security of corporate and public information.\n\nThe benefits of e-services in advancing businesses efficiency and in promoting good governance are huge; recognizing the importance of these benefits has resulted in number of international awards that are dedicated to recognize the best designed e-services. In the section, we will provide description of some international awards\n\nEuropean eGovernment Awards program started 2003 to recognize the best online public service in Europe. The aim of Awards is to encourage the deployment of e-services and to bring the attention to best practices in the field. The winners of the |4th European eGovernment Awards were announced in the award ceremony that took place at the 5th Ministerial eGovernment Conference on 19 November 2009 (Sweden); the winners in their respective categories are:\n\n\nSultan Qaboos Award for excellence in eGovernance (Started 2009) The award has five categories: Best eContent, Best eService, Best eProject, eEconomy, eReadiness.\n\neGovernment Excellence Awards (Started 2007) The program has three categories: Government Awards: Best eContent, Best eService, Best eProject, eEconomy, eEducation, eMaturity Business Awards: Best ICT solution Provider, eEconomy, eEducation Citizen Awards: Best eContent, eCitizen.\n\nPhilippines e-Service Awards (Started 2001) Categories: Outstanding Client Application of the Year, Outstanding Customer Application of the year, Groundbreaking Technology of the Year, Most Progressive Homegrown Company of the Year.\n\nThere are some journals particularly interested for “e-Service “. Some of these are:\n\nMajor conferences considering e-service as one of the themes are:\n\n\n"}
{"id": "900498", "url": "https://en.wikipedia.org/wiki?curid=900498", "title": "Early adopter", "text": "Early adopter\n\nAn early adopter (sometimes misspelled as \"early adapter\" or \"early adaptor\") or lighthouse customer is an early customer of a given company, product, or technology. The term originates from Everett M. Rogers' \"Diffusion of Innovations\" (1962).\n\nTypically this will be a customer who, in addition to using the vendor's product or technology, will also provide considerable and candid feedback to help the vendor refine its future product releases, as well as the associated means of distribution, service, and support. \n\nThe relationship is synergistic, with the customer having early (and sometimes unique, or at least uniquely early) access to an advantageous new product or technology, but he or she also serves as a kind of guinea pig.\n\nIn exchange for being an early adopter, and thus being exposed to the problems, risks, and annoyances common to early-stage product testing and deployment, the lighthouse customer is sometimes given especially attentive vendor assistance and support, even to the point of having personnel at the customer's work site to assist with implementation.\nThe customer is sometimes given preferential pricing, terms, and conditions, although new technology is often very expensive, so the early adopter still often pays quite a lot.\n\nThe vendor, on the other hand, benefits from receiving early\nrevenues, and also from a lighthouse customer's endorsement and assistance in further developing the product and its go-to-market mechanisms. Acquiring lighthouse customers is a common step in new product development and implementation. The real-world focus that this type of relationship can bring to a vendor can be extremely valuable.\n\nEarly adoption does come with pitfalls: early versions of products may be buggy and/or prone to malfunction. Furthermore, more efficient, and sometimes less expensive, versions of the product usually appear a few months after the initial release (Apple iPhone). The trend of new technology costing more at release is often referred to as the \"early adopter tax\".\n\n"}
{"id": "8710238", "url": "https://en.wikipedia.org/wiki?curid=8710238", "title": "Electric tweezers", "text": "Electric tweezers\n\nElectric tweezers are an electronic device intended to permanently remove hair. The design incorporates a pair of tweezers at the tip. A button on the side of the handle is used to simultaneously close the tweezer tips and turn on the high-frequency electrical signal. The electrical signal is intended to cause the connection of the hair to its root to be weakened and to stop hair growth from the root in a manner similar to electrolysis.\n\nSome electric tweezers have been described using the term \"electrolysis tweezer epilator\" or \"tweezer epilator\", but their operation is quite different from that of epilators.\n\nThe US FDA has a definition of \"permanent hair removal\", which these devices have been unable to pass. The FDA definition is such that a device can qualify and yet be ineffective for some people.\n\nPlucking (tweezing) is often described as \"time consuming\". Because the tweezers operate on only one hair at a time and it requires several seconds of application on each hair, this technique is even slower than normal tweezing. The US FDA suggest that, because of the difficulty of using these devices, many people end up effectively only using them as tweezers, with no permanent hair removal.\n"}
{"id": "7363430", "url": "https://en.wikipedia.org/wiki?curid=7363430", "title": "Electronic authentication", "text": "Electronic authentication\n\nElectronic authentication is the process of establishing confidence in user identities electronically presented to an information system. Digital authentication or e-authentication may be used synonymously when referring to the authentication process that confirms or certifies a person's identity and works. \nWhen used in conjunction with an electronic signature, it can provide evidence whether data received has been tampered with after being signed by its original sender. In a time where fraud and identity theft has become rampant, electronic authentication can be a more secure method of verifying that a person is who they say they are when performing transactions online.\n\nThere are various e-authentication methods that can be used to authenticate a user's identify ranging from a password to higher levels of security that utilize multifactor authentication (MFA). Depending on the level of security used, the user might need to prove his or her identity through the use of security tokens, challenge questions or being in possession of a certificate from a third-party certificate authority that attests to their identity.\n\n The American National Institute of Standards and Technology (NIST) has developed a generic electronic authentication model that provides a basic framework on how the authentication process is accomplished regardless of jurisdiction or geographic region. According to this model, the enrollment process begins with an individual applying to a Credential Service Provider (CSP). The CSP will need to prove the applicant's identity before proceeding with the transaction. Once the applicant's identity has been confirmed by the CSP, he or she receives the status of \"subscriber\", is given an authenticator, such as a token and a credential, which may be in the form of a username.\n\nThe CSP is responsible for managing the credential along with the subscriber's enrollment data for the life of the credential. The subscriber will be tasked with maintaining the authenticators. An example of this is when a user normally uses a specific computer to do their online banking. If he or she attempts to access their bank account from another computer, the authenticator will not be present. In order to gain access, the subscriber would need to verify their identity to the CSP, which might be in the form of answering a challenge question successfully before being given access.\n\nThe need for authentication has been prevalent throughout history. In ancient times, people would identify each other through eye contact and physical appearance. The Sumerians in ancient Mesopotamia attested to the authenticity of their writings by using seals embellished with identifying symbols. As time moved on, the most common way to provide authentication would be the handwritten signature.\n\nThere are three generally accepted factors that are used to establish a digital identity for electronic authentication, including:\nOut of the three factors, the biometric factor is the most convenient and convincing to prove an individual's identity. However, having to rely on this sole factor can be expensive to sustain. Although having their own unique weaknesses, by combining two or more factors allows for reliable authentication. It is always recommended to use multifactor authentication for that reason.\n\nAuthentication systems are often categorized by the number of factors that they incorporate. The three factors often considered as the cornerstone of authentication are:\nSomething you know (for example, a password)\nSomething you have (for example, an ID badge or a cryptographic key)\nSomething you are (for example, a voice print, thumb print or other biometric)\n\nMultifactor authentication is generally more secure than single-factor authentication. But, some multi-factor authentication approaches are still vulnerable to cases like man-in-the-middle attacks and Trojan attacks. Common methods used in authentication systems are summarized below.\n\nTokens generically are something the claimant possesses and controls that may be used to authenticate the claimant's identity. In e-authentication, the claimant authenticates to a system or application over a network. Therefore, a token used for e-authentication is a secret and the token must be protected. The token may, for example, be a cryptographic key, that is protected by encrypting it under a password. An impostor must steal the encrypted key and learn the password to use the token.\n\nPasswords and PINs are categorized as \"something you know\" method. A combination of numbers, symbols, and mixed cases are considered to be stronger than all-letter password. Also, the adoption of Transport Layer Security (TLS) or Secure Socket Layer (SSL) features during the information transmission process will as well create an encrypted channel for data exchange and to further protect information delivered. Currently, most security attacks target on password-based authentication systems.\n\nThis type of authentication has two parts. One is a public key, the other is a private key. A public key is issued by a Certification Authority and is available to any user or server. A private key is known by the user only.\n\nThe user shares a unique key with an authentication server. When the user sends a randomly generated message (the challenge) encrypted by the secret key to the authentication server, if the message can be matched by the server using its shared secret key, the user is authenticated.\nWhen implemented together with the password authentication, this method also provides a possible solution for two-factor authentication systems.\n\nThe user receives password by reading the message in the cell phone, and types back the password to complete the authentication. Short Message Service (SMS) is very effective when cell phones are commonly adopted. SMS is also suitable against man-in-the-middle (MITM) attacks, since the use of SMS does not involve the Internet.\n\nBiometric authentication is the use of unique physical attributes and body measurements as the intermediate for better identification and access control. Physical characteristics that are often used for authentication include fingerprints, voice recognition, face, recognition, and iris scans because all of these are unique to every individual separately. Traditionally, Biometric Authentication based on token-based identification systems, such as passport, and nowadays becomes one of the most secure identification systems to user protections. A new technological innovation which provides a wide variety of either behavioral or physical characteristics which are defining the proper concept of Biometric Authentication.\n\nDigital identity authentication refers to the combined use of device, behavior, location and other data, including email address, account and credit card information, to authenticate online users in real time.\n\nPaper credentials are documents that attest to the identity or other attributes of an individual or entity called the subject of the credentials. Some common paper credentials include passports, birth certificates, driver's licenses, and employee identity cards. The credentials themselves are authenticated in a variety of ways: traditionally perhaps by a signature or a seal, special papers and inks, high quality engraving, and today by more complex mechanisms, such as holograms, that make the credentials recognizable and difficult to copy or forge. In some cases, simple possession of the credentials is sufficient to establish that the physical holder of the credentials is indeed the subject of the credentials. More commonly, the credentials contain biometric information such as the subject's description, a picture of the subject or the handwritten signature of the subject that can be used to authenticate that the holder of the credentials is indeed the subject of the credentials. When these paper credentials are presented in-person, authentication biometrics contained in those credentials can be checked to confirm that the physical holder of the credential is the subject.\n\nElectronic identity credentials bind a name and perhaps other attributes to a token. There are a variety of electronic credential types in use today, and new types of credentials are constantly being created (eID, electronic voter ID card, biometric passports, bank cards, etc.) At a minimum, credentials include identifying information that permits recovery of the records of the registration associated with the credentials and a name that is associated with the subscriber.\n\nIn any authenticated on-line transaction, the verifier is the party that verifies that the claimant has possession and control of the token that verifies his or her identity. A claimant authenticates his or her identity to a verifier by the use of a token and an authentication protocol. This is called Proof of Possession (PoP). Many PoP protocols are designed so that a verifier, with no knowledge of the token before the authentication protocol run, learns nothing about the token from the run. The verifier and CSP may be the same entity, the verifier and relying party may be the same entity or they may all three be separate entities. It is undesirable for verifiers to learn shared secrets unless they are a part of the same entity as the CSP that registered the tokens. Where the verifier and the relying party are separate entities, the verifier must convey the result of the authentication protocol to the relying party. The object created by the verifier to convey this result is called an assertion.\n\nThere are four types of authentication schemes: local authentication, centralized authentication, global centralized authentication, global authentication and web application (portal).\n\nWhen using a local authentication scheme, the application retains the data that pertains to the user's credentials. This information is not usually shared with other applications. The onus is on the user to maintain and remember the types and number of credentials that are associated with the service in which they need to access. This is a high risk scheme because of the possibility that the storage area for passwords might become compromised.\n\nUsing the central authentication scheme allows for each user to use the same credentials to access various services. Each application is different and must be designed with interfaces and the ability to interact with a central system to successfully provide authentication for the user. This allows the user to access important information and be able to access private keys that will allow he or she to electronically sign documents.\n\nUsing a third party through a global centralized authentication scheme allows the user direct access to authentication services. This then allows the user to access the particular services they need.\n\nThe most secure scheme is the global centralized authentication and web application (portal). It is ideal for E-Government use because it allows a wide range of services. It uses a single authentication mechanism involving a minimum of two factors to allow access to required services and the ability to sign documents.\n\nOften, authentication and digital signing are applied in conjunction. In advanced electronic signatures, the signatory has authenticated and uniquely linked to a signature. In the case of a qualified electronic signature as defined in the eIDAS-regulation, the signer's identity is even certified by a qualified trust service provider. This linking of signature and authentication firstly supports the probative value of the signature – commonly referred to as non-repudiation of origin. The protection of the message on the network-level is called non-repudiation of emission. The authenticated sender and the message content are linked to each other. If a 3rd party tries to change the message content, the signature loses validity.\n\nBiometric authentication can be defined by many different procedures and sensors which are being used to produce security. Biometric can be separated into physical or behavioral security. Physical protection is based on identification through fingerprint, face, hand, iris, etc. On the other hand, behavioral safety is succeeded by keystroke, signature, and voice. The main point is that all of these different procedures and mechanism that exist, produce the same homogeneous result, namely the security of the system and users. When thinking of the decoupling of hardware, the hardware is not coded in the same form by digitization which directly makes decoupling more difficult. Because of unlinkability and irreversibility of biometric templates, this technology can secure user authentication.\n\nBiometric authentication has a substantial impact on digital traces. For example when the user decides to use his fingerprint to protect his data on his smartphone, then the system memorizes the input so it will be able to be re-used again. During this procedure and many other similar applications proves that the digital trace is vital and exist on biometric authentication.\n\nAnother characteristic of biometric authentication is that it combines different components such as security tokens with computer systems to protect the user. Another example is the connection between devices, such as camera and computer systems to scan the user’s retina and produce new ways of security. So biometric authentication could be defined by connectivity as long it connects different applications or components and through these users are getting connected and can work under the same roof and especially on a safe environment on the cyber world.\n\nAs new kinds of cybercrime are appearing, the ways of authentication must be able to adapt. This adaptation means that it is always ready for evolution and updating, and so it will be able to protect the users at any time. At first biometric authentication started in the sampler form of user’s access and defining user profiles and policies. Over time the need of biometric authentication became more complex, so cybersecurity organizations started reprogramming their products/technology from simple personal user’s access to allow interoperability of identities across multiple solutions. Through this evolution, business value also rises.\n\nWhen developing electronic systems, there are some industry standards requiring United States agencies to ensure the transactions provide an appropriate level of assurance. Generally, servers adopt the US' Office of Management and Budget's (OMB's) E-Authentication Guidance for Federal Agencies (M-04-04) as a guideline, which is published to help federal agencies provide secure electronic services that protect individual privacy. It asks agencies to check whether their transactions require e-authentication, and determine a proper level of assurance.\n\nIt established four levels of assurance:\n\nAssurance Level 1: Little or no confidence in the asserted identity's validity.\nAssurance Level 2: Some confidence in the asserted identity's validity. \nAssurance Level 3: High confidence in the asserted identity's validity. \nAssurance Level 4: Very high confidence in the asserted identity's validity.\n\nThe OMB proposes a five-step process to determine the appropriate assurance level for their applications:\n\nThe required level of authentication assurance are assessed through the factors below:\n\nNational Institute of Standards and Technology (NIST) guidance defines technical requirements for each of the four levels of assurance in the following areas:\n\nTriggered by the growth of new cloud solutions and online transactions, person-to-machine and machine-to-machine identities play a significant role in identifying individuals and accessing information. According to the Office of Management and Budget in the U.S, more than $70 million was spent on identity management solutions in both 2013 and 2014.\n\nGovernments use e-authentication systems to offer services and reduce time people traveling to a government office. Services ranging from applying for visas to renewing driver's licenses can all be achieved in a more efficient and flexible way. Infrastructure to support e-authentication is regarded as an important component in successful e-government. Poor coordination and poor technical design might be major barriers to electronic authentication.\n\nIn several countries there has been established nationwide common e-authentication schemes to ease the reuse of digital identities in different electronic services. Other policy initiatives have included the creation of frameworks for electronic authentication, in order to establish common levels of trust and possibly interoperability between different authentication schemes.\n\nE-authentication is a centerpiece of the United States government's effort to expand electronic government, or e-government, as a way of making government more effective and efficient and easier to access. The e-authentication service enables users to access government services online using log-in IDs (identity credentials) from other web sites that both the user and the government trust.\n\nE-authentication is a government-wide partnership that is supported by the agencies that comprise the Federal CIO Council. The United States General Services Administration (GSA) is the lead agency partner. E-authentication works through an association with a trusted credential issuer, making it necessary for the user to log into the issuer's site to obtain the authentication credentials. Those credentials or e-authentication ID are then transferred the supporting government web site causing authentication.The system was created in response a December 16, 2003 memorandum was issued through the Office of Management and Budget. Memorandum M04-04 Whitehouse. That memorandum updates the guidance issued in the \"Paperwork Elimination Act\" of 1998, 44 U.S.C. § 3504 and implements section 203 of the E-Government Act, 44 U.S.C. ch. 36.\n\nNIST provides guidelines for digital authentication standards and does away with most knowledge-based authentication methods. A stricter standard has been drafted on more complicated passwords that at least 8 characters long or passphrases that are at least 64 characters long.\n\nIn Europe, eIDAS provides guidelines to be used for electronic authentication in regards to electronic signatures and certificate services for website authentication. Once confirmed by the issuing Member State, other participating States are required to accept the user's electronic signature as valid for cross border transactions.\n\nUnder eIDAS, electronic identification refers to a material/immaterial unit that contains personal identification data to be used for authentication for an online service. Authentication is referred to as an electronic process that allows for the electronic identification of a natural or legal person. A trust service is an electronic service that is used to create, verify and validate electronic signatures, in addition to creating, verifying and validating certificates for website authentication.\n\nArticle 8 of eIDAS allows for the authentication mechanism that is used by a natural or legal person to use electronic identification methods in confirming their identity to a relying party. Annex IV provides requirements for qualified certificates for website authentication.\nE-authentication is a centerpiece of the Russia government's effort to expand e-government, as a way of making government more effective and efficient and easier for the Russian people to access. The e-authentication service enables users to access government services online using log-in IDs (identity credentials) they already have from web sites that they and the government trust.\n\nApart from government services, e-authentication is also widely used in other technology and industries. These new applications combine the features of authorizing identities in traditional database and new technology to provide a more secure and diverse use of e-authentication. Some examples are described below.\n\nMobile authentication is the verification of a user's identity through the use a mobile device. It can be treated as an independent field or it can also be applied with other multifactor authentication schemes in the e-authentication field.\n\nFor mobile authentication, there are five levels of application sensitivity from Level 0 to Level 4. Level 0 is for public use over a mobile device and requires no identity authentications, while level 4 has the most multi-procedures to identify users. For either level, mobile authentication is relatively easy to process. Firstly, users send a one-time password (OTP) through offline channels. Then, a server identifies the information and makes adjustment in the database. Since only the user has the access to a PIN code and can send information through their mobile devices, there is a low risk of attacks.\n\nIn the early 1980s, electronic data interchange (EDI) systems was implemented, which was considered as an early representative of E-commerce. But ensuring its security is not a significant issue since the systems are all constructed around closed networks. However, more recently, business-to-consumer transactions have transformed. Remote transacting parties have forced the implementation of E-commerce authentication systems.\n\nGenerally speaking, the approaches adopted in E-commerce authentication are basically the same as e-authentication. The difference is E-commerce authentication is a more narrow field that focuses on the transactions between customers and suppliers. A simple example of E-commerce authentication includes a client communicating with a merchant server via the Internet. The merchant server usually utilizes a web server to accept client requests, a database management system to manage data and a payment gateway to provide online payment services.\n\nTo keep up with the evolution of services in the digital world, there is continued need for security mechanisms. While passwords will continued to be used, it is important to rely on authentication mechanisms, most importantly multifactor authentication. As the usage of e-signatures continues to significantly expand throughout the United States, the EU and throughout the world, there is expectation that regulations such as eIDAS will eventually be amended to reflect changing conditions along with regulations in the United States.\n\n"}
{"id": "49594449", "url": "https://en.wikipedia.org/wiki?curid=49594449", "title": "Eltro Information Rate Changer", "text": "Eltro Information Rate Changer\n\nThe Eltro Information Rate Changer is an analog recording tool used to modulate pitch without affecting tempo. It is best known as the effect used in \"\" for the Hal 9000 computer. Patents for the device date back to the 1920s.\n\n"}
{"id": "49107824", "url": "https://en.wikipedia.org/wiki?curid=49107824", "title": "Epos Now", "text": "Epos Now\n\nEpos Now is a cloud-based software provider, specialising in the design and manufacture of electronic point of sale (commonly referred to as EPOS), which encompasses features including but not limited to reporting, stock control, and CRM for retail and hospitality businesses. Epos Now software can be operated from any device or platform with a web-browser or by using Epos Now's IOS or Android app.\n\nEpos Now's UK headquarters are located in Norwich, England and their US headquarters are in Orlando, Florida. Founded in June 2011, the company was a pioneer in cloud-technology in the EPOS industry and is notable for being the first EPOS company to introduce an AppStore to their customers, which allows users to customise their system. The Epos Now AppStore was introduced in December 2014. Epos Now's innovation was recognised in 2016, when the company won a Queen's Award for Enterprise, the UK's highest accolade for business success.\n\nEpos Now is the UK's 13th fastest growing private technology company, and the fastest growing EPOS company in the UK.\n\nEpos Now was founded by Jacyn Heavens in 2011 with no external investment. CEO and founder Jacyn Heavens identified a gap in the market for an affordable EPOS system whilst managing a bar, and after searching for an EPOS system that would be suitable for his business, he started building software that would suit business owners like himself. The Epos Now Appstore was officially launched in September 2015, making it the first Appstore to be released within the EPOS industry. There are over 20,000 businesses using Epos Now.\n\nIn 2015, Epos Now was awarded 'Epos Innovation of the Year' by Retail Systems, and was shortlisted for EDP Business Awards and Tech Cities 2015.\n\nIn 2016 Epos Now was shortlisted for the UK Cloud Awards in the \"Most Innovative SMB Product of the Year\" category and the Engagement & Loyalty Awards in the \"Most Innovative Technology\" category. Epos Now was also a finalist in 2016 The Grocer Gold Awards, for the \"Technology Supplier of the Year\" category.\n\nOn 21 April 2016 it was announced the Epos Now had won a Queen's Award for Enterprise in the Innovation category.\n\nEpos Now were named Gold Stevie Award winners for The International Business Awards as \"The Most Innovative company of the Year 2016\".\n\nEpos Now won three Eastern Daily Press Business Awards in 2016 in the categories of; Employer of the Year, Tech Innovator of the year and Business of the Year sponsored by Barclays. \n\nThe company made the Tech Track 100 three years running, being named the 13th and 32nd and 53rd fastest growing tech company in the UK and named 30th fastest growing company in 2016's Deloitte Fast 50 with a growth of 597%. Epos Now were named 179th fastest growing tech company in the Deloitte 2016 Fast 500 EMEA. Epos Now were named Europes 46th fastest growing company in the Financial Times 1000 listing.\n\nEpos Now were recognised for their commitment to the professional development of their employees winning a Princess Royal Training Award in 2017.\n\nEpos Now were noted as 'Best SaaS Product for small business/SMBs' in the 2017 SaaS Awards.\n"}
{"id": "8518824", "url": "https://en.wikipedia.org/wiki?curid=8518824", "title": "Extensible Forms Description Language", "text": "Extensible Forms Description Language\n\nExtensible Forms Description Language (XFDL) is a high-level computer language that facilitates defining a form as a single, stand-alone object using elements and attributes from the Extensible Markup Language (XML). Technically, it is a class of XML originally specified in a World Wide Web Consortium (W3C) Note. See Specifications below for links to the current versions of XFDL. XFDL It offers precise control over form layout, permitting replacement of existing business/government forms with electronic documents in a human-readable, open standard.\n\nIn addition to precision layout control, XFDL provides multiple page capabilities, step-by-step guided user experiences, and digital signatures. XFDL also provides a syntax for in-line mathematical and conditional expressions and data validation constraints as well as custom items, options, and external code functions. Current versions of XFDL (see Specifications below) are capable of providing these interactive features via open standard markup languages including XForms, XPath, XML Schema and XML Signatures.\n\nXFDL not only supports multiple digital signatures, but the signatures can apply to specific sections of a form and prevent changes to signed content.\n\nThese advantages to XFDL led large organizations such as the United States Army and Air Force to migrate to XFDL from using forms in other formats. Later, though, the lack of portable software capable of creating XFDL led them to investigate moving away from it. The Army migrated to Adobe fillable PDFs in 2014.\n\n\n\n"}
{"id": "21977757", "url": "https://en.wikipedia.org/wiki?curid=21977757", "title": "Flying syringe", "text": "Flying syringe\n\nFlying syringe is a phrase that is used to refer to proposed, but not yet created, genetically modified mosquitoes that inject vaccines into people when they bite them.\n\nIn 2008 the Gates Foundation awarded $100,000 to Hiroyuki Matsuoka of Jichi Medical University in Japan to do research on them, with a condition that any discoveries that were funded by the grant must be made available at affordable prices in the developing world. If Matsuoka proves that his idea has merit, he will be eligible for an additional $1 million of funding. \"The Washington Post\" referred to flying syringes as a \"bold idea\".\n"}
{"id": "39217725", "url": "https://en.wikipedia.org/wiki?curid=39217725", "title": "Hacking Health", "text": "Hacking Health\n\nHacking Health is a social organization that pairs innovators with healthcare experts to build solutions to front-line healthcare problems through the use of emerging technology.\n\nThe organization started off in Montreal, Quebec, Canada in 2012 with a weekend-long hackathon to encourage collaboration between healthcare professionals and IT experts. Since then, Hacking Health events have been held in cities across Canada, the USA and internationally.\n\nHeld over a weekend, Hacking Health hackathons consist of 200-300 participants where designers and developers collaborate with doctors, nurses, clinic managers and other healthcare professionals to develop prototypes that can be put to test in clinics and hospitals. The event also attracts industry professionals, venture capitalists and entrepreneurs.\n\nHeld typically in the evening, Hacking Health Café meetups are regular events to bring together entrepreneurs in the healthcare industry and foster relationships between technology talent and healthcare experts. These shorter, casual events help interested parties stay up-to-date on local collaborations, projects and start-ups.\n\n\nHacking Health has spread globally since 2013 to Cape Town, Strasbourg, Hong Kong, Zurich, Bucharest, and Detroit.\n\n2013\n\nStrasbourg (France)\n\n2014\n\n\n2015\n\n\n2016\n\n\n2017\n\n\n"}
{"id": "5768303", "url": "https://en.wikipedia.org/wiki?curid=5768303", "title": "Handheld television", "text": "Handheld television\n\nA handheld television is a portable device for watching television that usually uses a TFT LCD or OLED color display. Many of these devices resemble handheld transistor radios.\n\nIn the 1970s and early 1980s, Panasonic and Sinclair Research released the first TVs which were small enough to fit in a large pocket; called the Panasonic IC TV MODEL TR-001 and MTV-1. Since LCD technology was not yet mature at the time, the TV used a minuscule CRT which set the record for being the smallest CRT on a commercially marketed product.\n\nLater in 1982, Sony released the first model of the Watchman, a pun on Walkman. It had grayscale video at first. Several years later, a color model with an active-matrix LCD was released. Some smartphones integrate a television receiver, although Internet broadband video is far more common.\n\nSince the switch-over to digital broadcasting, handheld TVs have reduced in size and improved in quality. The major current manufacturers of DVB-T standard (common throughout Europe) handheld TVs are August International, ODYS and Xoro.\n\nThese devices often have stereo 1⁄8 inch (3.5 mm) phono plugs for composite video-analog mono audio relay to serve them as composite monitors; also, some models have mono 3.5 mm jacks for the broadcast signal that is usually relayed via F connector or Belling-Lee connector on standard television models. \n\nSome include HDMI, USB and SD ports.\n\nScreen sizes vary from . Some handheld televisions also double as portable DVD players and USB personal video recorders.\n\nPortable televisions cannot fit in a pocket, but often run on batteries and include a cigarette lighter receptacle plug.\n\nPocket televisions fit in a pocket.\n\nWearable televisions sometimes are made in the form of a wristwatch.\n\n\n\n"}
{"id": "27313582", "url": "https://en.wikipedia.org/wiki?curid=27313582", "title": "Handover", "text": "Handover\n\nIn cellular telecommunications, the terms handover or handoff refer to the process of transferring an ongoing call or data session from one channel connected to the core network to another channel. In satellite communications it is the process of transferring satellite control responsibility from one earth station to another without loss or interruption of service.\n\nAmerican English uses the term \"handoff\", and this is most commonly used within some American organizations such as 3GPP2 and in American originated technologies such as CDMA2000. In British English the term \"handover\" is more common, and is used within international and European organisations such as ITU-T, IETF, ETSI and 3GPP, and standardised within European originated standards such as GSM and UMTS. The term handover is more common than handoff in academic research publications and literature, while handoff is slightly more common within the IEEE and ANSI organisations.\n\nIn telecommunications there may be different reasons why a handover might be conducted:\n\nThe most basic form of handover is when a phone call in progress is redirected from its current cell (called \"source\") to a new cell (called \"target\"). In terrestrial networks the source and the target cells may be served from two different cell sites or from one and the same cell site (in the latter case the two cells are usually referred to as two \"sectors\" on that cell site). Such a handover, in which the source and the target are different cells (even if they are on the same cell site) is called \"inter-cell\" handover. The purpose of inter-cell handover is to maintain the call as the subscriber is moving out of the area covered by the source cell and entering the area of the target cell.\n\nA special case is possible, in which the source and the target are one and the same cell and only the used channel is changed during the handover. Such a handover, in which the cell is not changed, is called \"intra-cell\" handover. The purpose of intra-cell handover is to change one channel, which may be interfered or fading with a new clearer or less fading channel.\n\nIn addition to the above classification of \"inter-cell\" and \"intra-cell\" classification of handovers, they also can be divided into hard and soft handovers:\nHandover can also be classified on the basis of handover techniques used. Broadly they can be classified into three types:\n\nAn advantage of the hard handover is that at any moment in time one call uses only one channel. The hard handover event is indeed very short and usually is not perceptible by the user. In the old analog systems it could be heard as a click or a very short beep; in digital systems it is unnoticeable. Another advantage of the hard handover is that the phone's hardware does not need to be capable of receiving two or more channels in parallel, which makes it cheaper and simpler. A disadvantage is that if a handover fails the call may be temporarily disrupted or even terminated abnormally. Technologies which use hard handovers, usually have procedures which can re-establish the connection to the source cell if the connection to the target cell cannot be made. However re-establishing this connection may not always be possible (in which case the call will be terminated) and even when possible the procedure may cause a temporary interruption to the call.\n\nOne advantage of the soft handovers is that the connection to the source cell is broken only when a reliable connection to the target cell has been established and therefore the chances that the call will be terminated abnormally due to failed handovers are lower. However, by far a bigger advantage comes from the mere fact that simultaneously channels in multiple cells are maintained and the call could only fail if all of the channels are interfered or fade at the same time. Fading and interference in different channels are unrelated and therefore the probability of them taking place at the same moment in all channels is very low. Thus the reliability of the connection becomes higher when the call is in a soft handover. Because in a cellular network the majority of the handovers occur in places of poor coverage, where calls would frequently become unreliable when their channel is interfered or fading, soft handovers bring a significant improvement to the reliability of the calls in these places by making the interference or the fading in a single channel not critical. This advantage comes at the cost of more complex hardware in the phone, which must be capable of processing several channels in parallel. Another price to pay for soft handovers is use of several channels in the network to support just a single call. This reduces the number of remaining free channels and thus reduces the capacity of the network. By adjusting the duration of soft handovers and the size of the areas in which they occur, the network engineers can balance the benefit of extra call reliability against the price of reduced capacity.\n\nWhile theoretically speaking soft handovers are possible in any technology, analog or digital, the cost of implementing them for analog technologies is prohibitively high and none of the technologies that were commercially successful in the past (e.g. AMPS, TACS, NMT, etc.) had this feature. Of the digital technologies, those based on FDMA also face a higher cost for the phones (due to the need to have multiple parallel radio-frequency modules) and those based on TDMA or a combination of TDMA/FDMA, in principle, allow not so expensive implementation of soft handovers. However, none of the 2G (second-generation) technologies have this feature (e.g. GSM, D-AMPS/IS-136, etc.). On the other hand, all CDMA based technologies, 2G and 3G (third-generation), have soft handovers. On one hand, this is facilitated by the possibility to design not so expensive phone hardware supporting soft handovers for CDMA and on the other hand, this is necessitated by the fact that without soft handovers CDMA networks may suffer from substantial interference arising due to the so-called \"near-far\" effect..\n\nIn all current commercial technologies based on FDMA or on a combination of TDMA/FDMA (e.g. GSM, AMPS, IS-136/DAMPS, etc.) changing the channel during a hard handover is realised by changing the pair of used transmit/receive frequencies.\n\nFor the practical realisation of handovers in a cellular network each cell is assigned a list of potential target cells, which can be used for handing over calls from this source cell to them. These potential target cells are called \"neighbors\" and the list is called \"neighbor list\". Creating such a list for a given cell is not trivial and specialized computer tools are used. They implement different algorithms and may use for input data from field measurements or computer predictions of radio wave propagation in the areas covered by the cells.\n\nDuring a call one or more parameters of the signal in the channel in the source cell are monitored and assessed in order to decide when a handover may be necessary. The downlink (forward link) and/or uplink (reverse link) directions may be monitored. The handover may be requested by the phone or by the base station (BTS) of its source cell and, in some systems, by a BTS of a neighboring cell. The phone and the BTSes of the neighboring cells monitor each other's signals and the best target candidates are selected among the neighboring cells. In some systems, mainly based on CDMA, a target candidate may be selected among the cells which are not in the neighbor list. This is done in an effort to reduce the probability of interference due to the aforementioned near-far effect.\n\nIn analog systems the parameters used as criteria for requesting a hard handover are usually the \"received signal power\" and the \"received signal-to-noise ratio\" (the latter may be estimated in an analog system by inserting additional tones, with frequencies just outside the captured voice-frequency band at the transmitter and assessing the form of these tones at the receiver). In non-CDMA 2G digital systems the criteria for requesting hard handover may be based on estimates of the received signal power, bit error rate (BER) and block error/erasure rate (BLER), received quality of speech (RxQual), distance between the phone and the BTS (estimated from the radio signal propagation delay) and others. In CDMA systems, 2G and 3G, the most common criterion for requesting a handover is Ec/Io ratio measured in the pilot channel (CPICH) and/or RSCP.\n\nIn CDMA systems, when the phone in soft or softer handover is connected to several cells simultaneously, it processes the received in parallel signals using a rake receiver. Each signal is processed by a module called \"rake finger\". A usual design of a rake receiver in mobile phones includes three or more rake fingers used in soft handover state for processing signals from as many cells and one additional finger used to search for signals from other cells. The set of cells, whose signals are used during a soft handover, is referred to as the \"active set\". If the search finger finds a sufficiently-strong signal (in terms of high Ec/Io or RSCP) from a new cell this cell is added to the active set. The cells in the neighbour list (called in CDMA \"neighbouring set\") are checked more frequently than the rest and thus a handover with a neighbouring cell is more likely, however a handover with others cells outside the neighbor list is also allowed (unlike in GSM, IS-136/DAMPS, AMPS, NMT, etc.).\n\nThere are occurrences where a handoff is unsuccessful. Much research has been dedicated to this problem. The source of the problem was discovered in the late 1980s. Because frequencies cannot be reused in adjacent cells, when a user moves from one cell to another, a new frequency must be allocated for the call. If a user moves into a cell when all available channels are in use, the user’s call must be terminated. Also, there is the problem of signal interference where adjacent cells overpower each other resulting in receiver desensitization.\n\nThere are also inter-technology handovers where a call's connection is transferred from one access technology to another, e.g. a call being transferred from GSM to UMTS or from CDMA IS-95 to cdma2000.\n\nThe 3GPP UMA/GAN standard enables GSM/UMTS handoff to Wi-Fi and vice versa.\n\nDifferent systems have different methods for handling and managing handoff request. Some systems handle handoff in same way as they handle new originating call. In such system the probability that the handoff will not be served is equal to blocking probability of new originating call. But if the call is terminated abruptly in the middle of conversation then it is more annoying than the new originating call being blocked. So in order to avoid this abrupt termination of ongoing call handoff request should be given priority to new call this is called as handoff prioritization.\n\nThere are two techniques for this:\n\n\n\n\n"}
{"id": "14348925", "url": "https://en.wikipedia.org/wiki?curid=14348925", "title": "Hicks-neutral technical change", "text": "Hicks-neutral technical change\n\nHicks-neutral technical change is change in the production function of a business or industry which satisfies certain economic neutrality conditions. The concept of Hicks neutrality was first put forth in 1932 by John Hicks in his book \"The Theory of Wages\". A change is considered to be Hicks neutral if the change does not affect the balance of labor and capital in the products' production function. More formally, given the Solow model production function\na Hicks-neutral change is one which only changes formula_2.\n\n\n"}
{"id": "21182020", "url": "https://en.wikipedia.org/wiki?curid=21182020", "title": "History of paper", "text": "History of paper\n\nPaper, a thin unwoven material made from milled plant fibers, is primarily used for writing, artwork, and packaging; it is commonly white. The first papermaking process was documented in China during the Eastern Han period (25–220 CE), traditionally attributed to the court official Cai Lun. During the 8th century, Chinese papermaking spread to the Islamic world, where pulp mills and paper mills were used for papermaking and money making. By the 11th century, papermaking was brought to Europe. By the 13th century, papermaking was refined with paper mills utilizing waterwheels in Spain. Later European improvements to the papermaking process came in the 19th century with the invention of wood-based papers.\n\nAlthough precursors such as papyrus and amate existed in the Mediterranean world and pre-Columbian Americas, respectively, these materials are not defined as true paper. Nor is true parchment considered paper; used principally for writing, parchment is heavily prepared animal skin that predates paper and possibly papyrus. In the twentieth century with the advent of plastic manufacture some plastic \"paper\" was introduced, as well as paper-plastic laminates, paper-metal laminates, and papers infused or coated with different products that give them special properties. \n\nThe word \"paper\" is etymologically derived from \"papyrus\", Ancient Greek for the \"Cyperus papyrus\" plant. Papyrus is a thick, paper-like material produced from the pith of the \"Cyperus papyrus\" plant which was used in ancient Egypt and other Mediterranean societies for writing long before paper was used in China.\n\nPapyrus is prepared by cutting off thin ribbon-like strips of the interior of the \"Cyperus papyrus\", and then laying out the strips side-by-side to make a sheet. A second layer is then placed on top, with the strips running at right angle to the first. The two layers are the pounded together into a sheet. The result is very strong, but has an uneven surface, especially at the edges of the strips. When used in scrolls, repeated rolling and unrolling causes the strips to come apart again, typically along vertical lines. This effect can be seen in many ancient papyrus documents.\n\nPaper contrasts with papyrus in that the plant material is broken down through maceration or disintegration before the paper is pressed. This produces a much more even surface, and no natural weak direction in the material which falls apart over time.\n\nArchaeological evidence of papermaking predates the traditional attribution given to Cai Lun, an imperial eunuch official of the Han dynasty (202 BCE – CE 220), thus the exact date or inventor of paper can not be deduced. The earliest extant paper fragment was unearthed at Fangmatan in Gansu province, and was likely part of a map, dated to 179–141 BCE. Fragments of paper have also been found at Dunhuang dated to 65 BCE and at Yumen pass, dated to 8 BCE.\n\n\"Cai Lun's\" invention, recorded hundreds of years after it took place, is dated to 105 CE. The innovation is a type of paper made of mulberry and other bast fibres along with fishing nets, old rags, and hemp waste which reduced the cost of paper production, which prior to this, and later, in the West, depended solely on rags.\n\nDuring the Shang (1600–1050 BCE) and Zhou (1050–256 BCE) dynasties of ancient China, documents were ordinarily written on bone or bamboo (on tablets or on bamboo strips sewn and rolled together into scrolls), making them very heavy, awkward, and hard to transport. The light material of silk was sometimes used as a recording medium, but was normally too expensive to consider. The Han dynasty Chinese court official Cai Lun (c. 50–121 CE) is credited as the inventor of a method of papermaking (inspired by wasps and bees) using rags and other plant fibers in 105 CE. However, the discovery of specimens bearing written Chinese characters in 2006 at Fangmatan in north-east China's Gansu Province suggests that paper was in use by the ancient Chinese military more than 100 years before Cai, in 8 BCE, and possibly much earlier as the map fragment found at the Fangmatan tomb site dates from the early 2nd century BCE. It therefore would appear that \"Cai Lun's contribution was to improve this skill systematically and scientifically, fix a recipe for papermaking\".\n\nThe record in the \"Twenty-Four Histories\" says\n\nThe production process may have originated from the practice of pounding and stirring rags in water, after which the matted fibres were collected on a mat. The bark of paper mulberry was particularly valued and high quality paper was developed in the late Han period using the bark of \"tan\" (檀; sandalwood). In the Eastern Jin period a fine bamboo screen-mould treated with insecticidal dye for permanence was used in papermaking. After printing was popularized during the Song dynasty the demand for paper grew substantially. In the year 1101, 1.5 million sheets of paper were sent to the capital.\n\nAmong the earliest known uses of paper was padding and wrapping delicate bronze mirrors according to archaeological evidence dating to the reign of Emperor Wu of Han from the 2nd century BCE. Padding doubled as both protection for the object as well as the user in cases where poisonous \"medicine\" were involved, as mentioned in the official history of the period. Although paper was used for writing by the 3rd century CE, paper continued to be used for wrapping (and other) purposes. Toilet paper was used in China from around the late 6th century. In 589, the Chinese scholar-official Yan Zhitui (531–591) wrote: \"Paper on which there are quotations or commentaries from Five Classics or the names of sages, I dare not use for toilet purposes\". An Arab traveler who visited China wrote of the curious Chinese tradition of toilet paper in 851, writing: \"... [the Chinese] do not wash themselves with water when they have done their necessities; but they only wipe themselves with paper\".\n\nDuring the Tang dynasty (618–907) paper was folded and sewn into square bags to preserve the flavor of tea. In the same period, it was written that tea was served from baskets with multi-colored paper cups and paper napkins of different size and shape. During the Song dynasty (960–1279) the government produced the world's first known paper-printed money, or banknote (\"see Jiaozi and Huizi\"). Paper money was bestowed as gifts to government officials in special paper envelopes. During the Yuan dynasty (1271–1368), the first well-documented Europeans in Medieval China, the Venetian merchant Marco Polo remarked how the Chinese burned paper effigies shaped as male and female servants, camels, horses, suits of clothing and armor while cremating the dead during funerary rites.\n\nAccording to Timothy Hugh Barrett, paper played a pivotal role in early Chinese written culture, and a \"strong reading culture seems to have developed quickly after its introduction, despite political fragmentation.\" Indeed the introduction of paper had immense consequences for the book world. It meant books would no longer have to be circulated in small sections or bundles, but in their entirety. Books could now be carried by hand rather than transported by cart. As a result individual collections of literary works increased in the following centuries.\n\nTextual culture seems to have been more developed in the south by the early 5th century, with individuals owning collections of several thousand scrolls. In the north an entire palace collection might have been only a few thousand scrolls in total. By the early 6th century, scholars in both the north and south were capable of citing upwards of 400 sources in commentaries on older works. A small compilation text from the 7th century included citations to over 1,400 works.\n\nThe personal nature of texts was remarked upon by a late 6th century imperial librarian. According to him, the possession of and familiarity with a few hundred scrolls was what it took to be socially accepted as an educated man. \n\nAccording to Endymion Wilkinson, one consequence of the rise of paper in China was that \"it rapidly began to surpass the Mediterranean empires in book production.\" During the Tang dynasty, China became the world leader in book production. In addition the gradual spread of woodblock printing from the late Tang and Song further boosted their lead ahead of the rest of the world.\n\nHowever despite the initial advantage afforded to China by the paper medium, by the 9th century its spread and development in the middle east had closed the gap between the two regions. Between the 9th to early 12th centuries, libraries in Cairo, Baghdad, and Cordoba held collections larger than even the ones in China, and dwarfed those in Europe. From about 1500 the maturation of paper making and printing in Southern Europe also had an effect in closing the gap with the Chinese. The Venetian Domenico Grimani's collection numbered 15,000 volumes by the time of his death in 1523. After 1600, European collections completely overtook those in China. The Bibliotheca Augusta numbered 60,000 volumes in 1649 and surged to 120,000 in 1666. In the 1720s the Bibliotheque du Roi numbered 80,000 books and the Cambridge University 40,000 in 1715. After 1700, libraries in North America also began to overtake those of China, and toward the end of the century, Thomas Jefferson's private collection numbered 4,889 titles in 6,487 volumes. The European advantage only increased further into the 19th century as national collections in Europe and America exceeded a million volumes while a few private collections, such as that of Lord Action, reached 70,000.\n\nPaper became central to the three arts of China – poetry, painting, and calligraphy. In later times paper constituted one of the 'Four Treasures of the Scholar's Studio,' alongside the brush, the ink, and the inkstone.\n\nAfter its origin in central China, the production and use of paper spread steadily. It is clear that paper was used at Dunhuang by 150 CE, in Loulan in the modern-day province of Xinjiang by 200, and in Turpan by 399. Paper was concurrently introduced in Japan sometime between the years 280 and 610.\n\nPaper spread to Vietnam in the 3rd century.\n\nPaper spread to Korea in the 4th century.\n\nPaper spread to Japan in the 5th century.\n\nPaper spread to India in the 7th century. However, the use of paper was not widespread there until the 12th century.\n\nAfter the defeat of the Chinese in the Battle of Talas in 751 (present day Kyrgyzstan), the invention spread to the Middle East.\n\nThe legend goes, the secret of papermaking was obtained from two Chinese prisoners from the Battle of Talas, which led to the first paper mill in the Islamic world being founded in Samarkand in Sogdia (modern-day Uzbekistan). There was a tradition that Muslims would release their prisoners if they could teach ten Muslims any valuable knowledge. There are records of paper being made at Gilgit in Pakistan by the sixth century, in Samarkand by 751, in Baghdad by 793, in Egypt by 900, and in Fes, Morocco around 1100.\n\nThe laborious process of paper making was refined and machinery was designed for bulk manufacturing of paper. Production began in Baghdad, where a method was invented to make a thicker sheet of paper, which helped transform papermaking from an art into a major industry. The use of water-powered pulp mills for preparing the pulp material used in papermaking, dates back to Samarkand in the 8th century, though this should not be confused with paper mills (see \"Paper mills\" section below). The Muslims also introduced the use of trip hammers (human- or animal-powered) in the production of paper, replacing the traditional Chinese mortar and pestle method. In turn, the trip hammer method was later employed by the Chinese. Historically, trip hammers were often powered by a water wheel, and are known to have been used in China as long ago as 40 BCE or maybe even as far back as the Zhou Dynasty (1050 BCE–221 BCE).\n\nBy the 9th century, Muslims were using paper regularly, although for important works like copies of the revered Qur'an, vellum was still preferred. Advances in book production and bookbinding were introduced.\nIn Muslim countries they made books lighter—sewn with silk and bound with leather-covered paste boards; they had a flap that wrapped the book up when not in use. As paper was less reactive to humidity, the heavy boards were not needed.\nBy the 12th century in Marrakech in Morocco a street was named \"Kutubiyyin\" or book sellers which contained more than 100 bookshops.\n\nIn 1035 a Persian traveler visiting markets in Cairo noted that vegetables, spices and hardware were wrapped in paper for the customers after they were sold.\nSince the First Crusade in 1096, paper manufacturing in Damascus had been interrupted by wars, but its production continued in two other centres. Egypt continued with the thicker paper, while Iran became the center of the thinner papers. Papermaking was diffused across the Islamic world, from where it was diffused further west into Europe. Paper manufacture was introduced to India in the 13th century by Muslim merchants, where it almost wholly replaced traditional writing materials.\n\nThe oldest known paper document in the West is the Mozarab Missal of Silos from the 11th century, probably using paper made in the Islamic part of the Iberian Peninsula. They used hemp and linen rags as a source of fiber. The first recorded paper mill in the Iberian Peninsula was in Xàtiva in 1056.\nPapermaking reached Europe as early as 1085 in Toledo and was firmly established in Xàtiva, Spain by 1150. It is clear that France had a paper mill by 1190, and by 1276 mills were established in Fabriano, Italy and in Treviso and other northern Italian towns by 1340. Papermaking then spread further northwards, with evidence of paper being made in Troyes, France by 1348, in Holland sometime around 1340–1350, in Mainz, Germany in 1320, and in Nuremberg by 1390 in a mill set up by Ulman Stromer. This was just about the time when the woodcut printmaking technique was transferred from fabric to paper in the old master print and popular prints. There was a paper mill in Switzerland by 1432 and the first mill in England was set up by John Tate in 1490 near Stevenage in Hertfordshire, but the first commercially successful paper mill in Britain did not occur before 1588 when John Spilman set up a mill near Dartford in Kent. During this time, paper making spread to Poland by 1491, to Austria by 1498, to Russia by 1576, to the Netherlands by 1586, to Denmark by 1596, and to Sweden by 1612.\n\nArab prisoners who settled in a town called Borgo Saraceno in the Italian Province of Ferrara introduced Fabriano artisans in the Province of Ancona the technique of making paper by hand. At the time they were renowned for their wool-weaving and manufacture of cloth. Fabriano papermakers considered the process of making paper by hand an art form and were able to refine the process to successfully compete with parchment which was the primary medium for writing at the time. They developed the application of stamping hammers to reduce rags to pulp for making paper, sizing paper by means of animal glue, and creating watermarks in the paper during its forming process. The Fabriano used glue obtained by boiling scrolls or scraps of animal skin to size the paper; it is suggested that this technique was recommended by the local tanneries. The introduction of the first European watermarks in Fabriano was linked to applying metal wires on a cover laid against the mould which was used for forming the paper.\n\nThey adapted the waterwheels from the fuller's mills to drive a series of three wooden hammers per trough. The hammers were raised by their heads by cams fixed to a waterwheel's axle made from a large tree trunk.\n\nIn the Americas, archaeological evidence indicates that a similar bark-paper writing material was used by the Mayans no later than the 5th century CE. Called \"amatl\" or \"amate\", it was in widespread use among Mesoamerican cultures until the Spanish conquest. The earliest sample of amate was found at Huitzilapa near the Magdalena Municipality, Jalisco, Mexico, belonging to the shaft tomb culture. It is dated to 75 BCE.\n\nThe production of amate is much more similar to paper than papyrus. The bark material is soaked in water, or in modern methods boiled, so that it breaks down into a mass of fibres. They are then laid out in a frame and pressed into sheets. It is a true paper product in that the material is not in its original form, but the base material has much larger fibres than those used in modern papers. As a result, amate has a rougher surface than modern paper, and may dry into a sheet with hills and valleys as the different length fibres shrink.\n\nEuropean papermaking spread to the Americas first in Mexico by 1575 and then in Philadelphia by 1690.\n\nThe use of human and animal powered mills was known to Chinese and Muslim papermakers. However, evidence for water-powered paper mills is elusive among both prior to the 11th century. The general absence of the use of water-powered paper mills in Muslim papermaking prior to the 11th century is suggested by the habit of Muslim authors at the time to call a production center not a \"mill\", but a \"paper manufactory\".\n\nDonald Hill has identified a possible reference to a water-powered paper mill in Samarkand, in the 11th-century work of the Persian scholar Abu Rayhan Biruni, but concludes that the passage is \"too brief to enable us to say with certainty\" that it refers to a water-powered paper mill. This is seen by Halevi as evidence of Samarkand first harnessing waterpower in the production of paper, but notes that it is not known if waterpower was applied to papermaking elsewhere across the Islamic world at the time. Burns remains sceptical, given the isolated occurrence of the reference and the prevalence of manual labour in Islamic papermaking elsewhere prior to the 13th century.\n\nClear evidence of a water-powered paper mill dates to 1282 in the Spanish Kingdom of Aragon. A decree by the Christian king Peter III addresses the establishment of a royal \"molendinum\", a proper hydraulic mill, in the paper manufacturing centre of Xàtiva. The crown innovation was operated by the Muslim Mudéjar community in the Moorish quarter of Xàtiva,though it appears to have been resented by sections of the local Muslim papermakering community; the document guarantees them the right to continue the way of traditional papermaking by beating the pulp manually and grants them the right to be exempted from work in the new mill. Paper making centers began to multiply in the late 13th century in Italy, reducing the price of paper to one sixth of parchment and then falling further; paper making centers reached Germany a century later.\n\nThe first paper mill north of the Alps was established in Nuremberg by Ulman Stromer in 1390; it is later depicted in the lavishly illustrated \"Nuremberg Chronicle\". From the mid-14th century onwards, European paper milling underwent a rapid improvement of many work processes.\n\nBefore the industrialisation of the paper production the most common fibre source was recycled fibres from used textiles, called rags. The rags were from hemp, linen and cotton. A process for removing printing inks from recycled paper was invented by German jurist Justus Claproth in 1774. Today this method is called deinking. It was not until the introduction of wood pulp in 1843 that paper production was not dependent on recycled materials from ragpickers.\n\nAlthough cheaper than vellum, paper remained expensive, at least in book-sized quantities, through the centuries, until the advent of steam-driven paper making machines in the 19th century, which could make paper with fibres from wood pulp. Although older machines predated it, the Fourdrinier papermaking machine became the basis for most modern papermaking. Nicholas Louis Robert of Essonnes, France, was granted a patent for a continuous paper making machine in 1799. At the time he was working for Leger Didot with whom he quarrelled over the ownership of the invention. Didot sent his brother-in-law, John Gamble, to meet Sealy and Henry Fourdrinier, stationers of London, who agreed to finance the project. Gamble was granted British patent 2487 on 20 October 1801. With the help particularly of Bryan Donkin, a skilled and ingenious mechanic, an improved version of the Robert original was installed at Frogmore Paper Mill, Hertfordshire, in 1803, followed by another in 1804. A third machine was installed at the Fourdriniers' own mill at Two Waters. The Fourdriniers also bought a mill at St Neots intending to install two machines there and the process and machines continued to develop.\n\nHowever, experiments with wood showed no real results in the late 18th century and at the start of the 19th century. By 1800, Matthias Koops (in London, England) further investigated the idea of using wood to make paper, and in 1801 he wrote and published a book titled \"Historical account of the substances which have been used to describe events, and to convey ideas, from the earliest date, to the invention of paper.\" His book was printed on paper made from wood shavings (and adhered together). No pages were fabricated using the pulping method (from either rags or wood). He received financial support from the royal family to make his printing machines and acquire the materials and infrastructure needed to start his printing business. But his enterprise was short lived. Only a few years following his first and only printed book (the one he wrote and printed), he went bankrupt. The book was very well done (strong and had a fine appearance), but it was very costly.\n\nThen in the 1830s and 1840s, two men on two different continents took up the challenge, but from a totally new perspective. Both Friedrich Gottlob Keller and Charles Fenerty began experiments with wood but using the same technique used in paper making; instead of pulping rags, they thought about pulping wood. And at about the same time, by mid-1844, they announced their findings. They invented a machine which extracted the fibres from wood (exactly as with rags) and made paper from it. Charles Fenerty also bleached the pulp so that the paper was white. This started a new era for paper making. By the end of the 19th-century almost all printers in the western world were using wood in lieu of rags to make paper.\n\nTogether with the invention of the practical fountain pen and the mass-produced pencil of the same period, and in conjunction with the advent of the steam driven rotary printing press, wood based paper caused a major transformation of the 19th century economy and society in industrialized countries. With the introduction of cheaper paper, schoolbooks, fiction, non-fiction, and newspapers became gradually available by 1900. Cheap wood based paper also meant that keeping personal diaries or writing letters became possible and so, by 1850, the clerk, or writer, ceased to be a high-status job.\n\nThe original wood-based paper was acidic due to the use of alum and more prone to disintegrate over time, through processes known as slow fires. Documents written on more expensive rag paper were more stable. Mass-market paperback books still use these cheaper mechanical papers (see below), but book publishers can now use acid-free paper for hardback and trade paperback books.\n\nDetermining the provenance of paper is a complex process that can be done in a variety of ways. The easiest way is using a known sheet of paper as an exemplar. Using known sheets can produce an exact identification. Next, comparing watermarks with those contained in catalogs or trade listings can yield useful results. Inspecting the surface can also determine age and location by looking for distinct marks from the production process. Chemical and fiber analysis can be used to establish date of creation and perhaps location.\n\n\n"}
{"id": "8312093", "url": "https://en.wikipedia.org/wiki?curid=8312093", "title": "History of wind power", "text": "History of wind power\n\nWind power has been used as long as humans have put sails into the wind. For more than two millennia wind-powered machines have ground grain and pumped water. Wind power was widely available and not confined to the banks of fast-flowing streams, or later, requiring sources of fuel. Wind-powered pumps drained the polders of the Netherlands, and in arid regions such as the American mid-west or the Australian outback, wind pumps provided water for livestock and steam engines.\n\nWith the development of electric power, wind power found new applications in lighting buildings remote from centrally-generated power. Throughout the 20th century parallel paths developed small wind plants suitable for farms or residences, and larger utility-scale wind generators that could be connected to electricity grids for remote use of power. Today wind powered generators operate in every size range between tiny plants for battery charging at isolated residences, up to near-gigawatt sized offshore wind farms that provide electricity to national electrical networks.\n\nBy 2014, over 240,000 commercial-sized wind turbines were operating in the world, producing 4% of the world's electricity.\n\nSailboats and sailing ships have been using wind power for at least 5,500 years, and architects have used wind-driven natural ventilation in buildings since similarly ancient times. The use of wind to provide mechanical power came somewhat later in antiquity.\n\nThe Babylonian emperor Hammurabi planned to use wind power for his ambitious irrigation project in the 17th century BC.\n\nThe windwheel of the engineer Heron of Alexandria in 1st-century AD Roman Egypt is the earliest known instance of using a wind-driven wheel to power a machine. Another early example of a wind-driven wheel was the prayer wheel, which has been used in ancient India, Tibet, and China since the 4th century.\n\nThe first practical windmills were in use in Sistan, a region in Iran and bordering Afghanistan, at least by the 9th century and possibly as early as the 7th century. These \"Panemone windmills\" were horizontal windmills, which had long vertical driveshafts with six to twelve rectangular sails covered in reed matting or cloth. These windmills were used to pump water, and in the gristmilling and sugarcane industries. The use of windmills became widespread across the Middle East and Central Asia, and later spread to China and India. Vertical windmills were later used extensively in Northwestern Europe to grind flour beginning in the 1180s, and many examples still exist. By 1000 AD, windmills were used to pump seawater for salt-making in China and Sicily.\n\nWind-powered automata are known from the mid-8th century: wind-powered statues that \"turned with the wind over the domes of the four gates and the palace complex of the Round City of Baghdad\". The \"Green Dome of the palace was surmounted by the statue of a horseman carrying a lance that was believed to point toward the enemy. This public spectacle of wind-powered statues had its private counterpart in the 'Abbasid palaces where automata of various types were predominantly displayed.\"\n\nThe first windmills in Europe appear in sources dating to the twelfth century. These early European windmills were sunk post mills. The earliest certain reference to a windmill dates from 1185, in Weedley, Yorkshire, although a number of earlier but less certainly dated twelfth-century European sources referring to windmills have also been adduced. While it is sometimes argued that crusaders may have been inspired by windmills in the Middle East, this is unlikely since the European vertical windmills were of significantly different design than the horizontal windmills of Afghanistan. Lynn White Jr., a specialist in medieval European technology, asserts that the European windmill was an \"independent invention;\" he argues that it is unlikely that the Afghanistan-style horizontal windmill had spread as far west as the Levant during the Crusader period. In medieval England rights to waterpower sites were often confined to nobility and clergy, so wind power was an important resource to a new middle class. In addition, windmills, unlike water mills, were not rendered inoperable by the freezing of water in the winter.\n\nBy the 14th century Dutch windmills were in use to drain areas of the Rhine River delta.\n\nWindmills were used to pump water for salt making on the island of Bermuda, and on Cape Cod during the American revolution.\nIn Mykonos and in other islands of Greece windmills were used to mill flour and remained in use until the early 20th century. Many of them are now refurbished to be inhabited.\n\nThe first wind turbine used for the production of electricity was built in Scotland in July 1887 by Prof James Blyth of Anderson's College, Glasgow (the precursor of Strathclyde University). Blyth's 10 m high, cloth-sailed wind turbine was installed in the garden of his holiday cottage at Marykirk in Kincardineshire and was used to charge accumulators developed by the Frenchman Camille Alphonse Faure, to power the lighting in the cottage, thus making it the first house in the world to have its electricity supplied by wind power. Blyth offered the surplus electricity to the people of Marykirk for lighting the main street, however, they turned down the offer as they thought electricity was \"the work of the devil.\" Although he later built a wind turbine to supply emergency power to the local Lunatic Asylum, Infirmary and Dispensary of Montrose, the invention never really caught on as the technology was not considered to be economically viable.\n\nAcross the Atlantic, in Cleveland, Ohio a larger and heavily engineered machine was designed and constructed in the winter of 1887-1888 by Charles F. Brush, this was built by his engineering company at his home and operated from 1886 until 1900. The Brush wind turbine had a rotor 17 m (56 foot) in diameter and was mounted on an 18 m (60 foot) tower. Although large by today's standards, the machine was only rated at 12 kW; it turned relatively slowly since it had 144 blades. The connected dynamo was used either to charge a bank of batteries or to operate up to 100 incandescent light bulbs, three arc lamps, and various motors in Brush's laboratory. The machine fell into disuse after 1900 when electricity became available from Cleveland's central stations, and was abandoned in 1908.\n\nIn 1891 Danish scientist, Poul la Cour, constructed a wind turbine to generate electricity, which was used to produce hydrogen by electrolysis to be stored for use in experiments and to light the Askov High school. He later solved the problem of producing a steady supply of power by inventing a regulator, the Kratostate, and in 1895 converted his windmill into a prototype electrical power plant that was used to light the village of Askov.\n\nIn Denmark there were about 2,500 windmills by 1900, used for mechanical loads such as pumps and mills, producing an estimated combined peak power of about 30 MW.\n\nIn the American midwest between 1850 and 1900, a large number of small windmills, perhaps six million, were installed on farms to operate irrigation pumps. Firms such as Star, Eclipse, Fairbanks-Morse, and Aeromotor became famed suppliers in North and South America.\n\nDevelopment in the 20th century might be usefully divided into the periods:\n\nIn Denmark wind power was an important part of a decentralized electrification in the first quarter of the 20th century, partly because of Poul la Cour from his first practical development in 1891 at Askov. By 1908 there were 72 wind-driven electric generators from 5 kW to 25 kW. The largest machines were on 24 m (79 ft) towers with four-bladed 23 m (75 ft) diameter rotors. In 1957 Johannes Juul installed a 24 m diameter wind turbine at Gedser, which ran from 1957 until 1967. This was a three-bladed, horizontal-axis, upwind, stall-regulated turbine similar to those now used for commercial wind power development.\n\nIn 1927 the brothers Joe Jacobs and Marcellus Jacobs opened a factory, Jacobs Wind in Minneapolis to produce wind turbine generators for farm use. These would typically be used for lighting or battery charging, on farms out of reach of central-station electricity and distribution lines. In 30 years the firm produced about 30,000 small wind turbines, some of which ran for many years in remote locations in Africa and on the Richard Evelyn Byrd expedition to Antarctica. Many other manufacturers produced small wind turbine sets for the same market, including companies called Wincharger, Miller Airlite, Universal Aeroelectric, Paris-Dunn, Airline and Winpower.\n\nIn 1931 the Darrieus wind turbine was invented, with its vertical axis providing a different mix of design tradeoffs from the conventional horizontal-axis wind turbine. The vertical orientation accepts wind from any direction with no need for adjustments, and the heavy generator and gearbox equipment can rest on the ground instead of atop a tower.\n\nBy the 1930s windmills were widely used to generate electricity on farms in the United States where distribution systems had not yet been installed. Used to replenish battery storage banks, these machines typically had generating capacities of a few hundred watts to several kilowatts. Beside providing farm power, they were also used for isolated applications such as electrifying bridge structures to prevent corrosion. In this period, high tensile steel was cheap, and windmills were placed atop prefabricated open steel lattice towers.\n\nThe most widely used small wind generator produced for American farms in the 1930s was a two-bladed horizontal-axis machine manufactured by the Wincharger Corporation. It had a peak output of 200 watts. Blade speed was regulated by curved air brakes near the hub that deployed at excessive rotational velocities. These machines were still being manufactured in the United States during the 1980s. In 1936, the U.S. started a rural electrification project that killed the natural market for wind-generated power, since network power distribution provided a farm with more dependable usable energy for a given amount of capital investment.\n\nIn Australia, the Dunlite Corporation built hundreds of small wind generators to provide power at isolated postal service stations and farms. These machines were manufactured from 1936 until 1970.\n\nA forerunner of modern horizontal-axis utility-scale wind generators was the WIME D-30 in service in Balaklava, near Yalta, USSR from 1931 until 1942. This was a 100 kW generator on a 30 m (100 ft) tower, connected to the local 6.3 kV distribution system. It had a three-bladed 30 metre rotor on a steel lattice tower. It was reported to have an annual load factor of 32 per cent, not much different from current wind machines.\n\nIn 1941 the world's first megawatt-size wind turbine was connected to the local electrical distribution system on the mountain known as Grandpa's Knob in Castleton, Vermont, United States. It was designed by Palmer Cosslett Putnam and manufactured by the S. Morgan Smith Company. This 1.25 MW Smith-Putnam turbine operated for 1100 hours before a blade failed at a known weak point, which had not been reinforced due to war-time material shortages. No similar-sized unit was to repeat this \"bold experiment\" for about forty years.\n\nDuring the Second World War, small wind generators were used on German U-boats to recharge submarine batteries as a fuel-conserving measure. In 1946 the lighthouse and residences on the island of Neuwerk were partly powered by an 18 kW wind turbine 15 metres in diameter, to economize on diesel fuel. This installation ran for around 20 years before being replaced by a submarine cable to the mainland.\n\nThe Station d'Etude de l'Energie du Vent at Nogent-le-Roi in France operated an experimental 800 KVA wind turbine from 1956 to 1966.\n\nFrom 1974 through the mid-1980s the United States government worked with industry to advance the technology and enable large commercial wind turbines. The NASA wind turbines were developed under a program to create a utility-scale wind turbine industry in the U.S. With funding from the National Science Foundation and later the United States Department of Energy (DOE), a total of 13 experimental wind turbines were put into operation, in four major wind turbine designs. This research and development program pioneered many of the multi-megawatt turbine technologies in use today, including: steel tube towers, variable-speed generators, composite blade materials, partial-span pitch control, as well as aerodynamic, structural, and acoustic engineering design capabilities. The large wind turbines developed under this effort set several world records for diameter and power output. The MOD-2 wind turbine cluster of three turbines produced 7.5 megawatts of power in 1981. In 1987, the MOD-5B was the largest single wind turbine operating in the world with a rotor diameter of nearly 100 meters and a rated power of 3.2 megawatts. It demonstrated an availability of 95 percent, an unparalleled level for a new first-unit wind turbine. The MOD-5B had the first large-scale variable speed drive train and a sectioned, two-blade rotor that enabled easy transport of the blades. The 4 megawatt WTS-4 held the world record for power output for over 20 years. Although the later units were sold commercially, none of these two-bladed machines were ever put into mass production. When oil prices declined by a factor of three from 1980 through the early 1990s, many turbine manufacturers, both large and small, left the business. The commercial sales of the NASA/Boeing Mod-5B, for example, came to an end in 1987 when Boeing Engineering and Construction announced they were \"planning to leave the market because low oil prices are keeping windmills for electricity generation uneconomical.\"\n\nLater, in the 1980s, California provided tax rebates for wind power. These rebates funded the first major use of wind power for utility electricity. These machines, gathered in large wind parks such as at Altamont Pass would be considered small and un-economic by modern wind power development standards.\n\nA giant change took place in 1978 when the world's was constructed. It pioneered many technologies used in modern wind turbines and allowed Vestas, Siemens and others to get the parts they needed. Especially important was the novel wing construction using help from German aeronautics specialists. The power plant was capable of delivering 2MW, had a tubular tower, pitch controlled wings and three blades. It was built by the teachers and students of the Tvind school. Before completion these \"amateurs\" were much ridiculed. The turbine still runs today and looks almost identical to the newest most modern mills.\n\nDanish commercial wind power development stressed incremental improvements in capacity and efficiency based on extensive serial production of turbines, in contrast with development models requiring extensive steps in unit size based primarily on theoretical extrapolation. A practical consequence is that all commercial wind turbines resemble the \"Danish model\", a light-weight three-blade upwind design.\n\nAll major horizontal axis turbines today rotate the same way (clockwise) to present a coherent view. However, early turbines rotated counter-clockwise like the old windmills, but a shift occurred from 1978 and on. The individualist-minded blade supplier Økær made the decision to change direction in order to be distinguished from the collective Tvind and their small wind turbines. Some of the blade customers were companies that later evolved into Vestas, Siemens, Enercon and Nordex. Public demand required that all turbines rotate the same way, and the success of these companies made clockwise the new standard.\n\nIn the 1970s many people began to desire a self-sufficient life-style. Solar cells were too expensive for small-scale electrical generation, so some turned to windmills. At first they built ad-hoc designs using wood and automobile parts. Most people discovered that a reliable wind generator is a moderately complex engineering project, well beyond the ability of most amateurs. Some began to search for and rebuild farm wind generators from the 1930s, of which Jacobs Wind Electric Company machines were especially sought after. Hundreds of Jacobs machines were reconditioned and sold during the 1970s.\n\nFollowing experience with reconditioned 1930s wind turbines, a new generation of American manufacturers started building and selling small wind turbines not only for battery-charging but also for interconnection to electricity networks. An early example would be Enertech Corporation of Norwich, Vermont, which began building 1.8 kW models in the early 1980s.\n\nIn the 1990s, as aesthetics and durability became more important, turbines were placed atop tubular steel or reinforced concrete towers. Small generators are connected to the tower on the ground, then the tower is raised into position. Larger generators are hoisted into position atop the tower and there is a ladder or staircase inside the tower to allow technicians to reach and maintain the generator, while protected from the weather.\n\nAs the 21st century began, fossil fuel was still relatively cheap, but rising concerns over energy security, global warming, and eventual fossil fuel depletion led to an expansion of interest in all available forms of renewable energy. The fledgling commercial wind power industry began expanding at a robust growth rate of about 25% per year, driven by the ready availability of large wind resources, and falling costs due to improved technology and wind farm management.\n\nThe steady run-up in oil prices after 2003 led to increasing fears that peak oil was imminent, further increasing interest in commercial wind power. Even though wind power generates electricity rather than liquid fuels, and thus is not an immediate substitute for petroleum in most applications (especially transport), fears over petroleum shortages only added to the urgency to expand wind power. Earlier oil crises had already caused many utility and industrial users of petroleum to shift to coal or natural gas. Wind power showed potential for replacing natural gas in electricity generation on a cost basis.\n\nTechnological innovations, enabled by advances in computer aided engineering, continue to drive new developments in the application of wind power. By 2015, the largest wind turbine were 8MW capacity Vestas V164 for offshore use. By 2014, over 240,000 commercial-sized wind turbines were operating in the world, producing 4% of the world's electricity. Total installed capacity exceeded 336GW in 2014 with China, the U.S., Germany, Spain and Italy leading in installations.\n\nOffshore wind power began to expand beyond fixed-bottom, shallow-water turbines beginning late in the first decade of the 2000s. The world's first operational deep-water \"large-capacity\" floating wind turbine, Hywind, became operational in the North Sea off Norway in late 2009\n\nat a cost of some 400 million kroner (around US$62 million) to build and deploy.\n\nThese floating turbines are a very different construction technology—closer to floating oil rigs rather—than traditional fixed-bottom, shallow-water monopile foundations that are used in the other large offshore wind farms to date.\n\nBy late 2011, Japan announced plans to build a multiple-unit floating wind farm, with six 2-megawatt turbines, off the Fukushima coast of northeast Japan where the 2011 tsunami and nuclear disaster has created a scarcity of electric power. \nAfter the evaluation phase is complete in 2016, \"Japan plans to build as many as 80 floating wind turbines off Fukushima by 2020\" at a cost of some 10-20 billion Yen.\n\nAirborne wind energy systems use airfoils or turbines supported in the air by buoyancy or by aerodynamic lift. The purpose is to eliminate the expense of tower construction, and allow extraction of wind energy from steadier, faster, winds higher in the atmosphere. As yet no grid-scale plants have been constructed. Many design concepts have been demonstrated.\n\n\n\n"}
{"id": "37482023", "url": "https://en.wikipedia.org/wiki?curid=37482023", "title": "HusITa", "text": "HusITa\n\nhusITa (Human Services Information Technology Applications) is an international virtual associationand a registered US non-profit organizationestablished with the mission of promoting the ethical and effective use of information technology in the human services. The main focus of husITa, and the claim to expertise of its associates, is situated at the intersection of three core domains: information technology, human services, and social development. husITa pursues its mission through international conferences, publications and research dissemination directed at technology applications and innovations that promote social well-being.\n\nFor much of its early history husITa operated as an informal international network of human service academics and practitioners. One of the outcomes of its first international conferencehusITa1 held in 1987 in Birmingham, Englandwas the establishment of a working group to determine the feasibility of an international body 'to highlight the importance of human service computing, to guide developments, and to foster international co-operation'.\n\nThe working group was composed of Hein de Graaf (Netherlands), Walter LaMendola (USA), Dick Schoech (USA), and Stuart Toole (UK). Initial projects identified by the working group included the development of research agendas, position papers, repositories of information, and promoting a second husITa conference in 1989. Bryan Glastonbury was later added to the group as secretary. The working group met in Colorado, Denver for three days in May 1988 and published a report on the issues that a husITa international organization would need to address.\n\nAlthough the 1988 Denver meeting agreed its objectives, husITa wasn't formally established as an organization for another twelve years. The structure of the formal organization was later agreed to at Denver in 2000. The founding members at the Denver 2000 meeting were: Hein de Graaf, Walter LaMendola, Rob MacFadden, Jo Ann Regan, Jackie Rafferty, Jan Steyaert, Dick Schoech, Stuart Toole, and Victor Savtschenko.\n\nhusITa's objectives (agreed by the 1988 Denver working group) are to:\n\nThe \"Journal of Technology in Human Services\" is the official journal of husITa. Formerly known as \"Computers in Human Services\" it was launched in 1985 as a Haworth Press publication. Dick Schoech, a professor of social work at the University of Texas at Arlington, was its founding editor. The \"Journal of Technology in Human Services\" is a peer-reviewed, refereed journal now published by Taylor & Francis. Its scope includes the potential of information and communication technologies in mental health, developmental disability, welfare, addictions, education, and other human services. The current Editor-in-Chief is Dr. Lauri Goldkind (Associate Professor, Fordham University, USA), with Dr. Chitat Chan (Assistant Professor, Hong Kong Polytechnic University, Hong Kong) serves as the Associate Editor-in-Chief.\n\nhusITa1: husITa's first international conference was held between in September 1987 in Birmingham, England.\n\nhusITa2: \"Computer Technology and Human Services in the 90’s: Advancing Theory and Practice\", June 1991 in New Brunswick, New Jersey.\n\nhusITa3: \"Information Technology and the Quality of Life and Services\", June 1993 in Maastricht, the Netherlands. The same year saw the formation of a husITa Foundation in the Netherlands which continued until its disestablishment in 2003.\n\nhusITa4: \"Information Technology in the Human Services: Dreams and Realities\", June 1996 in Lapland, Finland.\n\nhusITa5: \"Social Services in the Information Society: Closing the GAP\", August–September 1999 in Budapest, Hungary.\n\nhusITa6: \"Technology and Human Services in a Multicultural Society\", September 2001, in Charleston, South Carolina. However, the conference was cut short as a result of the terrorist attacks in the USA on 11 September 2001. A brief husITa board meeting was held, the by-laws were approved, and officers were elected.\n\nhusITa7: \"Digital Inclusion-Building a Digital Inclusive Society\", August 2004 in Hong Kong, China. It had been delayed from its planned date of 2003 due to an outbreak of SARS.\n\nhusITa8: \"Information Technology and Diversity in Human Services: Promoting Strength Through Difference\", August 2007 in Toronto, Canada.\n\nhusITa9: \"ICT as a Context for Human Services\", June 2010 in Hong Kong, China. This event was held in conjunction with the 2010 Joint World Conference on Social Work and Social Development.\n\nhusITa14: \"Sustainable & Ethical use of Technology\". July 9–12, Melbourne, Australia. Held in conjunction with the 2014 Joint World Conference on Social Work, Education, and Social Development.\n\nhusITa was built on the activity of an international network of human service organizations, academics and practitioners in the USA, the UK and the Netherlands. The section below highlights some of the key organizations, people and events.\n\nIn 1978, Gunther R. Geiss, aprofessor of social work at Adelphi University, New York, conducted a survey of US schools of social work. The survey sought to identify faculty members who had used computers in their administrative, teaching or research activities, or who had consulted or participated in the design and development of computer-based information systems. There were over 80 positive responses indicating a wide range of activities and levels of involvement. The survey initiated the development of a system to track and communicate with individuals with expertise in computers and human services.\n\nWalter LaMendola, professor of social work at the University of Denver in Colorado, described an incident during a social work conference in 1979 suggesting early indications of the resistance of some social work professionals to computer use in the human services. This is a theme which has continued throughout the history of technology use in the human services and continues to the present day. Some aspects of this resistance can be considered as a well-founded concern about the ethical issues surrounding human service technology applications. However, other aspects of technology resistance seem to be a less rational form of Neo-Luddism.\n\nGrowing interest in the use of technology in the human services led a group of US human service technology specialists, meeting at a Council of Social Work Education conference in Louisville Kentucky in 1981, to form the Computer Use In Social Services Network (CUSSN). By the end of 1981 the network had over 350 members. The CUSSN newsletter continued in print until 1992 when it was merged with the first academic journal on human service technology \"Computers in Human Services\".\n\nIn 1984 Gunther Geiss was sponsored by the Silberman Fund to organize the Wye Plantation Conference on Human Services Technology. Conference members developed pre-conference position papers via EYES: a centralized email system.\n\nIn 1985, CUSSN developed CUSSNet CUSSNet, a PC and FidoNet based networking system that automatically exchanged emails between members each night during off-peak telephone hours. FidoNet was a PC distributed email, bulletin board, and file sharing system that preceded the Internet. CUSSNet quickly developed nodes in major cities in the US, the UK, and the Netherlands.\n\nThe name husITa (Human Service Information Technology Applications) was coined in 1983 by Walter LaMendola and Brian Klepinger at the University of Denver.\n\nThe Human Service Microcomputer Conference was held in Seattle.\n\nIn 1984 Stuart Toole formed Computer Applications in Social Work (CASW) in the UK to set up and run national conferences and to publish the CASW journal.\n\nBased on the success of the first UK technology conferences, Stuart Toole, Walter LaMendola, and Brian Klepinger agreed to pursue an international conference in 1987: this conference was to become HUSITA's first international conference HUSITA1.\n\nThe CASW journal was later renamed New Technology in the Human Services in [when] and continued in publication under this title until it was closed in 2003.\n\nIn 1986 the second UK conference held on social welfare computing was held.\n\nIn 1985 Bryan Glastonbury, from the University of Southampton published \"Computers in Social Work\", the first major academic text on technology and human services. In the same year the University of Southampton began publishing the journal New Technology in the Human Services under the editorship of Bryan Glastonbury. In the same year the University of Southampton established the Centre for Human Service Technology (CHST) with Jackie Rafferty as Director. In 2007, Jan Steyaert joined as adjunct research professor and together they edited a special issue of the British Journal of Social Work on \"social work in the digital age\".\nThe Centre for Human Service Technology is based at the University of Southampton in England. CHST is an international, multi-disciplinary research center focused on influencing the appropriate use of technology in social work practice and education and researching its implementation and impact.\n\nIn July 1997 the CTI Centre for Human Services held a conference on \"Social Services and Learning Technology\" hosted by the Institute for Health and Community Services at the University of Bournemouth.\n\nIn 2003 the journal New Technology in Human Services ceased publication.\n\nIn 1986 Hein de Graaf, Director of the CREON foundation in the Netherlands, organised the first of a series of three-day gatherings. The CREON foundation was a Dutch foundation for computer research, expertise and support in field of the human services. The gatherings, called WELCOM, were designed to increase the knowledge and understanding of information technology in the Dutch human services. WELCOM1 was held in 1986 in Bussum with Walter LaMendola as the main international speaker. WELCOM2, a smaller Dutch-only conference, took place in 1987, again in Bussum. The next WELCOM event, WELCOM3: a combined conference and fair, was held jointly with HUSITA3 in Maastricht in 1993.\n\nFollowing the success of HUSITA's first international conference held in Birmingham in 1987 the Dutch Ministry of Social Welfare, Health and Cultural Affairs organized an informal meeting of European experts in the field of Information Technology and Human Services. The meeting was one of the outcomes of a feasibility study carried out by the CREON foundation, concerning international cooperation in this field. A main conclusion of this feasibility study was that an international (European) network should be established for the exchange of products, ideas, expertise, experiences and skills with respect to the introduction and use of IT in the human services. As a first step in this approach the informal meeting of experts was organized. The European network of organizations was called ENITH (European Network Information Technology and Human services).\n\nIn 1992 an ENITH3 Expert Meeting on “IT Applications and the Quality of Life and Services” was held in The Netherlands.\n\nIn 1994, from September 21 to September 23 an ENITH4 conference was held in Berlin, Germany. Bernd kolleck chaired this conference.\n\nIn September 1995 a CAUSA5/ENITH5 conference on “The Impact of Information Technology on Social Policy,” was held in Eindhoven, The Netherlands. Jan Steyaert chaired this conference.\n"}
{"id": "32303617", "url": "https://en.wikipedia.org/wiki?curid=32303617", "title": "IControlPad", "text": "IControlPad\n\nThe iControlPad is a wireless game controller compatible with a variety of smartphones, tablets, and personal computers. It is designed for use as either a standalone gamepad or attached to appropriately sized devices, such as the iPhone, using a clamp system. Due to this, the iControlPad is able to add traditional physical gaming controls to devices which otherwise rely on inputs such as touchscreens and accelerometers.\n\nThe iControlPad's input controls include an eight-directional D-pad, dual analog nubs, six digital face buttons, and two digital trigger buttons on the gamepad's reverse. The sides of the iControlPad are detachable, with two different attachment types: rubber grips, for using the controller as a standard wireless gamepad; or plastic clamps, for connecting with a suitable handheld, such as a smartphone or iPod Touch. A mini USB port on the bottom of the iControlPad can be used to charge the internal 1500mAh battery, update the device's firmware, and charge attached devices using a USB On-The-Go connection and an appropriate adapter.\n\nThe iControlPad, a Bluetooth device, can be run in a wide variety of modes, including as a HID keyboard, mouse, joystick, and gamepad, among others, allowing compatibility with equipment which is limited to only certain types of input. One of the iControlPad's modes mimics the protocol used by the iCade, an arcade cabinet released for the Apple iPad, facilitating compatibility between apps designed for the iCade and the iControlPad hardware.\n\nDue to the iControlPad's ability to operate as a Bluetooth keyboard—by mapping the D-pad and buttons to standard keyboard keys—it is able to communicate with devices such as those running Apple's iOS, including the iPhone and iPad, which do not support Bluetooth gamepads. Since iOS natively supports keyboards, apps can be developed with iControlPad compatibility using either its own protocol or that of the iCade. Thus, the iControlPad is able to control video games and video game console emulators across multiple platforms.\n\nDevelopment of the iControlPad began in 2007, with testing using a hacked SNES gamepad to connect to an iPhone over the dock connection. Once the serial connection was working, the first prototype iControlPad was produced, using a design styled after the Sony PSP. This earliest concept was a one-piece case enveloping the iPhone, with a D-pad on the left side, and four face buttons on the right in a landscape orientation, and was first revealed in 2008.\n\nBy November 2009, a completely redesigned iControlPad prototype was under development. This much larger version moved the controls below the screen and added two analog nubs and two trigger buttons to the controller. This design, which featured clamps to attach it to the iPhone, was much closer to the version that was ultimately released, and would soon go into production.\n\nHowever, one large change was made very late in development. The team had secretly added Bluetooth support to the iControlPad, in order to increase compatibility beyond the iPhone and its proprietary connection. This proved fortunate when Apple began exercising its rights over the dock connector, suing an unlicensed accessory maker. Thus, the iControlPad team were forced to adapt to use the Bluetooth connection for the iPhone, and it was this version which finally became available for order in February 2011.\n\n\n\n\nReception for the iControlPad has been mostly positive. Register Hardware noted that while \"patience and geekery\" were required to get the controller working, the iControlPad \"almost perfectly solves the touchscreen game control conundrum\". Gadgetoid homed in on the device's usefulness for classic gaming, remarking that it was \"awesome [...] for emulation on the go\". TouchArcade's reviewer said while playing games with the iControlPad that \"the experience feels great\", but that \"[he couldn't] recommend that the typical gamer run out right now and grab one,\" due to its limited support on the iTunes App Store.\n\nEarly reviews were mixed on the quality of the controls, with DroidGamers describing them as \"very loose\", while, conversely, Register Hardware said \"the analogue nubs and face buttons work extremely well\". The controller's responsiveness was later improved by replacing the original rubber keymat with a larger one. In their review, Gadgetoid lauded the inputs as having \"a great tactile feel and a liberal amount of travel with a good response.\"\n\nA successor, the iControlPad 2, was successfully funded via Kickstarter in October 2012. As of November 2013, it has been cancelled, with little to no refunds.\n\n"}
{"id": "16531537", "url": "https://en.wikipedia.org/wiki?curid=16531537", "title": "Industrious Revolution", "text": "Industrious Revolution\n\nThe Industrious Revolution is the title given to a period of time, usually given as between 1600 and 1800 that led up to the Industrial Revolution. It is a term first coined by the Japanese demographic historian Akira Hayami , and accepted by other historians to help further explain the advent of the Industrial Revolution. Much of this theory deals with the spending behaviours of families in the period. It also deals with the production and consumption of goods. In fact, Industrious Revolutions are often characterized by a rise in demand for \"market-supplied goods\", which will minimize the value of domestic goods, before the ultimate consumption. Industrious Revolutions often occur during a period where labour wages have stagnated or decreased. The theory of a pre-industrial Industrious Revolution is contested within the history community.\n\nHayami introduced the concept of Industrious Revolution in a Japanese-language work published in 1967. It was coined to compare the labour-intensive technologies of Tokugawa Japan (1603-1868) with the capital-intensive technologies of Britain's Industrial Revolution. Hayami observed that the two countries took different paths due to the different mix of factor endowments (capital for Britain and labour for Japan). He introduced Industrious Revolution to describe the Japanese developmental trajectory, which - lacking the British capital - exploited the benefits of increasing labour absorption. \n\nThis proposed Industrious Revolution does not aim to replace the Industrial Revolution in history; rather it is designed to supplement it. By revamping the history of the period directly preceding the Industrial Revolution, some historians hope to ensure that people get a better understanding of a particular aspect of the Early Modern Period.\nThe basic picture painted of the pre-Industrial Revolution is that the Industrial Revolution was the result of a surplus of money and crops, which led to the development of new technology. This new technology eventually developed into factories. The Industrious Revolution addresses this belief, saying instead, that the overwhelming desire for more goods directly preceded the Industrial Revolution. The theory states that during the Industrious Revolution there was an increase in demand for goods, but that supply did not rise as quickly.\nEventually, some achievements of industry and agriculture, as well as the decisions made by households, helped to increase the supply, as well as the demand for goods. These behaviours, when combined constitute an Industrious Revolution. A quick summation of the differences between the Industrious Revolution and the Industrial Revolution is that the former is concerned with demand, and the latter is supply based. The right mindset to a productional economy and world may have increased the supply of technology, but they would have had little impact on invention without a demand for new techniques. \nThe theory of an Industrious Revolution, as put forward by historian Jan de Vries, claims that there were two parts to the Industrious Revolution. First, there was a reduction of leisure time as the utility of monetary income rose. Second, the focus of labour shifted from goods and services to marketable goods. \n\nSome within the field of cultural history have theorized that a further motive for an industrious revolution is a shift in the view on what it takes for an individual to be considered independent. Independence was equated with land ownership prior to an industrious revolution. Wage earners were granted a degraded and dependent status. Typically, as in the case of pre-industrial Great Britain and America, these dependent sorts were deemed unworthy of full citizenship rights. Industrious revolutions shift the meaning of independence from land-holding to earning a competency. A wage earner could now be considered independent if he/she earned enough money to support a household. Hard work was viewed as essential for reaching this goal.\n\nOne of the suggested hallmarks of an Industrious Revolution is that of increased work days. However, according to historians Gregory Clark and Ysbrand Van Der Werf, there has been no information found to suggest an increase in work days, in the time period between the Medieval Period and the nineteenth century. These records even indicate that before 1750, some people were working three hundred days per year. Even in the period preceding the Industrial Revolution people were working at least two hundred and ninety days in a year. So, this information would demonstrate that there was very little increase in days worked. Since increase in work loads is one of the suggested hallmarks of an Industrious Revolution, this would work against the theory.\nClark and Van Der Werf have also examined the output of a couple of English industries. For one, they looked at records of the saw mills in England. Between 1300 and 1800, the period directly preceding and following the proposed Industrious Revolution, the estimated amount of lumber sawed increased about eighty percent. However, this increase in lumber sawed can be attributed to new technologies, and not in fact the influence of an Industrious Revolution. In contrast, they mention the threshing industry. Unlike the lumber sawing business, this industry shows “clear downward movement” in threshing rates, after which there are no longer any trends. This information would help to disprove the idea of an Industrious Revolution, since, as it has been put forth, there is no universal trend displaying an increase of work habits.\n\nIn a later work, Hayami cited that de Vries and other theorists' interpretations did not use the term in the same way he does. Hayami noted that these saw Industrious Revolution and Industrial Revolution as a continuum while the original idea considers the two revolutions as opposing concepts. Hayami also stressed that the term explained how the Japanese became industrious for some reason at one point and that eventually they will no longer be. \n\nPrior the proposed era of the Industrious Revolution most goods were produced either by household or by guilds. There were many households involved in the production of marketable goods. Most of what was produced by these households were things that involved cloth- textiles, clothing, as well as art, and tapestries. These would be produced by the households, or by their respective guilds. It was even possible for guilds and merchants to outsource into more rural areas, to get some of the work done. These merchants would bring the raw materials to the workers, who would then, using the supplied materials, make the goods. For example, young girls would be hired to make silk, because they were the only people believed to have hands dexterous enough to make the silk properly. Other occupations such as knitting, a job that was never organized into guilds, could easily be done within the household.\n\nThe income of the household became dependent upon the quality and the quantity of everyone's work. Even if people were not working for an individual guild they could still supply and make items not controlled by the guilds. These would be small, but necessary items like wooden dishes, or soaps. So, basically, much of production was done by, or for, guilds. This would indicate that much of what was done was not done for one individual household, but for a larger group or organization. \n\nPrior to the Industrious Revolution, the household was the major site of production, and could be comparable to a factory. However, things were to change a bit during the Industrious Revolution. If the theory is to be believed, then there was a shift in the running of the household. The everyday goods and products used by the household would slowly shift from mostly homemade, to mostly \"commercially produced goods\". At the same time, the women would be more than likely to be able to attain jobs outside of the household. This is also seen within the context of the Industrial Revolution, where women would often find small jobs to help supplement their husband's wages. This would demonstrate the gradual movement away from the household as a centre of production.\n\nThe patterns of consumption present in England at this time period were similar to what one might find in Europe, as well as in some of the Ottoman territories. This is unsurprising, however, since most of Europe and the Ottoman Empire were all connected through trade. Through these trade connections, people were able to buy many of the luxury goods that they desired. Very common amongst the nobility was the idea of Conspicuous consumption. This can be traced back, even into the Middle Ages. People, the nobility especially, had trade connections throughout Europe, and many of them would use these connections to buy the works of art, etc. that they desired. This does not extend only to the rich however: even Medieval peasants enjoyed imported luxury items; there is evidence to suggest that some English peasants drank imported French wine.\n\nPeople were consuming products well before the Industrious Revolution. There were stock exchanges all over Europe, even London. People were also clearly making products for consumption, as the large number of guilds existing within Europe at that point would suggest. What would cause the period between 1600 and 1800 to be labeled an Industrious Revolution, by looking at the patterns of consumption, is the rise in demand for these products. So, what would set this period in time apart would be more demands for luxury items. Especially those items that could not be produced in the homes or by the guilds. During the proposed Industrious Revolution, this demand for luxury items would be greater than the supply could accommodate. A rise in the rate at which people were consuming goods, especially when combined with other factors of the times, could have potentially heralded the Industrious Revolution.\n\n"}
{"id": "10908329", "url": "https://en.wikipedia.org/wiki?curid=10908329", "title": "Institute of Scientific and Technical Communicators", "text": "Institute of Scientific and Technical Communicators\n\nThe Institute of Scientific and Technical Communicators (ISTC) is the UK's largest professional association for those involved in technical communication and information design. It encourages professional education and standards, provides guidance about the value of using professional communicators, and provides research resources and networking opportunities for its members and affiliates.\n\nThe ISTC as it is today was created in 1972 from an amalgamation of three associations: the Presentation of Technical Information Group (founded in 1948), the Technical Publications Association and the Institute of Technical Publicity & Publications. The institute was incorporated as a limited company on 19 July 1972.\n\nThe ISTC is managed by its Council, with day-to-day running handled by a professional administration company. Members have the opportunity to contribute their ideas, skills and time to specific Steering Groups and can stand for Council.\n\nThe ISTC is a member of the Professional Associations Research Network (PARN). This organisation provides a network for professional bodies and offers services and events on subjects such as governance, Continuing Professional Development, ethics and standards, and member relations. The ISTC was one of the sponsors of a project in 2006 to explore the issues affecting professional associations as they grow.\n\nThe ISTC has a Code of Professional Practice, with which it expects its members to comply. This was developed in accordance with research carried out by PARN and its author was cited in PARN's third book on ethics.\n\nTechnical communication encompasses a wide range of activities, all with the common thread of communicating complex or important information in the most effective way for the reader. Technical communicators create information that affects people in virtually every industry and area of society. Examples of roles that may be involved in technical communication are:\n\n\nThe ISTC offers several grades of individual membership. There are two corporate grades, Member (MISTC) and Fellow (FISTC), and two non-corporate grades, Associate and Student. Corporate grades are entitled to vote at the ISTC's AGM. Members cite a sense of community as an important factor in their continued membership.\n\nOrganisations can join the ISTC as Business Affiliates.\n\nThe ISTC publishes periodicals and books:\n\nThe ISTC organises an annual conference, Technical Communication UK, which is held in September.\n\nThe ISTC makes annual awards:\n\nThe ISTC participates in the international technical communication profession through its membership of two umbrella organisations:\n\nThe ISTC was one of the founder members of both organisations, INTECOM in 1970 and TCeurope in 2002.\n\n"}
{"id": "49008928", "url": "https://en.wikipedia.org/wiki?curid=49008928", "title": "List of Windows 10 Mobile devices", "text": "List of Windows 10 Mobile devices\n\nThis is a list of all devices coming natively with Microsoft's Windows 10 Mobile operating system. The list also includes devices running two additional flavours of Windows 10 for mobile devices, Windows 10 Mobile Enterprise and Windows 10 IoT Mobile Enterprise. All devices below come with SD card support.\n\nProcessors supported are Qualcomm's Snapdragon 210, 212, 410, 617, 800, 801, 808, 810 and 820 as well as Rockchip's RK3288.\n\n"}
{"id": "21016331", "url": "https://en.wikipedia.org/wiki?curid=21016331", "title": "Location awareness", "text": "Location awareness\n\nLocation awareness refers to devices that can passively or actively determine their location. Navigational instruments provide location coordinates for vessels and vehicles. Surveying equipment identifies location with respect to a well-known locationa wireless communications device. \"Network location awareness\" (NLA) describes the location of a node in a network.\n\nThe term applies to navigating, real-time locating and positioning support with global, regional or local scope. The term has been applied to traffic, logistics, business administration and leisure applications. Location awareness is supported by navigation systems, positioning systems and/or locating services.\n\nLocation awareness without the active participation of the device is known as non-cooperative locating or detection.\n\nThe term originated for configurations settings of network systems and addressed network entities. \"Network location awareness\" (NLA) services collect network configuration and location information, and notify applications when this information changes. With the advent of global positioning systems and radio-equipped mobile devices, the term was redefined to include consumer-focused applications.\n\nWhile location awareness began as a matter of static user location, the notion was extended to reflect movement. Context models have been proposed to support context-aware applications which use location to tailor interfaces, refine application-relevant data, increase the precision of information retrieval, discover services, make user interaction implicit and build smart environments. For example, a location-aware mobile phone may confirm that it is currently in a building.\n\nDescription in logical terms uses a structured textual form. International standardisation offers a common method using ISO/TS 16952 as originated with German standards DIN EN 61346 and DIN EN 81346.\n\nLocation in mathematical terms offers coordinates that refer to a nominated point of reference.\n\nLocation in network terms relates to locating network nodes. These include:\n\n\n\"Crisp\" locating offers precise coordinates, using wireless signals or optical sighting, possibly with phase angle measurements. Coordinates are relative to either a standardized system of coordinates, e.g. WGS84, or a fixed object such as a building plan. Real-time locating adds timely delivery of results, especially for moving targets. Real time locating is defined with ISO/IEC 19762-5 and ISO/IEC 24730-1. Fuzzy locating offers less precision, e.g., presence \"near\" a point of reference. Measuring wireless power levels can supply this degree of precision. Less sophisticated systems can use wireless distance measurements to estimate a point of reference in polar coordinates (distance and direction) from another site. Index locating indicates presence at a known location, as with fixed RFID readers and RFID tags.\n\nLocation-aware systems address the acquisition of coordinates in a grid (for example using distance metrics and lateration algorithms) or at least distances to reference points (for example discriminating presence at a certain choke point on a corridor or in a room of a building).\n\nNavigation and reckoning are key concerns for seafarers, aviators and professional drivers. The task is to dynamically determine the current location and the time, distance and direction to destination. radar served for regional demand and NAVSTAR satellite systems for global demand. GPS and similar systems have become ubituitous in long-haul transport operation and are becoming a standard automobile feature.\n\nSurveying is the static complement to navigating. It is essential for delineating land ownership and for architects and civil engineers designing construction projects. Optical surveying technology preceded LASER triangulating aids.\n\nCurrently location awareness is applied to design innovative process controls, and is integral to ubiquitous and wearable computing. On mobile devices, location aware search can prioritize results that are close to the device. Conversely, the device location can be disclosed to others, at some cost to the bearer's privacy.\n\nRFID provides a time/location reference for an object, but does not indicate that the object remains at that location, which is sufficient for applications that limit access, such as tracking objects entering and leaving a warehouse, or for objects moving on a fixed route, such as charging tolls for crossing a bridge.\n\nLocation awareness enables new applications for ubiquitous computing systems and mobile phones. Such applications include the automatic reconfiguration of a computing device to suit the location in which it is currently being used (examples include ControlPlane and Locamatic), or publishing a user's location to appropriate members of a social network, and allowing retailers to publish special offers to potential customers who are near to the retailers. Allegedly, individuals gain self confidence with confirmation of current whereabouts.\n\nWhile governments have created global systems for computing locations, independent localized systems exist at scales ranging from one building to sub-national regions.\n\nSuch solutions may apply concepts of RTLS and WPAN, wireless LAN or DECT, with results in proprietary terms of floor plans or room numbers. Local systems degrade as distance from the locality increases. Applications include the automatic reconfiguration of a computing device to suit the location in which it is currently being used.\n\nThis approach uses for example mobile phone systems, such as 3GPP, GSM or LTE, typically returning information in standardized coordinates as with WGS84 in standardized formats such as NMEA for outdoor usage or in symbolic coordinates referring to street addresses.\n\nThis approach relies on GPS technology, currently supplied by NAVSTAR and may in future employ the pending Galileo (satellite navigation) system, generally adopting WGS84 and NMEA. Applications include avalanche rescue or emergency and mountain rescue as well as with search and rescue, (SAR) and combat search and rescue, (CSAR).\n\n\n"}
{"id": "10550813", "url": "https://en.wikipedia.org/wiki?curid=10550813", "title": "Ministry of Earth Sciences", "text": "Ministry of Earth Sciences\n\nThe Ministry of Earth Sciences was formed in the year 2006 from a merger of the India Meteorological Department (IMD), the National Centre for Medium Range Weather Forecasting (NCMRWF), the Indian Institute of Tropical Meteorology (IITM), Pune, and Earth Risk Evaluation Centre (EREC), and the Ministry of Ocean Development.\n\nCurrently, the ministry is headed by Dr. Harsh Vardhan.\n\nThe Ministry’s mandate is to look after Atmospheric Sciences, Ocean Science & Technology and Seismology in an integrated manner.\n\n\n\nAll institutions under ESSO are connected through National Knowledge Network (NKN) and its Common User Group (CUG).\n\nAdithya HPC located at IITM, Pune is one of the largest computation facility in India.\n\n\n"}
{"id": "22402748", "url": "https://en.wikipedia.org/wiki?curid=22402748", "title": "Mobile Telephone Switching Office", "text": "Mobile Telephone Switching Office\n\nThe Mobile Telephone Switching Office (MTSO) is the mobile equivalent of a PSTN Central Office. The MTSO contains the switching equipment or Mobile Switching Center (MSC) for routing mobile phone calls. It also contains the equipment for controlling the cell sites that are connected to the MSC.\n\nThe systems in the MTSO are the heart of a cellular system. It is responsible for interconnecting calls with the local and long distance landline telephone companies, compiling billing information (with the help of its CBM/SDM), etc. It also provides resources needed to efficiently serve a mobile subscriber such as registration, authentication, location updating and call routing. Its subordinate BSC/RNC are responsible for assigning frequencies to each call, reassigning frequencies for handoffs, controlling handoffs so a mobile phone leaving one cell (formally known as BTS)'s coverage area, can be switched automatically to a channel in the next cell.\n\nAll cellular systems have at least one MTSO which will contain at least one MSC. The MSC is responsible for switching calls to mobile units as well as to the local telephone system, recording billing data and processing data from the cell site controllers.\n\nThe MSC is connected to a close telephone exchange by a trunk group. This provides an interface to the (Public Switched Telephone Network) (PSTN). It also provides connectivity to the PSTN. The region to be served by a Cellular Geographic Serving Area(CGSA) is split into geographic cells. These cells are ideally hexagonal in shape and they are initially laid out with their centers about 4 to 8 miles apart from each other. Other MTSO equipment, the cell site controllers provide control functions for a group of cell sites and actions of mobile phones through command and control data channels. To achieve this, there has to be a method of connectivity between the MTSO and the cell site. This may be by DS1, DS3, OCn or Ethernet circuits.\n\nThe MSC also plays a major role in call routing. When a mobile phone is turned on, it listens for the network operator's SID (System Identification Code) on the control channel. If it cannot find any control channels to listen to then it assumes it is outside the range and displays a message indicating no service. If it finds a control channel to listen to, receives the SID and then compares it to the SID programmed into the phone. If both SIDs match then it knows that it is communicating with a cell in its home system.\n\nThe phone also transmits a registration request along with the SID. If the subscriber has previously registered to a particular MSC then the MSC will have a record in its VLR and will therefore know the subscriber last registered location. If the subscriber is unknown to the MSC's VLR, it will query the HLR to obtain the subscriber's profile and save it for a set length of time in its VLR. Anytime a MSC successfully registers a subscriber the HLR record is also updated. This will help when a call is received outside of the MSC's coverage area or for an incoming PSTN call.\n\nA subscriber's VLR profile has a LAC(Location Area Code - Area server by cluster of BTS/cell sites) CID (Cell ID) as well as a list of allowed and disallowed services/features and other information.\n\nWith the subscriber's VLR Profile the MSC can determine the last \"known\" LAC/CID for this subscriber and knows which BTS to use when it needs to ring/page that phone for an incoming call. When the MSC gets the call, it checks its database for the location of the phone. Then it picks a frequency pair the phone will use in that cell to take the call. The MSC communicates with the phone over the control channel to tell it which frequencies to use, and once the phone and the tower switch on those frequencies, the call is connected.\n\nAs a mobile unit engaged in a call moves away from a cell site or formally known as Base transceiver station and its signal weakens, the BSC(GSM) or RNC(3G UMTS) will automatically instruct it to tune to a different frequency, one assigned to the newly entered BTS. This process is called handoff. The BSC/RNC determines when handoff should take place by analyzing measurements of radio signal strength made by the present controlling cell site and by its neighbors. The returning instructions for handoff sent during a call must use the voice channel. The data regarding the new channel are sent rapidly (in about 50 milliseconds), and the entire returning process takes only about 300 milliseconds. After handoff, if the SID on the control channel does not match the SID programmed into the phone, then the phone assumes that it is roaming.\n\nThe MSC also performs handovers/handoffs which occurs when a call needs to be handed off to a different BSC/RNC it serves or to completely new MSC. A MSC can serve many BSCs/RNCs which in turn server many BTS. As a result, a MSC can serve a large area typically hundreds of miles. Highly populated areas require more BTSs and BSCs/RNCs which will in turn obviously, will reduce the geographic coverage of a DMSC. The above in a MSC is what's considered as its mobility management. The BTS, BSC/RNC are the RAN/UTRAN (Radio Access Network & UMTS Radio Access Network) subset in the mobile network. The remaining functions of a MSC are identical to a PSTN switch.\n\n\n"}
{"id": "39207065", "url": "https://en.wikipedia.org/wiki?curid=39207065", "title": "Mobile forms", "text": "Mobile forms\n\nA mobile form is an electronic or digital form application that functions on a smartphone or tablet device. Mobile forms enable users to collect data using mobile devices, and then to send the results back to the source. Mobile forms exist to replace paper forms as a more productive means of data collection, eliminating the need to transcribe or scan paper data results into a back office system.\n\nDepending on the mobile form application provider, some mobile form solutions allow offices to dispatch data to mobile form applications. In addition, other mobile form applications can be connected with various cloud services, servers, and social media platforms.\n\nDepending on the business, the motivating factors to deploying mobile forms may vary. Some businesses implement mobile forms to speed up processes, while others institute mobile forms with field users to reduce costs associated with transporting paper forms back and forth. Furthermore, green-minded businesses implement mobile forms in order to be more environmentally friendly, thus reducing their reliance on paper, ink printing, and subsequent waste.\n\nAdvanced mobile form features include signature capture, bar code capture, photo capture, GPS location form info, time form info, and skip logic.\n\nUses for mobile forms include:\n\n"}
{"id": "48590779", "url": "https://en.wikipedia.org/wiki?curid=48590779", "title": "Mobile procurement", "text": "Mobile procurement\n\nMobile procurement is mobile business software that helps organizations streamline their procurement process from a mobile device. Features of mobile procurement software include mobile purchase order creation, on-demand notifications, and real-time analytics. What makes mobile procurement successful is the ability to leverage software-side servers to move data along. The key benefit for organizations using mobile procurement systems is the ability to track business operations using any ordinary mobile device.\n\nMobile procurement software is generally represented in the form of custom applications provided as an add-on feature of a larger ERP solution. As such, if the goal of a mobile procurement system should be to complement existing information systems, mobile procurement software should typically involve the needs of procurement professionals before implementation. Assuring that necessary features are prioritized over having many features is key to successful user adoption.\n\nIt also simplifies order management processes by removing the confusion and disorder often seen with paper-based procurement. Mobile procurement provides a clearer view of the steps behind procurement and give companies the insight needed to consistently secure the best possible total cost of ownership.\n\nMobile enterprise application software is the use of office software applications on a mobile platform in a way that adapts to different devices and networks. Many organizations use these applications to increase employee productivity and streamline business operations.\n\nThe first common mobile enterprise application was the implementation of email. Now, 53% of emails are opened on a mobile device, a 45% increase in three years. The next wave of popular mobile enterprise application software was the advent of the CRM software. This allows salespeople to stay up-to-date with the business and manage customer relationships crucial to success from anywhere.\nAnother enterprise software application being leveraged on a mobile platform is procurement. This trend is led by the consumer shopping experience. Over 36% of online sales on Black Friday in 2015 came from mobile shopping, according to IBM. As more shoppers are migrating to mobile applications, procurement departments need to keep up with user preferences. This mobile platform provides capabilities to complete the procurement process from start to finish. This includes searching for items and services, comparing vendors and prices, submitting purchase requests, approving requests, electronic signing, purchase orders and invoicing. Being able to accomplish this all in one place from any device significantly streamlines business procurement and operations.\n\nUsers leverage mobile procurement for various reasons, and the tool can benefit businesses in many ways.\n\nUser Convenience\nThe clear advantage of using mobile procurement is the ability to use it anywhere from any device. This eliminates interruptions in the process or the need to complete a request from a single device. Users can search, compare and request items from anywhere. Suppliers can receive and sign purchase orders electronically. Checking inventory levels of physical locations provides a strong utility for mobile procurement. \n\nSave Time\nMobile procurement saves time by reducing approval cycle time. A manager can make approvals from anywhere, not just his office. Users can request things they need when they need it, instead of after the fact, breaking corporate policy. Procurement now fills idle time rather than being a major item on a to-do list. This keeps employees and business moving forward. A fast procurement turnaround increases outcomes.\n\nVisibility\nFull visibility of suppliers and information helps businesses make informed decisions for better results and quicker processes. User analytics also help companies shop smarter in the long run.\n\nMobile interaction now exceeds desktop Web interaction by 9%. Now that mobile is the dominant form of online activity, mobile procurement is simply a natural step into today’s digital landscape. Mobile procurement platforms can be approached in two ways: native apps or Web apps using HTML5.\n\nNative Apps\nA native app is a mobile application developed specifically for use on mobile devices, launched directly from the home screen. In the United States, use of mobile apps greatly exceeds use of mobile Web browsing, but that trend is not worldwide.\n\nUsing a native app requires development for all operating systems and brands. A fully operational native app needs to be designed for iOS, Android, Microsoft, all their various generations, and at different resolutions and orientations. For total success of an app, users will expect it to work on everything from a high-resolution tablet down to an Apple Watch.\n\nThe other factor to consider with native apps is the way they access backend data. This is often more complex than a simple push to read data that takes place on a Web browser. Apps are developed, depending on their purpose and functionality, to access other applications such as the camera or local storage.\n\nNative apps can be the preferred use of mobile procurement in environments with either no IT access or highly secure networks. Financial institutions value privacy and security when considering mobile tools operating in their network. Oil and gas environments are often remote and require apps that can work with limited Internet access.\n\nA downside to mobile apps is the memory they occupy on a device. Because of this, many businesses will steer away from apps that require a great deal of storage.\n\nWeb Apps and HTML5 \nA Web app is run by a browser and is really a responsive Web site. Responsive sites change and adapt to any digital environment, including operating system, screen size and orientation. This has several advantages for mobile procurement platforms.\n\nResponsive design can adjust to any browser on any device. The design is such that images, text, and user experience adjust based on the size and resolution of the platform. Adaptive web design is based on predetermined parameters for each platform. Both are ideal for any business that plans on interacting with customers on any type of mobile device.\n\nHTML, JavaScript and CSS3 seamlessly integrate backend systems with browsers and user interfaces. This creates an easy-to-use experience for all users on any device. Although some designs are scaled down, which can limit functionality, responsive and adaptive web design is easily accessible for any customer. And functionality is all about priority. If a scaled down site still performs the necessary functions, users will still find the advantages of increased turnarounds and time saving.\n\nThe widespread availability of Wi-Fi increases the availability of HTML5 sites, reducing concerns about accessibility. With Wi-Fi everywhere, mobile procurement can happen in any place on a responsive site.\n\nElegantly designed responsive and adaptive sites are the ideal solution for mobile procurement because of the simplicity of creating and implementing one. If a user’s experience is smooth and consistent, they will have no problem accessing a mobile procurement platform from any device. With the advancement of tools and technology it is currently possible to provide a native feel and minimal native feature set to your mobile products. Examples would be AngularJS, which allows for rapid web app development while preserving quality. \n\nAdditional references:\nhttp://www.digitalpurchaseorder.com/purchase-order-apps/\nhttps://www.vroozi.com/platform/mobile-procurement/\nhttp://www.coupa.com/why-coupa/mobile/\nhttps://www.procurify.com/features#mobile\nhttp://www.sap.com/pc/tech/mobile/software/applications/lob/apps/procurement.html\nhttp://spendmatters.com/2013/11/19/forresters-duncan-jones-mobile-buying-shift-eprocurement/\nhttp://spendmatters.com/2015/06/01/mobile-apps-a-passing-fad-or-a-procurement-solution-must-have/\n"}
{"id": "59091880", "url": "https://en.wikipedia.org/wiki?curid=59091880", "title": "Multimodal anthropology", "text": "Multimodal anthropology\n\nMultimodal anthropology is an emerging subfield of social cultural anthropology that encompasses anthropological research and knowledge production across multiple traditional and new media platforms and practices including film, video, photography, theatre, design, podcast, mobile apps, interactive games, web-based social networking, immersive 360 video and augmented reality. As characterized in American Anthropologist\",\" multimodal anthropology is an \"anthropology that works across multiple media, but one that also engages in public anthropology and collaborative anthropology through a field of differentially linked media platforms\" (Collins, Durington & Gill). A multimodal approach also encourages anthropologist to reconsider the ways in which they conduct their research, to pay close attention to the role various media technologies and digital devices plays in the lives of their interlocutors, and how they these technologies redefine what fieldwork looks like.\n\nMultimodal anthropology is not a new concept. It has been a fundamental part of anthropological research and fieldwork from the early days of the disciple. Anthropologists have been experimenting with different forms media technologies throughout the twentieth century whenever confronted with the limitation of text-based ethnography. Multimodal is a term that has readily been used since the 1970s in varied disciplines as psychotherapy, phonetics, genetics, literature and medicine to characterize different approaches to carrying out scientific research that involves to a certain degree, thinking outside of the box. In the early 1990s, semioticians used the terms to discuss different forms of communication across different media, eventually including digital media.\n\nTechnological advances in the later part of the twentieth century, the accessibility to photography, film cameras and audio recorders led to the emergence of visual anthropology as a sub discipline dedicated to the study and production of ethnographic photography, film and media. Building in this legacy, multimodal anthropology seeks to expand the boundaries of visual anthropology to incorporate emerging technologies of twenty-first century including mobile networking, social media, geo-mapping, virtual reality, podcasting, interactive design, along with other traditional forms of learning and knowledge production like art and drawing that were often sidelined within visual anthropology, such as interactive gaming, theatre, performance, graphic novels, ethnofiction and experimental ethnography. As Samuel Collins, Matthew Durington and Harjant Gill note in their introductory essay on title \"Multimodality: An Invitation,\" published in American Anthropologist, \"multimodal anthropologies does not attempt – or desire – to supplant visual anthropology. Rather it seeks to include traditional forms of visual anthropology while simultaneously broadening the purview of the discipline to engage in variety of media forms that exist today.\"\n\n"}
{"id": "639115", "url": "https://en.wikipedia.org/wiki?curid=639115", "title": "Neolithic Revolution", "text": "Neolithic Revolution\n\nThe Neolithic Revolution, Neolithic Demographic Transition, Agricultural Revolution, or First Agricultural Revolution was the wide-scale transition of many human cultures during the Neolithic period from a lifestyle of hunting and gathering to one of agriculture and settlement, making an increasingly larger population possible. These settled communities permitted humans to observe and experiment with plants to learn how they grew and developed. This new knowledge led to the domestication of plants.\n\nArchaeological data indicates that the domestication of various types of plants and animals happened in separate locations worldwide, starting in the geological epoch of the Holocene around 12,500 years ago. It was the world's first historically verifiable revolution in agriculture. The Neolithic Revolution greatly narrowed the diversity of foods available, resulting in a downturn in human nutrition.\n\nThe Neolithic Revolution involved far more than the adoption of a limited set of food-producing techniques. During the next millennia it would transform the small and mobile groups of hunter-gatherers that had hitherto dominated human pre-history into sedentary (non-nomadic) societies based in built-up villages and towns. These societies radically modified their natural environment by means of specialized food-crop cultivation, with activities such as irrigation and deforestation which allowed the production of surplus food. Other developments found very widely are the domestication of animals, pottery, polished stone tools, and rectangular houses.\n\nThese developments, sometimes called the Neolithic package, provided the basis for centralized administrations and political structures, hierarchical ideologies, depersonalized systems of knowledge (e.g. writing), densely populated settlements, specialization and division of labour, more trade, the development of non-portable art and architecture, and property ownership. The earliest known civilization developed in Sumer in southern Mesopotamia (); its emergence also heralded the beginning of the Bronze Age.\n\nThe relationship of the above-mentioned Neolithic characteristics to the onset of agriculture, their sequence of emergence, and empirical relation to each other at various Neolithic sites remains the subject of academic debate, and varies from place to place, rather than being the outcome of universal laws of social evolution. The Levant saw the earliest developments of the Neolithic Revolution from around 10,000 BCE, followed by sites in the wider Fertile Crescent.\n\nThe term \"Neolithic Revolution\" was coined in 1923 by V. Gordon Childe to describe the first in a series of agricultural revolutions in Middle Eastern history. The period is described as a \"revolution\" to denote its importance, and the great significance and degree of change affecting the communities in which new agricultural practices were gradually adopted and refined.\n\nThe beginning of this process in different regions has been dated from 10,000 to 8,000 BC in the Fertile Crescent and perhaps 8000 BC in the Kuk Early Agricultural Site of Melanesia. This transition everywhere seems associated with a change from a largely nomadic hunter-gatherer way of life to a more settled, agrarian-based one, with the inception of the domestication of various plant and animal species—depending on the species locally available, and probably also influenced by local culture. Recent archaeological research suggests that in some regions such as the Southeast Asian peninsula, the transition from hunter-gatherer to agriculturalist was not linear, but region-specific.\n\nThere are several competing (but not mutually exclusive) theories as to the factors that drove populations to take up agriculture. The most prominent of these are:\n\nOnce agriculture started gaining momentum, around 9000 BC, human activity resulted in the selective breeding of cereal grasses (beginning with emmer, einkorn and barley), and not simply of those that would favour greater caloric returns through larger seeds. Plants with traits such as small seeds or bitter taste would have been seen as undesirable. Plants that rapidly shed their seeds on maturity tended not to be gathered at harvest, therefore not stored and not seeded the following season; years of harvesting selected for strains that retained their edible seeds longer.\n\nSeveral plant species, the \"pioneer crops\" or Neolithic founder crops, were identified by Daniel Zohary, who highlighted the importance of the three cereals, and suggested that domestication of flax, peas, chickpeas, bitter vetch and lentils came a little later. Based on analysis of the genes of domesticated plants, he preferred theories of a single, or at most a very small number of domestication events for each taxon that spread in an arc from the Levantine corridor around the Fertile Crescent and later into Europe. Gordon Hillman and Stuart Davies carried out experiments with wild wheat varieties to show that the process of domestication would have occurred over a relatively short period of between 20 and 200 years. Some of these pioneering attempts failed at first and crops were abandoned, sometimes to be taken up again and successfully domesticated thousands of years later: rye, tried and abandoned in Neolithic Anatolia, made its way to Europe as weed seeds and was successfully domesticated in Europe, thousands of years after the earliest agriculture. Wild lentils presented a different problem: most of the wild seeds do not germinate in the first year; the first evidence of lentil domestication, breaking dormancy in their first year, was found in the early Neolithic at Jerf el Ahmar (in modern Syria), and quickly spread south to the Netiv HaGdud site in the Jordan Valley. This process of domestication allowed the founder crops to adapt and eventually become larger, more easily harvested, more dependable in storage and more useful to the human population.\n\nSelectively propagated figs, wild barley and wild oats were cultivated at the early Neolithic site of Gilgal I, where in 2006 archaeologists found caches of seeds of each in quantities too large to be accounted for even by intensive gathering, at strata datable to c. 11,000 years ago. Some of the plants tried and then abandoned during the Neolithic period in the Ancient Near East, at sites like Gilgal, were later successfully domesticated in other parts of the world.\n\nOnce early farmers perfected their agricultural techniques like irrigation, their crops would yield surpluses that needed storage. Most hunter gatherers could not easily store food for long due to their migratory lifestyle, whereas those with a sedentary dwelling could store their surplus grain. Eventually granaries were developed that allowed villages to store their seeds longer. So with more food, the population expanded and communities developed specialized workers and more advanced tools.\n\nThe process was not as linear as was once thought, but a more complicated effort, which was undertaken by different human populations in different regions in many different ways.\n\nEarly agriculture is believed to have originated and become widespread in Southwest Asia around 10,000–9,000 BP, though earlier individual sites have been identified. The Fertile Crescent region of Southwest Asia is the centre of domestication for three cereals (einkorn wheat, emmer wheat and barley), four legumes (lentil, pea, bitter vetch and chickpea), and flax. Domestication was a slow process involving multiple sites for each crop.\n\nFinds of large quantities of seeds and a grinding stone at the paleolithic site of Ohalo II in the vicinity of the Sea of Galilee, dated to around 19,400 BP, has shown some of the earliest evidence for advanced planning of plant food consumption and suggests that humans at Ohalo II processed the grain before consumption. Tell Aswad is the oldest site of agriculture, with domesticated emmer wheat dated to 10,800 BP. Soon after came hulled, two-row barley found domesticated earliest at Jericho in the Jordan valley and Iraq ed-Dubb in Jordan. Other sites in the Levantine corridor that show the first evidence of agriculture include Wadi Faynan 16 and Netiv Hagdud. Jacques Cauvin noted that the settlers of Aswad did not domesticate on site, but \"arrived, perhaps from the neighbouring Anti-Lebanon, already equipped with the seed for planting\". In the Eastern Fertile Crescent, evidence of cultivation of wild plants has been found in Choga Gholan in Iran dated to 12,000 BP, suggesting there were multiple regions in the Fertile Crescent where domestication evolved roughly contemporaneously. The Heavy Neolithic Qaraoun culture has been identified at around fifty sites in Lebanon around the source springs of the River Jordan, but never reliably dated.\n\nNorthern China appears to have been the domestication center for foxtail millet (\"Setaria italica\") and broomcorn millet (\"Panicum miliaceum\") with evidence of domestication of these species approximately 8,000 years ago.<ref name=\"doi10.1093/aob/mcm048\"></ref> These species were subsequently widely cultivated in the Yellow River basin (7,500 years ago). Rice was domesticated in southern China later on. Soybean was domesticated in northern China 4,500 years ago. Orange and peach also originated in China. They were cultivated around 2500 BC.\nOn the African continent, three areas have been identified as independently developing agriculture: the Ethiopian highlands, the Sahel and West Africa. By contrast, Agriculture in the Nile River Valley is thought to have developed from the original Neolithic Revolution in the Fertile Crescent. \nMany grinding stones are found with the early Egyptian Sebilian and Mechian cultures and evidence has been found of a neolithic domesticated crop-based economy dating around 7,000 BP.\nUnlike the Middle East, this evidence appears as a \"false dawn\" to agriculture, as the sites were later abandoned, and permanent farming then was delayed until 6,500 BP with the Tasian and Badarian cultures and the arrival of crops and animals from the Near East.\n\nBananas and plantains, which were first domesticated in Southeast Asia, most likely Papua New Guinea, were re-domesticated in Africa possibly as early as 5,000 years ago. Asian yams and taro were also cultivated in Africa.\n\nThe most famous crop domesticated in the Ethiopian highlands is coffee. In addition, khat, ensete, noog, teff and finger millet were also domesticated in the Ethiopian highlands. Crops domesticated in the Sahel region include sorghum and pearl millet. The kola nut was first domesticated in West Africa. Other crops domesticated in West Africa include African rice, yams and the oil palm.\n\nAgriculture spread to Central and Southern Africa in the Bantu expansion during the 1st millennium BC to 1st millennium AD.\n\nMaize (corn), beans and squash were among the earliest crops domesticated in Mesoamerica, with maize beginning about 4000 BC, squash as early as 6000 BC, and beans by no later than 4000 BC. Potatoes and manioc were domesticated in South America. In what is now the eastern United States, Native Americans domesticated sunflower, sumpweed and goosefoot around 2500 BC. Sedentary village life based on farming did not develop until the second millennium BC, referred to as the formative period.\n\nEvidence of drainage ditches at Kuk Swamp on the borders of the Western and Southern Highlands of Papua New Guinea shows evidence of the cultivation of taro and a variety of other crops, dating back to 11,000 BP. Two potentially significant economic species, taro (\"Colocasia esculenta\") and yam (\"Dioscorea\" sp.), have been identified dating at least to 10,200 calibrated years before present (cal BP). Further evidence of bananas and sugarcane dates to 6,950 to 6,440 BP. This was at the altitudinal limits of these crops, and it has been suggested that cultivation in more favourable ranges in the lowlands may have been even earlier. CSIRO has found evidence that taro was introduced into the Solomon Islands for human use, from 28,000 years ago, making taro cultivation the earliest crop in the world. It seems to have resulted in the spread of the Trans–New Guinea languages from New Guinea east into the Solomon Islands and west into Timor and adjacent areas of Indonesia. This seems to confirm the theories of Carl Sauer who, in \"Agricultural Origins and Dispersals\", suggested as early as 1952 that this region was a centre of early agriculture.\n\nWhen hunter-gathering began to be replaced by sedentary food production it became more profitable to keep animals close at hand. Therefore, it became necessary to bring animals permanently to their settlements, although in many cases there was a distinction between relatively sedentary farmers and nomadic herders. The animals' size, temperament, diet, mating patterns, and life span were factors in the desire and success in domesticating animals. Animals that provided milk, such as cows and goats, offered a source of protein that was renewable and therefore quite valuable. The animal’s ability as a worker (for example ploughing or towing), as well as a food source, also had to be taken into account. Besides being a direct source of food, certain animals could provide leather, wool, hides, and fertilizer. Some of the earliest domesticated animals included dogs (East Asia, about 15,000 years ago), sheep, goats, cows, and pigs.\n\nThe Middle East served as the source for many animals that could be domesticated, such as sheep, goats and pigs. This area was also the first region to domesticate the dromedary. Henri Fleisch discovered and termed the Shepherd Neolithic flint industry from the Bekaa Valley in Lebanon and suggested that it could have been used by the earliest nomadic shepherds. He dated this industry to the Epipaleolithic or Pre-Pottery Neolithic as it is evidently not Paleolithic, Mesolithic or even Pottery Neolithic. The presence of these animals gave the region a large advantage in cultural and economic development. As the climate in the Middle East changed and became drier, many of the farmers were forced to leave, taking their domesticated animals with them. It was this massive emigration from the Middle East that would later help distribute these animals to the rest of Afroeurasia. This emigration was mainly on an east-west axis of similar climates, as crops usually have a narrow optimal climatic range outside of which they cannot grow for reasons of light or rain changes. For instance, wheat does not normally grow in tropical climates, just like tropical crops such as bananas do not grow in colder climates. Some authors, like Jared Diamond, have postulated that this East-West axis is the main reason why plant and animal domestication spread so quickly from the Fertile Crescent to the rest of Eurasia and North Africa, while it did not reach through the North-South axis of Africa to reach the Mediterranean climates of South Africa, where temperate crops were successfully imported by ships in the last 500 years. Similarly, the African Zebu of central Africa and the domesticated bovines of the fertile-crescent — separated by the dry sahara desert — were not introduced into each other's region.\n\nDespite the significant technological advance, the Neolithic revolution did not lead immediately to a rapid growth of population. Its benefits appear to have been offset by various adverse effects,\nmostly diseases and warfare.\n\nThe introduction of agriculture has not necessarily led to unequivocal progress. The nutritional standards of the growing Neolithic populations were inferior to that of hunter-gatherers. Several ethnological and archaeological studies conclude that the transition to cereal-based diets caused a reduction in life expectancy and stature, an increase in infant mortality and infectious diseases, the development of chronic, inflammatory or degenerative diseases (such as obesity, type 2 diabetes and cardiovascular diseases) and multiple nutritional deficiencies, including vitamin deficiencies, iron deficiency anemia and mineral disorders affecting bones (such as osteoporosis and rickets) and teeth. Average height went down from 5'10\" (178 cm) for men and 5'6\" (168 cm) for women to 5'5\" (165 cm) and 5'1\" (155 cm), respectively, and it took until the twentieth century for average human height to come back to the pre-Neolithic Revolution levels.\n\nThe traditional view is that agricultural food production supported a denser population, which in turn supported larger sedentary communities, the accumulation of goods and tools, and specialization in diverse forms of new labor. The development of larger societies led to the development of different means of decision making and to governmental organization. Food surpluses made possible the development of a social elite who were not otherwise engaged in agriculture, industry or commerce, but dominated their communities by other means and monopolized decision-making. Jared Diamond (in The World Until Yesterday) identifies the availability of milk and cereal grains as permitting mothers to raise both an older (e.g. 3 or 4 year old) and a younger child concurrently. The result is that a population can increase more rapidly. Diamond, in agreement with feminist scholars such as V. Spike Peterson, points out that agriculture brought about deep social divisions and encouraged gender inequality. \n\nAndrew Sherratt has argued that following upon the Neolithic Revolution was a second phase of discovery that he refers to as the secondary products revolution. Animals, it appears, were first domesticated purely as a source of meat. The Secondary Products Revolution occurred when it was recognised that animals also provided a number of other useful products. These included:\n\nSherratt argued that this phase in agricultural development enabled humans to make use of the energy possibilities of their animals in new ways, and permitted permanent intensive subsistence farming and crop production, and the opening up of heavier soils for farming. It also made possible nomadic pastoralism in semi arid areas, along the margins of deserts, and eventually led to the domestication of both the dromedary and Bactrian camel. Overgrazing of these areas, particularly by herds of goats, greatly extended the areal extent of deserts.\n\nLiving in one spot would have more easily permitted the accrual of personal possessions and an attachment to certain areas of land. From such a position, it is argued, prehistoric people were able to stockpile food to survive lean times and trade unwanted surpluses with others. Once trade and a secure food supply were established, populations could grow, and society would have diversified into food producers and artisans, who could afford to develop their trade by virtue of the free time they enjoyed because of a surplus of food. The artisans, in turn, were able to develop technology such as metal weapons. Such relative complexity would have required some form of social organisation to work efficiently, so it is likely that populations that had such organisation, perhaps such as that provided by religion, were better prepared and more successful. In addition, the denser populations could form and support legions of professional soldiers. Also, during this time property ownership became increasingly important to all people. Ultimately, Childe argued that this growing social complexity, all rooted in the original decision to settle, led to a second Urban Revolution in which the first cities were built.\n\nThroughout the development of sedentary societies, disease spread more rapidly than it had during the time in which hunter-gatherer societies existed. Inadequate sanitary practices and the domestication of animals may explain the rise in deaths and sickness following the Neolithic Revolution, as diseases jumped from the animal to the human population. Some examples of infectious diseases spread from animals to humans are influenza, smallpox, and measles. In concordance with a process of natural selection, the humans who first domesticated the big mammals quickly built up immunities to the diseases as within each generation the individuals with better immunities had better chances of survival. In their approximately 10,000 years of shared proximity with animals, such as cows, Eurasians and Africans became more resistant to those diseases compared with the indigenous populations encountered outside Eurasia and Africa. For instance, the population of most Caribbean and several Pacific Islands have been completely wiped out by diseases. 90% or more of many populations of the Americas were wiped out by European and African diseases before recorded contact with European explorers or colonists. Some cultures like the Inca Empire did have a large domestic mammal, the llama, but llama milk was not drunk, nor did llamas live in a closed space with humans, so the risk of contagion was limited. According to bioarchaeological research, the effects of agriculture on physical and dental health in Southeast Asian rice farming societies from 4000 to 1500 B.P. was not detrimental to the same extent as in other world regions.\n\nIn his book \"Guns, Germs, and Steel\", Jared Diamond argues that Europeans and East Asians benefited from an advantageous geographical location that afforded them a head start in the Neolithic Revolution. Both shared the temperate climate ideal for the first agricultural settings, both were near a number of easily domesticable plant and animal species, and both were safer from attacks of other people than civilizations in the middle part of the Eurasian continent. Being among the first to adopt agriculture and sedentary lifestyles, and neighboring other early agricultural societies with whom they could compete and trade, both Europeans and East Asians were also among the first to benefit from technologies such as firearms and steel swords.\nThe dispersal of Neolithic culture from the Middle East has recently been associated with the distribution of human genetic markers. In Europe, the spread of the Neolithic culture has been associated with distribution of the E1b1b lineages and Haplogroup J that are thought to have arrived in Europe from North Africa and the Near East respectively. In Africa, the spread of farming, and notably the Bantu expansion, is associated with the dispersal of Y-chromosome haplogroup E1b1a from West Africa.\n\n\n\n"}
{"id": "2074318", "url": "https://en.wikipedia.org/wiki?curid=2074318", "title": "RailTel Corporation of India", "text": "RailTel Corporation of India\n\nRailTel Corporation of India Ltd. is a \"Miniratna\" (public sector) enterprise of Government of India focusing on providing broadband and VPN services. RailTel was formed in September 2000 with the objective of creating nationwide broadband, telecom and multimedia network, to modernise train control operation and safety system of Indian Railways. RailTel's network passes through around 5,000 stations across the country, covering all major commercial centres.\n\nThe Indian Railways (IR) was initially solely dependent on the Department of Telecom (now BSNL) for their control and administrative communication circuits. To increase circuit efficiency, the Railways began building up its own communication systems from early 1970s based on overhead telephone lines, quad cables and microwave signalling. In 1983, the Railway Reforms Committee decided to introduce optical fibre cable (OFC) based communications in IR to provide safety, reliability, availability and serviceability through use of a dedicated network. The decision was also taken to create a network independent of the DoT and replace the existing microwave telecom systems (60% of which had reached end of life) with OFC.\n\nIndian Railways commissioned the first OFC on the Churchgate–Virar line in Mumbai in 1988 for train operation and control purpose, which consisted of 60 km of network across 28 stations. The network was expanded in Central India with the commissioning of 900 km of OFC network in 1991–92 across Durg–Nagpur, Nagpur–Itarsi and Itarsi–Bhusaval sections of the Howrah–Nagpur–Mumbai line, and in Eastern India with the commissioning of 60 km of OFC network in Tatanagar–Chakradhrapur section of the same line.\n\nThe second National Telecom Policy in 1999 opened the National Long-Distance segment under favourable licensing conditions with revenue sharing to assist mobile network operators to spread their networks across India. In 2000, the Government announced the formation of a telecom corporation to build a nationwide broadband multimedia telecommunication network. RailTel was established in September 2000 as a Public Sector Undertaking (PSU), wholly owned by the Indian Railways.\n\nRailTel, in collaboration with Google, provides free WiFi access at selected railways stations across India. Google chose railway stations as the location to provide free WiFi because stations have access to reliable power supply and fibre provided by RailTel, and because the passengers at a station come from all demographics of India.\n\nThe free WiFi service was launched at Mumbai Central railway station in January 2016. In April 2016, the service was expanded to 9 more railway stations. In June 2016, Google announced that free Wi-Fi was available across 19 stations in India and was being used by over 1.5 million people. Google and RailTel plan to provide free WiFi at 100 railway stations across the country by the end of 2016.\n\nIn September 2016, Google announced a public WiFi initiative called Google Station. Google plans to expand free WiFi coverage under the initiative to locations such as cafes and malls across India, and later expand worldwide.\n\nIn June 2018, Google announced that it's Free Wi-Fi project is now powering 400 Indian railway stations. As a result, there are now more than 8 million people accessing the internet each month via the project.\n\nBased on its nationwide fibre network, RailTel offers various bandwidth intensive application to its customers. One such initiative is RailWire, a joint venture with MSOs to provide Voice, Video and Multimedia access on a single wire at a customer's home or office.\n\nRailTel has received the 12th National Awards for Excellence in Cost Management 2014.\n"}
{"id": "565362", "url": "https://en.wikipedia.org/wiki?curid=565362", "title": "Recife Antigo", "text": "Recife Antigo\n\nRecife Antigo (Old Recife) is the historical section of central Recife, Brazil. It is located on the Island of Recife, near the Recife harbor. This historic area has been recently recovered and now holds several clubs, bars and a high-tech center called \"Porto Digital\".\n\nRecife Antigo consists of the initial Portuguese settlement in the 16th century around the port. Sugar cane production from Pernambuco was delivered to Portugal through Recife's port. While Recife had port functions, Olinda was the capital. In 1630, the Dutch invaded Pernambuco, set Olinda partially on fire and Recife became the seat of the Dutch government. Count John Maurice of Nassau-Siegen became Governor-General of the Dutch colony and built a new town on a neighboring island. This city was named Mauritsstadt and the Palacio do Campo das Princesas, seat of the State of Pernambuco government, is built on its ruins. \n\nThe Dutch were forced out in 1654 of a Recife with good infrastructure, for they had built canals and improved the port and the defenses of it. A flourishing Jewish community lived in Recife under them and they had to leave it because of the Portuguese Inquisition. Thus, a group of 24 Portuguese Jews who had previously migrated from Portugal to the Netherlands because of antisemitism, headed farther North with the Dutch, where New Amsterdam --present-day Manhattan-- was founded. The first Synagogue built in the Americas, the Kahal Zur Israel Synagogue, is located in Recife Antigo, on Rua do Bom Jesus, formerly Rua dos Judeus, or Street of the Jews. The Portuguese synagogue was founded in lower Manhattan and it is located on Central Park West in Manhattan nowadays under the name Portuguese & Spanish synagogue. \n"}
{"id": "10721277", "url": "https://en.wikipedia.org/wiki?curid=10721277", "title": "Redshift (theory)", "text": "Redshift (theory)\n\nRedshift is a techno-economic theory suggesting hypersegmentation of information technology markets based on whether individual computing needs are over or under-served by Moore's law, which predicts the doubling of computing transistors (and therefore roughly computing power) every two years. The theory,\nproposed and named by New Enterprise Associates partner and former Sun Microsystems CTO Greg Papadopoulos, categorized a series of high growth markets (redshifting) while predicting slower GDP-driven growth in traditional computing markets (blueshifting). Papadopoulos predicted the result will be a fundamental redesign of components comprising computing systems.\n\nAccording to the Redshift theory, applications \"redshift\" when they grow dramatically faster than Moore's Law allows, growing quickly in their absolute number of systems. In these markets, customers are running out of datacenter real-estate, power and cooling infrastructure. According to Dell Senior Vice President Brad Anderson, “Businesses requiring hyperscale computing environments – where infrastructure deployments are measured by up to millions of servers, storage and networking equipment – are changing the way they approach IT.”\n\nWhile various Redshift proponents offer minor alterations on the original presentation, “Redshifting” generally includes:\n\nThese are companies that drive heavy Internet traffic. This includes popular web-portals like Google, Yahoo, AOL and MSN. It also includes telecoms, multimedia, television over IP, online games like World of Warcraft and others. This segment has been enabled by widespread availability of high-bandwidth Internet connections to consumers through a DSL or cable modem. A simple way to understand this market is that for every byte of content served to a PC, mobile phone or other device over a network, there must exist computing systems to send it over the network.\n\nThese are companies that do complex simulations that involve (for example) weather, stock markets or drug-design simulations. This is a generally elastic market because businesses frequently spend every \"available\" dollar budgeted for IT. A common anecdote claims that cutting the cost of computing by half causes customers in this segment to buy at least twice as much, because each marginal IT dollar spent contributes to business advantage.\n\nThese are companies that aggregate traditional computing applications and offer them as services, typically in the form of Software as a Service (SaaS). For example, companies that deploy CRM are over-served by Moore's Law, but companies that aggregate CRM functions and offer them as a service, such as Salesforce.com, grow faster than Moore's Law.\n\nA prime example of redshift was a crisis at eBay. In 1999 eBay suffered a database crisis when a single Oracle Database running on the fastest Sun machine available (these tracking Moore's law in this period) was not enough to cope with eBay's growth. The solution was to massively parallelise their system architecture.\n\nRedshift theory suggests that traditional computing markets, such as those serving enterprise resource planning or customer relationship management applications, have reached relative saturation in industrialized nations. Thereafter, proponents argued further market growth will closely follow gross domestic product growth, which typically remains under 10% for most countries annually. Given that Moore's Law continues to predict accurately the rate of computing transistor growth, which roughly translates into computing power doubling every two years, the Redshift theory suggests that traditional computing markets will ultimately contract as a percentage of computing expenditures over time.\n\nFunctionally, this means “Blueshifting” customers can satisfy computing requirement growth by swapping in faster processors without increasing the absolute number of computing systems.\n\nPapadopoulos argued that while traditional computing markets remain the dominant source of revenue through the late 2000s, a shift to hypergrowth markets will inevitably occur. When that shift occurs, he argued computing (but not computers) will become a utility, and differentiation in\nthe IT market will be based upon a company's ability to deliver computing at massive scale, efficiently and with predictable service levels, much like electricity at that time.\n\nIf computing is to be delivered as a utility, Nicholas Carr suggested Papadopoulos' vision compares with Microsoft researcher Jim Hamilton, who both agree that computing is most efficiently generated in shipping containers. Industry analysts are also beginning to quantify Redshifting and Blueshifting markets. According to International Data Corporation vice president Matthew Eastwood, \"IDC believes that the IT market is in a period of hyper segmentation... This a class of customers that is Moore's law driven and as price performance gains continue, IDC believes that these organizations will accelerate their consumption of IT infrastructure.”\n\nKey portions of Papadopoulos' theory were first presented by Sun Microsystems CEO Jonathan Schwartz in late 2006. Papadopoulos later gave a full presentation on Redshift to Sun's annual Analyst Summit in February 2007. The term Redshift refers to what happens when electromagnetic radiation, usually visible light, moves away from an observer. Papadopoulos chose this term to reflect growth markets because redshift helped cosmologists explain the expansion of the universe.\n\nPapadopoulos originally depicted traditional IT markets as green to represent their revenue base, but later changed them to “blueshift,” which occurs when a light source moves toward an observer, similar to what would happen during a contraction of the universe.\n\n"}
{"id": "1212721", "url": "https://en.wikipedia.org/wiki?curid=1212721", "title": "Release notes", "text": "Release notes\n\nRelease notes are documents that are distributed with software products, sometimes when the product is still in the development or test state (e.g., a beta release). For products that have already been in use by clients, the release note is delivered to the customer when an update is released.\n\nRelease notes are documents that are shared with end users, customers and clients of an organization. The definition of the terms 'End Users', 'Clients' and 'Customers' are very relative in nature and might have various interpretations based on the specific context. For instance, Quality Assurance group within a software development organization can be interpreted as an internal customer. They detail the corrections, changes or enhancements made to the service or product the company provides. This document is usually circulated only after the product or service is thoroughly tested and approved against the specification provided by the development team. However this document might also be provided as an artifact accompanying the deliverables for System Testing and System Integration Testing and other managed environments especially with reference to an information technology organization.\n\nRelease notes can also contain test results and information about the test procedure. This kind of information gives readers of the release note more confidence in the fix/change done; this information also enables implementer of the change to conduct rudimentary acceptance tests.\n\nThere is no standard format for release notes that is followed throughout different organizations. Organizations normally adopt their own formatting styles based on the requirement and type of the information to be circulated. The content of release notes also vary according to the release type. For products that are at testing stage and that are newly released, the content is usually more descriptive compared to release notes for bug fixes and feature enhancements, which are usually brief. \nRelease notes may include the following sections:\n\nA release note is usually a terse summary of recent changes, enhancements and bug fixes in a particular software release. It is not a substitute for user guides. Release notes are frequently written in the present tense and provide information that is clear, correct, and complete.\n\n\n"}
{"id": "9992916", "url": "https://en.wikipedia.org/wiki?curid=9992916", "title": "RingGo", "text": "RingGo\n\nRingGo is a pay by phone parking service, based in the UK. It processes over 2 million phone parking transactions a month and over 6 million individual UK motorists have used the service. Offered in public car parks and on-street locations across the United Kingdom it is now deployed in support of over 100 local authorities. The first major implementation (from June 2006) was for the First Great Western Railway in 60 stations. The project by the FirstGroup, was an early step in the trend away from the established system of paying for parking using electro-mechanical machines which dispense paper tickets – the pay and display model.\n\nRailways were a major area of early stage growth. Franchises that adopted the service included Network Rail, Chiltern Railways, South West Trains, First Capital Connect, National Express East Anglia, c2c, First TransPennine Express, Arriva Trains Wales and East Coast Railway. The service has also been offered in NCP operated and managed car parks since March 2010.\n\nParking was an early area in which electronic money as opposed to cash-money made considerable advances although it has been overtaken since in sectors such as mass transportation.\n\nOn 1 October 2009, Richmond Council introduced variable parking charges which were linked to the carbon dioxide emissions of vehicles parking there (carbon metered parking). Payment of the service was either via RingGo or by a Richmond smart card, which could be used in the machine.\n\nMilton Keynes Council, Richmond Council, Lewisham Council and, from November 2014, Westminster council provide online parking permits that are an extension of the RingGo phone parking service\n\nThe RingGo brand is owned by Cobalt Telephone Technologies Ltd, a UK private company headquartered in Basingstoke, Hampshire. Cobalt was acquired by the Parkmobile Group in August 2012. The Parkmobile group is a global specialist in phone parking and has similar operations in support of cities, railways and municipalities in North America, The Netherlands, Belgium and Germany.\n\n"}
{"id": "375140", "url": "https://en.wikipedia.org/wiki?curid=375140", "title": "Room-temperature superconductor", "text": "Room-temperature superconductor\n\nA room-temperature superconductor is a hypothetical material that would be capable of exhibiting superconductivity at operating temperatures above 0 °C (273.15 K). While this is not strictly \"room temperature\", which would be approximately 20–25 °C, it is the temperature at which ice forms and can be reached and easily maintained in an everyday environment. The highest temperature known superconducting material is highly pressurized hydrogen sulfide, the transition temperature of which is , the highest accepted superconducting critical temperature as of 2015. By substituting a small part of sulfur with phosphorus and using even higher pressures, it has been predicted that it may be possible to raise the critical temperature to above 0 °C and achieve room-temperature superconductivity. Previously the record was held by the cuprates, which have demonstrated superconductivity at atmospheric pressure at temperatures as high as , and under high pressure.\n\nAlthough some researchers doubt whether room-temperature superconductivity is actually achievable, superconductivity has repeatedly been discovered at temperatures that were previously unexpected or held to be impossible.\n\nClaims of \"near-room temperature\" transient effects date from the early 1950s and some suggest that in fact the breakthrough might have been made more than once but could not be made stable enough and/or reproducible as the relationship between isotope number and T was not known at the time.\n\nFinding a room temperature superconductor \"would have enormous technological importance and, for example, help to solve the world’s energy problems, provide for faster computers, allow for novel memory-storage devices, and enable ultra-sensitive sensors, among many other possibilities.\"\n\nSince the discovery of high-temperature superconductors, several materials have been reported to be room-temperature superconductors, although none of these reports has been confirmed.\n\nIn 2000, while extracting electrons from diamond during ion implantation work, Johan Prins claimed to have observed a phenomenon that he explained as room-temperature superconductivity within a phase formed on the surface of oxygen-doped type IIa diamonds in a vacuum.\n\nIn 2003, a group of researchers published results on high-temperature superconductivity in palladium hydride (PdH: x>1) and an explanation in 2004.\nIn 2007 the same group published results suggesting a superconducting transition temperature of 260 K. The superconducting critical temperature increases as the density of hydrogen inside the palladium lattice increases. This work has not been corroborated by other groups.\n\nIn 2012, an \"Advanced Materials\" article claimed superconducting behavior of graphite powder after treatment with pure water at temperatures as high as 300 K and above.[Unreliable source] So far, the authors have not been able to demonstrate the occurrence of a clear Meissner phase and the vanishing of the material's resistance.\n\nIn 2014, an article published in \"Nature\" suggested that some materials, notably YBCO (yttrium barium copper oxide), could be made to superconduct at room temperature using infrared laser pulses.\n\nIn 2015, an article published in \"Nature\" by researchers of the Max Planck Institute suggested that under certain conditions such as extreme pressure HS transitioned to a superconductive form HS at around 1.5 million times atmospheric pressure in a diamond anvil cell. The critical temperature is 203 K which would be the highest T ever recorded and their research suggests that other hydrogen compounds could superconduct at up to 260 K which would match up with the original research of Ashcroft.\n\nIn 2018, Dev Kumar Thapa and Anshu Pandey from the Solid State and Structural Chemistry Unit of the Indian Institute of Science in Bangalore claimed the observation of superconductivity at ambient pressure and room temperature in films and pellets of a nanostructured material that is composed of silver particles embedded in a gold matrix.Due to similar noise patterns of supposedly independent plots and the not peer-reviewed nature of this publication, the results were called into question.\n\nIn 2018, researchers noted a possible superconducting phase at 260K in lanthanum decahydride at elevated (200GPa) pressure.\n\nOther research also suggests a link between the palladium hydride containing small impurities of sulfur as a plausible explanation for the anomalous resistance drops noticed by other researchers, and hydrogen absorption by cuprates has been suggested in light of the recent results in HS as a plausible explanation for transient resistance drops or \"USO\" noticed\nin the 1990s during research after the discovery of YBCO.\n\nTheoretical work by Neil Ashcroft predicted that solid metallic hydrogen at extremely high pressure (~500 GPa) should become superconducting at approximately room-temperature because of its extremely high speed of sound and expected strong coupling between the conduction electrons and the lattice vibrations (phonons). This prediction is yet to be experimentally verified, as yet the pressure to achieve metallic hydrogen is not known but may be of the order of 500 GPa.\n\nA team at Harvard has claimed to make metallic hydrogen and reports a pressure of 495 GPa. Though the exact critical temperature has not yet been determined, weak signs of a Meissner effect at 250K may have appeared in magnetometer tests.\n\nIn 1964, William A. Little proposed the possibility of high temperature superconductivity in organic polymers. This proposal is based on the exciton-mediated electron pairing, as opposed to phonon-mediated pairing in BCS theory.\n"}
{"id": "34706451", "url": "https://en.wikipedia.org/wiki?curid=34706451", "title": "Rumford furnace", "text": "Rumford furnace\n\nA Rumford furnace is a kiln for the industrial scale production in the 19th century of calcium oxide, popularly known as quicklime or burnt lime. It was named after its inventor, Benjamin Thompson, also known as Count Rumford, and is sometimes called a Rüdersdorf furnace after the location where it was first built and from where the design rapidly spread throughout Europe.\n\nRumford’s innovation was to separate the combustion chambers for the limestone and the fuel. In previous designs, the fuel (traditionally wood or charcoal, later coal) was mixed with the limestone before burning, which meant that the quicklime end product was contaminated with ash and had to be laboriously cleaned. By separating the two it was possible to produce large quantities of high-quality quicklime for the booming 19th-century European construction industry. \n\nIn a Rumford furnace, fuel compartments surround the actual combustion chamber, each one being connected to it by a single transverse channel through which the hot air produced by the burning fuel flows into the stack and moves upward through the pile of limestone. The fuel ash waste falls into a separate chamber, from which it can be removed.\nThe limestone in the central shaft collapses as it burns to quicklime, making space for new limestone to be added to the top of the stack. The quicklime cools down as it slowly descends through the space below the channel connecting the stack to the firing compartment and is finally removed through an outlet at the bottom of the shaft. \nThe upper part of the limestone shaft extends above the fuel compartments and its walls contain separate cavities filled with coal dust. This insulation supports the prewarming of the limestone before it moves into the vicinity of the hot air source.\n\nThe first Rumford kiln was constructed in 1802 in Rüdersdorf near Berlin, and one of them can still be explored at the Museumspark Rüdersdorf. The innovation rapidly spread throughout Europe because it made it possible to produce quicklime far more efficiently than predecessor technologies. These required either batch production, waiting for each load to be assembled, burn and cool before removal, or carefully mixing the fuel and limestone in the appropriate proportions, burning them together, and then cleaning the contaminated limestone afterward.\nIn a Rumford kiln continuous operation was possible for the first time, since the addition of fuel and limestone as well as the removal of ash and quicklime could be separately optimized according to need, and all these operations could take place simultaneously.\n\n"}
{"id": "39383574", "url": "https://en.wikipedia.org/wiki?curid=39383574", "title": "Smart Mobility Architecture", "text": "Smart Mobility Architecture\n\nComputer-on-Modules integrate the core function of a bootable computer, like SoC, as well as additional circuitry, including DRAM, boot-flash, voltage distribution, Ethernet and display transmitter. The modules are deployed together with an application-specific carrierboard, whose size and form can be defined to meet customer-specific requirements. The carrierboard executes the required interfaces and can integrate, if required, any further functionalities, such as audio codecs, touch controller, wireless communication interfaces, etc.\n\nThe SMARC specification outlines both the dimensions of the module and the positioning of the anchor points as well as the connector to the carrierboard and, most importantly, the executed interfaces with the pin-out. The pin-out is optimized for ARM and low-power SoC interfaces and is distinguished from classical PC interfaces by its target-oriented focus on low-power and mobile applications.\n\nSMARC is based on the ULP-COM form factor which was introduced by the companies Kontron and Adlink in 2011. During the specification process by the SGET the standard was renamed to SMARC.\n\nSMARC defines two module sizes: \n\nSMARC Computer-on-Modules have 314 card edge contacts on the printed circuit board (PCB) of the module which is plugged via a low-profile connector on the carrierboard. In most cases, the connector has a construction height of 4.3 mm. It is also used for Mobile PCI Express Module 3.0 graphic cards, which naturally have completely different pin assignments.\n\nSignal transmission is carried out via a total of 314 pins. 33 of these are reserved signal lines for power supply and grounding, so that with SMARC a total of 281 signal lines are effectively available. ARM- and SoC-typical energy-saving interfaces, like, for instance, parallel LCD for display connection, mobile industry processor interfaces for cameras, Serial Peripheral Interface (SPI) for general peripheral connection, I²S for audio and I2C are included. Besides these, classical computer interfaces such as USB, SATA and PCI Express are also defined.\n\nIn the current version of the SMARC specification not all of the 314 signal lines are assigned to fixed I/Os. The Alternate Function Block (AFB) has free pins available for different requirements. This is to ensure that the SMARC specification can flexibly accommodate up and coming technical developments which today are not foreseeable while remaining fully compatibility to previous designs. On the one hand, extended versions of the SMARC specification can assign new standard functions to these 20 AFB signal lines. On the other hand, the SMARC specification 1.0 currently lists the MOST (Media Oriented System Transport) bus, Dual Gigabit Ethernet, Super Speed USB, or industrial network protocols which from a current point of view could be imagined as or might be assigned as interfaces of the AFB.\n\nThe SMARC hardware specification V1.0 is supervised by the SGET. The specification is freely available as a download on the SGET website.\n\n\n"}
{"id": "26031935", "url": "https://en.wikipedia.org/wiki?curid=26031935", "title": "Technology scouting", "text": "Technology scouting\n\nTechnology scouting is an element of technology management in which \nIt is a starting point of a long term and interactive matching process between external technologies and internal requirements of an existing organization for strategic purposes. This matching may also be aided by technology roadmapping. Technology scouting is also known to be part of competitive intelligence, which firms apply as a tool of competitive strategy. It can also be regarded as a method of technology forecasting or in the broader context also an element of corporate foresight. Technology scouting may also be applied as an element of an open innovation approach. Technology scouting is seen as an essential element of a modern technology management system.\n\nThe technology scout is either an employee of the company or an external consultant who engages in boundary spanning processes to tap into novel knowledge and span internal boundaries. He or she may be assigned part-time or full-time to the scouting task. The desired characteristics of a technology scout are similar to the characteristics associated with the technological gatekeeper. These characteristics include being a lateral thinker, knowledgeable in science and technology, respected inside the company, cross-disciplinary orientated, and imaginative personality. Technology scouts would also often play a vital role in a formalised technology foresight process.\n\nDocumented case studies include:\n\n\n"}
{"id": "3200382", "url": "https://en.wikipedia.org/wiki?curid=3200382", "title": "Upper ontology", "text": "Upper ontology\n\nIn information science, an upper ontology (also known as a top-level ontology or foundation ontology) is an ontology (in the sense used in information science) which consists of very general terms (such as \"object\", \"property\", \"relation\") that are common across all domains. An important function of an upper ontology is to support broad semantic interoperability among a large number of domain-specific ontologies by providing a common starting point for the formulation of definitions. Terms in the domain ontology are ranked \"under\" the terms in the upper ontology, and the former stand to the latter in subclass relations.\n\nA number of upper ontologies have been proposed, each with its own proponents. Each upper ontology can be considered as a computational implementation of natural philosophy, which itself is a more empirical method for investigating the topics within the philosophical discipline of physical ontology.\n\nLibrary classification systems predate upper ontology systems. Though library classifications organize and categorize knowledge using general concepts that are the same across all knowledge domains, neither system is a replacement for the other.\n\nAny standard foundational ontology is likely to be contested among different groups, each with their own idea of \"what exists\". One factor exacerbating the failure to arrive at a common approach has been the lack of open-source applications that would permit the testing of different ontologies in the same computational environment. The differences have thus been debated largely on theoretical grounds, or are merely the result of personal preferences. Foundational ontologies can however be compared on the basis of adoption for the purposes of supporting interoperability across domain ontologies.\n\nNo particular upper ontology has yet gained widespread acceptance as a de facto standard. Different organizations have attempted to define standards for specific domains. The 'Process Specification Language' (PSL) created by the National Institute for Standards and Technology (NIST) is one example.\n\nAnother important factor leading to the absence of wide adoption of any existing upper ontology is the complexity. Some upper ontologies -- Cyc is often cited as an example in this regard—are very large, ranging up to thousands of elements (classes, relations), with complex interactions among them and with a complexity similar to that of a human natural language, and the learning process can be even longer than for a natural language because of the unfamiliar format and logical rules. The motivation to overcome this learning barrier is largely absent because of the paucity of publicly accessible examples of use. As a result, those building domain ontologies for local applications tend to create the simplest possible domain-specific ontology, not related to any upper ontology. Such domain ontologies may function adequately for the local purpose, but they are very time-consuming to relate accurately to other domain ontologies.\n\nTo solve this problem some genuinely top level ontologies have been developed, which are deliberately designed to have minimal overlap with any domain ontologies. Examples are Basic Formal Ontology and the DOLCE (see below).\n\nHistorically, many attempts in many societies have been made to impose or define a single set of concepts as more primal, basic, foundational, authoritative, true or rational than all others. A common objection to such attempts points out that humans lack the sort of transcendent perspective - or \"God's eye view\" - that would be required to achieve this goal. Humans are bound by language or culture, and so lack the sort of objective perspective from which to observe the whole terrain of concepts and derive any one standard.\n\nAnother objection is the problem of formulating definitions. Top level ontologies are designed to maximize support for interoperability across a large number of terms. Such ontologies must therefore consist of terms expressing very general concepts, but such concepts are so basic to our understanding that there is no way in which they can be defined, since the very process of definition implies that a less basic (and less well understood) concept is defined in terms of concepts that are more basic and so (ideally) more well understood. Very general concepts can often only be elucidated, for example by means of examples, or paraphrase.\n\n\nThose who doubt the feasibility of general purpose ontologies are more inclined to ask “what specific purpose do we have in mind for this conceptual map of entities and what practical difference will this ontology make?” This pragmatic philosophical position surrenders all hope of devising the encoded ontology version of “everything that is the case,” (Wittgenstein, Tractatus Logico-Philosophicus).\n\nFinally there are objections similar to those against artificial intelligence. Technically, the complex concept acquisition and the social / linguistic interactions of human beings suggests any axiomatic foundation of \"most basic\" concepts must be cognitive, biological or otherwise difficult to characterize since we don't have axioms for such systems. Ethically, any general-purpose ontology could quickly become an actual tyranny by recruiting adherents into a political program designed to propagate it and its funding means, and possibly defend it by violence. Historically, inconsistent and irrational belief systems have proven capable of commanding obedience to the detriment or harm of persons both inside and outside a society that accepts them. How much more harmful would a consistent rational one be, were it to contain even one or two basic assumptions incompatible with human life?\n\nMany of those who doubt the possibility of developing wide agreement on a common upper ontology fall into one of two traps:\n\nIn fact, different representations of assertions about the real world (though not philosophical models), if they accurately reflect the world, must be logically consistent, even if they focus on different aspects of the same physical object or phenomenon. If any two assertions about the real world are logically inconsistent, one or both must be wrong, and that is a topic for experimental investigation, not for ontological representation. In practice, representations of the real world are created as and known to be approximations to the basic reality, and their use is circumscribed by the limits of error of measurements in any given practical application. Ontologies are entirely capable of representing approximations, and are also capable of representing situations in which different approximations have different utility. Objections based on the different ways people perceive things attack a simplistic, impoverished view of ontology. The objection that there are logically incompatible models of the world are true, but in an upper ontology those different models can be represented as different theories, and the adherents of those theories can use them in preference to other theories, while preserving the logical consistency of the \"necessary\" assumptions of the upper ontology. The \"necessary\" assumptions provide the logical vocabulary with which to specify the meanings of all of the incompatible models. It has never been demonstrated that incompatible models cannot be properly specified with a common, more basic set of concepts, while there are examples of incompatible theories that can be logically specified with only a few basic concepts.\n\nMany of the objections to upper ontology refer to the problems of life-critical decisions or non-axiomatized problem areas such as law or medicine or politics that are difficult even for humans to understand. Some of these objections do not apply to physical objects or standard abstractions that are defined into existence by human beings and closely controlled by them for mutual good, such as standards for electrical power system connections or the signals used in traffic lights. No single general metaphysics is required to agree that some such standards are desirable. For instance, while time and space can be represented many ways, some of these are already used in interoperable artifacts like maps or schedules.\n\nObjections to the feasibility of a common upper ontology also do not take into account the possibility of forging agreement on an ontology that contains all of the \"primitive\" ontology elements that can be combined to create any number of more specialized concept representations. Adopting this tactic permits effort to be focused on agreement only on a limited number of ontology elements. By agreeing on the meanings of that inventory of basic concepts, it becomes possible to create and then accurately and automatically interpret an infinite number of concept representations as combinations of the basic ontology elements. Any domain ontology or database that uses the elements of such an upper ontology to specify the meanings of its terms will be automatically and accurately interoperable with other ontologies that use the upper ontology, even though they may each separately define a large number of domain elements not defined in other ontologies. In such a case, proper interpretation will require that the logical descriptions of domain-specific elements be transmitted along with any data that is communicated; the data will then be automatically interpretable because the domain element descriptions, based on the upper ontology, will be properly interpretable by any system that can properly use the upper ontology. In effect elements in different domain ontologies can be *translated* into each other using the common upper ontology. An upper ontology based on such a set of primitive elements can include alternative views, provided that they are logically compatible. Logically incompatible models can be represented as alternative theories, or represented in a specialized extension to the upper ontology. The proper use of alternative theories is a piece of knowledge that can itself be represented in an ontology. Users that develop new domain ontologies and find that there are semantic primitives needed for their domain but missing from the existing common upper ontology can add those new primitives by the accepted procedure, expanding the common upper ontology as necessary.\n\nMost proponents of an upper ontology argue that several good ones may be created with perhaps different emphasis. Very few are actually arguing to discover just one within natural language or even an academic field. Most are simply standardizing some existing communication. Another view advanced is that there is almost total overlap of the different ways that upper ontologies have been formalized, in the sense that different ontologies focus on a different aspect of the same entities, but the different views are complementary and not contradictory to each other; as a result, an internally consistent ontology that contains all the views, with means of translating the different views into the other, is feasible. Such an ontology has not thus far been constructed, however, because it would require a large project to develop so as to include all of the alternative views in the separately developed upper ontologies, along with their translations. The main barrier to construction of such an ontology is not the technical issues, but the reluctance of funding agencies to provide the funds for a large enough consortium of developers and users.\n\nSeveral common arguments against upper ontology can be examined more clearly by separating issues of concept definition (ontology), language (lexicons), and facts (knowledge). For instance, people have different terms and phrases for the same concept. However, that does not necessarily mean that those people are referring to different concepts. They may simply be using different language or idiom. Formal ontologies typically use linguistic labels to refer to concepts, but the terms that label ontology elements mean no more and no less than what their axioms say they mean. Labels are similar to variable names in software, evocative rather than definitive. The proponents of a common upper ontology point out that the meanings of the elements (classes, relations, rules) in an ontology depend only on their logical form, and not on the labels, which are usually chosen merely to make the ontologies more easily usable by their human developers. In fact, the labels for elements in an ontology need not be words - they could be, for example, images of instances of a particular type, or videos of an action that is represented by a particular type. It cannot be emphasized too strongly that words are *not* what are represented in an ontology, but entities in the real world, or abstract entities (concepts) in the minds of people. Words are not equivalent to ontology elements, but words *label* ontology elements. There can be many words that label a single concept, even in a single language (synonymy), and there can be many concepts labeled by a single word (ambiguity). Creating the mappings between human language and the elements of an ontology is the province of Natural Language Understanding. But the ontology itself stands independently as a logical and computational structure. For this reason, finding agreement on the structure of an ontology is actually easier than developing a controlled vocabulary, because all different interpretations of a word can be included, each *mapped* to the same word in the different terminologies.\n\nA second argument is that people believe different things, and therefore can't have the same ontology. However, people can assign different truth values to a particular assertion while accepting the validity of certain underlying claims, facts, or way of expressing an argument with which they disagree. (Using, for instance, the issue/position/argument form.) This objection to upper ontologies ignores the fact that a single ontology can represent different belief systems, representing them as different belief systems, without taking a position on the validity of either.\n\nEven arguments about the existence of a thing require a certain sharing of a concept, even though its existence in the real world may be disputed. Separating belief from naming and definition also helps to clarify this issue, and show how concepts can be held in common, even in the face of differing belief. For instance, wiki as a medium may permit such confusion but disciplined users can apply dispute resolution methods to sort out their conflicts. It is also argued that most people share a common set of \"semantic primitives\", fundamental concepts, to which they refer when they are trying to explain unfamiliar terms to other people. An ontology that includes representations of those semantic primitives could in such a case be used to create logical descriptions of any term that a person may wish to define logically. That ontology would be one form of upper ontology, serving as a logical \"interlingua\" that can translate ideas in one terminology to its logical equivalent in another terminology.\n\nAdvocates argue that most disagreement about the viability of an upper ontology can be traced to the conflation of ontology, language and knowledge, or too-specialized areas of knowledge: many people, or agents or groups will have areas of their respective internal ontologies that do not overlap. If they can cooperate and share a conceptual map at all, this may be so very useful that it outweighs any disadvantages that accrue from sharing. To the degree it becomes harder to share concepts the deeper one probes, the more valuable such sharing tends to get. If the problem is as basic as opponents of upper ontologies claim, then, it also applies to a group of humans trying to cooperate, who might need machine assistance to communicate easily.\n\nIf nothing else, such ontologies are implied by machine translation, used when people cannot practically communicate. Whether \"upper\" or not, these seem likely to proliferate.\n\nTable contains data mainly from \nPlease expand the table if you have an ongoing UO project. Note that the lack of fresh releases does not imply inactivity or uselessness. So, columns will need some refinement.\n\nThe Basic Formal Ontology (BFO) framework developed by Barry Smith and his associates consists of a series of sub-ontologies at different levels of granularity. The ontologies are divided into two varieties: relating to continuant entities such as three-dimensional enduring objects, and occurrent entities (primarily) processes conceived as unfolding in successive phases through time. BFO thus incorporates both three-dimensionalist and four-dimensionalist perspectives on reality within a single framework. Interrelations are defined between the two types of ontologies in a way which gives BFO the facility to deal with both static/spatial and dynamic/temporal features of reality. A continuant domain ontology descending from BFO can be conceived as an inventory of entities existing at a time. Each occurrent ontology can be conceived as an inventory of processes unfolding through a given interval of time. Both BFO itself and each of its extension sub-ontologies can be conceived as a window on a certain portion of reality at a given level of granularity. More than 200 extension ontologies of BFO have been created, applying the BFO architecture to different domains through the strategy of downward population. The Cell Ontology, for example, populates downward from BFO by importing the BFO branch terminating with object, and defining a cell as a subkind of object. Other examples of ontologies extending BFO are the Ontology for Biomedical Investigations (OBI) and the ontologies of the Open Biomedical Ontologies Foundry. In addition to these examples, BFO and extensions are increasingly being use in defense and security domains, for example in the AIRS framework. BFO serves as the upper level of the Sustainable Development Goals (SDG) Interface Ontology developed by the United Nations Environment Programme. BFO has been documented in the textbook Building Ontologies with Basic Formal Ontology, published by MIT Press in 2015.\n\nBusiness Objects Reference Ontology is an upper ontology designed for developing ontological or semantic models for large complex operational applications that consists of a top ontology as well as a process for constructing the ontology. It is built upon a series of clear metaphysical choices to provide a solid (metaphysical) foundation. A key choice was for an extensional (and hence, four-dimensional) ontology which provides it a simple criteria of identity. Elements of it have appeared in a number of standards. For example, the ISO standard, ISO 15926 – Industrial automation systems and integration – was heavily influenced by an early version. The IDEAS (International Defence Enterprise Architecture Specification for exchange) standard is based upon BORO, which in turn was used to develop DODAF 2.0.\n\nAlthough \"CIDOC object-oriented Conceptual Reference Model\" (CRM) is a domain ontology, specialised to the purposes of representing cultural heritage, a subset called CRM Core is a generic upper ontology, including:\n\nA persistent item is a physical or conceptional item that has a persistent identity recognized within the duration of its existence by its identification rather than by its continuity or by observation. A persistent item is comparable to an endurant.A propositional object is a set of statements about real or imaginary things.A symbolic object is a sign/symbol or an aggregation of signs or symbols.\n\nCOSMO (COmmon Semantic MOdel, available at http://micra.com/COSMO/COSMO.owl) is an ontology that was initiated as a project of the COSMO working group of the Ontology and taxonomy Coordinating Working Group, with the goal of developing a foundation ontology that can serve to enable broad general Semantic Interoperability. The current version is an OWL ontology, but a Common-Logic compliant version is anticipated in the future. The ontology and explanatory files are available at the COSMO site. The goal of the COSMO working group was to develop a foundation ontology by a collaborative process that will allow it to represent all of the basic ontology elements that all members feel are needed for their applications. The development of COSMO is fully open, and any comments or suggestions from any sources are welcome. After some discussion and input from members in 2006, the development of the COSMO has been continued primarily by Patrick Cassidy, the chairman of the COSMO Working Group. Contributions and suggestions from any interested party are still welcome and encouraged. Many of the types (OWL classes) in the current COSMO have been taken from the OpenCyc OWL version 0.78, and from the SUMO. Other elements were taken from other ontologies (such as BFO and DOLCE), or developed specifically for COSMO. Development of the COSMO initially focused on including representations of all of the words in the Longman Dictionary of Contemporary English (LDOCE) controlled defining vocabulary (2148 words). These words are sufficient to define (linguistically) all of the entries in the LDOCE. It is hypothesized that the ontological representations of the concepts represented by those terms will be sufficient to specify the meanings of any specialized ontology element, thereby serving as a basis for general Semantic Interoperability. Interoperability via COSMO is enabled by using the COSMO (or an ontology derived from it) as an interlingua by which other domain ontologies can be translated into each other's terms and thereby accurately communicate. As new domains are linked into COSMO, additional semantic primitives may be recognized and added to its structure. The current (January 2016) OWL version of COSMO has over 8000 types (OWL classes), over 1000 relations, and over 3000 restrictions. The COSMO itself (COSMO.owl) and other related and explanatory files can be obtained at http://micra.com/COSMO.\n\nA well-known and quite comprehensive ontology available today is Cyc, a proprietary system under development since 1986, consisting of a foundation ontology and several domain-specific ontologies (called \"microtheories\"). A subset of that ontology has been released for free under the name OpenCyc, and a more or less unabridged version is made available for free non-commercial use under the name ResearchCyc.\n\nDescriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) is the first module of the WonderWeb foundational ontologies library, developed by Nicola Guarino and his associates at the Laboratory for Applied Ontology (LOA). As implied by its acronym, DOLCE has a clear \"cognitive bias\", in that it aims at capturing the ontological categories underlying natural language and human common sense. DOLCE, however, does not commit to a strictly referentialist metaphysics related to the intrinsic nature of the world. Rather, the categories it introduces are thought of as cognitive artifacts, which are ultimately depending on human perception, cultural imprints and social conventions. In this sense, they intend to be just \"descriptive\" (vs \"prescriptive\") notions, that assist in making already formed conceptualizations explicit.\n\nThe general formal ontology (GFO), developed by Heinrich Herre and his colleagues of the research group Onto-Med in Leipzig, is a realistic ontology integrating processes and objects. It attempts to include many aspects of recent philosophy, which is reflected both in its taxonomic tree and its axiomatizations. GFO allows for different axiomatizations of its categories (such as the existence of atomic time-intervals vs. dense time). The basic principles of GFO are published in the Onto-Med Report Nr. 8 and in \"General Formal Ontology (GFO): A Foundational Ontology for Conceptual Modelling\".\n\nTwo GFO specialties, among others, are its account of persistence and its time model. Regarding persistence, the distinction between endurants (objects) and perdurants (processes) is made explicit within GFO by the introduction of a special category, a persistent. A persistant is a special category with the intention that its instances \"remain identical\" (over time). With respect to time, time intervals are taken as primitive in GFO, and time-points (called \"time boundaries\") as derived. Moreover, time-points may coincide, which is convenient for modelling instantaneous changes.\n\ngist is developed and supported by Semantic Arts. gist (not an acronym – it means to get the essence of) is a “minimalist upper ontology”. gist is targeted at enterprise information systems, although it has been applied to healthcare delivery applications. \nThe major attributes of gist are:\ngist has been used to build Enterprise Ontologies for a number of major commercial and governmental agencies including: Procter & Gamble, Sentara Healthcare, Washington State Department of Labor & Industries, LexisNexis, Sallie Mae and two major Financial Services firms.\ngist is freely available with a Creative Commons share alike license. There are 18 small ontologies that make up gist. Gist can be downloaded all at once by loading or importing gistCore at gist7. \ngist is actively maintained, and has been in use for 10 years. As of November 2017 it is at version 7.5.2.\n\ngist was the subject of a paper exploring how to bridge modeling differences between ontologies \nIn a paper describing the OQuaRE methodology for evaluating ontologies, the gist unit of measure ontology scored the highest in the manual evaluation against 10 other unit of measure ontologies, and scored above average in the automated evaluation. The authors stated \"This ontology could easily be tested and validated, its knowledge could be effectively reused and adapted for different specified environments\" \n\nThe upper ontology developed by the IDEAS Group is higher-order, extensional and 4D. It was developed using the BORO Method. The IDEAS ontology is not intended for reasoning and inference purposes; its purpose is to be a precise model of business.\n\nISO 15926 is an International Standard for the representation of process plant life-cycle information. This representation is specified by a generic, conceptual data model that is suitable as the basis for implementation in a shared database or data warehouse. The data model is designed to be used in conjunction with reference data: standard instances that represent information common to a number of users, process plants, or both. The support for a specific life-cycle activity depends on the use of appropriate reference data in conjunction with the data model. To enable integration of life-cycle information the model excludes all information constraints that are appropriate only to particular applications within the scope.\nISO 15926-2 defines a generic model with 201 entity types. It has been prepared by Technical Committee ISO/TC 184, Industrial automation systems and integration, Subcommittee SC 4, Industrial data.\n\nMarineTLO is an upperontology for the marine domain (also applicable to the terrestrial domain), developed by the Information Systems Laboratory at the Institute of Computer Science,\nFoundation for Research and Technology - Hellas (FORTH-ICS).\nIts purpose is to tackle the need for having integrated sets of facts about marine species,\nand thus to assist research about species and biodiversity.\nIt provides a unified and coherent core model for schema mapping which enables formulating and\nanswering queries which cannot be answered by any individual source.\n\nPROTON (PROTo ONtology) is a basic subsumption hierarchy which provides coverage of most of the upper-level concepts necessary for semantic annotation, indexing, and retrieval.\n\nThe Suggested Upper Merged Ontology (SUMO) is another comprehensive ontology project. It includes an upper ontology, created by the IEEE working group P1600.1 (originally by Ian Niles and Adam Pease). It is extended with many domain ontologies and a complete set of links to WordNet. It is open source.\n\nUpper Mapping and Binding Exchange Layer (UMBEL) is an ontology of 28,000 reference concepts that maps to a simplified subset of the OpenCyc ontology, that is intended to provide a way of linking the precise OpenCyc ontology with less formal ontologies. It also has formal mappings to Wikipedia, DBpedia, PROTON and GeoNames. It has been developed and maintained as open source by Structured Dynamics.\n\nThe Unified Foundational Ontology (UFO), developed by Giancarlo Guizzardi and associates, incorporating developments from GFO, DOLCE and the Ontology of Universals underlying OntoClean in a single coherent foundational ontology. The core categories of UFO (UFO-A) have been completely formally characterized in Giancarlo Guizzardi's Ph.D. thesis and further extended at the Ontology and Conceptual Modelling Research Group (NEMO) in Brazil with cooperators from Brandenburg University of Technology (Gerd Wagner) and Laboratory for Applied Ontology (LOA). UFO-A has been employed to analyze structural conceptual modeling constructs such as object types and taxonomic relations, associations and relations between associations, roles, properties, datatypes and weak entities, and parthood relations among objects. More recent developments incorporate an ontology of events in UFO (UFO-B), as well as an ontology of social and intentional aspects (UFO-C). The combination of UFO-A, B and C has been used to analyze, redesign and integrate reference conceptual models in a number of complex domains such as, for instance, Enterprise Modeling, Software Engineering, Service Science, Petroleum and Gas, Telecommunications, and Bioinformatics. Another recent development aimed towards a clear account of services and service-related concepts, and provided for a commitment-based account of the notion of service (UFO-S),\nUFO is the foundational ontology for OntoUML, an ontology modeling language.\n\nWordNet, a freely available database originally designed as a semantic network based on psycholinguistic principles, was expanded by addition of definitions and is now also viewed as a dictionary. It qualifies as an upper ontology by including the most general concepts as well as more specialized concepts, related to each other not only by the subsumption relations, but by other semantic relations as well, such as part-of and cause. However, unlike Cyc, it has not been formally axiomatized so as to make the logical relations between the concepts precise. It has been widely used in Natural language processing research.\n\nYAMATO is developed by Riichiro Mizoguchi, formerly at the Institute of Scientific and Industrial Research of the University of Osaka, and now at the Japan Advanced Institute of Science and Technology. Major features of YAMATO are:\n\nYAMATO has been extensively used for developing other, more applied, ontologies such as a medical ontology, an ontology of gene, an ontology of learning/instructional theories, an ontology of sustainability science, and an ontology of the cultural domain .\n\nONSET, the foundational ontology selection and explanation tool, assists the domain ontology developer in selecting the most appropriate foundational ontology. The domain ontology developer provides the requirements/answers one or more questions, and ONSET computes the selection of the appropriate foundational ontology and explains why. The current version (v2 of 24 April 2013) includes DOLCE, BFO, GFO, SUMO, YAMATO and GIST.\n\nROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO. Features of ROMULUS include:\n\n\n"}
{"id": "56351412", "url": "https://en.wikipedia.org/wiki?curid=56351412", "title": "Uzawa's theorem", "text": "Uzawa's theorem\n\nUzawa's theorem, also known as the steady state growth theorem, is a theorem in economic growth theory concerning the form that technological change can take in the Solow–Swan and Ramsey–Cass–Koopmans growth models. It was first proved by Japanese economist Hirofumi Uzawa.\n\nOne general version of the theorem consists of two parts. The first states that, under the normal assumptions of the Solow and Neoclassical models, if (after some time T) capital, investment, consumption, and output are increasing at constant exponential rates, these rates must be equivalent. Building on this result, the second part asserts that, within such a balanced growth path, the production function, formula_1 (where formula_2 is technology, formula_3 is capital, and formula_4 is labor), can be rewritten such that technological change affects output solely as a scalar on labor (i.e. formula_5) a property known as labor-augmenting or Harrod-neutral technological change. \n\nUzawa's theorem demonstrates a significant limitation of the commonly used Neoclassical and Solow models. Imposing the assumption of balanced growth within such models requires that technological change by labor-augmenting. By contraposition, any production function for which it is not possible to represent the effect of technology as a scalar on labor cannot produce a balanced growth path.\n\nThroughout this page, a dot over a variable will denote its derivative with respect to time (i.e. formula_6). Also, the growth rate of a variable formula_7 will be denoted formula_8.\n\nUzawa's theorem \n\nModel with aggregate production function formula_9, where formula_10 and formula_11 represents technology at time t (where formula_12 is an arbitrary subset of formula_13 for some natural number formula_14). Assume that formula_15 exhibits constant returns to scale in formula_3 and formula_4. The growth in capital at time t is given by\n\nformula_18\n\nwhere formula_19 is the depreciation rate and formula_20 is consumption at time t. \n\nSuppose that population grows at a constant rate, formula_21, and that there exists some time formula_22 such that for all formula_23, formula_24, formula_25, and formula_26. Then \n\n1. formula_27; and\n\n2. For any formula_28 , there exists a function formula_29 that is homogeneous of degree 1 in its two arguments, such that the aggregate production function can be represented as \"formula_30\", where formula_31 and formula_32.\n\nFor any constant formula_33, formula_34.\n\nProof: Observe that for any formula_35, formula_36. Therefore,\nformula_37.\n\nWe first show that the growth rate of investment formula_38 must equal the growth rate of capital formula_39 (i.e. formula_40) \n\nThe resource constraint at time formula_41 implies \nBy definition of formula_43, formula_44 for all formula_23 . Therefore, the previous equation implies\nfor all formula_23. The left-hand side is a constant, while the right-hand side grows at formula_48 (by Lemma 1). Therefore, formula_49 and thus\nFrom national income accounting for a closed economy, final goods in the economy must either be consumed or invested, thus for all formula_41\nDifferentiating with respect to time yields\nDividing both sides by formula_54 yields\nSince formula_57 and formula_58 are constants, formula_59 is a constant. Therefore, the growth rate of formula_59 is zero. By Lemma 1, it implies that \nSimilarly, formula_62. Therefore, formula_63. \n\nNext we show that for any formula_23, the production function can be represented as one with labor-augmenting technology.\n\nThe production function at time formula_65 is \nThe constant return to scale property of production (formula_15 is homogeneous of degree one in formula_3 and formula_4) implies that for any formula_23, multiplying both sides of the previous equation by formula_71 yields\nNote that formula_73 because formula_74(refer to solution to differential equations for proof of this step). Thus, the above equation can be rewritten as\nFor any formula_23, define \nand\nCombining the two equations yields \nBy construction, formula_81 is also homogeneous of degree one in its two arguments.\n\nMoreover, by Lemma 1, the growth rate of formula_82 is given by\n"}
{"id": "6828605", "url": "https://en.wikipedia.org/wiki?curid=6828605", "title": "Woodward effect", "text": "Woodward effect\n\nThe Woodward effect, also referred to as a Mach effect, is part of a hypothesis proposed by James F. Woodward in 1990. The hypothesis states that transient mass fluctuations arise in any object that absorbs internal energy while undergoing a proper acceleration. Harnessing this effect could generate a reactionless thrust, which Woodward and others claim to measure in various experiments.\n\nHypothetically, the Woodward effect would allow for field propulsion spacecraft engines that would not have to expel matter. Such a proposed engine, is sometimes called a Mach effect thruster (MET) or a Mach Effect Gravitational Assist (MEGA) drive. So far, experimental results have not strongly supported this hypothesis, but experimental research on this effect, and its potential applications, continues. The anomalous thrust detected in some RF resonant cavity thruster (EmDrive/Cannae drive) experiments may be explained by the same type of Mach effect proposed by Woodward.\n\nThe Space Studies Institute was selected as part of NASA's Innovative Advanced Concepts program as a Phase I proposal in April 2017 for Mach Effect research. The year after, NASA awarded a NIAC Phase II grant to the SSI to further develop these propellantless thrusters.\n\nThe effect is controversial within mainstream physics because the underlying mathematics appears faulty, and the effect, if found to be real, would violate momentum conservation and energy conservation.\n\nAccording to Woodward, at least three Mach effects are theoretically possible: vectored impulse thrust, open curvature of spacetime, and closed curvature of spacetime.\n\nThe first effect, the Woodward effect, is the minimal energy effect of the hypothesis. The Woodward effect is focused primarily on proving the hypothesis and providing the basis of a Mach effect impulse thruster. In the first of three general Mach effects for propulsion or transport, the Woodward effect is an impulse effect usable for in-orbit satellite station-keeping, spacecraft reaction control systems, or at best, thrust within the solar system. The second and third effects are open and closed space-time effects. Open curved space-time effects can be applied in a field generation system to produce warp fields. Closed-curve space-time effects would be part of a field generation system to generate wormholes.\n\nThe third Mach effect is a closed-curve spacetime effect or closed timelike curve called a benign wormhole. Closed-curve space is generally known as a wormhole or black hole. Prompted by Carl Sagan for the scientific basis of wormhole transport in the movie \"Contact\", Kip Thorne developed the theory of benign wormholes. The generation, stability, and traffic control of transport through a benign wormhole is only theoretical at present. One difficulty is the requirement for energy levels approximating a \"Jupiter size mass\".\n\nKenneth Nordtvedt showed in 1988 that gravitomagnetism, which is an effect predicted by general relativity but hadn't yet been observed at that time and was even challenged by the scientific community, is inevitably a real effect because it is a direct consequence of the gravitational vector potential. He subsequently showed that the gravitomagnetism interaction (not to be confused with the Nordtvedt effect), like inertial frame dragging and the Lense–Thirring precession, is typically a Mach effect.\n\nThe Woodward effect is based on the relativistic effects theoretically derived from Mach's principle on inertia within general relativity, attributed by Albert Einstein to Ernst Mach. Mach's Principle is generally defined as \"the local inertia frame that is completely determined by the dynamic fields in the Universe.\" The conjecture comes from a thought experiment:\n\nA formulation of Mach's principle was first proposed as a vector theory of gravity, modeled on Maxwell's formalism for electrodynamics, by Dennis Sciama in 1953, who then reformulated it in a tensor formalism equivalent to general relativity in 1964.\n\nIn this paper, Sciama stated that instantaneous inertial forces in all accelerating objects are produced by a primordial gravity-based inertial radiative field created by distant cosmic matter and propagating both forward \"and\" backward in time at light speed:\nSciama's inertial-induction idea has been shown to be correct in Einstein's general relativity for any Friedmann–Robertson–Walker cosmology. According to Woodward, the derivation of Mach effects is relativistically invariant, so the conservation laws are satisfied, and no \"new physics\" is involved besides general relativity.\n\nAs previously formulated by Sciama, Woodward suggests that the Wheeler–Feynman absorber theory would be the correct way to understand the action of instantaneous inertial forces in Machian terms.\n\nThe Wheeler-Feynman absorber theory is an interpretation of electrodynamics that starts from the idea that a solution to the electromagnetic field equations has to be symmetric with respect to time-inversion, as are the field equations themselves. Wheeler and Feynman showed that the propagating solutions to classical wave equations can either be \"retarded\" (i.e. propagate forward in time) or \"advanced\" (propagate backward in time). The absorber theory has been used to explain quantum entanglement and led to the transactional interpretation of quantum mechanics, as well as the Hoyle-Narlikar theory of gravity, a Machian version of Einstein's general relativity. Fred Hoyle and Jayant Narlikar originally developed their cosmological model as a quasi steady state model of the universe, adding a \"Creation field\" generating matter out of empty space, an hypothesis contradicted by recent observations. When the C-field is not used, ignoring the parts regarding mass creation, the theory is no longer steady state and becomes a Machian extension of general relativity. This modern development is known as the \"Gravitational Absorber Theory\".\n\nAs the gravitational absorber theory reduces to general relativity in the limit of a smooth fluid model of particle distribution, both theories make the same predictions. Except in the Machian approach, a mass changing effect emerges from the general equation of motion, from which Woodward's transient mass equation can be derived. A resulting force suitable for Mach effect thrusters can then be calculated.\n\nWhile the Hoyle-Narlikar derivation of the Mach effect transient terms is done from a fully nonlinear, covariant formulation, it has been shown Woordward's transient mass equation can also be retrieved from linearized general relativity.\n\nThe following has been detailed by Woodward in various peer-reviewed papers throughout the last twenty years.\n\nAccording to Woodward, a transient mass fluctuation arises in an object when it absorbs \"internal\" energy as it is accelerated. Several devices could be built to store internal energy during accelerations. A measurable effect needs to be driven at a high frequency, so macroscopic mechanical systems are out of question since the rate at which their internal energy could be modified is too limited. The only systems that could run at a high frequency are electromagnetic energy storage devices. For fast transient effects, batteries are ruled out. A magnetic energy storage device like an inductor using a high-permeability core material to transfer the magnetic energy could be specially built. But capacitors are preferable to inductors because compact devices storing energy at a very high energy density without electrical breakdown are readily available. Shielding electrical interferences are easier than shielding magnetic ones. Ferroelectric materials can be used to make high-frequency electro-mechanical actuators, and they are themselves capacitors so they can be used for both energy storage and acceleration. Finally, capacitors are cheap and available in various configurations. So Mach effect experiments have always relied on capacitors so far.\n\nWhen the dielectric of a capacitor is submitted to a varying electric power (charge or discharge), Woodward's hypothesis predicts a transient mass fluctuation arises according to the transient mass equation (TME):\n\nwhere:\n\nThis equation is not the full Woodward equation as seen in the book. There is a third term, formula_8which Woodward discounts because his gauge setsformula_9; the derivatives of this quantity must therefore be negligible.\n\nThe previous equation shows that when the dielectric material of a capacitor is cyclically charged then discharged while being accelerated, its mass density fluctuates, by around plus or minus its rest mass value. Therefore, a device can be made to oscillate either in a linear or orbital path, such that its mass density is higher while the mass is moving forward, and lower while moving backward, thus creating an acceleration of the device in the forward direction, i.e. a thrust. This effect, used repeatedly, does not expel any particle and thus would represent a type of apparent propellantless propulsion, which seems to be in contradiction with Newton's third law of motion. However, Woodward states there is no violation of momentum conservation in Mach effects:\nTwo terms are important for propulsion on the right-hand side of the previous equation:\n\nApplications of propellantless propulsion include straight-line thruster or impulse engine, open curved fields for starship warp drives, and even the possibility of closed curved fields such as traversable benign wormholes.\n\nThe mass of the electron is positive according to the mass–energy equivalence \"E\" = \"mc\" but this invariant mass is made from the bare mass of the electron \"clothed\" by a virtual photon cloud. According to quantum field theory, as those virtual particles have an energy more than twice the bare mass of the electron, mandatory for pair production in renormalization, the nonelectromagnetic bare mass of the \"unclothed\" electron has to be \"negative\".\n\nUsing the ADM formalism, Woodward proposes that the physical interpretation of the \"wormhole term\" in his transient mass equation could be a way to expose the negative bare mass of the electron, in order to produce large quantities of exotic matter that could be used in a warp drive to propel a spacecraft or generate traversable wormholes.\n\nCurrent spacecraft achieve a change in velocity by the expulsion of propellant, the extraction of momentum from stellar radiation pressure or the stellar wind or the utilisation of a gravity assist (\"slingshot\") from a planet or moon. These methods are limiting in that rocket propellants have to be accelerated as well and are eventually depleted, and the stellar wind or the gravitational fields of planets can only be utilized locally in the Solar System. In interstellar space and bereft of the above resources, different forms of propulsion are needed to propel a spacecraft, and they are referred to as advanced or .\n\nIf the Woodward effect is confirmed and if an engine can be designed to use applied Mach effects, then a spacecraft may be possible that could maintain a steady acceleration into and through interstellar space without the need to carry along propellants. Woodward presented a paper about the concept at the NASA Breakthrough Propulsion Physics Program Workshop conference in 1997, and continued to publish on this subject thereafter.\n\nEven ignoring for the moment the impact on interstellar travel, future spacecraft driven by impulse engines based on Mach effects would represent an astounding breakthrough in terms of interplanetary spaceflight alone, enabling the rapid colonization of the entire solar system. Travel times being limited only by the specific power of the available power supplies and the acceleration human physiology can endure, they would allow crews to reach any moon or planet in our solar system in less than three weeks. For example, a typical one-way trip at an acceleration of 1 g from the Earth to the Moon would last only about 4 hours; to Mars, 2 to 5 days; to the asteroid belt, 5 to 6 days; and to Jupiter, 6 to 7 days.\n\nAs shown by the transient mass fluctuation equation above, exotic matter could be theoretically created. A large quantity of negative energy density would be the key element needed to create warp drives as well as traversable wormholes. As such, if proven to be scientifically valid, practically feasible and scaling as predicted by the hypothesis, the Woodward effect could not only be used for interplanetary travel, but also for apparent faster-than-light interstellar travel:\n\nTwo patents have been issued to Woodward and associates based on how the Woodward effect might be used in practical devices for producing thrust:\n\nWoodward and his associates have claimed since the 1990s to have successfully measured forces at levels great enough for practical use and also claim to be working on the development of a practical prototype thruster. No practical working devices have yet been publicly demonstrated.\n\nThe NIAC contract awarded in 2017 by NASA for the development of Mach effect thrusters is a primary three-task effort, two experimental and one analytical:\n\n\nA former type of Mach effect thruster was the Mach-Lorentz thruster (MLT). It used a charging capacitor embedded in a magnetic field created by a magnetic coil. A Lorentz force, the cross product between the electric field and the magnetic field, appears and acts upon the ions inside the capacitor dielectric. In such electromagnetic experiments, the power can be applied at frequencies of several megahertz, unlike PZT stack actuators where frequency is limited to tens of kilohertz. The photograph shows the components of a Woodward effect test article used in a 2006 experiment.\n\nHowever, a problem with some of these devices was discovered in 2007 by physicist Nembo Buldrini, who called it the \"Bulk Acceleration Conjecture\":\n\nTo address this issue, Woodward started to design and build a new kind of device known as a MET (Mach Effect Thruster) and later a MEGA drive (Mach Effect Gravitational Assist drive), using capacitors and a series of thick PZT disks. This ceramic is piezoelectric, so it can be used as an electromechanical actuator to accelerate an object placed against it: its crystalline structure expands when a certain electrical polarity is applied, then contracts when the opposite field is applied, and the stack of discs vibrates.\n\nIn the first tests, Woodward simply used a capacitor between two stacks of PZT disks. The capacitor, while being electrically charged to change its internal energy density, is shuttled back and forth between the PZT actuators. Piezoelectric materials can also generate a measurable voltage potential across their two faces when pressed, so Woodward first used some small portions of PZT material as little accelerometers put on the surface of the stack, to precisely tune the device with the power supply. Then Woodward realized that PZT material and the dielectric of a capacitor were very similar, so he built devices that are made exclusively of PZT disks, without any conventional capacitor, applying different signals to different portions of the cylindrical stack. The available picture taken by his graduate student Tom Mahood in 1999 shows a typical all-PZT stack with different disks:\nDuring forward acceleration and before the transient mass change in the capacitor decays, the resultant increased momentum is transferred forward to a bulk \"reaction mass\" through an elastic collision (the brass end cap on the left in the picture). Conversely, the following decrease in the mass density takes place during its backward movement.\nWhile operating, the PZT stack is isolated in a Faraday cage and put on a sensitive torsion arm for thrust measurements, inside a vacuum chamber. Throughout the years, a wide variety of different types of devices and experimental setups have been tested. The force measuring setups have ranged from various load cell devices to ballistic pendulums to multiple torsion arm pendulums, in which movement is actually observed. Those setups have been improved against spurious effects by isolating and canceling thermal transfers, vibration and electromagnetic interference, while getting better current feeds and bearings. Null tests were also conducted.\n\nIn the future, Woodward plans to scale thrust levels, switching from the current piezoelectric dielectric ceramics (PZT stacks) to new high-κ dielectric nanocomposite polymers, like PMN, PMN-PT or CCTO. Nevertheless, such materials are new, quite difficult to find, and are electrostrictive, not piezoelectric.\n\nIn 2013, the Space Studies Institute announced the Exotic Propulsion Initiative, a new project privately funded that aims to replicate Woodward's experiments and then, if proven successful, fully develop exotic propulsion. Gary Hudson, president and CEO of SSI, presented the program at the 2014 NASA Institute for Advanced Concepts Symposium, and a NIAC phase I grant was awarded in April 2017 to develop a better theoretical model and technical solutions for greater efficiency as a TRL-1 technology: reduction of heating and longer operating time using \"chirped pulses\"; and design of a dedicated electronic circuit with better frequency impedance matching. The concept of an interstellar mission to Proxima Centauri b was also detailed. Consecutively to these achievements, a NIAC Phase II grant was awarded in March 2018 to test an improved design with a higher operational frequency to increase the output thrust.\n\nAnother type of claimed propellantless thruster, called the EmDrive by its inventor British engineer Roger Shawyer, has been proposed to work due to a Mach effect:\n\nThe asymmetric resonant microwave cavity would act as a capacitor where:\nWhen a polymer insert is placed asymmetrically in the cavity, its dielectric properties result in greater asymmetry, while decreasing the cavity \"Q\" factor.\nThe cavity's acceleration is a function of all the above factors, and the model can explain the acceleration of the cavity with and without a dielectric.\n\nFrom his initial paper onward Woodward has claimed that this effect is detectable with modern technology. He and others have performed and continue to perform experiments to detect the small forces that are predicted to be produced by this effect. So far some groups claim to have detected forces at the levels predicted and other groups have detected forces at much greater than predicted levels or nothing at all. To date there has been no announcement conclusively confirming proof for the existence of this effect or ruling it out.\n\n\n\n\n\n\n\n\n\n\n\nAll inertial frames are in a state of constant, rectilinear motion with respect to one another; they are not accelerating in the sense that an accelerometer at rest in one would detect zero acceleration. Despite their ubiquitous nature, inertial frames are still not fully understood. That they exist is certain, but what causes them to exist – and whether these sources could constitute reaction-media – are still unknown. Marc Millis, of the NASA Breakthrough Propulsion Physics Program, stated \" \"For example, the notion of thrusting without propellant evokes objections of violating conservation of momentum. This, in turn, suggests that space drive research must address conservation of momentum. From there it is found that many relevant unknowns still linger regarding the source of the inertial frames against which conservation is referenced. Therefore, research should revisit the unfinished physics of inertial frames, but in the context of propulsive interactions.\" \" Mach's principle is generally defined within general relativity as \"the local inertia frame is completely determined by the dynamic fields in the universe.\" Rovelli evaluated a number of versions of \"Mach's principle\" that exist in the literature. Some are partially correct and some have been dismissed as incorrect.\n\nA challenge to the mathematical foundations of Woodward's hypothesis were raised in a paper published by the Oak Ridge National Laboratory in 2001. In the paper, John Whealton noted that the experimental results of Oak Ridge scientists can be explained in terms of force contributions due to time-varying thermal expansion, and stated that a laboratory demonstration produced 100 times the Woodward effect without resorting to non-Newtonian explanations. In response, Woodward published a criticism of Whealton's math and understanding of the physics involved, and built an experiment attempting to demonstrate the flaw.\n\nA rate of change in momentum represents a force, whereby F \"ma. Whealton et al. use the technical definition, Fd(\"mv)/d\"t\", which can be expanded to F\"m\" dv/d\"t\" + d\"m\"/d\"t\" v. This second term has both delta mass and v, which is measured instantaneously; this term will, in general, cancel out the force from the inertial response terms predicted by Woodward. Woodward argued that the d\"m\"/d\"t\" v term does not represent a physical force on the device, because it vanishes in a frame where the device is momentarily stationary.\n\nIn an appendix to his thesis, Mahood argues that the unexpectedly small magnitude of the results in his experiments are a confirmation of the cancellation predicted by Whealton; the results are instead due to higher-order mass transients which are not exactly cancelled. Mahood would later describe this argument as \"one of the very few things I've done in my life that I actually regret\".\n\nAlthough the momentum and energy exchange with distant matter guarantees global conservation of energy and momentum, this field exchange is supplied at no material cost, unlike the case with conventional fuels. For this reason, when the field exchange is ignored, a propellantless thruster behaves locally like a free energy device. This is immediately apparent from basic Newtonian analysis: if constant power produces constant thrust, then input energy is linear with time and output (kinetic) energy is quadratic with time. Thus there exists a break-even time (or distance or velocity) of operation, above which more energy is output than is input. The longer it is allowed to accelerate, the more pronounced will this effect become, as simple Newtonian physics predicts.\n\nConsidering those conservation issues, a Mach effect thruster relies on Mach's principle, hence it is not an electrical to kinetic transducer, i.e. it does not convert electric energy to kinetic energy. Rather, a Mach effect thruster is a gravinertial transistor that controls the flow of gravinertial flux, in and out of the active mass of the thruster. The primary power into the thruster is contained in the flux of the gravitational field, not the electricity that powers the device. Failing to account for this flux, is much the same as failing to account for the wind on a sail. Mach effects are relativistic by nature, and considering a spaceship accelerating with a Mach effect thruster, the propellant is not accelerating with the ship, so the situation should be treated as an accelerating and therefore non-inertial reference frame, where F does not equal \"m\"a. Keith H. Wanser, professor of physics at California State University, Fullerton, published a paper in 2013 concerning the conservation issues of Mach effect thrusters.\n\nIn 2009, Harold \"Sonny\" White of NASA proposed the Quantum Vacuum Fluctuation (QVF) conjecture, a non-relativistic hypothesis based on quantum mechanics to produce momentum fluxes even in empty outer space. Where Sciama's gravinertial field of Wheeler–Feynman absorber theory is used in the Woodward effect, the White conjecture replaces the Sciama gravinertial field with the quantum electrodynamic vacuum field. The local reactive forces are generated and conveyed by momentum fluxes created in the QED vacuum field by the same process used to create momentum fluxes in the gravinertial field. White uses MHD plasma rules to quantify this local momentum interaction where in comparison Woodward applies condensed matter physics.\n\nBased on the White conjecture, the proposed theoretical device is called a quantum vacuum plasma thruster (QVPT) or Q-thruster. No experiments have been performed to date. Unlike a Mach effect thruster instantaneously exchanging momentum with the distant cosmic matter through the advanced/retarded waves (Wheeler–Feynman absorber theory) of the radiative gravinertial field, Sonny's \"Q-thruster\" would appear to violate momentum conservation, for the thrust would be produced by pushing off virtual \"Q\" particle/antiparticle pairs that would annihilate after they have been pushed on. However, it would not necessarily violate the law of conservation of energy, as it requires an electric current to function, much like any \"standard\" MHD thruster, and cannot produce more kinetic energy than its equivalent net energy input.\n\nWoodward and Fearn showed why the amount of electron-positron virtual pairs of the quantum vacuum, used by White as a virtual plasma propellant, cannot account for thrusts in any isolated, closed electromagnetic system such as the QVPT or the EmDrive.\n\nWoodward's claims in his papers and in space technology conference press releases of a potential breakthrough technology for spaceflight have generated interest in the popular press\nand university news as well as the space news media. Woodward also gave a video interview for the TV show Ancient Aliens, season 6, episode 12. However doubters do exist.\n\n\n"}
