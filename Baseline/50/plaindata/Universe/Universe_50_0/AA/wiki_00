{"id": "193939", "url": "https://en.wikipedia.org/wiki?curid=193939", "title": "Alternative energy", "text": "Alternative energy\n\nAlternative energy is any energy source that is an alternative to fossil fuel. These alternatives are intended to address concerns about fossil fuels, such as its high carbon dioxide emissions, an important factor in global warming. Marine energy, hydroelectric, wind, geothermal and solar power are all alternative sources of energy.\n\nThe nature of what constitutes an alternative energy source has changed considerably over time, as have controversies regarding energy use. Because of the variety of energy choices and differing goals of their advocates, defining some energy types as \"alternative\" is considered very controversial.\n\nHistorians of economies have examined the key transitions to alternative energies and regard the transitions as pivotal in bringing about significant economic change. Prior to the shift to an alternative energy, supplies of the dominant energy type became erratic, accompanied by rapid increases in energy prices.\n\nIn the late medieval period, coal was the new alternative fuel to save the society from overuse of the dominant fuel, wood. The deforestation had resulted in shortage of wood, at that time soft coal appeared as a savior. Historian Norman F. Cantor describes how:\n\nEuropeans had lived in the midst of vast forests throughout the earlier medieval centuries. After 1250 they became so skilled at deforestation that by 1500 AD they were running short of wood for heating and cooking... By 1500 Europe was on the edge of a fuel and nutritional disaster, [from] which it was saved in the sixteenth century only by the burning of soft coal and the cultivation of potatoes and maize.\n\nWhale oil was the dominant form of lubrication and fuel for lamps in the early 19th century, but the depletion of the whale stocks by mid century caused whale oil prices to skyrocket setting the stage for the adoption of petroleum which was first commercialized in Pennsylvania in 1859.\n\nIn 1917, Alexander Graham Bell advocated ethanol from corn, wheat and other foods as an alternative to coal and oil, stating that the world was in measurable distance of depleting these fuels. For Bell, the problem requiring an alternative was lack of renewability of orthodox energy sources. Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter. Brazil's ethanol fuel program uses modern equipment and cheap sugar cane as feedstock, and the residual cane-waste (bagasse) is used to process heat and power. There are no longer light vehicles in Brazil running on pure gasoline. By the end of 2008 there were 35,000 filling stations throughout Brazil with at least one ethanol pump.\n\nCellulosic ethanol can be produced from a diverse array of feedstocks, and involves the use of the whole crop. This new approach should increase yields and reduce the carbon footprint because the amount of energy-intensive fertilizers and fungicides will remain the same, for a higher output of usable material. As of 2008, there are nine commercial cellulosic ethanol plants which are either operating, or under construction, in the United States.\n\nSecond-generation biofuels technologies are able to manufacture biofuels from inedible biomass and could hence prevent conversion of food into fuel.\" As of July 2010, there is one commercial second-generation (2G) ethanol plant Inbicon Biomass Refinery, which is operating in Denmark.\n\nIn the 1970s, President Jimmy Carter's administration advocated coal gasification as an alternative to expensive imported oil. The program, including the Synthetic Fuels Corporation was scrapped when petroleum prices plummeted in the 1980s. The carbon footprint and environmental impact of coal gasification are both very high.\n\n\nIce storage air conditioning and thermal storage heaters are methods of shifting consumption to use low cost off-peak electricity. When compared to resistance heating, heat pumps conserve electrical power (or in rare cases mechanical or thermal power) by collecting heat from a cool source such as a body of water, the ground or the air.\n\nThermal storage technologies allow heat or cold to be stored for periods of time ranging from diurnal to interseasonal, and can involve storage of sensible energy (i.e. by changing the temperature of a medium) or latent energy (e.g. through phase changes of a medium (i.e. changes from solid to liquid or vice versa), such as between water and slush or ice). Energy sources can be natural (via solar-thermal collectors, or dry cooling towers used to collect winter's cold), waste energy (such as from HVAC equipment, industrial processes or power plants), or surplus energy (such as seasonally from hydropower projects or intermittently from wind farms). The Drake Landing Solar Community (Alberta, Canada) is illustrative. Borehole thermal energy storage allows the community to get 97% of its year-round heat from solar collectors on the garage roofs. The storages can be insulated tanks, borehole clusters in substrates ranging from gravel to bedrock, deep aquifers, or shallow pits that are lined and insulated. Some applications require inclusion of a heat pump.\n\nRenewable energy is generated from natural resources—such as sunlight, wind, rain, tides and geothermal heat—which are renewable (naturally replenished). When comparing the processes for producing energy, there remain several fundamental differences between renewable energy and fossil fuels. The process of producing oil, coal, or natural gas fuel is a difficult and demanding process that requires a great deal of complex equipment, physical and chemical processes. On the other hand, alternative energy can be widely produced with basic equipment and natural processes. Wood, the most renewable and available alternative fuel, emits the same amount of carbon when burned as would be emitted if it degraded naturally. Nuclear power is an alternative to fossil fuels that is non-renewable, like fossil fuels, nuclear ones are a finite resource.\n\nA renewable energy source such as biomass is sometimes regarded as a good alternative to providing heat and electricity with fossil fuels. Biofuels are not inherently ecologically friendly for this purpose, while burning biomass is carbon-neutral, air pollution is still produced. For example, the Netherlands, once leader in use of palm oil as a biofuel, has suspended all subsidies for palm oil due to the scientific evidence that their use \"may sometimes create more environmental harm than fossil fuels\". The Netherlands government and environmental groups are trying to trace the origins of imported palm oil, to certify which operations produce the oil in a responsible manner. Regarding biofuels from foodstuffs, the realization that converting the entire grain harvest of the US would only produce 16% of its auto fuel needs, and the decimation of Brazil's absorbing tropical rain forests to make way for biofuel production has made it clear that placing energy markets in competition with food markets results in higher food prices and insignificant or negative impact on energy issues such as global warming or dependence on foreign energy. Recently, alternatives to such undesirable sustainable fuels are being sought, such as commercially viable sources of cellulosic ethanol.\n\nCarbon-neutral fuels are synthetic fuels (including methane, gasoline, diesel fuel, jet fuel or ammonia) produced by hydrogenating waste carbon dioxide recycled from power plant flue-gas emissions, recovered from automotive exhaust gas, or derived from carbonic acid in seawater. Commercial fuel synthesis companies suggest they can produce synthetic fuels for less than petroleum fuels when oil costs more than $55 per barrel. Renewable methanol (RM) is a fuel produced from hydrogen and carbon dioxide by catalytic hydrogenation where the hydrogen has been obtained from water electrolysis. It can be blended into transportation fuel or processed as a chemical feedstock.\n\nThe George Olah carbon dioxide recycling plant operated by Carbon Recycling International in Grindavík, Iceland has been producing 2 million liters of methanol transportation fuel per year from flue exhaust of the Svartsengi Power Station since 2011. It has the capacity to produce 5 million liters per year. A 250 kilowatt methane synthesis plant was constructed by the Center for Solar Energy and Hydrogen Research (ZSW) at Baden-Württemberg and the Fraunhofer Society in Germany and began operating in 2010. It is being upgraded to 10 megawatts, scheduled for completion in autumn, 2012. Audi has constructed a carbon-neutral liquefied natural gas (LNG) plant in Werlte, Germany. The plant is intended to produce transportation fuel to offset LNG used in their A3 Sportback g-tron automobiles, and can keep 2,800 metric tons of CO out of the environment per year at its initial capacity. Other commercial developments are taking place in Columbia, South Carolina, Camarillo, California, and Darlington, England.\n\nSuch fuels are considered carbon-neutral because they do not result in a net increase in atmospheric greenhouse gases. To the extent that synthetic fuels displace fossil fuels, or if they are produced from waste carbon or seawater carbonic acid, and their combustion is subject to carbon capture at the flue or exhaust pipe, they result in negative carbon dioxide emission and net carbon dioxide removal from the atmosphere, and thus constitute a form of greenhouse gas remediation.\n\nSuch renewable fuels alleviate the costs and dependency issues of imported fossil fuels without requiring either electrification of the vehicle fleet or conversion to hydrogen or other fuels, enabling continued compatible and affordable vehicles. Carbon-neutral fuels offer relatively low cost energy storage, alleviating the problems of wind and solar intermittency, and they enable distribution of wind, water, and solar power through existing natural gas pipelines.\n\nNighttime wind power is considered the most economical form of electrical power with which to synthesize fuel, because the load curve for electricity peaks sharply during the day, but wind tends to blow slightly more at night than during the day, so, the price of nighttime wind power is often much less expensive than any alternative. Germany has built a 250 kilowatt synthetic methane plant which they are scaling up to 10 megawatts.\n\nAlgae fuel is a biofuel which is derived from algae. During photosynthesis, algae and other photosynthetic organisms capture carbon dioxide and sunlight and convert it into oxygen and biomass. This is usually done by placing the algae between two panes of glass. The algae creates three forms of energy fuel: heat (from its growth cycle), biofuel (the natural \"oil\" derived from the algae), and biomass (from the algae itself, as it is harvested upon maturity).\n\nThe heat can be used to power building systems (such as heat process water) or to produce energy. Biofuel is oil extracted from the algae upon maturity, and used to create energy similar to the use of biodiesel. The biomass is the matter left over after extracting the oil and water, and can be harvested to produce combustible methane for energy production, similar to the warmth felt in a compost pile or the methane collected from biodegradable materials in a landfill. Additionally, the benefits of algae biofuel are that it can be produced industrially, as well as vertically (i.e. as a building facade), thereby obviating the use of arable land and food crops (such as soy, palm, and canola).\n\nBiomass briquettes are being developed in the developing world as an alternative to charcoal. The technique involves the conversion of almost any plant matter into compressed briquettes that typically have about 70% the calorific value of charcoal. There are relatively few examples of large scale briquette production. One exception is in North Kivu, in eastern Democratic Republic of Congo, where forest clearance for charcoal production is considered to be the biggest threat to Mountain Gorilla habitat. The staff of Virunga National Park have successfully trained and equipped over 3500 people to produce biomass briquettes, thereby replacing charcoal produced illegally inside the national park, and creating significant employment for people living in extreme poverty in conflict affected areas.\nBiogas digestion harnesses the methane gas that is released when organic waste breaks down in an anaerobic environment. This gas can be retrieved from landfill sites or sewage systems. The gas can be used as a fuel for heat or, more commonly, electricity generation.\n\nThe methane gas that is collected and refined can be used as an energy source for various products.\n\nHydrogen gas is a completely clean burning fuel; its only by-product is water. It also contains relatively high amount of energy compared with other fuels due to its chemical structure.\n\n2H + O → 2HO + High Energy\n\nHigh Energy + 2HO → 2H + O\n\nThis requires a high-energy input, making commercial hydrogen very inefficient. Use of a biological vector as a means to split water, and therefore produce hydrogen gas, would allow for the only energy input to be solar radiation. Biological vectors can include bacteria or more commonly algae. This process is known as biological hydrogen production. It requires the use of single celled organisms to create hydrogen gas through fermentation. Without the presence of oxygen, also known as an anaerobic environment, regular cellular respiration cannot take place and a process known as fermentation takes over. A major by-product of this process is hydrogen gas. If this could be implemented on a large scale, then sunlight, nutrients and water could create hydrogen gas to be used as a dense source of energy. Large-scale production has proven difficult. Not until 1999, was it even possible to induce these anaerobic conditions by sulfur deprivation. Since the fermentation process is an evolutionary back up, turned on during stress, the cells would die after a few days. In 2000, a two-stage process was developed to take the cells in and out of anaerobic conditions and therefore keep them alive. For the last ten years, finding a way to do this on a large-scale has been the main goal of research. Careful work is being done to ensure an efficient process before large-scale production, however once a mechanism is developed, this type of production could solve our energy needs.\n\nHydroelectricity provided 75% of the worlds renewable electricity in 2013. Much of the electricity used today is a result of the heyday of conventional hydroelectric development between 1960 and 1980, which has virtually ceased in Europe and North America due to environmental concerns. Globally there is a trend towards more hydroelectricity. From 2004 to 2014 the installed capacity rose from 715 to 1,055 GW. A popular alternative to the large dams of the past is run-of-the-river where there is no water stored behind a dam and generation usually varies with seasonal rainfall. Using run-of-the-river in wet seasons and solar in dry seasons can balance seasonal variations for both. Another move away from large dams is small hydro, these tend to be situated high up on tributaries, rather than on main rivers in valley bottoms.\n\nOffshore wind farms are similar to land-based wind farms, but are located on the ocean. Offshore wind farms can be placed in water up to deep, whereas floating wind turbines can float in water up to deep.\nThe advantage of having a floating wind farm is to be able to harness the winds from the open ocean. Without any obstructions such as hills, trees and buildings, winds from the open ocean can reach up to speeds twice as fast as coastal areas.\n\nSignificant generation of offshore wind energy already contributes to electricity needs in Europe and Asia and now the first offshore wind farms are under development in U.S. waters. While the offshore wind industry has grown dramatically over the last several decades, especially in Europe, there is still uncertainty associated with how the construction and operation of these wind farms affect marine animals and the marine environment.\n\nTraditional offshore wind turbines are attached to the seabed in shallower waters within the nearshore marine environment. As offshore wind technologies become more advanced, floating structures have begun to be used in deeper waters where more wind resources exist.\n\nMarine and Hydrokinetic (MHK) or marine energy development includes projects using the following devices:\n\nIn the year 2015 ten new reactors came online and 67 more were under construction including the first eight new Generation III+ AP1000 reactors in the US and China and the first four new Generation III EPR reactors in Finland, France and China. Reactors are also under construction in Belarus, Brazil, India, Iran, Japan, Pakistan, Russia, Slovakia, South Korea, Turkey, Ukraine and United Arab Emirates.\n\nThorium is a fissionable material for possible future use in a thorium-based reactor. Proponents of thorium reactors claims several potential advantages over a uranium fuel cycle, such as thorium's greater abundance, better resistance to nuclear weapons proliferation, and reduced plutonium and actinide production. Thorium reactors can be modified to produce Uranium-233, which can then be processed into highly enriched uranium, which has been tested in low yield weapons, and is unproven on a commercial scale.\n\nAs an emerging economic sector, there are limited stock market investment opportunities in alternative energy available to the general public. The public can buy shares of alternative energy companies from various stock markets, with wildly volatile returns. The recent IPO of SolarCity demonstrates the nascent nature of this sector- within a few weeks, it already had achieved the second highest market cap within the alternative energy sector.\n\nInvestors can also choose to invest in ETFs (exchange-traded funds) that track an alternative energy index, such as the WilderHill New Energy Index. Additionally, there are a number of mutual funds, such as Calvert's Global Alternative Energy Mutual Fund that are a bit more proactive in choosing the selected investments.\n\nRecently, Mosaic Inc. launched an online platform allowing residents of California and New York to invest directly in solar. Investing in solar projects had previously been limited to accredited investors, or a small number of willing banks.\n\nOver the last three years publicly traded alternative energy companies have been very volatile, with some 2007 returns in excess of 100%, some 2008 returns down 90% or more, and peak-to-trough returns in 2009 again over 100%. In general there are three sub-segments of \"alternative\" energy investment: solar energy, wind energy and hybrid electric vehicles. Alternative energy sources which are renewable and have lower carbon emissions than fossil fuels are hydropower, wind energy, solar energy, geothermal energy, and bio fuels. Each of these four segments involve very different technologies and investment concerns.\n\nFor example, photovoltaic solar energy is based on semiconductor processing and accordingly, benefits from steep cost reductions similar to those realized in the microprocessor industry (i.e., driven by larger scale, higher module efficiency, and improving processing technologies). PV solar energy is perhaps the only energy technology whose electricity generation cost could be reduced by half or more over the next five years. Better and more efficient manufacturing process and new technology such as advanced thin film solar cell is a good example of that helps to reduce industry cost.\n\nThe economics of solar PV electricity are highly dependent on silicon pricing and even companies whose technologies are based on other materials (e.g., First Solar) are impacted by the balance of supply and demand in the silicon market. In addition, because some companies sell completed solar cells on the open market (e.g., Q-Cells), this creates a low barrier to entry for companies that want to manufacture solar modules, which in turn can create an irrational pricing environment.\n\nIn contrast, because wind power has been harnessed for over 100 years, its underlying technology is relatively stable. Its economics are largely determined by siting (e.g., how hard the wind blows and the grid investment requirements) and the prices of steel (the largest component of a wind turbine) and select composites (used for the blades). Because current wind turbines are often in excess of 100 meters high, logistics and a global manufacturing platform are major sources of competitive advantage. These issues and others were explored in a research report by Sanford Bernstein.\n\nDue to steadily rising gas prices in 2008 with the US national average price per gallon of regular unleaded gas rising above $4.00 at one point, there has been a steady movement towards developing higher fuel efficiency and more alternative fuel vehicles for consumers. In response, many smaller companies have rapidly increased research and development into radically different ways of powering consumer vehicles. Hybrid and battery electric vehicles are commercially available and are gaining wider industry and consumer acceptance worldwide.\n\nFor example, Nissan USA introduced the world's first mass-production electric vehicle, the Nissan Leaf. A plug-in hybrid car, the Chevrolet Volt also has been produced, using an electric motor to drive the wheels, and a small four-cylinder engine to generate additional electricity.\n\nBefore alternative energy becomes mainstream there are a few crucial obstacles that it must overcome. First there must be increased understanding of how alternative energies are beneficial; secondly the availability components for these systems must increase; and lastly the pay-back period must be decreased.\n\nFor example, electric vehicles (EV) and plug-in hybrid electric vehicles (PHEV) are on the rise. The continue adoption of these vehicles depend on investment in public charging infrastructure, as well as implementing much more alternative energy for future transportation.\n\nThere are numerous organizations within the academic, federal, and commercial sectors conducting large scale advanced research in the field of alternative energy. This research spans several areas of focus across the alternative energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.\n\nIn the US, multiple federally supported research organizations have focused on alternative energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners. Sandia has a total budget of $2.4 billion while NREL has a budget of $375 million.\n\nWith the increasing consumption levels of energy, it is projected that the levels would increase by 21% in 2030. The cost of the renewables was relatively cheaper at $2.5m/MW as compared to the non-renewables & 2.7m/MW. Evidently, the use of renewable energy is a cost effective method of obtaining energy. Additionally, their use also dispenses with the trade-off that has existed between environmental conservation and economic growth.\n\nMechanical energy associated with human activities such as blood circulation, respiration, walking, typing and running, is ubiquitous but usually wasted. It has attracted tremendous attention from researchers around the globe to find methods to scavenge such mechanical energies. The best solution currently is to use piezoelectric materials, which can generate flow of electrons when deformed. Various devices using piezoelectric materials have been built to scavenge mechanical energy. Considering that the piezoelectric constant of the material plays a critical role in the overall performance of a piezoelectric device, one critical research direction to improve device efficiency is to find new material of large piezoelectric response. Lead Magnesium Niobate-Lead Titanate (PMN-PT) is a next-generation piezoelectric material with super high piezoelectric constant when ideal composition and orientation are obtained. In 2012, PMN-PT Nanowires with a very high piezoelectric constant were fabricated by a hydro-thermal approach and then assembled into an energy-harvesting device. The record-high piezoelectric constant was further improved by the fabrication of a single-crystal PMN-PT nanobelt, which was then used as the essential building block for a piezoelectric nanogenerator.\n\nSolar energy can be used for heating, cooling or electrical power generation using the sun.\n\nSolar heat has long been employed in passively and actively heated buildings, as well as district heating systems. Examples of the latter are the Drake Landing Solar Community is Alberta, Canada, and numerous district systems in Denmark and Germany. In Europe, there are two programs for the application of solar heat: the Solar District Heating (SDH) and the International Energy Agency's Solar Heating and Cooling (SHC) program.\n\nThe obstacles preventing the large-scale implementation of solar powered energy generation is the inefficiency of current solar technology and the cost. Currently, photovoltaic (PV) panels only have the ability to convert around 16% of the sunlight that hits them into electricity.\n\nBoth Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), have heavily funded solar research programs. The NREL solar program has a budget of around $75 million and develops research projects in the areas of photovoltaic (PV) technology, solar thermal energy, and solar radiation. The budget for Sandia's solar division is unknown, however it accounts for a significant percentage of the laboratory's $2.4 billion budget.\n\nSeveral academic programs have focused on solar research in recent years. The Solar Energy Research Center (SERC) at University of North Carolina (UNC) has the sole purpose of developing a cost-effective solar technology. In 2008, researchers at Massachusetts Institute of Technology (MIT) developed a method to store solar energy by using it to produce hydrogen fuel from water. Such research is targeted at addressing the obstacle that solar development faces of storing energy for use during nighttime hours when the sun is not shining. The Zhangebei National Wind and Solar Energy Storage and Transmission Demonstration Project northwest of Beijing, uses batteries to store 71 MWh, integrating wind and solar energy on the grid with frequency and voltage regulation.\n\nIn February 2012, North Carolina-based Semprius Inc., a solar development company backed by German corporation Siemens, announced that they had developed the world's most efficient solar panel. The company claims that the prototype converts 33.9% of the sunlight that hits it to electricity, more than double the previous high-end conversion rate.\n\nWind energy research dates back several decades to the 1970s when NASA developed an analytical model to predict wind turbine power generation during high winds. Today, both Sandia National Laboratories and National Renewable Energy Laboratory have programs dedicated to wind research. Sandia's laboratory focuses on the advancement of materials, aerodynamics, and sensors. The NREL wind projects are centered on improving wind plant power production, reducing their capital costs, and making wind energy more cost effective overall.\n\nThe Field Laboratory for Optimized Wind Energy (FLOWE) at Caltech was established to research alternative approaches to wind energy farming technology practices that have the potential to reduce the cost, size, and environmental impact of wind energy production.\n\nRenewable energies such as wind, solar, biomass and geothermal combined, supplied 1.3% of global final energy consumption in 2013.\n\nBiomass can be regarded as \"biological material\" derived from living, or recently living organisms. It most often refers to plants or plant-derived materials which are specifically called lignocellulosic biomass. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: \"thermal\", \"chemical\", and \"biochemical\" methods. Wood remains the largest biomass energy source today; examples include forest residues (such as dead trees, branches and tree stumps), yard clippings, wood chips and even municipal solid waste. In the second sense, biomass includes plant or animal matter that can be converted into fibers or other industrial chemicals, including biofuels. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo, and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).\n\nBiomass, biogas and biofuels are burned to produce heat/power and in doing so harm the environment. Pollutants such as sulphurous oxides (SO), nitrous oxides (NO), and particulate matter (PM) are produced from this combustion. The World Health Organisation estimates that 7 million premature deaths are caused each year by air pollution, and biomass combustion is a major contributor of it. The use of biomas is carbon neutral over time, but is otherwise similar to burning fossil fuels.\n\nAs the primary source of biofuels in North America, many organizations are conducting research in the area of ethanol production. On the Federal level, the USDA conducts a large amount of research regarding ethanol production in the United States. Much of this research is targeted toward the effect of ethanol production on domestic food markets.\n\nThe National Renewable Energy Laboratory has conducted various ethanol research projects, mainly in the area of cellulosic ethanol. Cellulosic ethanol has many benefits over traditional corn based-ethanol. It does not take away or directly conflict with the food supply because it is produced from wood, grasses, or non-edible parts of plants. Moreover, some studies have shown cellulosic ethanol to be more cost effective and economically sustainable than corn-based ethanol. Sandia National Laboratories conducts in-house cellulosic ethanol research and is also a member of the Joint BioEnergy Institute (JBEI), a research institute founded by the United States Department of Energy with the goal of developing cellulosic biofuels.\n\nFrom 1978 to 1996, the National Renewable Energy Laboratory experimented with using algae as a biofuels source in the \"Aquatic Species Program.\" A self-published article by Michael Briggs, at the University of New Hampshire Biofuels Group, offers estimates for the realistic replacement of all motor vehicle fuel with biofuels by utilizing algae that have a natural oil content greater than 50%, which Briggs suggests can be grown on algae ponds at wastewater treatment plants. This oil-rich algae can then be extracted from the system and processed into biofuels, with the dried remainder further reprocessed to create ethanol.\n\nThe production of algae to harvest oil for biofuels has not yet been undertaken on a commercial scale, but feasibility studies have been conducted to arrive at the above yield estimate. In addition to its projected high yield, algaculture— unlike food crop-based biofuels — does not entail a decrease in food production, since it requires neither farmland nor fresh water. Many companies are pursuing algae bio-reactors for various purposes, including scaling up biofuels production to commercial levels.\n\nSeveral groups in various sectors are conducting research on Jatropha curcas, a poisonous shrub-like tree that produces seeds considered by many to be a viable source of biofuels feedstock oil. Much of this research focuses on improving the overall per acre oil yield of Jatropha through advancements in genetics, soil science, and horticultural practices. SG Biofuels, a San Diego-based Jatropha developer, has used molecular breeding and biotechnology to produce elite hybrid seeds of Jatropha that show significant yield improvements over first generation varieties. The Center for Sustainable Energy Farming (CfSEF) is a Los Angeles-based non-profit research organization dedicated to Jatropha research in the areas of plant science, agronomy, and horticulture. Successful exploration of these disciplines is projected to increase Jatropha farm production yields by 200-300% in the next ten years.\n\nGeothermal energy is produced by tapping into the heat within the earths crust. It is considered sustainable because that thermal energy is constantly replenished. However, the science of geothermal energy generation is still young and developing economic viability. Several entities, such as the National Renewable Energy Laboratory and Sandia National Laboratories are conducting research toward the goal of establishing a proven science around geothermal energy. The International Centre for Geothermal Research (IGC), a German geosciences research organization, is largely focused on geothermal energy development research.\n\nOver $1 billion has been spent on the research and development of hydrogen fuel in the United States. Both the National Renewable Energy Laboratory and Sandia National Laboratories have departments dedicated to hydrogen research. Much of this work centers on hydrogen storage and fuel cell technologies\n\nThe generation of alternative energy on the scale needed to replace fossil energy, in an effort to reverse global climate change, is likely to have significant negative environmental impacts. For example, biomass energy generation would have to increase 7-fold to supply current primary energy demand, and up to 40-fold by 2100 given economic and energy growth projections. Humans already appropriate 30 to 40% of all photosynthetically fixed carbon worldwide, indicating that expansion of additional biomass harvesting is likely to stress ecosystems, in some cases precipitating collapse and extinction of animal species that have been deprived of vital food sources. The total amount of energy capture by vegetation in the United States each year is around 58 quads (61.5 EJ), about half of which is already harvested as agricultural crops and forest products. The remaining biomass is needed to maintain ecosystem functions and diversity. Since annual energy use in the United States is ca. 100 quads, biomass energy could supply only a very small fraction. To supply the current worldwide energy demand solely with biomass would require more than 10% of the Earth's land surface, which is comparable to the area use for all of world agriculture (i.e., ca. 1500 million hectares), indicating that further expansion of biomass energy generation will be difficult without precipitating an ethical conflict, given current world hunger statistics, over growing plants for biofuel versus food.\n\nGiven environmental concerns (e.g., fish migration, destruction of sensitive aquatic ecosystems, etc.) about building new dams to capture hydroelectric energy, further expansion of conventional hydropower in the United States is unlikely. Windpower, if deployed on the large scale necessary to substitute fossil energy, is likely to face public resistance. If 100% of U.S. energy demand were to be supplied by wind power, about 80 million hectares (i.e., more than 40% of all available farmland in the United States) would have to be covered with wind turbines (50m hub height and 250 to 500 m apart). It is therefore not surprising that the major environmental impact of wind power is related to land use and less to wildlife (birds, bats, etc.) mortality. Unless only a relatively small fraction of electricity is generated by wind turbines in remote locations, it is unlikely that the public will tolerate large windfarms given concerns about blade noise and aesthetics.\n\nBiofuels are different from fossil fuels in regard to net greenhouse gases but are similar to fossil fuels in that biofuels contribute to air pollution. Burning produces airborne carbon particulates, carbon monoxide and nitrous oxides.\n\nRenewable alternative forms of energy have faced opposition from multiple groups, including conservatives and liberals. Around twelve states have passed proposals written to inhibit the alternative energy movement. Kansas lawmakers struck down a bill to phase out renewable energy mandates but face the possibility of the bill reappearing.\n\nThe opposition cites the potentially high cost of branching out to these alternatives in order to support the continuation and reliance on fossil fuels. Ohio's mandate to phase in alternative energy faces opposition who believe higher electricity prices will result, while supporters fear the loss of economic development and jobs that alternative energy could bring.\n\nWith nuclear meltdowns in Chernobyl and Fukushima, nuclear power presents a constant danger and is more unlikely to be a popular alternative source. The costs of maintaining nuclear facilities, the potential risk of meltdowns, and the cost of cleaning up meltdowns are cited as reasons behind the movement away from the use of nuclear energy. In some countries nuclear power plants cannot compete with fossil fuels currently due to the latter's lower price and availability. Nuclear power plants also face competition from the increasing renewable energy subsidies.\n\nSolar panels are an icon of the 'green power' movement, however the process of manufacturing the quartz based panels can be detrimental to the environment. Raw quartz (silica) is used to create solar cells must be mined using harsh chemicals that harm the surrounding environment, as well as those working in the mines. Silicosis is a form of lung disease that is caused by the inhalation of crystalline silica dust resulting in nodule lesions in the lungs. The silica must be cultivated into metallurgical-grade silicon, the process requiring a massive amount of energy as the quartz is placed into electric arc furnaces. The metallurgical grade silica must be processed into polysilicon. This process also produces tetrachloride, a toxic substance that, if not disposed of correctly, can be harmful to the surrounding environment. Hydrochloric acid is formed when tetrachloride interacts with water, lowering water and soil pH. Incidents of tetrachloride spills are common in China, as the production of solar panels has shifted from Europe and the United States to Asian countries within the early 2000s. Because of such, the villagers of Gaolong are unable to leave their homes due to air and soil becoming toxic. This was due to Luoyang Zhongui High-Technology Co. repeatedly dumped tetrachloride in a nearby field for almost a year.\n\n\n"}
{"id": "39141747", "url": "https://en.wikipedia.org/wiki?curid=39141747", "title": "Bio fireplace", "text": "Bio fireplace\n\nA bio fireplace (also on bio-ethanol fireplace, ethanol fireplace) is a type of fireplace or furnace with combined zones generation of heat and technological process – combustion of fuel (denatured alcohol). This fireplace can be installed without a chimney and gives the real flame, not imitation. The main part of each bio fireplace is the burner. The burner is a metal container with various shape and dimensions. The burner is filled with fuel, usually it is bioethanol. Fuel is poured into the burner and lit with an extended candle lighter as suggested. Bio fireplaces should be operated with care, since bioethanol is highly flammable. Severe burn accidents can happen. Bioethanol-fueled decorative fireplaces are dangerous even by intended use. Gases from combustion, like carbon dioxide and nitrogen dioxide, volatile organic compounds, and particulate emissions are released into the room. Bio fireplace - irrespective of the type of fuel used are a source of fine and ultrafine particles and have a considerable influence on the quality of the indoor air if ventilation is not provided. It may generate odors.\n\nEthanol fireplaces are available in several different designs.There are manual ethanol fireplaces and automatic ethanol fireplaces. Automatic ethanol fireplace flames have no direct contact with fuel. Fuel is in stored in a reservoir and then heated until the alcohol vapors evaporate into the burner. These vapors are then ignited by a spark system in the unit. User can turn the flames on or off as well as regulate the flame size with a remote control, mobile app or smart home system. A microprocessor controls burning process, using numerous sensors to keep burning parameters stable. If the sensors detect any issues- such as an earth quake, low oxygen or excessively high temperatures- they will extinguish the flame. Wall mounted designs can be built into a drywall wall or be recessed into the wall. Free standing or stand alone ethanol fireplaces are portable, and can be used in any architectural setting. Table top ethanol fireplaces are the smallest versions of ethanol fireplaces and are often used as a decoration.\n\n"}
{"id": "42739", "url": "https://en.wikipedia.org/wiki?curid=42739", "title": "Bubble fusion", "text": "Bubble fusion\n\nBubble fusion is the non-technical name for a nuclear fusion reaction hypothesized to occur inside extraordinarily large collapsing gas bubbles created in a liquid during acoustic cavitation. The more technical name is sonofusion.\n\nThe term was coined in 2002 with the release of a report by Rusi Taleyarkhan and collaborators that claimed to have observed evidence of sonofusion. The claim was quickly surrounded by controversy, including allegations ranging from experimental error to academic fraud. Subsequent publications claiming independent verification of sonofusion were also highly controversial.\n\nEventually, an investigation by Purdue University found that Taleyarkhan had engaged in falsification of independent verification, and had included a student as an author on a paper when he had not participated in the research. He was subsequently stripped of his professorship. One of his funders, the Office of Naval Research reviewed the report by Purdue and barred him from federal funding for 28 months.\n\nUS patent 4,333,796, filed by Hugh Flynn in 1978, appears to be the earliest documented reference to a sonofusion-type reaction.\n\nIn the March 8, 2002 issue of the peer-reviewed journal \"Science\", Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone () showed measurements of tritium and neutron output consistent with the occurrence of fusion. The neutron emission was also reported to be coincident with the sonoluminescence pulse, a key indicator that its source was fusion caused by the heat and pressure inside the collapsing bubbles.\n\nThe results were so startling, that the Oak Ridge National Laboratory asked two independent researchers, D. Shapira and M. J. Saltmarsh, to repeat the experiment using more sophisticated neutron detection equipment. They reported that the neutron release was consistent with random coincidence. A rebuttal by Taleyarkhan and the other authors of the original report argued that the Shapira and Saltmarsh report failed to account for significant differences in experimental setup, including over an inch of shielding between the neutron detector and the sonoluminescing acetone. According to Taleyarkhan \"et al.\", when properly considering those differences, the results were consistent with fusion.\n\nAs early as 2002, while experimental work was still in progress, Aaron Galonsky of Michigan State University, in a letter to the journal \"Science\"\nexpressed doubts about the claim made by the Taleyarkhan team. In Galonsky's opinion, the observed neutrons were too high in energy to be from a deuterium-deuterium (d-d) fusion reaction. In their response (published on the same page), the Taleyarkhan team provided detailed counter-arguments and concluded that the energy was \"reasonably close\" to that which was expected from a fusion reaction.\n\nIn February 2005 the documentary series \"Horizon\" commissioned two leading sonoluminescence researchers, Seth Putterman and Kenneth S. Suslick, to reproduce Taleyarkhan's work. Using similar acoustic parameters, deuterated acetone, similar bubble nucleation, and a much more sophisticated neutron detection device, the researchers could find no evidence of a fusion reaction.\n\nIn 2004, new reports of bubble fusion were published by the Taleyarkhan group, claiming that the results of previous experiments had been replicated under more stringent experimental conditions. These results differed from the original results in that fusion was claimed to occur over longer times than previously reported. The original report only claimed neutron emission from the initial bubble collapse following bubble nucleation, whereas this report claimed neutron emission many acoustic cycles later.\n\nIn July 2005, two of Taleyarkhan's students at Purdue University published evidence confirming the previous result. They used the same acoustic chamber, the same deuterated acetone fluid and a similar bubble nucleation system. In this report, no neutron-sonoluminescence coincidence was attempted. An article in \"Nature\" raised issues about the validity of the research and complaints from his Purdue colleagues (see full analysis elsewhere in this page). Charges of misconduct were raised, and Purdue University opened an investigation. It concluded in 2008 that Taleyarkhan's name should have appeared in the author list because of his deep involvement in many steps of the research, that he added one author that had not really participated in the paper just to overcome the criticism of one reviewer, and that this was part of an attempt of \"an effort to falsify the scientific record by assertion of independent confirmation\". The investigation did not address the validity of the experimental results.\n\nIn January 2006, a paper published in the journal \"Physical Review Letters\" by Taleyarkhan in collaboration with researchers from Rensselaer Polytechnic Institute reported statistically significant evidence of fusion.\n\nIn November 2006, in the midst of accusations concerning Taleyarkhan's research standards, two different scientists visited the meta-stable fluids research lab at Purdue University to measure neutrons, using Taleyarkhan's equipment. Dr. Edward R. Forringer and undergraduates David Robbins and Jonathan Martin of LeTourneau University presented two papers at the American Nuclear Society Winter Meeting that reported replication of neutron emission. Their experimental setup was similar to previous experiments in that it used a mixture of deuterated acetone, deuterated benzene, tetrachloroethylene and uranyl nitrate. Notably, however, it operated without an external neutron source and used two types of neutron detectors. They claimed a liquid scintillation detector measured neutron levels at 8 standard deviations above the background level, while plastic detectors measured levels at 3.8 standard deviations above the background. When the same experiment was performed with non-deuterated control liquid, the measurements were within one standard deviation of background, indicating that the neutron production had only occurred during cavitation of the deuterated liquid. William M. Bugg, emeritus physics professor at the University of Tennessee also traveled to Taleyarkhan's lab to repeat the experiment with his equipment. He also reported neutron emission, using plastic neutron detectors. Taleyarkhan claimed these visits counted as independent replications by experts, but Forringer later recognized that he was not an expert, and Bugg later said that Taleyarkhan performed the experiments and he had only watched.\n\nIn March 2006, \"Nature\" published a special report that called into question the validity of the results of the Purdue experiments. The report quotes Brian Naranjo of the University of California, Los Angeles to the effect that neutron energy spectrum reported in the 2006 paper by Taleyarkhan, et al. was statistically inconsistent with neutrons produced by the proposed fusion reaction and instead highly consistent with neutrons produced by the radioactive decay of Californium 252, an isotope commonly used as a laboratory neutron source .\n\nThe response of Taleyarkhan \"et al.\", published in \"Physical Review Letters\", attempts to refute Naranjo's hypothesis as to the cause of the neutrons detected.\n\nTsoukalas, head of the School of Nuclear Engineering at Purdue, and several of his colleagues at Purdue, had convinced Taleyarkhan to move to Purdue and attempt a joint replication. In the 2006 \"Nature\" report they detail several troubling issues when trying to collaborate with Taleyarkhan. He reported positive results from certain set of raw data, but his colleagues had also examined that set and it only contained negative results. He never showed his colleagues the raw data corresponding to the positive results, despite several requests. He moved the equipment from a shared laboratory to his own laboratory, thus impeding review by his colleagues, and he didn't give any advance warning or explanation for the move. Taleyarkhan convinced his colleagues that they shouldn't publish a paper with their negative results. Taleyarkhan then insisted that the university's press release present his experiment as \"peer-reviewed\" and \"independent\", when the co-authors were working in his laboratory under his supervision, and his peers in the faculty were not allowed to review the data. In summary, Taleyarkhan's colleagues at Purdue said he placed obstacles to peer review of his experiments, and they had serious doubts about the validity of the research.\n\n\"Nature\" also revealed that the process of anonymous peer-review had not been followed, and that the journal \"Nuclear Engineering and Design\" was not independent from the authors. Taleyarkhan was co-editor of the journal, and the paper was only peer-reviewed by his co-editor, with Taleyarkhan's knowledge.\n\nIn 2002 Taleyarkhan filed a patent application on behalf of the United States Department of Energy, while working in Oak Ridge. \"Nature\" reported that the patent had been rejected in 2005 by the US Patent Office. The examiner called the experiment a variation of discredited cold fusion, found that there was \"no reputable evidence of record to support any allegations or claims that the invention is capable of operating as indicated\", and found that there was not enough detail for others to replicate the invention. The field of fusion suffered from many flawed claims, thus the examiner asked for additional proof that the radiation was generated from fusion and not from other sources. An appeal was not filed because the Department of Energy had dropped the claim in December 2005.\n\nDoubts among Purdue University's Nuclear Engineering faculty as to whether the positive results reported from sonofusion experiments conducted there were truthful prompted the university to initiate a review of the research, conducted by Purdue's Office of the Vice President for Research. In a March 9, 2006 article entitled \"Evidence for bubble fusion called into question\", \"Nature\" interviewed several of Taleyarkhan's colleagues who suspected something was amiss.\n\nOn February 7, 2007, the Purdue University administration determined that \"the evidence does not support the allegations of research misconduct and that no further investigation of the allegations is warranted\". Their report also stated that \"vigorous, open debate of the scientific merits of this new technology is the most appropriate focus going forward.\" In order to verify that the investigation was properly conducted, House Representative Brad Miller requested full copies of its documents and reports by March 30, 2007. His congressional report concluded that \"Purdue deviated from its own procedures in investigating this case and did not conduct a thorough investigation\"; in response, Purdue announced that it would re-open its investigation.\n\nIn June 2008, a multi-institutional team including Taleyarkhan published a paper in Nuclear Engineering and Design to \"clear up misconceptions generated by a webposting of UCLA which served as the basis for the \"Nature\" article of March 2006\", according to a press release.\n\nOn July 18, 2008, Purdue University announced that a committee with members from five institutions had investigated 12 allegations of research misconduct against Rusi Taleyarkhan. It concluded that two allegations were founded—that Taleyarkhan had claimed independent confirmation of his work when in reality the apparent confirmations were done by Taleyarkhan's former students and was not as \"independent\" as Taleyarkhan implied, and that Taleyarkhan had included a colleague's name on one of his papers who had not actually been involved in the research (\"the sole apparent motivation for the addition of Mr. Butt was a desire to overcome a reviewer's criticism\", the report concluded).\n\nTaleyarkhan's appeal of the report's conclusions was rejected. He said the two allegations of misconduct were trivial administrative issues and had nothing to do with the discovery of bubble nuclear fusion or the underlying science, and that \"all allegations of fraud and fabrication have been dismissed as invalid and without merit — thereby supporting the underlying science and experimental data as being on solid ground\". A researcher questioned by the LA Times said that the report had not clarified whether bubble fusion was real or not, but that the low quality of the papers and the doubts cast by the report had destroyed Taleyarkhan's credibility with the scientific community.\n\nOn August 27, 2008 he was stripped of his named Arden Bement Jr. Professorship, and forbidden to be a thesis advisor for graduate students for at least the next 3 years.\n\nDespite the findings against him, Taleyarkhan received a $185,000 grant from the National Science Foundation between September 2008 and August 2009 to investigate bubble fusion. In 2009 the Office of Naval Research debarred him for 28 months, until September 2011, from receiving U.S. Federal Funding. During that period his name was listed in the 'Excluded Parties List' to prevent him from receiving further grants from any government agency.\n\n\n"}
{"id": "491964", "url": "https://en.wikipedia.org/wiki?curid=491964", "title": "Cat's cradle", "text": "Cat's cradle\n\nCat's cradle is one of the oldest games in recorded human history, and involves creating various string figures, either individually or by passing a loop of string back and forth between two or more players. The true origin of the name is debated, though the first known reference is in \"The light of nature pursued\" by Abraham Tucker in 1768. The type of string, the specific figures, their order, and the names of the figures vary. Independent versions of this game have been found in indigenous cultures throughout the world, including in Africa, Eastern Asia, the Pacific Islands, Australia, the Americas, and the Arctic.\n\nThe game consists of two or more players making a sequence of string figures, each altering the figure made by the previous player. The game begins with one player making the eponymous figure \"Cat's Cradle\" (above). After each figure, the next player manipulates that figure and removes the string figure from the hands of the previous player with one of a few simple motions and tightens the loop to create another figure, for example, \"Diamonds\". \"Diamonds\" might then lead to \"Candles\", for example, and then \"Manger\"—an inverted \"Cat's Cradle\"—and so on. Most of the core figures allow a choice between two or more subsequent figures: for example, \"Fish in a Dish\" can become \"Cat's Eye\" or \"Manger\". The game ends when a player makes a mistake or creates a dead-end figure, such as \"Two Crowns\", which cannot be turned into anything else.\n\nThe game also may be played solo, as is done in Japan.\n\nThe origin of the name \"cat's cradle\" is debated but the first known reference is in \"The light of nature pursued\" by Abraham Tucker in 1768. \"An ingenious play they call cat's cradle; one ties the two ends of a packthread together, and then winds it about his fingers, another with both hands takes it off perhaps in the shape of a gridiron, the first takes it from him again in another form, and so on alternately changing the packthread into a multitude of figures whose names I forget, it being so many years since I played at it myself.\"The name may have come from a corruption of cratch-cradle, or manger cradle (although this derivation is disputed by the OED). The connection between the two words, \"cratches\" and \"cradle\", may come from the Christian story of the birth of Jesus, in which a manger is used as a cradle.\n\nIn an 1858 \"Punch\" cartoon it is referred to as \"scratch cradle\", a name supported by Brewer's 1898 \"Dictionary\". As \"Cat's cradle\" often is used to refer to string figures and games in general, Jayne uses \"Real Cat's-Cradle\" to refer to the specific game.\n\nDifferent cultures have different names for the game, and often different names for the individual figures. The French word for manger is \"crèche\", and cattle feed racks are still known as \"cratches\". In Russia the whole game is called simply, \"the game of string\", and the \"diamonds\" pattern is called \"carpet\", with other pattern names such as \"field\", \"fish\", and \"sawhorse\" for the other figures—a \"cat\" isn't mentioned. The game may have originated in China. In China the game is called \"fan sheng\" (), or catch cradle. In some regions of the U.S., this game also is known as \"Jack in the Pulpit\".\n\nGeneva Hultenius, Maryann DiVona, and Rita Divona completed 21,200 changes of cat's cradles in 21 hours in Chula Vista, California between August 17–18, 1974. The \"Guinness Book of World Records\" reported it as a world record in the 1975 and 1976 editions.\n\nJane Muir and Robyn Lawrick completed 22,700 changes of cat's cradles in 21 hours at Calgary Market Mall, Alberta, Canada on August 25, 1976.\n\n\n"}
{"id": "21716908", "url": "https://en.wikipedia.org/wiki?curid=21716908", "title": "Ceramic heat cell", "text": "Ceramic heat cell\n\nA ceramic heat cell, also known as Caloric Porous Structure Cell (CPSC), is a ceramic device to burn a fuel without flame, converting the thermal energy to mechanical work. In principle any fuel that can be vapourised and pre-mixed with air can be used. \"Ultra vaporized steam\" replaces the exploding air/fuel mixture as the work medium. The flameless combustion keeps emissions of hydrocarbons, carbon monoxide, and nitrous oxides extremely low. Exhaust gas can be recirculated, which further reduces emissions.\n\nA ceramic heat cell made by German firm Enginion is claimed to generate up to 30 megawatts of power per cubic meter, operating at a moderate temperature of 1200 °C, with output varying from 5% to 100% of this, responding in 5 milliseconds. Emissions are claimed to be well below 10 parts per million even without exhaust gas recirculation. A glow plug is used to start the cell.\n"}
{"id": "1170160", "url": "https://en.wikipedia.org/wiki?curid=1170160", "title": "Chirality (mathematics)", "text": "Chirality (mathematics)\n\nIn geometry, a figure is chiral (and said to have chirality) if it is not identical to its mirror image, or, more precisely, if it cannot be mapped to its mirror image by rotations and translations alone. An object that is not chiral is said to be achiral. In 3 dimensions, not all achiral objects have a mirror plane. For example, a 3-dimensional object with inversion centre as its only nontrivial symmetry operation is achiral but has no mirror plane.\n\nA chiral object and its mirror image are said to be enantiomorphs. The word \"chirality\" is derived from the Greek (cheir), the hand, the most familiar chiral object; the word \"enantiomorph\" stems from the Greek (enantios) 'opposite' + (morphe) 'form'. A non-chiral figure is called achiral or amphichiral.\n\nSome chiral three-dimensional objects, such as the helix, can be assigned a right or left handedness, according to the right-hand rule.\n\nMany other familiar objects exhibit the same chiral symmetry of the human body, such as gloves and shoes. Right shoes differ from left shoes only by being mirror images of each other. In contrast thin gloves may not be considered chiral if you can wear them inside-out.\n\nThe J, L, S and Z-shaped \"tetrominoes\" of the popular video game Tetris also exhibit chirality, but only in a two-dimensional space. Individually they contain no mirror symmetry in the plane.\n\nA figure is achiral if and only if its symmetry group contains at least one \"orientation-reversing\" isometry. (In Euclidean geometry any isometry can be written as formula_1 with an orthogonal matrix formula_2 and a vector formula_3. The determinant of formula_2 is either 1 or −1 then. If it is −1 the isometry is \"orientation-reversing\", otherwise it is orientation-preserving.)\n\nSee for a full mathematical definition of chirality.\n\nIn three dimensions, every figure that possesses a mirror plane of symmetry \"S\", an inversion center of symmetry \"S\", or a higher improper rotation (rotoreflection) \"S\" axis of symmetry is achiral. (A \"plane of symmetry\" of a figure formula_5 is a plane formula_6, such that formula_5 is invariant under the mapping formula_8, when formula_6 is chosen to be the formula_10-formula_11-plane of the coordinate system. A \"center of symmetry\" of a figure formula_5 is a point formula_13, such that formula_5 is invariant under the mapping formula_15, when formula_13 is chosen to be the origin of the coordinate system.) Note, however, that there are achiral figures lacking both plane and center of symmetry. An example is the figure\n\nwhich is invariant under the orientation reversing isometry formula_18 and thus achiral, but it has neither plane nor center of symmetry. The figure\n\nalso is achiral as the origin is a center of symmetry, but it lacks a plane of symmetry.\n\nNote also that achiral figures can have a center axis.\n\nIn two dimensions, every figure which possesses an axis of symmetry is achiral, and it can be shown that every \"bounded\" achiral figure must have an axis of symmetry. (An \"axis of symmetry\" of a figure formula_5 is a line formula_21, such that formula_5 is invariant under the mapping formula_23, when formula_21 is chosen to be the formula_10-axis of the coordinate system.) For that reason, a triangle is achiral if it is equilateral or isosceles, and is chiral if it is scalene.\n\nConsider the following pattern:\n\nThis figure is chiral, as it is not identical to its mirror image:\n\nBut if one prolongs the pattern in both directions to infinity, one receives an (unbounded) achiral figure which has no axis of symmetry. Its symmetry group is a frieze group generated by a single glide reflection.\n\nA knot is called achiral if it can be continuously deformed into its mirror image, otherwise it is called a chiral knot. For example, the unknot and the figure-eight knot are achiral, whereas the trefoil knot is chiral.\n\n\n"}
{"id": "3684088", "url": "https://en.wikipedia.org/wiki?curid=3684088", "title": "Color superconductivity", "text": "Color superconductivity\n\nColor superconductivity is a phenomenon predicted to occur in quark matter if the baryon density is sufficiently high (well above nuclear density) and the temperature is not too high (well below 10 kelvin). Color superconducting phases are to be contrasted with the normal phase of quark matter, which is just a weakly interacting Fermi liquid of quarks.\n\nIn theoretical terms, a color superconducting phase is a state in which the quarks near the Fermi surface become correlated in Cooper pairs, which condense. In phenomenological terms, a color superconducting phase breaks some of the symmetries of the underlying theory, and has a very different spectrum of excitations and very different transport properties from the normal phase.\n\nIt is well known that at low temperature many metals become superconductors. A metal can be viewed as a Fermi liquid of electrons, and below a critical temperature, an attractive phonon-mediated interaction between the electrons near the Fermi surface causes them to pair up and form a condensate of Cooper pairs, which via the Anderson-Higgs mechanism makes the photon massive, leading to the characteristic behaviors of a superconductor; infinite conductivity and the exclusion of magnetic fields (Meissner effect). The crucial ingredients for this to occur are:\nThese ingredients are also present in sufficiently dense quark matter, leading physicists to expect that something similar will happen in that context:\n\nThe fact that a Cooper pair of quarks carries a net color charge, as well as a net electric charge, means that some of the gluons (which mediate the strong interaction just as photons mediate electromagnetism) become massive in a phase with a condensate of quark Cooper pairs, so such a phase is called a \"color superconductor\". Actually, in many color superconducting phases the photon itself does not become massive, but mixes with one of the gluons to yield a new massless \"rotated photon\". This is an MeV-scale echo of the mixing of the hypercharge and W bosons that originally yielded the photon at the TeV scale of electroweak symmetry breaking.\n\nUnlike an electrical superconductor, color-superconducting quark matter comes in many varieties, each of which is a separate phase of matter. This is because quarks, unlike electrons, come in many species. There are three different colors (red, green, blue) and in the core of a compact star we expect three different flavors (up, down, strange), making nine species in all. Thus in forming the Cooper pairs there is a 9×9 color-flavor matrix of possible pairing patterns. The differences between these patterns are very physically significant: different patterns break different symmetries of the underlying theory, leading to different excitation spectra and different transport properties.\n\nIt is very hard to predict which pairing patterns will be favored in nature. In principle this question could be decided by a QCD calculation, since QCD is the theory that fully describes the strong interaction. In the limit of infinite density, where the strong interaction becomes weak because of asymptotic freedom, controlled calculations can be performed, and it is known that the favored phase in three-flavor quark matter is the \"color-flavor-locked\" phase. But at the densities that exist in nature these calculations are unreliable, and the only known alternative is the brute-force computational approach of lattice QCD, which unfortunately has a technical difficulty (the \"sign problem\") that renders it useless for calculations at high quark density and low temperature.\n\nPhysicists are currently pursuing the following lines of research on color superconductivity:\n\nThe only known place in the universe where the baryon density might possibly be high enough to produce quark matter, and the temperature is low enough for color superconductivity to occur, is the core of a compact star (often called a \"neutron star\", a term which prejudges the question of its actual makeup). There are many open questions here:\n\n\n"}
{"id": "42649528", "url": "https://en.wikipedia.org/wiki?curid=42649528", "title": "Commercial uses of armor", "text": "Commercial uses of armor\n\nArmor has been used in the military for a long period of time during the course of history, but is becoming more frequently seen in the public sector as time passes. There are many different forms and ways that armor is being commercially used throughout the world today. The most popular and well-known uses are body and vehicle armor. There are other commercial uses including aircraft armor and armored glass.\n\nAramid fiber is the generic name of a group of synthetic fibers which are used in vehicle armor, fire equipment, tires, and bulletproof vests. The material has extremely high strength while still being flexible, and is very resistant to heat (it does not melt). In 2011 worldwide production of aramids was more than 60,000 tons.\n\nThe Para-aramid with the most production worldwide is Kevlar. It was commerciality introduced by Du Pont in 1972. In 1998 Kevlar accounted for 85% of the global market of para-aramid fibers. Kevlar is the material most often used in bulletproof vests. Different forms of Kevlar are anywhere from five to twenty times stronger than steel per weight. The disadvantages of Kevlar are that it degrades with exposure to UV light, is expensive to produce, and is difficult to shape.\n\nMeta aramids are aramid compounds that have a different molecular structure, giving them different physical properties to para aramid fibers. The most popular product of this kind is Nomex. Nomex has less strength than Kevlar, but it has extremely high heat, flame, ultraviolet, chemical, and radiation resistance. Meta aramids are commonly found in firefighting equipment, as well as military pilot suits and helmets.\n\nDragon Skin armor uses multiple ceramic and titanium composite two-inch discs that overlap, forming an armor that looks like scales of a dragon. This type of structure allows the vests to stay relatively light and flexible. Dragon skin has two main benefits over other types of body armor: It can take multiple hits from bullets and it doesn’t compromise the entire structure, and it deals with the blunt force of a bullet or projectile much better than most other types of body armor because the impact energy of a bullet can disperse over a larger area. In a test, using 5.56 mm, 7.62mm, and 9mm steel core, armor-piercing ammunition, and with more than a dozen shots, the dragon skin armor remains completely intact and no bullet penetrated through.\n\nSpider silk has some amazing natural properties and is one of nature’s strongest materials. Spider’s dragline silk has a tensile strength (force by which it can be stretched without breaking) similar to alloy steel, or about half as strong as Kevlar. Weight for weight it is five times stronger than steel. One apparent difference between the other armor materials is that the silk can be stretched up to 5 times its relaxed length without breaking. Darwin's bark spider produces a silk that is more than 10 times tougher than Kevlar and is the toughest biological material ever studied. Spider’s silk elasticity and strength allow for unique applications compared to the aramid fibers, for example it could be used as a protective body armor that would allow for coverage of the entire head due to its flexibility. It has the potential to be used in numerous military, construction, and medical applications.\n\nFollowing the bombing of the Pan Am Flight 103 in 1988 the Transportation Security Laboratories have been developing ways to reduce the damage to an airplane by placing a hardened film around the cargo bay and overhead compartments. They have also changed the shape of the cargo bay to provide more security and to reduce the force of any explosion. \nTo improve the circumstances in a case where an aircraft turbine engine fails, the Federal Aviation Administration (FAA) is working to design specific armor to protect the vital parts of the plane to assure safe flying until landing is accomplished. This armored barrier would prevent fragments from engine failure from damaging other sections of the airplane. High strength polymer fibers have been found to be the most effective material for this specific use.\n\nArmor Glass International, Inc. was founded by Michael Fjetland, BBA/JD, to \"armor glass\" under the trademark Armor Glass® to provide security from breach of the glass by natural disasters, explosions, burglars, hurricanes, tornadoes, hail, golf balls or other harmful events. One of the main products offered by this company is security film. This type of film is 8 mil thick, is rated for a Large Missile Impact (Level C 4.5 lb.) and is placed on the inside of a window or other source of glass, the weakest link of every building, to create a more durable and defensive layer. Studies have shown that breach of a window by wind-borne debris hurled by hurricane-force winds is what leads to roof uplift and structural collapse. This protective film is used on many buildings in Washington D.C. such as the Pentagon, Smithsonian, Congress, etc. but is also used commercially throughout the world for any person or company striving for extra protection against specific unpredictable encounters.\n\nThe average person who purchases an automobile according to the International Armoring Corporation fits into at least one of the three categories. They have money, believe there is a genuine threat against their lives, or are a government official. Upon receiving a down payment by the client the armoring process begins. At the beginning of the process the company assigned with the project meets with the client. This meeting is to determine how the automobile should be customized to fit the client’s needs based on type of car, threat level, and defensive options. When determining type of car the customer has many options as any automobile can be armored. However, the weight of the armor can vary from 500 to 2000 pounds requiring special suspension and engines upgrades to be installed. After the car type is chosen the customer is asked about their perceived threat level. This helps the manufactures to determine what ballistic protection level the car needs customized for (See also, International Armoring Corporation). Ballistic protection levels range Type 1 to Type IV and are governed by National Institute of Justice Standard 0108.01. Once the preliminary review is completed and the specifications are finalized the manufacture begin the project.\n\nAccording to patent US 4352316 A* there are several steps when it comes to armoring a civilian automobile. First, the automobile is stripped of its interior. Second, door frames are rebuilt to include armor plating and bullet proof windows are added. Third, the vertical portions, top, and bottom of the automobile are enforced with armoring plating. Finally, the car’s battery and engine are encased in armor plating. The objective of the plating is to prevent bullets from penetrating the automobile and entering into the passenger cabin. During the installation process various materials are used including; bulletproof glass, ballistic nylon, run flat inserts, and Lexan. Outside of the basic armoring package several defensive options are available to help improve the security of the automobile comprising; dual battery system, DVR security camera system, electric door handles, flashing front strobe lights, night vision systems, self-sealing fuel tank, and siren/loudspeaker system, etc.\n\nThe US body armor industry is worth $802 million a year with a decrease just over nine percent in the last five years according to a market research done by IBISWorld. This is the result of the conclusion of the war in Afghanistan and the withdrawal of troops. There are currently around 80 companies in the US that are specializing in body armor from head to toe. The top four companies are said to control almost half of the market. The market is expected to come back from this 9% low due to needs for law enforcement and other private security firms. Just the body armor industry alone profits 39.3 million in profit. The military takes the majority with 72%, law enforcement take 14.2%, the commercial use has the remainder of the 13.8%. The report states that the use of robots has reduced the need for body armor in highly dangerous situations.\n\nIn a report done by Ibisworld.com commercial uses of vehicle armor only share an 8% of the 7.2 billion dollar industry. The market has been on a steady decline, 12% over the past five years and expected to drop another 2% over the next five years. 68% of the market is taken by the military and government. It profits just under 1 billion dollars a year. Exports of commercial armored vehicles are on the rise the majority of the exports go the United Arab Emirates about 29%. Most of the other majority are exported to the middle east.\n\n\n"}
{"id": "54474095", "url": "https://en.wikipedia.org/wiki?curid=54474095", "title": "Contained earth", "text": "Contained earth\n\nContained earth (CE) is a structurally designed natural building material that combines containment, inexpensive reinforcement, and strongly cohesive earthen walls. CE is earthbag construction that can be calibrated for several seismic risk levels based on building soil strength and plan standards for adequate bracing.\n\nThere is a recognized need for structural understanding of alternative building materials. Construction guidelines for CE are currently under development, based on the New Zealand's performance-based code for adobe and rammed earth. \n\nCE is differentiated from contained gravel (CG) or contained sand (CS) by the use of damp, tamped, cured cohesive fill. CE can be modular, built in poly-propylene rice bag material containers, or solid, built in mesh tubing that allows earthen fill to solidify between courses. \nCG, filled with pumice or ordinary gravel and/ or small stones, is often used as water-resistant base walls under CE, which also provides an effective capillary break. Soilbags used mostly in horizontal applications by civil engineers contain loose fill which includes both CG and CS. CG courses, like soilbags, may contribute base isolation and/or vibration damping qualities, although out-of-plane strength needs research. \n\nFor clarity, earthbag built with a low cohesion fill, or filled with dry soil that does not solidify, is not CE but CS. Uncured CE also performs structurally like CS.\n\nBuilders used to working without engineers are proud of earthbag's unlimited variations. Few trainers discuss risk levels of building sites, or recommend accurate tests of soil strength, even though soil strength is a key factor of improved seismic performance for earthen walls. \n\nNeed for or use of metal components are disputed, including rebar hammered into walls and barbed wire between courses, although static friction of smooth bag-to-bag surfaces of heavy modular CE walls is 0.4 with no adhesion. \n\nEngineering knowledge of earthbag has been growing. More is known about the performance of walls made with sand or dry or uncured soil than about the overwhelming majority of earthbag buildings which have used damp, cohesive soil fill. Reports based on tests of soilbags and loose or granular fill (or uncured fill) assumes that soil strength is less important to wall strength than bag fabric strength for. However, shear tests show clearly that stronger cured, cohesive fill increases contained earth wall strength substantially.\n\nEarthbag developed gradually without structural analysis, first for small domes, then for vertical wall buildings of many shapes. Although domes passed structural testing in California, no structural information was extracted from tests of the inherently stable shapes. Builders borrowed guidelines for adobe to recommend plan details, but code developed in low seismic risk New Mexico does not address issues for higher risk areas. California's seismic risk levels are almost three times as high as New Mexico's, and risk worldwide rises much higher. \n\nEarthbag is often tried after disasters in the developing world, including Sri Lanka's 2004 tsunami, Haiti's 2010 earthquake and Nepal's 2015 earthquake. \n\nCE walls fail in shear tests when barbs flex or bend back or (with weak soil fill) by chipping cured bag fill. CS walls or uncured CE walls fail differently, by slitting bag fabric as barbs move through loose fill.\n\nBecause no earthbag buildings were seriously damaged by seismic motion up to 0.8 g in Nepal's 2015 quakes, Nepal's building code recognizes earthbag, although the code does not discuss soil strengths or improved reinforcement. Nepal requires buildings to resist 1.5 g risk although hazard maps show higher values. Better trainers assume the use of cohesive soil and barbed wire, and recommend vertical rebar, buttresses, and bond beams, but rule of thumb earthbag techniques should be differentiated from contained earth that follows more complete guidelines.\n\nEarthquake damage results confirm the validity of New Zealand's detailed standards for non-engineered adobe and rammed earth which allow unreinforced buildings to 0.6 g force levels.\n\nAlthough earthbag without specific guidelines may often be this strong, conventional adobe can have severe damage at levels below 0.2 g forces. Non-traditional earthbag built with barbed wire, barely cohesive soil and no rebar can have half the shear strength of NZ's unreinforced adobe. Somewhere between 0.3 and 0.6 g forces, CE guidelines become important.\n\nBased on static shear testing (Stouter, P. May 2017):\nThe following approximate guidelines assume a single story of wide walls with 2 strands of 4 point barbed wire per course. Check NZS 4299 for bracing wall spacing and size of bracing walls and/ or buttresses. Vertical rebar must be spaced on center average and embedded in wall fill while damp. Follow NZS 4299 restrictions on building size, site slope, climate, and uses.\nDiscuss foundation concerns with an engineer, since NZS 4299 assumes a full reinforced concrete footing.\n\nFor comparison to NZS 4299 the following risk levels are based roughly on 0.2 second spectral acceleration (Ss) from 2% probability of exceedance in 50 years. Builders may refer to the Unified Facilities Handbook online for these values for some cities worldwide. These risk levels are based on ultimate strength, but deformation limits may require stiffer detailing or lower risk levels.\n\nMedium strength soil: unconfined compressive strength\n\nStrong soil: unconfined compressive strength\n\nAdditional research and engineering analysis is needed to create valid CE manuals.\n"}
{"id": "45197932", "url": "https://en.wikipedia.org/wiki?curid=45197932", "title": "Cramer–Castillon problem", "text": "Cramer–Castillon problem\n\nIn geometry, the Cramer–Castillon problem is a problem stated by the Swiss mathematician Gabriel Cramer solved by the italian mathematician, resident in Berlin, Jean de Castillon in 1776.\n\nThe problem consists of (see the image):\n\nGiven a circle formula_1 and three points formula_2 in the same plane and not on formula_1, to construct every possible triangle inscribed in formula_1 whose sides (or their elongations) pass through formula_2 respectively.\n\nCenturies before, Pappus of Alexandria had solved a special case: when the three points are collinear. But the general case had the reputation of being very difficult.\n\nAfter the geometrical construction of Castillon, Lagrange found an analytic solution, easier than Castillon's. In the beginning of the 19th century, Lazare Carnot generalized it to formula_6 points.\n\n"}
{"id": "46250924", "url": "https://en.wikipedia.org/wiki?curid=46250924", "title": "Cubical set", "text": "Cubical set\n\nIn topology, a branch of mathematics, a cubical set is a set-valued contravariant functor on the category of (various) \"n\"-cubes. See the references for the more precise definitions.\n\n\n"}
{"id": "23001219", "url": "https://en.wikipedia.org/wiki?curid=23001219", "title": "Diffusing-wave spectroscopy", "text": "Diffusing-wave spectroscopy\n\nDiffusing-wave spectroscopy (DWS) is an optical technique derived from dynamic light scattering (DLS) that studies the dynamics of scattered light in the limit of strong multiple scattering. It has been widely used in the past to study colloidal suspensions, emulsions, foams, gels, biological media and other forms of soft matter. If carefully calibrated, DWS allows the quantitative measurement of microscopic motion in a soft material, from which the rheological properties of the complex medium can be extracted via the microrheology approach.\n\nLaser light is sent to the sample and the outcoming transmitted or backscattered light is detected by an optoelectric sensor. The light intensity detected is the result of the interference of all the optical waves coming from the different light paths.\n\nThe signal is analysed by calculating the intensity autocorrelation function called g. \nformula_1\n\nFor the case of non-interacting particles suspended in a (complex) fluid a direct relation between g-1 and the mean squared displacement of the particles <Δr> can be established. Let's note P(s) the probability density function (PDF) of the photon path length s. The relation can be written as follows:\n\nformula_2\n\nwith formula_3 and formula_4 is the transport mean free path of scattered light.\n\nFor simple cell geometries, it is thus possible to calculate the mean squared displacement of the particles <Δr> from the measured g-1 values analytically. For example, for the backscattering geometry, an infinitely thick cell, large laser spot illumination and detection of photons coming from the center of the spot, the relationship between g-1 and <Δr> is:\n\nformula_5, γ value is around 2.\n\nFor less thick cells and in transmission, the relationship depends also on l* (the transport length).\n\nThis technique either uses a camera to detect many speckle grains (see speckle pattern) or a ground glass to create a large number of speckle realizations (Echo-DWS ). In both cases an average over a large number of statistically independent intensity values is obtained, allowing a much faster data acquisition time.\nformula_6\n\nMSDWS is particularly adapted for the study of slow dynamics and non ergodic media. Echo-DWS allows seamless integration of MSDWS in a traditional DWS-scheme with superior temporal resolution down to 12 ns. Camera based adaptive image processing allows online measurement of particle dynamics for example during drying.\n\n"}
{"id": "4316600", "url": "https://en.wikipedia.org/wiki?curid=4316600", "title": "Eagle-bone whistle", "text": "Eagle-bone whistle\n\nThe eagle bone whistle is a highly sacred religious object, used by some members of Native American spiritual societies in particularly sacred ceremonies. They are made from bones of either the American bald eagle or the American golden eagle, and are considered extremely powerful spiritual objects.\n\nEagle bone whistles are only used in certain ceremonies in the Southwest and Plains cultures. The eagle bone whistle may be considered as a ceremonial or sacred object which may not be considered a musical instrument, if music is defined as entertainment: \"There is no time or need...to wallow in distinctions between a feather-and-bone raptor and a bone whistle avian mysticism; one would no doubt end in dichotomous Western readings thereof.\"\n\nThe whistle is used in some Peyote ceremonies of some sects of the Native American Church. Eagle bone whistles are used in a number of Sun Dance cultures, such as the Crow. The eagle-bone whistle is also used by the Lakota people in certain ceremonies, such as some Sun Dances.\n\nNavajo/Ute flutist R. Carlos Nakai claims to use an \"eagle-bone whistle\" (or possibly an imitation one) on multiple albums.\n\nBoth the bald and golden eagle are protected by federal law: the Migratory Bird Treaty Act of 1918 (MBTA) prohibits the taking, killing, possession, transportation, and importation of migratory birds, their eggs, parts, and nests except as authorized under a valid permit as outlined at 50 CFR 21.11 The MBTA authorizes and directs the Secretary of the Interior to determine if, and by what means, the hunting of migratory birds should be allowed, as well as to adopt and implement suitable regulations permitting and governing the hunting of any type of migratory bird (for example, hunting seasons for ducks and geese). The Eagle feather law is another name for the exemptions to this act that are sometimes granted to enrolled members of federally recognized Native American tribes. Penalties under the MBTA include a maximum of two years imprisonment and $250,000 fine for a felony conviction and six months imprisonment or $5,000 fine for a misdemeanor conviction. Fines double if the violator is an organization rather than an individual. These laws would apply to the collection and use of eagle bone whistles.\n\n"}
{"id": "43400705", "url": "https://en.wikipedia.org/wiki?curid=43400705", "title": "Ekeby oak tree", "text": "Ekeby oak tree\n\nThe Ekeby oak tree () is an oak tree in Ekerö outside Stockholm, Sweden, close to Ekebyhov Castle. It is the largest living deciduous tree in Sweden by volume. \n\nThe Ekeby oak is approximately 500 years old. It was declared a natural monument in 1956. There are many old trees around Ekebyhov Castle; the oak, sometimes called \"Ekeröjätten\" (the Ekerö giant) stands alone in a field south of the castle, where it had no competition for space from other trees. It was measured in 2008 as the largest tree by volume in Sweden.\n"}
{"id": "40059247", "url": "https://en.wikipedia.org/wiki?curid=40059247", "title": "Energy democracy", "text": "Energy democracy\n\nEnergy democracy is a political, economic, social and cultural concept that merges the technological energy transition with a strengthening of democracy and public participation. The concept is connected with an ongoing decentralization of energy systems with energy efficiency and renewable energy being used also for a strengthened local energy ownership. With new green technologies available, such a transition is possibly involving new actors: prosumers, renewable energy co-operatives and municipal, community-owned power stations which replace centralised, power corporations.\n\nThis concept is promoted by renewable energy business sector, local communities, labour unions (e.g. Global Labour Institute, Trade Unions for Energy Democracy), think tanks (e.g. Green Institute Foundation) etc. and NGO (e.g. Rosa Luxemburg Foundation).\n\nThere are various concepts of Energy Democracy. One early concept has been published by the Berlin-based group gegenstrom 2012. The thesis paper calls for “a 100% transition to renewable energy as quick as possible\" and a reform of ownership of energy production: “dash energy corporations! Socialise energy provision! The gamete of energy democracy will be publicly owned city utilities (Stadtwerke) and energy cooperatives”. The concept inspired the German climate camps in 2011, as a collection of theses and arguments. The 2012 climate camp Lausitzcamp in Lusatia published a short summary of the concept: “Energy Democracy means ensuring that everyone has access to enough energy. However, the energy must be produced in a way that it neither harms nor endangers the environmental or people. Concretely, this means leaving fossil fuels in the ground, socializing and democratizing the means of production and changing our attitude towards energy consumption.” \n\nIn 2014, a concept of energy democracy was promoted by the city of Boulogne-Billancourt in France. For its participation in the Bloomberg mayors challenge, the city presented an innovative vision of Energy democracy based on the reduction of the use of fossil fuels and a system of incentives to encourage citizens in reducing their energy consumption.\n\nEnergy-democracy has been a successful concept of grass roots movements to combine resistance against fossil fuel exploitation like coal or shale gas with a positive alternative agenda. \"The hope, moreover, is that energy democracy might offer new spaces for collaboration between ecological movements and movements for social, economic and workplace justice.\"\n\nAcademic, James Goodman, details the difference between energy policy in Germany and India, exploring public participation in approaches to renewable energy. He concludes that 'energy policies are found to be increasingly embedded in the wider 'climate dialectic', forcing new, more transformative possibilities onto the agenda.'\n\n"}
{"id": "20560973", "url": "https://en.wikipedia.org/wiki?curid=20560973", "title": "Energy functional", "text": "Energy functional\n\nIn the energy methods of simulating the dynamics of complex structures, a state of the system is often described as an element of an appropriate function space. To be in this state, the system pays a certain cost in terms of energy required by the state. This energy is a scalar quantity, a function of the state, hence the term \"functional\". The system tends to develop from the state with higher energy (higher cost) to the state with lower energy, thus local minima of this functional are usually related to the stable stationary states. Studying such states is part of the optimization problems, where the terms \"energy functional\" or \"cost functional\" are often used to describe the objective function.\n\n\n"}
{"id": "10274", "url": "https://en.wikipedia.org/wiki?curid=10274", "title": "Enthalpy", "text": "Enthalpy\n\nEnthalpy , a property of a thermodynamic system, is equal to the system's internal energy plus the product of its pressure and volume. For processes at constant pressure, the heat absorbed or released equals the change in enthalpy.\n\nThe unit of measurement for enthalpy in the International System of Units (SI) is the joule. Other historical conventional units still in use include the British thermal unit (BTU) and the calorie.\n\nEnthalpy comprises a system's internal energy, which is the energy required to create the system, plus the amount of work required to make room for it by displacing its environment and establishing its volume and pressure.\n\nEnthalpy is defined as a state function that depends only on the prevailing equilibrium state identified by the system's internal energy, pressure, and volume. It is an extensive quantity.\n\nEnthalpy is the preferred expression of system energy changes in many chemical, biological, and physical measurements at constant pressure, because it simplifies the description of energy transfer. At constant pressure, the enthalpy change equals the energy transferred from the environment through heating or work other than expansion work.\n\nThe total enthalpy, \"H\", of a system cannot be measured directly. The same situation exists in classical mechanics: only a change or difference in energy carries physical meaning. Enthalpy itself is a thermodynamic potential, so in order to measure the enthalpy of a system, we must refer to a defined reference point; therefore what we measure is the change in enthalpy, Δ\"H\". The Δ\"H\" is a positive change in endothermic reactions, and negative in heat-releasing exothermic processes.\n\nFor processes under constant pressure, Δ\"H\" is equal to the change in the internal energy of the system, plus the pressure-volume work p ΔV done by the system on its surroundings (which is > 0 for an expansion and < 0 for a contraction). This means that the change in enthalpy under such conditions is the heat absorbed or released by the system through a chemical reaction or by external heat transfer. Enthalpies for chemical substances at constant pressure usually refer to standard state: most commonly 1 bar pressure. Standard state does not, strictly speaking, specify a temperature (see standard state), but expressions for enthalpy generally reference the standard heat of formation at 25 °C.\n\nEnthalpy of ideal gases and incompressible solids and liquids does not depend on pressure, unlike entropy and Gibbs energy. Real materials at common temperatures and pressures usually closely approximate this behavior, which greatly simplifies enthalpy calculation and use in practical designs and analyses.\n\nThe word \"enthalpy\" was coined relatively late, in the early 20th century, in analogy with the 19th-century terms \"energy\" (introduced in its modern sense by Thomas Young in 1802) and \"entropy\" (coined in analogy to \"energy\" by Rudolf Clausius in 1865).\nIt uses the root of Greek \"warmth, heat\" \nwhere \"energy\" has \"work\" and \"entropy\" \"transformation\", by analogy expressing the idea of \"heat-content\" where energy refers to \"work-content\" and entropy to \"transformation-content\".\nThe term does in fact stand in for the older term \"heat content\", \na term which is now mostly deprecated as misleading, as refers to the amount of heat absorbed in a process at constant pressure only, \nbut not in the general case (when pressure is variable). \nJosiah Willard Gibbs used the term \"a heat function for constant pressure\" for clarity.\n\nIntroduction of the concept of \"heat content\" is associated with Benoît Paul Émile Clapeyron and Rudolf Clausius (Clausius–Clapeyron relation, 1850).\n\nThe term \"enthalpy\" first appeared in print in 1909. It is attributed to Heike Kamerlingh Onnes, who most likely introduced it orally the year before, at the first meeting of the Institute of Refrigeration in Paris.\nIt gained currency only in the 1920s, notably with the \"Mollier Steam Tables and Diagrams\", published in 1927.\n\nUntil the 1920s, the symbol was used, somewhat inconsistently, for \"heat\" in general. \nThe definition of as strictly limited to enthalpy or \"heat content at constant pressure\" was formally proposed by Alfred W. Porter in 1922.\n\nThe enthalpy of a thermodynamic system is defined as\nwhere\nEnthalpy is an extensive property. This means that, for homogeneous systems, the enthalpy is proportional to the size of the system. It is convenient to introduce the specific enthalpy \"h\" = , where \"m\" is the mass of the system, or the molar enthalpy \"H\" = , where \"n\" is the number of moles (\"h\" and \"H\" are intensive properties). For inhomogeneous systems the enthalpy is the sum of the enthalpies of the composing subsystems:\n\nwhere the label \"k\" refers to the various subsystems. In case of continuously varying \"p\", \"T\" or composition, the summation becomes an integral:\n\nwhere \"ρ\" is the density.\n\nThe enthalpy of homogeneous systems can be viewed as function \"H\"(\"S\",\"p\") of the entropy \"S\" and the pressure \"p\", and a differential relation for it can be derived as follows. We start from the first law of thermodynamics for closed systems for an infinitesimal process:\n\nHere, \"δQ\" is a small amount of heat added to the system, and \"δW\" a small amount of work performed by the system. In a homogeneous system only reversible processes can take place, so the second law of thermodynamics gives , with \"T\" the absolute temperature of the system. Furthermore, if only \"pV\" work is done, . As a result,\n\nAdding \"d\"(\"pV\") to both sides of this expression gives\n\nor\n\nSo\n\nThe above expression of \"dH\" in terms of entropy and pressure may be unfamiliar to some readers. However, there are expressions in terms of more familiar variables such as temperature and pressure:\n\nHere \"C\" is the heat capacity at constant pressure and \"α\" is the coefficient of (cubic) thermal expansion:\n\nWith this expression one can, in principle, determine the enthalpy if \"C\" and \"V\" are known as functions of \"p\" and \"T\".\n\nNote that for an ideal gas, \"αT\" = 1, so that\n\nIn a more general form, the first law describes the internal energy with additional terms involving the chemical potential and the number of particles of various types. The differential statement for \"dH\" then becomes\n\nwhere \"μ\" is the chemical potential per particle for an \"i\"-type particle, and \"N\" is the number of such particles. The last term can also be written as (with \"dn\" the number of moles of component \"i\" added to the system and, in this case, \"μ\" the molar chemical potential) or as (with \"dm\" the mass of component \"i\" added to the system and, in this case, \"μ\" the specific chemical potential).\n\nThe \"U\" term can be interpreted as the energy required to create the system, and the \"pV\" term as the work that would be required to \"make room\" for the system if the pressure of the environment remained constant. When a system, for example, \"n\" moles of a gas of volume \"V\" at pressure \"p\" and temperature \"T\", is created or brought to its present state from absolute zero, energy must be supplied equal to its internal energy \"U\" plus \"pV\", where \"pV\" is the work done in pushing against the ambient (atmospheric) pressure.\n\nIn basic physics and statistical mechanics it may be more interesting to study the internal properties of the system and therefore the internal energy is used. In basic chemistry, experiments are often conducted at constant atmospheric pressure, and the pressure-volume work represents an energy exchange with the atmosphere that cannot be accessed or controlled, so that Δ\"H\" is the expression chosen for the heat of reaction.\n\nFor a heat engine a change in its internal energy is the difference between the heat input and the pressure-volume work done by the working substance while a change in its enthalpy is the difference between the heat input and the work done by the engine:\nwhere the work W done by the engine is: \n\nIn order to discuss the relation between the enthalpy increase and heat supply, we return to the first law for closed systems: . We apply it to the special case with a uniform pressure at the surface. In this case the work term can be split into two contributions, the so-called \"pV\" work, given by (where here \"p\" is the pressure at the surface, \"dV\" is the increase of the volume of the system) and all other types of work \"δW′\", such as by a shaft or by electromagnetic interaction. So we write . In this case the first law reads:\n\nor\n\nFrom this relation we see that the increase in enthalpy of a system is equal to the added heat:\n\nprovided that the system is under constant pressure (\"dp\" = 0) and that the only work done by the system is expansion work (\"δW\"' = 0).\n\nIn thermodynamics, one can calculate enthalpy by determining the requirements for creating a system from \"nothingness\"; the mechanical work required, \"pV\", differs based upon the conditions that obtain during the creation of the thermodynamic system.\n\nEnergy must be supplied to remove particles from the surroundings to make space for the creation of the system, assuming that the pressure \"p\" remains constant; this is the \"pV\" term. The supplied energy must also provide the change in internal energy, \"U\", which includes activation energies, ionization energies, mixing energies, vaporization energies, chemical bond energies, and so forth. Together, these constitute the change in the enthalpy \"U\" + \"pV\". For systems at constant pressure, with no external work done other than the \"pV\" work, the change in enthalpy is the heat received by the system.\n\nFor a simple system, with a constant number of particles, the difference in enthalpy is the maximum amount of thermal energy derivable from a thermodynamic process in which the pressure is held constant.\n\nThe total enthalpy of a system cannot be measured directly, the \"enthalpy change\" of a system is measured instead. Enthalpy change is defined by the following equation:\n\nwhere\n\nFor an exothermic reaction at constant pressure, the system's change in enthalpy equals the energy released in the reaction, including the energy retained in the system and lost through expansion against its surroundings. In a similar manner, for an endothermic reaction, the system's change in enthalpy is equal to the energy \"absorbed\" in the reaction, including the energy \"lost by\" the system and \"gained\" from compression from its surroundings. If Δ\"H\" is positive, the reaction is endothermic, that is heat is absorbed by the system due to the products of the reaction having a greater enthalpy than the reactants. On the other hand, if Δ\"H\" is negative, the reaction is exothermic, that is the overall decrease in enthalpy is achieved by the generation of heat.\n\nFrom the definition of enthalpy as formula_19 the enthalpy change at constant pressure formula_20 However for most chemical reactions, the work term formula_21 is much smaller than the internal energy change formula_22 which is approximately equal to formula_23 As an example, for the combustion of carbon monoxide 2 CO(g) + O(g) → 2 CO(g), formula_24 = –566.0 kJ and formula_22 = –563.5 kJ. Since the differences are so small, reaction enthalpies are often loosely described as reaction energies and analyzed in terms of bond energies.\n\nThe specific enthalpy of a uniform system is defined as \"h\" = where \"m\" is the mass of the system. The SI unit for specific enthalpy is joule per kilogram. It can be expressed in other specific quantities by , where \"u\" is the specific internal energy, \"p\" is the pressure, and \"v\" is specific volume, which is equal to , where \"ρ\" is the density.\n\nAn enthalpy change describes the change in enthalpy observed in the constituents of a thermodynamic system when undergoing a transformation or chemical reaction. It is the difference between the enthalpy after the process has completed, i.e. the enthalpy of the products, and the initial enthalpy of the system, i.e. the reactants. These processes are reversible and the enthalpy for the reverse process is the negative value of the forward change.\n\nA common standard enthalpy change is the enthalpy of formation, which has been determined for a large number of substances. Enthalpy changes are routinely measured and compiled in chemical and physical reference works, such as the CRC Handbook of Chemistry and Physics. The following is a selection of enthalpy changes commonly recognized in thermodynamics.\n\nWhen used in these recognized terms the qualifier \"change\" is usually dropped and the property is simply termed \"enthalpy of 'process\"'. Since these properties are often used as reference values it is very common to quote them for a standardized set of environmental parameters, or standard conditions, including: \nFor such standardized values the name of the enthalpy is commonly prefixed with the term \"standard\", e.g. \"standard enthalpy of formation\".\n\nChemical properties:\n\nPhysical properties:\n\nIn thermodynamic open systems, matter may flow in and out of the system boundaries. The first law of thermodynamics for open systems states: The increase in the internal energy of a system is equal to the amount of energy added to the system by matter flowing in and by heating, minus the amount lost by matter flowing out and in the form of work done by the system:\n\nwhere \"U\" is the average internal energy entering the system, and \"U\" is the average internal energy leaving the system.\n\nThe region of space enclosed by the boundaries of the open system is usually called a control volume, and it may or may not correspond to physical walls. If we choose the shape of the control volume such that all flow in or out occurs perpendicular to its surface, then the flow of matter into the system performs work as if it were a piston of fluid pushing mass into the system, and the system performs work on the flow of matter out as if it were driving a piston of fluid. There are then two types of work performed: \"flow work\" described above, which is performed on the fluid (this is also often called \"pV work\"), and \"shaft work\", which may be performed on some mechanical device.\n\nThese two types of work are expressed in the equation\n\nSubstitution into the equation above for the control volume (cv) yields:\n\nThe definition of enthalpy, \"H\", permits us to use this thermodynamic potential to account for both internal energy and \"pV\" work in fluids for open systems:\n\nIf we allow also the system boundary to move (e.g. due to moving pistons), we get a rather general form of the first law for open systems. In terms of time derivatives it reads:\n\nwith sums over the various places \"k\" where heat is supplied, matter flows into the system, and boundaries are moving. The \"Ḣ\" terms represent enthalpy flows, which can be written as\n\nwith \"ṁ\" the mass flow and \"ṅ\" the molar flow at position \"k\" respectively. The term represents the rate of change of the system volume at position \"k\" that results in \"pV\" power done by the system. The parameter \"P\" represents all other forms of power done by the system such as shaft power, but it can also be e.g. electric power produced by an electrical power plant.\n\nNote that the previous expression holds true only if the kinetic energy flow rate is conserved between system inlet and outlet. Otherwise, it has to be included in the enthalpy balance. During steady-state operation of a device (\"see turbine, pump, and engine\"), the average may be set equal to zero. This yields a useful expression for the average power generation for these devices in the absence of chemical reactions:\n\nwhere the angle brackets denote time averages. The technical importance of the enthalpy is directly related to its presence in the first law for open systems, as formulated above.\n\nThe enthalpy values of important substances can be obtained using commercial software. Practically all relevant material properties can be obtained either in tabular or in graphical form. There are many types of diagrams, such as \"h\"–\"T\" diagrams, which give the specific enthalpy as function of temperature for various pressures, and \"h\"–\"p\" diagrams, which give \"h\" as function of \"p\" for various \"T\". One of the most common diagrams is the temperature–specific entropy diagram (\"T\"–\"s\"-diagram). It gives the melting curve and saturated liquid and vapor values together with isobars and isenthalps. These diagrams are powerful tools in the hands of the thermal engineer.\n\nThe points a through h in the figure play a role in the discussion in this section.\n\nOne of the simple applications of the concept of enthalpy is the so-called throttling process, also known as Joule-Thomson expansion. It concerns a steady adiabatic flow of a fluid through a flow resistance (valve, porous plug, or any other type of flow resistance) as shown in the figure. This process is very important, since it is at the heart of domestic refrigerators, where it is responsible for the temperature drop between ambient temperature and the interior of the refrigerator. It is also the final stage in many types of liquefiers.\n\nFor a steady state flow regime, the enthalpy of the system (dotted rectangle) has to be constant. Hence\n\nSince the mass flow is constant, the specific enthalpies at the two sides of the flow resistance are the same:\n\nthat is, the enthalpy per unit mass does not change during the throttling. The consequences of this relation can be demonstrated using the \"T\"–\"s\" diagram above. Point c is at 200 bar and room temperature (300 K). A Joule–Thomson expansion from 200 bar to 1 bar follows a curve of constant enthalpy of roughly 425 kJ/kg (not shown in the diagram) lying between the 400 and 450 kJ/kg isenthalps and ends in point d, which is at a temperature of about 270 K. Hence the expansion from 200 bar to 1 bar cools nitrogen from 300 K to 270 K. In the valve, there is a lot of friction, and a lot of entropy is produced, but still the final temperature is below the starting value!\n\nPoint e is chosen so that it is on the saturated liquid line with . It corresponds roughly with and . Throttling from this point to a pressure of 1 bar ends in the two-phase region (point f). This means that a mixture of gas and liquid leaves the throttling valve. Since the enthalpy is an extensive parameter, the enthalpy in f (\"h\") is equal to the enthalpy in g (\"h\") multiplied by the liquid fraction in f (\"x\") plus the enthalpy in h (\"h\") multiplied by the gas fraction in f . So\n\nWith numbers: , so \"x\" = 0.64. This means that the mass fraction of the liquid in the liquid–gas mixture that leaves the throttling valve is 64%.\n\nA power \"P\" is applied e.g. as electrical power. If the compression is adiabatic, the gas temperature goes up. In the reversible case it would be at constant entropy, which corresponds with a vertical line in the \"T\"–\"s\" diagram. For example, compressing nitrogen from 1 bar (point a) to 2 bar (point b) would result in a temperature increase from 300 K to 380 K. In order to let the compressed gas exit at ambient temperature \"T\", heat exchange, e.g. by cooling water, is necessary. In the ideal case the compression is isothermal. The average heat flow to the surroundings is \"Q̇\". Since the system is in the steady state the first law gives\n\nThe minimal power needed for the compression is realized if the compression is reversible. In that case the second law of thermodynamics for open systems gives\n\nEliminating \"Q̇\" gives for the minimal power\n\nFor example, compressing 1 kg of nitrogen from 1 bar to 200 bar costs at least . With the data, obtained with the \"T\"–\"s\" diagram, we find a value of 476 kJ/kg.\n\nThe relation for the power can be further simplified by writing it as\n\nWith , this results in the final relation\n\n\n\n"}
{"id": "9285", "url": "https://en.wikipedia.org/wiki?curid=9285", "title": "Ethical naturalism", "text": "Ethical naturalism\n\nEthical naturalism (also called moral naturalism or naturalistic cognitivistic definism) is the meta-ethical view which claims that:\n\n\nIt is important to distinguish the versions of ethical naturalism which have received the most sustained philosophical interest, for example, Cornell realism, from the position that \"the way things are is always the way they ought to be\", which few ethical naturalists hold. Ethical naturalism does, however, reject the fact-value distinction: it suggests that inquiry into the natural world can increase our moral knowledge in just the same way it increases our scientific knowledge. Indeed, proponents of ethical naturalism have argued that humanity needs to invest in the science of morality, a broad and loosely defined field that uses evidence from biology, primatology, anthropology, psychology, neuroscience, and other areas to classify and describe moral behavior.\n\nEthical naturalism encompasses any reduction of ethical properties, such as 'goodness', to non-ethical properties; there are many different examples of such reductions, and thus many different varieties of ethical naturalism. Hedonism, for example, is the view that goodness is ultimately just pleasure.\n\n\nEthical naturalism has been criticized most prominently by ethical non-naturalist G. E. Moore, who formulated the open-question argument. Garner and Rosen say that a common definition of \"natural property\" is one \"which can be discovered by sense observation or experience, experiment, or through any of the available means of science.\" They also say that a good definition of \"natural property\" is problematic but that \"it is only in criticism of naturalism, or in an attempt to distinguish between naturalistic and nonnaturalistic definist theories, that such a concept is needed.\" R. M. Hare also criticised ethical naturalism because of its fallacious definition of the terms 'good' or 'right' explaining how value-terms being part of our prescriptive moral language are not reducible to descriptive terms: \"Value-terms have a special function in language, that of commending; and so they plainly cannot be defined in terms of other words which themselves do not perform this function\".\n\nWhen it comes to the moral questions that we might ask, it can be difficult to argue that there is not necessarily some level of meta-ethical relativism – and failure to address this matter is criticized as ethnocentrism. \n\nAs a broad example of relativism, we would no doubt see very different moral systems in an alien race that can only survive by occasionally ingesting one another. As a narrow example, there would be further specific moral opinions for each individual of that species.\n\nSome forms of moral realism are compatible with some degree of meta-ethical relativism. This argument rests on the assumption that one can have a \"moral\" discussion on various scales; that is, what is \"good\" for: a certain part of your being (leaving open the possibility of conflicting motives), you as a single individual, your family, your society, your species, your type of species. For example, a moral universalist (and certainly an absolutist) might argue that, just as one can discuss what is 'good and evil' at an individual's level, so too can one make certain \"moral\" propositions with truth values relative at the level of the species. In other words, the moral relativist need not deem \"all\" moral propositions as necessarily subjective. The answer to \"is free speech normally good for human societies?\" is relative in a sense, but the moral realist would argue that an individual can be incorrect in this matter. This may be the philosophical equivalent of the more pragmatic arguments made by some scientists.\n\nMoral nihilists maintain that any talk of an objective morality is incoherent and better off using other terms. Proponents of moral science like Ronald A. Lindsay have counter-argued that their way of understanding \"morality\" as a practical enterprise is the way we ought to have understood it in the first place. He holds the position that the alternative seems to be the elaborate philosophical reduction of the word \"moral\" into a vacuous, useless term. Lindsay adds that it is important to reclaim the specific word \"Morality\" because of the connotations it holds with many individuals.\n\nAuthor Sam Harris has argued that we overestimate the relevance of many arguments against the science of morality, arguments he believes scientists happily and rightly disregard in other domains of science like physics. For example, scientists may find themselves attempting to argue against philosophical skeptics, when Harris says they should be practically asking – as they would in any other domain – \"why would we listen to a solipsist in the first place?\" This, Harris contends, is part of what it means to practice a science of morality.\n\nPhysicist Sean Carroll believes that conceiving of morality as a science could be a case of scientific imperialism and insists that what is \"good for conscious creatures\" is not an adequate working definition of \"moral\". In opposition, Vice President at the Center for Inquiry, John Shook, claims that this working definition is more than adequate for science at present, and that disagreement should not immobilize the scientific study of ethics.\n\nIn the collective \"The End of Christianity\", Richard Carrier's chapter \"Moral Facts Naturally Exist (and Science Could Find Them)\" sets out to propose a form of moral realism centered on human satisfaction..\nIn modern times, many thinkers discussing the fact–value distinction and the is–ought problem have settled on the idea that one cannot derive \"ought\" from \"is\". Conversely, Harris maintains that the fact-value distinction is a confusion, proposing that values are really a certain kind of fact. Specifically, Harris suggests that values amount to empirical statements about \"the flourishing of conscious creatures in a society\". He argues that there are objective answers to moral questions, even if some are difficult or impossible to possess in practice. In this way, he says, science can tell us what to value. Harris adds that we do not demand absolute certainty from predictions in physics so we should not demand that of a science studying morality (see \"The Moral Landscape\").\n\n\n"}
{"id": "47860163", "url": "https://en.wikipedia.org/wiki?curid=47860163", "title": "Exergonix, Inc.", "text": "Exergonix, Inc.\n\nExergonix Inc, is an energy storage company based in Kansas City, Missouri. It was founded in 2010, after spinning out of Kokam America, Inc., which was acquired by Dow Chemical Company in 2009. Exergonix develops, manufactures and deploys community-level and grid energy storage to supply peak-shaving, demand-management, and smart grid capabilities to the grid and micro-grid.\n\nThe company designs and builds renewable energy solutions for a wide range of applications including storage for solar and wind power, utility management, the military, telecommunications cell towers and emergency back-up.\n\nAccording to the company president’s statement to the United States Senate Committee on Commerce, Science, and Transportation in 2012, the company’s mission is to make renewable energy cost effective and will allow nations around the world to implement a worldwide, workable, smart and decentralized utility grid. The company makes a “promising energy storage application”.\n\nThe facility sits atop 83 acres of land on Missouri Route 291 and U.S. Route 50, previously owned by Pfizer, Inc. Valued at $90 million, the company raised another $2 million from local angel investor group, and a $1.4 million package from the City of Lee’s Summit, Missouri Also from Senators Kit Bond and Jim Talent, the company further secured $8 million in federal funding from a defense spending bill, to bring a new manufacturing plant to Missouri. In 2011, the \"Green Energy Park\" was dialed back to $50 million, after $100 million federal money fell through.\nThe CEO had approached state legislators to seek $650 million for a manufacturing plant in the same area.\n\nThe CEO approached the United States Department of Defense with new battery technology and although the agency was interested, its budget was committed. Senator Kit Bond was then approached with the request to include the money in the department's budget.\n\nNissanka said: \"I would say that the reason we got the money has nothing to do with a high-power lobbyist, we got it because of our technology. Earmarks unfortunately are the only way you can drive new technology into the market.\" Nissanka provided testimony during Senator Roy Blunt’s attempt at Revitalize American Manufacturing and Innovation Act of 2013.\n\nExergonix received $2 million to build and install a 1 megawatt utility storage at Whiteman Air Force Base in Knob Noster, Missouri in order to demonstrate the ability of the US military to provide safe and effective energy security solutions compounded with renewable energy. The battery technology offered an improved cycle life, lighter weight, wider operating temperature range and true maintenance-free design. Applications include many aerospace and defense uses, such as weapon platforms, aircraft and future land warrior programs.\n\nIn 2010, as part of Emanuel Cleaver’s Green Impact Zone in Kansas City, Exergonix along with Siemens, Electric Power Research Institute, Landis+Gyr, Intergraph, Tendril, City of Kansas City and the Mid-America Regional Council launched a pilot project for SmartGrid development under Office of Electricity Delivery and Energy Reliability with funding from American Recovery and Reinvestment Act of 2009 in partnership with Kansas City Power and Light. The project aimed to generate greater energy efficiency, reduced cost, improved reliability, more transparent and interactive information, and an improved environmental footprint.\n\nPresident Barack Obama expressed bright futures talking to Smith Electric Vehicles’s USA branch about clean technology jobs, where Kokam America was the battery supplier, and CEO Don Nissanka (also formerly Kokam America President and CEO) was on the board. Besides Smith Electric, Nissanka had customers in the EV market, but he declined to disclose any of their names. He was also on the 18-member taskforce on Automotive Jobs Task Force, appointed by Governor Jay Nixon in 2009.\n\n\"The future is here for us in terms of the battery technology,\" said Nissanka\n\nNissanka also initiated the “Missouri Innovation Campus” (MIC) which was Obama’s travel stop for a speech at University of Central Missouri.\n\n“I think the president is right on, I think the administration has done a good job of putting enough money into this economy right now. We’re in a depression and we needed this money to fuel the economy and fuel the jobs. I see it happening. It’s happening around the country”.\n\nAccording to \"InformationWeek\", the company along with a handful of other battery storage firms, shares Tesla’s ambition of disruptive home power consumption.\n\nIn May 2016, the company acquired Coda Energy.\n\n\n"}
{"id": "8609949", "url": "https://en.wikipedia.org/wiki?curid=8609949", "title": "Foreline", "text": "Foreline\n\nA foreline is a vacuum line between the pumps of a multistage vacuum system. No longer exclusively used by scientists in research, vacuum systems are used in numerous industries that include food production and the manufacturing of electronic components. A classic foreline is a hose or tube that connects a rotary vane pump to the outlet of an oil diffusion pump, although vacuum technology now uses a vast array of roughing pumps and high vacuum pumps.\n\nAlthough the term has been used for many decades in the high vacuum system industry and in literature concerning vacuum systems, the word foreline has not yet been added to any dictionary.\n"}
{"id": "43194879", "url": "https://en.wikipedia.org/wiki?curid=43194879", "title": "Geometric transformation", "text": "Geometric transformation\n\nA geometric transformation is any bijection of a set having some geometric structure to itself or another such set. Specifically, \"A geometric transformation is a function whose domain and range are sets of points. Most often the domain and range of a geometric transformation are both R or both R. Often geometric transformations are required to be 1-1 functions, so that they have inverses.\" The study of geometry may be approached via the study of these transformations.\n\nGeometric transformations can be classified by the dimension of their operand sets (thus distinguishing between planar transformations and those of space, for example). They can also be classified according to the properties they preserve:\n\nEach of these classes contains the previous one.\n\n\nTransformations of the same type form groups that may be sub-groups of other transformation groups.\n\n\n"}
{"id": "2068726", "url": "https://en.wikipedia.org/wiki?curid=2068726", "title": "History of Earth", "text": "History of Earth\n\nThe history of Earth concerns the development of planet Earth from its formation to the present day. Nearly all branches of natural science have contributed to understanding of the main events of Earth's past, characterized by constant geological change and biological evolution.\n\nThe geological time scale (GTS), as defined by international convention, depicts the large spans of time from the beginning of the Earth to the present, and its divisions chronicle some definitive events of Earth history. (In the graphic: Ga means \"billion years ago\"; Ma, \"million years ago\".) Earth formed around 4.54 billion years ago, approximately one-third the age of the universe, by accretion from the solar nebula. Volcanic outgassing probably created the primordial atmosphere and then the ocean, but the early atmosphere contained almost no oxygen. Much of the Earth was molten because of frequent collisions with other bodies which led to extreme volcanism. While Earth was in its earliest stage (Early Earth), a giant impact collision with a planet-sized body named Theia is thought to have formed the Moon. Over time, the Earth cooled, causing the formation of a solid crust, and allowing liquid water on the surface.\n\nThe Hadean eon represents the time before a reliable (fossil) record of life; it began with the formation of the planet and ended 4.0 billion years ago. The following Archean and Proterozoic eons produced the beginnings of life on Earth and its earliest evolution. The succeeding eon is the Phanerozoic, divided into three eras: the Palaeozoic, an era of arthropods, fishes, and the first life on land; the Mesozoic, which spanned the rise, reign, and climactic extinction of the non-avian dinosaurs; and the Cenozoic, which saw the rise of mammals. Recognizable humans emerged at most 2 million years ago, a vanishingly small period on the geological scale.\n\nThe earliest undisputed evidence of life on Earth dates at least from 3.5 billion years ago, during the Eoarchean Era, after a geological crust started to solidify following the earlier molten Hadean Eon. There are microbial mat fossils such as stromatolites found in 3.48 billion-year-old sandstone discovered in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in southwestern Greenland as well as \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, \"If life arose relatively quickly on Earth … then it could be common in the universe.\"\n\nPhotosynthetic organisms appeared between 3.2 and 2.4 billion years ago and began enriching the atmosphere with oxygen. Life remained mostly small and microscopic until about 580 million years ago, when complex multicellular life arose, developed over time, and culminated in the Cambrian Explosion about 541 million years ago. This sudden diversification of life forms produced most of the major phyla known today, and divided the Proterozoic Eon from the Cambrian Period of the Paleozoic Era. It is estimated that 99 percent of all species that ever lived on Earth, over five billion, have gone extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million are documented, but over 86 percent have not been described. However, it was recently claimed that 1 trillion species currently live on Earth, with only one-thousandth of one percent described.\n\nThe Earth's crust has constantly changed since its formation, as has life has since its first appearance. Species continue to evolve, taking on new forms, splitting into daughter species, or going extinct in the face of ever-changing physical environments. The process of plate tectonics continues to shape the Earth's continents and oceans and the life they harbor. Human activity is now a dominant force affecting global change, harming the biosphere, the Earth's surface, hydrosphere, and atmosphere with the loss of wild lands, over-exploitation of the oceans, production of greenhouse gases, degradation of the ozone layer, and general degradation of soil, air, and water quality.\nIn geochronology, time is generally measured in mya (megayears or million years), each unit representing the period of approximately 1,000,000 years in the past. The history of Earth is divided into four great eons, starting 4,540 mya with the formation of the planet. Each eon saw the most significant changes in Earth's composition, climate and life. Each eon is subsequently divided into eras, which in turn are divided into periods, which are further divided into epochs.\n\nThe history of the Earth can be organized chronologically according to the geologic time scale, which is split into intervals based on stratigraphic analysis.\n\nThe standard model for the formation of the Solar System (including the Earth) is the solar nebula hypothesis. In this model, the Solar System formed from a large, rotating cloud of interstellar dust and gas called the solar nebula. It was composed of hydrogen and helium created shortly after the Big Bang 13.8 Ga (billion years ago) and heavier elements ejected by supernovae. About 4.5 Ga, the nebula began a contraction that may have been triggered by the shock wave from a nearby supernova. A shock wave would have also made the nebula rotate. As the cloud began to accelerate, its angular momentum, gravity, and inertia flattened it into a protoplanetary disk perpendicular to its axis of rotation. Small perturbations due to collisions and the angular momentum of other large debris created the means by which kilometer-sized protoplanets began to form, orbiting the nebular center.\n\nThe center of the nebula, not having much angular momentum, collapsed rapidly, the compression heating it until nuclear fusion of hydrogen into helium began. After more contraction, a T Tauri star ignited and evolved into the Sun. Meanwhile, in the outer part of the nebula gravity caused matter to condense around density perturbations and dust particles, and the rest of the protoplanetary disk began separating into rings. In a process known as runaway accretion, successively larger fragments of dust and debris clumped together to form planets. Earth formed in this manner about 4.54 billion years ago (with an uncertainty of 1%) and was largely completed within 10–20 million years. The solar wind of the newly formed T Tauri star cleared out most of the material in the disk that had not already condensed into larger bodies. The same process is expected to produce accretion disks around virtually all newly forming stars in the universe, some of which yield planets.\n\nThe proto-Earth grew by accretion until its interior was hot enough to melt the heavy, siderophile metals. Having higher densities than the silicates, these metals sank. This so-called \"iron catastrophe\" resulted in the separation of a primitive mantle and a (metallic) core only 10 million years after the Earth began to form, producing the layered structure of Earth and setting up the formation of Earth's magnetic field. J. A. Jacobs was the first to suggest that the inner core—a solid center distinct from the liquid outer core—is freezing and growing out of the liquid outer core due to the gradual cooling of Earth's interior (about 100 degrees Celsius per billion years).\n\nThe first eon in Earth's history, the \"Hadean\", begins with the Earth's formation and is followed by the \"Archean\" eon at 3.8 Ga. The oldest rocks found on Earth date to about 4.0 Ga, and the oldest detrital zircon crystals in rocks to about 4.4 Ga, soon after the formation of the Earth's crust and the Earth itself. The giant impact hypothesis for the Moon's formation states that shortly after formation of an initial crust, the proto-Earth was impacted by a smaller protoplanet, which ejected part of the mantle and crust into space and created the Moon.\n\nFrom crater counts on other celestial bodies, it is inferred that a period of intense meteorite impacts, called the \"Late Heavy Bombardment\", began about 4.1 Ga, and concluded around 3.8 Ga, at the end of the Hadean. In addition, volcanism was severe due to the large heat flow and geothermal gradient. Nevertheless, detrital zircon crystals dated to 4.4 Ga show evidence of having undergone contact with liquid water, suggesting that the Earth already had oceans or seas at that time.\n\nBy the beginning of the Archean, the Earth had cooled significantly. Present life forms could not have survived at Earth's surface, because the Archean atmosphere lacked oxygen hence had no ozone layer to block ultraviolet light. Nevertheless, it is believed that primordial life began to evolve by the early Archean, with candidate fossils dated to around 3.5 Ga. Some scientists even speculate that life could have begun during the early Hadean, as far back as 4.4 Ga, surviving the possible Late Heavy Bombardment period in hydrothermal vents below the Earth's surface.\n\nEarth's only natural satellite, the Moon, is larger relative to its planet than any other satellite in the solar system. During the Apollo program, rocks from the Moon's surface were brought to Earth. Radiometric dating of these rocks shows that the Moon is 4.53 ± 0.01 billion years old, formed at least 30 million years after the solar system. New evidence suggests the Moon formed even later, 4.48 ± 0.02 Ga, or 70–110 million years after the start of the Solar System.\n\nTheories for the formation of the Moon must explain its late formation as well as the following facts. First, the Moon has a low density (3.3 times that of water, compared to 5.5 for the earth) and a small metallic core. Second, there is virtually no water or other volatiles on the moon. Third, the Earth and Moon have the same oxygen isotopic signature (relative abundance of the oxygen isotopes). Of the theories proposed to account for these phenomena, one is widely accepted: The \"giant impact hypothesis\" proposes that the Moon originated after a body the size of Mars (sometimes named Theia) struck the proto-Earth a glancing blow.\n\nThe collision released about 100 million times more energy than the more recent Chicxulub impact that is believed to have caused the extinction of the dinosaurs. It was enough to vaporize some of the Earth's outer layers and melt both bodies. A portion of the mantle material was ejected into orbit around the Earth. The giant impact hypothesis predicts that the Moon was depleted of metallic material, explaining its abnormal composition. The ejecta in orbit around the Earth could have condensed into a single body within a couple of weeks. Under the influence of its own gravity, the ejected material became a more spherical body: the Moon.\n\nMantle convection, the process that drives plate tectonics, is a result of heat flow from the Earth's interior to the Earth's surface. It involves the creation of rigid tectonic plates at mid-oceanic ridges. These plates are destroyed by subduction into the mantle at subduction zones. During the early Archean (about 3.0 Ga) the mantle was much hotter than today, probably around , so convection in the mantle was faster. Although a process similar to present-day plate tectonics did occur, this would have gone faster too. It is likely that during the Hadean and Archean, subduction zones were more common, and therefore tectonic plates were smaller.\n\nThe initial crust, formed when the Earth's surface first solidified, totally disappeared from a combination of this fast Hadean plate tectonics and the intense impacts of the Late Heavy Bombardment. However, it is thought that it was basaltic in composition, like today's oceanic crust, because little crustal differentiation had yet taken place. The first larger pieces of continental crust, which is a product of differentiation of lighter elements during partial melting in the lower crust, appeared at the end of the Hadean, about 4.0 Ga. What is left of these first small continents are called cratons. These pieces of late Hadean and early Archean crust form the cores around which today's continents grew.\n\nThe oldest rocks on Earth are found in the North American craton of Canada. They are tonalites from about 4.0 Ga. They show traces of metamorphism by high temperature, but also sedimentary grains that have been rounded by erosion during transport by water, showing that rivers and seas existed then. Cratons consist primarily of two alternating types of terranes. The first are so-called greenstone belts, consisting of low-grade metamorphosed sedimentary rocks. These \"greenstones\" are similar to the sediments today found in oceanic trenches, above subduction zones. For this reason, greenstones are sometimes seen as evidence for subduction during the Archean. The second type is a complex of felsic magmatic rocks. These rocks are mostly tonalite, trondhjemite or granodiorite, types of rock similar in composition to granite (hence such terranes are called TTG-terranes). TTG-complexes are seen as the relicts of the first continental crust, formed by partial melting in basalt.\n\nEarth is often described as having had three atmospheres. The first atmosphere, captured from the solar nebula, was composed of light (atmophile) elements from the solar nebula, mostly hydrogen and helium. A combination of the solar wind and Earth's heat would have driven off this atmosphere, as a result of which the atmosphere is now depleted of these elements compared to cosmic abundances. After the impact which created the moon, the molten Earth released volatile gases; and later more gases were released by volcanoes, completing a second atmosphere rich in greenhouse gases but poor in oxygen. Finally, the third atmosphere, rich in oxygen, emerged when bacteria began to produce oxygen about 2.8 Ga.\n\nIn early models for the formation of the atmosphere and ocean, the second atmosphere was formed by outgassing of volatiles from the Earth's interior. Now it is considered likely that many of the volatiles were delivered during accretion by a process known as \"impact degassing\" in which incoming bodies vaporize on impact. The ocean and atmosphere would, therefore, have started to form even as the Earth formed. The new atmosphere probably contained water vapor, carbon dioxide, nitrogen, and smaller amounts of other gases.\n\nPlanetesimals at a distance of 1 astronomical unit (AU), the distance of the Earth from the Sun, probably did not contribute any water to the Earth because the solar nebula was too hot for ice to form and the hydration of rocks by water vapor would have taken too long. The water must have been supplied by meteorites from the outer asteroid belt and some large planetary embryos from beyond 2.5 AU. Comets may also have contributed. Though most comets are today in orbits farther away from the Sun than Neptune, computer simulations show that they were originally far more common in the inner parts of the solar system.\n\nAs the Earth cooled, clouds formed. Rain created the oceans. Recent evidence suggests the oceans may have begun forming as early as 4.4 Ga. By the start of the Archean eon, they already covered much of the Earth. This early formation has been difficult to explain because of a problem known as the faint young Sun paradox. Stars are known to get brighter as they age, and at the time of its formation the Sun would have been emitting only 70% of its current power. Thus, the Sun has become 30% brighter in the last 4.5 billion years. Many models indicate that the Earth would have been covered in ice. A likely solution is that there was enough carbon dioxide and methane to produce a greenhouse effect. The carbon dioxide would have been produced by volcanoes and the methane by early microbes. Another greenhouse gas, ammonia, would have been ejected by volcanos but quickly destroyed by ultraviolet radiation.\n\nOne of the reasons for interest in the early atmosphere and ocean is that they form the conditions under which life first arose. There are many models, but little consensus, on how life emerged from non-living chemicals; chemical systems created in the laboratory fall well short of the minimum complexity for a living organism.\n\nThe first step in the emergence of life may have been chemical reactions that produced many of the simpler organic compounds, including nucleobases and amino acids, that are the building blocks of life. An experiment in 1953 by Stanley Miller and Harold Urey showed that such molecules could form in an atmosphere of water, methane, ammonia and hydrogen with the aid of sparks to mimic the effect of lightning. Although atmospheric composition was probably different from that used by Miller and Urey, later experiments with more realistic compositions also managed to synthesize organic molecules. Computer simulations show that extraterrestrial organic molecules could have formed in the protoplanetary disk before the formation of the Earth.\n\nAdditional complexity could have been reached from at least three possible starting points: self-replication, an organism's ability to produce offspring that are similar to itself; metabolism, its ability to feed and repair itself; and external cell membranes, which allow food to enter and waste products to leave, but exclude unwanted substances.\n\nEven the simplest members of the three modern domains of life use DNA to record their \"recipes\" and a complex array of RNA and protein molecules to \"read\" these instructions and use them for growth, maintenance, and self-replication.\n\nThe discovery that a kind of RNA molecule called a ribozyme can catalyze both its own replication and the construction of proteins led to the hypothesis that earlier life-forms were based entirely on RNA. They could have formed an RNA world in which there were individuals but no species, as mutations and horizontal gene transfers would have meant that the offspring in each generation were quite likely to have different genomes from those that their parents started with. RNA would later have been replaced by DNA, which is more stable and therefore can build longer genomes, expanding the range of capabilities a single organism can have. Ribozymes remain as the main components of ribosomes, the \"protein factories\" of modern cells.\n\nAlthough short, self-replicating RNA molecules have been artificially produced in laboratories, doubts have been raised about whether natural non-biological synthesis of RNA is possible. The earliest ribozymes may have been formed of simpler nucleic acids such as PNA, TNA or GNA, which would have been replaced later by RNA. Other pre-RNA replicators have been posited, including crystals and even quantum systems.\n\nIn 2003 it was proposed that porous metal sulfide precipitates would assist RNA synthesis at about and at ocean-bottom pressures near hydrothermal vents. In this hypothesis, the proto-cells would be confined in the pores of the metal substrate until the later development of lipid membranes.\n\nAnother long-standing hypothesis is that the first life was composed of protein molecules. Amino acids, the building blocks of proteins, are easily synthesized in plausible prebiotic conditions, as are small peptides (polymers of amino acids) that make good catalysts. A series of experiments starting in 1997 showed that amino acids and peptides could form in the presence of carbon monoxide and hydrogen sulfide with iron sulfide and nickel sulfide as catalysts. Most of the steps in their assembly required temperatures of about and moderate pressures, although one stage required and a pressure equivalent to that found under of rock. Hence, self-sustaining synthesis of proteins could have occurred near hydrothermal vents.\n\nA difficulty with the metabolism-first scenario is finding a way for organisms to evolve. Without the ability to replicate as individuals, aggregates of molecules would have \"compositional genomes\" (counts of molecular species in the aggregate) as the target of natural selection. However, a recent model shows that such a system is unable to evolve in response to natural selection.\n\nIt has been suggested that double-walled \"bubbles\" of lipids like those that form the external membranes of cells may have been an essential first step. Experiments that simulated the conditions of the early Earth have reported the formation of lipids, and these can spontaneously form liposomes, double-walled \"bubbles\", and then reproduce themselves. Although they are not intrinsically information-carriers as nucleic acids are, they would be subject to natural selection for longevity and reproduction. Nucleic acids such as RNA might then have formed more easily within the liposomes than they would have outside.\n\nSome clays, notably montmorillonite, have properties that make them plausible accelerators for the emergence of an RNA world: they grow by self-replication of their crystalline pattern, are subject to an analog of natural selection (as the clay \"species\" that grows fastest in a particular environment rapidly becomes dominant), and can catalyze the formation of RNA molecules. Although this idea has not become the scientific consensus, it still has active supporters.\n\nResearch in 2003 reported that montmorillonite could also accelerate the conversion of fatty acids into \"bubbles\", and that the bubbles could encapsulate RNA attached to the clay. Bubbles can then grow by absorbing additional lipids and dividing. The formation of the earliest cells may have been aided by similar processes.\n\nA similar hypothesis presents self-replicating iron-rich clays as the progenitors of nucleotides, lipids and amino acids.\n\nIt is believed that of this multiplicity of protocells, only one line survived. Current phylogenetic evidence suggests that the last universal ancestor (LUA) lived during the early Archean eon, perhaps 3.5 Ga or earlier. This LUA cell is the ancestor of all life on Earth today. It was probably a prokaryote, possessing a cell membrane and probably ribosomes, but lacking a nucleus or membrane-bound organelles such as mitochondria or chloroplasts. Like modern cells, it used DNA as its genetic code, RNA for information transfer and protein synthesis, and enzymes to catalyze reactions. Some scientists believe that instead of a single organism being the last universal common ancestor, there were populations of organisms exchanging genes by lateral gene transfer.\n\nThe Proterozoic eon lasted from 2.5 Ga to 542 Ma (million years) ago. In this time span, cratons grew into continents with modern sizes. The change to an oxygen-rich atmosphere was a crucial development. Life developed from prokaryotes into eukaryotes and multicellular forms. The Proterozoic saw a couple of severe ice ages called snowball Earths. After the last Snowball Earth about 600 Ma, the evolution of life on Earth accelerated. About 580 Ma, the Ediacaran biota formed the prelude for the Cambrian Explosion.\n\nThe earliest cells absorbed energy and food from the surrounding environment. They used fermentation, the breakdown of more complex compounds into less complex compounds with less energy, and used the energy so liberated to grow and reproduce. Fermentation can only occur in an \"anaerobic\" (oxygen-free) environment. The evolution of photosynthesis made it possible for cells to derive energy from the Sun.\n\nMost of the life that covers the surface of the Earth depends directly or indirectly on photosynthesis. The most common form, oxygenic photosynthesis, turns carbon dioxide, water, and sunlight into food. It captures the energy of sunlight in energy-rich molecules such as ATP, which then provide the energy to make sugars. To supply the electrons in the circuit, hydrogen is stripped from water, leaving oxygen as a waste product. Some organisms, including purple bacteria and green sulfur bacteria, use an anoxygenic form of photosynthesis that uses alternatives to hydrogen stripped from water as electron donors; examples are hydrogen sulfide, sulfur and iron. Such extremophile organisms are restricted to otherwise inhospitable environments such as hot springs and hydrothermal vents.\n\nThe simpler anoxygenic form arose about 3.8 Ga, not long after the appearance of life. The timing of oxygenic photosynthesis is more controversial; it had certainly appeared by about 2.4 Ga, but some researchers put it back as far as 3.2 Ga. The latter \"probably increased global productivity by at least two or three orders of magnitude\". Among the oldest remnants of oxygen-producing lifeforms are fossil stromatolites.\n\nAt first, the released oxygen was bound up with limestone, iron, and other minerals. The oxidized iron appears as red layers in geological strata called banded iron formations that formed in abundance during the Siderian period (between 2500 Ma and 2300 Ma). When most of the exposed readily reacting minerals were oxidized, oxygen finally began to accumulate in the atmosphere. Though each cell only produced a minute amount of oxygen, the combined metabolism of many cells over a vast time transformed Earth's atmosphere to its current state. This was Earth's third atmosphere.\n\nSome oxygen was stimulated by solar ultraviolet radiation to form ozone, which collected in a layer near the upper part of the atmosphere. The ozone layer absorbed, and still absorbs, a significant amount of the ultraviolet radiation that once had passed through the atmosphere. It allowed cells to colonize the surface of the ocean and eventually the land: without the ozone layer, ultraviolet radiation bombarding land and sea would have caused unsustainable levels of mutation in exposed cells.\n\nPhotosynthesis had another major impact. Oxygen was toxic; much life on Earth probably died out as its levels rose in what is known as the \"oxygen catastrophe\". Resistant forms survived and thrived, and some developed the ability to use oxygen to increase their metabolism and obtain more energy from the same food.\n\nThe natural evolution of the Sun made it progressively more luminous during the Archean and Proterozoic eons; the Sun's luminosity increases 6% every billion years. As a result, the Earth began to receive more heat from the Sun in the Proterozoic eon. However, the Earth did not get warmer. Instead, the geological record suggests it cooled dramatically during the early Proterozoic. Glacial deposits found in South Africa date back to 2.2 Ga, at which time, based on paleomagnetic evidence, they must have been located near the equator. Thus, this glaciation, known as the Huronian glaciation, may have been global. Some scientists suggest this was so severe that the Earth was frozen over from the poles to the equator, a hypothesis called Snowball Earth.\n\nThe Huronian ice age might have been caused by the increased oxygen concentration in the atmosphere, which caused the decrease of methane (CH) in the atmosphere. Methane is a strong greenhouse gas, but with oxygen it reacts to form CO, a less effective greenhouse gas. When free oxygen became available in the atmosphere, the concentration of methane could have decreased dramatically, enough to counter the effect of the increasing heat flow from the Sun.\n\nHowever, the term Snowball Earth is more commonly used to describe later extreme ice ages during the Cryogenian period. There were four periods, each lasting about 10 million years, between 750 and 580 million years ago, when the earth is thought to have been covered with ice apart from the highest mountains, and average temperatures were about . The snowball may have been partly due to the location of the supercontintent Rodinia straddling the Equator. Carbon dioxide combines with rain to weather rocks to form carbonic acid, which is then washed out to sea, thus extracting the greenhouse gas from the atmosphere. When the continents are near the poles, the advance of ice covers the rocks, slowing the reduction in carbon dioxide, but in the Cryogienian the weathering of Rodinia was able to continue unchecked until the ice advanced to the tropics. The process may have finally been reversed by the emission of carbon dioxide from volcanoes or the destabilization of methane gas hydrates. According to the alternative Slushball Earth theory, even at the height of the ice ages there was still open water at the Equator.\n\nModern taxonomy classifies life into three domains. The time of their origin is uncertain. The Bacteria domain probably first split off from the other forms of life (sometimes called Neomura), but this supposition is controversial. Soon after this, by 2 Ga, the Neomura split into the Archaea and the Eukarya. Eukaryotic cells (Eukarya) are larger and more complex than prokaryotic cells (Bacteria and Archaea), and the origin of that complexity is only now becoming known. \n\nAround this time, the first proto-mitochondrion was formed. A bacterial cell related to today's \"Rickettsia\", which had evolved to metabolize oxygen, entered a larger prokaryotic cell, which lacked that capability. Perhaps the large cell attempted to digest the smaller one but failed (possibly due to the evolution of prey defenses). The smaller cell may have tried to parasitize the larger one. In any case, the smaller cell survived inside the larger cell. Using oxygen, it metabolized the larger cell's waste products and derived more energy. Part of this excess energy was returned to the host. The smaller cell replicated inside the larger one. Soon, a stable symbiosis developed between the large cell and the smaller cells inside it. Over time, the host cell acquired some genes from the smaller cells, and the two kinds became dependent on each other: the larger cell could not survive without the energy produced by the smaller ones, and these, in turn, could not survive without the raw materials provided by the larger cell. The whole cell is now considered a single organism, and the smaller cells are classified as organelles called mitochondria.\n\nA similar event occurred with photosynthetic cyanobacteria entering large heterotrophic cells and becoming chloroplasts. Probably as a result of these changes, a line of cells capable of photosynthesis split off from the other eukaryotes more than 1 billion years ago. There were probably several such inclusion events. Besides the well-established endosymbiotic theory of the cellular origin of mitochondria and chloroplasts, there are theories that cells led to peroxisomes, spirochetes led to cilia and flagella, and that perhaps a DNA virus led to the cell nucleus, though none of them are widely accepted.\n\nArchaeans, bacteria, and eukaryotes continued to diversify and to become more complex and better adapted to their environments. Each domain repeatedly split into multiple lineages, although little is known about the history of the archaea and bacteria. Around 1.1 Ga, the supercontinent Rodinia was assembling. The plant, animal, and fungi lines had split, though they still existed as solitary cells. Some of these lived in colonies, and gradually a division of labor began to take place; for instance, cells on the periphery might have started to assume different roles from those in the interior. Although the division between a colony with specialized cells and a multicellular organism is not always clear, around 1 billion years ago, the first multicellular plants emerged, probably green algae. Possibly by around 900 Ma true multicellularity had also evolved in animals.\n\nAt first, it probably resembled today's sponges, which have totipotent cells that allow a disrupted organism to reassemble itself. As the division of labor was completed in all lines of multicellular organisms, cells became more specialized and more dependent on each other; isolated cells would die.\n\nReconstructions of tectonic plate movement in the past 250 million years (the Cenozoic and Mesozoic eras) can be made reliably using fitting of continental margins, ocean floor magnetic anomalies and paleomagnetic poles. No ocean crust dates back further than that, so earlier reconstructions are more difficult. Paleomagnetic poles are supplemented by geologic evidence such as orogenic belts, which mark the edges of ancient plates, and past distributions of flora and fauna. The further back in time, the scarcer and harder to interpret the data get and the more uncertain the reconstructions.\n\nThroughout the history of the Earth, there have been times when continents collided and formed a supercontinent, which later broke up into new continents. About 1000 to 830 Ma, most continental mass was united in the supercontinent Rodinia. Rodinia may have been preceded by Early-Middle Proterozoic continents called Nuna and Columbia.\n\nAfter the break-up of Rodinia about 800 Ma, the continents may have formed another short-lived supercontinent around 550 Ma. The hypothetical supercontinent is sometimes referred to as Pannotia or Vendia. The evidence for it is a phase of continental collision known as the Pan-African orogeny, which joined the continental masses of current-day Africa, South America, Antarctica and Australia. The existence of Pannotia depends on the timing of the rifting between Gondwana (which included most of the landmass now in the Southern Hemisphere, as well as the Arabian Peninsula and the Indian subcontinent) and Laurentia (roughly equivalent to current-day North America). It is at least certain that by the end of the Proterozoic eon, most of the continental mass lay united in a position around the south pole.\n\nThe end of the Proterozoic saw at least two Snowball Earths, so severe that the surface of the oceans may have been completely frozen. This happened about 716.5 and 635 Ma, in the Cryogenian period. The intensity and mechanism of both glaciations are still under investigation and harder to explain than the early Proterozoic Snowball Earth.\nMost paleoclimatologists think the cold episodes were linked to the formation of the supercontinent Rodinia. Because Rodinia was centered on the equator, rates of chemical weathering increased and carbon dioxide (CO) was taken from the atmosphere. Because CO is an important greenhouse gas, climates cooled globally.\nIn the same way, during the Snowball Earths most of the continental surface was covered with permafrost, which decreased chemical weathering again, leading to the end of the glaciations. An alternative hypothesis is that enough carbon dioxide escaped through volcanic outgassing that the resulting greenhouse effect raised global temperatures. Increased volcanic activity resulted from the break-up of Rodinia at about the same time.\n\nThe Cryogenian period was followed by the Ediacaran period, which was characterized by a rapid development of new multicellular lifeforms. Whether there is a connection between the end of the severe ice ages and the increase in diversity of life is not clear, but it does not seem coincidental. The new forms of life, called Ediacara biota, were larger and more diverse than ever. Though the taxonomy of most Ediacaran life forms is unclear, some were ancestors of groups of modern life. Important developments were the origin of muscular and neural cells. None of the Ediacaran fossils had hard body parts like skeletons. These first appear after the boundary between the Proterozoic and Phanerozoic eons or Ediacaran and Cambrian periods.\n\nThe Phanerozoic is the current eon on Earth, which started approximately 542 million years ago. It consists of three eras: The Paleozoic, Mesozoic, and Cenozoic, and is the time when multi-cellular life greatly diversified into almost all the organisms known today.\n\nThe Paleozoic (\"old life\") era was the first and longest era of the Phanerozoic eon, lasting from 542 to 251 Ma. During the Paleozoic, many modern groups of life came into existence. Life colonized the land, first plants, then animals. Two major extinctions occurred. The continents formed at the break-up of Pannotia and Rodinia at the end of the Proterozoic slowly moved together again, forming the supercontinent Pangaea in the late Paleozoic.\n\nThe Mesozoic (\"middle life\") era lasted from 251 Ma to  Ma. It is subdivided into the Triassic, Jurassic, and Cretaceous periods. The era began with the Permian–Triassic extinction event, the most severe extinction event in the fossil record; 95% of the species on Earth died out. It ended with the Cretaceous–Paleogene extinction event that wiped out the dinosaurs..\n\nThe Cenozoic (\"new life\") era began at  Ma, and is subdivided into the Paleogene, Neogene, and Quaternary periods. These three periods are further split into seven sub-divisions, with the Paleogene composed of The Paleocene, Eocene, and Oligocene, the Neogene divided into the Miocene, Pliocene, and the Quaternary composed of the Pleistocene, and Holocene. Mammals, birds, amphibians, crocodilians, turtles, and lepidosaurs survived the Cretaceous–Paleogene extinction event that killed off the non-avian dinosaurs and many other forms of life, and this is the era during which they diversified into their modern forms.\n\nAt the end of the Proterozoic, the supercontinent Pannotia had broken apart into the smaller continents Laurentia, Baltica, Siberia and Gondwana. During periods when continents move apart, more oceanic crust is formed by volcanic activity. Because young volcanic crust is relatively hotter and less dense than old oceanic crust, the ocean floors rise during such periods. This causes the sea level to rise. Therefore, in the first half of the Paleozoic, large areas of the continents were below sea level.\n\nEarly Paleozoic climates were warmer than today, but the end of the Ordovician saw a short ice age during which glaciers covered the south pole, where the huge continent Gondwana was situated. Traces of glaciation from this period are only found on former Gondwana. During the Late Ordovician ice age, a few mass extinctions took place, in which many brachiopods, trilobites, Bryozoa and corals disappeared. These marine species could probably not contend with the decreasing temperature of the sea water.\n\nThe continents Laurentia and Baltica collided between 450 and 400 Ma, during the Caledonian Orogeny, to form Laurussia (also known as Euramerica). Traces of the mountain belt this collision caused can be found in Scandinavia, Scotland, and the northern Appalachians. In the Devonian period (416–359 Ma) Gondwana and Siberia began to move towards Laurussia. The collision of Siberia with Laurussia caused the Uralian Orogeny, the collision of Gondwana with Laurussia is called the Variscan or Hercynian Orogeny in Europe or the Alleghenian Orogeny in North America. The latter phase took place during the Carboniferous period (359–299 Ma) and resulted in the formation of the last supercontinent, Pangaea.\n\nBy 180 Ma, Pangaea broke up into Laurasia and Gondwana.\n\nThe rate of the evolution of life as recorded by fossils accelerated in the Cambrian period (542–488 Ma). The sudden emergence of many new species, phyla, and forms in this period is called the Cambrian Explosion. The biological fomenting in the Cambrian Explosion was unpreceded before and since that time. Whereas the Ediacaran life forms appear yet primitive and not easy to put in any modern group, at the end of the Cambrian most modern phyla were already present. The development of hard body parts such as shells, skeletons or exoskeletons in animals like molluscs, echinoderms, crinoids and arthropods (a well-known group of arthropods from the lower Paleozoic are the trilobites) made the preservation and fossilization of such life forms easier than those of their Proterozoic ancestors. For this reason, much more is known about life in and after the Cambrian than about that of older periods. Some of these Cambrian groups appear complex but are seemingly quite different from modern life; examples are \"Anomalocaris\" and \"Haikouichthys\". More recently, however, these seem to have found a place in modern classification. \n\nDuring the Cambrian, the first vertebrate animals, among them the first fishes, had appeared. A creature that could have been the ancestor of the fishes, or was probably closely related to it, was \"Pikaia\". It had a primitive notochord, a structure that could have developed into a vertebral column later. The first fishes with jaws (Gnathostomata) appeared during the next geological period, the Ordovician. The colonisation of new niches resulted in massive body sizes. In this way, fishes with increasing sizes evolved during the early Paleozoic, such as the titanic placoderm \"Dunkleosteus\", which could grow long.\n\nThe diversity of life forms did not increase greatly because of a series of mass extinctions that define widespread biostratigraphic units called \"biomeres\". After each extinction pulse, the continental shelf regions were repopulated by similar life forms that may have been evolving slowly elsewhere. By the late Cambrian, the trilobites had reached their greatest diversity and dominated nearly all fossil assemblages.\n\nOxygen accumulation from photosynthesis resulted in the formation of an ozone layer that absorbed much of the Sun's ultraviolet radiation, meaning unicellular organisms that reached land were less likely to die, and prokaryotes began to multiply and become better adapted to survival out of the water. Prokaryote lineages had probably colonized the land as early as 2.6 Ga even before the origin of the eukaryotes. For a long time, the land remained barren of multicellular organisms. The supercontinent Pannotia formed around 600 Ma and then broke apart a short 50 million years later. Fish, the earliest vertebrates, evolved in the oceans around 530 Ma. A major extinction event occurred near the end of the Cambrian period, which ended 488 Ma.\n\nSeveral hundred million years ago, plants (probably resembling algae) and fungi started growing at the edges of the water, and then out of it. The oldest fossils of land fungi and plants date to 480–460 Ma, though molecular evidence suggests the fungi may have colonized the land as early as 1000 Ma and the plants 700 Ma. Initially remaining close to the water's edge, mutations and variations resulted in further colonization of this new environment. The timing of the first animals to leave the oceans is not precisely known: the oldest clear evidence is of arthropods on land around 450 Ma, perhaps thriving and becoming better adapted due to the vast food source provided by the terrestrial plants. There is also unconfirmed evidence that arthropods may have appeared on land as early as 530 Ma.\n\nAt the end of the Ordovician period, 443 Ma, additional extinction events occurred, perhaps due to a concurrent ice age. Around 380 to 375 Ma, the first tetrapods evolved from fish. Fins evolved to become limbs that the first tetrapods used to lift their heads out of the water to breathe air. This would let them live in oxygen-poor water, or pursue small prey in shallow water. They may have later ventured on land for brief periods. Eventually, some of them became so well adapted to terrestrial life that they spent their adult lives on land, although they hatched in the water and returned to lay their eggs. This was the origin of the amphibians. About 365 Ma, another period of extinction occurred, perhaps as a result of global cooling. Plants evolved seeds, which dramatically accelerated their spread on land, around this time (by approximately 360 Ma).\n\nAbout 20 million years later (340 Ma), the amniotic egg evolved, which could be laid on land, giving a survival advantage to tetrapod embryos. This resulted in the divergence of amniotes from amphibians. Another 30 million years (310 Ma) saw the divergence of the synapsids (including mammals) from the sauropsids (including birds and reptiles). Other groups of organisms continued to evolve, and lines diverged—in fish, insects, bacteria, and so on—but less is known of the details.\nAfter yet another, the most severe extinction of the period (251~250 Ma), around 230 Ma, dinosaurs split off from their reptilian ancestors. The Triassic–Jurassic extinction event at 200 Ma spared many of the dinosaurs, and they soon became dominant among the vertebrates. Though some mammalian lines began to separate during this period, existing mammals were probably small animals resembling shrews.\n\nThe boundary between avian and non-avian dinosaurs is not clear, but \"Archaeopteryx\", traditionally considered one of the first birds, lived around 150 Ma.\n\nThe earliest evidence for the angiosperms evolving flowers is during the Cretaceous period, some 20 million years later (132 Ma).\n\nThe first of five great mass extinctions was the Ordovician-Silurian extinction. Its possible cause was the intense glaciation of Gondwana, which eventually led to a snowball earth. 60% of marine invertebrates became extinct and 25% of all families.\n\nThe second mass extinction was the Late Devonian extinction, probably caused by the evolution of trees, which could have led to the depletion of greenhouse gases (like CO2) or the eutrophication of water. 70% of all species became extinct.\n\nThe third mass extinction was the Permian-Triassic, or the Great Dying, event was possibly caused by some combination of the Siberian Traps volcanic event, an asteroid impact, methane hydrate gasification, sea level fluctuations, and a major anoxic event. Either the proposed Wilkes Land crater in Antarctica or Bedout structure off the northwest coast of Australia may indicate an impact connection with the Permian-Triassic extinction. But it remains uncertain whether either these or other proposed Permian-Triassic boundary craters are either real impact craters or even contemporaneous with the Permian-Triassic extinction event. This was by far the deadliest extinction ever, with about 57% of all families and 83% of all genera killed.\n\nThe fourth mass extinction was the Triassic-Jurassic extinction event in which almost all synapsids and archosaurs became extinct, probably due to new competition from dinosaurs.\n\nThe fifth and most recent mass extinction was the K-T extinction. In 66 Ma, a asteroid struck Earth just off the Yucatán Peninsula – somewhere in the south western tip of then Laurasia – where the Chicxulub crater is today. This ejected vast quantities of particulate matter and vapor into the air that occluded sunlight, inhibiting photosynthesis. 75% of all life, including the non-avian dinosaurs, became extinct, marking the end of the Cretaceous period and Mesozoic era.\n\nThe first true mammals evolved in the shadows of dinosaurs and other large archosaurs that filled the world by the late Triassic. The first mammals were very small, and were probably nocturnal to escape predation. Mammal diversification truly began only after the Cretaceous-Paleogene extinction event. By the early Paleocene the earth recovered from the extinction, and mammalian diversity increased. Creatures like \"Ambulocetus\" took to the oceans to eventually evolve into whales, whereas some creatures, like primates, took to the trees. This all changed during the mid to late Eocene when the circum-Antarctic current formed between Antarctica and Australia which disrupted weather patterns on a global scale. Grassless savannas began to predominate much of the landscape, and mammals such as \"Andrewsarchus\" rose up to become the largest known terrestrial predatory mammal ever, and early whales like \"Basilosaurus\" took control of the seas. \n\nThe evolution of grass brought a remarkable change to the Earth's landscape, and the new open spaces created pushed mammals to get bigger and bigger. Grass started to expand in the Miocene, and the Miocene is where many modern- day mammals first appeared. Giant ungulates like \"Paraceratherium\" and \"Deinotherium\" evolved to rule the grasslands. The evolution of grass also brought primates down from the trees, and started human evolution. The first big cats evolved during this time as well. The Tethys Sea was closed off by the collision of Africa and Europe.\n\nThe formation of Panama was perhaps the most important geological event to occur in the last 60 million years. Atlantic and Pacific currents were closed off from each other, which caused the formation of the Gulf Stream, which made Europe warmer. The land bridge allowed the isolated creatures of South America to migrate over to North America, and vice versa. Various species migrated south, leading to the presence in South America of llamas, the spectacled bear, kinkajous and jaguars.\n\nThree million years ago saw the start of the Pleistocene epoch, which featured dramatic climactic changes due to the ice ages. The ice ages led to the evolution of modern man in Saharan Africa and expansion. The mega-fauna that dominated fed on grasslands that, by now, had taken over much of the subtropical world. The large amounts of water held in the ice allowed for various bodies of water to shrink and sometimes disappear such as the North Sea and the Bering Strait. It is believed by many that a huge migration took place along Beringia which is why, today, there are camels (which evolved and became extinct in North America), horses (which evolved and became extinct in North America), and Native Americans. The ending of the last ice age coincided with the expansion of man, along with a massive die out of ice age mega-fauna. This extinction, nicknamed \"the Sixth Extinction\", has been going ever since.\n\nA small African ape living around 6 Ma was the last animal whose descendants would include both modern humans and their closest relatives, the chimpanzees. Only two branches of its family tree have surviving descendants. Very soon after the split, for reasons that are still unclear, apes in one branch developed the ability to walk upright. Brain size increased rapidly, and by 2 Ma, the first animals classified in the genus \"Homo\" had appeared. Of course, the line between different species or even genera is somewhat arbitrary as organisms continuously change over generations. Around the same time, the other branch split into the ancestors of the common chimpanzee and the ancestors of the bonobo as evolution continued simultaneously in all life forms.\n\nThe ability to control fire probably began in \"Homo erectus\" (or \"Homo ergaster\"), probably at least 790,000 years ago but perhaps as early as 1.5 Ma. The use and discovery of controlled fire may even predate \"Homo erectus\". Fire was possibly used by the early Lower Paleolithic (Oldowan) hominid \"Homo habilis\" or strong australopithecines such as \"Paranthropus.\"\n\nIt is more difficult to establish the origin of language; it is unclear whether \"Homo erectus\" could speak or if that capability had not begun until \"Homo sapiens\". As brain size increased, babies were born earlier, before their heads grew too large to pass through the pelvis. As a result, they exhibited more plasticity, and thus possessed an increased capacity to learn and required a longer period of dependence. Social skills became more complex, language became more sophisticated, and tools became more elaborate. This contributed to further cooperation and intellectual development. Modern humans (\"Homo sapiens\") are believed to have originated around 200,000 years ago or earlier in Africa; the oldest fossils date back to around 160,000 years ago.\n\nThe first humans to show signs of spirituality are the Neanderthals (usually classified as a separate species with no surviving descendants); they buried their dead, often with no sign of food or tools. However, evidence of more sophisticated beliefs, such as the early Cro-Magnon cave paintings (probably with magical or religious significance) did not appear until 32,000 years ago. Cro-Magnons also left behind stone figurines such as Venus of Willendorf, probably also signifying religious belief. By 11,000 years ago, \"Homo sapiens\" had reached the southern tip of South America, the last of the uninhabited continents (except for Antarctica, which remained undiscovered until 1820 AD). Tool use and communication continued to improve, and interpersonal relationships became more intricate.\n\nThroughout more than 90% of its history, \"Homo sapiens\" lived in small bands as nomadic hunter-gatherers. As language became more complex, the ability to remember and communicate information resulted, according to a theory proposed by Richard Dawkins, in a new replicator: the meme. Ideas could be exchanged quickly and passed down the generations. Cultural evolution quickly outpaced biological evolution, and history proper began. Between 8500 and 7000 BC, humans in the Fertile Crescent in the Middle East began the systematic husbandry of plants and animals: agriculture. This spread to neighboring regions, and developed independently elsewhere, until most \"Homo sapiens\" lived sedentary lives in permanent settlements as farmers. Not all societies abandoned nomadism, especially those in isolated areas of the globe poor in domesticable plant species, such as Australia. However, among those civilizations that did adopt agriculture, the relative stability and increased productivity provided by farming allowed the population to expand.\n\nAgriculture had a major impact; humans began to affect the environment as never before. Surplus food allowed a priestly or governing class to arise, followed by increasing division of labor. This led to Earth's first civilization at Sumer in the Middle East, between 4000 and 3000 BC. Additional civilizations quickly arose in ancient Egypt, at the Indus River valley and in China. The invention of writing enabled complex societies to arise: record-keeping and libraries served as a storehouse of knowledge and increased the cultural transmission of information. Humans no longer had to spend all their time working for survival, enabling the first specialized occupations (e.g. craftsmen, merchants, priests, etc...). Curiosity and education drove the pursuit of knowledge and wisdom, and various disciplines, including science (in a primitive form), arose. This in turn led to the emergence of increasingly larger and more complex civilizations, such as the first empires, which at times traded with one another, or fought for territory and resources.\n\nBy around 500 BC, there were advanced civilizations in the Middle East, Iran, India, China, and Greece, at times expanding, at times entering into decline. In 221 BC, China became a single polity that would grow to spread its culture throughout East Asia, and it has remained the most populous nation in the world. The fundamentals of Western civilization were largely shaped in Ancient Greece, with the world's first democratic government and major advances in philosophy, science, and mathematics, and in Ancient Rome in law, government, and engineering. The Roman Empire was Christianized by Emperor Constantine in the early 4th century and declined by the end of the 5th. Beginning with the 7th century, Christianization of Europe began. In 610, Islam was founded and quickly became the dominant religion in Western Asia. The House of Wisdom was established in Abbasid-era Baghdad, Iraq. It is considered to have been a major intellectual center during the Islamic Golden Age, where Muslim scholars in Baghdad and Cairo flourished from the ninth to the thirteenth centuries until the Mongol sack of Baghdad in 1258 AD. In 1054 AD the Great Schism between the Roman Catholic Church and the Eastern Orthodox Church led to the prominent cultural differences between Western and Eastern Europe.\n\nIn the 14th century, the Renaissance began in Italy with advances in religion, art, and science. At that time the Christian Church as a political entity lost much of its power. In 1492, Christopher Columbus reached the Americas, initiating great changes to the new world. European civilization began to change beginning in 1500, leading to the scientific and industrial revolutions. That continent began to exert political and cultural dominance over human societies around the world, a time known as the Colonial era (also see Age of Discovery). In the 18th century a cultural movement known as the Age of Enlightenment further shaped the mentality of Europe and contributed to its secularization. From 1914 to 1918 and 1939 to 1945, nations around the world were embroiled in world wars. Established following World War I, the League of Nations was a first step in establishing international institutions to settle disputes peacefully. After failing to prevent World War II, mankind's bloodiest conflict, it was replaced by the United Nations. After the war, many new states were formed, declaring or being granted independence in a period of decolonization. The United States and Soviet Union became the world's dominant superpowers for a time, and they held an often-violent rivalry known as the Cold War until the dissolution of the latter. In 1992, several European nations joined in the European Union. As transportation and communication improved, the economies and political affairs of nations around the world have become increasingly intertwined. This globalization has often produced both conflict and cooperation.\n\nChange has continued at a rapid pace from the mid-1940s to today. Technological developments include nuclear weapons, computers, genetic engineering, and nanotechnology. Economic globalization, spurred by advances in communication and transportation technology, has influenced everyday life in many parts of the world. Cultural and institutional forms such as democracy, capitalism, and environmentalism have increased influence. Major concerns and problems such as disease, war, poverty, violent radicalism, and recently, human-caused climate change have risen as the world population increases.\n\nIn 1957, the Soviet Union launched the first artificial satellite into orbit and, soon afterward, Yuri Gagarin became the first human in space. Neil Armstrong, an American, was the first to set foot on another astronomical object, the Moon. Unmanned probes have been sent to all the known planets in the solar system, with some (such as Voyager) having left the solar system. Five space agencies, representing over fifteen countries, have worked together to build the International Space Station. Aboard it, there has been a continuous human presence in space since 2000. The World Wide Web became a part of everyday life in the 1990s, and since then has become an indispensable source of information in the developed world.\n\n"}
{"id": "4538124", "url": "https://en.wikipedia.org/wiki?curid=4538124", "title": "Integrated gasification combined cycle", "text": "Integrated gasification combined cycle\n\nAn integrated gasification combined cycle (IGCC) is a technology that uses a high pressure gasifier to turn coal and other carbon based fuels into pressurized gas—synthesis gas (syngas). It can then remove impurities from the syngas prior to the power generation cycle. Some of these pollutants, such as sulfur, can be turned into re-usable byproducts through the Claus process. This results in lower emissions of sulfur dioxide, particulates, mercury, and in some cases carbon dioxide. With additional process equipment, a water-gas shift reaction can increase gasification efficiency and reduce carbon monoxide emissions by converting it to carbon dioxide. The resulting carbon dioxide from the shift reaction can be separated, compressed, and stored through sequestration. Excess heat from the primary combustion and syngas fired generation is then passed to a steam cycle, similar to a combined cycle gas turbine. This process results in improved thermodynamic efficiency compared to conventional pulverized coal combustion.\n\nCoal can be found in abundance in the USA and many other countries and its price has remained relatively constant in recent years. Of the traditional fossil fuels - oil, coal, and natural gas - coal is used as a feedstock for 40% of global electricity generation. Fossil fuel consumption and its contribution to large-scale, detrimental environmental changes is becoming a pressing issue, especially in light of the Paris Agreement. In particular, coal contains more CO per BTU than oil or natural gas and is responsible for 43% of CO emissions from fuel combustion. Thus, the lower emissions that IGCC technology allows through gasification and pre-combustion carbon capture is discussed as a way to addressing aforementioned concerns.\n\nBelow is a schematic flow diagram of an IGCC plant:\nThe gasification process can produce syngas from a wide variety of carbon-containing feedstocks, such as high-sulfur coal, heavy petroleum residues, and biomass.\n\nThe plant is called \"integrated\" because (1) the syngas produced in the gasification section is used as fuel for the gas turbine in the combined cycle and (2) the steam produced by the syngas coolers in the gasification section is used by the steam turbine in the combined cycle.\nIn this example the syngas produced is used as fuel in a gas turbine which produces electrical power. In a normal combined cycle, so-called \"waste heat\" from the gas turbine exhaust is used in a Heat Recovery Steam Generator (HRSG) to make steam for the steam turbine cycle. An IGCC plant improves the overall process efficiency by adding the higher-temperature steam produced by the gasification process to the steam turbine cycle. This steam is then used in steam turbines to produce additional electrical power.\n\nIGCC plants are advantageous in comparison to conventional coal power plants due to their high thermal efficiency, low non-carbon greenhouse gas emissions, and capability to process low grade coal. The disadvantages include higher capital and maintenance costs, and the amount of CO2 released without pre-combustion capture.\n\n\nA major drawback of using coal as a fuel source is the emission of carbon dioxide and other pollutants, including sulfur dioxide, nitrogen oxide, mercury, and particulates. Almost all coal-fired power plants use pulverized coal combustion, which grinds the coal to increase the surface area, burns it to make steam, and runs the steam through a turbine to generate electricity. Pulverized coal plants can only capture carbon dioxide after combustion when it is diluted and harder to separate. In comparison, gasification in IGCC allows for separation and capture of the concentrated and pressurized carbon dioxide before combustion. Syngas cleanup includes filters to remove bulk particulates, scrubbing to remove fine particulates, and solid adsorbents for mercury removal. Additionally, hydrogen gas is used as fuel, which produces no pollutants under combustion.\n\nIGCC also consumes less water than traditional pulverized coal plants. In a pulverized coal plant, coal is burned to produce steam, which is then used to create electricity using a steam turbine. Then steam exhaust must then be condensed with cooling water, and water is lost by evaporation. In IGCC, water consumption is reduced by combustion in a gas turbine, which uses the generated heat to expand air and drive the turbine. Steam is only used to capture the heat from the combustion turbine exhaust for use in a secondary steam turbine. Currently, the major drawback is the high capital cost compared to other forms of power production.\n\nThe DOE Clean Coal Demonstration Project helped construct 3 IGCC plants: Edwarsport Power Station in Edwardsport, Indiana, Polk Power Station in Tampa, Florida (online 1996), and Pinon Pine in Reno, Nevada. In the Reno demonstration project, researchers found that then-current IGCC technology would not work more than 300 feet (100m) above sea level. The DOE report in reference 3 however makes no mention of any altitude effect, and most of the problems were associated with the solid waste extraction system. The Wabash River and Polk Power stations are currently operating, following resolution of demonstration start-up problems, but the Piñon Pine project encountered significant problems and was abandoned.\n\nThe US DOE's Clean Coal Power Initiative (CCPI Phase 2) selected the Kemper Project as one of two projects to demonstrate the feasibility of low emission coal-fired power plants. Mississippi Power began construction on the Kemper Project in Kemper County, Mississippi, in 2010 and is poised to begin operation in 2016, though there have been many delays. In March, the projected date was further pushed back from early 2016 to August 31, 2016, adding $110 million to the total and putting the project 3 years behind schedule. The electrical plant is a flagship Carbon Capture and Storage (CCS) project that burns lignite coal and utilizes pre-combustion IGCC technology with a projected 65% emission capture rate.\n\nThe first generation of IGCC plants polluted less than contemporary coal-based technology, but also polluted water; for example, the Wabash River Plant was out of compliance with its water permit during 1998–2001\nbecause it emitted arsenic, selenium and cyanide. The Wabash River Generating Station is now wholly owned and operated by the Wabash River Power Association.\n\nIGCC is now touted as \"capture ready\" and could potentially be used to capture and store carbon dioxide. (See FutureGen)Poland's Kędzierzyn will soon host a Zero-Emission Power & Chemical Plant that combines coal gasification technology with Carbon Capture & Storage (CCS). This installation had been planned, but there has been no information about it since 2009. Other operating IGCC plants in existence around the world are the Alexander (formerly Buggenum) in the Netherlands, Puertollano in Spain, and JGC in Japan.\n\nThe Texas Clean Energy project plans to build a 400 MW IGCC facility that will incorporate carbon capture, utilization and storage (CCUS) technology. The project will be the first coal power plant in the United States to combine IGCC and 90% carbon capture and storage. Commercial operation is due to start in 2018.\n\nThere are several advantages and disadvantages when compared to conventional post combustion carbon capture and various variations \nA key issue in implementing IGCC is its high capital cost, which prevents it from competing with other power plant technologies. Currently, ordinary pulverized coal plants are the lowest cost power plant option. The advantage of IGCC comes from the ease of retrofitting existing power plants that could offset the high capital cost. In a 2007 model, IGCC with CCS is the lowest-cost system in all cases. This model compared estimations of levelized cost of electricity, showing IGCC with CCS to cost 71.9 $US2005/MWh, pulverized coal with CCS to cost 88 $US2005/MWh, and natural gas combined cycle with CCS to cost 80.6 $US2005/MWh. The levelized cost of electricity was noticeably sensitive to the price of natural gas and the inclusion of carbon storage and transport costs.\n\nThe potential benefit of retrofitting has so far, not offset the cost of IGCC with carbon capture technology. A 2013 report by the U.S. Energy Information Administration demonstrates that the overnight cost of IGCC with CCS has increased 19% since 2010. Amongst the three power plant types, pulverized coal with CCS has an overnight capital cost of $5,227 (2012 dollars)/kW, IGCC with CCS has an overnight capital cost of $6,599 (2012 dollars)/kW, and natural gas combined cycle with CCS has an overnight capital cost of $2,095 (2012 dollars)/kW. Pulverized coal and NGCC costs did not change significantly since 2010. The report further relates that the 19% increase in IGCC cost is due to recent information from IGCC projects that have gone over budget and cost more than expected.\n\nRecent testimony in regulatory proceedings show the cost of IGCC to be twice that predicted by Goddell, from $96 to 104/MWhr. That's before addition of carbon capture and sequestration (sequestration has been a mature technology at both Weyburn in Canada (for enhanced oil recovery) and Sleipner in the North Sea at a commercial scale for the past ten years)—capture at a 90% rate is expected to have a $30/MWh additional cost.\n\nWabash River was down repeatedly for long stretches due to gasifier problems. The gasifier problems have not been remedied—subsequent projects, such as Excelsior's Mesaba Project, have a third gasifier and train built in. However, the past year has seen Wabash River running reliably, with availability comparable to or better than other technologies.\n\nThe Polk County IGCC has design problems. First, the project was initially shut down because of corrosion in the slurry pipeline that fed slurried coal from the rail cars into the gasifier. A new coating for the pipe was developed. Second, the thermocoupler was replaced in less than two years; an indication that the gasifier had problems with a variety of feedstocks; from bituminous to sub-bituminous coal. The gasifier was designed to also handle lower rank lignites. Third, unplanned down time on the gasifier because of refractory liner problems, and those problems were expensive to repair. The gasifier was originally designed in Italy to be half the size of what was built at Polk. Newer ceramic materials may assist in improving gasifier performance and longevity. Understanding the operating problems of the current IGCC plant is necessary to improve the design for the IGCC plant of the future. (Polk IGCC Power Plant, http://www.clean-energy.us/projects/polk_florida.html.) Keim, K., 2009, IGCC A Project on Sustainability Management Systems for Plant Re-Design and Re-Image. This is an unpublished paper from Harvard University)\n\nGeneral Electric is currently designing an IGCC model plant that should introduce greater reliability. GE's model features advanced turbines optimized for the coal syngas. Eastman's industrial gasification plant in Kingsport, TN uses a GE Energy solid-fed gasifier. Eastman, a fortune 500 company, built the facility in 1983 without any state or federal subsidies and turns a profit.\n\nThere are several refinery-based IGCC plants in Europe that have demonstrated good availability (90-95%) after initial shakedown periods. Several factors help this performance:\n\n\nAnother IGCC success story has been the 250 MW Buggenum plant in The Netherlands, which was commissioned in 1994 and closed in 2013, had good availability. This coal-based IGCC plant was originally designed to use up to 30% biomass as a supplemental feedstock. The owner, NUON, was paid an incentive fee by the government to use the biomass. NUON has constructed a 1,311 MW IGCC plant in the Netherlands, comprising three 437 MW STEG units. The Nuon Magnum IGCC power plant was commissioned in 2011, and was officially opened in June 2013. Mitsubishi Heavy Industries has been awarded to construct the power plant. Following a deal with environmental organizations, NUON has been prohibited from using the Magnum plant to burn coal and biomass, until 2020. Because of high gas prices in the Netherlands, two of the three units are currently offline, whilst the third unit sees only low usage levels. The relatively low 59% efficiency of the Magnum plant means that more efficient CCGT plants (such as the Hemweg 9 plant) are preferred to provide (backup) power.\n\nA new generation of IGCC-based coal-fired power plants has been proposed, although none is yet under construction. Projects are being developed by AEP, Duke Energy, and Southern Company in the US, and in Europe by ZAK/PKE, Centrica (UK), E.ON and RWE (both Germany) and NUON (Netherlands). In Minnesota, the state's Dept. of Commerce analysis found IGCC to have the highest cost, with an emissions profile not significantly better than pulverized coal. In Delaware, the Delmarva and state consultant analysis had essentially the same results.\n\nThe high cost of IGCC is the biggest obstacle to its integration in the power market; however, most energy executives recognize that carbon regulation is coming soon. Bills requiring carbon reduction are being proposed again both the House and the Senate, and with the Democratic majority it seems likely that with the next President there will be a greater push for carbon regulation. The Supreme Court decision requiring the EPA to regulate carbon (Commonwealth of Massachusetts et al. v. Environmental Protection Agency et al.)[20] also speaks to the likelihood of future carbon regulations coming sooner, rather than later. With carbon capture, the cost of electricity from an IGCC plant would increase approximately 33%. For a natural gas CC, the increase is approximately 46%. For a pulverized coal plant, the increase is approximately 57%. This potential for less expensive carbon capture makes IGCC an attractive choice for keeping low cost coal an available fuel source in a carbon constrained world. However, the industry needs a lot more experience to reduce the risk premium. IGCC with CCS requires some sort of mandate, higher carbon market price, or regulatory framework to properly incentivize the industry.\n\nIn Japan, electric power companies, in conjunction with Mitsubishi Heavy Industries has been operating a 200 t/d IGCC pilot plant since the early '90s. In September 2007, they started up a 250 MW demo plant in Nakoso. It runs on air-blown (not oxygen) dry feed coal only. It burns PRB coal with an unburned carbon content ratio of <0.1% and no detected leaching of trace elements. It employs not only \"F\" type turbines but \"G\" type as well. (see gasification.org link below)\n\nNext generation IGCC plants with CO capture technology will be expected to have higher thermal efficiency and to hold the cost down because of simplified systems compared to conventional IGCC. The main feature is that instead of using oxygen and nitrogen to gasify coal, they use oxygen and CO. The main advantage is that it is possible to improve the performance of cold gas efficiency and to reduce the unburned carbon (char).\n\nAs a reference for powerplant efficiency:\n\nThe CO extracted from gas turbine exhaust gas is utilized in this system. Using a closed gas turbine system capable of capturing the CO by direct compression and liquefication obviates the need for a separation and capture system.\n\nPre-combustion CO removal is much easier than CO removal from flue gas in post-combustion capture due to the high concentration of CO after the water-gas-shift reaction and the high pressure of the syngas. During pre-combustion in IGCC, the partial pressure of CO is nearly 1000 times higher than in post-combustion flue gas. Due to the high concentration of CO pre-combustion, physical solvents, such as Selexol and Rectisol, are preferred for the removal of CO vs that of chemical solvents. Physical solvents work by absorbing the acid gases without the need of a chemical reaction as in traditional amine based solvents. The solvent can then be regenerated, and the CO desorbed, by reducing the pressure. The biggest obstacle with physical solvents is the need for the syngas to be cooled before separation and reheated afterwards for combustion. This requires energy and decreases overall plant efficiency.\n\nNational and international test codes are used to standardize the procedures and definitions used to test IGCC Power Plants. Selection of the test code to be used is an agreement between the purchaser and the manufacturer, and has some significance to the design of the plant and associated systems. In the United States, The American Society of Mechanical Engineers published the Performance Test Code for IGCC Power Generation Plants (PTC 47) in 2006 which provides procedures for the determination of quantity and quality of fuel gas by its flow rate, temperature, pressure, composition, heating value, and its content of contaminants.\n\nIn 2007, the New York State Attorney General's office demanded full disclosure of \"financial risks from greenhouse gases\" to the shareholders of electric power companies proposing the development of IGCC coal-fired power plants. \"Any one of the several new or likely regulatory initiatives for CO emissions from power plants - including state carbon controls, EPA's regulations under the Clean Air Act, or the enactment of federal global warming legislation - would add a significant cost to carbon-intensive coal generation\"; U.S. Senator Hillary Clinton from New York has proposed that this full risk disclosure be required of all publicly traded power companies nationwide. This honest disclosure has begun to reduce investor interest in all types of existing-technology coal-fired power plant development, including IGCC.\n\nSenator Harry Reid (Majority Leader of the 2007/2008 U.S. Senate) told the 2007 Clean Energy Summit that he will do everything he can to stop construction of proposed new IGCC coal-fired electric power plants in Nevada. Reid wants Nevada utility companies to invest in solar energy, wind energy and geothermal energy instead of coal technologies. Reid stated that global warming is a reality, and just one proposed coal-fired plant would contribute to it by burning seven million tons of coal a year. The long-term healthcare costs would be far too high, he claimed (no source attributed). \"I'm going to do everything I can to stop these plants.\", he said. \"There is no clean coal technology. There is cleaner coal technology, but there is no clean coal technology.\"\n\nOne of the most efficient ways to treat the HS gas from an IGCC plant is by converting it into sulphuric acid in a wet gas sulphuric acid process WSA process. However, the majority of the HS treating plants utilize the modified Claus process, as the sulphur market infrastructure and the transportation costs of sulphuric acid versus sulphur are in favour of sulphur production.\n\n\n"}
{"id": "5774599", "url": "https://en.wikipedia.org/wiki?curid=5774599", "title": "Lakes of Wada", "text": "Lakes of Wada\n\nIn mathematics, the are three disjoint connected open sets of the plane or open unit square with the counterintuitive property that they all have the same boundary. In other words, for any point selected on the boundary of \"one\" of the lakes, the other two lakes' boundaries also contain that point.\n\nMore than two sets with the same boundary are said to have the Wada property; examples include Wada basins in dynamical systems. This property is rare in real-world systems.\n\nThe lakes of Wada were introduced by , who credited the discovery to Takeo Wada. His construction is similar to the construction by of an indecomposable continuum, and in fact it is possible for the common boundary of the three sets to be an indecomposable continuum.\n\nThe Lakes of Wada are formed by starting with a closed unit square of dry land, and then digging 3 lakes according to the following rule:\n\nAfter an infinite number of days, the three lakes are still disjoint connected open sets, and the remaining dry land is the boundary of each of the 3 lakes.\n\nFor example, the first five days might be (see the image on the right):\n\nA variation of this construction can produce a countable infinite number of connected lakes with the same boundary: instead of extending the lakes in the order 1, 2, 0, 1, 2, 0, 1, 2, 0, ..., extend them in the order 0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, ...and so on.\n\nWada basins are certain special basins of attraction studied in the mathematics of non-linear systems. A basin having the property that every neighborhood of every point on the boundary of that basin intersects at least three basins is called a Wada basin, or said to have the Wada property. Unlike the Lakes of Wada, Wada basins are often disconnected.\n\nAn example of Wada basins is given by the Newton–Raphson method applied to a cubic polynomial with distinct roots, such as see the picture.\n\nA physical system that demonstrates Wada basins is the pattern of reflections between three spheres in contact—see chaotic scattering.\n\nIn chaos theory, Wada basins arise very frequently. Usually, the Wada property can be seen in the basin of attraction of dissipative dynamical systems. \nBut the exit basins of Hamiltonian system can also show the Wada property. In the context of the chaotic scattering of systems with multiple exit, basin of exit shows the Wada property. \nM. A. F. Sanjuán et al. had shown that in the Henon-Heiles system the exit basins have this Wada property.\n\n\n"}
{"id": "921245", "url": "https://en.wikipedia.org/wiki?curid=921245", "title": "List of interstellar and circumstellar molecules", "text": "List of interstellar and circumstellar molecules\n\nThis is a list of molecules that have been detected in the interstellar medium and circumstellar envelopes, grouped by the number of component atoms. The chemical formula is listed for each detected compound, along with any ionized form that has also been observed.\n\nThe molecules listed below were detected by spectroscopy. Their spectral features are generated by transitions of component electrons between different energy levels, or by rotational or vibrational spectra. Detection usually occurs in radio, microwave, or infrared portions of the spectrum.\n\nInterstellar molecules are formed by chemical reactions within very sparse interstellar or circumstellar clouds of dust and gas. Usually this occurs when a molecule becomes ionized, often as the result of an interaction with a cosmic ray. This positively charged molecule then draws in a nearby reactant by electrostatic attraction of the neutral molecule's electrons. Molecules can also be generated by reactions between neutral atoms and molecules, although this process is generally slower. The dust plays a critical role of shielding the molecules from the ionizing effect of ultraviolet radiation emitted by stars.\n\nThe chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old.\n\nThe first carbon-containing molecule detected in the interstellar medium was the methylidyne radical (CH) in 1937. From the early 1970s it was becoming evident that interstellar dust consisted of a large component of more complex organic molecules (COMs), probably polymers. Chandra Wickramasinghe proposed the existence of polymeric composition based on the molecule formaldehyde (HCO).\nFred Hoyle and Chandra Wickramasinghe later proposed the identification of bicyclic aromatic compounds from an analysis of the ultraviolet extinction absorption at 2175 Å, thus demonstrating the existence of polycyclic aromatic hydrocarbon molecules in space.\n\nIn 2004, scientists reported detecting the spectral signatures of anthracene and pyrene in the ultraviolet light emitted by the Red Rectangle nebula (no other such complex molecules had ever been found before in outer space). This discovery was considered a confirmation of a hypothesis that as nebulae of the same type as the Red Rectangle approach the ends of their lives, convection currents cause carbon and hydrogen in the nebulae's core to get caught in stellar winds, and radiate outward. As they cool, the atoms supposedly bond to each other in various ways and eventually form particles of a million or more atoms. The scientists inferred that since they discovered polycyclic aromatic hydrocarbons (PAHs) — which may have been vital in the formation of early life on Earth — in a nebula, by necessity they must originate in nebulae.\n\nIn 2010, fullerenes (or \"buckyballs\") were detected in nebulae. Fullerenes have been implicated in the origin of life; according to astronomer Letizia Stanghellini, \"It's possible that buckyballs from outer space provided seeds for life on Earth.\"\n\nIn October 2011, scientists found using spectroscopy that cosmic dust contains complex organic compounds (\"amorphous organic solids with a mixed aromatic-aliphatic structure\") that could be created naturally, and rapidly, by stars. The compounds are so complex that their chemical structures resemble the makeup of coal and petroleum; such chemical complexity was previously thought to arise only from living organisms. These observations suggest that organic compounds introduced on Earth by interstellar dust particles could serve as basic ingredients for life due to their surface-catalytic activities. One of the scientists suggested that these compounds may have been related to the development of life on Earth and said that, \"If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.\"\n\nIn August 2012, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary \"IRAS 16293-2422\", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.\n\nIn September 2012, NASA scientists reported that PAHs, subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation, and hydroxylation, to more complex organics — \"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons \"for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.\"\n\nPAHs are found everywhere in deep space and, in June 2013, PAHs were detected in the upper atmosphere of Titan, the largest moon of the planet Saturn.\n\nIn 2013, Dwayne Heard at the University of Leeds suggested that quantum mechanical tunneling could explain a reaction his group observed taking place, at a significantly higher than expected rate, between cold (around 63 kelvins) hydroxyl and methanol molecules, apparently bypassing intramolecular energy barriers which would have to be overcome by thermal energy or ionization events for the same rate to exist at warmer temperatures. The proposed tunneling mechanism may help explain the common observation of fairly complex molecules (up to tens of atoms) in interstellar space.\n\nA particularly large and rich region for detecting interstellar molecules is Sagittarius B2 (Sgr B2). This giant molecular cloud lies near the center of the Milky Way galaxy and is a frequent target for new searches. About half of the molecules listed below were first found near Sgr B2, and nearly every other molecule has since been detected in this feature. A rich source of investigation for circumstellar molecules is the relatively nearby star CW Leonis (IRC +10216), where about 50 compounds have been identified.\n\nIn March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.\n\nIn October 2016, astronomers reported that the very basic chemical ingredients of life—the carbon-hydrogen molecule (CH, or methylidyne radical), the carbon-hydrogen positive ion (CH cation) and the carbon ion (C cation)—are the result, in large part, of ultraviolet light from stars, rather than in other ways, such as the result of turbulent events related to supernovae and young stars, as thought earlier.\n\nTo explain the observed ratios of isomeric compounds, the minimum energy principle has been used. In the majority of cases, it explains that some organic entities have greater abundance than their isomers due to the lower total energies of the first one. However, a few exceptions where the principle fails are also known.\n\nAnother approach ignores energy and deals only with the molecular complexity estimated by the information entropy index. It speculates that the points of several natural compounds (urea, pyrimidine, dihydroxyacetone, uracil, cytosine, glycine, and alanine) fall into the range of the values typical for the known interstellar molecules that indicates high probability of their detection in interstellar environment. Additionally the molecules with maximal information entropy, i.e. the most complex compounds, make up approximately a half of the interstellar set and their percentage is decreased with the size. This trend may be associated with the different stabilities of the molecules with uniform (usually more stable) and diversified (usually less stable) chemical structures, so the detectable molecules with a large size must possess symmetric structure more probably than non-symmetric. The remarkable detection of low-entropy (highly symmetric) fullerene molecules supports this assumption. It is also noted that information entropy reflects the depth of hydrogenation of interstellar entities: the molecules with maximal information entropy are hydrogen-poor whereas the others are mainly hydrogen-rich.\n\nThe following tables list molecules that have been detected in the interstellar medium, grouped by the number of component atoms. If there is no entry in the molecule column, only the ionized form has been detected. For molecules where no designation was given in the scientific literature, that field is left empty. Mass is given in atomic mass units. The total number of unique species, including distinct ionization states, is listed in parentheses in each section header.\n\nMost of the molecules detected so far are organic. Only one inorganic species has been observed in molecules which contain at least five atoms, SiH. Larger molecules have so far all had at least one carbon atom, with no N−N or O−O bonds.\n\nThese molecules all contain one or more deuterium atoms, a heavier isotope of hydrogen.\nEvidence for the existence of the following molecules has been reported in scientific literature, but the detections are either described as tentative by the authors, or have been challenged by other researchers. They await independent confirmation.\n\n"}
{"id": "11730924", "url": "https://en.wikipedia.org/wiki?curid=11730924", "title": "Merit order", "text": "Merit order\n\nThe merit order is a way of ranking available sources of energy, especially electrical generation, based on ascending order of price (which may reflect the order of their short-run marginal costs of production) together with amount of energy that will be generated. In a centralized management, the ranking is so that those with the lowest marginal costs are the first ones to be brought online to meet demand, and the plants with the highest marginal costs are the last to be brought on line. Dispatching generation in this way minimizes the cost of production of electricity. Sometimes generating units must be started out of merit order, due to transmission congestion, system reliability or other reasons.\n\nThe high demand for electricity during peak demand pushes up the bidding price for electricity, and the relatively inexpensive baseload power supply mix is supplemented by ‘peaking power plants,' which charge a premium for their electricity.\n\nIncreasing the supply of renewable energy tends to lower the average price per unit of electricity because wind energy and solar energy have very low marginal costs: they do not have to pay for fuel, and the sole contributors to their marginal cost is operational and maintenance. With cost often reduced by feed-in-tariff revenue, their electricity is as a result, less costly on the spot market than that from coal or natural gas, and transmission companies buy from them first. Moreover, solar energy is typically most abundant in the middle of the day, coinciding closely with peak demand in warm climates, so that it is in the best position to displace coal and natural gas electricity when those sources are charging the highest premium. Solar and wind electricity therefore substantially reduce the amount of highly priced peak electricity that transmission companies need to buy, reducing the overall cost. A study by the Fraunhofer Institute found that this \"merit order effect\" had allowed solar power to reduce the price of electricity on the German energy exchange by 10% on average, and by as much as 40% in the early afternoon, in 2007; as more solar electricity is fed into the grid, peak prices will come down even further. By 2006, the \"merit order effect\" meant that the savings in electricity costs to German consumers more than offset for the support payments paid for renewable electricity generation.\n\nA 2013 study estimates the merit order effect of both wind and photovoltaic electricity generation in Germany between the years 2008 and 2012. For each additional GWh of renewables fed into the grid, the price of electricity in the day-ahead market is reduced by 0.11–0.13¢/kWh. The total merit order effect of wind and photovoltaics ranges from 0.5¢/kWh in 2010 to more than 1.1¢/kWh in 2012.\n\nThe zero marginal cost of wind energy does not, however, translate, into zero marginal cost of peak load electricity in a competitive open electricity market system as wind supply cannot be dispatched to meet peak demand. The purpose of the merit order was to enable the lowest net cost electricity to be dispatched first thus minimising overall electricity system costs to consumers. Intermittent wind might be able to supply this economic function provided peak wind supply and peak demand coincide both in time and quantity. On the other hand, solar energy tends to be most abundant during peak energy demand in warm climates, maximizing its ability to displace coal and natural gas power.\n\nA study by the Fraunhofer Institute in Karlsruhe, Germany found that windpower saves German consumers €5billion a year. It is estimated to have lowered prices in European countries with high wind generation by between 3 and 23€/MWh. On the other hand, renewable energy in Germany increased the price for electricity, consumers there now pay 52.8 €/MWh more only for renewable energy (see German Renewable Energy Sources Act), average price for electricity in Germany now is increased to 26¢/kWh. Increasing electrical grid costs for new transmission, market trading and storage associated with wind and solar are not included in the marginal cost of power sources, instead grid costs are combined with source costs at the consumer end.\n\n"}
{"id": "14504482", "url": "https://en.wikipedia.org/wiki?curid=14504482", "title": "Micro power source", "text": "Micro power source\n\n[1] La O` G.J., In H.J., Crumlin E., Barbastathis G., Shao-Horn Y. Resent advances in microdevices for electrochemical energy conversion and storage // Int. J. Energy Res. 2007. V.31. P.548-575.\n\n[2] Curtright A.E., Bouwman P.J., Wartena R.C., Swider-Lyons K.E. Power sources for nanotechnology // International Journal of Nanotechnology. 2004. V.1. Nos.1/2. P.226-239\n"}
{"id": "17596012", "url": "https://en.wikipedia.org/wiki?curid=17596012", "title": "Minimum mass", "text": "Minimum mass\n\nIn astronomy, minimum mass is the lower-bound calculated mass of observed objects such as planets, stars and binary systems, nebulae, and black holes.\n\nMinimum mass is a widely cited statistic for extrasolar planets detected by the radial velocity method, and is determined using the binary mass function. This method reveals planets by measuring changes in the movement of stars in the line-of-sight, so the real orbital inclinations and true masses of the planets are generally unknown.\n\nIf inclination can be determined, the true mass can be obtained from the calculated minimum mass using the following relationship:\n\nFor orbiting bodies in extrasolar stellar and planetary systems, an inclination of 0° or 180° corresponds to a face-on orbit (which cannot be observed by radial velocity), whereas an inclination of 90° corresponds to an edge-on orbit (for which the true mass equals the minimum mass).\n\n"}
{"id": "45603322", "url": "https://en.wikipedia.org/wiki?curid=45603322", "title": "Murray's Hypocycloidal Engine", "text": "Murray's Hypocycloidal Engine\n\nMurray's Hypocycloidal Engine, now in Thinktank, Birmingham Science Museum, England, was made around 1805 and is the world's third-oldest working steam engine and the oldest working engine with a hypocycloidal gear.\n\nDesigned by Matthew Murray, and made by Fenton, Murray and Wood of Holbeck, Leeds, it is one of only two of the type to survive; the other is located at The Henry Ford, Michigan, United States.\n\nThe single-cylinder engine was used by John Bradley & Co of Stourbridge from 1805 until 1931, and by N. Hingley & Sons Ltd of Netherton from 1931 until 1961, when it was acquired by Birmingham City Council for their science museum.\n\nMurray patented the hypocycloidal arrangement in 1802.\n\n"}
{"id": "22671489", "url": "https://en.wikipedia.org/wiki?curid=22671489", "title": "Office of Electricity Delivery and Energy Reliability", "text": "Office of Electricity Delivery and Energy Reliability\n\nThe Office of Electricity Delivery and Energy Reliability (OE) is a program office within the United States Department of Energy. The mission of OE is \"to lead national efforts to modernize the electric grid; enhance security and reliability of the energy infrastructure; and facilitate recovery from disruptions to energy supply.\"\n\nThe Office is responsible for leading efforts to modernize the electric grid. This is done through the development and implementation of national policy pertaining to electric grid reliability, and through the management of research, development, and demonstration activities for \"next generation\" electric grid infrastructure technologies. The Office analyzes electricity congestion, the designation of draft National Interest Electric Transmission Corridors, the coordination of energy corridors across federal lands, and workforce issues related to the electricity utility industry.\n\nThe Office of Electricity Delivery and Energy Reliability works with the United States Department of Homeland Security and other agencies to enhance the security of the nation's critical energy infrastructure.\n\nThe Office is under the general supervision of the Under Secretary of Energy for Energy and Environment. The Office is administered by the Assistant Secretary of Energy for Electricity Delivery and Energy Reliability (ASE-OE), who is appointed by the President of the United States. The current ASE-OE is Patricia A. Hoffman, who was appointed by President Barack Obama in June 2010. The Assistant Secretary is assisted in running the Office by three Deputy Assistant Secretaries, who are all career employees. Each of the Deputy Assistant Secretaries oversee a different branch of the Office's work.\n\n"}
{"id": "1722616", "url": "https://en.wikipedia.org/wiki?curid=1722616", "title": "Physical body", "text": "Physical body\n\nIn physics, a physical body or physical object (or simply a body or object) is an identifiable collection of matter, which may be constrained by an identifiable boundary, and may move as a unit by translation or rotation, in 3-dimensional space.\n\nIn common usage, an object is a collection of matter within a defined contiguous boundary in 3-dimensional space. The boundary must be defined and identified by the properties of the material. The boundary may change over time. The boundary is usually the visible or tangible surface of the object. The matter in the object is constrained (to a greater or lesser degree) to move as one object. The boundary may move in space relative to other objects that it is not attached to (through translation and rotation). An object's boundary may also deform and change over time in other ways.\n\nAlso in common usage, an object is not constrained to consist of the same collection of matter. Atoms or parts of an object may change over time. An object is defined by the simplest representation of the boundary consistent with the observations. However the laws of Physics only apply directly to objects that consist of the same collection of matter.\n\nEach object has a unique identity, independent of any other properties. Two objects may be identical, in all properties except position, but still remain distinguishable. In most cases the boundaries of two objects may not overlap at any point in time. The property of identity allows objects to be counted.\n\nExamples of models of physical bodies include, but are not limited to a particle, several interacting smaller bodies (particles or other), and continuous media.\n\nThe common conception of physical objects includes that they have extension in the physical world, although there do exist theories of quantum physics and cosmology which may challenge this. In modern physics, \"extension\" is understood in terms of the spacetime: roughly speaking, it means that for a given moment of time the body has some location in the space, although not necessarily a point. A physical body as a whole is assumed to have such quantitative properties as mass, momentum, electric charge, other conserving quantities, and possibly other quantities.\n\nA body with known composition and described in an adequate physical theory is an example of physical system.\n\nAn object is known by the application of senses. The properties of an object are inferred by learning and reasoning based on the information perceived. Abstractly, an object is a construction of our mind consistent with the information provided by our senses, using Occam's razor.\n\nIn common usage an object is the material inside the boundary of an object, in 3-dimensional space. The boundary of an object is a contiguous surface which may be used to determine what is inside, and what is outside an object. An object is a single piece of material, whose extent is determined by a description based on the properties of the material. An imaginary sphere of granite within a larger block of granite would not be considered an identifiable object, in common usage. A fossilized scull encased in a rock may be considered an object because it is possible to determine the extent of the scull based on the properties of the material.\n\nFor a rigid body, the boundary of an object may change over time by continuous translation and rotation. For a deformable body the boundary may also be continuously deformed over time in other ways.\n\nAn object has an identity. In general two objects with identical properties, other than position at an instance in time, may be distinguished as two objects and may not occupy the same space at the same time (excluding component objects). An object's identity may be tracked using the continuity of the change in its boundary over time. The identity of objects allows objects to be arranged in sets and counted.\n\nThe material in an object may change over time. For example, a rock may wear away or have pieces broken off it. The object will be regarded as the same object after the addition or removal of material, if the system may be more simply described with the continued existence of the object, than in any other way. The addition or removal of material may discontinuously change the boundary of the object. The continuation of the objects identity is then based on the description of the system by continued identify being simpler than without continued identity.\n\nFor example, a particular car might have all its wheels changed, and still be regarded as the same car.\n\nThe identity of an object may not split. If an object is broken into two pieces at most one of the pieces has the same identity. An object's identity may also be destroyed if the simplest description of the system at a point in time changes from identifying the object to not identifying it. Also an object's identity is created at the first point in time that the simplest model of the system consistent with perception identifies it.\n\nAn object may be composed of components. A component is an object completely within the boundary of a containing object.\n\nIn classical mechanics a physical body is collection of matter having properties including mass, velocity, momentum and energy. The matter exists in a volume of three-dimensional space. This space is its extension.\n\nUnder Newtonian gravity the gravitational field further away than the furthest extent of an object is determined only by the mass and the position of the center of mass.\n\nInteractions between objects are partly described by orientation and external shape.\n\nIn continuum mechanics an object may be described as a collection of sub objects, down to an infinitesimal division, which interact with each other by forces which may be described internally by pressure and mechanical stress.\n\nIn quantum mechanics an object is a particle or collection of particles. Until measured, a particle does not have a physical position. A particle is defined by a probability distribution of finding the particle at a particular position. There is a limit to the accuracy with which the position and velocity may be measured. A particle or collection of particles is described by a quantum state.\n\nThese ideas vary from the common usage understanding of what an object is.\n\nIn particle physics, there is a debate as to whether some elementary particles are not bodies, but are points without extension in physical space within space-time, or are always extended in at least one dimension of space as in string theory or M theory.\n\nIn some branches of psychology, depending on school of thought, a physical body is a physical object with physical properties, as compared to mental objects. In (reductionistic) behaviorism, a physical body and its properties are the (only) meaningful objects of study.\nWhile in the modern day behavioral psychotherapy it is still only the means for goal oriented behavior modifications, in Body Psychotherapy it is not a means only anymore, but its felt sense is a goal of its own. In cognitive psychology, physical bodies as they occur in biology are studied in order to understand the mind, which may not be a physical body, as in functionalist schools of thought.\n\nA physical body is an enduring object that exists throughout a particular trajectory of space and orientation over a particular duration of time, and which is located in the world of physical space (i.e., as studied by physics). This contrasts with abstract objects such as mathematical objects which do not exist at any particular time or place.\nExamples are a cloud, a human body, a weight, a billiard ball, a table, or a proton. This is contrasted with abstract objects such as mental objects, which exist in the mental world, and mathematical objects. Other examples that are not physical bodies are emotions, the concept of \"justice\", a feeling of hatred, or the number \"3\". In some philosophies, like the Idealism of George Berkeley, a physical body is a mental object, but still has extension in the space of a visual field.\n\n"}
{"id": "16509656", "url": "https://en.wikipedia.org/wiki?curid=16509656", "title": "Plant efficiency", "text": "Plant efficiency\n\nThe efficiency of a plant is the percentage of the total energy content of a power plant's fuel that is converted into electricity. The remaining energy is usually lost to the environment as heat unless it is used for district heating.\n\nThis is complicated by the fact that there are two different ways to measure the fuel energy input—LCV = Lower Calorific Value, which is the same as NCV = Net Calorific Value, or, HCV = Higher Calorific Value which is the same as GCV, Gross Calorific Value.\n\nDepending on which convention is used, a differences of 10% in the apparent efficiency of a gas fired plant can arise, so it is very important to know which convention, HCV or LCV (NCV or GCV) is being used.\n\nElectric Turbo Compounding (ETC) is a technology solution to the challenge of improving energy efficiency for the stationary power generation industry.\n\nFossil fuel based power generation is predicted to continue for decades, especially in developing economies. This is against the global need to reduce carbon emissions, of which, a high percentage is produced by the power sector worldwide.\n\nETC works by making gas and diesel-powered gensets (Electric Generators) work more effectively and cleaner, by recovering waste energy from the exhaust to improve power density and fuel efficiency.\n\n\n"}
{"id": "58610020", "url": "https://en.wikipedia.org/wiki?curid=58610020", "title": "Powering Past Coal Alliance", "text": "Powering Past Coal Alliance\n\nThe Powering Past Coal Alliance is a group of over 70 countries, cities, regions and organisations aiming to accelerate the phase out of traditional or unabated coal power.\n\nThe Alliance was launched by Canada and the UK at the COP23 climate summit in November 2017. Announcing the launch, Climate Action Network-Canada Executive Director Catherine Abreu said:\n\nMembership has grown rapidly. By the end of the summit membership was over 20 countries, regions and organisations. Within a month membership had grown to over 50 countries, regions and organisations. In September 2018 membership stood at 74: 29 countries, 17 regions and 28 organisations.\n\nIn October 2018 the South Korean province of South Chungcheong became the largest user of coal power to join the Alliance.\n\nAlliance members agree that:\n\n\nMembers of the Powering Past Coal Alliance at the start of October 2018 were:\n\n\n"}
{"id": "16649410", "url": "https://en.wikipedia.org/wiki?curid=16649410", "title": "Pushforward (homology)", "text": "Pushforward (homology)\n\nIn algebraic topology, the pushforward of a continuous function formula_1 : formula_2 between two topological spaces is a homomorphism formula_3 between the homology groups for formula_4.\n\nHomology is a functor which converts a topological space formula_5 into a sequence of homology groups formula_6. (Often, the collection of all such groups is referred to using the notation formula_7; this collection has the structure of a graded ring.) In any category, a functor must induce a corresponding morphism. The pushforward is the morphism corresponding to the homology functor.\n\nWe build the pushforward homomorphism as follows (for singular or simplicial homology):\n\nFirst we have an induced homomorphism between the singular or simplicial chain complex formula_8 and formula_9 defined by composing each singular n-simplex formula_10 : formula_11 with formula_1 to obtain a singular n-simplex of formula_13, formula_14 : formula_15. Then we extend formula_16 linearly via formula_17.\n\nThe maps formula_16 : formula_19 satisfy formula_20 where formula_21 is the boundary operator between chain groups, so formula_22 defines a chain map.\n\nWe have that formula_16 takes cycles to cycles, since formula_24 implies formula_25. Also formula_16 takes boundaries to boundaries since formula_27.\n\nHence formula_16 induces a homomorphism between the homology groups formula_29 for formula_30.\n\nTwo basic properties of the push-forward are:\n\nA main result about the push-forward is the homotopy invariance: if two maps formula_38 are homotopic, then they induce the same homomorphism formula_39.\n\nThis immediately implies that the homology groups of homotopy equivalent spaces are isomorphic:\n\nThe maps formula_40 induced by a homotopy equivalence formula_41 are isomorphisms for all formula_42.\n\n"}
{"id": "16538579", "url": "https://en.wikipedia.org/wiki?curid=16538579", "title": "Renewable Electricity and the Grid", "text": "Renewable Electricity and the Grid\n\nRenewable Electricity and the Grid: The Challenge of Variability is a 2007 book edited by Godfrey Boyle which examines the significance of the issue of variability of renewable energy supplies in the electricity grid.\n\nThe energy available from sun, wind, waves, and tides varies in ways which may not match variations in consumer energy demand. Assimilating these fluctuations can affect the operation and economics of electricity networks and markets. There are many myths and misunderstandings surrounding this topic. \"Renewable Electricity and the Grid\" presents technical and operational solutions to the problem of reconciling the differing patterns of power supply and demand.\n\nChapters of \"Renewable Electricity and the Grid\" are authored by leading experts, who explain and quantify the impacts of renewable energy variability. Godfrey Boyle (editor) is Director of the Energy and Environment Research Unit at the UK Open University and has written the textbooks \"Energy Systems and Sustainability\" (2003) and \"Renewable Energy: Power for a Sustainable Future\" (2004). He is a Fellow of the Institution of Engineering and Technology and a Trustee of the National Energy Foundation.\n\nOther authors include:\nDr Bob Everett, open University,\nDr Mark Barret, Open University,\nDr Fred Starr, EU Energy Center at Petten\nDave Andrews, Wessex Water, Energy Manager\nBrian Hurley - Airtricity\n\n"}
{"id": "40922605", "url": "https://en.wikipedia.org/wiki?curid=40922605", "title": "Residential Customer Equivalent", "text": "Residential Customer Equivalent\n\nResidential Customer Equivalent (RCE) is a unit of measures used by the energy industry to denote the typical annual commodity consumption by a single-family residential customer. Also known as \"RCE\" for short, a single RCE represents 1,000 therms of natural gas or 10,000 kWh of electricity.\n\nRCE is often used to help normalize the size of energy companies. Energy companies serve a number of customers that is typically different from the RCE value consumed by those customers. For example, an LDC or ESCO may serve 50,000 customers but many of those can be commercial or industrial customers, so that same company can be said to serve 400,000 RCE.\n"}
{"id": "23017591", "url": "https://en.wikipedia.org/wiki?curid=23017591", "title": "Resonance (particle physics)", "text": "Resonance (particle physics)\n\nIn particle physics, a resonance is the peak located around a certain energy found in differential cross sections of scattering experiments. These peaks are associated with subatomic particles, which include a variety of bosons, quarks and hadrons (such as nucleons, delta baryons or upsilon mesons) and their excitations. In common usage, \"resonance\" only describes particles with very short lifetimes, mostly high-energy hadrons existing for or less.\n\nThe width of the resonance (\"Γ\") is related to the mean lifetime (\"τ\") of the particle (or its excited state) by the relation\n\nwhere \"h\" is the Planck constant and formula_2.\n\nThus, the lifetime of a particle is the direct inverse of the particle's resonance width. For example, the charged pion has the second-longest lifetime of any meson, at . Therefore, its resonance width is very small, about or about 6.11 MHz. Pions are generally not considered as \"resonances\". The charged rho meson has a very short lifetime, about . Correspondingly, its resonance width is very large, at 149.1 MeV or about 36 ZHz. This amounts to nearly one-fifth of the particle's rest mass.\n\n"}
{"id": "911516", "url": "https://en.wikipedia.org/wiki?curid=911516", "title": "Rolling blackout", "text": "Rolling blackout\n\nA rolling blackout, also referred to as rotational load shedding or feeder rotation, is an intentionally engineered electrical power shutdown where electricity delivery is stopped for non-overlapping periods of time over different parts of the distribution region. Rolling blackouts are a last-resort measure used by an electric utility company to avoid a total blackout of the power system. They are a type of demand response for a situation where the demand for electricity exceeds the power supply capability of the network. Rolling blackouts may be localised to a specific part of the electricity network or may be more widespread and affect entire countries and continents. Rolling blackouts generally result from two causes: insufficient generation capacity or inadequate transmission infrastructure to deliver sufficient power to the area where it is needed.\n\nRolling blackouts are a common or even a normal daily event in many developing countries where electricity generation capacity is underfunded or infrastructure is poorly managed. In well managed under-capacity systems, blackouts are scheduled in advance and advertised to allow people to work around them, but in most cases they happen without warning, typically whenever the transmission frequency falls below the 'safe' limit. Rolling blackouts are also used as a response strategy to cope with reduced output beyond reserve capacity from power stations taken offline unexpectedly such as through an extreme weather event.\n\nRolling blackouts in developed countries sometimes occur due to economic forces at the expense of system reliability (such as in the California electricity crisis of 2000-2001).\n"}
{"id": "42714267", "url": "https://en.wikipedia.org/wiki?curid=42714267", "title": "Sally Benson (professor)", "text": "Sally Benson (professor)\n\nSally M. Benson is a professor of energy engineering at Stanford University. In 2014, she was appointed as director of the Precourt Institute for Energy, the university's hub of energy research and education. Benson will continue on as director of Stanford’s Global Climate and Energy Project (GCEP), a position she has had since 2007.\n\nBenson has won various awards, including the 2012 Greenman Award, Michel T. Halbouty Distinguished Lecture Award from the Geological Society, and the ARCS American Pacesetter Award. Benson received a B.S. in geology from Barnard College, Columbia University, and a Ph.D. in materials and mineral engineering from the University of California-Berkeley.\n\nBenson has held several positions with the Lawrence Berkeley National Laboratory, Berkeley, California. These include 1980–2007, Staff scientist (director 1993–1997), Earth Sciences Division; 2001–2004, Deputy director for operations; 1997–2001, Associate laboratory director, Energy Sciences.\n\n\n"}
{"id": "1736264", "url": "https://en.wikipedia.org/wiki?curid=1736264", "title": "Second moment of area", "text": "Second moment of area\n\nThe 2nd moment of area, also known as moment of inertia of plane area, area moment of inertia, or second area moment, is a geometrical property of an area which reflects how its points are distributed with regard to an arbitrary axis. The second moment of area is typically denoted with either an formula_1 for an axis that lies in the plane or with a formula_2 for an axis perpendicular to the plane. In both cases, it is calculated with a multiple integral over the object in question. Its dimension is L (length) to the fourth power. Its unit of dimension when working with the International System of Units is meters to the fourth power, m, or inches to the fourth power, in, when working in the Imperial System of Units.\n\nIn structural engineering, the second moment of area of a beam is an important property used in the calculation of the beam's deflection and the calculation of stress caused by a moment applied to the beam. The planar second moment of area provides insight into a beam's resistance to bending due to an applied moment, force, or distributed load perpendicular to its neutral axis, as a function of its shape. The polar second moment of area provides insight into a beam's resistance to torsional deflection, due to an applied moment parallel to its cross-section, as a function of its shape. \n\nThe second moment of area for an arbitrary shape  with respect to an arbitrary axis formula_6 is defined as\n\nwhere\n\nFor example, when the desired reference axis is the x-axis, the second moment of area formula_12 (often denoted as formula_13) can be computed in Cartesian coordinates as\n\nThe second moment of the area is crucial in Euler–Bernoulli theory of slender beams.\n\nMore generally, the product moment of area is defined as\n\nIt is sometimes necessary to calculate the second moment of area of a shape with respect to an formula_16 axis different to the centroidal axis of the shape. However, it is often easier to derive the second moment of area with respect to its centroidal axis, formula_17, and use the parallel axis theorem to derive the second moment of area with respect to the formula_16 axis. The parallel axis theorem states\n\nwhere\n\nA similar statement can be made about a formula_24 axis and the parallel centroidal formula_25 axis. Or, in general, any centroidal formula_26 axis and a parallel formula_27 axis.\n\nFor the simplicity of calculation, it is often desired to define the polar moment of area (with respect to a perpendicular axis) in terms of two area moments of inertia (both with respect to in-plane axes). The simplest case relates formula_28 to formula_13 and formula_30.\n\nThis relationship relies on the Pythagorean theorem which relates formula_17 and formula_25 to formula_9 and on the linearity of integration.\n\nFor more complex areas, it is often easier to divide the area into a series of \"simpler\" shapes. The second moment of area for the entire shape is the sum of the second moment of areas of all of its parts about a common axis. This can include shapes that are \"missing\" (i.e. holes, hollow shapes, etc.), in which case the second moment of area of the \"missing\" areas are subtracted, rather than added. In other words, the second moment of area of \"missing\" parts are considered negative for the method of composite shapes.\n\nSee list of second moments of area for other shapes.\n\nConsider a rectangle with base formula_35 and height formula_36 whose centroid is located at the origin. formula_13 represents the second moment of area with respect to the x-axis; formula_30 represents the second moment of area with respect to the y-axis; formula_28 represents the polar moment of inertia with respect to the z-axis.\n\nUsing the perpendicular axis theorem we get the value of formula_28.\n\nConsider an annulus whose center is at the origin, outside radius is formula_43, and inside radius is formula_44. Because of the symmetry of the annulus, the centroid also lies at the origin. We can determine the polar moment of inertia, formula_28, about the formula_46 axis by the method of composite shapes. This polar moment of inertia is equivalent to the polar moment of inertia of a circle with radius formula_43 minus the polar moment of inertia of a circle with radius formula_44, both centered at the origin. First, let us derive the polar moment of inertia of a circle with radius formula_49 with respect to the origin. In this case, it is easier to directly calculate formula_28 as we already have formula_51, which has both an formula_17 and formula_25 component. Instead of obtaining the second moment of area from Cartesian coordinates as done in the previous section, we shall calculate formula_13 and formula_28 directly using polar coordinates.\n\nNow, the polar moment of inertia about the formula_46 axis for an annulus is simply, as stated above, the difference of the second moments of area of a circle with radius formula_43 and a circle with radius formula_44.\n\nAlternatively, we could change the limits on the formula_61 integral the first time around to reflect the fact that there is a hole. This would be done like this.\n\nThe second moment of area for any simple polygon on the XY-plane can be computed in general by summing contributions from each segment of the polygon after essentially chopping the area up into a set of triangles.\nA polygon is assumed to have formula_63 vertices, numbered in counter-clockwise fashion. If polygon vertices are numbered clockwise, returned values will be negative, but absolute values will be correct.\n\nAttention, the following formulae must be trusted\n\nwhere formula_65 are the coordinates of the formula_66-th polygon vertex, for formula_67. Also, formula_68 are assumed to be equal to the coordinates of the first vertex, i.e., formula_69 and formula_70.\n\n"}
{"id": "50398478", "url": "https://en.wikipedia.org/wiki?curid=50398478", "title": "Selection principle", "text": "Selection principle\n\nIn mathematics, a selection principle is a rule asserting\nthe possibility of obtaining mathematically significant objects by \nselecting elements from given sequences of sets. The theory of selection principles\nstudies these principles and their relations to other mathematical properties.\nSelection principles mainly describe covering properties, \nmeasure- and category-theoretic properties, and local properties in \ntopological spaces, especially function spaces. Often, the \ncharacterization of a mathematical property using a selection \nprinciple is a nontrivial task leading to new insights on the \ncharacterized property.\n\nIn 1924, Karl Menger\nintroduced the following basis property for metric spaces: \nEvery basis of the topology contains a sequence of sets with vanishing \ndiameters that covers the space. Soon thereafter, \nWitold Hurewicz \nobserved that Menger's basis property is equivalent to the \nfollowing selective property: for every sequence of open covers of the space, \none can select finitely many open sets from each cover in the sequence, such that the selected sets cover the space.\nTopological spaces having this covering property are called Menger spaces.\n\nHurewicz's reformulation of Menger's property was the first important \ntopological property described by a selection principle. \nLet formula_1 and formula_2 be classes of mathematical objects.\nIn 1996, Marion Scheepers \nintroduced the following selection hypotheses,\ncapturing a large number of classic mathematical properties:\n\n\nIn the case where the classes formula_1 and formula_2 consist of covers of some ambient space, Scheepers also introduced the following selection principle.\n\n\nLater, Boaz Tsaban identified the prevalence of the following related principle:\n\nThe notions thus defined are \"selection principles\". An instantiation of a selection principle, by considering specific classes formula_1 and formula_2, gives a \"selection (or: selective) property\". However, these terminologies are used interchangeably in the literature.\n\nFor a set formula_25 and a family formula_26 of subsets of formula_27, the star of formula_28 in formula_26 is the set formula_30.\n\nIn 1999, Ljubisa D.R. Kocinac introduced the following \"star selection principles\":\n\n\nCovering properties form the kernel of the theory of selection principles. Selection properties that are not covering properties are often studied by using implications to and from selective covering properties of related spaces.\n\nLet formula_27 be a topological space. An \"open cover\" of formula_27 is a family of open sets whose union is the entire space formula_43 For technical reasons, we also request that the entire space formula_27 is not a member of the cover. The class of open covers of the space formula_27 is denoted by formula_46. (Formally, formula_47, but usually the space formula_27 is fixed in the background.) The above-mentioned property of Menger is, thus, formula_49. In 1942, Fritz Rothberger considered Borel's strong measure zero sets, and introduced a topological variation later called Rothberger space (also known as \"Cformula_50 space\"). In the notation of selections, Rothberger's property is the property formula_51.\n\nAn open cover formula_52 of formula_27 is point-cofinite if it has infinitely many elements, and every point formula_54 belongs to all but finitely many sets formula_55. (This type of cover was considered by Gerlits and Nagy, in the third item of a certain list in their paper. The list was enumerated by Greek letters, and thus these covers are often called formula_56-covers.) The class of point-cofinite open covers of formula_27 is denoted by formula_58. A topological space is a Hurewicz space if it satisfies formula_59.\n\nAn open cover formula_52 of formula_27 is an formula_62-cover if every finite subset of formula_27 is contained in some member of formula_52. The class of formula_62-covers of formula_27 is denoted by formula_67. A topological space is a γ-space if it satisfies formula_68.\n\nBy using star selection hypotheses one obtains properties such as star-Menger (formula_69), star-Rothberger (formula_70) and star-Hurewicz (formula_71).\n\nThere are 36 selection properties of the form formula_72, for formula_73 and formula_74. Some of them are trivial (hold for all spaces, or fail for all spaces). Restricting attention to Lindelöf spaces, the diagram below, known as the \"Scheepers Diagram\", presents nontrivial selection properties of the above form, and every nontrivial selection property is equivalent to one in the diagram. Arrows denote implications.\n\nSelection principles also capture important non-covering properties.\n\nLet formula_75 be a topological space, and formula_76. The class of sets formula_28 in the space formula_75 that have the point formula_79 in their closure is denoted by formula_80. The class formula_81 consists of the \"countable\" elements of the class formula_80. The class of sequences in formula_75 that converge to formula_79 is denoted by formula_85.\n\n\nThere are close connections between selection principles and Topological Games.\n\nLet formula_27 be a topological space. The Menger game formula_102 played on formula_27 is a game for two players, Alice and Bob. It has an inning per each natural number formula_104. At the formula_105 inning, Alice chooses an open cover formula_106 of formula_27,\nand Bob chooses a finite subset formula_108 of formula_52. \nIf the family formula_110 is a cover of the space formula_27, then Bob wins the game. Otherwise, Alice wins.\n\nA strategy for a player is a function determining the move of the player, given the earlier moves of both players. A strategy for a player is a winning strategy if each play where this player sticks to this strategy is won by this player.\n\nIn a similar way, we define games for other selection principles from the given Scheepers Diagram. In all these cases a topological space has a property from the Scheepers Diagram if and only if Alice has no winning strategy in the corresponding game.\n\n\nSubsets of the real line formula_125 (with the induced subspace topology) holding selection principle properties, most notably Menger and Hurewicz spaces, can be characterized by their continuous images in the Baire space formula_126. For functions formula_127, write formula_128 if formula_129 for all but finitely many natural numbers formula_130. Let formula_28 be a subset of formula_126. The set formula_133 is bounded if there is a function formula_134 such that formula_135 for all functions formula_136. The set formula_133 is dominating if for each function formula_138 there is a function formula_139 such that formula_135.\n\n\nLet P be a property of spaces. A space formula_27 is productively P if, for each space formula_75 with property P, the product space formula_146 has property P.\n\n\n\nLet formula_27 be a Tychonoff space, and formula_155 be the space of continuous functions formula_156 with pointwise convergence topology. \n\n"}
{"id": "15611", "url": "https://en.wikipedia.org/wiki?curid=15611", "title": "Simon–Ehrlich wager", "text": "Simon–Ehrlich wager\n\nThe Simon-Ehrlich Wager describes a 1980 scientific wager between business professor Julian L. Simon and biologist Paul Ehrlich, betting on a mutually agreed-upon measure of resource scarcity over the decade leading up to 1990. The widely-followed contest originated in the pages of \"Social Science Quarterly\", where Simon challenged Ehrlich to put his money where his mouth was. In response to Ehrlich's published claim that \"If I were a gambler, I would take even money that England will not exist in the year 2000\" Simon offered to take that bet, or, more realistically, \"to stake US$10,000 ... on my belief that the cost of non-government-controlled raw materials (including grain and oil) will not rise in the long run.\"\n\nSimon challenged Ehrlich to choose any raw material he wanted and a date more than a year away, and he would wager on the inflation-adjusted prices decreasing as opposed to increasing. Ehrlich chose copper, chromium, nickel, tin, and tungsten. The bet was formalized on September 29, 1980, with September 29, 1990 as the payoff date. Ehrlich lost the bet, as all five commodities that were bet on declined in price from 1980 through 1990, the wager period.\n\nIn 1968, Ehrlich published \"The Population Bomb\", which argued that mankind was facing a demographic catastrophe with the rate of population growth quickly outstripping growth in the supply of food and resources. Simon was highly skeptical of such claims, so proposed a wager, telling Ehrlich to select any raw material he wanted and select \"any date more than a year away,\" and Simon would bet that the commodity's price on that date would be lower than what it was at the time of the wager.\n\nEhrlich and his colleagues picked five metals that they thought would undergo big price increases: chromium, copper, nickel, tin, and tungsten. Then, on paper, they bought $200 worth of each, for a total bet of $1,000, using the prices on September 29, 1980, as an index. They designated September 29, 1990, 10 years hence, as the payoff date. If the inflation-adjusted prices of the various metals rose in the interim, Simon would pay Ehrlich the combined difference. If the prices fell, Ehrlich et al. would pay Simon.\n\nBetween 1980 and 1990, the world's population grew by more than 800 million, the largest increase in one decade in all of history. But by September 1990, the price of each of Ehrlich's selected metals had fallen. Chromium, which had sold for $3.90 a pound in 1980, was down to $3.70 in 1990. Tin, which was $8.72 a pound in 1980, was down to $3.88 a decade later.\n\nAs a result, in October 1990, Paul Ehrlich mailed Julian Simon a check for $576.07 to settle the wager in Simon's favor.\n\nJulian Simon won because the price of three of the five metals went down in nominal terms and all five of the metals fell in price in inflation-adjusted terms, with both tin and tungsten falling by more than half. In his book \"Betrayal of Science and Reason\", Ehrlich wrote that Simon \"[asserted] that humanity would never run out of anything\". Ehrlich added that he and fellow scientists viewed renewable resources as more important indicators of the state of planet Earth, but that he decided to go along with the bet anyway. Afterward, Simon offered to raise the wager to $20,000 and to use any resources at any time that Ehrlich preferred. Ehrlich countered with a challenge to bet that temperatures would increase in the future. The two were unable to reach an agreement on the terms of a second wager before Simon died.\n\nEhrlich could have won if the bet had been for a different ten-year period. Ehrlich wrote that the five metals in question had increased in price between the years 1950 to 1975. Asset manager Jeremy Grantham wrote that if the Simon–Ehrlich wager had been for a longer period (from 1980 to 2011), then Simon would have lost on four of the five metals. He also noted that if the wager had been expanded to \"all of the most important commodities,\" instead of just five metals, over that longer period of 1980 to 2011, then Simon would have lost \"by a lot.\" Economist Mark J. Perry noted that for an even longer period of time, from 1934 to 2013, the inflation-adjusted price of the Dow Jones-AIG Commodity Index showed \"an overall significant downward trend\" and concluded that Simon was \"more right than lucky\". Economist Tim Worstall wrote that \"The end result of all of this is that yes, it is true that Ehrlich could have, would have, won the bet depending upon the starting date. ... But the long term trend for metals at least is downwards.\"\n\nThe price of raw and other natural commodities such as oil, gold, and uranium have risen substantially in recent years, due to increased demand from China, India, and other industrializing countries. However, Simon has argued that this price increase is not necessarily contrary to his cornucopian theory. Ehrlich has dismissed the bet as a side issue and stated that the main worry is environmental problems like the ozone hole, acid rain, and global warming.\n\nUnderstanding that Simon wanted to bet again, Ehrlich and climatologist Stephen Schneider counter-offered, challenging Simon to bet on 15 current trends, betting $1000 that each will get worse (as in the previous wager) over a ten-year future period.\n\nThe trends they bet would continue to worsen were:\n\nSimon declined Ehrlich and Schneider's offer to bet, and used the following analogy to explain why he did so:\n\nIn his 1981 book \"The Ultimate Resource\", Simon noted that not all decreases in resources or increases in unwanted effects correspond to overall decreases in human wellbeing. Hence there can be an \"optimal level of pollution\" which accepts some increases in certain kinds of pollution in a way that increases overall wellbeing, while acknowledging that any increase in pollution is nevertheless a cost which must be considered in any such calculation (p.143). Some of the trends listed above are actually predicted by Simon's theory of resource development, and do not in themselves even count as costs (as pollution does). E.g., he pointed out that due to increased efficiency, the amount of cropland required and actually used to grow food for each person has decreased over time and is likely to continue to do so (p.5). The same might potentially be true of decreased reliance on firewood in developing countries, and per capita use of specific food sources like rice, wheat, and fish, if economic development makes a diverse range of alternative foods available. Some have also proven false, e.g., the amount of ozone in the lower atmosphere has decreased from 1994 to 2004.\n\nIn 1996, Simon bet $1000 with David South, professor of the Auburn University School of Forestry, that the inflation-adjusted price of timber would decrease in the following five years. Simon paid out early on the bet in 1997 (before his death in 1998) based on his expectation that prices would remain above 1996 levels (which they did).\n\nIn 1999, when \"The Economist\" headlined an article entitled, \"$5 a barrel oil soon?\" and with oil trading in the $12/barrel range, David South offered $1000 to any economist who would bet with him that the price of oil would be greater than $12/barrel in 2010. No economist took him up on the offer. However, in October 2000, Zagros Madjd-Sadjadi, an economist with The University of the West Indies, bet $1000 with David South that the inflation-adjusted price of oil would decrease to an inflation-adjusted price of $25 by 2010 (down from what was then $30/barrel). Madjd-Sadjadi paid South an inflation-adjusted $1,242 in January 2010. The price of oil at the time was $81/barrel.\n\n\n\n"}
{"id": "37738", "url": "https://en.wikipedia.org/wiki?curid=37738", "title": "Soil", "text": "Soil\n\nSoil is a mixture of organic matter, minerals, gases, liquids, and organisms that together support life. Earth's body of soil is the pedosphere, which has four important functions: it is a medium for plant growth; it is a means of water storage, supply and purification; it is a modifier of Earth's atmosphere; it is a habitat for organisms; all of which, in turn, modify the soil.\n\nThe pedosphere interfaces with the lithosphere, the hydrosphere, the atmosphere, and the biosphere. The term \"pedolith\", used commonly to refer to the soil, translates to \"ground stone\". Soil consists of a solid phase of minerals and organic matter (the soil matrix), as well as a porous phase that holds gases (the soil atmosphere) and water (the soil solution). Accordingly, soils are often treated as a three-state system of solids, liquids, and gases.\n\nSoil is a product of the influence of climate, relief (elevation, orientation, and slope of terrain), organisms, and its parent materials (original minerals) interacting over time. It continually undergoes development by way of numerous physical, chemical and biological processes, which include weathering with associated erosion. Given its complexity and strong internal connectedness, it is considered an ecosystem by soil ecologists.\n\nMost soils have a dry bulk density (density of soil taking into account voids when dry) between 1.1 and 1.6 g/cm, while the soil particle density is much higher, in the range of 2.6 to 2.7 g/cm. Little of the soil of planet Earth is older than the Pleistocene and none is older than the Cenozoic, although fossilized soils are preserved from as far back as the Archean.\n\nSoil science has two basic branches of study: edaphology and pedology. Edaphology is concerned with the influence of soils on living things. Pedology is focused on the formation, description (morphology), and classification of soils in their natural environment. In engineering terms, soil is included in the broader concept of regolith, which also includes other loose material that lies above the bedrock, as can be found on the Moon and other celestial objects, as well. Soil is also commonly referred to as earth or dirt; some scientific definitions distinguish \"dirt\" from \"soil\" by restricting the former term specifically to the displaced soil.\n\nSoil is a major component of the Earth's ecosystem. The world's ecosystems are impacted in far-reaching ways by the processes carried out in the soil, from ozone depletion and global warming to rainforest destruction and water pollution. With respect to Earth's carbon cycle, soil is an important carbon reservoir, and it is potentially one of the most reactive to human disturbance and climate change. As the planet warms, it has been predicted that soils will add carbon dioxide to the atmosphere due to increased biological activity at higher temperatures, a positive feedback (amplification). This prediction has, however, been questioned on consideration of more recent knowledge on soil carbon turnover.\n\nSoil acts as an engineering medium, a habitat for soil organisms, a recycling system for nutrients and organic wastes, a regulator of water quality, a modifier of atmospheric composition, and a medium for plant growth, making it a critically important provider of ecosystem services. Since soil has a tremendous range of available niches and habitats, it contains most of the Earth's genetic diversity. A gram of soil can contain billions of organisms, belonging to thousands of species, mostly microbial and in the main still unexplored. Soil has a mean prokaryotic density of roughly 10 organisms per gram, whereas the ocean has no more than 10 procaryotic organisms per milliliter (gram) of seawater. Organic carbon held in soil is eventually returned to the atmosphere through the process of respiration carried out by heterotrophic organisms, but a substantial part is retained in the soil in the form of soil organic matter; tillage usually increases the rate of soil respiration, leading to the depletion of soil organic matter. Since plant roots need oxygen, ventilation is an important characteristic of soil. This ventilation can be accomplished via networks of interconnected soil pores, which also absorb and hold rainwater making it readily available for uptake by plants. Since plants require a nearly continuous supply of water, but most regions receive sporadic rainfall, the water-holding capacity of soils is vital for plant survival.\n\nSoils can effectively remove impurities, kill disease agents, and degrade contaminants, this latter property being called natural attenuation. Typically, soils maintain a net absorption of oxygen and methane and undergo a net release of carbon dioxide and nitrous oxide. Soils offer plants physical support, air, water, temperature moderation, nutrients, and protection from toxins. Soils provide readily available nutrients to plants and animals by converting dead organic matter into various nutrient forms.\n\nA typical soil is about 50% solids (45% mineral and 5% organic matter), and 50% voids (or pores) of which half is occupied by water and half by gas. The percent soil mineral and organic content can be treated as a constant (in the short term), while the percent soil water and gas content is considered highly variable whereby a rise in one is simultaneously balanced by a reduction in the other. The pore space allows for the infiltration and movement of air and water, both of which are critical for life existing in soil. Compaction, a common problem with soils, reduces this space, preventing air and water from reaching plant roots and soil organisms.\n\nGiven sufficient time, an undifferentiated soil will evolve a soil profile which consists of two or more layers, referred to as soil horizons, that differ in one or more properties such as in their texture, structure, density, porosity, consistency, temperature, color, and reactivity. The horizons differ greatly in thickness and generally lack sharp boundaries; their development is dependent on the type of parent material, the processes that modify those parent materials, and the soil-forming factors that influence those processes. The biological influences on soil properties are strongest near the surface, while the geochemical influences on soil properties increase with depth. Mature soil profiles typically include three basic master horizons: A, B, and C. The solum normally includes the A and B horizons. The living component of the soil is largely confined to the solum, and is generally more prominent in the A horizon.\n\nThe soil texture is determined by the relative proportions of the individual particles of sand, silt, and clay that make up the soil. The interaction of the individual mineral particles with organic matter, water, gases via biotic and abiotic processes causes those particles to flocculate (stick together) to form aggregates or peds. Where these aggregates can be identified, a soil can be said to be developed, and can be described further in terms of color, porosity, consistency, reaction (acidity), etc.\n\nWater is a critical agent in soil development due to its involvement in the dissolution, precipitation, erosion, transport, and deposition of the materials of which a soil is composed. The mixture of water and dissolved or suspended materials that occupy the soil pore space is called the soil solution. Since soil water is never pure water, but contains hundreds of dissolved organic and mineral substances, it may be more accurately called the soil solution. Water is central to the dissolution, precipitation and leaching of minerals from the soil profile. Finally, water affects the type of vegetation that grows in a soil, which in turn affects the development of the soil, a complex feedback which is exemplified in the dynamics of banded vegetation patterns in semi-arid regions.\n\nSoils supply plants with nutrients, most of which are held in place by particles of clay and organic matter (colloids) The nutrients may be adsorbed on clay mineral surfaces, bound within clay minerals (absorbed), or bound within organic compounds as part of the living organisms or dead soil organic matter. These bound nutrients interact with soil water to buffer the soil solution composition (attenuate changes in the soil solution) as soils wet up or dry out, as plants take up nutrients, as salts are leached, or as acids or alkalis are added.\n\nPlant nutrient availability is affected by soil pH, which is a measure of the hydrogen ion activity in the soil solution. Soil pH is a function of many soil forming factors, and is generally lower (more acid) where weathering is more advanced.\n\nMost plant nutrients, with the exception of nitrogen, originate from the minerals that make up the soil parent material. Some nitrogen originates from rain as dilute nitric acid and ammonia, but most of the nitrogen is available in soils as a result of nitrogen fixation by bacteria. Once in the soil-plant system, most nutrients are recycled through living organisms, plant and microbial residues (soil organic matter), mineral-bound forms, and the soil solution. Both living microorganisms and soil organic matter are of critical importance to this recycling, and thereby to soil formation and soil fertility. Microbial activity in soils may release nutrients from minerals or organic matter for use by plants and other microorganisms, sequester (incorporate) them into living cells, or cause their loss from the soil by volatilisation (loss to the atmosphere as gases) or leaching.\n\nThe history of the study of soil is intimately tied to humans' urgent need to provide food for themselves and forage for our animals. Throughout history, civilizations have prospered or declined as a function of the availability and productivity of their soils.\n\nThe Greek historian Xenophon (450–355 BCE) is credited with being the first to expound upon the merits of green-manuring crops: \"But then whatever weeds are upon the ground, being turned into earth, enrich the soil as much as dung.\"\n\nColumella's \"Husbandry,\" circa 60 CE, advocated the use of lime and that clover and alfalfa (green manure) should be turned under, and was used by 15 generations (450 years) under the Roman Empire until its collapse. From the fall of Rome to the French Revolution, knowledge of soil and agriculture was passed on from parent to child and as a result, crop yields were low. During the European Dark Ages, Yahya Ibn al-'Awwam's handbook, with its emphasis on irrigation, guided the people of North Africa, Spain and the Middle East; a translation of this work was finally carried to the southwest of the United States when under Spanish influence. Olivier de Serres, considered as the father of French agronomy, was the first to suggest the abandonment of fallowing and its replacement by hay meadows within crop rotations, and he highlighted the importance of soil (the French terroir) in the management of vineyards. His famous book \"Le Théâtre d’Agriculture et mesnage des champs\" contributed to the rise of modern, sustainable agriculture and to the collapse of old agricultural practices such as the lifting of forest litter for the amendment of crops (the French \"soutrage\") and assarting, which ruined the soils of western Europe during Middle Ages and even later on according to regions.\n\nExperiments into what made plants grow first led to the idea that the ash left behind when plant matter was burned was the essential element but overlooked the role of nitrogen, which is not left on the ground after combustion, a belief which prevailed until the 19th century. In about 1635, the Flemish chemist Jan Baptist van Helmont thought he had proved water to be the essential element from his famous five years' experiment with a willow tree grown with only the addition of rainwater. His conclusion came from the fact that the increase in the plant's weight had apparently been produced only by the addition of water, with no reduction in the soil's weight. John Woodward (d. 1728) experimented with various types of water ranging from clean to muddy and found muddy water the best, and so he concluded that earthy matter was the essential element. Others concluded it was humus in the soil that passed some essence to the growing plant. Still others held that the vital growth principal was something passed from dead plants or animals to the new plants. At the start of the 18th century, Jethro Tull demonstrated that it was beneficial to cultivate (stir) the soil, but his opinion that the stirring made the fine parts of soil available for plant absorption was erroneous.\n\nAs chemistry developed, it was applied to the investigation of soil fertility. The French chemist Antoine Lavoisier showed in about 1778 that plants and animals must [combust] oxygen internally to live and was able to deduce that most of the 165-pound weight of van Helmont's willow tree derived from air. It was the French agriculturalist Jean-Baptiste Boussingault who by means of experimentation obtained evidence showing that the main sources of carbon, hydrogen and oxygen for plants were air and water, while nitrogen was taken from soil. Justus von Liebig in his book \"Organic chemistry in its applications to agriculture and physiology\" (published 1840), asserted that the chemicals in plants must have come from the soil and air and that to maintain soil fertility, the used minerals must be replaced. Liebig nevertheless believed the nitrogen was supplied from the air. The enrichment of soil with guano by the Incas was rediscovered in 1802, by Alexander von Humboldt. This led to its mining and that of Chilean nitrate and to its application to soil in the United States and Europe after 1840.\n\nThe work of Liebig was a revolution for agriculture, and so other investigators started experimentation based on it. In England John Bennet Lawes and Joseph Henry Gilbert worked in the Rothamsted Experimental Station, founded by the former, and (re)discovered that plants took nitrogen from the soil, and that salts needed to be in an available state to be absorbed by plants. Their investigations also produced the \"superphosphate\", consisting in the acid treatment of phosphate rock. This led to the invention and use of salts of potassium (K) and nitrogen (N) as fertilizers. Ammonia generated by the production of coke was recovered and used as fertiliser. Finally, the chemical basis of nutrients delivered to the soil in manure was understood and in the mid-19th century chemical fertilisers were applied. However, the dynamic interaction of soil and its life forms still awaited discovery.\n\nIn 1856 J. Thomas Way discovered that ammonia contained in fertilisers was transformed into nitrates, and twenty years later Robert Warington proved that this transformation was done by living organisms. In 1890 Sergei Winogradsky announced he had found the bacteria responsible for this transformation.\n\nIt was known that certain legumes could take up nitrogen from the air and fix it to the soil but it took the development of bacteriology towards the end of the 19th century to lead to an understanding of the role played in nitrogen fixation by bacteria. The symbiosis of bacteria and leguminous roots, and the fixation of nitrogen by the bacteria, were simultaneously discovered by the German agronomist Hermann Hellriegel and the Dutch microbiologist Martinus Beijerinck.\n\nCrop rotation, mechanisation, chemical and natural fertilisers led to a doubling of wheat yields in western Europe between 1800 and 1900.\n\nThe scientists who studied the soil in connection with agricultural practices had considered it mainly as a static substrate. However, soil is the result of evolution from more ancient geological materials, under the action of biotic and abiotic (not associated with life) processes. After studies of the improvement of the soil commenced, others began to study soil genesis and as a result also soil types and classifications.\n\nIn 1860, in Mississippi, Eugene W. Hilgard studied the relationship among rock material, climate, and vegetation, and the type of soils that were developed. He realised that the soils were dynamic, and considered soil types classification. Unfortunately his work was not continued. At the same time Vasily Dokuchaev (about 1870) was leading a team of soil scientists in Russia who conducted an extensive survey of soils, finding that similar basic rocks, climate and vegetation types lead to similar soil layering and types, and established the concepts for soil classifications. Due to language barriers, the work of this team was not communicated to western Europe until 1914 through a publication in German by Konstantin Dmitrievich Glinka, a member of the Russian team.\n\nCurtis F. Marbut was influenced by the work of the Russian team, translated Glinka's publication into English, and as he was placed in charge of the U. S. National Cooperative Soil Survey, applied it to a national soil classification system.\n\nSoil formation, or pedogenesis, is the combined effect of physical, chemical, biological and anthropogenic processes working on soil parent material. Soil is said to be formed when organic matter has accumulated and colloids are washed downward, leaving deposits of clay, humus, iron oxide, carbonate, and gypsum, producing a distinct layer called the B horizon. This is a somewhat arbitrary definition as mixtures of sand, silt, clay and humus will support biological and agricultural activity before that time. These constituents are moved from one level to another by water and animal activity. As a result, layers (horizons) form in the soil profile. The alteration and movement of materials within a soil causes the formation of distinctive soil horizons. However, more recent definitions of soil embrace soils without any organic matter, such as those regoliths that formed on Mars and analogous conditions in planet Earth deserts.\n\nAn example of the development of a soil would begin with the weathering of lava flow bedrock, which would produce the purely mineral-based parent material from which the soil texture forms. Soil development would proceed most rapidly from bare rock of recent flows in a warm climate, under heavy and frequent rainfall. Under such conditions, plants (in a first stage nitrogen-fixing lichens and cyanobacteria then epilithic higher plants) become established very quickly on basaltic lava, even though there is very little organic material. The plants are supported by the porous rock as it is filled with nutrient-bearing water that carries minerals dissolved from the rocks. Crevasses and pockets, local topography of the rocks, would hold fine materials and harbour plant roots. The developing plant roots are associated with mineral-weathering mycorrhizal fungi that assist in breaking up the porous lava, and by these means organic matter and a finer mineral soil accumulate with time. Such initial stages of soil development have been described on volcanoes, inselbergs, and glacial moraines.\n\nHow soil formation proceeds is influenced by at least five classic factors that are intertwined in the evolution of a soil. They are: parent material, climate, topography (relief), organisms, and time. When reordered to climate, relief, organisms, parent material, and time, they form the acronym CROPT.\n\nThe mineral material from which a soil forms is called parent material. Rock, whether its origin is igneous, sedimentary, or metamorphic, is the source of all soil mineral materials and the origin of all plant nutrients with the exceptions of nitrogen, hydrogen and carbon. As the parent material is chemically and physically weathered, transported, deposited and precipitated, it is transformed into a soil.\n\nTypical soil parent mineral materials are:\nParent materials are classified according to how they came to be deposited. Residual materials are mineral materials that have weathered in place from primary bedrock. Transported materials are those that have been deposited by water, wind, ice or gravity. Cumulose material is organic matter that has grown and accumulates in place.\n\nResidual soils are soils that develop from their underlying parent rocks and have the same general chemistry as those rocks. The soils found on mesas, plateaux, and plains are residual soils. In the United States as little as three percent of the soils are residual.\n\nMost soils derive from transported materials that have been moved many miles by wind, water, ice and gravity.\n\nCumulose parent material is not moved but originates from deposited organic material. This includes peat and muck soils and results from preservation of plant residues by the low oxygen content of a high water table. While peat may form sterile soils, muck soils may be very fertile.\n\nThe weathering of parent material takes the form of physical weathering (disintegration), chemical weathering (decomposition) and chemical transformation. Generally, minerals that are formed under high temperatures and pressures at great depths within the Earth's mantle are less resistant to weathering, while minerals formed at low temperature and pressure environment of the surface are more resistant to weathering. Weathering is usually confined to the top few meters of geologic material, because physical, chemical, and biological stresses and fluctuations generally decrease with depth. Physical disintegration begins as rocks that have solidified deep in the Earth are exposed to lower pressure near the surface and swell and become mechanically unstable. Chemical decomposition is a function of mineral solubility, the rate of which doubles with each 10 °C rise in temperature, but is strongly dependent on water to effect chemical changes. Rocks that will decompose in a few years in tropical climates will remain unaltered for millennia in deserts. Structural changes are the result of hydration, oxidation, and reduction. Chemical weathering mainly results from the excretion of organic acids and chelating compounds by bacteria and fungi, thought to increase under present-day greenhouse effect.\n\n\nOf the above, hydrolysis and carbonation are the most effective, in particular in regions of high rainfall, temperature and physical erosion. Chemical weathering becomes more effective as the surface area of the rock increases, thus is favoured by physical disintegration. This stems in latitudinal and altitudinal climate gradients in regolith formation.\n\nSaprolite is a particular example of a residual soil formed from the transformation of granite, metamorphic and other types of bedrock into clay minerals. Often called [weathered granite], saprolite is the result of weathering processes that include: hydrolysis, chelation from organic compounds, hydration (the solution of minerals in water with resulting cation and anion pairs) and physical processes that include freezing and thawing. The mineralogical and chemical composition of the primary bedrock material, its physical features, including grain size and degree of consolidation, and the rate and type of weathering transforms the parent material into a different mineral. The texture, pH and mineral constituents of saprolite are inherited from its parent material. This process is also called \"arenization\", resulting in the formation of sandy soils (granitic arenas), thanks to the much higher resistance of quartz compared to other mineral components of granite (micas, amphiboles, feldspars).\n\nThe principal climatic variables influencing soil formation are effective precipitation (i.e., precipitation minus evapotranspiration) and temperature, both of which affect the rates of chemical, physical, and biological processes. Temperature and moisture both influence the organic matter content of soil through their effects on the balance between primary production and decomposition: the colder or drier the climate the lesser atmospheric carbon is fixed as organic matter while the lesser organic matter is decomposed.\n\nClimate is the dominant factor in soil formation, and soils show the distinctive characteristics of the climate zones in which they form, with a feedback to climate through transfer of carbon stocked in soil horizons back to the atmosphere. If warm temperatures and abundant water are present in the profile at the same time, the processes of weathering, leaching, and plant growth will be maximized. According to the climatic determination of biomes, humid climates favor the growth of trees. In contrast, grasses are the dominant native vegetation in subhumid and semiarid regions, while shrubs and brush of various kinds dominate in arid areas.\n\nWater is essential for all the major chemical weathering reactions. To be effective in soil formation, water must penetrate the regolith. The seasonal rainfall distribution, evaporative losses, site topography, and soil permeability interact to determine how effectively precipitation can influence soil formation. The greater the depth of water penetration, the greater the depth of weathering of the soil and its development. Surplus water percolating through the soil profile transports soluble and suspended materials from the upper layers (eluviation) to the lower layers (illuviation), including clay particles and dissolved organic matter. It may also carry away soluble materials in the surface drainage waters. Thus, percolating water stimulates weathering reactions and helps differentiate soil horizons. Likewise, a deficiency of water is a major factor in determining the characteristics of soils of dry regions. Soluble salts are not leached from these soils, and in some cases they build up to levels that curtail plant and microbial growth. Soil profiles in arid and semi-arid regions are also apt to accumulate carbonates and certain types of expansive clays (calcrete or caliche horizons). In tropical soils, when the soil has been deprived of vegetation (e.g. by deforestation) and thereby is submitted to intense evaporation, the upward capillary movement of water, which has dissolved iron and aluminum salts, is responsible for the formation of a superficial hard pan of laterite or bauxite, respectively, which is improper for cutivation, a known case of irreversible soil degradation (lateritization, bauxitization).\n\nThe direct influences of climate include:\n\nClimate directly affects the rate of weathering and leaching. Wind moves sand and smaller particles (dust), especially in arid regions where there is little plant cover, depositing it close or far from the entrainment source. The type and amount of precipitation influence soil formation by affecting the movement of ions and particles through the soil, and aid in the development of different soil profiles. Soil profiles are more distinct in wet and cool climates, where organic materials may accumulate, than in wet and warm climates, where organic materials are rapidly consumed. The effectiveness of water in weathering parent rock material depends on seasonal and daily temperature fluctuations, which favour tensile stresses in rock minerals, and thus their mechanical disaggregation, a process called \"thermal fatigue\". By the same process freeze-thaw cycles are an effective mechanism which breaks up rocks and other consolidated materials.\n\nClimate also indirectly influences soil formation through the effects of vegetation cover and biological activity, which modify the rates of chemical reactions in the soil.\n\nThe topography, or relief, is characterized by the inclination (slope), elevation, and orientation of the terrain. Topography determines the rate of precipitation or runoff and rate of formation or erosion of the surface soil profile. The topographical setting may either hasten or retard the work of climatic forces.\n\nSteep slopes encourage rapid soil loss by erosion and allow less rainfall to enter the soil before running off and hence, little mineral deposition in lower profiles. In semiarid regions, the lower effective rainfall on steeper slopes also results in less complete vegetative cover, so there is less plant contribution to soil formation. For all of these reasons, steep slopes prevent the formation of soil from getting very far ahead of soil destruction. Therefore, soils on steep terrain tend to have rather shallow, poorly developed profiles in comparison to soils on nearby, more level sites.\n\nIn swales and depressions where runoff water tends to concentrate, the regolith is usually more deeply weathered and soil profile development is more advanced. However, in the lowest landscape positions, water may saturate the regolith to such a degree that drainage and aeration are restricted. Here, the weathering of some minerals and the decomposition of organic matter are retarded, while the loss of iron and manganese is accelerated. In such low-lying topography, special profile features characteristic of wetland soils may develop. Depressions allow the accumulation of water, minerals and organic matter and in the extreme, the resulting soils will be saline marshes or peat bogs. Intermediate topography affords the best conditions for the formation of an agriculturally productive soil.\n\nSoil is the most abundant ecosystem on Earth, but the vast majority of organisms in soil are microbes, a great many of which have not been described. There may be a population limit of around one billion cells per gram of soil, but estimates of the number of species vary widely from 50,000 per gram to over a million per gram of soil. The total number of organisms and species can vary widely according to soil type, location, and depth.\n\nPlants, animals, fungi, bacteria and humans affect soil formation (see soil biomantle and stonelayer). Soil animals, including soil macrofauna and soil mesofauna, mix soils as they form burrows and pores, allowing moisture and gases to move about, a process called bioturbation. In the same way, plant roots penetrate soil horizons and open channels upon decomposition. Plants with deep taproots can penetrate many metres through the different soil layers to bring up nutrients from deeper in the profile. Plants have fine roots that excrete organic compounds (sugars, organic acids, mucigel), slough off cells (in particular at their tip) and are easily decomposed, adding organic matter to soil, a process called \"rhizodeposition\". Micro-organisms, including fungi and bacteria, effect chemical exchanges between roots and soil and act as a reserve of nutrients in a soil biological \"hotspot\" called rhizosphere. The growth of roots through the soil stimulates microbial populations, stimulating in turn the activity of their predators (notably amoeba), thereby increasing the mineralization rate, and in last turn root growth, a positive feedback called the soil microbial loop. Out of root influence, in the bulk soil, most bacteria are in a quiescent stage, forming microaggregates, i.e. mucilaginous colonies to which clay particles are glued, offering them a protection against desiccation and predation by soil microfauna (bacteriophagous protozoa and nematodes). Microaggregates (20-250 µm) are ingested by soil mesofauna and macrofauna, and bacterial bodies are partly or totally digested in their guts.\n\nHumans impact soil formation by removing vegetation cover with erosion, waterlogging, lateritization or podzolization (according to climate and topography) as the result. Their tillage also mixes the different soil layers, restarting the soil formation process as less weathered material is mixed with the more developed upper layers, resulting in net increased rate of mineral weathering.\n\nEarthworms, ants, termites, moles, gophers, as well as some millipedes and tenebrionid beetles mix the soil as they burrow, significantly affecting soil formation. Earthworms ingest soil particles and organic residues, enhancing the availability of plant nutrients in the material that passes through their bodies. They aerate and stir the soil and create stable soil aggregates, after having disrupted links between soil particles during the intestinal transit of ingested soil, thereby assuring ready infiltration of water. In addition, as ants and termites build mounds, they transport soil materials from one horizon to another. Other important functions are fulfilled by earthworms in the soil ecosystem, in particular their intense mucus production, both within the intestine and as a lining in their galleries, exert a priming effect on soil microflora, giving them the status of ecosystem engineers, which they share with ants and termites.\n\nIn general, the mixing of the soil by the activities of animals, sometimes called pedoturbation, tends to undo or counteract the tendency of other soil-forming processes that create distinct horizons. Termites and ants may also retard soil profile development by denuding large areas of soil around their nests, leading to increased loss of soil by erosion. Large animals such as gophers, moles, and prairie dogs bore into the lower soil horizons, bringing materials to the surface. Their tunnels are often open to the surface, encouraging the movement of water and air into the subsurface layers. In localized areas, they enhance mixing of the lower and upper horizons by creating, and later refilling, underground tunnels. Old animal burrows in the lower horizons often become filled with soil material from the overlying A horizon, creating profile features known as crotovinas.\n\nVegetation impacts soils in numerous ways. It can prevent erosion caused by excessive rain that might result from surface runoff. Plants shade soils, keeping them cooler and slow evaporation of soil moisture, or conversely, by way of transpiration, plants can cause soils to lose moisture, resulting in complex and highly variable relationships between leaf area index (measuring light interception) and moisture loss: more generally plants prevent soil from desiccation during driest months while they dry it during moister months, thereby acting as a buffer against strong moisture variation. Plants can form new chemicals that can break down minerals, both directly and indirectly through mycorrhizal fungi and rhizosphere bacteria, and improve the soil structure. The type and amount of vegetation depends on climate, topography, soil characteristics and biological factors, mediated or not by human activities. Soil factors such as density, depth, chemistry, pH, temperature and moisture greatly affect the type of plants that can grow in a given location. Dead plants and fallen leaves and stems begin their decomposition on the surface. There, organisms feed on them and mix the organic material with the upper soil layers; these added organic compounds become part of the soil formation process.\n\nHuman activities widely influence soil formation. For example, it is believed that Native Americans regularly set fires to maintain several large areas of prairie grasslands in Indiana and Michigan, although climate and mammalian grazers (e.g. bisons) are also advocated to explain the maintenance of the Great Plains of North America. In more recent times, human destruction of natural vegetation and subsequent tillage of the soil for crop production has abruptly modified soil formation. Likewise, irrigating soil in an arid region drastically influences soil-forming factors, as does adding fertilizer and lime to soils of low fertility.\n\nTime is a factor in the interactions of all the above. While a mixture of sand, silt and clay constitute the texture of a soil and the aggregation of those components produces peds, the development of a distinct B horizon marks the development of a soil or pedogenesis. With time, soils will evolve features that depend on the interplay of the prior listed soil-forming factors. It takes decades to several thousand years for a soil to develop a profile, although the notion of soil development has been criticized, soil being in a constant state-of-change under the influence of fluctuating soil-forming factors. That time period depends strongly on climate, parent material, relief, and biotic activity. For example, recently deposited material from a flood exhibits no soil development as there has not been enough time for the material to form a structure that further defines soil. The original soil surface is buried, and the formation process must begin anew for this deposit. Over time the soil will develop a profile that depends on the intensities of biota and climate. While a soil can achieve relative stability of its properties for extended periods, the soil life cycle ultimately ends in soil conditions that leave it vulnerable to erosion. Despite the inevitability of soil retrogression and degradation, most soil cycles are long.\n\nSoil-forming factors continue to affect soils during their existence, even on \"stable\" landscapes that are long-enduring, some for millions of years. Materials are deposited on top or are blown or washed from the surface. With additions, removals and alterations, soils are always subject to new conditions. Whether these are slow or rapid changes depends on climate, topography and biological activity.\n\nThe physical properties of soils, in order of decreasing importance for ecosystem services such as crop production, are texture, structure, bulk density, porosity, consistency, temperature, colour and resistivity. Soil texture is determined by the relative proportion of the three kinds of soil mineral particles, called soil separates: sand, silt, and clay. At the next larger scale, soil structures called peds or more commonly \"soil aggregates\" are created from the soil separates when iron oxides, carbonates, clay, silica and humus, coat particles and cause them to adhere into larger, relatively stable secondary structures. Soil bulk density, when determined at standardized moisture conditions, is an estimate of soil compaction. Soil porosity consists of the void part of the soil volume and is occupied by gases or water. Soil consistency is the ability of soil materials to stick together. Soil temperature and colour are self-defining. Resistivity refers to the resistance to conduction of electric currents and affects the rate of corrosion of metal and concrete structures which are buried in soil. These properties vary through the depth of a soil profile, i.e. through soil horizons. Most of these properties determine the aeration of the soil and the ability of water to infiltrate and to be held within the soil.\n\nThe mineral components of soil are sand, silt and clay, and their relative proportions determine a soil's texture. Properties that are influenced by soil texture include porosity, permeability, infiltration, shrink-swell rate, water-holding capacity, and susceptibility to erosion. In the illustrated USDA textural classification triangle, the only soil in which neither sand, silt nor clay predominates is called loam. While even pure sand, silt or clay may be considered a soil, from the perspective of conventional agriculture a loam soil with a small amount of organic material is considered \"ideal\", inasmuch as fertilizers or manure are currently used to mitigate nutrient losses due to crop yields in the long term. The mineral constituents of a loam soil might be 40% sand, 40% silt and the balance 20% clay by weight. Soil texture affects soil behaviour, in particular, its retention capacity for nutrients (e.g., cation exchange capacity) and water.\n\nSand and silt are the products of physical and chemical weathering of the parent rock; clay, on the other hand, is most often the product of the precipitation of the dissolved parent rock as a secondary mineral, except when derived from the weathering of mica. It is the surface area to volume ratio (specific surface area) of soil particles and the unbalanced ionic electric charges within those that determine their role in the fertility of soil, as measured by its cation exchange capacity. Sand is least active, having the least specific surface area, followed by silt; clay is the most active. Sand's greatest benefit to soil is that it resists compaction and increases soil porosity, although this property stands only for pure sand, not for sand mixed with smaller minerals which fill the voids among sand grains. Silt is mineralogically like sand but with its higher specific surface area it is more chemically and physically active than sand. But it is the clay content of soil, with its very high specific surface area and generally large number of negative charges, that gives a soil its high retention capacity for water and nutrients. Clay soils also resist wind and water erosion better than silty and sandy soils, as the particles bond tightly to each other,\nand that with a strong mitigation effect of organic matter.\n\nSand is the most stable of the mineral components of soil; it consists of rock fragments, primarily quartz particles, ranging in size from in diameter. Silt ranges in size from . Clay cannot be resolved by optical microscopes as its particles are or less in diameter and a thickness of only 10 angstroms (10 m). In medium-textured soils, clay is often washed downward through the soil profile (a process called eluviation) and accumulates in the subsoil (a process called illuviation). There is no clear relationship between the size of soil mineral components and their mineralogical nature: sand and silt particles can be calcareous as well as siliceous, while textural clay () can be made of very fine quartz particles as well as of multi-layered secondary minerals. Soil mineral components belonging to a given textural class may thus share properties linked to their specific surface area (e.g. moisture retention) but not those linked to their chemical composition (e.g. cation exchange capacity).\n\nSoil components larger than are classed as rock and gravel and are removed before determining the percentages of the remaining components and the textural class of the soil, but are included in the name. For example, a sandy loam soil with 20% gravel would be called gravelly sandy loam.\n\nWhen the organic component of a soil is substantial, the soil is called organic soil rather than mineral soil. A soil is called organic if:\n\n\nThe clumping of the soil textural components of sand, silt and clay causes aggregates to form and the further association of those aggregates into larger units creates soil structures called peds (a contraction of the word pedolith). The adhesion of the soil textural components by organic substances, iron oxides, carbonates, clays, and silica, the breakage of those aggregates from expansion-contraction caused by freezing-thawing and wetting-drying cycles, and the build-up of aggregates by soil animals, microbial colonies and root tips shape soil into distinct geometric forms. The peds evolve into units which have various shapes, sizes and degrees of development. A soil clod, however, is not a ped but rather a mass of soil that results from mechanical disturbance of the soil such as cultivation. Soil structure affects aeration, water movement, conduction of heat, plant root growth and resistance to erosion. Water, in turn, has a strong effect on soil structure, directly via the dissolution and precipitation of minerals, the mechanical destruction of aggregates (slaking) and indirectly by promoting plant, animal and microbial growth.\n\nSoil structure often gives clues to its texture, organic matter content, biological activity, past soil evolution, human use, and the chemical and mineralogical conditions under which the soil formed. While texture is defined by the mineral component of a soil and is an innate property of the soil that does not change with agricultural activities, soil structure can be improved or destroyed by the choice and timing of farming practices.\n\nSoil structural classes:\n\n\nAt the largest scale, the forces that shape a soil's structure result from swelling and shrinkage that initially tend to act horizontally, causing vertically oriented prismatic peds. This mechanical process is mainly exemplified in the development of vertisols. Clayey soil, due to its differential drying rate with respect to the surface, will induce horizontal cracks, reducing columns to blocky peds. Roots, rodents, worms, and freezing-thawing cycles further break the peds into smaller peds of a more or less spherical shape.\n\nAt a smaller scale, plant roots extend into voids (macropores) and remove water causing macroporosity to increase and microporosity to decrease, thereby decreasing aggregate size. At the same time, root hairs and fungal hyphae create microscopic tunnels that break up peds.\n\nAt an even smaller scale, soil aggregation continues as bacteria and fungi exude sticky polysaccharides which bind soil into smaller peds. The addition of the raw organic matter that bacteria and fungi feed upon encourages the formation of this desirable soil structure.\n\nAt the lowest scale, the soil chemistry affects the aggregation or dispersal of soil particles. The clay particles contain polyvalent cations which give the faces of clay layers localized negative charges. At the same time, the edges of the clay plates have a slight positive charge, thereby allowing the edges to adhere to the negative charges on the faces of other clay particles or to flocculate (form clumps). On the other hand, when monovalent ions, such as sodium, invade and displace the polyvalent cations, they weaken the positive charges on the edges, while the negative surface charges are relatively strengthened. This leaves negative charge on the clay faces that repel other clay, causing the particles to push apart, and by doing so deflocculate clay suspensions. As a result, the clay disperses and settles into voids between peds, causing those to close. In this way the open structure of the soil is destroyed and the soil is made impenetrable to air and water. Such sodic soil (also called haline soil) tends to form columnar peds near the surface.\n\nSoil particle density is typically 2.60 to 2.75 grams per cm and is usually unchanging for a given soil. Soil particle density is lower for soils with high organic matter content, and is higher for soils with high iron-oxides content. Soil bulk density is equal to the dry mass of the soil divided by the volume of the soil; i.e., it includes air space and organic materials of the soil volume. Thereby soil bulk density is always less than soil particle density and is a good indicator of soil compaction. The soil bulk density of cultivated loam is about 1.1 to 1.4 g/cm (for comparison water is 1.0 g/cm). Contrary to particle density, soil bulk density is highly variable for a given soil, with a strong causal relationship with soil biological activity and management strategies. However, it has been shown that, depending on species and the size of their aggregates (faeces), earthworms may either increase or decrease soil bulk density. A lower bulk density by itself does not indicate suitability for plant growth due to the confounding influence of soil texture and structure. A high bulk density is indicative of either soil compaction or a mixture of soil textural classes in which small particles fill the voids among coarser particles. Hence the positive correlation between the fractal dimension of soil, considered as a porous medium, and its bulk density, that explains the poor hydraulic conductivity of silty clay loam in the absence of a faunal structure.\n\nPore space is that part of the bulk volume of soil that is not occupied by either mineral or organic matter but is open space occupied by either gases or water. In a productive, medium-textured soil the total pore space is typically about 50% of the soil volume. Pore size varies considerably; the smallest pores (cryptopores; <0.1 µm) hold water too tightly for use by plant roots; plant-available water is held in ultramicropores, micropores and mesopores (0.1–75 µm); and macropores (>75 µm) are generally air-filled when the soil is at field capacity.\n\nSoil texture determines total volume of the smallest pores; clay soils have smaller pores, but more total pore space than sands, despite of a much lower permeability. Soil structure has a strong influence on the larger pores that affect soil aeration, water infiltration and drainage. Tillage has the short-term benefit of temporarily increasing the number of pores of largest size, but these can be rapidly degraded by the destruction of soil aggregation.\n\nThe pore size distribution affects the ability of plants and other organisms to access water and oxygen; large, continuous pores allow rapid transmission of air, water and dissolved nutrients through soil, and small pores store water between rainfall or irrigation events. Pore size variation also compartmentalizes the soil pore space such that many microbial and faunal organisms are not in direct competition with one another, which may explain not only the large number of species present, but the fact that functionally redundant organisms (organisms with the same ecological niche) can co-exist within the same soil.\n\nConsistency is the ability of soil to stick to itself or to other objects (cohesion and adhesion, respectively) and its ability to resist deformation and rupture. It is of approximate use in predicting cultivation problems and the engineering of foundations. Consistency is measured at three moisture conditions: air-dry, moist, and wet. In those conditions the consistency quality depends upon the clay content. In the wet state, the two qualities of stickiness and plasticity are assessed. A soil's resistance to fragmentation and crumbling is assessed in the dry state by rubbing the sample. Its resistance to shearing forces is assessed in the moist state by thumb and finger pressure. Additionally, the cemented consistency depends on cementation by substances other than clay, such as calcium carbonate, silica, oxides and salts; moisture content has little effect on its assessment. The measures of consistency border on subjective compared to other measures such as pH, since they employ the apparent feel of the soil in those states.\n\nThe terms used to describe the soil consistency in three moisture states and a last not affected by the amount of moisture are as follows:\n\n\nSoil consistency is useful in estimating the ability of soil to support buildings and roads. More precise measures of soil strength are often made prior to construction.\n\nSoil temperature depends on the ratio of the energy absorbed to that lost. Soil has a temperature range between -20 to 60 °C, with a mean annual temperature from -10 to 26 °C according to biomes. Soil temperature regulates seed germination, breaking of seed dormancy, plant and root growth and the availability of nutrients. Soil temperature has important seasonal, monthly and daily variations, fluctuations in soil temperature being much lower with increasing soil depth. Heavy mulching (a type of soil cover) can slow the warming of soil in summer, and, at the same time, reduce fluctuations in surface temperature.\n\nMost often, agricultural activities must adapt to soil temperatures by:\n\n\nSoil temperatures can be raised by drying soils or the use of clear plastic mulches. Organic mulches slow the warming of the soil.\n\nThere are various factors that affect soil temperature, such as water content, soil color, and relief (slope, orientation, and elevation), and soil cover (shading and insulation), in addition to air temperature. The color of the ground cover and its insulating properties have a strong influence on soil temperature. Whiter soil tends to have a higher albedo than blacker soil cover, which encourages whiter soils to have lower soil temperatures. The specific heat of soil is the energy required to raise the temperature of soil by 1 °C. The specific heat of soil increases as water content increases, since the heat capacity of water is greater than that of dry soil. The specific heat of pure water is ~ 1 calorie per gram, the specific heat of dry soil is ~ 0.2 calories per gram, hence, the specific heat of wet soil is ~ 0.2 to 1 calories per gram (0.8 to 4.2 kJ per kilogram). Also, a tremendous energy (~540 cal/g or 2260 kJ/kg) is required to evaporate water (known as the heat of vaporization). As such, wet soil usually warms more slowly than dry soil – wet surface soil is typically 3 to 6 °C colder than dry surface soil.\n\nSoil heat flux refers to the rate at which heat energy moves through the soil in response to a temperature difference between two points in the soil. The heat flux density is the amount of energy that flows through soil per unit area per unit time and has both magnitude and direction. For the simple case of conduction into or out of the soil in the vertical direction, which is most often applicable the heat flux density is:\n\nIn SI units\n\nHeat flux is in the direction opposite the temperature gradient, hence the minus sign. That is to say, if the temperature of the surface is higher than at depth x the negative sign will result in a positive value for the heat flux q, and which is interpreted as the heat being conducted into the soil.\n\nSoil temperature is important for the survival and early growth of seedlings. Soil temperatures affect the anatomical and morphological character of root systems. All physical, chemical, and biological processes in soil and roots are affected in particular because of the increased viscosities of water and protoplasm at low temperatures. In general, climates that do not preclude survival and growth of white spruce above ground are sufficiently benign to provide soil temperatures able to maintain white spruce root systems. In some northwestern parts of the range, white spruce occurs on permafrost sites and although young unlignified roots of conifers may have little resistance to freezing, the root system of containerized white spruce was not affected by exposure to a temperature of less than 30 °C.\n\nOptimum temperatures for tree root growth range between 10 °C and 25 °C in general and for spruce in particular. In 2-week-old white spruce seedlings that were then grown for 6 weeks in soil at temperatures of 15 °C, 19 °C, 23 °C, 27 °C, and 31 °C; shoot height, shoot dry weight, stem diameter, root penetration, root volume, and root dry weight all reached maxima at 19 °C.\n\nHowever, whereas strong positive relationships between soil temperature (5 °C to 25 °C) and growth have been found in trembling aspen and balsam poplar, white and other spruce species have shown little or no changes in growth with increasing soil temperature. Such insensitivity to soil low temperature may be common among a number of western and boreal conifers.\n\nSoil temperatures are increasing worldwide under the influence of present-day global climate warming, with opposing views about expected effects on carbon capture and storage and feedback loops to climate change Most threats are about permafrost thawing and attended effects on carbon destocking and ecosystem collapse.\n\nSoil colour is often the first impression one has when viewing soil. Striking colours and contrasting patterns are especially noticeable. The Red River of the South carries sediment eroded from extensive reddish soils like Port Silt Loam in Oklahoma. The Yellow River in China carries yellow sediment from eroding loess soils. Mollisols in the Great Plains of North America are darkened and enriched by organic matter. Podsols in boreal forests have highly contrasting layers due to acidity and leaching.\n\nIn general, color is determined by the organic matter content, drainage conditions, and degree of oxidation. Soil color, while easily discerned, has little use in predicting soil characteristics. It is of use in distinguishing boundaries of horizons within a soil profile, determining the origin of a soil's parent material, as an indication of wetness and waterlogged conditions, and as a qualitative means of measuring organic, iron oxide and clay contents of soils. Color is recorded in the Munsell color system as for instance 10YR3/4 \"Dusky Red\", with 10YR as \"hue\", 3 as \"value\" and 4 as \"chroma\". Munsell color dimensions (hue, value and chroma) can be averaged among samples and treated as quantitative parameters, displaying significant correlations with various soil and vegetation properties.\n\nSoil color is primarily influenced by soil mineralogy. Many soil colours are due to various iron minerals. The development and distribution of colour in a soil profile result from chemical and biological weathering, especially redox reactions. As the primary minerals in soil parent material weather, the elements combine into new and colourful compounds. Iron forms secondary minerals of a yellow or red colour, organic matter decomposes into black and brown humic compounds, and manganese and sulfur can form black mineral deposits. These pigments can produce various colour patterns within a soil. Aerobic conditions produce uniform or gradual colour changes, while reducing environments (anaerobic) result in rapid colour flow with complex, mottled patterns and points of colour concentration.\n\nSoil resistivity is a measure of a soil's ability to retard the conduction of an electric current. The electrical resistivity of soil can affect the rate of galvanic corrosion of metallic structures in contact with the soil. Higher moisture content or increased electrolyte concentration can lower resistivity and increase conductivity, thereby increasing the rate of corrosion. Soil resistivity values typically range from about 1 to 100000 Ω·m, extreme values being for saline soils and dry soils overlaying cristalline rocks, respectively.\n\nWater that enters a field is removed from a field by runoff, drainage, evaporation or transpiration. Runoff is the water that flows on the surface to the edge of the field; drainage is the water that flows through the soil downward or toward the edge of the field underground; evaporative water loss from a field is that part of the water that evaporates into the atmosphere directly from the field's surface; transpiration is the loss of water from the field by its evaporation from the plant itself.\n\nWater affects soil formation, structure, stability and erosion but is of primary concern with respect to plant growth. Water is essential to plants for four reasons:\n\n\nIn addition, water alters the soil profile by dissolving and re-depositing minerals, often at lower levels, and possibly leaving the soil sterile in the case of extreme rainfall and drainage. In a loam soil, solids constitute half the volume, gas one-quarter of the volume, and water one-quarter of the volume of which only half will be available to most plants, with a strong variation according to matric potential.\n\nA flooded field will drain the gravitational water under the influence of gravity until water's adhesive and cohesive forces resist further drainage at which point it is said to have reached field capacity. At that point, plants must apply suction to draw water from a soil. The water that plants may draw from the soil is called the available water. Once the available water is used up the remaining moisture is called unavailable water as the plant cannot produce sufficient suction to draw that water in. A plant must produce suction that increases from zero for a flooded field to 1/3 bar at field dry condition (one bar is a little less than one atmosphere pressure). At 15 bar suction, wilting point, seeds will not germinate, plants begin to wilt and then die. Water moves in soil under the influence of gravity, osmosis and capillarity. When water enters the soil, it displaces air from interconnected macropores by buoyancy, and breaks aggregates into which air is entrapped, a process called slaking.\n\nThe rate at which a soil can absorb water depends on the soil and its other conditions. As a plant grows, its roots remove water from the largest pores (macropores) first. Soon the larger pores hold only air, and the remaining water is found only in the intermediate- and smallest-sized pores (micropores). The water in the smallest pores is so strongly held to particle surfaces that plant roots cannot pull it away. Consequently, not all soil water is available to plants, with a strong dependence on texture. When saturated, the soil may lose nutrients as the water drains. Water moves in a draining field under the influence of pressure where the soil is locally saturated and by capillarity pull to drier parts of the soil. Most plant water needs are supplied from the suction caused by evaporation from plant leaves (transpiration) and a lower fraction is supplied by suction created by osmotic pressure differences between the plant interior and the soil solution. Plant roots must seek out water and grow preferentially in moister soil microsites, but some parts of the root system are also able to remoisten dry parts of the soil. Insufficient water will damage the yield of a crop. Most of the available water is used in transpiration to pull nutrients into the plant.\n\nWater is retained in a soil when the adhesive force of attraction that water's hydrogen atoms have for the oxygen of soil particles is stronger than the cohesive forces that water's hydrogen feels for other water oxygen atoms. When a field is flooded, the soil pore space is completely filled by water. The field will drain under the force of gravity until it reaches what is called field capacity, at which point the smallest pores are filled with water and the largest with water and gases. The total amount of water held when field capacity is reached is a function of the specific surface area of the soil particles. As a result, high clay and high organic soils have higher field capacities. The total force required to pull or push water out of soil is termed suction and usually expressed in units of bars (10 pascal) which is just a little less than one-atmosphere pressure. Alternatively, the terms \"soil moisture tension\" or water potential may be used.\n\nThe forces with which water is held in soils determine its availability to plants. Forces of adhesion hold water strongly to mineral and humus surfaces and less strongly to itself by cohesive forces. A plant's root may penetrate a very small volume of water that is adhering to soil and be initially able to draw in water that is only lightly held by the cohesive forces. But as the droplet is drawn down, the forces of adhesion of the water for the soil particles produce increasingly higher suction, finally up to 15 bar. At 15 bar suction, the soil water amount is called wilting point. At that suction the plant cannot sustain its water needs as water is still being lost from the plant by transpiration, the plant's turgidity is lost, and it wilts, although stomatal closure may decrease transpiration and thus may retard wilting below the wilting point, in particular under adaptation or acclimatization to drought. The next level, called air-dry, occurs at 1000 bar suction. Finally the oven dry condition is reached at 10,000 bar suction. All water below wilting percentage is called unavailable water.\n\nWhen the soil moisture content is optimal for plant growth, the water in the large and intermediate size pores can move about in the soil and be easily used by plants. The amount of water remaining in a soil drained to field capacity and the amount that is available are functions of the soil type. Sandy soil will retain very little water, while clay will hold the maximum amount. The time required to drain a field from flooded condition for a clay loam that begins at 43% water by weight to a field capacity of 22% is six days, whereas a sand loam that is flooded to its maximum of 22% water will take two days to reach field capacity of 11% water. The available water for the clay loam might be 11% whereas for the sand loam it might be only 8% by weight.\n\nThe above are average values for the soil textures as the percentages of sand, silt and clay vary.\n\nWater moves through soil due to the force of gravity, osmosis and capillarity. At zero to one-third bar suction, water is pushed through soil from the point of its application under the force of gravity and the pressure gradient created by the pressure of the water; this is called saturated flow. At higher suction, water movement is pulled by capillarity from wetter toward drier soil. This is caused by water's adhesion to soil solids, and is called unsaturated flow.\n\nWater infiltration and movement in soil is controlled by six factors:\n\n\nWater infiltration rates range from per hour for high clay soils to per hour for sand and well stabilised and aggregated soil structures. Water flows through the ground unevenly, in the form of so-called \"gravity fingers\", because of the surface tension between water particles.\n\nTree roots, whether living or dead, create preferential channels for rainwater flow through soil, magnifying infiltration rates of water up to 27 times.\n\nFlooding temporarily increases soil permeability in river beds, helping to recharge aquifers.\n\nWater applied to a soil is pushed by pressure gradients from the point of its application where it is saturated locally, to less saturated areas, such as the vadose zone. Once soil is completely wetted, any more water will move downward, or percolate out of the range of plant roots, carrying with it clay, humus, nutrients, primarily cations, and various contaminants, including pesticides, pollutants, viruses and bacteria, potentially causing groundwater contamination. In order of decreasing solubility, the leached nutrients are:\n\nIn the United States percolation water due to rainfall ranges from zero inches just east of the Rocky Mountains to twenty or more inches in the Appalachian Mountains and the north coast of the Gulf of Mexico.\n\nSoil physics (Darcy-type model) predicts that at suctions less than one-third bar, water moves theoretically in all directions via unsaturated flow at a rate that is dependent on the square of the diameter of the water-filled pores, but there is still not an adequate physical theory linking all types of waterflow in soil. Preferential flow occurs along interconnected macropores, crevices, root and worm channels, which drain water under gravity. Water is also pulled by capillary action due to the adhesion force of water to the soil solids, producing a suction gradient from wet towards drier soil and from macropores to micropores. Water flow (also called hydraulic conductivity) is primarily from coarse-textured soil into fine-textured soil horizons and is slowest in fine-textured soils such as clay.\n\nOf equal importance to the storage and movement of water in soil is the means by which plants acquire it and their nutrients. Most soil water is taken up by plants as passive absorption caused by the pulling force of water evaporating (transpiring) from the long column of water (xylem sap flow) that leads from the plant's roots to its leaves, according to the cohesion-tension theory. The upward movement of water and solutes (hydraulic lift) is regulated in the roots by the endodermis and in the plant foliage by stomatal conductance, and can be interrupted in root and shoot xylem vessels by cavitation, also called \"xylem embolism\". In addition, the high concentration of salts within plant roots creates an osmotic pressure gradient that pushes soil water into the roots. Osmotic absorption becomes more important during times of low water transpiration caused by lower temperatures (for example at night) or high humidity, and the reverse occurs under high temperature or low humidity. It is these process that cause guttation and wilting, respectively.\n\nRoot extension is vital for plant survival. A study of a single winter rye plant grown for four months in one cubic foot of loam soil showed that the plant developed 13,800,000 roots, a total of 385 miles in length with 2,550 square feet in surface area; and 14 billion hair roots of 6,600 miles total length and 4,320 square feet total area; for a total surface area of 6,870 square feet (83 ft squared). The total surface area of the loam soil was estimated to be 560,000 square feet. In other words, the roots were in contact with only 1.2% of the soil. However, root extension should be viewed as a dynamic process, allowing new roots to explore a new volume of soil each day, increasing dramatically the total volume of soil explored over a given growth period, and thus the volume of water taken up by the root system over this period. Root architecture, i.e. the spatial configuration of the root system, plays a prominent role in the adaptation of plants to soil water and nutrient availabiity, and thus in plant productivity.\n\nRoots must seek out water as the unsaturated flow of water in soil can move only at a rate of up to 2.5 cm (one inch) per day; as a result they are constantly dying and growing as they seek out high concentrations of soil moisture. Insufficient soil moisture, to the point of causing wilting, will cause permanent damage and crop yields will suffer. When grain sorghum was exposed to soil suction as low as 13.0 bar during the seed head emergence through bloom and seed set stages of growth, its production was reduced by 34%.\n\nOnly a small fraction (0.1% to 1%) of the water used by a plant is held within the plant. The majority is ultimately lost via transpiration, while evaporation from the soil surface is also substantial, the transpiration:evaporation ratio varying according to vegetation type and climate, peaking in tropical rainforests and dipping in steppes and deserts. Transpiration plus evaporative soil moisture loss is called evapotranspiration. Evapotranspiration plus water held in the plant totals to consumptive use, which is nearly identical to evapotranspiration.\n\nThe total water used in an agricultural field includes surface runoff, drainage and consumptive use. The use of loose mulches will reduce evaporative losses for a period after a field is irrigated, but in the end the total evaporative loss (plant plus soil) will approach that of an uncovered soil, while more water is immediately available for plant growth. Water use efficiency is measured by the transpiration ratio, which is the ratio of the total water transpired by a plant to the dry weight of the harvested plant. Transpiration ratios for crops range from 300 to 700. For example, alfalfa may have a transpiration ratio of 500 and as a result 500 kilograms of water will produce one kilogram of dry alfalfa.\n\nThe atmosphere of soil, or soil gas, is radically different from the atmosphere above. The consumption of oxygen by microbes and plant roots, and their release of carbon dioxide, decrease oxygen and increase carbon dioxide concentration. Atmospheric CO concentration is 0.04%, but in the soil pore space it may range from 10 to 100 times that level, thus potentially contributing to the inhibition of root respiration. Calcareous soils regulate CO concentration thanks to carbonate buffering, contrary to acid soils in which all CO respired accumulates in the soil pore system. At extreme levels CO is toxic. This suggests a possible negative feedback control of soil CO concentration through its inhibitory effects on root and microbial respiration (also called 'soil respiration'). In addition, the soil voids are saturated with water vapour, at least until the point of maximal hygroscopicity, beyond which a vapour-pressure deficit occurs in the soil pore space. Adequate porosity is necessary, not just to allow the penetration of water, but also to allow gases to diffuse in and out. Movement of gases is by diffusion from high concentrations to lower, the diffusion coefficient decreasing with soil compaction. Oxygen from above atmosphere diffuses in the soil where it is consumed and levels of carbon dioxide in excess of above atmosphere diffuse out with other gases (including greenhouse gases) as well as water. Soil texture and structure strongly affect soil porosity and gas diffusion. It is the total pore space (porosity) of soil, not the pore size, and the degree of pore interconnection (or conversely pore sealing), together with water content, air turbulence and temperature, that determine the rate of diffusion of gases into and out of soil. Platy soil structure and soil compaction (low porosity) impede gas flow, and a deficiency of oxygen may encourage anaerobic bacteria to reduce (strip oxygen) from nitrate NO to the gases N, NO, and NO, which are then lost to the atmosphere, thereby depleting the soil of nitrogen. Aerated soil is also a net sink of methane CH but a net producer of methane (a strong heat-absorbing greenhouse gas) when soils are depleted of oxygen and subject to elevated temperatures.\n\nSoil atmosphere is also the seat of emissions of volatiles other than carbon and nitrogen oxides from various soil organisms, e.g. roots, bacteria, fungi, animals. These volatiles are used as chemical cues, making soil atmosphere the seat of interaction networks playing a decisive role in the stability, dynamics and evolution of soil ecosystems. Biogenic soil volatile organic compounds are exchanged with the aboveground atmosphere, in which they are just 1–2 orders of magnitude lower than those from aboveground vegetation.\n\nWe humans can get some idea of the soil atmosphere through the well-known 'after-the-rain' scent, when infiltering rainwater flushes out the whole soil atmosphere after a drought period, or when soil is excavated, a bulk property attributed in a reductionist manner to particular biochemical compounds such as petrichor or geosmin.\n\nSoil particles can be classified by their chemical composition (mineralogy) as well as their size. The particle size distribution of a soil, its texture, determines many of the properties of that soil, in particular hydraulic conductivity and water potential but the mineralogy of those particles can strongly modify those properties. The mineralogy of the finest soil particles, clay, is especially important.\n\nGravel, sand and silt are the larger soil particles, and their mineralogy is often inherited from the parent material of the soil, but may include products of weathering (such as concretions of calcium carbonate or iron oxide), or residues of plant and animal life (such as silica phytoliths). Quartz is the most common mineral in the sand or silt fraction as it is resistant to chemical weathering, except under hot climate; other common minerals are feldspars, micas and ferromagnesian minerals such as pyroxenes, amphiboles and olivines, which are dissolved or transformed in clay under the combined influence of physico-chemical and biological processes.\n\nDue to its high specific surface area and its unbalanced negative electric charges, clay is the most active mineral component of soil. It is a colloidal and most often a crystalline material. In soils, clay is a soil textural class and is defined in a physical sense as any mineral particle less than in effective diameter. Many soil minerals, such as gypsum, carbonates, or quartz, are small enough to be classified as clay based on their physical size, but chemically they do not afford the same utility as do mineralogically-defined clay minerals. Chemically, clay minerals are a range of phyllosilicate minerals with certain reactive properties.\n\nBefore the advent of X-ray diffraction clay was thought to be very small particles of quartz, feldspar, mica, hornblende or augite, but it is now known to be (with the exception of mica-based clays) a precipitate with a mineralogical composition that is dependent on but different from its parent materials and is classed as a secondary mineral. The type of clay that is formed is a function of the parent material and the composition of the minerals in solution. Clay minerals continue to be formed as long as the soil exists. Mica-based clays result from a modification of the primary mica mineral in such a way that it behaves and is classed as a clay. Most clays are crystalline, but some clays or some parts of clay minerals are amorphous. The clays of a soil are a mixture of the various types of clay, but one type predominates.\n\nTypically there are four main groups of clay minerals: kaolinite, montmorillonite-smectite, illite, and chlorite. Most clays are crystalline and most are made up of three or four planes of oxygen held together by planes of aluminium and silicon by way of ionic bonds that together form a single layer of clay. The spatial arrangement of the oxygen atoms determines clay's structure. Half of the weight of clay is oxygen, but on a volume basis oxygen is ninety percent. The layers of clay are sometimes held together through hydrogen bonds, sodium or potassium bridges and as a result will swell less in the presence of water. Clays such as montmorillonite have layers that are loosely attached and will swell greatly when water intervenes between the layers.\n\nIn a wider sense clays can be classified as:\n\n\nAlumino-silica clays or aluminosilicate clays are characterised by their regular crystalline or quasi-crystalline structure. Oxygen in ionic bonds with silicon forms a tetrahedral coordination (silicon at the center) which in turn forms sheets of silica. Two sheets of silica are bonded together by a plane of aluminium which forms an octahedral coordination, called alumina, with the oxygens of the silica sheet above and that below it. Hydroxyl ions (OH) sometimes substitute for oxygen. During the clay formation process, Al may substitute for Si in the silica layer, and as much as one fourth of the aluminium Al may be substituted by Zn, Mg or Fe in the alumina layer. The substitution of lower-valence cations for higher-valence cations (isomorphous substitution) gives clay a local negative charge on an oxygen atom that attracts and holds water and positively charged soil cations, some of which are of value for plant growth. Isomorphous substitution occurs during the clay's formation and does not change with time.\n\nThe carbonate and sulfate minerals are much more soluble and hence are found primarily in desert soils where leaching is less active.\n\nAmorphous clays are young, and commonly found in volcanic ash. They are mixtures of alumina and silica which have not formed the ordered crystal shape of alumino-silica clays which time would provide. The majority of their negative charges originates from hydroxyl ions, which can gain or lose a hydrogen ion (H) in response to soil pH, in such way was as to buffer the soil pH. They may have either a negative charge provided by the attached hydroxyl ion (OH), which can attract a cation, or lose the hydrogen of the hydroxyl to solution and display a positive charge which can attract anions. As a result, they may display either high CEC in an acid soil solution, or high anion exchange capacity in a basic soil solution.\n\nSesquioxide clays are a product of heavy rainfall that has leached most of the silica from alumino-silica clay, leaving the less soluble oxides iron hematite (FeO), iron hydroxide (Fe(OH)), aluminium hydroxide gibbsite (Al(OH)), hydrated manganese birnessite (MnO). It takes hundreds of thousands of years of leaching to create sesquioxide clays. \"Sesqui\" is Latin for \"one and one-half\": there are three parts oxygen to two parts iron or aluminium; hence the ratio is one and one-half (not true for all). They are hydrated and act as either amorphous or crystalline. They are not sticky and do not swell, and soils high in them behave much like sand and can rapidly pass water. They are able to hold large quantities of phosphates. Sesquioxides have low CEC but are able to hold anions as well as cations. Such soils range from yellow to red in colour. Such clays tend to hold phosphorus so tightly that it is unavailable for absorption by plants.\n\nHumus is the final state of decomposition of organic matter. While it may linger for a thousand years, on the larger scale of the age of the mineral soil components, it is temporary. It is composed of the very stable lignins (30%) and complex sugars (polyuronides, 30%), proteins (30%), waxes, and fats that are resistant to breakdown by microbes. Its chemical assay is 60% carbon, 5% nitrogen, some oxygen and the remainder hydrogen, sulfur, and phosphorus. On a dry weight basis, the CEC of humus is many times greater than that of clay.\n\nIn the extreme environment of high temperatures and the leaching caused by the heavy rain of tropical rain forests, the clay and organic colloids are largely destroyed. The heavy rains wash the alumino-silicate clays from the soil leaving only sesquioxide clays of low CEC. The high temperatures and humidity allow bacteria and fungi to virtually dissolve any organic matter on the rain-forest floor overnight and much of the nutrients are volatilized or leached from the soil and lost. However, carbon in the form of charcoal is far more stable than soil colloids and is capable of performing many of the functions of the soil colloids of sub-tropical soils. Soil containing substantial quantities of charcoal, of an anthropogenic origin, is called terra preta. Research into terra preta is still young but is promising. Fallow periods \"on the Amazonian Dark Earths can be as short as 6 months, whereas fallow periods on oxisols are usually 8 to 10 years long\"\n\nThe chemistry of a soil determines its ability to supply available plant nutrients and affects its physical properties and the health of its microbial population. In addition, a soil's chemistry also determines its corrosivity, stability, and ability to absorb pollutants and to filter water. It is the surface chemistry of mineral and organic colloids that determines soil's chemical properties. \"A colloid is a small, insoluble, nondiffusible particle larger than a molecule but small enough to remain suspended in a fluid medium without settling. Most soils contain organic colloidal particles called humus as well as the inorganic colloidal particles of clays.\" The very high specific surface area of colloids and their net charges, gives soil its ability to hold and release ions. Negatively charged sites on colloids attract and release cations in what is referred to as cation exchange. Cation-exchange capacity (CEC) is the amount of exchangeable cations per unit weight of dry soil and is expressed in terms of milliequivalents of positively charged ions per 100 grams of soil (or centimoles of positive charge per kilogram of soil; cmol/kg). Similarly, positively charged sites on colloids can attract and release anions in the soil giving the soil anion exchange capacity (AEC).\n\nThe cation exchange, that takes place between colloids and soil water, buffers (moderates) soil pH, alters soil structure, and purifies percolating water by adsorbing cations of all types, both useful and harmful.\n\nThe negative or positive charges on colloid particles make them able to hold cations or anions, respectively, to their surfaces. The charges result from four sources.\n\n\nCations held to the negatively charged colloids resist being washed downward by water and out of reach of plants' roots, thereby preserving the fertility of soils in areas of moderate rainfall and low temperatures.\n\nThere is a hierarchy in the process of cation exchange on colloids, as they differ in the strength of adsorption by the colloid and hence their ability to replace one another. If present in equal amounts in the soil water solution:\n\nAl replaces H replaces Ca replaces Mg replaces K same as NH replaces Na\n\nIf one cation is added in large amounts, it may replace the others by the sheer force of its numbers. This is called mass action. This is largely what occurs with the addition of fertiliser.\n\nAs the soil solution becomes more acidic (low pH, and an abundance of H), the other cations more weakly bound to colloids are pushed into solution as hydrogen ions occupy those sites. A low pH may cause hydrogen of hydroxyl groups to be pulled into solution, leaving charged sites on the colloid available to be occupied by other cations. This ionisation of hydroxyl groups on the surface of soil colloids creates what is described as pH-dependent charges. Unlike permanent charges developed by isomorphous substitution, pH-dependent charges are variable and increase with increasing pH. Freed cations can be made available to plants but are also prone to be leached from the soil, possibly making the soil less fertile. Plants are able to excrete H into the soil and by that means, change the pH of the soil near the root and push cations off the colloids, thus making those available to the plant.\n\nCation exchange capacity should be thought of as the soil's ability to remove cations from the soil water solution and sequester those to be exchanged later as the plant roots release hydrogen ions to the solution. CEC is the amount of exchangeable hydrogen cation (H) that will combine with 100 grams dry weight of soil and whose measure is one milliequivalents per 100 grams of soil (1 meq/100 g). Hydrogen ions have a single charge and one-thousandth of a gram of hydrogen ions per 100 grams dry soil gives a measure of one milliequivalent of hydrogen ion. Calcium, with an atomic weight 40 times that of hydrogen and with a valence of two, converts to (40/2) x 1 milliequivalent = 20 milliequivalents of hydrogen ion per 100 grams of dry soil or 20 meq/100 g. The modern measure of CEC is expressed as centimoles of positive charge per kilogram (cmol/kg) of oven-dry soil.\n\nMost of the soil's CEC occurs on clay and humus colloids, and the lack of those in hot, humid, wet climates, due to leaching and decomposition respectively, explains the relative sterility of tropical soils. Live plant roots also have some CEC.\n\nAnion exchange capacity should be thought of as the soil's ability to remove anions from the soil water solution and sequester those for later exchange as the plant roots release carbonate anions to the soil water solution. Those colloids which have low CEC tend to have some AEC. Amorphous and sesquioxide clays have the highest AEC, followed by the iron oxides. Levels of AEC are much lower than for CEC. Phosphates tend to be held at anion exchange sites.\n\nIron and aluminum hydroxide clays are able to exchange their hydroxide anions (OH) for other anions. The order reflecting the strength of anion adhesion is as follows:\n\nThe amount of exchangeable anions is of a magnitude of tenths to a few milliequivalents per 100 g dry soil. As pH rises, there are relatively more hydroxyls, which will displace anions from the colloids and force them into solution and out of storage; hence AEC decreases with increasing pH (alkalinity).\n\nSoil reactivity is expressed in terms of pH and is a measure of the acidity or alkalinity of the soil. More precisely, it is a measure of hydrogen ion concentration in an aqueous solution and ranges in values from 0 to 14 (acidic to basic) but practically speaking for soils, pH ranges from 3.5 to 9.5, as pH values beyond those extremes are toxic to life forms.\n\nAt 25 °C an aqueous solution that has a pH of 3.5 has 10 moles H (hydrogen ions) per litre of solution (and also 10 mole/litre OH). A pH of 7, defined as neutral, has 10 moles hydrogen ions per litre of solution and also 10 moles of OH per litre; since the two concentrations are equal, they are said to neutralise each other. A pH of 9.5 has 10 moles hydrogen ions per litre of solution (and also 10 mole per litre OH). A pH of 3.5 has one million times more hydrogen ions per litre than a solution with pH of 9.5 (9.5 - 3.5 = 6 or 10) and is more acidic.\n\nThe effect of pH on a soil is to remove from the soil or to make available certain ions. Soils with high acidity tend to have toxic amounts of aluminium and manganese. Plants which need calcium need moderate alkalinity, but most minerals are more soluble in acid soils. Soil organisms are hindered by high acidity, and most agricultural crops do best with mineral soils of pH 6.5 and organic soils of pH 5.5.\n\nIn high rainfall areas, soils tend to acidity as the basic cations are forced off the soil colloids by the mass action of hydrogen ions from the rain as those attach to the colloids. High rainfall rates can then wash the nutrients out, leaving the soil sterile. Once the colloids are saturated with H, the addition of any more hydrogen ions or aluminum hydroxyl cations drives the pH even lower (more acidic) as the soil has been left with no buffering capacity. In areas of extreme rainfall and high temperatures, the clay and humus may be washed out, further reducing the buffering capacity of the soil. In low rainfall areas, unleached calcium pushes pH to 8.5 and with the addition of exchangeable sodium, soils may reach pH 10. Beyond a pH of 9, plant growth is reduced. High pH results in low micro-nutrient mobility, but water-soluble chelates of those nutrients can correct the deficit. Sodium can be reduced by the addition of gypsum (calcium sulphate) as calcium adheres to clay more tightly than does sodium causing sodium to be pushed into the soil water solution where it can be washed out by an abundance of water.\n\nThere are acid-forming cations (hydrogen and aluminium) and there are base-forming cations. The fraction of the base-forming cations that occupy positions on the soil colloids is called the base saturation percentage. If a soil has a CEC of 20 meq and 5 meq are aluminium and hydrogen cations (acid-forming), the remainder of positions on the colloids (20-5 = 15 meq) are assumed occupied by base-forming cations, so that the percentage base saturation is 15/20 x 100% = 75% (the compliment 25% is assumed acid-forming cations). When the soil pH is 7 (neutral), base saturation is 100 percent and there are no hydrogen ions stored on the colloids. Base saturation is almost in direct proportion to pH (increases with increasing pH). It is of use in calculating the amount of lime needed to neutralise an acid soil. The amount of lime needed to neutralize a soil must take account of the amount of acid forming ions on the colloids not just those in the soil water solution. The addition of enough lime to neutralize the soil water solution will be insufficient to change the pH, as the acid forming cations stored on the soil colloids will tend to restore the original pH condition as they are pushed off those colloids by the calcium of the added lime.\n\nThe resistance of soil to change in pH, as a result of the addition of acid or basic material, is a measure of the buffering capacity of a soil and (for a particular soil type) increases as the CEC increases. Hence, pure sand has almost no buffering ability, while soils high in colloids have high buffering capacity. Buffering occurs by cation exchange and neutralisation.\n\nThe addition of a small amount highly basic aqueous ammonia to a soil will cause the ammonium to displace hydrogen ions from the colloids, and the end product is water and colloidally fixed ammonium, but little permanent change overall in soil pH.\n\nThe addition of a small amount of lime, Ca(OH), will displace hydrogen ions from the soil colloids, causing the fixation of calcium to colloids and the evolution of CO and water, with little permanent change in soil pH.\n\nThe above are examples of the buffering of soil pH. The general principal is that an increase in a particular cation in the soil water solution will cause that cation to be fixed to colloids (buffered) and a decrease in solution of that cation will cause it to be withdrawn from the colloid and moved into solution (buffered). The degree of buffering is often related to the CEC of the soil; the greater the CEC, the greater the buffering capacity of the soil.\n\nSixteen elements or nutrients are essential for plant growth and reproduction. They are carbon C, hydrogen H, oxygen O, nitrogen N, phosphorus P, potassium K, sulfur S, calcium Ca, magnesium Mg, iron Fe, boron B, manganese Mn, copper Cu, zinc Zn, molybdenum Mo, nickel Ni and chlorine Cl. Nutrients required for plants to complete their life cycle are considered essential nutrients. Nutrients that enhance the growth of plants but are not necessary to complete the plant's life cycle are considered non-essential. With the exception of carbon, hydrogen and oxygen, which are supplied by carbon dioxide and water, and nitrogen, provided through nitrogen fixation, the nutrients derive originally from the mineral component of the soil.\n\nPlant uptake of nutrients can only proceed when they are present in a plant-available form. In most situations, nutrients are absorbed in an ionic form from (or together with) soil water. Although minerals are the origin of most nutrients, and the bulk of most nutrient elements in the soil is held in crystalline form within primary and secondary minerals, they weather too slowly to support rapid plant growth. For example, The application of finely ground minerals, feldspar and apatite, to soil seldom provides the necessary amounts of potassium and phosphorus at a rate sufficient for good plant growth, as most of the nutrients remain bound in the crystals of those minerals.\n\nThe nutrients adsorbed onto the surfaces of clay colloids and soil organic matter provide a more accessible reservoir of many plant nutrients (e.g. K, Ca, Mg, P, Zn). As plants absorb the nutrients from the soil water, the soluble pool is replenished from the surface-bound pool. The decomposition of soil organic matter by microorganisms is another mechanism whereby the soluble pool of nutrients is replenished – this is important for the supply of plant-available N, S, P, and B from soil.\n\nGram for gram, the capacity of humus to hold nutrients and water is far greater than that of clay minerals. All in all, small amounts of humus may remarkably increase the soil's capacity to promote plant growth.\n\nNutrients in the soil are taken up by the plant through its roots. To be taken up by a plant, a nutrient element must be located near the root surface; however, the supply of nutrients in contact with the root is rapidly depleted. There are three basic mechanisms whereby nutrient ions dissolved in the soil solution are brought into contact with plant roots:\n\n\nAll three mechanisms operate simultaneously, but one mechanism or another may be most important for a particular nutrient. For example, in the case of calcium, which is generally plentiful in the soil solution, mass flow alone can usually bring sufficient amounts to the root surface. However, in the case of phosphorus, diffusion is needed to supplement mass flow. For the most part, nutrient ions must travel some distance in the soil solution to reach the root surface. This movement can take place by mass flow, as when dissolved nutrients are carried along with the soil water flowing toward a root that is actively drawing water from the soil. In this type of movement, the nutrient ions are somewhat analogous to leaves floating down a stream. In addition, nutrient ions continually move by diffusion from areas of greater concentration toward the nutrient-depleted areas of lower concentration around the root surface. That process is due to random motion of molecules. By this means, plants can continue to take up nutrients even at night, when water is only slowly absorbed into the roots as transpiration has almost stopped. Finally, root interception comes into play as roots continually grow into new, undepleted soil.\n\nIn the above table, phosphorus and potassium nutrients move more by diffusion than they do by mass flow in the soil water solution, as they are rapidly taken up by the roots creating a concentration of almost zero near the roots (the plants cannot transpire enough water to draw more of those nutrients near the roots). The very steep concentration gradient is of greater influence in the movement of those ions than is the movement of those by mass flow. The movement by mass flow requires the transpiration of water from the plant causing water and solution ions to also move toward the roots. Movement by root interception is slowest as the plants must extend their roots.\n\nPlants move ions out of their roots in an effort to move nutrients in from the soil. Hydrogen H is exchanged for other cations, and carbonate (HCO) and hydroxide (OH) anions are exchanged for nutrient anions. As plant roots remove nutrients from the soil water solution, they are replenished as other ions move off of clay and humus (by ion exchange or desorption), are added from the weathering of soil minerals, and are released by the decomposition of soil organic matter. Plants derive a large proportion of their anion nutrients from decomposing organic matter, which typically holds about 95 percent of the soil nitrogen, 5 to 60 percent of the soil phosphorus and about 80 percent of the soil sulfur. Where crops are produced, the replenishment of nutrients in the soil must usually be augmented by the addition of fertilizer or organic matter.\n\nBecause nutrient uptake is an active metabolic process, conditions that inhibit root metabolism may also inhibit nutrient uptake. Examples of such conditions include waterlogging or soil compaction resulting in poor soil aeration, excessively high or low soil temperatures, and above-ground conditions that result in low translocation of sugars to plant roots.\n\nPlants obtain their carbon from atmospheric carbon dioxide. About 45% of a plant's dry mass is carbon; plant residues typically have a carbon to nitrogen ratio (C/N) of between 13:1 and 100:1. As the soil organic material is digested by arthropods and micro-organisms, the C/N decreases as the carbonaceous material is metabolized and carbon dioxide (CO) is released as a byproduct which then finds its way out of the soil and into the atmosphere. The nitrogen is sequestered in the bodies of the living matter of those decomposing organisms and so it builds up in the soil. Normal CO concentration in the atmosphere is 0.03%, this can be the factor limiting plant growth. In a field of maize on a still day during high light conditions in the growing season, the CO concentration drops very low, but under such conditions the crop could use up to 20 times the normal concentration. The respiration of CO by soil micro-organisms decomposing soil organic matter contributes an important amount of CO to the photosynthesising plants. Within the soil, CO concentration is 10 to 100 times that of atmospheric levels but may rise to toxic levels if the soil porosity is low or if diffusion is impeded by flooding.\n\nNitrogen is the most critical element obtained by plants from the soil and nitrogen deficiency often limits plant growth. Plants can use the nitrogen as either the ammonium cation (NH) or the anion nitrate (NO). Usually, most of the nitrogen in soil is bound within organic compounds that make up the soil organic matter, and must be mineralized to the ammonium or nitrate form before it can be taken up by most plants. The total nitrogen content depends largely on the soil organic matter content, which in turn depends on the climate, vegetation, topography, age and soil management. Soil nitrogen typically decreases by 0.2 to 0.3% for every temperature increase by 10 °C. Usually, grassland soils contain more soil nitrogen than forest soils. Cultivation decreases soil nitrogen by exposing soil organic matter to decomposition by microorganisms, and soils under no-tillage maintain more soil nitrogen than tilled soils.\n\nSome micro-organisms are able to metabolise organic matter and release ammonium in a process called \"mineralisation\". Others take free ammonium and oxidise it to nitrate. Nitrogen-fixing bacteria are capable of metabolising N into the form of ammonia in a process called nitrogen fixation. Both ammonium and nitrate can be \"immobilized\" by their incorporation into the microbes' living cells, where it is temporarily sequestered in the form of amino acids and protein. Nitrate may also be lost from the soil when bacteria metabolise it to the gases N and NO. The loss of gaseous forms of nitrogen to the atmosphere due to microbial action is called \"denitrification\". Nitrogen may also be \"leached\" from the soil if it is in the form of nitrate or lost to the atmosphere as ammonia due to a chemical reaction of ammonium with alkaline soil by way of a process called \"volatilisation\". Ammonium may also be sequestered in clay by \"fixation\". A small amount of nitrogen is added to soil by rainfall.\n\nIn the process of mineralisation, microbes feed on organic matter, releasing ammonia (NH), ammonium (NH) and other nutrients. As long as the carbon to nitrogen ratio (C/N) of fresh residues in the soil is above 30:1, nitrogen will be in short supply and other bacteria will feed on the ammonium and incorporate its nitrogen into their cells in the immobilization process. In that form the nitrogen is said to be \"immobilised\". Later, when such bacteria die, they too are \"mineralised\" and some of the nitrogen is released as ammonium and nitrate. If the C/N is less than 15, ammonia is freed to the soil, where it may be used by bacteria which oxidise it to nitrate (nitrification). Bacteria may on average add nitrogen per acre, and in an unfertilised field, this is the most important source of usable nitrogen. In a soil with 5% organic matter perhaps 2 to 5% of that is released to the soil by such decomposition. It occurs fastest in warm, moist, well aerated soil. The mineralisation of 3% of the organic material of a soil that is 4% organic matter overall, would release of nitrogen as ammonium per acre.\n\nIn nitrogen fixation, rhizobium bacteria convert N to ammonia (NH). Rhizobia share a symbiotic relationship with host plants, since rhizobia supply the host with nitrogen and the host provides rhizobia with nutrients and a safe environment. It is estimated that such symbiotic bacteria in the root nodules of legumes add 45 to 250 pounds of nitrogen per acre per year, which may be sufficient for the crop. Other, free-living nitrogen-fixing bacteria and blue-green algae live independently in the soil and release nitrate when their dead bodies are converted by way of mineralisation.\n\nSome amount of usable nitrogen is fixed by lightning as nitric oxide (NO) and nitrogen dioxide (NO). Nitrogen dioxide is soluble in water to form nitric acid (HNO) solution of H and NO. Ammonia, NH, previously released from the soil or from combustion, may fall with precipitation as nitric acid at a rate of about five pounds nitrogen per acre per year.\n\nWhen bacteria feed on soluble forms of nitrogen (ammonium and nitrate), they temporarily sequester that nitrogen in their bodies in a process called \"immobilisation\". At a later time when those bacteria die, their nitrogen may be released as ammonium by the processes of mineralisation.\n\nProtein material is easily broken down, but the rate of its decomposition is slowed by its attachment to the crystalline structure of clay and when trapped between the clay layers. The layers are small enough that bacteria cannot enter. Some organisms can exude extracellular enzymes that can act on the sequestered proteins. However, those enzymes too may be trapped on the clay crystals.\n\nAmmonium fixation occurs when ammonium pushes potassium ions from between the layers of clay such as illite or montmorillonite. Only a small fraction of soil nitrogen is held this way.\n\nUsable nitrogen may be lost from soils when it is in the form of nitrate, as it is easily leached. Further losses of nitrogen occur by denitrification, the process whereby soil bacteria convert nitrate (NO) to nitrogen gas, N or NO. This occurs when poor soil aeration limits free oxygen, forcing bacteria to use the oxygen in nitrate for their respiratory process. Denitrification increases when oxidisable organic material is available and when soils are warm and slightly acidic. Denitrification may vary throughout a soil as the aeration varies from place to place. Denitrification may cause the loss of 10 to 20 percent of the available nitrates within a day and when conditions are favourable to that process, losses of up to 60 percent of nitrate applied as fertiliser may occur.\n\n\"Ammonium volatilisation\" occurs when ammonium reacts chemically with an alkaline soil, converting NH to NH. The application of ammonium fertiliser to such a field can result in volatilisation losses of as much as 30 percent.\n\nAfter nitrogen, phosphorus is probably the element most likely to be deficient in soils. The soil mineral apatite is the most common mineral source of phosphorus. While there is on average 1000 lb of phosphorus per acre in the soil, it is generally in the form of phosphates with low solubility. Total phosphorus is about 0.1 percent by weight of the soil, but only one percent of that is available. Of the part available, more than half comes from the mineralisation of organic matter. Agricultural fields may need to be fertilised to make up for the phosphorus that has been removed in the crop.\n\nWhen phosphorus does form solubilised ions of HPO, they rapidly form insoluble phosphates of calcium or hydrous oxides of iron and aluminum. Phosphorus is largely immobile in the soil and is not leached but actually builds up in the surface layer if not cropped. The application of soluble fertilisers to soils may result in zinc deficiencies as zinc phosphates form. Conversely, the application of zinc to soils may immobilise phosphorus again as zinc phosphate. Lack of phosphorus may interfere with the normal opening of the plant leaf stomata, resulting in plant temperatures 10 percent higher than normal. Phosphorus is most available when soil pH is 6.5 in mineral soils and 5.5 in organic soils.\n\nThe amount of potassium in a soil may be as much as 80,000 lb per acre-foot, of which only 150 lb is available for plant growth. Common mineral sources of potassium are the mica biotite and potassium feldspar, KAlSiO. When solubilised, half will be held as exchangeable cations on clay while the other half is in the soil water solution. Potassium fixation often occurs when soils dry and the potassium is bonded between layers of illite clay. Under certain conditions, dependent on the soil texture, intensity of drying, and initial amount of exchangeable potassium, the fixed percentage may be as much as 90 percent within ten minutes. Potassium may be leached from soils low in clay.\n\nCalcium is one percent by weight of soils and is generally available but may be low as it is soluble and can be leached. It is thus low in sandy and heavily leached soil or strongly acidic mineral soil. Calcium is supplied to the plant in the form of exchangeable ions and moderately soluble minerals. Calcium is more available on the soil colloids than is potassium because the common mineral calcite, CaCO, is more soluble than potassium-bearing minerals.\n\nMagnesium is one of the dominant exchangeable cations in most soils (as are calcium and potassium). Primary minerals that weather to release magnesium include hornblende, biotite and vermiculite. Soil magnesium concentrations are generally sufficient for optimal plant growth, but highly weathered and sandy soils may be magnesium deficient due to leaching by heavy precipitation.\n\nMost sulfur is made available to plants, like phosphorus, by its release from decomposing organic matter. Deficiencies may exist in some soils (especially sandy soils) and if cropped, sulfur needs to be added. The application of large quantities of nitrogen to fields that have marginal amounts of sulfur may cause sulfur deficiency in the rapidly growing plants by the plant's growth outpacing the supply of sulfur. A 15-ton crop of onions uses up to 19 lb of sulfur and 4 tons of alfalfa uses 15 lb per acre. Sulfur abundance varies with depth. In a sample of soils in Ohio, United States, the sulfur abundance varied with depths, 0-6 inches, 6-12 inches, 12-18 inches, 18-24 inches in the amounts: 1056, 830, 686, 528 lb per acre respectively.\n\nThe micronutrients essential in plant life, in their order of importance, include iron, manganese, zinc, copper, boron, chlorine and molybdenum. The term refers to plants' needs, not to their abundance in soil. They are required in very small amounts but are essential to plant health in that most are required parts of some enzyme system which speeds up plants' metabolisms. They are generally available in the mineral component of the soil, but the heavy application of phosphates can cause a deficiency in zinc and iron by the formation of insoluble zinc and iron phosphates. Iron deficiency may also result from excessive amounts of heavy metals or calcium minerals (lime) in the soil. Excess amounts of soluble boron, molybdenum and chloride are toxic.\n\nNutrients which enhance the health but whose deficiency does not stop the life cycle of plants include: cobalt, strontium, vanadium, silicon and nickel. As their importance are evaluated they may be added to the list of essential plant nutrients.\n\nSoil organic matter is made up of organic compounds and includes plant, animal and microbial material, both living and dead. A typical soil has a biomass composition of 70% microorganisms, 22% macrofauna, and 8% roots. The living component of an acre of soil may include 900 lb of earthworms, 2400 lb of fungi, 1500 lb of bacteria, 133 lb of protozoa and 890 lb of arthropods and algae.\n\nA small part of the organic matter consists of the living cells such as bacteria, molds, and actinomycetes that work to break down the dead organic matter. Were it not for the action of these micro-organisms, the entire carbon dioxide part of the atmosphere would be sequestered as organic matter in the soil.\n\nChemically, organic matter is classed as follows:\n\n\nMost living things in soils, including plants, insects, bacteria, and fungi, are dependent on organic matter for nutrients and/or energy. Soils have organic compounds in varying degrees of decomposition which rate is dependent on the temperature, soil moisture, and aeration. Bacteria and fungi feed on the raw organic matter, which are fed upon by amoebas, which in turn are fed upon by nematodes and arthropods. Organic matter holds soils open, allowing the infiltration of air and water, and may hold as much as twice its weight in water. Many soils, including desert and rocky-gravel soils, have little or no organic matter. Soils that are all organic matter, such as peat (histosols), are infertile. In its earliest stage of decomposition, the original organic material is often called raw organic matter. The final stage of decomposition is called humus.\n\nIn grassland, much of the organic matter added to the soil is from the deep, fibrous, grass root systems. By contrast, tree leaves falling on the forest floor are the principal source of soil organic matter in the forest. Another difference is the frequent occurrence in the grasslands of fires that destroy large amounts of aboveground material but stimulate even greater contributions from roots. Also, the much greater acidity under any forests inhibits the action of certain soil organisms that otherwise would mix much of the surface litter into the mineral soil. As a result, the soils under grasslands generally develop a thicker A horizon with a deeper distribution of organic matter than in comparable soils under forests, which characteristically store most of their organic matter in the forest floor (O horizon) and thin A horizon.\n\nHumus refers to organic matter that has been decomposed by soil flora and fauna to the point where it is resistant to further breakdown. Humus usually constitutes only five percent of the soil or less by volume, but it is an essential source of nutrients and adds important textural qualities crucial to soil health and plant growth. Humus also hold bits of undecomposed organic matter which feed arthropods and worms which further improve the soil. The end product, humus, is soluble in water and forms a weak acid that can attack silicate minerals. Humus is a colloid with a high cation and anion exchange capacity that on a dry weight basis is many times greater than that of clay colloids. It also acts as a buffer, like clay, against changes in pH and soil moisture.\n\nHumic acids and fulvic acids, which begin as raw organic matter, are important constituents of humus. After the death of plants and animals, microbes begin to feed on the residues, resulting finally in the formation of humus. With decomposition, there is a reduction of water-soluble constituents, cellulose and hemicellulose, and nutrients such as nitrogen, phosphorus, and sulfur. As the residues break down, only stable molecules made of aromatic carbon rings, oxygen and hydrogen remain in the form of humin, lignin and lignin complexes collectively called humus. While the structure of humus has few nutrients, it is able to attract and hold cation and anion nutrients by weak bonds that can be released into the soil solution in response to changes in soil pH.\n\nLignin is resistant to breakdown and accumulates within the soil. It also reacts with amino acids, which further increases its resistance to decomposition, including enzymatic decomposition by microbes. Fats and waxes from plant matter have some resistance to decomposition and persist in soils for a while. Clay soils often have higher organic contents that persist longer than soils without clay as the organic molecules adhere to and are stabilised by the clay. Proteins normally decompose readily, but when bound to clay particles, they become more resistant to decomposition. Clay particles also absorb the enzymes exuded by microbes which would normally break down proteins. The addition of organic matter to clay soils can render that organic matter and any added nutrients inaccessible to plants and microbes for many years. High soil tannin (polyphenol) content can cause nitrogen to be sequestered in proteins or cause nitrogen immobilisation.\n\nHumus formation is a process dependent on the amount of plant material added each year and the type of base soil. Both are affected by climate and the type of organisms present. Soils with humus can vary in nitrogen content but typically have 3 to 6 percent nitrogen. Raw organic matter, as a reserve of nitrogen and phosphorus, is a vital component affecting soil fertility. Humus also absorbs water, and expands and shrinks between dry and wet states, increasing soil porosity. Humus is less stable than the soil's mineral constituents, as it is reduced by microbial decomposition, and over time its concentration diminshes without the addition of new organic matter. However, humus may persist over centuries if not millennia.\n\nThe production, accumulation and degradation of organic matter are greatly dependent on climate. Temperature, soil moisture and topography are the major factors affecting the accumulation of organic matter in soils. Organic matter tends to accumulate under wet or cold conditions where decomposer activity is impeded by low temperature or excess moisture which results in anaerobic conditions. Conversely, excessive rain and high temperatures of tropical climates enables rapid decomposition of organic matter and leaching of plant nutrients; forest ecosystems on these soils rely on efficient recycling of nutrients and plant matter to maintain their productivity. Excessive slope may encourage the erosion of the top layer of soil which holds most of the raw organic material that would otherwise eventually become humus.\n\nCellulose and hemicellulose undergo fast decomposition by fungi and bacteria, with a half-life of 12–18 days in a temperate climate. Brown rot fungi can decompose the cellulose and hemicellulose, leaving the lignin and phenolic compounds behind. Starch, which is an energy storage system for plants, undergoes fast decomposition by bacteria and fungi. Lignin consists of polymers composed of 500 to 600 units with a highly branched, amorphous structure. Lignin undergoes very slow decomposition, mainly by white rot fungi and actinomycetes; its half-life under temperate conditions is about six months.\n\nA horizontal layer of the soil, whose physical features, composition and age are distinct from those above and beneath, is referred to as a soil horizon. The naming of a horizon is based on the type of material of which it is composed. Those materials reflect the duration of specific processes of soil formation. They are labelled using a shorthand notation of letters and numbers which describe the horizon in terms of its colour, size, texture, structure, consistency, root quantity, pH, voids, boundary characteristics and presence of nodules or concretions. No soil profile has all the major horizons. Some may have only one horizon.\n\nThe exposure of parent material to favourable conditions produces mineral soils that are marginally suitable for plant growth. That growth often results in the accumulation of organic residues. The accumulated organic layer called the O horizon produces a more active soil due to the effect of the organisms that live within it. Organisms colonise and break down organic materials, making available nutrients upon which other plants and animals can live. After sufficient time, humus moves downward and is deposited in a distinctive organic surface layer called the A horizon.\n\nSoil is classified into categories in order to understand relationships between different soils and to determine the suitability of a soil for a particular use. One of the first classification systems was developed by Russian scientist Dokuchaev around 1880. It was modified a number of times by American and European researchers, and developed into the system commonly used until the 1960s. It was based on the idea that soils have a particular morphology based on the materials and factors that form them. In the 1960s, a different classification system began to emerge which focused on soil morphology instead of parental materials and soil-forming factors. Since then it has undergone further modifications. The World Reference Base for Soil Resources (WRB) aims to establish an international reference base for soil classification.\n\nThere are fourteen soil orders at the top level of the Australian Soil Classification. They are: Anthroposols, Organosols, Podosols, Vertosols, Hydrosols, Kurosols, Sodosols, Chromosols, Calcarosols, Ferrosols, Dermosols, Kandosols, Rudosols and Tenosols.\n\nThe EU's soil taxonomy is based on a new standard soil classification in the World Reference Base for Soil Resources produced by the UN's Food and Agriculture Organization. According to this, the major soils in the European Union are:\n\nA taxonomy is an arrangement in a systematic manner; the USDA soil taxonomy has six levels of classification. They are, from most general to specific: order, suborder, great group, subgroup, family and series. Soil properties that can be measured quantitatively are used in this classification system – they include: depth, moisture, temperature, texture, structure, cation exchange capacity, base saturation, clay mineralogy, organic matter content and salt content. There are 12 soil orders (the top hierarchical level) in soil taxonomy. The names of the orders end with the suffix \"-sol\". The criteria for the different soil orders include properties that reflect major differences in the genesis of soils. The orders are:\n\nThe percentages listed above are for land area free of ice. \"Soils of Mountains\", which constitute the balance (11.6%), have a mixture of those listed above, or are classified as \"Rugged Mountains\" which have no soil.\n\nThe above soil orders in sequence of increasing degree of development are Entisols, Inceptisols, Aridisols, Mollisols, Alfisols, Spodosols, Ultisols, and Oxisols. Histosols and Vertisols may appear in any of the above at any time during their development.\n\nThe soil suborders within an order are differentiated on the basis of soil properties and horizons which depend on soil moisture and temperature. Forty-seven suborders are recognized in the United States.\n\nThe soil great group category is a subdivision of a suborder in which the kind and sequence of soil horizons distinguish one soil from another. About 185 great groups are recognized in the United States. Horizons marked by clay, iron, humus and hard pans and soil features such as the expansion-contraction of clays (that produce self-mixing provided by clay), temperature, and marked quantities of various salts are used as distinguishing features.\n\nThe great group categories are divided into three kinds of soil subgroups: typic, intergrade and extragrade. A typic subgroup represents the basic or 'typical' concept of the great group to which the described subgroup belongs. An intergrade subgroup describes the properties that suggest how it grades towards (is similar to) soils of other soil great groups, suborders or orders. These properties are not developed or expressed well enough to cause the soil to be included within the great group towards which they grade, but suggest similarities. Extragrade features are aberrant properties which prevent that soil from being included in another soil classification. About 1,000 soil subgroups are defined in the United States.\n\nA soil family category is a group of soils within a subgroup and describes the physical and chemical properties which affect the response of soil to agricultural management and engineering applications. The principal characteristics used to differentiate soil families include texture, mineralogy, pH, permeability, structure, consistency, the locale's precipitation pattern, and soil temperature. For some soils the criteria also specify the percentage of silt, sand and coarse fragments such as gravel, cobbles and rocks. About 4,500 soil families are recognised in the United States.\n\nA family may contain several soil series which describe the physical location using the name of a prominent physical feature such as a river or town near where the soil sample was taken. An example would be Merrimac for the Merrimack River in New Hampshire. More than 14,000 soil series are recognised in the United States. This permits very specific descriptions of soils.\n\nA soil phase of series, originally called 'soil type' describes the soil surface texture, slope, stoniness, saltiness, erosion, and other conditions.\n\nSoil is used in agriculture, where it serves as the anchor and primary nutrient base for plants; however, as demonstrated by hydroponics, it is not essential to plant growth if the soil-contained nutrients can be dissolved in a solution. The types of soil and available moisture determine the species of plants that can be cultivated.\n\nSoil material is also a critical component in the mining, construction and landscape development industries. Soil serves as a foundation for most construction projects. The movement of massive volumes of soil can be involved in surface mining, road building and dam construction. Earth sheltering is the architectural practice of using soil for external thermal mass against building walls. Many building materials are soil based.\n\nSoil resources are critical to the environment, as well as to food and fibre production. Soil provides minerals and water to plants. Soil absorbs rainwater and releases it later, thus preventing floods and drought. Soil cleans water as it percolates through it. Soil is the habitat for many organisms: the major part of known and unknown biodiversity is in the soil, in the form of invertebrates (earthworms, woodlice, millipedes, centipedes, snails, slugs, mites, springtails, enchytraeids, nematodes, protists), bacteria, archaea, fungi and algae; and most organisms living above ground have part of them (plants) or spend part of their life cycle (insects) below-ground. Above-ground and below-ground biodiversities are tightly interconnected, making soil protection of paramount importance for any restoration or conservation plan.\n\nThe biological component of soil is an extremely important carbon sink since about 57% of the biotic content is carbon. Even on desert crusts, cyanobacteria, lichens and mosses capture and sequester a significant amount of carbon by photosynthesis. Poor farming and grazing methods have degraded soils and released much of this sequestered carbon to the atmosphere. Restoring the world's soils could offset the effect of increases in greenhouse gas emissions and slow global warming, while improving crop yields and reducing water needs.\n\nWaste management often has a soil component. Septic drain fields treat septic tank effluent using aerobic soil processes. Landfills use soil for daily cover. Land application of waste water relies on soil biology to aerobically treat BOD.\n\nOrganic soils, especially peat, serve as a significant fuel resource; but wide areas of peat production, such as sphagnum bogs, are now protected because of patrimonial interest.\n\nGeophagy is the practice of eating soil-like substances. Both animals and human cultures occasionally consume soil for medicinal, recreational, or religious purposes. It has been shown that some monkeys consume soil, together with their preferred food (tree foliage and fruits), in order to alleviate tannin toxicity.\n\nSoils filter and purify water and affect its chemistry. Rain water and pooled water from ponds, lakes and rivers percolate through the soil horizons and the upper rock strata, thus becoming groundwater. Pests (viruses) and pollutants, such as persistent organic pollutants (chlorinated pesticides, polychlorinated biphenyls), oils (hydrocarbons), heavy metals (lead, zinc, cadmium), and excess nutrients (nitrates, sulfates, phosphates) are filtered out by the soil. Soil organisms metabolise them or immobilise them in their biomass and necromass, thereby incorporating them into stable humus. The physical integrity of soil is also a prerequisite for avoiding landslides in rugged landscapes.\n\nLand degradation refers to a human-induced or natural process which impairs the capacity of land to function. Soils degradation involves the acidification, contamination, desertification, erosion or salination.\n\nSoil acidification is beneficial in the case of alkaline soils, but it degrades land when it lowers crop productivity and increases soil vulnerability to contamination and erosion. Soils are often initially acid because their parent materials were acid and initially low in the basic cations (calcium, magnesium, potassium and sodium). Acidification occurs when these elements are leached from the soil profile by rainfall or by the harvesting of forest or agricultural crops. Soil acidification is accelerated by the use of acid-forming nitrogenous fertilizers and by the effects of acid precipitation.\n\nSoil contamination at low levels is often within a soil's capacity to treat and assimilate waste material. Soil biota can treat waste by transforming it; soil colloids can adsorb the waste material. Many waste treatment processes rely on this treatment capacity. Exceeding treatment capacity can damage soil biota and limit soil function. Derelict soils occur where industrial contamination or other development activity damages the soil to such a degree that the land cannot be used safely or productively. Remediation of derelict soil uses principles of geology, physics, chemistry and biology to degrade, attenuate, isolate or remove soil contaminants to restore soil functions and values. Techniques include leaching, air sparging, chemical amendments, phytoremediation, bioremediation and natural degradation.\n\nDesertification is an environmental process of ecosystem degradation in arid and semi-arid regions, often caused by human activity. It is a common misconception that droughts cause desertification. Droughts are common in arid and semiarid lands. Well-managed lands can recover from drought when the rains return. Soil management tools include maintaining soil nutrient and organic matter levels, reduced tillage and increased cover. These practices help to control erosion and maintain productivity during periods when moisture is available. Continued land abuse during droughts, however, increases land degradation. Increased population and livestock pressure on marginal lands accelerates desertification.\n\nErosion of soil is caused by water, wind, ice, and movement in response to gravity. More than one kind of erosion can occur simultaneously. Erosion is distinguished from weathering, since erosion also transports eroded soil away from its place of origin (soil in transit may be described as sediment). Erosion is an intrinsic natural process, but in many places it is greatly increased by human activity, especially poor land use practices. These include agricultural activities which leave the soil bare during times of heavy rain or strong winds, overgrazing, deforestation, and improper construction activity. Improved management can limit erosion. Soil conservation techniques which are employed include changes of land use (such as replacing erosion-prone crops with grass or other soil-binding plants), changes to the timing or type of agricultural operations, terrace building, use of erosion-suppressing cover materials (including cover crops and other plants), limiting disturbance during construction, and avoiding construction during erosion-prone periods.\n\nA serious and long-running water erosion problem occurs in China, on the middle reaches of the Yellow River and the upper reaches of the Yangtze River. From the Yellow River, over 1.6 billion tons of sediment flow each year into the ocean. The sediment originates primarily from water erosion (gully erosion) in the Loess Plateau region of northwest China.\n\nSoil piping is a particular form of soil erosion that occurs below the soil surface. It causes levee and dam failure, as well as sink hole formation. Turbulent flow removes soil starting at the mouth of the seep flow and the subsoil erosion advances up-gradient. The term sand boil is used to describe the appearance of the discharging end of an active soil pipe.\n\nSoil salination is the accumulation of free salts to such an extent that it leads to degradation of the agricultural value of soils and vegetation. Consequences include corrosion damage, reduced plant growth, erosion due to loss of plant cover and soil structure, and water quality problems due to sedimentation. Salination occurs due to a combination of natural and human-caused processes. Arid conditions favour salt accumulation. This is especially apparent when soil parent material is saline. Irrigation of arid lands is especially problematic. All irrigation water has some level of salinity. Irrigation, especially when it involves leakage from canals and overirrigation in the field, often raises the underlying water table. Rapid salination occurs when the land surface is within the capillary fringe of saline groundwater. Soil salinity control involves watertable control and flushing with higher levels of applied water in combination with tile drainage or another form of subsurface drainage.\n\nSoils which contain high levels of particular clays, such as smectites, are often very fertile. For example, the smectite-rich clays of Thailand's Central Plains are among the most productive in the world.\n\nMany farmers in tropical areas, however, struggle to retain organic matter in the soils they work. In recent years, for example, productivity has declined in the low-clay soils of northern Thailand. Farmers initially responded by adding organic matter from termite mounds, but this was unsustainable in the long-term. Scientists experimented with adding bentonite, one of the smectite family of clays, to the soil. In field trials, conducted by scientists from the International Water Management Institute in cooperation with Khon Kaen University and local farmers, this had the effect of helping retain water and nutrients. Supplementing the farmer's usual practice with a single application of 200 kg bentonite per rai (6.26 rai = 1 hectare) resulted in an average yield increase of 73%. More work showed that applying bentonite to degraded sandy soils reduced the risk of crop failure during drought years.\n\nIn 2008, three years after the initial trials, IWMI scientists conducted a survey among 250 farmers in northeast Thailand, half of whom had applied bentonite to their fields. The average improvement for those using the clay addition was 18% higher than for non-clay users. Using the clay had enabled some farmers to switch to growing vegetables, which need more fertile soil. This helped to increase their income. The researchers estimated that 200 farmers in northeast Thailand and 400 in Cambodia had adopted the use of clays, and that a further 20,000 farmers were introduced to the new technique.\n\nIf the soil is too high in clay, adding gypsum, washed river sand and organic matter will balance the composition. Adding organic matter (like ramial chipped wood for instance) to soil which is depleted in nutrients and too high in sand will boost its quality.\n\n"}
{"id": "13690575", "url": "https://en.wikipedia.org/wiki?curid=13690575", "title": "Solar power", "text": "Solar power\n\nSolar power is the conversion of energy from sunlight into electricity, either directly using photovoltaics (PV), indirectly using concentrated solar power, or a combination. Concentrated solar power systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Photovoltaic cells convert light into an electric current using the photovoltaic effect.\n\nPhotovoltaics were initially solely used as a source of electricity for small and medium-sized applications, from the calculator powered by a single solar cell to remote homes powered by an off-grid rooftop PV system. Commercial concentrated solar power plants were first developed in the 1980s. The 392 MW Ivanpah installation is the largest concentrating solar power plant in the world, located in the Mojave Desert of California.\n\nAs the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and utility-scale solar power stations with hundreds of megawatts are being built. Solar PV is rapidly becoming an inexpensive, low-carbon technology to harness renewable energy from the Sun. The current largest photovoltaic power station in the world is the 850 MW Longyangxia Dam Solar Park, in Qinghai, China.\n\nThe International Energy Agency projected in 2014 that under its \"high renewables\" scenario, by 2050, solar photovoltaics and concentrated solar power would contribute about 16 and 11 percent, respectively, of the worldwide electricity consumption, and solar would be the world's largest source of electricity. Most solar installations would be in China and India. In 2017, solar power provided 1.7% of total worldwide electricity production, growing at 35% per annum.\n\nMany industrialized nations have installed significant solar power capacity into their grids to supplement or provide an alternative to conventional energy sources while an increasing number of less developed nations have turned to solar to reduce dependence on expensive imported fuels \"(see solar power by country)\". Long distance transmission allows remote renewable energy resources to displace fossil fuel consumption. Solar power plants use one of two technologies:\n\nA solar cell, or photovoltaic cell (PV), is a device that converts light into electric current using the photovoltaic effect. The first solar cell was constructed by Charles Fritts in the 1880s. The German industrialist Ernst Werner von Siemens was among those who recognized the importance of this discovery. In 1931, the German engineer Bruno Lange developed a photo cell using silver selenide in place of copper oxide, although the prototype selenium cells converted less than 1% of incident light into electricity. Following the work of Russell Ohl in the 1940s, researchers Gerald Pearson, Calvin Fuller and Daryl Chapin created the silicon solar cell in 1954. These early solar cells cost 286 USD/watt and reached efficiencies of 4.5–6%.\n\nThe array of a photovoltaic power system, or PV system, produces direct current (DC) power which fluctuates with the sunlight's intensity. \nFor practical use this usually requires conversion to certain desired voltages or alternating current (AC), through the use of inverters. \nMultiple solar cells are connected inside modules. \nModules are wired together to form arrays, then tied to an inverter, which produces power at the desired voltage, and for AC, the desired frequency/phase.\n\nMany residential PV systems are connected to the grid wherever available, especially in developed countries with large markets. \nIn these grid-connected PV systems, use of energy storage is optional. \nIn certain applications such as satellites, lighthouses, or in developing countries, batteries or additional power generators are often added as back-ups. Such stand-alone power systems permit operations at night and at other times of limited sunlight.\n\nConcentrated solar power (CSP), also called \"concentrated solar thermal\", uses lenses or mirrors and tracking systems to concentrate sunlight, then use the resulting heat to generate electricity from conventional steam-driven turbines.\n\nA wide range of concentrating technologies exists: among the best known are the parabolic trough, the compact linear Fresnel reflector, the Stirling dish and the solar power tower. Various techniques are used to track the sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage. Thermal storage efficiently allows up to 24-hour electricity generation.\n\nA \"parabolic trough\" consists of a linear parabolic reflector that concentrates light onto a receiver positioned along the reflector's focal line. The receiver is a tube positioned along the focal points of the linear parabolic mirror and is filled with a working fluid. The reflector is made to follow the sun during daylight hours by tracking along a single axis. Parabolic trough systems provide the best land-use factor of any solar technology. The SEGS plants in California and Acciona's Nevada Solar One near Boulder City, Nevada are representatives of this technology.\n\n\"Compact Linear Fresnel Reflectors\" are CSP-plants which use many thin mirror strips instead of parabolic mirrors to concentrate sunlight onto two tubes with working fluid. This has the advantage that flat mirrors can be used which are much cheaper than parabolic mirrors, and that more reflectors can be placed in the same amount of space, allowing more of the available sunlight to be used. Concentrating linear fresnel reflectors can be used in either large or more compact plants.\n\nThe \"Stirling solar dish\" combines a parabolic concentrating dish with a Stirling engine which normally drives an electric generator. The advantages of Stirling solar over photovoltaic cells are higher efficiency of converting sunlight into electricity and longer lifetime.\nParabolic dish systems give the highest efficiency among CSP technologies. \nThe 50 kW Big Dish in Canberra, Australia is an example of this technology.\n\nA \"solar power tower\" uses an array of tracking reflectors (heliostats) to concentrate light on a central receiver atop a tower. Power towers can achieve higher (thermal-to-electricity conversion) efficiency than linear tracking CSP schemes and better energy storage capability than dish stirling technologies. The PS10 Solar Power Plant and PS20 solar power plant are examples of this technology.\n\nA hybrid system combines (C)PV and CSP with one another or with other forms of generation such as diesel, wind and biogas. The combined form of generation may enable the system to modulate power output as a function of demand or at least reduce the fluctuating nature of solar power and the consumption of non renewable fuel. Hybrid systems are most often found on islands.\n\nThe idea is to increase the efficiency of the combined solar/thermoelectric system to convert the solar radiation into useful electricity.\n\n</div>\n\nThe early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce. Charles Fritts installed the world's first rooftop photovoltaic solar array, using 1%-efficient selenium cells, on a New York City roof in 1884. However, development of solar technologies stagnated in the early 20th century in the face of the increasing availability, economy, and utility of coal and petroleum. In 1974 it was estimated that only six private homes in all of North America were entirely heated or cooled by functional solar power systems. The 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world and brought renewed attention to developing solar technologies. Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the US and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the United States (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer–ISE). \nBetween 1970 and 1983 installations of photovoltaic systems grew rapidly, but falling oil prices in the early 1980s moderated the growth of photovoltaics from 1984 to 1996.\n\nIn the mid-1990s, development of both, residential and commercial rooftop solar as well as utility-scale photovoltaic power stations, began to accelerate again due to supply issues with oil and natural gas, global warming concerns, and the improving economic position of PV relative to other energy technologies. In the early 2000s, the adoption of feed-in tariffs—a policy mechanism, that gives renewables priority on the grid and defines a fixed price for the generated electricity—led to a high level of investment security and to a soaring number of PV deployments in Europe.\n\nFor several years, worldwide growth of solar PV was driven by European deployment, but has since shifted to Asia, especially China and Japan, and to a growing number of countries and regions all over the world, including, but not limited to, Australia, Canada, Chile, India, Israel, Mexico, South Africa, South Korea, Thailand, and the United States.\n\nWorldwide growth of photovoltaics has averaged 40% per year from 2000 to 2013 and total installed capacity reached 303 GW at the end of 2016 with China having the most cumulative installations (78 GW) and Honduras having the highest theoretical percentage of annual electricity usage which could be generated by solar PV (12.5%). The largest manufacturers are located in China.\n\nConcentrated solar power (CSP) also started to grow rapidly, increasing its capacity nearly tenfold from 2004 to 2013, albeit from a lower level and involving fewer countries than solar PV. \nAs of the end of 2013, worldwide cumulative CSP-capacity reached 3,425 MW.\n\nIn 2010, the International Energy Agency predicted that global solar PV capacity could reach 3,000 GW or 11% of projected global electricity generation by 2050—enough to generate 4,500 TWh of electricity. \nFour years later, in 2014, the agency projected that, under its \"high renewables\" scenario, solar power could supply 27% of global electricity generation by 2050 (16% from PV and 11% from CSP).\n\nThe Desert Sunlight Solar Farm is a 550 MW power plant in Riverside County, California, that uses thin-film CdTe-modules made by First Solar. As of November 2014, the 550 megawatt Topaz Solar Farm was the largest photovoltaic power plant in the world. This was surpassed by the 579 MW Solar Star complex. The current largest photovoltaic power station in the world is Longyangxia Dam Solar Park, in Gonghe County, Qinghai, China.\n\nCommercial concentrating solar power (CSP) plants, also called \"solar thermal power stations\", were first developed in the 1980s. The 377 MW Ivanpah Solar Power Facility, located in California's Mojave Desert, is the world’s largest solar thermal power plant project. Other large CSP plants include the Solnova Solar Power Station (150 MW), the Andasol solar power station (150 MW), and Extresol Solar Power Station (150 MW), all in Spain. The principal advantage of CSP is the ability to efficiently add thermal storage, allowing the dispatching of electricity over up to a 24-hour period. Since peak electricity demand typically occurs at about 5 pm, many CSP power plants use 3 to 5 hours of thermal storage.\n\nThe typical cost factors for solar power include the costs of the modules, the frame to hold them, wiring, inverters, labour cost, any land that might be required, the grid connection, maintenance and the solar insolation that location will receive. Adjusting for inflation, it cost $96 per watt for a solar module in the mid-1970s. Process improvements and a very large boost in production have brought that figure down to 68 cents per watt in February 2016, according to data from Bloomberg New Energy Finance. Palo Alto California signed a wholesale purchase agreement in 2016 that secured solar power for 3.7 cents per kilowatt-hour. And in sunny Dubai large-scale solar generated electricity sold in 2016 for just 2.99 cents per kilowatt-hour – \"competitive with any form of fossil-based electricity — and cheaper than most.\"\n\nPhotovoltaic systems use no fuel, and modules typically last 25 to 40 years. Thus, capital costs make up most of the cost of solar power. Operations and maintenance costs for new utility-scale solar plants in the US are estimated to be 9 percent of the cost of photovoltaic electricity, and 17 percent of the cost of solar thermal electricity. Governments have created various financial incentives to encourage the use of solar power, such as feed-in tariff programs. Also, Renewable portfolio standards impose a government mandate that utilities generate or acquire a certain percentage of renewable power regardless of increased energy procurement costs. In most states, RPS goals can be achieved by any combination of solar, wind, biomass, landfill gas, ocean, geothermal, municipal solid waste, hydroelectric, hydrogen, or fuel cell technologies.\n\nThe PV industry is beginning to adopt levelized cost of electricity (LCOE) as the unit of cost. The electrical energy generated is sold in units of kilowatt-hours (kWh). As a rule of thumb, and depending on the local insolation, 1 watt-peak of installed solar PV capacity generates about 1 to 2 kWh of electricity per year. This corresponds to a capacity factor of around 10–20%. The product of the local cost of electricity and the insolation determines the break even point for solar power. The International Conference on Solar Photovoltaic Investments, organized by EPIA, has estimated that PV systems will pay back their investors in 8 to 12 years. As a result, since 2006 it has been economical for investors to install photovoltaics for free in return for a long term power purchase agreement. Fifty percent of commercial systems in the United States were installed in this manner in 2007 and over 90% by 2009.\n\nShi Zhengrong has said that, as of 2012, unsubsidised solar power is already competitive with fossil fuels in India, Hawaii, Italy and Spain. He said \"We are at a tipping point. No longer are renewable power sources like solar and wind a luxury of the rich. They are now starting to compete in the real world without subsidies\". \"Solar power will be able to compete without subsidies against conventional power sources in half the world by 2015\".\n\nIn its 2014 edition of the \"Technology Roadmap: Solar Photovoltaic Energy\" report, the International Energy Agency (IEA) published prices for residential, commercial and utility-scale PV systems for eight major markets as of 2013 \"(see table below)\". However, DOE's SunShot Initiative has reported much lower U.S. installation prices. In 2014, prices continued to decline. The SunShot Initiative modeled U.S. system prices to be in the range of $1.80 to $3.29 per watt. Other sources identify similar price ranges of $1.70 to $3.50 for the different market segments in the U.S., and in the highly penetrated German market, prices for residential and small commercial rooftop systems of up to 100 kW declined to $1.36 per watt (€1.24/W) by the end of 2014. In 2015, Deutsche Bank estimated costs for small residential rooftop systems in the U.S. around $2.90 per watt. Costs for utility-scale systems in China and India were estimated as low as $1.00 per watt.\n\nGrid parity, the point at which the cost of photovoltaic electricity is equal to or cheaper than the price of grid power, is more easily achieved in areas with abundant sun and high costs for electricity such as in California and Japan. In 2008, The levelized cost of electricity for solar PV was $0.25/kWh or less in most of the OECD countries. By late 2011, the fully loaded cost was predicted to fall below $0.15/kWh for most of the OECD and to reach $0.10/kWh in sunnier regions. These cost levels are driving three emerging trends: vertical integration of the supply chain, origination of power purchase agreements (PPAs) by solar power companies, and unexpected risk for traditional power generation companies, grid operators and wind turbine manufacturers.\n\nGrid parity was first reached in Spain in 2013, Hawaii and other islands that otherwise use fossil fuel (diesel fuel) to produce electricity, and most of the US is expected to reach grid parity by 2015.\n\nIn 2007, General Electric's Chief Engineer predicted grid parity without subsidies in sunny parts of the United States by around 2015; other companies predicted an earlier date: the cost of solar power will be below grid parity for more than half of residential customers and 10% of commercial customers in the OECD, as long as grid electricity prices do not decrease through 2010.\n\nThe productivity of solar power in a region depends on solar irradiance, which varies through the day and is influenced by latitude and climate.\n\nThe locations with highest annual solar irradiance lie in the arid tropics and subtropics. Deserts lying in low latitudes usually have few clouds, and can receive sunshine for more than ten hours a day. These hot deserts form the \"Global Sun Belt\" circling the world. This belt consists of extensive swathes of land in Northern Africa, Southern Africa, Southwest Asia, Middle East, and Australia, as well as the much smaller deserts of North and South America. Africa's eastern Sahara Desert, also known as the Libyan Desert, has been observed to be the sunniest place on Earth according to NASA.\n\nDifferent measurements of solar irradiance (direct normal irradiance, global horizontal irradiance) are mapped below :\n\nIn cases of self consumption of the solar energy, the payback time is calculated based on how much electricity is not purchased from the grid. For example, in Germany, with electricity prices of 0.25 €/kWh and insolation of 900 kWh/kW, one kWp will save €225 per year, and with an installation cost of 1700 €/KWp the system cost will be returned in less than seven years. However, in many cases, the patterns of generation and consumption do not coincide, and some or all of the energy is fed back into the grid. The electricity is sold, and at other times when energy is taken from the grid, electricity is bought. The relative costs and prices obtained affect the economics. In many markets, the price paid for sold PV electricity is significantly lower than the price of bought electricity, which incentivizes self consumption. Moreover, separate self consumption incentives have been used in e.g. Germany and Italy. Grid interaction regulation has also included limitations of grid feed-in in some regions in Germany with high amounts of installed PV capacity. By increasing self consumption, the grid feed-in can be limited without curtailment, which wastes electricity.\n\nA good match between generation and consumption is key for high self consumption, and should be considered when deciding where to install solar power and how to dimension the installation. The match can be improved with batteries or controllable electricity consumption. However, batteries are expensive and profitability may require provision of other services from them besides self consumption increase. Hot water storage tanks with electric heating with heat pumps or resistance heaters can provide low-cost storage for self consumption of solar power. Shiftable loads, such as dishwashers, tumble dryers and washing machines, can provide controllable consumption with only a limited effect on the users, but their effect on self consumption of solar power may be limited.\n\nThe political purpose of incentive policies for PV is to facilitate an initial small-scale deployment to begin to grow the industry, even where the cost of PV is significantly above grid parity, to allow the industry to achieve the economies of scale necessary to reach grid parity. The policies are implemented to promote national energy independence, high tech job creation and reduction of CO emissions. Three incentive mechanisms are often used in combination as investment subsidies: the authorities refund part of the cost of installation of the system, the electricity utility buys PV electricity from the producer under a multiyear contract at a guaranteed rate, and Solar Renewable Energy Certificates (SRECs)\n\nWith investment subsidies, the financial burden falls upon the taxpayer, while with feed-in tariffs the extra cost is distributed across the utilities' customer bases. While the investment subsidy may be simpler to administer, the main argument in favour of feed-in tariffs is the encouragement of quality. Investment subsidies are paid out as a function of the nameplate capacity of the installed system and are independent of its actual power yield over time, thus rewarding the overstatement of power and tolerating poor durability and maintenance. Some electric companies offer rebates to their customers, such as Austin Energy in Texas, which offers $2.50/watt installed up to $15,000.\n\nIn net metering the price of the electricity produced is the same as the price supplied to the consumer, and the consumer is billed on the difference between production and consumption. Net metering can usually be done with no changes to standard electricity meters, which accurately measure power in both directions and automatically report the difference, and because it allows homeowners and businesses to generate electricity at a different time from consumption, effectively using the grid as a giant storage battery. With net metering, deficits are billed each month while surpluses are rolled over to the following month. Best practices call for perpetual roll over of kWh credits. Excess credits upon termination of service are either lost, or paid for at a rate ranging from wholesale to retail rate or above, as can be excess annual credits. In New Jersey, annual excess credits are paid at the wholesale rate, as are left over credits when a customer terminates service.\n\nWith feed-in tariffs, the financial burden falls upon the consumer. They reward the number of kilowatt-hours produced over a long period of time, but because the rate is set by the authorities, it may result in perceived overpayment. The price paid per kilowatt-hour under a feed-in tariff exceeds the price of grid electricity. Net metering refers to the case where the price paid by the utility is the same as the price charged.\n\nThe complexity of approvals in California, Spain and Italy has prevented comparable growth to Germany even though the return on investment is better. In some countries, additional incentives are offered for BIPV compared to stand alone PV.\n\nAlternatively, SRECs allow for a market mechanism to set the price of the solar generated electricity subsity. In this mechanism, a renewable energy production or consumption target is set, and the utility (more technically the Load Serving Entity) is obliged to purchase renewable energy or face a fine (Alternative Compliance Payment or ACP). The producer is credited for an SREC for every 1,000 kWh of electricity produced. If the utility buys this SREC and retires it, they avoid paying the ACP. In principle this system delivers the cheapest renewable energy, since the all solar facilities are eligible and can be installed in the most economic locations. Uncertainties about the future value of SRECs have led to long-term SREC contract markets to give clarity to their prices and allow solar developers to pre-sell and hedge their credits.\n\nFinancial incentives for photovoltaics differ across countries, including Australia, China, Germany, Israel, Japan, and the United States and even across states within the US.\n\nThe Japanese government through its Ministry of International Trade and Industry ran a successful programme of subsidies from 1994 to 2003. By the end of 2004, Japan led the world in installed PV capacity with over 1.1 GW.\n\nIn 2004, the German government introduced the first large-scale feed-in tariff system, under the German Renewable Energy Act, which resulted in explosive growth of PV installations in Germany. At the outset the FIT was over 3x the retail price or 8x the industrial price. The principle behind the German system is a 20-year flat rate contract. The value of new contracts is programmed to decrease each year, in order to encourage the industry to pass on lower costs to the end users. The programme has been more successful than expected with over 1GW installed in 2006, and political pressure is mounting to decrease the tariff to lessen the future burden on consumers.\n\nSubsequently, Spain, Italy, Greece—that enjoyed an early success with domestic solar-thermal installations for hot water needs—and France introduced feed-in tariffs. None have replicated the programmed decrease of FIT in new contracts though, making the German incentive relatively less and less attractive compared to other countries. The French and Greek FIT offer a high premium (EUR 0.55/kWh) for building integrated systems. California, Greece, France and Italy have 30–50% more insolation than Germany making them financially more attractive. The Greek domestic \"solar roof\" programme (adopted in June 2009 for installations up to 10 kW) has internal rates of return of 10–15% at current commercial installation costs, which, furthermore, is tax free.\n\nIn 2006 California approved the 'California Solar Initiative', offering a choice of investment subsidies or FIT for small and medium systems and a FIT for large systems. The small-system FIT of $0.39 per kWh (far less than EU countries) expires in just 5 years, and the alternate \"EPBB\" residential investment incentive is modest, averaging perhaps 20% of cost. All California incentives are scheduled to decrease in the future depending as a function of the amount of PV capacity installed.\n\nAt the end of 2006, the Ontario Power Authority (OPA, Canada) began its Standard Offer Program, a precursor to the Green Energy Act, and the first in North America for distributed renewable projects of less than 10 MW. The feed-in tariff guaranteed a fixed price of $0.42 CDN per kWh over a period of twenty years. Unlike net metering, all the electricity produced was sold to the OPA at the given rate.\n\nThe overwhelming majority of electricity produced worldwide is used immediately, since storage is usually more expensive and because traditional generators can adapt to demand. However both solar power and wind power are variable renewable energy, meaning that all available output must be taken whenever it is available by moving through transmission lines to \"where it can be used now\". Since solar energy is not available at night, storing its energy is potentially an important issue particularly in off-grid and for future 100% renewable energy scenarios to have continuous electricity availability.\n\nSolar electricity is inherently variable and predictable by time of day, location, and seasons. In addition solar is intermittent due to day/night cycles and unpredictable weather. How much of a special challenge solar power is in any given electric utility varies significantly. In a summer peak utility, solar is well matched to daytime cooling demands. In winter peak utilities, solar displaces other forms of generation, reducing their capacity factors.\n\nIn an electricity system without grid energy storage, generation from stored fuels (coal, biomass, natural gas, nuclear) must be go up and down in reaction to the rise and fall of solar electricity (see load following power plant). While hydroelectric and natural gas plants can quickly follow solar being intermittent due to the weather, coal, biomass and nuclear plants usually take considerable time to respond to load and can only be scheduled to follow the predictable variation. Depending on local circumstances, beyond about 20–40% of total generation, grid-connected intermittent sources like solar tend to require investment in some combination of grid interconnections, energy storage or demand side management. Integrating large amounts of solar power with existing generation equipment has caused issues in some cases. For example, in Germany, California and Hawaii, electricity prices have been known to go negative when solar is generating a lot of power, displacing existing baseload generation contracts.\n\nConventional hydroelectricity works very well in conjunction with solar power, water can be held back or released from a reservoir behind a dam as required. Where a suitable river is not available, pumped-storage hydroelectricity uses solar power to pump water to a high reservoir on sunny days then the energy is recovered at night and in bad weather by releasing water via a hydroelectric plant to a low reservoir where the cycle can begin again. \nHowever, this cycle can lose 20% of the energy to round trip inefficiencies, this plus the construction costs add to the expense of implementing high levels of solar power.\n\nConcentrated solar power plants may use thermal storage to store solar energy, such as in high-temperature molten salts. These salts are an effective storage medium because they are low-cost, have a high specific heat capacity, and can deliver heat at temperatures compatible with conventional power systems. This method of energy storage is used, for example, by the Solar Two power station, allowing it to store 1.44 TJ in its 68 m³ storage tank, enough to provide full output for close to 39 hours, with an efficiency of about 99%.\n\nIn stand alone PV systems batteries are traditionally used to store excess electricity. With grid-connected photovoltaic power system, excess electricity can be sent to the electrical grid. Net metering and feed-in tariff programs give these systems a credit for the electricity they produce. This credit offsets electricity provided from the grid when the system cannot meet demand, effectively trading with the grid instead of storing excess electricity. Credits are normally rolled over from month to month and any remaining surplus settled annually. \nWhen wind and solar are a small fraction of the grid power, other generation techniques can adjust their output appropriately, but as these forms of variable power grow, additional balance on the grid is needed. As prices are rapidly declining, PV systems increasingly use rechargeable batteries to store a surplus to be later used at night. Batteries used for grid-storage stabilize the electrical grid by leveling out peak loads usually for several minutes, and in rare cases for hours. In the future, less expensive batteries could play an important role on the electrical grid, as they can charge during periods when generation exceeds demand and feed their stored energy into the grid when demand is higher than generation.\n\nAlthough not permitted under the US National Electric Code, it is technically possible to have a “plug and play” PV microinverter. A recent review article found that careful system design would enable such systems to meet all technical, though not all safety requirements. There are several companies selling plug and play solar systems available on the web, but there is a concern that if people install their own it will reduce the enormous employment advantage solar has over fossil fuels.\n\nCommon battery technologies used in today's home PV systems include, the valve regulated lead-acid battery– a modified version of the conventional lead–acid battery, nickel–cadmium and lithium-ion batteries. Lead-acid batteries are currently the predominant technology used in small-scale, residential PV systems, due to their high reliability, low self discharge and investment and maintenance costs, despite shorter lifetime and lower energy density. However, lithium-ion batteries have the potential to replace lead-acid batteries in the near future, as they are being intensively developed and lower prices are expected due to economies of scale provided by large production facilities such as the Gigafactory 1. In addition, the Li-ion batteries of plug-in electric cars may serve as a future storage devices in a vehicle-to-grid system. Since most vehicles are parked an average of 95 percent of the time, their batteries could be used to let electricity flow from the car to the power lines and back. Other rechargeable batteries used for distributed PV systems include, sodium–sulfur and vanadium redox batteries, two prominent types of a molten salt and a flow battery, respectively.\n\nThe combination of wind and solar PV has the advantage that the two sources complement each other because the peak operating times for each system occur at different times of the day and year. The power generation of such solar hybrid power systems is therefore more constant and fluctuates less than each of the two component subsystems. Solar power is seasonal, particularly in northern/southern climates, away from the equator, suggesting a need for long term seasonal storage in a medium such as hydrogen or pumped hydroelectric. The Institute for Solar Energy Supply Technology of the University of Kassel pilot-tested a combined power plant linking solar, wind, biogas and hydrostorage to provide load-following power from renewable sources.\n\nResearch is also undertaken in this field of artificial photosynthesis. It involves the use of nanotechnology to store solar electromagnetic energy in chemical bonds, by splitting water to produce hydrogen fuel or then combining with carbon dioxide to make biopolymers such as methanol. Many large national and regional research projects on artificial photosynthesis are now trying to develop techniques integrating improved light capture, quantum coherence methods of electron transfer and cheap catalytic materials that operate under a variety of atmospheric conditions. Senior researchers in the field have made the public policy case for a Global Project on Artificial Photosynthesis to address critical energy security and environmental sustainability issues.\n\nUnlike fossil fuel based technologies, solar power does not lead to any harmful emissions during operation, but the production of the panels leads to some amount of pollution.\n\nThe life-cycle greenhouse-gas emissions of solar power are in the range of 22 to 46 gram (g) per kilowatt-hour (kWh) depending on if solar thermal or solar PV is being analyzed, respectively. With this potentially being decreased to 15 g/kWh in the future. For comparison (of weighted averages), a combined cycle gas-fired power plant emits some 400–599 g/kWh, an oil-fired power plant 893 g/kWh, a coal-fired power plant 915–994 g/kWh or with carbon capture and storage some 200 g/kWh, and a geothermal high-temp. power plant 91–122 g/kWh. \nThe life cycle emission intensity of hydro, wind and nuclear power are lower than solar's as of 2011 as published by the IPCC, and discussed in the article Life-cycle greenhouse-gas emissions of energy sources. Similar to all energy sources were their total life cycle emissions primarily lay in the construction and transportation phase, the switch to low carbon power in the manufacturing and transportation of solar devices would further reduce carbon emissions. BP Solar owns two factories built by Solarex (one in Maryland, the other in Virginia) in which all of the energy used to manufacture solar panels is produced by solar panels. A 1-kilowatt system eliminates the burning of approximately 170 pounds of coal, 300 pounds of carbon dioxide from being released into the atmosphere, and saves up to 105 gallons of water consumption monthly.\n\nThe US National Renewable Energy Laboratory (NREL), in harmonizing the disparate estimates of life-cycle GHG emissions for solar PV, found that the most critical parameter was the solar insolation of the site: GHG emissions factors for PV solar are inversely proportional to insolation. For a site with insolation of 1700 kWh/m2/year, typical of southern Europe, NREL researchers estimated GHG emissions of 45 ge/kWh. Using the same assumptions, at Phoenix, USA, with insolation of 2400 kWh/m2/year, the GHG emissions factor would be reduced to 32 g of COe/kWh.\n\nThe New Zealand Parliamentary Commissioner for the Environment found that the solar PV would have little impact on the country's greenhouse gas emissions. The country already generates 80 percent of its electricity from renewable resources (primarily hydroelectricity and geothermal) and national electricity usage peaks on winter evenings whereas solar generation peaks on summer afternoons, meaning a large uptake of solar PV would end up displacing other renewable generators before fossil-fueled power plants.\n\nThe energy payback time (EPBT) of a power generating system is the time required to generate as much energy as is consumed during production and lifetime operation of the system. Due to improving production technologies the payback time has been decreasing constantly since the introduction of PV systems in the energy market. \nIn 2000 the energy payback time of PV systems was estimated as 8 to 11 years and in 2006 this was estimated to be 1.5 to 3.5 years for crystalline silicon PV systems and 1–1.5 years for thin film technologies (S. Europe). These figures fell to 0.75–3.5 years in 2013, with an average of about 2 years for crystalline silicon PV and CIS systems.\n\nAnother economic measure, closely related to the energy payback time, is the energy returned on energy invested (EROEI) or energy return on investment (EROI), which is the ratio of electricity generated divided by the energy required to build \"and maintain\" the equipment. (This is not the same as the economic return on investment (ROI), which varies according to local energy prices, subsidies available and metering techniques.) With expected lifetimes of 30 years, the EROEI of PV systems are in the range of 10 to 30, thus generating enough energy over their lifetimes to reproduce themselves many times (6–31 reproductions) depending on what type of material, balance of system (BOS), and the geographic location of the system.\n\nSolar power includes plants with among the lowest water consumption per unit of electricity (photovoltaic), and also power plants with among the highest water consumption (concentrating solar power with wet-cooling systems).\n\nPhotovoltaic power plants use very little water for operations. Life-cycle water consumption for utility-scale operations is estimated to be 12 gallons per megawatt-hour for flat-panel PV solar. Only wind power, which consumes essentially no water during operations, has a lower water consumption intensity.\n\nConcentrating solar power plants with wet-cooling systems, on the other hand, have the highest water-consumption intensities of any conventional type of electric power plant; only fossil-fuel plants with carbon-capture and storage may have higher water intensities. A 2013 study comparing various sources of electricity found that the median water consumption during operations of concentrating solar power plants with wet cooling was 810 ga/MWhr for power tower plants and 890 gal/MWhr for trough plants. This was higher than the operational water consumption (with cooling towers) for nuclear (720 gal/MWhr), coal (530 gal/MWhr), or natural gas (210). A 2011 study by the National Renewable Energy Laboratory came to similar conclusions: for power plants with cooling towers, water consumption during operations was 865 gal/MWhr for CSP trough, 786 gal/MWhr for CSP tower, 687 gal/MWhr for coal, 672 gal/MWhr for nuclear, and 198 gal/MWhr for natural gas. The Solar Energy Industries Association noted that the Nevada Solar One trough CSP plant consumes 850 gal/MWhr. The issue of water consumption is heightened because CSP plants are often located in arid environments where water is scarce.\n\nIn 2007, the US Congress directed the Department of Energy to report on ways to reduce water consumption by CSP. The subsequent report noted that dry cooling technology was available that, although more expensive to build and operate, could reduce water consumption by CSP by 91 to 95 percent. A hybrid wet/dry cooling system could reduce water consumption by 32 to 58 percent. A 2015 report by NREL noted that of the 24 operating CSP power plants in the US, 4 used dry cooling systems. The four dry-cooled systems were the three power plants at the Ivanpah Solar Power Facility near Barstow, California, and the Genesis Solar Energy Project in Riverside County, California. Of 15 CSP projects under construction or development in the US as of March 2015, 6 were wet systems, 7 were dry systems, 1 hybrid, and 1 unspecified.\n\nAlthough many older thermoelectric power plants with once-through cooling or cooling ponds \"use\" more water than CSP, meaning that more water passes through their systems, most of the cooling water returns to the water body available for other uses, and they \"consume\" less water by evaporation. For instance, the median coal power plant in the US with once-through cooling uses 36,350 gal/MWhr, but only 250 gal/MWhr (less than one percent) is lost through evaporation. Since the 1970s, the majority of US power plants have used recirculating systems such as cooling towers rather than once-through systems.\n\nOne issue that has often raised concerns is the use of cadmium (Cd), a toxic heavy metal that has the tendency to accumulate in ecological food chains. It is used as semiconductor component in CdTe solar cells and as buffer layer for certain CIGS cells in the form of CdS. \nThe amount of cadmium used in thin-film PV modules is relatively small (5–10 g/m²) and with proper recycling and emission control techniques in place the cadmium emissions from module production can be almost zero. Current PV technologies lead to cadmium emissions of 0.3–0.9 microgram/kWh over the whole life-cycle. Most of these emissions arise through the use of coal power for the manufacturing of the modules, and coal and lignite combustion leads to much higher emissions of cadmium. Life-cycle cadmium emissions from coal is 3.1 microgram/kWh, lignite 6.2, and natural gas 0.2 microgram/kWh.\n\nIn a life-cycle analysis it has been noted, that if electricity produced by photovoltaic panels were used to manufacture the modules instead of electricity from burning coal, cadmium emissions from coal power usage in the manufacturing process could be entirely eliminated.\n\nIn the case of crystalline silicon modules, the solder material, that joins together the copper strings of the cells, contains about 36 percent of lead (Pb). Moreover, the paste used for screen printing front and back contacts contains traces of Pb and sometimes Cd as well. It is estimated that about 1,000 metric tonnes of Pb have been used for 100 gigawatts of c-Si solar modules. However, there is no fundamental need for lead in the solder alloy.\n\nSome media sources have reported that concentrated solar power plants have injured or killed large numbers of birds due to intense heat from the concentrated sunrays. This adverse effect does not apply to PV solar power plants, and some of the claims may have been overstated or exaggerated.\n\nA 2014-published life-cycle analysis of land use for various sources of electricity concluded that the large-scale implementation of solar and wind potentially reduces pollution-related\nenvironmental impacts. The study found that the land-use footprint, given in square meter-years per megawatt-hour (ma/MWh), was lowest for wind, natural gas and rooftop PV, with 0.26, 0.49 and 0.59, respectively, and followed by utility-scale solar PV with 7.9. For CSP, the footprint was 9 and 14, using parabolic troughs and solar towers, respectively. The largest footprint had coal-fired power plants with 18 ma/MWh.\n\nConcentrator photovoltaics (CPV) systems employ sunlight concentrated onto photovoltaic surfaces for the purpose of electrical power production. Contrary to conventional photovoltaic systems, it uses lenses and curved mirrors to focus sunlight onto small, but highly efficient, multi-junction solar cells. Solar concentrators of all varieties may be used, and these are often mounted on a solar tracker in order to keep the focal point upon the cell as the sun moves across the sky. Luminescent solar concentrators (when combined with a PV-solar cell) can also be regarded as a CPV system. Concentrated photovoltaics are useful as they can improve efficiency of PV-solar panels drastically.\n\nIn addition, most solar panels on spacecraft are also made of high efficient multi-junction photovoltaic cells to derive electricity from sunlight when operating in the inner Solar System.\n\nFloatovoltaics are an emerging form of PV systems that float on the surface of irrigation canals, water reservoirs, quarry lakes, and tailing ponds. Several systems exist in France, India, Japan, Korea, the United Kingdom and the United States. These systems reduce the need of valuable land area, save drinking water that would otherwise be lost through evaporation, and show a higher efficiency of solar energy conversion, as the panels are kept at a cooler temperature than they would be on land. Although not floating, other dual-use facilities with solar power include fisheries.\n\n\n"}
{"id": "3655251", "url": "https://en.wikipedia.org/wiki?curid=3655251", "title": "Sprung mass", "text": "Sprung mass\n\nSprung mass (or sprung weight), in a vehicle with a suspension, such as an automobile, motorcycle, or a tank, is the portion of the vehicle's total mass that is supported by the suspension, including in most applications approximately half of the weight of the suspension itself. The sprung mass typically includes the body, frame, the internal components, passengers, and cargo, but does not include the mass of the components at the other end of the suspension components (including the wheels, wheel bearings, brake rotors, calipers, and/or continuous tracks (also called caterpillar tracks), if any), which are part of the vehicle's unsprung mass.\n\nThe larger the ratio of sprung mass to unsprung mass, the less the body and vehicle occupants are affected by bumps, dips, and other surface imperfections such as small bridges. However, a large sprung mass to unsprung mass ratio can also be deleterious to vehicle control.\n"}
{"id": "20650838", "url": "https://en.wikipedia.org/wiki?curid=20650838", "title": "Superglass", "text": "Superglass\n\nA superglass is a phase of matter which is characterized (at the same time) by superfluidity and a frozen amorphous structure.\n\nJ.C. Séamus Davis theorised that frozen helium-4 (at 0.2 K and 50 Atm) may be a superglass.\n\n\n"}
{"id": "1561900", "url": "https://en.wikipedia.org/wiki?curid=1561900", "title": "Switchgear", "text": "Switchgear\n\nIn an electric power system, switchgear is the combination of electrical disconnect switches, fuses or circuit breakers used to control, protect and isolate electrical equipment. Switchgear is used both to de-energize equipment to allow work to be done and to clear faults downstream. This type of equipment is directly linked to the reliability of the electricity supply.\n\nThe earliest central power stations used simple open knife switches, mounted on insulating panels of marble or asbestos. Power levels and voltages rapidly escalated, making opening manually operated switches too dangerous for anything other than isolation of a de-energized circuit. Oil-filled equipment allowed arc energy to be contained and safely controlled. By the early 20th century, a switchgear line-up would be a metal-enclosed structure with electrically operated switching elements, using oil circuit breakers. Today, oil-filled equipment has largely been replaced by air-blast, vacuum, or SF equipment, allowing large currents and power levels to be safely controlled by automatic equipment.\n\nHigh-voltage switchgear was invented at the end of the 19th century for operating motors and other electric machines. The technology has been improved over time and can now be used with voltages up to 1,100 kV.\n\nTypically, switchgear in substations are located on both the high- and low-voltage sides of large power transformers. The switchgear on the low-voltage side of the transformers may be located in a building, with medium-voltage circuit breakers for distribution circuits, along with metering, control, and protection equipment. For industrial applications, a transformer and switchgear line-up may be combined in one housing, called a unitized substation (USS).\n\nA switchgear has 2 types of components:\n\nOne of the basic functions of switchgear is protection, which is interruption of short-circuit and overload fault currents while maintaining service to unaffected circuits. Switchgear also provides isolation of circuits from power supplies. Switchgear is also used to enhance system availability by allowing more than one source to feed a load.\n\nSwitchgears are as old as electricity generation. The first models were very primitive: all components were simply fixed to a wall. Later they were mounted on wooden panels. For reasons of fire protection, the wood was replaced by slate or marble. This led to a further improvement, because the switching and measuring devices could be attached to the front, while the wiring was on the back.\n\nSwitchgear for lower voltages may be entirely enclosed within a building. For higher voltages (over about 66 kV), switchgear is typically mounted outdoors and insulated by air, although this requires a large amount of space. Gas-insulated switchgear saves space compared with air-insulated equipment, although the equipment cost is higher. Oil insulated switchgear presents an oil spill hazard.\n\nSwitches may be manually operated or have motor drives to allow for remote control.\n\nA switchgear may be a simple open-air isolator switch or it may be insulated by some other substance. An effective although more costly form of switchgear is the gas-insulated switchgear (GIS), where the conductors and contacts are insulated by pressurized sulfur hexafluoride gas (SF). Other common types are oil or vacuum insulated switchgear.\n\nThe combination of equipment within the switchgear enclosure allows them to interrupt fault currents of thousands of amps. A circuit breaker (within a switchgear enclosure) is the primary component that interrupts fault currents. The quenching of the arc when the circuit breaker pulls apart the contacts (disconnects the circuit) requires careful design. Circuit breakers fall into these six types:\n\nOil circuit breakers rely upon vaporization of some of the oil to blast a jet of oil along the path of the arc. The vapor released by the arcing consists of hydrogen gas.\nMineral oil has better insulating property than air. Whenever there is a separation of current carrying contacts in the oil, the arc in circuit breaker is initialized at the moment of separation of contacts, and due to this arc the oil is vaporized and decomposed in mostly hydrogen gas and ultimately creates a hydrogen bubble around the electric arc. This highly compressed gas bubble around the arc prevents re-striking of the arc after current reaches zero crossing of the cycle. The oil circuit breaker is one of the oldest type of circuit breakers.\n\nAir circuit breakers may use compressed air (puff) or the magnetic force of the arc itself to elongate the arc. As the length of the sustainable arc is dependent on the available voltage, the elongated arc will eventually exhaust itself. Alternatively, the contacts are rapidly swung into a small sealed chamber, the escaping of the displaced air thus blowing out the arc.\n\nCircuit breakers are usually able to terminate all current flow very quickly: typically between 30 ms and 150 ms depending upon the age and construction of the device.\n\nGas (SF) circuit breakers sometimes stretch the arc using a magnetic field, and then rely upon the dielectric strength of the SF gas to quench the stretched arc.\n\nHybrid switchgear is a type which combines the components of traditional air-insulated switchgear (AIS) and SF gas-insulated switchgear (GIS) technologies. It is characterized by a compact and modular design, which encompasses several different functions in one module.\n\nCircuit breakers with vacuum interrupters have minimal arcing characteristics (as there is nothing to ionize other than the contact material), so the arc quenches when it is stretched by a small amount (<2–8 mm). Near zero current the arc is not hot enough to maintain a plasma, and current ceases; the gap can then withstand the rise of voltage. Vacuum circuit breakers are frequently used in modern medium-voltage switchgear to 40,500 volts. Unlike the other types, they are inherently unsuitable for interrupting DC faults. The reason vacuum circuit breakers are unsuitable for breaking high DC voltages is that with DC there is no \"current zero\" period. The plasma arc can feed itself by continuing to gasify the contact material.\n\nBreakers that use carbon dioxide as the insulating and arc extinguishing medium work on the same principles as a sulfur hexafluoride (SF) breaker. Because SF is a greenhouse gas more potent than CO, by switching from SF to CO it is possible to reduce the greenhouse gas emissions by 10 tons during the product lifecycle.\n\nCircuit breakers and fuses disconnect when current exceeds a predetermined safe level. However they cannot sense other critical faults, such as unbalanced currents—for example, when a transformer winding contacts ground. By themselves, circuit breakers and fuses cannot distinguish between short circuits and high levels of electrical demand.\n\nDifferential protection depends upon Kirchhoff's current law, which states that the sum of currents entering or leaving a circuit node must equal zero. Using this principle to implement differential protection, any section of a conductive path may be considered a node. The conductive path could be a transmission line, a winding of a transformer, a winding in a motor, or a winding in the stator of an alternator. This form of protection works best when both ends of the conductive path are physically close to each other. This scheme was invented in Great Britain by Charles Hesterman Merz and Bernard Price.\n\nTwo identical current transformers are used for each winding of a transformer, stator, or other device. The current transformers are placed around opposite ends of a winding. The current through both ends should be identical. A protective relay detects any imbalance in currents, and trips circuit breakers to isolate the device. In the case of a transformer, the circuit breakers on both the primary and secondary would open.\n\nA short circuit at the end of a long transmission line appears similar to a normal load, because the impedance of the transmission line limits the fault current. A distance relay detects a fault by comparing the voltage and current on the transmission line. A large current along with a voltage drop indicates a fault.\n\nSeveral different classifications of switchgear can be made:\n\n\nA single line-up may incorporate several different types of devices, for example, air-insulated bus, vacuum circuit breakers, and manually operated switches may all exist in the same row of cubicles.\n\nRatings, design, specifications and details of switchgear are set by a multitude of standards. In North America mostly IEEE and ANSI standards are used, much of the rest of the world uses IEC standards, sometimes with local national derivatives or variations.\n\nTo help ensure safe operation sequences of switchgear, trapped key interlocking provides predefined scenarios of operation. For example, if only one of two sources of supply are permitted to be connected at a given time, the interlock scheme may require that the first switch must be opened to release a key that will allow closing the second switch. Complex schemes are possible.\n\nIndoor switchgear can also be type tested for internal arc containment (e.g., IEC 62271-200). This test is important for user safety as modern switchgear is capable of switching large currents.\n\nSwitchgear is often inspected using thermal imaging to assess the state of the system and predict failures before they occur. Other methods include partial discharge (PD) testing, using either fixed or portable testers, and acoustic emission testing using surface-mounted transducers (for oil equipment) or ultrasonic detectors used in outdoor switchyards. Temperature sensors fitted to cables to the switchgear can permanently monitor temperature build-up. SF equipment is invariably fitted with alarms and interlocks to warn of loss of pressure, and to prevent operation if the pressure falls too low.\n\nThe increasing awareness of dangers associated with high fault levels has resulted in network operators specifying closed-door operations for earth switches and racking breakers. Many European power companies have banned operators from switch rooms while operating. Remote racking systems are available which allow an operator to rack switchgear from a remote location without the need to wear a protective arc flash hazard suit. Switchgear systems require continuous maintenance and servicing to remain safe to use and fully optimized to provide such high voltages.\n\n\n"}
{"id": "28022779", "url": "https://en.wikipedia.org/wiki?curid=28022779", "title": "Trett", "text": "Trett\n\nTrett (or tret) was an allowance made up until the early 19th century, for waste, dust, and other impurity in items in commerce, generally amounting to 4 pounds in each 104 pounds (3.85%). It fell into disuse because merchants preferred to simply adjust the price, rather than make a calculation for trett.\n\nTrett was most commonly used in calculating the true weight for imported commodities to Great Britain which would be sold by the pound avoirdupois. From the gross weight, tare weight would be deducted, to account for the weight of any container that the commodity was enclosed in. The remainder was known as the \"suttle\". Trett was deducted from the suttle at the rate of four pounds for every 104 pounds of commodity, to take account of any dust, sand, and other impurity included in the suttle. The remainder, after trett was deducted, became known as the \"neat weight\" or \"nett weight\", a term used in common for commodities in which no trett was to be deducted, but from which tare had been deducted.\n\nBy the early 19th century, trett had fallen into disuse,\nwith merchants preferring to make allowances in the price for any impurity, as such allowance was far easier than the arithmetic calculation for trett. It was not used at custom houses, nor was it available at the British East India Company's warehouses.\n\nTrett varied considerably from the British usage, for goods in Ireland. For most goods there, trett was deducted at the rate of 1 pound per 112 pounds (0.89%). However, in Cork, trett was deducted after tare for wool at the rate of 8 pounds per 20 stone (2.86%), and in Dublin 8 pounds per 21 stone (2.72%). Additionally in Dublin, trett was deducted at the rate of per large cask of butter, per raw hide, and per beef carcasse.\n\n"}
{"id": "474589", "url": "https://en.wikipedia.org/wiki?curid=474589", "title": "Ultra-high-energy cosmic ray", "text": "Ultra-high-energy cosmic ray\n\nIn astroparticle physics, an ultra-high-energy cosmic ray (UHECR) is a cosmic ray particle with a kinetic energy greater than eV, far beyond both the rest mass and energies typical of other cosmic ray particles.\n\nAn extreme-energy cosmic ray (EECR) is an UHECR with energy exceeding (about 8 joule), the so-called Greisen–Zatsepin–Kuzmin limit (GZK limit). This limit should be the maximum energy of cosmic ray protons that have traveled long distances (about 160 million light years), since higher-energy protons would have lost energy over that distance due to scattering from photons in the cosmic microwave background (CMB). It follows that EECR could not be survivors from the early universe, but are cosmologically \"young\", emitted somewhere in the Local Supercluster by some unknown physical process. If an EECR is not a proton, but a nucleus with formula_1 nucleons, then the GZK limit applies to its nucleons, which carry only a fraction formula_2 of the total energy of the nucleus. For an iron nucleus, the corresponding limit would be .\n\nThese particles are extremely rare; between 2004 and 2007, the initial runs of the Pierre Auger Observatory (PAO) detected 27 events with estimated arrival energies above , i.e., about one such event every four weeks in the 3000 km area surveyed by the observatory.\n\nThere is evidence that these highest-energy cosmic rays might be iron nuclei, rather than the protons that make up most cosmic rays.\n\nThe postulated (hypothetical) sources of EECR are known as Zevatrons, named in analogy to Lawrence Berkeley National Laboratory's Bevatron and Fermilab's Tevatron, and therefore capable of accelerating particles to 1 ZeV (10 eV, zetta-electronvolt). In 2004 there was a consideration of the possibility of galactic jets acting as Zevatrons, due to diffusive acceleration of particles caused by shock waves inside the jets. In particular, models suggested that shock waves from the nearby M87 galactic jet could accelerate an iron nucleus to ZeV ranges. In 2007, PAO tentatively associated EECR with extragalactic supermassive black holes at the center of nearby galaxies called active galactic nuclei (AGN). Extremely high energies might be explained also by the Centrifugal mechanism of acceleration in the magnetospheres of AGN. Although newer results indicate that fewer than 40% of these cosmic rays seemed to be coming from the AGN, a much weaker correlation than previously reported. A more speculative suggestion by Grib and Pavlov (2007, 2008) envisages the decay of superheavy dark matter by means of the Penrose process.\n\nThe first observation of a cosmic ray particle with an energy exceeding (16 J) was made by Dr John D Linsley and Livio Scarsi at the Volcano Ranch experiment in New Mexico in 1962.\n\nCosmic ray particles with even higher energies have since been observed. Among them was the Oh-My-God particle observed by the University of Utah's Fly's Eye experiment on the evening of 15 October 1991 over Dugway Proving Ground, Utah. Its observation was a shock to astrophysicists, who estimated its energy to be approximately (50 J)—in other words, an atomic nucleus with kinetic energy equal to that of a baseball () traveling at about .\n\nThe energy of this particle is some 40 million times that of the highest energy protons that have been produced in any terrestrial particle accelerator. However, only a small fraction of this energy would be available for an interaction with a proton or neutron on Earth, with most of the energy remaining in the form of kinetic energy of the products of the interaction (see Collider#Explanation). The effective energy available for such a collision is the square root of double the product of the particle's energy and the mass energy of the proton, which for this particle gives , roughly 50 times the collision energy of the Large Hadron Collider.\n\nSince the first observation, by the University of Utah's Fly's Eye Cosmic Ray Detector, at least fifteen similar events have been recorded, confirming the phenomenon. These very high energy cosmic ray particles are very rare; the energy of most cosmic ray particles is between 10 MeV and 10 GeV.\n\n\nPierre Auger Observatory is an international cosmic ray observatory designed to detect ultra-high-energy cosmic ray particles (with energies beyond 10 eV). These high-energy particles have an estimated arrival rate of just 1 per square kilometer per century, therefore, in order to record a large number of these events, the Auger Observatory has created a detection area of 3,000 km² (the size of Rhode Island, USA) in Mendoza Province, western Argentina.\nA larger cosmic-ray detector array is also planned for the northern hemisphere as part of the Pierre Auger complex.\nThe Pierre Auger Observatory, in addition to obtaining directional information from the cluster of water tanks used to observe the cosmic-ray-shower components, also has four telescopes trained on the night sky to observe fluorescence of the nitrogen molecules as the shower particles traverse the sky, giving further directional information on the original cosmic ray particle.\n\nIn September 2017, data from 12 years of observations from PAO supported an extragalactic source (e.g. outside of Earth's galaxy) for the origin of extremely high energy cosmic rays.\n\nOne suggested source of UHECR particles is their origination from neutron stars. In young neutron stars with spin periods of <10ms, the magnetohydrodynamic (MHD) forces from the quasi-neutral fluid of superconducting protons and electrons existing in a neutron superfluid accelerate iron nuclei to UHECR velocities. The magnetic field produced by the neutron superfluid in rapidly rotating stars creates a magnetic field of 10–10 tesla, at which point the neutron star is classified as a magnetar. This magnetic field is the strongest in the observed universe and creates the relativistic MHD wind believed to accelerate iron nuclei remaining from the supernova to the necessary energy.\n\nAnother hypothesized source of UHECRs from neutron stars is during neutron star to strange star combustion. This hypothesis relies on the assumption that strange matter is the ground state of matter which has no experimental or observational data to support it. Due to the immense gravitational pressures from the neutron star, it is believed that small pockets of matter consisting of \"up\", \"down\", and \"strange\" quarks in equilibrium acting as a single hadron (as opposed to a number of baryons). This will then combust the entire star to strange matter, at which point the neutron star becomes a strange star and its magnetic field breaks down, which occurs because the protons and neutrons in the quasi-neutral fluid have become strangelets. This magnetic field breakdown releases large amplitude electromagnetic waves (LAEMWs). The LAEMWs accelerate light ion remnants from the supernova to UHECR energies.\n\nVery high energy cosmic ray electrons might be explained by the Centrifugal mechanism of acceleration in the magnetospheres of the Crab-like Pulsars.\n\nInteractions with blue-shifted cosmic microwave background radiation limit the distance that these particles can travel before losing energy; this is known as the Greisen–Zatsepin–Kuzmin limit or GZK limit.\n\nThe source of such high energy particles has been a mystery for many years. Recent results from the Pierre Auger Observatory show that ultra-high-energy cosmic ray arrival directions appear to be correlated with extragalactic supermassive black holes at the center of nearby galaxies called active galactic nuclei (AGN).\nHowever, since the angular correlation scale used is fairly large (3.1 degrees) these results do not unambiguously identify the origins of such cosmic ray particles. The AGN could merely be closely associated with the actual sources, for example in galaxies or other astrophysical objects that are clumped with matter on large scales within 100 Mpc.\n\nSome of the supermassive black holes in AGN are known to be rotating, as in the Seyfert galaxy MCG 6-30-15 with time-variability in their inner accretion disks. Black hole spin is a potentially effective agent to drive UHECR production, provided ions are suitably launched to circumvent limiting factors deep within the galactic nucleus, notably curvature radiation and inelastic scattering with radiation from the inner disk. Low-luminosity, intermittent Seyfert galaxies may meet the requirements with the formation of a linear accelerator several light years away from the nucleus, yet within their extended ion tori whose UV radiation ensures a supply of ionic contaminants. The corresponding electric fields are small, on the order of 10 V/cm, whereby the observed UHECRs are indicative for the astronomical size of the source. Improved statistics by the Pierre Auger Observatory will be instrumental in identifying the presently tentative association of UHECRs (from the Local Universe) with Seyferts and LINERs.\n\nOther possible sources of the UHECR are:\n\n\nIt is hypothesized that active galactic nuclei are capable of converting dark matter into high energy protons. Yuri Pavlov and Andrey Grib at the Alexander Friedmann Laboratory for Theoretical Physics at St. Petersburg hypothesize that dark matter particles are about 15 times heavier than protons, and that they can decay into pairs of heavier virtual particles of a type that interacts with ordinary matter. Near an active galactic nucleus, one of these particles can fall into the black hole, while the other escapes, as described by the Penrose process. Some of those particles will collide with incoming particles; these are very high energy collisions which, according to Pavlov, can form ordinary visible protons with very high energy. Pavlov then claims that evidence of such processes are ultra-high-energy cosmic ray particles. Ultra-high energy cosmic ray particles may also be produced by the decay of super-heavy dark matter \"X particles\" such as Holeums. Such very energetic decay products, carrying a fraction of the mass of the X particle, are believed to be a plausible explanation for the observed ultra-high energy cosmic rays (UHECR).\nHigh energy cosmic ray particles traversing intergalactic space suffer the GZK cutoff above 10 eV due to interactions with cosmic background radiation if the primary cosmic ray particles are protons or nuclei. The Pierre Auger Project, HiRes and Yakutsk Extensive Air Shower Array found the GZK cutoff, while Akeno-AGASA observed the events above the cutoff (11 events in the past 10 years). The result of the Akeno-AGASA experiment is smooth near the GZK cutoff energy. If one assumes that the Akeno-AGASA result is correct and consider its implication, a possible explanation for the AGASA data on GZK cutoff violation would be a shower caused by dark matter particles. A dark matter particle is not constrained by the GZK cutoff, since it interacts weakly with cosmic background radiation. Recent measurements by the Pierre Auger Project have found a correlation between the direction of high energy cosmic ray particles and the location of AGN.\n\n\n\n"}
{"id": "21546940", "url": "https://en.wikipedia.org/wiki?curid=21546940", "title": "Waste heat recovery unit", "text": "Waste heat recovery unit\n\nA waste heat recovery unit (WHRU) is an energy recovery heat exchanger that transfers heat from process outputs at high temperature to another part of the process for some purpose, usually increased efficiency. The WHRU is a tool involved in cogeneration. Waste heat may be extracted from sources such as hot flue gases from a diesel generator, steam from cooling towers, or even waste water from cooling processes such as in steel cooling.\n\nWaste heat found in the exhaust gas of various processes or even from the exhaust stream of a conditioning unit can be used to preheat the incoming gas. This is one of the basic methods for recovery of waste heat. Many steel making plants use this process as an economic method to increase the production of the plant with lower fuel demand.\nThere are many different commercial recovery units for the transferring of energy from hot medium space to lower one:\nParticulate Filters (DPF) to capture emission by maintaining higher temperatures adjacent to the converter and tail pipes to reduce the amount of emissions from the exhaust.\n\nElectric Turbo Compounding (ETC) is a technology solution to the challenge of improving energy efficiency for the stationary power generation industry.\n\nFossil fuel based power generation is predicted to continue for decades, especially in developing economies. This is against the global need to reduce carbon emissions, of which, a high percentage is produced by the power sector worldwide.\n\nETC works by making gas and diesel-powered gensets (Electric Generators) work more effectively and cleaner, by recovering waste energy from the exhaust to improve power density and fuel efficiency.\n\n\n\nAccording to a report done by Energetics Incorporated for the DOE in November 2004 titled \"Technology Roadmap\" and several others done by the European commission, the majority of energy production from conventional and renewable resources are lost to the atmosphere due to onsite (equipment inefficiency and losses due to waste heat) and offsite (cable and transformers losses) losses, that sums to be around 66% loss in electricity value. \nWaste heat of different degrees could be found in final products of a certain process or as a by-product in industry such as the slag in steelmaking plants. Units or devices that could recover the waste heat and transform it into electricity are called WHRUs or heat to power units. For example, an Organic Rankine cycle unit uses an organic fluid as the working fluid. The fluid has a lower boiling point than water to allow it to boil at low temperature, to form a superheated gas that could drive the blade of a turbine and thus a generator. Thermoelectric (Seebeck, Peltier, Thomson effects) units may also be called WHRU, since they use the heat differential between two plates to produce DC Power.\n\nShape-memory alloys can also be used to recover low temperature waste heat and convert it to mechanical action or electricity. \nA WHRB is different from a Heat Recovery Steam Generator (HRSG) in the sense that the heated medium does not change phase.\n\n\nAdvantages:\n\nThese systems have many benefits which could be direct or indirect.\n\n\nDisadvantages:\n\n\nThe Cyclone waste heat engine is designed to generate electricity from recovered waste heat energy using a steam cycle.\n\nInternational Wastewater Systems is another company addressing waste heat recovery systems. Focused on multi-unit residential, publicly shared buildings, industrial applications and district energy systems, their systems use the energy in waste water for domestic hot water production, building space heating and cooling.\n\nMotorsport series Formula One introduced waste heat recovery units in 2014 under the name MGU-H. The MGU-H will be abandoned for the 2021 due to development costs.\n\n\nWaste Heat Recovery Unit based on Organic Rankine Cycle. ENERBASQUE (http://www.enerbasque.com)\n"}
